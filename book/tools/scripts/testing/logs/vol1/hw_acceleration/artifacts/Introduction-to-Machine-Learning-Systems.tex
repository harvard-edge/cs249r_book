% Options for packages loaded elsewhere
% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode,linktoc=all,pdfwindowui,pdfpagemode=FullScreen,pdfpagelayout=TwoPageRight}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  9pt,
  letterpaper,
  abstract,
  titlepage]{scrbook}
\usepackage{xcolor}
\usepackage[left=1in,marginparwidth=2.0666666666667in,textwidth=4.1333333333333in,marginparsep=0.3in]{geometry}
\usepackage{amsmath,amssymb}
\setcounter{secnumdepth}{3}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
% Make \paragraph and \subparagraph free-standing
\makeatletter
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}{
    \@ifstar
      \xxxParagraphStar
      \xxxParagraphNoStar
  }
  \newcommand{\xxxParagraphStar}[1]{\oldparagraph*{#1}\mbox{}}
  \newcommand{\xxxParagraphNoStar}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}{
    \@ifstar
      \xxxSubParagraphStar
      \xxxSubParagraphNoStar
  }
  \newcommand{\xxxSubParagraphStar}[1]{\oldsubparagraph*{#1}\mbox{}}
  \newcommand{\xxxSubParagraphNoStar}[1]{\oldsubparagraph{#1}\mbox{}}
\fi
\makeatother

\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{242,244,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{#1}}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.82,0.10,0.26}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{#1}}
\newcommand{\BuiltInTok}[1]{\textcolor[rgb]{0.88,0.40,0.10}{#1}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.41,0.45,0.49}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.86,0.20,0.18}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.58,0.00,0.30}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{#1}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.75,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.88,0.40,0.10}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.42,0.37,0.78}{#1}}
\newcommand{\ImportTok}[1]{\textcolor[rgb]{0.00,0.45,0.70}{\textbf{#1}}}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.73,0.49,0.84}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.10,0.10,0.10}{#1}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{#1}}
\newcommand{\RegionMarkerTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.12,0.55,0.30}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{#1}}}}

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\newsavebox\pandoc@box
\newcommand*\pandocbounded[1]{% scales image to fit in text height/width
  \sbox\pandoc@box{#1}%
  \Gscale@div\@tempa{\textheight}{\dimexpr\ht\pandoc@box+\dp\pandoc@box\relax}%
  \Gscale@div\@tempb{\linewidth}{\wd\pandoc@box}%
  \ifdim\@tempb\p@<\@tempa\p@\let\@tempa\@tempb\fi% select the smaller of both
  \ifdim\@tempa\p@<\p@\scalebox{\@tempa}{\usebox\pandoc@box}%
  \else\usebox{\pandoc@box}%
  \fi%
}
% Set default figure placement to htbp
\def\fps@figure{htbp}
\makeatother
% definitions for citeproc citations
\NewDocumentCommand\citeproctext{}{}
\NewDocumentCommand\citeproc{mm}{%
  \begingroup\def\citeproctext{#2}\cite{#1}\endgroup}
\makeatletter
 % allow citations to break across lines
 \let\@cite@ofmt\@firstofone
 % avoid brackets around text for \cite:
 \def\@biblabel#1{}
 \def\@cite#1#2{{#1\if@tempswa , #2\fi}}
\makeatother
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newenvironment{CSLReferences}[2] % #1 hanging-indent, #2 entry-spacing
 {\begin{list}{}{%
  \setlength{\itemindent}{0pt}
  \setlength{\leftmargin}{0pt}
  \setlength{\parsep}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1
   \setlength{\leftmargin}{\cslhangindent}
   \setlength{\itemindent}{-1\cslhangindent}
  \fi
  % set entry spacing
  \setlength{\itemsep}{#2\baselineskip}}}
 {\end{list}}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{\hfill\break\parbox[t]{\linewidth}{\strut\ignorespaces#1\strut}}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}

% =============================================================================
% LATEX HEADER CONFIGURATION FOR MLSYSBOOK PDF
% =============================================================================
% This file contains all LaTeX package imports, custom commands, and styling
% definitions for the PDF output of the Machine Learning Systems textbook.
%
% Key Features:
% - Harvard crimson branding throughout
% - Custom part/chapter/section styling
% - Professional table formatting with colored headers
% - Margin notes with custom styling
% - TikZ-based part dividers
% - Page numbering (Roman for frontmatter, Arabic for mainmatter)
%
% Note: This file is included via _quarto-pdf.yml and affects PDF output only.
% HTML/EPUB styling is handled separately via CSS files.
% =============================================================================

% =============================================================================
% PACKAGE IMPORTS
% =============================================================================

% Layout and positioning
% \usepackage[outercaption, ragged]{sidecap}  % Commented out to make figure captions inline instead of in margin
\usepackage{adjustbox}      % Adjusting box dimensions
\usepackage{afterpage}      % Execute commands after page break
\usepackage{morefloats}     % Increase number of floats
\usepackage{array}          % Enhanced table column formatting
\usepackage{atbegshi}       % Insert content at page beginning
%\usepackage{changepage}     % Change page dimensions mid-document
\usepackage{emptypage}      % Clear headers/footers on empty pages

% Language and text
\usepackage[english]{babel} % English language support
\usepackage{microtype}      % Improved typography and hyphenation

% Captions and floats
\usepackage{caption}
% Caption styling configuration
%\captionsetup[table]{belowskip=5pt}
\captionsetup{format=plain}
\DeclareCaptionLabelFormat{mylabel}{#1
#2:\hspace{1.0ex}}
\DeclareCaptionFont{ninept}{\fontsize{7pt}{8}\selectfont #1}

% Figure captions: Small font, bold label, ragged right
\captionsetup[figure]{labelfont={bf,ninept},labelsep=space,
belowskip=2pt,aboveskip=6pt,labelformat=mylabel,
justification=raggedright,singlelinecheck=false,font={ninept}}

% Table captions: Small font, bold label, ragged right
\captionsetup[table]{belowskip=6pt,labelfont={bf,ninept},labelsep=none,
labelformat=mylabel,justification=raggedright,singlelinecheck=false,font={ninept}}

% Typography fine-tuning
\emergencystretch=5pt       % Allow extra stretch to avoid overfull boxes

% Utility packages
\usepackage{etoolbox}       % For patching commands and environments

% Page layout and headers
\usepackage{fancyhdr}       % Custom headers and footers
\usepackage{geometry}       % Page dimensions and margins

% Graphics and figures
\usepackage{graphicx}       % Include graphics
\usepackage{float}          % Improved float placement
\usepackage[skins,breakable]{tcolorbox} % Coloured and framed text boxes
\tcbset{before upper=\setlength{\parskip}{3pt}}

% Tables
\usepackage{longtable}      % Multi-page tables

% Fonts and typography
\usepackage{fontspec}       % Font selection for LuaLaTeX
\usepackage{mathptmx}       % Times-like math fonts
\usepackage{newpxtext}      % Palatino-like font for body text

% Colors and visual elements
\usepackage[dvipsnames]{xcolor}  % Extended color support
\usepackage{tikz}           % Programmatic graphics
\usetikzlibrary{positioning}
\usetikzlibrary{calc}
\usepackage{tikzpagenodes}  % TikZ positioning relative to page

% Code listings
\usepackage{listings}       % Code highlighting

% Hyperlinks
\usepackage{hyperref}       % Clickable links in PDF

% Conditional logic
\usepackage{ifthen}         % If-then-else commands

% Math symbols
\usepackage{amsmath}        % AMS math extensions
\usepackage{amssymb}        % AMS math symbols
\usepackage{latexsym}       % Additional LaTeX symbols
\usepackage{pifont}         % Zapf Dingbats symbols
\providecommand{\blacklozenge}{\ding{117}}  % Black diamond symbol

% Lists
\usepackage{enumitem}       % Customizable lists

% Margin notes and sidenotes
\usepackage{marginfix}      % Fixes margin note overflow
\usepackage{marginnote}     % Margin notes
\usepackage{sidenotes}      % Academic-style sidenotes
\renewcommand\raggedrightmarginnote{\sloppy}
\renewcommand\raggedleftmarginnote{\sloppy}

% Typography improvements
\usepackage{ragged2e}       % Better ragged text
\usepackage[all]{nowidow}   % Prevent widows and orphans
\usepackage{needspace}      % Ensure minimum space on page

% Section formatting
\usepackage[explicit]{titlesec}  % Custom section titles
\usepackage{tocloft}        % Table of contents formatting

% QR codes and icons
\usepackage{fontawesome5}   % Font Awesome icons
\usepackage{qrcode}         % QR code generation
\qrset{link, height=15mm}

% =============================================================================
% FLOAT CONFIGURATION
% =============================================================================
% Allow more floats per page to handle figure-heavy chapters
\extrafloats{200}
\setcounter{topnumber}{12}       % Max floats at top of page
\setcounter{bottomnumber}{12}    % Max floats at bottom of page
\setcounter{totalnumber}{24}     % Max floats per page
\setcounter{dbltopnumber}{8}     % Max floats at top of two-column page
\renewcommand{\topfraction}{.95}  % Max fraction of page for top floats
\renewcommand{\bottomfraction}{.95}
\renewcommand{\textfraction}{.05}  % Min fraction of page for text
\renewcommand{\floatpagefraction}{.7}  % Min fraction of float page
\renewcommand{\dbltopfraction}{.95}

% Prevent "Float(s) lost" errors by flushing floats more aggressively
\usepackage{placeins}  % Provides \FloatBarrier

% =============================================================================
% COLOR DEFINITIONS
% =============================================================================
% Harvard crimson - primary brand color used throughout
\definecolor{crimson}{HTML}{A51C30}

% Quiz element colors
\definecolor{quiz-question-color1}{RGB}{225,243,248}  % Light blue background
\definecolor{quiz-question-color2}{RGB}{17,158,199}   % Blue border
\definecolor{quiz-answer-color1}{RGB}{250,234,241}    % Light pink background
\definecolor{quiz-answer-color2}{RGB}{152,14,90}      % Magenta border

% =============================================================================
% LIST FORMATTING
% =============================================================================
% Tighter list spacing for academic style
\def\tightlist{}
\setlist{itemsep=1pt, parsep=1pt, topsep=0pt,after={\vspace{0.3\baselineskip}}}
\let\tightlist\relax

\makeatletter
\@ifpackageloaded{framed}{}{\usepackage{framed}}
\@ifpackageloaded{fancyvrb}{}{\usepackage{fancyvrb}}
\makeatother

\makeatletter
%New float "codelisting" has been updated
\AtBeginDocument{%
\floatstyle{ruled}
\newfloat{codelisting}{!htb}{lop}
\floatname{codelisting}{Listing}
\floatplacement{codelisting}{!htb}
\captionsetup[codelisting]{labelfont={bf,ninept},labelformat=mylabel,
  singlelinecheck=false,width=\linewidth,labelsep=none,font={ninept}}%
\renewenvironment{snugshade}{%
   \def\OuterFrameSep{3pt}%
   \def\FrameCommand{\fboxsep=5pt\colorbox{shadecolor}}%
   \MakeFramed{\advance\hsize-\width\FrameRestore}%
   \leftskip 0.5em \rightskip 0.5em%
   \small% decrease font size
   }{\endMakeFramed}%
}
\makeatother

%The space before and after the verbatim environment "Highlighting" has been reduced
\fvset{listparameters=\setlength{\topsep}{0pt}\setlength{\partopsep}{0pt}}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{framesep=0mm,commandchars=\\\{\}}

\makeatletter
\renewcommand\fs@ruled{\def\@fs@cfont{\bfseries}\let\@fs@capt\floatc@ruled
\def\@fs@pre{\hrule height.8pt depth0pt \kern2pt}%
\def\@fs@post{\kern2pt\hrule\relax}%
\def\@fs@mid{\kern2pt\hrule\kern1pt}%space between float and caption
\let\@fs@iftopcapt\iftrue}
\makeatother


% =============================================================================
% HYPHENATION RULES
% =============================================================================
% Explicit hyphenation points for technical terms to avoid bad breaks
\hyphenation{
  light-weight
  light-weight-ed
  de-vel-op-ment
  un-der-stand-ing
  mod-els
  prin-ci-ples
  ex-per-tise
  com-pli-cat-ed
  blue-print
  per‧for‧mance
  com-mu-ni-ca-tion
  par-a-digms
  hy-per-ten-sion
  a-chieved
}

% =============================================================================
% CODE LISTING CONFIGURATION
% =============================================================================
% Settings for code blocks using listings package
\lstset{
breaklines=true,              % Automatic line wrapping
breakatwhitespace=true,       % Break at whitespace only
basicstyle=\ttfamily,         % Monospace font
frame=none,                   % No frame around code
keepspaces=true,              % Preserve spaces
showspaces=false,             % Don't show space characters
showtabs=false,               % Don't show tab characters
columns=flexible,             % Flexible column width
belowskip=0pt,               % Minimal spacing
aboveskip=0pt
}

% =============================================================================
% PAGE GEOMETRY
% =============================================================================
% MIT Press trim size: 7" x 10" (per publisher specifications)
% This is a standard academic textbook format providing good readability
% for technical content with figures and code blocks.
% Wide outer margin accommodates sidenotes/margin notes.
\geometry{
  paperwidth=7in,
  paperheight=10in,
  top=0.875in,
  bottom=0.875in,
  inner=0.875in,              % Inner margin (binding side)
  outer=1.75in,               % Outer margin (includes space for sidenotes)
  footskip=30pt,
  marginparwidth=1.25in,      % Width for margin notes
  twoside                     % Different left/right pages
}

% =============================================================================
% SIDENOTE STYLING
% =============================================================================
% Custom sidenote design with crimson vertical bar
\renewcommand{\thefootnote}{\textcolor{crimson}{\arabic{footnote}}}

% Save original sidenote command
\makeatletter
\@ifundefined{oldsidenote}{
  \let\oldsidenote\sidenote%
}{}
\makeatother

% Redefine sidenote with vertical crimson bar
\renewcommand{\sidenote}[1]{%
  \oldsidenote{%
    \noindent
    \color{crimson!100}                        % Crimson vertical line
    \raisebox{0em}{%
      \rule{0.5pt}{1.5em}                      % Thin vertical line
    }
    \hspace{0.3em}                             % Space after line
    \color{black}                              % Reset text color
    \footnotesize #1                           % Sidenote content
  }%
}

% =============================================================================
% FLOAT HANDLING
% =============================================================================
% Patch LaTeX's output routine to handle float overflow gracefully
% The "Float(s) lost" error occurs in \@doclearpage when \@currlist is not empty
% This patch silently clears pending floats that can't be placed
\makeatletter
\let\orig@doclearpage\@doclearpage
\def\@doclearpage{%
  \ifx\@currlist\@empty\else
    \global\let\@currlist\@empty
    \typeout{Warning: Floats cleared to prevent overflow}%
  \fi
  \orig@doclearpage
}
\makeatother

% Additional safety for structural commands
\let\originalbackmatter\backmatter
\renewcommand{\backmatter}{%
  \clearpage%
  \originalbackmatter%
}

\let\originalfrontmatter\frontmatter
\renewcommand{\frontmatter}{%
  \clearpage%
  \originalfrontmatter%
}

\let\originalmainmatter\mainmatter
\renewcommand{\mainmatter}{%
  \clearpage%
  \originalmainmatter%
}

% =============================================================================
% PAGE HEADERS AND FOOTERS
% =============================================================================
% Ensure chapters use fancy page style (not plain)
\patchcmd{\chapter}{\thispagestyle{plain}}{\thispagestyle{fancy}}{}{}

% Main page style with crimson headers
\pagestyle{fancy}
\fancyhf{}                                              % Clear all
\fancyhead[LE]{\small\color{crimson}\nouppercase{\rightmark}}  % Left even: section
\fancyhead[RO]{\color{crimson}\thepage}                 % Right odd: page number
\fancyhead[LO]{\small\color{crimson}\nouppercase{\leftmark}}   % Left odd: chapter
\fancyhead[RE]{\color{crimson}\thepage}                 % Right even: page number
\renewcommand{\headrulewidth}{0.4pt}                    % Thin header line
\renewcommand{\footrulewidth}{0pt}                      % No footer line

% Plain page style (for chapter openings)
\fancypagestyle{plain}{
  \fancyhf{}
  \fancyfoot[C]{\color{crimson}\thepage}                % Centered page number
  \renewcommand{\headrulewidth}{0pt}
  \renewcommand{\footrulewidth}{0pt}
}

% =============================================================================
% KOMA-SCRIPT FONT ADJUSTMENTS
% =============================================================================
% Apply crimson color to all heading levels
\addtokomafont{disposition}{\rmfamily\color{crimson}}
\addtokomafont{chapter}{\color{crimson}}
\addtokomafont{section}{\color{crimson}}
\addtokomafont{subsection}{\color{crimson}}

% =============================================================================
% ABSTRACT ENVIRONMENT
% =============================================================================
\newenvironment{abstract}{
  \chapter*{\abstractname}
  \addcontentsline{toc}{chapter}{\abstractname}
  \small
}{
  \clearpage
}

% =============================================================================
% HYPERLINK CONFIGURATION
% =============================================================================
% Crimson-colored links throughout, two-page PDF layout
\hypersetup{
  linkcolor=crimson,
  citecolor=crimson,
  urlcolor=crimson,
  pdfpagelayout=TwoPageRight,   % Two-page spread view
  pdfstartview=Fit               % Initial zoom fits page
}

% =============================================================================
% PART SUMMARY SYSTEM
% =============================================================================
% Allows adding descriptive text below part titles
\newcommand{\partsummary}{}     % Empty by default
\newif\ifhaspartsummary%
\haspartsummaryfalse%

\newcommand{\setpartsummary}[1]{%
  \renewcommand{\partsummary}{#1}%
  \haspartsummarytrue%
}

% Additional colors for part page backgrounds
\definecolor{BrownLL}{RGB}{233,222,220}
\definecolor{BlueDD}{RGB}{62,100,125}
\colorlet{BlueDD}{magenta}

% ===============================================================================
% PART STYLING SYSTEM
% ===============================================================================
%
% This system provides three distinct visual styles for book organization:
%
% 1. NUMBERED PARTS (\part{title}) - For main book sections
%    - Roman numerals (I, II, III, etc.) in top right corner
%    - Crimson title with horizontal lines above/below
%    - "Part I" label in sidebar
%    - Used for: foundations, principles, optimization, deployment, etc.
%
% 2. UNNUMBERED PARTS (\part*{title}) - For special sections like "Labs"
%    - Division-style geometric background (left side)
%    - No Roman numerals
%    - Used for: labs section
%
% 3. DIVISIONS (\division{title}) - For major book divisions
%    - Clean geometric background with centered title
%    - Used for: frontmatter, main_content, backmatter
%
% The Lua filter (inject-parts.lua) automatically routes parts by {key:xxx} commands
% to the appropriate LaTeX command based on the key name.
% ===============================================================================

% NUMBERED PARTS: Roman numeral styling for main book sections
\titleformat{\part}[display]
{\thispagestyle{empty}}{}{20pt}{
\begin{tikzpicture}[remember picture,overlay]
%%%
%%
\node[crimson,align=flush right,
inner sep=0,outer sep=0mm,draw=none,%
anchor=east,minimum height=31mm, text width=1.2\textwidth,
yshift=-30mm,font={%
\fontsize{98pt}{104}\selectfont\bfseries}]  (BG) at (current page text area.north east){\thepart};
%
\node[black,inner sep=0mm,draw=none,
anchor=mid,text width=1.2\textwidth,
 minimum height=35mm, align=right,
node distance=7mm,below=of BG,
font={\fontsize{30pt}{34}\selectfont}]
(BGG)  {\hyphenchar\font=-1 \color{black}\MakeUppercase {#1}};
\draw [crimson,line width=3pt] ([yshift=0mm]BGG.north west) -- ([yshift=0mm]BGG.north east);
\draw [crimson,line width=2pt] ([yshift=0mm]BGG.south west) -- ([yshift=0mm]BGG.south east);
%
\node[fill=crimson,text=white,rotate=90,%
anchor=south west,minimum height=15mm,
minimum width=40mm,font={%
\fontsize{20pt}{20}\selectfont\bfseries}](BP)  at
(current page text area.south east)
{{\sffamily Part}~\thepart};
%
\path[red](BP.north west)-|coordinate(PS)(BGG.south west);
%
% Part summary box commented out for cleaner design
% \ifhaspartsummary
% \node[inner sep=4pt,text width=0.7\textwidth,draw=none,fill=BrownLL!40,
% align=justify,font={\fontsize{9pt}{12}\selectfont},anchor=south west]
% at (PS) {\partsummary};
% \fi
\end{tikzpicture}
}[]

\renewcommand{\thepart}{\Roman{part}}

% UNNUMBERED PARTS: Division-style background for special sections
\titleformat{name=\part,numberless}[display]
{\thispagestyle{empty}}{}{20pt}{
\begin{tikzpicture}[remember picture,overlay]
%%%
\coordinate(S1)at([yshift=-200mm]current page.north west);
\draw[draw=none,fill=BlueDD!7](S1)--++(45:16)coordinate(S2)-
|(S2|-current page.north west)--(current page.north west)coordinate(S3)--(S1);
%
\coordinate(E1)at([yshift=-98mm]current page.north west);
\draw[draw=none,fill=BlueDD!15](E1)--(current page.north west)coordinate(E2)
--++(0:98mm)coordinate(E3)--(E1);
%
\coordinate(D1)at([yshift=15mm]current page.south west);
\draw[draw=none,fill=BlueDD!40,opacity=0.5](D1)--++(45:5.5)coordinate(D2)
-|(D2|-current page.north west)--(current page.north west)coordinate(D3)--(D1);
%%%%
\path[red](S2)-|(S2-|current page.east)coordinate(SS2);
%PART
\node[crimson,align=flush right,inner sep=0,outer sep=0mm,draw=none,anchor=south,
font={\fontsize{48pt}{48}\selectfont\bfseries}]  (BG) at ($(S2)!0.5!(SS2)$){\hphantom{Part}};
%%%
\path[green]([yshift=15mm]D2)-|coordinate(TPD)(BG.south east);
\node[inner sep=0mm,draw=none,anchor=south east,%text width=0.9\textwidth,
align=right,font={\fontsize{40pt}{40}\selectfont}]
(BGG) at (TPD)  {\color{crimson}\MakeUppercase {#1}};%\MakeUppercase {}
\end{tikzpicture}
}

% Define \numberedpart command for numbered parts
\newcommand{\numberedpart}[1]{%
\FloatBarrier%  % Flush all pending floats before part break
\clearpage
\thispagestyle{empty}
\stepcounter{part}%
\begin{tikzpicture}[remember picture,overlay]
%%%
%%
\node[crimson,align=flush right,
inner sep=0,outer sep=0mm,draw=none,%
anchor=east,minimum height=31mm, text width=1.2\textwidth,
yshift=-30mm,font={%
\fontsize{98pt}{104}\selectfont\bfseries}]  (BG) at (current page text area.north east){\thepart};
%
\node[black,inner sep=0mm,draw=none,
anchor=mid,text width=1.2\textwidth,
 minimum height=35mm, align=right,
node distance=7mm,below=of BG,
font={\fontsize{30pt}{34}\selectfont}]
(BGG)  {\hyphenchar\font=-1 \color{black}\MakeUppercase {#1}};
\draw [crimson,line width=3pt] ([yshift=0mm]BGG.north west) -- ([yshift=0mm]BGG.north east);
\draw [crimson,line width=2pt] ([yshift=0mm]BGG.south west) -- ([yshift=0mm]BGG.south east);
%
\node[fill=crimson,text=white,rotate=90,%
anchor=south west,minimum height=15mm,
minimum width=40mm,font={%
\fontsize{20pt}{20}\selectfont\bfseries}](BP)  at
(current page text area.south east)
{{\sffamily Part}~\thepart};
%
\path[red](BP.north west)-|coordinate(PS)(BGG.south west);
%
% Part summary box commented out for cleaner design
% \ifhaspartsummary
% \node[inner sep=4pt,text width=0.7\textwidth,draw=none,fill=BrownLL!40,
% align=justify,font={\fontsize{9pt}{12}\selectfont},anchor=south west]
% at (PS) {\partsummary};
% \fi
\end{tikzpicture}
\clearpage
}



% DIVISIONS: Clean geometric styling with subtle tech elements
% Used for frontmatter, main_content, and backmatter divisions
\newcommand{\division}[1]{%
\FloatBarrier%  % Flush all pending floats before division break
\clearpage
\thispagestyle{empty}
\begin{tikzpicture}[remember picture,overlay]

% Clean geometric background (original design)
\coordinate(S1)at([yshift=-200mm]current page.north west);
\draw[draw=none,fill=BlueDD!7](S1)--++(45:16)coordinate(S2)-
|(S2|-current page.north west)--(current page.north west)coordinate(S3)--(S1);

\coordinate(E1)at([yshift=-98mm]current page.north west);
\draw[draw=none,fill=BlueDD!15](E1)--(current page.north west)coordinate(E2)
--++(0:98mm)coordinate(E3)--(E1);

\coordinate(D1)at([yshift=15mm]current page.south west);
\draw[draw=none,fill=BlueDD!40,opacity=0.5](D1)--++(45:5.5)coordinate(D2)
-|(D2|-current page.north west)--(current page.north west)coordinate(D3)--(D1);

% Subtle tech elements - positioned in white areas for better visibility
% Upper right white area - more visible
\draw[crimson!40, line width=0.8pt] ([xshift=140mm,yshift=-60mm]current page.north west) -- ++(40mm,0);
\draw[crimson!40, line width=0.8pt] ([xshift=150mm,yshift=-70mm]current page.north west) -- ++(30mm,0);
\draw[crimson!35, line width=0.7pt] ([xshift=160mm,yshift=-60mm]current page.north west) -- ++(0,-15mm);
\draw[crimson!35, line width=0.7pt] ([xshift=170mm,yshift=-70mm]current page.north west) -- ++(0,10mm);

% Circuit nodes - upper right
\fill[crimson!50] ([xshift=160mm,yshift=-60mm]current page.north west) circle (1.5mm);
\fill[white] ([xshift=160mm,yshift=-60mm]current page.north west) circle (0.8mm);
\fill[crimson!50] ([xshift=170mm,yshift=-70mm]current page.north west) circle (1.3mm);
\fill[white] ([xshift=170mm,yshift=-70mm]current page.north west) circle (0.6mm);

% Lower right white area - enhanced visibility
\draw[crimson!45, line width=0.9pt] ([xshift=140mm,yshift=-190mm]current page.north west) -- ++(45mm,0);
\draw[crimson!45, line width=0.9pt] ([xshift=150mm,yshift=-200mm]current page.north west) -- ++(35mm,0);
\draw[crimson!40, line width=0.8pt] ([xshift=160mm,yshift=-190mm]current page.north west) -- ++(0,-20mm);
\draw[crimson!40, line width=0.8pt] ([xshift=170mm,yshift=-200mm]current page.north west) -- ++(0,15mm);

% Additional connecting lines in lower right
\draw[crimson!35, line width=0.7pt] ([xshift=130mm,yshift=-180mm]current page.north west) -- ++(25mm,0);
\draw[crimson!35, line width=0.7pt] ([xshift=145mm,yshift=-180mm]current page.north west) -- ++(0,-25mm);

% Circuit nodes - lower right (more prominent)
\fill[crimson!55] ([xshift=160mm,yshift=-190mm]current page.north west) circle (1.6mm);
\fill[white] ([xshift=160mm,yshift=-190mm]current page.north west) circle (0.9mm);
\fill[crimson!55] ([xshift=170mm,yshift=-200mm]current page.north west) circle (1.4mm);
\fill[white] ([xshift=170mm,yshift=-200mm]current page.north west) circle (0.7mm);
\fill[crimson!50] ([xshift=145mm,yshift=-180mm]current page.north west) circle (1.2mm);
\fill[white] ([xshift=145mm,yshift=-180mm]current page.north west) circle (0.6mm);

% Title positioned in center - clean and readable
\node[inner sep=0mm,draw=none,anchor=center,text width=0.8\textwidth,
align=center,font={\fontsize{40pt}{40}\selectfont}]
(BGG) at (current page.center)  {\color{crimson}\MakeUppercase {#1}};

\end{tikzpicture}
\clearpage
}

% LAB DIVISIONS: Circuit-style neural network design for lab sections
% Used specifically for lab platform sections (arduino, xiao, grove, etc.)
\newcommand{\labdivision}[1]{%
\FloatBarrier%  % Flush all pending floats before lab division break
\clearpage
\thispagestyle{empty}
\begin{tikzpicture}[remember picture,overlay]
% Circuit background with subtle gradient
\coordinate(S1)at([yshift=-200mm]current page.north west);
\draw[draw=none,fill=BlueDD!5](S1)--++(45:16)coordinate(S2)-
|(S2|-current page.north west)--(current page.north west)coordinate(S3)--(S1);

% TOP AREA: Circuit lines in upper white space
\draw[crimson!50, line width=1.5pt] ([xshift=30mm,yshift=-40mm]current page.north west) -- ++(60mm,0);
\draw[crimson!40, line width=1pt] ([xshift=120mm,yshift=-50mm]current page.north west) -- ++(50mm,0);
\draw[crimson!50, line width=1.5pt] ([xshift=40mm,yshift=-70mm]current page.north west) -- ++(40mm,0);

% Connecting lines in top area
\draw[crimson!30, line width=1pt] ([xshift=60mm,yshift=-40mm]current page.north west) -- ++(0,-20mm);
\draw[crimson!30, line width=1pt] ([xshift=145mm,yshift=-50mm]current page.north west) -- ++(0,10mm);

% Neural nodes in top area
\fill[crimson!70] ([xshift=60mm,yshift=-40mm]current page.north west) circle (2.5mm);
\fill[white] ([xshift=60mm,yshift=-40mm]current page.north west) circle (1.5mm);
\fill[crimson!60] ([xshift=145mm,yshift=-50mm]current page.north west) circle (2mm);
\fill[white] ([xshift=145mm,yshift=-50mm]current page.north west) circle (1mm);
\fill[crimson!80] ([xshift=80mm,yshift=-70mm]current page.north west) circle (2mm);
\fill[white] ([xshift=80mm,yshift=-70mm]current page.north west) circle (1mm);

% BOTTOM AREA: Circuit lines in lower white space
\draw[crimson!50, line width=1.5pt] ([xshift=20mm,yshift=-200mm]current page.north west) -- ++(70mm,0);
\draw[crimson!40, line width=1pt] ([xshift=110mm,yshift=-210mm]current page.north west) -- ++(60mm,0);
\draw[crimson!50, line width=1.5pt] ([xshift=35mm,yshift=-230mm]current page.north west) -- ++(45mm,0);

% Connecting lines in bottom area
\draw[crimson!30, line width=1pt] ([xshift=55mm,yshift=-200mm]current page.north west) -- ++(0,-20mm);
\draw[crimson!30, line width=1pt] ([xshift=140mm,yshift=-210mm]current page.north west) -- ++(0,15mm);

% Neural nodes in bottom area
\fill[crimson!70] ([xshift=55mm,yshift=-200mm]current page.north west) circle (2.5mm);
\fill[white] ([xshift=55mm,yshift=-200mm]current page.north west) circle (1.5mm);
\fill[crimson!60] ([xshift=140mm,yshift=-210mm]current page.north west) circle (2mm);
\fill[white] ([xshift=140mm,yshift=-210mm]current page.north west) circle (1mm);
\fill[crimson!80] ([xshift=80mm,yshift=-230mm]current page.north west) circle (2mm);
\fill[white] ([xshift=80mm,yshift=-230mm]current page.north west) circle (1mm);

% SIDE AREAS: Subtle circuit elements on left and right edges
\draw[crimson!30, line width=1pt] ([xshift=15mm,yshift=-120mm]current page.north west) -- ++(20mm,0);
\draw[crimson!30, line width=1pt] ([xshift=175mm,yshift=-130mm]current page.north west) -- ++(15mm,0);
\fill[crimson!50] ([xshift=25mm,yshift=-120mm]current page.north west) circle (1.5mm);
\fill[white] ([xshift=25mm,yshift=-120mm]current page.north west) circle (0.8mm);
\fill[crimson!50] ([xshift=185mm,yshift=-130mm]current page.north west) circle (1.5mm);
\fill[white] ([xshift=185mm,yshift=-130mm]current page.north west) circle (0.8mm);

% Title positioned in center - CLEAN AREA
\node[inner sep=0mm,draw=none,anchor=center,text width=0.8\textwidth,
align=center,font={\fontsize{44pt}{44}\selectfont\bfseries}]
(BGG) at (current page.center)  {\color{crimson}\MakeUppercase {#1}};

\end{tikzpicture}
\clearpage
}

% Define \lab command for lab styling (different visual treatment)
\newcommand{\lab}[1]{%
\begin{tikzpicture}[remember picture,overlay]
%%%
% Different background pattern for labs
\coordinate(S1)at([yshift=-200mm]current page.north west);
\draw[draw=none,fill=BlueDD!15](S1)--++(45:16)coordinate(S2)-
|(S2|-current page.north west)--(current page.north west)coordinate(S3)--(S1);
%
\coordinate(E1)at([yshift=-98mm]current page.north west);
\draw[draw=none,fill=BlueDD!25](E1)--(current page.north west)coordinate(E2)
--++(0:98mm)coordinate(E3)--(E1);
%
\coordinate(D1)at([yshift=15mm]current page.south west);
\draw[draw=none,fill=BlueDD!60,opacity=0.7](D1)--++(45:5.5)coordinate(D2)
-|(D2|-current page.north west)--(current page.north west)coordinate(D3)--(D1);
%%%%
\path[red](S2)-|(S2-|current page.east)coordinate(SS2);
%LAB - Different styling
\node[crimson,align=flush right,inner sep=0,outer sep=0mm,draw=none,anchor=south,
font={\fontsize{48pt}{48}\selectfont\bfseries}]  (BG) at ($(S2)!0.5!(SS2)$){\hphantom{Workshop}};
%%%
\path[green]([yshift=15mm]D2)-|coordinate(TPD)(BG.south east);
\node[inner sep=0mm,draw=none,anchor=south east,%text width=0.9\textwidth,
align=right,font={\fontsize{40pt}{40}\selectfont}]
(BGG) at (TPD)  {\color{crimson}\MakeUppercase {#1}};%\MakeUppercase {}
\end{tikzpicture}
\thispagestyle{empty}
\clearpage
}

% =============================================================================
% SECTION FORMATTING
% =============================================================================
% All section levels use crimson color and are ragged right

% Section (Large, bold, crimson)
\titleformat{\section}
  {\normalfont\Large\bfseries\color{crimson}\raggedright}
  {\thesection}
  {0.5em}
  {#1}
\titlespacing*{\section}{0pc}{14pt plus 4pt minus 4pt}{6pt plus 2pt minus 2pt}[0pc]

% Subsection (large, bold, crimson)
\titleformat{\subsection}
  {\normalfont\large\bfseries\color{crimson}\raggedright}
  {\thesubsection}
  {0.5em}
  {#1}
\titlespacing*{\subsection}{0pc}{12pt plus 4pt minus 4pt}{5pt plus 1pt minus 2pt}[0pc]

% Subsubsection (normal size, bold, crimson)
\titleformat{\subsubsection}
  {\normalfont\normalsize\bfseries\color{crimson}\raggedright}
  {\thesubsubsection}
  {0.5em}
  {#1}
\titlespacing*{\subsubsection}{0pc}{12pt plus 4pt minus 4pt}{5pt plus 1pt minus 2pt}[0pc]

% Paragraph (run-in, bold, crimson, ends with period)
\titleformat{\paragraph}[runin]
  {\normalfont\normalsize\bfseries\color{crimson}}
  {\theparagraph}
  {0.5em}
  {#1}
  [\textbf{.}]
  \titlespacing*{\paragraph}{0pc}{6pt plus 2pt minus 2pt}{0.5em}[0pc]

% Subparagraph (run-in, italic, crimson, ends with period)
\titleformat{\subparagraph}[runin]
  {\normalfont\normalsize\itshape\color{crimson}}
  {\thesubparagraph}
  {0.5em}
  {#1}
  [\textbf{.}]
  \titlespacing*{\subparagraph}{0pc}{6pt plus 2pt minus 2pt}{0.5em}[0pc]

% =============================================================================
% CHAPTER FORMATTING
% =============================================================================
% Numbered chapters: "Chapter X" prefix, huge crimson title
\titleformat{\chapter}[display]
  {\normalfont\huge\bfseries\color{crimson}}
  {\chaptername\ \thechapter}
  {20pt}
  {\Huge #1}
  []

% Unnumbered chapters: no prefix, huge crimson title
\titleformat{name=\chapter,numberless}
  {\normalfont\huge\bfseries\color{crimson}}
  {}
  {0pt}
  {\Huge #1}
  []

\renewcommand{\chaptername}{Chapter}
% =============================================================================
% TABLE OF CONTENTS FORMATTING
% =============================================================================
\setcounter{tocdepth}{2}                      % Show chapters, sections, subsections

% TOC spacing adjustments for number widths and indentation
\setlength{\cftchapnumwidth}{2em}             % Chapter number width
\setlength{\cftsecnumwidth}{2.75em}           % Section number width
\setlength{\cftsubsecnumwidth}{3.25em}        % Subsection number width
\setlength{\cftsubsubsecnumwidth}{4em}        % Subsubsection number width
\setlength{\cftsubsecindent}{4.25em}          % Subsection indent
\setlength{\cftsubsubsecindent}{7.5em}        % Subsubsection indent

% Chapter entries in TOC: bold crimson with "Chapter" prefix
\renewcommand{\cftchapfont}{\bfseries\color{crimson}}
\renewcommand{\cftchappresnum}{\color{crimson}Chapter~}

% Custom formatting for division entries (styled like parts)
\newcommand{\divisionchapter}[1]{%
  \addvspace{12pt}%
  \noindent\hfil\bfseries\color{crimson}#1\hfil\par%
  \addvspace{6pt}%
}

% Adjust TOC spacing for "Chapter" prefix
\newlength{\xtraspace}
\settowidth{\xtraspace}{\cftchappresnum\cftchapaftersnum}
\addtolength{\cftchapnumwidth}{\xtraspace}

% Unnumbered chapters with TOC entry
\newcommand{\likechapter}[1]{%
    \chapter*{#1}
    \addcontentsline{toc}{chapter}{\textcolor{crimson}{#1}}
}

% =============================================================================
% PAGE NUMBERING SYSTEM
% =============================================================================
% Implements traditional book numbering:
% - Roman numerals (i, ii, iii...) for frontmatter
% - Arabic numerals (1, 2, 3...) for mainmatter
% Automatically switches at first numbered chapter
\makeatletter
\newif\if@firstnumbered%
\@firstnumberedtrue%
\newif\if@firstunnumbered%
\@firstunnumberedtrue%

\newcounter{lastRomanPage}
\setcounter{lastRomanPage}{1}

% Start document with Roman numerals (frontmatter)
\AtBeginDocument{
  \pagenumbering{roman}
  \renewcommand{\thepage}{\roman{page}}
}

% Intercept chapter command
\let\old@chapter\chapter%
\renewcommand{\chapter}{%
  \@ifstar{\unnumbered@chapter}{\numbered@chapter}%
}

% Numbered chapters: switch to Arabic on first occurrence
\newcommand{\numbered@chapter}[1]{%
  \if@firstnumbered%
    \cleardoublepage%
    \setcounter{lastRomanPage}{\value{page}}%
    \pagenumbering{arabic}%
    \@firstnumberedfalse%
  \else
    \setcounter{page}{\value{page}}%
  \fi
  \setcounter{sidenote}{1}                    % Reset footnote counter per chapter
  \old@chapter{#1}%
}

% Unnumbered chapters: stay in Roman numerals
\newcommand{\unnumbered@chapter}[1]{%
  \if@firstunnumbered%
    \clearpage
    \setcounter{lastRomanPage}{\value{page}}%
    \pagenumbering{roman}%
    \@firstunnumberedfalse%
  \fi
  \setcounter{sidenote}{1}
  \old@chapter*{#1}%
}
\makeatother

% =============================================================================
% TABLE SIZING AND SPACING
% =============================================================================
% Make tables slightly smaller to fit more content
\AtBeginEnvironment{longtable}{\scriptsize}

% Increase vertical spacing in table cells (default is 1.0)
\renewcommand{\arraystretch}{1.3}

% Prefer placing figures and tables at the top of pages
\makeatletter
\renewcommand{\fps@figure}{t}  % Default placement: top of page
\renewcommand{\fps@table}{t}   % Default placement: top of page
\makeatother

% =============================================================================
% LONGTABLE PAGE BREAKING FIXES (Windows compatibility)
% =============================================================================
% Prevent "Infinite glue shrinkage" errors on Windows LaTeX builds
% by giving longtable more flexibility in page breaking

% Allow more flexible page breaking (vs strict \flushbottom)
\raggedbottom

% Process more rows before attempting page break (default is 20)
\setcounter{LTchunksize}{50}

% Add extra stretch for longtable environments specifically
\AtBeginEnvironment{longtable}{%
  \setlength{\emergencystretch}{3em}%
  \setlength{\parskip}{0pt plus 1pt}%
}

% =============================================================================
% TABLE STYLING - Clean tables with crimson borders
% =============================================================================
% Professional table appearance with:
% - Clean white background (no colored rows)
% - Crimson-colored borders
% - Good spacing for readability
%
% Note: Headers are automatically bolded by Quarto when using **text** in source
\usepackage{booktabs}      % Professional table rules (\toprule, \midrule, \bottomrule)
\usepackage{colortbl}      % For colored borders (\arrayrulecolor)

% Global table styling - crimson borders
\setlength{\arrayrulewidth}{0.5pt}          % Thinner borders than default
%\arrayrulecolor{crimson}                    % Crimson borders matching brand

\setcounter{chapter}{0}

% =============================================================================
% DROP CAPS (Lettrine)
% =============================================================================
% Decorative large first letter at chapter openings, following the tradition
% of Hennessy & Patterson and other MIT Press textbooks.
% Usage in QMD: \lettrine{T}{he first sentence...}
\usepackage{lettrine}
\renewcommand{\LettrineFontHook}{\color{crimson}\bfseries}
\setcounter{DefaultLines}{3}          % Drop cap spans 3 lines
\renewcommand{\DefaultLoversize}{0.1} % Slight oversize for visual weight
\renewcommand{\DefaultLraise}{0}      % No vertical shift
\setlength{\DefaultNindent}{0.5em}    % Indent of continuation text
\setlength{\DefaultSlope}{0pt}        % No slope on continuation

% =============================================================================
% RUNNING HEADERS — Truncation Safety
% =============================================================================
% Long chapter/section titles can overflow the header. These marks truncate
% gracefully so headers stay within the text block.
\renewcommand{\chaptermark}[1]{%
  \markboth{\thechapter.\ #1}{}}
\renewcommand{\sectionmark}[1]{%
  \markright{\thesection\ #1}}

% =============================================================================
% EPIGRAPH ENVIRONMENT
% =============================================================================
% For chapter-opening quotations. Renders as right-aligned italic block
% with attribution in small caps below.
% Usage: \epigraph{Quote text}{Author Name, \textit{Source}}
\newcommand{\bookepigraph}[2]{%
  \vspace{1em}%
  \begin{flushright}%
    \begin{minipage}{0.75\textwidth}%
      \raggedleft\itshape\small #1\\[0.5em]%
      \normalfont\small --- #2%
    \end{minipage}%
  \end{flushright}%
  \vspace{1.5em}%
}

% =============================================================================
% THUMB INDEX TABS
% =============================================================================
% Colored tabs on the outer page edge for quick chapter navigation.
% Each Part gets a different vertical position; chapters within a Part
% share the same tab position. Visible when flipping through the book.
\newcounter{thumbindex}
\setcounter{thumbindex}{0}
\newlength{\thumbtabheight}
\setlength{\thumbtabheight}{16mm}     % Height of each tab
\newlength{\thumbtabwidth}
\setlength{\thumbtabwidth}{8mm}       % Width protruding from edge
\newlength{\thumbtabgap}
\setlength{\thumbtabgap}{1mm}         % Gap between tabs

% Advance to next thumb tab position (call at each \part)
\newcommand{\nextthumb}{%
  \stepcounter{thumbindex}%
}

% Draw the thumb tab on every page (placed in header via fancyhdr)
\newcommand{\drawthumb}{%
  \ifnum\value{thumbindex}>0%
    \begin{tikzpicture}[remember picture,overlay]
      \pgfmathsetmacro{\thumboffset}{%
        20 + (\value{thumbindex}-1) * (16 + 1)}  % mm from top
      \ifodd\value{page}%
        % Odd pages: tab on right edge
        \fill[crimson!80]
          ([yshift=-\thumboffset mm]current page.north east)
          rectangle +(-\thumbtabwidth, -\thumbtabheight);
        \node[white,font=\tiny\bfseries,rotate=90]
          at ([yshift=-\thumboffset mm - 0.5\thumbtabheight,
               xshift=-0.5\thumbtabwidth]current page.north east)
          {\Roman{thumbindex}};
      \else
        % Even pages: tab on left edge
        \fill[crimson!80]
          ([yshift=-\thumboffset mm]current page.north west)
          rectangle +(\thumbtabwidth, -\thumbtabheight);
        \node[white,font=\tiny\bfseries,rotate=-90]
          at ([yshift=-\thumboffset mm - 0.5\thumbtabheight,
               xshift=0.5\thumbtabwidth]current page.north west)
          {\Roman{thumbindex}};
      \fi
    \end{tikzpicture}%
  \fi
}

% Hook into fancyhdr to draw thumb on every content page
\AddToHook{shipout/foreground}{%
  \drawthumb%
}

% =============================================================================
% CROP / BLEED MARKS
% =============================================================================
% For final print submission, uncomment the line below to add crop marks.
% MIT Press production will advise on exact requirements.
% \usepackage[cam,center,width=7.5in,height=10.5in]{crop}

% =============================================================================
% PDF/A ARCHIVAL COMPLIANCE
% =============================================================================
% MIT Press increasingly requires PDF/A for long-term preservation.
% This embeds all fonts and removes transparency.
% Note: pdfx must be loaded early; if it conflicts with hyperref,
% MIT Press production can handle the conversion post-build.
% Uncomment when ready for final submission:
% \usepackage[a-3u]{pdfx}

% =============================================================================
% ENHANCED WIDOW / ORPHAN CONTROL
% =============================================================================
% Prevent single lines at top/bottom of pages and breaks before equations
\clubpenalty=10000          % No orphans (single first line at bottom)
\widowpenalty=10000         % No widows (single last line at top)
\displaywidowpenalty=10000  % No widow before display math
\predisplaypenalty=10000    % No page break just before display math
\postdisplaypenalty=0       % Allow break after display math (natural)
\usepackage{needspace}
\let\Needspace\needspace
\makeatletter
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{plain}
\@ifundefined{c@chapter}{\newfloat{vid}{h}{lovid}}{\newfloat{vid}{h}{lovid}[chapter]}
\floatname{vid}{Video}
\newcommand*\listofvids{\listof{vid}{List of Videos}}
\makeatother
\makeatletter
\@ifpackageloaded{tcolorbox}{}{\usepackage[skins,breakable]{tcolorbox}}
\@ifpackageloaded{fontawesome5}{}{\usepackage{fontawesome5}}
\definecolor{quarto-callout-color}{HTML}{909090}
\definecolor{quarto-callout-note-color}{HTML}{0758E5}
\definecolor{quarto-callout-important-color}{HTML}{CC1914}
\definecolor{quarto-callout-warning-color}{HTML}{EB9113}
\definecolor{quarto-callout-tip-color}{HTML}{00A047}
\definecolor{quarto-callout-caution-color}{HTML}{FC5300}
\definecolor{quarto-callout-color-frame}{HTML}{acacac}
\definecolor{quarto-callout-note-color-frame}{HTML}{4582ec}
\definecolor{quarto-callout-important-color-frame}{HTML}{d9534f}
\definecolor{quarto-callout-warning-color-frame}{HTML}{f0ad4e}
\definecolor{quarto-callout-tip-color-frame}{HTML}{02b875}
\definecolor{quarto-callout-caution-color-frame}{HTML}{fd7e14}
\makeatother
\makeatletter
\@ifpackageloaded{bookmark}{}{\usepackage{bookmark}}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\makeatletter
\@ifpackageloaded{sidenotes}{}{\usepackage{sidenotes}}
\@ifpackageloaded{marginnote}{}{\usepackage{marginnote}}
\makeatother
\newcommand{\fbxIconPath}{assets/images/icons/callouts}
\newcommand{\fbxIconFormat}{pdf}
\makeatletter
\@ifpackageloaded{tcolorbox}{}{\usepackage[many]{tcolorbox}}
\makeatother
%%%% ---foldboxy preamble ----- %%%%%

% Load xstring for string manipulation
\RequirePackage{xstring}

% Icon path and format configuration - can be overridden in filter-metadata
\providecommand{\fbxIconPath}{assets/images/icons/callouts}
\providecommand{\fbxIconFormat}{pdf}

% Helper command to include icon with hyphen-to-underscore conversion
% This ensures consistency: callout-quiz-question -> callout_quiz_question
\newcommand{\fbxIncludeIcon}[2]{%
  \StrSubstitute{#1}{-}{_}[\fbxIconName]%
  \includegraphics[width=#2]{\fbxIconPath/icon_\fbxIconName.\fbxIconFormat}%
}

% Legacy fallback colors (keep for compatibility)
\definecolor{fbx-default-color1}{HTML}{c7c7d0}
\definecolor{fbx-default-color2}{HTML}{a3a3aa}
\definecolor{fbox-color1}{HTML}{c7c7d0}
\definecolor{fbox-color2}{HTML}{a3a3aa}

% arguments: #1 typelabelnummer: #2 titel: #3
\newenvironment{fbx}[3]{%
\begin{tcolorbox}[
  enhanced,
  breakable,
  %fontupper=\fontsize{8pt}{10pt}\selectfont,  % 95% of body text (10pt -> 9.5pt)
  before skip=8pt,  % space above box (increased)
  after skip=8pt,   % space below box (increased)
  attach boxed title to top*={xshift=0pt},
  boxed title style={
  %fuzzy shadow={1pt}{-1pt}{0mm}{0.1mm}{gray},
  arc=1.5pt,
  rounded corners=north,
  sharp corners=south,
  top=6pt,          % Adjusted for ~40px equivalent height
  bottom=5pt,       % Adjusted for ~40px equivalent height
  overlay={
      \node [left,outer sep=0em, black,draw=none,anchor=west,
        rectangle,fill=none,inner sep=0pt]
        at ([xshift=4mm]frame.west) {\fbxIncludeIcon{#1}{4.2mm}};
    },
  },
  colframe=#1-color2,             % Border color (auto-generated from YAML)
  colbacktitle=#1-color1,         % Background color (auto-generated from YAML)
  colback=white,
  coltitle=black,
  titlerule=0mm,
  toprule=0.5pt,
  bottomrule=0.5pt,
  leftrule=2.2pt,
  rightrule=0.5pt,
  outer arc=1.5pt,
  arc=1.5pt,
  left=0.5em,       % increased left padding
  bottomtitle=1.5mm, % increased title bottom margin
  toptitle=1.5mm,    % increased title top margin
  title=\hspace{2.5em}\protect#2\hspace{0.5em}\protect#3, % Protect parameters
  extras middle and last={top=4pt} % increased continuation spacing
]}
{\end{tcolorbox}}


% boxed environment with right border
\newenvironment{fbxSimple}[3]{\begin{tcolorbox}[
  enhanced,
  breakable,
  %fontupper=\fontsize{8pt}{10pt}\selectfont,  % 95% of body text (10pt -> 9.5pt)
  before skip=8pt,  % space above box (increased)
  after skip=8pt,   % space below box (increased)
  attach boxed title to top*={xshift=0pt},
  boxed title style={
  %fuzzy shadow={1pt}{-1pt}{0mm}{0.1mm}{gray},
  arc=1.5pt,
  rounded corners=north,
  sharp corners=south,
  top=6pt,          % Adjusted for ~40px equivalent height
  bottom=5pt,       % Adjusted for ~40px equivalent height
  overlay={
      \node [left,outer sep=0em, black,draw=none,anchor=west,
        rectangle,fill=none,inner sep=0pt]
        at ([xshift=3mm]frame.west) {\fbxIncludeIcon{#1}{4.2mm}};
    },
  },
  colframe=#1-color2,             % Border color (auto-generated from YAML)
  colbacktitle=#1-color1,         % Background color (auto-generated from YAML)
  colback=white,
  coltitle=black,
  titlerule=0mm,
  toprule=0.5pt,
  bottomrule=0.5pt,
  leftrule=2.2pt,
  rightrule=0.5pt,
  outer arc=1.5pt,
  arc=1.5pt,
  left=0.5em,       % increased left padding
  bottomtitle=1.5mm, % increased title bottom margin
  toptitle=1.5mm,    % increased title top margin
  title=\hspace{2.5em}\protect#2\hspace{0.5em}\protect#3, % Protect parameters
  boxsep=1pt,
  extras first={bottom=0pt},
  extras last={top=0pt,bottom=-4pt},
  overlay first={
    \draw[line width=1pt,white] ([xshift=2.2pt]frame.south west)-- ([xshift=-0.5pt]frame.south east);
  },
  overlay last={
    \draw[line width=1pt,white] ([xshift=2.2pt]frame.north west)-- ([xshift=-0.5pt]frame.north east);
   }
]}
{\end{tcolorbox}}

%%%% --- end foldboxy preamble ----- %%%%%
%%==== colors from yaml ===%
\definecolor{callout-definition-color1}{HTML}{F0F4F8}
\definecolor{callout-definition-color2}{HTML}{1B4F72}
\definecolor{callout-principle-color1}{HTML}{F3F2FA}
\definecolor{callout-principle-color2}{HTML}{3D3B8E}
\definecolor{callout-checkpoint-color1}{HTML}{E8F5E9}
\definecolor{callout-checkpoint-color2}{HTML}{2E7D32}
\definecolor{callout-example-color1}{HTML}{F0F8F6}
\definecolor{callout-example-color2}{HTML}{148F77}
\definecolor{callout-resource-slides-color1}{HTML}{E0F2F1}
\definecolor{callout-resource-slides-color2}{HTML}{20B2AA}
\definecolor{callout-colab-color1}{HTML}{FFF5E6}
\definecolor{callout-colab-color2}{HTML}{FF6B35}
\definecolor{callout-perspective-color1}{HTML}{F7F8FA}
\definecolor{callout-perspective-color2}{HTML}{4A5568}
\definecolor{callout-takeaways-color1}{HTML}{FDF2F7}
\definecolor{callout-takeaways-color2}{HTML}{BE185D}
\definecolor{callout-notebook-color1}{HTML}{F2F7FF}
\definecolor{callout-notebook-color2}{HTML}{2C5282}
\definecolor{callout-resource-exercises-color1}{HTML}{E0F2F1}
\definecolor{callout-resource-exercises-color2}{HTML}{20B2AA}
\definecolor{callout-quiz-answer-color1}{HTML}{E8F2EA}
\definecolor{callout-quiz-answer-color2}{HTML}{4a7c59}
\definecolor{callout-quiz-question-color1}{HTML}{F0F0F8}
\definecolor{callout-quiz-question-color2}{HTML}{5B4B8A}
\definecolor{callout-code-color1}{HTML}{F2F4F8}
\definecolor{callout-code-color2}{HTML}{D1D7E0}
\definecolor{callout-theorem-color1}{HTML}{F5F0FF}
\definecolor{callout-theorem-color2}{HTML}{6B46C1}
\definecolor{callout-chapter-connection-color1}{HTML}{FDF2F7}
\definecolor{callout-chapter-connection-color2}{HTML}{A51C30}
\definecolor{callout-lighthouse-color1}{HTML}{FDF8E6}
\definecolor{callout-lighthouse-color2}{HTML}{B8860B}
\definecolor{callout-resource-videos-color1}{HTML}{E0F2F1}
\definecolor{callout-resource-videos-color2}{HTML}{20B2AA}
%=============%

\usepackage{hyphenat}
\usepackage{ifthen}
\usepackage{calc}
\usepackage{calculator}



\usepackage{graphicx}
\usepackage{geometry}
\usepackage{afterpage}
\usepackage{tikz}
\usetikzlibrary{calc}
\usetikzlibrary{fadings}
\usepackage[pagecolor=none]{pagecolor}


% Set the titlepage font families







% Set the coverpage font families

\usepackage{bookmark}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  pdftitle={Introduction to Machine Learning Systems},
  pdfauthor={Vijay Janapa Reddi},
  colorlinks=true,
  linkcolor={Maroon},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}


\title{Introduction to Machine Learning Systems}
\author{Vijay Janapa Reddi}
\date{February 1, 2026}
\begin{document}
%%%%% begin titlepage extension code

  \begin{frontmatter}

\begin{titlepage}
% This is a combination of Pandoc templating and LaTeX
% Pandoc templating https://pandoc.org/MANUAL.html#templates
% See the README for help

\thispagestyle{empty}

\newgeometry{top=-100in}

% Page color

\newcommand{\coverauthorstyle}[1]{{\fontsize{20}{24.0}\selectfont
{#1}}}

\begin{tikzpicture}[remember picture, overlay, inner sep=0pt, outer sep=0pt]

\tikzfading[name=fadeout, inner color=transparent!0,outer color=transparent!100]
\tikzfading[name=fadein, inner color=transparent!100,outer color=transparent!0]
\node[anchor=south west, rotate=0, opacity=1] at ($(current page.south west)+(0.225\paperwidth, 9)$) {
\includegraphics[width=\paperwidth, keepaspectratio]{assets/images/covers/cover-image-transparent-vol1.png}};

% Title
\newcommand{\titlelocationleft}{0.075\paperwidth}
\newcommand{\titlelocationbottom}{0.4\paperwidth}
\newcommand{\titlealign}{left}

\begin{scope}{%
\fontsize{52}{62.4}\selectfont
\node[anchor=north
west, align=left, rotate=0] (Title1) at ($(current page.south west)+(\titlelocationleft,\titlelocationbottom)$)  [text width = 0.9\paperwidth]  {{\nohyphens{Machine
Learning Systems}}};
}
\end{scope}

% Author
\newcommand{\authorlocationleft}{.925\paperwidth}
\newcommand{\authorlocationbottom}{0.175\paperwidth}
\newcommand{\authoralign}{right}

\begin{scope}
{%
\fontsize{20}{24.0}\selectfont
\node[anchor=north
east, align=right, rotate=0] (Author1) at ($(current page.south west)+(\authorlocationleft,\authorlocationbottom)$)  [text width = 6in]  {\coverauthorstyle{Vijay
Janapa Reddi\\}};
}
\end{scope}

% Footer
\newcommand{\footerlocationleft}{0.075\paperwidth}
\newcommand{\footerlocationbottom}{0.475\paperwidth}
\newcommand{\footerlocationalign}{left}

\begin{scope}
{%
\fontsize{25}{30.0}\selectfont
 \node[anchor=north west, align=left, rotate=0] (Footer1) at %
($(current page.south west)+(\footerlocationleft,\footerlocationbottom)$)  [text width = 0.9\paperwidth]  {{\nohyphens{Introduction
to}}};
}
\end{scope}

\end{tikzpicture}
\clearpage
\restoregeometry
%%% TITLE PAGE START

% Set up alignment commands
%Page
\newcommand{\titlepagepagealign}{
\ifthenelse{\equal{left}{right}}{\raggedleft}{}
\ifthenelse{\equal{left}{center}}{\centering}{}
\ifthenelse{\equal{left}{left}}{\raggedright}{}
}


\newcommand{\titleandsubtitle}{
% Title and subtitle
{{\huge{\bfseries{\nohyphens{Introduction to Machine Learning
Systems}}}}\par
}%
}
\newcommand{\titlepagetitleblock}{
\titleandsubtitle
}

\newcommand{\authorstyle}[1]{{\large{#1}}}

\newcommand{\affiliationstyle}[1]{{\large{#1}}}

\newcommand{\titlepageauthorblock}{
{\authorstyle{\nohyphens{Vijay Janapa
Reddi}{\textsuperscript{1}}\textsuperscript{,}{\textsuperscript{,*}}}}}

\newcommand{\titlepageaffiliationblock}{
\hangindent=1em
\hangafter=1
{\affiliationstyle{
{1}.~Harvard University


\vspace{1\baselineskip}
* \textit{Correspondence:}~Vijay Janapa Reddi~vj@eecs.harvard.edu
}}
}
\newcommand{\headerstyled}{%
{}
}
\newcommand{\footerstyled}{%
{\large{}}
}
\newcommand{\datestyled}{%
{February 1, 2026}
}


\newcommand{\titlepageheaderblock}{\headerstyled}

\newcommand{\titlepagefooterblock}{
\footerstyled
}

\newcommand{\titlepagedateblock}{
\datestyled
}

%set up blocks so user can specify order
\newcommand{\titleblock}{{

{\titlepagetitleblock}
}

\vspace{4\baselineskip}
}

\newcommand{\authorblock}{{\titlepageauthorblock}

\vspace{2\baselineskip}
}

\newcommand{\affiliationblock}{{\titlepageaffiliationblock}

\vspace{0pt}
}

\newcommand{\logoblock}{}

\newcommand{\footerblock}{}

\newcommand{\dateblock}{{\titlepagedateblock}

\vspace{0pt}
}

\newcommand{\headerblock}{}

\thispagestyle{empty} % no page numbers on titlepages


\newcommand{\vrulecode}{\textcolor{black}{\rule{\vrulewidth}{\textheight}}}
\newlength{\vrulewidth}
\setlength{\vrulewidth}{2pt}
\newlength{\B}
\setlength{\B}{\ifdim\vrulewidth > 0pt 0.05\textwidth\else 0pt\fi}
\newlength{\minipagewidth}
\ifthenelse{\equal{left}{left} \OR \equal{left}{right} }
{% True case
\setlength{\minipagewidth}{\textwidth - \vrulewidth - \B - 0.1\textwidth}
}{
\setlength{\minipagewidth}{\textwidth - 2\vrulewidth - 2\B - 0.1\textwidth}
}
\ifthenelse{\equal{left}{left} \OR \equal{left}{leftright}}
{% True case
\raggedleft % needed for the minipage to work
\vrulecode
\hspace{\B}
}{%
\raggedright % else it is right only and width is not 0
}
% [position of box][box height][inner position]{width}
% [s] means stretch out vertically; assuming there is a vfill
\begin{minipage}[b][\textheight][s]{\minipagewidth}
\titlepagepagealign
\titleblock

Prof.~Vijay Janapa Reddi

School of Engineering and Applied Sciences

Harvard University

\vspace{80mm}

With heartfelt gratitude to the community for their invaluable
contributions and steadfast support.

\vfill

February 1, 2026

\vfill
\par

\end{minipage}\ifthenelse{\equal{left}{right} \OR \equal{left}{leftright} }{
\hspace{\B}
\vrulecode}{}
\clearpage
%%% TITLE PAGE END
\end{titlepage}
\setcounter{page}{1}
\end{frontmatter}

%%%%% end titlepage extension code

% =============================================================================
% HALF-TITLE PAGE (Volume I)
% =============================================================================
% Standard academic book sequence: half-title -> blank -> title page -> copyright
% The half-title shows only the book title -- no author, no publisher, no date.
\thispagestyle{empty}
\begin{center}
\vspace*{0.3\textheight}
{\fontsize{24pt}{28pt}\selectfont\bfseries\color{crimson} Introduction to\\[0.4em] Machine Learning Systems}\\[2em]
{\large\itshape Volume~I}
\vfill
\end{center}
\clearpage
\thispagestyle{empty}\null\clearpage  % Blank verso (back of half-title)

\renewcommand*\contentsname{Table of contents}
{
\hypersetup{linkcolor=}
\setcounter{tocdepth}{2}
\tableofcontents
}
\listoffigures
\listoftables

\mainmatter
\bookmarksetup{startatroot}

\chapter*{Welcome to Volume I}\label{welcome-to-volume-i}
\addcontentsline{toc}{chapter}{Welcome to Volume I}

\markboth{Welcome to Volume I}{Welcome to Volume I}

\bookmarksetup{startatroot}

\chapter{Hardware Acceleration}\label{sec-ai-acceleration}

\marginnote{\begin{footnotesize}

\emph{DALL·E 3 Prompt: Create an intricate and colorful representation
of a System on Chip (SoC) design in a rectangular format. Showcase a
variety of specialized machine learning accelerators and chiplets, all
integrated into the processor. Provide a detailed view inside the chip,
highlighting the rapid movement of electrons. Each accelerator and
chiplet should be designed to interact with neural network neurons,
layers, and activations, emphasizing their processing speed. Depict the
neural networks as a network of interconnected nodes, with vibrant data
streams flowing between the accelerator pieces, showcasing the enhanced
computation speed.}

\end{footnotesize}}

\noindent
\pandocbounded{\includegraphics[keepaspectratio]{contents/vol1/hw_acceleration/images/png/cover_ai_hardware.png}}

\section*{Purpose}\label{purpose}
\addcontentsline{toc}{section}{Purpose}

\markright{Purpose}

\emph{Why does moving data cost more than computing it, and how does
this inversion dictate the design of every modern AI accelerator?}

Compression reduces model complexity through fewer parameters, lower
precision, and more efficient architectures. But even a compressed model
must execute on physical hardware, and here a counterintuitive truth
emerges: the fundamental surprise of modern computing is that
\emph{arithmetic is nearly free while memory access is expensive}. In
the time it takes to fetch a single value from memory, a processor could
perform thousands of calculations. This ``Memory Wall'' explains
\emph{why} GPUs and TPUs exist: they are not merely faster at math, but
architected specifically to hide, amortize, and minimize the crushing
cost of moving data. The same inversion explains \emph{why} some
optimizations that reduce theoretical computation fail to improve actual
runtime: if the operation was already memory-bound, computing less does
nothing because the bottleneck was never computation. Understanding this
reality transforms hardware selection from comparing peak FLOPS to
analyzing whether workload characteristics align with \emph{what} the
hardware actually accelerates.

\begin{tcolorbox}[enhanced jigsaw, toptitle=1mm, left=2mm, opacityback=0, colframe=quarto-callout-tip-color-frame, coltitle=black, toprule=.15mm, opacitybacktitle=0.6, leftrule=.75mm, arc=.35mm, breakable, titlerule=0mm, colbacktitle=quarto-callout-tip-color!10!white, colback=white, bottomtitle=1mm, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Learning Objectives}, rightrule=.15mm, bottomrule=.15mm]

\begin{itemize}
\tightlist
\item
  Explain why systolic arrays and tensor cores achieve 10-100× better
  efficiency than general-purpose processors for matrix operations
\item
  Calculate arithmetic intensity and use the roofline model to determine
  compute-bound versus memory-bound workloads
\item
  Predict performance bottlenecks by quantifying the memory wall:
  bandwidth limits, energy costs, and cache hierarchy trade-offs
\item
  Select appropriate dataflow strategies (weight-stationary,
  output-stationary, input-stationary) based on workload reuse
  priorities
\item
  Analyze compiler optimizations including kernel fusion, tiling, and
  memory planning for efficient hardware execution
\item
  Evaluate accelerator choices for specific deployment scenarios using
  quantitative cost-performance analysis
\item
  Identify common pitfalls such as ignoring bandwidth limits, expecting
  linear scaling, or optimizing for peak FLOPS
\end{itemize}

\end{tcolorbox}

\section{AI Hardware Acceleration
Fundamentals}\label{sec-ai-acceleration-ai-hardware-acceleration-fundamentals-9b28}

We have optimized the \textbf{Data} in \textbf{?@sec-data-selection} and
compressed the \textbf{Algorithm} (Model) in
\textbf{?@sec-model-compression}. Now we turn to the final component of
the AI Triad: the \textbf{Machine}. Hardware acceleration exists because
of a fundamental asymmetry in modern computing: arithmetic is
\emph{cheap}, but moving data is \emph{expensive}. In the time a modern
GPU computes a thousand floating-point operations, a single value
travels from main memory. This inversion, where computation is the
abundant resource and bandwidth is the scarce one, is the reason
specialized hardware matters for machine learning.

\phantomsection\label{callout-definitionux2a-1.1}
\begin{fbx}{callout-definition}{Definition:}{Hardware Acceleration}
\phantomsection\label{callout-definition*-1.1}
\textbf{\emph{Hardware Acceleration}} is the architectural strategy of
trading \textbf{General-Purpose Programmability} for \textbf{Compute
Density}. By eliminating control logic (branch prediction, out-of-order
execution) for predictable workloads, accelerators dedicate maximum
silicon area to arithmetic units, achieving efficiency gains bounded
only by the \textbf{Memory Wall}.

\end{fbx}

This definition frames the chapter's central engineering tradeoff.
General-purpose processors devote significant silicon area to branch
prediction, speculative execution, and complex cache coherence
protocols. Accelerators strip away that generality, filling the die with
arithmetic units tuned to the regular, data-parallel patterns that
characterize neural network computation. The result is
order-of-magnitude improvements in throughput per watt for the workloads
that match these patterns.

Hardware alone, however, cannot achieve these gains. The algorithms must
be designed to leverage what the hardware offers, and the hardware must
be built to accelerate the operations algorithms actually use. This
symbiosis motivates a complementary principle: \emph{hardware-software
co-design}.

\phantomsection\label{callout-definitionux2a-1.2}
\begin{fbx}{callout-definition}{Definition:}{Hardware-Software Co-design}
\phantomsection\label{callout-definition*-1.2}
\textbf{\emph{Hardware-Software Co-design}} is the practice of breaking
abstraction layers to expose \textbf{Hardware Primitives} directly to
\textbf{Algorithmic Logic}. By tailoring algorithms to physical
constraints (e.g., quantization for INT8 accelerators) and tailoring
silicon to algorithmic patterns (e.g., Sparse Tensor Cores), it bypasses
the inefficiencies of general-purpose instruction sets.

\end{fbx}

Co-design explains \emph{why} the compression techniques introduced in
\textbf{?@sec-model-compression} deliver real speedups. Quantization
from FP32 to INT8 yields 2-4x acceleration not because of fewer bits in
the abstract, but because accelerators pack 4x more INT8 operations into
the same silicon area. Structured pruning improves performance while
unstructured pruning often does not, because structured patterns
preserve the regular memory access patterns that hardware can optimize.
Throughout this chapter, the physical constraints of silicon will reveal
\emph{why} some theoretically promising algorithmic optimizations
succeed in practice and others fail.

Hardware acceleration targets specific terms in the \textbf{Iron Law of
ML Systems}. While data selection reduced the total data and model
compression reduced the ops per sample, hardware acceleration increases
the rate at which those ops execute by maximizing the Throughput and
Bandwidth denominators. Yet acceleration has a hard ceiling, established
by \emph{the iron law of acceleration}.

\phantomsection\label{callout-notebookux2a-1.3}
\begin{fbx}{callout-notebook}{AI Engineer’s Notebook:}{The Iron Law of Acceleration}
\phantomsection\label{callout-notebook*-1.3}
Hardware acceleration does not speed up the entire system; it \emph{only
speeds up the parallelizable fraction (\(P\))}. This is governed by
\textbf{Amdahl's Law for AI} (\citeproc{ref-amdahl1967validity}{Amdahl
1967}):

\[ Speedup = \frac{1}{(1 - P) + \frac{P}{S}} \]

\begin{itemize}
\tightlist
\item
  \textbf{\(P\) (Parallel Fraction):} The matrix multiplications
  (typically 90-99\% of an ML workload).
\item
  \textbf{\(S\) (Speedup):} The raw speed advantage of the GPU/TPU over
  the CPU (typically 100x-1000x).
\item
  \textbf{\(1-P\) (Serial Fraction):} Data loading, Python overhead, and
  kernel launch latency.
\end{itemize}

\textbf{The Pitfall:} If data loading takes 10\% of the time
(\(P=0.9\)), even an \textbf{infinite speed} accelerator (\(S=\infty\))
can only achieve a \textbf{10x} total speedup. The ``boring'' serial
part dominates the ``exciting'' AI part.

\end{fbx}

Amdahl's Law is not merely theoretical: it explains \emph{why} many GPU
upgrades disappoint in practice. The following heatmap
(Figure~\ref{fig-iron-law-heatmap}) visualizes this ``Acceleration
Wall,'' showing that unless your workload is highly parallelizable
(\(P > 0.99\)), investing in faster hardware yields diminishing returns.

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{contents/vol1/hw_acceleration/hw_acceleration_files/figure-pdf/fig-iron-law-heatmap-1.pdf}}

}

\caption{\label{fig-iron-law-heatmap}\textbf{The Iron Law Heatmap}:
Total system speedup as a function of Accelerator Speed (\(S\)) and
Parallel Fraction (\(P\)). The `Accelerator Wall' at the top reveals
that if a workload is even slightly serial (\(P < 0.9\)), increasing
hardware speed yields almost no benefit. Realizing the potential of
1000x accelerators requires engineering workloads with near-perfect
(\(P > 0.999\)) parallelism.}

\end{figure}%

Amdahl's Law is not merely theoretical: it explains \emph{why} many GPU
upgrades disappoint in practice. Before examining specific hardware
architectures, test your intuition about these fundamental limits.

\phantomsection\label{callout-checkpointux2a-1.4}
\begin{fbx}{callout-checkpoint}{Checkpoint:}{The Parallelism Gate}
\phantomsection\label{callout-checkpoint*-1.4}

Hardware speedups are capped by sequential bottlenecks.

\textbf{Amdahl's Reality}

\begin{itemize}
\tightlist
\item[$\square$]
  \textbf{Serial Bottlenecks}: Why does a 1000x faster GPU only speed up
  training by 5x if data loading is slow? (Because
  \(Speedup \le 1/(1-P)\)).
\item[$\square$]
  \textbf{Workload Variation}: Why does ResNet (compute-bound) scale
  better than MobileNet (latency-bound)? (ResNet spends more time in
  parallelizable matrix math).
\end{itemize}

\end{fbx}

To see Amdahl's Law in action, consider how the parallel fraction \(P\)
differs dramatically between workload archetypes on the same hardware.

\phantomsection\label{lighthouse-amdahl-h100}
\begin{fbx}{callout-lighthouse}{Lighthouse:}{Amdahl's Law on H100}
\phantomsection\label{lighthouse-amdahl-h100}
\textbf{ResNet-50 inference on NVIDIA H100:}

\begin{itemize}
\tightlist
\item
  H100 delivers \(S = `{python} hw_speedup`\times\) speedup over CPU for
  matrix multiply (\texttt{\{python\}\ h100\_tflops\_int8} TOPS INT8
  vs.~\textasciitilde8 TOPS on baseline CPU without AMX extensions)
\item
  Typical inference has \(P = `{python} p_resnet`\)
  (\texttt{\{python\}\ f"\{p\_resnet*100:.0f\}"}\% parallelizable,
  \texttt{\{python\}\ f"\{serial\_resnet*100:.0f\}"}\% serial: data
  loading, preprocessing, postprocessing)
\end{itemize}

\[Speedup = \frac{1}{(1-0.95) + \frac{0.95}{500}} = \frac{1}{0.05 + 0.0019} = \frac{1}{0.0519} \approx `{python} amdahl_resnet_str`\times\]

Despite a \texttt{\{python\}\ hw\_speedup}× hardware advantage, total
system speedup is only
\textbf{\texttt{\{python\}\ f"\{amdahl\_resnet:.0f\}"}×}. The
\texttt{\{python\}\ f"\{serial\_resnet*100:.0f\}"}\% serial fraction
caps practical gains.

\textbf{Contrast with GPT-2 (autoregressive):}

\begin{itemize}
\tightlist
\item
  Same H100, but GPT-2 token generation has \(P = `{python} p_gpt2`\)
  (\texttt{\{python\}\ f"\{(1-p\_gpt2)*100:.0f\}"}\% serial: KV-cache
  updates, sampling, Python overhead)
\end{itemize}

\[Speedup = \frac{1}{(1-0.80) + \frac{0.80}{500}} = \frac{1}{0.20 + 0.0016} \approx `{python} amdahl_gpt2_str`\times\]

The \textbf{Bandwidth Hog} archetype suffers more from serial
bottlenecks. Even infinite accelerator speed yields only
\(1/(1-P) = `{python} amdahl_gpt2_ceil_str`\times\) maximum speedup.
This is \emph{why} LLM inference optimization focuses on reducing the
serial fraction (batching, speculative decoding) rather than raw
hardware speed.

\end{fbx}

These examples reveal that the critical question for any hardware
optimization is not ``how fast is the chip?'' but rather: \emph{is this
workload limited by how fast we can compute, or how fast we can move
data?} The answer determines which accelerator to choose, which
optimizations matter, and whether a 10x more powerful chip will actually
help. The \textbf{Roofline Model} (formally defined in
\textbf{?@sec-system-foundations-roofline-model-5f7c}) provides the
analytical framework for answering this question. It plots an
operation's \textbf{arithmetic intensity} (operations per byte of memory
traffic) against hardware capabilities, revealing whether performance is
capped by compute or bandwidth. A dense matrix multiplication with high
arithmetic intensity benefits from more TFLOPS; a LayerNorm with low
arithmetic intensity benefits from more memory bandwidth. ResNet-50's
convolutions are compute-bound while GPT-2's attention layers are
memory-bound, and this distinction is precisely \emph{why} these
architectures require different optimization strategies.

With this analytical lens in place, the chapter proceeds through four
major topics. First, we trace the historical evolution of
domain-specific architectures, from floating-point coprocessors through
graphics processors to contemporary AI accelerators. Second, we examine
the computational primitives that characterize ML workloads (matrix
multiplication, vector operations, and nonlinear activation functions)
and analyze how specialized hardware optimizes these operations through
innovations such as systolic arrays and tensor cores. Third, we turn to
memory hierarchy design, where data movement energy costs exceeding
computation costs by more than 100x make on-chip buffer optimization and
high-bandwidth memory interfaces critical. Fourth, the software stack:
compiler optimization and runtime system support determine the extent to
which theoretical hardware capabilities translate into measurable
performance. Throughout, the focus remains on single-machine systems;
multi-machine coordination is covered in Volume II.

\section{Evolution of Hardware
Specialization}\label{sec-ai-acceleration-evolution-hardware-specialization-fdb7}

Computing architectures follow a recurring pattern: as workloads grow in
complexity, general-purpose processors become inefficient, prompting
specialized hardware development. Machine learning acceleration
represents the latest stage in this evolution, following a trajectory
observed in floating-point arithmetic, graphics processing, and digital
signal processing. Understanding this history serves a practical
purpose, since the architectural innovations that addressed
floating-point bottlenecks in the 1980s, graphics throughput in the
1990s, and media processing in the 2000s inform today's AI accelerator
designs. Each era confronted the same constraint introduced in the
Purpose section: data movement costs dominate computation costs, and
specialization succeeds by minimizing unnecessary data movement.

Modern ML accelerators (GPUs with tensor cores, Google's
TPUs\sidenote{\textbf{TPU Origins}: Google secretly developed the Tensor
Processing Unit (TPU) starting in 2013 when they realized CPUs couldn't
handle the computational demands of their neural networks. The TPUv1,
deployed in 2015, delivered 15-30\(\times\) better performance per watt
than contemporary GPUs for inference. This breakthrough significantly
changed how the industry approached AI hardware, proving that
domain-specific architectures could dramatically outperform
general-purpose processors for neural network workloads. The evolution
across multiple TPU generations has yielded important design lessons
(\citeproc{ref-jouppi2021ten}{Norman P. Jouppi et al. 2021}). }, Apple's
Neural Engine) emerged from these established architectural principles.
This section traces the evolution through four phases: specialized
computing origins, parallel graphics processing, domain-specific
architectures, and the emergence of ML-specific hardware. Each phase
reveals design principles that remain relevant for understanding and
optimizing contemporary AI systems. The magnitude of the gains from
domain-specific design became unmistakable in 2015, when Google's first
TPU delivered an \emph{efficiency shock} that reshaped the industry's
approach to AI hardware.

\phantomsection\label{callout-exampleux2a-1.6}
\begin{fbx}{callout-example}{Example:}{The TPUv1 vs. K80 Efficiency Shock}
\phantomsection\label{callout-example*-1.6}
\textbf{The Comparison}: In 2015, Google deployed its first Tensor
Processing Unit (TPUv1) and compared it to the dominant GPU of the era,
the NVIDIA K80.

\textbf{The Shock}: The TPUv1 was not just slightly faster; it was
\textbf{15x-30x faster} on inference workloads. More importantly, it
achieved \textbf{30x-80x better performance-per-watt}.

\textbf{The Reason}: The K80 was a general-purpose processor (good for
graphics, physics, diverse math). The TPU was a \textbf{Domain-Specific
Architecture (DSA)} built for \emph{one thing}: 8-bit integer matrix
multiplication. It stripped away caches, branch prediction, and
out-of-order execution logic to fill the chip with pure arithmetic units
(Systolic Arrays).

\textbf{The Legacy}: This result ended the ``General Purpose'' era for
AI. It proved that tailoring silicon to the \textbf{Algorithmic
Primitive} (Matrix Multiply) yields order-of-magnitude gains that
Moore's Law alone could not deliver for decades.

\end{fbx}

Hardware specialization enhances performance by implementing frequently
executed patterns in dedicated circuits, but introduces tradeoffs in
flexibility, silicon area, and programming complexity. The principles
that shaped early floating-point and graphics accelerators now inform AI
hardware design.

\subsection{Specialized
Computing}\label{sec-ai-acceleration-specialized-computing-22ce}

Hardware specialization emerges when specific computational patterns
become the primary system bottleneck, preventing general-purpose
processors from scaling efficiently. Historically, this progression
follows three distinct phases: the \textbf{Precision Bottleneck} (scalar
floating-point), the \textbf{Throughput Bottleneck} (parallel graphics),
and the \textbf{Integration Bottleneck} (memory-compute locality).

The first phase, the \textbf{Precision Bottleneck}, occurred when
scientific and engineering applications required high-precision decimal
math that general-purpose CPUs performed poorly. In the late 1970s, CPUs
typically emulated floating-point operations in software, requiring
hundreds of cycles for a single multiplication. This scalar inefficiency
led to the first major instance of hardware specialization: the
mathematics coprocessor.

The Intel 8087 (1980)\sidenote{\textbf{Intel 8087 Impact}: The 8087
coprocessor transformed scientific computing by providing dedicated
hardware for the IEEE 754 floating-point standard. This success
established the economic model for hardware specialization: premium
pricing for dramatic performance jumps in specific domains (CAD,
simulation). } addressed this bottleneck by offloading
arithmetic-intensive tasks to a dedicated unit. By implementing
floating-point logic in hardware rather than software emulation, the
8087 achieved up to 100\(\times\) performance gains for scientific
workloads (\citeproc{ref-fisher_8087_1981}{Fisher 1981}). This
established a fundamental principle: when a specific data type or
operation consumes the majority of execution cycles, moving it to
specialized silicon provides 10-100x improvements.

As specialized functions like floating-point math proved their value,
they followed a recurring pattern of \textbf{integration}. The Intel
486DX (1989) moved the FPU directly onto the CPU die, eliminating the
off-chip communication latency and making high-precision math a standard
feature rather than an optional accelerator
(\citeproc{ref-patterson2021computer}{Patterson and Hennessy 2021}).
This cycle (specialization to solve a bottleneck, followed by
integration into the general-purpose stack) repeats across every era of
hardware evolution.

This progression from specialization to integration has shaped modern
computing. Each domain (graphics, signal processing, machine learning)
introduced specialized architectures that were later absorbed into
general-purpose platforms.

Figure~\ref{fig-timeline} traces this trajectory: each era produced
accelerators addressing the dominant computational bottleneck of the
period. The capabilities enabling today's real-time translation,
recommendations, and on-device inference build directly on principles
established in earlier specialization waves.

\begin{figure}[htb]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/1a4864eb094108203d9b3137b260085718d301c9.pdf}}

}

\caption{\label{fig-timeline}\textbf{Hardware Specialization Timeline.}
Computing architectures progressively incorporate specialized
accelerators to address emerging performance bottlenecks, from
floating-point units to graphics processors and machine learning
accelerators. Each era produced hardware tailored to the dominant
computational patterns of its period.}

\end{figure}%

\subsection{Parallel Computing and Graphics
Processing}\label{sec-ai-acceleration-parallel-computing-graphics-processing-4654}

The principles established through floating-point acceleration provided
a blueprint for addressing subsequent computational challenges. As
computing applications diversified, new computational patterns emerged
that exceeded the capabilities of general-purpose processors, and each
domain contributed unique insights to hardware acceleration strategies.

Graphics processing emerged as a primary driver of hardware
specialization in the 1990s. Early graphics accelerators focused on
specific operations like bitmap transfers and polygon filling. The
introduction of fixed-function graphics accelerators with NVIDIA's
GeForce 256 in 1999 represented a significant advancement in specialized
computing. The GeForce 256 implemented hardware-accelerated transform
and lighting (T\&L), moving these computations from CPU to dedicated
silicon. While not yet programmable, these Graphics Processing Units
(GPUs) demonstrated how fixed-function parallel architectures could
efficiently handle data-parallel workloads, achieving 50-100\(\times\)
speedups in 3D rendering tasks like texture mapping and vertex
transformation. The transition to programmable shaders with the GeForce
3 (2001) and unified shader architectures with the GeForce 8 (2006)
eventually enabled GPU computing for general-purpose workloads. By 2004,
high-end GPUs could process over 100 million polygons per second
(\citeproc{ref-owens2008gpu}{Owens et al. 2008}).

Concurrently, Digital Signal Processing (DSP) processors established
parallel data path architectures with specialized multiply-accumulate
units and circular buffers optimized for filtering and transform
operations. Texas Instruments' TMS32010 (1983) demonstrated how
domain-specific instruction sets could dramatically improve performance
for signal processing applications
(\citeproc{ref-lyons2011understanding}{Lyons 2011}).

Network processing introduced additional patterns of specialization.
Network processors developed unique architectures to handle packet
processing at line rate, incorporating multiple processing cores,
specialized packet manipulation units, and sophisticated memory
management systems. Intel's IXP2800 network processor demonstrated how
multiple levels of hardware specialization could be combined to address
complex processing requirements.

These diverse domains share common characteristics: identification of
domain-specific computational patterns, development of specialized
processing elements and memory hierarchies, creation of domain-specific
programming models, and progressive evolution toward more flexible
architectures. This pattern of architectural co-evolution established
the foundation for contemporary AI hardware design. The GPU's success in
parallelizing 3D graphics pipelines enabled its adoption for training
deep neural networks, exemplified by AlexNet\sidenote{\textbf{AlexNet's
GPU Revolution}: AlexNet's breakthrough wasn't just algorithmic. It
demonstrated that GPUs could train deep networks substantially faster
than CPUs for the dominant linear-algebra workloads
(\citeproc{ref-krizhevsky2012alexnet}{Krizhevsky, Sutskever, and Hinton
2017}). The team split the 8-layer network across two GPUs, reducing
training time from weeks to days and helping catalyze the shift toward
GPU-centric deep learning development. The enduring scale takeaway is
that matching the workload to parallel hardware can yield
order-of-magnitude improvements in time-to-train. } in 2012, which
executed on consumer-grade NVIDIA GPUs. DSP innovations in low-power
signal processing facilitated real-time inference on edge devices,
including voice assistants and wearables. Together, these domains
informed ML hardware designs and demonstrated that accelerators could be
deployed across both cloud and embedded contexts.

\subsection{Emergence of Domain-Specific
Architectures}\label{sec-ai-acceleration-emergence-domainspecific-architectures-e56e}

These diverse acceleration patterns converged in a broader architectural
shift. The emergence of domain-specific architectures
(DSA)\sidenote{\textbf{Domain-Specific Architectures (DSA)}: Computing
architectures optimized for specific application domains rather than
general-purpose computation. Unlike CPUs designed for flexibility, DSAs
sacrifice programmability for dramatic efficiency gains. Google's TPU
achieves 15-30\(\times\) better performance per watt than GPUs for
neural networks, while video codecs provide 100-1000\(\times\)
improvements over software decoding. The 2018 Turing Award recognized
this shift as the defining trend in modern computer architecture. }
marks a transition in computer system design, driven by two converging
factors: the breakdown of traditional scaling laws
(\citeproc{ref-esmaeilzadeh2011dark}{Esmaeilzadeh et al. 2011}) and the
increasing computational demands of specialized workloads. The slowdown
of Moore's Law\sidenote{\textbf{Moore's Law}: Intel co-founder Gordon
Moore's 1965 observation that transistor density doubles every 18-24
months. This exponential scaling drove computing progress for decades,
enabling everything from smartphones to supercomputers. However, power
density constraints and rising manufacturing complexity slowed the
historical pace of cost-effective scaling. As advanced-node development
costs rose from millions to billions of dollars, the industry shifted
toward parallelism and specialization. }, which previously ensured
predictable enhancements in transistor density every 18 to 24 months,
and the end of Dennard scaling\sidenote{\textbf{Dennard Scaling}: Robert
Dennard's 1974 principle that as transistors shrink, their power density
remains constant, allowing higher frequencies without increased power
consumption. This enabled CPUs to reach 3+ GHz by 2005. However, quantum
effects and leakage current ended Dennard scaling around 2005, forcing
architects to prioritize efficiency over raw speed and leading to the
multi-core revolution. } (\citeproc{ref-dennard1974design}{Dennard et
al. 1974}), which permitted frequency increases without corresponding
power increases, created a performance and efficiency bottleneck in
general-purpose computing. As John Hennessy and David Patterson noted in
their 2017 Turing Lecture
(\citeproc{ref-HennessyPatterson2017Turing}{Hennessy and Patterson
2019})\sidenote{\textbf{Hennessy \& Patterson}: John Hennessy (former
President of Stanford) and David Patterson (UC Berkeley Professor) are
the ``godfathers'' of modern computer architecture. They pioneered
Reduced Instruction Set Computing (RISC), transforming processor design
from an art into a quantitative science. Their textbook \emph{Computer
Architecture: A Quantitative Approach} is the definitive reference in
the field. They received the 2017 ACM A.M. Turing Award for these
contributions. Patterson later joined Google Brain to help design the
TPU, directly applying their principles to AI acceleration. }, these
limitations signaled the onset of a new era in computer architecture
centered on domain-specific solutions that optimize hardware for
specialized workloads.

\subsection{The Technology S-Curve: Why We Must
Shift}\label{sec-ai-acceleration-technology-s-curve}

To understand the gravity of this transition, we must view it through
the lens of the \textbf{Technology S-Curve}. Every computing paradigm
follows a distinct lifecycle characterized by three phases:
\textbf{Ferment} (initial slow progress), \textbf{Take-off} (exponential
growth), and \textbf{Saturation} (diminishing returns due to physical
limits).

:::

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{contents/vol1/hw_acceleration/hw_acceleration_files/figure-pdf/fig-tech-s-curve-3.pdf}}

}

\caption{\label{fig-tech-s-curve}\textbf{The Twin S-Curves of Modern
Computing}. General-purpose CPUs (gray) enjoyed decades of exponential
growth driven by Moore's Law and Dennard Scaling. As physics constrained
this curve around 2010 (Saturation), the industry was forced to jump to
a new curve: Domain Specific Architectures (blue). We are currently in
the \textbf{Take-off} phase of this new paradigm, where massive
efficiency gains are unlocked by specializing hardware for linear
algebra, albeit at the cost of general programmability.}

\end{figure}%

As Figure~\ref{fig-tech-s-curve} illustrates, general-purpose computing
has entered its saturation phase. The ``easy'' gains from shrinking
transistors are gone. To sustain the exponential growth required by AI
models (which are growing 4--10\(\times\) faster than Moore's Law), we
cannot simply wait for the next CPU generation. We must shift to a new
curve---one defined not by clock speed, but by \textbf{Architecture}. To
understand how we reached this inflection point, we must first examine
the mechanics of the scaling laws that once fueled the general-purpose
era.

Historically, improvements in processor performance depended on
semiconductor process scaling and increasing clock speeds. As power
density limitations restricted further frequency scaling and transistor
miniaturization encountered increasing physical and economic
constraints, architects explored alternative approaches to sustain
computational growth. The result was a shift toward domain-specific
architectures, which dedicate silicon resources to optimize computation
for specific application domains, trading flexibility for efficiency.

Domain-specific architectures achieve superior performance and energy
efficiency through several key principles:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Customized data paths}: Design processing paths specifically
  optimized for target application patterns, enabling direct hardware
  execution of common operations. For example, matrix multiplication
  units in AI accelerators implement \textbf{systolic arrays} (grid-like
  networks of processing elements that rhythmically compute and pass
  data through neighboring units) tailored for neural network
  computations.
\item
  \textbf{Specialized memory hierarchies}: Optimize memory systems
  around domain-specific access patterns and data reuse characteristics.
  This includes custom cache configurations, prefetching logic, and
  memory controllers tuned for expected workloads.
\item
  \textbf{Reduced instruction overhead}: Implement domain-specific
  instruction sets that minimize decode and dispatch complexity by
  encoding common operation sequences into single instructions. This
  improves both performance and energy efficiency.
\item
  \textbf{Direct hardware implementation}: Create dedicated circuit
  blocks that natively execute frequently used operations without
  software intervention. This eliminates instruction processing overhead
  and maximizes throughput.
\end{enumerate}

Modern smartphones illustrate these principles compellingly. They can
decode 4K video at 60 frames per second while consuming only a few watts
of power, despite video processing requiring billions of operations per
second. This efficiency is achieved through dedicated hardware video
codecs that implement industry standards such as H.264/AVC (introduced
in 2003) and H.265/HEVC (finalized in 2013)
(\citeproc{ref-sullivan2012overview}{Sullivan et al. 2012}). These
specialized circuits provide 100--1000\(\times\) improvements in both
performance and power efficiency compared to software-based decoding on
general-purpose processors.

The trend toward specialization continues to accelerate, with new
architectures emerging for an expanding range of domains. Genomics
processing benefits from custom accelerators that optimize sequence
alignment and variant calling, reducing the time required for DNA
analysis (\citeproc{ref-Shang2018GenomicsAccel}{Shang, Wang, and Liu
2018}). Similarly, blockchain computation has produced
application-specific integrated circuits
(ASICs)\sidenote{\textbf{Application-Specific Integrated Circuits
(ASICs)}: Custom silicon chips designed for a single application,
offering maximum efficiency by eliminating unused features. For
well-defined workloads, ASICs can achieve orders-of-magnitude better
energy efficiency than general-purpose processors (often (10\^{}3) to
(10\^{}5\times) in specialized domains). However, their inflexibility
means they can become obsolete if algorithms or standards change (for
example, when cryptocurrency networks change consensus mechanisms). }
optimized for cryptographic hashing, substantially increasing the
efficiency of mining operations
(\citeproc{ref-Taylor2017ASICMining}{Bedford Taylor 2017}).

This shift represents an important engineering lesson: the era of
``free'' performance gains from general-purpose scaling is over. For
decades, software engineers could rely on Moore's Law to accelerate
existing code without architectural changes. The breakdown of Dennard
scaling forced a fundamental change: we can no longer wait for faster
CPUs to solve computational bottlenecks. Instead, we must design the
hardware to fit the algorithm. This necessity of hardware-software
co-design is why modern AI engineering requires deep understanding of
the underlying silicon. Performance is now determined by how well the
algorithm's memory access patterns and parallelism map to the
specialized physical structures of domain-specific architectures.

\subsection{Machine Learning Hardware
Specialization}\label{sec-ai-acceleration-machine-learning-hardware-specialization-09c5}

Machine learning constitutes a computational domain with unique
characteristics that have driven the development of specialized hardware
architectures. Unlike traditional computing workloads that exhibit
irregular memory access patterns and diverse instruction streams, neural
networks are characterized by predictable patterns: dense matrix
multiplications, regular data flow, and tolerance for reduced precision.
These characteristics enable specialized hardware optimizations that
would be ineffective for general-purpose computing but provide
substantial speedups for ML workloads. The hardware built to exploit
these patterns constitutes a class of devices known as \emph{ML
accelerators}.

\phantomsection\label{callout-definitionux2a-1.7}
\begin{fbx}{callout-definition}{Definition:}{ML Accelerator}
\phantomsection\label{callout-definition*-1.7}
\textbf{\emph{Machine Learning Accelerators}} are domain-specific
processors optimized for \textbf{Dense Linear Algebra}. They achieve
order-of-magnitude efficiency gains over CPUs by maximizing
\textbf{Parallel Throughput} and \textbf{Data Reuse} (via systolic
arrays or tensor cores), specifically addressing the \textbf{Integration
Bottleneck} of moving data to compute.

\end{fbx}

Machine learning computational requirements reveal limitations in
traditional processors. CPUs achieve only 5-10\% utilization on neural
network workloads, delivering approximately 100
GFLOPS\sidenote{\textbf{GFLOPS/TOPS Performance Metrics}: GFLOPS (10⁹
floating-point ops/second) measures floating-point throughput; TOPS
(10¹² ops/second) typically measures INT8 operations in AI accelerators.
The A100 delivers \texttt{\{python\}\ a100\_tflops\_fp16} TFLOPS for
FP16/BF16 Tensor Core operations
(\texttt{\{python\}\ a100\_tflops\_tf32} TFLOPS for TF32, or
\texttt{\{python\}\ a100\_tflops\_fp16} TFLOPS with sparsity enabled);
Apple A17 NPU achieves \texttt{\{python\}\ mobile\_tops} INT8 TOPS. Real
workload performance depends on memory bandwidth, achieving 10-30\% of
peak on typical ML models. } while consuming hundreds of watts. This
inefficiency results from architectural mismatches: CPUs optimize for
single-thread performance and irregular memory access, while neural
networks require massive parallelism and predictable data streams. The
memory bandwidth\sidenote{Memory bandwidth (introduced in
\textbf{?@sec-introduction}) measures data transfer rate between memory
and processors. For accelerator design, the gap between CPU memory (tens
to hundreds of GB/s) and HBM (terabytes per second) explains why
specialized hardware achieves 10-100x speedups on bandwidth-bound neural
network operations. } constraint becomes particularly severe: a single
neural network layer may require accessing gigabytes of parameters,
overwhelming CPU cache hierarchies\sidenote{\textbf{Cache}: From French
``cacher'' (to hide), describing memory that stores frequently accessed
data out of sight from the programmer. The IBM System/360 Model 85
(1968) introduced the first commercial cache. Modern CPUs use
multi-level hierarchies (L1/L2/L3) with progressively larger capacity
but higher latency. Neural networks often exceed cache capacity, forcing
frequent DRAM accesses that reduce effective throughput. } designed for
kilobyte-scale working sets.

The energy economics of data movement influence accelerator design.
Accessing data from DRAM can consume on the order of (10\^{}2)× more
energy than a multiply-accumulate operation (exact values vary by
technology node and design), making minimizing data movement a primary
optimization target. This disparity helps explain the progression from
repurposed graphics processors to purpose-built neural network
accelerators. TPUs and other custom accelerators can sustain high
utilization on dense kernels by implementing systolic arrays and other
architectures that maximize data reuse while minimizing movement.

Training and inference present distinct computational profiles that
influence accelerator design. Training requires high-precision
arithmetic (FP32 or FP16) for gradient computation and weight updates,
bidirectional data flow for
backpropagation\sidenote{\textbf{Backpropagation}: Short for ``backward
propagation of errors,'' formalized by Rumelhart, Hinton, and Williams
in 1986, though the mathematical foundation (reverse-mode automatic
differentiation) traces to the 1960s. The name describes its mechanism:
errors flow backward from output to input, propagating gradients through
the network via the chain rule. This bidirectional data flow requires
storing all intermediate activations, increasing memory requirements
2-3x versus inference-only forward passes. }, and large memory capacity
for storing activations. Inference can exploit reduced precision (INT8
or INT4), requires only forward computation, and prioritizes latency
over throughput\sidenote{\textbf{Latency vs Throughput}: ``Latency''
from Latin ``latere'' (to lie hidden), originally described the delay
before something becomes apparent. ``Throughput'' emerged from
industrial manufacturing, measuring production rate. In computing,
latency measures single-request response time (milliseconds), while
throughput measures processing rate (requests/second). Training
optimizes throughput for batch processing; inference prioritizes latency
for real-time responses. }. These differences drive specialized
architectures: training accelerators maximize FLOPS and memory
bandwidth, while inference accelerators optimize for energy efficiency
and deterministic latency.

Deployment context shapes architectural choices. Datacenter accelerators
often operate within \emph{hundreds of watts} to maximize throughput for
training massive models. Edge devices must deliver real-time inference
within \emph{tight power budgets} (often milliwatts to a few watts),
driving architectures that minimize unnecessary data movement. Mobile
processors balance performance with battery life, while automotive
systems prioritize deterministic response times for safety-critical
applications. This diversity has produced a rich ecosystem of
specialized accelerators, each optimized for specific deployment
scenarios and computational requirements. The following examples
illustrate how different deployment contexts drive distinct
architectural priorities.

In data centers, training accelerators such as NVIDIA H100 and Google
TPUv4 can reduce model iteration time substantially through massive
parallelism and high-bandwidth memory systems. These systems prioritize
raw computational throughput, accepting high power consumption to
achieve petaflop-scale performance. The economics can support this
trade-off when faster iteration reduces both time-to-deploy and the
cumulative cost of long-running training jobs.

At the opposite extreme, edge deployment requires different optimization
strategies. Processing-in-memory architectures reduce data movement by
integrating compute more directly with memory. Dynamic voltage scaling
can reduce power substantially during low-intensity operations.
Neuromorphic designs process only changing inputs, which can yield large
power reductions for temporal workloads, sometimes approaching
orders-of-magnitude improvements. These techniques enable sophisticated
AI models to operate continuously on battery power, supporting
applications from smartphone photography to autonomous sensors that can
function for months to years without external power.

The success of application-specific accelerators demonstrates that no
single architecture can efficiently address all ML workloads. A massive
installed base of edge devices demands architectures optimized for
energy efficiency and real-time latency targets, while cloud-scale
training continues advancing the boundaries of computational throughput.
This diversity drives continued innovation in specialized architectures,
each optimized for its specific deployment context and computational
requirements. However, despite this diversity, all accelerators operate
under the same physical constraints. Verify your understanding of the
energy physics driving this specialization.

\phantomsection\label{callout-checkpointux2a-1.8}
\begin{fbx}{callout-checkpoint}{Checkpoint:}{The Accelerator Gate}
\phantomsection\label{callout-checkpoint*-1.8}

Hardware specialization is driven by energy physics.

\textbf{The Energy Inversion}

\begin{itemize}
\tightlist
\item[$\square$]
  \textbf{Data Movement Cost}: Can you explain why moving data from DRAM
  costs 100x more energy than computing on it?
\item[$\square$]
  \textbf{Architectural Response}: How do \textbf{Systolic Arrays} (TPU)
  and \textbf{Tensor Cores} (GPU) minimize this cost? (They reuse data
  registers for many ops before writing back).
\end{itemize}

\textbf{Selection Logic}

\begin{itemize}
\tightlist
\item[$\square$]
  \textbf{Training vs.~Inference}: Why do training chips need massive
  HBM bandwidth, while inference chips prioritize low latency and INT8
  ops?
\end{itemize}

\end{fbx}

This historical progression reveals a key pattern: each wave of hardware
specialization responded to a specific computational bottleneck.
Floating-point coprocessors addressed arithmetic precision limitations.
GPUs addressed graphics throughput limitations. But what bottleneck does
AI acceleration address? Understanding this question matters because it
reveals \emph{why} modern accelerators are designed the way they are,
and why simply adding more transistors to general-purpose processors
cannot solve this challenge. Before examining this integration
bottleneck in detail, Table~\ref{tbl-hw-evolution} summarizes the key
milestones in hardware specialization. While these accelerators
initially emerged to optimize domain-specific workloads such as
floating-point operations, graphics rendering, and media processing,
they also introduced architectural strategies that persist in
contemporary systems. The specialization principles from earlier
generations now underpin the design of modern AI accelerators and
provide context for understanding how hardware specialization continues
to enable scalable, efficient execution of machine learning workloads
across diverse deployment environments.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.0833}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2569}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3194}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3264}}@{}}
\caption{\textbf{Hardware Specialization Trends.} Successive computing
eras progressively integrate specialized hardware to accelerate
prevalent workloads, moving from general-purpose CPUs to domain-specific
architectures and ultimately to customizable AI accelerators. Tailoring
hardware to computational patterns improves performance and energy
efficiency, driving innovation in machine learning
systems.}\label{tbl-hw-evolution}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Era}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Computational Pattern}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Architecture Examples}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Characteristics}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Era}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Computational Pattern}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Architecture Examples}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Characteristics}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{1980s} & Floating-Point \& Signal Processing & FPU, DSP &
\begin{minipage}[t]{\linewidth}\raggedright
Single-purpose engines

Focused instruction sets

Coprocessor interfaces
\end{minipage} \\
\textbf{1990s} & 3D Graphics \& Multimedia & GPU, SIMD Units &
\begin{minipage}[t]{\linewidth}\raggedright
Many identical compute units

Regular data patterns

Wide memory interfaces
\end{minipage} \\
\textbf{2000s} & Real-time Media Coding & Media Codecs, Network
Processors & \begin{minipage}[t]{\linewidth}\raggedright
Fixed-function pipelines

High throughput processing

Power-performance optimization
\end{minipage} \\
\textbf{2010s} & Deep Learning Tensor Operations & TPU, GPU Tensor Cores
& \begin{minipage}[t]{\linewidth}\raggedright
Matrix multiplication units

Massive parallelism

Memory bandwidth optimization
\end{minipage} \\
\textbf{2020s} & Application-Specific Acceleration & ML Engines, Smart
NICs, Domain Accelerators & \begin{minipage}[t]{\linewidth}\raggedright
Workload-specific datapaths

Customized memory hierarchies

Application-optimized designs
\end{minipage} \\
\end{longtable}

What distinguishes AI acceleration from earlier specialization waves is
the scale of integration required. AI accelerators must work seamlessly
with frameworks like TensorFlow, PyTorch, and JAX. They require
sophisticated compiler support for graph-level transformations, kernel
fusion, and memory scheduling. They must also deploy across environments
from data centers to mobile devices, each with distinct performance and
efficiency requirements. This system-level transformation requires tight
hardware-software coupling, a theme that will recur throughout this
chapter.

But first, we must understand \emph{what} bottleneck AI accelerators are
designed to solve. Unlike floating-point coprocessors that addressed
arithmetic precision or GPUs that addressed graphics throughput, AI
accelerators target a qualitatively different constraint. The answer
determines every subsequent architectural decision.

\subsection{The Integration Bottleneck: Why AI Needs Specialized
Hardware}\label{sec-ai-acceleration-integration-bottleneck-ai-needs-specialized-hardware-0b41}

Machine learning represents a computational domain where the primary
performance limit has shifted from \textbf{arithmetic} to
\textbf{integration}. While early coprocessors solved the
\emph{Precision Bottleneck} (8087) and GPUs solved the \emph{Throughput
Bottleneck} (rasterization), modern AI workloads are constrained by the
\textbf{Integration Bottleneck}: the energy and latency cost of moving
massive amounts of data between memory and thousands of parallel compute
units.

Neural networks are characterized by three unique properties that drive
this shift:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Massive Parallelism}: Unlike general-purpose code with complex
  branching, neural networks execute billions of independent matrix
  multiplications and convolutions. This regular structure allows
  replacing complex CPU control logic with dense arrays of processing
  elements (systolic arrays).
\item
  \textbf{Predictable Data Flow}: Data movement in deep learning is
  mathematically determined by the network's layers. This predictability
  enables hardware to ``prefetch'' data into local
  scratchpads\sidenote{\textbf{Scratchpad}: From the physical scratch
  pads used for quick calculations before computers. In hardware,
  scratchpad memory is software-managed fast memory near compute units,
  contrasting with caches that are hardware-managed. Unlike caches that
  guess what data to keep, scratchpads give programmers explicit control
  over what data resides in fast memory, enabling the predictable data
  movement that ML workloads require. }, bypassing the expensive
  random-access cache hierarchies of CPUs.
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  \textbf{Tolerance for Reduced Precision}: Neural networks typically
  remain robust even when using 8-bit or 4-bit integers instead of
  64-bit floating-point numbers. This flexibility allows architects to
  fit 10\(\times\) more compute units in the same silicon area.
\end{enumerate}

The primary engineering challenge is no longer ``how fast can we
calculate?'' but ``how close can we keep the data to the calculation?''
In modern accelerators, accessing data from external memory (DRAM) can
consume \(100\times\) more energy than the actual arithmetic operation.
This disparity drives the ``Anatomy of an Accelerator''
(Figure~\ref{fig-accelerator-anatomy}), prioritizing high-bandwidth
memory (HBM)\sidenote{HBM (introduced in
\textbf{?@sec-dnn-architectures}) achieves 2-10x higher bandwidth than
GDDR memory through 3D die stacking with thousands of TSVs. From a
hardware architecture perspective, HBM's 2-3 TB/s bandwidth (vs.~500-700
GB/s for GDDR6X) transforms memory-bound ML workloads toward
compute-bound performance. The trade-off is higher manufacturing cost,
limiting HBM to data center accelerators where bandwidth justifies the
premium. } and large on-chip scratchpads to minimize data movement.

The evolution from the Intel 8087 to the Google TPU reveals a consistent
pattern: hardware evolves to fit the algorithm's dominant bottleneck.
Where the 8087 addressed floating-point operations that consumed 80\% of
scientific computing time, modern AI accelerators address matrix
operations that constitute over 95\% of neural network computation. This
concentration of demand explains why specialized AI silicon achieves
100-1000\(\times\) performance improvements over general-purpose
processors.

The constraints identified above (massive parallelism, predictable data
flow, and tolerance for reduced precision) shape accelerator
architecture. Before examining the computational primitives that exploit
these characteristics, we examine the architectural organization that
enables their efficient execution. Modern AI accelerators achieve their
dramatic performance improvements through a carefully orchestrated
hierarchy of specialized components operating in concert.

The processing substrate consists of an array of processing elements,
each containing dedicated computational units optimized for specific
operations: tensor cores execute matrix multiplication, vector units
perform element-wise operations, and special function units compute
activation functions. These processing elements are organized in a grid
topology that enables massive parallelism, with dozens to hundreds of
units operating simultaneously on different portions of the computation,
exploiting the data-level parallelism inherent in neural network
workloads.

The memory hierarchy forms an equally critical architectural component.
High-bandwidth memory provides the aggregate throughput required to
sustain these numerous processing elements, while a multi-level cache
hierarchy from shared L2 caches down to per-element L1 caches and
scratchpads minimizes the energy cost of data movement. This
hierarchical organization embodies a core design principle: in AI
accelerators, data movement typically consumes more energy than
computation itself, necessitating architectural strategies that
prioritize data reuse by maintaining frequently accessed values
(including weights and partial results) in proximity to compute units.

The host interface establishes connectivity between the specialized
accelerator and the broader computing system, enabling coordination
between general-purpose CPUs that manage program control flow and the
accelerator that executes computationally intensive neural network
operations.

This architectural partitioning reflects specialization at the system
level: CPUs address control flow, conditional logic, and system
coordination, while accelerators focus on the regular, massively
parallel arithmetic operations that dominate neural network execution.
Figure~\ref{fig-accelerator-anatomy} shows how specialized compute
units, hierarchical memory subsystems, and host connectivity integrate
to form a system optimized for AI workloads.

\begin{figure}[htb]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/7f29359437cc0db6a5ac070abeb820ec79ca2f78.pdf}}

}

\caption{\label{fig-accelerator-anatomy}\textbf{Anatomy of a Modern AI
Accelerator}: AI accelerators integrate specialized processing elements
containing tensor cores, vector units, and special function units,
supported by a hierarchical memory system from high-bandwidth memory
down to local caches. This architecture maximizes data reuse and
parallel execution while minimizing energy-intensive data movement,
forming the foundation for 100-1000× performance improvements over
general-purpose processors.}

\end{figure}%

\section{AI Compute
Primitives}\label{sec-ai-acceleration-ai-compute-primitives-2c99}

The accelerator architecture presented in
Figure~\ref{fig-accelerator-anatomy} raises an immediate question:
\emph{why} these specific components? The tensor cores, vector units,
and hierarchical memory exist not by accident but because neural network
computations repeatedly invoke a small set of operations. Understanding
these computational patterns, which we call compute primitives, reveals
why specialized hardware achieves 100-1000x improvements over
general-purpose processors. The transition from CPUs achieving
approximately 100 GFLOPS to accelerators delivering 100,000+ GFLOPS
reflects architectural optimization for these specific patterns, which
appear repeatedly across all neural network architectures regardless of
application domain or model size.

These patterns manifest in a small number of core computational
operations. Regardless of the layer type, whether fully connected,
convolutional, or attention-based layers, the underlying operation
typically involves multiplying input values by learned weights and
accumulating the results. This repeated multiply-accumulate process
dominates neural network execution and defines the arithmetic foundation
of AI workloads. The regularity and frequency of these operations have
led to the development of AI compute primitives: hardware-level
abstractions optimized to execute these core computations with high
efficiency.

These recurring multiply-accumulate operations exhibit a key property:
they are highly structured and data-parallel, enabling architectural
specialization. Building on the parallelization principles established
in
Section~\ref{sec-ai-acceleration-parallel-computing-graphics-processing-4654},
these patterns emphasize predictable data reuse and fixed operation
sequences. AI compute primitives distill these patterns into reusable
architectural units that support high-throughput and energy-efficient
execution.

Listing~\ref{lst-dense_layer_def} demonstrates how a dense layer
decomposes at the framework level, encapsulating thousands of
multiply-accumulate operations in a single high-level call.

\begin{codelisting}

\caption{\label{lst-dense_layer_def}\textbf{Dense Layer Abstraction}:
High-level framework APIs encapsulate 131,072 multiply-accumulate
operations (256 inputs times 512 outputs) in a single function call,
hiding the computational complexity from developers while enabling
automatic hardware optimization.}

\centering{

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Framework abstracts compute{-}intensive operations}
\NormalTok{dense }\OperatorTok{=}\NormalTok{ Dense(}\DecValTok{512}\NormalTok{)(input\_tensor)  }\CommentTok{\# 256×512 = 131K MACs per sample}
\end{Highlighting}
\end{Shaded}

}

\end{codelisting}%

Listing~\ref{lst-dense_expansion} reveals how the framework expands this
high-level call into mathematical operations.

\begin{codelisting}

\caption{\label{lst-dense_expansion}\textbf{Matrix Operation Expansion}:
Each dense layer decomposes into matrix multiplication and element-wise
operations, exposing the dominant compute pattern that consumes over
95\% of neural network execution time.}

\centering{

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Linear transformation: O(input\_dim × output\_dim × batch) operations}
\NormalTok{output }\OperatorTok{=}\NormalTok{ (}
\NormalTok{    matmul(}\BuiltInTok{input}\NormalTok{, weights) }\OperatorTok{+}\NormalTok{ bias}
\NormalTok{)  }\CommentTok{\# Matrix multiply dominates cost}
\NormalTok{output }\OperatorTok{=}\NormalTok{ activation(output)  }\CommentTok{\# Element{-}wise: O(output\_dim × batch)}
\end{Highlighting}
\end{Shaded}

}

\end{codelisting}%

At the processor level, Listing~\ref{lst-loop_level_dense} reveals how
nested loops multiply inputs and weights, sum the results, and apply a
nonlinear function, exposing the O(batch times input times output)
complexity that accelerators must handle efficiently.

\begin{codelisting}

\caption{\label{lst-loop_level_dense}\textbf{Processor-Level Execution}:
Nested loops reveal the O(batch times input times output)
multiply-accumulate operations that accelerators must execute, with 4
million MACs for typical batch=32, input=256, output=512
configurations.}

\centering{

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Total operations: batch\_size × output\_size × input\_size MACs}
\ControlFlowTok{for}\NormalTok{ n }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(batch\_size):  }\CommentTok{\# Batch dimension: parallelizable}
    \ControlFlowTok{for}\NormalTok{ m }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(output\_size):  }\CommentTok{\# Output neurons: parallelizable}
        \BuiltInTok{sum} \OperatorTok{=}\NormalTok{ bias[m]  }\CommentTok{\# Initialize accumulator}
        \ControlFlowTok{for}\NormalTok{ k }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(input\_size):  }\CommentTok{\# Reduction dimension: sequential}
            \BuiltInTok{sum} \OperatorTok{+=} \BuiltInTok{input}\NormalTok{[n, k] }\OperatorTok{*}\NormalTok{ weights[k, m]  }\CommentTok{\# MAC operation}
\NormalTok{        output[n, m] }\OperatorTok{=}\NormalTok{ activation(}\BuiltInTok{sum}\NormalTok{)  }\CommentTok{\# Non{-}linear transformation}
\CommentTok{\# Example: 32 × 512 × 256 = 4.2M multiply{-}accumulate operations}
\end{Highlighting}
\end{Shaded}

}

\end{codelisting}%

This transformation reveals four computational characteristics. First,
data-level parallelism enables simultaneous execution across independent
operations. Second, structured matrix operations define the
computational workloads. Third, predictable data movement patterns drive
memory optimization strategies. Fourth, frequent nonlinear
transformations motivate the development of specialized function units.

The design of AI compute primitives follows three architectural
criteria. First, the primitive must be used frequently enough to justify
dedicated hardware resources. Second, its specialized implementation
must offer substantial performance or energy efficiency gains relative
to general-purpose alternatives. Third, the primitive must remain stable
across generations of neural network architectures to ensure long-term
applicability. These considerations shape the inclusion of primitives
such as vector operations, matrix operations, and special function units
in modern ML accelerators. Together, they serve as the architectural
foundation for efficient and scalable neural network execution.

\subsection{Vector
Operations}\label{sec-ai-acceleration-vector-operations-19bf}

Vector operations provide the first level of hardware acceleration by
processing multiple data elements simultaneously. This parallelism
exists at multiple scales, from individual neurons to entire layers,
making vector processing necessary for efficient neural network
execution. Framework-level code translates to hardware instructions,
revealing how vector processing enables neural accelerators.

\textbf{High-Level Framework Operations.} Machine learning frameworks
hide hardware complexity through high-level abstractions. These
abstractions decompose into progressively lower-level operations,
revealing opportunities for hardware acceleration.
Listing~\ref{lst-linear_layer_highlevel} illustrates this principle
through a linear layer's execution flow, where a single function call
transforms 256 input features into 512 outputs.

\begin{codelisting}

\caption{\label{lst-linear_layer_highlevel}\textbf{Linear Layer}: Neural
networks transform input data into a higher-dimensional space using
linear mappings to enable complex feature extraction.}

\centering{

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{layer }\OperatorTok{=}\NormalTok{ nn.Linear(}\DecValTok{256}\NormalTok{, }\DecValTok{512}\NormalTok{)  }\CommentTok{\# 256 inputs to}
\CommentTok{\# 512 outputs}
\NormalTok{output }\OperatorTok{=}\NormalTok{ layer(input\_tensor)  }\CommentTok{\# Process a batch of inputs}
\end{Highlighting}
\end{Shaded}

}

\end{codelisting}%

This abstraction represents a fully connected layer that transforms
input features through learned weights.
Listing~\ref{lst-linear_math_internal} exposes the mathematical
operations behind this high-level expression, revealing hardware
acceleration opportunities.

\begin{codelisting}

\caption{\label{lst-linear_math_internal}\textbf{Fully Connected Layer}:
Each output is computed as a weighted sum of all inputs plus a bias,
followed by an activation function transformation. Linear
transformations enable complex model architectures in neural networks.}

\centering{

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Z }\OperatorTok{=}\NormalTok{ matmul(weights, }\BuiltInTok{input}\NormalTok{) }\OperatorTok{+}\NormalTok{ bias  }\CommentTok{\# Each output needs all inputs}
\NormalTok{output }\OperatorTok{=}\NormalTok{ activation(Z)  }\CommentTok{\# Transform each result}
\end{Highlighting}
\end{Shaded}

}

\end{codelisting}%

During processor execution, these mathematical operations decompose into
explicit computational steps. Listing~\ref{lst-loop_linear_layer}
demonstrates how nested loops implement the multiply-accumulate
operations.

\begin{codelisting}

\caption{\label{lst-loop_linear_layer}\textbf{Linear Layer Computation}:
Each output neuron is computed by summing weighted inputs from all
features, followed by an activation function application. Understanding
this process helps in grasping the fundamental building blocks of neural
networks.}

\centering{

\begin{Shaded}
\begin{Highlighting}[]
\ControlFlowTok{for}\NormalTok{ batch }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\DecValTok{32}\NormalTok{):            }\CommentTok{\# Process 32 samples at once}
    \ControlFlowTok{for}\NormalTok{ out\_neuron }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\DecValTok{512}\NormalTok{):  }\CommentTok{\# Compute each output neuron}
        \BuiltInTok{sum} \OperatorTok{=} \FloatTok{0.0}
        \ControlFlowTok{for}\NormalTok{ in\_feature }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\DecValTok{256}\NormalTok{): }\CommentTok{\# Each output needs}
                                      \CommentTok{\# all inputs}
            \BuiltInTok{sum} \OperatorTok{+=} \BuiltInTok{input}\NormalTok{[batch, in\_feature] }\OperatorTok{*}
\NormalTok{                         weights[out\_neuron, in\_feature]}
\NormalTok{        output[batch, out\_neuron] }\OperatorTok{=}\NormalTok{ activation(}\BuiltInTok{sum} \OperatorTok{+}
\NormalTok{                                    bias[out\_neuron])}
\end{Highlighting}
\end{Shaded}

}

\end{codelisting}%

\textbf{Sequential Scalar Execution.} Traditional scalar processors
execute these operations sequentially, processing individual values one
at a time. For the linear layer example above with a batch of 32
samples, computing the outputs requires over 4 million
multiply-accumulate operations. Each operation involves loading an input
value and a weight value, multiplying them, and accumulating the result.
This sequential approach becomes highly inefficient when processing the
massive number of identical operations required by neural networks.

To address this inefficiency, modern processors use vector processing to
execute multiple operations simultaneously.

\textbf{Parallel Vector Execution.} Vector processing units achieve this
transformation by operating on multiple data elements simultaneously.
Listing~\ref{lst-riscv_vector_mac} reveals these capabilities through
RISC-V\sidenote{\textbf{RISC-V for AI}: RISC-V, the open-source
instruction set architecture from UC Berkeley (2010), is becoming
important for AI accelerators because it's freely customizable.
Companies like SiFive and Google have created RISC-V chips with custom
AI extensions. Unlike proprietary architectures, RISC-V allows hardware
designers to add specialized ML instructions without licensing fees,
potentially democratizing AI hardware development beyond the current
duopoly of x86 and ARM. } assembly code, where a single instruction
processes eight data elements in parallel.

\begin{codelisting}

\caption{\label{lst-riscv_vector_mac}\textbf{Vectorized
Multiply-Accumulate Loop}: This loop showcases how RISC-V vector
instructions enable efficient batch processing by performing 8
multiply-add operations simultaneously, reducing computational latency
in neural network training. (\citeproc{ref-riscv_manual}{Waterman et al.
2013})}

\centering{

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{vsetvli t0}\OperatorTok{,}\NormalTok{ a0}\OperatorTok{,}\NormalTok{ e32   \# }\OperatorTok{\textless{}}\DecValTok{1}\OperatorTok{\textgreater{}}
\NormalTok{loop\_batch}\OperatorTok{:}
\NormalTok{    loop\_neuron}\OperatorTok{:}
\NormalTok{        vxor}\OperatorTok{.}\NormalTok{vv v0}\OperatorTok{,}\NormalTok{ v0}\OperatorTok{,}\NormalTok{ v0    \# }\OperatorTok{\textless{}}\DecValTok{2}\OperatorTok{\textgreater{}}
\NormalTok{        loop\_feature}\OperatorTok{:}
\NormalTok{            vle32}\OperatorTok{.}\NormalTok{v v1}\OperatorTok{,} \OperatorTok{(}\NormalTok{in\_ptr}\OperatorTok{)}\NormalTok{    \# }\OperatorTok{\textless{}}\DecValTok{3}\OperatorTok{\textgreater{}}
\NormalTok{            vle32}\OperatorTok{.}\NormalTok{v v2}\OperatorTok{,} \OperatorTok{(}\NormalTok{wt\_ptr}\OperatorTok{)}\NormalTok{    \# }\OperatorTok{\textless{}}\DecValTok{3}\OperatorTok{\textgreater{}}
\NormalTok{            vfmacc}\OperatorTok{.}\NormalTok{vv v0}\OperatorTok{,}\NormalTok{ v1}\OperatorTok{,}\NormalTok{ v2    \# }\OperatorTok{\textless{}}\DecValTok{4}\OperatorTok{\textgreater{}}
\NormalTok{            add in\_ptr}\OperatorTok{,}\NormalTok{ in\_ptr}\OperatorTok{,} \DecValTok{32}\NormalTok{  \# }\OperatorTok{\textless{}}\DecValTok{5}\OperatorTok{\textgreater{}}
\NormalTok{            add wt\_ptr}\OperatorTok{,}\NormalTok{ wt\_ptr}\OperatorTok{,} \DecValTok{32}\NormalTok{  \# }\OperatorTok{\textless{}}\DecValTok{5}\OperatorTok{\textgreater{}}
\NormalTok{            bnez feature\_cnt}\OperatorTok{,}\NormalTok{ loop\_feature}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Vector Length Configuration}: Configures the vector units to
  process 32-bit elements, automatically determining how many operations
  happen in parallel based on hardware width (VLEN).
\item
  \textbf{Vector Initialization}: Clears the accumulator vector
  \texttt{v0} (containing e.g., 8 parallel sums) using an exclusive-OR
  operation, which is more efficient than a load immediate.
\item
  \textbf{Vector Loads}: Loads continuous 32-bit input and weight values
  from memory into vector registers \texttt{v1} and \texttt{v2} in a
  single instruction, maximizing memory bandwidth utilization.
\item
  \textbf{Fused Multiply-Accumulate}: Performs parallel multiply-add
  operations (\(v_0 = v_0 + v_1 \times v_2\)). This is the core
  computational primitive, doubling throughput compared to separate
  multiply and add instructions.
\item
  \textbf{Pointer Arithmetic}: Updates memory pointers by the vector
  byte length to prepare for the next data chunk.
\end{enumerate}

}

\end{codelisting}%

This vector implementation processes eight data elements in parallel,
reducing both computation time and energy consumption. Vector load
instructions transfer eight values simultaneously, maximizing memory
bandwidth utilization. The vector multiply-accumulate instruction
processes eight pairs of values in parallel, dramatically reducing the
total instruction count from over 4 million to approximately 500,000.

Key vector operations map directly to common deep learning patterns.
Table~\ref{tbl-vector} enumerates how operations such as reduction,
gather, scatter, and masked operations appear frequently in pooling,
embedding lookups, and attention mechanisms, clarifying the direct
mapping between low-level vector hardware and high-level machine
learning workloads.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2290}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4122}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3511}}@{}}
\caption{\textbf{Vector Operations.} Core vector operations, including
reduction, gather, and scatter, accelerate computation and efficiently
process data in parallel. These operations enable efficient
implementation of common layers like pooling, embedding lookups, and
attention mechanisms within deep learning
models.}\label{tbl-vector}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Vector Operation}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Description}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Neural Network Application}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Vector Operation}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Description}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Neural Network Application}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Reduction} & Combines elements across a vector (e.g., sum, max)
& Pooling layers, attention score computation \\
\textbf{Gather} & Loads multiple non-consecutive memory elements &
Embedding lookups, sparse operations \\
\textbf{Scatter} & Writes to multiple non-consecutive memory locations &
Gradient updates for embeddings \\
\textbf{Masked operations} & Selectively operates on vector elements &
Attention masks, padding handling \\
\textbf{Vector-scalar broadcast} & Applies scalar to all vector elements
& Bias addition, scaling operations \\
\end{longtable}

Vector processing efficiency gains extend beyond instruction count
reduction. Memory bandwidth utilization improves as vector loads
transfer multiple values per operation. Energy efficiency increases
because control logic is shared across multiple operations. These
improvements compound across the deep layers of modern neural networks,
where billions of operations execute for each forward pass.

To understand why these architectural choices persist, we trace their
origins to the foundational systems that first demonstrated their value.

\textbf{Vector Processing History.} The principles underlying vector
operations have long been central to high-performance computing. In the
1970s and 1980s, vector processors emerged as an architectural solution
for scientific computing, weather modeling, and physics simulations,
where large arrays of data required efficient parallel processing. Early
systems such as the Cray-1\sidenote{\textbf{Cray-1 Vector Legacy}: The
Cray-1 (1975) cost \$8.8 million (approximately \$40-45 million in 2024
dollars) but could perform 160 million floating-point operations per
second, 1000x faster than typical computers. Its 64-element vector
registers and pipelined vector units established the architectural
template that modern AI accelerators still follow: process many data
elements simultaneously with specialized hardware pipelines. }, one of
the first commercially successful supercomputers, introduced dedicated
vector units to perform arithmetic operations on entire data vectors in
a single instruction. These vector units dramatically improved
computational throughput compared to traditional scalar execution
(\citeproc{ref-jordan1982guide}{Jordan 1982}).

These concepts have reemerged in machine learning, where neural networks
exhibit structure suited to vectorized execution. The same operations,
such as vector addition, multiplication, and reduction, that once
accelerated numerical simulations now drive the execution of machine
learning workloads. While the scale and specialization of modern AI
accelerators differ from their historical predecessors, the underlying
architectural principles remain the same.

Vector operations establish the foundation for neural network
acceleration by enabling efficient parallel processing of independent
data elements. While vector operations excel at element-wise
transformations like activation functions, neural networks also require
structured computations that combine multiple input features to produce
output features, transformations that naturally express themselves as
matrix operations. This need for coordinated computation across multiple
dimensions simultaneously leads to the next architectural primitive:
matrix operations.

\subsection{Matrix
Operations}\label{sec-ai-acceleration-matrix-operations-508d}

Matrix operations form the computational workhorse of neural networks,
transforming high-dimensional data through structured patterns of
weights, activations, and gradients
(\citeproc{ref-Goodfellow-et-al-2016}{Goodfellow, Courville, and Bengio
2013}). While vector operations process elements independently, matrix
operations orchestrate computations across multiple dimensions
simultaneously. These operations reveal patterns that drive hardware
acceleration strategies.

\subsubsection{Matrix Operations in Neural
Networks}\label{sec-ai-acceleration-matrix-operations-neural-networks-527a}

Neural network computations decompose into hierarchical matrix
operations. Listing~\ref{lst-linear_matrix_hierarchy} captures this
hierarchy through a linear layer that transforms input features into
output neurons over a batch.

\begin{codelisting}

\caption{\label{lst-linear_matrix_hierarchy}\textbf{Matrix Operations}:
Neural networks perform transformations using matrix multiplications and
biases to achieve output predictions. Training requires careful
management of input batches and activation functions to optimize model
performance.}

\centering{

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{layer }\OperatorTok{=}\NormalTok{ nn.Linear(}\DecValTok{256}\NormalTok{, }\DecValTok{512}\NormalTok{)  }\CommentTok{\# Layer transforms 256 inputs to}
\CommentTok{\# 512 outputs}
\NormalTok{output }\OperatorTok{=}\NormalTok{ layer(input\_batch)  }\CommentTok{\# Process a batch of 32 samples}

\CommentTok{\# Framework Internal: Core operations}
\NormalTok{Z }\OperatorTok{=}\NormalTok{ matmul(weights, }\BuiltInTok{input}\NormalTok{)  }\CommentTok{\# Matrix: transforms [256 x 32]}
\CommentTok{\# input to [512 x 32] output}
\NormalTok{Z }\OperatorTok{=}\NormalTok{ Z }\OperatorTok{+}\NormalTok{ bias  }\CommentTok{\# Vector: adds bias to each}
\CommentTok{\# output independently}
\NormalTok{output }\OperatorTok{=}\NormalTok{ relu(Z)  }\CommentTok{\# Vector: applies activation to}
\CommentTok{\# each element independently}
\end{Highlighting}
\end{Shaded}

}

\end{codelisting}%

This computation demonstrates the scale of matrix operations in neural
networks. Each output neuron (512 total) must process all input features
(256 total) for every sample in the batch (32 samples). The weight
matrix alone contains \(256 \times 512 =\)
\texttt{\{python\}\ wm\_params\_str} parameters that define these
transformations, illustrating why efficient matrix multiplication
dominates performance considerations.

Neural networks employ matrix operations across diverse architectural
patterns beyond simple linear layers.

\textbf{Types of Matrix Computations in Neural Networks.} Matrix
operations appear consistently across modern neural architectures.
Convolution operations transform into matrix multiplications through the
im2col technique\sidenote{\textbf{Im2col (Image-to-Column)}: A
preprocessing technique that converts convolution operations into matrix
multiplications by unfolding image patches into column vectors. A 3×3
convolution on a 224×224 image creates a matrix with
\textasciitilde50,000 columns, enabling efficient GEMM execution but
increasing memory usage 9× due to overlapping patches. This
transformation explains why convolutions are actually matrix operations
in modern ML accelerators. }, enabling efficient execution on
matrix-optimized hardware. Listing~\ref{lst-matrix_patterns} illustrates
these diverse applications.

\begin{codelisting}

\caption{\label{lst-matrix_patterns}\textbf{Linear Layers}: Layer
transformations combine input features to produce hidden
representations. Matrix operations in neural networks enable efficient
feature extraction and transformation, forming the backbone of many
machine learning architectures.}

\centering{

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{hidden }\OperatorTok{=}\NormalTok{ matmul(weights, inputs)}
\CommentTok{\# weights: [out\_dim x in\_dim], inputs: [in\_dim x batch]}
\CommentTok{\# Result combines all inputs for each output}

\CommentTok{\# Attention Mechanisms {-} Multiple matrix operations}
\NormalTok{Q }\OperatorTok{=}\NormalTok{ matmul(Wq, inputs)}
\CommentTok{\# Project inputs to query space [query\_dim x batch]}
\NormalTok{K }\OperatorTok{=}\NormalTok{ matmul(Wk, inputs)}
\CommentTok{\# Project inputs to key space [key\_dim x batch]}
\NormalTok{attention }\OperatorTok{=}\NormalTok{ matmul(Q, K.T)}
\CommentTok{\# Compare all queries with all keys [query\_dim x key\_dim]}

\CommentTok{\# Convolutions {-} Matrix multiply after reshaping}
\NormalTok{patches }\OperatorTok{=}\NormalTok{ im2col(}\BuiltInTok{input}\NormalTok{)}
\CommentTok{\# Convert [H x W x C] image to matrix of patches}
\NormalTok{output }\OperatorTok{=}\NormalTok{ matmul(kernel, patches)}
\CommentTok{\# Apply kernels to all patches simultaneously}
\end{Highlighting}
\end{Shaded}

}

\end{codelisting}%

This pervasive pattern of matrix multiplication has direct implications
for hardware design: the need for efficient matrix operations drives the
development of specialized hardware architectures that can handle these
computations at scale.

\subsubsection{Matrix Operations Hardware
Acceleration}\label{sec-ai-acceleration-matrix-operations-hardware-acceleration-514a}

The computational demands of matrix operations have driven specialized
hardware optimizations. Listing~\ref{lst-matrix_unit} demonstrates how
modern processors implement dedicated matrix units that process entire
16x16 blocks simultaneously, achieving 32x higher throughput than vector
processing alone.

\begin{codelisting}

\caption{\label{lst-matrix_unit}\textbf{Matrix Unit Operation}: Enables
efficient block-wise matrix multiplication and accumulation in
hardware-accelerated systems, demonstrating how specialized units
streamline computational tasks for AI/ML operations.}

\centering{

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mload mr1}\OperatorTok{,} \OperatorTok{(}\NormalTok{weight\_ptr}\OperatorTok{)}\NormalTok{     \# Load e}\OperatorTok{.}\NormalTok{g}\OperatorTok{.,} \DecValTok{16}\ErrorTok{x16}\NormalTok{ block of}
                            \PreprocessorTok{\# }\ErrorTok{weight matrix}
\NormalTok{mload mr2}\OperatorTok{,} \OperatorTok{(}\NormalTok{input\_ptr}\OperatorTok{)}\NormalTok{      \# Load corresponding input block}
\NormalTok{matmul}\OperatorTok{.}\NormalTok{mm mr3}\OperatorTok{,}\NormalTok{ mr1}\OperatorTok{,}\NormalTok{ mr2     \# Multiply and accumulate entire}
                            \PreprocessorTok{\# }\ErrorTok{blocks at once}
\NormalTok{mstore }\OperatorTok{(}\NormalTok{output\_ptr}\OperatorTok{),}\NormalTok{ mr3    \# Store computed output block}
\end{Highlighting}
\end{Shaded}

}

\end{codelisting}%

This matrix processing unit can handle \(16\times16\) blocks of the
linear layer computation described earlier, processing 256
multiply-accumulate operations simultaneously compared to the 8
operations possible with vector processing. These matrix operations
complement vectorized computation by enabling structured many-to-many
transformations. The interplay between matrix and vector operations
shapes the efficiency of neural network execution.

Matrix operations provide computational capabilities for neural networks
through coordinated parallel processing across multiple dimensions.
While they enable transformations such as attention mechanisms and
convolutions, their performance depends on efficient data handling.
Conversely, vector operations are optimized for one-to-one
transformations like activation functions and layer normalization. The
distinction between these operations highlights the importance of
dataflow patterns in neural accelerator design, examined next
(\citeproc{ref-Hwu2011GPU}{Hwu 2011}).

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1412}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1529}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.4000}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2941}}@{}}
\caption{\textbf{Operation Characteristics.} Matrix operations excel at
many-to-many transformations common in neural network layers, while
vector operations efficiently handle one-to-one transformations like
activation functions and normalization. These distinctions guide the
selection of appropriate computational primitives for different machine
learning tasks.}\label{tbl-matrix}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Operation Type}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Best For}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Examples}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Key Characteristic}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Operation Type}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Best For}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Examples}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Key Characteristic}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Matrix Operations} & Many-to-many transforms & Layer
transformations, attention, convolutions & Each output depends on
multiple inputs \\
\textbf{Vector Operations} & One-to-one transforms & Activation
functions, layer normalization, element-wise gradients & Each output
depends only on corresponding input \\
\end{longtable}

\textbf{Historical Foundations of Matrix Computation.} Matrix operations
have long served as a cornerstone of computational mathematics, with
applications extending from numerical simulations to graphics processing
(\citeproc{ref-Golub1996Matrix}{Golub and Loan 1996}). The structured
nature of matrix multiplications and transformations made them natural
targets for acceleration in early computing architectures. In the 1980s
and 1990s, specialized digital signal processors (DSPs) and graphics
processing units (GPUs) optimized for matrix computations played a
critical role in accelerating workloads such as image processing,
scientific computing, and 3D rendering
(\citeproc{ref-owens2008gpu}{Owens et al. 2008}).

The widespread adoption of machine learning has reinforced the
importance of efficient matrix computation. Neural networks, built on
matrix multiplications and tensor operations, have driven the
development of dedicated hardware architectures that extend beyond
traditional vector processing. Modern tensor processing units (TPUs) and
AI accelerators implement matrix multiplication at scale, reflecting the
same architectural principles that once underpinned early scientific
computing and graphics workloads.

Table~\ref{tbl-matrix} contrasts matrix and vector operations, revealing
how different computational patterns map to neural network primitives.
While matrix operations provide the computational backbone for neural
networks, they represent only part of the acceleration challenge. Neural
networks also depend critically on non-linear transformations that
cannot be efficiently expressed through linear algebra alone.

\subsection{Special Function
Units}\label{sec-ai-acceleration-special-function-units-ed00}

While vector and matrix operations efficiently handle the linear
transformations in neural networks, non-linear functions present unique
computational challenges that require dedicated hardware solutions.
Special Function Units (SFUs) provide hardware acceleration for these
computations, completing the set of core processing primitives needed
for efficient neural network execution.

\textbf{Non-Linear Functions.} Non-linear functions enable neural
networks to model complex relationships
(\citeproc{ref-Goodfellow-et-al-2016}{Goodfellow, Courville, and Bengio
2013}). Listing~\ref{lst-nonlinear_layer} presents a typical neural
network layer sequence that combines linear transformations with
non-linear activations.

\begin{codelisting}

\caption{\label{lst-nonlinear_layer}\textbf{Non-Linear Transformations}:
Neural networks process input data through a sequence of linear
transformations followed by non-linear activations to capture complex
patterns. This layer sequence enhances model expressiveness and learning
capabilities.}

\centering{

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{layer }\OperatorTok{=}\NormalTok{ nn.Sequential(}
\NormalTok{    nn.Linear(}\DecValTok{256}\NormalTok{, }\DecValTok{512}\NormalTok{), nn.ReLU(), nn.BatchNorm1d(}\DecValTok{512}\NormalTok{)}
\NormalTok{)}
\NormalTok{output }\OperatorTok{=}\NormalTok{ layer(input\_tensor)}
\end{Highlighting}
\end{Shaded}

}

\end{codelisting}%

This sequence introduces multiple non-linear transformations that extend
beyond simple matrix operations. Listing~\ref{lst-nonlinear_math} breaks
down these operations into their mathematical components, exposing the
computational complexity that hardware must address.

\begin{codelisting}

\caption{\label{lst-nonlinear_math}\textbf{Non-linear Transformations}:
Neural networks apply linear and non-linear operations to transform
input data into meaningful features for learning. Machine learning
models use these transformations to capture complex patterns in data
efficiently.}

\centering{

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Z }\OperatorTok{=}\NormalTok{ matmul(weights, }\BuiltInTok{input}\NormalTok{) }\OperatorTok{+}\NormalTok{ bias  }\CommentTok{\# Linear transformation}
\NormalTok{H }\OperatorTok{=} \BuiltInTok{max}\NormalTok{(}\DecValTok{0}\NormalTok{, Z)  }\CommentTok{\# ReLU activation}
\NormalTok{mean }\OperatorTok{=}\NormalTok{ reduce\_mean(H, axis}\OperatorTok{=}\DecValTok{0}\NormalTok{)  }\CommentTok{\# BatchNorm statistics}
\NormalTok{var }\OperatorTok{=}\NormalTok{ reduce\_mean((H }\OperatorTok{{-}}\NormalTok{ mean) }\OperatorTok{**} \DecValTok{2}\NormalTok{)  }\CommentTok{\# Variance computation}
\NormalTok{output }\OperatorTok{=}\NormalTok{ gamma }\OperatorTok{*}\NormalTok{ (H }\OperatorTok{{-}}\NormalTok{ mean) }\OperatorTok{/}\NormalTok{ sqrt(var }\OperatorTok{+}\NormalTok{ eps) }\OperatorTok{+}\NormalTok{ beta}
\CommentTok{\# Normalization}
\end{Highlighting}
\end{Shaded}

}

\end{codelisting}%

\textbf{Hardware Implementation of Non-Linear Functions.} The
computational complexity of these operations becomes apparent when
examining their implementation on traditional processors. These
seemingly simple mathematical operations translate into complex
sequences of instructions. Consider the computation of batch
normalization: calculating the square root requires multiple iterations
of numerical approximation, while exponential functions in operations
like softmax need series expansion or lookup tables
(\citeproc{ref-Ioffe2015}{Ioffe and Szegedy 2015}). Even a simple ReLU
activation introduces branching logic that can disrupt instruction
pipelining. Listing~\ref{lst-traditional_overhead} demonstrates these
inefficiencies.

\begin{codelisting}

\caption{\label{lst-traditional_overhead}\textbf{ReLU and BatchNorm
Operations}: Neural networks process input data through conditional
operations that can disrupt instruction pipelining and multiple passes
required for normalization, highlighting efficiency challenges in
traditional implementations. (\citeproc{ref-ieee_spectrum_relu}{Cass
2020})}

\centering{

\begin{Shaded}
\begin{Highlighting}[]
\ControlFlowTok{for}\NormalTok{ batch }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\DecValTok{32}\NormalTok{):}
    \ControlFlowTok{for}\NormalTok{ feature }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\DecValTok{512}\NormalTok{):}
       \CommentTok{\# ReLU: Requires branch prediction and potential}
       \CommentTok{\# pipeline stalls}
\NormalTok{       z }\OperatorTok{=}\NormalTok{ matmul\_output[batch, feature]}
\NormalTok{       h }\OperatorTok{=} \BuiltInTok{max}\NormalTok{(}\FloatTok{0.0}\NormalTok{, z)    }\CommentTok{\# Conditional operation}

       \CommentTok{\# BatchNorm: Multiple passes over data}
\NormalTok{       mean\_sum[feature] }\OperatorTok{+=}\NormalTok{ h    }\CommentTok{\# First pass for mean}
\NormalTok{       var\_sum[feature] }\OperatorTok{+=}\NormalTok{ h }\OperatorTok{*}\NormalTok{ h }\CommentTok{\# Additional pass for variance}

\NormalTok{       temp[batch, feature] }\OperatorTok{=}\NormalTok{ h  }\CommentTok{\# Extra memory storage needed}

\CommentTok{\# Normalization requires complex arithmetic}
\ControlFlowTok{for}\NormalTok{ feature }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\DecValTok{512}\NormalTok{):}
\NormalTok{    mean }\OperatorTok{=}\NormalTok{ mean\_sum[feature] }\OperatorTok{/}\NormalTok{ batch\_size}
\NormalTok{    var }\OperatorTok{=}\NormalTok{ (var\_sum[feature] }\OperatorTok{/}\NormalTok{ batch\_size) }\OperatorTok{{-}}\NormalTok{ mean }\OperatorTok{*}\NormalTok{ mean}

    \CommentTok{\# Square root computation: Multiple iterations}
\NormalTok{    scale }\OperatorTok{=}\NormalTok{ gamma[feature] }\OperatorTok{/}\NormalTok{ sqrt(var }\OperatorTok{+}\NormalTok{ eps)  }\CommentTok{\# Iterative}
                                              \CommentTok{\# approximation}
\NormalTok{    shift }\OperatorTok{=}\NormalTok{ beta[feature] }\OperatorTok{{-}}\NormalTok{ mean }\OperatorTok{*}\NormalTok{ scale}

    \CommentTok{\# Additional pass over data for final computation}
    \ControlFlowTok{for}\NormalTok{ batch }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\DecValTok{32}\NormalTok{):}
\NormalTok{        output[batch, feature] }\OperatorTok{=}\NormalTok{ temp[batch, feature] }\OperatorTok{*}
\NormalTok{                                 scale }\OperatorTok{+}\NormalTok{ shift}
\end{Highlighting}
\end{Shaded}

}

\end{codelisting}%

These operations introduce several key inefficiencies:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Multiple passes over data, increasing memory bandwidth requirements
\item
  Complex arithmetic requiring many instruction cycles
\item
  Conditional operations that can cause pipeline stalls
\item
  Additional memory storage for intermediate results
\item
  Poor utilization of vector processing units
\end{enumerate}

More specifically, each operation introduces distinct challenges. Batch
normalization requires multiple passes through data: one for mean
computation, another for variance, and a final pass for output
transformation. Each pass loads and stores data through the memory
hierarchy. Operations that appear simple in mathematical notation often
expand into many instructions. The square root computation typically
requires 10-20 iterations of numerical methods like Newton-Raphson
approximation for suitable precision
(\citeproc{ref-Goldberg1991}{Goldberg 1991}). Conditional operations
like ReLU's max function require branch instructions that can stall the
processor's pipeline. The implementation needs temporary storage for
intermediate values, increasing memory usage and bandwidth consumption.
While vector units excel at regular computations, functions like
exponentials and square roots often require scalar operations that
cannot fully utilize vector processing capabilities.

\textbf{Hardware Acceleration.} SFUs address these inefficiencies
through dedicated hardware implementation. Modern ML accelerators
include specialized circuits that transform these complex operations
into single-cycle or fixed-latency computations.
Listing~\ref{lst-sfu_vector_ops} demonstrates this efficiency: loading a
vector of values allows the accelerator to apply ReLU, sigmoid, and
square root operations directly in 1-8 cycles, eliminating multiple
passes and complex instruction sequences.

\begin{codelisting}

\caption{\label{lst-sfu_vector_ops}\textbf{Hardware Acceleration}:
Single-cycle non-linear operations enable efficient vector processing in
ML accelerators, demonstrating how specialized hardware reduces
computational latency.}

\centering{

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{vld}\OperatorTok{.}\NormalTok{v v1}\OperatorTok{,} \OperatorTok{(}\NormalTok{input\_ptr}\OperatorTok{)}\NormalTok{    \# Load vector of values}
\NormalTok{vrelu}\OperatorTok{.}\NormalTok{v v2}\OperatorTok{,}\NormalTok{ v1           \# Single}\OperatorTok{{-}}\NormalTok{cycle ReLU on entire vector}
\NormalTok{vsigm}\OperatorTok{.}\NormalTok{v v3}\OperatorTok{,}\NormalTok{ v1           \# Fixed}\OperatorTok{{-}}\NormalTok{latency sigmoid computation}
\NormalTok{vtanh}\OperatorTok{.}\NormalTok{v v4}\OperatorTok{,}\NormalTok{ v1           \# Direct hardware tanh implementation}
\NormalTok{vrsqrt}\OperatorTok{.}\NormalTok{v v5}\OperatorTok{,}\NormalTok{ v1          \# Fast reciprocal square root}
\end{Highlighting}
\end{Shaded}

}

\end{codelisting}%

Each SFU implements a specific function through specialized circuitry.
For instance, a ReLU unit performs the comparison and selection in
dedicated logic, eliminating branching overhead. Square root operations
use hardware implementations of algorithms like Newton-Raphson with
fixed iteration counts, providing predictable latency bounds.
Exponential and logarithmic functions often combine small lookup tables
with hardware interpolation circuits
(\citeproc{ref-Lauterbach2019}{Costa et al. 2019}). Table~\ref{tbl-sfu}
summarizes the various hardware implementations and their typical
latencies, spanning from single-cycle activations to logarithmic-time
reductions.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2110}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2018}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3670}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2018}}@{}}
\caption{\textbf{Special Function Units.} Dedicated hardware
implementations of common mathematical functions (like relu, sigmoid,
and reciprocal square root) accelerate machine learning computations by
eliminating software overhead and enabling parallel processing of vector
data. Typical latencies of 1 to 2 cycles per function demonstrate the
performance gains achieved through specialized
circuitry.}\label{tbl-sfu}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Function Unit}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Operation}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Implementation Strategy}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Typical Latency}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Function Unit}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Operation}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Implementation Strategy}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Typical Latency}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Activation Unit} & ReLU, sigmoid, tanh & Piece-wise
approximation circuits & 1-2 cycles \\
\textbf{Statistics Unit} & Mean, variance & Parallel reduction trees &
log(N) cycles \\
\textbf{Exponential Unit} & exp, log & Table lookup + hardware
interpolation & 2-4 cycles \\
\textbf{Root/Power Unit} & sqrt, rsqrt & Fixed-iteration Newton-Raphson
& 4-8 cycles \\
\end{longtable}

\textbf{SFUs History.} The need for efficient non-linear function
evaluation has shaped computer architecture for decades. Early
processors incorporated hardware support for complex mathematical
functions, such as logarithms and trigonometric operations, to
accelerate workloads in scientific computing and signal processing
(\citeproc{ref-Smith1997}{Smith 1997}). In the 1970s and 1980s,
floating-point co-processors were introduced to handle complex
mathematical operations separately from the main CPU
(\citeproc{ref-palmer_8087_1981}{Palmer 1980}). In the 1990s,
instruction set extensions such as Intel's SSE and ARM's NEON provided
dedicated hardware for vectorized mathematical transformations,
improving efficiency for multimedia and signal processing applications.

Machine learning workloads have reintroduced a strong demand for
specialized functional units, as activation functions, normalization
layers, and exponential transformations are central to neural network
computations. Rather than relying on iterative software approximations,
modern AI accelerators implement fast, fixed-latency SFUs for these
operations, mirroring historical trends in scientific computing.

With vector operations, matrix operations, and special function units
now established as the core computational primitives, the next question
is how these components are organized into complete execution units.
Understanding this organization reveals both the theoretical
capabilities and the practical performance characteristics that
developers encounter when targeting contemporary AI accelerators.

\subsection{Compute Units and Execution
Models}\label{sec-ai-acceleration-compute-units-execution-models-f406}

The vector operations, matrix operations, and special function units
examined previously represent the core computational primitives in AI
accelerators. Modern AI processors package these primitives into
distinct execution units, such as SIMD units, tensor cores, and
processing elements, which define how computations are structured and
exposed to users. Understanding this organization reveals both the
theoretical capabilities and practical performance characteristics that
developers can exploit in contemporary AI accelerators.

\subsubsection{Mapping Primitives to Execution
Units}\label{sec-ai-acceleration-mapping-primitives-execution-units-ccb6}

The progression from computational primitives to execution units follows
a structured hierarchy that reflects the increasing complexity and
specialization of AI accelerators:

\begin{itemize}
\tightlist
\item
  Vector operations → SIMD/SIMT units that enable parallel processing of
  independent data elements
\item
  Matrix operations → Tensor cores and systolic arrays that provide
  structured matrix multiplication
\item
  Special functions → Dedicated hardware units integrated within
  processing elements
\end{itemize}

Each execution unit combines these computational primitives with
specialized memory and control mechanisms, optimizing both performance
and energy efficiency. This structured packaging allows hardware vendors
to expose standardized programming interfaces while implementing diverse
underlying architectures tailored to specific workload requirements. The
choice of execution unit significantly influences overall system
efficiency, affecting data locality, compute density, and workload
adaptability. Subsequent sections examine how these execution units
operate within AI accelerators to maximize performance across different
machine learning tasks.

\subsubsection{Evolution from SIMD to SIMT
Architectures}\label{sec-ai-acceleration-evolution-simd-simt-architectures-e1fd}

Single Instruction Multiple Data (SIMD)\sidenote{\textbf{SIMD (Single
Instruction, Multiple Data)}: From Michael Flynn's 1966 taxonomy
classifying computer architectures by instruction and data streams. SIMD
describes machines where one instruction operates on multiple data
elements simultaneously. SIMT (Single Instruction, Multiple Thread),
coined by NVIDIA, extends this to many lightweight threads sharing
instruction fetch. While CPUs use wide SIMD units (e.g., AVX-512), GPUs
coordinate tens of thousands of threads concurrently through SIMT,
enabling the massive parallelism neural networks require. } execution
applies identical operations to multiple data elements in parallel,
minimizing instruction overhead while maximizing data throughput. This
execution model is widely used to accelerate workloads with regular,
independent data parallelism, such as neural network computations. The
ARM Scalable Vector Extension (SVE) provides a representative example of
how modern architectures implement SIMD operations efficiently.
Listing~\ref{lst-arm_sve_vector} demonstrates this approach.

\begin{codelisting}

\caption{\label{lst-arm_sve_vector}\textbf{Vector Operation}: Vector
multiplication and addition operations enable efficient parallel
processing in machine learning models. (\citeproc{ref-ARM2020}{Ltd.
2020})}

\centering{

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ptrue p0}\OperatorTok{.}\NormalTok{s              \# Create predicate }\ControlFlowTok{for}\NormalTok{ vector length}
\NormalTok{ld1w z0}\OperatorTok{.}\NormalTok{s}\OperatorTok{,}\NormalTok{ p0}\OperatorTok{/}\NormalTok{z}\OperatorTok{,} \OperatorTok{[}\NormalTok{x0}\OperatorTok{]}\NormalTok{   \# Load vector of inputs}
\NormalTok{fmul z1}\OperatorTok{.}\NormalTok{s}\OperatorTok{,}\NormalTok{ z0}\OperatorTok{.}\NormalTok{s}\OperatorTok{,}\NormalTok{ z0}\OperatorTok{.}\NormalTok{s   \# Multiply elements}
\NormalTok{fadd z2}\OperatorTok{.}\NormalTok{s}\OperatorTok{,}\NormalTok{ z1}\OperatorTok{.}\NormalTok{s}\OperatorTok{,}\NormalTok{ z0}\OperatorTok{.}\NormalTok{s   \# Add elements}
\NormalTok{st1w z2}\OperatorTok{.}\NormalTok{s}\OperatorTok{,}\NormalTok{ p0}\OperatorTok{,} \OperatorTok{[}\NormalTok{x1}\OperatorTok{]}\NormalTok{     \# Store results}
\end{Highlighting}
\end{Shaded}

}

\end{codelisting}%

Processor architectures continue to expand SIMD capabilities to
accommodate increasing computational demands. Intel's Advanced Matrix
Extensions (AMX) (\citeproc{ref-intel2021amx}{I. Corporation 2021}) and
ARM's SVE2 architecture (\citeproc{ref-stephens2017arm}{Stephens et al.
2017}) provide flexible SIMD execution, enabling software to scale
across different hardware implementations.

To address these limitations, SIMT extends SIMD principles by enabling
parallel execution across multiple independent threads, each maintaining
its own program counter and architectural state
(\citeproc{ref-lindholm2008nvidia}{Lindholm et al. 2008};
\citeproc{ref-nickolls2008scalable}{Nickolls et al. 2008}). This model
maps naturally to matrix computations, where each thread processes
different portions of a workload while still benefiting from shared
instruction execution. In NVIDIA's GPU architectures, each Streaming
Multiprocessor (SM)\sidenote{\textbf{Streaming Multiprocessor (SM)}: A
GPU compute unit containing many lightweight execution lanes, shared
memory, and schedulers. SMs execute threads in a SIMT fashion, where
groups of threads follow the same instruction stream while operating on
different data, enabling massive parallelism on regular numerical
workloads. } coordinates thousands of threads executing in parallel,
allowing for efficient scaling of neural network computations. Threads
are organized into warps\sidenote{\textbf{Warp}: From weaving
terminology, where the warp consists of parallel threads held taut on a
loom while the weft weaves through them. NVIDIA adopted this metaphor
for its fundamental execution unit of 32 threads that execute in
lock-step, emphasizing how these parallel threads are ``woven'' together
in coordinated execution. All threads in a warp share instruction fetch;
if they diverge in control flow, the warp serializes execution paths,
reducing efficiency. }, which are the basic execution units that enable
SIMT efficiency. Listing~\ref{lst-cuda_simt} shows this parallel
processing model in action.

\begin{codelisting}

\caption{\label{lst-cuda_simt}\textbf{SIMT Execution}: Each thread
processes a unique output element in parallel, demonstrating how SIMT
enables efficient matrix multiplication on GPUs.}

\centering{

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{\_\_global\_\_ }\DataTypeTok{void}\NormalTok{ matrix\_multiply}\OperatorTok{(}\DataTypeTok{float}\OperatorTok{*}\NormalTok{ C}\OperatorTok{,} \DataTypeTok{float}\OperatorTok{*}\NormalTok{ A}\OperatorTok{,} \DataTypeTok{float}\OperatorTok{*}
\NormalTok{                                B}\OperatorTok{,} \DataTypeTok{int}\NormalTok{ N}\OperatorTok{)} \OperatorTok{\{}
    \CommentTok{// Each thread processes one output element}
    \DataTypeTok{int}\NormalTok{ row }\OperatorTok{=}\NormalTok{ blockIdx}\OperatorTok{.}\NormalTok{y }\OperatorTok{*}\NormalTok{ blockDim}\OperatorTok{.}\NormalTok{y }\OperatorTok{+}\NormalTok{ threadIdx}\OperatorTok{.}\NormalTok{y}\OperatorTok{;}
    \DataTypeTok{int}\NormalTok{ col }\OperatorTok{=}\NormalTok{ blockIdx}\OperatorTok{.}\NormalTok{x }\OperatorTok{*}\NormalTok{ blockDim}\OperatorTok{.}\NormalTok{x }\OperatorTok{+}\NormalTok{ threadIdx}\OperatorTok{.}\NormalTok{x}\OperatorTok{;}

    \DataTypeTok{float}\NormalTok{ sum }\OperatorTok{=} \FloatTok{0.0}\BuiltInTok{f}\OperatorTok{;}
    \ControlFlowTok{for} \OperatorTok{(}\DataTypeTok{int}\NormalTok{ k }\OperatorTok{=} \DecValTok{0}\OperatorTok{;}\NormalTok{ k }\OperatorTok{\textless{}}\NormalTok{ N}\OperatorTok{;}\NormalTok{ k}\OperatorTok{++)} \OperatorTok{\{}
        \CommentTok{// Threads in a warp execute in parallel}
\NormalTok{        sum }\OperatorTok{+=}\NormalTok{ A}\OperatorTok{[}\NormalTok{row }\OperatorTok{*}\NormalTok{ N }\OperatorTok{+}\NormalTok{ k}\OperatorTok{]} \OperatorTok{*}\NormalTok{ B}\OperatorTok{[}\NormalTok{k }\OperatorTok{*}\NormalTok{ N }\OperatorTok{+}\NormalTok{ col}\OperatorTok{];}
    \OperatorTok{\}}
\NormalTok{    C}\OperatorTok{[}\NormalTok{row }\OperatorTok{*}\NormalTok{ N }\OperatorTok{+}\NormalTok{ col}\OperatorTok{]} \OperatorTok{=}\NormalTok{ sum}\OperatorTok{;}
\OperatorTok{\}}
\end{Highlighting}
\end{Shaded}

}

\end{codelisting}%

SIMT execution allows neural network computations to scale efficiently
across thousands of threads while maintaining flexibility for divergent
execution paths. Similar execution models appear in AMD's RDNA and
Intel's Xe architectures, reinforcing SIMT as a core mechanism for AI
acceleration.

\subsubsection{Tensor
Cores}\label{sec-ai-acceleration-tensor-cores-771f}

While SIMD and SIMT units provide efficient execution of vector
operations, neural networks rely heavily on matrix computations that
require specialized execution units for structured multi-dimensional
processing. The energy economics of matrix operations drive this
specialization: traditional scalar processing can require multiple
off-chip memory accesses per operation, while tensor cores amortize data
movement across entire matrix blocks. Tensor processing units extend
SIMD and SIMT principles by enabling efficient matrix operations through
dedicated hardware blocks---\textbf{tensor cores}---that execute matrix
multiplications and accumulations on matrix tiles. In many cases, this
shifts the dominant cost from off-chip data movement toward on-chip
reuse and arithmetic, depending on the kernel mix and memory behavior.

Tensor cores\sidenote{\textbf{Tensor Core}: The term ``tensor'' derives
from Latin ``tendere'' (to stretch), originally describing mathematical
objects that transform under coordinate changes. In ML, tensors
generalize matrices to arbitrary dimensions. NVIDIA introduced tensor
cores in the V100 (2017) to accelerate the small matrix operations
(4x4x4 tiles) common in neural networks. On supported kernels and
reduced-precision modes, tensor cores deliver large speedups over
conventional GPU execution, with modern accelerators reaching hundreds
of TFLOPS for reduced-precision dense kernels. } provide an example of
this approach. Listing~\ref{lst-tensor_core_op} exposes matrix
computation capabilities through specialized instructions that use
dedicated hardware blocks.

\begin{codelisting}

\caption{\label{lst-tensor_core_op}\textbf{Tensor Core Operation}:
Matrix multiplications are performed in parallel across entire matrix
blocks, optimizing computational efficiency for neural network
training.}

\centering{

\begin{Shaded}
\begin{Highlighting}[]

\NormalTok{Tensor Core Operation }\OperatorTok{(}\NormalTok{example GPU}\OperatorTok{):}
\NormalTok{mma}\OperatorTok{.}\NormalTok{sync}\OperatorTok{.}\NormalTok{aligned}\OperatorTok{.}\NormalTok{m16n16k16}\OperatorTok{.}\NormalTok{f16}\OperatorTok{.}\NormalTok{f16}
  \OperatorTok{\{}\NormalTok{d0}\OperatorTok{,}\NormalTok{d1}\OperatorTok{,}\NormalTok{d2}\OperatorTok{,}\NormalTok{d3}\OperatorTok{\},}     \CommentTok{// Destination registers}
  \OperatorTok{\{}\NormalTok{a0}\OperatorTok{,}\NormalTok{a1}\OperatorTok{,}\NormalTok{a2}\OperatorTok{,}\NormalTok{a3}\OperatorTok{\},}     \CommentTok{// Source matrix A}
  \OperatorTok{\{}\NormalTok{b0}\OperatorTok{,}\NormalTok{b1}\OperatorTok{,}\NormalTok{b2}\OperatorTok{,}\NormalTok{b3}\OperatorTok{\},}     \CommentTok{// Source matrix B}
  \OperatorTok{\{}\NormalTok{c0}\OperatorTok{,}\NormalTok{c1}\OperatorTok{,}\NormalTok{c2}\OperatorTok{,}\NormalTok{c3}\OperatorTok{\}}      \CommentTok{// Accumulator}
\end{Highlighting}
\end{Shaded}

}

\end{codelisting}%

A single tensor core instruction processes an entire matrix block while
maintaining intermediate results in local registers, significantly
improving computational efficiency compared to implementations based on
scalar or vector operations. This structured approach enables hardware
to achieve high throughput while reducing the burden of explicit loop
unrolling and data management at the software level.

Tensor processing unit architectures differ based on design priorities.
Some GPU families incorporate tensor cores optimized for general-purpose
deep learning acceleration. TPU-style designs utilize large-scale matrix
units arranged in systolic arrays to maximize sustained training
throughput on dense tensor kernels. Mobile
NPUs\sidenote{\textbf{On-Device Neural Engine Strategy}: Many mobile
SoCs include dedicated neural engines (NPUs) to enable on-device ML
within tight battery and thermal envelopes. These blocks can accelerate
common inference kernels efficiently, supporting interactive features
such as vision and speech processing without requiring cloud
connectivity. } integrate smaller matrix processors optimized for
low-power inference workloads, while some server CPUs introduce matrix
instruction extensions (AMX-class tiles) designed for datacenter
inference and mixed workloads.

The increasing specialization of AI hardware has driven significant
performance improvements in deep learning workloads.
Figure~\ref{fig-ai-performance} charts the trajectory of AI accelerator
performance in NVIDIA GPUs, highlighting the transition from
general-purpose floating-point execution units to highly optimized
tensor processing cores.

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{contents/vol1/hw_acceleration/images/png/int8_tops_nvidia.png}}

}

\caption{\label{fig-ai-performance}\textbf{GPU Performance Scaling}:
NVIDIA GPUs experienced approximately a \textasciitilde1,000\(\times\)
increase in integer 8-bit TOPS (tera operations per second) over a
decade, from 4 TOPS on the K20X to 4,000 TOPS on the H100. This
three-orders-of-magnitude gain was driven by architectural innovations
transitioning from floating-point to tensor core acceleration.}

\end{figure}%

\subsubsection{Processing
Elements}\label{sec-ai-acceleration-processing-elements-daa1}

The highest level of execution unit organization integrates multiple
tensor cores with local memory into processing elements (PEs). A
processing element serves as the primary building block in many AI
accelerators, combining different computational units to efficiently
execute neural network operations. Each PE typically includes vector
units for element-wise operations, tensor cores for matrix computation,
special function units for non-linear transformations, and dedicated
memory resources to optimize data locality and minimize data movement
overhead.

Processing elements balance computational density with memory access
efficiency. Their design varies across different architectures to
support diverse workloads and scalability requirements. Graphcore's
Intelligence Processing Unit (IPU) distributes computation across 1,472
tiles, each containing independent processing elements optimized for
fine-grained parallelism (\citeproc{ref-Graphcore2020}{Graphcore 2020}).
Cerebras extends this approach in the CS-2 system, integrating 850,000
processing elements across a wafer-scale device to accelerate sparse
computations. Tesla's D1 processor arranges processing elements with
substantial local memory, optimizing throughput and latency for
real-time autonomous vehicle workloads
(\citeproc{ref-Tesla2021}{Quinnell 2024}).

Processing elements provide the structural foundation for large-scale AI
acceleration. Their efficiency depends not only on computational
capability but also on interconnect strategies and memory hierarchy
design. Architectural choices impact performance across different AI
workloads.

Beyond the basic organization of processing elements, modern
accelerators employ increasingly sophisticated techniques to extract
performance from neural network workloads. Tensor processing units have
enabled substantial efficiency gains in AI workloads by using
hardware-accelerated matrix computation. Their role continues to evolve
as architectures incorporate support for advanced execution techniques,
including structured sparsity and workload-specific optimizations. One
such technique is N:M structured sparsity, which enables hardware to
exploit model sparsity without sacrificing memory access efficiency.

\subsubsection{N:M Structured Sparsity
Mechanics}\label{sec-ai-acceleration-nm-sparsity}

While unstructured pruning reduces model size, it rarely translates to
hardware speedup because memory access becomes irregular. Hardware
accelerators solve this with \textbf{N:M Structured
Sparsity}\sidenote{\textbf{Sparsity}: From Latin ``sparsus''
(scattered), describing how zeros are distributed throughout a matrix
rather than densely packed. In ML, sparse matrices have most elements as
zero. The N:M notation indicates that exactly N elements must be
non-zero in every M-element block, combining the computational benefits
of zeros with the predictable access patterns hardware requires. }, a
pattern-based approach that enforces regularity.

\textbf{The 2:4 Pattern}: NVIDIA's Sparse Tensor Cores utilize a 2:4
pattern, requiring that in every contiguous block of 4 values, at least
2 must be zero. This constraint allows the hardware to compress the
matrix by 50\% in memory and metadata.

\textbf{Hardware Execution}:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Compression}: The hardware stores only the 2 non-zero values
  and 2 bits of metadata (indices) for every 4-element block.
\item
  \textbf{Compute}: During matrix multiplication, the Sparse Tensor Core
  reads the metadata to select the corresponding activations and
  performs math only on the non-zero weights.
\item
  \textbf{Result}: This effectively doubles the FLOPs/byte ratio,
  providing a theoretical \(2\times\) speedup over dense matrix
  multiplication with minimal accuracy loss, provided the model is
  fine-tuned to respect the 2:4 constraint.
\end{enumerate}

To understand why ``Structured'' patterns are required for hardware
speedup, consider how sparse matrices are actually stored in memory. As
shown in Figure~\ref{fig-sparse-formats}, a sparse format (like CSR or
Block Sparse) must store indices alongside values. If the sparsity is
random, the index overhead and irregular access kill performance.
Structured sparsity---whether at the \textbf{Large Block} scale or the
\textbf{Fine-Grained N:M} scale---makes this indexing predictable and
compact, allowing hardware to fetch data efficiently.

\begin{figure}[htb]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/65e7d21d7516b185c243b4fb0d5a5cecd8f224f9.pdf}}

}

\caption{\label{fig-sparse-formats}\textbf{Sparse Storage Formats}:
Hardware efficiency depends on how sparse matrices are stored.
\textbf{Dense} storage (top left) is simple but wasteful for zeros.
\textbf{Block Sparse} (top right) and \textbf{CSR} (bottom) compress the
matrix by storing only non-zero values and their indices. Structured
sparsity (like N:M or Blocks) makes this indexing predictable, allowing
hardware to fetch data and skip zeros efficiently.}

\end{figure}%

Beyond structured sparsity optimizations, different hardware
architectures implement matrix operations through distinct computational
structures. Systolic arrays represent one such approach that has proven
particularly effective for AI workloads.

\subsubsection{Systolic
Arrays}\label{sec-ai-acceleration-systolic-arrays-6fa8}

While tensor cores package matrix operations into structured
computational units, systolic arrays provide an alternative approach
optimized for continuous data flow and operand reuse. The core
motivation for systolic architectures stems from the energy efficiency
constraints discussed earlier: minimizing the impact of memory access
penalties through architectural design. Quantifying \emph{the energy
advantage of pulsing data} through the array reveals why this
architecture has become central to modern AI accelerators.

This architecture provides \emph{the energy advantage of pulsing data}.

\phantomsection\label{callout-notebookux2a-1.9}
\begin{fbx}{callout-notebook}{AI Engineer’s Notebook:}{The Energy Advantage of Pulsing Data}
\phantomsection\label{callout-notebook*-1.9}
\textbf{Systolic Arrays vs.~Traditional Vector Units}: The ``Systolic''
(heartbeat) metaphor is not just about timing; it reflects a fundamental
energy efficiency advantage. We can quantify the energy advantage of
systolic dataflow over traditional vector units using the \textbf{Energy
Corollary}:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Vector Unit}: Loads \(A\), loads \(B\), computes
  \(A \times B + C\), writes \(C\).

  \begin{itemize}
  \tightlist
  \item
    \textbf{Data Movement}: 3 loads + 1 write = 4 DRAM accesses (per
    operation).
  \item
    \textbf{Energy}:
    \(\approx 4 \times 640\text{ pJ} = \mathbf{`{python} vector_energy_str` \text{ pJ/OP}}\).
  \end{itemize}
\item
  \textbf{Systolic Array (\(128 \times 128\) size)}: Loads \(A\) and
  \(B\) once at the edges. Data ``pulses'' through 128 processing
  elements.

  \begin{itemize}
  \tightlist
  \item
    \textbf{Data Movement}: 2 loads per 128 operations =
    \texttt{\{python\}\ systolic\_access\_str} DRAM accesses (per
    operation).
  \item
    \textbf{Energy}:
    \(\approx `{python} systolic_access_str` \times 640\text{ pJ} + 1 \text{ pJ (compute)} \approx \mathbf{`{python} systolic_energy_str` \text{ pJ/OP}}\).
  \end{itemize}
\end{enumerate}

\textbf{The Systems Conclusion}: A systolic array is
\textbf{\texttt{\{python\}\ energy\_ratio\_str}x more energy-efficient}
than a naive vector unit for large matrix multiplications. - This
efficiency is what allows a Google TPU to pack 100,000+ MAC units into a
single chip without melting. - \textbf{The Limitation}: This ``Energy
Dividend'' only pays out if the matrix is large enough to fill the
array. For small matrices (common in real-time inference), the array is
under-utilized, and the energy efficiency drops back toward the vector
unit baseline.

\end{fbx}

A systolic array arranges processing elements in a grid pattern, where
data flows rhythmically between neighboring units in a synchronized
manner, enabling each operand to participate in multiple computations as
it propagates through the array. This structured movement minimizes
external memory accesses by maximizing local data reuse. A single weight
value can contribute to dozens of operations as it moves through the
processing elements, transforming the energy profile from memory-bound
to compute-efficient execution.

Kung and Leiserson\sidenote{\textbf{Systolic Array}: Named from the
Greek ``systole'' (contraction), referring to the rhythmic pumping of
the heart. H.T. Kung and Charles Leiserson chose this term at CMU in
1979 because data pulses through the processing element grid in waves,
like blood through cardiac chambers. Each element contracts (computes)
and passes data to neighbors in a coordinated rhythm. Google's TPUs
revived systolic designs for neural networks by pairing them with
software stacks optimized for dense linear algebra, achieving high
throughput when workloads map well to this rhythmic dataflow pattern. }
(\citeproc{ref-kung1979systolic}{H. T. Kung and Leiserson 1979}) first
introduced systolic arrays, formalizing their use in parallel computing
architectures for efficient matrix operations
(\citeproc{ref-Kung1982}{Kung 1982}). Unlike general-purpose execution
units, systolic arrays exploit spatial and temporal locality by reusing
operands as they propagate through the grid. Google's TPU exemplifies
this architectural approach: in the TPUv4, a \(128\times128\) systolic
array of multiply-accumulate units processes matrix operations by
streaming data through the array in a pipelined manner
(\citeproc{ref-jouppi2017datacenter}{Norman P. Jouppi et al. 2017b}).
Figure~\ref{fig-systolic-array} captures this dataflow architecture.

The systolic array architecture achieves computational efficiency
through synchronized data movement across a structured grid of
processing elements. Systolic arrays organize computation around four
components:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Control Unit}: Coordinates timing and data distribution across
  the array, maintaining synchronized operation throughout the
  computational grid
\item
  Data Streams: Input matrices propagate through coordinated pathways
  where matrix A elements traverse horizontally while matrix B elements
  flow vertically through the processing grid
\item
  \textbf{Processing Element Grid}: Individual processing elements
  execute multiply-accumulate operations on streaming data, generating
  partial results that accumulate toward the final computation
\item
  \textbf{Output Collection}: Results aggregate at designated output
  boundaries where accumulated partial sums form complete matrix
  elements
\end{enumerate}

Because systolic arrays physically fix how data flows through the grid,
designers must decide which operand to keep stationary, a choice that
permanently shapes the hardware's affinity for certain workloads.

\phantomsection\label{callout-perspectiveux2a-1.10}
\begin{fbx}{callout-perspective}{Systems Perspective:}{Matching Architecture to Workload}
\phantomsection\label{callout-perspective*-1.10}
\textbf{The Architects' Dilemma}: Systolic arrays must choose which data
to keep stationary (in registers) to minimize movement. This choice
hard-codes the hardware's preference for certain model types.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1404}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1287}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1696}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.5497}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Strategy}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Stationary Item}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Optimized For}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Example Workload}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Weight-Stationary} & Weights (\(W\)) & High Reuse of Weights &
\textbf{CNNs (Conv2D)}: Filters are small and reused across the entire
image. \\
\textbf{Output-Stationary} & Partial Sums (\(C\)) & High Reuse of
Accumulators & \textbf{Large Batch MatMul}: Accumulating results for
many inputs against a large weight matrix. \\
\textbf{Row-Stationary} & Input Rows (\(A\)) & Data Reuse &
\textbf{General MatMul}: Balancing input and weight reuse. \\
\end{longtable}

There is no ``perfect'' accelerator. A chip optimized for
Weight-Stationary flow (like early TPUs) excels at CNNs but might
struggle with the massive, changing weights of LLMs, which push
architectures toward different reuse patterns.

\end{fbx}

The synchronized data flow ensures that matrix element A{[}i,k{]}
encounters corresponding B{[}k,j{]} elements at precise temporal
intervals, executing the multiply-accumulate operations required for
matrix multiplication C{[}i,j{]} = Σ A{[}i,k{]} × B{[}k,j{]}. This
systematic reuse of operands across multiple processing elements
substantially reduces memory bandwidth requirements by eliminating
redundant data fetches from external memory subsystems.

Consider the multiplication of 2×2 matrices A and B within a systolic
array. During the first computational cycle, element A{[}0,0{]}=2
propagates horizontally while B{[}0,0{]}=1 moves vertically, converging
at processing element PE(0,0) to execute the multiplication 2×1=2. In
the subsequent cycle, the same A{[}0,0{]}=2 advances to PE(0,1) where it
encounters B{[}0,1{]}=3, computing 2×3=6. Concurrently, A{[}0,1{]}=4
enters PE(0,0) to engage with the next B matrix element. This
coordinated data movement enables systematic operand reuse across
multiple computational operations, eliminating redundant memory accesses
and exemplifying the efficiency principle underlying systolic array
architectures.

\begin{figure}[htb]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/98326e34546eaa0d52edc0611d072452434a1e4e.pdf}}

}

\caption{\label{fig-systolic-array}\textbf{Systolic Array Dataflow}: A
control unit feeds input data streams into a grid of processing
elements, each performing multiply-accumulate operations. Data flows
horizontally and vertically through the array in a pipelined manner,
maximizing operand reuse and minimizing memory access, as exemplified by
Google's TPUv4.}

\end{figure}%

Each processing element in the array performs a multiply-accumulate
operation in every cycle:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Receives an input activation from above
\item
  Receives a weight value from the left
\item
  Multiplies these values and adds to its running sum
\item
  Passes the input activation downward and the weight value rightward to
  neighboring elements
\end{enumerate}

This structured computation model minimizes data movement between global
memory and processing elements, improving both efficiency and
scalability. As systolic arrays operate in a streaming fashion, they are
particularly effective for high-throughput workloads such as deep
learning training and inference.

One common systolic array implementation shows the general structure,
yet systolic architectures vary significantly across different
accelerator designs. Training-focused architectures like Google's TPU
employ large arrays optimized for high computational throughput, while
inference-oriented designs found in edge devices prioritize energy
efficiency with smaller configurations.

The underlying principle remains consistent: data flows systematically
through processing elements, with inputs moving horizontally and
vertically to compute partial sums in a synchronized fashion. However,
as detailed in
Section~\ref{sec-ai-acceleration-understanding-ai-memory-wall-3ea9},
practical effectiveness is ultimately constrained by memory bandwidth
bottlenecks.

A 128×128 systolic array capable of
\texttt{\{python\}\ systolic\_ops\_str} operations per cycle requires
continuous data feed to maintain utilization. Each cycle demands fresh
input activations and weight parameters that must traverse from off-chip
memory through on-chip buffers to the array edges. The TPU's 1,200 GB/s
on-chip bandwidth enables high utilization, but even this substantial
bandwidth becomes limiting when processing large transformer models
where memory requirements exceed on-chip capacity.

Recall from \textbf{?@sec-model-compression} that quantization reduces
model memory footprint by converting FP32 weights to INT8
representations. This optimization directly addresses the memory
bandwidth constraints identified here. Converting 32-bit floating-point
weights to 8-bit integers reduces memory traffic by 4×, transforming
bandwidth-bound operations into compute-bound workloads where systolic
arrays can achieve higher utilization. Similarly, structured pruning
removes entire rows or columns of weight matrices, reducing both the
data volume that must traverse memory hierarchies and the computation
required. These algorithmic optimizations prove valuable precisely
because they target the memory bottleneck that limits accelerator
performance in practice.

\subsubsection{Numerics in AI
Acceleration}\label{sec-ai-acceleration-numerics-ai-acceleration-f7be}

Building on the quantization and mixed-precision techniques established
in \textbf{?@sec-model-compression}, this section examines how AI
accelerators implement hardware support for reduced-precision formats.
The efficiency of AI accelerators is not determined by computational
power alone but also by how effectively the hardware supports different
numerical representations. The choice of numerical format shapes the
balance between accuracy, throughput, and energy consumption,
influencing how different execution units, such as SIMD and SIMT units,
tensor cores, and systolic arrays, are designed and deployed.

\paragraph{Precision
Trade-offs}\label{sec-ai-acceleration-precision-tradeoffs-8fa8}

Numerical precision represents a key design parameter in modern AI
accelerators. While higher precision formats provide mathematical
stability and accuracy, they come with substantial costs in terms of
power consumption, memory bandwidth, and computational throughput.
Hardware architects must balance these factors when designing
accelerator datapaths.

The evolution of AI hardware reflects this co-design between software
optimization and hardware capability. Early GPU architectures supported
only FP32 for deep learning workloads, but as the precision trade-offs
from \textbf{?@sec-model-compression} demonstrated that reduced
precision could maintain model accuracy, hardware vendors responded by
adding native support for FP16, BF16, and integer formats. This hardware
evolution enables software optimizations to translate directly into
performance gains, as reduced-precision operations execute on dedicated
circuits optimized for those specific formats.

The transition from high-precision to lower-precision formats is deeply
integrated into hardware execution models. As detailed in
Section~\ref{sec-ai-acceleration-evolution-simd-simt-architectures-e1fd},
SIMD and SIMT units provide flexible support for multiple precisions.
Tensor cores (Section~\ref{sec-ai-acceleration-tensor-cores-771f})
accelerate computation using reduced-precision arithmetic, while
systolic arrays (Section~\ref{sec-ai-acceleration-systolic-arrays-6fa8})
optimize performance by minimizing memory bandwidth constraints through
low-precision formats that maximize operand reuse.

Despite the advantages of reduced precision, deep learning models cannot
always rely solely on low-bit representations. To address this
challenge, modern AI accelerators implement mixed-precision computing,
where different numerical formats are used at different stages of
execution. These precision choices affect model fairness and
reliability. For example, matrix multiplications may be performed in
FP16 or BF16, while accumulations are maintained in FP32 to prevent
precision loss. Similarly, inference engines use INT8 arithmetic while
preserving key activations in higher precision when necessary.

\paragraph{Mixed-Precision
Computing}\label{sec-ai-acceleration-mixedprecision-computing-656f}

Modern AI accelerators increasingly support mixed-precision execution,
allowing different numerical formats to be used at various stages of
computation. Training workloads often use FP16 or BF16 for matrix
multiplications, while maintaining FP32 accumulations to preserve
precision. The software implementation of mixed-precision training,
including loss scaling techniques and framework support, is covered in
\textbf{?@sec-ai-training-mixedprecision-training-9218}. Inference
workloads, by contrast, optimize for INT8 or even INT4, achieving high
efficiency while retaining acceptable accuracy.

This shift toward precision diversity is evident in the evolution of AI
hardware. Early architectures such as NVIDIA Volta provided limited
support for lower precision beyond FP16, whereas later architectures,
including Turing and Ampere, expanded the range of supported formats.
Table~\ref{tbl-nvidia-numerics} traces this progression: Ampere GPUs
introduced TF32 as a hybrid between FP32 and FP16, alongside broader
support for BF16, INT8, and INT4.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1727}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1000}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3727}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3364}}@{}}
\caption{\textbf{Precision Support Evolution.} GPU architectures
progressively expanded support for lower-precision data types, enabling
performance gains and efficiency improvements in AI workloads. Early
architectures primarily utilized FP32, while later generations
incorporated FP16, BF16, INT8, and INT4 to accelerate both training and
inference tasks.}\label{tbl-nvidia-numerics}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Architecture}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Year}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Supported Tensor Core Precisions}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Supported CUDA Core Precisions}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Architecture}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Year}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Supported Tensor Core Precisions}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Supported CUDA Core Precisions}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Volta} & 2017 & FP16 & FP64, FP32, FP16 \\
\textbf{Turing} & 2018 & FP16, INT8 & FP64, FP32, FP16, INT8 \\
\textbf{Ampere} & 2020 & FP64, TF32, bfloat16, FP16, INT8, INT4 & FP64,
FP32, FP16, bfloat16, INT8 \\
\end{longtable}

Newer architectures incorporate a growing diversity of numerical
formats, reflecting the need for greater flexibility across different AI
workloads. This trend suggests that future AI accelerators will continue
expanding support for adaptive precision, balancing computational
efficiency against model accuracy.

The precision format used in hardware design has several implications.
By adopting lower-precision formats, the data transferred between
execution units and memory is reduced, leading to decreased memory
bandwidth requirements and storage. Tensor cores and systolic arrays can
process more lower-precision elements in parallel, thereby increasing
the effective throughput in terms of FLOPs. Energy efficiency is also
improved, as integer-based computations (e.g., INT8) require lower power
compared to floating-point arithmetic, a clear advantage for inference
workloads.

As AI models continue to scale in size, accelerator architectures are
evolving to support more efficient numerical formats. Future designs are
expected to incorporate adaptive precision techniques, dynamically
adjusting computation precision based on workload characteristics.
Understanding how these execution units and precision formats integrate
into complete accelerator architectures reveals the full picture of AI
hardware design.

\subsubsection{Architectural
Integration}\label{sec-ai-acceleration-architectural-integration-01b6}

The organization of computational primitives into execution units
determines the efficiency of AI accelerators. While SIMD, tensor cores,
and systolic arrays serve as building blocks, their integration into
full-chip architectures varies significantly across different AI
processors. The choice of execution units, their numerical precision
support, and their connectivity impact how effectively hardware can
scale for deep learning workloads.

Modern AI processors exhibit a range of design trade-offs based on their
intended applications. Some architectures, such as NVIDIA's A100,
integrate large numbers of tensor cores optimized for FP16-based
training, while Google's TPUv4 prioritizes high-throughput BF16 matrix
multiplications. Inference-focused processors, such as Intel's Sapphire
Rapids, incorporate INT8-optimized tensor cores to maximize efficiency.
The Apple M1, designed for mobile workloads, employs smaller processing
elements optimized for low-power FP16 execution.
Table~\ref{tbl-execution-units} compares these architectural
configurations, revealing how design choices reflect the growing
flexibility in numerical precision and execution unit organization.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.1810}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.1466}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2155}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2241}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2069}}@{}}
\caption{\textbf{AI Processor Configurations.} Modern AI processors
prioritize different execution unit characteristics for specific
workloads: NVIDIA A100 leverages wide SIMD and tensor cores for
training, Google TPUv4 emphasizes high-throughput BF16 matrix
multiplication, Intel Sapphire Rapids focuses on INT8-optimized
inference, and Apple M1 prioritizes low-power FP16 execution. These
variations in SIMD width, tensor core size, and processing element count
reflect the growing diversity in AI hardware
architectures.}\label{tbl-execution-units}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Processor}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{SIMD Width}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Tensor Core Size}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Processing Elements}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Primary Workloads}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Processor}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{SIMD Width}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Tensor Core Size}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Processing Elements}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Primary Workloads}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{NVIDIA A100} & 1024-bit & \(4\times4\times4\) FP16 & 108 SMs &
Training, HPC \\
\textbf{Google TPUv4} & 128-wide & \(128\times128\) BF16 & 2 cores/chip
& Training \\
\textbf{Intel Sapphire} & 512-bit AVX & \(32\times32\) INT8/BF16 & 56
cores & Inference \\
\textbf{Apple M1} & 128-bit NEON & \(16\times16\) FP16 & 8 NPU cores &
Mobile inference \\
\end{longtable}

These configurations optimize for different deep learning workloads
across architectures. Training accelerators prioritize high-throughput
floating-point tensor operations, whereas inference processors focus on
low-precision integer execution for efficiency. Meanwhile, mobile
accelerators balance precision and power efficiency to meet real-time
constraints.

\subsection{Cost-Performance
Analysis}\label{sec-ai-acceleration-costperformance-analysis-e925}

While architectural specifications define computational potential,
practical deployment decisions require understanding cost-performance
trade-offs across different accelerator options. However, raw
computational metrics alone provide an incomplete picture. The dominant
constraint in modern AI acceleration is not compute capacity but data
movement efficiency.

The energy differential established earlier (where memory access costs
dominate computation) drives the entire specialized hardware revolution.
This disparity helps explain why many accelerators achieve only a
fraction of peak compute on memory-bound workloads, while architectures
that maximize data reuse (e.g., systolic arrays on dense matrix kernels)
can sustain substantially higher utilization under favorable conditions.

Consider an organization choosing between ``more of an older
accelerator'' versus ``fewer of a newer accelerator.'' Peak FLOPS can be
misleading for transformer-style workloads with low arithmetic
intensity, where training is often memory-bandwidth bound rather than
compute-bound. In such cases, bandwidth per dollar and achievable
utilization can matter more than headline compute, so a newer
accelerator with substantially higher bandwidth can deliver materially
better \emph{sustained} performance even if peak FLOPS improves by a
smaller factor.

These dynamics help explain the rapid adoption of newer accelerators
despite higher unit prices. For memory-bound workloads, improvements in
effective bandwidth (and the software stack's ability to use it) can
dominate real-world performance. Cloud deployment further complicates
the analysis, as rental pricing, utilization, and operational overheads
can change the break-even point between purchasing and renting hardware.

Table~\ref{tbl-accelerator-economics} provides representative
cost-performance data for common accelerators. Note that these figures
are approximate and vary by vendor, region, and purchase volume; the key
insight is the trend rather than the absolute numbers. \textbf{Observe
how the cost per TFLOP has collapsed with each generation, even as the
absolute power requirement (TDP) has climbed to nearly 1,000 Watts for
flagship units, reflecting the industry's shift toward density over raw
unit cost.}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.1342}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.1946}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2953}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.1812}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.1745}}@{}}
\caption{\textbf{Accelerator Cost-Performance Comparison.} Hardware
costs evaluated against computational capabilities for optimal
deployment strategy selection. Newer accelerators offer better
price-performance ratios, though total cost of ownership includes power
consumption, cooling requirements, and infrastructure costs. Prices are
approximate list prices and vary by region and volume; TPU pricing
estimated from cloud
rates.}\label{tbl-accelerator-economics}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Accelerator}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{List Price (USD)}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Peak FLOPS (FP16)}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Memory Bandwidth}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Price/Performance}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Accelerator}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{List Price (USD)}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Peak FLOPS (FP16)}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Memory Bandwidth}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Price/Performance}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{NVIDIA V100} & \texttt{\{python\}\ v100\_price\_str} &
\texttt{\{python\}\ v100\_tflops} TFLOPS & \texttt{\{python\}\ v100\_bw}
GB/s & \texttt{\{python\}\ v100\_pp\_str} \\
\textbf{NVIDIA A100} & \texttt{\{python\}\ a100\_price\_str} &
\texttt{\{python\}\ a100\_tflops\_fp16} TFLOPS &
\texttt{\{python\}\ a100\_bw} GB/s &
\texttt{\{python\}\ a100\_pp\_str} \\
\textbf{NVIDIA H100} & \texttt{\{python\}\ h100\_price\_str} &
\texttt{\{python\}\ h100\_tflops\_tf32} TFLOPS (TF32) &
\texttt{\{python\}\ h100\_bw} GB/s &
\texttt{\{python\}\ h100\_pp\_str} \\
\textbf{Google TPUv4} & \texttt{\{python\}\ tpu\_price\_str} &
\texttt{\{python\}\ tpuv4\_tflops} TFLOPS (BF16) &
\texttt{\{python\}\ tpuv4\_bw} GB/s &
\texttt{\{python\}\ tpu\_pp\_str} \\
\textbf{Intel Gaudi 2} & \texttt{\{python\}\ gaudi\_price\_str} & 200
TFLOPS (INT8) & 800 GB/s & \texttt{\{python\}\ gaudi\_pp\_str} \\
\end{longtable}

The table reveals several important patterns. First, price-performance
improves with each generation, but the gains are not uniform across
workload types. Second, memory bandwidth often improves faster than the
price-performance ratio suggests, making newer accelerators
disproportionately valuable for memory-bound workloads. Third, the
``best'' accelerator depends heavily on workload characteristics: a
transformer training workload that is memory-bandwidth bound may benefit
more from H100's \texttt{\{python\}\ h100\_bw} GB/s bandwidth than from
raw FLOPS improvements. That bandwidth consistently emerges as the
deciding economic factor raises a deeper question: what physical
constraints make memory access, rather than arithmetic, the dominant
cost in modern AI systems?

Framework selection significantly impacts these economic decisions.
Detailed hardware-framework optimization strategies are covered in
\textbf{?@sec-ai-frameworks}, while performance evaluation methodologies
are discussed in \textbf{?@sec-benchmarking-ai}.

The preceding sections revealed impressive computational machinery:
vector units achieving 8× parallelism through SIMD execution, matrix
operations processing 256 elements simultaneously, and tensor cores
executing 16×16×16 fused multiply-accumulate blocks in single cycles. An
NVIDIA A100's tensor cores can execute
\texttt{\{python\}\ a100\_tflops\_fp16} trillion operations per second,
and an H100 pushes this further to nearly 2 petaFLOPS in FP8 precision.
At these rates, the pure arithmetic for a ResNet-50 forward pass could
complete in microseconds.

Yet real ResNet-50 inference takes milliseconds, not microseconds. The
gap between theoretical capability and practical performance reveals the
chapter's central tension, first posed in the Purpose section:
computational capability has outpaced our ability to feed data to
processors. Moving data from memory costs 100-1000× more energy than
arithmetic, and memory bandwidth grows at roughly 20\% annually while
compute throughput doubles every two years. This disparity determines
whether those \texttt{\{python\}\ a100\_tflops\_fp16} TFLOPS translate
to 30 TFLOPS of sustained performance (10\% utilization) or 250 TFLOPS
(80\% utilization). The memory systems examined next explain why this
gap exists and what architectural innovations address it.

\section{AI Memory
Systems}\label{sec-ai-acceleration-ai-memory-systems-0057}

The execution units examined in previous sections (SIMD units, tensor
cores, and systolic arrays) provide impressive computational throughput:
modern accelerators achieve 100 to 1000 TFLOPS for neural network
operations. Yet these theoretical capabilities remain unrealized in
practice when memory subsystems cannot supply data at sufficient rates.
This constraint, termed the AI memory wall, represents the dominant
bottleneck in real-world accelerator performance.

Unlike conventional workloads, ML models require frequent access to
large volumes of parameters, activations, and intermediate results,
leading to substantial memory bandwidth demands. This challenge
intersects with the data management strategies covered in
\textbf{?@sec-data-engineering-ml}. Modern AI hardware addresses these
demands through advanced memory hierarchies, efficient data movement
techniques, and compression strategies that promote efficient execution.

Four perspectives inform memory system design. First, we quantify the
growing disparity between computational throughput and memory bandwidth,
revealing why the AI memory wall represents the dominant performance
constraint in modern accelerators. Second, we explore how memory
hierarchies balance competing demands for speed, capacity, and energy
efficiency through carefully structured tiers from on-chip SRAM to
off-chip DRAM. Third, we analyze communication patterns between host
systems and accelerators, exposing transfer bottlenecks that limit
end-to-end performance. Finally, we examine how different neural network
architectures (multilayer perceptrons, convolutional networks, and
transformers) create distinct memory pressure patterns that inform
hardware design decisions and optimization strategies.

\subsection{Understanding the AI Memory
Wall}\label{sec-ai-acceleration-understanding-ai-memory-wall-3ea9}

The AI memory wall represents the primary bottleneck constraining modern
accelerator performance: the growing disparity between computational
throughput and memory bandwidth that prevents accelerators from
achieving their theoretical capabilities. While compute units can
execute millions of operations per second through specialized primitives
like vector operations and matrix multiplications, they depend
critically on memory systems to supply the continuous stream of weights,
activations, and intermediate results these operations require.

\phantomsection\label{callout-definitionux2a-1.11}
\begin{fbx}{callout-definition}{Definition:}{AI Memory Wall}
\phantomsection\label{callout-definition*-1.11}
\textbf{\emph{The AI Memory Wall}} is the fundamental bottleneck where
\textbf{Arithmetic Throughput} outpaces \textbf{Memory Bandwidth}.
Defined by the divergence of exponential compute growth vs.~linear
bandwidth growth, it dictates that performance is no longer bounded by
FLOPS, but by the \textbf{Energy and Latency} cost of moving data,
necessitating architectures focused on \textbf{Locality} and
\textbf{Reuse}. This phenomenon is the modern manifestation of the
\textbf{Von Neumann Bottleneck}\sidenote{\textbf{Von Neumann
Bottleneck}: John von Neumann's 1945 reference architecture separated
the processing unit from the memory unit, connected by a shared bus.
While this simplified programming (code and data are the same), it
created a fundamental physical limit: the processor can only work as
fast as data moves across that bus. Modern AI accelerators attack this
75-year-old bottleneck by moving memory \emph{into} the compute units
(SRAM) or stacking it directly on top (HBM). }.

\end{fbx}

The underlying cause of this wall is physical: moving data costs orders
of magnitude more energy than processing it. As visualized in
Figure~\ref{fig-energy-hierarchy}, the ``Horowitz Numbers'' reveal the
fundamental energy constants of silicon.

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{contents/vol1/hw_acceleration/hw_acceleration_files/figure-pdf/fig-energy-hierarchy-5.pdf}}

}

\caption{\label{fig-energy-hierarchy}\textbf{The Energy Hierarchy}:
Energy cost per operation (Log Scale) based on the `Horowitz Numbers.'
Fetching data from off-chip DRAM costs \textasciitilde128x more energy
than an SRAM access and \textasciitilde20,000x more than an INT8
addition. This fundamental physical disparity dictates that AI
accelerators must prioritize data locality (keeping weights in
SRAM/Registers) over raw arithmetic throughput to remain within power
budgets.}

\end{figure}%

\subsubsection{Quantifying the Compute-Memory Performance
Gap}\label{sec-ai-acceleration-quantifying-computememory-performance-gap-1526}

The severity of this constraint becomes apparent when examining scaling
trends. Over the past two decades, peak computational capabilities have
grown substantially faster than DRAM bandwidth
(\citeproc{ref-gholami2024ai}{Gholami et al. 2024}). This divergence
creates a widening gap where accelerators possess massive computational
power but cannot access data quickly enough to utilize it.
Representative high-end accelerators can deliver on the order of
(10\^{}3) TFLOPS of peak tensor throughput (e.g., NVIDIA H100 delivering
989 TFLOPS in FP16 or nearly 2,000 TFLOPS in FP8) while providing
approximately 3.35 TB/s of memory bandwidth. This implies a ridge point
on the order of (10\^{}2) operations per byte to fully utilize compute,
which can exceed the arithmetic intensity of many practical neural
network workloads.

The memory wall manifests through three critical
constraints.\sidenote{\textbf{The Firehose vs.~Straw}: A compute-bound
system is like a massive firehose trying to push water through a very
fine filter (the math). A memory-bound system is like having a powerful
pump but being forced to use a tiny drinking straw (the bus); the pump
sits idle because it cannot get enough water. } First, the energy
disparity: accessing DRAM can consume orders of magnitude more energy
than a multiply-accumulate operation
(\citeproc{ref-Horowitz2014}{Horowitz 2014a};
\citeproc{ref-sze2017efficient}{Sze et al. 2017a}), which often shifts
bottlenecks from raw compute to power and data movement. Second, the
bandwidth limitation: even TB/s memory systems may not feed large
parallel compute arrays continuously on memory-bound workloads, leaving
compute underutilized. Third, the latency hierarchy: off-chip memory
access can require hundreds of cycles, creating pipeline stalls that
cascade through parallel execution units.

Figure~\ref{fig-compute-memory-imbalance} illustrates this ``AI Memory
Wall,'' which continues to widen, making memory bandwidth rather than
compute the primary constraint in AI acceleration.

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{contents/vol1/hw_acceleration/hw_acceleration_files/figure-pdf/fig-compute-memory-imbalance-7.pdf}}

}

\caption{\label{fig-compute-memory-imbalance}\textbf{The
Compute-Bandwidth Divergence}: Computational capability and memory
bandwidth plotted on a log scale (2000--2025). While arithmetic
throughput (FLOPs) has grown exponentially, memory bandwidth has
improved at a significantly slower linear rate. This widening `Systems
Gap' defines the AI Memory Wall, forcing architects to design systems
that minimize data movement to avoid idling powerful compute units.}

\end{figure}%

The impact of this divergence is quantified in
Figure~\ref{fig-rising-ridge}, which shows how the hardware's ``Ridge
Point'' has skyrocketed, making sparse operations increasingly
inefficient.

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{contents/vol1/hw_acceleration/hw_acceleration_files/figure-pdf/fig-rising-ridge-1.pdf}}

}

\caption{\label{fig-rising-ridge}\textbf{The Rising Ridge}: Hardware
Arithmetic Intensity (FLOPs/Byte) over time. As compute capability
(FLOPs) grows faster than memory bandwidth (Bytes/s), the `Ridge
Point'---the intensity required to saturate the chip---skyrockets. This
trend explains why architectures with high data reuse (like
Transformers) have flourished while sparse or low-reuse architectures
(like RNNs) face a growing `Hardware Tax' that makes them increasingly
inefficient on modern silicon.}

\end{figure}%

Beyond performance limitations, memory access imposes a significant
energy cost. Fetching data from off-chip DRAM consumes far more energy
than performing arithmetic operations
(\citeproc{ref-Horowitz2014}{Horowitz 2014a}). This inefficiency is
particularly evident in machine learning models, where large parameter
sizes, frequent memory accesses, and non-uniform data movement patterns
exacerbate memory bottlenecks. The energy differential drives
architectural decisions: Google's TPU achieves 30-83\(\times\) better
energy efficiency than contemporary GPUs by minimizing data movement
through systolic arrays and large on-chip memory. These design choices
demonstrate that energy constraints, not computational limits, often
determine practical deployment feasibility.

\subsubsection{Memory Access Patterns in ML
Workloads}\label{sec-ai-acceleration-memory-access-patterns-ml-workloads-a960}

To make these energy costs concrete, we can trace a single tensor
through every level of the memory hierarchy during a real inference
pass.

\phantomsection\label{callout-lighthouseux2a-1.12}
\begin{fbx}{callout-lighthouse}{Lighthouse:}{Life of a Tensor: The KWS Journey}
\phantomsection\label{callout-lighthouse*-1.12}
Recall the 1-second audio clip from
\textbf{?@sec-ml-system-architecture}. Here is its physical journey
through the hardware during inference:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{DRAM (HBM)}: The tensor starts here.

  \begin{itemize}
  \tightlist
  \item
    \textbf{Size}: 16,000 samples \(\times\) 2 bytes (FP16) =
    \textbf{\texttt{\{python\}\ kws\_tensor\_str} KB}.
  \item
    \textbf{Latency}: Fetching this from off-chip memory takes
    \textbf{\textasciitilde100 ns} (plus queuing delay).
  \item
    \textbf{Energy}: Cost is \textbf{\textasciitilde200 pJ/bit}. High
    cost.
  \end{itemize}
\item
  \textbf{L2 Cache}: The GPU's DMA engine pulls it here.

  \begin{itemize}
  \tightlist
  \item
    \textbf{Latency}: \textasciitilde20 ns.
  \item
    \textbf{Access}: Shared across multiple Streaming Multiprocessors
    (SMs).
  \end{itemize}
\item
  \textbf{L1 Cache / Shared Memory}: A specific SM claims a tile of the
  audio.

  \begin{itemize}
  \tightlist
  \item
    \textbf{Latency}: \textasciitilde1-2 ns.
  \item
    \textbf{Locality}: Critical step. If the data leaves this level, we
    pay the ``HBM Tax'' again.
  \end{itemize}
\item
  \textbf{Registers}: The Tensor Core operates here.

  \begin{itemize}
  \tightlist
  \item
    \textbf{Latency}: \textasciitilde0 ns (single cycle).
  \item
    \textbf{Throughput}: \texttt{\{python\}\ a100\_tflops\_fp16} TFLOPS.
  \item
    \textbf{Energy}: Cost is \textbf{\textasciitilde0.1 pJ/bit}.
  \end{itemize}
\end{enumerate}

\textbf{The Systems Insight}: The ``Speed of Light'' limit means we
cannot compute faster than we can move data from Step 1 to Step 4. The
\textbf{Roofline} is determined by the bandwidth of the Step 1
\(\rightarrow\) Step 2 link.

\end{fbx}

Machine learning workloads place substantial demands on memory systems
due to the large volume of data involved in computation. Unlike
traditional compute-bound applications, where performance is often
dictated by the speed of arithmetic operations, ML workloads are
characterized by high data movement requirements. An accelerator's
efficiency depends not only on its computational throughput but also on
its ability to continuously supply data to processing units without
introducing stalls or delays.

A neural network processes multiple types of data throughout its
execution, each with distinct memory access patterns:

\begin{itemize}
\tightlist
\item
  \textbf{Model parameters (weights and biases)}: Machine learning
  models, particularly those used in large-scale applications such as
  natural language processing and computer vision, often contain
  millions to billions of parameters. Storing and accessing these
  weights efficiently is necessary for maintaining throughput.
\item
  \textbf{Intermediate activations}: During both training and inference,
  each layer produces intermediate results that must be temporarily
  stored and retrieved for subsequent operations. These activations can
  contribute significantly to memory overhead, particularly in deep
  architectures.
\item
  \textbf{Gradients (during training)}: Backpropagation requires storing
  and accessing gradients for every parameter, further increasing the
  volume of data movement between compute units and memory.
\end{itemize}

As models increase in size and complexity, improvements in memory
capacity and bandwidth become increasingly important. Although
specialized compute units accelerate operations like matrix
multiplications, their overall performance depends on the continuous,
efficient delivery of data to the processing elements. In large-scale
applications such as natural language processing and computer vision,
models often incorporate millions to billions of parameters
(\citeproc{ref-Brown2020}{Brown et al. 2020}), and achieving high
performance requires minimizing delays and stalls caused by inefficient
data movement between memory and compute units
(\citeproc{ref-Narayanan2021}{Narayanan et al. 2021};
\citeproc{ref-Huang2019}{Xingyu 2019}).

One way to quantify this challenge is by comparing the data transfer
time with the time required for computations. Specifically, we define
the memory transfer time as \[
T_{\text{mem}} = \frac{D_{\text{vol}}}{BW},
\] where \(D_{\text{vol}}\) is the total data volume and \(BW\) is the
available memory bandwidth. In contrast, the compute time is given by \[
T_{\text{compute}} = \frac{\text{FLOPs}}{R_{\text{peak}}},
\] with the number of floating-point operations (FLOPs) divided by the
peak hardware throughput, \(R_{\text{peak}}\). When
\(T_{\text{mem}} > T_{\text{compute}}\), the system becomes
memory-bound, meaning that the processing elements spend more time
waiting for data than performing computations. This imbalance
demonstrates the need for memory-optimized architectures and efficient
data movement strategies to sustain high performance.

Figure~\ref{fig-memory-wall} quantifies this disparity for specific
models and hardware generations, showing how model parameter counts have
outpaced memory bandwidth improvements. The gap between these curves,
from AlexNet to trillion-parameter models, represents the engineering
challenge that drives accelerator memory system design.

\subsubsection{Irregular Memory
Access}\label{sec-ai-acceleration-irregular-memory-access-c6ec}

Unlike traditional computing workloads, where memory access follows
well-structured and predictable patterns, machine learning models often
exhibit irregular memory access behaviors that make efficient data
retrieval a challenge. These irregularities arise due to the nature of
ML computations, where memory access patterns are influenced by factors
such as batch size, layer type, and sparsity. As a result, standard
caching mechanisms and memory hierarchies often struggle to optimize
performance, leading to increased memory latency and inefficient
bandwidth utilization.

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{contents/vol1/hw_acceleration/hw_acceleration_files/figure-pdf/fig-memory-wall-3.pdf}}

}

\caption{\label{fig-memory-wall}\textbf{Model Size vs.~Hardware
Bandwidth.} Model parameter counts and hardware memory bandwidth plotted
from 2012 to 2024, showing how model growth from AlexNet to
trillion-parameter models has far outpaced bandwidth improvements across
GPU and TPU generations.}

\end{figure}%

To better understand how ML workloads differ from traditional computing
workloads, it is useful to compare their respective memory access
patterns. Traditional workloads, such as scientific computing,
general-purpose CPU applications, and database processing, typically
exhibit well-defined memory access characteristics that benefit from
standard caching and prefetching techniques. ML workloads, on the other
hand, introduce highly dynamic access patterns
(Table~\ref{tbl-traditional-vs-ml-mem}) that challenge conventional
memory optimization strategies.

One key source of irregularity in ML workloads stems from batch size and
execution order. The way input data is processed in batches directly
affects memory reuse, creating a complex optimization challenge. Small
batch sizes decrease the likelihood of reusing cached activations and
weights, resulting in frequent memory fetches from slower, off-chip
memory. Larger batch sizes can improve reuse and amortize memory access
costs, but simultaneously place higher demands on available memory
bandwidth, potentially creating congestion at different memory hierarchy
levels. This delicate balance requires careful consideration of model
architecture and available hardware resources.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2023}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4277}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3642}}@{}}
\caption{\textbf{Memory Access Characteristics.} Traditional workloads
exhibit predictable, sequential memory access benefiting from standard
caching, while machine learning workloads introduce irregular and
dynamic patterns due to sparsity and data dependencies. These
differences inform the design of memory systems that efficiently support
modern AI applications.}\label{tbl-traditional-vs-ml-mem}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Feature}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Traditional Computing Workloads}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Machine Learning Workloads}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Feature}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Traditional Computing Workloads}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Machine Learning Workloads}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Memory Access Pattern} & Regular and predictable (e.g.,
sequential reads, structured patterns) & Irregular and dynamic (e.g.,
sparsity, attention mechanisms) \\
\textbf{Cache Locality} & High temporal and spatial locality & Often low
locality, especially in large models \\
\textbf{Data Reuse} & Structured loops with frequent data reuse & Sparse
and dynamic reuse depending on layer type \\
\textbf{Data Dependencies} & Well-defined dependencies allow efficient
prefetching & Variable dependencies based on network structure \\
\textbf{Workload Example} & Scientific computing (e.g., matrix
factorizations, physics simulations) & Neural networks (e.g., CNNs,
Transformers, sparse models) \\
\textbf{Memory Bottleneck} & DRAM latency, cache misses & Off-chip
bandwidth constraints, memory fragmentation \\
\textbf{Impact on Energy Consumption} & Moderate, driven by FLOP-heavy
execution & High, dominated by data movement costs \\
\end{longtable}

Different neural network layers interact with memory in distinct ways
beyond batch size considerations. Convolutional layers benefit from
spatial locality, as neighboring pixels in an image are processed
together, enabling efficient caching of small weight kernels.
Conversely, fully connected layers require frequent access to large
weight matrices, often leading to more randomized memory access patterns
that poorly align with standard caching policies. Transformers introduce
additional complexity, as attention mechanisms demand accessing large
key-value pairs stored across varied memory locations. The dynamic
nature of sequence length and attention span renders traditional
prefetching strategies ineffective, resulting in unpredictable memory
latencies.

Another significant factor contributing to irregular memory access is
sparsity\sidenote{\textbf{Sparsity in Neural Networks}: The property
that most weights or activations in a neural network are zero or
near-zero, enabling computational and memory optimizations. Natural
sparsity occurs when ReLU activations zero out 50-90\% of values, while
artificial sparsity results from pruning techniques that remove 90-99\%
of weights with minimal accuracy loss. Sparse networks can be
10-100\(\times\) smaller and faster, but require specialized hardware
support (like NVIDIA's 2:4 sparsity in A100) or software optimization to
realize benefits, as standard dense hardware performs zero
multiplications inefficiently. } in neural networks. Many modern ML
models employ techniques such as weight pruning, activation sparsity,
and structured sparsity to reduce computational overhead. However, these
optimizations often lead to non-uniform memory access, as sparse
representations necessitate fetching scattered elements rather than
sequential blocks, making hardware caching less effective. Models that
incorporate dynamic computation paths, such as Mixture of Experts and
Adaptive Computation Time, introduce highly non-deterministic memory
access patterns, where the active neurons or model components can vary
with each inference step. This variability challenges efficient
prefetching and caching strategies.

These irregularities have significant consequences. ML workloads often
experience reduced cache efficiency, as activations and weights may not
be accessed in predictable sequences. This leads to increased reliance
on off-chip memory traffic, which slows down execution and consumes more
energy. Irregular access patterns contribute to memory fragmentation,
where the way data is allocated and retrieved results in inefficient
utilization of available memory resources. The combined effect is that
ML accelerators frequently encounter memory bottlenecks that limit their
ability to fully utilize available compute power.

The irregular access patterns and memory wall constraints examined above
create formidable challenges, but they also reveal optimization
opportunities. Although individual memory accesses may appear
unpredictable, ML workloads exhibit structured reuse patterns at a
higher level: the same weights are applied across batch elements, the
same kernels slide across spatial dimensions, and the same attention
patterns recur across sequence positions. Hardware designers exploit
these regularities through carefully structured memory hierarchies that
maintain frequently accessed data close to compute units, even when the
specific access sequence varies.

This insight motivates the hierarchical memory architectures found in
all modern AI accelerators: rather than treating memory as a monolithic
resource, these systems organize storage into distinct tiers, each
optimized for different access patterns and reuse characteristics.

\subsection{Memory
Hierarchy}\label{sec-ai-acceleration-memory-hierarchy-1839}

Modern AI accelerators implement sophisticated memory hierarchies that
balance speed, capacity, and energy efficiency by exploiting these
structured reuse patterns. While general-purpose computing contends with
unpredictable memory access, ML workloads exhibit structured reuse that
can be optimized through careful data organization across multiple
memory levels.

At the highest level, large-capacity but slow storage devices provide
long-term model storage. At the lowest level, high-speed registers and
caches ensure that compute units can access operands with minimal
latency. Between these extremes, intermediate memory levels, such as
scratchpad memory, high-bandwidth memory, and off-chip DRAM, offer
trade-offs between performance and capacity.

Table~\ref{tbl-memory-hierarchy} summarizes the multiple memory levels
employed by modern AI accelerators, each with distinct latency,
bandwidth, and capacity properties that directly influence how neural
network data should be allocated.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2349}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.1325}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.0964}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.0904}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.4277}}@{}}
\caption{\textbf{Memory Hierarchy Trade-Offs.} AI accelerators use a
multi-level memory hierarchy to balance performance and capacity. Each
level provides distinct latency, bandwidth, and capacity characteristics
that dictate how neural network components (weights, activations, and
intermediate results) should be allocated to minimize bottlenecks and
maximize throughput.}\label{tbl-memory-hierarchy}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Memory Level}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Approx. Latency}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Bandwidth}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Capacity}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Example Use in Deep Learning}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Memory Level}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Approx. Latency}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Bandwidth}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Capacity}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Example Use in Deep Learning}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Registers} & \textasciitilde1 cycle & Highest & Few values &
Storing operands for immediate computation \\
\textbf{L1/L2 Cache (SRAM)} & \textasciitilde1-10 ns & High & KiBs-MiBs
& Caching frequently accessed activations and small weight blocks \\
\textbf{Scratchpad Memory} & \textasciitilde5-20 ns & High & MiBs &
Software-managed storage for intermediate computations \\
\textbf{High-Bandwidth Memory (HBM)} & \textasciitilde100 ns & Very High
& GiBs & Storing large model parameters and activations for high-speed
access \\
\textbf{Off-Chip DRAM (DDR, GDDR, LPDDR)} & \textasciitilde50-150 ns &
Moderate & GBs-TBs & Storing entire model weights that do not fit
on-chip \\
\textbf{Flash Storage (SSD/NVMe)} & \textasciitilde100 µs - 1 ms & Low &
TBs & Storing pre-trained models and checkpoints for later loading \\
\end{longtable}

A natural question arises from this hierarchy: why not simply build
larger, faster off-chip memory and eliminate the need for on-chip SRAM
entirely? The answer is rooted in physics, specifically the \emph{speed
of light limit} on signal propagation within and between chips.

\phantomsection\label{callout-notebookux2a-1.13}
\begin{fbx}{callout-notebook}{AI Engineer’s Notebook:}{The Speed of Light Limit}
\phantomsection\label{callout-notebook*-1.13}
\textbf{Problem}: Why do we need on-chip SRAM? Why can't we just fetch
everything from HBM?

\textbf{The Physics}:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Distance}: On a large 700mm² chip, signals travel
  \textasciitilde20mm.
\item
  \textbf{Speed}: Signals in silicon travel at \(\approx 0.5c\) (half
  speed of light).
\item
  \textbf{Latency}: 20mm takes \(\approx 130 \text{ ps}\).
\item
  \textbf{Clock Cycle}: At 2 GHz, a cycle is \(500 \text{ ps}\).
\item
  \textbf{DRAM}: Off-chip HBM is centimeters away + protocol overhead =
  \textbf{100+ cycles}.
\end{enumerate}

\textbf{The Systems Conclusion}: You cannot fetch data from DRAM in a
single cycle. It is physically impossible. You \emph{must} have local
registers and SRAM (L1) to feed compute units at 2 GHz. The ``Memory
Wall'' is partially a \textbf{Distance Wall}.

\end{fbx}

\subsubsection{On-Chip
Memory}\label{sec-ai-acceleration-onchip-memory-72d1}

Each level of the memory hierarchy serves a distinct role in AI
acceleration, with different trade-offs in speed, capacity, and
accessibility. Registers, located within compute cores, provide the
fastest access but can only store a few operands at a time. These are
best utilized for immediate computations, where the operands needed for
an operation can be loaded and consumed within a few cycles. However,
because register storage is so limited, frequent memory accesses are
required to fetch new operands and store intermediate results.

To reduce the need for constant data movement between registers and
external memory, small but fast caches serve as an intermediary buffer.
These caches store recently accessed activations, weights, and
intermediate values, ensuring that frequently used data remains
available with minimal delay. However, the size of caches is limited,
making them insufficient for storing full feature maps or large weight
tensors in machine learning models. As a result, only the most
frequently used portions of a model's parameters or activations can
reside here at any given time.

For larger working datasets, many AI accelerators include scratchpad
memory, which offers more storage than caches but with a key difference:
it allows explicit software control over what data is stored and when it
is evicted. Unlike caches, which rely on hardware-based eviction
policies, scratchpad memory enables machine learning workloads to retain
key values such as activations and filter weights for multiple layers of
computation. This capability is useful in models like convolutional
neural networks, where the same input feature maps and filter weights
are reused across multiple operations. By keeping this data in
scratchpad memory rather than reloading it from external memory,
accelerators can significantly reduce unnecessary memory transfers and
improve overall efficiency (\citeproc{ref-Chen2016}{Chen, Emer, and Sze
2017}).

\subsubsection{Off-Chip
Memory}\label{sec-ai-acceleration-offchip-memory-ecdb}

Beyond on-chip memory, high-bandwidth memory provides rapid access to
larger model parameters and activations that do not fit within caches or
scratchpad buffers. HBM achieves its high performance by stacking
multiple memory dies and using wide memory interfaces, allowing it to
transfer large amounts of data with minimal latency compared to
traditional DRAM. Because of its high bandwidth and lower latency, HBM
is often used to store entire layers of machine learning models that
must be accessed quickly during execution. However, its cost and power
consumption limit its use primarily to high-performance AI accelerators,
making it less common in power-constrained environments such as edge
devices.

When a machine learning model exceeds the capacity of on-chip memory and
HBM, it must rely on off-chip DRAM, such as DDR, GDDR, or LPDDR. While
DRAM offers significantly greater storage capacity, its access latency
is higher, meaning that frequent retrievals from DRAM can introduce
execution bottlenecks. To make effective use of DRAM, models must be
structured so that only the necessary portions of weights and
activations are retrieved at any given time, minimizing the impact of
long memory fetch times.

At the highest level of the hierarchy, flash storage and solid-state
drives (SSDs) store large pre-trained models, datasets, and checkpointed
weights. These storage devices offer large capacities but are too slow
for real-time execution, requiring models to be loaded into faster
memory tiers before computation begins. For instance, in training
scenarios, checkpointed models stored in SSDs must be loaded into DRAM
or HBM before resuming computation, as direct execution from SSDs would
be too slow to maintain efficient accelerator utilization
(\citeproc{ref-Narayanan2021}{Narayanan et al. 2021}).

The memory hierarchy thus balances competing objectives of speed,
capacity, and energy efficiency. However, moving data through multiple
memory levels introduces bottlenecks that limit accelerator performance.
Data transfers between memory levels incur latency costs, particularly
for off-chip accesses. Limited bandwidth restricts data flow between
memory tiers. Memory capacity constraints force constant data movement
as models exceed local storage. These constraints make memory bandwidth
the primary determinant of real-world accelerator performance, a topic
we examine next.

\subsection{Memory Bandwidth and Architectural
Trade-offs}\label{sec-ai-acceleration-memory-bandwidth-architectural-tradeoffs-435c}

Building on the memory wall analysis established in
Section~\ref{sec-ai-acceleration-understanding-ai-memory-wall-3ea9},
this section quantifies how specific bandwidth characteristics impact
system performance across different deployment scenarios.

Modern accelerators exhibit distinct bandwidth-capacity trade-offs.
Representative datacenter accelerators can provide memory bandwidth on
the order of a few TB/s, often paired with tens of GB of high-bandwidth
memory, optimizing for flexibility across diverse workloads. Other
designs prioritize energy efficiency and data reuse by combining high
internal bandwidth with smaller on-chip memory. These choices affect
which workloads are easiest to run efficiently: memory-intensive models
benefit from high effective bandwidth and sufficient capacity, while
compute-intensive kernels can perform well when architectures maximize
on-chip reuse.

Different neural network operations achieve varying bandwidth
utilization: transformer attention mechanisms often achieve lower
fractions of peak bandwidth due to irregular access patterns,
convolutional layers can achieve higher fractions through predictable
spatial access patterns, and fully connected layers can approach peak
bandwidth when batch sizes are large enough.

As established earlier, on-chip memory access typically consumes energy
in the single-digit-to-tens of picojoules per access, while external
DRAM can be on the order of hundreds of picojoules per access, an
orders-of-magnitude energy penalty. AI accelerators minimize DRAM access
through three key strategies: weight stationarity (keeping model
parameters in on-chip memory), input stationarity (buffering input
activations locally), and output stationarity (accumulating partial sums
on-chip).

Memory bandwidth scaling follows different trajectories across
accelerator designs:

\begin{itemize}
\tightlist
\item
  \textbf{GPU scaling}: Bandwidth increases with memory channels,
  reaching on the order of 1 TB/s and, in high-end systems, a few TB/s,
  enabling larger model support
\item
  \textbf{TPU scaling}: Bandwidth and utilization are strongly
  influenced by systolic array dataflow and on-chip reuse, often trading
  flexibility for efficiency on dense tensor kernels
\item
  \textbf{Mobile accelerator scaling}: Mobile SoCs can deliver on the
  order of hundreds of GB/s of unified memory bandwidth within a
  few-watt power envelope, requiring careful workload scheduling and
  thermal management
\end{itemize}

HBM provides far higher bandwidth than commodity DDR memory, but at
substantially higher cost and packaging complexity. High-bandwidth
accelerators therefore trade higher memory-system cost for higher
sustained performance on bandwidth-bound workloads. Edge accelerators
often sacrifice bandwidth to meet tight cost and power targets while
maintaining sufficient performance for inference workloads.

These bandwidth characteristics directly influence deployment decisions:
cloud training prioritizes raw bandwidth for maximum model capacity,
edge inference optimizes bandwidth efficiency for energy constraints,
and mobile deployment balances bandwidth with cost limitations. Beyond
the accelerator's internal memory system, however, data must also flow
between the host CPU and the accelerator, introducing another potential
bottleneck.

\subsection{Host-Accelerator
Communication}\label{sec-ai-acceleration-hostaccelerator-communication-bb7a}

Machine learning accelerators, such as GPUs and TPUs, achieve high
computational throughput through parallel execution. However, their
efficiency is often constrained by data movement between the host (CPU)
and accelerator memory. Compared to many traditional workloads that keep
most data within a single memory domain, AI workloads can require
frequent transfers between CPU memory and accelerator memory,
introducing latency, consuming bandwidth, and affecting overall
performance.

Host-accelerator data movement follows a structured sequence. Before
computation begins, data is copied from CPU memory to the accelerator's
memory. The CPU then issues execution instructions, and the accelerator
processes the data in parallel. Once computation completes, the results
are stored in accelerator memory and transferred back to the CPU.
Figure~\ref{fig-host-accelerator-data-movement} details these sequential
steps, each introducing potential inefficiencies that must be managed to
optimize performance.

\begin{figure}[htb]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/81793c445f3dc6a68c48c3a7c6b179584d6565c5.pdf}}

}

\caption{\label{fig-host-accelerator-data-movement}\textbf{Host-Accelerator
Data Transfer}: AI workloads require frequent data movement between CPU
memory and accelerators. The four sequential steps of copying input
data, issuing execution instructions, parallel computation, and
transferring results each introduce potential performance bottlenecks.}

\end{figure}%

The key challenges in host-accelerator data movement include latency,
bandwidth constraints, and synchronization overheads. Optimizing data
transfers through efficient memory management and interconnect
technologies is critical for maximizing accelerator utilization.

\textbf{Data Transfer Patterns.} The efficiency of ML accelerators
depends not only on their computational power but also on the continuous
supply of data. Even high-performance GPUs and TPUs remain underutilized
if data transfers are inefficient. Host and accelerator memory exist as
separate domains, requiring explicit transfers over interconnects such
as PCIe, NVLink, or proprietary links. Ineffective data movement causes
execution stalls, making transfer optimization a priority.

\textbf{Node-Level Interconnect Topology.} To optimize data movement, we
must understand the physical topology of the compute node. A typical AI
server is not a flat mesh of connected devices but a hierarchy of
bandwidths that tapers as we move away from the chip.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Device-Device Interconnect (NVLink / Infinity Fabric)}: Modern
  multi-GPU nodes use specialized high-speed bridges like NVLink to
  connect accelerators directly, bypassing the host CPU. Bandwidth
  ranges from \texttt{\{python\}\ nvlink\_a100} to
  \texttt{\{python\}\ nvlink\_h100} GB/s per GPU. The primary use case
  is gradient synchronization (AllReduce)\sidenote{\textbf{AllReduce}:
  From MPI (Message Passing Interface) terminology, where ``reduce''
  operations combine values across processes using a function like sum
  or max, and ``all'' means every process receives the result. MPI
  standardized these collective operations in 1994. In distributed
  training, AllReduce aggregates gradients from all GPUs (the reduce),
  then distributes the averaged result back to all GPUs (the all). Ring
  AllReduce algorithms achieve optimal bandwidth utilization by having
  each process send and receive simultaneously. } during distributed
  training. This bandwidth is critical for scaling; without it,
  multi-GPU training often scales poorly.
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\item
  \textbf{Host-Device Interconnect (PCIe)}: The link between the CPU and
  the accelerator. Bandwidth ranges from 32 to 64 GB/s (PCIe Gen4/Gen5).
  This link represents the ``Data Loading Bottleneck'': all training
  data must pass through this thin pipe. Even with 8 GPUs providing 5
  TB/s of aggregate compute bandwidth, the system is fed by a single
  \textasciitilde64 GB/s PCIe switch.
\item
  \textbf{Node-Network Interconnect (NIC)}: The link to the outside
  world, connecting to other nodes. Bandwidth ranges from 25 to 50 GB/s
  (\texttt{\{python\}\ ib\_hdr} to \texttt{\{python\}\ ib\_ndr} Gbps
  Ethernet/InfiniBand). This interconnect limits scaling across multiple
  nodes.
\end{enumerate}

These three levels produce a characteristic bandwidth taper:

\[
\begin{aligned}
\text{HBM (2000 GB/s)} &\gg \text{NVLink (900 GB/s)} \\
&\gg \text{PCIe (64 GB/s)} \gg \text{Network (50 GB/s)}
\end{aligned}
\]

System efficiency depends on keeping data as high up this hierarchy as
possible. Once data drops to PCIe or Network speeds, it encounters a
30-100\(\times\) slowdown.

This structured sequence begins with step (1), where data is copied from
CPU memory to accelerator memory, as GPUs cannot directly access host
memory at high speeds. A direct memory access
(DMA)\sidenote{\textbf{Direct Memory Access (DMA)}: Hardware mechanism
that enables devices to transfer data to and from memory without CPU
intervention. DMA engines free the CPU to perform other tasks while data
moves between system memory and accelerators. In representative systems,
host-accelerator links can provide bandwidth on the order of tens of
GB/s (PCIe-class), while some proprietary intra-node interconnects can
provide bandwidth on the order of hundreds of GB/s. This asynchronous
capability is critical for AI workloads where data movement can overlap
with computation, improving overall system utilization. } engine
typically handles this transfer without consuming CPU cycles. In step
(2), the CPU issues execution commands via APIs like CUDA, ROCm, or
OpenCL. Step (3) involves parallel execution on the accelerator, where
stalls can occur if data is not available when needed. Finally, in step
(4), computed results are copied back to CPU memory for further
processing.

Latency and bandwidth limitations significantly impact AI workloads.
PCIe-class host interconnects are typically much slower than an
accelerator's on-package high-bandwidth memory, so large transfers can
become bottlenecks, particularly in deep learning tasks. Additionally,
synchronization overheads arise when computation must wait for data
transfers to complete. Efficient scheduling and overlapping transfers
with execution are necessary to mitigate these inefficiencies.

\textbf{Data Transfer Mechanisms.} The movement of data between the host
(CPU) and the accelerator (GPU, TPU, or other AI hardware) depends on
the interconnect technology that links the two processing units. The
choice of interconnect determines the bandwidth available for transfers,
the latency of communication, and the overall efficiency of
host-accelerator execution. The most commonly used transfer mechanisms
include PCIe (Peripheral Component Interconnect Express), NVLink, Direct
Memory Access, and Unified Memory Architectures. Each of these plays an
important role in optimizing the four-step data movement process
described earlier.

\textbf{PCIe Interface.} Most accelerators communicate with the CPU via
PCIe, the industry-standard interconnect for data movement. PCIe
provides bandwidth on the order of tens of GB/s, which is still
significantly lower than HBM bandwidth within accelerators, making host
transfers a potential bottleneck for large AI workloads.

PCIe also introduces latency overheads due to its packet-based
communication and memory-mapped I/O model. Frequent small transfers are
inefficient, so batching data movement reduces overhead. Computation
commands, issued over PCIe, further contribute to latency, requiring
careful optimization of execution scheduling.

\textbf{NVLink Interface.} To address the bandwidth limitations of PCIe,
NVIDIA developed NVLink, a proprietary high-speed interconnect that
provides significantly higher bandwidth between GPUs and, in some
configurations, between the CPU and GPU. Unlike PCIe, which operates as
a shared bus, NVLink enables direct point-to-point communication between
connected devices, reducing contention and improving efficiency for AI
workloads.

For host-accelerator transfers, NVLink can be used in step (1) to
transfer input data from main memory to GPU memory at speeds far
exceeding PCIe in supported configurations. This can significantly
reduce the data movement bottleneck, allowing accelerators to access
input data with lower latency. In multi-GPU configurations, NVLink also
accelerates peer-to-peer transfers, allowing accelerators to exchange
data without routing through main memory, thereby optimizing step (3) of
the computation process.

Although NVLink offers substantial performance benefits, it is not
universally available. Unlike PCIe, which is an industry standard across
all accelerators, NVLink is specific to NVIDIA hardware, limiting its
applicability to systems designed with NVLink-enabled GPUs.

\textbf{DMA for Data Transfers.} In conventional memory transfers, the
CPU issues load/store instructions, consuming processing cycles. DMA
offloads this task, enabling asynchronous data movement without CPU
intervention.

During data transfers, the CPU initiates a DMA request, allowing data to
be copied to accelerator memory in the background. Similarly, result
transfers back to main memory occur without blocking execution, enabling
AI workloads to overlap computation with data movement for improved
accelerator utilization.

\textbf{Unified Memory.} While PCIe, NVLink, and DMA optimize explicit
memory transfers, some AI workloads require a more flexible memory model
that eliminates the need for manual data copying. Unified Memory
provides an abstraction that allows both the host and accelerator to
access a single, shared memory space, automatically handling data
movement when needed.

With Unified Memory, data does not need to be explicitly copied between
CPU and GPU memory before execution. Instead, when a computation
requires a memory region that is currently located in host memory, the
system automatically migrates it to the accelerator, handling step (1)
transparently. Similarly, when computed results are accessed by the CPU,
step (4) occurs automatically, eliminating the need for manual memory
management.

Although Unified Memory simplifies programming, it introduces
performance trade-offs. Since memory migrations occur on demand, they
can lead to unpredictable latencies, particularly if large datasets need
to be transferred frequently. Additionally, since Unified Memory is
implemented through page migration techniques, small memory accesses can
trigger excessive data movement, further reducing efficiency.

For AI workloads that require fine-grained memory control, explicit data
transfers using PCIe, NVLink, and DMA often provide better performance.
However, for applications where ease of development is more important
than absolute speed, Unified Memory offers a convenient alternative.

\textbf{Data Transfer Overheads.} Host-accelerator data movement
introduces overheads that impact AI workload execution. Unlike on-chip
memory accesses, which occur at nanosecond latencies, host-accelerator
transfers traverse system interconnects, adding latency, bandwidth
constraints, and synchronization delays.

Interconnect latency affects transfer speed, with PCIe, the standard
host-accelerator link, incurring significant overhead due to
packet-based transactions and memory-mapped I/O. This makes frequent
small transfers inefficient. Faster alternatives like NVLink reduce
latency and improve bandwidth but are limited to specific hardware
ecosystems.

Synchronization delays further contribute to inefficiencies. Synchronous
transfers block execution until data movement completes, ensuring data
consistency but introducing idle time. Asynchronous transfers allow
computation and data movement to overlap, reducing stalls but requiring
careful coordination to avoid execution mismatches.

Together, interconnect latency, bandwidth limitations, and
synchronization overheads determine AI workload efficiency, making
transfer optimization essential for achieving high performance.

The transfer mechanisms examined thus far apply uniformly across all
neural network types, but different architectures impose dramatically
different memory demands. A convolutional layer processing images
exhibits regular spatial locality, while a transformer's attention
mechanism requires accessing distant tokens across long sequences. These
architectural differences create distinct memory pressure patterns that
directly influence accelerator design and optimization strategies.

\subsection{Model Memory
Pressure}\label{sec-ai-acceleration-model-memory-pressure-f95e}

Machine learning models impose varying memory access patterns that
significantly influence accelerator performance. The way data is
transferred between the host and accelerator, how frequently memory is
accessed, and the efficiency of caching mechanisms all determine overall
execution efficiency. While multilayer perceptrons (MLPs), convolutional
neural networks (CNNs), and transformer networks each require large
parameter sets, their distinct memory demands necessitate tailored
optimization strategies for accelerators. Understanding these
differences provides insight into why different hardware architectures
exhibit varying levels of efficiency across workloads.

To ground this analysis, we return to the Lighthouse Examples introduced
in \textbf{?@sec-introduction}: \textbf{ResNet-50} represents CNN
workloads with high spatial reuse, \textbf{GPT-2/Llama} exemplifies
transformer memory pressure, \textbf{DLRM} illustrates sparse embedding
lookups that stress memory systems differently than dense operations,
and \textbf{MobileNet} demonstrates efficiency-optimized architectures
with depthwise convolutions. These examples will recur throughout the
remainder of this chapter as we analyze how memory characteristics
translate to hardware utilization.

\subsubsection{Multilayer
Perceptrons}\label{sec-ai-acceleration-multilayer-perceptrons-0bbc}

MLPs, also referred to as fully connected networks, are among the
simplest neural architectures. Each layer consists of a dense matrix
multiplication, requiring every neuron to interact with all neurons in
the preceding layer. This results in high memory bandwidth demands,
particularly for weights, as every input activation contributes to a
large set of computations.

From a memory perspective, MLPs rely on large, dense weight matrices
that frequently exceed on-chip memory capacity, necessitating off-chip
memory accesses. Since accelerators cannot directly access host memory
at high speed, data transfers must be explicitly managed via
interconnects such as PCIe or NVLink. These transfers introduce latency
and consume bandwidth, affecting execution efficiency.

Despite their bandwidth-heavy nature, MLPs exhibit regular and
predictable memory access patterns, making them amenable to
optimizations such as prefetching and streaming memory accesses.
Dedicated AI accelerators mitigate transfer overhead by staging weight
matrices in fast SRAM caches and overlapping data movement with
computation through direct memory access engines, reducing execution
stalls. These optimizations allow accelerators to sustain high
throughput even when handling large parameter sets
(\citeproc{ref-Chen2016}{Chen, Emer, and Sze 2017}).

\subsubsection{Convolutional Neural
Networks}\label{sec-ai-acceleration-convolutional-neural-networks-3085}

Convolutional Neural Networks (CNNs) are widely used in image processing
and computer vision tasks. Unlike MLPs, which require dense matrix
multiplications, CNNs process input feature maps using small filter
kernels that slide across the image. This localized computation
structure results in high spatial data reuse, where the same input
pixels contribute to multiple convolutions.

CNN accelerators benefit from on-chip memory optimizations, as
convolution filters exhibit extensive reuse, allowing weights to be
stored in fast local SRAM instead of frequently accessing off-chip
memory. However, activation maps require careful management due to their
size. Since accessing main memory over interconnects like PCIe
introduces latency and bandwidth bottlenecks, CNN accelerators employ
tiling techniques to divide feature maps into smaller regions that fit
within on-chip buffers. This minimizes costly external memory transfers,
improving overall efficiency (\citeproc{ref-Chen2016}{Chen, Emer, and
Sze 2017}).

While CNN workloads are more memory-efficient than MLPs, managing
intermediate activations remains a challenge. Accelerators use
hierarchical caching strategies and DMA engines to optimize memory
movement, ensuring that computations are not stalled by inefficient
host-accelerator data transfers. These memory optimizations help CNN
accelerators maintain high throughput by reducing reliance on off-chip
memory bandwidth. Pioneering architectures like Eyeriss introduced
row-stationary dataflows to maximize data reuse for convolutional
workloads (\citeproc{ref-chen2016eyeriss}{Chen et al. 2016}).

\subsubsection{Transformer
Networks}\label{sec-ai-acceleration-transformer-networks-638c}

The transformer architectures introduced in
\textbf{?@sec-dnn-architectures} have become the dominant architecture
for natural language processing and are increasingly used in other
domains such as vision and speech recognition. Unlike CNNs, which rely
on local computations, transformers perform global
attention\sidenote{\textbf{Attention}: Borrowed from cognitive
psychology, where attention describes the brain's selective focus on
relevant stimuli while filtering distractions. Bahdanau, Cho, and Bengio
introduced ``attention'' to neural networks in 2014, using the term
because the mechanism lets models ``attend to'' relevant parts of input
sequences. The analogy is apt: just as humans selectively focus on
important information, attention mechanisms learn to weight different
input positions based on relevance to the current computation. This
leads to irregular and bandwidth-intensive memory access patterns, as
large key-value matrices must be fetched and updated frequently. }
mechanisms, where each token in an input sequence can interact with all
other tokens.

These models are particularly challenging for accelerators due to their
massive parameter sizes, which often exceed on-chip memory capacity. As
a result, frequent memory transfers between host and accelerator
introduce substantial latency overheads, particularly when relying on
interconnects such as PCIe. Unified Memory architectures can mitigate
some of these issues by dynamically handling data movement, but they
introduce additional latency due to unpredictable on-demand memory
migrations. Because transformers are memory-bound rather than
compute-bound, accelerators optimized for them rely on high-bandwidth
memory, tensor tiling, and memory partitioning to sustain performance
(\citeproc{ref-Brown2020}{Brown et al. 2020}).

Additionally, attention caching mechanisms and specialized tensor
layouts reduce redundant memory fetches, improving execution efficiency.
Given the bandwidth limitations of traditional interconnects,
NVLink-enabled architectures offer significant advantages for
large-scale transformer training, as they provide higher throughput and
lower latency compared to PCIe. DMA-based asynchronous memory transfers
enable overlapping computation with data movement, reducing execution
stalls (\citeproc{ref-Narayanan2021}{Narayanan et al. 2021}).

\subsection{ML Accelerators
Implications}\label{sec-ai-acceleration-ml-accelerators-implications-c962}

The diverse memory requirements of MLPs, CNNs, and Transformers
highlight the need to tailor memory architectures to specific workloads.
Table~\ref{tbl-model-mem-compare} reveals how memory access patterns
vary dramatically across model types.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.1406}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.1406}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.1797}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2578}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2578}}@{}}
\caption{\textbf{ML Model Memory Access.} Different machine learning
models exhibit distinct memory access patterns and bottlenecks due to
variations in weight size, activation reuse, and data sparsity.
Transformers demand high bandwidth and capacity due to their massive,
sparsely accessed weights, while CNNs benefit from spatial locality and
high activation reuse, reducing memory
pressure.}\label{tbl-model-mem-compare}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Model Type}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Weight Size}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Activation Reuse}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Memory Access Pattern}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Primary Bottleneck}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Model Type}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Weight Size}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Activation Reuse}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Memory Access Pattern}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Primary Bottleneck}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{MLP (Dense)} & Large, dense & Low & Regular, sequential
(streamed) & Bandwidth (off-chip) \\
\textbf{CNN} & Small, reused & High & Spatial locality & Feature map
movement \\
\textbf{Transformer} & Massive, sparse & Low & Irregular, high-bandwidth
& Memory capacity + Interconnect \\
\end{longtable}

Each model type presents unique challenges that directly impact
accelerator design. MLPs benefit from fast streaming access to dense
weight matrices, making memory bandwidth a critical factor in
performance, especially when transferring large weights from host memory
to accelerator memory. CNNs, with their high activation reuse and
structured memory access patterns, can exploit on-chip caching and
tiling strategies to minimize off-chip memory transfers. Transformers,
however, impose significant demands on both bandwidth and capacity, as
attention mechanisms require frequent access to large key-value
matrices, leading to high interconnect traffic and increased memory
pressure.

To address these challenges, modern AI accelerators incorporate
multi-tier memory hierarchies that balance speed, capacity, and energy
efficiency. On-chip SRAM caches and scratchpad memories store frequently
accessed data, while high-bandwidth external memory provides scalability
for large models. Efficient interconnects, such as NVLink, help
alleviate host-accelerator transfer bottlenecks, particularly in
transformer workloads where memory movement constraints can dominate
execution time.

As ML workloads continue to grow in complexity, memory efficiency
becomes as critical as raw compute power. The analysis reveals how
memory systems dominate accelerator performance: DRAM access has 100x or
higher energy cost than on-chip arithmetic, carefully structured memory
hierarchies can improve effective bandwidth substantially, and different
neural network architectures create distinct memory pressure patterns.
These constraints (bandwidth limitations, energy costs, and
communication overheads) determine whether theoretical computational
capabilities translate into real-world performance. But how do we know
if a specific workload is limited by compute or memory on a given
accelerator? The memory wall analysis establishes \emph{why} memory
matters, but practitioners need a quantitative framework to predict
\emph{which} operations will bottleneck on a specific hardware
configuration. The Roofline Model provides this analytical lens,
transforming hardware selection from intuition into engineering.

\section{Measuring Hardware
Efficiency}\label{sec-ai-acceleration-measuring-efficiency}

The Roofline Model answers this question by plotting arithmetic
intensity against attainable performance, revealing whether each
operation hits a compute ceiling or a memory bandwidth ceiling. Rather
than relying on peak FLOPS figures, which reflect marketing rather than
achievable throughput, the Roofline Model provides a quantitative
framework that maps any workload onto a specific hardware platform and
immediately exposes the binding constraint. This section develops that
framework and applies it to the neural network architectures analyzed
above.

\subsection{The Roofline
Model}\label{sec-ai-acceleration-roofline-model}

The roofline model\sidenote{\textbf{Roofline Model}
(\citeproc{ref-williams2009roofline}{Williams, Waterman, and Patterson
2009}): Named for its visual appearance when plotted: a flat horizontal
line (the compute ceiling) meets a sloped line (the memory ceiling)
forming a shape resembling a building roofline. Samuel Williams, Andrew
Waterman, and David Patterson introduced this visualization at UC
Berkeley in 2009. The metaphor captures how workloads hit different
``ceilings'' depending on their arithmetic intensity, making performance
bounds intuitive at a glance. }
(\citeproc{ref-williams2009roofline}{Williams, Waterman, and Patterson
2009}) provides the standard framework for understanding whether
workloads are compute-bound or memory-bound, directly connecting the
memory wall discussion to practical performance analysis. This model
enables quantitative reasoning about accelerator utilization and guides
optimization decisions.

\textbf{Model Definition}: Performance is bounded by two ceilings:
\[\text{Attainable Performance} = \min(\text{Peak Compute}, \text{Peak Bandwidth} \times \text{Arithmetic Intensity})\]

The key metric that determines which ceiling a workload hits is
\emph{arithmetic intensity}, the ratio of computation to memory traffic.

\phantomsection\label{callout-definitionux2a-1.14}
\begin{fbx}{callout-definition}{Definition:}{Arithmetic Intensity}
\phantomsection\label{callout-definition*-1.14}
\textbf{\emph{Arithmetic Intensity}} is the measure of
\textbf{Computational Density}, defined as the ratio of FLOPs performed
to Bytes transferred. It serves as the independent variable in the
\textbf{Roofline Model}, determining the \textbf{Regime of Operation}:
low intensity workloads are \textbf{Bandwidth-Bound} (waiting for data),
while high intensity workloads are \textbf{Compute-Bound} (waiting for
math).

\end{fbx}

\textbf{Arithmetic Intensity (AI)} measures operations per byte of
memory traffic:
\[\text{AI} = \frac{\text{FLOPs}}{\text{Bytes Transferred}}\]

The roofline visualization shows performance (TFLOPS) on the vertical
axis and arithmetic intensity (FLOPS/byte) on the horizontal axis. At
low arithmetic intensity, performance increases linearly with intensity
(memory-bound region). Above a threshold called the ridge point,
performance saturates at peak compute (compute-bound region).

\textbf{Hardware Ridge Points}: The ridge point determines the
arithmetic intensity threshold where the transition from memory-bound to
compute-bound occurs. Table~\ref{tbl-ridge-points} quantifies how
different accelerators exhibit distinct characteristics based on their
compute-to-bandwidth ratios:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1957}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2899}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3116}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1884}}@{}}
\caption{\textbf{Hardware Ridge Points.} Representative,
order-of-magnitude ridge points for different accelerators, determined
by their compute-to-bandwidth ratios. Higher ridge points require more
operations per byte to achieve peak
utilization.}\label{tbl-ridge-points}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Accelerator}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Peak FP16}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Bandwidth}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Ridge Point}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Accelerator}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Peak FP16}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Bandwidth}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Ridge Point}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{GPU (2017-era)} & (\sim 10\^{}2) TFLOPS & (\sim 10\^{}3) GB/s &
(\sim 10\^{}2) FLOP/byte \\
\textbf{GPU (2020-era)} & (\sim 10\^{}2) TFLOPS & (\sim 10\^{}3) GB/s to
(\sim 10\^{}0) TB/s & (\sim 10\^{}2) FLOP/byte \\
\textbf{GPU (2023-era)} & (\sim 10\^{}3) TFLOPS & a few TB/s &
(\sim 10\^{}2) FLOP/byte \\
\textbf{TPU-class (2023-era)} & (\sim 10\^{}2) to (\sim 10\^{}3) TFLOPS
& (\sim 1) TB/s & (\sim 10\^{}2) FLOP/byte \\
\end{longtable}

These ridge point values reveal a surprising trend: as hardware has
become more powerful, keeping it fully utilized has become harder. The
following analysis illustrates this utilization gap.

\phantomsection\label{callout-notebookux2a-1.15}
\begin{fbx}{callout-notebook}{AI Engineer’s Notebook:}{The Utilization Gap}
\phantomsection\label{callout-notebook*-1.15}
\textbf{The Utilization Physics}: Why is it harder to get 100\%
utilization on an H100 than a V100?

\textbf{Metric}: The Ridge Point
(\(R = \text{Peak FLOPS} / \text{Peak Bandwidth}\)). This number tells
you how many math operations you \emph{must} perform for every byte of
data you load to keep the compute units busy.

\textbf{The Evolution}:

\begin{itemize}
\tightlist
\item
  \textbf{V100 (2017)}:
  \(`{python} v100_tflops` \text{ TF} / 0.9 \text{ TB/s} \approx \mathbf{`{python} v100_ridge` \text{ Ops/Byte}}\).
\item
  \textbf{A100 (2020)}:
  \(`{python} a100_tflops_fp16` \text{ TF} / `{python} a100_bw_tbs` \text{ TB/s} \approx \mathbf{`{python} a100_ridge` \text{ Ops/Byte}}\).
\item
  \textbf{H100 (2023)}:
  \(`{python} h100_tflops_fp16` \text{ TF} / `{python} h100_bw_tbs` \text{ TB/s} \approx \mathbf{`{python} h100_ridge` \text{ Ops/Byte}}\).
\end{itemize}

\textbf{The Systems Conclusion}: The ``bar'' for compute intensity has
doubled. * An algorithm with AI = 200 Ops/Byte was
\textbf{compute-bound} (good) on A100. * That \emph{same algorithm} is
\textbf{bandwidth-bound} (bad) on H100. * This explains why ``legacy''
code often sees only 2-3x speedup on H100 (bandwidth ratio) instead of
the advertised 6-9x (FLOPs ratio).

\end{fbx}

We can determine this balance by explicitly \emph{calculating the ridge
point} for our specific hardware.

\phantomsection\label{callout-notebookux2a-1.16}
\begin{fbx}{callout-notebook}{AI Engineer’s Notebook:}{Calculating the Ridge Point}
\phantomsection\label{callout-notebook*-1.16}
\textbf{Problem}: You are using an \textbf{NVIDIA H100} GPU. You want to
know if your attention layer is compute-bound or bandwidth-bound.

\textbf{The Math}:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Peak Compute}: \texttt{\{python\}\ h100\_tflops\_fp16} TFLOPS
  (FP16).
\item
  \textbf{Peak Bandwidth}: \texttt{\{python\}\ h100\_bw\_tbs} TB/s.
\item
  \textbf{Ridge Point}:
  \(`{python} h100_tflops_fp16` \text{ TFLOPS} / `{python} h100_bw_tbs` \text{ TB/s} \approx \mathbf{`{python} h100_ridge` \text{ Ops/Byte}}\).
\end{enumerate}

\textbf{The Systems Conclusion}: To achieve 100\% utilization of an
H100, your code must perform \textbf{\texttt{\{python\}\ h100\_ridge}
mathematical operations for every 1 byte} it pulls from memory.

\begin{itemize}
\tightlist
\item
  A standard \textbf{ReLU} performs 1 operation for every 8 bytes (0.125
  Ops/Byte). It is \textbf{2,300x below the roofline}.
\item
  A large \textbf{Dense MatMul} (batch=128) might reach 300 Ops/Byte. It
  is \textbf{Compute-Bound}.
\end{itemize}

\textbf{The Engineering Moral}: Most of the time, your ``Ferrari'' (the
H100) is stuck in traffic because you aren't feeding it enough ``Fuel''
(Data Bandwidth). This is why \textbf{Kernel Fusion} is the most
important optimization, as explored in
Section~\ref{sec-ai-acceleration-kernel-fusion-7faf}.

\end{fbx}

Table~\ref{tbl-roofline-operations} maps common neural network
operations to the Roofline model:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2400}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2700}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2100}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2600}}@{}}
\caption{\textbf{Operations on the Roofline.} Neural network layers span
a wide range of arithmetic intensities. By mapping these operations to
the \textbf{Lighthouse Examples}, ResNet-50 emerges as compute-bound
(high AI) while MobileNet and DLRM are memory-bound (low
AI).}\label{tbl-roofline-operations}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Operation}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Arithmetic Intensity}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Classification}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Lighthouse Example}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Operation}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Arithmetic Intensity}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Classification}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Lighthouse Example}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Conv2D (Dense)} & 50-200 FLOP/byte & Compute-bound &
\textbf{ResNet-50} \\
\textbf{Dense MatMul} & 64-256 FLOP/byte & Compute-bound & \textbf{GPT-2
(Projections)} \\
\textbf{Depthwise Conv} & 10-20 FLOP/byte & Memory-bound &
\textbf{MobileNet} \\
\textbf{Attention Softmax} & 2-5 FLOP/byte & Memory-bound &
\textbf{GPT-2 (Generation)} \\
\textbf{LayerNorm} & 5-10 FLOP/byte & Memory-bound & \textbf{GPT-2 /
Llama} \\
\textbf{Embedding lookup} & \textless1 FLOP/byte & Memory-bound &
\textbf{DLRM} \\
\end{longtable}

To see how these intensity values translate into real performance
predictions, we can work through a complete transformer layer and
compute the arithmetic intensity of each sub-operation.

\phantomsection\label{notebook-transformer-layers}
\begin{fbx}{callout-notebook}{AI Engineer’s Notebook:}{Transformer Layer Analysis}
\phantomsection\label{notebook-transformer-layers}
For a transformer with hidden\_dim=768, batch=32, seq=512:

\emph{Attention QKV Projection}:

\begin{itemize}
\tightlist
\item
  FLOPs: \(3 \times 32 \times 512 \times 768 \times 768\) =
  \texttt{\{python\}\ qkv\_flops\_b\_str} billion FLOPs
\item
  Bytes: (input + weights + output)
  \(= (32 \times 512 \times 768 + 3 \times 768 \times 768 + 32 \times 512 \times 768 \times 3) \times 2\)
  ≈ \texttt{\{python\}\ qkv\_mb\_str} MB
\item
  AI = \texttt{\{python\}\ qkv\_flops\_b\_str}B /
  \texttt{\{python\}\ qkv\_mb\_str}M = \texttt{\{python\}\ qkv\_ai\_str}
  FLOP/byte, which is \textbf{compute-bound on A100} (above
  \texttt{\{python\}\ a100\_ridge} threshold)
\end{itemize}

\emph{Softmax}:

\begin{itemize}
\tightlist
\item
  FLOPs: \(32 \times 12 \times 512 \times 512 \times 3\) ≈
  \texttt{\{python\}\ softmax\_flops\_m\_str}M FLOPs (exp, sum, div)
\item
  Bytes: \(32 \times 12 \times 512 \times 512 \times 2 \times 2\) =
  \texttt{\{python\}\ softmax\_mb\_str} MB
\item
  AI = \texttt{\{python\}\ softmax\_flops\_m\_str}M /
  \texttt{\{python\}\ softmax\_mb\_str}M =
  \texttt{\{python\}\ softmax\_ai\_str} FLOP/byte, which is
  \textbf{memory-bound}
\end{itemize}

This analysis explains why FlashAttention focuses on reducing memory
traffic in attention rather than reducing FLOPs.

\end{fbx}

\textbf{Optimization Implications}:

For \textbf{memory-bound operations}: reduce data movement through
operator fusion, use reduced precision (FP16, INT8), and increase
arithmetic intensity through algorithmic changes like FlashAttention.

For \textbf{compute-bound operations}: maximize hardware utilization
through batching and parallelism, use Tensor Cores and specialized
compute units, and optimize compute efficiency through tiling and
scheduling.

\textbf{Calculating Memory Bandwidth Bounds}: The roofline model's
memory-bound region is determined by the peak memory bandwidth. For an
operation to achieve performance \(P\) (TFLOPS) in the memory-bound
regime, the required bandwidth is:
\[\text{Required Bandwidth} = \frac{P}{\text{AI}} \text{ bytes/sec}\]

When Required Bandwidth exceeds Peak Bandwidth, performance is capped
at: \[P_{\text{attainable}} = \text{Peak Bandwidth} \times \text{AI}\]

A \emph{convolutional layer analysis} demonstrates how these formulas
apply in practice.

\phantomsection\label{notebook-conv-analysis}
\begin{fbx}{callout-notebook}{AI Engineer’s Notebook:}{Convolutional Layer Analysis}
\phantomsection\label{notebook-conv-analysis}
Consider a Conv2D layer with input shape (batch=32, channels=128,
height=56, width=56), output channels=256, kernel size 3×3 on an A100
GPU:

\emph{Computational Requirements}:

\begin{itemize}
\tightlist
\item
  Output size: \(32 \times 256 \times 56 \times 56 =\)
  \texttt{\{python\}\ conv\_out\_m\_str}M elements
\item
  FLOPs per output: \(128 \times 3 \times 3 \times 2 =\)
  \texttt{\{python\}\ conv\_flops\_per\_out\_str} (multiply-add)
\item
  Total FLOPs: \texttt{\{python\}\ conv\_out\_m\_str}M \(\times\)
  \texttt{\{python\}\ conv\_flops\_per\_out\_str} \(=\)
  \texttt{\{python\}\ conv\_total\_gflops\_str} billion FLOPs
\end{itemize}

\emph{Memory Traffic Analysis}:

\begin{itemize}
\tightlist
\item
  Input: \(32 \times 128 \times 56 \times 56 \times 2 =\)
  \texttt{\{python\}\ conv\_input\_mb\_str} MB (FP16)
\item
  Weights: \(256 \times 128 \times 3 \times 3 \times 2 \approx\)
  \texttt{\{python\}\ conv\_weights\_mb\_str} MB (FP16)
\item
  Output: \(32 \times 256 \times 56 \times 56 \times 2 =\)
  \texttt{\{python\}\ conv\_output\_mb\_str} MB (FP16)
\item
  Total: \texttt{\{python\}\ conv\_total\_mb\_str} MB
\end{itemize}

\emph{Arithmetic Intensity}:
\[\text{AI} = \frac{`{python} conv_total_gflops_str` \text{ GFLOPs}}{`{python} conv_total_mb_str` \text{ MB}} = `{python} conv_ai_str` \text{ FLOP/byte}\]

This is \textbf{well above} A100's ridge point of
\texttt{\{python\}\ a100\_ridge} FLOP/byte, making this operation
\textbf{compute-bound}. The layer will achieve near-peak performance of
\textasciitilde{}\texttt{\{python\}\ a100\_tflops\_fp16} TFLOPS (FP16
with Tensor Cores).

\end{fbx}

The convolutional layer's high arithmetic intensity arises from its
weight reuse pattern: the same 3×3 kernel is applied across all spatial
locations, amortizing the cost of loading weights across millions of
output computations. This is the architectural pattern that makes CNNs
so efficient on modern accelerators.

However, not all layers in a neural network exhibit this favorable
profile. The fully connected (dense) layers that typically appear at the
end of classification networks, or as the projection layers in
transformers, have different arithmetic intensity characteristics.
Understanding this contrast is essential for predicting where
bottlenecks will occur in end-to-end model execution.

\phantomsection\label{callout-notebookux2a-1.19}
\begin{fbx}{callout-notebook}{AI Engineer’s Notebook:}{Dense Layer Analysis}
\phantomsection\label{callout-notebook*-1.19}
Consider a fully connected layer: input (batch=32, features=2048) →
output (batch=32, features=2048) on the same A100:

\emph{Computational Requirements}:

\begin{itemize}
\tightlist
\item
  Matrix multiply: \((32 \times 2048) \times (2048 \times 2048)\)
\item
  Total FLOPs: \(2 \times 32 \times 2048 \times 2048 =\)
  \texttt{\{python\}\ dense\_total\_mflops\_str} million FLOPs
\end{itemize}

\emph{Memory Traffic Analysis}:

\begin{itemize}
\tightlist
\item
  Input: \(32 \times 2048 \times 2 =\)
  \texttt{\{python\}\ dense\_input\_kb\_str} KB (FP16)
\item
  Weights: \(2048 \times 2048 \times 2 =\)
  \texttt{\{python\}\ dense\_weights\_mb\_str} MB (FP16)
\item
  Output: \(32 \times 2048 \times 2 =\)
  \texttt{\{python\}\ dense\_output\_kb\_str} KB (FP16)
\item
  Total: \texttt{\{python\}\ dense\_total\_mb\_str} MB
\end{itemize}

\emph{Arithmetic Intensity}:
\[\text{AI} = \frac{`{python} dense_total_mflops_str` \text{ MFLOPs}}{`{python} dense_total_mb_str` \text{ MB}} = `{python} dense_ai_str` \text{ FLOP/byte}\]

This is \textbf{below} A100's ridge point of
\texttt{\{python\}\ a100\_ridge} FLOP/byte, making this operation
\textbf{memory-bound}. Attainable performance:
\[P_{\text{attainable}} = `{python} a100_bw` \text{ GB/s} \times `{python} dense_ai_str` \text{ FLOP/byte} = `{python} dense_attainable_str` \text{ TFLOPS}\]

This is only 20\% of peak compute capability, demonstrating the memory
wall effect for small batch sizes.

\end{fbx}

The dense layer's lower arithmetic intensity stems from limited weight
reuse: each weight element is used only once per batch element, whereas
convolutional weights are reused across spatial dimensions. This
difference explains why transformer inference (dominated by dense
projections) is typically memory-bound while CNN inference can be
compute-bound.

The situation becomes even more extreme for element-wise operations like
normalization layers. These operations perform very little computation
relative to the data they touch, as a \emph{LayerNorm analysis} reveals.
Each element is loaded, transformed by a simple formula, and written
back, leaving essentially no opportunity for data reuse.

\phantomsection\label{callout-notebookux2a-1.20}
\begin{fbx}{callout-notebook}{AI Engineer’s Notebook:}{LayerNorm Analysis}
\phantomsection\label{callout-notebook*-1.20}
LayerNorm with input shape (batch=32, seq=512, hidden=768):

\emph{Computational Requirements}:

\begin{itemize}
\tightlist
\item
  Elements: \(32 \times 512 \times 768 =\)
  \texttt{\{python\}\ ln\_elements\_m\_str}M
\item
  Operations per element: mean (1 ADD), variance (2 ADD, 1 MUL),
  normalize (1 ADD, 1 MUL, 1 DIV) ≈ 6 FLOPs
\item
  Total FLOPs: \texttt{\{python\}\ ln\_elements\_m\_str}M \(\times\) 6
  \(=\) \texttt{\{python\}\ ln\_total\_mflops\_str}M FLOPs
\end{itemize}

\emph{Memory Traffic}:

\begin{itemize}
\tightlist
\item
  Input: \texttt{\{python\}\ ln\_elements\_m\_str}M \(\times\) 2 \(=\)
  \texttt{\{python\}\ ln\_input\_mb\_str} MB
\item
  Parameters (scale, bias): \(768 \times 2 \times 2 =\)
  \texttt{\{python\}\ ln\_params\_kb\_str} KB (negligible)
\item
  Output: \texttt{\{python\}\ ln\_elements\_m\_str}M \(\times\) 2 \(=\)
  \texttt{\{python\}\ ln\_output\_mb\_str} MB
\item
  Total: \texttt{\{python\}\ ln\_total\_mb\_str} MB
\end{itemize}

\emph{Arithmetic Intensity}:
\[\text{AI} = \frac{`{python} ln_total_mflops_str` \text{ MFLOPs}}{`{python} ln_total_mb_str` \text{ MB}} = `{python} ln_ai_str` \text{ FLOP/byte}\]

This is \textbf{severely memory-bound} (156× below ridge point).
Performance is limited to:
\[P_{\text{attainable}} = 2000 \text{ GB/s} \times `{python} ln_ai_str` \text{ FLOP/byte} = `{python} ln_attainable_str` \text{ TFLOPS}\]

This represents less than 1\% of A100's compute capacity, explaining why
normalization layers contribute negligible compute time but significant
latency.

\end{fbx}

\textbf{Optimization Strategy Selection by Arithmetic Intensity}:

The roofline analysis directly informs optimization priorities:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{High AI (\textgreater200 FLOP/byte)}: Compute-bound operations
  like large convolutions

  \begin{itemize}
  \tightlist
  \item
    Priority: Maximize compute utilization
  \item
    Techniques: Use Tensor Cores, optimize thread block dimensions,
    maximize occupancy
  \item
    Impact: Can approach 90-95\% of peak TFLOPS
  \end{itemize}
\item
  \textbf{Medium AI (20-200 FLOP/byte)}: Borderline operations like
  medium-sized dense layers

  \begin{itemize}
  \tightlist
  \item
    Priority: Balance compute and memory optimization
  \item
    Techniques: Increase batch size to improve AI, use register tiling,
    fuse with adjacent operations
  \item
    Impact: Can move from memory-bound to compute-bound regime
  \end{itemize}
\item
  \textbf{Low AI (\textless20 FLOP/byte)}: Memory-bound operations like
  small dense layers, element-wise operations

  \begin{itemize}
  \tightlist
  \item
    Priority: Reduce memory traffic
  \item
    Techniques: Aggressive operator fusion, reduce precision (FP16 →
    INT8), algorithmic changes
  \item
    Impact: 2-4× speedup possible through fusion alone
  \end{itemize}
\item
  \textbf{Very Low AI (\textless2 FLOP/byte)}: Severely memory-bound
  operations like normalization, activation functions

  \begin{itemize}
  \tightlist
  \item
    Priority: Eliminate memory round-trips
  \item
    Techniques: Mandatory fusion with adjacent operations, in-place
    computation where possible
  \item
    Impact: Can achieve 10× speedup through fusion (e.g., LayerNorm +
    GELU → single fused kernel)
  \end{itemize}
\end{enumerate}

One of the most accessible levers for shifting an operation's position
on the roofline is increasing \emph{batch size and arithmetic
intensity}.

\phantomsection\label{callout-notebookux2a-1.21}
\begin{fbx}{callout-notebook}{AI Engineer’s Notebook:}{Batch Size and Arithmetic Intensity}
\phantomsection\label{callout-notebook*-1.21}
Increasing batch size improves AI for matrix operations by amortizing
weight loading:

For dense layer \((B \times M) \times (M \times N)\):
\[\text{AI} = \frac{2BMN}{2BM + 2MN + 2BN} \approx \frac{2BMN}{2MN} = B \text{ as } B \to \infty\]

Example: Dense layer with M=N=2048 - Batch=1: AI = 2 FLOP/byte
(memory-bound) - Batch=32: AI = 32 FLOP/byte (memory-bound) - Batch=256:
AI = 186 FLOP/byte (compute-bound on A100)

This explains the 10-100× throughput improvement from batching in
production inference systems, as MLPerf inference scenarios demonstrate.

\end{fbx}

The batch size analysis reveals why inference serving systems are
designed around batching: it changes the arithmetic intensity regime of
memory-bound workloads. However, batching introduces latency trade-offs,
since requests must wait in a queue until a batch forms. This tension
between throughput (favoring large batches) and latency (favoring small
batches) is a central challenge in ML serving systems, explored in depth
in \textbf{?@sec-model-serving-systems}.

For workloads where batching is impractical, such as interactive LLM
generation where users expect streaming responses, the arithmetic
intensity remains inherently low. Understanding this ceiling is
essential for setting realistic performance expectations.

This bandwidth constraint creates \emph{the throughput ceiling}.

\phantomsection\label{callout-notebookux2a-1.22}
\begin{fbx}{callout-notebook}{AI Engineer’s Notebook:}{The Throughput Ceiling}
\phantomsection\label{callout-notebook*-1.22}
\textbf{The Problem:} Predict the maximum possible utilization of an
NVIDIA A100 when running GPT-2 inference (batch size 1).

\textbf{1. The Hardware Constraints (The Denominators)}

\begin{itemize}
\tightlist
\item
  \textbf{Peak Compute:} \texttt{\{python\}\ a100\_tflops\_fp16} TFLOPS
  (FP16 Tensor Core).
\item
  \textbf{Peak Bandwidth:} \texttt{\{python\}\ a100\_bw\_tbs} TB/s
  (HBM2e).
\item
  \textbf{Ridge Point (Compute/BW):}
  \(`{python} a100_tflops_fp16` / `{python} a100_bw_tbs` = \mathbf{`{python} a100_ridge` \text{ FLOPs/Byte}}\)
  (for FP16 Tensor Core).

  \begin{itemize}
  \tightlist
  \item
    \emph{Meaning:} To saturate this chip at FP16 precision, you must
    perform \texttt{\{python\}\ a100\_ridge} operations for every byte
    loaded. The ridge point varies by precision: FP32 operations
    (\texttt{\{python\}\ a100\_tflops\_fp32} TFLOPS peak) have a ridge
    point of only
    \textasciitilde{}\texttt{\{python\}\ a100\_ridge\_fp32} FLOP/byte.
  \end{itemize}
\end{itemize}

\textbf{2. The Workload Characteristics (The Numerator)}

\begin{itemize}
\tightlist
\item
  \textbf{Model:} GPT-2 XL (1.5B parameters).
\item
  \textbf{Operation:} Autoregressive generation (1 token at a time).
\item
  \textbf{Data Movement:} Must load all weights
  (\texttt{\{python\}\ gpt2\_weight\_gb\_str} GB @ FP16) for every
  token.
\item
  \textbf{Compute:} Vector-Matrix multiplication.
  \(2 \times \text{Params} \approx `{python} gpt2_decode_gflops_str` \text{ GFLOPs}\).
\item
  \textbf{Arithmetic Intensity:}
  \[ \frac{`{python} gpt2_decode_gflops_str` \text{ GFLOPs}}{`{python} gpt2_weight_gb_str` \text{ GB}} = \mathbf{`{python} gpt2_decode_ai_str` \text{ FLOP/Byte}} \]
\end{itemize}

\textbf{3. The Prediction (Iron Law)}

Since Actual Intensity (\(`{python} gpt2_decode_ai_str`\)) \(\ll\) Ridge
Point (\(`{python} a100_ridge`\)), the system is \textbf{Bandwidth
Bound}.

\begin{itemize}
\tightlist
\item
  \textbf{Maximum Throughput:}
  \(`{python} gpt2_decode_ai_str` \text{ FLOP/Byte} \times `{python} a100_bw_tbs` \text{ TB/s} = \mathbf{`{python} gpt2_max_tflops_str` \text{ TFLOPS}}\).
\item
  \textbf{Utilization Ceiling:}
  \[ \frac{`{python} gpt2_max_tflops_str` \text{ TFLOPS (Actual)}}{`{python} a100_tflops_fp16` \text{ TFLOPS (Peak)}} \approx \mathbf{`{python} gpt2_utilization_str`\%} \]
\end{itemize}

\textbf{The Systems Conclusion:} Without batching or caching, a \$15,000
GPU runs at \textbf{less than 1\% efficiency} on LLM inference. This
``Utilization Gap'' drives the need for Key-Value Caching and
Quantization.

\end{fbx}

As this derivation demonstrates, the Roofline model provides the
diagnostic framework for identifying whether operations are
compute-bound or memory-bound. Knowing that a workload is memory-bound
at \texttt{\{python\}\ gpt2\_utilization\_str}\% utilization is only the
first step; the next challenge is translating this diagnosis into
efficient execution plans that exploit accelerator architectures. This
is the domain of hardware mapping: the art of assigning computations to
processing elements and scheduling data movement to maximize the
utilization that the Roofline analysis reveals as possible.

\section{Hardware Mapping Fundamentals for Neural
Networks}\label{sec-ai-acceleration-hardware-mapping-fundamentals-neural-networks-f9a9}

The Roofline analysis taught us to diagnose whether specific operations
are compute-bound or memory-bound on given hardware. We saw that
ResNet-50's convolutions achieve high arithmetic intensity (50-200
FLOP/byte) and operate in the compute-bound regime, while GPT-2's
attention layers achieve only 2-5 FLOP/byte and are severely
memory-bound. But diagnosis is only half the challenge. Once we know
that LayerNorm achieves just 1-2 FLOP/byte on an A100, the question
becomes: how do we execute it efficiently despite this limitation? This
is the domain of hardware mapping, the art of translating abstract
computational graphs into concrete execution plans that exploit
accelerator architectures while respecting their constraints.

The memory system challenges examined in
Section~\ref{sec-ai-acceleration-understanding-ai-memory-wall-3ea9}
established \emph{why} memory access dominates modern AI systems: DRAM
access consumes 100-200x more energy than a multiply-accumulate
operation (\citeproc{ref-horowitz2014computing}{Horowitz 2014b}). The
Roofline model established \emph{how to measure} whether a workload is
compute-bound or memory-bound. This section addresses the critical
follow-up: \emph{how to map} computations to maximize data reuse and
minimize the energy-intensive transfers that the Roofline analysis
revealed as the primary bottleneck.

Efficient execution of machine learning models on specialized AI
acceleration hardware requires a structured approach to computation,
ensuring that available resources are fully utilized while minimizing
performance bottlenecks. These mapping considerations become
particularly critical in distributed training scenarios, as explored in
\textbf{?@sec-ai-training}. Unlike general-purpose processors, which
rely on dynamic task scheduling, AI accelerators operate under a
structured execution model that maximizes throughput by carefully
assigning computations to processing elements. This process, known as
mapping, dictates how computations are distributed across hardware
resources, influencing execution speed, memory access patterns, and
overall efficiency.

\phantomsection\label{callout-definitionux2a-1.23}
\begin{fbx}{callout-definition}{Definition:}{Mapping in AI Acceleration}
\phantomsection\label{callout-definition*-1.23}
\textbf{\emph{Mapping in AI Acceleration}} is the binding of the
\textbf{Logical Computation Graph} to the \textbf{Physical Hardware
Topology}. It optimizes the \textbf{Spatiotemporal Schedule}---deciding
\emph{where} data resides (spatial) and \emph{when} it moves
(temporal)---to minimize the \textbf{Energy-Delay Product} of execution
under strict memory bandwidth constraints.

\end{fbx}

Mapping machine learning models onto AI accelerators presents several
challenges due to hardware constraints and the diversity of model
architectures. Given the hierarchical memory system of modern
accelerators, mapping strategies must carefully manage when and where
data is accessed to minimize latency and power overhead while ensuring
that compute units remain actively engaged. Poor mapping decisions can
lead to underutilized compute resources, excessive data movement, and
increased execution time, ultimately reducing overall efficiency.

Mapping encompasses three aspects that form the foundation of effective
AI accelerator design.

\begin{itemize}
\tightlist
\item
  \textbf{Computation Placement}: Systematically assigns operations
  (e.g., matrix multiplications, convolutions) to processing elements to
  maximize parallelism and reduce idle time.
\item
  \textbf{Memory Allocation}: Carefully determines where model
  parameters, activations, and intermediate results reside within the
  memory hierarchy to optimize access efficiency.
\item
  \textbf{Dataflow and Execution Scheduling}: Structures the movement of
  data between compute units to reduce bandwidth bottlenecks and ensure
  smooth, continuous execution.
\end{itemize}

Effective mapping strategies minimize off-chip memory accesses, maximize
compute utilization, and efficiently manage data movement across
different levels of the memory hierarchy. In practice, \emph{the role of
the compiler} is central to achieving these goals.

\phantomsection\label{callout-perspectiveux2a-1.24}
\begin{fbx}{callout-perspective}{Systems Perspective:}{The Role of the Compiler}
\phantomsection\label{callout-perspective*-1.24}
Developers rarely perform this complex mapping manually. Instead, a
specialized \textbf{compiler} (like NVIDIA's NVCC or Google's XLA) takes
the high-level model from the framework and automatically explores the
mapping search space to find an optimal execution plan for the target
hardware. The compiler is the critical software layer that translates
the model's computational graph into an efficient hardware-specific
dataflow, balancing the three interrelated aspects of computation
placement, memory allocation, and execution scheduling described above.
This compiler support is examined in detail in
Section~\ref{sec-ai-acceleration-compiler-support-172e}.

\end{fbx}

Key mapping choices influence execution efficiency and lay the
groundwork for optimization strategies that refine these decisions.

\subsection{Placement and
Allocation}\label{sec-ai-acceleration-computation-placement-23d2}

\textbf{Computation Placement.} Computation placement is the process of
strategically assigning operations to an accelerator's processing
elements (PEs) to maximize parallelism, minimize idle time, and reduce
unnecessary data movement. Modern accelerators contain enormous numbers
of PEs: the NVIDIA H100 has over 16,000 streaming processors and more
than 500 tensor cores (\citeproc{ref-nvidia2022h100}{Choquette 2023}),
TPUs use systolic arrays of thousands of multiply-accumulate units
(\citeproc{ref-jouppi_tpu_2017}{Norman P. Jouppi et al. 2017c}), and
wafer-scale processors like Cerebras' CS-2 integrate over 850,000 cores
(\citeproc{ref-Cerebras2021}{Systems 2021}). At these scales, even small
placement inefficiencies compound into significant performance losses
because idle cores and redundant memory transfers waste both time and
energy.

The difficulty of placement depends on workload regularity. CNNs exhibit
structured, spatially local computation: a \(256\times256\) image can be
tiled across thousands of GPU cores with each tile processed
independently, yielding balanced utilization. Transformers are harder
because self-attention requires every token to interact with every
other, creating non-uniform demands where attention score computation is
far heavier than other operations. Graph Neural Networks (GNNs) are
harder still, as sparse, dynamically changing graph structures make
static partitioning ineffective (\citeproc{ref-Zheng2020}{Zheng et al.
2020}). Table~\ref{tbl-placement-challenges} summarizes the core
challenges that placement strategies must address across these workload
types.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.1486}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4699}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3775}}@{}}
\caption{\textbf{Computation Placement Challenges.} Effective neural
network deployment requires strategic allocation of computations to
processing elements, balancing workload distribution, data movement
costs, and hardware constraints to maximize execution efficiency. These
challenges guide the design of mapping strategies that optimize resource
utilization and minimize communication
overhead.}\label{tbl-placement-challenges}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Challenge}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Impact on Execution}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Key Considerations for Placement}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Challenge}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Impact on Execution}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Key Considerations for Placement}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Workload Imbalance} & Some processing elements finish early
while others remain overloaded, leading to idle compute resources. &
Distribute operations evenly to prevent stalls and ensure full
utilization of PEs. \\
\textbf{Irregular Computation Patterns} & Models like transformers and
GNNs introduce non-uniform computation demands, making static placement
difficult. & Use adaptive placement strategies that adjust execution
based on workload characteristics. \\
\textbf{Excessive Data Movement} & Frequent memory transfers introduce
latency and increase power consumption. & Keep frequently used data
close to the compute units and minimize off-chip memory accesses. \\
\textbf{Limited Interconnect Bandwidth} & Poorly placed operations can
create congestion, slowing data movement between PEs. & Optimize spatial
and temporal placement to reduce communication overhead. \\
\textbf{Model-Specific Execution Needs} & CNNs, transformers, and GNNs
require different execution patterns, making a single placement strategy
ineffective. & Tailor placement strategies to match the computational
structure of each model type. \\
\end{longtable}

Because a well-placed workload can reduce latency by 10 to 100 times
while a poorly placed one leaves thousands of PEs idle, modern
accelerators increasingly rely on runtime-aware scheduling that adapts
placement to real-time workload behavior rather than static execution
plans. Placement decisions also interact directly with the next concern:
where the data those PEs need actually resides in the memory hierarchy.

\textbf{Memory Allocation.} While computation placement determines where
operations execute, memory allocation defines where data resides and how
it flows through the memory hierarchy during execution. The primary goal
is to keep frequently accessed data as close as possible to the
processing elements, minimizing latency and power consumption. GPUs
achieve this through a mix of global memory, shared memory, and
registers with careful tiling strategies
(\citeproc{ref-nvidia2020ampere}{N. Corporation 2020}). TPUs use on-chip
SRAM scratchpads where activations and weights must be preloaded to
sustain systolic array execution (Figure~\ref{fig-systolic-array}), with
weights streamed in perfect synchronization with input activations to
maintain pipelined computation flow
(\citeproc{ref-jouppi_tpu_2017}{Norman P. Jouppi et al. 2017c}).
Wafer-scale processors demand sophisticated memory partitioning to avoid
excessive interconnect traffic (\citeproc{ref-Cerebras2021}{Systems
2021}). Unlike general-purpose computing, where caches abstract memory
management, AI accelerators require explicit data placement strategies
because poor allocation leads to three compounding penalties: increased
memory latency when data must be fetched from higher-latency tiers,
higher power consumption from off-chip accesses that cost orders of
magnitude more energy than on-chip storage, and reduced computational
throughput when processing elements stall waiting for data.

The severity of these penalties varies by workload. CNNs rely on
structured, localized access patterns and benefit from well-defined
memory layouts that facilitate predictable reuse
(\citeproc{ref-chen2016eyeriss}{Chen et al. 2016}). Transformer models
require frequent access to large parameter sets and intermediate
activations, making them highly sensitive to memory bandwidth
constraints. GNNs introduce the greatest challenge, as their irregular
and sparse data structures produce unpredictable access patterns that
resist static allocation strategies. Table~\ref{tbl-memory-allocation}
summarizes these allocation challenges. As model sizes continue to grow,
accelerators must dynamically manage memory resources rather than
relying on static allocation schemes, and memory capacity increasingly
dictates how large a model can be deployed on a given accelerator.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.1703}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3886}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4367}}@{}}
\caption{\textbf{Memory Allocation Challenges.} Efficient memory
management in AI accelerators balances data access speed with hardware
constraints, mitigating performance bottlenecks caused by latency,
bandwidth limitations, and irregular data patterns. Complex models such
as transformers and graph networks impose variable and demanding memory
requirements that amplify these
challenges.}\label{tbl-memory-allocation}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Challenge}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Impact on Execution}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Key Considerations for Allocation}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Challenge}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Impact on Execution}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Key Considerations for Allocation}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{High Memory Latency} & Slow data access delays execution and
reduces throughput. & Prioritize placing frequently accessed data in
faster memory locations. \\
\textbf{Limited On-Chip Storage} & Small local memory constrains the
amount of data available near compute units. & Allocate storage
efficiently to maximize data availability without exceeding hardware
limits. \\
\textbf{High Off-Chip Bandwidth Demand} & Frequent access to external
memory increases delays and power consumption. & Reduce unnecessary
memory transfers by carefully managing when and how data is moved. \\
\textbf{Irregular Memory Access Patterns} & Some models require
accessing data unpredictably, leading to inefficient memory usage. &
Organize memory layout to align with access patterns and minimize
unnecessary data movement. \\
\textbf{Model-Specific Memory Needs} & Different models require
different allocation strategies to optimize performance. & Tailor
allocation decisions based on the structure and execution
characteristics of the workload. \\
\end{longtable}

\subsection{Combinatorial
Complexity}\label{sec-ai-acceleration-combinatorial-complexity-ea33}

The efficient execution of machine learning models on AI accelerators
requires careful consideration of placement and allocation. Placement
involves spatial assignment of computations and data, while allocation
covers temporal distribution of resources. These decisions are
interdependent, and each introduces trade-offs that impact performance,
energy efficiency, and scalability.
Table~\ref{tbl-combinatorial-complexity} enumerates the key trade-offs
between computation placement and resource allocation that shape overall
performance. Placement decisions influence parallelism, memory access
patterns, and communication overhead, while allocation strategies
determine how resources are distributed over time to balance execution
efficiency. The interplay between these factors requires a careful
balance to avoid bottlenecks such as excessive synchronization, memory
congestion, or underutilized compute resources. Optimizing these
trade-offs is necessary for ensuring that AI accelerators operate at
peak efficiency.

Each of these dimensions requires balancing trade-offs between placement
and allocation. For instance, spatially distributing computations across
multiple processing elements can increase throughput; however, if data
allocation is not optimized, memory bandwidth limitations may introduce
bottlenecks. Likewise, allocating resources for fine-grained
computations may enhance flexibility but, without appropriate placement
strategies, may lead to excessive synchronization overhead.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.1594}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4263}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4104}}@{}}
\caption{\textbf{Placement-Allocation Trade-Offs.} AI accelerator
performance depends on strategically mapping computations to hardware
and allocating resources over time, balancing parallelism, memory
access, and execution efficiency. Careful consideration of these
interdependent factors is essential for maximizing throughput and
minimizing energy
consumption.}\label{tbl-combinatorial-complexity}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Dimension}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Placement Considerations}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Allocation Considerations}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Dimension}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Placement Considerations}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Allocation Considerations}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Computational Granularity} & Fine-grained placement enables
greater parallelism but increases synchronization overhead. &
Coarse-grained allocation reduces synchronization overhead but may limit
flexibility. \\
\textbf{Spatial vs.~Temporal Mapping} & Spatial placement enhances
parallel execution but can lead to resource contention and memory
congestion. & Temporal allocation balances resource sharing but may
reduce overall throughput. \\
\textbf{Memory and Data Locality} & Placing data closer to compute units
minimizes latency but may reduce overall memory availability. &
Allocating data across multiple memory levels increases capacity but
introduces higher access costs. \\
\textbf{Communication and Synchronization} & Co-locating compute units
reduces communication latency but may introduce contention. & Allocating
synchronization mechanisms mitigates stalls but can introduce additional
overhead. \\
\textbf{Dataflow and Execution Ordering} & Static placement simplifies
execution but limits adaptability to workload variations. & Dynamic
allocation improves adaptability but adds scheduling complexity. \\
\end{longtable}

Because AI accelerator architectures impose constraints on both where
computations execute and how resources are assigned over time, selecting
an effective mapping strategy requires a coordinated approach to
placement and allocation.

\subsubsection{Exploring the Configuration
Space}\label{sec-ai-acceleration-exploring-configuration-space-f010}

The efficiency of AI accelerators is determined not only by their
computational capabilities but also by how neural network computations
are mapped to hardware resources. Mapping defines how computations are
assigned to processing elements, how data is placed and moved through
the memory hierarchy, and how execution is scheduled. The choices made
in this process significantly impact performance, influencing compute
utilization, memory bandwidth efficiency, and energy consumption.

Mapping machine learning models to hardware presents a large and complex
design space. Unlike traditional computational workloads, model
execution involves multiple interacting factors (computation, data
movement, parallelism, and scheduling), each introducing constraints and
tradeoffs. The hierarchical memory structure of accelerators further
complicates this process by imposing limits on bandwidth, latency, and
data reuse, so effective mapping strategies must carefully balance
competing objectives to maximize efficiency.

At the heart of this design space lie three interconnected aspects: data
placement, computation scheduling, and data movement timing. Data
placement refers to the allocation of data across various memory
hierarchies, such as on-chip buffers, caches, and off-chip DRAM, and its
effective management is critical because it influences both latency and
energy consumption. Inefficient placement often results in frequent,
costly memory accesses, whereas strategic placement ensures that data
used regularly remains in fast-access storage. Computation scheduling
governs the order in which operations execute, impacting compute
efficiency and memory access patterns; for instance, some execution
orders may optimize parallelism while introducing synchronization
overheads, and others may improve data locality at the expense of
throughput. Meanwhile, timing in data movement is equally important, as
transferring data between memory levels incurs significant latency and
energy costs. Efficient mapping strategies thus focus on minimizing
unnecessary transfers by reusing data and overlapping communication with
computation to enhance overall performance.

These factors define a vast combinatorial design space, where small
variations in mapping decisions can lead to large differences in
performance and energy efficiency. A poor mapping strategy can result in
underutilized compute resources, excessive data movement, or imbalanced
workloads, creating bottlenecks that degrade overall efficiency.
Conversely, a well-designed mapping maximizes both throughput and
resource utilization, making efficient use of available hardware.

Because of the interconnected nature of mapping decisions, there is no
single optimal solution. Different workloads and hardware architectures
demand different approaches. Different mapping choices shape the
execution of machine learning workloads.

Mapping machine learning computations onto specialized hardware requires
balancing multiple constraints such as compute efficiency, memory
bandwidth, and execution scheduling. The challenge arises from the vast
number of possible ways to assign computations to processing elements,
order execution, and manage data movement. Each decision contributes to
a high-dimensional search space, where even minor variations in mapping
choices can significantly impact performance.

Unlike traditional workloads with predictable execution patterns,
machine learning models introduce diverse computational structures that
require flexible mappings adapted to data reuse, parallelization
opportunities, and memory constraints. The search space grows
combinatorially, making exhaustive search infeasible. Three sources of
variation contribute to this complexity:

\textbf{Ordering Computation and Execution.} Machine learning workloads
are often structured as nested loops that iterate over various
dimensions of computation. For instance, a matrix multiplication kernel
may loop over batch size (\(N\)), input features (\(C\)), and output
features (\(K\)). The order in which these loops execute has a profound
effect on data locality, reuse patterns, and computational efficiency.

The number of ways to arrange \(d\) loops follows a factorial growth
pattern: \[
\mathcal{O} = d!
\] which scales rapidly. A typical convolutional layer may involve up to
seven loop dimensions, leading to: \[
7! = 5,040 \text{ possible execution orders.}
\]

When considering multiple memory levels, the search space expands as: \[
(d!)^l
\] where \(l\) is the number of memory hierarchy levels. This rapid
expansion shows why execution order optimization matters: poor loop
ordering can lead to excessive memory traffic, while an optimized order
improves cache utilization (\citeproc{ref-sze2017efficient}{Sze et al.
2017a}).

\textbf{Parallelization Across Processing Elements.} Modern AI
accelerators use thousands of processing elements to maximize
parallelism, but determining which computations should be parallelized
is non-trivial. Excessive parallelization can introduce synchronization
overheads and increased bandwidth demands, while insufficient
parallelization leads to underutilized hardware.

The number of ways to distribute computations among parallel units
follows the binomial coefficient: \[
\mathcal{P} = \frac{d!}{(d-k)!}
\] where \(d\) is the number of loops, and \(k\) is the number selected
for parallel execution. For a six-loop computation where three loops are
chosen for parallel execution, the number of valid configurations is: \[
\frac{6!}{(6-3)!} = 120.
\]

Even for a single layer, there can be hundreds of valid parallelization
strategies, each affecting data synchronization, memory contention, and
overall compute efficiency. Expanding this across multiple layers and
model architectures further magnifies the complexity.

\textbf{Memory Placement and Data Movement.} The hierarchical memory
structure of AI accelerators introduces additional constraints, as data
must be efficiently placed across registers, caches, shared memory, and
off-chip DRAM. Data placement impacts latency, bandwidth consumption,
and energy efficiency. Frequent access to slow memory creates
bottlenecks, while optimized placement reduces costly memory transfers.

The number of ways to allocate data across memory levels follows an
exponential growth function: \[
\mathcal{M} = n^{d \times l}
\] where:

\begin{itemize}
\tightlist
\item
  \(n\) = number of placement choices per level,
\item
  \(d\) = number of computational dimensions,
\item
  \(l\) = number of memory hierarchy levels.
\end{itemize}

For a model with:

\begin{itemize}
\tightlist
\item
  \(d = 5\) computational dimensions,
\item
  \(l = 3\) memory levels,
\item
  \(n = 4\) possible placement choices per level,
\end{itemize}

\noindent the number of possible memory allocations is: \[
4^{5 \times 3} = 4^{15} = 1,073,741,824.
\]

This highlights how even a single layer may have over a billion possible
memory configurations, making manual optimization impractical.

\textbf{Mapping Search Space.} By combining the complexity from
computation ordering, parallelization, and memory placement, the total
mapping search space can be approximated as: \[
\mathcal{S} = \left( n^d \times d! \times \frac{d!}{(d-k)!} \right)^l
\] where:

\begin{itemize}
\tightlist
\item
  \(n^d\) represents memory placement choices,
\item
  \(d!\) accounts for computation ordering choices,
\item
  \(\frac{d!}{(d-k)!}\) captures parallelization possibilities,
\item
  \(l\) is the number of memory hierarchy levels.
\end{itemize}

This equation illustrates the exponential growth of the search space,
making brute-force search infeasible for all but the simplest cases.

The combinatorial explosion revealed by this analysis, with potentially
billions of valid configurations for a single neural network layer,
poses a practical question: how do practitioners routinely achieve
near-optimal performance despite this vast search space? Exhaustive
enumeration is clearly impossible, yet production systems consistently
achieve 60-80\% of theoretical peak performance. The answer lies in a
small set of principled dataflow patterns that capture the essential
trade-offs, constraining the search to well-understood strategies that
have proven effective across diverse workloads. These patterns reduce
the seemingly intractable configuration space to a manageable set of
strategic choices.

\section{Dataflow Optimization
Strategies}\label{sec-ai-acceleration-dataflow-optimization-strategies-ce52}

This section introduces the principled dataflow patterns through three
questions that structure all dataflow decisions:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Which data stays local?} Weight-stationary, output-stationary,
  and input-stationary strategies each make different choices about what
  to cache near compute units, trading off different memory access
  patterns.
\item
  \textbf{How is data organized?} Tensor layouts (NHWC vs.~NCHW)
  determine whether memory accesses align with hardware preferences,
  with performance impacts of 2-5x.
\item
  \textbf{How are operations combined?} Kernel fusion and tiling
  restructure computation to minimize memory traffic, often achieving
  2-10x speedups through reduced data movement alone.
\end{enumerate}

By mastering these patterns, you will be able to reason about 90\% of
dataflow optimization decisions without exhaustive search. We examine
each question in turn, then see how they combine for specific neural
network architectures including ResNet-50, GPT-2, and MLPs.

Mapping strategies establish \emph{where} computations execute and
\emph{where} data resides within an accelerator's architecture, but they
do not specify \emph{how} data flows through processing elements during
execution. A systolic array might process a matrix multiplication with
weights stored in local memory, but the order in which weights, inputs,
and outputs move through the array directly determines memory bandwidth
consumption and energy efficiency. These dataflow patterns, termed
optimization strategies, represent the critical implementation dimension
that translates abstract mapping decisions into concrete execution
plans.

The choice among weight-stationary, input-stationary, and
output-stationary approaches directly impacts whether an accelerator
operates in the compute-bound or memory-bound region. Understanding
these trade-offs matters because compilers
(Section~\ref{sec-ai-acceleration-compiler-support-172e}) and runtime
systems (Section~\ref{sec-ai-acceleration-runtime-support-f94f}) must
select appropriate dataflow patterns based on computational
characteristics and memory hierarchy capabilities analyzed in
Section~\ref{sec-ai-acceleration-memory-hierarchy-1839}.

To overcome the combinatorial challenge, AI accelerators rely on
structured mapping strategies that systematically balance computational
efficiency, data locality, and parallel execution. Rather than
evaluating every possible configuration, these approaches use a
combination of heuristic, analytical, and machine learning-based
techniques to find high-performance mappings efficiently.

\subsection{Building Blocks of Mapping
Strategies}\label{sec-ai-acceleration-building-blocks-mapping-strategies-4932}

To navigate the complexity of mapping decisions, practitioners rely on a
set of foundational techniques that optimize execution across data
movement, memory access, and computation efficiency.

Key techniques include data movement strategies, which determine where
data is staged during computation in order to reduce redundant
transfers, such as in weight stationary, output stationary, and input
stationary approaches. Memory-aware tensor layouts also play an
important role by influencing memory access patterns and cache
efficiency through the organization of data in formats such as row-major
or channel-major.

Other strategies involve kernel fusion, a method that minimizes
redundant memory writes by combining multiple operations into a single
computational step. Tiling is employed as a technique that partitions
large computations into smaller, memory-friendly blocks to improve cache
efficiency and reduce memory bandwidth requirements. Finally, balancing
computation and communication is necessary for managing the trade-offs
between parallel execution and memory access to achieve high throughput.

Each of these building blocks forms the basis for both heuristic and
model-driven optimization techniques.

\subsubsection{Data Movement
Patterns}\label{sec-ai-acceleration-data-movement-patterns-3b06}

While computational mapping determines where and when operations occur,
its success depends heavily on how efficiently data is accessed and
transferred across the memory hierarchy. As discussed in
Section~\ref{sec-ai-acceleration-irregular-memory-access-c6ec}, machine
learning workloads exhibit irregular access patterns that challenge
standard caching mechanisms. This irregularity makes data movement
strategy critical to overall system performance.

Even when computational units are mapped efficiently, poor data movement
strategies can severely degrade performance, leading to frequent memory
stalls and underutilized hardware resources. If data cannot be supplied
to processing elements at the required rate, computational units remain
idle, increasing latency, memory traffic, and energy consumption
(\citeproc{ref-chen2016eyeriss}{Chen et al. 2016}).

Listing~\ref{lst-matmul_data_movement} illustrates how data movement
inefficiencies affect the backbone computation of many machine learning
models through a typical matrix multiplication operation.

\begin{codelisting}

\caption{\label{lst-matmul_data_movement}\textbf{Matrix Multiplication}:
Data movement bottlenecks can lead to underutilized hardware resources,
illustrating the importance of efficient data flow in optimizing machine
learning model performance. Via This operation}

\centering{

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#\# Matrix multiplication where:}
\CommentTok{\#\# weights: [512 x 256] {-} model parameters}
\CommentTok{\#\# input:   [256 x 32]  {-} batch of activations}
\CommentTok{\#\# Z:       [512 x 32]  {-} output activations}

\CommentTok{\#\# Computing each output element Z[i,j]:}
\ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\DecValTok{512}\NormalTok{):}
    \ControlFlowTok{for}\NormalTok{ j }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\DecValTok{32}\NormalTok{):}
        \ControlFlowTok{for}\NormalTok{ k }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\DecValTok{256}\NormalTok{):}
\NormalTok{            Z[i, j] }\OperatorTok{+=}\NormalTok{ weights[i, k] }\OperatorTok{*} \BuiltInTok{input}\NormalTok{[k, j]}
\end{Highlighting}
\end{Shaded}

}

\end{codelisting}%

This computation reveals several critical dataflow challenges. The first
challenge is the number of memory accesses required. For each output
\(Z[i, j]\), the computation must fetch an entire row of weights from
the weight matrix and a full column of activations from the input
matrix. Since the weight matrix contains 512 rows and the input matrix
contains 32 columns, this results in repeated memory accesses that place
a significant burden on memory bandwidth.

The second challenge comes from weight reuse. The same weights are
applied to multiple inputs, meaning that an ideal mapping strategy
should maximize weight locality to avoid redundant memory fetches.
Without proper reuse, the accelerator would waste bandwidth loading the
same weights multiple times (\citeproc{ref-chen2018tvm}{0001 et al.
2018a}).

The third challenge involves the accumulation of intermediate results.
Since each element in \(Z[i,j]\) requires contributions from 256
different weight-input pairs, partial sums must be stored and retrieved
before the final value is computed. If these intermediate values are
stored inefficiently, the system will require frequent memory accesses,
further increasing bandwidth demands.

One way to mitigate these challenges is to use SIMD and SIMT execution
models, which allow multiple values to be fetched in parallel. However,
even with these optimizations, data movement remains a bottleneck. The
issue is not just how quickly data is retrieved but how often it must be
moved and where it is placed within the memory hierarchy
(\citeproc{ref-han2016eie}{Han et al. 2016}).

Given that data movement is 100-1000x more expensive than computation,
the single most important goal of an accelerator is to minimize memory
access. Dataflow strategies achieve this by maximizing data reuse. The
question is: which data is most valuable to keep local? To address this,
accelerators implement dataflow strategies that determine which data
remains fixed in memory and which data is streamed dynamically. These
strategies represent different answers to the central question of data
locality: weight-stationary keeps model parameters local,
input-stationary maintains activation data, and output-stationary
preserves intermediate results. Each approach trades off different
memory access patterns to maximize data reuse and minimize the
energy-intensive transfers that constitute the primary bottleneck in AI
acceleration.

\paragraph{Weight
Stationary}\label{sec-ai-acceleration-weight-stationary-156a}

The Weight Stationary strategy keeps weights fixed in local memory,
while input activations and partial sums are streamed through the
system. Weight stationary approaches prove particularly beneficial in
CNNs and matrix multiplications, where the same set of weights is
applied across multiple inputs. By ensuring weights remain stationary,
this method reduces redundant memory fetches, which helps alleviate
bandwidth bottlenecks and improves energy efficiency.

A key advantage of weight stationary is that it maximizes weight reuse,
reducing the frequency of memory accesses to external storage. Since
weight parameters are often shared across multiple computations, keeping
them in local memory eliminates unnecessary data movement, lowering the
overall energy cost of computation. This makes it particularly effective
for architectures where weights represent the dominant memory overhead,
such as systolic arrays and custom accelerators designed for machine
learning.

Listing~\ref{lst-weight_stationary} demonstrates how Weight Stationary
execution keeps weights fixed in local memory while streaming inputs and
accumulating partial sums.

\begin{codelisting}

\caption{\label{lst-weight_stationary}\textbf{Weight Stationary Matrix
Multiplication}: Weight stationary matrix multiplication keeps weights
fixed in local memory while input activations stream through,
demonstrating how it maximizes weight reuse to reduce energy costs.}

\centering{

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#\# Weight Stationary Matrix Multiplication}
\CommentTok{\#\# {-} Weights remain fixed in local memory}
\CommentTok{\#\# {-} Input activations stream through}
\CommentTok{\#\# {-} Partial sums accumulate for final output}

\ControlFlowTok{for}\NormalTok{ weight\_block }\KeywordTok{in}\NormalTok{ weights:  }\CommentTok{\# Load and keep weights stationary}
\NormalTok{    load\_to\_local(weight\_block)  }\CommentTok{\# Fixed in local storage}
    \ControlFlowTok{for}\NormalTok{ input\_block }\KeywordTok{in}\NormalTok{ inputs:  }\CommentTok{\# Stream inputs dynamically}
        \ControlFlowTok{for}\NormalTok{ output\_block }\KeywordTok{in}\NormalTok{ outputs:  }\CommentTok{\# Compute results}
\NormalTok{            output\_block }\OperatorTok{+=}\NormalTok{ compute(weight\_block, input\_block)}
            \CommentTok{\# Reuse weights across inputs}
\end{Highlighting}
\end{Shaded}

}

\end{codelisting}%

In weight stationary execution, weights are loaded once into local
memory and remain fixed throughout the computation while inputs stream
dynamically, reducing redundant memory accesses. Partial sums accumulate
efficiently, minimizing unnecessary data movement. Because weights need
not be reloaded for each new computation, bandwidth requirements drop
significantly, making this dataflow highly effective for workloads with
heavy weight reuse patterns such as CNNs and matrix multiplications.

However, while this strategy reduces weight-related memory traffic, it
introduces trade-offs in input and output movement. Since inputs must be
streamed dynamically while weights remain fixed, the efficiency of this
approach depends on how well input activations can be delivered to the
computational units without causing stalls. Additionally, partial sums,
which represent intermediate results, must be carefully accumulated to
avoid excessive memory traffic. The total performance gain depends on
the size of available on-chip memory, as storing larger weight matrices
locally can become a constraint in models with millions or billions of
parameters.

The weight stationary strategy is well-suited for workloads where
weights exhibit high reuse and memory bandwidth is a limiting factor. It
is commonly employed in CNNs, systolic arrays, and matrix multiplication
kernels, where structured weight reuse leads to significant performance
improvements. However, for models where input or output reuse is more
critical, alternative dataflow strategies, such as output stationary or
input stationary, may provide better trade-offs.

\paragraph{Output
Stationary}\label{sec-ai-acceleration-output-stationary-54e5}

Weight stationary keeps weights local and streams inputs through the
system. But what if the dominant cost is not weight loading but the
frequent writes of partial sums? In fully connected layers and
transformer attention mechanisms, each output element accumulates
contributions from hundreds or thousands of weight-input pairs. Writing
those intermediate partial sums to external memory after every
accumulation step would create a write-bandwidth bottleneck far more
severe than the read overhead that weight stationary addresses. The
Output Stationary strategy inverts the priority: it keeps partial sums
fixed in local memory while streaming both weights and input activations
through the system, so that each output element is written to external
memory only once, after all its contributions have been accumulated
(\citeproc{ref-chen2016eyeriss}{Chen et al. 2016}).

Listing~\ref{lst-output_stationary} demonstrates how accumulating
partial sums locally minimizes memory writes and enhances efficiency
during matrix multiplication.

\begin{codelisting}

\caption{\label{lst-output_stationary}\textbf{Output Stationary
Execution}: Accumulates partial sums locally to reduce memory writes and
enhance efficiency during matrix multiplication, making it ideal for
transformer-based models.}

\centering{

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#\# {-} Partial sums remain in local memory}
\CommentTok{\#\# {-} Weights and input activations stream through dynamically}
\CommentTok{\#\# {-} Final outputs are written only once}

\ControlFlowTok{for}\NormalTok{ output\_block }\KeywordTok{in}\NormalTok{ outputs:  }\CommentTok{\# Keep partial sums stationary}
\NormalTok{    accumulator }\OperatorTok{=} \DecValTok{0}  \CommentTok{\# Initialize accumulation buffer}
    \ControlFlowTok{for}\NormalTok{ weight\_block, input\_block }\KeywordTok{in} \BuiltInTok{zip}\NormalTok{(weights, inputs):}
\NormalTok{        accumulator }\OperatorTok{+=}\NormalTok{ compute(weight\_block, input\_block)}
        \CommentTok{\# Accumulate partial sums}
\NormalTok{    store\_output(accumulator)  }\CommentTok{\# Single write to memory}
\end{Highlighting}
\end{Shaded}

}

\end{codelisting}%

In this implementation, the accumulator buffer stays in local registers
or scratchpad throughout the inner loop; weights and inputs stream in,
contribute to the running sum, and are discarded. The final result is
written out only once per output element, eliminating the repeated write
traffic that would otherwise dominate bandwidth.

This approach aligns naturally with systolic arrays, where computation
progresses through a grid of processing elements and partial sums can
flow along one axis without leaving the chip. The trade-off is that both
weights and activations must now be streamed dynamically, so the system
must sustain high read bandwidth for two data streams simultaneously.
Parallel implementations also require careful synchronization when
multiple PEs contribute to the same output element. Output stationary is
therefore most effective for workloads where accumulation dominates,
such as fully connected layers and attention mechanisms, but less
suitable when input reuse is the critical bottleneck.

\paragraph{Input
Stationary}\label{sec-ai-acceleration-input-stationary-6c7b}

The two strategies examined so far each fix a different operand in local
memory: weight stationary fixes weights to reduce read bandwidth for
parameters, and output stationary fixes partial sums to reduce write
bandwidth for accumulations. The third strategy completes the picture by
fixing the remaining operand: input activations. In transformer models,
a single input token participates in computations across multiple
attention heads and layers; in batch processing, the same activation
batch feeds into many different weight matrices. When activation reuse
is the dominant memory cost, keeping inputs stationary and streaming
weights through the system yields the best energy and bandwidth
trade-off.

Listing~\ref{lst-input_stationary} illustrates this approach, maximizing
reuse by keeping input activations stationary in local memory while
dynamically streaming weights.

\begin{codelisting}

\caption{\label{lst-input_stationary}\textbf{Input Stationary}: This
approach keeps input activations stationary while dynamically streaming
weights to maximize memory reuse and reduce energy consumption.}

\centering{

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#\# {-} Input activations remain in local memory}
\CommentTok{\#\# {-} Weights stream through dynamically}
\CommentTok{\#\# {-} Partial sums accumulate and are written out}

\ControlFlowTok{for}\NormalTok{ input\_block }\KeywordTok{in}\NormalTok{ inputs:  }\CommentTok{\# Keep input activations stationary}
\NormalTok{    load\_to\_local(input\_block)  }\CommentTok{\# Fixed in local storage}
    \ControlFlowTok{for}\NormalTok{ weight\_block }\KeywordTok{in}\NormalTok{ weights:  }\CommentTok{\# Stream weights dynamically}
        \ControlFlowTok{for}\NormalTok{ output\_block }\KeywordTok{in}\NormalTok{ outputs:  }\CommentTok{\# Compute results}
\NormalTok{            output\_block }\OperatorTok{+=}\NormalTok{ compute(weight\_block, input\_block)}
            \CommentTok{\# Reuse inputs across weights}
\end{Highlighting}
\end{Shaded}

}

\end{codelisting}%

Here, input activations are loaded once and held fixed while weights
stream through. Partial sums accumulate and are eventually written out,
but unlike output stationary, the accumulation buffer is not the primary
beneficiary of locality; instead, the input data is.

The trade-off mirrors the other two strategies: weights must now be
streamed dynamically, so the system needs sustained read bandwidth for
the weight stream, and partial sums require buffering before write-back.
Input stationary is most effective in transformers (where each token is
reused across attention heads), recurrent networks (where the hidden
state participates in repeated computations), and large-batch inference
(where the same activation batch feeds many weight matrices).

Taken together, the three dataflow strategies illustrate a fundamental
design choice rather than a hierarchy of quality. Weight stationary
minimizes read traffic for parameters and suits CNNs with small, heavily
reused filters. Output stationary minimizes write traffic for
accumulations and suits fully connected layers with high fan-in. Input
stationary minimizes read traffic for activations and suits transformers
and batch processing with high activation reuse. No single strategy
dominates; the optimal choice depends on which data element has the
highest reuse ratio relative to its size, a determination that the
compiler and hardware designer must make based on the specific workload
and memory hierarchy.

\subsubsection{Memory-Efficient Tensor
Layouts}\label{sec-ai-acceleration-memoryefficient-tensor-layouts-e250}

Efficient execution of machine learning workloads depends not only on
how data moves (dataflow strategies) but also on how data is stored and
accessed in memory. Tensor layouts, the arrangement of multidimensional
data in memory, can significantly impact memory access efficiency, cache
performance, and computational throughput. Poorly chosen layouts can
lead to excessive memory stalls, inefficient cache usage, and increased
data movement costs.

In AI accelerators, tensor layout optimization is particularly important
because data is frequently accessed in patterns dictated by the
underlying hardware architecture. Choosing the right layout ensures that
memory accesses align with hardware-friendly access patterns, minimizing
overhead from costly memory transactions
(\citeproc{ref-nvidia2021cudnn}{N. Corporation 2021}).

While developers can sometimes manually specify tensor layouts, the
choice is often determined automatically by machine learning frameworks
(e.g., TensorFlow, PyTorch, JAX), compilers, or AI accelerator runtimes.
Low-level optimization tools such as cuDNN (for NVIDIA GPUs), XLA (for
TPUs), and MLIR (for custom accelerators) may rearrange tensor layouts
dynamically to optimize performance (\citeproc{ref-xla2020}{He 2023a}).
In high-level frameworks, layout transformations are typically applied
transparently, but developers working with custom kernels or low-level
libraries (e.g., CUDA, Metal, or OpenCL) may have direct control over
tensor format selection.

For example, in PyTorch, users can manually modify layouts using
tensor.permute() or tensor.contiguous() to ensure efficient memory
access (\citeproc{ref-paszke2019pytorch}{Paszke et al. 2019}). In
TensorFlow, layout optimizations are often applied internally by the XLA
compiler, choosing between NHWC (row-major) and NCHW (channel-major)
based on the target hardware (\citeproc{ref-tensorflow2022}{Brain
2022}). Hardware-aware machine learning libraries, such as cuDNN for
GPUs or OneDNN for CPUs, enforce specific memory layouts to maximize
cache locality and SIMD efficiency. Ultimately, while developers may
have some control over tensor layout selection, most layout decisions
are driven by the compiler and runtime system, ensuring that tensors are
stored in memory in a way that best suits the underlying hardware.

\paragraph{Row-Major
Layout}\label{sec-ai-acceleration-rowmajor-layout-741f}

Row-major layout refers to the way multi-dimensional tensors are stored
in memory, where elements are arranged row by row, ensuring that all
values in a given row are placed contiguously before moving to the next
row. This storage format is widely used in general-purpose CPUs and some
machine learning frameworks because it aligns naturally with sequential
memory access patterns, making it more cache-efficient for certain types
of operations (\citeproc{ref-oneDNN2021}{Intel 2021}).

To understand how row-major layout works, consider a single RGB image
represented as a tensor of shape (Height, Width, Channels). If the image
has a size of \(3\times 3\) pixels with 3 channels (RGB), the
corresponding tensor is structured as (3, 3, 3). The values are stored
in memory as follows: \begin{gather*}
I(0,0,0), I(0,0,1), I(0,0,2), I(0,1,0), I(0,1,1), \\
I(0,1,2), I(0,2,0), I(0,2,1), I(0,2,2), \ldots
\end{gather*}

Each row is stored contiguously, meaning all pixel values in the first
row are placed sequentially in memory before moving on to the second
row. This ordering is advantageous because CPUs and cache hierarchies
are optimized for sequential memory access. When data is accessed in a
row-wise fashion, such as when applying element-wise operations like
activation functions or basic arithmetic transformations, memory fetches
are efficient, and cache utilization is maximized
(\citeproc{ref-sodani2017knl}{Sodani 2015}).

The efficiency of row-major storage becomes particularly evident in
CPU-based machine learning workloads, where operations such as batch
normalization, matrix multiplications, and element-wise arithmetic
frequently process rows of data sequentially. Since modern CPUs employ
cache prefetching mechanisms, a row-major layout allows the next
required data values to be preloaded into cache ahead of execution,
reducing memory latency and improving overall computational throughput.

However, row-major layout can introduce inefficiencies when performing
operations that require accessing data across channels rather than
across rows. Consider a convolutional layer that applies a filter across
multiple channels of an input image. Since channel values are
interleaved in row-major storage, the convolution operation must jump
across memory locations to fetch all the necessary channel values for a
given pixel. These strided memory accesses can be costly on hardware
architectures that rely on vectorized execution and coalesced memory
access, such as GPUs and TPUs.

Despite these limitations, row-major layout remains a dominant storage
format in CPU-based machine learning frameworks. TensorFlow, for
instance, defaults to the NHWC (row-major) format on CPUs, ensuring that
cache locality is optimized for sequential processing. However, when
targeting GPUs, frameworks often rearrange data dynamically to take
advantage of more efficient memory layouts, such as channel-major
storage, which aligns better with parallelized computation.

\paragraph{Channel-Major
Layout}\label{sec-ai-acceleration-channelmajor-layout-d6a9}

In contrast to row-major layout, channel-major layout arranges data in
memory such that all values for a given channel are stored together
before moving to the next channel. This format is particularly
beneficial for GPUs, TPUs, and other AI accelerators, where vectorized
operations and memory coalescing significantly impact computational
efficiency.

To understand how channel-major layout works, consider the same RGB
image tensor of size (Height, Width, Channels) = (3, 3, 3). Instead of
storing pixel values row by row, the data is structured channel-first in
memory as follows: \begin{gather*}
I(0,0,0), I(1,0,0), I(2,0,0), I(0,1,0), I(1,1,0), I(2,1,0), \ldots, \\
I(0,0,1), I(1,0,1), I(2,0,1), \ldots, I(0,0,2), I(1,0,2), I(2,0,2), \ldots
\end{gather*}

In this format, all red channel values for the entire image are stored
first, followed by all green values, and then all blue values. This
ordering allows hardware accelerators to efficiently load and process
data across channels in parallel, which is important for convolution
operations and SIMD (Single Instruction, Multiple Data) execution models
(\citeproc{ref-chetlur2014cudnn}{Chetlur et al. 2014}).

The advantage of channel-major layout becomes clear when performing
convolutions in machine learning models. Convolutional layers process
images by applying a shared set of filters across all channels. When the
data is stored in a channel-major format, a convolution kernel can load
an entire channel efficiently, reducing the number of scattered memory
fetches. This reduces memory latency, improves throughput, and enhances
data locality for matrix multiplications, which are central to machine
learning workloads.

Because GPUs and TPUs rely on memory coalescing\sidenote{\textbf{Memory
Coalescing}: From Latin ``coalescere'' (to grow together), describing
how separate things merge into one. In GPU architecture, coalescing
combines multiple memory requests from threads in a warp into a single
efficient transaction when threads access consecutive addresses.
Uncoalesced access (scattered addresses) reduces bandwidth by 10-20x,
making tensor layouts and data organization critical for GPU
performance. }, a technique in which consecutive threads fetch
contiguous memory addresses, channel-major layout aligns naturally with
the way these processors execute parallel computations. For example, in
NVIDIA GPUs, each thread in a warp (a group of threads executed
simultaneously) processes different elements of the same channel,
ensuring that memory accesses are efficient and reducing the likelihood
of strided memory accesses, which can degrade performance.

Despite its advantages in machine learning accelerators, channel-major
layout can introduce inefficiencies when running on general-purpose
CPUs. Since CPUs optimize for sequential memory access, storing all
values for a single channel before moving to the next disrupts cache
locality for row-wise operations. This is why many machine learning
frameworks (e.g., TensorFlow, PyTorch) default to row-major (NHWC) on
CPUs and channel-major (NCHW) on GPUs, optimizing for the strengths of
each hardware type.

Modern AI frameworks and compilers often transform tensor layouts
dynamically depending on the execution environment. For instance,
TensorFlow and PyTorch automatically switch between
NHWC\sidenote{\textbf{NHWC vs NCHW}: Tensor layout formats where letters
indicate dimension order: N(batch), H(height), W(width), C(channels).
NHWC stores data row-by-row with channels interleaved (CPU-friendly),
while NCHW groups all values for each channel together (GPU-friendly). A
224×224 RGB image in NHWC stores as {[}R1,G1,B1,R2,G2,B2,\ldots{]} while
NCHW stores as {[}R1,R2,\ldots,G1,G2,\ldots,B1,B2,\ldots{]}. This
seemingly minor difference can impact performance by 2-5\(\times\)
depending on hardware. } and NCHW based on whether a model is running on
a CPU, GPU, or TPU, ensuring that the memory layout aligns with the most
efficient execution path.

\paragraph{Comparing Row-Major and Channel-Major
Layouts}\label{sec-ai-acceleration-comparing-rowmajor-channelmajor-layouts-e410}

Both row-major (NHWC) and channel-major (NCHW) layouts serve distinct
purposes in machine learning workloads, with their efficiency largely
determined by the hardware architecture, memory access patterns, and
computational requirements. The choice of layout directly influences
cache utilization, memory bandwidth efficiency, and processing
throughput. Table~\ref{tbl-major} contrasts the performance trade-offs
and hardware compatibility between these two approaches.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2041}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3878}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4014}}@{}}
\caption{\textbf{Data Layout Strategies.} Row-major (NHWC) and
channel-major (NCHW) layouts optimize memory access patterns for
different hardware architectures; NHWC suits CPUs and element-wise
operations, while NCHW accelerates GPU and TPU-based convolution
operations. Choosing the appropriate layout significantly impacts
performance by maximizing cache utilization and memory bandwidth
efficiency.}\label{tbl-major}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Feature}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Row-Major (NHWC)}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Channel-Major (NCHW)}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Feature}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Row-Major (NHWC)}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Channel-Major (NCHW)}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Memory Storage Order} & Pixels are stored row-by-row, channel
interleaved & All values for a given channel are stored together
first \\
\textbf{Best for} & CPUs, element-wise operations & GPUs, TPUs,
convolution operations \\
\textbf{Cache Efficiency} & High cache locality for sequential row
access & Optimized for memory coalescing across channels \\
\textbf{Convolution Performance} & Requires strided memory accesses
(inefficient on GPUs) & Efficient for GPU convolution kernels \\
\textbf{Memory Fetching} & Good for operations that process rows
sequentially & Optimized for SIMD execution across channels \\
\textbf{Default in Frameworks} & Default on CPUs (e.g., TensorFlow NHWC)
& Default on GPUs (e.g., cuDNN prefers NCHW) \\
\end{longtable}

The decision to use row-major (NHWC) or channel-major (NCHW) layouts is
not always made manually by developers. Instead, machine learning
frameworks and AI compilers often determine the optimal layout
dynamically based on the target hardware and operation type. CPUs tend
to favor NHWC due to cache-friendly sequential memory access, while GPUs
perform better with NCHW, which reduces memory fetch overhead for
machine learning computations.

In practice, modern AI compilers such as TensorFlow's XLA and PyTorch's
TorchScript perform automatic layout transformations, converting tensors
between NHWC and NCHW as needed to optimize performance across different
processing units. This ensures that machine learning models achieve the
highest possible throughput without requiring developers to manually
specify tensor layouts.

\subsubsection{Kernel
Fusion}\label{sec-ai-acceleration-kernel-fusion-7faf}

One of the most impactful optimization techniques in AI acceleration
involves reducing the overhead of intermediate data movement between
operations. Kernel fusion\sidenote{\textbf{Kernel}: From Old English
``cyrnel'' (seed, grain), the innermost essential part. In operating
systems, the kernel is the core that manages hardware resources. GPU
computing borrowed this term for the fundamental unit of parallel
execution launched on the device. In ML, a ``kernel'' is a compiled
function that executes on an accelerator, with ``kernel fusion''
combining multiple such functions into one to eliminate intermediate
memory traffic. } transforms multiple separate computations into unified
operations, dramatically improving memory efficiency and execution
performance. This subsection first analyzes the memory bottlenecks
created by intermediate writes, then explores how fusion techniques
eliminate these inefficiencies.

\paragraph{Intermediate Memory
Write}\label{sec-ai-acceleration-intermediate-memory-write-f140}

AI model performance is often constrained by memory bandwidth and
intermediate memory writes rather than pure arithmetic operations. Every
time an operation produces an intermediate result that must be written
to memory and later read back, execution stalls from the data movement
overhead.

Building on software optimization techniques from
\textbf{?@sec-model-compression} and memory bandwidth constraints
established in
Section~\ref{sec-ai-acceleration-understanding-ai-memory-wall-3ea9},
kernel fusion represents the critical bridge between software
optimization and hardware acceleration. Many AI workloads introduce
unnecessary intermediate memory writes, leading to increased memory
bandwidth consumption and reduced execution efficiency
(\citeproc{ref-nvidia2017gpu}{N. Corporation 2017}).

Listing~\ref{lst-naive_execution} reveals how each operation becomes a
separate kernel in a naïve execution model, forcing intermediate results
to be written to memory and then read back for the next operation.

\begin{codelisting}

\caption{\label{lst-naive_execution}\textbf{Naïve Execution}: Each step
writes intermediate results to memory before processing the next,
leading to increased bandwidth usage and reduced efficiency.}

\centering{

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ torch}

\CommentTok{\#\# Input tensor}
\NormalTok{X }\OperatorTok{=}\NormalTok{ torch.randn(}\DecValTok{1024}\NormalTok{, }\DecValTok{1024}\NormalTok{).cuda()}

\CommentTok{\#\# Step{-}by{-}step execution (naïve approach)}
\NormalTok{X1 }\OperatorTok{=}\NormalTok{ torch.relu(X)  }\CommentTok{\# Intermediate tensor stored}
\CommentTok{\# in memory}
\NormalTok{X2 }\OperatorTok{=}\NormalTok{ torch.batch\_norm(X1)  }\CommentTok{\# Another intermediate tensor stored}
\NormalTok{Y }\OperatorTok{=} \FloatTok{2.0} \OperatorTok{*}\NormalTok{ X2 }\OperatorTok{+} \FloatTok{1.0}  \CommentTok{\# Final result}
\end{Highlighting}
\end{Shaded}

}

\end{codelisting}%

Each operation produces an intermediate tensor that must be written to
memory and retrieved for the next operation. On large tensors, this
overhead of moving data can outweigh the computational cost of the
operations (\citeproc{ref-shazeer2018mesh}{Shazeer et al. 2018}).
\textbf{?@tbl-memory-footprint} illustrates the memory overhead in a
naïve execution model. While only the final result \(Y\) is needed,
storing multiple intermediate tensors creates unnecessary memory traffic
and inefficient memory usage.

\textbf{Tensor} \textbar{} \textbf{Size (MiB) for 1024 \(\times\) 1024
Tensor} \textbar{}\\
\textbf{X} \textbar{} \texttt{\{python\}\ tensor\_mb\_str} MiB
\textbar{}\\
\textbf{X'} \textbar{} \texttt{\{python\}\ tensor\_mb\_str} MiB
\textbar{}\\
\textbf{X'\,'} \textbar{} \texttt{\{python\}\ tensor\_mb\_str} MiB
\textbar{}\\
\textbf{Y} \textbar{} \texttt{\{python\}\ tensor\_mb\_str} MiB
\textbar{}\\
\textbf{Total Memory} \textbar{}
\textbf{\texttt{\{python\}\ total\_mb\_str} MiB} \textbar{}

: \textbf{Intermediate Tensor Storage.} Naive execution models require
substantial memory to store intermediate tensors generated by each
operation. For a 1024x1024 tensor, storing intermediate results (even
when only the final output is needed) quadruples the total memory
footprint from \texttt{\{python\}\ tensor\_mb\_str} MiB to
\texttt{\{python\}\ total\_mb\_str} MiB. Minimizing intermediate data
storage is essential for improving memory efficiency.
\{\#tbl-memory-footprint\}

The three intermediate tensors waste both memory capacity and bandwidth,
limiting scalability on AI accelerators where data movement dominates
execution cost.

\paragraph{Kernel Fusion for Memory
Efficiency}\label{sec-ai-acceleration-kernel-fusion-memory-efficiency-f227}

Kernel fusion minimizes intermediate memory writes, reducing the memory
footprint and bandwidth consumption of machine learning workloads
(\citeproc{ref-jia2018beyond}{Zhihao Jia, Zaharia, and Aiken 2018}).

Kernel fusion merges multiple computation steps into a single, optimized
operation, eliminating the need for storing and reloading intermediate
tensors. Instead of executing each layer or element-wise operation
separately, in which each step writes its output to memory before the
next step begins, fusion enables direct data propagation between
operations, keeping computations within high-speed registers or local
memory.

A common machine learning sequence might involve applying a nonlinear
activation function (e.g., ReLU), followed by batch normalization, and
then scaling the values for input to the next layer. In a naïve
implementation, each of these steps generates an intermediate tensor,
which is written to memory, read back, and then modified again: \[
X' = \text{ReLU}(X)
X'' = \text{BatchNorm}(X')
Y = \alpha \cdot X'' + \beta
\]

With kernel fusion, these operations are combined into a single
computation step, allowing the entire transformation to occur without
generating unnecessary intermediate tensors: \[
Y = \alpha \cdot \text{BatchNorm}\big(\text{ReLU}(X)\big) + \beta
\]

Table~\ref{tbl-fusion-benefits} highlights the impact of operation
fusion on memory efficiency. By keeping intermediate results in
registers or local memory rather than writing them to main memory,
fusion significantly reduces memory traffic. This optimization is
especially beneficial on highly parallel architectures like GPUs and
TPUs, where minimizing memory accesses translates directly into improved
execution throughput. Compared to the naïve execution model, fused
execution eliminates the need for storing intermediate tensors,
dramatically lowering the total memory footprint and improving overall
efficiency.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3864}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3523}}@{}}
\caption{\textbf{Operation Fusion Benefits.} Fused execution reduces
memory usage by eliminating the need to store intermediate tensors,
directly improving efficiency on memory-bound hardware like GPUs and
TPUs. Memory consumption drops from \texttt{\{python\}\ naive\_mb\_str}
in naive execution to \texttt{\{python\}\ fused\_mb\_str} with fused
operations.}\label{tbl-fusion-benefits}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Execution Model}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Intermediate Tensors Stored}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Total Memory Usage (MiB)}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Execution Model}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Intermediate Tensors Stored}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Total Memory Usage (MiB)}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Naïve Execution} & X', X'\,' &
\texttt{\{python\}\ naive\_mb\_str} \\
\textbf{Fused Execution} & None & \texttt{\{python\}\ fused\_mb\_str} \\
\end{longtable}

\paragraph{Performance Benefits and
Constraints}\label{sec-ai-acceleration-performance-benefits-constraints-1b74}

Kernel fusion brings several key advantages that enhance memory
efficiency and computation throughput. By reducing memory accesses,
fused kernels ensure that intermediate values stay within registers
instead of being repeatedly written to and read from memory. This
significantly lowers memory traffic, which is one of the primary
bottlenecks in machine learning workloads. GPUs and TPUs, in particular,
benefit from kernel fusion because high-bandwidth memory is a scarce
resource, and reducing memory transactions leads to better utilization
of compute units (\citeproc{ref-nvidia2020ampere}{N. Corporation 2020}).

However, not all operations can be fused. Element-wise operations, such
as ReLU, batch normalization, and simple arithmetic transformations, are
ideal candidates for fusion since their computations depend only on
single elements from the input tensor. In contrast, operations with
complex data dependencies, such as matrix multiplications and
convolutions, involve global data movement, making direct fusion
impractical. These operations require values from multiple input
elements to compute a single output, which prevents them from being
executed as a single fused kernel.

Another major consideration is register pressure. Fusing multiple
operations means all temporary values must be kept in registers rather
than memory. While this eliminates redundant memory writes, it also
increases register demand. If a fused kernel exceeds the available
registers per thread, the system must spill excess values into shared
memory, introducing additional latency and potentially negating the
benefits of fusion. On GPUs, where thread occupancy (the number of
threads that can run in parallel) is limited by available registers,
excessive fusion can reduce parallelism, leading to diminishing returns.

Different AI accelerators and compilers handle fusion in distinct ways.
NVIDIA GPUs, for example, favor warp-level parallelism, where
element-wise fusion is straightforward. TPUs, on the other hand,
prioritize systolic array execution, which is optimized for
matrix-matrix operations rather than element-wise fusion
(\citeproc{ref-nvidia2020ampere}{N. Corporation 2020}). AI compilers
such as XLA (TensorFlow), TorchScript (PyTorch), TensorRT (NVIDIA), and
MLIR automatically detect fusion opportunities and apply heuristics to
balance memory savings and execution efficiency
(\citeproc{ref-xla2021}{He 2023b}).

Despite its advantages, fusion is not always beneficial. Some AI
frameworks allow developers to disable fusion selectively, especially
when debugging performance issues or making frequent model
modifications. The decision to fuse operations must consider trade-offs
between memory efficiency, register usage, and hardware execution
constraints to ensure that fusion leads to tangible performance
improvements.

These fusion decisions are ultimately about data locality. Use the
following checkpoint to consolidate your understanding of data movement
strategies.

\phantomsection\label{callout-checkpointux2a-1.25}
\begin{fbx}{callout-checkpoint}{Checkpoint:}{Data Movement and Kernel Fusion}
\phantomsection\label{callout-checkpoint*-1.25}
At this point, you should be able to answer the first two questions from
the roadmap:

\textbf{Which data stays local?} The weight-stationary,
output-stationary, and input-stationary patterns each make a principled
choice about which data to cache near compute units. Weight-stationary
(used in Google's TPU) maximizes weight reuse for CNN workloads.
Output-stationary (used in NVIDIA's tensor cores) reduces partial sum
memory traffic for fully connected layers. Input-stationary minimizes
input reloads for models with shared inputs across multiple filters.

\textbf{How are operations combined?} Kernel fusion eliminates
intermediate memory writes by merging consecutive operations (Conv2D +
BatchNorm + ReLU becomes a single kernel). This optimization is most
effective for element-wise operations that share data dependencies and
can achieve 2-10x speedups by avoiding round-trips to DRAM.

The remaining question, \emph{how is data organized}, brings us to
tiling: the technique of partitioning computations into memory-friendly
blocks. Tiling complements the stationary strategies by ensuring that
whichever data we choose to keep local actually fits in fast memory.

\end{fbx}

\subsubsection{Memory-Efficient Tiling
Strategies}\label{sec-ai-acceleration-memoryefficient-tiling-strategies-9fce}

While modern AI accelerators offer high computational throughput, their
performance is often limited by memory bandwidth rather than raw
processing power. If data cannot be supplied to processing units fast
enough, execution stalls occur, leading to wasted cycles and inefficient
hardware utilization.

Tiling\sidenote{\textbf{Tiling}: Borrowed from floor or mosaic tiling,
where a large surface is covered by repeating smaller pieces. In
computing, Monica Lam popularized the term in her 1991 PhD thesis on
cache optimization. Just as physical tiles tessellate to cover a floor,
computational tiles partition large matrices into blocks that fit in
fast memory. The technique is also called ``blocking'' or ``loop
blocking'' in compiler literature. By doing so, tiling increases data
reuse, minimizes memory fetches, and improves overall computational
efficiency. } is a technique used to mitigate this issue by
restructuring computations into smaller, memory-friendly subproblems.
Instead of processing entire matrices or tensors at once, which leads to
excessive memory traffic, tiling partitions computations into smaller
blocks (tiles) that fit within fast local memory (e.g., caches, shared
memory, or registers) (\citeproc{ref-lam1991cache}{Lam, Rothberg, and
Wolf 1991}).

Matrix multiplication, widely used in AI models, demonstrates
inefficient memory access when implemented naively.
Listing~\ref{lst-naive_matmul} shows how, without tiling, repeated
memory accesses for the same data lead to unnecessary bandwidth
consumption.

\begin{codelisting}

\caption{\label{lst-naive_matmul}\textbf{Naïve Matrix Multiplication}:
Direct implementation without tiling requires O(N\^{}3) memory accesses
for N×N matrices, repeatedly fetching the same elements from slow DRAM
memory and limiting performance to a fraction of theoretical peak
throughput.}

\centering{

\begin{Shaded}
\begin{Highlighting}[]
\ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(N):}
    \ControlFlowTok{for}\NormalTok{ j }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(N):}
        \ControlFlowTok{for}\NormalTok{ k }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(N):}
\NormalTok{            C[i, j] }\OperatorTok{+=}\NormalTok{ A[i, k] }\OperatorTok{*}\NormalTok{ B[k, j]  }\CommentTok{\# Repeatedly fetching}
            \CommentTok{\# A[i, k] and B[k, j]}
\end{Highlighting}
\end{Shaded}

}

\end{codelisting}%

Each iteration requires loading elements from matrices \(A\) and \(B\)
multiple times from memory, causing excessive data movement. As the size
of the matrices increases, the memory bottleneck worsens, limiting
performance.

Tiling addresses this problem by ensuring that smaller portions of
matrices are loaded into fast memory, reused efficiently, and only
written back to main memory when necessary. This technique is especially
important in AI accelerators, where memory accesses dominate execution
time. Figure~\ref{fig-tiling-diagram} visualizes how breaking large
matrices into smaller tiles enables computation to proceed efficiently
by maximizing data reuse in fast memory. The following sections examine
the principles of tiling, its different strategies, and the key
trade-offs involved in selecting an effective tiling approach.

\begin{figure}[htb]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/2f0dbb5a7672d050fbe0171e5458626e0394bc42.pdf}}

}

\caption{\label{fig-tiling-diagram}\textbf{Matrix Tiling}: Partitioning
large matrices into smaller tiles optimizes data reuse and reduces
memory access overhead during computation. This technique improves
performance on AI accelerators by enabling efficient loading and
processing of data in fast memory, minimizing transfers from slower main
memory.}

\end{figure}%

\paragraph{Tiling
Fundamentals}\label{sec-ai-acceleration-tiling-fundamentals-e9e6}

Tiling is based on a simple but powerful principle: instead of operating
on an entire data structure at once, computations are divided into
smaller tiles that fit within the available fast memory. By structuring
execution around these tiles, data reuse is maximized, reducing
redundant memory accesses and improving overall efficiency.

Consider matrix multiplication, a key operation in machine learning
workloads. The operation computes \(C = A \times B\) where each element
\(C[i,j] = \sum_{k} A[i,k] \times B[k,j]\). The naive implementation
shown earlier in Listing~\ref{lst-naive_matmul} demonstrates the core
problem: every iteration of the innermost loop fetches elements from
matrices \(A\) and \(B\) from memory, performs a multiplication, and
updates matrix \(C\). Because matrices are large, the processor
repeatedly reloads the same values from memory, even though they were
just used in previous computations.

This data movement overhead is expensive: fetching from DRAM is
100-1000x slower than accessing on-chip cache or registers. The solution
is tiling.

\paragraph{Performance Benefits of
Tiling}\label{sec-ai-acceleration-performance-benefits-tiling-e7bd}

Instead of computing one element at a time and constantly moving data in
and out of slow memory, tiling processes submatrices (tiles) at a time,
keeping frequently used values in fast memory. The idea is to divide the
matrices into smaller blocks that fit within the processor's cache or
shared memory, ensuring that once a block is loaded, it is reused
multiple times before moving to the next one.

Listing~\ref{lst-tiled_matmul} demonstrates how processing blocks of
data improves memory locality by ensuring frequently used values remain
in fast memory.

\begin{codelisting}

\caption{\label{lst-tiled_matmul}\textbf{Tiled Matrix Multiplication}:
This approach divides matrices into smaller blocks to optimize memory
usage by reusing data within processor cache, thereby improving
computational efficiency.}

\centering{

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{TILE\_SIZE }\OperatorTok{=} \DecValTok{32}  \CommentTok{\# Choose a tile size based on}
\CommentTok{\# hardware constraints}

\CommentTok{\# Spatial tiling: partition data via loop bounds.}
\CommentTok{\# No explicit loads — tiles defined by index ranges.}
\ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\DecValTok{0}\NormalTok{, N, TILE\_SIZE):}
    \ControlFlowTok{for}\NormalTok{ j }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\DecValTok{0}\NormalTok{, N, TILE\_SIZE):}
        \ControlFlowTok{for}\NormalTok{ k }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\DecValTok{0}\NormalTok{, N, TILE\_SIZE):}
            \CommentTok{\# Each tile computed independently}
            \ControlFlowTok{for}\NormalTok{ ii }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(i, i }\OperatorTok{+}\NormalTok{ TILE\_SIZE):}
                \ControlFlowTok{for}\NormalTok{ jj }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(j, j }\OperatorTok{+}\NormalTok{ TILE\_SIZE):}
                    \ControlFlowTok{for}\NormalTok{ kk }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(k, k }\OperatorTok{+}\NormalTok{ TILE\_SIZE):}
\NormalTok{                        C[ii, jj] }\OperatorTok{+=}\NormalTok{ A[ii, kk] }\OperatorTok{*}\NormalTok{ B[kk, jj]}
\end{Highlighting}
\end{Shaded}

}

\end{codelisting}%

This restructuring significantly improves performance for three main
reasons:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Better Memory Reuse}: Instead of fetching elements from \(A\)
  and \(B\) repeatedly from slow memory, this approach loads a small
  tile of data into fast memory, performs multiple computations using
  it, and only then moves on to the next tile. This minimizes redundant
  memory accesses.
\item
  \textbf{Reduced Memory Bandwidth Usage}: Since each tile is used
  multiple times before being evicted, memory traffic is reduced.
  Instead of repeatedly accessing DRAM, most required data is available
  in L1/L2 cache or shared memory, leading to faster execution.
\item
  \textbf{Increased Compute Efficiency}: Processors spend less time
  waiting for data and more time performing useful computations. In
  architectures like GPUs and TPUs, where thousands of parallel
  processing units operate simultaneously, tiling ensures that data is
  read and processed in a structured manner, avoiding unnecessary
  stalls.
\end{enumerate}

This technique is particularly effective in AI accelerators, where
machine learning workloads consist of large matrix multiplications and
tensor transformations. Without tiling, these workloads quickly become
memory-bound, meaning performance is constrained by how fast data can be
retrieved rather than by the raw computational power of the processor.

\paragraph{Tiling
Methods}\label{sec-ai-acceleration-tiling-methods-6257}

While the general principle of tiling remains the same, which involves
partitioning large computations into smaller subproblems to improve
memory reuse, there are different ways to apply tiling based on the
structure of the computation and hardware constraints. The two primary
tiling strategies are spatial tiling and temporal tiling. These
strategies optimize different aspects of computation and memory access,
and in practice, they are often combined to achieve the best
performance.

\textbf{Spatial Tiling.} Spatial tiling focuses on partitioning data
structures into smaller blocks that fit within fast memory. The tiled
matrix multiplication in Listing~\ref{lst-tiled_matmul} demonstrates
this approach: each tile of \(A\) and \(B\) is loaded into cache or
shared memory before processing, ensuring that the same data does not
need to be fetched repeatedly from slower memory. The tile is fully used
before moving to the next block, minimizing redundant memory accesses.

Spatial tiling is particularly beneficial for large tensors that exceed
fast memory capacity. By breaking computations into smaller tiles, data
movement between memory levels is minimized, keeping operations
localized within cache hierarchies.

\textbf{Temporal Tiling.} While spatial tiling optimizes how data is
partitioned, temporal tiling focuses on reorganizing the computation
itself to improve data reuse over time. Many machine learning workloads
involve operations where the same data is accessed repeatedly across
multiple iterations. Without temporal tiling, this often results in
redundant memory fetches, leading to inefficiencies. Temporal tiling,
also known as loop blocking, restructures the computation to ensure that
frequently used data stays in fast memory for as long as possible before
moving on to the next computation.

A classic example where temporal tiling is beneficial is convolutional
operations, where the same set of weights is applied to multiple input
regions. Without loop blocking, these weights might be loaded from
memory multiple times for each computation. With temporal tiling, the
computation is reordered so that the weights remain in fast memory
across multiple inputs, reducing unnecessary memory fetches and
improving overall efficiency.

Listing~\ref{lst-loop_blocking} illustrates how loop blocking
restructures computation to keep weights in fast memory across multiple
inputs, reducing redundant fetches.

\begin{codelisting}

\caption{\label{lst-loop_blocking}\textbf{Temporal Tiling}: Reduces
redundant memory accesses by caching weights in fast memory across
multiple matrix multiplications.}

\centering{

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Temporal tiling: reorder computation so data}
\CommentTok{\# stays in fast memory across iterations.}
\ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\DecValTok{0}\NormalTok{, N, TILE\_SIZE):}
    \ControlFlowTok{for}\NormalTok{ j }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\DecValTok{0}\NormalTok{, N, TILE\_SIZE):}
        \ControlFlowTok{for}\NormalTok{ k }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\DecValTok{0}\NormalTok{, N, TILE\_SIZE):}
            \CommentTok{\# Explicitly load tiles into fast memory}
\NormalTok{            A\_tile }\OperatorTok{=}\NormalTok{ A[i:i}\OperatorTok{+}\NormalTok{TILE\_SIZE, k:k}\OperatorTok{+}\NormalTok{TILE\_SIZE]}
\NormalTok{            B\_tile }\OperatorTok{=}\NormalTok{ B[k:k}\OperatorTok{+}\NormalTok{TILE\_SIZE, j:j}\OperatorTok{+}\NormalTok{TILE\_SIZE]}

            \CommentTok{\# Reuse loaded tiles for all inner iterations}
            \ControlFlowTok{for}\NormalTok{ ii }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(TILE\_SIZE):}
                \ControlFlowTok{for}\NormalTok{ jj }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(TILE\_SIZE):}
                    \ControlFlowTok{for}\NormalTok{ kk }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(TILE\_SIZE):}
\NormalTok{                        C[i}\OperatorTok{+}\NormalTok{ii, j}\OperatorTok{+}\NormalTok{jj] }\OperatorTok{+=}\NormalTok{ A\_tile[ii, kk] }\OperatorTok{*}
\NormalTok{                                         B\_tile[kk, jj]}
\end{Highlighting}
\end{Shaded}

}

\end{codelisting}%

Temporal tiling improves performance by ensuring that the data loaded
into fast memory is used multiple times before being evicted. In this
implementation, small tiles of matrices \(A\) and \(B\) are explicitly
loaded into temporary storage before performing computations, reducing
memory fetch overhead. This restructuring allows the computation to
process an entire tile before moving to the next, thereby reducing the
number of times data must be loaded from slower memory.

This technique is particularly useful in workloads where certain values
are used repeatedly, such as convolutions, recurrent neural networks
(RNNs), and self-attention mechanisms in transformers. By applying loop
blocking, AI accelerators can significantly reduce memory stalls and
improve execution throughput.

\paragraph{Tiling Challenges and
Trade-offs}\label{sec-ai-acceleration-tiling-challenges-tradeoffs-e9c9}

While tiling significantly improves performance by optimizing memory
reuse and reducing redundant memory accesses, it introduces several
challenges and trade-offs. Selecting the right tile size is important,
as it directly affects computational efficiency and memory bandwidth
usage. If the tile size is too small, the benefits of tiling diminish,
as memory fetches still dominate execution time. On the other hand, if
the tile size is too large, it may exceed the available fast memory,
causing cache thrashing and performance degradation.

Load balancing is another key concern. In architectures such as GPUs and
TPUs, computations are executed in parallel across thousands of
processing units. If tiles are not evenly distributed, some units may
remain idle while others are overloaded, leading to suboptimal
utilization of computational resources. Effective tile scheduling
ensures that parallel execution remains balanced and efficient.

Data movement overhead is also an important consideration. Although
tiling reduces the number of slow memory accesses, transferring tiles
between different levels of memory still incurs a cost. This is
especially relevant in hierarchical memory systems, where accessing data
from cache is much faster than accessing it from DRAM. Efficient memory
prefetching and scheduling strategies are required to minimize latency
and ensure that data is available when needed.

Beyond spatial and temporal tiling, hybrid approaches combine elements
of both strategies to achieve optimal performance. Hybrid tiling adapts
to workload-specific constraints by dynamically adjusting tile sizes or
reordering computations based on real-time execution conditions. For
example, some AI accelerators use spatial tiling for matrix
multiplications while employing temporal tiling for weight reuse in
convolutional layers.

Other methods exist for optimizing memory usage and computational
efficiency beyond tiling. Techniques such as register blocking, double
buffering, and hierarchical tiling extend the basic tiling principles to
further optimize execution. AI compilers and runtime systems, such as
TensorFlow XLA, TVM, and MLIR, automatically select tiling strategies
based on hardware constraints, enabling fine-tuned performance
optimization without manual intervention.

Table~\ref{tbl-tiling-strategies} provides a comparative overview of
spatial, temporal, and hybrid tiling approaches, highlighting their
respective benefits and trade-offs.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.0969}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3605}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2829}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2519}}@{}}
\caption{\textbf{Tiling Strategies.} Spatial, temporal, and hybrid
tiling optimize memory access patterns for improved performance. Spatial
tiling maximizes data reuse within fast memory, temporal tiling exploits
loop structure for reduced accesses, and hybrid tiling combines both
approaches. AI compilers and runtime systems use these techniques to
automatically optimize model execution on diverse
hardware.}\label{tbl-tiling-strategies}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Aspect}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Spatial Tiling (Data Tiling)}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Temporal Tiling (Loop Blocking)}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Hybrid Tiling}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Aspect}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Spatial Tiling (Data Tiling)}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Temporal Tiling (Loop Blocking)}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Hybrid Tiling}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Primary Goal} & Reduce memory accesses by keeping data in fast
memory longer & Increase data reuse across loop iterations & Adapt
dynamically to workload constraints \\
\textbf{Optimization Focus} & Partitioning data structures into smaller,
memory-friendly blocks & Reordering computations to maximize reuse
before eviction & Balancing spatial and temporal reuse strategies \\
\textbf{Memory Usage} & Improves cache locality and reduces DRAM access
& Keeps frequently used data in fast memory for multiple iterations &
Minimizes data movement while ensuring high reuse \\
\textbf{Common Use Cases} & Matrix multiplications, CNNs, self-attention
in transformers & Convolutions, recurrent neural networks (RNNs),
iterative computations & AI accelerators with hierarchical memory, mixed
workloads \\
\textbf{Performance Gains} & Reduced memory bandwidth requirements,
better cache utilization & Lower memory fetch latency, improved data
locality & Maximized efficiency across multiple hardware types \\
\textbf{Challenges} & Requires careful tile size selection, inefficient
for workloads with minimal spatial reuse & Can increase register
pressure, requires loop restructuring & Complexity in tuning tile size
and execution order dynamically \\
\textbf{Best When} & Data is large and needs to be partitioned for
efficient processing & The same data is accessed multiple times across
iterations & Both data partitioning and iteration-based reuse are
important \\
\end{longtable}

As machine learning models continue to grow in size and complexity,
tiling remains a critical tool for improving hardware efficiency,
ensuring that AI accelerators operate at their full potential. While
manual tiling strategies can provide substantial benefits, modern
compilers and hardware-aware optimization techniques further enhance
performance by automatically selecting the most effective tiling
strategies for a given workload.

\subsection{Applying Mapping Strategies to Neural
Networks}\label{sec-ai-acceleration-applying-mapping-strategies-neural-networks-3110}

While these foundational mapping techniques apply broadly, their
effectiveness varies based on the computational structure, data access
patterns, and parallelization opportunities of different neural network
architectures. Each architecture imposes distinct constraints on data
movement, memory hierarchy, and computation scheduling, requiring
tailored mapping strategies to optimize performance.

A structured approach to mapping is required to address the
combinatorial explosion of choices that arise when assigning
computations to AI accelerators. Rather than treating each model as a
separate optimization problem, we recognize that the same principles
apply across different architectures; only their priority shifts based
on workload characteristics. The goal is to systematically select and
apply mapping strategies that maximize efficiency for different types of
machine learning models.

These principles apply to three representative AI workloads, each
characterized by distinct computational demands. CNNs benefit from
spatial data reuse, making weight-stationary execution and the
application of tiling techniques especially effective. In contrast,
Transformers are inherently memory-bound and rely on strategies such as
efficient KV-cache management, fused attention mechanisms, and highly
parallel execution to mitigate memory traffic. MLPs, which involve
substantial matrix multiplication operations, demand the use of
structured tiling, optimized weight layouts, and memory-aware execution
to enhance overall performance.

Despite their differences, each of these models follows a common set of
mapping principles, with variations in how optimizations are
prioritized. Table~\ref{tbl-mapping-strategies} summarizes the
suitability of different optimization strategies for CNNs, Transformers,
and MLPs.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.1273}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.0982}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.0873}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.0727}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.6036}}@{}}
\caption{\textbf{Architecture-Specific Mapping Strategies.} Each neural
network architecture benefits from different optimization priorities
based on its computational and memory
characteristics.}\label{tbl-mapping-strategies}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Optimization Technique}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{CNNs}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Transformers}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{MLPs}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Rationale}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Optimization Technique}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{CNNs}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Transformers}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{MLPs}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Rationale}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Dataflow Strategy} & Weight Stationary & Activation Stationary &
Weight Stationary & CNNs reuse filters across spatial locations;
Transformers reuse activations (KV-cache); MLPs reuse weights across
batches. \\
\textbf{Memory-Aware Tensor Layouts} & NCHW (Channel-Major) & NHWC
(Row-Major) & NHWC & CNNs favor channel-major for convolution
efficiency; Transformers and MLPs prioritize row-major for fast memory
access. \\
\textbf{Kernel Fusion} & Convolution + Activation & Fused Attention &
GEMM Fusion & CNNs optimize convolution+activation fusion; Transformers
fuse attention mechanisms; MLPs benefit from fused matrix
multiplications. \\
\textbf{Tiling for Memory Efficiency} & Spatial Tiling & Temporal Tiling
& Blocked Tiling & CNNs tile along spatial dimensions; Transformers use
loop blocking to improve sequence memory efficiency; MLPs use blocked
tiling for large matrix multiplications. \\
\end{longtable}

With the mapping landscape summarized in
Table~\ref{tbl-mapping-strategies}, we now examine \emph{why} each
architecture maps the way it does. The table captures the specific
strategy choices; the following subsections explain the architectural
insight behind each one.

\textbf{Convolutional Neural Networks.} The defining characteristic of
CNNs, from a hardware mapping perspective, is spatial weight reuse. A
single small filter is applied to every spatial location in the input
feature map, meaning the same weights participate in hundreds or
thousands of multiply-accumulate operations before the next filter is
needed. This reuse pattern makes weight stationary execution the natural
choice: pinning filter weights in fast on-chip memory and streaming
activations through the compute units avoids repeatedly fetching the
same weights from slower external memory. The result is high arithmetic
intensity with modest bandwidth demand, which is precisely the profile
that tensor cores and systolic arrays are designed to exploit.

This spatial regularity also enables aggressive fusion and tiling.
Because convolution, batch normalization, and activation are applied at
every spatial position in lockstep, compilers can fuse the entire
sequence into a single kernel that never writes intermediate results to
main memory. Spatial tiling then partitions the feature map into
subregions sized to fit within on-chip SRAM, so the fused kernel
processes each tile entirely from fast memory before moving to the next.
The combination of weight stationarity, kernel fusion, and spatial
tiling is what makes CNNs among the most hardware-friendly
architectures, routinely achieving 70 to 80 percent of peak accelerator
throughput.

\textbf{Transformer Architectures.} Where CNNs are defined by weight
reuse, Transformers are defined by the memory pressure of the key-value
(KV) cache. During attention computation, every query vector must access
stored key and value pairs across the entire sequence length. As
sequences grow, the KV cache can consume gigabytes of high-bandwidth
memory, making memory bandwidth rather than raw compute the primary
bottleneck. This access pattern motivates activation stationary
execution: keeping the KV cache resident in fast memory while streaming
queries through minimizes the costly round trips to external DRAM that
would otherwise dominate execution time.

The memory-bound nature of attention also explains why fused attention
kernels, such as FlashAttention
(\citeproc{ref-dao2022flashattention}{Dao et al. 2022}), deliver
outsized performance gains for Transformers. By fusing the query-key dot
product, softmax normalization, and value-weighted summation into a
single kernel that tiles along the sequence dimension, these
implementations avoid materializing the full attention matrix in main
memory. This temporal tiling approach processes sequence blocks that fit
within on-chip SRAM, reducing memory traffic from quadratic in sequence
length to near-linear. For Transformers, the mapping strategy is
fundamentally an exercise in memory management rather than compute
scheduling.

\textbf{Multi-Layer Perceptrons.} MLPs present the most straightforward
mapping problem because their computation reduces almost entirely to
dense General Matrix Multiplication (GEMM)\sidenote{\textbf{GEMM
(General Matrix Multiplication)}: The operation C = αAB + βC that
underlies most neural network computations. The name reflects its role
as the ``general'' case encompassing vector-matrix and matrix-vector
products as special cases. GEMM accounts for 90-95\% of computation time
in training deep networks and is the primary target of AI hardware
optimization. Optimized libraries like cuBLAS (NVIDIA) and oneDNN
(Intel) achieve 80-95\% of theoretical peak through register blocking,
vectorization, and hierarchical tiling. Modern AI accelerators are
essentially specialized GEMM engines. }. Each fully connected layer
multiplies an activation matrix by a weight matrix, and GEMM accounts
for 90 to 95 percent of MLP computation time. The weight matrix is fixed
across all samples in a batch, so weight stationary execution allows the
accelerator to load weights once and reuse them across every batch
element, with reuse scaling linearly with batch size. This makes MLPs
highly sensitive to batching: a batch size of one leaves the weight
matrix underutilized, while large batches push arithmetic intensity into
the compute-bound regime where accelerators operate most efficiently.

Because MLP layers are typically followed by activation functions and
bias additions, GEMM fusion combines these steps into a single kernel,
avoiding intermediate memory writes. Blocked tiling partitions the large
matrix multiplications into sub-blocks sized for the accelerator's
shared memory, ensuring high cache utilization throughout computation.
The simplicity of the MLP mapping, dominated by a single primitive with
predictable access patterns, is precisely why hardware vendors optimize
GEMM libraries so aggressively: gains in GEMM performance translate
directly to MLP throughput.

\subsection{Hybrid Mapping
Strategies}\label{sec-ai-acceleration-hybrid-mapping-strategies-3e8c}

The preceding subsections treat each architecture in isolation, but real
models rarely consist of a single layer type. A vision transformer, for
example, begins with a convolution-like patch embedding that benefits
from weight stationary mapping, proceeds through self-attention layers
that require activation stationary execution for efficient KV-cache
reuse, and concludes with MLP blocks whose dense GEMM operations demand
blocked tiling and fusion (\citeproc{ref-Dosovitskiy2020ViT}{Dosovitskiy
et al. 2020}). No single dataflow strategy is optimal across all these
layers.

\textbf{Layer-Specific Mapping.} Hybrid mapping addresses this
heterogeneity by allowing the accelerator to switch strategies at layer
boundaries. Each layer presents a different balance of compute
intensity, data reuse, and memory access pattern, and the optimal
mapping must shift accordingly (\citeproc{ref-sze2020efficient}{Sze et
al. 2017b}). Rather than committing to one dataflow for the entire
model, hybrid approaches select weight stationary execution for layers
with high weight reuse, activation stationary execution for attention
layers with large KV caches, and output stationary execution for layers
where minimizing write traffic matters most.

\textbf{Hardware Implementations of Hybrid Strategies.} Modern
accelerators provide the architectural features needed to realize hybrid
mapping in practice. Google TPUs switch between weight stationary and
activation stationary modes depending on whether a layer is
convolutional or attention-based. NVIDIA GPUs use fused kernels
alongside flexible memory layouts that enable different strategies
within the same model. Graphcore IPUs select execution strategies
dynamically on a per-layer basis to optimize memory access. These
implementations require programmable memory hierarchies, efficient
interconnects, and specialized execution pipelines, reinforcing the
hardware-software co-design principle.

However, hybrid mapping remains a design-time optimization. In
production workloads, execution conditions change dynamically due to
varying input sizes, memory contention, and hardware resource
availability. Machine learning compilers and runtime systems extend
these static mapping choices by introducing dynamic scheduling, memory
optimizations, and automatic tuning, ensuring that deep learning
workloads operate efficiently across diverse accelerators and deployment
environments.

The mapping strategies and dataflow optimizations examined in preceding
sections represent the ``what'' of efficient execution: which data to
keep local, how to tile computations, and which parallelization
strategies to employ. Determining optimal configurations for specific
hardware and workloads, however, requires systematic automation. This is
where machine learning compilers become indispensable: they transform
abstract mapping principles into concrete execution plans tailored to
target accelerators, bridging the gap between high-level model
definitions and low-level hardware instructions.

\section{Compiler
Support}\label{sec-ai-acceleration-compiler-support-172e}

Machine learning compilers automate the translation of dataflow
strategies into executable code, addressing a fundamental challenge: the
mapping decisions analyzed above must be instantiated differently for
each hardware target. To see why this matters, consider what happens
when you compile ResNet-50 for GPU inference:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Graph optimization} fuses the 49 Conv2D-BatchNorm-ReLU
  sequences into 49 single kernels, eliminating 98 intermediate memory
  writes that would otherwise consume bandwidth
\item
  \textbf{Kernel selection} chooses Tensor Core implementations for the
  3x3 convolutions, exploiting the high arithmetic intensity (50-200
  FLOP/byte) we calculated in the Roofline analysis
\item
  \textbf{Memory planning} determines that intermediate activations
  require approximately 2.1 GiB at batch size 32, fitting comfortably in
  the A100's 40 GiB HBM
\item
  \textbf{Computation scheduling} overlaps memory transfers for layer
  N+1 with computation of layer N, hiding a substantial portion of
  transfer latency
\end{enumerate}

The result: inference time drops from approximately
\texttt{\{python\}\ naive\_inference\_ms} ms (naive execution) to
approximately \texttt{\{python\}\ optimized\_inference\_ms} ms
(optimized), roughly a \texttt{\{python\}\ compiler\_speedup\_str}x
improvement from compilation alone, before any algorithmic changes to
the model. This concrete example illustrates how the dataflow strategies
from the previous section, including kernel fusion
(Section~\ref{sec-ai-acceleration-kernel-fusion-7faf}) and tiling
(Section~\ref{sec-ai-acceleration-memoryefficient-tiling-strategies-9fce}),
translate into real performance through systematic compiler
optimization.

This process exemplifies the hardware-software co-design principle
established in
Section~\ref{sec-ai-acceleration-ai-hardware-acceleration-fundamentals-9b28},
where machine learning compilers bridge high-level model representations
with low-level hardware execution. The compiler optimizes models by
restructuring computations, selecting efficient execution kernels, and
maximizing hardware utilization (\citeproc{ref-chen_tvmlang_2018}{0001
et al. 2018b}). Unlike traditional compilers designed for
general-purpose computing, ML workloads require specialized approaches
for tensor computations and parallel execution.

\subsection{Compiler Design Differences for ML
Workloads}\label{sec-ai-acceleration-compiler-design-differences-ml-workloads-0698}

Machine learning workloads introduce challenges that traditional
compilers were not designed to handle. Unlike conventional software
execution, which primarily involves sequential or multi-threaded program
flow, machine learning models are expressed as computation graphs that
describe large-scale tensor operations. These graphs require specialized
optimizations that traditional compilers cannot efficiently apply
(\citeproc{ref-cui_mlcompilers_2019}{Cui, Li, and Xie 2019}).

Table~\ref{tbl-ml-vs-traditional-compilers} outlines the key differences
between traditional compilers and those designed for machine learning
workloads. While traditional compilers optimize linear program execution
through techniques like instruction scheduling and register allocation,
ML compilers focus on optimizing computation graphs for efficient tensor
operations. This distinction is critical, as ML compilers must
incorporate domain-specific transformations such as kernel fusion,
memory-aware scheduling, and hardware-accelerated execution plans to
achieve high performance on specialized accelerators like GPUs and TPUs.

This comparison highlights why machine learning models require a
different compilation approach. Machine learning compilers must
transform entire computation graphs, apply tensor-aware memory
optimizations, and schedule operations across thousands of parallel
processing elements, requirements that make traditional compiler
techniques insufficient for modern deep learning workloads.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.1899}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3924}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4114}}@{}}
\caption{\textbf{Compiler Optimization Priorities.} Traditional and
machine learning compilers diverge in their optimization targets:
traditional compilers prioritize efficient execution of sequential code,
while ML compilers focus on optimizing tensor operations within
computation graphs for specialized hardware. ML compilers incorporate
domain-specific transformations such as kernel fusion and memory-aware
scheduling, unlike the instruction scheduling and register allocation
techniques used in conventional
compilation.}\label{tbl-ml-vs-traditional-compilers}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Aspect}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Traditional Compiler}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Machine Learning Compiler}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Aspect}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Traditional Compiler}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Machine Learning Compiler}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Input Representation} & Linear program code (C, Python) &
Computational graph (ML models) \\
\textbf{Execution Model} & Sequential or multi-threaded execution &
Massively parallel tensor-based execution \\
\textbf{Optimization Priorities} & Instruction scheduling, loop
unrolling, register allocation & Graph transformations, kernel fusion,
memory-aware execution \\
\textbf{Memory Management} & Stack and heap memory allocation & Tensor
layout transformations, tiling, memory-aware scheduling \\
\textbf{Target Hardware} & CPUs (general-purpose execution) & GPUs,
TPUs, and custom accelerators \\
\textbf{Compilation Output} & CPU-specific machine code &
Hardware-specific execution plan (kernels, memory scheduling) \\
\end{longtable}

\phantomsection\label{callout-perspectiveux2a-1.26}
\begin{fbx}{callout-perspective}{Systems Perspective:}{The Hidden Optimization Layer}
\phantomsection\label{callout-perspective*-1.26}
Most practitioners never interact directly with ML compilers, yet
compiler quality often determines whether your model achieves 20\% or
80\% of hardware peak performance. When you call
\texttt{model.compile()} in Keras, \texttt{torch.compile()} in PyTorch,
or deploy through TensorRT, you're invoking sophisticated optimization
pipelines that:

\begin{itemize}
\tightlist
\item
  \textbf{Fuse operations} you never explicitly combined (Conv2D +
  BatchNorm + ReLU → single kernel)
\item
  \textbf{Reorder computations} to improve memory locality (tiling large
  matrix multiplies)
\item
  \textbf{Select kernels} from libraries containing hundreds of
  hand-tuned implementations
\item
  \textbf{Transform tensor layouts} between what your code expects and
  what hardware prefers
\end{itemize}

This matters practically: the same model definition can run 2-5× faster
simply by switching compilation backends (e.g., PyTorch eager mode
vs.~torch.compile with different backends). When performance doesn't
meet expectations, compiler configuration and backend selection are
often the first optimization levers, requiring no changes to model
architecture or training procedure.

\end{fbx}

\subsection{ML Compilation
Pipeline}\label{sec-ai-acceleration-ml-compilation-pipeline-7676}

Machine learning models, as defined in modern frameworks, are initially
represented in a high-level computation graph that describes operations
on tensors. However, these representations are not directly executable
on hardware accelerators such as GPUs, TPUs, and custom AI chips. To
achieve efficient execution, models must go through a compilation
process that transforms them into optimized execution plans suited for
the target hardware (\citeproc{ref-tensorflow_xla_2020}{Brain 2020}).

The machine learning compilation workflow consists of several key
stages, each responsible for applying specific optimizations that ensure
minimal memory overhead, maximum parallel execution, and optimal compute
utilization. These stages include:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Graph Optimization}: The computation graph is restructured to
  eliminate inefficiencies.
\item
  \textbf{Kernel Selection}: Each operation is mapped to an optimized
  hardware-specific implementation.
\item
  \textbf{Memory Planning}: Tensor layouts and memory access patterns
  are optimized to reduce bandwidth consumption.
\item
  \textbf{Computation Scheduling}: Workloads are distributed across
  parallel processing elements to maximize hardware utilization.
\item
  \textbf{Code Generation}: The optimized execution plan is translated
  into machine-specific instructions for execution.
\end{enumerate}

At each stage, the compiler applies theoretical optimizations discussed
earlier such as kernel fusion, tiling, data movement strategies, and
computation placement, ensuring that these optimizations are
systematically incorporated into the final execution plan.

By understanding this workflow, we can see how machine learning
acceleration is realized not just through hardware improvements but also
through compiler-driven software optimizations.

\subsection{Graph
Optimization}\label{sec-ai-acceleration-graph-optimization-f888}

AI accelerators provide specialized hardware to speed up computation,
but raw model representations are not inherently optimized for execution
on these accelerators. Machine learning frameworks define models using
high-level computation graphs, where nodes represent operations (such as
convolutions, matrix multiplications, and activations), and edges define
data dependencies. However, if executed as defined, these graphs often
contain redundant operations, inefficient memory access patterns, and
suboptimal execution sequences that can prevent the hardware from
operating at peak efficiency.

For example, in a Transformer model, the self-attention mechanism
involves repeated accesses to the same key-value pairs across multiple
attention heads. If compiled naïvely, the model may reload the same data
multiple times, leading to excessive memory traffic
(\citeproc{ref-shoeybi_megatron_2020}{Shoeybi et al. 2019a}). Similarly,
in a CNN, applying batch normalization and activation functions as
separate operations after each convolution leads to unnecessary
intermediate memory writes, increasing memory bandwidth usage. These
inefficiencies are addressed during graph optimization, where the
compiler restructures the computation graph to eliminate unnecessary
operations and improve memory locality
(\citeproc{ref-chen_tvmlang_2018}{0001 et al. 2018b}).

The graph optimization phase of compilation is responsible for
transforming this high-level computation graph into an optimized
execution plan before it is mapped to hardware. Rather than requiring
manual optimization, the compiler systematically applies transformations
that improve data movement, reduce redundant computations, and
restructure operations for efficient parallel execution
(\citeproc{ref-nvidia_tensorRT_2021}{NVIDIA 2021}).

At this stage, the compiler is still working at a hardware-agnostic
level, focusing on high-level restructuring that improves efficiency
before more hardware-specific optimizations are applied later.

\textbf{Computation Graph Optimization.} Graph optimization transforms
the computation graph through a series of structured techniques designed
to enhance execution efficiency. One key technique is kernel fusion,
which merges consecutive operations to eliminate unnecessary memory
writes and reduce the number of kernel launches. This approach is
particularly effective in convolutional neural networks, where fusing
convolution, batch normalization, and activation functions notably
accelerates processing. Another important technique is computation
reordering, which adjusts the execution order of operations to improve
data locality and maximize parallel execution. For instance, in
Transformer models, such reordering enables the reuse of cached
key-value pairs rather than reloading them repeatedly from memory,
thereby reducing latency.

Additionally, redundant computation elimination plays an important role.
By identifying and removing duplicate or unnecessary operations, this
method is especially beneficial in models with residual connections
where common subexpressions might otherwise be redundantly computed.
Memory-aware dataflow adjustments enhance overall performance by
refining tensor layouts and optimizing memory movement. For example,
tiling matrix multiplications to meet the structural requirements of
systolic arrays in TPUs ensures that hardware resources are utilized
optimally. This combined approach not only reduces unnecessary
processing but also aligns data storage and movement with the
accelerator's strengths, leading to efficient execution across diverse
AI workloads. Together, these techniques prepare the model for
acceleration by minimizing overhead and ensuring an optimal balance
between computational and memory resources.

\textbf{Implementation in AI Compilers.} Modern AI compilers perform
graph optimization through the use of automated pattern recognition and
structured rewrite rules, systematically transforming computation graphs
to maximize efficiency without manual intervention. For example,
Google's XLA (Accelerated Linear Algebra) in TensorFlow applies
graph-level transformations such as fusion and layout optimizations that
streamline execution on TPUs and GPUs. Similarly, TVM (Tensor Virtual
Machine) not only refines tensor layouts and adjusts computational
structures but also tunes execution strategies across diverse hardware
backends, which is particularly beneficial for deploying models on
embedded TinyML devices with strict memory constraints.

NVIDIA's TensorRT, another specialized deep learning compiler, focuses
on minimizing kernel launch overhead by fusing operations and optimizing
execution scheduling on GPUs, thereby improving utilization and reducing
inference latency in large-scale convolutional neural network
applications. Additionally, MLIR (Multi-Level Intermediate
Representation) facilitates flexible graph optimization across various
AI accelerators by enabling multi-stage transformations that improve
execution order and memory access patterns, thus easing the transition
of models from CPU-based implementations to accelerator-optimized
versions. These compilers preserve the mathematical integrity of the
models while rewriting the computation graph to ensure that the
subsequent hardware-specific optimizations can be effectively applied.
The practical impact of these transformations is substantial: without
proper graph optimization, a large Transformer model running on an edge
device may experience excessive memory stalls due to suboptimal data
access patterns, whereas effective graph restructuring can reduce memory
bandwidth consumption and latency enough to enable real-time inference
on resource-constrained devices.
\{\#sec-ai-acceleration-graph-optimization-importance-9ccb\}

With the computation graph now fully optimized, the next step in
compilation is kernel selection, where the compiler determines which
hardware-specific implementation should be used for each operation. This
ensures that the structured execution plan is translated into optimized
low-level instructions for the target accelerator.

\subsection{Kernel
Selection}\label{sec-ai-acceleration-kernel-selection-df01}

At this stage, the compiler translates the abstract operations in the
computation graph into optimized low-level functions, ensuring that
execution is performed as efficiently as possible given the constraints
of the target accelerator. A kernel is a specialized implementation of a
computational operation designed to run efficiently on a particular
hardware architecture. Most accelerators, including GPUs, TPUs, and
custom AI chips, provide multiple kernel implementations for the same
operation, each optimized for different execution scenarios. Choosing
the right kernel for each operation is critical for maximizing
computational throughput, minimizing memory stalls, and ensuring that
the accelerator's specialized processing elements are fully utilized
(\citeproc{ref-nvidia_tensorRT_2021}{NVIDIA 2021}).

Kernel selection builds upon the graph optimization phase, ensuring that
the structured execution plan is mapped to the most efficient
implementation available. While graph optimization eliminates
inefficiencies at the model level, kernel selection ensures that each
individual operation is executed using the most efficient
hardware-specific routine. The effectiveness of this process directly
impacts the model's overall performance, as poor kernel choices can
nullify the benefits of prior optimizations by introducing unnecessary
computation overhead or memory bottlenecks
(\citeproc{ref-chen_tvmlang_2018}{0001 et al. 2018b}).

In a Transformer model, the matrix multiplications that dominate
self-attention computations can be executed using different strategies
depending on the available hardware. On a CPU, a general-purpose matrix
multiplication routine is typically employed, exploiting vectorized
execution to improve efficiency. In contrast, on a GPU, the compiler may
select an implementation that leverages tensor cores to accelerate
matrix multiplications using mixed-precision arithmetic. When the model
is deployed on a TPU, the operation can be mapped onto a systolic array,
ensuring that data flows through the accelerator in a manner that
maximizes reuse and minimizes off-chip memory accesses. Additionally,
for inference workloads, an integer arithmetic kernel may be preferable,
as it facilitates computations in INT8 instead of floating-point
precision, thereby reducing power consumption without significantly
compromising accuracy.

In many cases, compilers do not generate custom kernels from scratch but
instead select from vendor-optimized kernel libraries that provide
highly tuned implementations for different architectures. For instance,
cuDNN and cuBLAS offer optimized kernels for deep learning on NVIDIA
GPUs, while oneDNN provides optimized execution for Intel architectures.
Similarly, ACL (Arm Compute Library) is optimized for Arm-based devices,
and Eigen and BLIS provide efficient CPU-based implementations of deep
learning operations. These libraries allow the compiler to choose
pre-optimized, high-performance kernels rather than having to reinvent
execution strategies for each hardware platform.

\subsubsection{Implementation in AI
Compilers}\label{sec-ai-acceleration-implementation-ai-compilers-c917}

AI compilers use heuristics\sidenote{\textbf{Heuristic}: From Greek
``heuriskein'' (to discover), sharing roots with ``eureka.'' A heuristic
is a practical rule-of-thumb that finds good solutions quickly without
exhaustively searching all possibilities. In AI compilers, heuristics
encode expert knowledge about hardware behavior, enabling fast kernel
selection decisions that would be intractable to compute optimally given
the exponential search space of possible configurations. }, profiling,
and cost models to determine the best kernel for each operation. These
strategies ensure that each computation is executed in a way that
maximizes throughput and minimizes memory bottlenecks.

In rule-based selection, the compiler applies predefined heuristics
based on the known capabilities of the hardware. For instance, XLA, the
compiler used in TensorFlow, automatically selects tensor core-optimized
kernels for NVIDIA GPUs when mixed-precision execution is enabled. These
predefined rules allow the compiler to make fast, reliable decisions
about which kernel to use without requiring extensive analysis.

Profile-guided selection takes a more dynamic approach, benchmarking
different kernel options and choosing the one that performs best for a
given workload. TVM, an open-source AI compiler, uses AutoTVM to
empirically evaluate kernel performance, tuning execution strategies
based on real-world execution times. By testing different kernels before
deployment, profile-guided selection helps ensure that operations are
assigned to the most efficient implementation under actual execution
conditions.

Another approach, cost model-based selection, relies on performance
predictions to estimate execution time and memory consumption for
various kernels before choosing the most efficient one. MLIR, a compiler
infrastructure designed for machine learning workloads, applies this
technique to determine the most effective tiling and memory access
strategies (\citeproc{ref-mlir_framework_2021}{Lattner et al. 2020}). By
modeling how different kernels interact with the accelerator's compute
units and memory hierarchy, the compiler can select the kernel that
minimizes execution cost while maximizing performance.

Many AI compilers also incorporate precision-aware kernel selection,
where the selected kernel is optimized for specific numerical formats
such as FP32, FP16, BF16, or INT8. Training workloads often prioritize
higher precision (FP32, BF16) to maintain model accuracy, whereas
inference workloads favor lower precision (FP16, INT8) to increase speed
and reduce power consumption. For example, an NVIDIA GPU running
inference with TensorRT can dynamically select FP16 or INT8 kernels
based on a model's accuracy constraints. This trade-off between
precision and performance is a key aspect of kernel selection,
especially when deploying models in resource-constrained environments.

Some compilers go beyond static kernel selection and implement adaptive
kernel tuning, where execution strategies are adjusted at runtime based
on the system's workload and available resources. AutoTVM in TVM
measures kernel performance across different workloads and dynamically
refines execution strategies. TensorRT applies real-time optimizations
based on batch size, memory constraints, and GPU load, adjusting kernel
selection dynamically. Google's TPU compiler takes a similar approach,
optimizing kernel selection based on cloud resource availability and
execution environment constraints. The consequences of poor kernel
selection are significant: if a Transformer model running on a GPU is
assigned a non-tensor-core kernel for its matrix multiplications, it may
execute at only a fraction of the possible performance. Conversely, if a
model designed for FP32 execution is forced to run on an INT8-optimized
kernel, it may experience numerical instability that degrades accuracy.
Kernel selection is therefore as much about maintaining numerical
correctness as it is about optimizing performance.
\{\#sec-ai-acceleration-kernel-selection-importance-3c3f\}

With kernel selection complete, the next stage in compilation involves
memory planning and computation scheduling, where the compiler
determines how data is allocated across the memory hierarchy and how
kernels are launched for execution. As kernel selection determines what
to execute, these subsequent phases dictate when and how those
operations run, ensuring that AI accelerators operate at peak
efficiency.

\subsection{Memory
Planning}\label{sec-ai-acceleration-memory-planning-fb9f}

The memory planning phase ensures that data is allocated and accessed in
a way that minimizes memory bandwidth consumption, reduces latency, and
maximizes cache efficiency (\citeproc{ref-zhang2020optimizing}{Zhang,
Li, and Ouyang 2020}). Even with the most optimized execution plan, a
model can still suffer from severe performance degradation if memory is
not managed efficiently.

Machine learning workloads are often memory-intensive. They require
frequent movement of large tensors between different levels of the
memory hierarchy. The compiler must determine how tensors are stored,
how they are accessed, and how intermediate results are handled to
ensure that memory does not become a bottleneck.

The memory planning phase optimizes tensor layouts, memory access
patterns, and buffer reuse to prevent unnecessary stalls and memory
contention during execution. Tensors are arranged in formats that align
with hardware access patterns, minimizing format conversions. Memory
accesses are structured to reduce cache misses and stalls, lowering
overall bandwidth consumption. Buffer reuse reduces redundant memory
allocations by managing intermediate results so that completed buffers
are reclaimed promptly. Together, these strategies ensure that data is
efficiently placed and accessed, enhancing both computational
performance and energy efficiency.

\subsubsection{Implementation in AI
Compilers}\label{sec-ai-acceleration-implementation-ai-compilers-2ae0}

Memory planning is a complex problem because AI models must balance
memory availability, reuse, and access efficiency while operating across
multiple levels of the memory hierarchy. AI compilers use several key
strategies to manage memory effectively and prevent unnecessary data
movement.

The first step in memory planning is tensor layout optimization, where
the compiler determines how tensors should be arranged in memory to
maximize locality and prevent unnecessary data format conversions.
Different hardware accelerators have different preferred storage
layouts. For instance, NVIDIA GPUs often use row-major storage (NHWC
format), while TPUs favor channel-major layouts (NCHW format) to
optimize memory coalescing (\citeproc{ref-abadi2016tensorflow}{Abadi et
al. 2016}). The compiler automatically transforms tensor layouts based
on the expected access patterns of the target hardware, ensuring that
memory accesses are aligned for maximum efficiency.

Beyond layout optimization, memory planning also includes buffer
allocation and reuse, where the compiler minimizes memory footprint by
reusing intermediate storage whenever possible. Deep learning workloads
generate many temporary tensors, such as activations and gradients,
which can quickly overwhelm on-chip memory if not carefully managed.
Instead of allocating new memory for each tensor, the compiler analyzes
the computation graph to identify opportunities for buffer reuse,
ensuring that intermediate values are stored and overwritten efficiently
(\citeproc{ref-moreau2018relay}{Jones 2018}).

Another critical aspect of memory planning is minimizing data movement
between different levels of the memory hierarchy. AI accelerators
typically have a mix of high-speed on-chip memory (such as caches or
shared SRAM) and larger, but slower, external DRAM. If tensor data is
repeatedly moved between these memory levels, the model may become
memory-bound, reducing computational efficiency. To prevent this,
compilers use tiling strategies that break large computations into
smaller, memory-friendly chunks, allowing execution to fit within fast,
local memory and reducing the need for costly off-chip memory accesses.
The consequences of neglecting memory planning are concrete: a CNN
running on a GPU may achieve high computational efficiency in theory,
but if its convolutional feature maps are stored in an incompatible
layout that necessitates repeated format conversions, the resulting
overhead can negate the gains from graph optimization and kernel
selection entirely.
\{\#sec-ai-acceleration-memory-planning-importance-e987\}

With memory allocation determined, the compiler must next decide when
and where each computation executes.

\subsection{Computation
Scheduling}\label{sec-ai-acceleration-computation-scheduling-7ccd}

With graph optimization completed, kernels selected, and memory planning
finalized, computation scheduling determines the execution order and
resource assignment for each operation. This phase determines when and
where each computation should be executed, ensuring that workloads are
efficiently distributed across available processing elements while
avoiding unnecessary stalls and resource contention
(\citeproc{ref-Rajbhandari2020}{Rajbhandari et al. 2020};
\citeproc{ref-Zheng2020}{Zheng et al. 2020}).

AI accelerators achieve high performance through massive parallelism,
but without an effective scheduling strategy, computational units may
sit idle, memory bandwidth may be underutilized, and execution
efficiency may degrade. Computation scheduling is responsible for
ensuring that all processing elements remain active, execution
dependencies are managed correctly, and workloads are distributed
optimally (\citeproc{ref-Jia2019}{Ziheng Jia et al. 2019}).

The scheduling phase coordinates parallel execution, synchronization,
and resource allocation. Task partitioning decomposes computations into
units that can be distributed among multiple compute cores. Execution
order optimization determines the sequence for launching operations,
maximizing hardware performance while reducing stalls. Resource
allocation and synchronization ensure that compute cores, memory
bandwidth, and shared caches are utilized without contention.

\subsubsection{Implementation in AI
Compilers}\label{sec-ai-acceleration-implementation-ai-compilers-ff25}

Computation scheduling is highly dependent on the underlying hardware
architecture, as different AI accelerators have unique execution models
that must be considered when determining how workloads are scheduled. AI
compilers implement several key strategies to optimize scheduling for
efficient execution.

One of the most critical aspects of scheduling is task partitioning,
where the compiler divides large computational graphs into smaller,
manageable units that can be executed in parallel. On GPUs, this
typically means mapping matrix multiplications and convolutions to
thousands of CUDA cores, while on TPUs, tasks are partitioned to fit
within systolic arrays that operate on structured data flows
(\citeproc{ref-norrie2021design}{Norrie et al. 2021}). In CPUs,
partitioning is often focused on breaking computations into vectorized
chunks that align with SIMD execution. The goal is to map workloads to
available processing units efficiently, ensuring that each core remains
active throughout execution.

Scheduling involves optimizing execution order to minimize dependencies
and maximize throughput beyond task partitioning. Many AI models include
operations that can be computed independently (e.g., different batches
in a batch processing pipeline) alongside operations that have strict
dependencies (e.g., recurrent layers in an RNN). AI compilers analyze
these dependencies and attempt to rearrange execution where possible,
reducing idle time and improving parallel efficiency. For example, in
Transformer models, scheduling may prioritize preloading attention
matrices into memory while earlier layers are still executing, ensuring
that data is ready when needed (\citeproc{ref-Shoeybi2019}{Shoeybi et
al. 2019b}).

Another critical aspect of computation scheduling is resource allocation
and synchronization, where the compiler determines how compute cores
share memory and coordinate execution. Modern AI accelerators often
support overlapping computation and data transfers, meaning that while
one task executes, the next task can begin fetching its required data.
Compilers take advantage of this by scheduling tasks in a way that hides
memory latency, ensuring that execution remains compute-bound rather
than memory-bound (\citeproc{ref-Chen2018}{0001 et al. 2018c}). TensorRT
and XLA, for example, employ streaming execution strategies where
multiple kernels are launched in parallel, and synchronization is
carefully managed to prevent execution stalls
(\citeproc{ref-GoogleXLA}{Google 2025}). Poor scheduling decisions can
negate the benefits of all prior compilation phases: a CNN with highly
optimized kernels and efficient memory layouts will still suffer reduced
throughput if compute units remain idle between kernel launches, and a
Transformer on a TPU may underperform if attention layers are not
scheduled to overlap with memory transfers.
\{\#sec-ai-acceleration-computation-scheduling-importance-04a1\}

With scheduling complete, the final compilation stage translates this
optimized execution plan into hardware-specific instructions.

\subsubsection{Code
Generation}\label{sec-ai-acceleration-code-generation-85c8}

Unlike the previous phases, which required AI-specific optimizations,
code generation follows many of the same principles as traditional
compilers. This process includes instruction selection, register
allocation, and final optimization passes, ensuring that execution makes
full use of hardware-specific features such as vectorized execution,
memory prefetching, and instruction reordering.

For CPUs and GPUs, AI compilers typically generate machine code or
optimized assembly instructions, while for TPUs,
FPGAs\sidenote{\textbf{FPGA (Field-Programmable Gate Array)}:
``Field-programmable'' means configurable after leaving the factory, in
the customer's ``field'' of deployment, contrasting with
factory-programmed ASICs. Xilinx (now AMD) introduced the first
commercial FPGA in 1985. The ``gate array'' refers to the matrix of
logic blocks that can be wired together through programmable
interconnects. FPGAs achieve 2-10x better performance per watt than GPUs
for specific workloads but require hardware description languages
(Verilog/VHDL), limiting adoption compared to GPU programming. }, and
other accelerators, the output may be optimized bytecode or execution
graphs that are interpreted by the hardware's runtime system.

At this point, the compilation pipeline is complete: the original
high-level model representation has been transformed into an optimized,
executable format tailored for efficient execution on the target
hardware. The combination of graph transformations, kernel selection,
memory-aware execution, and parallel scheduling ensures that AI
accelerators run workloads with maximum efficiency, minimal memory
overhead, and optimal computational throughput.

\subsection{Compilation-Runtime
Support}\label{sec-ai-acceleration-compilationruntime-support-0206}

The compiler transforms high-level machine learning models into
optimized execution plans tailored to specialized hardware. Throughout
this section, we have seen how graph optimization restructures
computation, kernel selection maps operations to hardware-efficient
implementations, memory planning optimizes data placement, and
computation scheduling ensures efficient parallel execution. Together,
these phases enable AI models to fully utilize modern accelerators with
high throughput, minimal memory overhead, and efficient execution
pipelines.

However, the compiler optimizations examined throughout this section
share a critical limitation: they all occur \emph{before} execution
begins. Graph restructuring that fuses Conv2D-BatchNorm-ReLU into single
kernels, kernel selection that chooses Tensor Core implementations,
memory planning that schedules activations across HBM, and computation
scheduling that overlaps transfers with computation all produce a
single, optimized execution plan based on assumptions about batch sizes,
dedicated hardware availability, and clean memory state.

Production AI systems inhabit a dynamic world that rarely matches these
static assumptions. Batch sizes vary from 1 (latency-sensitive single
requests) to 128 (throughput-oriented batch serving) within the same
deployment. GPU memory fragments during long-running inference servers,
forcing suboptimal tensor layouts. Multiple workloads compete for
accelerator resources in multi-tenant cloud environments. Thermal
throttling reduces sustained performance below the peaks observed in
short benchmarks. The runtime system bridges static optimization and
dynamic reality, continuously adapting execution to actual conditions
rather than assumed conditions.

\section{Runtime
Support}\label{sec-ai-acceleration-runtime-support-f94f}

The gap between compile-time optimization and production reality
motivates AI runtimes, which extend static compilation with real-time
adaptation (\citeproc{ref-nvidia_tensorRT_2021}{NVIDIA 2021}). AI
workloads operate in varied execution environments, where factors such
as fluctuating batch sizes, shared hardware resources, memory
contention, and latency constraints necessitate real-time adaptation.
Precompiled execution plans, optimized for a fixed set of assumptions,
may become suboptimal when actual runtime conditions change. AI runtimes
bridge this gap by providing a dynamic layer of execution management
that extends compile-time optimizations with real-time decision-making.
Unlike traditional compiled programs that execute a fixed sequence of
instructions, AI workloads require adaptive control over memory
allocation, kernel execution, and resource scheduling. AI runtimes
continuously monitor execution conditions and make on-the-fly
adjustments to ensure that machine learning models fully utilize
available hardware while maintaining efficiency and performance targets.

At a high level, AI runtimes manage three critical aspects of execution:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Kernel Execution Management}: AI runtimes dynamically select
  and dispatch computation kernels based on the current system state,
  ensuring that workloads are executed with minimal latency.
\item
  \textbf{Memory Adaptation and Allocation}: Since AI workloads
  frequently process large tensors with varying memory footprints,
  runtimes adjust memory allocation dynamically to prevent bottlenecks
  and excessive data movement (\citeproc{ref-deepmind_gpipe_2019}{Huang
  et al. 2019}).
\item
  \textbf{Execution Scaling}: AI runtimes handle workload distribution
  across multiple accelerators, supporting large-scale execution in
  multi-chip, multi-node, or cloud environments
  (\citeproc{ref-mirhoseini_device_placement_2017}{Mirhoseini et al.
  2017}).
\end{enumerate}

By dynamically handling these execution aspects, AI runtimes complement
compiler-based optimizations. Understanding how AI runtimes differ from
traditional software runtimes clarifies why machine learning workloads
require specialized execution strategies.

\subsection{Runtime Architecture Differences for ML
Systems}\label{sec-ai-acceleration-runtime-architecture-differences-ml-systems-932e}

Traditional software runtimes are designed for managing general-purpose
program execution, primarily handling sequential and multi-threaded
workloads on CPUs. These runtimes allocate memory, schedule tasks, and
optimize execution at the level of individual function calls and
instructions. In contrast, AI runtimes are specialized for machine
learning workloads, which require massively parallel computation,
large-scale tensor operations, and dynamic memory management.

Table~\ref{tbl-runtime-comparison} highlights the key differences
between traditional and AI runtimes. One of the key distinctions lies in
execution flow. Traditional software runtimes operate on a predictable,
structured execution model where function calls and CPU threads follow a
predefined control path. AI runtimes, however, execute computational
graphs, requiring complex scheduling decisions that account for
dependencies between tensor operations, parallel kernel execution, and
efficient memory access.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2308}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3154}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4462}}@{}}
\caption{\textbf{Runtime Execution Models.} Traditional runtimes
prioritize sequential or multi-threaded instruction processing, while AI
runtimes use massively parallel tensor operations for accelerated
computation on machine learning workloads. This divergence necessitates
specialized AI runtime architectures designed for efficient
parallelization and memory management of large-scale tensor
data.}\label{tbl-runtime-comparison}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Aspect}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Traditional Runtime}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{AI Runtime}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Aspect}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Traditional Runtime}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{AI Runtime}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Execution Model} & Sequential or multi-threaded execution &
Massively parallel tensor execution \\
\textbf{Task Scheduling} & CPU thread management & Kernel dispatch
across accelerators \\
\textbf{Memory Management} & Static allocation (stack/heap) & Dynamic
tensor allocation, buffer reuse \\
\textbf{Optimization Priorities} & Low-latency instruction execution &
Minimizing memory stalls, maximizing parallel execution \\
\textbf{Adaptability} & Mostly static execution plan & Adapts to batch
size and hardware availability \\
\textbf{Target Hardware} & CPUs (general-purpose execution) & GPUs,
TPUs, and custom accelerators \\
\end{longtable}

Memory management is another major differentiator. Traditional software
runtimes handle small, frequent memory allocations, optimizing for cache
efficiency and low-latency access. AI runtimes, in contrast, must
dynamically allocate, reuse, and optimize large tensors, ensuring that
memory access patterns align with accelerator-friendly execution. Poor
memory management in AI workloads can lead to performance bottlenecks,
particularly due to excessive off-chip memory transfers and inefficient
cache usage.

AI runtimes are inherently designed for adaptability. While traditional
runtimes often follow a mostly static execution plan, AI workloads
typically operate in highly variable execution environments, such as
cloud-based accelerators or multi-tenant hardware. As a result, AI
runtimes must continuously adjust batch sizes, reallocate compute
resources, and manage real-time scheduling decisions to maintain high
throughput and minimize execution delays.

Rather than simply managing CPU processes, AI runtimes must oversee
large-scale tensor execution, multi-device coordination, and real-time
workload adaptation. The practical implications become apparent when
models move from development to production.

\phantomsection\label{callout-perspectiveux2a-1.27}
\begin{fbx}{callout-perspective}{Systems Perspective:}{When Production Differs from Development}
\phantomsection\label{callout-perspective*-1.27}
Runtime behavior often surprises engineers who optimized their models in
development environments. Common production surprises include:

\textbf{Variable batch sizes}: Training uses fixed batch sizes, but
production inference may receive single requests (batch=1) or bursts
(batch=64+). Runtimes must handle both efficiently. Single requests need
latency optimization; bursts need throughput optimization.

\textbf{Memory fragmentation}: Long-running inference servers gradually
fragment GPU memory. Runtimes implement defragmentation strategies, but
understanding when to restart services or pre-allocate memory pools
requires awareness of these dynamics.

\textbf{Multi-tenant interference}: Cloud accelerators are often shared.
Your model might run 20\% slower when a neighbor's workload competes for
memory bandwidth. Production systems need monitoring to detect and
respond to this interference.

\textbf{Thermal throttling}: Sustained workloads may trigger thermal
throttling that wasn't observed in short benchmarks. The A100 SXM
operates at \texttt{\{python\}\ a100\_tdp}W TDP while the A100 PCIe
operates at 300W TDP; these represent different form factors with
different cooling requirements, not boost versus sustained states.
Production performance depends on which variant is deployed and whether
thermal limits are approached.

Understanding runtime adaptation mechanisms helps engineers design
systems that degrade gracefully rather than fail unexpectedly.

\end{fbx}

\subsection{Dynamic Kernel
Execution}\label{sec-ai-acceleration-dynamic-kernel-execution-33fc}

Dynamic kernel execution is the process of mapping machine learning
models to hardware and optimizing runtime execution. While static
compilation provides a solid foundation, efficient execution of machine
learning workloads requires real-time adaptation to fluctuating
conditions such as available memory, data sizes, and computational
loads. The runtime functions as an intermediary that continuously
adjusts execution strategies to match both the constraints of the
underlying hardware and the characteristics of the workload.

When mapping a machine learning model to hardware, individual
computational operations, including matrix multiplications,
convolutions, and activation functions, must be assigned to the most
appropriate processing units. This mapping is not fixed; it must be
modified during runtime in response to changes in input data, memory
availability, and overall system load. Dynamic kernel execution allows
the runtime to make real-time decisions regarding kernel selection,
execution order, and memory management, ensuring that workloads remain
efficient despite these changing conditions.

For example, consider an AI accelerator executing a deep neural network
(DNN) for image classification. If an incoming batch of high-resolution
images requires significantly more memory than expected, a statically
planned execution may cause cache thrashing or excessive off-chip memory
accesses. Instead, a dynamic runtime can adjust tiling strategies on the
fly, breaking down tensor operations into smaller tiles that fit within
the high-speed on-chip memory. This prevents memory stalls and ensures
optimal utilization of caches.

Similarly, when running a transformer-based NLP model, the sequence
length of input text may vary between inference requests. A static
execution plan optimized for a fixed sequence length may lead to
underutilization of compute resources when processing shorter sequences
or excessive memory pressure with longer sequences. Dynamic kernel
execution can mitigate this by selecting different kernel
implementations based on the actual sequence length, dynamically
adjusting memory allocations and execution strategies to maintain
efficiency.

Overlapping computation with memory movement is an important strategy to
mitigate performance bottlenecks. AI workloads often encounter delays
due to memory-bound issues, where data movement between memory
hierarchies limits computation speed. To combat this, AI runtimes
implement techniques like asynchronous execution and double buffering,
ensuring that computations proceed without waiting for memory transfers
to complete. In a large-scale model, for instance, image data can be
prefetched while computations are performed on the previous batch, thus
maintaining a steady flow of data and avoiding pipeline stalls.

Another practical example is the execution of convolutional layers in a
CNN on a GPU. If multiple convolution kernels need to be scheduled, a
static scheduling approach may lead to inefficient resource utilization
due to variation in layer sizes and compute requirements. By dynamically
scheduling kernel execution, AI runtimes can prioritize smaller kernels
when compute units are partially occupied, improving hardware
utilization. For instance, in NVIDIA's TensorRT runtime, fusion of small
kernels into larger execution units is done dynamically to avoid launch
overhead, optimizing latency-sensitive inference tasks.

By dynamically adjusting execution strategies in response to real-time
system conditions, AI runtimes optimize both training and inference
performance across various hardware platforms. Beyond execution strategy
adaptation, runtimes must also select which specific kernel
implementations to invoke for each operation.

\subsection{Runtime Kernel
Selection}\label{sec-ai-acceleration-runtime-kernel-selection-1ffe}

While compilers perform an initial selection of kernels based on static
analysis, AI runtimes often need to override these decisions during
execution. Real-time factors, such as available memory, hardware
utilization, and workload priorities, may differ significantly from the
assumptions made during compilation. By dynamically selecting and
switching kernels at runtime, AI runtimes can adapt to these changing
conditions, ensuring that models continue to perform efficiently.

For instance, consider transformer-based language models, where a
significant portion of execution time is spent on matrix
multiplications. The AI runtime must determine the most efficient way to
execute these operations based on the current system state. If the model
is running on a GPU with specialized Tensor Cores, the runtime may
switch from a standard FP32 kernel to an FP16 kernel to take advantage
of hardware acceleration (\citeproc{ref-shoeybi_megatron_2020}{Shoeybi
et al. 2019a}). Conversely, if the lower precision of FP16 causes
unacceptable numerical instability, the runtime can opt for
mixed-precision execution, selectively using FP32 where higher precision
is necessary.

Memory constraints also influence kernel selection. When memory
bandwidth is limited, the runtime may adjust its execution strategy,
reordering operations or changing the tiling strategy to fit
computations into the available cache rather than relying on slower main
memory. For example, a large matrix multiplication may be broken into
smaller chunks, ensuring that the computation fits into the on-chip
memory of the GPU, reducing overall latency.

Additionally, batch size can influence kernel selection. For workloads
that handle a mix of small and large batches, the AI runtime may choose
a latency-optimized kernel for small batches and a throughput-optimized
kernel for large-scale batch processing. This adjustment ensures that
the model continues to operate efficiently across different execution
scenarios, without the need for manual tuning. Once the appropriate
kernel is selected, the runtime must schedule its execution to maximize
hardware utilization.

\subsection{Kernel Scheduling and
Utilization}\label{sec-ai-acceleration-kernel-scheduling-utilization-99d6}

Effective kernel scheduling ensures that selected kernels execute in a
way that maximizes parallelism and resource utilization. Unlike
traditional task schedulers, which are designed to manage CPU threads,
AI runtimes must coordinate a much larger number of tasks across
parallel execution units such as GPU cores, tensor processing units, or
custom AI accelerators (\citeproc{ref-google_tpu_2017}{Norman P. Jouppi
et al. 2017a}). Effective scheduling ensures that these computational
resources are kept fully engaged, preventing bottlenecks and maximizing
throughput.

For example, in image recognition models that use convolutional layers,
operations can be distributed across multiple processing units, enabling
different filters to run concurrently. This parallelization ensures that
the available hardware is fully utilized, speeding up execution.
Similarly, batch normalization and activation functions must be
scheduled efficiently to avoid unnecessary delays. If these operations
are not interleaved with other computations, they may block the pipeline
and reduce overall throughput.

Efficient kernel scheduling can also be influenced by real-time memory
management . AI runtimes ensure that intermediate data, such as feature
maps in deep neural networks, are preloaded into cache before they are
needed. This proactive management helps prevent delays caused by waiting
for data to be loaded from slower memory tiers, ensuring continuous
execution.

These techniques enable AI runtimes to ensure optimal resource
utilization and efficient parallel computation, both necessary for
high-performance execution of machine learning models, particularly in
environments that require scaling across multiple hardware accelerators.
The compiler and runtime systems examined thus far optimize execution
within single accelerators, but the largest AI workloads exceed what any
single chip can deliver. Single-chip optimizations achieve impressive
results: ResNet-50 inference accelerates from 47 ms to 8 ms through
compiler optimization alone, and the dataflow strategies we examined can
push GPU utilization from 20\% to 80\% of peak throughput. Yet for the
largest AI workloads, even perfectly optimized single-chip execution
proves insufficient.

Consider the scale of training GPT-3, which required approximately
\(3.14 \times 10^{23}\) floating-point operations
(\citeproc{ref-Brown2020}{Brown et al. 2020}). Even at the H100's peak
FP16 throughput of nearly 2 petaFLOPS, completing this computation on a
single accelerator would require over five years of continuous operation
at theoretical peak, and considerably longer at realistic utilization
rates of 40-60\%. Real-time inference serving for global applications
like ChatGPT or Google Search demands throughput beyond any single
accelerator's capacity, requiring distributed inference across hundreds
of chips. These computational requirements necessitate scaling beyond
single-chip systems, introducing different engineering challenges from
those we have examined.

\section{Scaling Beyond Single
Accelerators}\label{sec-ai-acceleration-scaling-beyond-single}

This section provides awareness of multi-chip scaling while maintaining
our Volume I focus on single-machine systems. The techniques we have
covered, dataflow optimization, kernel fusion, memory hierarchy
exploitation, and compiler optimization, remain the foundation for
efficient execution even in distributed settings. Each individual
accelerator in a multi-chip system must still be optimized using these
principles. However, multi-chip architectures introduce additional
concerns around communication overhead, memory coherence, and fault
tolerance that transform the optimization landscape. The detailed
implementation of distributed training systems, including gradient
synchronization protocols, parameter server architectures, and
cluster-scale orchestration, is covered in advanced treatments of
machine learning infrastructure.

When single-accelerator capacity proves insufficient, AI systems must
scale across multiple chips. Understanding these scaling approaches is
important for practitioners who will encounter multi-chip systems in
production environments, even when working primarily with
single-accelerator deployments.

\subsection{Multi-Chip Scaling
Approaches}\label{sec-ai-acceleration-multichip-scaling-approaches}

Modern AI systems employ several strategies to scale beyond individual
accelerators, each with distinct trade-offs.

One approach partitions large designs into smaller, modular dies
interconnected within a single package (chiplet-based architectures).
This approach bypasses manufacturing limits of monolithic chips while
maintaining relatively low communication latency within the package.

When even greater compute capacity is required, systems connect multiple
discrete accelerators, each with dedicated memory and compute resources.
This enables workloads to be split using data parallelism (each
accelerator processes different batches) or model parallelism (different
accelerators handle different network layers). High-bandwidth intra-node
interconnects can enable efficient gradient synchronization across the
system, though realized performance depends on topology and collective
communication efficiency.

At data center scale, purpose-built interconnect fabrics enable hundreds
of accelerators to work together. Cluster topology and collective
communication algorithms become central determinants of scaling
efficiency, and near-linear scaling is achievable on some workloads when
communication overhead is controlled.

The most aggressive scaling approach treats an entire silicon wafer as a
unified compute fabric. Wafer-scale integration platforms (e.g.,
Cerebras WSE-class systems) integrate extremely large numbers of
transistors and cores on a single device, reducing or eliminating
inter-chip communication overhead. This approach introduces its own
challenges in thermal dissipation, fault tolerance, and manufacturing
yield, representing the frontier of single-system compute density.

\subsection{Why Scaling Introduces New
Constraints}\label{sec-ai-acceleration-why-scaling-constraints}

The transition from single-chip to multi-chip architectures introduces
qualitatively different constraints that fundamentally transform system
optimization.

Communication overhead emerges as the primary limit on scaling
efficiency. Amdahl's Law\sidenote{Recall from
\textbf{?@sec-ml-system-architecture} that Amdahl's Law limits speedup
based on the serial fraction of computation. For distributed training,
this serial fraction includes gradient synchronization, which at 5\%
overhead caps maximum speedup at 20x regardless of GPU count. This
explains why scaling efficiency degrades at large cluster sizes and
motivates algorithmic innovations like gradient compression and
communication-computation overlap. } quantifies how communication during
gradient synchronization creates sequential bottlenecks. For
hundred-billion-parameter-scale models, AllReduce operations can require
exchanging hundreds of gigabytes of gradients per training step.

This communication overhead explains why scaling to very large
accelerator counts can show diminishing returns without algorithmic
innovations like gradient compression, overlap, or alternative
parallelization strategies.

Memory coherence presents another challenge at scale. Ensuring all
processors see consistent views of shared memory adds 10-50 ns latency
per access in traditional coherence protocols. For AI accelerators with
thousands of cores, this overhead becomes prohibitive, forcing explicit
memory management where programmers control data placement and
synchronization manually.

As systems grow larger, fault tolerance requirements increase
correspondingly. Large-scale systems must handle component failures
gracefully since the probability of at least one failure increases with
system size. TPU Pods implement sophisticated consensus algorithms to
maintain training consistency when optical links fail, while wafer-scale
systems incorporate redundant cores to tolerate localized silicon
defects.

Perhaps most significantly, the energy costs of data movement come to
dominate system design. Moving data across a TPU Pod's optical
interconnect can consume orders of magnitude more energy than on-chip
communication within individual TPUs. This energy differential
transforms distributed training into a careful balance between
computation parallelism and communication efficiency, a concern that
shapes both hardware architecture and algorithm design.

Data center scaling and edge deployment represent opposite ends of a
deployment spectrum, yet they share the same fundamental principles.
Data center scaling asks ``how do we coordinate many powerful chips?''
while edge scaling asks ``how do we fit useful AI into a few constrained
watts?'' Both questions share a common answer: match workload
characteristics to hardware capabilities while minimizing data movement.
The principles of compute specialization, memory hierarchy optimization,
and workload mapping apply at both scales; only the constraints differ.
Data centers optimize for aggregate throughput within power budgets
measured in megawatts; edge devices optimize for responsiveness within
power budgets measured in milliwatts. To make this concrete: the same
ResNet-50 inference we analyzed throughout this chapter must also
execute within a 5 watt power envelope and a 30 ms latency target on a
smartphone, constraints that require a fundamentally different approach
to the same acceleration principles.

\section{Heterogeneous SoC AI
Acceleration}\label{sec-ai-acceleration-heterogeneous-soc-ai-acceleration-b1bb}

At the edge end of the deployment spectrum, the hardware acceleration
principles established in this chapter (specialized compute units,
memory hierarchy optimization, and workload mapping strategies) must
operate under dramatically different constraints. A smartphone's SoC
operates within a 3-7 watt sustained power budget (with brief peaks to
10-15W), autonomous vehicles require deterministic sub-100ms latency for
perception-to-action loops, and IoT sensors must function for months to
years on battery power. These constraints necessitate heterogeneous
System-on-Chip (SoC) architectures that integrate CPU cores, GPU
shaders, digital signal processors (DSPs), and dedicated neural
processing units (NPUs) within a single chip. Orchestrating these
diverse processors to achieve optimal performance under strict power,
thermal, and latency requirements demands fundamentally different
approaches than data center deployments.

\phantomsection\label{callout-lighthouseux2a-1.28}
\begin{fbx}{callout-lighthouse}{Lighthouse:}{The Case for Heterogeneous Microcontrollers}
\phantomsection\label{callout-lighthouse*-1.28}
\textbf{The Extreme Edge}: The \textbf{Smart Doorbell} (Wake Vision)
pushes heterogeneity to its logical limit. Unlike a smartphone SoC with
a multi-watt budget, a doorbell camera often runs on a microcontroller
with a \textbf{milliwatt budget}.

To achieve real-time person detection (30 FPS) within this envelope,
modern MCUs adopt the same heterogeneous strategy as their larger mobile
cousins but at a micro-scale. A typical architecture pairs a
general-purpose core (e.g., Cortex-M) for system logic with a dedicated
micro-NPU (e.g., Ethos-U) for CNN acceleration. The NPU executes the
Wake Vision MobileNet model at 50-100\(\times\) better energy efficiency
than the CPU could achieve alone. Without this specialized acceleration,
the ``always-on'' promise of the Smart Doorbell would remain physically
impossible.

\end{fbx}

\subsection{Mobile SoC Architecture
Evolution}\label{sec-ai-acceleration-mobile-soc-architecture-evolution-6ca8}

Qualcomm's Snapdragon AI Engine exemplifies heterogeneous computing for
mobile AI, coordinating CPU cores, GPU shaders, a DSP, and a dedicated
NPU\sidenote{NPUs (introduced in \textbf{?@sec-ml-system-architecture})
excel at low-power inference on common operator patterns but have less
programmability than GPUs. From an architecture perspective, NPUs
achieve efficiency by hardcoding common dataflows (systolic arrays for
matrix multiplication) rather than supporting general computation,
trading flexibility for 10-100x better energy efficiency on supported
operations. } across a shared memory hierarchy. Modern mobile SoCs use
workload distribution so that computer vision kernels can execute on the
GPU's parallel shaders, audio processing can use DSP arithmetic units,
and transformer attention mechanisms can utilize NPU-optimized matrix
engines. This coordination requires careful scheduling to meet real-time
constraints while managing thermal throttling and battery life.

While Qualcomm's approach emphasizes diverse processor specialization,
vertically integrated strategies highlight how tight hardware-software
co-design can enable sophisticated heterogeneous execution. Unified
memory architectures can reduce explicit data copying overhead, and
different compute blocks can be scheduled for different operator types
(for example, matrix-heavy layers on an NPU, convolutional operators on
a GPU, and control flow on the CPU). This coordination supports
interactive on-device experiences, though realized latency depends on
the full pipeline and device thermal conditions.

Beyond vertically integrated solutions, IP licensing models allow SoC
designers to customize processor combinations based on target
applications, mixing CPU, GPU, DSP, and NPU blocks. This modular
flexibility allows automotive SoCs to emphasize deterministic real-time
processing while smartphone SoCs optimize for interactive performance
and battery efficiency.

\subsection{Strategies for Dynamic Workload
Distribution}\label{sec-ai-acceleration-strategies-dynamic-workload-distribution-a421}

With multiple specialized processors available on heterogeneous SoCs,
the critical challenge becomes intelligently distributing neural network
operations across these resources to maximize performance while
respecting power and latency constraints.

Modern neural networks require intelligent partitioning across
heterogeneous processors based on operation characteristics and current
system state. Convolutional layers with regular data access patterns
typically execute efficiently on GPU shader cores, while fully connected
layers with irregular sparsity patterns may perform better on
general-purpose CPU cores with large caches. Attention mechanisms in
transformers benefit from NPU matrix engines when sequences are long,
but may execute more efficiently on CPU when sequence lengths are small
due to the NPU setup overhead.

Beyond static operation-to-processor mapping, heterogeneous SoCs
implement dynamic processor selection based on multiple constraints:

\begin{itemize}
\tightlist
\item
  \textbf{Power Budget}: During battery operation, the system may route
  computations to lower-power DSP cores rather than high-performance GPU
  cores
\item
  \textbf{Thermal State}: When approaching thermal limits, workloads
  shift from power-hungry NPU to more efficient CPU execution
\item
  \textbf{Latency Requirements}: Safety-critical automotive applications
  prioritize deterministic CPU execution over potentially faster but
  variable NPU processing
\item
  \textbf{Concurrent Workload Interference}: Multiple AI applications
  may require load balancing across available processors to maintain
  Quality of Service
\end{itemize}

Compounding the processor selection challenge, shared memory
architectures require sophisticated arbitration when multiple processors
access LPDDR simultaneously. The Snapdragon 8 Gen 3's memory controller
implements priority-based scheduling where camera processing receives
higher priority than background AI tasks, ensuring real-time video
processing while background neural networks adapt their execution
patterns to available memory bandwidth. This arbitration becomes
critical during memory-intensive operations like large language model
inference, where parameter streaming from DRAM must be carefully
coordinated across processors.

\subsection{Power and Thermal
Management}\label{sec-ai-acceleration-power-thermal-management-6c00}

Mobile AI workloads must maintain high performance while operating
within strict power budgets and thermal envelopes. These constraints
require sophisticated coordination across heterogeneous processors.

Heterogeneous SoCs implement coordinated DVFS across multiple processors
to optimize the power-performance envelope. When one processor increases
frequency to meet latency demands, the system may reduce voltage on
other processors to maintain total power budget. This coordination
becomes complex in AI workloads where computational phases may shift
rapidly between processors. The system must predict upcoming workload
transitions to preemptively adjust operating points while avoiding
voltage/frequency oscillations that degrade efficiency.

When DVFS alone cannot maintain the power envelope, mobile SoCs
implement thermal throttling through intelligent task migration rather
than simple frequency reduction. When the NPU approaches thermal limits
during intensive neural network processing, the runtime system can
migrate layers to the GPU or CPU while maintaining computational
throughput. This approach preserves performance during thermal events,
though it requires sophisticated workload characterization to predict
execution time and power consumption across different processors.

Beyond real-time power and thermal management, mobile AI systems must
also adapt their computational strategies based on battery state and
charging status. During low battery conditions, the system may switch
from high-accuracy models to efficient approximations, migrate workloads
from power-hungry NPU to energy-efficient DSP, or reduce inference
frequency while maintaining application responsiveness. Conversely,
during charging, the system can enable higher-performance models and
increase processing frequency to deliver enhanced user experiences.

\subsection{Automotive Heterogeneous AI
Systems}\label{sec-ai-acceleration-automotive-heterogeneous-ai-systems-deda}

Automotive applications introduce unique heterogeneous computing
challenges that combine mobile-style power efficiency with hard
real-time latency requirements and functional safety requirements. This
combination demands fundamentally different architectural approaches.

Automotive SoCs aim to provide deterministic inference latency for
safety-critical functions while supporting advanced driver assistance
systems (ADAS). The Snapdragon Ride platform coordinates multiple AI
accelerators across safety domains. Redundant processing elements
support functional safety objectives while high-performance accelerators
handle perception, planning, and control algorithms. This architecture
requires temporal isolation between safety-critical and convenience
functions, implemented through hardware partitioning and time-triggered
scheduling.

These safety requirements become even more complex when considering that
modern vehicles integrate multiple AI-enabled SoCs for different
domains. Vision processing SoCs handle camera-based perception, radar
processing SoCs manage RF sensor data, while central compute platforms
coordinate high-level decision making. These distributed systems must
maintain temporal coherence across sensor modalities with
microsecond-precision timing, requiring specialized inter-SoC
communication protocols and distributed synchronization mechanisms.

Extending beyond the vehicle's internal sensors, vehicle-to-everything
(V2X) communication adds another layer of heterogeneous processing where
AI algorithms must coordinate local sensor processing with information
received from other vehicles and infrastructure. This requires ultra-low
latency processing chains where 5G modems, AI accelerators, and control
systems operate within millisecond deadlines while maintaining
functional safety requirements.

\subsection{Software Stack
Challenges}\label{sec-ai-acceleration-software-stack-challenges-255c}

The architectural sophistication of heterogeneous SoCs creates
substantial software development challenges that span programming
models, memory management, and runtime optimization.

Programming heterogeneous SoCs requires frameworks that abstract
processor differences while exposing performance-critical optimization
opportunities. OpenCL and Vulkan provide cross-processor execution, but
achieving optimal performance requires processor-specific optimizations
that complicate portable development. Modern ML frameworks like
TensorFlow Lite and PyTorch Mobile implement automatic processor
selection, but developers still need to understand heterogeneous
execution patterns to achieve optimal results.

Complicating the programming challenge further, heterogeneous SoCs with
shared memory architectures require sophisticated memory management that
considers processor-specific caching behaviors, memory access patterns,
and coherency requirements. CPU caches may interfere with GPU memory
access patterns, while NPU direct memory access (DMA) operations must be
synchronized with CPU cache operations to maintain data consistency.

To address the complexity of manual optimization across these
dimensions, advanced heterogeneous SoCs implement machine learning-based
runtime optimization that learns from execution patterns to improve
processor selection, thermal management, and power optimization. These
systems collect telemetry on workload characteristics, processor
utilization, and power consumption to build models that predict optimal
execution strategies for new workloads.

This heterogeneous approach to AI acceleration represents the direction
of computing, where no single processor architecture can optimally
handle the diverse computational patterns in modern AI applications.
Understanding these coordination challenges is essential for developing
efficient mobile AI systems that deliver high performance while meeting
the strict power, thermal, and real-time constraints of edge deployment
scenarios.

The complexity of hardware acceleration, spanning data center
architectures to heterogeneous mobile SoCs, creates opportunities for
misconception and suboptimal design decisions. The following section
distills common errors that waste expensive accelerator resources and
lead to deployments achieving only a fraction of theoretical
performance.

\section{Fallacies and
Pitfalls}\label{sec-ai-acceleration-fallacies-pitfalls-dc1f}

Hardware acceleration involves counterintuitive performance
characteristics where impressive specifications mask underlying
bottlenecks. The fallacies and pitfalls below capture hardware selection
and optimization errors that waste expensive accelerator resources and
lead to deployments that achieve only 10-30\% of theoretical
performance.

\paragraph*{\texorpdfstring{Fallacy: \emph{More specialized hardware
always provides better performance than general-purpose
alternatives.}}{Fallacy: More specialized hardware always provides better performance than general-purpose alternatives.}}\label{fallacy-more-specialized-hardware-always-provides-better-performance-than-general-purpose-alternatives.}
\addcontentsline{toc}{paragraph}{Fallacy: \emph{More specialized
hardware always provides better performance than general-purpose
alternatives.}}

Engineers assume specialized accelerators automatically outperform
general-purpose processors for all AI workloads. In reality, specialized
hardware achieves peak performance only when workloads match
architectural assumptions. As demonstrated in
Section~\ref{sec-ai-acceleration-roofline-model}, operations must exceed
the accelerator's ridge point to be compute-bound; an A100 GPU has a
ridge point of \texttt{\{python\}\ a100\_ridge} FLOP/byte, meaning
operations with arithmetic intensity below this threshold are
memory-bound regardless of the accelerator's
\texttt{\{python\}\ a100\_tflops\_fp16} TFLOPS peak compute. A
transformer attention softmax with AI = 2-5 FLOP/byte achieves only 4-10
TFLOPS (3\% utilization) on an A100, while achieving 80-90\% of a CPU's
lower peak because CPUs have ridge points of 10-20 FLOP/byte. Models
with irregular memory access, small batch sizes, or dynamic computation
graphs may perform better on flexible processors. Effective hardware
selection requires matching workload arithmetic intensity to
architectural ridge points, not assuming specialization always wins.

\paragraph*{\texorpdfstring{Pitfall: \emph{Ignoring memory bandwidth
limitations when selecting acceleration
strategies.}}{Pitfall: Ignoring memory bandwidth limitations when selecting acceleration strategies.}}\label{pitfall-ignoring-memory-bandwidth-limitations-when-selecting-acceleration-strategies.}
\addcontentsline{toc}{paragraph}{Pitfall: \emph{Ignoring memory
bandwidth limitations when selecting acceleration strategies.}}

Practitioners focus on peak TFLOPS without analyzing whether their
workloads can achieve compute-bound performance. As quantified in
Section~\ref{sec-ai-acceleration-understanding-ai-memory-wall-3ea9},
accessing DRAM consumes 100-200 pJ per access versus 1-10 pJ for on-chip
memory, creating orders-of-magnitude energy penalties. An accelerator
advertising 300 TFLOPS with 2 TB/s bandwidth has a ridge point of 150
FLOP/byte; LayerNorm operations with AI = 1.5 FLOP/byte achieve only 3
TFLOPS (1\% utilization). Organizations deploy expensive high-compute
accelerators for memory-bound workloads, achieving 10-20\% utilization
when lower-cost, bandwidth-optimized alternatives would perform
identically. Teams must calculate workload arithmetic intensity and
compare against hardware ridge points before purchasing accelerators.

\paragraph*{\texorpdfstring{Fallacy: \emph{Hardware acceleration
benefits scale linearly with additional
accelerators.}}{Fallacy: Hardware acceleration benefits scale linearly with additional accelerators.}}\label{fallacy-hardware-acceleration-benefits-scale-linearly-with-additional-accelerators.}
\addcontentsline{toc}{paragraph}{Fallacy: \emph{Hardware acceleration
benefits scale linearly with additional accelerators.}}

Teams expect 8 GPUs to train 8× faster than 1 GPU. Multi-accelerator
scaling introduces communication overhead that violates linear scaling
assumptions. As noted in
Section~\ref{sec-ai-acceleration-scaling-beyond-single}, AllReduce
operations for gradient synchronization can require exchanging hundreds
of gigabytes per training step for large models. With NVLink at 600 GB/s
bidirectional, synchronizing 1 GB of gradients requires 1.67 ms; for a
50 ms training step, this represents 3.3\% overhead with perfect
overlap. Without overlap, 8-GPU setups achieve 7.5× speedup (94\%
efficiency) at best, and typical workloads see 6-7× (75-87\% efficiency)
due to load imbalance and synchronization barriers. Small models with
insufficient parallel work achieve even worse scaling, sometimes seeing
3-4× speedup on 8 GPUs (37-50\% efficiency).

\paragraph*{\texorpdfstring{Fallacy: \emph{Peak FLOPS specifications
determine real-world accelerator
performance.}}{Fallacy: Peak FLOPS specifications determine real-world accelerator performance.}}\label{fallacy-peak-flops-specifications-determine-real-world-accelerator-performance.}
\addcontentsline{toc}{paragraph}{Fallacy: \emph{Peak FLOPS
specifications determine real-world accelerator performance.}}

Vendors advertise peak FLOPS as the definitive measure of accelerator
capability, but real-world performance equals Peak FLOPS \(\times\)
Utilization, where utilization is dictated by the Roofline Model. An
A100 advertises \texttt{\{python\}\ a100\_tflops\_fp16} TFLOPS at FP16,
yet transformer training typically achieves only 120 to 180 TFLOPS (40
to 60\% utilization) because memory-bound operations such as attention
and LayerNorm drag down the average throughput. Recommendation models
fare even worse, often reaching only 10 to 30 TFLOPS (3 to 10\%
utilization) due to sparse, irregular memory access patterns that leave
compute units idle. Engineers should budget projects based on sustained
throughput, measured or estimated via the Roofline Model, rather than
peak marketing specifications.

\paragraph*{\texorpdfstring{Pitfall: \emph{Deploying small-batch
inference workloads on high-compute
accelerators.}}{Pitfall: Deploying small-batch inference workloads on high-compute accelerators.}}\label{pitfall-deploying-small-batch-inference-workloads-on-high-compute-accelerators.}
\addcontentsline{toc}{paragraph}{Pitfall: \emph{Deploying small-batch
inference workloads on high-compute accelerators.}}

Teams deploy high-throughput training accelerators (A100, H100) for
latency-sensitive inference with batch size 1-4. Small batches severely
reduce arithmetic intensity: a dense layer with M=N=2048 achieves AI = 2
FLOP/byte at batch=1 versus AI = 186 FLOP/byte at batch=256. At batch=1,
an A100 achieves 4 TFLOPS (1.3\% utilization) due to memory bottlenecks,
while a lower-cost T4 achieves 3.5 TFLOPS. For FP16 Tensor Core
operations, the T4's peak is 65 TFLOPS with a ridge point of
approximately 203 FLOP/byte (65 TFLOPS / 320 GB/s); for FP32 operations,
the peak drops to 8.1 TFLOPS with a ridge point of only 25 FLOP/byte.
Either way, small-batch inference remains memory-bound, but the T4's
lower cost makes it more economical for these workloads. The cost
difference is substantial: A100 instances cost 3-4× more than T4
instances for identical latency. Inference deployments should match
batch size to accelerator characteristics, using high-compute
accelerators only for batched serving where arithmetic intensity exceeds
ridge points.

\paragraph*{\texorpdfstring{Pitfall: \emph{Vendor-specific optimizations
without considering long-term
portability.}}{Pitfall: Vendor-specific optimizations without considering long-term portability.}}\label{pitfall-vendor-specific-optimizations-without-considering-long-term-portability.}
\addcontentsline{toc}{paragraph}{Pitfall: \emph{Vendor-specific
optimizations without considering long-term portability.}}

Organizations optimize exclusively for specific vendors to maximize
performance without considering system flexibility. As discussed in
Section~\ref{sec-ai-acceleration-compiler-support-172e}, deep
integration with vendor-specific libraries (CUDA, TensorRT, XLA) and
custom kernels creates lock-in. A codebase with 50+ hand-written CUDA
kernels requires 6-12 engineer-months to port to a different accelerator
vendor, delaying hardware upgrades and preventing multi-vendor
deployments. While vendor-specific optimizations provide 20-40\%
performance gains, they should be isolated behind hardware abstraction
layers. Maintaining portable code paths enables vendor competition,
hardware flexibility, and faster adoption of emerging accelerators while
still capturing most performance benefits through framework-level
optimizations.

\phantomsection\label{callout-checkpointux2a-1.29}
\begin{fbx}{callout-checkpoint}{Checkpoint:}{Feasibility Assessment: Can You Run It?}
\phantomsection\label{callout-checkpoint*-1.29}
Before procuring hardware, validate feasibility by calculating these
hard constraints:

\textbf{1. Memory Capacity Check} * \textbf{Formula}:
\(M_{req} = \text{Weights} + \text{KV Cache} + \text{Activation Buffer}\)
* \textbf{Constraint}: \(M_{req} < M_{device}\) * \textbf{Example}:
Running Llama-7B (14GB weights @ FP16) on a 16GB GPU leaves only
\texttt{\{python\}\ headroom\_str}GB for context. Long prompts will OOM
(Out of Memory).

\textbf{2. Bandwidth Check} * \textbf{Formula}:
\(T_{token} = \frac{\text{Model Size}}{\text{Memory Bandwidth}}\) *
\textbf{Constraint}: \(T_{token} < \text{Latency Target}\) *
\textbf{Example}: Serving a 70B model (140GB) on a GPU with 1TB/s
bandwidth yields \(\approx `{python} token_latency_ms_str`\)ms per
token. If you need 50ms latency, this hardware fails regardless of
compute power.

\textbf{3. Compute Check} * \textbf{Formula}:
\(T_{process} = \frac{\text{Ops}}{\text{Peak FLOPS} \times \text{Utilization}}\)
* \textbf{Constraint}: \(T_{process} < \text{Throughput Target}\) *
\textbf{Example}: Processing video at 30 FPS requires completing all
inference within \texttt{\{python\}\ frame\_budget\_str}ms.

\end{fbx}

This checklist synthesizes the principles developed throughout this
chapter, translating theoretical understanding into practical
engineering decisions.

Beyond performance optimization, hardware decisions carry broader
implications. As AI systems scale to planetary deployment, the
environmental impact of these choices demands consideration alongside
traditional metrics.

\section{Hardware Responsibility: The Sustainability
Dimension}\label{sec-ai-acceleration-sustainability}

Beyond raw performance, we must evaluate hardware through the lens of
\textbf{Silicon Sustainability}. In the era of planetary-scale AI,
``Performance per Watt'' is not merely a mobile constraint but a global
environmental mandate. Quantifying \emph{the carbon ROI of specialized
silicon} makes the case concrete.

\phantomsection\label{callout-notebookux2a-1.30}
\begin{fbx}{callout-notebook}{AI Engineer’s Notebook:}{The Carbon ROI of Specialized Silicon}
\phantomsection\label{callout-notebook*-1.30}
\textbf{The Problem}: Should you run your inference fleet on generic
\textbf{CPUs} or invest in specialized \textbf{NPUs} (Neural Processing
Units)?

\textbf{The Physics}: Specialized hardware achieves higher
\textbf{Arithmetic Intensity} while using fewer transistors for control
logic.

\begin{itemize}
\tightlist
\item
  \textbf{CPU Inference}: 100 Watts for 1 TFLOP (Efficiency = 0.01
  TFLOPS/W).
\item
  \textbf{NPU Inference}: 5 Watts for 10 TFLOPS (Efficiency = 2.0
  TFLOPS/W).
\item
  \textbf{The Gap}: The NPU is \textbf{200x more energy-efficient} per
  operation.
\end{itemize}

\textbf{The Calculation}:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Workload}: 1 Billion inferences per day.
\item
  \textbf{CPU Energy}: \textasciitilde2,400 kWh/day.
\item
  \textbf{NPU Energy}: \textasciitilde12 kWh/day.
\item
  \textbf{Carbon Savings}: At 0.4 kg CO2/kWh, switching to NPUs saves
  \textbf{\textasciitilde350 metric tons of CO2 per year}.
\end{enumerate}

\textbf{The Systems Conclusion}: Custom silicon is the ultimate
``Green'' technology for ML. Investing in specialized accelerators like
the \textbf{Lighthouse NPU} isn't just about speed; it is the single
most effective way to reduce the carbon footprint of intelligence.

\end{fbx}

\section{Summary}\label{sec-ai-acceleration-summary-a5f8}

The preceding sections established a decision framework for hardware
selection and a sustainability perspective grounding these choices in
broader responsibility. Hardware acceleration has emerged as the enabler
that transforms machine learning from academic curiosity to practical
reality, reshaping how we design both computational systems and the
algorithms that run on them. The evolution from general-purpose
processors to specialized AI accelerators reflects a shift toward
domain-specific computing where hardware and software are co-designed to
optimize specific computational patterns. The journey from CPUs through
GPUs to specialized TPUs, NPUs, and wafer-scale systems demonstrates how
understanding workload characteristics drives architectural innovation,
creating opportunities for orders-of-magnitude performance improvements
through targeted specialization.

The technical challenges of AI acceleration span multiple layers of the
computing stack, from low-level memory hierarchy optimization to
high-level compiler transformations and runtime orchestration. Memory
bandwidth limitations create bottlenecks that require sophisticated
techniques like data tiling, kernel fusion, and hierarchy-aware
scheduling to overcome. Mapping neural network computations to hardware
involves complex trade-offs between different dataflow patterns, memory
allocation strategies, and execution scheduling approaches that must
balance computational efficiency with resource utilization.

Building on these foundational concepts, the emergence of multi-chip and
distributed acceleration systems introduces additional complexities
around communication overhead, memory coherence, and workload
partitioning that require careful system-level optimization.

\phantomsection\label{callout-takeawaysux2a-1.31}
\begin{fbx}{callout-takeaways}{Takeaways:}{Key Takeaways}
\phantomsection\label{callout-takeaways*-1.31}

\begin{itemize}
\item
  \textbf{The Roofline model identifies performance bottlenecks}:
  Plotting arithmetic intensity against throughput reveals whether
  workloads are memory-bound (attention, embeddings) requiring bandwidth
  optimization, or compute-bound (convolutions, GEMMs) requiring FLOPS
  optimization.
\item
  \textbf{Memory bandwidth constrains performance}: GPU compute capacity
  has grown orders of magnitude faster than memory bandwidth over the
  past two decades. Most inference workloads are memory-bound, making
  data movement optimization the primary concern.
\item
  \textbf{Hardware-software co-design can achieve 10--100× performance
  improvements}: Matching algorithm patterns to architectural
  capabilities (systolic arrays for dense GEMM, sparse accelerators for
  pruned models) typically outperforms raw hardware upgrades.
\item
  \textbf{Tensor Cores require specific conditions}: FP16 inputs,
  appropriate tensor dimensions, and sufficient batch size are necessary
  for peak utilization. Batch size directly affects arithmetic intensity
  and determines whether workloads reach the compute-bound regime.
\item
  \textbf{Arithmetic intensity determines optimization strategy}:
  Operations with low arithmetic intensity (1--2 FLOP/byte, like
  LayerNorm) are memory-bound; operations with high intensity (50--200
  FLOP/byte, like convolutions) are compute-bound. The ridge point
  (e.g., \texttt{\{python\}\ a100\_ridge} FLOP/byte for A100) marks the
  transition.
\end{itemize}

\end{fbx}

Engineers who internalize the Roofline model and arithmetic intensity
analysis gain a powerful diagnostic framework: when inference runs
slower than expected, they can immediately determine whether the
bottleneck lies in compute throughput, memory bandwidth, or software
overhead, and then select the appropriate optimization strategy. This
systems-level understanding transforms hardware selection from vendor
comparison into principled engineering.

\phantomsection\label{callout-chapter-connectionux2a-1.32}
\begin{fbxSimple}{callout-chapter-connection}{What’s Next:}{From Optimization to Validation}
\phantomsection\label{callout-chapter-connection*-1.32}
We have now optimized the full DAM stack: data selection minimized
training requirements, model compression reduced algorithmic complexity,
and hardware acceleration maximized machine throughput. But optimization
without measurement is guesswork. In \textbf{?@sec-benchmarking-ai}, we
move from theoretical FLOPs to measured latency, applying the Roofline
Model and statistical methods to validate our optimization claims
against reality.

\end{fbxSimple}

\phantomsection\label{refs}
\begin{CSLReferences}{1}{0}
\bibitem[\citeproctext]{ref-chen_tvmlang_2018}
0001, Tianqi Chen, Thierry Moreau, Ziheng Jiang, Lianmin Zheng, Eddie Q.
Yan, Haichen Shen, Meghan Cowan, et al. 2018b. {``TVM: An Automated
End-to-End Optimizing Compiler for Deep Learning.''} In \emph{13th
USENIX Symposium on Operating Systems Design and Implementation (OSDI
18)}, 578--94.
\url{https://www.usenix.org/conference/osdi18/presentation/chen}.

\bibitem[\citeproctext]{ref-Chen2018}
---------, et al. 2018c. {``TVM: An Automated End-to-End Optimizing
Compiler for Deep Learning.''} In \emph{OSDI}, 578--94.
\url{https://www.usenix.org/conference/osdi18/presentation/chen}.

\bibitem[\citeproctext]{ref-chen2018tvm}
---------, et al. 2018a. {``TVM: An Automated End-to-End Optimizing
Compiler for Deep Learning.''} In \emph{OSDI}, 578--94.
\url{https://www.usenix.org/conference/osdi18/presentation/chen}.

\bibitem[\citeproctext]{ref-abadi2016tensorflow}
Abadi, Martín, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen,
Craig Citro, Greg S. Corrado, et al. 2016. {``TensorFlow: Large-Scale
Machine Learning on Heterogeneous Distributed Systems.''} \emph{arXiv
Preprint arXiv:1603.04467}, March.
\url{http://arxiv.org/abs/1603.04467v2}.

\bibitem[\citeproctext]{ref-amdahl1967validity}
Amdahl, Gene M. 1967. {``Validity of the Single Processor Approach to
Achieving Large Scale Computing Capabilities.''} In \emph{Proceedings of
the April 18-20, 1967, Spring Joint Computer Conference on - AFIPS '67
(Spring)}, 483. AFIPS '67 (Spring). New York, NY, USA: ACM Press.
\url{https://doi.org/10.1145/1465482.1465560}.

\bibitem[\citeproctext]{ref-Taylor2017ASICMining}
Bedford Taylor, Michael. 2017. {``The Evolution of Bitcoin Hardware.''}
\emph{Computer} 50 (9): 58--66.
\url{https://doi.org/10.1109/mc.2017.3571056}.

\bibitem[\citeproctext]{ref-tensorflow_xla_2020}
Brain, Google. 2020. {``XLA: Optimizing Compiler for Machine
Learning.''} \emph{TensorFlow Blog}.
\url{https://www.tensorflow.org/xla}.

\bibitem[\citeproctext]{ref-tensorflow2022}
---------. 2022. \emph{TensorFlow Documentation}.
\url{https://www.tensorflow.org/}.

\bibitem[\citeproctext]{ref-Brown2020}
Brown, Tom B., Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan,
Prafulla Dhariwal, Arvind Neelakantan, et al. 2020. {``Language Models
Are Few-Shot Learners.''} \emph{NeurIPS}, May.
\url{http://arxiv.org/abs/2005.14165v4}.

\bibitem[\citeproctext]{ref-ieee_spectrum_relu}
Cass, Stephen. 2020. {``The History of the ReLU.''} \emph{IEEE
Spectrum}. \url{https://spectrum.ieee.org/the-history-of-the-relu}.

\bibitem[\citeproctext]{ref-Chen2016}
Chen, Yu-Hsin, Joel Emer, and Vivienne Sze. 2017. {``Using Dataflow to
Optimize Energy Efficiency of Deep Neural Network Accelerators.''}
\emph{IEEE Micro} 37 (3): 12--21.
\url{https://doi.org/10.1109/mm.2017.54}.

\bibitem[\citeproctext]{ref-chen2016eyeriss}
Chen, Yu-Hsin, Tushar Krishna, Joel S. Emer, and Vivienne Sze. 2016.
{``Eyeriss: A Spatial Architecture for Energy-Efficient Dataflow for
Convolutional Neural Networks.''} \emph{IEEE Journal of Solid-State
Circuits} 51 (1): 186--98.
\url{https://doi.org/10.1109/JSSC.2015.2488709}.

\bibitem[\citeproctext]{ref-chetlur2014cudnn}
Chetlur, Sharan, Cliff Woolley, Philippe Vandermersch, Jonathan Cohen,
John Tran, Bryan Catanzaro, and Evan Shelhamer. 2014. {``cuDNN:
Efficient Primitives for Deep Learning.''} \emph{arXiv Preprint
arXiv:1410.0759}, October. \url{http://arxiv.org/abs/1410.0759v3}.

\bibitem[\citeproctext]{ref-nvidia2022h100}
Choquette, Jack. 2023. {``NVIDIA Hopper H100 GPU: Scaling
Performance.''} \emph{IEEE Micro} 43 (3): 9--17.
\url{https://doi.org/10.1109/mm.2023.3256796}.

\bibitem[\citeproctext]{ref-intel2021amx}
Corporation, Intel. 2021. {``Intel Advanced Matrix Extensions (Intel
AMX).''} In \emph{Intel Architecture Instruction Set Extensions
Programming Reference}. Intel
Corporation.\href{\%0A\%20\%20\%20\%20https://software.intel.com/content/www/us/en/develop/download/intel-architecture-instruction-set-extensions-programming-reference.html\%0A\%20\%20}{https://software.intel.com/content/www/us/en/develop/download/intel-architecture-instruction-set-extensions-programming-reference.html
}.

\bibitem[\citeproctext]{ref-nvidia2017gpu}
Corporation, NVIDIA. 2017. {``NVIDIA Tesla V100 GPU Architecture.''}
NVIDIA Whitepaper.
\url{https://images.nvidia.com/content/volta-architecture/pdf/volta-architecture-whitepaper.pdf}.

\bibitem[\citeproctext]{ref-nvidia2020ampere}
---------. 2020. {``NVIDIA A100 Tensor Core GPU Architecture.''} NVIDIA
Whitepaper.\href{\%0A\%20\%20\%20\%20https://images.nvidia.com/aem-dam/en-zz/Solutions/data-center/nvidia-ampere-architecture-whitepaper.pdf\%0A\%20\%20}{https://images.nvidia.com/aem-dam/en-zz/Solutions/data-center/nvidia-ampere-architecture-whitepaper.pdf
}.

\bibitem[\citeproctext]{ref-nvidia2021cudnn}
---------. 2021. \emph{NVIDIA cuDNN Developer Guide}. NVIDIA
Corporation.
\url{https://docs.nvidia.com/deeplearning/cudnn/developer-guide/index.html}.

\bibitem[\citeproctext]{ref-Lauterbach2019}
Costa, Tiago, Chen Shi, Kevin Tien, and Kenneth L. Shepard. 2019. {``A
CMOS 2D Transmit Beamformer with Integrated PZT Ultrasound Transducers
for Neuromodulation.''} In \emph{2019 IEEE Custom Integrated Circuits
Conference (CICC)}, 1--4. IEEE.
\url{https://doi.org/10.1109/cicc.2019.8780236}.

\bibitem[\citeproctext]{ref-cui_mlcompilers_2019}
Cui, Hongyi, Jiajun Li, and Peng et al. Xie. 2019. {``A Survey on
Machine Learning Compilers: Taxonomy, Challenges, and Future
Directions.''} \emph{ACM Computing Surveys} 52 (4): 1--39.

\bibitem[\citeproctext]{ref-dao2022flashattention}
Dao, Tri, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher Ré.
2022. {``FlashAttention: Fast and Memory-Efficient Exact Attention with
IO-Awareness.''} In \emph{Advances in Neural Information Processing
Systems 35 (NeurIPS 2022)}, 16344--59. Curran Associates,
Inc.\href{\%0A\%20\%20\%20\%20https://proceedings.neurips.cc/paper/_files/paper/2022/hash/67d57c32e20fd0a7a302cb81d36e40d5-Abstract-Conference.html\%0A\%20\%20}{https://proceedings.neurips.cc/paper\textbackslash\_files/paper/2022/hash/67d57c32e20fd0a7a302cb81d36e40d5-Abstract-Conference.html
}.

\bibitem[\citeproctext]{ref-dennard1974design}
Dennard, R. H., F. H. Gaensslen, Hwa-Nien Yu, V. L. Rideout, E. Bassous,
and A. R. LeBlanc. 1974. {``Design of Ion-Implanted MOSFET's with Very
Small Physical Dimensions.''} \emph{IEEE Journal of Solid-State
Circuits} 9 (5): 256--68.
\url{https://doi.org/10.1109/jssc.1974.1050511}.

\bibitem[\citeproctext]{ref-Dosovitskiy2020ViT}
Dosovitskiy, Alexey, Lucas Beyer, Alexander Kolesnikov, Dirk
Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, et al.
2020. {``An Image Is Worth 16x16 Words: Transformers for Image
Recognition at Scale.''} \emph{International Conference on Learning
Representations (ICLR)}, October.
\url{http://arxiv.org/abs/2010.11929v2}.

\bibitem[\citeproctext]{ref-esmaeilzadeh2011dark}
Esmaeilzadeh, Hadi, Emily Blem, Renee St. Amant, Karthikeyan
Sankaralingam, and Doug Burger. 2011. {``Dark Silicon and the End of
Multicore Scaling.''} In \emph{Proceedings of the 38th Annual
International Symposium on Computer Architecture}, 365--76. IEEE; ACM.
\url{https://doi.org/10.1145/2000064.2000108}.

\bibitem[\citeproctext]{ref-fisher_8087_1981}
Fisher, Lawrence D. 1981. {``The 8087 Numeric Data Processor.''}
\emph{IEEE Computer} 14 (7): 19--29.
\url{https://doi.org/10.1109/MC.1981.1653991}.

\bibitem[\citeproctext]{ref-gholami2024ai}
Gholami, Amir, Zhewei Yao, Sehoon Kim, Coleman Hooper, Michael W.
Mahoney, and Kurt Keutzer. 2024. {``AI and Memory Wall.''} \emph{IEEE
Micro} 44 (3): 33--39. \url{https://doi.org/10.1109/mm.2024.3373763}.

\bibitem[\citeproctext]{ref-Goldberg1991}
Goldberg, David. 1991. {``What Every Computer Scientist Should Know
about Floating-Point Arithmetic.''} \emph{ACM Computing Surveys} 23 (1):
5--48. \url{https://doi.org/10.1145/103162.103163}.

\bibitem[\citeproctext]{ref-Golub1996Matrix}
Golub, Gene H., and Charles F. Van Loan. 1996. \emph{Matrix
Computations}. Johns Hopkins University Press.

\bibitem[\citeproctext]{ref-Goodfellow-et-al-2016}
Goodfellow, Ian J., Aaron Courville, and Yoshua Bengio. 2013. {``Scaling
up Spike-and-Slab Models for Unsupervised Feature Learning.''}
\emph{IEEE Transactions on Pattern Analysis and Machine Intelligence} 35
(8): 1902--14. \url{https://doi.org/10.1109/tpami.2012.273}.

\bibitem[\citeproctext]{ref-GoogleXLA}
Google. 2025. {``XLA: Optimizing Compiler for Machine Learning.''}
\url{https://tensorflow.org/xla}.

\bibitem[\citeproctext]{ref-Graphcore2020}
Graphcore. 2020. {``The Colossus MK2 IPU Processor.''} \emph{Graphcore
Technical Paper}.

\bibitem[\citeproctext]{ref-han2016eie}
Han, Song, Xingyu Liu, Huizi Mao, Jing Pu, Ardavan Pedram, Mark A.
Horowitz, and William J. Dally. 2016. {``EIE: Efficient Inference Engine
on Compressed Deep Neural Network.''} In \emph{2016 ACM/IEEE 43rd Annual
International Symposium on Computer Architecture (ISCA)}, 243--54. IEEE.
\url{https://doi.org/10.1109/isca.2016.30}.

\bibitem[\citeproctext]{ref-xla2020}
He, Xuzhen. 2023a. {``Accelerated Linear Algebra Compiler for
Computationally Efficient Numerical Models: Success and Potential Area
of Improvement.''} \emph{PLOS ONE} 18 (2): e0282265.
\url{https://doi.org/10.1371/journal.pone.0282265}.

\bibitem[\citeproctext]{ref-xla2021}
---------. 2023b. {``Accelerated Linear Algebra Compiler for
Computationally Efficient Numerical Models: Success and Potential Area
of Improvement.''} \emph{PLOS ONE} 18 (2): e0282265.
\url{https://doi.org/10.1371/journal.pone.0282265}.

\bibitem[\citeproctext]{ref-HennessyPatterson2017Turing}
Hennessy, John L., and David A. Patterson. 2019. {``A New Golden Age for
Computer Architecture.''} \emph{Communications of the ACM} 62 (2):
48--60. \url{https://doi.org/10.1145/3282307}.

\bibitem[\citeproctext]{ref-Horowitz2014}
Horowitz, Mark. 2014a. {``1.1 Computing's Energy Problem (and What We
Can Do about It).''} In \emph{2014 IEEE International Solid-State
Circuits Conference Digest of Technical Papers (ISSCC)}, 10--14. IEEE.
\url{https://doi.org/10.1109/isscc.2014.6757323}.

\bibitem[\citeproctext]{ref-horowitz2014computing}
---------. 2014b. {``1.1 Computing's Energy Problem (and What We Can Do
about It).''} In \emph{2014 IEEE International Solid-State Circuits
Conference Digest of Technical Papers (ISSCC)}, 10--14. IEEE; IEEE.
\url{https://doi.org/10.1109/isscc.2014.6757323}.

\bibitem[\citeproctext]{ref-deepmind_gpipe_2019}
Huang, Yanping et al. 2019. {``GPipe: Efficient Training of Giant Neural
Networks Using Pipeline Parallelism.''} In \emph{Advances in Neural
Information Processing Systems (NeurIPS)}.

\bibitem[\citeproctext]{ref-Hwu2011GPU}
Hwu, Wen-mei W. 2011. {``Introduction.''} In \emph{GPU Computing Gems
Emerald Edition}, xix--xx. Elsevier.
\url{https://doi.org/10.1016/b978-0-12-384988-5.00064-4}.

\bibitem[\citeproctext]{ref-oneDNN2021}
Intel, Corporation. 2021. \emph{oneDNN: Intel's Deep Learning Neural
Network Library}. \url{https://github.com/oneapi-src/oneDNN}.

\bibitem[\citeproctext]{ref-Ioffe2015}
Ioffe, Sergey, and Christian Szegedy. 2015. {``Batch Normalization:
Accelerating Deep Network Training by Reducing Internal Covariate
Shift.''} \emph{International Conference on Machine Learning (ICML)},
February, 448--56. \url{http://arxiv.org/abs/1502.03167v3}.

\bibitem[\citeproctext]{ref-jia2018beyond}
Jia, Zhihao, Matei Zaharia, and Alex Aiken. 2018. {``Beyond Data and
Model Parallelism for Deep Neural Networks.''} \emph{arXiv Preprint
arXiv:1807.05358}, July. \url{http://arxiv.org/abs/1807.05358v1}.

\bibitem[\citeproctext]{ref-Jia2019}
Jia, Ziheng, Nathan Tillman, Luis Vega, Po-An Ouyang, Matei Zaharia, and
Joseph E. Gonzalez. 2019. {``Optimizing DNN Computation with Relaxed
Graph Substitutions.''} \emph{Conference on Machine Learning and Systems
(MLSys)}.

\bibitem[\citeproctext]{ref-moreau2018relay}
Jones, Gareth A. 2018. {``Joining Dessins Together.''} \emph{arXiv
Preprint arXiv:1810.03960}, October.
\url{http://arxiv.org/abs/1810.03960v1}.

\bibitem[\citeproctext]{ref-jordan1982guide}
Jordan, T. L. 1982. {``A Guide to Parallel Computation and Some Cray-1
Experiences.''} In \emph{Parallel Computations}, 1--50. Elsevier.
\url{https://doi.org/10.1016/b978-0-12-592101-5.50006-3}.

\bibitem[\citeproctext]{ref-google_tpu_2017}
Jouppi, Norman P et al. 2017a. {``In-Datacenter Performance Analysis of
a Tensor Processing Unit.''} \emph{Proceedings of the 44th Annual
International Symposium on Computer Architecture (ISCA)}.

\bibitem[\citeproctext]{ref-jouppi2021ten}
Jouppi, Norman P., Doe Hyun Yoon, Matthew Ashcraft, Mark Gottscho,
Thomas B. Jablin, George Kurian, James Laudon, et al. 2021. {``Ten
Lessons from Three Generations Shaped Google's TPUv4i : Industrial
Product.''} In \emph{2021 ACM/IEEE 48th Annual International Symposium
on Computer Architecture (ISCA)}, 64:1--14. 5. IEEE.
\url{https://doi.org/10.1109/isca52012.2021.00010}.

\bibitem[\citeproctext]{ref-jouppi_tpu_2017}
Jouppi, Norman P., Cliff Young, Nishant Patil, David Patterson, Gaurav
Agrawal, Raminder Bajwa, Sarah Bates, et al. 2017c. {``In-Datacenter
Performance Analysis of a Tensor Processing Unit.''} In
\emph{Proceedings of the 44th Annual International Symposium on Computer
Architecture}, 1--12. ACM.
\url{https://doi.org/10.1145/3079856.3080246}.

\bibitem[\citeproctext]{ref-jouppi2017datacenter}
---------, et al. 2017b. {``In-Datacenter Performance Analysis of a
Tensor Processing Unit.''} In \emph{Proceedings of the 44th Annual
International Symposium on Computer Architecture}, 1--12. ISCA '17. New
York, NY, USA: ACM. \url{https://doi.org/10.1145/3079856.3080246}.

\bibitem[\citeproctext]{ref-krizhevsky2012alexnet}
Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E. Hinton. 2017.
{``ImageNet Classification with Deep Convolutional Neural Networks.''}
\emph{Communications of the ACM} 60 (6): 84--90.
\url{https://doi.org/10.1145/3065386}.

\bibitem[\citeproctext]{ref-Kung1982}
Kung. 1982. {``Why Systolic Architectures?''} \emph{Computer} 15 (1):
37--46. \url{https://doi.org/10.1109/mc.1982.1653825}.

\bibitem[\citeproctext]{ref-kung1979systolic}
Kung, Hsiang Tsung, and Charles E Leiserson. 1979. {``Systolic Arrays
(for VLSI).''} In \emph{Sparse Matrix Proceedings 1978}, 1:256--82.
Society for industrial; applied mathematics Philadelphia, PA, USA.

\bibitem[\citeproctext]{ref-lam1991cache}
Lam, Monica D., Edward E. Rothberg, and Michael E. Wolf. 1991. {``The
Cache Performance and Optimizations of Blocked Algorithms.''} In
\emph{Proceedings of the Fourth International Conference on
Architectural Support for Programming Languages and Operating Systems -
ASPLOS-IV}, 63--74. ACM Press.
\url{https://doi.org/10.1145/106972.106981}.

\bibitem[\citeproctext]{ref-mlir_framework_2021}
Lattner, Chris, Mehdi Amini, Uday Bondhugula, Albert Cohen, Andy Davis,
Jacques Pienaar, River Riddle, Tatiana Shpeisman, Nicolas Vasilache, and
Oleksandr Zinenko. 2020. {``MLIR: A Compiler Infrastructure for the End
of Moore's Law.''} \emph{arXiv Preprint arXiv:2002.11054}, February.
\url{http://arxiv.org/abs/2002.11054v2}.

\bibitem[\citeproctext]{ref-lindholm2008nvidia}
Lindholm, Erik, John Nickolls, Stuart Oberman, and John Montrym. 2008.
{``NVIDIA Tesla: A Unified Graphics and Computing Architecture.''}
\emph{IEEE Micro} 28 (2): 39--55.
\url{https://doi.org/10.1109/mm.2008.31}.

\bibitem[\citeproctext]{ref-ARM2020}
Ltd., Arm. 2020. \emph{Arm Cortex-M55 Processor Technical Reference
Manual}. Arm Holdings.
\url{https://developer.arm.com/documentation/101051/latest/}.

\bibitem[\citeproctext]{ref-lyons2011understanding}
Lyons, Richard G. 2011. \emph{Understanding Digital Signal Processing}.
3rd ed. Prentice Hall.

\bibitem[\citeproctext]{ref-mirhoseini_device_placement_2017}
Mirhoseini, Azalia et al. 2017. {``Device Placement Optimization with
Reinforcement Learning.''} \emph{International Conference on Machine
Learning (ICML)}.

\bibitem[\citeproctext]{ref-Narayanan2021}
Narayanan, Deepak, Mohammad Shoeybi, Jared Casper, Patrick LeGresley,
Mostofa Patwary, Vijay Anand Korthikanti, Dmitri Vainbrand, et al. 2021.
{``Efficient Large-Scale Language Model Training on GPU Clusters Using
Megatron-LM.''} \emph{NeurIPS}, April.
\url{http://arxiv.org/abs/2104.04473v5}.

\bibitem[\citeproctext]{ref-nickolls2008scalable}
Nickolls, John, Ian Buck, Michael Garland, and Kevin Skadron. 2008.
{``Scalable Parallel Programming with CUDA: Is CUDA the Parallel
Programming Model That Application Developers Have Been Waiting For?''}
\emph{Queue} 6 (2): 40--53.
\url{https://doi.org/10.1145/1365490.1365500}.

\bibitem[\citeproctext]{ref-norrie2021design}
Norrie, Thomas, Nishant Patil, Doe Hyun Yoon, George Kurian, Sheng Li,
James Laudon, Cliff Young, Norman Jouppi, and David Patterson. 2021.
{``The Design Process for Google's Training Chips: TPUv2 and TPUv3.''}
\emph{IEEE Micro} 41 (2): 56--63.
\url{https://doi.org/10.1109/mm.2021.3058217}.

\bibitem[\citeproctext]{ref-nvidia_tensorRT_2021}
NVIDIA. 2021. {``TensorRT: High-Performance Deep Learning Inference
Library.''} \emph{NVIDIA Developer Blog}.
\url{https://developer.nvidia.com/tensorrt}.

\bibitem[\citeproctext]{ref-owens2008gpu}
Owens, J. D., M. Houston, D. Luebke, S. Green, J. E. Stone, and J. C.
Phillips. 2008. {``GPU Computing.''} \emph{Proceedings of the IEEE} 96
(5): 879--99. \url{https://doi.org/10.1109/jproc.2008.917757}.

\bibitem[\citeproctext]{ref-palmer_8087_1981}
Palmer, John F. 1980. {``The INTEL® 8087 Numeric Data Processor.''} In
\emph{Proceedings of the May 19-22, 1980, National Computer Conference
on - AFIPS '80}, 887. ACM Press.
\url{https://doi.org/10.1145/1500518.1500674}.

\bibitem[\citeproctext]{ref-paszke2019pytorch}
Paszke, Adam, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury,
Gregory Chanan, Trevor Killeen, et al. 2019. {``PyTorch: An Imperative
Style, High-Performance Deep Learning Library.''} In \emph{Advances in
Neural Information Processing Systems}, 32:8024--35.
\url{https://proceedings.neurips.cc/paper/2019/hash/bdbca288fee7f92f2bfa9f7012727740-Abstract.html}.

\bibitem[\citeproctext]{ref-patterson2021computer}
Patterson, David A, and John L Hennessy. 2021. {``Computer Architecture:
A Quantitative Approach.''} In \emph{Proceedings of the ACM/IEEE
International Symposium on Computer Architecture}. Morgan Kaufmann.

\bibitem[\citeproctext]{ref-Tesla2021}
Quinnell, Eric. 2024. {``Tesla Transport Protocol over Ethernet (TTPoE):
A New Lossy, Exa-Scale Fabric for the Dojo AI Supercomputer.''} In
\emph{2024 IEEE Hot Chips 36 Symposium (HCS)}, 1--23. IEEE.
\url{https://doi.org/10.1109/hcs61935.2024.10664947}.

\bibitem[\citeproctext]{ref-Rajbhandari2020}
Rajbhandari, Samyam, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. 2020.
{``ZeRO: Memory Optimization Towards Training Trillion Parameter
Models.''} In \emph{Proceedings of the International Conference for High
Performance Computing, Networking, Storage and Analysis (SC)}.
\url{https://doi.org/10.5555/3433701.3433721}.

\bibitem[\citeproctext]{ref-Shang2018GenomicsAccel}
Shang, J., G. Wang, and Y. Liu. 2018. {``Accelerating Genomic Data
Analysis with Domain-Specific Architectures.''} \emph{IEEE Transactions
on Computers} 67 (7): 965--78.
\url{https://doi.org/10.1109/TC.2018.2799212}.

\bibitem[\citeproctext]{ref-shazeer2018mesh}
Shazeer, Noam, Youlong Cheng, Niki Parmar, Dustin Tran, Ashish Vaswani,
Penporn Koanantakool, Peter Hawkins, et al. 2018. {``Mesh-TensorFlow:
Deep Learning for Supercomputers.''} \emph{arXiv Preprint
arXiv:1811.02084}, November. \url{http://arxiv.org/abs/1811.02084v1}.

\bibitem[\citeproctext]{ref-shoeybi_megatron_2020}
Shoeybi, Mohammad, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared
Casper, and Bryan Catanzaro. 2019a. {``Megatron-LM: Training
Multi-Billion Parameter Language Models Using Model Parallelism.''}
\emph{arXiv Preprint arXiv:1909.08053}, September.
\url{http://arxiv.org/abs/1909.08053v4}.

\bibitem[\citeproctext]{ref-Shoeybi2019}
---------. 2019b. {``Megatron-LM: Training Multi-Billion Parameter
Language Models Using Model Parallelism.''} \emph{arXiv Preprint
arXiv:1909.08053}, September. \url{http://arxiv.org/abs/1909.08053v4}.

\bibitem[\citeproctext]{ref-Smith1997}
Smith, Steven W. 1997. \emph{The Scientist and Engineer's Guide to
Digital Signal Processing}. California Technical Publishing.
\url{https://www.dspguide.com/}.

\bibitem[\citeproctext]{ref-sodani2017knl}
Sodani, Avinash. 2015. {``Knights Landing (KNL): 2nd Generation Intel®
Xeon Phi Processor.''} In \emph{2015 IEEE Hot Chips 27 Symposium (HCS)},
1--24. IEEE. \url{https://doi.org/10.1109/hotchips.2015.7477467}.

\bibitem[\citeproctext]{ref-stephens2017arm}
Stephens, Nigel, Stuart Biles, Matthias Boettcher, Jacob Eapen, Mbou
Eyole, Giacomo Gabrielli, Matt Horsnell, et al. 2017. {``The ARM
Scalable Vector Extension.''} \emph{IEEE Micro} 37 (2): 26--39.
\url{https://doi.org/10.1109/mm.2017.35}.

\bibitem[\citeproctext]{ref-sullivan2012overview}
Sullivan, Gary J., Jens-Rainer Ohm, Woo-Jin Han, and Thomas Wiegand.
2012. {``Overview of the High Efficiency Video Coding (HEVC)
Standard.''} \emph{IEEE Transactions on Circuits and Systems for Video
Technology} 22 (12): 1649--68.
\url{https://doi.org/10.1109/tcsvt.2012.2221191}.

\bibitem[\citeproctext]{ref-Cerebras2021}
Systems, Cerebras. 2021. {``Wafer-Scale Deep Learning Acceleration with
the Cerebras CS-2.''} \emph{Cerebras Technical Paper}.

\bibitem[\citeproctext]{ref-sze2017efficient}
Sze, Vivienne, Yu-Hsin Chen, Tien-Ju Yang, and Joel Emer. 2017a.
{``Efficient Processing of Deep Neural Networks: A Tutorial and
Survey.''} \emph{Proceedings of the IEEE} 105 (12): 2295--2329.
\url{https://doi.org/10.1109/jproc.2017.2761740}.

\bibitem[\citeproctext]{ref-sze2020efficient}
Sze, Vivienne, Yu-Hsin Chen, Tien-Ju Yang, and Joel S. Emer. 2017b.
{``Efficient Processing of Deep Neural Networks: A Tutorial and
Survey.''} \emph{Proceedings of the IEEE} 105 (12): 2295--2329.
\url{https://doi.org/10.1109/jproc.2017.2761740}.

\bibitem[\citeproctext]{ref-riscv_manual}
Waterman, Andrew, Yunsup Lee, Rimas Avizienis, Henry Cook, David
Patterson, and Krste Asanovic. 2013. {``The RISC-v Instruction Set.''}
In \emph{2013 IEEE Hot Chips 25 Symposium (HCS)}, 1--1. RISC-V
Foundation; IEEE. \url{https://doi.org/10.1109/hotchips.2013.7478332}.

\bibitem[\citeproctext]{ref-williams2009roofline}
Williams, Samuel, Andrew Waterman, and David Patterson. 2009.
{``Roofline: An Insightful Visual Performance Model for Multicore
Architectures.''} \emph{Communications of the ACM} 52 (4): 65--76.
\url{https://doi.org/10.1145/1498765.1498785}.

\bibitem[\citeproctext]{ref-Huang2019}
Xingyu, Huang et al. 2019. {``Addressing the Memory Bottleneck in AI
Accelerators.''} \emph{IEEE Micro}.

\bibitem[\citeproctext]{ref-zhang2020optimizing}
Zhang, Y., J. Li, and H. Ouyang. 2020. {``Optimizing Memory Access for
Deep Learning Workloads.''} \emph{IEEE Transactions on Computer-Aided
Design of Integrated Circuits and Systems} 39 (11): 2345--58.

\bibitem[\citeproctext]{ref-Zheng2020}
Zheng, Lianmin, Ziheng Jia, Yida Gao, Jiacheng Lin, Song Han, Xuehai
Geng, Eric Zhao, and Tianqi Wu. 2020. {``Ansor: Generating
High-Performance Tensor Programs for Deep Learning.''} \emph{USENIX
Symposium on Operating Systems Design and Implementation (OSDI)},
863--79.

\end{CSLReferences}


\backmatter

\clearpage


\end{document}
