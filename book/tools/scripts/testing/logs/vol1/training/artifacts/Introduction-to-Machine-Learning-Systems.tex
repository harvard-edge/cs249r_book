% Options for packages loaded elsewhere
% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode,linktoc=all,pdfwindowui,pdfpagemode=FullScreen,pdfpagelayout=TwoPageRight}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  9pt,
  letterpaper,
  abstract,
  titlepage]{scrbook}
\usepackage{xcolor}
\usepackage[left=1in,marginparwidth=2.0666666666667in,textwidth=4.1333333333333in,marginparsep=0.3in]{geometry}
\usepackage{amsmath,amssymb}
\setcounter{secnumdepth}{3}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
% Make \paragraph and \subparagraph free-standing
\makeatletter
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}{
    \@ifstar
      \xxxParagraphStar
      \xxxParagraphNoStar
  }
  \newcommand{\xxxParagraphStar}[1]{\oldparagraph*{#1}\mbox{}}
  \newcommand{\xxxParagraphNoStar}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}{
    \@ifstar
      \xxxSubParagraphStar
      \xxxSubParagraphNoStar
  }
  \newcommand{\xxxSubParagraphStar}[1]{\oldsubparagraph*{#1}\mbox{}}
  \newcommand{\xxxSubParagraphNoStar}[1]{\oldsubparagraph{#1}\mbox{}}
\fi
\makeatother

\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{242,244,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{#1}}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.82,0.10,0.26}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{#1}}
\newcommand{\BuiltInTok}[1]{\textcolor[rgb]{0.88,0.40,0.10}{#1}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.41,0.45,0.49}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.86,0.20,0.18}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.58,0.00,0.30}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{#1}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.75,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.88,0.40,0.10}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.42,0.37,0.78}{#1}}
\newcommand{\ImportTok}[1]{\textcolor[rgb]{0.00,0.45,0.70}{\textbf{#1}}}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.73,0.49,0.84}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.10,0.10,0.10}{#1}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{#1}}
\newcommand{\RegionMarkerTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.12,0.55,0.30}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{#1}}}}

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\newsavebox\pandoc@box
\newcommand*\pandocbounded[1]{% scales image to fit in text height/width
  \sbox\pandoc@box{#1}%
  \Gscale@div\@tempa{\textheight}{\dimexpr\ht\pandoc@box+\dp\pandoc@box\relax}%
  \Gscale@div\@tempb{\linewidth}{\wd\pandoc@box}%
  \ifdim\@tempb\p@<\@tempa\p@\let\@tempa\@tempb\fi% select the smaller of both
  \ifdim\@tempa\p@<\p@\scalebox{\@tempa}{\usebox\pandoc@box}%
  \else\usebox{\pandoc@box}%
  \fi%
}
% Set default figure placement to htbp
\def\fps@figure{htbp}
\makeatother
% definitions for citeproc citations
\NewDocumentCommand\citeproctext{}{}
\NewDocumentCommand\citeproc{mm}{%
  \begingroup\def\citeproctext{#2}\cite{#1}\endgroup}
\makeatletter
 % allow citations to break across lines
 \let\@cite@ofmt\@firstofone
 % avoid brackets around text for \cite:
 \def\@biblabel#1{}
 \def\@cite#1#2{{#1\if@tempswa , #2\fi}}
\makeatother
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newenvironment{CSLReferences}[2] % #1 hanging-indent, #2 entry-spacing
 {\begin{list}{}{%
  \setlength{\itemindent}{0pt}
  \setlength{\leftmargin}{0pt}
  \setlength{\parsep}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1
   \setlength{\leftmargin}{\cslhangindent}
   \setlength{\itemindent}{-1\cslhangindent}
  \fi
  % set entry spacing
  \setlength{\itemsep}{#2\baselineskip}}}
 {\end{list}}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{\hfill\break\parbox[t]{\linewidth}{\strut\ignorespaces#1\strut}}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}

% =============================================================================
% LATEX HEADER CONFIGURATION FOR MLSYSBOOK PDF
% =============================================================================
% This file contains all LaTeX package imports, custom commands, and styling
% definitions for the PDF output of the Machine Learning Systems textbook.
%
% Key Features:
% - Harvard crimson branding throughout
% - Custom part/chapter/section styling
% - Professional table formatting with colored headers
% - Margin notes with custom styling
% - TikZ-based part dividers
% - Page numbering (Roman for frontmatter, Arabic for mainmatter)
%
% Note: This file is included via _quarto-pdf.yml and affects PDF output only.
% HTML/EPUB styling is handled separately via CSS files.
% =============================================================================

% =============================================================================
% PACKAGE IMPORTS
% =============================================================================

% Layout and positioning
% \usepackage[outercaption, ragged]{sidecap}  % Commented out to make figure captions inline instead of in margin
\usepackage{adjustbox}      % Adjusting box dimensions
\usepackage{afterpage}      % Execute commands after page break
\usepackage{morefloats}     % Increase number of floats
\usepackage{array}          % Enhanced table column formatting
\usepackage{atbegshi}       % Insert content at page beginning
%\usepackage{changepage}     % Change page dimensions mid-document
\usepackage{emptypage}      % Clear headers/footers on empty pages

% Language and text
\usepackage[english]{babel} % English language support
\usepackage{microtype}      % Improved typography and hyphenation

% Captions and floats
\usepackage{caption}
% Caption styling configuration
%\captionsetup[table]{belowskip=5pt}
\captionsetup{format=plain}
\DeclareCaptionLabelFormat{mylabel}{#1
#2:\hspace{1.0ex}}
\DeclareCaptionFont{ninept}{\fontsize{7pt}{8}\selectfont #1}

% Figure captions: Small font, bold label, ragged right
\captionsetup[figure]{labelfont={bf,ninept},labelsep=space,
belowskip=2pt,aboveskip=6pt,labelformat=mylabel,
justification=raggedright,singlelinecheck=false,font={ninept}}

% Table captions: Small font, bold label, ragged right
\captionsetup[table]{belowskip=6pt,labelfont={bf,ninept},labelsep=none,
labelformat=mylabel,justification=raggedright,singlelinecheck=false,font={ninept}}

% Typography fine-tuning
\emergencystretch=5pt       % Allow extra stretch to avoid overfull boxes

% Utility packages
\usepackage{etoolbox}       % For patching commands and environments

% Page layout and headers
\usepackage{fancyhdr}       % Custom headers and footers
\usepackage{geometry}       % Page dimensions and margins

% Graphics and figures
\usepackage{graphicx}       % Include graphics
\usepackage{float}          % Improved float placement
\usepackage[skins,breakable]{tcolorbox} % Coloured and framed text boxes
\tcbset{before upper=\setlength{\parskip}{3pt}}

% Tables
\usepackage{longtable}      % Multi-page tables

% Fonts and typography
\usepackage{fontspec}       % Font selection for LuaLaTeX
\usepackage{mathptmx}       % Times-like math fonts
\usepackage{newpxtext}      % Palatino-like font for body text

% Colors and visual elements
\usepackage[dvipsnames]{xcolor}  % Extended color support
\usepackage{tikz}           % Programmatic graphics
\usetikzlibrary{positioning}
\usetikzlibrary{calc}
\usepackage{tikzpagenodes}  % TikZ positioning relative to page

% Code listings
\usepackage{listings}       % Code highlighting

% Hyperlinks
\usepackage{hyperref}       % Clickable links in PDF

% Conditional logic
\usepackage{ifthen}         % If-then-else commands

% Math symbols
\usepackage{amsmath}        % AMS math extensions
\usepackage{amssymb}        % AMS math symbols
\usepackage{latexsym}       % Additional LaTeX symbols
\usepackage{pifont}         % Zapf Dingbats symbols
\providecommand{\blacklozenge}{\ding{117}}  % Black diamond symbol

% Lists
\usepackage{enumitem}       % Customizable lists

% Margin notes and sidenotes
\usepackage{marginfix}      % Fixes margin note overflow
\usepackage{marginnote}     % Margin notes
\usepackage{sidenotes}      % Academic-style sidenotes
\renewcommand\raggedrightmarginnote{\sloppy}
\renewcommand\raggedleftmarginnote{\sloppy}

% Typography improvements
\usepackage{ragged2e}       % Better ragged text
\usepackage[all]{nowidow}   % Prevent widows and orphans
\usepackage{needspace}      % Ensure minimum space on page

% Section formatting
\usepackage[explicit]{titlesec}  % Custom section titles
\usepackage{tocloft}        % Table of contents formatting

% QR codes and icons
\usepackage{fontawesome5}   % Font Awesome icons
\usepackage{qrcode}         % QR code generation
\qrset{link, height=15mm}

% =============================================================================
% FLOAT CONFIGURATION
% =============================================================================
% Allow more floats per page to handle figure-heavy chapters
\extrafloats{200}
\setcounter{topnumber}{12}       % Max floats at top of page
\setcounter{bottomnumber}{12}    % Max floats at bottom of page
\setcounter{totalnumber}{24}     % Max floats per page
\setcounter{dbltopnumber}{8}     % Max floats at top of two-column page
\renewcommand{\topfraction}{.95}  % Max fraction of page for top floats
\renewcommand{\bottomfraction}{.95}
\renewcommand{\textfraction}{.05}  % Min fraction of page for text
\renewcommand{\floatpagefraction}{.7}  % Min fraction of float page
\renewcommand{\dbltopfraction}{.95}

% Prevent "Float(s) lost" errors by flushing floats more aggressively
\usepackage{placeins}  % Provides \FloatBarrier

% =============================================================================
% COLOR DEFINITIONS
% =============================================================================
% Harvard crimson - primary brand color used throughout
\definecolor{crimson}{HTML}{A51C30}

% Quiz element colors
\definecolor{quiz-question-color1}{RGB}{225,243,248}  % Light blue background
\definecolor{quiz-question-color2}{RGB}{17,158,199}   % Blue border
\definecolor{quiz-answer-color1}{RGB}{250,234,241}    % Light pink background
\definecolor{quiz-answer-color2}{RGB}{152,14,90}      % Magenta border

% =============================================================================
% LIST FORMATTING
% =============================================================================
% Tighter list spacing for academic style
\def\tightlist{}
\setlist{itemsep=1pt, parsep=1pt, topsep=0pt,after={\vspace{0.3\baselineskip}}}
\let\tightlist\relax

\makeatletter
\@ifpackageloaded{framed}{}{\usepackage{framed}}
\@ifpackageloaded{fancyvrb}{}{\usepackage{fancyvrb}}
\makeatother

\makeatletter
%New float "codelisting" has been updated
\AtBeginDocument{%
\floatstyle{ruled}
\newfloat{codelisting}{!htb}{lop}
\floatname{codelisting}{Listing}
\floatplacement{codelisting}{!htb}
\captionsetup[codelisting]{labelfont={bf,ninept},labelformat=mylabel,
  singlelinecheck=false,width=\linewidth,labelsep=none,font={ninept}}%
\renewenvironment{snugshade}{%
   \def\OuterFrameSep{3pt}%
   \def\FrameCommand{\fboxsep=5pt\colorbox{shadecolor}}%
   \MakeFramed{\advance\hsize-\width\FrameRestore}%
   \leftskip 0.5em \rightskip 0.5em%
   \small% decrease font size
   }{\endMakeFramed}%
}
\makeatother

%The space before and after the verbatim environment "Highlighting" has been reduced
\fvset{listparameters=\setlength{\topsep}{0pt}\setlength{\partopsep}{0pt}}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{framesep=0mm,commandchars=\\\{\}}

\makeatletter
\renewcommand\fs@ruled{\def\@fs@cfont{\bfseries}\let\@fs@capt\floatc@ruled
\def\@fs@pre{\hrule height.8pt depth0pt \kern2pt}%
\def\@fs@post{\kern2pt\hrule\relax}%
\def\@fs@mid{\kern2pt\hrule\kern1pt}%space between float and caption
\let\@fs@iftopcapt\iftrue}
\makeatother


% =============================================================================
% HYPHENATION RULES
% =============================================================================
% Explicit hyphenation points for technical terms to avoid bad breaks
\hyphenation{
  light-weight
  light-weight-ed
  de-vel-op-ment
  un-der-stand-ing
  mod-els
  prin-ci-ples
  ex-per-tise
  com-pli-cat-ed
  blue-print
  per‧for‧mance
  com-mu-ni-ca-tion
  par-a-digms
  hy-per-ten-sion
  a-chieved
}

% =============================================================================
% CODE LISTING CONFIGURATION
% =============================================================================
% Settings for code blocks using listings package
\lstset{
breaklines=true,              % Automatic line wrapping
breakatwhitespace=true,       % Break at whitespace only
basicstyle=\ttfamily,         % Monospace font
frame=none,                   % No frame around code
keepspaces=true,              % Preserve spaces
showspaces=false,             % Don't show space characters
showtabs=false,               % Don't show tab characters
columns=flexible,             % Flexible column width
belowskip=0pt,               % Minimal spacing
aboveskip=0pt
}

% =============================================================================
% PAGE GEOMETRY
% =============================================================================
% MIT Press trim size: 7" x 10" (per publisher specifications)
% This is a standard academic textbook format providing good readability
% for technical content with figures and code blocks.
% Wide outer margin accommodates sidenotes/margin notes.
\geometry{
  paperwidth=7in,
  paperheight=10in,
  top=0.875in,
  bottom=0.875in,
  inner=0.875in,              % Inner margin (binding side)
  outer=1.75in,               % Outer margin (includes space for sidenotes)
  footskip=30pt,
  marginparwidth=1.25in,      % Width for margin notes
  twoside                     % Different left/right pages
}

% =============================================================================
% SIDENOTE STYLING
% =============================================================================
% Custom sidenote design with crimson vertical bar
\renewcommand{\thefootnote}{\textcolor{crimson}{\arabic{footnote}}}

% Save original sidenote command
\makeatletter
\@ifundefined{oldsidenote}{
  \let\oldsidenote\sidenote%
}{}
\makeatother

% Redefine sidenote with vertical crimson bar
\renewcommand{\sidenote}[1]{%
  \oldsidenote{%
    \noindent
    \color{crimson!100}                        % Crimson vertical line
    \raisebox{0em}{%
      \rule{0.5pt}{1.5em}                      % Thin vertical line
    }
    \hspace{0.3em}                             % Space after line
    \color{black}                              % Reset text color
    \footnotesize #1                           % Sidenote content
  }%
}

% =============================================================================
% FLOAT HANDLING
% =============================================================================
% Patch LaTeX's output routine to handle float overflow gracefully
% The "Float(s) lost" error occurs in \@doclearpage when \@currlist is not empty
% This patch silently clears pending floats that can't be placed
\makeatletter
\let\orig@doclearpage\@doclearpage
\def\@doclearpage{%
  \ifx\@currlist\@empty\else
    \global\let\@currlist\@empty
    \typeout{Warning: Floats cleared to prevent overflow}%
  \fi
  \orig@doclearpage
}
\makeatother

% Additional safety for structural commands
\let\originalbackmatter\backmatter
\renewcommand{\backmatter}{%
  \clearpage%
  \originalbackmatter%
}

\let\originalfrontmatter\frontmatter
\renewcommand{\frontmatter}{%
  \clearpage%
  \originalfrontmatter%
}

\let\originalmainmatter\mainmatter
\renewcommand{\mainmatter}{%
  \clearpage%
  \originalmainmatter%
}

% =============================================================================
% PAGE HEADERS AND FOOTERS
% =============================================================================
% Ensure chapters use fancy page style (not plain)
\patchcmd{\chapter}{\thispagestyle{plain}}{\thispagestyle{fancy}}{}{}

% Main page style with crimson headers
\pagestyle{fancy}
\fancyhf{}                                              % Clear all
\fancyhead[LE]{\small\color{crimson}\nouppercase{\rightmark}}  % Left even: section
\fancyhead[RO]{\color{crimson}\thepage}                 % Right odd: page number
\fancyhead[LO]{\small\color{crimson}\nouppercase{\leftmark}}   % Left odd: chapter
\fancyhead[RE]{\color{crimson}\thepage}                 % Right even: page number
\renewcommand{\headrulewidth}{0.4pt}                    % Thin header line
\renewcommand{\footrulewidth}{0pt}                      % No footer line

% Plain page style (for chapter openings)
\fancypagestyle{plain}{
  \fancyhf{}
  \fancyfoot[C]{\color{crimson}\thepage}                % Centered page number
  \renewcommand{\headrulewidth}{0pt}
  \renewcommand{\footrulewidth}{0pt}
}

% =============================================================================
% KOMA-SCRIPT FONT ADJUSTMENTS
% =============================================================================
% Apply crimson color to all heading levels
\addtokomafont{disposition}{\rmfamily\color{crimson}}
\addtokomafont{chapter}{\color{crimson}}
\addtokomafont{section}{\color{crimson}}
\addtokomafont{subsection}{\color{crimson}}

% =============================================================================
% ABSTRACT ENVIRONMENT
% =============================================================================
\newenvironment{abstract}{
  \chapter*{\abstractname}
  \addcontentsline{toc}{chapter}{\abstractname}
  \small
}{
  \clearpage
}

% =============================================================================
% HYPERLINK CONFIGURATION
% =============================================================================
% Crimson-colored links throughout, two-page PDF layout
\hypersetup{
  linkcolor=crimson,
  citecolor=crimson,
  urlcolor=crimson,
  pdfpagelayout=TwoPageRight,   % Two-page spread view
  pdfstartview=Fit               % Initial zoom fits page
}

% =============================================================================
% PART SUMMARY SYSTEM
% =============================================================================
% Allows adding descriptive text below part titles
\newcommand{\partsummary}{}     % Empty by default
\newif\ifhaspartsummary%
\haspartsummaryfalse%

\newcommand{\setpartsummary}[1]{%
  \renewcommand{\partsummary}{#1}%
  \haspartsummarytrue%
}

% Additional colors for part page backgrounds
\definecolor{BrownLL}{RGB}{233,222,220}
\definecolor{BlueDD}{RGB}{62,100,125}
\colorlet{BlueDD}{magenta}

% ===============================================================================
% PART STYLING SYSTEM
% ===============================================================================
%
% This system provides three distinct visual styles for book organization:
%
% 1. NUMBERED PARTS (\part{title}) - For main book sections
%    - Roman numerals (I, II, III, etc.) in top right corner
%    - Crimson title with horizontal lines above/below
%    - "Part I" label in sidebar
%    - Used for: foundations, principles, optimization, deployment, etc.
%
% 2. UNNUMBERED PARTS (\part*{title}) - For special sections like "Labs"
%    - Division-style geometric background (left side)
%    - No Roman numerals
%    - Used for: labs section
%
% 3. DIVISIONS (\division{title}) - For major book divisions
%    - Clean geometric background with centered title
%    - Used for: frontmatter, main_content, backmatter
%
% The Lua filter (inject-parts.lua) automatically routes parts by {key:xxx} commands
% to the appropriate LaTeX command based on the key name.
% ===============================================================================

% NUMBERED PARTS: Roman numeral styling for main book sections
\titleformat{\part}[display]
{\thispagestyle{empty}}{}{20pt}{
\begin{tikzpicture}[remember picture,overlay]
%%%
%%
\node[crimson,align=flush right,
inner sep=0,outer sep=0mm,draw=none,%
anchor=east,minimum height=31mm, text width=1.2\textwidth,
yshift=-30mm,font={%
\fontsize{98pt}{104}\selectfont\bfseries}]  (BG) at (current page text area.north east){\thepart};
%
\node[black,inner sep=0mm,draw=none,
anchor=mid,text width=1.2\textwidth,
 minimum height=35mm, align=right,
node distance=7mm,below=of BG,
font={\fontsize{30pt}{34}\selectfont}]
(BGG)  {\hyphenchar\font=-1 \color{black}\MakeUppercase {#1}};
\draw [crimson,line width=3pt] ([yshift=0mm]BGG.north west) -- ([yshift=0mm]BGG.north east);
\draw [crimson,line width=2pt] ([yshift=0mm]BGG.south west) -- ([yshift=0mm]BGG.south east);
%
\node[fill=crimson,text=white,rotate=90,%
anchor=south west,minimum height=15mm,
minimum width=40mm,font={%
\fontsize{20pt}{20}\selectfont\bfseries}](BP)  at
(current page text area.south east)
{{\sffamily Part}~\thepart};
%
\path[red](BP.north west)-|coordinate(PS)(BGG.south west);
%
% Part summary box commented out for cleaner design
% \ifhaspartsummary
% \node[inner sep=4pt,text width=0.7\textwidth,draw=none,fill=BrownLL!40,
% align=justify,font={\fontsize{9pt}{12}\selectfont},anchor=south west]
% at (PS) {\partsummary};
% \fi
\end{tikzpicture}
}[]

\renewcommand{\thepart}{\Roman{part}}

% UNNUMBERED PARTS: Division-style background for special sections
\titleformat{name=\part,numberless}[display]
{\thispagestyle{empty}}{}{20pt}{
\begin{tikzpicture}[remember picture,overlay]
%%%
\coordinate(S1)at([yshift=-200mm]current page.north west);
\draw[draw=none,fill=BlueDD!7](S1)--++(45:16)coordinate(S2)-
|(S2|-current page.north west)--(current page.north west)coordinate(S3)--(S1);
%
\coordinate(E1)at([yshift=-98mm]current page.north west);
\draw[draw=none,fill=BlueDD!15](E1)--(current page.north west)coordinate(E2)
--++(0:98mm)coordinate(E3)--(E1);
%
\coordinate(D1)at([yshift=15mm]current page.south west);
\draw[draw=none,fill=BlueDD!40,opacity=0.5](D1)--++(45:5.5)coordinate(D2)
-|(D2|-current page.north west)--(current page.north west)coordinate(D3)--(D1);
%%%%
\path[red](S2)-|(S2-|current page.east)coordinate(SS2);
%PART
\node[crimson,align=flush right,inner sep=0,outer sep=0mm,draw=none,anchor=south,
font={\fontsize{48pt}{48}\selectfont\bfseries}]  (BG) at ($(S2)!0.5!(SS2)$){\hphantom{Part}};
%%%
\path[green]([yshift=15mm]D2)-|coordinate(TPD)(BG.south east);
\node[inner sep=0mm,draw=none,anchor=south east,%text width=0.9\textwidth,
align=right,font={\fontsize{40pt}{40}\selectfont}]
(BGG) at (TPD)  {\color{crimson}\MakeUppercase {#1}};%\MakeUppercase {}
\end{tikzpicture}
}

% Define \numberedpart command for numbered parts
\newcommand{\numberedpart}[1]{%
\FloatBarrier%  % Flush all pending floats before part break
\clearpage
\thispagestyle{empty}
\stepcounter{part}%
\begin{tikzpicture}[remember picture,overlay]
%%%
%%
\node[crimson,align=flush right,
inner sep=0,outer sep=0mm,draw=none,%
anchor=east,minimum height=31mm, text width=1.2\textwidth,
yshift=-30mm,font={%
\fontsize{98pt}{104}\selectfont\bfseries}]  (BG) at (current page text area.north east){\thepart};
%
\node[black,inner sep=0mm,draw=none,
anchor=mid,text width=1.2\textwidth,
 minimum height=35mm, align=right,
node distance=7mm,below=of BG,
font={\fontsize{30pt}{34}\selectfont}]
(BGG)  {\hyphenchar\font=-1 \color{black}\MakeUppercase {#1}};
\draw [crimson,line width=3pt] ([yshift=0mm]BGG.north west) -- ([yshift=0mm]BGG.north east);
\draw [crimson,line width=2pt] ([yshift=0mm]BGG.south west) -- ([yshift=0mm]BGG.south east);
%
\node[fill=crimson,text=white,rotate=90,%
anchor=south west,minimum height=15mm,
minimum width=40mm,font={%
\fontsize{20pt}{20}\selectfont\bfseries}](BP)  at
(current page text area.south east)
{{\sffamily Part}~\thepart};
%
\path[red](BP.north west)-|coordinate(PS)(BGG.south west);
%
% Part summary box commented out for cleaner design
% \ifhaspartsummary
% \node[inner sep=4pt,text width=0.7\textwidth,draw=none,fill=BrownLL!40,
% align=justify,font={\fontsize{9pt}{12}\selectfont},anchor=south west]
% at (PS) {\partsummary};
% \fi
\end{tikzpicture}
\clearpage
}



% DIVISIONS: Clean geometric styling with subtle tech elements
% Used for frontmatter, main_content, and backmatter divisions
\newcommand{\division}[1]{%
\FloatBarrier%  % Flush all pending floats before division break
\clearpage
\thispagestyle{empty}
\begin{tikzpicture}[remember picture,overlay]

% Clean geometric background (original design)
\coordinate(S1)at([yshift=-200mm]current page.north west);
\draw[draw=none,fill=BlueDD!7](S1)--++(45:16)coordinate(S2)-
|(S2|-current page.north west)--(current page.north west)coordinate(S3)--(S1);

\coordinate(E1)at([yshift=-98mm]current page.north west);
\draw[draw=none,fill=BlueDD!15](E1)--(current page.north west)coordinate(E2)
--++(0:98mm)coordinate(E3)--(E1);

\coordinate(D1)at([yshift=15mm]current page.south west);
\draw[draw=none,fill=BlueDD!40,opacity=0.5](D1)--++(45:5.5)coordinate(D2)
-|(D2|-current page.north west)--(current page.north west)coordinate(D3)--(D1);

% Subtle tech elements - positioned in white areas for better visibility
% Upper right white area - more visible
\draw[crimson!40, line width=0.8pt] ([xshift=140mm,yshift=-60mm]current page.north west) -- ++(40mm,0);
\draw[crimson!40, line width=0.8pt] ([xshift=150mm,yshift=-70mm]current page.north west) -- ++(30mm,0);
\draw[crimson!35, line width=0.7pt] ([xshift=160mm,yshift=-60mm]current page.north west) -- ++(0,-15mm);
\draw[crimson!35, line width=0.7pt] ([xshift=170mm,yshift=-70mm]current page.north west) -- ++(0,10mm);

% Circuit nodes - upper right
\fill[crimson!50] ([xshift=160mm,yshift=-60mm]current page.north west) circle (1.5mm);
\fill[white] ([xshift=160mm,yshift=-60mm]current page.north west) circle (0.8mm);
\fill[crimson!50] ([xshift=170mm,yshift=-70mm]current page.north west) circle (1.3mm);
\fill[white] ([xshift=170mm,yshift=-70mm]current page.north west) circle (0.6mm);

% Lower right white area - enhanced visibility
\draw[crimson!45, line width=0.9pt] ([xshift=140mm,yshift=-190mm]current page.north west) -- ++(45mm,0);
\draw[crimson!45, line width=0.9pt] ([xshift=150mm,yshift=-200mm]current page.north west) -- ++(35mm,0);
\draw[crimson!40, line width=0.8pt] ([xshift=160mm,yshift=-190mm]current page.north west) -- ++(0,-20mm);
\draw[crimson!40, line width=0.8pt] ([xshift=170mm,yshift=-200mm]current page.north west) -- ++(0,15mm);

% Additional connecting lines in lower right
\draw[crimson!35, line width=0.7pt] ([xshift=130mm,yshift=-180mm]current page.north west) -- ++(25mm,0);
\draw[crimson!35, line width=0.7pt] ([xshift=145mm,yshift=-180mm]current page.north west) -- ++(0,-25mm);

% Circuit nodes - lower right (more prominent)
\fill[crimson!55] ([xshift=160mm,yshift=-190mm]current page.north west) circle (1.6mm);
\fill[white] ([xshift=160mm,yshift=-190mm]current page.north west) circle (0.9mm);
\fill[crimson!55] ([xshift=170mm,yshift=-200mm]current page.north west) circle (1.4mm);
\fill[white] ([xshift=170mm,yshift=-200mm]current page.north west) circle (0.7mm);
\fill[crimson!50] ([xshift=145mm,yshift=-180mm]current page.north west) circle (1.2mm);
\fill[white] ([xshift=145mm,yshift=-180mm]current page.north west) circle (0.6mm);

% Title positioned in center - clean and readable
\node[inner sep=0mm,draw=none,anchor=center,text width=0.8\textwidth,
align=center,font={\fontsize{40pt}{40}\selectfont}]
(BGG) at (current page.center)  {\color{crimson}\MakeUppercase {#1}};

\end{tikzpicture}
\clearpage
}

% LAB DIVISIONS: Circuit-style neural network design for lab sections
% Used specifically for lab platform sections (arduino, xiao, grove, etc.)
\newcommand{\labdivision}[1]{%
\FloatBarrier%  % Flush all pending floats before lab division break
\clearpage
\thispagestyle{empty}
\begin{tikzpicture}[remember picture,overlay]
% Circuit background with subtle gradient
\coordinate(S1)at([yshift=-200mm]current page.north west);
\draw[draw=none,fill=BlueDD!5](S1)--++(45:16)coordinate(S2)-
|(S2|-current page.north west)--(current page.north west)coordinate(S3)--(S1);

% TOP AREA: Circuit lines in upper white space
\draw[crimson!50, line width=1.5pt] ([xshift=30mm,yshift=-40mm]current page.north west) -- ++(60mm,0);
\draw[crimson!40, line width=1pt] ([xshift=120mm,yshift=-50mm]current page.north west) -- ++(50mm,0);
\draw[crimson!50, line width=1.5pt] ([xshift=40mm,yshift=-70mm]current page.north west) -- ++(40mm,0);

% Connecting lines in top area
\draw[crimson!30, line width=1pt] ([xshift=60mm,yshift=-40mm]current page.north west) -- ++(0,-20mm);
\draw[crimson!30, line width=1pt] ([xshift=145mm,yshift=-50mm]current page.north west) -- ++(0,10mm);

% Neural nodes in top area
\fill[crimson!70] ([xshift=60mm,yshift=-40mm]current page.north west) circle (2.5mm);
\fill[white] ([xshift=60mm,yshift=-40mm]current page.north west) circle (1.5mm);
\fill[crimson!60] ([xshift=145mm,yshift=-50mm]current page.north west) circle (2mm);
\fill[white] ([xshift=145mm,yshift=-50mm]current page.north west) circle (1mm);
\fill[crimson!80] ([xshift=80mm,yshift=-70mm]current page.north west) circle (2mm);
\fill[white] ([xshift=80mm,yshift=-70mm]current page.north west) circle (1mm);

% BOTTOM AREA: Circuit lines in lower white space
\draw[crimson!50, line width=1.5pt] ([xshift=20mm,yshift=-200mm]current page.north west) -- ++(70mm,0);
\draw[crimson!40, line width=1pt] ([xshift=110mm,yshift=-210mm]current page.north west) -- ++(60mm,0);
\draw[crimson!50, line width=1.5pt] ([xshift=35mm,yshift=-230mm]current page.north west) -- ++(45mm,0);

% Connecting lines in bottom area
\draw[crimson!30, line width=1pt] ([xshift=55mm,yshift=-200mm]current page.north west) -- ++(0,-20mm);
\draw[crimson!30, line width=1pt] ([xshift=140mm,yshift=-210mm]current page.north west) -- ++(0,15mm);

% Neural nodes in bottom area
\fill[crimson!70] ([xshift=55mm,yshift=-200mm]current page.north west) circle (2.5mm);
\fill[white] ([xshift=55mm,yshift=-200mm]current page.north west) circle (1.5mm);
\fill[crimson!60] ([xshift=140mm,yshift=-210mm]current page.north west) circle (2mm);
\fill[white] ([xshift=140mm,yshift=-210mm]current page.north west) circle (1mm);
\fill[crimson!80] ([xshift=80mm,yshift=-230mm]current page.north west) circle (2mm);
\fill[white] ([xshift=80mm,yshift=-230mm]current page.north west) circle (1mm);

% SIDE AREAS: Subtle circuit elements on left and right edges
\draw[crimson!30, line width=1pt] ([xshift=15mm,yshift=-120mm]current page.north west) -- ++(20mm,0);
\draw[crimson!30, line width=1pt] ([xshift=175mm,yshift=-130mm]current page.north west) -- ++(15mm,0);
\fill[crimson!50] ([xshift=25mm,yshift=-120mm]current page.north west) circle (1.5mm);
\fill[white] ([xshift=25mm,yshift=-120mm]current page.north west) circle (0.8mm);
\fill[crimson!50] ([xshift=185mm,yshift=-130mm]current page.north west) circle (1.5mm);
\fill[white] ([xshift=185mm,yshift=-130mm]current page.north west) circle (0.8mm);

% Title positioned in center - CLEAN AREA
\node[inner sep=0mm,draw=none,anchor=center,text width=0.8\textwidth,
align=center,font={\fontsize{44pt}{44}\selectfont\bfseries}]
(BGG) at (current page.center)  {\color{crimson}\MakeUppercase {#1}};

\end{tikzpicture}
\clearpage
}

% Define \lab command for lab styling (different visual treatment)
\newcommand{\lab}[1]{%
\begin{tikzpicture}[remember picture,overlay]
%%%
% Different background pattern for labs
\coordinate(S1)at([yshift=-200mm]current page.north west);
\draw[draw=none,fill=BlueDD!15](S1)--++(45:16)coordinate(S2)-
|(S2|-current page.north west)--(current page.north west)coordinate(S3)--(S1);
%
\coordinate(E1)at([yshift=-98mm]current page.north west);
\draw[draw=none,fill=BlueDD!25](E1)--(current page.north west)coordinate(E2)
--++(0:98mm)coordinate(E3)--(E1);
%
\coordinate(D1)at([yshift=15mm]current page.south west);
\draw[draw=none,fill=BlueDD!60,opacity=0.7](D1)--++(45:5.5)coordinate(D2)
-|(D2|-current page.north west)--(current page.north west)coordinate(D3)--(D1);
%%%%
\path[red](S2)-|(S2-|current page.east)coordinate(SS2);
%LAB - Different styling
\node[crimson,align=flush right,inner sep=0,outer sep=0mm,draw=none,anchor=south,
font={\fontsize{48pt}{48}\selectfont\bfseries}]  (BG) at ($(S2)!0.5!(SS2)$){\hphantom{Workshop}};
%%%
\path[green]([yshift=15mm]D2)-|coordinate(TPD)(BG.south east);
\node[inner sep=0mm,draw=none,anchor=south east,%text width=0.9\textwidth,
align=right,font={\fontsize{40pt}{40}\selectfont}]
(BGG) at (TPD)  {\color{crimson}\MakeUppercase {#1}};%\MakeUppercase {}
\end{tikzpicture}
\thispagestyle{empty}
\clearpage
}

% =============================================================================
% SECTION FORMATTING
% =============================================================================
% All section levels use crimson color and are ragged right

% Section (Large, bold, crimson)
\titleformat{\section}
  {\normalfont\Large\bfseries\color{crimson}\raggedright}
  {\thesection}
  {0.5em}
  {#1}
\titlespacing*{\section}{0pc}{14pt plus 4pt minus 4pt}{6pt plus 2pt minus 2pt}[0pc]

% Subsection (large, bold, crimson)
\titleformat{\subsection}
  {\normalfont\large\bfseries\color{crimson}\raggedright}
  {\thesubsection}
  {0.5em}
  {#1}
\titlespacing*{\subsection}{0pc}{12pt plus 4pt minus 4pt}{5pt plus 1pt minus 2pt}[0pc]

% Subsubsection (normal size, bold, crimson)
\titleformat{\subsubsection}
  {\normalfont\normalsize\bfseries\color{crimson}\raggedright}
  {\thesubsubsection}
  {0.5em}
  {#1}
\titlespacing*{\subsubsection}{0pc}{12pt plus 4pt minus 4pt}{5pt plus 1pt minus 2pt}[0pc]

% Paragraph (run-in, bold, crimson, ends with period)
\titleformat{\paragraph}[runin]
  {\normalfont\normalsize\bfseries\color{crimson}}
  {\theparagraph}
  {0.5em}
  {#1}
  [\textbf{.}]
  \titlespacing*{\paragraph}{0pc}{6pt plus 2pt minus 2pt}{0.5em}[0pc]

% Subparagraph (run-in, italic, crimson, ends with period)
\titleformat{\subparagraph}[runin]
  {\normalfont\normalsize\itshape\color{crimson}}
  {\thesubparagraph}
  {0.5em}
  {#1}
  [\textbf{.}]
  \titlespacing*{\subparagraph}{0pc}{6pt plus 2pt minus 2pt}{0.5em}[0pc]

% =============================================================================
% CHAPTER FORMATTING
% =============================================================================
% Numbered chapters: "Chapter X" prefix, huge crimson title
\titleformat{\chapter}[display]
  {\normalfont\huge\bfseries\color{crimson}}
  {\chaptername\ \thechapter}
  {20pt}
  {\Huge #1}
  []

% Unnumbered chapters: no prefix, huge crimson title
\titleformat{name=\chapter,numberless}
  {\normalfont\huge\bfseries\color{crimson}}
  {}
  {0pt}
  {\Huge #1}
  []

\renewcommand{\chaptername}{Chapter}
% =============================================================================
% TABLE OF CONTENTS FORMATTING
% =============================================================================
\setcounter{tocdepth}{2}                      % Show chapters, sections, subsections

% TOC spacing adjustments for number widths and indentation
\setlength{\cftchapnumwidth}{2em}             % Chapter number width
\setlength{\cftsecnumwidth}{2.75em}           % Section number width
\setlength{\cftsubsecnumwidth}{3.25em}        % Subsection number width
\setlength{\cftsubsubsecnumwidth}{4em}        % Subsubsection number width
\setlength{\cftsubsecindent}{4.25em}          % Subsection indent
\setlength{\cftsubsubsecindent}{7.5em}        % Subsubsection indent

% Chapter entries in TOC: bold crimson with "Chapter" prefix
\renewcommand{\cftchapfont}{\bfseries\color{crimson}}
\renewcommand{\cftchappresnum}{\color{crimson}Chapter~}

% Custom formatting for division entries (styled like parts)
\newcommand{\divisionchapter}[1]{%
  \addvspace{12pt}%
  \noindent\hfil\bfseries\color{crimson}#1\hfil\par%
  \addvspace{6pt}%
}

% Adjust TOC spacing for "Chapter" prefix
\newlength{\xtraspace}
\settowidth{\xtraspace}{\cftchappresnum\cftchapaftersnum}
\addtolength{\cftchapnumwidth}{\xtraspace}

% Unnumbered chapters with TOC entry
\newcommand{\likechapter}[1]{%
    \chapter*{#1}
    \addcontentsline{toc}{chapter}{\textcolor{crimson}{#1}}
}

% =============================================================================
% PAGE NUMBERING SYSTEM
% =============================================================================
% Implements traditional book numbering:
% - Roman numerals (i, ii, iii...) for frontmatter
% - Arabic numerals (1, 2, 3...) for mainmatter
% Automatically switches at first numbered chapter
\makeatletter
\newif\if@firstnumbered%
\@firstnumberedtrue%
\newif\if@firstunnumbered%
\@firstunnumberedtrue%

\newcounter{lastRomanPage}
\setcounter{lastRomanPage}{1}

% Start document with Roman numerals (frontmatter)
\AtBeginDocument{
  \pagenumbering{roman}
  \renewcommand{\thepage}{\roman{page}}
}

% Intercept chapter command
\let\old@chapter\chapter%
\renewcommand{\chapter}{%
  \@ifstar{\unnumbered@chapter}{\numbered@chapter}%
}

% Numbered chapters: switch to Arabic on first occurrence
\newcommand{\numbered@chapter}[1]{%
  \if@firstnumbered%
    \cleardoublepage%
    \setcounter{lastRomanPage}{\value{page}}%
    \pagenumbering{arabic}%
    \@firstnumberedfalse%
  \else
    \setcounter{page}{\value{page}}%
  \fi
  \setcounter{sidenote}{1}                    % Reset footnote counter per chapter
  \old@chapter{#1}%
}

% Unnumbered chapters: stay in Roman numerals
\newcommand{\unnumbered@chapter}[1]{%
  \if@firstunnumbered%
    \clearpage
    \setcounter{lastRomanPage}{\value{page}}%
    \pagenumbering{roman}%
    \@firstunnumberedfalse%
  \fi
  \setcounter{sidenote}{1}
  \old@chapter*{#1}%
}
\makeatother

% =============================================================================
% TABLE SIZING AND SPACING
% =============================================================================
% Make tables slightly smaller to fit more content
\AtBeginEnvironment{longtable}{\scriptsize}

% Increase vertical spacing in table cells (default is 1.0)
\renewcommand{\arraystretch}{1.5}

% Prefer placing figures and tables at the top of pages
\makeatletter
\renewcommand{\fps@figure}{t}  % Default placement: top of page
\renewcommand{\fps@table}{t}   % Default placement: top of page
\makeatother

% =============================================================================
% LONGTABLE PAGE BREAKING FIXES (Windows compatibility)
% =============================================================================
% Prevent "Infinite glue shrinkage" errors on Windows LaTeX builds
% by giving longtable more flexibility in page breaking

% Allow more flexible page breaking (vs strict \flushbottom)
\raggedbottom

% Process more rows before attempting page break (default is 20)
\setcounter{LTchunksize}{50}

% Add extra stretch for longtable environments specifically
\AtBeginEnvironment{longtable}{%
  \setlength{\emergencystretch}{3em}%
  \setlength{\parskip}{0pt plus 1pt}%
}

% =============================================================================
% TABLE STYLING - Clean tables with crimson borders
% =============================================================================
% Professional table appearance with:
% - Clean white background (no colored rows)
% - Crimson-colored borders
% - Good spacing for readability
%
% Note: Headers are automatically bolded by Quarto when using **text** in source
\usepackage{booktabs}      % Professional table rules (\toprule, \midrule, \bottomrule)
\usepackage{colortbl}      % For colored borders (\arrayrulecolor)

% Global table styling - crimson borders
\setlength{\arrayrulewidth}{0.5pt}          % Thinner borders than default
%\arrayrulecolor{crimson}                    % Crimson borders matching brand

\setcounter{chapter}{0}

% =============================================================================
% DROP CAPS (Lettrine)
% =============================================================================
% Decorative large first letter at chapter openings, following the tradition
% of Hennessy & Patterson and other MIT Press textbooks.
% Usage in QMD: \lettrine{T}{he first sentence...}
\usepackage{lettrine}
\renewcommand{\LettrineFontHook}{\color{crimson}\bfseries}
\setcounter{DefaultLines}{3}          % Drop cap spans 3 lines
\renewcommand{\DefaultLoversize}{0.1} % Slight oversize for visual weight
\renewcommand{\DefaultLraise}{0}      % No vertical shift
\setlength{\DefaultNindent}{0.5em}    % Indent of continuation text
\setlength{\DefaultSlope}{0pt}        % No slope on continuation

% =============================================================================
% RUNNING HEADERS — Truncation Safety
% =============================================================================
% Long chapter/section titles can overflow the header. These marks truncate
% gracefully so headers stay within the text block.
\renewcommand{\chaptermark}[1]{%
  \markboth{\thechapter.\ #1}{}}
\renewcommand{\sectionmark}[1]{%
  \markright{\thesection\ #1}}

% =============================================================================
% EPIGRAPH ENVIRONMENT
% =============================================================================
% For chapter-opening quotations. Renders as right-aligned italic block
% with attribution in small caps below.
% Usage: \epigraph{Quote text}{Author Name, \textit{Source}}
\newcommand{\bookepigraph}[2]{%
  \vspace{1em}%
  \begin{flushright}%
    \begin{minipage}{0.75\textwidth}%
      \raggedleft\itshape\small #1\\[0.5em]%
      \normalfont\small --- #2%
    \end{minipage}%
  \end{flushright}%
  \vspace{1.5em}%
}

% =============================================================================
% THUMB INDEX TABS
% =============================================================================
% Colored tabs on the outer page edge for quick chapter navigation.
% Each Part gets a different vertical position; chapters within a Part
% share the same tab position. Visible when flipping through the book.
\newcounter{thumbindex}
\setcounter{thumbindex}{0}
\newlength{\thumbtabheight}
\setlength{\thumbtabheight}{16mm}     % Height of each tab
\newlength{\thumbtabwidth}
\setlength{\thumbtabwidth}{8mm}       % Width protruding from edge
\newlength{\thumbtabgap}
\setlength{\thumbtabgap}{1mm}         % Gap between tabs

% Advance to next thumb tab position (call at each \part)
\newcommand{\nextthumb}{%
  \stepcounter{thumbindex}%
}

% Draw the thumb tab on every page (placed in header via fancyhdr)
\newcommand{\drawthumb}{%
  \ifnum\value{thumbindex}>0%
    \begin{tikzpicture}[remember picture,overlay]
      \pgfmathsetmacro{\thumboffset}{%
        20 + (\value{thumbindex}-1) * (16 + 1)}  % mm from top
      \ifodd\value{page}%
        % Odd pages: tab on right edge
        \fill[crimson!80]
          ([yshift=-\thumboffset mm]current page.north east)
          rectangle +(-\thumbtabwidth, -\thumbtabheight);
        \node[white,font=\tiny\bfseries,rotate=90]
          at ([yshift=-\thumboffset mm - 0.5\thumbtabheight,
               xshift=-0.5\thumbtabwidth]current page.north east)
          {\Roman{thumbindex}};
      \else
        % Even pages: tab on left edge
        \fill[crimson!80]
          ([yshift=-\thumboffset mm]current page.north west)
          rectangle +(\thumbtabwidth, -\thumbtabheight);
        \node[white,font=\tiny\bfseries,rotate=-90]
          at ([yshift=-\thumboffset mm - 0.5\thumbtabheight,
               xshift=0.5\thumbtabwidth]current page.north west)
          {\Roman{thumbindex}};
      \fi
    \end{tikzpicture}%
  \fi
}

% Hook into fancyhdr to draw thumb on every content page
\AddToHook{shipout/foreground}{%
  \drawthumb%
}

% =============================================================================
% CROP / BLEED MARKS
% =============================================================================
% For final print submission, uncomment the line below to add crop marks.
% MIT Press production will advise on exact requirements.
% \usepackage[cam,center,width=7.5in,height=10.5in]{crop}

% =============================================================================
% PDF/A ARCHIVAL COMPLIANCE
% =============================================================================
% MIT Press increasingly requires PDF/A for long-term preservation.
% This embeds all fonts and removes transparency.
% Note: pdfx must be loaded early; if it conflicts with hyperref,
% MIT Press production can handle the conversion post-build.
% Uncomment when ready for final submission:
% \usepackage[a-3u]{pdfx}

% =============================================================================
% ENHANCED WIDOW / ORPHAN CONTROL
% =============================================================================
% Prevent single lines at top/bottom of pages and breaks before equations
\clubpenalty=10000          % No orphans (single first line at bottom)
\widowpenalty=10000         % No widows (single last line at top)
\displaywidowpenalty=10000  % No widow before display math
\predisplaypenalty=10000    % No page break just before display math
\postdisplaypenalty=0       % Allow break after display math (natural)
\usepackage{needspace}
\let\Needspace\needspace
\makeatletter
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{plain}
\@ifundefined{c@chapter}{\newfloat{vid}{h}{lovid}}{\newfloat{vid}{h}{lovid}[chapter]}
\floatname{vid}{Video}
\newcommand*\listofvids{\listof{vid}{List of Videos}}
\makeatother
\makeatletter
\@ifpackageloaded{tcolorbox}{}{\usepackage[skins,breakable]{tcolorbox}}
\@ifpackageloaded{fontawesome5}{}{\usepackage{fontawesome5}}
\definecolor{quarto-callout-color}{HTML}{909090}
\definecolor{quarto-callout-note-color}{HTML}{0758E5}
\definecolor{quarto-callout-important-color}{HTML}{CC1914}
\definecolor{quarto-callout-warning-color}{HTML}{EB9113}
\definecolor{quarto-callout-tip-color}{HTML}{00A047}
\definecolor{quarto-callout-caution-color}{HTML}{FC5300}
\definecolor{quarto-callout-color-frame}{HTML}{acacac}
\definecolor{quarto-callout-note-color-frame}{HTML}{4582ec}
\definecolor{quarto-callout-important-color-frame}{HTML}{d9534f}
\definecolor{quarto-callout-warning-color-frame}{HTML}{f0ad4e}
\definecolor{quarto-callout-tip-color-frame}{HTML}{02b875}
\definecolor{quarto-callout-caution-color-frame}{HTML}{fd7e14}
\makeatother
\makeatletter
\@ifpackageloaded{bookmark}{}{\usepackage{bookmark}}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\makeatletter
\@ifpackageloaded{sidenotes}{}{\usepackage{sidenotes}}
\@ifpackageloaded{marginnote}{}{\usepackage{marginnote}}
\makeatother
\newcommand{\fbxIconPath}{assets/images/icons/callouts}
\newcommand{\fbxIconFormat}{pdf}
\makeatletter
\@ifpackageloaded{tcolorbox}{}{\usepackage[many]{tcolorbox}}
\makeatother
%%%% ---foldboxy preamble ----- %%%%%

% Load xstring for string manipulation
\RequirePackage{xstring}

% Icon path and format configuration - can be overridden in filter-metadata
\providecommand{\fbxIconPath}{assets/images/icons/callouts}
\providecommand{\fbxIconFormat}{pdf}

% Helper command to include icon with hyphen-to-underscore conversion
% This ensures consistency: callout-quiz-question -> callout_quiz_question
\newcommand{\fbxIncludeIcon}[2]{%
  \StrSubstitute{#1}{-}{_}[\fbxIconName]%
  \includegraphics[width=#2]{\fbxIconPath/icon_\fbxIconName.\fbxIconFormat}%
}

% Legacy fallback colors (keep for compatibility)
\definecolor{fbx-default-color1}{HTML}{c7c7d0}
\definecolor{fbx-default-color2}{HTML}{a3a3aa}
\definecolor{fbox-color1}{HTML}{c7c7d0}
\definecolor{fbox-color2}{HTML}{a3a3aa}

% arguments: #1 typelabelnummer: #2 titel: #3
\newenvironment{fbx}[3]{%
\begin{tcolorbox}[
  enhanced,
  breakable,
  %fontupper=\fontsize{8pt}{10pt}\selectfont,  % 95% of body text (10pt -> 9.5pt)
  before skip=8pt,  % space above box (increased)
  after skip=8pt,   % space below box (increased)
  attach boxed title to top*={xshift=0pt},
  boxed title style={
  %fuzzy shadow={1pt}{-1pt}{0mm}{0.1mm}{gray},
  arc=1.5pt,
  rounded corners=north,
  sharp corners=south,
  top=6pt,          % Adjusted for ~40px equivalent height
  bottom=5pt,       % Adjusted for ~40px equivalent height
  overlay={
      \node [left,outer sep=0em, black,draw=none,anchor=west,
        rectangle,fill=none,inner sep=0pt]
        at ([xshift=4mm]frame.west) {\fbxIncludeIcon{#1}{4.2mm}};
    },
  },
  colframe=#1-color2,             % Border color (auto-generated from YAML)
  colbacktitle=#1-color1,         % Background color (auto-generated from YAML)
  colback=white,
  coltitle=black,
  titlerule=0mm,
  toprule=0.5pt,
  bottomrule=0.5pt,
  leftrule=2.2pt,
  rightrule=0.5pt,
  outer arc=1.5pt,
  arc=1.5pt,
  left=0.5em,       % increased left padding
  bottomtitle=1.5mm, % increased title bottom margin
  toptitle=1.5mm,    % increased title top margin
  title=\hspace{2.5em}\protect#2\hspace{0.5em}\protect#3, % Protect parameters
  extras middle and last={top=4pt} % increased continuation spacing
]}
{\end{tcolorbox}}


% boxed environment with right border
\newenvironment{fbxSimple}[3]{\begin{tcolorbox}[
  enhanced,
  breakable,
  %fontupper=\fontsize{8pt}{10pt}\selectfont,  % 95% of body text (10pt -> 9.5pt)
  before skip=8pt,  % space above box (increased)
  after skip=8pt,   % space below box (increased)
  attach boxed title to top*={xshift=0pt},
  boxed title style={
  %fuzzy shadow={1pt}{-1pt}{0mm}{0.1mm}{gray},
  arc=1.5pt,
  rounded corners=north,
  sharp corners=south,
  top=6pt,          % Adjusted for ~40px equivalent height
  bottom=5pt,       % Adjusted for ~40px equivalent height
  overlay={
      \node [left,outer sep=0em, black,draw=none,anchor=west,
        rectangle,fill=none,inner sep=0pt]
        at ([xshift=3mm]frame.west) {\fbxIncludeIcon{#1}{4.2mm}};
    },
  },
  colframe=#1-color2,             % Border color (auto-generated from YAML)
  colbacktitle=#1-color1,         % Background color (auto-generated from YAML)
  colback=white,
  coltitle=black,
  titlerule=0mm,
  toprule=0.5pt,
  bottomrule=0.5pt,
  leftrule=2.2pt,
  rightrule=0.5pt,
  outer arc=1.5pt,
  arc=1.5pt,
  left=0.5em,       % increased left padding
  bottomtitle=1.5mm, % increased title bottom margin
  toptitle=1.5mm,    % increased title top margin
  title=\hspace{2.5em}\protect#2\hspace{0.5em}\protect#3, % Protect parameters
  boxsep=1pt,
  extras first={bottom=0pt},
  extras last={top=0pt,bottom=-4pt},
  overlay first={
    \draw[line width=1pt,white] ([xshift=2.2pt]frame.south west)-- ([xshift=-0.5pt]frame.south east);
  },
  overlay last={
    \draw[line width=1pt,white] ([xshift=2.2pt]frame.north west)-- ([xshift=-0.5pt]frame.north east);
   }
]}
{\end{tcolorbox}}

%%%% --- end foldboxy preamble ----- %%%%%
%%==== colors from yaml ===%
\definecolor{callout-chapter-connection-color1}{HTML}{FDF2F7}
\definecolor{callout-chapter-connection-color2}{HTML}{A51C30}
\definecolor{callout-quiz-answer-color1}{HTML}{E8F2EA}
\definecolor{callout-quiz-answer-color2}{HTML}{4a7c59}
\definecolor{callout-example-color1}{HTML}{F0F8F6}
\definecolor{callout-example-color2}{HTML}{148F77}
\definecolor{callout-code-color1}{HTML}{F2F4F8}
\definecolor{callout-code-color2}{HTML}{D1D7E0}
\definecolor{callout-takeaways-color1}{HTML}{FDF2F7}
\definecolor{callout-takeaways-color2}{HTML}{BE185D}
\definecolor{callout-principle-color1}{HTML}{F3F2FA}
\definecolor{callout-principle-color2}{HTML}{3D3B8E}
\definecolor{callout-resource-exercises-color1}{HTML}{E0F2F1}
\definecolor{callout-resource-exercises-color2}{HTML}{20B2AA}
\definecolor{callout-resource-slides-color1}{HTML}{E0F2F1}
\definecolor{callout-resource-slides-color2}{HTML}{20B2AA}
\definecolor{callout-lighthouse-color1}{HTML}{FDF8E6}
\definecolor{callout-lighthouse-color2}{HTML}{B8860B}
\definecolor{callout-theorem-color1}{HTML}{F5F0FF}
\definecolor{callout-theorem-color2}{HTML}{6B46C1}
\definecolor{callout-resource-videos-color1}{HTML}{E0F2F1}
\definecolor{callout-resource-videos-color2}{HTML}{20B2AA}
\definecolor{callout-perspective-color1}{HTML}{F7F8FA}
\definecolor{callout-perspective-color2}{HTML}{4A5568}
\definecolor{callout-checkpoint-color1}{HTML}{E8F5E9}
\definecolor{callout-checkpoint-color2}{HTML}{2E7D32}
\definecolor{callout-notebook-color1}{HTML}{F2F7FF}
\definecolor{callout-notebook-color2}{HTML}{2C5282}
\definecolor{callout-colab-color1}{HTML}{FFF5E6}
\definecolor{callout-colab-color2}{HTML}{FF6B35}
\definecolor{callout-quiz-question-color1}{HTML}{F0F0F8}
\definecolor{callout-quiz-question-color2}{HTML}{5B4B8A}
\definecolor{callout-definition-color1}{HTML}{F0F4F8}
\definecolor{callout-definition-color2}{HTML}{1B4F72}
%=============%

\usepackage{hyphenat}
\usepackage{ifthen}
\usepackage{calc}
\usepackage{calculator}



\usepackage{graphicx}
\usepackage{geometry}
\usepackage{afterpage}
\usepackage{tikz}
\usetikzlibrary{calc}
\usetikzlibrary{fadings}
\usepackage[pagecolor=none]{pagecolor}


% Set the titlepage font families







% Set the coverpage font families

\usepackage{bookmark}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  pdftitle={Introduction to Machine Learning Systems},
  pdfauthor={Vijay Janapa Reddi},
  colorlinks=true,
  linkcolor={Maroon},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}


\title{Introduction to Machine Learning Systems}
\author{Vijay Janapa Reddi}
\date{February 1, 2026}
\begin{document}
%%%%% begin titlepage extension code

  \begin{frontmatter}

\begin{titlepage}
% This is a combination of Pandoc templating and LaTeX
% Pandoc templating https://pandoc.org/MANUAL.html#templates
% See the README for help

\thispagestyle{empty}

\newgeometry{top=-100in}

% Page color

\newcommand{\coverauthorstyle}[1]{{\fontsize{20}{24.0}\selectfont
{#1}}}

\begin{tikzpicture}[remember picture, overlay, inner sep=0pt, outer sep=0pt]

\tikzfading[name=fadeout, inner color=transparent!0,outer color=transparent!100]
\tikzfading[name=fadein, inner color=transparent!100,outer color=transparent!0]
\node[anchor=south west, rotate=0, opacity=1] at ($(current page.south west)+(0.225\paperwidth, 9)$) {
\includegraphics[width=\paperwidth, keepaspectratio]{assets/images/covers/cover-image-transparent-vol1.png}};

% Title
\newcommand{\titlelocationleft}{0.075\paperwidth}
\newcommand{\titlelocationbottom}{0.4\paperwidth}
\newcommand{\titlealign}{left}

\begin{scope}{%
\fontsize{52}{62.4}\selectfont
\node[anchor=north
west, align=left, rotate=0] (Title1) at ($(current page.south west)+(\titlelocationleft,\titlelocationbottom)$)  [text width = 0.9\paperwidth]  {{\nohyphens{Machine
Learning Systems}}};
}
\end{scope}

% Author
\newcommand{\authorlocationleft}{.925\paperwidth}
\newcommand{\authorlocationbottom}{0.175\paperwidth}
\newcommand{\authoralign}{right}

\begin{scope}
{%
\fontsize{20}{24.0}\selectfont
\node[anchor=north
east, align=right, rotate=0] (Author1) at ($(current page.south west)+(\authorlocationleft,\authorlocationbottom)$)  [text width = 6in]  {\coverauthorstyle{Vijay
Janapa Reddi\\}};
}
\end{scope}

% Footer
\newcommand{\footerlocationleft}{0.075\paperwidth}
\newcommand{\footerlocationbottom}{0.475\paperwidth}
\newcommand{\footerlocationalign}{left}

\begin{scope}
{%
\fontsize{25}{30.0}\selectfont
 \node[anchor=north west, align=left, rotate=0] (Footer1) at %
($(current page.south west)+(\footerlocationleft,\footerlocationbottom)$)  [text width = 0.9\paperwidth]  {{\nohyphens{Introduction
to}}};
}
\end{scope}

\end{tikzpicture}
\clearpage
\restoregeometry
%%% TITLE PAGE START

% Set up alignment commands
%Page
\newcommand{\titlepagepagealign}{
\ifthenelse{\equal{left}{right}}{\raggedleft}{}
\ifthenelse{\equal{left}{center}}{\centering}{}
\ifthenelse{\equal{left}{left}}{\raggedright}{}
}


\newcommand{\titleandsubtitle}{
% Title and subtitle
{{\huge{\bfseries{\nohyphens{Introduction to Machine Learning
Systems}}}}\par
}%
}
\newcommand{\titlepagetitleblock}{
\titleandsubtitle
}

\newcommand{\authorstyle}[1]{{\large{#1}}}

\newcommand{\affiliationstyle}[1]{{\large{#1}}}

\newcommand{\titlepageauthorblock}{
{\authorstyle{\nohyphens{Vijay Janapa
Reddi}{\textsuperscript{1}}\textsuperscript{,}{\textsuperscript{,*}}}}}

\newcommand{\titlepageaffiliationblock}{
\hangindent=1em
\hangafter=1
{\affiliationstyle{
{1}.~Harvard University


\vspace{1\baselineskip}
* \textit{Correspondence:}~Vijay Janapa Reddi~vj@eecs.harvard.edu
}}
}
\newcommand{\headerstyled}{%
{}
}
\newcommand{\footerstyled}{%
{\large{}}
}
\newcommand{\datestyled}{%
{February 1, 2026}
}


\newcommand{\titlepageheaderblock}{\headerstyled}

\newcommand{\titlepagefooterblock}{
\footerstyled
}

\newcommand{\titlepagedateblock}{
\datestyled
}

%set up blocks so user can specify order
\newcommand{\titleblock}{{

{\titlepagetitleblock}
}

\vspace{4\baselineskip}
}

\newcommand{\authorblock}{{\titlepageauthorblock}

\vspace{2\baselineskip}
}

\newcommand{\affiliationblock}{{\titlepageaffiliationblock}

\vspace{0pt}
}

\newcommand{\logoblock}{}

\newcommand{\footerblock}{}

\newcommand{\dateblock}{{\titlepagedateblock}

\vspace{0pt}
}

\newcommand{\headerblock}{}

\thispagestyle{empty} % no page numbers on titlepages


\newcommand{\vrulecode}{\textcolor{black}{\rule{\vrulewidth}{\textheight}}}
\newlength{\vrulewidth}
\setlength{\vrulewidth}{2pt}
\newlength{\B}
\setlength{\B}{\ifdim\vrulewidth > 0pt 0.05\textwidth\else 0pt\fi}
\newlength{\minipagewidth}
\ifthenelse{\equal{left}{left} \OR \equal{left}{right} }
{% True case
\setlength{\minipagewidth}{\textwidth - \vrulewidth - \B - 0.1\textwidth}
}{
\setlength{\minipagewidth}{\textwidth - 2\vrulewidth - 2\B - 0.1\textwidth}
}
\ifthenelse{\equal{left}{left} \OR \equal{left}{leftright}}
{% True case
\raggedleft % needed for the minipage to work
\vrulecode
\hspace{\B}
}{%
\raggedright % else it is right only and width is not 0
}
% [position of box][box height][inner position]{width}
% [s] means stretch out vertically; assuming there is a vfill
\begin{minipage}[b][\textheight][s]{\minipagewidth}
\titlepagepagealign
\titleblock

Prof.~Vijay Janapa Reddi

School of Engineering and Applied Sciences

Harvard University

\vspace{80mm}

With heartfelt gratitude to the community for their invaluable
contributions and steadfast support.

\vfill

February 1, 2026

\vfill
\par

\end{minipage}\ifthenelse{\equal{left}{right} \OR \equal{left}{leftright} }{
\hspace{\B}
\vrulecode}{}
\clearpage
%%% TITLE PAGE END
\end{titlepage}
\setcounter{page}{1}
\end{frontmatter}

%%%%% end titlepage extension code

% =============================================================================
% HALF-TITLE PAGE (Volume I)
% =============================================================================
% Standard academic book sequence: half-title -> blank -> title page -> copyright
% The half-title shows only the book title -- no author, no publisher, no date.
\thispagestyle{empty}
\begin{center}
\vspace*{0.3\textheight}
{\fontsize{24pt}{28pt}\selectfont\bfseries\color{crimson} Introduction to\\[0.4em] Machine Learning Systems}\\[2em]
{\large\itshape Volume~I}
\vfill
\end{center}
\clearpage
\thispagestyle{empty}\null\clearpage  % Blank verso (back of half-title)

\renewcommand*\contentsname{Table of contents}
{
\hypersetup{linkcolor=}
\setcounter{tocdepth}{2}
\tableofcontents
}
\listoffigures
\listoftables

\mainmatter
\bookmarksetup{startatroot}

\chapter*{Welcome to Volume I}\label{welcome-to-volume-i}
\addcontentsline{toc}{chapter}{Welcome to Volume I}

\markboth{Welcome to Volume I}{Welcome to Volume I}

\bookmarksetup{startatroot}

\chapter{Model Training}\label{sec-ai-training}

\marginnote{\begin{footnotesize}

\emph{DALL·E 3 Prompt: An illustration for AI training, depicting a
neural network with neurons that are being repaired and firing. The
scene includes a vast network of neurons, each glowing and firing to
represent activity and learning. Among these neurons, small figures
resembling engineers and scientists are actively working, repairing and
tweaking the neurons. These miniature workers symbolize the process of
training the network, adjusting weights and biases to achieve
convergence. The entire scene is a visual metaphor for the intricate and
collaborative effort involved in AI training, with the workers
representing the continuous optimization and learning within a neural
network. The background is a complex array of interconnected neurons,
creating a sense of depth and complexity.}

\end{footnotesize}}

\noindent
\pandocbounded{\includegraphics[keepaspectratio]{contents/vol1/training/images/png/ai_training.png}}

\section*{Purpose}\label{purpose}
\addcontentsline{toc}{section}{Purpose}

\markright{Purpose}

\emph{Why does training a model cost millions while running it costs
pennies?}

Architectures define the computational patterns; frameworks translate
those patterns into executable code. The process that gives models their
capabilities---training---is extraordinarily expensive. A forward pass
computes a prediction; training requires that forward pass plus a
backward pass for gradients, plus optimizer state that often exceeds the
model size itself, plus repetition across billions of examples until
statistical patterns crystallize into learned behavior. This
multiplicative cost structure---memory for weights, gradients, and
optimizer states; compute for forward, backward, and update steps;
repetition across epochs and hyperparameter searches---creates a
million-to-one asymmetry between training and inference costs. We study
training systems because this efficiency gap serves as the \emph{primary
gatekeeper to AI innovation}---determining not just \emph{how fast} we
can learn, but \emph{who} can afford to learn at all.

\begin{tcolorbox}[enhanced jigsaw, leftrule=.75mm, coltitle=black, opacitybacktitle=0.6, arc=.35mm, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Learning Objectives}, bottomtitle=1mm, left=2mm, colback=white, rightrule=.15mm, titlerule=0mm, toprule=.15mm, colbacktitle=quarto-callout-tip-color!10!white, breakable, toptitle=1mm, colframe=quarto-callout-tip-color-frame, bottomrule=.15mm, opacityback=0]

\begin{itemize}
\tightlist
\item
  Calculate computational requirements (FLOPs) and memory footprints
  (activation storage, optimizer states) for neural network training
\item
  Compare optimization algorithms (SGD, Adam, AdamW) based on
  convergence speed, memory overhead, and computational cost
\item
  Identify performance bottlenecks in training pipelines by analyzing
  profiling data to distinguish compute-bound, memory-bound, and
  data-bound scenarios
\item
  Apply memory optimization strategies including activation
  checkpointing and gradient accumulation to train large models within
  GPU memory constraints
\item
  Construct efficient single-machine training pipelines using data
  prefetching, mixed-precision arithmetic, and gradient accumulation
\item
  Analyze when single-machine training becomes infeasible due to memory
  exhaustion, unacceptable training duration, or dataset scale
\item
  Evaluate GPU and TPU architectures for training workloads by comparing
  throughput, memory bandwidth, and cost-performance trade-offs
\end{itemize}

\end{tcolorbox}

\section{Training Systems
Fundamentals}\label{sec-ai-training-training-systems-fundamentals-05d2}

Running GPT-2 once costs a fraction of a cent. Training GPT-2 cost
approximately \$50,000 in 2019. Running GPT-4 once costs a few cents.
Training GPT-4 cost an estimated \$100 million. This million-to-one
asymmetry between inference and training is not an engineering failure
awaiting a clever solution---it is the \emph{fundamental economics of
learning}. Inference executes a fixed function; training discovers what
that function should be by iterating through billions of examples,
computing gradients for every parameter, and slowly refining weights
until statistical patterns crystallize into useful behavior. The cost is
not in the computation per se but in the sheer volume of computation
required: millions of forward passes, each followed by a backward pass
that costs twice as much, repeated across datasets measured in
terabytes.

A single forward pass through GPT-2 requires roughly \(10^{10}\)
floating-point operations. Training requires millions of such passes,
and each backward pass costs approximately twice as much as the forward
pass, yielding a total computational budget on the order of \(10^{23}\)
operations (\citeproc{ref-brown2020language}{Brown et al. 2020}). This
asymmetry explains \emph{why} training systems engineering is a distinct
discipline---and \emph{why} access to training infrastructure
increasingly determines who can participate in AI development at all.

\phantomsection\label{callout-definitionux2a-1.1}
\begin{fbx}{callout-definition}{Definition:}{Training Systems}
\phantomsection\label{callout-definition*-1.1}
\textbf{\emph{Machine Learning Training Systems}} are distributed
engines optimized for \textbf{Throughput} and \textbf{Convergence}. They
orchestrate the movement of massive datasets through computational
graphs to minimize the \textbf{Time-to-Accuracy}, balancing the
competing constraints of \textbf{Memory Capacity}, \textbf{Compute
Intensity}, and \textbf{Network Bandwidth}.

\end{fbx}

Three characteristics make training workloads fundamentally different
from general-purpose computing, and concrete numbers illustrate why each
matters. First, \emph{computational intensity}: GPT-2's \(10^{23}\)
FLOPs spread over days of wall-clock time demands sustained
petaFLOPS-scale throughput from hardware that rarely exceeds 30 to 70
percent utilization. Second, \emph{memory pressure}: storing 1.5 billion
weights requires 6 GiB in FP32, but the Adam optimizer adds two
additional state tensors per parameter (another 12 GiB), and activation
storage across 48 transformer layers can double or triple the total,
easily exceeding a single GPU's memory capacity. Third, \emph{data
dependencies}: each gradient update depends on the result of the
previous one, creating sequential bottlenecks that limit how much
parallelism the system can exploit.

Each of these challenges opens a corresponding optimization pathway.
Computational intensity can be addressed through hardware acceleration
and precision reduction. Memory pressure responds to techniques like
\textbf{gradient checkpointing}, which trades recomputation for reduced
activation storage, and \textbf{mixed-precision training}, which halves
the memory footprint of weights and activations. Data dependencies
motivate pipeline designs that overlap computation with data movement so
the GPU never sits idle waiting for the next batch. This chapter focuses
on single-machine and single-node multi-GPU training; scaling to
hundreds of machines across network boundaries introduces communication
and fault tolerance challenges that are the subject of \emph{Distributed
Training Systems} in Volume II. We use the \emph{Iron Law of Training
Performance} as our organizing framework, examining how each
optimization technique targets specific terms in the performance
equation. Before formalizing that framework, consider how these
constraints interact in practice when a single \emph{gradient explosion}
can erase days of expensive computation.

\phantomsection\label{callout-exampleux2a-1.2}
\begin{fbx}{callout-example}{Example:}{The 3AM Gradient Explosion}
\phantomsection\label{callout-example*-1.2}

\textbf{The Scenario}: You are training a 7B parameter LLM. The loss
curve has been decreasing smoothly for 4 days. You go to sleep.

\textbf{The Failure}: At 3:00 AM, the training loss suddenly spikes from
2.5 to NaN (Not a Number). The training crashes. You have lost 4 days of
compute (\(\approx \$5,000\)).

\textbf{The Physics}: A single batch contained an outlier with extremely
high activation values. In \textbf{Mixed Precision (FP16)}, these values
exceeded the dynamic range (\(> 65,504\)), causing an overflow to
Infinity. The gradients became Infinite, updating all weights to NaN.

\textbf{The Systems Fix}:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Checkpointing}: Save model state every hour so you only lose 1
  hour, not 4 days.
\item
  \textbf{Gradient Clipping}: Cap the norm of gradient vectors to
  prevent single-batch spikes from destroying the weights.
\item
  \textbf{BF16}: Use Brain Float 16 format, which trades precision for
  range, making overflows far less likely than in standard FP16.
\end{enumerate}

\end{fbx}

\subsection{The Iron Law of Training
Performance}\label{sec-ai-training-iron-law-training-performance-a53f}

Frameworks provide abstractions for expressing training algorithms, but
training systems engineering determines whether those algorithms can
execute within physical resource limits. The Iron Law provides the
organizing framework for understanding how every optimization technique
improves training time. This is a specialized application of the general
\textbf{Iron Law of ML Systems} introduced in
\textbf{?@sec-silicon-contract}, focused specifically on maximizing
computational throughput.

\phantomsection\label{callout-definitionux2a-1.3}
\begin{fbx}{callout-definition}{Definition:}{The Iron Law of Training Performance}
\phantomsection\label{callout-definition*-1.3}
\textbf{\emph{The Iron Law of Training Performance}} states that
training time is purely a function of \textbf{Arithmetic Intensity} and
\textbf{Hardware Efficiency}. It decomposes performance into three
orthogonal terms: the \textbf{Algorithmic Work} (Total Operations), the
\textbf{Hardware Capability} (Peak Throughput), and the \textbf{System
Efficiency} (Utilization), dictating that speedups must come from
reducing Ops, increasing Peak, or improving Utilization.

\[ T_{train} = \frac{O}{R_{peak} \times \eta} \]

where \(O\) (\textbf{Total Operations}) is the FLOPs required for one
epoch times the number of epochs, \(R_{peak}\) (\textbf{Peak
Throughput}) is the hardware's theoretical FLOP/s capacity, and \(\eta\)
(\textbf{Utilization}) is the fraction of peak actually achieved
(typically 30-70\% for training workloads).

This equation reveals three levers for improvement: reduce total
operations through algorithmic innovation, increase peak throughput
through hardware utilization, or improve utilization through better
pipeline orchestration. Each optimization technique in this chapter
pulls one or more of these levers, as summarized in
Table~\ref{tbl-iron-law-mapping}.

\end{fbx}

The challenge of maintaining high utilization at scale is illustrated in
Figure~\ref{fig-communication-tax}. While compute-bound workloads scale
efficiently, bandwidth-bound workloads like LLM training suffer from
synchronization overhead that grows with cluster size, creating a
``tax'' that degrades effective throughput.

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{contents/vol1/training/training_files/figure-pdf/fig-communication-tax-output-1.pdf}}

}

\caption{\label{fig-communication-tax}\textbf{The Communication Tax}:
Effective Throughput vs.~GPU Count (Log-Log Scale). Ideal scaling
(dashed gray) is linear. Compute-bound workloads like ResNet (Blue)
scale efficiently because communication is a small fraction of step
time. Bandwidth-bound workloads like LLMs (Red) suffer from the
`Communication Tax,' where synchronization overhead grows with cluster
size, causing efficiency to drop. This divergence explains why network
bandwidth is often the bottleneck in large-scale training.}

\end{figure}%

The gap between theoretical peak performance and actual training speed
is often 2-3x. Before examining specific optimization techniques, verify
your understanding of \emph{why} this gap exists.

\phantomsection\label{callout-checkpointux2a-1.4}
\begin{fbx}{callout-checkpoint}{Checkpoint:}{The Physics of Training}
\phantomsection\label{callout-checkpoint*-1.4}

Training speed is governed by the utilization of hardware peaks.

\textbf{The Utilization Gap}

\begin{itemize}
\tightlist
\item[$\square$]
  \textbf{Peak vs.~Real}: Why is 100\% GPU utilization impossible?
  (Memory bandwidth stalls, kernel launch overhead, communication
  latencies).
\item[$\square$]
  \textbf{Batch Size Physics}: Why does increasing batch size generally
  improve \textbf{MFU} (Model FLOPs Utilization)? (It increases
  arithmetic intensity, moving us from memory-bound to compute-bound).
\end{itemize}

\textbf{Precision Economics}

\begin{itemize}
\tightlist
\item[$\square$]
  \textbf{Mixed Precision}: How does FP16/BF16 double throughput?
  (Tensor Cores run 2x faster, and memory bandwidth effectively
  doubles).
\end{itemize}

\end{fbx}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3333}}@{}}
\caption{\textbf{Iron Law Optimization Mapping.} Optimization techniques
mapped to Iron Law terms. Understanding which term a technique affects
guides optimization strategy
selection.}\label{tbl-iron-law-mapping}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Technique}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Term Affected}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Mechanism}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Technique}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Term Affected}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Mechanism}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Mixed Precision (FP16/BF16)} & Peak Throughput ↑ & Tensor Cores
operate at up to 16× higher FLOP/s \\
\textbf{Data Prefetching} & Utilization ↑ & Reduces GPU idle time
waiting for data \\
\textbf{Gradient Checkpointing} & Total Operations ↑ & Adds
recomputation, but enables larger models \\
\textbf{Gradient Accumulation} & Utilization ↑ & Maintains high batch
parallelism efficiency \\
\textbf{Operator Fusion} & Utilization ↑ & Reduces memory bandwidth
bottlenecks \\
\textbf{FlashAttention} & Total Operations ↓ Utilization ↑ & Algorithmic
improvement reduces FLOP count Tiling improves memory access patterns \\
\end{longtable}

The Iron Law provides a static framework for reasoning about training
performance. But the history of deep learning reveals how the
\emph{binding constraint} has shifted over time as hardware and
algorithms co-evolved. Understanding this evolution helps explain why
certain techniques emerged when they did.

\phantomsection\label{callout-perspectiveux2a-1.5}
\begin{fbx}{callout-perspective}{Systems Perspective:}{Training Systems Evolution}
\phantomsection\label{callout-perspective*-1.5}
The Iron Law framework shows \emph{how} training systems co-evolved with
hardware capabilities. Each generation faced different bottlenecks, and
each generation's constraints drove the next generation's innovations:

\begin{itemize}
\tightlist
\item
  \textbf{1986}: Backpropagation algorithm formalized
  (\citeproc{ref-rumelhart1986learning}{Rumelhart, Hinton, and Williams
  1986}) (see
  \textbf{?@sec-system-foundations-backpropagation-algorithm-3175} for
  the derivation). Training a 3-layer network on toy datasets required
  days on CPU workstations. The bottleneck was raw compute throughput
  (Peak Throughput in Iron Law terms).
\item
  \textbf{2012}: AlexNet demonstrated GPU training
  (\citeproc{ref-krizhevsky2012imagenet}{Krizhevsky, Sutskever, and
  Hinton 2017}). Two GTX 580 GPUs reduced ImageNet training from weeks
  to days---a 10× improvement that launched the deep learning era. GPUs
  increased Peak Throughput dramatically.
\item
  \textbf{2017}: Transformers introduced attention mechanisms
  (\citeproc{ref-vaswani2017attention}{Vaswani et al. 2025}). NVIDIA
  Volta GPUs with Tensor Cores enabled mixed-precision training,
  delivering 5× speedup over previous generation. Tensor Cores further
  increased Peak Throughput for specific operations.
\item
  \textbf{2020}: GPT-3 training used over 10,000 V100 GPUs on
  Microsoft's Azure supercomputer, consuming an estimated \$4.6M in
  compute (\citeproc{ref-brown2020language}{Brown et al.
  2020}).\sidenote{GPT-3 was released in June 2020, before A100 GPUs
  were widely available. The training infrastructure comprised Microsoft
  Azure's V100-based supercomputer. Later estimates suggest equivalent
  training on 1,024 A100s would take approximately 34 days. } At this
  scale, Utilization became critical---idle GPUs wasted thousands of
  dollars per hour.
\end{itemize}

\begin{itemize}
\tightlist
\item
  \textbf{2023}: Training efficiency improved 10× through the techniques
  examined in this chapter. FlashAttention reduces Total Operations
  while improving Utilization; gradient checkpointing trades Operations
  for memory capacity; ZeRO optimization maximizes Utilization across
  distributed systems.
\end{itemize}

Memory limits motivated gradient checkpointing; bandwidth limits
motivated FlashAttention; cost limits motivated mixed precision. The
Iron Law explains \emph{why} each technique matters: they each pull
different levers in the fundamental performance equation.

\end{fbx}

\subsection{Running Example: Training
GPT-2}\label{sec-ai-training-running-example-training-gpt2-19cd}

To ground the abstract principles of training systems in concrete
engineering decisions, we use GPT-2 as a recurring worked example
throughout this chapter. This \emph{lighthouse model} is large enough to
expose real systems bottlenecks yet small enough to reason about without
massive cluster infrastructure.

\phantomsection\label{callout-lighthouseux2a-1.6}
\begin{fbx}{callout-lighthouse}{Lighthouse:}{Lighthouse Example: Training GPT-2}
\phantomsection\label{callout-lighthouse*-1.6}
\textbf{Why this model?} GPT-2 (1.5B) serves as our primary case study
for \textbf{large-scale training} because it sits at the ``sweet spot''
of systems complexity. It is large enough to require distributed
training and serious memory optimizations, yet small enough to
comprehend without the massive infrastructure complexity of
trillion-parameter clusters.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3333}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Property}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Specification}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Systems Implication}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Parameters} & 1.5 Billion (XL) & Requires \textasciitilde3 GiB
(FP16) or \textasciitilde6 GiB (FP32) for weights alone. \\
\textbf{Architecture} & 48 Layers, 1600 Dim & Deep pipeline creates
heavy activation memory pressure. \\
\textbf{Dataset} & OpenWebText (40GB) & I/O throughput must match
high-speed accelerator compute. \\
\textbf{Compute} & \textasciitilde{} \(10^{23}\) FLOPs total & Training
takes days/weeks; demands parallelization. \\
\end{longtable}

\textbf{Key Systems Challenge:} Training GPT-2 is primarily
\textbf{memory-bound} (due to activation storage) and
\textbf{compute-intensive} (requiring massive matrix multiplications).
It forces us to move beyond simple training loops to sophisticated
pipelines that manage data movement as carefully as computation.

\end{fbx}

\subsection{Running Example: Training DLRM (Recommendation
Systems)}\label{sec-ai-training-running-example-training-dlrm-4a2b}

While GPT-2 illustrates dense, compute-intensive workloads,
recommendation systems present a fundamentally different challenge:
massive sparsity. The Deep Learning Recommendation Model (DLRM) serves
as our lighthouse for \textbf{Scale-Out} systems issues.

\phantomsection\label{callout-lighthouseux2a-1.7}
\begin{fbx}{callout-lighthouse}{Lighthouse:}{Lighthouse Example: Training DLRM}
\phantomsection\label{callout-lighthouse*-1.7}
\textbf{Why this model?} DLRM represents the ``Sparse'' regime of the
Iron Law. Unlike GPT-2, which is bound by FLOPs and Activation Memory,
DLRM is bound by \textbf{Parameter Capacity} and \textbf{Network
Bandwidth}. It powers the revenue engines of major internet companies
(ads, search, feed ranking).

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3333}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Property}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Specification}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Systems Implication}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Parameters} & 100 Billion to 10 Trillion (mostly embeddings) &
Embedding tables exceed single-node memory (requires TBs). \\
\textbf{Architecture} & Embedding Tables + MLP & Low arithmetic
intensity; highly memory-bandwidth bound. \\
\textbf{Inputs} & Sparse Features (IDs) & Random memory access patterns
defy caching. \\
\textbf{Compute} & Low FLOPs per byte & GPUs often underutilized;
typically trained on CPU clusters. \\
\end{longtable}

\textbf{Key Systems Challenge:} Training DLRM requires \textbf{Model
Parallelism} not for compute, but for capacity. The embedding tables are
sharded across hundreds of devices. The bottleneck becomes the
\textbf{All-to-All} communication pattern required to gather embeddings
for the dense MLP top-stack.

\end{fbx}

\textbf{Note on Precision:} Throughout this chapter, we reference
\textbf{FP32} (32-bit) and \textbf{FP16} (16-bit) floating-point
formats.

\begin{itemize}
\tightlist
\item
  \textbf{FP32}: Standard precision, high numerical stability.
\item
  \textbf{FP16}: Half precision, halves memory usage and accelerates
  math on modern hardware (like Tensor Cores).
\item
  \textbf{Mixed-Precision}: Combines both to get the speed of FP16 with
  the stability of FP32 (detailed in
  Section~\ref{sec-ai-training-mixedprecision-training-9218}).
\end{itemize}

Having established our running examples---GPT-2 for dense
compute-intensive workloads and DLRM for sparse, memory-bound
systems---we now situate training systems within the broader ML
development lifecycle.

\subsection{Training in the ML Development
Lifecycle}\label{sec-ai-training-training-ml-development-lifecycle-6341}

Training systems occupy a critical position in the machine learning
pipeline: they consume prepared data from upstream engineering
(\textbf{?@sec-data-engineering-ml}) and produce trained models for
downstream deployment
(\textbf{?@sec-machine-learning-operations-mlops}). This position
creates bidirectional dependencies. Data quality directly impacts
training stability, while training efficiency determines iteration
velocity during model development.

Modern training systems face three scaling challenges that define their
architecture. First, \textbf{data scale}: processing petabyte datasets
requires efficient I/O pipelines and distributed storage.

\phantomsection\label{callout-perspectiveux2a-1.8}
\begin{fbx}{callout-perspective}{Systems Perspective:}{The 10 GiB to 10 TiB Scale Factor}
\phantomsection\label{callout-perspective*-1.8}

\begin{itemize}
\tightlist
\item
  \textbf{At 10 GiB}: You can often fit the entire dataset in system
  RAM. Data loading is a one-time ``startup cost,'' and the disk
  bandwidth (\(B\)) doesn't matter after the first few seconds.
\item
  \textbf{At 10 TB}: Data becomes a continuous, high-pressure stream.
  You can no longer ``load'' the data; you must \textbf{orchestrate} its
  movement. The \(D\) term shifts from a storage bottleneck to a
  \textbf{Networking and I/O bottleneck}, requiring zero-copy paths and
  multi-worker prefetching just to keep the GPU from starving.
\end{itemize}

Scale is not just ``more data''; it is a transformation of the system's
physics.

\end{fbx}

Second, \textbf{model scale}: billion-parameter models demand
parallelization strategies including \textbf{data
parallelism}\sidenote{\textbf{Data Parallelism}: Replicates the model
across devices, each processing different batches. Gradient
synchronization introduces communication overhead that limits scaling
efficiency. } (replicate model, split data) and \textbf{model
parallelism}\sidenote{\textbf{Model Parallelism}: Splits the model
across devices when it exceeds single-device memory. Introduces pipeline
bubbles and coordination overhead. } (split model across devices).

Third, \textbf{infrastructure scale}: coordinating thousands of
accelerators introduces communication overhead that can dominate
training time. These challenges motivate the workflow management tools
(\textbf{?@sec-ai-development-workflow}) that automate training
orchestration.

\subsection{System Design
Principles}\label{sec-ai-training-system-design-principles-7058}

These lifecycle dependencies create specific system design requirements.
Training is not merely a mathematical optimization problem; it is a
system-driven process that requires careful orchestration of computing
hardware, memory, and data movement.

Training workflows consist of interdependent stages: data preprocessing,
forward and backward passes, and parameter updates, extending the basic
neural network concepts from
\textbf{?@sec-deep-learning-systems-foundations}. Each stage imposes
specific demands on system resources. The data preprocessing stage
relies on storage and I/O subsystems to provide computing hardware with
continuous input. \textbf{?@sec-data-engineering-ml} covers data
validation, corruption detection, feature engineering, schema
enforcement, and pipeline reliability strategies; this chapter examines
the efficiency of data movement, transformation throughput, and delivery
to computational resources during training.

System constraints often dictate the performance limits of training
workloads. Modern accelerators are frequently bottlenecked by memory
bandwidth, as data movement between memory hierarchies can be slower and
more energy-intensive than the computations themselves
(\citeproc{ref-patterson2021hardware}{Patterson and Hennessy 2021}). In
distributed setups, synchronization across devices introduces additional
latency, with interconnect performance (NVLink, InfiniBand) critically
affecting throughput. For example, training large Transformer
models\sidenote{\textbf{Transformer Training}: Large-scale transformer
training requires specialized techniques including gradient
checkpointing (saving memory by recomputing activations),
mixed-precision training (FP16 forward/backward with FP32 accumulation),
and sequence parallelism distributing long contexts across devices.
GPT-3 training used 1024 V100s for months, detailed in
\textbf{?@sec-dnn-architectures}. } requires partitioning data and model
parameters across multiple devices, introducing synchronization
challenges during gradient updates. Communication libraries such as
NVIDIA's Collective Communications Library (NCCL)
(\citeproc{ref-nvidia_nccl}{NVIDIA 2024b}) enable efficient gradient
sharing.

The hardware-software co-design principles discussed in
\textbf{?@sec-ai-acceleration} demonstrate \emph{how} understanding
system capabilities can inspire architectural innovations. \ldots{}
These adaptations illustrate \emph{how} physical system constraints
shape the trajectory of machine learning research and practice.

\section{Mathematical
Foundations}\label{sec-ai-training-mathematical-foundations-d894}

\textbf{?@sec-deep-learning-systems-foundations} established the
mathematical mechanics of neural network training: forward propagation
computes predictions through weighted sums and activation functions,
backpropagation applies the chain rule to compute gradients, and
optimization algorithms update parameters to minimize loss. Those
explanations focused on \emph{what} these operations compute and
\emph{why} they enable learning. This section shifts to \emph{what they
cost}---the FLOPs consumed, the memory required, and the bandwidth
demanded when these conceptually simple operations execute at scale.

Four dimensions of computational cost structure this analysis. First,
the FLOP counts of matrix operations that dominate training. Second, the
memory requirements for storing activations and optimizer states
simultaneously. Third, the bandwidth demands that determine whether
operations are compute-bound or memory-bound. Fourth, the arithmetic
intensity classifications that guide optimization strategy selection.
These dimensions provide the vocabulary for analyzing training
bottlenecks systematically.

The shift in perspective is essential because the same mathematical
operations that elegantly describe learning in equations become
significant engineering challenges in implementation. A matrix
multiplication is just \(C = AB\) in notation, but training GPT-2
requires executing that operation billions of times with matrices too
large to fit in fast memory. The activation function
\(f(x) = \max(0, x)\) appears trivial, yet the choice between ReLU and
sigmoid determines whether Tensor Cores can accelerate computation.
Understanding these system-level implications of familiar mathematics
enables practitioners to identify bottlenecks and apply targeted
optimizations.

Training systems must execute three categories of operations repeatedly:
forward propagation computes predictions through matrix multiplications
and activation functions, gradient computation calculates parameter
updates using stored activations and the chain rule, and parameter
updates apply gradients using optimization algorithms that maintain
momentum and adaptive learning rate state. Each category exhibits
distinct computational patterns and system requirements.

Matrix multiplications dominate forward and backward passes, accounting
for 60--90\% of training time (\citeproc{ref-he2016residual}{He et al.
2016}), which explains \emph{why} specialized matrix units (GPU tensor
cores, TPU systolic arrays) became central to training hardware.
Activation storage for gradient computation creates memory pressure
proportional to batch size and network depth, motivating techniques like
gradient checkpointing. The iterative dependencies between these
operations constrain parallelization strategies for scaling.

\subsection{Neural Network
Computation}\label{sec-ai-training-neural-network-computation-5660}

Neural network training consists of repeated matrix operations and
nonlinear transformations. These operations, while conceptually simple,
create the system-level challenges that dominate modern training
infrastructure. Foundational works by Rumelhart, Hinton, and Williams
(\citeproc{ref-rumelhart1986learning}{1986}) through the introduction of
backpropagation and the development of efficient matrix computation
libraries, e.g., BLAS (\citeproc{ref-dongarra1988extended}{Dongarra et
al. 1988}), laid the groundwork for modern training architectures.

\subsubsection{Mathematical Operations in Neural
Networks}\label{sec-ai-training-mathematical-operations-neural-networks-ddac}

At the heart of a neural network is the process of forward propagation,
which in its simplest case involves two primary operations: matrix
multiplication and the application of an activation function. Matrix
multiplication forms the basis of the linear transformation in each
layer of the network. This equation represents \emph{how} information
flows through each layer of a neural network:

At layer \(l\), the computation can be described as: \[
A^{(l)} = f\left(W^{(l)} A^{(l-1)} + b^{(l)}\right)
\] Where:

\begin{itemize}
\tightlist
\item
  \(A^{(l-1)}\) represents the activations from the previous layer (or
  the input layer for the first layer),
\item
  \(W^{(l)}\) is the weight matrix at layer \(l\), which contains the
  parameters learned by the network,
\item
  \(b^{(l)}\) is the bias vector for layer \(l\),
\item
  \(f(\cdot)\) is the activation function applied element-wise (e.g.,
  ReLU, sigmoid) to introduce non-linearity.
\end{itemize}

\subsubsection{Matrix
Operations}\label{sec-ai-training-matrix-operations-1f21}

Computational patterns in neural networks revolve around various types
of matrix operations. These operations and their evolution reveal
\emph{why} specific system designs and optimizations emerged in machine
learning training systems.

\paragraph{Dense Matrix-Matrix
Multiplication}\label{sec-ai-training-dense-matrixmatrix-multiplication-057f}

Matrix multiplication dominance has driven both algorithmic and hardware
innovations. Early neural network implementations relied on standard
CPU-based linear algebra libraries, but the scale of modern training
demanded specialized optimizations. Strassen's
algorithm\sidenote{\textbf{Strassen's Algorithm}: Developed by Volker
Strassen in 1969, this breakthrough reduced matrix multiplication from
O(n³) to O(n\^{}2.807) by using clever algebraic tricks with 7
multiplications instead of 8. While theoretically faster, it's only
practical for matrices larger than 500×500 due to overhead. Modern
implementations in libraries like Intel MKL switch between algorithms
based on matrix size, demonstrating how theoretical advances require
careful engineering for practical impact. } reduced the naive \(O(n^3)\)
complexity to approximately \(O(n^{2.81})\)
(\citeproc{ref-strassen1969gauss}{Strassen 1969}), and contemporary
hardware-accelerated libraries like cuBLAS
(\citeproc{ref-nvidia_cublas}{NVIDIA 2024a}) continue pushing
computational efficiency limits.

This computational dominance has driven system-level optimizations.
Systems implement blocked matrix computations for parallel processing
across multiple units. As neural architectures grew in scale, these
multiplications demanded significant memory resources, since weight
matrices and activation matrices must both remain accessible for the
backward pass during training. Hardware designs adapted to optimize for
these dense multiplication patterns while managing growing memory
requirements.

To illustrate the scale of these operations concretely, consider the
\emph{attention layer computations} in our GPT-2 lighthouse model.

\phantomsection\label{callout-notebookux2a-1.9}
\begin{fbx}{callout-notebook}{AI Engineer’s Notebook:}{GPT-2 Attention Layer Computation}
\phantomsection\label{callout-notebook*-1.9}
Each GPT-2 layer performs attention computations that exemplify dense
matrix multiplication demands. For a single attention head with
batch\_size=32, sequence\_length=1024, hidden\_dim=1600:

\textbf{Query, Key, Value Projections} (3 separate matrix
multiplications): \[
\text{FLOPS} = 3 \times (\text{batch} \times \text{seq} \times \text{hidden} \times \text{hidden})
\] \[
= 3 \times (32 \times 1024 \times 1600 \times 1600) \approx 252 \text{ billion FLOPS}
\]

\textbf{Attention Score Computation} (Q × K\^{}T): \[
\text{FLOPS} = \text{batch} \times \text{heads} \times \text{seq} \times \text{seq} \times \text{hidden/heads}
\] \[
= 32 \times 25 \times 1024 \times 1024 \times 64 = 53\.7 \text{ billion FLOPS}
\]

\textbf{Computation Scale}

\begin{itemize}
\tightlist
\item
  Total for one attention layer: \textasciitilde443B FLOPS forward pass
\item
  With 48 layers in GPT-2: \textasciitilde21.3 trillion FLOPS per
  training step
\item
  At 50,000 training steps: \textasciitilde1063 petaFLOPS total training
  computation
\end{itemize}

\textbf{System Implication:} A V100 GPU (125 TFLOPS peak FP16 with
Tensor Cores, 15.7 TFLOPS FP32 without) would require 0 seconds just for
the attention computations per step at 100\% utilization (theoretical
peak; practical throughput would be lower). Actual training steps take
180 to 220ms, requiring 8 to 32 GPUs to achieve this throughput
depending on utilization and interconnect efficiency.

\end{fbx}

\textbf{Matrix-Vector Operations.} Beyond matrix-matrix operations,
matrix-vector multiplication became essential with the introduction of
normalization techniques in neural architectures. Although
computationally simpler than matrix-matrix multiplication, these
operations present system challenges. They exhibit lower hardware
utilization due to their limited parallelization potential. This
characteristic influences hardware design and model architecture
decisions, particularly in networks processing sequential inputs or
computing layer statistics.

\textbf{Batched Operations.} Recognizing the limitations of
matrix-vector operations, the introduction of
batching\sidenote{\textbf{Batching in Neural Networks}: Unlike
traditional programming where data is processed one item at a time, ML
systems process multiple examples simultaneously to maximize GPU
utilization. A single example might achieve only 5-10\% GPU utilization,
while batches of 32-256 can reach 80-95\%. This shift from scalar to
tensor operations explains why ML systems require different programming
patterns and hardware optimizations than traditional applications. }
transformed matrix computation in neural networks. By processing
multiple inputs simultaneously, training systems convert matrix-vector
operations into more efficient matrix-matrix operations. This approach
improves hardware utilization but increases memory demands for storing
intermediate results. Modern implementations must balance batch sizes
against available memory, leading to specific optimizations in memory
management and computation scheduling.

The progression from matrix-vector to batched matrix-matrix operations
explains the hardware design choices in modern accelerators. Hardware
accelerators like Google's TPU (\citeproc{ref-jouppi2017tpu}{Jouppi et
al. 2017}) reflect this evolution, incorporating specialized matrix
units and memory hierarchies optimized for batched operations. These
hardware adaptations enable training of large-scale models like GPT-3
(\citeproc{ref-brown2020language}{Brown et al. 2020}) through efficient
handling of the matrix-matrix multiplication patterns that batching
produces.

\phantomsection\label{callout-perspectiveux2a-1.10}
\begin{fbx}{callout-perspective}{Systems Perspective:}{Why GPUs Dominate Training}
\phantomsection\label{callout-perspective*-1.10}
The matrix operations described above directly explain modern training
hardware architecture. GPUs dominate training for three reasons. First,
matrix multiplication's independent element calculations map perfectly
to thousands of GPU cores (NVIDIA A100 has 6,912 CUDA cores). Second,
specialized hardware units like Tensor Cores accelerate matrix
operations by 10--20× through dedicated hardware for the dominant
workload. Third, blocked matrix computation patterns enable efficient
use of GPU memory hierarchy (L1/L2 cache, shared memory, global memory).

When GPT-2 examples later show \emph{why} V100 GPUs achieve 2.4x speedup
with mixed precision, this acceleration comes from Tensor Cores
executing the matrix multiplications we just analyzed. Matrix operation
characteristics are prerequisite for appreciating \emph{why} pipeline
optimizations like mixed-precision training provide such substantial
benefits.

\end{fbx}

Matrix multiplications dominate training compute, but neural networks
require more than linear transformations. Between each layer's matrix
operations, activation functions introduce the nonlinearity that enables
networks to learn complex patterns. These functions appear
computationally trivial compared to matrix multiplication, yet their
implementation characteristics affect training efficiency in ways that
matter at scale.

\subsubsection{Activation
Functions}\label{sec-ai-training-activation-functions-faa7}

In \textbf{?@sec-deep-learning-systems-foundations}, we established the
mathematical properties of activation functions like sigmoid, tanh,
ReLU, and softmax. While their role is to introduce nonlinearity, their
implementation characteristics significantly impact training system
performance. From a systems perspective, the choice of activation
function determines computational cost, hardware utilization, and memory
access patterns during backpropagation.

The critical question for ML systems engineers is not \emph{what} these
functions do mathematically, but \emph{how} to implement them
efficiently at scale. This section analyzes the computational trade-offs
that determine real-world training efficiency.

\paragraph{Benchmarking Activation
Functions}\label{sec-ai-training-benchmarking-activation-functions-75c1}

The selection of an activation function directly influences training
throughput and hardware efficiency. Figure~\ref{fig-activation-perf}
quantifies these performance differences through CPU benchmarks on Apple
M2 hardware, revealing that Tanh executes in 0.61 seconds compared to
Sigmoid's 1.10 seconds, a 1.8\(\times\) speedup.

\begin{figure}[htb]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/91ece82020ef3ab702402b1fe0dce3f42ae3e064.pdf}}

}

\caption{\label{fig-activation-perf}\textbf{Activation Function
Execution Time}: CPU benchmarks on Apple M2 hardware reveal significant
variation: Tanh completes in 0.61 seconds, ReLU in 0.78 seconds, Softmax
in 0.91 seconds, and Sigmoid in 1.10 seconds. These differences directly
affect training throughput and real-time inference latency, making
activation function selection a system-level design decision.}

\end{figure}%

In production environments, modern hardware accelerators like GPUs alter
these relative performance characteristics through specialized hardware
units. System architects must consider three primary implementation
factors:

\textbf{Computational Complexity (Arithmetic Intensity).} Functions
requiring transcendental operations (exponential, logarithmic) are
significantly more expensive than simple thresholding. In software,
\texttt{exp()} takes 10--20 clock cycles compared to 1 cycle for basic
arithmetic\sidenote{\textbf{Sigmoid Computational Cost}: Computing
sigmoid requires expensive exponential operations. On CPU,
\texttt{exp()} takes 10--20 clock cycles vs.~1 cycle for basic
arithmetic. GPU implementations use 32-entry lookup tables with linear
interpolation, reducing cost to 3--4 cycles but still 3\(\times\) slower
than ReLU. This overhead compounds in deep networks with millions of
activations per forward pass. }. Modern GPUs and TPUs mitigate this
through lookup tables (LUTs) or piece-wise linear approximations, but
even optimized hardware-based sigmoid/tanh remains 3--4\(\times\) slower
than ReLU.

\textbf{Hardware Implementation and Branching.} ReLU represents a shift
toward hardware-optimized design. Its \(\max(0,x)\) operation requires
only a single comparison and conditional set, which translates to
minimal circuit complexity\sidenote{\textbf{ReLU Hardware Efficiency}:
ReLU requires just 1 instruction (\texttt{max(0,x)}) vs.~sigmoid's 10+
operations including exponentials. On NVIDIA GPUs, ReLU runs at 95\% of
peak FLOPS while sigmoid achieves only 30--40\%. ReLU's sparsity
(typically 50\% zeros) enables additional optimizations: sparse matrix
operations, reduced memory bandwidth, and compressed gradients during
backpropagation. }. GPUs can implement ReLU using a simple multiplexer
that checks the sign bit of the input. This simplicity enables extremely
high parallel throughput, allowing ReLU to operate at near-peak FLOPs
while complex functions achieve only 30--40\% hardware utilization.

\textbf{Memory Access and Sparsity.} The memory footprint of activations
is proportional to the batch size and network depth. ReLU's
characteristic of producing many zeros (typically 50\% sparsity) enables
system-level optimizations that other functions cannot exploit. Sparse
matrix operations and gradient compression techniques can reduce memory
bandwidth requirements, which is the primary bottleneck in large-scale
training. In contrast, global normalization functions like
Softmax\sidenote{\textbf{Softmax}: A ``soft'' (differentiable)
approximation to the argmax function. While argmax returns a hard
one-hot vector (1 for the maximum, 0 elsewhere), softmax returns a
probability distribution that smoothly approximates this behavior. The
name, coined by John Bridle in 1990, reflects this relationship: as
temperature approaches zero, softmax converges to argmax. This
differentiability enables gradient-based learning for classification
tasks. } create unique challenges; they require access to the entire
input vector simultaneously to compute the denominator, preventing the
independent element-wise parallelization possible with Sigmoid or ReLU.

Table~\ref{tbl-compare-activations} synthesizes these system-level
trade-offs, showing \emph{how} mathematical behavior translates into
operational constraints.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2500}}@{}}
\caption{\textbf{Activation Function Systems Comparison.} While
activation functions contribute only a fraction of total training time,
their implementation characteristics (computational complexity, hardware
utilization, and memory patterns) significantly impact the efficiency of
modern learning
pipelines.}\label{tbl-compare-activations}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Function}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Key Advantages}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Key Disadvantages}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{System Implications}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Function}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Key Advantages}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Key Disadvantages}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{System Implications}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Sigmoid} & Smooth gradients; bounded output in \((0, 1)\). &
Vanishing gradients; non-zero-centered output. & Exponential computation
adds overhead; LUT-based hardware implementation is required for
efficiency. \\
\textbf{Tanh} & Zero-centered output in \((-1, 1)\). & Vanishing
gradients at extremes. & Better convergence than sigmoid; similar
computational cost due to exponential terms. \\
\textbf{ReLU} & Extremely efficient computation; avoids vanishing
gradients for positive inputs. & Can suffer from ``dying ReLU''
(inactive neurons). & Single-instruction hardware implementation;
enables sparsity-based optimizations. \\
\textbf{Softmax} & Outputs probability distribution over classes. & High
computational cost; non-local dependencies. & Requires global
normalization; memory-intensive due to dependencies across the entire
input vector. \\
\end{longtable}

The choice of activation function should balance computational
considerations with their mathematical properties. This data underscores
the value of evaluating both theoretical and practical performance when
designing neural networks. For large-scale networks or real-time
applications, ReLU is often the best choice due to its efficiency and
scalability. However, for tasks requiring probabilistic outputs, such as
classification, softmax remains indispensable despite its computational
cost. Ultimately, the ideal activation function depends on the specific
task, network architecture, and hardware environment.

Our GPT-2 lighthouse model makes a specific \emph{activation function}
choice that illustrates these trade-offs in practice.

\phantomsection\label{callout-notebookux2a-1.11}
\begin{fbx}{callout-notebook}{AI Engineer’s Notebook:}{GPT-2 GELU Activation Function}
\phantomsection\label{callout-notebook*-1.11}
While Table~\ref{tbl-compare-activations} covers classical activation
functions, GPT-2 uses the Gaussian Error Linear Unit (GELU)
(\citeproc{ref-hendrycks2016gaussian}{Hendrycks and Gimpel 2016}),
defined as: \[
\text{GELU}(x) = x \cdot \Phi(x) = x \cdot \frac{1}{2}\left[1 + \text{erf}\left(\frac{x}{\sqrt{2}}\right)\right]
\]

where \(\Phi(x)\) is the cumulative distribution function of the
standard normal distribution.

\textbf{Why GELU for GPT-2?}

\begin{itemize}
\tightlist
\item
  Smoother gradients than ReLU, reducing the dying neuron problem
\item
  Stochastic regularization effect: acts like dropout by
  probabilistically dropping inputs
\item
  Better empirical performance on language modeling tasks
\end{itemize}

\textbf{System Performance Tradeoff}

\begin{itemize}
\tightlist
\item
  Computational cost: \textasciitilde3 to 4\(\times\) more expensive
  than ReLU (requires erf function evaluation)
\item
  Memory: Same as ReLU (element-wise operation)
\item
  Training time impact: For GPT-2's 48 layers, GELU adds
  \textasciitilde5 to 8\% to total forward pass time
\item
  Worth it: The improved model quality (lower perplexity) offsets the
  computational overhead
\end{itemize}

Frameworks implement fast approximation of GELU using optimized formulas
(Listing~\ref{lst-gelu-approx}). This approximation reduces
computational cost to approximately 1.5\(\times\) ReLU while maintaining
GELU's benefits, demonstrating \emph{how} production systems balance
mathematical properties with implementation efficiency.

\end{fbx}

\begin{codelisting}

\caption{\label{lst-gelu-approx}\textbf{GELU Approximation}: Fast
approximation avoids expensive erf() computation while preserving
activation properties.}

\centering{

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Fast GELU approximation used in production systems}
\CommentTok{\# Avoids expensive erf() computation while}
\CommentTok{\# preserving activation properties}
\NormalTok{gelu\_approx }\OperatorTok{=}\NormalTok{ (}
    \FloatTok{0.5} \OperatorTok{*}\NormalTok{ x }\OperatorTok{*}\NormalTok{ (}\DecValTok{1} \OperatorTok{+}\NormalTok{ tanh(sqrt(}\DecValTok{2} \OperatorTok{/}\NormalTok{ pi) }\OperatorTok{*}\NormalTok{ (x }\OperatorTok{+} \FloatTok{0.044715} \OperatorTok{*}\NormalTok{ x}\OperatorTok{**}\DecValTok{3}\NormalTok{)))}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

}

\end{codelisting}%

The GELU approximation highlights a broader pattern: compute cost is not
always the dominant concern. For activation functions, the real
bottleneck is often memory bandwidth rather than arithmetic operations.
This distinction between compute-bound and memory-bound operations
directly affects optimization priorities and recurs throughout our
analysis of training bottlenecks.

\phantomsection\label{callout-perspectiveux2a-1.12}
\begin{fbx}{callout-perspective}{Systems Perspective:}{Memory Bandwidth Bottlenecks}
\phantomsection\label{callout-perspective*-1.12}
Activation functions reveal a critical systems principle: not all
operations are compute-bound. While matrix multiplications saturate GPU
compute units, activation functions often become memory-bandwidth-bound
for three reasons. First, element-wise operations perform few
calculations per memory access (ReLU performs 1 operation per load).
Second, simple operations complete faster than memory transfer time,
limiting parallelism benefits. Third, modern GPUs have 10--100× more
compute throughput than memory bandwidth.

This explains \emph{why} activation function choice matters less than
expected. ReLU versus sigmoid shows only 2-3x difference despite vastly
different computational complexity, because both are bottlenecked by
memory access. The forward pass must carefully manage activation storage
to prevent memory bandwidth from limiting overall training throughput.

\end{fbx}

Forward pass operations and their computational characteristics
establish \emph{what} training systems must compute, but training
requires updating model parameters based on computed predictions. The
forward pass produces a loss value; optimization algorithms determine
\emph{how} to translate that loss into parameter adjustments that
improve future predictions.

\subsection{Optimization
Algorithms}\label{sec-ai-training-optimization-algorithms-c6a9}

Activation functions determine \emph{what} happens during a single
forward pass: signals transform through the network and produce a
prediction. But training requires thousands of passes, each followed by
parameter adjustments that gradually reduce prediction error.
Optimization algorithms translate gradients into parameter updates that
steer the model toward better performance, governing learning dynamics
across the full training trajectory.

These algorithms explore the complex, high-dimensional loss function
surface, identifying regions where the function achieves its lowest
values. The selection and design of optimization algorithms have
significant system-level implications, including computation efficiency,
memory requirements, and scalability. While this section covers
optimization algorithms used during training, advanced optimization
techniques including quantization, pruning, and knowledge distillation
are detailed in \textbf{?@sec-model-compression}, and systematic
hyperparameter optimization approaches are covered in
\textbf{?@sec-ai-development-workflow}.

\subsubsection{Gradient-Based Optimization
Methods}\label{sec-ai-training-gradientbased-optimization-methods-9798}

In
\textbf{?@sec-deep-learning-systems-foundations-parameter-update-algorithms-b592},
we introduced gradient descent as the fundamental optimization
algorithm: iteratively adjusting parameters in the direction of steepest
descent. That conceptual foundation assumed modest networks on single
devices. Here, we examine \emph{how} gradient descent and its variants
interact with real hardware constraints. The same mathematical operation
that elegantly adjusts weights becomes a significant systems challenge
when models contain billions of parameters and training data spans
terabytes.

\paragraph{Gradient
Descent}\label{sec-ai-training-gradient-descent-4034}

Gradient descent\sidenote{\textbf{Gradient}: From Latin ``gradus''
meaning step or degree, the same root as ``gradual'' and ``grade.'' In
calculus, the gradient points in the direction of steepest ascent, so
gradient \emph{descent} moves opposite to it. The term aptly captures
the iterative, step-by-step nature of optimization: each update takes a
small step downhill on the loss surface, with step size controlled by
the learning rate. } is the mathematical foundation of neural network
training, iteratively adjusting parameters to minimize a loss function.
In training systems, this mathematical operation translates into
specific computational patterns. For each iteration, the system must:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Compute forward pass activations
\item
  Calculate loss value
\item
  Compute gradients through backpropagation
\item
  Update parameters using the gradient values
\end{enumerate}

The computational demands of gradient descent scale with both model size
and dataset size. Computing gradients requires storing intermediate
activations during the forward pass for use in backpropagation. These
activations consume memory proportional to the depth of the network and
the number of examples being processed.

Traditional gradient descent processes the entire dataset in each
iteration. For a training set with 1 million examples, computing
gradients requires evaluating and storing results for each example
before performing a parameter update. This approach poses significant
system challenges:
\[ \text{Memory Required} = N \times \text{(Activation Memory + Gradient Memory)} \]

The memory requirements often exceed available hardware resources on
modern hardware. A ResNet-50 model processing ImageNet-scale datasets
would require hundreds of gigabytes of memory using this approach.
Processing the full dataset before each update creates long iteration
times, reducing the rate at which the model can learn from the data.

\subparagraph{Stochastic Gradient
Descent}\label{sec-ai-training-stochastic-gradient-descent-f356}

These system constraints led to the development of variants that better
align with hardware capabilities. The key insight was that exact
gradient computation, while mathematically appealing, is not necessary
for effective learning. SGD\sidenote{\textbf{Stochastic Gradient
Descent}: ``Stochastic'' derives from Greek ``stochastikos'' meaning
``able to guess'' or ``aim at a target,'' from ``stochos'' (target). The
term captures the essence: rather than computing exact gradients over
all data, we guess the gradient from random samples. Developed by
Robbins and Monro in 1951 for statistical optimization, SGD was first
applied to neural networks by Rosenblatt for the perceptron in 1958.
Today's ``mini-batch SGD'' (processing 32-512 examples) balances the
original single-example approach with full-batch methods. The stochastic
noise in updates often helps escape local minima. } represents a
fundamental shift in optimization strategy, estimating gradients using
individual training examples rather than the entire dataset. This
approach drastically reduces memory requirements since only one
example's activations and gradients need storage at any time.

However, processing single examples creates new system challenges.
Modern accelerators achieve peak performance through parallel
computation, processing multiple data elements simultaneously.
Single-example updates leave most computing resources idle, resulting in
poor hardware utilization. The frequent parameter updates also increase
memory bandwidth requirements, as weights must be read and written for
each example rather than amortizing these operations across multiple
examples.

\paragraph{Mini-batch
Processing}\label{sec-ai-training-minibatch-processing-4eb0}

Mini-batch gradient descent emerges as a practical compromise between
full-batch and stochastic methods, computing gradients over small
batches of examples that align well with modern GPU architectures
(\citeproc{ref-dean2012large}{Dean et al. 2012}). GPUs contain thousands
of cores designed for parallel computation, and mini-batch processing
allows these cores to simultaneously compute gradients for multiple
examples. The \textbf{batch size} \(B\) becomes a key system parameter,
influencing both computational efficiency and memory requirements.

\phantomsection\label{callout-definitionux2a-1.13}
\begin{fbx}{callout-definition}{Definition:}{Batch Processing}
\phantomsection\label{callout-definition*-1.13}
\textbf{\emph{Batch Processing}} is the strategy of trading
\textbf{Latency} for \textbf{Throughput}. By aggregating multiple
examples into a single tensor operation, it amortizes the fixed overhead
of kernel launches and memory transfers, shifting the workload from a
\textbf{Memory-Bound} regime to a \textbf{Compute-Bound} regime to
maximize accelerator utilization.

\end{fbx}

The relationship between batch size and system performance follows clear
patterns that reveal hardware-software trade-offs. Memory requirements
scale linearly with batch size, but the specific costs vary dramatically
by model architecture: \[
\begin{aligned}
\text{Memory Required} = B \times (&\text{Activation Memory} \\
                                   &+ \text{Gradient Memory} \\
                                   &+ \text{Parameter Memory})
\end{aligned}
\]

For concrete understanding, consider ResNet-50 training with different
batch sizes. At batch size 32, the model requires approximately 8 GiB of
activation memory, 4 GiB for gradients, and 200 MiB for parameters per
GPU. Doubling to batch size 64 doubles these memory requirements to 16
GiB activations and 8 GiB gradients. This linear scaling quickly
exhausts GPU memory, with high-end training GPUs typically providing
40--80 GiB of HBM.

Larger batches enable more efficient computation through improved
parallelism and better memory access patterns. GPU utilization
efficiency demonstrates this trade-off: batch sizes of 256 or higher
typically achieve over 90\% hardware utilization on modern training
accelerators, while smaller batches of 16--32 may only achieve 60--70\%
utilization due to insufficient parallelism to saturate the hardware.
Linear scaling rules for large-batch training help maintain convergence
speed (\citeproc{ref-goyal2017accurate}{Goyal et al. 2017}).

This establishes a central theme in training systems: the
hardware-software trade-off between memory constraints and computational
efficiency. Training systems must select batch sizes that maximize
hardware utilization while fitting within available memory. The optimal
choice often requires gradient accumulation when memory constraints
prevent using efficiently large batches, trading increased computation
for the same effective batch size.

\subsubsection{Adaptive and Momentum-Based
Optimizers}\label{sec-ai-training-adaptive-momentumbased-optimizers-f079}

SGD computes correct gradients but struggles with ill-conditioned loss
landscapes where some dimensions are steep (requiring small steps) while
others are shallow (benefiting from large steps). A single learning rate
either oscillates dangerously in steep dimensions or moves glacially in
shallow ones. Each subsequent optimizer we examine solves a specific
limitation of its predecessors: momentum smooths oscillations by
averaging gradient history, RMSprop adapts step sizes per parameter, and
Adam combines both strategies. Understanding this progression clarifies
why Adam became the default choice for transformer training while
revealing the system costs, specifically memory and computation, that
each refinement introduces (\citeproc{ref-kingma2014adam}{Kingma and Ba
2014}).

\textbf{Momentum-Based Methods.} Momentum
methods\sidenote{\textbf{Momentum}: Borrowed directly from physics,
where momentum (mass times velocity) describes an object's tendency to
continue moving. In optimization, the metaphor is apt: just as a ball
rolling downhill accumulates momentum and can roll through small bumps,
gradient updates accumulate velocity to overcome local irregularities in
the loss surface. The physics analogy, introduced by Polyak in 1964,
made this abstract optimization concept intuitive to researchers. }
address SGD's oscillation problem by accumulating a velocity vector
across iterations, smoothing out noisy gradient directions. From a
systems perspective, this smoothing comes at a cost: the training system
must maintain a velocity vector with the same dimensionality as the
parameter vector, effectively doubling the memory needed for
optimization state.

\textbf{Adaptive Learning Rate Methods.} While momentum smooths gradient
direction, it does not address the different scales of gradients across
parameters. RMSprop solves this by maintaining a moving average of
squared gradients for each parameter, automatically reducing step sizes
for parameters with historically large gradients. This per-parameter
adaptation requires storing the moving average \(s_t\), creating memory
overhead similar to momentum methods. The element-wise operations in
RMSprop also introduce additional computational steps compared to basic
gradient descent.

\textbf{Adam Optimization.} Adam\sidenote{\textbf{Adam (Adaptive Moment
Estimation)}: Introduced by Kingma and Ba in 2015, Adam became the
default optimizer for deep learning due to its robust performance across
diverse architectures. The algorithm maintains per-parameter learning
rates using first and second moment estimates, requiring 3x the memory
of SGD (parameters + two state vectors). For a 7B model in FP32, this
means 84 GiB for optimizer state alone, driving the adoption of
memory-efficient variants like 8-bit Adam (2x compression) and GaLoRE
(gradient low-rank projection). } combines the benefits of both momentum
and RMSprop: momentum's gradient smoothing addresses noisy updates,
while RMSprop's adaptive scaling handles parameter-specific step sizes.
This combination maintains two moving averages for each parameter:
\begin{gather*}
m_t = \beta_1 m_{t-1} + (1-\beta_1)\nabla L(\theta_t)
\\
v_t = \beta_2 v_{t-1} + (1-\beta_2)\big(\nabla L(\theta_t)\big)^2
\\
\theta_{t+1} = \theta_t - \alpha \frac{m_t}{\sqrt{v_t + \epsilon}}
\end{gather*}

The system implications of Adam are more substantial than previous
methods. The optimizer must store two additional vectors (\(m_t\) and
\(v_t\)) for each parameter, tripling the memory required for
optimization state. For a model with 100 million parameters using 32-bit
floating-point numbers, the additional memory requirement is
approximately 800 MB.

\subsubsection{Optimization Algorithm System
Implications}\label{sec-ai-training-optimization-algorithm-system-implications-f9f2}

\phantomsection\label{sec-ai-training-optimization-tradeoffs-77c5}{} The
choice of optimization algorithm creates specific patterns of
computation and memory access that influence training efficiency. Memory
requirements increase progressively from SGD (\(1\times\) model size)
through Momentum (\(2\times\)) to Adam (\(3\times\)), as quantified in
Table~\ref{tbl-optimizer-properties}. These memory costs must be
balanced against convergence\sidenote{\textbf{Convergence}: From Latin
``convergere'' (to incline together), combining ``con-'' (together) +
``vergere'' (to bend, turn). In optimization, convergence describes the
process by which iterative algorithms approach a stable solution, where
successive updates become smaller until parameters stabilize at a
minimum. Training is said to converge when the loss stops decreasing
meaningfully, typically requiring 10,000-100,000 iterations for large
models. } benefits. While Adam often requires fewer iterations to reach
convergence, its per-iteration memory and computation overhead may
impact training speed on memory-constrained systems. The concrete scale
of these \emph{GPT-2 optimizer memory requirements} illustrates just how
significant this overhead becomes for large models.

\phantomsection\label{notebook-gpt2-optimizer}
\begin{fbx}{callout-notebook}{AI Engineer’s Notebook:}{GPT-2 Optimizer Memory Requirements}
\phantomsection\label{notebook-gpt2-optimizer}
GPT-2 training uses the Adam optimizer with these hyperparameters:

\begin{itemize}
\tightlist
\item
  β₁ = 0.9 (momentum decay)
\item
  β₂ = 0.999 (second moment decay)
\item
  Learning rate: Warmed up from 0 to 2.5e-4 over first 500 steps, then
  cosine decay
\item
  Weight decay: 0.01
\item
  Gradient clipping: Global norm clipping at 1.0
\end{itemize}

\textbf{Memory Overhead Calculation}

For GPT-2's 1.5B parameters in FP32 (4 bytes each):

\begin{itemize}
\tightlist
\item
  Parameters: 1.5B × 4 bytes = 6.0 GiB
\item
  Gradients: 1.5B × 4 bytes = 6.0 GiB
\item
  Adam State (m, v): 1.5B × 8 bytes = 12.0 GiB
\item
  Total static memory: 24 GiB
\end{itemize}

This explains why GPT-2 training requires 32 GiB+ V100 GPUs even before
considering activation memory.

\textbf{System Decisions Driven by Optimizer}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Mixed precision training (FP16) reduces operation precision but
  requires keeping FP32 master weights, maintaining the static memory
  footprint at \textasciitilde24 GiB.
\item
  Gradient accumulation (splitting effective batches into smaller
  micro-batches) allows effective batch\_size=512 despite memory limits.
\end{enumerate}

Adam's memory overhead is a necessary trade-off for convergence. GPT-2
converges in \textasciitilde50K steps vs.~\textasciitilde150K+ steps
with SGD+Momentum, saving weeks of training time despite higher per-step
cost.

\end{fbx}

\textbf{Implementation Considerations.} The efficient implementation of
optimization algorithms in training frameworks depends on system-level
considerations that directly influence performance: memory bandwidth
management, operation fusion techniques, and numerical precision
optimization. Together, these factors determine the computational
efficiency, memory utilization, and scalability of optimizers across
hardware architectures.

Memory bandwidth presents the primary bottleneck in optimizer
implementation. Modern frameworks address this through operation fusion,
which reduces memory access overhead by combining multiple operations
into a single kernel. For example, the Adam optimizer's memory access
requirements can grow linearly with parameter size when operations are
performed separately:
\[ \text{Bandwidth}_{\text{separate}} = 5 \times \text{Size}_{\text{params}} \]

However, fusing these operations into a single computational kernel
significantly reduces the bandwidth requirement:
\[ \text{Bandwidth}_{\text{fused}} = 2 \times \text{Size}_{\text{params}} \]

These techniques have been effectively demonstrated in systems like
cuDNN and other GPU-accelerated frameworks that optimize memory
bandwidth usage and operation fusion
(\citeproc{ref-chetlur2014cudnn}{Chetlur et al. 2014};
\citeproc{ref-jouppi2017tpu}{Jouppi et al. 2017}).

Memory access patterns also affect the efficiency of cache utilization.
Sequential access to parameter and optimizer state vectors maximizes
cache hit rates and effective memory bandwidth. This principle is
evident in hardware such as GPUs and tensor processing units (TPUs),
where optimized memory layouts significantly improve performance
(\citeproc{ref-jouppi2017tpu}{Jouppi et al. 2017}).

Numerical precision represents another key tradeoff in implementation.
Empirical studies have shown that optimizer states remain stable even
when reduced precision formats, such as 16-bit floating-point (FP16),
are used. Transitioning from 32-bit to 16-bit formats reduces memory
requirements, as illustrated for the Adam optimizer:
\[ \text{Memory}_{\text{Adam-FP16}} = \frac{3}{2} \times \text{Size}_{\text{params}} \]

Mixed-precision training
(Section~\ref{sec-ai-training-mixedprecision-training-9218}) has been
shown to achieve comparable accuracy while significantly reducing memory
consumption and computational overhead
(\citeproc{ref-micikevicius2017mixed}{Micikevicius et al. 2017}).

These implementation factors determine the practical performance of
optimization algorithms in deep learning systems, requiring careful
alignment of memory, computational, and numerical strategies with the
underlying hardware architecture (\citeproc{ref-chen2015mxnet}{Chen et
al. 2015}).

\textbf{Optimizer Trade-offs.} Optimization algorithms in neural network
training sit at the intersection of algorithmic efficiency and system
performance. While optimizers were developed to improve model
convergence, their implementation significantly impacts memory usage,
computational requirements, and hardware utilization.

A deeper examination of popular optimization algorithms reveals their
varying impacts on system resources. Examine
Table~\ref{tbl-optimizer-properties} to see how memory costs scale from
1x for SGD to 3x for Adam, with corresponding differences in hardware
efficiency and convergence speed that directly influence training system
design decisions. SGD maintains minimal memory overhead, requiring
storage only for model parameters and current gradients. This
lightweight memory footprint comes at the cost of slower convergence and
potentially poor hardware utilization due to its sequential update
nature.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2000}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2000}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2000}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2000}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2000}}@{}}
\caption{\textbf{Optimizer Memory Footprint.} Different optimization
algorithms impose varying memory costs due to the storage of
intermediate values like gradients, velocities, and squared gradients.
Understanding these trade-offs is important for resource-constrained
deployments and large-scale model
training.}\label{tbl-optimizer-properties}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Property}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{SGD}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Momentum}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{RMSprop}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Adam}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Property}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{SGD}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Momentum}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{RMSprop}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Adam}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Memory Overhead} & None & Velocity terms & Squared gradients &
Both velocity and squared gradients \\
\textbf{Memory Cost} & \(1\times\) & \(2\times\) & \(2\times\) &
\(3\times\) \\
\textbf{Access Pattern} & Sequential & Sequential & Random & Random \\
\textbf{Operations/Parameter} & 2 & 3 & 4 & 5 \\
\textbf{Hardware Efficiency} & Low & Medium & High & Highest \\
\textbf{Convergence Speed} & Slowest & Medium & Fast & Fastest \\
\end{longtable}

Momentum methods introduce additional memory requirements by storing
velocity terms for each parameter, doubling the memory footprint
compared to SGD. This increased memory cost brings improved convergence
through better gradient estimation, while maintaining relatively
efficient memory access patterns. The sequential nature of momentum
updates allows for effective hardware prefetching and cache utilization.

RMSprop adapts learning rates per parameter by tracking squared gradient
statistics. Its memory overhead matches momentum methods, but its
computation patterns become more irregular. The algorithm requires
additional arithmetic operations for maintaining running averages and
computing adaptive learning rates, increasing computational intensity
from 3 to 4 operations per parameter.

Adam combines the benefits of momentum and adaptive learning rates, but
at the highest system resource cost. Variants like AdamW
(\citeproc{ref-loshchilov2019adamw}{Loshchilov and Hutter 2019})
decouple weight decay from the gradient update, improving generalization
performance. Table~\ref{tbl-optimizer-properties} reveals that it
maintains both velocity terms and squared gradient statistics, tripling
the memory requirements compared to SGD. The algorithm's computational
patterns involve 5 operations per parameter update, though these
operations often utilize hardware more effectively due to their regular
structure and potential for parallelization.

Training system designers must balance these trade-offs when selecting
optimization strategies. GPUs excel at the parallel computations
required by adaptive methods, while memory-constrained systems might
favor simpler optimizers. The choice of optimizer affects not only
training dynamics but also maximum feasible model size, achievable batch
size, hardware utilization efficiency, and overall training time to
convergence. Training frameworks continue developing techniques like
optimizer state sharding, mixed-precision storage, and fused operations
to better balance these competing demands.

\subsubsection{Framework Optimizer Interface and
Scheduling}\label{sec-ai-training-framework-optimizer-interface-82ff}

Frameworks provide standardized interfaces that abstract optimization
algorithms into practical training loops. The framework optimizer
interface follows a consistent pattern that separates gradient
computation from parameter updates. Listing~\ref{lst-adam-training}
demonstrates how Adam optimization integrates into a standard training
loop.

\begin{codelisting}

\caption{\label{lst-adam-training}\textbf{Adam Training Loop}: Standard
four-step optimization cycle with gradient clearing, forward pass,
backward pass, and parameter update.}

\centering{

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ torch}
\ImportTok{import}\NormalTok{ torch.nn }\ImportTok{as}\NormalTok{ nn}
\ImportTok{import}\NormalTok{ torch.optim }\ImportTok{as}\NormalTok{ optim}

\CommentTok{\# Initialize Adam optimizer with model parameters}
\CommentTok{\# and learning rate}
\NormalTok{optimizer }\OperatorTok{=}\NormalTok{ optim.Adam(}
\NormalTok{    model.parameters(), lr}\OperatorTok{=}\FloatTok{0.001}\NormalTok{, betas}\OperatorTok{=}\NormalTok{(}\FloatTok{0.9}\NormalTok{, }\FloatTok{0.999}\NormalTok{)}
\NormalTok{)}
\NormalTok{loss\_function }\OperatorTok{=}\NormalTok{ nn.CrossEntropyLoss()}

\CommentTok{\# Standard training loop implementing the four{-}step optimization cycle}
\ControlFlowTok{for}\NormalTok{ epoch }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(num\_epochs):}
    \ControlFlowTok{for}\NormalTok{ batch\_idx, (data, targets) }\KeywordTok{in} \BuiltInTok{enumerate}\NormalTok{(dataloader):}
        \CommentTok{\# Step 1: Clear accumulated gradients from previous iteration}
\NormalTok{        optimizer.zero\_grad()}

        \CommentTok{\# Step 2: Forward pass {-} compute model predictions}
\NormalTok{        predictions }\OperatorTok{=}\NormalTok{ model(data)}
\NormalTok{        loss }\OperatorTok{=}\NormalTok{ loss\_function(predictions, targets)}

        \CommentTok{\# Step 3: Backward pass {-} compute gradients via}
        \CommentTok{\# automatic differentiation}
\NormalTok{        loss.backward()}

        \CommentTok{\# Step 4: Parameter update {-} apply Adam optimization equations}
\NormalTok{        optimizer.step()}
\end{Highlighting}
\end{Shaded}

}

\end{codelisting}%

The \texttt{optimizer.zero\_grad()} call addresses a critical framework
implementation detail: gradients accumulate across calls to
\texttt{backward()}, requiring explicit clearing between batches. This
behavior enables gradient accumulation patterns for large effective
batch sizes but requires careful management in standard training loops.

The \texttt{optimizer.step()} method encapsulates the mathematical
update equations. For Adam optimization, this single call implements the
momentum estimation, squared gradient tracking, bias correction, and
parameter update computation automatically.
Listing~\ref{lst-adam-internals} illustrates the mathematical operations
that occur within the optimizer.

\begin{codelisting}

\caption{\label{lst-adam-internals}\textbf{Adam Optimizer Internals}:
Mathematical operations implemented by optimizer.step(), showing
momentum estimation, variance tracking, bias correction, and parameter
updates.}

\centering{

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Mathematical operations implemented by optimizer.step() for Adam}
\CommentTok{\# These computations happen automatically within the framework}

\CommentTok{\# Adam hyperparameters (typically β₁=0.9, β₂=0.999, ε=1e{-}8)}
\NormalTok{beta\_1, beta\_2, epsilon }\OperatorTok{=} \FloatTok{0.9}\NormalTok{, }\FloatTok{0.999}\NormalTok{, }\FloatTok{1e{-}8}
\NormalTok{learning\_rate }\OperatorTok{=} \FloatTok{0.001}

\CommentTok{\# For each parameter tensor in the model:}
\ControlFlowTok{for}\NormalTok{ param }\KeywordTok{in}\NormalTok{ model.parameters():}
    \ControlFlowTok{if}\NormalTok{ param.grad }\KeywordTok{is} \KeywordTok{not} \VariableTok{None}\NormalTok{:}
\NormalTok{        grad }\OperatorTok{=}\NormalTok{ param.grad.data  }\CommentTok{\# Current gradient}

        \CommentTok{\# Step 1: Update biased first moment estimate}
        \CommentTok{\# (momentum)}
        \CommentTok{\# m\_t = β₁ * m\_\{t{-}1\} + (1{-}β₁) * ∇L(θₜ)}
\NormalTok{        momentum\_buffer }\OperatorTok{=}\NormalTok{ (}
\NormalTok{            beta\_1 }\OperatorTok{*}\NormalTok{ momentum\_buffer }\OperatorTok{+}\NormalTok{ (}\DecValTok{1} \OperatorTok{{-}}\NormalTok{ beta\_1) }\OperatorTok{*}\NormalTok{ grad}
\NormalTok{        )}

        \CommentTok{\# Step 2: Update biased second moment estimate}
        \CommentTok{\# (squared gradients)}
        \CommentTok{\# v\_t = β₂ * v\_\{t{-}1\} + (1{-}β₂) * (∇L(θₜ))²}
\NormalTok{        variance\_buffer }\OperatorTok{=}\NormalTok{ beta\_2 }\OperatorTok{*}\NormalTok{ variance\_buffer }\OperatorTok{+}\NormalTok{ (}
            \DecValTok{1} \OperatorTok{{-}}\NormalTok{ beta\_2}
\NormalTok{        ) }\OperatorTok{*}\NormalTok{ grad.}\BuiltInTok{pow}\NormalTok{(}\DecValTok{2}\NormalTok{)}

        \CommentTok{\# Step 3: Compute bias{-}corrected estimates}
\NormalTok{        momentum\_corrected }\OperatorTok{=}\NormalTok{ momentum\_buffer }\OperatorTok{/}\NormalTok{ (}
            \DecValTok{1} \OperatorTok{{-}}\NormalTok{ beta\_1}\OperatorTok{**}\NormalTok{step\_count}
\NormalTok{        )}
\NormalTok{        variance\_corrected }\OperatorTok{=}\NormalTok{ variance\_buffer }\OperatorTok{/}\NormalTok{ (}
            \DecValTok{1} \OperatorTok{{-}}\NormalTok{ beta\_2}\OperatorTok{**}\NormalTok{step\_count}
\NormalTok{        )}

        \CommentTok{\# Step 4: Apply parameter update}
        \CommentTok{\# θ\_\{t+1\} = θₜ {-} α * m\_t / (√v\_t + ε)}
\NormalTok{        param.data }\OperatorTok{{-}=}\NormalTok{ (}
\NormalTok{            learning\_rate}
            \OperatorTok{*}\NormalTok{ momentum\_corrected}
            \OperatorTok{/}\NormalTok{ (variance\_corrected.sqrt() }\OperatorTok{+}\NormalTok{ epsilon)}
\NormalTok{        )}
\end{Highlighting}
\end{Shaded}

}

\end{codelisting}%

Framework implementations also handle the memory management challenges
in optimizer trade-offs. The optimizer automatically allocates storage
for momentum terms and squared gradient statistics, managing the
2--3\(\times\) memory overhead transparently while providing efficient
memory access patterns optimized for the underlying hardware.

\textbf{Learning Rate Scheduling Integration.} Frameworks integrate
learning rate scheduling directly into the optimizer interface, enabling
dynamic adjustment of the learning rate α during training. This
integration demonstrates how frameworks compose multiple optimization
techniques through modular design patterns.

Learning rate schedulers modify the optimizer's learning rate according
to predefined schedules, such as cosine annealing, exponential decay, or
step-wise reductions. Listing~\ref{lst-cosine-annealing} demonstrates
how to integrate cosine annealing with Adam optimization.

\begin{codelisting}

\caption{\label{lst-cosine-annealing}\textbf{Cosine Annealing
Scheduler}: Learning rate scheduling with cosine annealing integrated
into the training loop.}

\centering{

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ torch}
\ImportTok{import}\NormalTok{ torch.optim }\ImportTok{as}\NormalTok{ optim}
\ImportTok{import}\NormalTok{ torch.optim.lr\_scheduler }\ImportTok{as}\NormalTok{ lr\_scheduler}
\ImportTok{import}\NormalTok{ math}

\CommentTok{\# Initialize optimizer with initial learning rate}
\NormalTok{optimizer }\OperatorTok{=}\NormalTok{ optim.Adam(}
\NormalTok{    model.parameters(), lr}\OperatorTok{=}\FloatTok{0.001}\NormalTok{, weight\_decay}\OperatorTok{=}\FloatTok{1e{-}4}
\NormalTok{)}

\CommentTok{\# Configure cosine annealing scheduler}
\CommentTok{\# T\_max: number of epochs for one complete cosine cycle}
\CommentTok{\# eta\_min: minimum learning rate (default: 0)}
\NormalTok{scheduler }\OperatorTok{=}\NormalTok{ lr\_scheduler.CosineAnnealingLR(}
\NormalTok{    optimizer,}
\NormalTok{    T\_max}\OperatorTok{=}\DecValTok{100}\NormalTok{,  }\CommentTok{\# Complete cycle over 100 epochs}
\NormalTok{    eta\_min}\OperatorTok{=}\FloatTok{1e{-}6}\NormalTok{,  }\CommentTok{\# Minimum learning rate}
\NormalTok{)}

\CommentTok{\# Training loop with integrated learning rate scheduling}
\ControlFlowTok{for}\NormalTok{ epoch }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(num\_epochs):}
    \CommentTok{\# Track learning rate for monitoring}
\NormalTok{    current\_lr }\OperatorTok{=}\NormalTok{ optimizer.param\_groups[}\DecValTok{0}\NormalTok{][}\StringTok{"lr"}\NormalTok{]}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Epoch }\SpecialCharTok{\{}\NormalTok{epoch}\SpecialCharTok{\}}\SpecialStringTok{: Learning Rate = }\SpecialCharTok{\{}\NormalTok{current\_lr}\SpecialCharTok{:.6f\}}\SpecialStringTok{"}\NormalTok{)}

    \CommentTok{\# Standard training loop}
    \ControlFlowTok{for}\NormalTok{ batch\_idx, (data, targets) }\KeywordTok{in} \BuiltInTok{enumerate}\NormalTok{(dataloader):}
\NormalTok{        optimizer.zero\_grad()}
\NormalTok{        predictions }\OperatorTok{=}\NormalTok{ model(data)}
\NormalTok{        loss }\OperatorTok{=}\NormalTok{ loss\_function(predictions, targets)}
\NormalTok{        loss.backward()}
\NormalTok{        optimizer.step()}

    \CommentTok{\# Update learning rate at end of epoch}
    \CommentTok{\# Implements: lr = eta\_min + (eta\_max {-} eta\_min)}
    \CommentTok{\#             * (1 + cos(π * epoch / T\_max)) / 2}
\NormalTok{    scheduler.step()}
\end{Highlighting}
\end{Shaded}

}

\end{codelisting}%

This composition pattern allows practitioners to combine base
optimization algorithms (SGD, Adam) with scheduling strategies (cosine
annealing, linear warmup) without modifying the core mathematical
implementations.

The optimization algorithms above specify \emph{how} to update
parameters given gradients. But the gradients themselves must be
computed, and that computation introduces its own substantial costs.
Backpropagation traces error signals backward through the network to
attribute responsibility and compute the gradients that optimizers
consume.

\subsection{Backpropagation
Mechanics}\label{sec-ai-training-backpropagation-mechanics-0b64}

The optimization algorithms above specify how to update parameters given
gradients, but where do those gradients come from, and what does
computing them cost? The backpropagation algorithm answers the first
question; its memory and computational requirements answer the second,
revealing why training systems face such substantial resource
constraints.

The backpropagation algorithm\sidenote{\textbf{Backpropagation
Algorithm}: Independently rediscovered multiple times, backpropagation
was popularized by Rumelhart, Hinton, and Williams in 1986 (though
similar ideas appeared in Werbos 1974). This breakthrough enabled
training of deep networks by efficiently computing gradients in O(n)
time vs.~naive O(n²) approaches. Modern implementations require careful
memory management since storing all activations for a ResNet-50 consumes
1.2 GiB per image. } computes gradients by systematically moving
backward through a neural network's computational graph. In
\textbf{?@sec-deep-learning-systems-foundations-gradient-computation-backpropagation-dacf},
we established the mathematical foundation: the chain rule breaks
gradient computation into layer-by-layer operations, with each layer
receiving adjustment signals proportional to its contribution to the
final error. If terms like ``computational graph'' or ``gradient flow''
feel unfamiliar, the factory assembly line analogy in that section is
worth revisiting.

Here, we shift focus from \emph{what} backpropagation computes to
\emph{what it costs} to compute it at scale. The familiar equations from
\textbf{?@sec-deep-learning-systems-foundations} reappear because
understanding their structure reveals exactly \emph{what} must be stored
and \emph{when}. During the forward pass, each layer computes
activations \(a^{(l)} = f(W^{(l)}a^{(l-1)} + b^{(l)})\) that must be
retained for the backward pass. Computing
\(\frac{\partial L}{\partial W^{(l)}}\) requires access to these stored
activations, creating memory requirements that scale with network depth
and batch size.

A simple three-layer network processing MNIST requires kilobytes of
activation storage. GPT-2 processing a single batch requires over 30
gigabytes, more than most GPUs can hold. That gap defines the
engineering challenge this chapter addresses. Modern training systems
use autodifferentiation\sidenote{\textbf{Automatic Differentiation}: Not
to be confused with symbolic or numerical differentiation, autodiff
constructs a computational graph at runtime and applies the chain rule
systematically. PyTorch uses ``define-by-run'' (dynamic graphs built
during forward pass) while TensorFlow v1 used static graphs. This
enables complex architectures like RNNs and transformers where graph
structure changes dynamically, but requires careful memory management
since the entire forward computation graph must be preserved for the
backward pass. } to handle gradient computations automatically, but the
underlying memory and computation patterns remain the systems engineer's
responsibility to manage.

\subsubsection{Activation Memory
Requirements}\label{sec-ai-training-activation-memory-requirements-f44c}

Training systems must maintain intermediate values (activations) from
the forward pass to compute gradients during the backward pass. This
requirement compounds the memory demands of optimization algorithms. For
each layer l, the system must store:

\begin{itemize}
\tightlist
\item
  Input activations from the forward pass
\item
  Output activations after applying layer operations
\item
  Layer parameters being optimized
\item
  Computed gradients for parameter updates
\end{itemize}

Consider a batch of training examples passing through a network. The
forward pass computes and stores: \begin{gather*}
z^{(l)} = W^{(l)}a^{(l-1)} + b^{(l)}
\\
a^{(l)} = f(z^{(l)})
\end{gather*}

Both \(z^{(l)}\) and \(a^{(l)}\) must be cached for the backward pass.
This creates a multiplicative effect on memory usage: each layer's
memory requirement is multiplied by the batch size, and the optimizer's
memory overhead (discussed in the previous section) applies to each
parameter.

The total memory needed scales with:

\begin{itemize}
\tightlist
\item
  Network depth (number of layers)
\item
  Layer widths (number of parameters per layer)
\item
  Batch size (number of examples processed together)
\item
  Optimizer state (additional memory for algorithms like Adam)
\end{itemize}

This creates a complex set of trade-offs. Larger batch sizes enable more
efficient computation and better gradient estimates for optimization,
but require proportionally more memory for storing activations. More
sophisticated optimizers like Adam can achieve faster convergence but
require additional memory per parameter.

Quantifying these trade-offs for our GPT-2 lighthouse model reveals the
scale of the \emph{activation memory} challenge.

We can see this in detail by examining the \emph{GPT-2 activation memory
breakdown}.

\phantomsection\label{callout-notebookux2a-1.15}
\begin{fbx}{callout-notebook}{AI Engineer’s Notebook:}{GPT-2 Activation Memory Breakdown}
\phantomsection\label{callout-notebook*-1.15}
For GPT-2 with batch\_size=32, seq\_len=1024, hidden\_dim=1600, 48
layers:

\subsubsection{Per-Layer Activation
Memory}\label{per-layer-activation-memory}

\begin{itemize}
\tightlist
\item
  Attention activations: \texttt{batch\ ×\ seq\ ×\ hidden\ ×\ 4} (Q, K,
  V, output) = 32 × 1024 × 1600 × 4 × 2 bytes (FP16) = 419 MB
\item
  FFN activations: \texttt{batch\ ×\ seq\ ×\ (hidden\ ×\ 4)}
  (intermediate expansion) = 32 × 1024 × 6400 × 2 bytes = 419 MB
\item
  Layer norm states: Minimal (\textasciitilde10 MB per layer)
\item
  Total per layer: \textasciitilde850 MB
\end{itemize}

\subsubsection{Full Model Activation
Memory}\label{full-model-activation-memory}

\begin{itemize}
\tightlist
\item
  48 layers × \textasciitilde850 MiB = \textbf{40.7 GB} just for
  activations
\item
  Parameters (FP16): 3 GB
\item
  Gradients: 3 GB
\item
  Optimizer state (Adam, FP32): 12 GB
\item
  Peak memory during training: \textbf{\textasciitilde59 GiB}
\end{itemize}

This exceeds a single V100's 32 GiB capacity.

\subsubsection{System Solutions Applied}\label{system-solutions-applied}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Gradient checkpointing: Recompute activations during backward pass,
  reducing activation memory by 75\% (to \textasciitilde8 GiB) at cost
  of 33\% more compute
\item
  Activation CPU offloading: Store some activations in CPU RAM, transfer
  during backward pass
\item
  Mixed precision: FP16 activations (already applied above) vs FP32
  (would be 65 GB)
\item
  Reduced batch size: Use batch\_size=16 per GPU + gradient accumulation
  over 2 steps = effective batch\_size=32
\end{enumerate}

Most GPT-2 implementations use a training configuration of gradient
checkpointing and batch\_size=16 per GPU, fitting comfortably in 32 GB
V100s while maintaining training efficiency.

\end{fbx}

This breakdown illustrates the practical engineering decisions required
when GPU memory falls short. Before examining the mathematical details
of memory-computation trade-offs, check your understanding of these core
concepts.

\phantomsection\label{callout-checkpointux2a-1.16}
\begin{fbx}{callout-checkpoint}{Checkpoint:}{The Memory-Compute Tradeoff}
\phantomsection\label{callout-checkpoint*-1.16}

Training large models requires managing the memory wall.

\textbf{The Bottleneck}

\begin{itemize}
\tightlist
\item[$\square$]
  \textbf{Activation Memory}: Do you understand why activations (stored
  for backprop) dominate memory usage, often exceeding parameter size by
  10x?
\item[$\square$]
  \textbf{Optimization Strategy}: Can you explain how \textbf{Gradient
  Checkpointing} trades compute (re-calculating activations) for memory
  capacity?
\end{itemize}

\textbf{Scaling Limits}

\begin{itemize}
\tightlist
\item[$\square$]
  \textbf{Batch Size Constraints}: Why does memory capacity limit the
  maximum batch size, and how does \textbf{Gradient Accumulation} solve
  this without increasing memory?
\end{itemize}

\end{fbx}

\subsubsection{Memory-Computation
Trade-offs}\label{sec-ai-training-memorycomputation-tradeoffs-411e}

Training systems must balance memory usage against computational
efficiency. Each forward pass through the network generates a set of
activations that must be stored for the backward pass. For a neural
network with \(L\) layers, processing a batch of \(B\) examples requires
storing:
\[ \text{Memory per batch} = B \times \sum_{l=1}^L (s_l + a_l) \] where
\(s_l\) represents the size of intermediate computations (like
\(z^{(l)}\)) and \(a_l\) represents the activation outputs at layer l.

This memory requirement compounds with the optimizer's memory needs
discussed in the previous section. The total memory consumption of a
training system includes both the stored activations and the optimizer
state:
\[ \text{Total Memory} = \text{Memory per batch} + \text{Memory}_{\text{optimizer}} \]

To manage these substantial memory requirements, training systems use
several sophisticated strategies. Gradient checkpointing is a basic
approach, strategically recomputing some intermediate values during the
backward pass rather than storing them. While this increases
computational work, it can significantly reduce memory usage, enabling
training of deeper networks or larger batch sizes on memory-constrained
hardware (\citeproc{ref-chen2016training}{Chen et al. 2016}).

The efficiency of these memory management strategies depends heavily on
the underlying hardware architecture. GPU systems, with their high
computational throughput but limited memory bandwidth, often encounter
different bottlenecks than CPU systems. Memory bandwidth limitations on
GPUs mean that even when sufficient storage exists, moving data between
memory and compute units can become the primary performance constraint
(\citeproc{ref-jouppi2017tpu}{Jouppi et al. 2017}).

These hardware considerations naturally guide the implementation of
backpropagation in modern training systems. Responding to these
constraints, specialized memory-efficient algorithms for operations like
convolutions compute gradients in tiles or chunks, adapting to available
memory bandwidth. Dynamic memory management tracks the lifetime of
intermediate values throughout the computation graph, deallocating
memory as soon as tensors become unnecessary for subsequent computations
(\citeproc{ref-paszke2019pytorch}{Paszke et al. 2019}).

\subsection{Mathematical Foundations System
Implications}\label{sec-ai-training-mathematical-foundations-system-implications-7cd3}

The mathematical operations we have examined (forward propagation,
gradient computation, and parameter updates) define what training
systems must compute. Understanding these operations in mathematical
terms is essential, but implementing them in practical training systems
requires translating mathematical abstractions into orchestrated
computational workflows. This translation introduces distinct challenges
centered on resource coordination, timing, and data movement.

Before examining pipeline architecture in detail, one more analytical
tool is essential: understanding whether operations are limited by
compute throughput or memory bandwidth. This distinction, captured by
arithmetic intensity, determines which optimization strategies will
prove effective.

\subsection{Arithmetic Intensity and Training
Bottlenecks}\label{sec-ai-training-arithmetic-intensity-training-bottlenecks-4446}

To understand why certain optimizations matter more than others, we must
analyze whether operations are compute-bound or memory-bound. Arithmetic
intensity (AI) measures this relationship:

\[
\text{Arithmetic Intensity} = \frac{\text{FLOPs}}{\text{Bytes Moved}}
\]

Operations with high arithmetic intensity are compute-bound: their
performance is limited by the processor's computational throughput.
Operations with low arithmetic intensity are memory-bound: they spend
more time moving data than computing.

Consider Table~\ref{tbl-training-arithmetic-intensity}: dense matrix
multiplication achieves O(n) FLOP/byte (compute-bound), while activation
functions operate at just 0.25 FLOP/byte (memory-bound), explaining why
optimization strategies must differ fundamentally between these
operation types.

\begin{longtable}[]{@{}lrl@{}}
\caption{\textbf{Training Operation Classifications.} Different
operations in the training pipeline have vastly different arithmetic
intensities, determining whether they are limited by compute throughput
or memory
bandwidth.}\label{tbl-training-arithmetic-intensity}\tabularnewline
\toprule\noalign{}
\textbf{Operation} & \textbf{Arithmetic Intensity} &
\textbf{Classification} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\textbf{Operation} & \textbf{Arithmetic Intensity} &
\textbf{Classification} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Dense MatMul (large)} & O(n) FLOP/byte & Compute-bound \\
\textbf{Activation functions} & 0.25 FLOP/byte (FP16) & Memory-bound \\
\textbf{LayerNorm/BatchNorm} & \textasciitilde10 FLOP/byte &
Memory-bound \\
\textbf{Attention softmax} & \textasciitilde5 FLOP/byte &
Memory-bound \\
\end{longtable}

Figure~\ref{fig-training-roofline} visualizes these relationships on a
roofline diagram. Operations to the left of the ridge point (the
``knee'' where the sloped memory-bound region meets the flat
compute-bound region) are limited by memory bandwidth; operations to the
right are limited by compute throughput. The figure shows how GPT-2
training operations distribute across this landscape.

\begin{figure}[htb]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/f299acdc83adc95d04ee51a798a53c9e7e23073d.pdf}}

}

\caption{\label{fig-training-roofline}\textbf{Training Roofline Model}:
GPT-2 training operations mapped against arithmetic intensity on a
log-log roofline diagram. Matrix multiplications operate in the
compute-bound regime (right of the ridge point), while normalization and
activation operations fall in the memory-bound region (left).
FlashAttention shifts standard attention from below to above the ridge
point, demonstrating how algorithmic redesign can move operations into a
more efficient regime.}

\end{figure}%

Consider the GPT-2 Attention Layer where Q, K, V projections with
dimensions (B × S × H) multiplied by (H × H) produce BSH² FLOPs. Data
movement requires reading Q, K, V (3 × BSH × 2 bytes) plus writing the
output (BSH × 2 bytes). The arithmetic intensity equals BSH² divided by
(8BSH), which simplifies to H/8. For GPT-2 with H=768, this yields 96
FLOP/byte---below the A100's ridge point, making standard attention
memory-bound.

GPUs have characteristic hardware ridge points where operations
transition from memory-bound to compute-bound. The A100 with 312 TFLOPS
FP16 Tensor Core and 2.0 TB/s bandwidth has a ridge point of 153
FLOP/byte. The H100 SXM with 989 TFLOPS FP16 Tensor Core and 3.35 TB/s
bandwidth has a ridge point of approximately 295 FLOP/byte. Operations
below the ridge point are memory-bound; above are compute-bound.

\phantomsection\label{callout-perspectiveux2a-1.17}
\begin{fbx}{callout-perspective}{Systems Perspective:}{Peak FLOPS vs. Sustained Performance}
\phantomsection\label{callout-perspective*-1.17}
Hardware vendors often market ``Peak TFLOPS,'' but for a systems
engineer, this number is often a theoretical limit that is rarely
reached. The intensity gap reveals that most neural network
operations---especially in the backward pass---have arithmetic
intensities well below the hardware's ridge point. When an operation is
memory-bound (like LayerNorm or Softmax), doubling the hardware's peak
TFLOPS does \emph{nothing} for performance. This is why
\textbf{Mixed-Precision (FP16/BF16)} is so effective: it doesn't just
enable faster arithmetic; it halves the bytes moved per operation,
effectively doubling the ``Data Supply Rate'' and allowing the system to
reach a much higher percentage of its peak computational capability.
Successful optimization is the art of increasing arithmetic intensity
through kernel fusion and reducing data movement through precision
management.

\end{fbx}

Batch size directly influences arithmetic intensity. With batch=1, many
operations fall below the ridge point and become memory-bound. With
batch=32 or higher, most matrix operations exceed the ridge point and
become compute-bound. This explains why larger batches improve hardware
utilization: they shift operations into the compute-bound regime where
GPUs excel.

This analysis guides optimization strategy selection. For memory-bound
operations, reducing data movement through operator fusion, reduced
precision, or algorithmic improvements like FlashAttention provides the
largest gains. For compute-bound operations, increasing throughput
through Tensor Cores, parallelism, or quantization matters more. See
\textbf{?@sec-ai-acceleration} for detailed roofline model analysis and
hardware-specific optimization strategies.

Figure~\ref{fig-training-roofline} shows standard attention in the
memory-bound region while FlashAttention appears in the compute-bound
region---this shift represents the core insight of IO-aware algorithm
design. By never materializing the full \(N \times N\) attention matrix
and instead processing in tiles that fit in fast SRAM, FlashAttention
reduces memory traffic from \(O(N^2)\) to \(O(N)\), achieving 2-4×
speedups (\citeproc{ref-dao2022flashattention}{Dao et al. 2022}). We
examine the algorithm, its implementation, and when to use it in detail
in
Section~\ref{sec-ai-training-flash-attention-ioaware-attention-optimization-3da0}.

The arithmetic intensity analysis above reveals which operations
constrain training performance and why: matrix multiplications are
compute-bound while normalization and activation functions are
memory-bound, each requiring different optimization strategies.
FlashAttention exemplifies how understanding these bottlenecks enables
algorithmic solutions that shift operations from one regime to another.
But optimizing individual operations is insufficient. Training systems
must orchestrate data loading, computation, and parameter updates as a
unified pipeline, and the architecture of this pipeline determines
whether optimizations like FlashAttention translate into actual
throughput gains.

\section{Pipeline
Architecture}\label{sec-ai-training-pipeline-architecture-81c9}

The mathematical operations examined above define what training systems
must compute---for GPT-2, approximately 10 trillion FLOPs per training
step distributed across attention, feedforward, and normalization
operations. \textbf{?@sec-ai-frameworks} introduced how frameworks like
PyTorch and TensorFlow provide APIs for defining models and executing
forward passes; here we examine the \emph{system-level orchestration}
that makes those API calls efficient. Pipeline architecture determines
how to coordinate these computations across real hardware with finite
memory and bandwidth constraints, managing data loading, preprocessing,
GPU transfers, and parameter updates as a unified system rather than
isolated operations.

Figure~\ref{fig-training-pipeline} maps the complete training pipeline
architecture, showing how three main components interconnect: the data
pipeline for ingestion and preprocessing, the training loop that handles
model updates, and the evaluation pipeline for assessing performance.
Processed batches flow from the data pipeline to the training loop, and
evaluation metrics provide feedback to guide the training process.

\begin{figure}[htb]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/6345e73d5474b6370356cd6095c902215151a34a.pdf}}

}

\caption{\label{fig-training-pipeline}\textbf{Training System Overview}:
Machine learning systems organize training through interconnected data,
training, and evaluation pipelines. Data flows sequentially through
these components, with evaluation metrics providing feedback to guide
iterative model refinement and ensure reproducible results.}

\end{figure}%

\subsection{Architectural
Overview}\label{sec-ai-training-architectural-overview-5fc6}

As Figure~\ref{fig-training-pipeline} illustrates, the system-level
orchestration described above organizes into three interconnected
components. The data pipeline ingests raw data and transforms it into a
format suitable for the model. This data passes to the training loop,
where the model performs its core computations. Periodically, the
evaluation pipeline assesses performance using a separate validation
dataset. This modular organization enables efficient resource
utilization and clear separation of concerns.

\textbf{Data Pipeline.} The data pipeline manages the ingestion,
preprocessing, and batching of data for training. Raw data is loaded
from storage and transformed dynamically during training, with image
datasets undergoing preprocessing steps like normalization, resizing,
and augmentation (\citeproc{ref-lecun1998efficient}{LeCun et al. 1998}).
Once processed, the data is packaged into batches and handed off to the
training loop.

\textbf{Training Loop.} The training loop is the computational core of
the pipeline, where the model learns from the prepared data.
Figure~\ref{fig-training-loop} illustrates how this process unfolds
through three sequential steps on a single GPU: the forward pass
generates predictions from input data, gradient computation propagates
error signals backward through the network, and parameter updates apply
the optimizer to minimize the loss function.

\begin{figure}[htb]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/5839237fe0ab67915bb0fdc7bcc741566bf9209c.pdf}}

}

\caption{\label{fig-training-loop}\textbf{Single-GPU Training Loop}: The
three sequential steps of one training iteration: the forward pass
generates predictions, gradient computation propagates error signals
backward, and the optimizer applies parameter updates. GPUs parallelize
the underlying matrix operations, accelerating both the forward and
backward passes.}

\end{figure}%

Each iteration of the training loop involves several key steps:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Step 1 -- Forward Pass}: A batch of data from the dataset is
  passed through the neural network on the GPU to generate predictions.
  The model applies matrix multiplications and activation functions to
  transform the input into meaningful outputs.
\item
  \textbf{Step 2 -- Compute Gradients}: The predicted values are
  compared with the ground truth labels to compute the error using a
  loss function. The loss function outputs a scalar value that
  quantifies the model's performance. This error signal is then
  propagated backward through the network using backpropagation, which
  applies the chain rule of differentiation to compute gradients for
  each layer's parameters. These gradients indicate the necessary
  adjustments required to minimize the loss.
\item
  \textbf{Step 3 -- Update Parameters}: The computed gradients are
  passed to an optimizer, which updates the model's parameters to
  minimize the loss. Different optimization algorithms, such as SGD or
  Adam, influence how the parameters are adjusted. The choice of
  optimizer impacts convergence speed and stability.
\end{enumerate}

This process repeats iteratively across multiple batches and
epochs\sidenote{\textbf{Epoch}: Borrowed from astronomy, where it
denotes a reference point in time from which celestial measurements are
calculated. In ML, one epoch equals one complete pass through the
training dataset. The astronomical metaphor fits: just as astronomers
measure time from fixed reference points, ML practitioners measure
training progress in complete dataset cycles. Typical training requires
10-100 epochs, with each epoch providing the model another opportunity
to learn from every example. }, gradually refining the model to improve
its predictive accuracy.

\textbf{Evaluation Pipeline.} The evaluation pipeline provides periodic
feedback on the model's performance during training. Using a separate
validation dataset, predictions are compared against known outcomes to
compute metrics such as accuracy or loss. These metrics help monitor
progress and detect issues like overfitting or underfitting.

\textbf{Component Integration.} These three components are tightly
integrated to ensure an efficient workflow. Data preparation often
overlaps with computation, preprocessing the next batch while the
current batch is processed in the training loop. This integration
minimizes idle time for system resources and ensures training proceeds
without interruptions.

\subsection{Data Pipeline}\label{sec-ai-training-data-pipeline-8e71}

The architectural overview identified the data pipeline as the first
component in the training system. Its efficiency directly determines
whether expensive GPU resources remain fully utilized or sit idle
waiting for data. While this section focuses on the systems aspects of
data movement and preprocessing, the upstream data engineering practices
are covered in \textbf{?@sec-data-engineering-ml}.

\begin{figure}[htb]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/737177d27499b016e7d603e0da9776da4dc7ec70.pdf}}

}

\caption{\label{fig-data-pipeline}\textbf{CPU-to-GPU Data Flow}: Three
distinct zones compose the data pipeline: the storage zone houses raw
data on disk, the CPU preprocessing zone handles format conversion,
processing, and batching, and the GPU training zone distributes
preprocessed batches across multiple GPU workers for parallel
computation.}

\end{figure}%

The data pipeline running on the CPU bridges raw data storage and GPU
computation. Figure~\ref{fig-data-pipeline} breaks down this
architecture into three distinct zones: the storage zone houses raw data
on disk, the CPU preprocessing zone handles format conversion,
processing, and batching, and the GPU training zone distributes
preprocessed batches across multiple accelerators for parallel
computation.

In the storage zone, raw data resides on disk, typically in formats like
image files for computer vision tasks or text files for natural language
processing. The CPU preprocessing zone handles the transformation of
this raw data through multiple stages. For example, in an image
recognition model, these stages include:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Format conversion: Reading image files and converting them to
  standardized formats
\item
  Processing: Applying operations like resizing, normalization, and data
  augmentation
\item
  Batching: Organizing processed examples into batches for efficient GPU
  computation
\end{enumerate}

The final zone shows multiple GPUs receiving preprocessed batches for
training. This organization ensures that each GPU maintains a steady
supply of data, maximizing computational efficiency and minimizing idle
time. The effectiveness of this pipeline directly impacts training
performance, as any bottleneck in data preparation can leave expensive
GPU resources underutilized.

\subsubsection{Core
Components}\label{sec-ai-training-core-components-d28d}

The performance of machine learning systems is primarily constrained by
storage access speed, which determines the rate at which training data
can be retrieved. The data engineering practices described in
\textbf{?@sec-data-engineering-ml}---including data format selection
(Parquet, TFRecord, Arrow), data partitioning strategies, and data
locality optimization---directly impact these storage performance
characteristics. This section examines the systems-level implications of
data access patterns and throughput constraints during training.

This access speed is governed by two primary hardware constraints: disk
bandwidth and network bandwidth. The maximum theoretical throughput is
determined by the following relationship:
\[T_{\text{storage}} =\min(B_{\text{disk}}, B_{\text{network}})\] where
\(B_{\text{disk}}\) is the physical disk bandwidth (the rate at which
data can be read from storage devices) and \(B_{\text{network}}\)
represents the network bandwidth (the rate of data transfer across
distributed storage systems). Both quantities are measured in bytes per
second.

The actual throughput achieved during training operations falls below
this theoretical maximum due to non-sequential data access patterns. The
effective throughput can be expressed as:
\[T_{\text{effective}} = T_{\text{storage}} \times F_{\text{access}}\]
where \(F_{\text{access}}\) represents the access pattern factor. In
typical training scenarios, \(F_{\text{access}}\) approximates 0.1,
indicating that effective throughput achieves only 10\% of the
theoretical maximum. This significant reduction occurs because storage
systems are optimized for sequential access patterns rather than the
random access patterns common in training procedures.

This relationship between theoretical and effective throughput directly
affects system design and training optimization, informing decisions
about data pipeline architecture and training methodology.

\subsubsection{Preprocessing}\label{sec-ai-training-preprocessing-523c}

As the data becomes available, data preprocessing transforms raw input
data into a format suitable for model training. This process builds on
the data pipeline patterns established in
\textbf{?@sec-data-engineering-ml}, traditionally implemented through
Extract-Transform-Load (ETL) or Extract-Load-Transform (ELT)
pipelines\sidenote{\textbf{ETL vs ELT in ML}: Traditional data
warehousing used ETL (extract, transform, load) with expensive
transformation on powerful central servers. Modern ML systems often
prefer ELT (extract, load, transform) where raw data is loaded first,
then transformed on-demand during training. This shift enables data
augmentation (rotating images, adding noise) to create virtually
unlimited training variations from the same source data, which is
difficult to achieve in traditional ETL where transformations are fixed.
The broader data pipeline design patterns, including data quality
validation, feature engineering strategies, and schema enforcement that
precede training-time preprocessing, are detailed in
\textbf{?@sec-data-engineering-ml}. }, and is a critical determinant of
training system performance. The throughput of preprocessing operations
can be expressed mathematically as:
\[T_{\text{preprocessing}} = \frac{N_{\text{workers}}}{t_{\text{transform}}}\]

This equation captures two key factors:

\begin{itemize}
\tightlist
\item
  \(N_{\text{workers}}\) represents the number of parallel processing
  threads
\item
  \(t_{\text{transform}}\) represents the time required for each
  transformation operation
\end{itemize}

Training architectures employ multiple processing threads to ensure
preprocessing keeps pace with consumption rates. This parallel
processing approach is essential for maintaining high processor
utilization.

The final stage of preprocessing involves transferring the processed
data to computational devices (typically GPUs). The overall training
throughput is constrained by three factors, expressed as:
\[T_{\text{training}} =\min(T_{\text{preprocessing}}, B_{\text{GPU\_transfer}}, B_{\text{GPU\_compute}})\]
where:

\begin{itemize}
\tightlist
\item
  \(B_{\text{GPU\_transfer}}\) represents GPU memory bandwidth
\item
  \(B_{\text{GPU\_compute}}\) represents GPU computational throughput
\end{itemize}

This relationship illustrates a key principle in training system design:
the system's overall performance is limited by its slowest component.
Whether preprocessing speed, data transfer rates, or computational
capacity, the bottleneck stage determines the effective training
throughput of the entire system. These relationships guide system
architects toward balanced training pipelines where preprocessing
capacity aligns with computational resources, ensuring optimal resource
utilization.

Applying this throughput analysis to our GPT-2 lighthouse model reveals
where the \emph{data pipeline bottleneck} lies for language model
training.

\phantomsection\label{callout-exampleux2a-1.18}
\begin{fbx}{callout-example}{Example:}{GPT-2 Language Model Data Pipeline}
\phantomsection\label{callout-example*-1.18}
Training language models like GPT-2 requires a specialized data pipeline
optimized for text processing.

\subsubsection{Pipeline Stages}\label{pipeline-stages}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Raw Text Storage (Storage Zone)

  \begin{itemize}
  \tightlist
  \item
    OpenWebText dataset: \textasciitilde40GB raw text files
  \item
    Stored on NVMe SSD: 3.5 GB/s sequential read bandwidth
  \item
    Random access to different documents: \textasciitilde0.35 GB/s
    effective (F\_access ≈ 0.1)
  \end{itemize}
\item
  Tokenization (CPU Preprocessing Zone)

  \begin{itemize}
  \tightlist
  \item
    BPE (Byte-Pair Encoding) tokenizer (50,257 vocabulary) converts text
    to token IDs
  \item
    BPE segments text into subword units (e.g., ``unbreakable'' →
    {[}``un'', ``break'', ``able''{]})
  \item
    Processing rate: \textasciitilde500K tokens/second per CPU core
  \item
    For batch\_size=32, seq\_len=1024: need 32K tokens/batch
  \item
    Single core: 32K tokens ÷ 500K tokens/s = 66ms per batch
  \item
    Bottleneck: GPU forward pass only takes 80ms
  \end{itemize}
\item
  Batching \& Padding (CPU)

  \begin{itemize}
  \tightlist
  \item
    Pad sequences to uniform length (1024 tokens)
  \item
    Pack into tensors: {[}32, 1024{]} int64 = 256KB per batch
  \item
    Trivial time: \textless5ms
  \end{itemize}
\item
  GPU Transfer (PCIe)

  \begin{itemize}
  \tightlist
  \item
    PCIe Gen3 x16: 15.75 GB/s theoretical
  \item
    256KB per batch ÷ 15.75 GB/s = 0.016ms (negligible)
  \end{itemize}
\end{enumerate}

\subsubsection{Bottleneck Analysis}\label{bottleneck-analysis}

\begin{itemize}
\tightlist
\item
  Tokenization: 66ms
\item
  GPU compute: 80ms
\item
  Transfer: \textless1ms
\end{itemize}

System is balanced (tokenization ≈ GPU compute), but tokenization
becomes bottleneck with faster GPUs (A100: 45ms compute means
tokenization limits throughput).

\subsubsection{Optimization Applied}\label{optimization-applied}

\begin{itemize}
\tightlist
\item
  Multi-worker dataloading: 8 CPU workers tokenize in parallel → 66ms ÷
  8 = 8ms
\item
  Prefetching: Tokenize next batch while GPU processes current batch
\item
  Result: GPU utilization \textgreater95\%, training throughput: 380
  samples/second on 8×V100
\end{itemize}

Text tokenization is CPU-bound (unlike image preprocessing which is
I/O-bound). Language model training requires different pipeline
optimizations than vision models.

\end{fbx}

While data pipeline throughput determines how fast training data reaches
the GPU, multi-GPU training introduces a second bottleneck: the network
communication required to synchronize gradients across devices. The
following exercise quantifies when this communication overhead
dominates.

\phantomsection\label{notebook-network-wall}
\begin{fbx}{callout-notebook}{AI Engineer’s Notebook:}{The Network Wall}
\phantomsection\label{notebook-network-wall}
\textbf{Problem}: You are training a large model on 8 GPUs. You want to
know if the network is the bottleneck.

\textbf{The Math}: For a 7B parameter model with FP16 gradients:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Gradient Size}:
  \(7 \times 10^9 \times 2 \text{ bytes} = 14 \text{ GB}\) per step.
\item
  \textbf{AllReduce Cost}: Ring AllReduce sends
  \(2 \times 14 \text{ GB} = 28 \text{ GB}\) total.
\item
  \textbf{Network Time}: At 100 Gbps (12.5 GB/s) InfiniBand:
  \(28 / 12\.5 = 2\.2 \text{ s}\).
\item
  \textbf{Compute Time}: If forward + backward takes \(1 \text{ s}\),
  network is the bottleneck.
\end{enumerate}

\textbf{The Systems Insight}: The network becomes a wall when
\(t_{\text{communication}} > t_{\text{computation}}\). Solutions include
gradient compression (reduce data volume), overlapping computation with
communication (as implemented in Horovod
(\citeproc{ref-sergeev2018horovod}{Sergeev and Balso 2018})), and using
faster interconnects (NVLink at 900 GB/s vs InfiniBand at 12.5 GB/s).

\end{fbx}

\subsubsection{System
Implications}\label{sec-ai-training-system-implications-2539}

The relationship between data pipeline architecture and computational
resources directly determines the performance of machine learning
training systems. This relationship can be simply expressed through a
basic throughput equation:
\[T_{\text{system}} =\min(T_{\text{pipeline}}, T_{\text{compute}})\]
where \(T_{\text{system}}\) represents the overall system throughput,
constrained by both pipeline throughput (\(T_{\text{pipeline}}\)) and
computational speed (\(T_{\text{compute}}\)).

To illustrate these constraints, consider image classification systems.
The performance dynamics can be analyzed through two critical metrics.
The GPU Processing Rate (\(R_{\text{GPU}}\)) represents the maximum
number of images a GPU can process per second, determined by model
architecture complexity and GPU hardware capabilities. The Pipeline
Delivery Rate (\(R_{\text{pipeline}}\)) is the rate at which the data
pipeline can deliver preprocessed images to the GPU.

In this case, at a high level, the system's effective training speed is
governed by the lower of these two rates. When \(R_{\text{pipeline}}\)
is less than \(R_{\text{GPU}}\), the system experiences underutilization
of GPU resources. The degree of GPU utilization can be expressed as:
\[\text{GPU Utilization} = \frac{R_{\text{pipeline}}}{R_{\text{GPU}}} \times 100\%\]

Consider an example. A ResNet-50 model implemented on modern GPU
hardware might achieve a processing rate of 1000 images per second.
However, if the data pipeline can only deliver 200 images per second,
the GPU utilization would be merely 20\%, meaning the GPU remains idle
80\% of the time. This results in significantly reduced training
efficiency. This inefficiency persists even with more powerful GPU
hardware, as the pipeline throughput becomes the limiting factor in
system performance. This demonstrates why balanced system design, where
pipeline and computational capabilities are well matched, is necessary
for optimal training performance.

\subsubsection{Data Flows}\label{sec-ai-training-data-flows-0b2e}

Machine learning systems manage complex data flows through multiple
memory tiers\sidenote{\textbf{Memory Hierarchy in ML}: Unlike
traditional CPU programs that focus on cache locality, ML training
creates massive data flows between storage (TB datasets), system RAM (GB
models), and GPU memory (GB activations). The 1000x bandwidth gap
between storage (1-2 GB/s) and GPU memory (900+ GB/s) forces ML systems
to use sophisticated prefetching and caching strategies. Traditional
cache optimization (spatial/temporal locality) is less relevant than
managing bulk data transfers efficiently. } while coordinating pipeline
operations. The interplay between memory bandwidth constraints and
pipeline execution directly impacts training performance. The maximum
data transfer rate through the memory hierarchy is bounded by:
\[T_{\text{memory}} =\min(B_{\text{storage}}, B_{\text{system}}, B_{\text{accelerator}})\]
Where bandwidth varies significantly across tiers:

\begin{itemize}
\tightlist
\item
  Storage (\(B_{\text{storage}}\)): NVMe storage devices provide 1-2
  GB/s
\item
  System (\(B_{\text{system}}\)): Main memory transfers data at 50-100
  GB/s
\item
  Accelerator (\(B_{\text{accelerator}}\)): GPU memory achieves 900 GB/s
  or higher
\end{itemize}

These order-of-magnitude differences create distinct performance
characteristics that must be carefully managed. The total time required
for each training iteration comprises multiple pipelined operations:
\[t_{\text{iteration}} =\max(t_{\text{fetch}}, t_{\text{process}}, t_{\text{transfer}})\]

This equation captures three components: storage read time
(\(t_{\text{fetch}}\)), preprocessing time (\(t_{\text{process}}\)), and
accelerator transfer time (\(t_{\text{transfer}}\)).

Training architectures optimize performance by overlapping these
operations: when one batch undergoes preprocessing, the system
simultaneously fetches the next batch from storage while transferring
the previously processed batch to accelerator memory. Effective
pipelining minimizes idle time through careful buffer sizing and memory
allocation strategies.

\subsubsection{Practical
Architectures}\label{sec-ai-training-practical-architectures-d54d}

The ImageNet dataset provides a canonical example for understanding data
pipeline requirements. Storage performance in practical systems follows
a defined relationship between theoretical and practical throughput:
\[T_{\text{practical}} = 0.5 \times B_{\text{theoretical}}\]

To illustrate this relationship, consider an NVMe storage device with
3GB/s theoretical bandwidth. Such a device achieves approximately
1.5GB/s sustained read performance. However, the random access patterns
required for training data shuffling further reduce this effective
bandwidth by 90\%. System designers must account for this reduction
through careful memory buffer design.

The total memory requirements for the system scale with batch size
according to the following relationship:
\[M_{\text{required}} = (B_{\text{prefetch}} + B_{\text{processing}} + B_{\text{transfer}}) \times S_{\text{batch}}\]

In this equation, \(B_{\text{prefetch}}\) represents memory allocated
for data prefetching, \(B_{\text{processing}}\) represents memory
required for active preprocessing operations, \(B_{\text{transfer}}\)
represents memory allocated for accelerator transfers, and
\(S_{\text{batch}}\) represents the training batch size.

Preprocessing operations introduce additional computational
requirements. Common operations such as image resizing, augmentation,
and normalization consume CPU resources. These preprocessing operations
must satisfy a basic time constraint:
\[t_{\text{preprocessing}} < t_{\text{GPU\_compute}}\]

This inequality determines system efficiency. When preprocessing time
exceeds GPU computation time, accelerator utilization decreases
proportionally. The relationship between preprocessing and computation
time thus establishes efficiency limits in training system design.

\subsection{Forward Pass}\label{sec-ai-training-forward-pass-9695}

With the data pipeline providing prepared batches, we can now examine
how the training loop processes this data. The forward pass implements
the mathematical operations described in
Section~\ref{sec-ai-training-mathematical-operations-neural-networks-ddac},
where input data propagates through the model to generate predictions.
While the conceptual flow follows the layer-by-layer transformation
\(A^{(l)} = f\left(W^{(l)} A^{(l-1)} + b^{(l)}\right)\) established
earlier, the system-level implementation poses several challenges
critical for efficient execution.

\subsubsection{Compute
Operations}\label{sec-ai-training-compute-operations-83ee}

The forward pass orchestrates the computational patterns introduced in
Section~\ref{sec-ai-training-matrix-operations-1f21}, optimizing them
for specific neural network operations. Building on the matrix
multiplication foundations, the system must efficiently execute the
\(N \times M \times B\) floating-point operations required for each
layer, where typical layers with dimensions of \(512\times1024\)
processing batches of 64 samples execute over 33 million operations.

Modern neural architectures extend beyond these basic matrix operations
to include specialized computational patterns. Convolutional
networks\sidenote{\textbf{Convolutional Operations}: Sliding kernel
operations applying learned filters across spatial dimensions to detect
hierarchical features. A 3\(\times\) 3 convolution requires \(9K^2\)
multiplications for K-channel inputs; depthwise-separable variants
(MobileNet) reduce this by 8--9\(\times\). GPU implementations achieve
\textgreater90\% theoretical throughput through im2col matrix
transformations, detailed in \textbf{?@sec-dnn-architectures}. }, for
instance, perform systematic kernel operations across input tensors.
Consider a typical input tensor of dimensions
\(64 \times 224 \times 224 \times 3\) (batch size \(\times\) height
\(\times\) width \(\times\) channels) processed by \(7 \times 7\)
kernels. Each position requires 147 multiply-accumulate operations, and
with 64 filters operating across \(218 \times 218\) spatial dimensions,
the computational demands become substantial.

Transformer architectures introduce attention
mechanisms\sidenote{\textbf{Attention Mechanisms}: Dynamic weighting
schemes enabling models to focus on relevant input regions. Introduced
by Bahdanau et al.~(2014) for machine translation, attention computes
alignment scores between encoder/decoder states. Modern implementations
include cross-attention (between sequences) and self-attention (within
sequences), with softmax normalization ensuring weights sum to one. },
which compute similarity scores between sequences. These operations
combine matrix multiplications with softmax normalization, requiring
efficient broadcasting and reduction operations across varying sequence
lengths. The computational pattern here differs significantly from
convolutions, demanding flexible execution strategies from hardware
accelerators.

Throughout these networks, element-wise operations play a supporting
role. Activation functions like ReLU and sigmoid transform values
independently. While conceptually simple, these operations can become
bottlenecked by memory bandwidth rather than computational capacity, as
they perform relatively few calculations per memory access. Batch
normalization presents similar challenges, computing statistics and
normalizing values across batch dimensions while creating
synchronization points in the computation pipeline.

Modern hardware accelerators, particularly GPUs, optimize these diverse
computations through massive parallelization. Achieving peak performance
requires careful attention to hardware architecture. GPUs process data
in fixed-size blocks of threads called warps (in NVIDIA architectures)
or wavefronts (in AMD architectures). Peak efficiency occurs when matrix
dimensions align with these hardware-specific sizes. For instance,
NVIDIA GPUs typically achieve optimal performance when processing
matrices aligned to \(32\times32\) dimensions. This fixed-size execution
model creates a subtle but consequential effect that practitioners
frequently overlook.

\begin{tcolorbox}[enhanced jigsaw, leftrule=.75mm, coltitle=black, opacitybacktitle=0.6, arc=.35mm, title=\textcolor{quarto-callout-warning-color}{\faExclamationTriangle}\hspace{0.5em}{Wave Quantization and Tail Effects}, bottomtitle=1mm, left=2mm, colback=white, rightrule=.15mm, titlerule=0mm, toprule=.15mm, colbacktitle=quarto-callout-warning-color!10!white, breakable, toptitle=1mm, colframe=quarto-callout-warning-color-frame, bottomrule=.15mm, opacityback=0]

A common mistake in ML systems is treating batch size as a continuous
variable. In reality, GPU execution is \textbf{quantized} into ``waves''
of work.

\textbf{The Wave Effect}: An NVIDIA GPU executes work in warps of
\textbf{32 threads}. If your batch size is 32, all 32 threads are busy.
If your batch size is 33, the GPU must launch a second warp to process
the single remaining sample. This second warp uses only 1/32 (3\%) of
its potential compute power, but takes just as long to execute as the
first.

\textbf{Tail Effects at Scale}: On a large GPU like the H100 with 132
Streaming Multiprocessors (SMs), the hardware can process thousands of
threads in one ``wave.'' If your total workload is just slightly over a
wave boundary (e.g., 1.01 waves), the hardware must wait for a nearly
empty wave to finish before the next task begins.

\textbf{Quantitative Example}:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2500}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2500}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2500}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2500}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Batch Size}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Warps Needed}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Utilization}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Relative Time}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
32 & 1 & 100\% & 1.0× \\
33 & 2 & 52\% & \textasciitilde2.0× \\
64 & 2 & 100\% & 1.0× \\
65 & 3 & 68\% & \textasciitilde1.5× \\
\end{longtable}

\textbf{Engineering Rule}: Always choose batch sizes and hidden
dimensions that are \textbf{powers of 2} or multiples of 8/32/64 to
avoid this ``quantization tax.'' A batch of 32 is often faster than 33,
and a batch of 64 is often just as fast as 33.

Understanding these tail effects is the difference between a
practitioner who tunes by trial-and-error and an engineer who designs
for the hardware.

\end{tcolorbox}

Libraries like cuDNN (\citeproc{ref-chetlur2014cudnn}{Chetlur et al.
2014}) address these challenges by providing optimized implementations
for each operation type. These systems dynamically select algorithms
based on input dimensions, hardware capabilities, and memory
constraints. The selection process balances computational efficiency
with memory usage, often requiring empirical measurement to determine
optimal configurations for specific hardware setups.

These hardware utilization patterns reinforce the efficiency principles
established earlier. When batch size decreases from 32 to 16, GPU
utilization often drops due to incomplete warp occupation. The tension
between larger batch sizes (better utilization) and memory constraints
(forcing smaller batches) exemplifies how the central hardware-software
trade-offs permeate all levels of training system design.

\subsubsection{Memory
Management}\label{sec-ai-training-memory-management-c1ec}

Memory management is particularly important during the forward pass,
when intermediate activations must be stored for subsequent backward
propagation. Before examining how frameworks manage forward-pass memory,
it is useful to estimate the total VRAM required for training. The
following calculation demonstrates the practical process of
\emph{estimating VRAM requirements}.

\phantomsection\label{callout-notebookux2a-1.20}
\begin{fbx}{callout-notebook}{AI Engineer’s Notebook:}{Estimating VRAM Requirements}
\phantomsection\label{callout-notebook*-1.20}
\textbf{Problem}: Will your 7B parameter model fit on a 24 GiB GPU for
training?

\textbf{Given}: 7B parameters, mixed-precision training (FP16
weights/gradients, FP32 optimizer), Adam optimizer, 24 GiB GPU memory.

\textbf{The Math}:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Weights (FP16)}:
  \(7\text{B} \times 2 \text{ bytes} = \mathbf{14 \text{ GiB}}\).
\item
  \textbf{Gradients (FP16)}: Same size as weights =
  \(\mathbf{14 \text{ GiB}}\).
\item
  \textbf{Optimizer (Adam, FP32)}: Stores momentum \& variance.
  \(7\text{B} \times 8 \text{ bytes} = \mathbf{56 \text{ GiB}}\).
\item
  \textbf{Subtotal (before activations)}:
  \(14 + 14 + 56 = \mathbf{84 \text{ GiB}}\). Already exceeds 24 GiB.
\item
  \textbf{Activations}: Scale with batch size. Formula:
  \(\text{Batch} \times \text{SeqLen} \times \text{Hidden} \times \text{Layers} \times \text{Bytes}\).
  Example: Batch=1, Seq=2048, Hidden=4096, 32 Layers \(\approx\)
  \textbf{2 GiB} additional.
\end{enumerate}

\textbf{The Systems Conclusion}: The ``administrative tax'' (gradients +
optimizer states) is \(4\text{--}6\times\) larger than model weights.
Training a 7B model on a single 24 GiB GPU requires \textbf{quantization
(4-bit)} or \textbf{parameter sharding (FSDP/ZeRO)}.

\end{fbx}

The total memory footprint grows with both network depth and batch size,
following a basic relationship. \[
\text{Total Memory} \sim B \times \sum_{l=1}^{L} A_l
\] where \(B\) represents the batch size, \(L\) is the number of layers,
and \(A_l\) represents the activation size at layer \(l\). This simple
equation masks considerable complexity in practice.

Consider a representative large model like ResNet-50 (a widely-used
image classification architecture) processing images at \(224\times224\)
resolution with a batch size of 32. The initial convolutional layer
produces activation maps of dimension \(112\times112\times64\); for a
batch of 32 at single-precision (4 bytes), this requires approximately
100 MiB. As the network progresses through its 50 layers, the cumulative
memory demands grow substantially: the complete forward pass activations
total approximately 8 GiB, gradients require an additional 4 GiB, and
model parameters consume 200 MiB. This 12.2GB total represents over 30\%
of a high-end A100 GPU's 40 GiB memory capacity for a single batch.

The memory scaling patterns reveal critical hardware utilization
trade-offs. Doubling the batch size to 64 increases activation memory to
16GB and gradient memory to 8GB, totaling 24.2GB and approaching memory
limits. Training larger models at the scale of GPT-3 (175B parameters,
representing current large language models) requires approximately 700GB
just for parameters in FP32 (350GB in FP16), necessitating distributed
memory strategies across multiple high-memory nodes.

GPUs typically provide 40--80 GB of memory in high-end training
configurations, which must accommodate activations, model parameters,
gradients, and optimization states. This constraint has motivated
several memory management strategies:

Activation checkpointing trades computational cost for memory efficiency
by strategically discarding and recomputing activations during the
backward pass. Rather than storing all intermediate values, the system
maintains checkpoints at selected layers. During backpropagation, it
regenerates necessary activations from these checkpoints. While this
approach can reduce memory usage by 50\% or more, it typically increases
computation time by 20--30\%.

Mixed precision training offers another approach to memory efficiency.
By storing activations in half-precision (FP16) format instead of
single-precision (FP32), memory requirements are immediately halved.
Modern hardware architectures provide specialized support for these
reduced-precision operations, often maintaining computational throughput
while saving memory.

The relationship between batch size and memory usage creates practical
trade-offs in training regimes. While larger batch sizes can improve
computational efficiency, they proportionally increase memory demands. A
machine learning practitioner might start with large batch sizes during
initial development on smaller networks, then adjust downward when
scaling to deeper architectures or when working with memory-constrained
hardware.

This memory management challenge becomes particularly acute in
state-of-the-art models. Recent transformer architectures can require
tens of gigabytes just for activations, necessitating sophisticated
memory management strategies or distributed training approaches. These
memory constraints and management strategies are central to the design
and deployment of modern machine learning systems.

\subsection{Backward Pass}\label{sec-ai-training-backward-pass-5ded}

Following the forward pass's computation of predictions and loss, the
backward pass implements the backpropagation algorithm detailed in
Section~\ref{sec-ai-training-backpropagation-mechanics-0b64}. This
computationally intensive phase propagates gradients through the network
using the chain rule formulations established earlier. The system-level
implementation involves complex interactions between computation and
memory systems, requiring careful analysis of both computational demands
and data movement patterns.

\subsubsection{Compute
Operations}\label{sec-ai-training-compute-operations-5368}

The backward pass executes the gradient computations described in
Section~\ref{sec-ai-training-backpropagation-mechanics-0b64}, processing
parameter gradients in reverse order through the network's layers. As
established in that section, computing gradients requires matrix
operations that combine stored activations with gradient signals,
demanding twice the memory compared to forward computation.

The gradient computation
\(\frac{\partial L}{\partial W^{(l)}} = \delta^{(l)} \cdot \left(a^{(l-1)}\right)^T\)
forms the primary computational load, where gradient signals multiply
with transposed activations as detailed in the mathematical framework.
For layers with 1000 input features and 100 output features, this
results in millions of floating-point operations as calculated in the
algorithm mechanics analysis.

\subsubsection{Memory
Operations}\label{sec-ai-training-memory-operations-0ac1}

The backward pass moves large amounts of data between memory and compute
units. Each time a layer computes gradients, the GPU loads stored
activations from memory, reads incoming gradient signals, and writes the
computed gradients back. Consider a convolutional layer processing a
batch of 64 images at \(224\times 224\) pixels: the activation maps
alone occupy 0.38 GB, the gradient signals require 8.1 GB for 64
filters, and even the weight gradients need 0.037 GB.

These computations operate across a memory hierarchy where the processor
must retrieve activation values stored in HBM, transfer them to fast
SRAM for computation, and write results back. Each gradient calculation
triggers this sequence of memory transfers, making memory access
patterns a key factor in backward pass performance.

\subsubsection{Production
Considerations}\label{sec-ai-training-production-considerations-4c12}

Consider training a ResNet-50 model on the ImageNet dataset with a batch
of 64 images. The first convolutional layer applies 64 filters of size
\(7 \times 7\) to RGB images sized \(224\times 224\). During the
backward pass, this single layer's computation requires: \[
\text{Memory per image} = 224 \times 224 \times 64 \times 4 \text{ bytes}
\]

The total memory requirement multiplies by the batch size of 64,
reaching approximately 3.2 GB just for storing gradients. When we add
memory for activations, weight updates, and intermediate computations, a
single layer approaches the memory limits of many GPUs.

Deeper in the network, layers with more filters demand even greater
resources. A mid-network convolutional layer might use 256 filters,
quadrupling the memory and computation requirements. The backward pass
must manage these resources while maintaining efficient computation.
Each layer's computation can only begin after receiving gradient signals
from the subsequent layer, creating a strict sequential dependency in
memory usage and computation patterns.

This dependency means the GPU must maintain a large working set of
memory throughout the backward pass. As gradients flow backward through
the network, each layer temporarily requires peak memory usage during
its computation phase. The system cannot release this memory until the
layer completes its gradient calculations and passes the results to the
previous layer.

\subsection{Parameter Updates and
Optimizers}\label{sec-ai-training-parameter-updates-optimizers-b1a4}

After gradients are computed in the backward pass, the system must
allocate and manage memory for both parameters and gradients, then
perform the update computations. The choice of optimizer determines not
only the mathematical update rule, but also the system resources
required for training.

Listing~\ref{lst-param_update} demonstrates the complete parameter
update cycle in PyTorch: the forward pass computes predictions
(\texttt{outputs\ =\ model(inputs)}), the loss function quantifies
error, \texttt{loss.backward()} populates gradient tensors, and
\texttt{optimizer.step()} applies the update rule to all parameters
based on the configured optimizer (Adam, SGD, etc.).

\begin{codelisting}

\caption{\label{lst-param_update}\textbf{Parameter Update}: Computes
gradients and applies optimization to adjust model parameters based on
loss function. Training requires computing gradients through
backpropagation and then updating weights using an optimizer to minimize
loss, ensuring model performance improves over epochs.}

\centering{

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{loss.backward()  }\CommentTok{\# Compute gradients}
\NormalTok{optimizer.step()  }\CommentTok{\# Update parameters}
\end{Highlighting}
\end{Shaded}

}

\end{codelisting}%

These operations initiate a sequence of memory accesses and
computations. The system must load parameters from memory, compute
updates using the stored gradients, and write the modified parameters
back to memory. Different optimizers vary in their memory requirements
and computational patterns, directly affecting system performance and
resource utilization.

\subsubsection{Optimizer Memory in the Training
Loop}\label{sec-ai-training-optimizer-memory-training-loop-4383}

The memory scaling analysis from
Section~\ref{sec-ai-training-optimization-algorithm-system-implications-f9f2}---where
SGD requires \(1\times\), momentum requires \(2\times\), and Adam
requires \(3\times\) the parameter memory---manifests concretely during
each training iteration. Each parameter update involves reading current
values, accessing gradients, computing the update rule, and writing
modified parameters back to memory. For Adam, this includes updating and
accessing the momentum and variance buffers, creating substantial memory
traffic for large models.

At billion-parameter scale, optimizer state dominates the memory budget.
As quantified in the GPT-2 worked example
(Section~\ref{sec-ai-training-optimization-algorithm-system-implications-f9f2}),
a 1.5B parameter model requires 24 GB for optimizer state alone in
FP32---before accounting for activations. This challenge has motivated
memory-efficient optimizer variants.
Figure~\ref{fig-galore-llm-memory-breakdown} demonstrates how GaLoRE
addresses this constraint: by computing updates in a compressed space
(\citeproc{ref-zhao2024galorememoryefficientllmtraining}{Zhao et al.
2024}), the technique reduces the memory footprint dominated by
optimizer states to a fraction of its original size, enabling training
of larger models on fixed hardware.

\begin{figure}[htb]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/ff8b4e5663eccf5f6a601897acb6a68ae1c2783d.pdf}}

}

\caption{\label{fig-galore-llm-memory-breakdown}\textbf{Memory Footprint
Breakdown}: Memory usage of LLaMA-7B across four optimizer
configurations, decomposed into weights, activations, optimizer state,
weight gradients, and other components. The dashed red line marks the
RTX 4090 24 GB memory limit, illustrating how standard Adam exceeds
single-GPU capacity while GaLoRE compression reduces optimizer state
enough to fit within this budget.}

\end{figure}%

\subsubsection{Computational
Load}\label{sec-ai-training-computational-load-36b6}

The computational cost of parameter updates also depends on the
optimizer's complexity. For gradient descent, each update involves
simple gradient calculation and application. More sophisticated
optimizers like Adam require additional calculations, such as computing
running averages of gradients and their squares. This increases the
computational load per parameter update.

The efficiency of these computations on modern hardware like GPUs and
TPUs depends on how well the optimizer's operations can be parallelized.
While matrix operations in Adam may be efficiently handled by these
accelerators, some operations in complex optimizers might not
parallelize well, potentially leading to hardware underutilization.

The choice of optimizer directly impacts both system memory requirements
and computational load. More sophisticated optimizers often trade
increased memory usage and computational complexity for potentially
faster convergence, presenting important considerations for system
design and resource allocation in ML systems.

\subsubsection{Batch Size and Parameter
Updates}\label{sec-ai-training-batch-size-parameter-updates-4d0b}

Batch size, a critical hyperparameter\sidenote{\textbf{Hyperparameter}:
From Greek ``hyper'' (over, beyond) + ``parameter.'' While parameters
(weights, biases) are learned from data during training, hyperparameters
are set \emph{before} training and control the learning process itself.
The ``hyper-'' prefix indicates a higher level of abstraction:
hyperparameters are parameters \emph{about} parameters. Common examples
include learning rate, batch size, and number of layers. The term
emerged in Bayesian statistics where hyperparameters define prior
distributions over model parameters. } in machine learning systems,
significantly influences the parameter update process, memory usage, and
hardware efficiency. It determines the number of training examples
processed in a single iteration before the model parameters are updated.

A larger batch size provides a more accurate estimate of the true
gradient, allowing for larger learning steps. However, simply increasing
the batch size without adjusting the learning rate leads to the
\textbf{``Linear Scaling Failure''}.

If you double the batch size, you perform half as many updates per
epoch. If the learning rate remains constant, the model effectively
travels ``half the distance'' in weight space, causing underfitting. The
following plot (Figure~\ref{fig-linear-scaling-failure}) visualizes this
\textbf{Generalization Gap} and how the \textbf{Linear Scaling Rule}
(\(\text{LR}_{new} = k \times \text{LR}_{base}\)) corrects it.

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{contents/vol1/training/training_files/figure-pdf/fig-linear-scaling-failure-output-1.pdf}}

}

\caption{\label{fig-linear-scaling-failure}\textbf{The Linear Scaling
Failure.} Training Loss vs.~Steps. Curve A (Blue) represents a standard
baseline batch size. Curve B (Gray) shows what happens when batch size
is increased 8x without tuning: convergence slows dramatically because
weight updates are too infrequent. Curve C (Green) restores convergence
by scaling the learning rate linearly (8x LR), allowing the model to
take larger steps to compensate for fewer updates.}

\end{figure}%

Larger batch sizes generally provide more accurate gradient estimates,
potentially leading to faster convergence and more stable parameter
updates. However, they also increase memory demands proportionally: \[
\text{Memory for Batch} = \text{Batch Size} \times \text{Size of One Training Example}
\]

This increase in memory usage directly affects the parameter update
process, as it determines how much data is available for computing
gradients in each iteration.

Building on the efficiency patterns established in previous sections,
larger batches improve hardware utilization, particularly on GPUs and
TPUs optimized for parallel processing. This leads to more efficient
parameter updates and faster training times, provided sufficient memory
is available.

As discussed earlier, this computational efficiency comes with memory
costs. Systems with limited memory must reduce batch size, creating the
same fundamental trade-offs that shape training system architecture
throughout.

The choice of batch size interacts with various aspects of the
optimization process. For instance, it affects the frequency of
parameter updates: larger batches result in less frequent but
potentially more impactful updates. Batch size influences the behavior
of adaptive optimization algorithms, which may need to be tuned
differently depending on the batch size. In distributed training
scenarios, batch size often determines the degree of data parallelism,
impacting how gradient computations and parameter updates are
distributed across devices.

Determining the optimal batch size involves balancing these factors
within hardware constraints, often requiring experimentation to find the
configuration that maximizes both learning efficiency and hardware
utilization while ensuring effective parameter updates.

Beyond batch size tuning, practitioners must also confront the economic
reality of large-scale training. The compute cost itself becomes a
binding constraint that shapes every training decision, from hardware
selection to cluster sizing, a phenomenon best understood by examining
\emph{the utility bill} of a realistic training run.

\phantomsection\label{notebook-utility-bill}
\begin{fbx}{callout-notebook}{AI Engineer’s Notebook:}{The Utility Bill}
\phantomsection\label{notebook-utility-bill}
\textbf{Problem}: Is it cheaper to rent an H100 or buy it for training
Llama-2-70B?

\textbf{The Math}:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Workload}: Llama-2-70B (70B params, 2T tokens).
\item
  \textbf{Compute Required}:
  \(6 \times 70 \times 10^9 \times 2 \times 10^{12} \approx 8\.4 \times 10^{23}\)
  FLOPs.
\item
  \textbf{Hardware}: NVIDIA H100 (Peak: 1,000 TFLOPS FP16). Assumed
  Utilization: 50\% (500 TFLOPS).
\item
  \textbf{Time}:
  \(8\.4 \times 10^{23} / (500 \times 10^{12}) \approx 1\.68 \times 10^{9} \text{ seconds} \approx \mathbf{53 \text{ years}}\)
  (on 1 GPU).
\item
  \textbf{Cluster}: On 1,000 GPUs \(\rightarrow\) 19 days.
\end{enumerate}

\textbf{The Economics}:

\begin{itemize}
\tightlist
\item
  \textbf{Rental (\$3/hr)}:
  \(1,000 \text{ GPUs} \times 24 \text{ hrs} \times 19 \text{ days} \times \$3 \approx \mathbf{\$1\.40 \text{ Million}}\).
\item
  \textbf{Purchase (\$30k/GPU)}:
  \(1,000 \times \$30,000 = \mathbf{\$30 \text{ Million}}\).
\end{itemize}

\textbf{The Systems Conclusion}: You must train \textbf{21 models}
before buying becomes cheaper than renting. Cloud economics favors
bursty workloads like training; on-premise favors steady-state workloads
like inference.

\end{fbx}

The pipeline architecture established above, spanning data loading,
forward pass, backward pass, and parameter updates, provides the
\emph{what} of training systems. The mathematical foundations quantified
the FLOPs, memory, and bandwidth each stage demands. Together, these
sections establish what operations execute and what resources they
require.

But understanding \emph{what} must happen does not reveal \emph{where}
the system currently underperforms. A pipeline can be limited by any of
its stages, and optimizing the wrong stage wastes engineering effort
while leaving the actual bottleneck untouched.

\section{Identifying
Bottlenecks}\label{sec-ai-training-identifying-bottlenecks-f57f}

Before applying optimization techniques, you must diagnose which
constraint currently limits performance. This diagnostic step is
essential: the techniques in the next section, including prefetching,
mixed precision, and gradient accumulation, each target specific
bottlenecks. Applying the wrong optimization wastes engineering effort,
while applying the right one can yield 2-10x speedups.

The first step in diagnosis is establishing a meaningful measure of
training efficiency. Raw GPU utilization percentages can be misleading
because they include overhead from recomputation and padding. A more
precise metric captures only the useful training work performed per
second.

\phantomsection\label{callout-definitionux2a-1.22}
\begin{fbx}{callout-definition}{Definition:}{Model FLOPs Utilization (MFU)}
\phantomsection\label{callout-definition*-1.22}
\textbf{\emph{Model FLOPs Utilization (MFU)}} is the definitive metric
of \textbf{Training Efficiency}. It measures the fraction of the
hardware's theoretical peak throughput that is contributing to
\textbf{Model Convergence}, excluding ``waste'' FLOPs from recomputation
or padding. It serves as the primary KPI for system optimization,
separating meaningful progress from empty cycles.

\end{fbx}

Training bottlenecks fall into three categories, which map directly to
the \textbf{DAM Taxonomy} (\textbf{?@tbl-dam-taxonomy}).
Table~\ref{tbl-dam-training-bottlenecks} connects each DAM component to
the corresponding training bottleneck, its observable symptoms, and the
optimization techniques that address it.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2500}}@{}}
\caption{\textbf{DAM Taxonomy Applied to Training Bottlenecks.} Each
component of the DAM Taxonomy (Data, Algorithm, Machine) maps to a
distinct training bottleneck with characteristic symptoms. Profiling
reveals which component is the limiting factor, guiding practitioners to
the appropriate optimization
technique.}\label{tbl-dam-training-bottlenecks}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{DAM Component}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Bottleneck}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Symptoms}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Primary Solutions}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{DAM Component}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Bottleneck}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Symptoms}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Primary Solutions}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Algorithm} & Compute-bound & GPU utilization \textgreater90\%;
low memory bandwidth usage; arithmetic units are the limiting factor &
FlashAttention, mixed precision, faster hardware \\
\textbf{Machine} & Memory-bound & GPU utilization 50-80\%; high memory
bandwidth usage; arithmetic units idle waiting for data from memory &
Operator fusion, memory-efficient attention, reduced precision
formats \\
\textbf{Data} & Data-bound & Periodic GPU utilization drops to
near-zero; CPU fully utilized during gaps; pipeline cannot feed GPU fast
enough & Prefetching, pipeline overlap, faster storage, DataLoader
parallelism \\
\end{longtable}

\subsection{Profiling to Identify
Bottlenecks}\label{sec-ai-training-profiling-identify-bottlenecks-f306}

Profiling tools reveal which bottleneck dominates your workload.
Figure~\ref{fig-tf-bottleneck-trace} captures a data-bound pathology
through TensorFlow's profiler: the gaps in GPU activity (white regions
between compute blocks) reveal that the device frequently waits for
input data, with utilization dropping to zero during data loading
phases.

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{contents/vol1/training/images/png/tf_profiler.png}}

}

\caption{\label{fig-tf-bottleneck-trace}\textbf{Data-Bound Profiler
Trace}: TensorFlow profiler output capturing a data loading bottleneck
during training. The gaps in GPU activity (white regions between compute
blocks) indicate periods where the device idles while waiting for input
data, with utilization dropping to zero during data loading phases.}

\end{figure}%

Tools integrated into machine learning frameworks provide detailed
bottleneck analysis:

\begin{itemize}
\tightlist
\item
  \textbf{PyTorch Profiler} (\texttt{torch.profiler}): Shows time spent
  in each operation, memory allocation patterns, and GPU kernel
  execution
\item
  \textbf{TensorFlow Profiler}: Visualizes the training timeline,
  identifies input pipeline bottlenecks, and shows device placement
\item
  \textbf{NVIDIA Nsight Systems}: Low-level GPU profiling showing kernel
  execution, memory transfers, and synchronization points
\item
  \textbf{NVIDIA Nsight Compute}: Detailed kernel analysis showing
  arithmetic intensity, memory throughput, and occupancy
\end{itemize}

The profiling workflow follows a systematic pattern: run a
representative training iteration with profiling enabled, examine the
timeline for gaps (data-bound), check memory bandwidth utilization
(memory-bound vs.~compute-bound), and identify the dominant bottleneck
before selecting an optimization technique.

In practice, profiling reveals characteristic signatures for each
bottleneck type. Data-bound systems show periodic GPU utilization drops
to near-zero while CPU activity spikes during data loading phases.
Memory-bound systems maintain moderate GPU utilization (50-80\%) with
high memory bandwidth consumption, indicating that arithmetic units wait
for data movement. Compute-bound systems show sustained high GPU
utilization (\textgreater90\%) with the arithmetic units as the limiting
factor. These signatures map directly to the optimization techniques
that follow: prefetching for data bottlenecks, mixed precision and
operator fusion for memory bottlenecks, and algorithmic improvements or
hardware upgrades for compute bottlenecks.

\section{Pipeline
Optimizations}\label{sec-ai-training-pipeline-optimizations-cd9d}

Once bottlenecks are identified, targeted optimizations can address
them. Even well-designed pipeline architectures rarely achieve optimal
performance without such optimization. The gap between theoretical
hardware capability and realized training throughput often reaches
50--70\%: GPUs advertised at 300 TFLOPS may deliver only 90--150 TFLOPS
for training workloads, and distributed systems with aggregate 1000
TFLOPS capacity frequently achieve under 500 TFLOPS effective throughput
(\citeproc{ref-wang2019superneurons}{L. Wang et al. 2018}). This
efficiency gap stems from systematic bottlenecks that optimization
techniques can address.

Table~\ref{tbl-optimization-roadmap} extends the DAM-based bottleneck
classification from Table~\ref{tbl-dam-training-bottlenecks} by mapping
each bottleneck to the specific optimization technique that addresses
it:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 2\tabcolsep) * \real{0.5000}}
  >{\raggedright\arraybackslash}p{(\linewidth - 2\tabcolsep) * \real{0.5000}}@{}}
\caption{\textbf{Optimization Technique Roadmap.} Each primary
bottleneck category has targeted solutions that address specific
performance constraints, matching techniques to profiling results for
systematic optimization.}\label{tbl-optimization-roadmap}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Bottleneck}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Primary Solution(s)}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Bottleneck}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Primary Solution(s)}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Data Movement Latency} & Prefetching \& Pipeline Overlapping \\
\textbf{Compute Throughput} & Mixed-Precision Training \\
\textbf{Memory Capacity} & Gradient Accumulation \& Activation
Checkpointing \\
\end{longtable}

Training pipeline performance is constrained by three primary
bottlenecks that determine overall system efficiency.
Table~\ref{tbl-optimization-roadmap} maps each bottleneck category to
its targeted solution: data movement latency responds to prefetching and
pipeline overlapping, compute throughput improves through
mixed-precision training, and memory capacity constraints yield to
gradient accumulation and activation checkpointing. Data movement
latency emerges when training batches cannot flow from storage through
preprocessing to compute units fast enough to keep accelerators
utilized. Computational throughput limitations occur when mathematical
operations execute below hardware peak performance due to suboptimal
parallelization, precision choices, or kernel inefficiencies. Memory
capacity constraints restrict both the model sizes we can train and the
batch sizes we can process, directly limiting both model complexity and
training efficiency. These bottlenecks manifest differently across
system scales---a 100 GB model faces different constraints than a 1 GB
model---but their systematic identification and mitigation follows
consistent principles.

These bottlenecks interact in complex ways, illustrating the
Conservation of Complexity thesis from \textbf{?@sec-part-foundations}:
you cannot eliminate a bottleneck without shifting load elsewhere. When
data loading becomes a bottleneck, GPUs sit idle waiting for batches.
When computation is suboptimal, memory bandwidth goes underutilized.
When memory is constrained, we resort to smaller batches that reduce GPU
efficiency. Consider GPT-2: profiling reveals memory-bound attention
operations (50\% of time), data loading overhead (25\%), and
compute-bound matrix multiplications (25\%)---requiring a composition of
mixed precision, prefetching, and gradient checkpointing to address all
three constraints. The optimization challenge involves identifying which
bottleneck currently limits performance, then selecting techniques that
address that specific constraint without introducing new bottlenecks
elsewhere.

\subsection{Systematic Optimization
Framework}\label{sec-ai-training-systematic-optimization-framework-83b0}

The pipeline architecture established above creates opportunities for
targeted optimizations. Effective optimization follows a systematic
methodology that applies regardless of system scale or model
architecture. This three-phase framework provides the foundation for all
optimization work: profile to identify bottlenecks, select appropriate
techniques for the identified constraints, and compose solutions that
address multiple bottlenecks simultaneously without creating conflicts.

The profiling phase employs tools like PyTorch Profiler, TensorFlow
Profiler, or NVIDIA Nsight Systems to reveal where time is spent during
training iterations. These are the same profiling approaches introduced
in the overview, now applied systematically to quantify which bottleneck
dominates. A profile might show 40\% of time in data loading, 35\% in
computation, and 25\% in memory operations---clearly indicating data
loading as the primary target for optimization.

The selection phase matches optimization techniques to identified
bottlenecks. Each technique we examine targets specific constraints:
prefetching addresses data movement latency, mixed-precision training
tackles both computational throughput and memory constraints, and
gradient accumulation manages memory limitations. Selection requires
understanding not just which bottleneck exists, but the characteristics
of the hardware, model architecture, and training configuration that
influence technique effectiveness.

The composition phase combines multiple techniques to achieve cumulative
benefits. Prefetching and mixed-precision training complement each other
(one addresses data loading, the other computation and memory), allowing
simultaneous application. However, some combinations create conflicts:
aggressive prefetching increases memory pressure, potentially
conflicting with memory-constrained configurations. Successful
composition requires understanding technique interactions and
dependencies.

This systematic framework---profile, select, compose---applies three
core optimization techniques to the primary bottleneck categories.
Prefetching and overlapping targets data movement latency by
coordinating data transfer with computation. Mixed-precision training
addresses both computational throughput and memory constraints through
reduced precision arithmetic. Gradient accumulation and checkpointing
manages memory constraints by trading computation for memory usage.
These techniques are not mutually exclusive; effective optimization
often combines multiple approaches to achieve cumulative benefits. In
practice, high-impact, low-complexity optimizations like data
prefetching should be implemented first, while complex optimizations
such as gradient checkpointing require cost-benefit analysis that
accounts for development effort and debugging complexity.

Figure~\ref{fig-optimization-flowchart} provides a visual decision tree
that operationalizes this systematic framework. Starting from profiling
results, the flowchart guides practitioners through bottleneck
identification to technique selection, ensuring optimization effort
targets the actual constraint rather than perceived issues.

\begin{figure}[htb]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/a940418dc7dfbc13f8f86ec82312ef5d18615590.pdf}}

}

\caption{\label{fig-optimization-flowchart}\textbf{Training Optimization
Decision Flowchart}: Systematic approach to optimization selection based
on profiling results. Begin by measuring GPU utilization, then follow
the decision path to identify whether the bottleneck is data-bound,
memory-bound, or compute-bound. Each path leads to specific techniques
that address the identified constraint.}

\end{figure}%

The flowchart embodies a critical insight: optimization is iterative.
After applying a technique, re-profiling often reveals that a different
bottleneck has become dominant. A data-bound system that implements
prefetching may become memory-bound, requiring the next technique in the
decision tree. This iterative refinement continues until profiling shows
balanced resource utilization or acceptable training throughput.

\subsection{Data Prefetching and Pipeline
Overlapping}\label{sec-ai-training-data-prefetching-pipeline-overlapping-e984}

Prefetching and overlapping techniques illustrate the systematic
framework in action, targeting data movement latency bottlenecks by
coordinating data transfer with computation. This optimization proves
most effective when profiling reveals that computational units remain
idle while waiting for data transfers to complete.

Training machine learning models involves significant data movement
between storage, memory, and computational units. The data pipeline
consists of sequential transfers: from disk storage to CPU memory, CPU
memory to GPU memory, and through the GPU processing units.
Figure~\ref{fig-fetching-naive} exposes the inefficiency of sequential
data transfer: the GPU remains idle during file operations (Open 1, Open
2), and training steps cannot begin until read operations complete,
leaving expensive compute resources underutilized for significant
portions of each epoch.

\begin{figure}[htb]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/997d40583f345db1b3ddf9603212131137078f06.pdf}}

}

\caption{\label{fig-fetching-naive}\textbf{Sequential Data Fetching}:
File open, read, and train operations execute serially across two
epochs, with the GPU remaining idle during all file operations. The full
sequential pipeline spans approximately 90 seconds, establishing the
baseline that overlapped prefetching improves upon.}

\end{figure}%

Prefetching addresses these inefficiencies by loading data into memory
before its scheduled computation time. During the processing of the
current batch, the system loads and prepares subsequent batches,
maintaining a consistent supply of ready data
(\citeproc{ref-tensorflow_data_2015}{Abadi et al. 2015}).

Overlapping builds upon prefetching by coordinating multiple pipeline
stages to execute concurrently. The system processes the current batch
while simultaneously preparing future batches through data loading and
preprocessing operations. Compare Figure~\ref{fig-fetching-naive} with
Figure~\ref{fig-fetching-optimized}: the optimized pipeline completes
two epochs in approximately 55 seconds compared to 90 seconds with
sequential fetching, a 40\% speedup achieved by overlapping read and
train operations within each time slice.

\begin{figure}[htb]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/9472c6c56634801aa4500446044ad8d7e985d8fa.pdf}}

}

\caption{\label{fig-fetching-optimized}\textbf{Overlapped Data
Prefetching}: Read and train operations execute concurrently, with each
time slice overlapping data loading for the next batch with computation
on the current batch. Two epochs complete in approximately 55 seconds
compared to 90 seconds with sequential fetching, a 40\% speedup.}

\end{figure}%

These optimization techniques demonstrate particular value in scenarios
involving large-scale datasets, preprocessing-intensive data, multi-GPU
training configurations, or high-latency storage systems.

\subsubsection{Prefetching
Mechanics}\label{sec-ai-training-prefetching-mechanics-2ba2}

Training data undergoes three main stages: retrieval from storage,
transformation into a suitable format, and utilization in model
training. An unoptimized pipeline executes these stages sequentially,
leaving the GPU idle during data fetching and preprocessing. Prefetching
eliminates this waiting time by loading data asynchronously during model
computation. Data loaders operate as separate threads or processes,
preparing the next batch while the current batch trains. This ensures
immediate data availability for the GPU when the current batch
completes.

Overlapping extends this efficiency by coordinating all three pipeline
stages simultaneously. As the GPU processes one batch, preprocessing
begins on the next batch, while data fetching starts for the subsequent
batch. This coordination maintains constant activity across all pipeline
stages.

Machine learning frameworks (introduced in \textbf{?@sec-ai-frameworks})
implement these techniques through built-in utilities.
Listing~\ref{lst-dataloader_usage} demonstrates PyTorch's DataLoader
configuration, where \texttt{num\_workers=4} enables four parallel
preprocessing threads and \texttt{prefetch\_factor=2} maintains a buffer
of eight batches ready for GPU consumption.

\begin{codelisting}

\caption{\label{lst-dataloader_usage}\textbf{Pipeline Optimization}:
Machine learning workflows benefit from efficient data handling through
batching and prefetching to maintain constant GPU utilization.}

\centering{

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{loader }\OperatorTok{=}\NormalTok{ DataLoader(}
\NormalTok{    dataset, batch\_size}\OperatorTok{=}\DecValTok{32}\NormalTok{, num\_workers}\OperatorTok{=}\DecValTok{4}\NormalTok{, prefetch\_factor}\OperatorTok{=}\DecValTok{2}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

}

\end{codelisting}%

The parameters \texttt{num\_workers} and \texttt{prefetch\_factor}
control parallel processing and data buffering. Multiple worker
processes handle data loading and preprocessing concurrently, while
prefetch\_factor determines the number of batches prepared in advance.

Buffer management plays a key role in pipeline efficiency. The prefetch
buffer size requires careful tuning to balance resource utilization. A
buffer that is too small causes the GPU to wait for data preparation,
reintroducing the idle time these techniques aim to eliminate.
Conversely, allocating an overly large buffer consumes memory that could
otherwise store model parameters or larger batch sizes.

The implementation relies on effective CPU-GPU coordination. The CPU
manages data preparation tasks while the GPU handles computation. This
division of labor, combined with storage I/O operations, creates an
efficient pipeline that minimizes idle time across hardware resources.

These optimization techniques yield particular benefits in scenarios
involving slow storage access, complex data preprocessing, or large
datasets, with specific advantages that depend on the computational and
data characteristics of each training context.

\subsubsection{Prefetching
Benefits}\label{sec-ai-training-prefetching-benefits-f7d6}

Table~\ref{tbl-prefetching} contrasts traditional sequential pipelines
against optimized approaches across four critical dimensions: GPU
utilization improves from frequent idle periods to near-constant
activity, training time decreases through parallelism, resource usage
shifts from suboptimal to maximized, and scalability transforms from
bottleneck-limited to adaptable.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3333}}@{}}
\caption{\textbf{Pipeline Optimization.} Prefetching and overlapping
maximize hardware utilization and reduce training time by enabling
parallel data loading and computation, overcoming bottlenecks inherent
in sequential pipelines.}\label{tbl-prefetching}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Aspect}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Traditional Pipeline}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{With Prefetching \& Overlapping}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Aspect}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Traditional Pipeline}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{With Prefetching \& Overlapping}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{GPU Utilization} & Frequent idle periods & Near-constant
utilization \\
\textbf{Training Time} & Longer due to sequential operations & Reduced
through parallelism \\
\textbf{Resource Usage} & Often suboptimal & Maximized across available
hardware \\
\textbf{Scalability} & Limited by slowest component & Adaptable to
various bottlenecks \\
\end{longtable}

The improvement in GPU utilization represents the most critical
advantage. In traditional pipelines, the GPU remains idle while waiting
for data to be fetched and preprocessed. Asynchronous data loading and
overlapping ensure that the GPU consistently has data ready to process,
eliminating these delays. This parallelism minimizes latency between
training iterations: while the GPU processes the current batch, the data
loader fetches and preprocesses the next batch, enabling faster
completion of training cycles.

These techniques are also highly scalable and adaptable to various
hardware configurations. Prefetching buffers and overlapping mechanisms
can be tuned to match the specific requirements of a system, whether the
bottleneck lies in slow storage, limited network bandwidth, or
computational constraints.

\subsubsection{Practical
Considerations}\label{sec-ai-training-pipeline-practical-considerations-4ba0}

Prefetching and overlapping deliver the greatest gains when
preprocessing is computationally expensive relative to model
computation. A typical image classification pipeline involving random
cropping (10 ms), color jittering (15 ms), and normalization (5 ms) adds
30 ms of delay per batch without prefetching; overlapping these
operations with the previous batch's GPU computation eliminates this
stall entirely. NLP workloads similarly benefit when tokenization and
subword processing would otherwise block the training loop.

The primary trade-off is memory: prefetch buffers consume GPU or host
memory proportional to the buffer depth and batch size. With a prefetch
factor of 2 and batch size of 256 high-resolution images
(\(1024\times1024\) pixels), the buffer alone requires approximately 2
GB. Tuning \texttt{num\_workers} and \texttt{prefetch\_factor} requires
empirical testing, as excessive worker threads contend for CPU resources
while insufficient buffering reintroduces data stalls. A practical
starting point is setting \texttt{num\_workers} equal to the number of
available CPU cores, then profiling to verify that data loading no
longer appears as idle GPU time. When storage bandwidth already exceeds
compute demand, prefetching adds complexity without measurable
throughput improvement.

\subsection{Mixed-Precision
Training}\label{sec-ai-training-mixedprecision-training-9218}

While prefetching optimizes data movement, mixed-precision training
addresses both computational throughput limitations and memory capacity
constraints. This technique complements the quantization approaches
discussed in \textbf{?@sec-model-compression}, strategically using
reduced precision arithmetic where possible while maintaining numerical
stability. Mixed-precision proves most effective when profiling reveals
that training is constrained by GPU memory capacity or when
computational units are underutilized due to memory bandwidth
limitations.

Mixed-precision training combines FP32, 16-bit floating-point (FP16),
and brain floating-point (bfloat16) formats to reduce memory usage and
speed up computation while preserving model accuracy
(\citeproc{ref-micikevicius2017mixed}{Micikevicius et al. 2017};
\citeproc{ref-wang_bfloat16_2019}{Y. Wang and Kanwar 2019}).

A neural network trained in FP32 requires 4 bytes per parameter, while
both FP16 and bfloat16 use 2 bytes. For a model with \(10^9\)
parameters, this reduction cuts memory usage from 4 GB to 2 GB. This
memory reduction enables larger batch sizes and deeper architectures on
the same hardware.

The numerical precision differences between these formats shape their
use cases. Table~\ref{tbl-precision-comparison} reveals that BF16's
8-bit exponent matches FP32's dynamic range (\(10^{-45}\) minimum
representable), while FP16's 5-bit exponent limits its range to
\(6 \times 10^{-8}\), explaining why gradients below this threshold
underflow to zero without loss scaling. FP32 represents numbers from
approximately \(\pm1.18 \times 10^{-38}\) to \(\pm3.4 \times 10^{38}\)
with 7 decimal digits of precision. FP16 ranges from
\(\pm6.10 \times 10^{-5}\) to \(\pm65,504\) with 3-4 decimal digits of
precision. Bfloat16, developed by Google Brain, maintains the same
dynamic range as FP32 (\(\pm1.18 \times 10^{-38}\) to
\(\pm3.4 \times 10^{38}\)) but with reduced precision (3-4 decimal
digits). This range preservation makes bfloat16 particularly suited for
deep learning training, as it handles large and small gradients more
effectively than FP16.

\begin{longtable}[]{@{}lrrr@{}}
\caption{\textbf{Precision Format Comparison.} The choice between FP16
and BF16 depends on whether dynamic range (BF16's strength) or precision
(FP16's advantage) matters more for the specific workload. Minimum
normal values shown are the practical thresholds for training, as
subnormal values may flush to zero on many
GPUs.}\label{tbl-precision-comparison}\tabularnewline
\toprule\noalign{}
\textbf{Property} & \textbf{FP32} & \textbf{FP16} & \textbf{BF16} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\textbf{Property} & \textbf{FP32} & \textbf{FP16} & \textbf{BF16} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Exponent bits} & 8 & 5 & 8 \\
\textbf{Mantissa bits} & 23 & 10 & 7 \\
\textbf{Min normal value} & 10\^{}-38 & 6.1 x 10\^{}-5 & 10\^{}-38 \\
\textbf{Tensor Core speedup} & 1x & 16x & 16x \\
\end{longtable}

The choice between formats depends on model characteristics. Models with
gradient outliers, common in transformer architectures, generally
benefit from BF16's wider dynamic range. Models with well-conditioned
gradients may prefer FP16's greater mantissa precision. Regardless of
the reduced-precision format chosen for forward and backward passes,
certain operations require FP32 precision: loss accumulation, softmax
denominators, normalization variance computation, and optimizer state.
These requirements stem from the numerical sensitivity of these
operations rather than arbitrary convention.

Figure~\ref{fig-mixed-precision} traces the data flow through
mixed-precision training's seven-step cycle: FP32 master weights (step
7) convert to FP16 for the forward pass (step 1), loss is scaled (step
2) before backpropagation (step 3), scaled FP16 gradients copy to FP32
(step 4), loss scaling is removed (step 5), and gradients update the
FP32 master weights (step 6), completing the cycle that achieves 16x
Tensor Core speedup while preserving numerical stability through
strategic precision management.

\begin{figure}[htb]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/b71e54bd9bba6a9e036e73125623b3ddfaa7d395.pdf}}

}

\caption{\label{fig-mixed-precision}\textbf{Mixed Precision Training}:
The seven-step cycle: (1) FP32 master weights convert to FP16 for the
forward pass, (2) loss is scaled to prevent gradient underflow, (3)
backpropagation computes scaled FP16 gradients, (4) gradients copy to
FP32, (5) loss scaling is removed, (6) FP32 gradients update master
weights, and (7) the cycle repeats. This approach achieves Tensor Core
speedups while preserving numerical stability.}

\end{figure}%

Modern hardware architectures are specifically designed to accelerate
reduced precision computations. GPUs from NVIDIA include Tensor Cores
optimized for FP16 and bfloat16 operations
(\citeproc{ref-nvidia_tensors_fp16_2017}{Jia et al. 2018}). Google's
TPUs natively support bfloat16, as this format was specifically designed
for machine learning workloads. These architectural optimizations
typically enable an order of magnitude higher computational throughput
for reduced precision operations compared to FP32, making
mixed-precision training particularly efficient on modern hardware.

\subsubsection{FP16
Computation}\label{sec-ai-training-fp16-computation-374c}

The majority of operations in mixed-precision training, such as matrix
multiplications and activation functions, are performed in FP16. The
reduced precision allows these calculations to be executed faster and
with less memory consumption compared to FP32. FP16 operations are
particularly effective on modern GPUs equipped with Tensor Cores, which
are designed to accelerate computations involving half-precision values.
These cores perform FP16 operations natively, resulting in significant
speedups.

\subsubsection{FP32
Accumulation}\label{sec-ai-training-fp32-accumulation-4e2d}

FP16 is efficient, but its limited precision can lead to numerical
instability in critical operations like gradient updates.
Mixed-precision training retains FP32 precision for certain steps, such
as weight updates and gradient accumulation, avoiding gradient underflow
or overflow and ensuring the model converges correctly during training.

\subsubsection{Loss Scaling}\label{sec-ai-training-loss-scaling-f9f5}

One of the key challenges with FP16 is its reduced dynamic
range\sidenote{\textbf{FP16 Dynamic Range}: IEEE 754 half-precision
(FP16) has only 5 exponent bits vs.~8 in FP32, limiting its range to
±65,504 (vs.~±3.4×10³⁸ for FP32). More critically, FP16's smallest
representable positive number is 6×10⁻⁸, while gradients in deep
networks often fall below 10⁻¹⁰. This mismatch causes gradient
underflow, where tiny but important gradients become zero, stalling
training, hence the need for loss scaling techniques. Once the gradients
are computed, the scaling factor is reversed during the weight update
step to restore the original gradient magnitude. This process allows
FP16 to be used effectively without sacrificing numerical stability. },
which increases the likelihood of gradient values becoming too small to
be represented accurately. Loss scaling addresses this issue by
temporarily amplifying gradient values during backpropagation.
Specifically, the loss value is scaled by a large factor (e.g.,
\(2^{10}\)) before gradients are computed, ensuring they remain within
the representable range of FP16.

Machine learning frameworks provide built-in support for mixed-precision
training. PyTorch's \texttt{torch.cuda.amp} (Automatic Mixed Precision)
library automates the process of selecting which operations to perform
in FP16 or FP32, as well as applying loss scaling when necessary.

\subsubsection{Mixed-Precision
Benefits}\label{sec-ai-training-mixedprecision-benefits-d57b}

Mixed-precision benefits manifest across three dimensions that compound
in practice. First, memory consumption decreases by approximately 50\%:
a 1 billion parameter transformer requires 4 GB in FP32 but only 2 GB in
FP16 for weights alone, enabling larger batch sizes or deeper
architectures. Second, computational throughput increases dramatically
as Tensor Cores achieve 2-3\(\times\) speedup for matrix
multiplications, as detailed in
Section~\ref{sec-ai-training-mixedprecision-hardware-support-d7c1}.
Third, halving tensor sizes proportionally reduces inter-device
communication bandwidth requirements in distributed training.

These benefits compound: a practitioner might simultaneously double
batch size (memory savings), accelerate each iteration (Tensor Core
throughput), and reduce gradient synchronization time (smaller tensors).
Quantifying the \emph{GPT-2 mixed precision training impact} makes these
compounding gains concrete.

\phantomsection\label{callout-notebookux2a-1.23}
\begin{fbx}{callout-notebook}{AI Engineer’s Notebook:}{GPT-2 Mixed Precision Training Impact}
\phantomsection\label{callout-notebook*-1.23}
GPT-2 training heavily relies on mixed-precision (FP16) to fit within
GPU memory constraints.

\textbf{Memory Savings}

FP32 Baseline:

\begin{itemize}
\tightlist
\item
  Parameters: 1.5B × 4 bytes = 6.0 GB
\item
  Activations (batch=32): \textasciitilde65 GB
\item
  Gradients: 6.0 GB
\item
  Total: \textasciitilde77 GB (exceeds any single GPU)
\end{itemize}

FP16 Mixed Precision:

\begin{itemize}
\tightlist
\item
  Parameters (FP16): 1.5B × 2 bytes = 3.0 GB
\item
  Activations (FP16): \textasciitilde32.6 GB
\item
  Gradients (FP16): 3.0 GB
\item
  Optimizer state (FP32 master weights): 12.0 GB (Adam m, v)
\item
  Total: \textasciitilde51 GB (still tight, but manageable with
  optimizations)
\end{itemize}

With Mixed Precision + Gradient Checkpointing:

\begin{itemize}
\tightlist
\item
  Activations reduced to \textasciitilde8 GB (recompute during backward)
\item
  Total: \textasciitilde26 GB → fits comfortably in 32GB V100
\end{itemize}

\textbf{Computational Speedup}

On NVIDIA V100 (Tensor Cores enabled):

\begin{itemize}
\tightlist
\item
  FP32 throughput: \textasciitilde90 samples/sec
\item
  FP16 throughput: \textasciitilde220 samples/sec
\item
  Speedup: 2.4× faster training
\end{itemize}

\textbf{Critical Implementation Details}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Loss Scaling: Start with scale=2\^{}15, dynamically reduce if overflow
  detected. Gradients in attention layers can range from 10\^{}-6 to
  10\^{}3, so loss scaling prevents underflow.
\item
  FP32 Master Weights: Optimizer updates in FP32 prevent weight
  stagnation. Small learning rate (2.5e-4) × FP16 gradient might round
  to zero; FP32 accumulation preserves these tiny updates.
\item
  Selective FP32 Operations:

  \begin{itemize}
  \tightlist
  \item
    LayerNorm: Computed in FP32 (requires high precision for variance
    calculation)
  \item
    Softmax: Computed in FP32 (exponentials need full range)
  \item
    All else: FP16
  \end{itemize}
\end{enumerate}

\textbf{Training Cost Impact}

\begin{itemize}
\tightlist
\item
  FP32: \textasciitilde\$50,000 for 2 weeks on 32 V100s
\item
  FP16: \textasciitilde\$28,000 for 1.2 weeks on 32 V100s
\item
  Savings: \$22,000 + 6 days faster iteration
\end{itemize}

\textbf{Quality Impact:} Minimal. GPT-2 perplexity within 0.5\% of FP32
baseline, well within noise margin.

\end{fbx}

\subsubsection{Practical
Considerations}\label{sec-ai-training-mixedprecision-practical-considerations-a644}

Mixed-precision training has become the default for large-scale
workloads across NLP (\citeproc{ref-Devlin2019}{Devlin et al. 2018};
\citeproc{ref-brown2020language}{Brown et al. 2020}), computer vision
(\citeproc{ref-he2016residual}{He et al. 2016}), and reinforcement
learning. The benefits compound: halving tensor sizes from FP32 to FP16
simultaneously reduces memory consumption, accelerates Tensor Core
computation, and cuts inter-device communication bandwidth in
distributed settings.

The primary limitation is FP16's restricted dynamic range
\((\pm65{,}504)\). Gradient values below \(6 \times 10^{-5}\) underflow
to zero, requiring loss scaling factors (typically \(2^{8}\) to
\(2^{14}\)) to keep gradients representable. Recurrent architectures
with long sequences are particularly susceptible to accumulated
numerical errors. NaN values in gradients or activations, the telltale
sign of precision failures, appear more frequently in FP16 workflows and
may manifest differently than in FP32, complicating debugging. BF16
eliminates many of these issues by preserving FP32's dynamic range,
though at the cost of reduced mantissa precision. For models under 10M
parameters, the overhead of configuring mixed precision may exceed the
performance benefit.

\subsubsection{Mixed-Precision Hardware
Support}\label{sec-ai-training-mixedprecision-hardware-support-d7c1}

Understanding how modern hardware implements reduced-precision
arithmetic reveals why mixed-precision achieves substantial speedups
beyond mere memory savings. The performance gains from FP16 and BF16
computation stem from specialized hardware units designed explicitly for
low-precision tensor operations\sidenote{\textbf{Tensor}: From Latin
``tensus'' (stretched), past participle of ``tendere'' (to stretch).
Originally used in physics for stress/strain relationships in materials,
the term was adopted by mathematicians for multi-dimensional arrays that
transform in specific ways under coordinate changes. In ML, ``tensor''
simply means a multi-dimensional array: scalars (0D), vectors (1D),
matrices (2D), and higher-dimensional arrays (3D+). NVIDIA's ``Tensor
Cores'' perform fused multiply-accumulate on small matrix tiles,
optimized for the tensor operations that dominate neural network
computation. }, with architectural decisions that trade numerical range
or precision for dramatic increases in computational throughput.

\subsubsection*{Tensor Core
Architecture}\label{tensor-core-architecture}
\addcontentsline{toc}{subsubsection}{Tensor Core Architecture}

NVIDIA introduced Tensor Cores in their Volta architecture (2017) as
dedicated matrix multiplication units optimized for mixed-precision
workloads. Unlike standard CUDA cores that process scalar or small
vector operations, Tensor Cores perform \(4 \times 4\) matrix
multiply-accumulate operations in a single clock cycle. For FP16 inputs,
a single Tensor Core executes:

\[
D = A \times B + C
\]

where \(A\) and \(B\) are \(4 \times 4\) FP16 matrices, \(C\) is an FP32
accumulator, and \(D\) is the FP32 result. This accumulation in higher
precision prevents catastrophic cancellation errors that would occur if
intermediate products were stored in FP16.

\subsubsection*{Throughput Scaling}\label{throughput-scaling}
\addcontentsline{toc}{subsubsection}{Throughput Scaling}

The computational advantage of Tensor Cores becomes apparent when
comparing theoretical peak performance across precisions. An NVIDIA A100
GPU specifications:

\begin{itemize}
\tightlist
\item
  \textbf{FP32 throughput}: 19.5 TFLOPs (standard CUDA cores)
\item
  \textbf{FP16 Tensor Core throughput}: 312 TFLOPs (16× speedup)
\item
  \textbf{BF16 Tensor Core throughput}: 312 TFLOPs (same as FP16)
\item
  \textbf{FP8 Tensor Core throughput} (H100 SXM): 1,979 TFLOPs without
  sparsity (approximately 100× speedup over FP32)
\end{itemize}

This 16× theoretical speedup for FP16 materializes in practice because
matrix multiplications, the dominant operation in neural network
training, map naturally to Tensor Core operations. A transformer's
attention mechanism computing \(QK^T\) for a \((B, H, N, D)\) tensor
requires \(2 \times B \times H \times N^2 \times D\) FLOPs. On Tensor
Cores, this executes 16× faster than on CUDA cores, directly translating
to wall-clock speedups.

\subsubsection*{BF16 Hardware
Implementation}\label{bf16-hardware-implementation}
\addcontentsline{toc}{subsubsection}{BF16 Hardware Implementation}

Brain Float 16 (BF16) maintains FP32's 8-bit exponent while reducing the
mantissa to 7 bits. This design choice prioritizes dynamic range
preservation over precision, crucial for gradient-based learning where
values span many orders of magnitude. Google's TPUs natively support
BF16, while NVIDIA's Ampere architecture (A100) and newer provide full
hardware support.

The hardware advantage of BF16 over FP16 emerges in gradient
accumulation scenarios. Consider summing 1000 gradients with values
around \(10^{-4}\). FP16's smallest positive subnormal value is
approximately \(6 \times 10^{-8}\), but the smallest normal value is
\(6.1 \times 10^{-5}\).\sidenote{Many GPU implementations flush
subnormal numbers to zero for performance reasons, making the normal
minimum (\(6.1 \times 10^{-5}\)) the practical threshold. Loss scaling
addresses this by multiplying gradients before the backward pass to keep
values in the representable range. } In practice, gradients below
approximately \(10^{-7}\) may underflow to zero depending on hardware
behavior. BF16's smallest representable value matches FP32 at
approximately \(10^{-45}\), so no underflow occurs. FP32 has full range
but computes 2x slower.

For transformer training where attention gradients vary from
\(10^{-10}\) to \(10^3\), BF16's range prevents the loss scaling
complexity required for FP16, simplifying implementation without
sacrificing throughput.

\subsubsection*{FP8 Precision}\label{fp8-precision}
\addcontentsline{toc}{subsubsection}{FP8 Precision}

NVIDIA's Hopper architecture (H100) introduces FP8 support with two
formats. E4M3 uses 4 exponent bits and 3 mantissa bits (prioritizing
range), while E5M2 uses 5 exponent bits and 2 mantissa bits
(prioritizing precision).

FP8 training doubles Tensor Core throughput again (3.9 PFLOPs on H100
versus 1.98 PFLOPs for FP16). However, FP8's severely limited precision
requires per-tensor scaling factors maintained in higher precision,
adding algorithmic complexity. The decision tree becomes:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3333}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Precision}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{When to Use}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Hardware Requirement}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{FP8} & Maximum throughput on H100, with careful scaling & H100
or newer \\
\textbf{BF16} & Default for transformers, wide dynamic range & A100, TPU
v4+ \\
\textbf{FP16} & Computer vision, controlled gradients & V100, A100 \\
\textbf{FP32} & Numerical stability critical, small models & All GPUs \\
\end{longtable}

\subsubsection*{Memory Bandwidth
Utilization}\label{memory-bandwidth-utilization}
\addcontentsline{toc}{subsubsection}{Memory Bandwidth Utilization}

Reduced precision not only accelerates computation but also alleviates
memory bandwidth bottlenecks. Modern GPUs are increasingly compute-bound
rather than bandwidth-bound for large matrix operations, but data
movement still limits performance for smaller operations. A100's
specifications illustrate this:

\begin{itemize}
\tightlist
\item
  HBM2e bandwidth: 2,039 GB/s
\item
  FP32 throughput: 19.5 TFLOPs → requires
  \(19\.5 \times 10^{12} \times 4 \text{ bytes} = 78 \text{ TB/s}\) if
  every FLOP needs new data
\item
  Actual requirement (with data reuse): Much lower, but
  bandwidth-limited for operations with low arithmetic intensity
\end{itemize}

FP16 halves memory traffic for the same computation, effectively
doubling available bandwidth. For operations like layer normalization
(arithmetic intensity approximately 1 FLOP/byte), this bandwidth
doubling directly translates to speedups even without Tensor Core
involvement.

\subsubsection*{Practical Framework
Integration}\label{practical-framework-integration}
\addcontentsline{toc}{subsubsection}{Practical Framework Integration}

Modern frameworks abstract hardware complexity through automatic
operation routing, as discussed in \textbf{?@sec-ai-frameworks}. The
framework runtime determines which operations benefit from reduced
precision and which require FP32 for numerical stability.
Listing~\ref{lst-mixed-precision} illustrates this pattern.

\begin{codelisting}

\caption{\label{lst-mixed-precision}\textbf{Mixed Precision Training}:
Automatic precision selection with loss scaling to prevent gradient
underflow while maximizing Tensor Core utilization.}

\centering{

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ torch}
\ImportTok{from}\NormalTok{ torch.cuda.amp }\ImportTok{import}\NormalTok{ autocast, GradScaler}

\NormalTok{model }\OperatorTok{=}\NormalTok{ TransformerModel().cuda()}
\NormalTok{optimizer }\OperatorTok{=}\NormalTok{ torch.optim.Adam(model.parameters(), lr}\OperatorTok{=}\FloatTok{1e{-}4}\NormalTok{)}
\NormalTok{scaler }\OperatorTok{=}\NormalTok{ GradScaler()  }\CommentTok{\# Handles loss scaling automatically}

\ControlFlowTok{for}\NormalTok{ batch }\KeywordTok{in}\NormalTok{ dataloader:}
\NormalTok{    optimizer.zero\_grad()}

    \CommentTok{\# Automatic precision selection per operation}
    \ControlFlowTok{with}\NormalTok{ autocast(dtype}\OperatorTok{=}\NormalTok{torch.float16):  }\CommentTok{\# or torch.bfloat16}
\NormalTok{        output }\OperatorTok{=}\NormalTok{ model(batch)}
\NormalTok{        loss }\OperatorTok{=}\NormalTok{ criterion(output, target)}

    \CommentTok{\# Scale loss to prevent gradient underflow}
\NormalTok{    scaler.scale(loss).backward()}

    \CommentTok{\# Unscale gradients before optimizer step}
\NormalTok{    scaler.step(optimizer)}
\NormalTok{    scaler.update()  }\CommentTok{\# Adjust scaling factor dynamically}
\end{Highlighting}
\end{Shaded}

}

\end{codelisting}%

The \texttt{autocast} context automatically selects precision per
operation:

\begin{itemize}
\tightlist
\item
  \textbf{FP16/BF16}: Matrix multiplications, convolutions
\item
  \textbf{FP32}: Softmax, layer normalization, loss computation
\end{itemize}

This selective precision maximizes hardware utilization while
maintaining numerical stability.

\subsubsection*{Hardware-Aware Optimization
Strategy}\label{hardware-aware-optimization-strategy}
\addcontentsline{toc}{subsubsection}{Hardware-Aware Optimization
Strategy}

Optimal mixed-precision training requires matching the precision format
to hardware capabilities. Table~\ref{tbl-hw-precision-strategy}
summarizes the recommended precision strategy for each GPU generation,
reflecting the evolution from FP16-only support on Volta to native FP8
on Hopper.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3333}}@{}}
\caption{\textbf{Precision Strategy by GPU Architecture.} Each
generation introduces wider precision support, reducing the engineering
burden of loss scaling while increasing
throughput.}\label{tbl-hw-precision-strategy}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Architecture}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Recommended Precision}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Key Considerations}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Architecture}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Recommended Precision}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Key Considerations}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{V100 (Volta)} & FP16 with loss scaling & No BF16 support;
gradient clipping essential \\
\textbf{A100 (Ampere)} & BF16 for transformers; FP16 for CNNs & TF32
mode provides automatic 2-3× speedup for legacy FP32 code \\
\textbf{H100 (Hopper)} & FP8 via TransformerEngine & Requires FP8-aware
training recipes; 1,979 TFLOPs peak throughput \\
\end{longtable}

The performance impact across generations is substantial. Training our
lighthouse GPT-2 model (1.5B parameters) on a single GPU illustrates how
hardware and precision co-evolve: V100 achieves 18 samples/sec in FP32
and 45 samples/sec in FP16 (2.5× speedup), A100 reaches 165 samples/sec
in BF16 (9.2× over V100 FP32), and H100 delivers 380 samples/sec in FP8
(21× over V100 FP32). These speedups compound with the memory savings
discussed earlier, enabling both faster iteration and larger models. The
hardware-software co-design principle emerges clearly: algorithmic
techniques like mixed precision unlock specialized hardware
capabilities, while hardware features like Tensor Cores make certain
algorithms practical.

\subsection{Flash Attention: IO-Aware Attention
Optimization}\label{sec-ai-training-flash-attention-ioaware-attention-optimization-3da0}

Mixed-precision training addresses two bottlenecks: compute throughput
(Tensor Cores operate faster on FP16) and memory capacity (half the
bytes per value). But for transformer models during training, a third
bottleneck often dominates: memory bandwidth. The attention mechanism's
quadratic intermediate matrices must be repeatedly loaded and stored
during the forward pass and accessed again during backpropagation, and
even with reduced precision, the sheer volume of memory traffic can
leave compute units idle. This brings us to Flash Attention
(\citeproc{ref-dao2022flashattention}{Dao et al. 2022}), a critical
training optimization that complements mixed precision by fundamentally
restructuring how attention is computed. Rather than optimizing what
precision to use, Flash Attention optimizes how data flows between
memory hierarchies through strategic tiling and recomputation, achieving
2-4x training speedups while enabling training on sequences that would
otherwise cause out-of-memory errors. While Flash Attention also
benefits inference, its impact is most dramatic during training, where
both the forward and backward passes must process attention matrices.

\subsubsection{The Standard Attention Memory
Bottleneck}\label{sec-ai-training-standard-attention-memory-bottleneck-6f39}

As detailed in \textbf{?@sec-dnn-architectures}, standard self-attention
computes relationships between all positions in a sequence. For an input
sequence of length \(n\), the mechanism computes an \(n \times n\)
attention matrix:

\[ \text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V \]

The memory bottleneck emerges from materializing the \(n \times n\)
intermediate matrices for scores and probabilities. For a sequence
length of 4,096 tokens with embedding dimension 64 (typical for a single
attention head), the attention score matrix alone requires
\(4\,096^2 \times 4 \text{{ bytes}} = 64 \text{{ MB}}\) in FP32. With 16
attention heads, this grows to 1 GB just for intermediate attention
matrices, not including the keys, queries, values, or output tensors.

Modern GPU memory hierarchy exacerbates this bottleneck. HBM (High
Bandwidth Memory) provides 40--80 GB capacity with 1--2 TB/s bandwidth,
while SRAM (on-chip memory) provides only 20--40 MB capacity but
delivers 20+ TB/s bandwidth (10× faster). Standard attention stores
these large matrices in slow HBM and repeatedly loads them during the
backward pass. For GPT-2 scale models processing 2048-token sequences,
attention operations spend 70-80\% of execution time waiting for memory
transfers rather than computing, leaving expensive tensor cores
underutilized.

The backward pass compounds this problem. Computing gradients requires
storing attention scores from the forward pass:

\[
\frac{\partial L}{\partial Q} = \frac{\partial L}{\partial O} \cdot V^T \cdot P^T + \text{additional terms requiring } S
\]

Storing both \(S\) and \(P\) for all layers in HBM during forward pass
doubles memory requirements and creates multiple round-trips between HBM
and compute units during backpropagation.

\subsubsection{IO-Aware Attention Through
Tiling}\label{sec-ai-training-ioaware-attention-tiling-f02f}

Flash Attention eliminates the need to materialize full \(n \times n\)
attention matrices in HBM by computing attention incrementally through
tiling. Instead of computing the entire attention matrix at once, the
algorithm partitions \(Q\), \(K\), and \(V\) into blocks small enough to
fit in fast SRAM, computes attention scores for these blocks, and
incrementally accumulates results.

The key algorithmic insight relies on the mathematical structure of
softmax attention. Standard attention computes:

\[
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
\]

Flash Attention decomposes this computation by partitioning queries into
\(B_q\) blocks and keys/values into \(B_k\) blocks. For each query block
\(Q_i\) (size \(b \times d\)):

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Initialize output block \(O_i = \mathbf{0}\) and normalizer
  \(l_i = \mathbf{0}\) in SRAM
\item
  For each key-value block \((K_j, V_j)\):

  \begin{itemize}
  \tightlist
  \item
    Load \(Q_i\), \(K_j\), \(V_j\) into SRAM
  \item
    Compute attention scores: \(S_{ij} = Q_i K_j^T / \sqrt{d_k}\) (size
    \(b \times b\), fits in SRAM)
  \item
    Compute probabilities: \(P_{ij} = \text{softmax}(S_{ij})\) within
    SRAM
  \item
    Accumulate: Update \(O_i\) and \(l_i\) with \(P_{ij} V_j\)
  \item
    Discard \(S_{ij}\) and \(P_{ij}\) (no HBM storage)
  \end{itemize}
\item
  Write final \(O_i\) to HBM
\end{enumerate}

No \(n \times n\) matrix ever exists in HBM. The largest intermediate
tensor is \(b \times b\) (typically \(b = 128\)), requiring only 64 KB
for a \(128 \times 128\) FP32 matrix compared to 64 MB for the full
\(4096 \times 4096\) matrix.

The online softmax algorithm enables this decomposition. Traditional
softmax requires knowing all inputs before computing any output:
\(\text{softmax}(x)_i = e^{x_i} / \sum_j e^{x_j}\). Flash Attention uses
an incremental formulation that updates softmax statistics as new blocks
arrive, tracking the running maximum \(m\) (for numerical stability) and
denominator \(l\) as each block is processed, then rescaling accumulated
outputs accordingly.

\subsubsection{Memory and IO Complexity
Analysis}\label{sec-ai-training-memory-io-complexity-analysis-5da5}

Flash Attention achieves asymptotic improvements in both memory
footprint and memory IO operations, the true bottleneck in
bandwidth-limited scenarios.

\textbf{Memory Complexity.} - \textbf{Standard Attention}: \(O(n^2)\)
memory for storing \(S\) and \(P\) matrices across all sequence
positions - \textbf{Flash Attention}: \(O(n)\) memory, storing only
input/output tensors \((Q, K, V, O)\) plus a small constant SRAM buffer

For \(n = 4096\), \(d = 64\): Standard attention requires
\(4096^2 \times 4 \text{ bytes} = 67 \text{ MB}\) per head. Flash
Attention requires only
\((3 \times 4096 \times 64) \times 4 \text{ bytes} \approx 3 \text{ MB}\)
per head, a \textbf{21x reduction}.

\textbf{IO Complexity (Memory Reads/Writes).} Standard attention
performs:

\begin{itemize}
\tightlist
\item
  Forward pass: Read \(Q, K, V\) from HBM, write \(S, P, O\) to HBM:
  \(O(n \cdot d + n^2)\) bytes
\item
  Backward pass: Read \(Q, K, V, S, P, O, dO\) from HBM, write
  \(dQ, dK, dV\): \(O(n \cdot d + n^2)\) bytes
\item
  Total: \(O(n \cdot d + n^2)\) HBM accesses
\end{itemize}

Flash Attention performs different memory operations. In the forward
pass, it reads \(Q, K, V\) once and writes \(O\) once, requiring
\(O(n \cdot d)\) bytes. In the backward pass, it recomputes \(S, P\) in
SRAM from \(Q, K, V\) and writes \(dQ, dK, dV\), again requiring
\(O(n \cdot d)\) bytes. Total HBM accesses are \(O(n \cdot d)\).

For large sequence lengths where \(n \gg d\), Flash Attention reduces
memory traffic by a factor of \(n\). With \(n = 4096\) and \(d = 64\),
this represents a \textbf{64× reduction} in memory bandwidth
consumption.

\textbf{Computational Complexity.} Both approaches require \(O(n^2 d)\)
FLOPs for attention computation. Flash Attention performs additional
recomputation during backward pass (regenerating \(S\) and \(P\) from
saved \(Q, K, V\)), adding roughly 20\% more FLOPs. However, by
converting the workload from bandwidth-bound to compute-bound, Flash
Attention achieves net speedups despite higher FLOP counts since modern
GPUs have abundant compute capacity but limited memory bandwidth.

\subsubsection{Implementation and Hardware
Utilization}\label{sec-ai-training-implementation-hardware-utilization-20a8}

Flash Attention's performance gains materialize through careful
exploitation of GPU memory hierarchy. Modern frameworks integrate these
optimizations transparently, automatically selecting the most efficient
attention implementation based on hardware capabilities and input
characteristics. Listing~\ref{lst-flash-attention-comparison} contrasts
standard and optimized attention implementations.

\begin{codelisting}

\caption{\label{lst-flash-attention-comparison}\textbf{Attention
Implementation Comparison}: Standard attention materializes the full n×n
matrix in HBM, while Flash Attention uses PyTorch's optimized
implementation or the dedicated flash-attn library.}

\centering{

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ torch}
\ImportTok{import}\NormalTok{ torch.nn.functional }\ImportTok{as}\NormalTok{ F}


\CommentTok{\# Standard attention (materializes n×n matrix)}
\KeywordTok{def}\NormalTok{ standard\_attention(q, k, v):}
    \CommentTok{\# q, k, v: [batch, heads, seq\_len, head\_dim]}
\NormalTok{    scores }\OperatorTok{=}\NormalTok{ torch.matmul(q, k.transpose(}\OperatorTok{{-}}\DecValTok{2}\NormalTok{, }\OperatorTok{{-}}\DecValTok{1}\NormalTok{)) }\OperatorTok{/}\NormalTok{ (}
\NormalTok{        q.size(}\OperatorTok{{-}}\DecValTok{1}\NormalTok{) }\OperatorTok{**} \FloatTok{0.5}
\NormalTok{    )}
\NormalTok{    attn }\OperatorTok{=}\NormalTok{ F.softmax(scores, dim}\OperatorTok{={-}}\DecValTok{1}\NormalTok{)  }\CommentTok{\# n×n matrix in HBM}
\NormalTok{    output }\OperatorTok{=}\NormalTok{ torch.matmul(attn, v)}
    \ControlFlowTok{return}\NormalTok{ output}


\CommentTok{\# Flash Attention (no n×n materialization)}
\KeywordTok{def}\NormalTok{ flash\_attention(q, k, v):}
    \CommentTok{\# Automatically uses Flash Attention if available}
\NormalTok{    output }\OperatorTok{=}\NormalTok{ F.scaled\_dot\_product\_attention(q, k, v)}
    \ControlFlowTok{return}\NormalTok{ output}


\CommentTok{\# Explicit Flash Attention 2 (flash{-}attn library)}
\ImportTok{from}\NormalTok{ flash\_attn }\ImportTok{import}\NormalTok{ flash\_attn\_func}


\KeywordTok{def}\NormalTok{ flash\_attn\_2(q, k, v):}
    \CommentTok{\# q, k, v: [batch, seq\_len, heads, head\_dim]}
    \CommentTok{\# Different layout for optimized memory access}
\NormalTok{    output }\OperatorTok{=}\NormalTok{ flash\_attn\_func(q, k, v)}
    \ControlFlowTok{return}\NormalTok{ output}
\end{Highlighting}
\end{Shaded}

}

\end{codelisting}%

\subsubsection*{Benchmark Results}\label{benchmark-results}
\addcontentsline{toc}{subsubsection}{Benchmark Results}

Training a GPT-2 style transformer on NVIDIA A100 GPU (12 layers, 768
hidden dim, 12 heads) with varying sequence lengths:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 12\tabcolsep) * \real{0.1429}}
  >{\raggedright\arraybackslash}p{(\linewidth - 12\tabcolsep) * \real{0.1429}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 12\tabcolsep) * \real{0.1429}}
  >{\raggedright\arraybackslash}p{(\linewidth - 12\tabcolsep) * \real{0.1429}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 12\tabcolsep) * \real{0.1429}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 12\tabcolsep) * \real{0.1429}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 12\tabcolsep) * \real{0.1429}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Sequence Length}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Standard Forward}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Flash Forward}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Standard Backward}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Flash Backward}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Memory (Standard)}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Memory (Flash)}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
512 & 12 ms & 8 ms & 35 ms & 18 ms & 4.2 GB & 2.8 GB \\
2048 & 45 ms & 15 ms & 120 ms & 35 ms & 18 GB & 6 GB \\
4096 & OOM & 32 ms & OOM & 85 ms & \textgreater40 GB & 12 GB \\
8192 & OOM & 68 ms & OOM & 180 ms & \textgreater80 GB & 24 GB \\
\end{longtable}

Standard attention runs out of memory beyond 2048 tokens on a 40 GB
A100, while Flash Attention trains sequences up to 8192 tokens. Even at
2048 tokens where both fit, Flash Attention achieves 3× forward pass
speedup and 3.4× backward pass speedup.

Subsequent versions have continued improving performance: Flash
Attention 2 (2023) achieved 1.5-2× additional speedup through better
parallelism and register allocation, while Flash Attention 3 (2024)
exploits FP8 tensor cores and asynchronous memory operations on Hopper
GPUs to reach 740 TFLOPs on H100 (75\% of theoretical peak).

\subsubsection{When to Use Flash
Attention}\label{sec-ai-training-use-flash-attention-375d}

Flash Attention should be considered the default attention
implementation for transformer training with clear decision criteria:

\textbf{Always use Flash Attention when:} - Training any transformer
model with sequence length \textgreater{} 512 tokens - Sequence length
\textgreater{} 2048 tokens (essential, standard attention likely OOMs) -
Using modern GPUs (A100, H100) with hardware support - Memory is
constrained and larger batches are desired

\textbf{Flash Attention provides diminishing returns when:} - Sequence
length \textless{} 512 tokens (overhead of tiling not worthwhile) -
Using very old GPU architectures without fast SRAM - Non-attention
architectures (CNNs, MLPs)

\textbf{Practical Integration Considerations:}

Deep learning frameworks handle Flash Attention integration
transparently. PyTorch 2.0+ automatically selects Flash Attention when
available and appropriate. For optimal performance:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Ensure tensor layouts match library expectations (contiguous memory,
  correct dimension ordering)
\item
  Use FP16 or BF16 for maximum speedup (Flash Attention optimized for
  mixed precision)
\item
  Combine with gradient checkpointing for further memory savings (4-8×
  larger models trainable)
\end{enumerate}

The integration is typically a single-line change, as shown in
Listing~\ref{lst-flash-attention-migration}. \textbf{By swapping the
manual attention call for the framework's optimized primitive, the
developer delegates the complex tiling and SRAM management required to
bypass the HBM bandwidth bottleneck to the underlying library.}

\begin{codelisting}

\caption{\label{lst-flash-attention-migration}\textbf{Flash Attention
Migration}: Replacing manual attention with PyTorch's optimized scaled
dot-product attention.}

\centering{

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Old: Manual attention implementation}
\NormalTok{attn\_output }\OperatorTok{=}\NormalTok{ model.manual\_attention(q, k, v)}

\CommentTok{\# New: Flash Attention enabled}
\NormalTok{attn\_output }\OperatorTok{=}\NormalTok{ F.scaled\_dot\_product\_attention(q, k, v)}
\end{Highlighting}
\end{Shaded}

}

\end{codelisting}%

\subsubsection{Systems Implications and Broader
Principles}\label{sec-ai-training-systems-implications-broader-principles-c4f0}

Flash Attention exemplifies a fundamental systems engineering principle:
\textbf{IO-aware algorithm design}. The core insight recognizes that
modern accelerators are increasingly compute-abundant but
bandwidth-constrained. An algorithm's runtime is determined not by FLOP
count but by memory traffic.

This principle extends beyond attention:

\textbf{IO-aware matrix multiplication}: Tiling algorithms like those in
CUTLASS minimize DRAM traffic by maximizing data reuse in fast caches. A
naive \(n \times n\) matrix multiply performs \(O(n^3)\) FLOPs with
\(O(n^2)\) memory traffic, while blocked algorithms maintain \(O(n^3)\)
FLOPs but reduce cache misses through locality optimization.

\textbf{Communication-efficient distributed training}: Gradient
compression techniques apply similar principles, trading extra
computation (compression/decompression) for reduced network bandwidth
consumption.

\textbf{Edge deployment}: Low-power edge devices with limited memory
bandwidth benefit even more from IO-aware algorithms, where a 10\%
increase in FLOPs that halves memory traffic yields 3-5× energy savings.

Flash Attention's impact on practical model training capabilities is
substantial. By eliminating the \(O(n^2)\) memory bottleneck, it
enables:

\begin{itemize}
\tightlist
\item
  \textbf{4× longer sequences} on the same hardware (2K → 8K context for
  GPT-2 on A100)
\item
  \textbf{2× larger batch sizes} through freed memory (faster
  convergence)
\item
  \textbf{Deeper models} by reducing activation memory (more layers fit
  in same budget)
\end{itemize}

For a 7B parameter model training on A100 GPUs, Flash Attention
transforms training from infeasible (OOM at 2K context) to practical (8K
context with room for batch size 32), representing the difference
between a model that cannot be trained and one deployed in production.

The technique demonstrates that algorithmic innovation at the systems
level, exploiting hardware characteristics like memory hierarchy, can
provide order-of-magnitude improvements that no amount of hardware
scaling alone would achieve. This systems-aware algorithm design
philosophy, treating memory bandwidth as the primary constraint and
compute as abundant, increasingly defines performance optimization in
modern ML systems.

Flash Attention addresses memory bandwidth bottlenecks during
computation, but another class of memory constraints exists: the sheer
capacity required to store activations and optimizer states
simultaneously. When models or batch sizes exceed GPU memory capacity,
two complementary techniques trade computation for memory.

\subsection{Gradient Accumulation and
Checkpointing}\label{sec-ai-training-gradient-accumulation-checkpointing-0c47}

Training large models requires substantial memory for storing
activations, gradients, and model parameters simultaneously. When GPU
memory constrains the batch size or model complexity, gradient
accumulation and activation checkpointing address these limitations by
trading computation for memory. These techniques leverage the efficiency
principles explored in \textbf{?@sec-introduction} and have become
indispensable for modern deep learning workflows.

\subsubsection{Gradient Accumulation and Checkpointing
Mechanics}\label{sec-ai-training-gradient-accumulation-checkpointing-mechanics-fb09}

Gradient accumulation and activation checkpointing operate on distinct
principles, but both aim to optimize memory usage during training by
modifying how forward and backward computations are handled.

\paragraph{Gradient
Accumulation}\label{sec-ai-training-gradient-accumulation-308f}

Gradient accumulation simulates larger batch sizes by splitting a single
effective batch into smaller ``micro-batches.''
Figure~\ref{fig-grad-accumulation} illustrates this process: three
independent batches (green, red, blue) each compute their own loss
(\(L_1\), \(L_2\), \(L_3\)) and gradients (\(\delta_1\), \(\delta_2\),
\(\delta_3\)), which then sum to produce the combined gradient
\(\delta_1+\delta_2+\delta_3\) used for a single parameter update. This
approach achieves the same gradient as training with a batch three times
larger, without requiring the memory to hold all samples simultaneously.

\begin{figure}[htb]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/285cc35be864269ec9410018995ea55d5014321d.pdf}}

}

\caption{\label{fig-grad-accumulation}\textbf{Gradient Accumulation}:
Three micro-batches each compute independent losses and gradients, which
sum into a single combined gradient for one parameter update. This
simulates training with a batch three times larger without requiring the
memory to hold all samples simultaneously.}

\end{figure}%

This process allows models to achieve the benefits of training with
larger batch sizes, such as improved gradient estimates and convergence
stability, without requiring the memory to store an entire batch at
once. For instance, in PyTorch, this can be implemented by adjusting the
learning rate proportionally to the number of accumulated micro-batches
and calling \texttt{optimizer.step()} only after processing the entire
effective batch.

The key steps in gradient accumulation are:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Perform the forward pass for a micro-batch.
\item
  Compute the gradients during the backward pass.
\item
  Accumulate the gradients into a buffer without updating the model
  parameters.
\item
  Repeat steps 1-3 for all micro-batches in the effective batch.
\item
  Update the model parameters using the accumulated gradients after all
  micro-batches are processed.
\end{enumerate}

\textbf{Mathematical Equivalence}: The key insight is that gradient
accumulation produces mathematically identical results to training with
larger batches. For an effective batch size \(B = k \times b\) where
\(k\) is the number of accumulation steps and \(b\) is the micro-batch
size, Equation~\ref{eq-gradient-accumulation-equivalence} confirms that
the accumulated gradient equals the true batch gradient:

\begin{equation}\phantomsection\label{eq-gradient-accumulation-equivalence}{
\nabla L_B = \frac{1}{B}\sum_{i=1}^{B} \nabla L_i = \frac{1}{k}\sum_{j=1}^{k}\left(\frac{1}{b}\sum_{i \in \text{batch}_j} \nabla L_i\right)
}\end{equation}

This equivalence holds because gradients are linear operators. The
right-hand side shows that averaging \(k\) micro-batch gradients (each
computed over \(b\) examples) produces the same result as computing the
gradient over all \(B = kb\) examples at once. The optimizer receives
identical update directions regardless of whether the batch is processed
in one pass or accumulated over multiple passes.

\textbf{Memory vs Computation Trade-off}: Gradient accumulation
exchanges memory capacity for computation time according to:

\begin{itemize}
\tightlist
\item
  \textbf{Memory}: \(O(b)\) instead of \(O(B)\), yielding a \(k\times\)
  reduction in activation memory
\item
  \textbf{Computation}: Unchanged total FLOPs, as all \(B\) examples are
  still processed
\item
  \textbf{Time}: \(k\) forward and backward passes execute before each
  optimizer step, introducing synchronization overhead
\end{itemize}

The time overhead per accumulation step is typically 2-5\%, arising from
the additional synchronization and gradient buffer management. For \(k\)
accumulation steps with micro-batch time \(T_{\text{micro}}\) and
synchronization overhead \(T_{\text{sync}}\),
Equation~\ref{eq-gradient-accumulation-overhead} gives the effective
time per update:

\begin{equation}\phantomsection\label{eq-gradient-accumulation-overhead}{
T_{\text{effective}} = k \times T_{\text{micro}} + (k-1) \times T_{\text{sync}}
}\end{equation}

In practice, this overhead is small compared to the memory savings.
Training BERT-Large with effective batch size 256 using 8 accumulation
steps of micro-batch 32 reduces activation memory by 8\(\times\) while
adding only 10--15\% to wall-clock time.

When gradient accumulation is combined with distributed data parallelism
across multiple machines, additional considerations arise for gradient
synchronization timing and effective batch size calculation across the
cluster. These distributed training patterns are explored in advanced
distributed systems texts.

\paragraph{Activation
Checkpointing}\label{sec-ai-training-activation-checkpointing-2ee1}

Activation checkpointing reduces memory usage during the backward pass
by discarding and selectively recomputing activations. In standard
training, activations from the forward pass are stored in memory for use
in gradient computations during backpropagation. However, these
activations can consume significant memory, particularly in deep
networks.

With checkpointing, only a subset of the activations is retained during
the forward pass. Figure~\ref{fig-activation-checkpointing} visualizes
this memory-compute tradeoff: during the forward pass (top row), only
checkpoint nodes (green, solid) are retained while intermediate nodes
(white, dashed) are discarded. During the backward pass (bottom row),
these discarded activations are recomputed on demand (brown nodes) from
the nearest checkpoint, trading approximately 33\% additional compute
for memory savings that can exceed 70\% in deep networks.

The implementation involves three steps. First, split the model into
segments. Second, retain activations only at the boundaries of these
segments during the forward pass. Third, recompute activations for
intermediate layers during the backward pass when needed.

\begin{figure}[htb]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/59800dc8885baf6654097e8baf00d7247f16d6b6.pdf}}

}

\caption{\label{fig-activation-checkpointing}\textbf{Activation
Checkpointing}: Trading memory usage for recomputation during
backpropagation enables training deeper neural networks. By storing only
a subset of activations from the forward pass and recomputing others on
demand, this technique reduces peak memory requirements at the cost of
increased training time.}

\end{figure}%

Frameworks like PyTorch provide tools such as
\texttt{torch.utils.checkpoint} to simplify this process. Checkpointing
is particularly effective for very deep architectures, such as
transformers or large convolutional networks, where the memory required
for storing activations can exceed the GPU's capacity.

The synergy between gradient accumulation and checkpointing enables
training of larger, more complex models. Gradient accumulation manages
memory constraints related to batch size, while checkpointing optimizes
memory usage for intermediate activations. Together, these techniques
expand the range of models that can be trained on available hardware.

\subsubsection{Optimal Checkpoint Placement
Strategy}\label{sec-ai-training-optimal-checkpoint-placement-strategy-4a0d}

For a network with L layers, each storing A bytes of activations,
Table~\ref{tbl-checkpoint-tradeoffs} quantifies how the number and
placement of checkpoints determines the memory-compute tradeoff.
\textbf{Note that sub-linear checkpointing strategies can reduce memory
consumption from \(O(L)\) to \(O(\sqrt{L})\) with only a fractional
increase in total compute time, enabling the training of much deeper
models on existing hardware.}

\begin{longtable}[]{@{}lll@{}}
\caption{\textbf{Checkpointing Memory-Compute Tradeoffs.} Different
checkpoint strategies trade memory savings against recomputation
overhead. The optimal number of checkpoints balances these
factors.}\label{tbl-checkpoint-tradeoffs}\tabularnewline
\toprule\noalign{}
\textbf{Strategy} & \textbf{Memory Cost} & \textbf{Recompute Cost} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\textbf{Strategy} & \textbf{Memory Cost} & \textbf{Recompute Cost} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{No checkpointing} & L x A & 0 forward ops \\
\textbf{Checkpoint every layer} & A & (L-1) forward ops \\
\textbf{k checkpoints} & k x A + (L/k) x A & (L-k) forward ops \\
\end{longtable}

\textbf{Optimal Checkpoint Interval}: Setting the derivative of total
memory cost (k x A + (L/k) x A) to zero yields k\_optimal = sqrt(L).
This minimizes total memory while bounding recomputation overhead to
approximately 33\% additional forward time.

\textbf{Example: GPT-2 (48 transformer layers)}:

Without checkpointing: Memory = 48 x A (full activation storage)

Optimal checkpointing (sqrt(48) approximately equals 7 checkpoints):
Memory = 7 x A + (48/7) x A approximately equals 14 x A. This achieves
71\% memory savings with approximately 33\% compute overhead.

\textbf{Selective Checkpointing Strategy}: Not all operations are
equally expensive to recompute. Attention layers with QKV projections
have high memory cost (3 x B x S x H) but also high recompute cost
(three matrix multiplications). Feed-forward layers have high memory
cost (2 x B x S x 4H) but lower recompute cost (two matrix
multiplications). LayerNorm has low memory cost and very low recompute
cost. A common practical strategy is to checkpoint before attention
layers (high memory per compute ratio), skip FFN checkpoints (often fast
to recompute), and avoid checkpointing normalization layers. In
representative transformer workloads, selective checkpointing can
achieve large memory savings (for example, on the order of 60 to 80\%)
with moderate compute overhead (for example, on the order of 20 to
25\%), often outperforming uniform checkpoint placement.

\subsubsection{Memory and Computational
Benefits}\label{sec-ai-training-memory-computational-benefits-9372}

Gradient accumulation\sidenote{\textbf{Gradient Accumulation Impact}:
Enables effective batch sizes of 2048+ on single GPUs with only 32-64
micro-batch size, essential for transformer training. BERT-Large
training uses effective batch size of 256 (accumulated over 8 steps)
achieving 99.5\% of full-batch performance while reducing memory
requirements by 8x. The technique trades 10-15\% compute overhead for
massive memory savings. } simulates larger batch sizes without
increasing memory requirements for storing the full batch. Larger batch
sizes improve gradient estimates, leading to more stable convergence and
faster training. This flexibility proves particularly valuable when
training on high-resolution data where even a single batch may exceed
available memory.

Activation checkpointing\sidenote{\textbf{Activation Checkpointing
Trade-offs}: Reduces memory usage by 50--90\% at the cost of 15--30\%
additional compute time due to recomputation. For training GPT-3 on
V100s, checkpointing enables 2.8\(\times\) larger models (from 1.3 B to
3.7 B parameters) within 32 GB memory constraints, making it essential
for memory-bound large model training despite the compute penalty. }
significantly reduces the memory footprint of intermediate activations
during the forward pass, allowing training of deeper models. By
discarding and recomputing activations as needed, checkpointing frees up
memory for larger models, additional layers, or higher resolution data.
This is especially important in advanced architectures like transformers
that require substantial memory for intermediate computations.

Both techniques enhance scalability and cost efficiency by reducing
hardware requirements, lowering development costs for organizations
working within tight budgets.

Returning to our GPT-2 lighthouse model, \emph{gradient accumulation} is
essential for achieving the target batch size within V100 memory
constraints.

\phantomsection\label{callout-notebookux2a-1.24}
\begin{fbx}{callout-notebook}{AI Engineer’s Notebook:}{GPT-2 Gradient Accumulation Strategy}
\phantomsection\label{callout-notebook*-1.24}
GPT-2's training configuration demonstrates the essential role of
gradient accumulation.

\textbf{Memory Constraints}

\begin{itemize}
\tightlist
\item
  V100 32GB GPU with gradient checkpointing: Can fit batch\_size=16 (as
  shown in activation memory example)
\item
  Desired effective batch\_size: 512 (optimal for transformer
  convergence)
\item
  Problem: 512 ÷ 16 = 32 GPUs needed just for batch size
\end{itemize}

\textbf{Gradient Accumulation Solution}

Instead of 32 GPUs, use 8 GPUs with gradient accumulation:

Configuration:

\begin{itemize}
\tightlist
\item
  Per-GPU micro-batch: 16
\item
  Accumulation steps: 4
\item
  Effective batch per GPU: 16 × 4 = 64
\item
  Global effective batch: 8 GPUs × 64 = \textbf{512} ✓
\end{itemize}

Listing~\ref{lst-gradient-accumulation-loop} shows the training loop
with gradient accumulation.

\textbf{Performance Impact}

Without Accumulation (naive approach):

\begin{itemize}
\tightlist
\item
  32 GPUs × batch\_size=16 = 512 effective batch
\item
  Gradient sync: 32 GPUs → high communication overhead
\item
  Cost: \$16/hour × 32 GPUs = \$512/hour
\end{itemize}

With Accumulation (actual GPT-2 approach):

\begin{itemize}
\tightlist
\item
  8 GPUs × (16 × 4 accumulation) = 512 effective batch
\item
  Gradient sync: Only every 4 steps, only 8 GPUs
\item
  Cost: \$16/hour × 8 GPUs = \$128/hour
\item
  Savings: \$384/hour = 75\% cost reduction
\end{itemize}

\textbf{Tradeoff Analysis}

\begin{itemize}
\tightlist
\item
  Compute overhead: 4\(\times\) forward passes per update =
  \textasciitilde8\% slower (pipeline overlaps some cost)
\item
  Memory overhead: Gradient accumulation buffer = negligible (gradients
  already needed)
\item
  Communication benefit: Sync frequency reduced by 4× → communication
  time drops by 75\%
\item
  Cost benefit: Training 2 weeks on 8 GPUs = \$43.0K vs.~32 GPUs =
  \$172.0K
\end{itemize}

\textbf{Convergence Quality}

\begin{itemize}
\tightlist
\item
  Effective batch 512 with accumulation: Perplexity 18.3
\item
  True batch 512 without accumulation: Perplexity 18.2
\item
  Difference: 0.5\% (within noise margin)
\end{itemize}

\textbf{Why This Works:} Gradient accumulation is mathematically
equivalent to larger batches because gradients are additive: \[
\nabla L_{\text{batch}} = \frac{1}{N}\sum_{i=1}^N \nabla L(x_i) = \frac{1}{4}\sum_{j=1}^4 \left[\frac{1}{16}\sum_{k=1}^{16} \nabla L(x_{jk})\right]
\]

\textbf{Key Insight:} For memory-bound models like GPT-2, gradient
accumulation + moderate GPU count is more cost-effective than scaling to
many GPUs with small batches.

\end{fbx}

\begin{codelisting}

\caption{\label{lst-gradient-accumulation-loop}\textbf{Gradient
Accumulation Training Loop}: Accumulates gradients over multiple
micro-batches before synchronization, reducing communication overhead.}

\centering{

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{optimizer.zero\_grad()}
\ControlFlowTok{for}\NormalTok{ step }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\DecValTok{4}\NormalTok{):  }\CommentTok{\# Accumulation steps}
\NormalTok{    micro\_batch }\OperatorTok{=} \BuiltInTok{next}\NormalTok{(dataloader)  }\CommentTok{\# 16 samples}
\NormalTok{    loss }\OperatorTok{=}\NormalTok{ model(micro\_batch) }\OperatorTok{/} \DecValTok{4}  \CommentTok{\# Scale loss}
\NormalTok{    loss.backward()  }\CommentTok{\# Accumulate gradients}
\CommentTok{\# Now gradients represent 64 samples}
\NormalTok{all\_reduce(gradients)  }\CommentTok{\# Sync across 8 GPUs}
\NormalTok{optimizer.step()  }\CommentTok{\# Update with effective batch=512}
\end{Highlighting}
\end{Shaded}

}

\end{codelisting}%

\subsubsection{Practical
Considerations}\label{sec-ai-training-gradient-accumulation-practical-considerations-a5a4}

Gradient accumulation is most valuable when optimal batch sizes exceed
GPU memory capacity. Transformer
architectures\sidenote{\textbf{Transformer Batch Size Scaling}: Research
shows transformers achieve optimal performance with batch sizes of
256-4096 tokens, requiring gradient accumulation on most hardware. GPT-2
training improved perplexity by 0.3-0.5 points when increasing from
batch size 32 to 512, demonstrating the critical importance of large
effective batch sizes for language model convergence. } typically
converge best with batch sizes of 256-4096 tokens, far beyond what a
single GPU can hold. Accumulation bridges this gap without requiring
additional hardware. Activation checkpointing complements this by
enabling deeper architectures: models like GPT-3 and T5 rely on
checkpointing to fit within single-GPU memory, as do dual-network
configurations such as GANs.

Both techniques introduce explicit trade-offs. Activation checkpointing
adds approximately 33\% compute overhead from recomputation; in a
12-layer transformer with checkpoints every 4 layers, each intermediate
activation is recomputed up to three times during the backward pass.
Gradient accumulation reduces parameter update frequency, requiring
learning rate scaling proportional to the accumulation factor to
maintain convergence dynamics. When accumulating over 4 micro-batches to
simulate batch size 128, the learning rate must be scaled by \(4\times\)
to match the effective step size of a true batch. For models that do not
require large batch sizes or have shallow architectures with modest
activation memory, the added implementation complexity may not be
justified.

\subsection{Optimization Technique
Comparison}\label{sec-ai-training-optimization-technique-comparison-a89a}

Table~\ref{tbl-optimization} synthesizes the three core optimization
strategies, contrasting their primary goals, mechanisms, and trade-offs.
The comparison reveals that prefetching improves GPU utilization through
parallelism but increases memory overhead, mixed-precision accelerates
computation via FP16 but requires careful loss scaling, and gradient
accumulation enables larger effective batches but slows parameter
updates. Selecting an appropriate strategy depends on the specific
bottleneck identified through profiling.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2500}}@{}}
\caption{\textbf{Optimization Strategies.} Prefetching, mixed-precision
training, and gradient accumulation address distinct bottlenecks in AI
training pipelines: data transfer, memory consumption, and
backpropagation. Selecting an appropriate strategy balances
implementation complexity against gains in speed and resource
utilization, depending on hardware and workload
characteristics.}\label{tbl-optimization}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Aspect}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Prefetching and Overlapping}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Mixed-Precision Training}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Gradient Accumulation and Checkpointing}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Aspect}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Prefetching and Overlapping}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Mixed-Precision Training}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Gradient Accumulation and Checkpointing}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Primary Goal} & Minimize data transfer delays and maximize GPU
utilization & Reduce memory consumption and computational overhead &
Overcome memory limitations during backpropagation and parameter
updates \\
\textbf{Key Mechanism} & Asynchronous data loading and parallel
processing & Combining FP16 and FP32 computations & Simulating larger
batch sizes and selective activation storage \\
\textbf{Memory Impact} & Increases memory usage for prefetch buffer &
Reduces memory usage by using FP16 & Reduces memory usage for
activations and gradients \\
\textbf{Computation Speed} & Improves by reducing idle time &
Accelerates computations using FP16 & May slow down due to
recomputations in checkpointing \\
\textbf{Scalability} & Highly scalable, especially for large datasets &
Enables training of larger models & Allows training deeper models on
limited hardware \\
\textbf{Hardware Requirements} & Benefits from fast storage and
multi-core CPUs & Requires GPUs with FP16 support (e.g., Tensor Cores) &
Works on standard hardware \\
\textbf{Implementation Complexity} & Moderate (requires tuning of
prefetch parameters) & Low to moderate (with framework support) &
Moderate (requires careful segmentation and accumulation) \\
\textbf{Main Benefits} & Reduces training time, improves hardware
utilization & Faster training, larger models, reduced memory usage &
Enables larger batch sizes and deeper models \\
\textbf{Primary Challenges} & Tuning buffer sizes, increased memory
usage & Potential numerical instability, loss scaling needed & Increased
computational overhead, slower parameter updates \\
\textbf{Ideal Use Cases} & Large datasets, complex preprocessing &
Large-scale models, especially in NLP and computer vision & Very deep
networks, memory-constrained environments \\
\end{longtable}

These three techniques---prefetching, mixed precision, and gradient
accumulation---form the core optimization toolkit for single-machine
training. Applied systematically using the profiling methodology
established earlier, they can dramatically extend the capabilities of a
single device.

\subsection{Putting It All Together: GPT-2 Optimization
Walkthrough}\label{sec-ai-training-putting-together-gpt2-optimization-walkthrough-def7}

To demonstrate how these techniques compose in practice, let us walk
through optimizing GPT-2 (1.5B parameters) training on a single 32 GiB
V100 GPU.

\phantomsection\label{callout-exampleux2a-1.25}
\begin{fbx}{callout-example}{Example:}{GPT-2 Optimization on V100}
\phantomsection\label{callout-example*-1.25}
\textbf{Initial Configuration} (Naive Implementation):

\begin{itemize}
\tightlist
\item
  Model: GPT-2 XL (1.5B parameters)
\item
  Batch size: 32, Sequence length: 1024
\item
  Precision: FP32 throughout
\item
  Data loading: Single-threaded, synchronous
\end{itemize}

\subsubsection*{Step 1: Profile the
Baseline}\label{step-1-profile-the-baseline}
\addcontentsline{toc}{subsubsection}{Step 1: Profile the Baseline}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Memory breakdown:}
\NormalTok{  Parameters (FP32):      6.0 GiB}
\NormalTok{  Gradients (FP32):       6.0 GiB}
\NormalTok{  Optimizer (Adam):      12.0 GiB}
\NormalTok{  Activations:           65.0 GiB}
\NormalTok{  ─────────────────────────────────}
\NormalTok{  Total:                 89.0 GiB  ← OOM on 32 GiB GPU}
\end{Highlighting}
\end{Shaded}

\textbf{Bottleneck identified}: Memory exhaustion. Cannot even begin
training.

\subsubsection*{Step 2: Apply Mixed Precision
Training}\label{step-2-apply-mixed-precision-training}
\addcontentsline{toc}{subsubsection}{Step 2: Apply Mixed Precision
Training}

Enable AMP (Automatic Mixed Precision) with FP16 forward/backward, FP32
master weights:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Memory breakdown:}
\NormalTok{  Parameters (FP16):      3.0 GiB}
\NormalTok{  Gradients (FP16):       3.0 GiB}
\NormalTok{  Master Weights (FP32):  6.0 GiB}
\NormalTok{  Optimizer (Adam):      12.0 GiB}
\NormalTok{  Activations (FP16):    32.5 GiB}
\NormalTok{  Total:                 56.5 GiB  ← Still OOM}
\end{Highlighting}
\end{Shaded}

\textbf{Improvement}: 36\% memory reduction, but still exceeds 32 GiB.

\subsubsection*{Step 3: Apply Gradient
Checkpointing}\label{step-3-apply-gradient-checkpointing}
\addcontentsline{toc}{subsubsection}{Step 3: Apply Gradient
Checkpointing}

Checkpoint every 4 transformer layers, recompute activations during
backward pass:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Memory breakdown:}
\NormalTok{  Static Memory (AMP):   24.0 GiB}
\NormalTok{  Activations (ckpt):     8.0 GiB  ← 4× reduction}
\NormalTok{  Total:                 32.0 GiB  ✓ Fits in 32 GiB!}
\end{Highlighting}
\end{Shaded}

\textbf{Trade-off}: 33\% more compute (recomputation), but now fits in
memory.

\subsubsection*{Step 4: Profile for Throughput
Bottlenecks}\label{step-4-profile-for-throughput-bottlenecks}
\addcontentsline{toc}{subsubsection}{Step 4: Profile for Throughput
Bottlenecks}

With memory solved, profile shows:

\begin{itemize}
\tightlist
\item
  GPU utilization: 45\%
\item
  Data loading: 40\% of iteration time
\item
  Compute: 35\% of iteration time
\item
  Memory transfers: 25\% of iteration time
\end{itemize}

\textbf{Bottleneck identified}: Data-bound. GPU starving for data.

\subsubsection*{Step 5: Apply Prefetching and Data Pipeline
Optimization}\label{step-5-apply-prefetching-and-data-pipeline-optimization}
\addcontentsline{toc}{subsubsection}{Step 5: Apply Prefetching and Data
Pipeline Optimization}

Configure DataLoader with 8 workers, pin\_memory=True,
prefetch\_factor=2:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{After optimization:}

\NormalTok{{-} GPU utilization: 85\%  ← +40 percentage points}
\NormalTok{{-} Data loading: 5\% of iteration time (overlapped)}
\NormalTok{{-} Compute: 75\% of iteration time}
\NormalTok{{-} Memory transfers: 20\% of iteration time}
\end{Highlighting}
\end{Shaded}

\subsubsection*{Step 6: Final Profile and
Results}\label{step-6-final-profile-and-results}
\addcontentsline{toc}{subsubsection}{Step 6: Final Profile and Results}

\begin{longtable}[]{@{}llrl@{}}
\toprule\noalign{}
\textbf{Metric} & \textbf{Naive} & \textbf{Optimized} &
\textbf{Improvement} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Memory} & 101 GB & 26 GB & 3.9× reduction \\
\textbf{GPU utilization} & N/A & 85\% & Trainable \\
\textbf{Throughput} & N/A & 1,200 tokens/sec & --- \\
\textbf{Time per epoch} & N/A & 8.3 hours & --- \\
\end{longtable}

\textbf{Remaining bottleneck}: Compute-bound (as desired). The 85\%
utilization indicates good efficiency; remaining 15\% is overhead from
gradient synchronization, loss scaling, and kernel launch latency.

\end{fbx}

This walkthrough demonstrates three key principles:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Profile before optimizing}: Each optimization targeted a
  specific bottleneck revealed by profiling
\item
  \textbf{Techniques compose}: Mixed precision alone wasn't enough;
  combining it with checkpointing and prefetching achieved the goal
\item
  \textbf{Trade-offs are explicit}: We accepted 33\% more compute
  (checkpointing) to gain 4× memory reduction
\end{enumerate}

The systematic framework---profile, identify bottleneck, apply targeted
technique, re-profile---transforms optimization from trial-and-error
into engineering practice.

\subsection{Optimization Impact
Summary}\label{sec-ai-training-optimization-impact-summary-0213}

The GPT-2 case study demonstrates how the optimization techniques
examined in this section combine to transform infeasible training
requirements into practical configurations. Table~\ref{tbl-gpt2-summary}
quantifies the cumulative impact across memory, time, energy, and cost
dimensions:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2500}}@{}}
\caption{\textbf{GPT-2 Training Optimization Summary.} Applying
mixed-precision training and gradient checkpointing reduces memory from
89 GiB to 32 GiB, training time by 40\%, energy consumption by 58\%, and
carbon footprint proportionally.}\label{tbl-gpt2-summary}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Metric}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{FP32 Baseline}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Optimized}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Technique Applied}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Metric}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{FP32 Baseline}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Optimized}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Technique Applied}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Parameters} & 6.0 GiB & 3.0 GiB & Mixed precision (FP16) \\
\textbf{Gradients} & 6.0 GiB & 3.0 GiB & Mixed precision (FP16) \\
\textbf{Master Weights} & 0.0 GiB & 6.0 GiB & AMP Overhead \\
\textbf{Optimizer State (Adam)} & 12.0 GiB & 12.0 GiB & Unchanged (FP32
moments) \\
\textbf{Activations (batch=32)} & 65.0 GiB & 8.0 GiB & Gradient
checkpointing + FP16 \\
\textbf{Total Memory} & \textbf{89.0 GiB} & \textbf{32.0 GiB} & --- \\
\textbf{Training Time (32 V100s)} & 14 days & 8.4 days & 2.4× Tensor
Core speedup \\
\textbf{Energy Consumption} & 275,000 kWh & 115,000 kWh & Reduced time +
improved efficiency \\
\textbf{Electricity Cost (@\$0.10/kWh)} & \$27,500 & \$11,500 & --- \\
\textbf{Carbon Footprint} & \textasciitilde125 tons CO₂ &
\textasciitilde52 tons CO₂ & Regional grid average (0.45 kg/kWh) \\
\end{longtable}

This 3.9x memory reduction, combined with 1.7x computational speedup and
58\% energy reduction, exemplifies how systematic optimization
transforms hardware constraints into engineering design parameters. The
same optimizations that improve throughput also reduce energy
consumption and operational cost.

We have now exhausted the single-machine optimization toolkit. Mixed
precision extracts maximum throughput from Tensor Cores. Flash Attention
reduces bandwidth consumption to near-theoretical minimums. Gradient
checkpointing trades compute for memory at the most favorable ratios
possible. Prefetching hides data loading latency. When all these
techniques are applied and the training still takes too long or the
model still does not fit, a different approach becomes necessary:
spreading the computation across multiple devices.

\subsection{Scaling Training
Systems}\label{sec-ai-training-scaling-training-systems-adfd}

The optimization techniques examined throughout this chapter extend
single-device training capabilities substantially, but they cannot
overcome fundamental hardware limits. A single GPU has finite memory
capacity, finite compute throughput, and finite memory bandwidth. When
model size exceeds device memory even after gradient checkpointing, or
when training duration remains unacceptable even at peak utilization,
multi-accelerator training becomes necessary. This section examines when
and how to scale beyond single-device training, from multi-GPU
configurations within a single machine (the ML node scope of this
volume) to the threshold where distributed systems across multiple
machines become essential. The distributed training techniques that
operate across network boundaries are covered in detail in Volume II.

Scaling to multiple devices also amplifies the energy consumption and
environmental impact of training. The \emph{carbon footprint of
training} grows proportionally with cluster size, making efficiency
optimization not just a performance concern but an environmental one.

This scaling leads to a massive \emph{carbon footprint of training}.

\phantomsection\label{callout-notebookux2a-1.26}
\begin{fbx}{callout-notebook}{AI Engineer’s Notebook:}{The Carbon Footprint of Training}
\phantomsection\label{callout-notebook*-1.26}

\textbf{Scaling the Utility Bill}: Training large models is not just a
compute challenge; it's a massive energy sink. We can quantify the
environmental impact of scaling training using the \textbf{Energy
Corollary} to the Iron Law:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Workload}: Training a 7B parameter model for 1 trillion
  tokens.
\item
  \textbf{Compute}: \(\approx 4\.2 \times 10^{22}\) FLOPs.
\item
  \textbf{Efficiency}: 150 TFLOPS sustained on A100 (400W TDP).
\item
  \textbf{Time}: \(\approx 3\.2\) days on 1024 GPUs.
\item
  \textbf{Energy}:
  \((1024 \text{ GPUs} \times 400\text{W} + 128 \text{ hosts} \times 200\text{W}) \times 76 \text{ hours} \approx \mathbf{33\,056 \text{ kWh}}\)
\end{enumerate}

\textbf{The Systems Conclusion}: This single training run consumes as
much electricity as an average US household uses in \textbf{37 months}.

\begin{itemize}
\tightlist
\item
  \textbf{The Optimization Dividend}: Improving \textbf{Utilization}
  from 30\% to 60\% doesn't just halve the time; it saves
  \textasciitilde16,528 kWh of energy and reduces the carbon footprint
  by over \textbf{6.6 tons of CO2} (assuming average grid intensity).
\item
  \textbf{The True Cost}: Training systems engineering is the primary
  lever for sustainable AI. Every 1\% gain in efficiency at scale is
  equivalent to taking dozens of cars off the road for a year.
\end{itemize}

\end{fbx}

\subsection{The Evolution of Training
Infrastructure}\label{sec-ai-training-evolution-training-infrastructure-f3a6}

Computing system architectures have evolved through distinct
generations, each building upon previous advances while introducing
specialized optimizations for emerging application requirements
(Figure~\ref{fig-evolution-systems}). This progression demonstrates how
hardware adaptation to application needs shapes modern machine learning
systems.

\begin{figure}[htb]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/158cd70ddd2045d921a218db07bdfacb1ea85cbe.pdf}}

}

\caption{\label{fig-evolution-systems}\textbf{Computing System
Evolution}: Hardware advancements continuously adapted to the increasing
demands of machine learning workloads, transitioning from centralized
mainframes to specialized architectures optimized for parallel
processing and massive datasets.}

\end{figure}%

This architectural progression illuminates why traditional computing
systems proved insufficient for neural network training. As shown in
Table~\ref{tbl-computing-eras}, while HPC systems provided the
foundation for parallel numerical computation and warehouse-scale
systems demonstrated distributed processing at scale, neither fully
addressed the computational patterns of model training. Modern neural
networks combine intensive parameter updates, complex memory access
patterns, and coordinated distributed computation in ways that demanded
new architectural approaches.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2500}}@{}}
\caption{\textbf{Computing Era Characteristics.} Each computing era
optimized for different workload patterns. AI hypercomputing uniquely
combines HPC's parallel numerical computation with warehouse-scale's
distributed processing, while adding specialized support for
gradient-based optimization central to neural network
training.}\label{tbl-computing-eras}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Era}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Primary Workload}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Memory Patterns}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Processing Model}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Era}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Primary Workload}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Memory Patterns}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Processing Model}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Mainframe} & Sequential batch processing & Simple memory
hierarchy & Single instruction stream \\
\textbf{HPC} & Scientific simulation & Regular array access &
Synchronized parallel \\
\textbf{Warehouse-scale} & Internet services & Sparse, irregular access
& Independent parallel tasks \\
\textbf{AI Hypercomputing} & Neural network training & Parameter-heavy,
mixed access & Hybrid parallel, distributed \\
\end{longtable}

\subsection{Single-Node Multi-GPU
Training}\label{sec-ai-training-singlenode-multigpu-training-c87f}

Multi-GPU training within a single node, the scope of this volume,
predates large-scale distributed systems.
AlexNet\sidenote{\textbf{AlexNet}: Developed by Alex Krizhevsky, Ilya
Sutskever, and Geoffrey Hinton, AlexNet won ImageNet 2012 with 15.3\%
error rate (vs.~26.2\% for second place), using two GTX 580 GPUs for 5-6
days of training. The model was split across GPUs with cross-GPU
communication only at certain layers---an early form of model
parallelism that launched the deep learning revolution. } (2012)
famously split its model across two GTX 580 GPUs---not because the model
was too large, but because the 3GB memory per GPU couldn't hold both the
model and the batch activations. This single-node, multi-GPU
configuration remains common today and introduces the fundamental
parallelism strategies without the complexity of network communication.

\textbf{Data Parallelism} replicates the entire model on each GPU, with
each processing different batches. After computing gradients locally,
GPUs synchronize via gradient averaging.
Figure~\ref{fig-train-data-parallelism} illustrates this process: input
data splits into non-overlapping batches, each GPU computes forward and
backward passes independently, then gradients aggregate before updating
the shared model.

\begin{figure}[htb]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/11b57dfc245f7c276a23a8a41ac693b64c4e8156.pdf}}

}

\caption{\label{fig-train-data-parallelism}\textbf{Data Parallelism}:
Each GPU holds a complete model copy, processes different data batches,
then synchronizes gradients. This approach scales training throughput
linearly with GPU count when models fit in single-GPU memory.}

\end{figure}%

\textbf{Model Parallelism} partitions the model itself across GPUs,
which becomes necessary when the model exceeds single-GPU memory.
AlexNet used a simple form: certain layers resided on GPU 1, others on
GPU 2, with activations passing between them.
Figure~\ref{fig-model-parallelism} shows this sequential flow: data
moves through model partitions on different devices, with gradients
flowing backward during training.

\begin{figure}[htb]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/e58a6d07ac3ddf2386651f902698d124f674d6a6.pdf}}

}

\caption{\label{fig-model-parallelism}\textbf{Model Parallelism}: The
model is partitioned across devices, with intermediate activations
passing between them. This enables training models larger than
single-GPU memory at the cost of sequential dependencies.}

\end{figure}%

In practice, model parallelism typically partitions by layers.
Figure~\ref{fig-layers-blocks} shows how a 24-layer transformer might be
distributed: Device 1 handles blocks 1--6, Device 2 handles blocks
7--12, and so forth. This layer-wise partitioning minimizes cross-device
communication to the boundaries between partitions.

\begin{figure}[htb]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/53219208c6b2131741c7c709315a41744655363f.pdf}}

}

\caption{\label{fig-layers-blocks}\textbf{Layer-wise Partitioning}: A
24-layer transformer distributed across four devices, with each device
responsible for six consecutive transformer blocks. Communication occurs
only at partition boundaries.}

\end{figure}%

Within a single node, GPUs communicate via high-bandwidth interconnects
like NVLink\sidenote{\textbf{NVLink}: NVIDIA's high-bandwidth GPU
interconnect, introduced in 2016 with Pascal architecture. NVLink
provides 50-900 GB/s bidirectional bandwidth (depending on generation),
compared to 16-64 GB/s for PCIe. For training, this 10-50x bandwidth
advantage enables efficient gradient synchronization and model
parallelism within a node. A DGX H100 system uses NVLink to achieve 900
GB/s between any pair of 8 GPUs, making intra-node communication nearly
as fast as local memory access. } (up to 900 GB/s on modern systems),
making gradient synchronization and activation transfers fast. This
intra-node parallelism forms the building block for larger distributed
systems.

\phantomsection\label{callout-notebookux2a-1.27}
\begin{fbx}{callout-notebook}{AI Engineer’s Notebook:}{Data vs. Model Parallelism}
\phantomsection\label{callout-notebook*-1.27}

\textbf{The Physics of Splitting}: How do you split a model that is too
big or too slow?

\textbf{Scenario}: Training a model with parameters \(P\) and batch size
\(B\) across \(N\) GPUs.

\textbf{1. Data Parallelism (Split the Batch)}

\begin{itemize}
\tightlist
\item
  \textbf{Compute (\(O\))}: Split by \(N\) (Each GPU does \(1/N\) of the
  batch).
\item
  \textbf{Memory (\(D\))}: \textbf{Replicated}. Every GPU must hold the
  full model weights \(P\).
\item
  \textbf{Communication}: \textbf{Gradients}. Size \(\propto P\). Occurs
  at end of backward pass.
\item
  \textbf{Bottleneck}: When Model Size \(P >\) GPU Memory.
\end{itemize}

\textbf{2. Model Parallelism (Split the Weights)}

\begin{itemize}
\tightlist
\item
  \textbf{Compute (\(O\))}: Split by \(N\) (Each GPU computes part of
  the layer).
\item
  \textbf{Memory (\(D\))}: \textbf{Split}. Each GPU holds \(P/N\)
  weights.
\item
  \textbf{Communication}: \textbf{Activations}. Size
  \(\propto B \times \text{Width}\). Occurs at every layer boundary.
\item
  \textbf{Bottleneck}: When Activation Size is large (high communication
  frequency).
\end{itemize}

\textbf{The Systems Conclusion}:

\begin{itemize}
\tightlist
\item
  Use \textbf{Data Parallel} when the model fits in memory but training
  is too slow.
\item
  Use \textbf{Model Parallel} when the model is too big to fit in a
  single GPU's memory.
\end{itemize}

\end{fbx}

\subsection{Scaling Beyond a Single
Node}\label{sec-ai-training-scaling-beyond-single-node-a671}

When single-node multi-GPU training remains insufficient, distributed
training extends across multiple machines. This introduces network
communication bottlenecks (typically 10-100 Gbps between nodes vs.~900
GB/s within a node) and fault tolerance requirements absent from
single-node setups. Understanding \emph{the physics of synchronization}
explains why this bandwidth gap is so consequential.

\phantomsection\label{callout-perspectiveux2a-1.28}
\begin{fbx}{callout-perspective}{Systems Perspective:}{The Physics of Synchronization}
\phantomsection\label{callout-perspective*-1.28}
Recall the \textbf{Energy-Movement Invariant} from
\textbf{?@sec-data-engineering-ml}: moving data is 100--1,000× more
expensive than computing on it. In distributed training, this physical
law manifests as the \textbf{Communication Tax}.

When you synchronize gradients across a fleet of GPUs, you are moving
megabytes of data across a network or PCIe bus for every few
milliseconds of computation. If the energy required for communication
(\(E_{net}\)) exceeds the energy for computation (\(E_{compute}\)), your
system efficiency (\(\eta\)) collapses. This is why techniques like
\textbf{Mixed Precision}
(Section~\ref{sec-ai-training-mixedprecision-training-9218}) and
\textbf{Gradient Compression} are essential: they aren't just
``speedups''; they are fundamental tools for managing the physical
limits of distributed scaling.

\end{fbx}

Three additional strategies emerge:

\begin{itemize}
\tightlist
\item
  \textbf{Pipeline parallelism}: Combines model partitioning with
  microbatching to reduce device idle time
\item
  \textbf{Tensor parallelism}: Splits individual operations (like large
  matrix multiplications) across devices
\item
  \textbf{Hybrid strategies}: Production systems combine approaches; for
  example, tensor parallelism within nodes and data parallelism across
  nodes
\end{itemize}

The implementation details---gradient synchronization algorithms
(AllReduce\sidenote{\textbf{AllReduce}: A collective communication
primitive that aggregates data across all participating devices and
distributes the result back to each. For gradient synchronization,
AllReduce sums gradients from all GPUs so each has the identical
averaged gradient. Ring AllReduce
(\citeproc{ref-patarasuk2009bandwidth}{Patarasuk and Yuan 2009}),
popularized by Baidu in 2017, achieves bandwidth-optimal performance by
passing data in a ring topology, requiring only 2(N-1)/N of the data
volume (approaching 2x for large N) regardless of participant count,
making it the standard for data-parallel training. }, ring-reduce),
communication patterns (parameter server, peer-to-peer), fault tolerance
mechanisms, and scaling efficiency analysis for training runs spanning
thousands of GPUs---are covered in specialized documentation on
distributed training systems.

\subsection{Decision Framework: Single-Machine
vs.~Distributed}\label{sec-ai-training-decision-framework-singlemachine-vs-distributed-2045}

Before accepting the complexity of distributed training, practitioners
should systematically exhaust single-machine optimizations:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Apply mixed-precision training}
  (Section~\ref{sec-ai-training-mixedprecision-training-9218}) to reduce
  memory by \textasciitilde50\%
\item
  \textbf{Use gradient accumulation}
  (Section~\ref{sec-ai-training-gradient-accumulation-checkpointing-0c47})
  to simulate larger batch sizes
\item
  \textbf{Implement activation checkpointing}
  (Section~\ref{sec-ai-training-activation-checkpointing-2ee1}) to trade
  compute for memory
\item
  \textbf{Optimize data pipelines}
  (Section~\ref{sec-ai-training-data-prefetching-pipeline-overlapping-e984})
  to eliminate I/O bottlenecks
\end{enumerate}

Table~\ref{tbl-scaling-decision} provides quantitative guidance for
scaling decisions across different model and data scales.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3333}}@{}}
\caption{\textbf{Scaling Decision Guidelines.} Model size, dataset
scale, and available hardware determine when distributed training
complexity is justified. Single-machine optimization provides better
cost-efficiency below these
thresholds.}\label{tbl-scaling-decision}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Scale}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Typical Approach}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Rationale}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Scale}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Typical Approach}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Rationale}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{\textless1B params, \textless100GB} & Single GPU & All
optimizations fit; fastest iteration \\
\textbf{1-10B params, \textless1TB} & Single node (1-8 GPUs) & Model
parallelism within node avoids network \\
\textbf{10B+ params} & Multi-node cluster & Memory requirements exceed
single-node capacity \\
\textbf{\textgreater10TB dataset} & Multi-node + streaming & I/O
bandwidth requires distributed storage \\
\end{longtable}

Only when profiling reveals persistent bottlenecks despite these
optimizations should distributed approaches be considered. The
transition involves substantial complexity in infrastructure, debugging,
and operations that must be justified by genuine scaling requirements.

\subsection{The Physical Ceiling: When to
Scale}\label{sec-ai-training-physical-ceiling}

The optimizations examined in this chapter---mixed precision,
prefetching, and gradient accumulation---maximize the efficiency of a
single GPU. However, every hardware device has a \textbf{Physical
Ceiling}. For models like Llama-3 or GPT-4, even a fully optimized H100
GPU would take decades to complete training.

You must transition to \textbf{Distributed Training} when:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Memory Exhaustion}: The model weights, gradients, and
  optimizer states exceed the VRAM of a single GPU, even with 4-bit
  quantization.
\item
  \textbf{Training Wall-Clock Time}: The estimated time to convergence
  on a single device exceeds the project's timeline (typically
  \textgreater{} 2 weeks).
\item
  \textbf{Dataset Scale}: The time required to stream the dataset from
  storage to a single node creates an insurmountable IO bottleneck.
\end{enumerate}

Advanced systems engineering explores the distributed engineering
required to coordinate thousands of these optimized pipelines across
high-speed interconnects.

\section{Fallacies and
Pitfalls}\label{sec-ai-training-fallacies-pitfalls-cf7d}

Training involves counterintuitive resource trade-offs and scaling
behavior that defy intuitions from traditional software systems. The
following fallacies and pitfalls capture errors that waste compute
resources, delay research progress, and cause production training
failures.

\paragraph*{\texorpdfstring{Fallacy: \emph{Larger models always yield
better
performance.}}{Fallacy: Larger models always yield better performance.}}\label{fallacy-larger-models-always-yield-better-performance.}
\addcontentsline{toc}{paragraph}{Fallacy: \emph{Larger models always
yield better performance.}}

Engineers assume model scaling guarantees accuracy gains. In production,
scaling without sufficient data causes severe overfitting. As
established in
Section~\ref{sec-ai-training-mathematical-foundations-d894}, model
capacity must match dataset size. A 20B parameter model requires
approximately 120 GB memory (40 GB parameters FP16 + 80 GB optimizer
states) but delivers worse accuracy than a 7B model when trained on
datasets under 100M examples. Beyond critical thresholds, doubling model
size while holding data constant typically degrades validation accuracy
by 5 to 10 percent due to overfitting. Teams that pursue scale without
data budgets waste months of compute on models that underperform smaller
variants.

\paragraph*{\texorpdfstring{Pitfall: \emph{Assuming distributed training
automatically accelerates
development.}}{Pitfall: Assuming distributed training automatically accelerates development.}}\label{pitfall-assuming-distributed-training-automatically-accelerates-development.}
\addcontentsline{toc}{paragraph}{Pitfall: \emph{Assuming distributed
training automatically accelerates development.}}

Many practitioners add devices expecting proportional speedup.
Communication overhead destroys this assumption. Small models on 8 GPUs
with data parallelism spend 30 to 50 percent of time synchronizing
gradients, achieving only 4 to 6x speedup instead of 8x. As
Section~\ref{sec-ai-training-scaling-training-systems-adfd}
demonstrates, single-device training with optimized pipelines often
beats poorly configured distributed setups. A 7B model training on a
single A100 for 24 hours can outperform an 8-GPU cluster completing in 6
hours when synchronization overhead consumes theoretical speedup.
Organizations that reflexively distribute training burn budget on
infrastructure complexity without profiling whether data loading,
memory, or computation is the actual bottleneck.

\paragraph*{\texorpdfstring{Fallacy: \emph{Hyperparameters scale
linearly with model size and batch
size.}}{Fallacy: Hyperparameters scale linearly with model size and batch size.}}\label{fallacy-hyperparameters-scale-linearly-with-model-size-and-batch-size.}
\addcontentsline{toc}{paragraph}{Fallacy: \emph{Hyperparameters scale
linearly with model size and batch size.}}

This belief transfers learning rates from small experiments to
large-scale training without adjustment. Large batch training requires
the linear scaling rule: multiply learning rate by batch size ratio
(\citeproc{ref-goyal2017accurate}{Goyal et al. 2017}). Training
ResNet-50 with batch 512 uses learning rate 0.1; scaling to batch 4096
requires learning rate 0.8, not 0.1. Ignoring this relationship causes
training instability or divergence. As discussed in
Section~\ref{sec-ai-training-pipeline-optimizations-cd9d}, large-scale
training requires warmup schedules and adjusted momentum to maintain
convergence. Teams that apply small-scale hyperparameters to large
models experience training failures 3 to 5 days into multi-week runs,
wasting substantial compute budgets.

\paragraph*{\texorpdfstring{Pitfall: \emph{Treating mixed precision
training as a simple toggle without
validation.}}{Pitfall: Treating mixed precision training as a simple toggle without validation.}}\label{pitfall-treating-mixed-precision-training-as-a-simple-toggle-without-validation.}
\addcontentsline{toc}{paragraph}{Pitfall: \emph{Treating mixed precision
training as a simple toggle without validation.}}

Practitioners enable FP16 training expecting automatic 2x speedup and
memory savings. Numerical stability failures emerge unpredictably. As
shown in Section~\ref{sec-ai-training-pipeline-optimizations-cd9d},
mixed precision achieves 2.4x speedup on V100 Tensor Cores but requires
loss scaling to prevent gradient underflow. Models with large activation
magnitudes or small gradient values experience divergence when loss
scaling is misconfigured. A language model training for 48 hours can
diverge at step 10,000 due to accumulated numerical errors, forcing
restarts that waste days. Production training systems must validate
mixed precision convergence on representative workloads before deploying
at scale.

\paragraph*{\texorpdfstring{Pitfall: \emph{Optimizing memory and
computation
independently.}}{Pitfall: Optimizing memory and computation independently.}}\label{pitfall-optimizing-memory-and-computation-independently.}
\addcontentsline{toc}{paragraph}{Pitfall: \emph{Optimizing memory and
computation independently.}}

Engineers maximize batch size until GPU memory exhausts without
considering computational efficiency. As
Section~\ref{sec-ai-training-pipeline-optimizations-cd9d} establishes,
GPU utilization drops from 90 percent at batch 256 to 60-70 percent at
batch 16 due to insufficient parallelism. Conversely, gradient
accumulation simulates large batches within memory constraints by
accumulating gradients over multiple passes before updating. Training
ResNet-50 with gradient accumulation (effective batch 512, physical
batch 64) achieves 85 percent utilization versus 90 percent for native
batch 512, trading 5 percent efficiency for 8x memory reduction.
Organizations that tune these parameters independently miss this
trade-off, extending training time by 20 to 40 percent.

\paragraph*{\texorpdfstring{Pitfall: \emph{Neglecting data pipeline
optimization until GPU utilization
profiling.}}{Pitfall: Neglecting data pipeline optimization until GPU utilization profiling.}}\label{pitfall-neglecting-data-pipeline-optimization-until-gpu-utilization-profiling.}
\addcontentsline{toc}{paragraph}{Pitfall: \emph{Neglecting data pipeline
optimization until GPU utilization profiling.}}

Teams optimize model architecture and hyperparameters while data loading
creates 30 to 50 percent idle time. As illustrated in
Section~\ref{sec-ai-training-pipeline-optimizations-cd9d}, sequential
data fetching leaves GPUs waiting for I/O. Profiling reveals 40 percent
training time spent in data loading, yet computation receives
optimization attention first. Prefetching with pipeline parallelism
reduces wall-clock time by 40 percent (90 seconds to 55 seconds for two
epochs) by overlapping data loading with computation. Organizations that
defer data pipeline optimization waste weeks of researcher time on
models bottlenecked by preventable I/O stalls.

\section{Summary}\label{sec-ai-training-summary-2d06}

Training represents the computational heart of machine learning systems,
where mathematical algorithms, memory management strategies, and
hardware acceleration converge to transform data into capable models.
The seemingly simple concept of iterative parameter optimization
requires careful engineering solutions to handle the scale and
complexity of modern machine learning workloads. Forward and backward
propagation become orchestrations of matrix operations, memory
allocations, and gradient computations that must be balanced against
hardware constraints and performance requirements.

Single-machine training optimization demonstrates how computational
bottlenecks drive innovation rather than simply limiting capabilities.
Techniques like gradient accumulation, mixed precision training, and
activation checkpointing showcase how training systems can optimize
memory usage, computational throughput, and convergence stability
simultaneously. The interplay between these strategies reveals that
effective training system design requires deep understanding of both
algorithmic properties and hardware characteristics to achieve optimal
resource utilization. When single-machine limits are reached,
distributed approaches such as data parallelism and model parallelism
provide pathways to further scaling, though with increased system
complexity.

This co-design principle---where algorithms, software frameworks, and
hardware architectures evolve together---shapes modern training
infrastructure. Matrix operation patterns drove GPU Tensor Core
development, which frameworks exposed through mixed-precision APIs,
enabling algorithmic techniques like FP16 training that further
influenced next-generation hardware design. Understanding this feedback
loop between computational requirements and system capabilities enables
practitioners to make informed architectural decisions that leverage the
full potential of training systems.

The training optimizations explored throughout this chapter provide the
foundation for the model-level efficiency techniques and deployment
strategies examined in subsequent chapters. These systems principles
extend naturally from training infrastructure to production inference
systems, as the engineering insights gained from optimizing training
workflows inform the broader machine learning system lifecycle.

\phantomsection\label{callout-takeawaysux2a-1.29}
\begin{fbx}{callout-takeaways}{Takeaways:}{Key Takeaways}
\phantomsection\label{callout-takeaways*-1.29}

\begin{itemize}
\tightlist
\item
  \textbf{The Iron Law governs training}:
  \(T_{train} = \frac{O}{R_{peak} \times \eta}\). Every optimization
  affects one of these terms---identifying which term is affected is
  essential for effective optimization.
\item
  \textbf{Profiling precedes optimization}: The iterative loop is:
  profile → identify bottleneck → apply targeted fix → re-profile.
  Optimization without profiling typically wastes effort on
  non-bottlenecks.
\item
  \textbf{Mixed precision provides substantial performance gains}: FP16
  training with FP32 accumulation delivers approximately 2× throughput
  and 2× memory reduction with typically \textless1\% accuracy impact on
  most workloads.
\item
  \textbf{Gradient checkpointing trades compute for memory}: Recomputing
  activations during the backward pass enables training larger models
  (e.g., GPT-3 scales from 1.3B to 3.7B parameters on V100s) or achieves
  3--4× activation memory reduction. Essential when memory is the
  binding constraint.
\item
  \textbf{Single-machine optimizations should precede distributed
  training}: Distributed training adds communication overhead and
  complexity. A well-optimized single GPU often outperforms a
  poorly-optimized multi-GPU setup.
\item
  \textbf{Energy and cost scale linearly with training time}: The same
  optimizations that accelerate training also reduce carbon emissions
  and cloud costs. Efficiency improvements directly translate to reduced
  resource consumption.
\end{itemize}

\end{fbx}

\phantomsection\label{callout-chapter-connectionux2a-1.30}
\begin{fbxSimple}{callout-chapter-connection}{What’s Next:}{From Build to Optimize}
\phantomsection\label{callout-chapter-connection*-1.30}
We have built the power plant of modern AI: systems capable of training
models at the scale of GPT-2 and beyond. But these massive models are
often too heavy to fly in real-world environments like mobile phones or
embedded sensors. We turn next to \textbf{?@sec-data-selection}, where
we begin the optimization journey by maximizing the learning value of
every data sample before addressing the computational cost of the model
artifact itself.

\end{fbxSimple}

\FloatBarrier\clearpage

\setpartsummary{This part addresses the challenge of making ML systems efficient enough for real-world deployment. It explores techniques for model compression, hardware acceleration, and data selection to maximize learning per unit of computation.}

\addtocontents{toc}{\par\addvspace{12pt}\noindent\hfil\bfseries\color{crimson}Part~I~Optimization and Acceleration\color{black}\hfil\par\addvspace{6pt}}

\numberedpart{Optimization and Acceleration}

\haspartsummaryfalse

\phantomsection\label{refs}
\begin{CSLReferences}{1}{0}
\bibitem[\citeproctext]{ref-tensorflow_data_2015}
Abadi, Martín, Ashish Agarwal, Paul Barham, et al. 2015. {``TensorFlow:
Large-Scale Machine Learning on Heterogeneous Systems.''} Google Brain.

\bibitem[\citeproctext]{ref-brown2020language}
Brown, Tom B., Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan,
Prafulla Dhariwal, Arvind Neelakantan, et al. 2020. {``Language Models
Are Few-Shot Learners.''} Edited by Hugo Larochelle, Marc'Aurelio
Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin.
\emph{Advances in Neural Information Processing Systems} 33 (May):
1877--1901. \url{https://doi.org/10.48550/arxiv.2005.14165}.

\bibitem[\citeproctext]{ref-chen2015mxnet}
Chen, Tianqi, Mu Li, Yutian Li, Min Lin, Naiyan Wang, Minjie Wang,
Tianjun Xiao, Bing Xu, Chiyuan Zhang, and Zheng Zhang. 2015. {``MXNet: A
Flexible and Efficient Machine Learning Library for Heterogeneous
Distributed Systems.''} \emph{arXiv Preprint arXiv:1512.01274},
December. \url{http://arxiv.org/abs/1512.01274v1}.

\bibitem[\citeproctext]{ref-chen2016training}
Chen, Tianqi, Bing Xu, Chiyuan Zhang, and Carlos Guestrin. 2016.
{``Training Deep Nets with Sublinear Memory Cost.''} \emph{arXiv
Preprint arXiv:1604.06174}, April.
\url{http://arxiv.org/abs/1604.06174v2}.

\bibitem[\citeproctext]{ref-chetlur2014cudnn}
Chetlur, Sharan, Cliff Woolley, Philippe Vandermersch, Jonathan Cohen,
John Tran, Bryan Catanzaro, and Evan Shelhamer. 2014. {``cuDNN:
Efficient Primitives for Deep Learning.''} \emph{arXiv Preprint
arXiv:1410.0759}, October. \url{http://arxiv.org/abs/1410.0759v3}.

\bibitem[\citeproctext]{ref-dao2022flashattention}
Dao, Tri, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher Ré.
2022. {``FlashAttention: Fast and Memory-Efficient Exact Attention with
IO-Awareness.''} In \emph{Advances in Neural Information Processing
Systems 35 (NeurIPS 2022)}, 16344--59. Curran Associates,
Inc.\href{\%0A\%20\%20\%20\%20https://proceedings.neurips.cc/paper/_files/paper/2022/hash/67d57c32e20fd0a7a302cb81d36e40d5-Abstract-Conference.html\%0A\%20\%20}{https://proceedings.neurips.cc/paper\textbackslash\_files/paper/2022/hash/67d57c32e20fd0a7a302cb81d36e40d5-Abstract-Conference.html
}.

\bibitem[\citeproctext]{ref-dean2012large}
Dean, Jeffrey, Greg Corrado, Rajat Monga, Kai Chen 0010, Matthieu Devin,
Quoc V. Le, Mark Z. Mao, et al. 2012. {``Large Scale Distributed Deep
Networks.''} In \emph{Advances in Neural Information Processing Systems
25: 26th Annual Conference on Neural Information Processing Systems
2012. Proceedings of a Meeting Held December 3-6, 2012, Lake Tahoe,
Nevada, United States}, edited by Peter L. Bartlett, Fernando C. N.
Pereira, Christopher J. C. Burges, Léon Bottou, and Kilian Q.
Weinberger, 1232--40.
\url{https://proceedings.neurips.cc/paper/2012/hash/6aca97005c68f1206823815f66102863-Abstract.html}.

\bibitem[\citeproctext]{ref-Devlin2019}
Devlin, Jacob, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018.
{``BERT: Pre-Training of Deep Bidirectional Transformers for Language
Understanding,''} October, 4171--86.
\url{http://arxiv.org/abs/1810.04805v2}.

\bibitem[\citeproctext]{ref-dongarra1988extended}
Dongarra, Jack J., Jeremy Du Croz, Sven Hammarling, and Richard J.
Hanson. 1988. {``An Extended Set of FORTRAN Basic Linear Algebra
Subprograms.''} \emph{ACM Transactions on Mathematical Software} 14 (1):
1--17. \url{https://doi.org/10.1145/42288.42291}.

\bibitem[\citeproctext]{ref-goyal2017accurate}
Goyal, Priya, Piotr Dollár, Ross Girshick, Pieter Noordhuis, Lukasz
Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and Kaiming He.
2017. {``Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour.''}
\emph{CoRR} abs/1706.02677 (June).
\url{http://arxiv.org/abs/1706.02677v2}.

\bibitem[\citeproctext]{ref-he2016residual}
He, Kaiming, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. {``Deep
Residual Learning for Image Recognition.''} In \emph{2016 IEEE
Conference on Computer Vision and Pattern Recognition (CVPR)}, 770--78.
IEEE. \url{https://doi.org/10.1109/cvpr.2016.90}.

\bibitem[\citeproctext]{ref-hendrycks2016gaussian}
Hendrycks, Dan, and Kevin Gimpel. 2016. {``Gaussian Error Linear Units
(GELUs).''} \emph{arXiv Preprint arXiv:1606.08415}, June.
\url{http://arxiv.org/abs/1606.08415v5}.

\bibitem[\citeproctext]{ref-nvidia_tensors_fp16_2017}
Jia, Xianyan, Shutao Song, Wei He, Yangzihao Wang, Haidong Rong, Feihu
Zhou, Liqiang Xie, et al. 2018. {``Highly Scalable Deep Learning
Training System with Mixed-Precision: Training ImageNet in Four
Minutes.''} \emph{arXiv Preprint arXiv:1807.11205}, July.
\url{http://arxiv.org/abs/1807.11205v1}.

\bibitem[\citeproctext]{ref-jouppi2017tpu}
Jouppi, Norman P., Cliff Young, Nishant Patil, David Patterson, Gaurav
Agrawal, Raminder Bajwa, Sarah Bates, et al. 2017. {``In-Datacenter
Performance Analysis of a Tensor Processing Unit.''} In
\emph{Proceedings of the 44th Annual International Symposium on Computer
Architecture}, 1--12. ACM.
\url{https://doi.org/10.1145/3079856.3080246}.

\bibitem[\citeproctext]{ref-kingma2014adam}
Kingma, Diederik P., and Jimmy Ba. 2014. {``Adam: A Method for
Stochastic Optimization.''} \emph{ICLR}, December.
\url{http://arxiv.org/abs/1412.6980v9}.

\bibitem[\citeproctext]{ref-krizhevsky2012imagenet}
Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E. Hinton. 2017.
{``ImageNet Classification with Deep Convolutional Neural Networks.''}
\emph{Communications of the ACM} 60 (6): 84--90.
\url{https://doi.org/10.1145/3065386}.

\bibitem[\citeproctext]{ref-lecun1998efficient}
LeCun, Yann, Leon Bottou, Genevieve B. Orr, and Klaus -Robert Müller.
1998. {``Efficient BackProp.''} In \emph{Neural Networks: Tricks of the
Trade}, 1524:9--50. Springer Berlin Heidelberg.
\url{https://doi.org/10.1007/3-540-49430-8/_2}.

\bibitem[\citeproctext]{ref-loshchilov2019adamw}
Loshchilov, Ilya, and Frank Hutter. 2019. {``Decoupled Weight Decay
Regularization.''} In \emph{Proceedings of the International Conference
on Learning Representations (ICLR)}.
\url{https://openreview.net/forum?id=Bkg6RiCqY7}.

\bibitem[\citeproctext]{ref-micikevicius2017mixed}
Micikevicius, Paulius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich
Elsen, David Garcia, Boris Ginsburg, et al. 2017. {``Mixed Precision
Training.''} \emph{arXiv Preprint arXiv:1710.03740}, October.
\url{http://arxiv.org/abs/1710.03740v3}.

\bibitem[\citeproctext]{ref-nvidia_cublas}
NVIDIA. 2024a. {``cuBLAS: CUDA Basic Linear Algebra Subprograms.''}
\url{https://developer.nvidia.com/cublas}.

\bibitem[\citeproctext]{ref-nvidia_nccl}
---------. 2024b. {``NVIDIA Collective Communications Library (NCCL).''}
\url{https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/overview.html}.

\bibitem[\citeproctext]{ref-paszke2019pytorch}
Paszke, Adam, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury,
Gregory Chanan, Trevor Killeen, et al. 2019. {``PyTorch: An Imperative
Style, High-Performance Deep Learning Library.''} In \emph{Advances in
Neural Information Processing Systems}, 32:8024--35.
\url{https://proceedings.neurips.cc/paper/2019/hash/bdbca288fee7f92f2bfa9f7012727740-Abstract.html}.

\bibitem[\citeproctext]{ref-patarasuk2009bandwidth}
Patarasuk, Pitch, and Xin Yuan. 2009. {``Bandwidth Optimal All-Reduce
Algorithms for Clusters of Workstations.''} \emph{Journal of Parallel
and Distributed Computing} 69 (2): 117--24.
\url{https://doi.org/10.1016/j.jpdc.2008.09.002}.

\bibitem[\citeproctext]{ref-patterson2021hardware}
Patterson, David A., and John L. Hennessy. 2021. \emph{Computer
Architecture: A Quantitative Approach}. 6th ed. Morgan Kaufmann.

\bibitem[\citeproctext]{ref-rumelhart1986learning}
Rumelhart, David E., Geoffrey E. Hinton, and Ronald J. Williams. 1986.
{``Learning Representations by Back-Propagating Errors.''} \emph{Nature}
323 (6088): 533--36. \url{https://doi.org/10.1038/323533a0}.

\bibitem[\citeproctext]{ref-sergeev2018horovod}
Sergeev, Alexander, and Mike Del Balso. 2018. {``Horovod: Fast and Easy
Distributed Deep Learning in TensorFlow.''} \emph{CoRR} abs/1802.05799
(February). \url{http://arxiv.org/abs/1802.05799v3}.

\bibitem[\citeproctext]{ref-strassen1969gauss}
Strassen, Volker. 1969. {``Gaussian Elimination Is Not Optimal.''}
\emph{Numerische Mathematik} 13 (4): 354--56.
\url{https://doi.org/10.1007/bf02165411}.

\bibitem[\citeproctext]{ref-vaswani2017attention}
Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion
Jones, Aidan N.Gomez, Lukasz Kaiser, and Illia Polosukhin. 2025.
{``Attention Is All You Need.''} Shenzhen Medical Academy of Research;
Translation. \url{https://doi.org/10.65215/ctdc8e75}.

\bibitem[\citeproctext]{ref-wang2019superneurons}
Wang, Linnan, Jinmian Ye, Yiyang Zhao, Wei Wu, Ang Li, Shuaiwen Leon
Song, Zenglin Xu, and Tim Kraska. 2018. {``Superneurons: Dynamic GPU
Memory Management for Training Deep Neural Networks.''} In
\emph{Proceedings of the 23rd ACM SIGPLAN Symposium on Principles and
Practice of Parallel Programming}, 41--53. ACM.
\url{https://doi.org/10.1145/3178487.3178491}.

\bibitem[\citeproctext]{ref-wang_bfloat16_2019}
Wang, Y., and P. Kanwar. 2019. {``BFloat16: The Secret to High
Performance on Cloud TPUs.''} \emph{Google Cloud Blog}.

\bibitem[\citeproctext]{ref-zhao2024galorememoryefficientllmtraining}
Zhao, Jiawei, Zhenyu Zhang, Beidi Chen, Zhangyang Wang, Anima
Anandkumar, and Yuandong Tian. 2024. {``GaLore: Memory-Efficient LLM
Training by Gradient Low-Rank Projection,''} March.
\url{http://arxiv.org/abs/2403.03507v2}.

\end{CSLReferences}


\backmatter

\clearpage


\end{document}
