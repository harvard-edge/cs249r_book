% Options for packages loaded elsewhere
% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode,linktoc=all,pdfwindowui,pdfpagemode=FullScreen,pdfpagelayout=TwoPageRight}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  9pt,
  letterpaper,
  abstract,
  titlepage]{scrbook}
\usepackage{xcolor}
\usepackage[left=1in,marginparwidth=2.0666666666667in,textwidth=4.1333333333333in,marginparsep=0.3in]{geometry}
\usepackage{amsmath,amssymb}
\setcounter{secnumdepth}{3}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
% Make \paragraph and \subparagraph free-standing
\makeatletter
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}{
    \@ifstar
      \xxxParagraphStar
      \xxxParagraphNoStar
  }
  \newcommand{\xxxParagraphStar}[1]{\oldparagraph*{#1}\mbox{}}
  \newcommand{\xxxParagraphNoStar}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}{
    \@ifstar
      \xxxSubParagraphStar
      \xxxSubParagraphNoStar
  }
  \newcommand{\xxxSubParagraphStar}[1]{\oldsubparagraph*{#1}\mbox{}}
  \newcommand{\xxxSubParagraphNoStar}[1]{\oldsubparagraph{#1}\mbox{}}
\fi
\makeatother

\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{242,244,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{#1}}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.82,0.10,0.26}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{#1}}
\newcommand{\BuiltInTok}[1]{\textcolor[rgb]{0.88,0.40,0.10}{#1}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.41,0.45,0.49}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.86,0.20,0.18}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.58,0.00,0.30}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{#1}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.75,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.88,0.40,0.10}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.42,0.37,0.78}{#1}}
\newcommand{\ImportTok}[1]{\textcolor[rgb]{0.00,0.45,0.70}{\textbf{#1}}}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.73,0.49,0.84}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.10,0.10,0.10}{#1}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{#1}}
\newcommand{\RegionMarkerTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.12,0.55,0.30}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{#1}}}}

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\newsavebox\pandoc@box
\newcommand*\pandocbounded[1]{% scales image to fit in text height/width
  \sbox\pandoc@box{#1}%
  \Gscale@div\@tempa{\textheight}{\dimexpr\ht\pandoc@box+\dp\pandoc@box\relax}%
  \Gscale@div\@tempb{\linewidth}{\wd\pandoc@box}%
  \ifdim\@tempb\p@<\@tempa\p@\let\@tempa\@tempb\fi% select the smaller of both
  \ifdim\@tempa\p@<\p@\scalebox{\@tempa}{\usebox\pandoc@box}%
  \else\usebox{\pandoc@box}%
  \fi%
}
% Set default figure placement to htbp
\def\fps@figure{htbp}
\makeatother
% definitions for citeproc citations
\NewDocumentCommand\citeproctext{}{}
\NewDocumentCommand\citeproc{mm}{%
  \begingroup\def\citeproctext{#2}\cite{#1}\endgroup}
\makeatletter
 % allow citations to break across lines
 \let\@cite@ofmt\@firstofone
 % avoid brackets around text for \cite:
 \def\@biblabel#1{}
 \def\@cite#1#2{{#1\if@tempswa , #2\fi}}
\makeatother
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newenvironment{CSLReferences}[2] % #1 hanging-indent, #2 entry-spacing
 {\begin{list}{}{%
  \setlength{\itemindent}{0pt}
  \setlength{\leftmargin}{0pt}
  \setlength{\parsep}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1
   \setlength{\leftmargin}{\cslhangindent}
   \setlength{\itemindent}{-1\cslhangindent}
  \fi
  % set entry spacing
  \setlength{\itemsep}{#2\baselineskip}}}
 {\end{list}}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{\hfill\break\parbox[t]{\linewidth}{\strut\ignorespaces#1\strut}}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}

% =============================================================================
% LATEX HEADER CONFIGURATION FOR MLSYSBOOK PDF
% =============================================================================
% This file contains all LaTeX package imports, custom commands, and styling
% definitions for the PDF output of the Machine Learning Systems textbook.
%
% Key Features:
% - Harvard crimson branding throughout
% - Custom part/chapter/section styling
% - Professional table formatting with colored headers
% - Margin notes with custom styling
% - TikZ-based part dividers
% - Page numbering (Roman for frontmatter, Arabic for mainmatter)
%
% Note: This file is included via _quarto-pdf.yml and affects PDF output only.
% HTML/EPUB styling is handled separately via CSS files.
% =============================================================================

% =============================================================================
% PACKAGE IMPORTS
% =============================================================================

% Layout and positioning
% \usepackage[outercaption, ragged]{sidecap}  % Commented out to make figure captions inline instead of in margin
\usepackage{adjustbox}      % Adjusting box dimensions
\usepackage{afterpage}      % Execute commands after page break
\usepackage{morefloats}     % Increase number of floats
\usepackage{array}          % Enhanced table column formatting
\usepackage{atbegshi}       % Insert content at page beginning
%\usepackage{changepage}     % Change page dimensions mid-document
\usepackage{emptypage}      % Clear headers/footers on empty pages

% Language and text
\usepackage[english]{babel} % English language support
\usepackage{microtype}      % Improved typography and hyphenation

% Captions and floats
\usepackage{caption}
% Caption styling configuration
%\captionsetup[table]{belowskip=5pt}
\captionsetup{format=plain}
\DeclareCaptionLabelFormat{mylabel}{#1
#2:\hspace{1.0ex}}
\DeclareCaptionFont{ninept}{\fontsize{7pt}{8}\selectfont #1}

% Figure captions: Small font, bold label, ragged right
\captionsetup[figure]{labelfont={bf,ninept},labelsep=space,
belowskip=2pt,aboveskip=6pt,labelformat=mylabel,
justification=raggedright,singlelinecheck=false,font={ninept}}

% Table captions: Small font, bold label, ragged right
\captionsetup[table]{belowskip=6pt,labelfont={bf,ninept},labelsep=none,
labelformat=mylabel,justification=raggedright,singlelinecheck=false,font={ninept}}

% Typography fine-tuning
\emergencystretch=5pt       % Allow extra stretch to avoid overfull boxes

% Utility packages
\usepackage{etoolbox}       % For patching commands and environments

% Page layout and headers
\usepackage{fancyhdr}       % Custom headers and footers
\usepackage{geometry}       % Page dimensions and margins

% Graphics and figures
\usepackage{graphicx}       % Include graphics
\usepackage{float}          % Improved float placement
\usepackage[skins,breakable]{tcolorbox} % Coloured and framed text boxes
\tcbset{before upper=\setlength{\parskip}{3pt}}

% Tables
\usepackage{longtable}      % Multi-page tables

% Fonts and typography
\usepackage{fontspec}       % Font selection for LuaLaTeX
\usepackage{mathptmx}       % Times-like math fonts
\usepackage{newpxtext}      % Palatino-like font for body text

% Colors and visual elements
\usepackage[dvipsnames]{xcolor}  % Extended color support
\usepackage{tikz}           % Programmatic graphics
\usetikzlibrary{positioning}
\usetikzlibrary{calc}
\usepackage{tikzpagenodes}  % TikZ positioning relative to page

% Code listings
\usepackage{listings}       % Code highlighting

% Hyperlinks
\usepackage{hyperref}       % Clickable links in PDF

% Conditional logic
\usepackage{ifthen}         % If-then-else commands

% Math symbols
\usepackage{amsmath}        % AMS math extensions
\usepackage{amssymb}        % AMS math symbols
\usepackage{latexsym}       % Additional LaTeX symbols
\usepackage{pifont}         % Zapf Dingbats symbols
\providecommand{\blacklozenge}{\ding{117}}  % Black diamond symbol

% Lists
\usepackage{enumitem}       % Customizable lists

% Margin notes and sidenotes
\usepackage{marginfix}      % Fixes margin note overflow
\usepackage{marginnote}     % Margin notes
\usepackage{sidenotes}      % Academic-style sidenotes
\renewcommand\raggedrightmarginnote{\sloppy}
\renewcommand\raggedleftmarginnote{\sloppy}

% Typography improvements
\usepackage{ragged2e}       % Better ragged text
\usepackage[all]{nowidow}   % Prevent widows and orphans
\usepackage{needspace}      % Ensure minimum space on page

% Section formatting
\usepackage[explicit]{titlesec}  % Custom section titles
\usepackage{tocloft}        % Table of contents formatting

% QR codes and icons
\usepackage{fontawesome5}   % Font Awesome icons
\usepackage{qrcode}         % QR code generation
\qrset{link, height=15mm}

% =============================================================================
% FLOAT CONFIGURATION
% =============================================================================
% Allow more floats per page to handle figure-heavy chapters
\extrafloats{200}
\setcounter{topnumber}{12}       % Max floats at top of page
\setcounter{bottomnumber}{12}    % Max floats at bottom of page
\setcounter{totalnumber}{24}     % Max floats per page
\setcounter{dbltopnumber}{8}     % Max floats at top of two-column page
\renewcommand{\topfraction}{.95}  % Max fraction of page for top floats
\renewcommand{\bottomfraction}{.95}
\renewcommand{\textfraction}{.05}  % Min fraction of page for text
\renewcommand{\floatpagefraction}{.7}  % Min fraction of float page
\renewcommand{\dbltopfraction}{.95}

% Prevent "Float(s) lost" errors by flushing floats more aggressively
\usepackage{placeins}  % Provides \FloatBarrier

% =============================================================================
% COLOR DEFINITIONS
% =============================================================================
% NOTE: TikZ colors (BlueLine, GreenLine, RedLine, OrangeLine, etc.) are defined
% in the YAML config files under format > pdf > tikz > include-headers.
% Only colors specific to LaTeX packages (not TikZ) are defined here.

% Harvard crimson - primary brand color used throughout
\definecolor{crimson}{HTML}{A51C30}

% Quiz element colors
\definecolor{quiz-question-color1}{RGB}{225,243,248}  % Light blue background
\definecolor{quiz-question-color2}{RGB}{17,158,199}   % Blue border
\definecolor{quiz-answer-color1}{RGB}{250,234,241}    % Light pink background
\definecolor{quiz-answer-color2}{RGB}{152,14,90}      % Magenta border

% =============================================================================
% LIST FORMATTING
% =============================================================================
% Tighter list spacing for academic style
\def\tightlist{}
\setlist{itemsep=1pt, parsep=1pt, topsep=0pt,after={\vspace{0.3\baselineskip}}}
\let\tightlist\relax

\makeatletter
\@ifpackageloaded{framed}{}{\usepackage{framed}}
\@ifpackageloaded{fancyvrb}{}{\usepackage{fancyvrb}}
\makeatother

\makeatletter
%New float "codelisting" has been updated
\AtBeginDocument{%
\floatstyle{ruled}
\newfloat{codelisting}{!htb}{lop}
\floatname{codelisting}{Listing}
\floatplacement{codelisting}{!htb}
\captionsetup[codelisting]{labelfont={bf,ninept},labelformat=mylabel,
  singlelinecheck=false,width=\linewidth,labelsep=none,font={ninept}}%
\renewenvironment{snugshade}{%
   \def\OuterFrameSep{3pt}%
   \def\FrameCommand{\fboxsep=5pt\colorbox{shadecolor}}%
   \MakeFramed{\advance\hsize-\width\FrameRestore}%
   \leftskip 0.5em \rightskip 0.5em%
   \small% decrease font size
   }{\endMakeFramed}%
}
\makeatother

%The space before and after the verbatim environment "Highlighting" has been reduced
\fvset{listparameters=\setlength{\topsep}{0pt}\setlength{\partopsep}{0pt}}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{framesep=0mm,commandchars=\\\{\}}

\makeatletter
\renewcommand\fs@ruled{\def\@fs@cfont{\bfseries}\let\@fs@capt\floatc@ruled
\def\@fs@pre{\hrule height.8pt depth0pt \kern2pt}%
\def\@fs@post{\kern2pt\hrule\relax}%
\def\@fs@mid{\kern2pt\hrule\kern1pt}%space between float and caption
\let\@fs@iftopcapt\iftrue}
\makeatother


% =============================================================================
% HYPHENATION RULES
% =============================================================================
% Explicit hyphenation points for technical terms to avoid bad breaks
\hyphenation{
  light-weight
  light-weight-ed
  de-vel-op-ment
  un-der-stand-ing
  mod-els
  prin-ci-ples
  ex-per-tise
  com-pli-cat-ed
  blue-print
  per‧for‧mance
  com-mu-ni-ca-tion
  par-a-digms
  hy-per-ten-sion
  a-chieved
}

% =============================================================================
% CODE LISTING CONFIGURATION
% =============================================================================
% Settings for code blocks using listings package
\lstset{
breaklines=true,              % Automatic line wrapping
breakatwhitespace=true,       % Break at whitespace only
basicstyle=\ttfamily,         % Monospace font
frame=none,                   % No frame around code
keepspaces=true,              % Preserve spaces
showspaces=false,             % Don't show space characters
showtabs=false,               % Don't show tab characters
columns=flexible,             % Flexible column width
belowskip=0pt,               % Minimal spacing
aboveskip=0pt
}

% =============================================================================
% PAGE GEOMETRY
% =============================================================================
% MIT Press trim size: 7" x 10" (per publisher specifications)
% This is a standard academic textbook format providing good readability
% for technical content with figures and code blocks.
% Wide outer margin accommodates sidenotes/margin notes.
\geometry{
  paperwidth=7in,
  paperheight=10in,
  top=0.875in,
  bottom=0.875in,
  inner=0.875in,              % Inner margin (binding side)
  outer=1.75in,               % Outer margin (includes space for sidenotes)
  footskip=30pt,
  marginparwidth=1.25in,      % Width for margin notes
  twoside                     % Different left/right pages
}

% =============================================================================
% SIDENOTE STYLING
% =============================================================================
% Custom sidenote design with crimson vertical bar
\renewcommand{\thefootnote}{\textcolor{crimson}{\arabic{footnote}}}

% Save original sidenote command
\makeatletter
\@ifundefined{oldsidenote}{
  \let\oldsidenote\sidenote%
}{}
\makeatother

% Redefine sidenote with vertical crimson bar
\renewcommand{\sidenote}[1]{%
  \oldsidenote{%
    \noindent
    \color{crimson!100}                        % Crimson vertical line
    \raisebox{0em}{%
      \rule{0.5pt}{1.5em}                      % Thin vertical line
    }
    \hspace{0.3em}                             % Space after line
    \color{black}                              % Reset text color
    \footnotesize #1                           % Sidenote content
  }%
}

% =============================================================================
% FLOAT HANDLING
% =============================================================================
% Patch LaTeX's output routine to handle float overflow gracefully
% The "Float(s) lost" error occurs in \@doclearpage when \@currlist is not empty
% This patch silently clears pending floats that can't be placed
\makeatletter
\let\orig@doclearpage\@doclearpage
\def\@doclearpage{%
  \ifx\@currlist\@empty\else
    \global\let\@currlist\@empty
    \typeout{Warning: Floats cleared to prevent overflow}%
  \fi
  \orig@doclearpage
}
\makeatother

% Additional safety for structural commands
\let\originalbackmatter\backmatter
\renewcommand{\backmatter}{%
  \clearpage%
  \originalbackmatter%
}

\let\originalfrontmatter\frontmatter
\renewcommand{\frontmatter}{%
  \clearpage%
  \originalfrontmatter%
}

\let\originalmainmatter\mainmatter
\renewcommand{\mainmatter}{%
  \clearpage%
  \originalmainmatter%
}

% =============================================================================
% PAGE HEADERS AND FOOTERS
% =============================================================================
% Ensure chapters use fancy page style (not plain)
\patchcmd{\chapter}{\thispagestyle{plain}}{\thispagestyle{fancy}}{}{}

% Main page style with crimson headers
\pagestyle{fancy}
\fancyhf{}                                              % Clear all
\fancyhead[LE]{\small\color{crimson}\nouppercase{\rightmark}}  % Left even: section
\fancyhead[RO]{\color{crimson}\thepage}                 % Right odd: page number
\fancyhead[LO]{\small\color{crimson}\nouppercase{\leftmark}}   % Left odd: chapter
\fancyhead[RE]{\color{crimson}\thepage}                 % Right even: page number
\renewcommand{\headrulewidth}{0.4pt}                    % Thin header line
\renewcommand{\footrulewidth}{0pt}                      % No footer line

% Plain page style (for chapter openings)
\fancypagestyle{plain}{
  \fancyhf{}
  \fancyfoot[C]{\color{crimson}\thepage}                % Centered page number
  \renewcommand{\headrulewidth}{0pt}
  \renewcommand{\footrulewidth}{0pt}
}

% =============================================================================
% KOMA-SCRIPT FONT ADJUSTMENTS
% =============================================================================
% Apply crimson color to all heading levels
\addtokomafont{disposition}{\rmfamily\color{crimson}}
\addtokomafont{chapter}{\color{crimson}}
\addtokomafont{section}{\color{crimson}}
\addtokomafont{subsection}{\color{crimson}}

% =============================================================================
% ABSTRACT ENVIRONMENT
% =============================================================================
\newenvironment{abstract}{
  \chapter*{\abstractname}
  \addcontentsline{toc}{chapter}{\abstractname}
  \small
}{
  \clearpage
}

% =============================================================================
% HYPERLINK CONFIGURATION
% =============================================================================
% Crimson-colored links throughout, two-page PDF layout
\hypersetup{
  linkcolor=crimson,
  citecolor=crimson,
  urlcolor=crimson,
  pdfpagelayout=TwoPageRight,   % Two-page spread view
  pdfstartview=Fit               % Initial zoom fits page
}

% =============================================================================
% PART SUMMARY SYSTEM
% =============================================================================
% Allows adding descriptive text below part titles
\newcommand{\partsummary}{}     % Empty by default
\newif\ifhaspartsummary%
\haspartsummaryfalse%

\newcommand{\setpartsummary}[1]{%
  \renewcommand{\partsummary}{#1}%
  \haspartsummarytrue%
}

% Additional colors for part page backgrounds
\definecolor{BrownLL}{RGB}{233,222,220}
\definecolor{BlueDD}{RGB}{62,100,125}
\colorlet{BlueDD}{magenta}

% ===============================================================================
% PART STYLING SYSTEM
% ===============================================================================
%
% This system provides three distinct visual styles for book organization:
%
% 1. NUMBERED PARTS (\part{title}) - For main book sections
%    - Roman numerals (I, II, III, etc.) in top right corner
%    - Crimson title with horizontal lines above/below
%    - "Part I" label in sidebar
%    - Used for: foundations, principles, optimization, deployment, etc.
%
% 2. UNNUMBERED PARTS (\part*{title}) - For special sections like "Labs"
%    - Division-style geometric background (left side)
%    - No Roman numerals
%    - Used for: labs section
%
% 3. DIVISIONS (\division{title}) - For major book divisions
%    - Clean geometric background with centered title
%    - Used for: frontmatter, main_content, backmatter
%
% The Lua filter (inject-parts.lua) automatically routes parts by {key:xxx} commands
% to the appropriate LaTeX command based on the key name.
% ===============================================================================

% NUMBERED PARTS: Roman numeral styling for main book sections
\titleformat{\part}[display]
{\thispagestyle{empty}}{}{20pt}{
\begin{tikzpicture}[remember picture,overlay]
%%%
%%
\node[crimson,align=flush right,
inner sep=0,outer sep=0mm,draw=none,%
anchor=east,minimum height=31mm, text width=1.2\textwidth,
yshift=-30mm,font={%
\fontsize{98pt}{104}\selectfont\bfseries}]  (BG) at (current page text area.north east){\thepart};
%
\node[black,inner sep=0mm,draw=none,
anchor=mid,text width=1.2\textwidth,
 minimum height=35mm, align=right,
node distance=7mm,below=of BG,
font={\fontsize{30pt}{34}\selectfont}]
(BGG)  {\hyphenchar\font=-1 \color{black}\MakeUppercase {#1}};
\draw [crimson,line width=3pt] ([yshift=0mm]BGG.north west) -- ([yshift=0mm]BGG.north east);
\draw [crimson,line width=2pt] ([yshift=0mm]BGG.south west) -- ([yshift=0mm]BGG.south east);
%
\node[fill=crimson,text=white,rotate=90,%
anchor=south west,minimum height=15mm,
minimum width=40mm,font={%
\fontsize{20pt}{20}\selectfont\bfseries}](BP)  at
(current page text area.south east)
{{\sffamily Part}~\thepart};
%
\path[red](BP.north west)-|coordinate(PS)(BGG.south west);
%
% Part summary box commented out for cleaner design
% \ifhaspartsummary
% \node[inner sep=4pt,text width=0.7\textwidth,draw=none,fill=BrownLL!40,
% align=justify,font={\fontsize{9pt}{12}\selectfont},anchor=south west]
% at (PS) {\partsummary};
% \fi
\end{tikzpicture}
}[]

\renewcommand{\thepart}{\Roman{part}}

% UNNUMBERED PARTS: Division-style background for special sections
\titleformat{name=\part,numberless}[display]
{\thispagestyle{empty}}{}{20pt}{
\begin{tikzpicture}[remember picture,overlay]
%%%
\coordinate(S1)at([yshift=-200mm]current page.north west);
\draw[draw=none,fill=BlueDD!7](S1)--++(45:16)coordinate(S2)-
|(S2|-current page.north west)--(current page.north west)coordinate(S3)--(S1);
%
\coordinate(E1)at([yshift=-98mm]current page.north west);
\draw[draw=none,fill=BlueDD!15](E1)--(current page.north west)coordinate(E2)
--++(0:98mm)coordinate(E3)--(E1);
%
\coordinate(D1)at([yshift=15mm]current page.south west);
\draw[draw=none,fill=BlueDD!40,opacity=0.5](D1)--++(45:5.5)coordinate(D2)
-|(D2|-current page.north west)--(current page.north west)coordinate(D3)--(D1);
%%%%
\path[red](S2)-|(S2-|current page.east)coordinate(SS2);
%PART
\node[crimson,align=flush right,inner sep=0,outer sep=0mm,draw=none,anchor=south,
font={\fontsize{48pt}{48}\selectfont\bfseries}]  (BG) at ($(S2)!0.5!(SS2)$){\hphantom{Part}};
%%%
\path[green]([yshift=15mm]D2)-|coordinate(TPD)(BG.south east);
\node[inner sep=0mm,draw=none,anchor=south east,%text width=0.9\textwidth,
align=right,font={\fontsize{40pt}{40}\selectfont}]
(BGG) at (TPD)  {\color{crimson}\MakeUppercase {#1}};%\MakeUppercase {}
\end{tikzpicture}
}

% Define \numberedpart command for numbered parts
\newcommand{\numberedpart}[1]{%
\FloatBarrier%  % Flush all pending floats before part break
\clearpage
\thispagestyle{empty}
\stepcounter{part}%
\begin{tikzpicture}[remember picture,overlay]
%%%
%%
\node[crimson,align=flush right,
inner sep=0,outer sep=0mm,draw=none,%
anchor=east,minimum height=31mm, text width=1.2\textwidth,
yshift=-30mm,font={%
\fontsize{98pt}{104}\selectfont\bfseries}]  (BG) at (current page text area.north east){\thepart};
%
\node[black,inner sep=0mm,draw=none,
anchor=mid,text width=1.2\textwidth,
 minimum height=35mm, align=right,
node distance=7mm,below=of BG,
font={\fontsize{30pt}{34}\selectfont}]
(BGG)  {\hyphenchar\font=-1 \color{black}\MakeUppercase {#1}};
\draw [crimson,line width=3pt] ([yshift=0mm]BGG.north west) -- ([yshift=0mm]BGG.north east);
\draw [crimson,line width=2pt] ([yshift=0mm]BGG.south west) -- ([yshift=0mm]BGG.south east);
%
\node[fill=crimson,text=white,rotate=90,%
anchor=south west,minimum height=15mm,
minimum width=40mm,font={%
\fontsize{20pt}{20}\selectfont\bfseries}](BP)  at
(current page text area.south east)
{{\sffamily Part}~\thepart};
%
\path[red](BP.north west)-|coordinate(PS)(BGG.south west);
%
% Part summary box commented out for cleaner design
% \ifhaspartsummary
% \node[inner sep=4pt,text width=0.7\textwidth,draw=none,fill=BrownLL!40,
% align=justify,font={\fontsize{9pt}{12}\selectfont},anchor=south west]
% at (PS) {\partsummary};
% \fi
\end{tikzpicture}
\clearpage
}



% DIVISIONS: Clean geometric styling with subtle tech elements
% Used for frontmatter, main_content, and backmatter divisions
\newcommand{\division}[1]{%
\FloatBarrier%  % Flush all pending floats before division break
\clearpage
\thispagestyle{empty}
\begin{tikzpicture}[remember picture,overlay]

% Clean geometric background (original design)
\coordinate(S1)at([yshift=-200mm]current page.north west);
\draw[draw=none,fill=BlueDD!7](S1)--++(45:16)coordinate(S2)-
|(S2|-current page.north west)--(current page.north west)coordinate(S3)--(S1);

\coordinate(E1)at([yshift=-98mm]current page.north west);
\draw[draw=none,fill=BlueDD!15](E1)--(current page.north west)coordinate(E2)
--++(0:98mm)coordinate(E3)--(E1);

\coordinate(D1)at([yshift=15mm]current page.south west);
\draw[draw=none,fill=BlueDD!40,opacity=0.5](D1)--++(45:5.5)coordinate(D2)
-|(D2|-current page.north west)--(current page.north west)coordinate(D3)--(D1);

% Subtle tech elements - positioned in white areas for better visibility
% Upper right white area - more visible
\draw[crimson!40, line width=0.8pt] ([xshift=140mm,yshift=-60mm]current page.north west) -- ++(40mm,0);
\draw[crimson!40, line width=0.8pt] ([xshift=150mm,yshift=-70mm]current page.north west) -- ++(30mm,0);
\draw[crimson!35, line width=0.7pt] ([xshift=160mm,yshift=-60mm]current page.north west) -- ++(0,-15mm);
\draw[crimson!35, line width=0.7pt] ([xshift=170mm,yshift=-70mm]current page.north west) -- ++(0,10mm);

% Circuit nodes - upper right
\fill[crimson!50] ([xshift=160mm,yshift=-60mm]current page.north west) circle (1.5mm);
\fill[white] ([xshift=160mm,yshift=-60mm]current page.north west) circle (0.8mm);
\fill[crimson!50] ([xshift=170mm,yshift=-70mm]current page.north west) circle (1.3mm);
\fill[white] ([xshift=170mm,yshift=-70mm]current page.north west) circle (0.6mm);

% Lower right white area - enhanced visibility
\draw[crimson!45, line width=0.9pt] ([xshift=140mm,yshift=-190mm]current page.north west) -- ++(45mm,0);
\draw[crimson!45, line width=0.9pt] ([xshift=150mm,yshift=-200mm]current page.north west) -- ++(35mm,0);
\draw[crimson!40, line width=0.8pt] ([xshift=160mm,yshift=-190mm]current page.north west) -- ++(0,-20mm);
\draw[crimson!40, line width=0.8pt] ([xshift=170mm,yshift=-200mm]current page.north west) -- ++(0,15mm);

% Additional connecting lines in lower right
\draw[crimson!35, line width=0.7pt] ([xshift=130mm,yshift=-180mm]current page.north west) -- ++(25mm,0);
\draw[crimson!35, line width=0.7pt] ([xshift=145mm,yshift=-180mm]current page.north west) -- ++(0,-25mm);

% Circuit nodes - lower right (more prominent)
\fill[crimson!55] ([xshift=160mm,yshift=-190mm]current page.north west) circle (1.6mm);
\fill[white] ([xshift=160mm,yshift=-190mm]current page.north west) circle (0.9mm);
\fill[crimson!55] ([xshift=170mm,yshift=-200mm]current page.north west) circle (1.4mm);
\fill[white] ([xshift=170mm,yshift=-200mm]current page.north west) circle (0.7mm);
\fill[crimson!50] ([xshift=145mm,yshift=-180mm]current page.north west) circle (1.2mm);
\fill[white] ([xshift=145mm,yshift=-180mm]current page.north west) circle (0.6mm);

% Title positioned in center - clean and readable
\node[inner sep=0mm,draw=none,anchor=center,text width=0.8\textwidth,
align=center,font={\fontsize{40pt}{40}\selectfont}]
(BGG) at (current page.center)  {\color{crimson}\MakeUppercase {#1}};

\end{tikzpicture}
\clearpage
}

% LAB DIVISIONS: Circuit-style neural network design for lab sections
% Used specifically for lab platform sections (arduino, xiao, grove, etc.)
\newcommand{\labdivision}[1]{%
\FloatBarrier%  % Flush all pending floats before lab division break
\clearpage
\thispagestyle{empty}
\begin{tikzpicture}[remember picture,overlay]
% Circuit background with subtle gradient
\coordinate(S1)at([yshift=-200mm]current page.north west);
\draw[draw=none,fill=BlueDD!5](S1)--++(45:16)coordinate(S2)-
|(S2|-current page.north west)--(current page.north west)coordinate(S3)--(S1);

% TOP AREA: Circuit lines in upper white space
\draw[crimson!50, line width=1.5pt] ([xshift=30mm,yshift=-40mm]current page.north west) -- ++(60mm,0);
\draw[crimson!40, line width=1pt] ([xshift=120mm,yshift=-50mm]current page.north west) -- ++(50mm,0);
\draw[crimson!50, line width=1.5pt] ([xshift=40mm,yshift=-70mm]current page.north west) -- ++(40mm,0);

% Connecting lines in top area
\draw[crimson!30, line width=1pt] ([xshift=60mm,yshift=-40mm]current page.north west) -- ++(0,-20mm);
\draw[crimson!30, line width=1pt] ([xshift=145mm,yshift=-50mm]current page.north west) -- ++(0,10mm);

% Neural nodes in top area
\fill[crimson!70] ([xshift=60mm,yshift=-40mm]current page.north west) circle (2.5mm);
\fill[white] ([xshift=60mm,yshift=-40mm]current page.north west) circle (1.5mm);
\fill[crimson!60] ([xshift=145mm,yshift=-50mm]current page.north west) circle (2mm);
\fill[white] ([xshift=145mm,yshift=-50mm]current page.north west) circle (1mm);
\fill[crimson!80] ([xshift=80mm,yshift=-70mm]current page.north west) circle (2mm);
\fill[white] ([xshift=80mm,yshift=-70mm]current page.north west) circle (1mm);

% BOTTOM AREA: Circuit lines in lower white space
\draw[crimson!50, line width=1.5pt] ([xshift=20mm,yshift=-200mm]current page.north west) -- ++(70mm,0);
\draw[crimson!40, line width=1pt] ([xshift=110mm,yshift=-210mm]current page.north west) -- ++(60mm,0);
\draw[crimson!50, line width=1.5pt] ([xshift=35mm,yshift=-230mm]current page.north west) -- ++(45mm,0);

% Connecting lines in bottom area
\draw[crimson!30, line width=1pt] ([xshift=55mm,yshift=-200mm]current page.north west) -- ++(0,-20mm);
\draw[crimson!30, line width=1pt] ([xshift=140mm,yshift=-210mm]current page.north west) -- ++(0,15mm);

% Neural nodes in bottom area
\fill[crimson!70] ([xshift=55mm,yshift=-200mm]current page.north west) circle (2.5mm);
\fill[white] ([xshift=55mm,yshift=-200mm]current page.north west) circle (1.5mm);
\fill[crimson!60] ([xshift=140mm,yshift=-210mm]current page.north west) circle (2mm);
\fill[white] ([xshift=140mm,yshift=-210mm]current page.north west) circle (1mm);
\fill[crimson!80] ([xshift=80mm,yshift=-230mm]current page.north west) circle (2mm);
\fill[white] ([xshift=80mm,yshift=-230mm]current page.north west) circle (1mm);

% SIDE AREAS: Subtle circuit elements on left and right edges
\draw[crimson!30, line width=1pt] ([xshift=15mm,yshift=-120mm]current page.north west) -- ++(20mm,0);
\draw[crimson!30, line width=1pt] ([xshift=175mm,yshift=-130mm]current page.north west) -- ++(15mm,0);
\fill[crimson!50] ([xshift=25mm,yshift=-120mm]current page.north west) circle (1.5mm);
\fill[white] ([xshift=25mm,yshift=-120mm]current page.north west) circle (0.8mm);
\fill[crimson!50] ([xshift=185mm,yshift=-130mm]current page.north west) circle (1.5mm);
\fill[white] ([xshift=185mm,yshift=-130mm]current page.north west) circle (0.8mm);

% Title positioned in center - CLEAN AREA
\node[inner sep=0mm,draw=none,anchor=center,text width=0.8\textwidth,
align=center,font={\fontsize{44pt}{44}\selectfont\bfseries}]
(BGG) at (current page.center)  {\color{crimson}\MakeUppercase {#1}};

\end{tikzpicture}
\clearpage
}

% Define \lab command for lab styling (different visual treatment)
\newcommand{\lab}[1]{%
\begin{tikzpicture}[remember picture,overlay]
%%%
% Different background pattern for labs
\coordinate(S1)at([yshift=-200mm]current page.north west);
\draw[draw=none,fill=BlueDD!15](S1)--++(45:16)coordinate(S2)-
|(S2|-current page.north west)--(current page.north west)coordinate(S3)--(S1);
%
\coordinate(E1)at([yshift=-98mm]current page.north west);
\draw[draw=none,fill=BlueDD!25](E1)--(current page.north west)coordinate(E2)
--++(0:98mm)coordinate(E3)--(E1);
%
\coordinate(D1)at([yshift=15mm]current page.south west);
\draw[draw=none,fill=BlueDD!60,opacity=0.7](D1)--++(45:5.5)coordinate(D2)
-|(D2|-current page.north west)--(current page.north west)coordinate(D3)--(D1);
%%%%
\path[red](S2)-|(S2-|current page.east)coordinate(SS2);
%LAB - Different styling
\node[crimson,align=flush right,inner sep=0,outer sep=0mm,draw=none,anchor=south,
font={\fontsize{48pt}{48}\selectfont\bfseries}]  (BG) at ($(S2)!0.5!(SS2)$){\hphantom{Workshop}};
%%%
\path[green]([yshift=15mm]D2)-|coordinate(TPD)(BG.south east);
\node[inner sep=0mm,draw=none,anchor=south east,%text width=0.9\textwidth,
align=right,font={\fontsize{40pt}{40}\selectfont}]
(BGG) at (TPD)  {\color{crimson}\MakeUppercase {#1}};%\MakeUppercase {}
\end{tikzpicture}
\thispagestyle{empty}
\clearpage
}

% =============================================================================
% SECTION FORMATTING
% =============================================================================
% All section levels use crimson color and are ragged right

% Section (Large, bold, crimson)
\titleformat{\section}
  {\normalfont\Large\bfseries\color{crimson}\raggedright}
  {\thesection}
  {0.5em}
  {#1}
\titlespacing*{\section}{0pc}{14pt plus 4pt minus 4pt}{6pt plus 2pt minus 2pt}[0pc]

% Subsection (large, bold, crimson)
\titleformat{\subsection}
  {\normalfont\large\bfseries\color{crimson}\raggedright}
  {\thesubsection}
  {0.5em}
  {#1}
\titlespacing*{\subsection}{0pc}{12pt plus 4pt minus 4pt}{5pt plus 1pt minus 2pt}[0pc]

% Subsubsection (normal size, bold, crimson)
\titleformat{\subsubsection}
  {\normalfont\normalsize\bfseries\color{crimson}\raggedright}
  {\thesubsubsection}
  {0.5em}
  {#1}
\titlespacing*{\subsubsection}{0pc}{12pt plus 4pt minus 4pt}{5pt plus 1pt minus 2pt}[0pc]

% Paragraph (run-in, bold, crimson)
\titleformat{\paragraph}[runin]
  {\normalfont\normalsize\bfseries\color{crimson}}
  {\theparagraph}
  {0.5em}
  {#1}
\titlespacing*{\paragraph}{0pc}{6pt plus 2pt minus 2pt}{0.5em}[0pc]

% Subparagraph (run-in, italic, crimson)
\titleformat{\subparagraph}[runin]
  {\normalfont\normalsize\itshape\color{crimson}}
  {\thesubparagraph}
  {0.5em}
  {#1}
\titlespacing*{\subparagraph}{0pc}{6pt plus 2pt minus 2pt}{0.5em}[0pc]

% =============================================================================
% CHAPTER FORMATTING
% =============================================================================
% Numbered chapters: "Chapter X" prefix, huge crimson title
\titleformat{\chapter}[display]
  {\normalfont\huge\bfseries\color{crimson}}
  {\chaptername\ \thechapter}
  {20pt}
  {\Huge #1}
  []

% Unnumbered chapters: no prefix, huge crimson title
\titleformat{name=\chapter,numberless}
  {\normalfont\huge\bfseries\color{crimson}}
  {}
  {0pt}
  {\Huge #1}
  []

\renewcommand{\chaptername}{Chapter}
% =============================================================================
% TABLE OF CONTENTS FORMATTING
% =============================================================================
\setcounter{tocdepth}{2}                      % Show chapters, sections, subsections

% TOC spacing adjustments for number widths and indentation
\setlength{\cftchapnumwidth}{2em}             % Chapter number width
\setlength{\cftsecnumwidth}{2.75em}           % Section number width
\setlength{\cftsubsecnumwidth}{3.25em}        % Subsection number width
\setlength{\cftsubsubsecnumwidth}{4em}        % Subsubsection number width
\setlength{\cftsubsecindent}{4.25em}          % Subsection indent
\setlength{\cftsubsubsecindent}{7.5em}        % Subsubsection indent

% Chapter entries in TOC: bold crimson with "Chapter" prefix
\renewcommand{\cftchapfont}{\bfseries\color{crimson}}
\renewcommand{\cftchappresnum}{\color{crimson}Chapter~}

% Custom formatting for division entries (styled like parts)
\newcommand{\divisionchapter}[1]{%
  \addvspace{12pt}%
  \noindent\hfil\bfseries\color{crimson}#1\hfil\par%
  \addvspace{6pt}%
}

% Adjust TOC spacing for "Chapter" prefix
\newlength{\xtraspace}
\settowidth{\xtraspace}{\cftchappresnum\cftchapaftersnum}
\addtolength{\cftchapnumwidth}{\xtraspace}

% Unnumbered chapters with TOC entry
\newcommand{\likechapter}[1]{%
    \chapter*{#1}
    \addcontentsline{toc}{chapter}{\textcolor{crimson}{#1}}
}

% =============================================================================
% PAGE NUMBERING SYSTEM
% =============================================================================
% Implements traditional book numbering:
% - Roman numerals (i, ii, iii...) for frontmatter
% - Arabic numerals (1, 2, 3...) for mainmatter
% Automatically switches at first numbered chapter
\makeatletter
\newif\if@firstnumbered%
\@firstnumberedtrue%
\newif\if@firstunnumbered%
\@firstunnumberedtrue%

\newcounter{lastRomanPage}
\setcounter{lastRomanPage}{1}

% Start document with Roman numerals (frontmatter)
\AtBeginDocument{
  \pagenumbering{roman}
  \renewcommand{\thepage}{\roman{page}}
}

% Intercept chapter command
\let\old@chapter\chapter%
\renewcommand{\chapter}{%
  \@ifstar{\unnumbered@chapter}{\numbered@chapter}%
}

% Numbered chapters: switch to Arabic on first occurrence
\newcommand{\numbered@chapter}[1]{%
  \if@firstnumbered%
    \cleardoublepage%
    \setcounter{lastRomanPage}{\value{page}}%
    \pagenumbering{arabic}%
    \@firstnumberedfalse%
  \else
    \setcounter{page}{\value{page}}%
  \fi
  \setcounter{sidenote}{1}                    % Reset footnote counter per chapter
  \old@chapter{#1}%
}

% Unnumbered chapters: stay in Roman numerals
\newcommand{\unnumbered@chapter}[1]{%
  \if@firstunnumbered%
    \clearpage
    \setcounter{lastRomanPage}{\value{page}}%
    \pagenumbering{roman}%
    \@firstunnumberedfalse%
  \fi
  \setcounter{sidenote}{1}
  \old@chapter*{#1}%
}
\makeatother

% =============================================================================
% TABLE SIZING AND SPACING
% =============================================================================
% Make tables slightly smaller to fit more content
\AtBeginEnvironment{longtable}{\scriptsize}

% Increase vertical spacing in table cells (default is 1.0)
\renewcommand{\arraystretch}{1.5}

% Prefer placing figures and tables at the top of pages
\makeatletter
\renewcommand{\fps@figure}{t}  % Default placement: top of page
\renewcommand{\fps@table}{t}   % Default placement: top of page
\makeatother

% =============================================================================
% LONGTABLE PAGE BREAKING FIXES (Windows compatibility)
% =============================================================================
% Prevent "Infinite glue shrinkage" errors on Windows LaTeX builds
% by giving longtable more flexibility in page breaking

% Allow more flexible page breaking (vs strict \flushbottom)
\raggedbottom

% Process more rows before attempting page break (default is 20)
\setcounter{LTchunksize}{50}

% Add extra stretch for longtable environments specifically
\AtBeginEnvironment{longtable}{%
  \setlength{\emergencystretch}{3em}%
  \setlength{\parskip}{0pt plus 1pt}%
}

% =============================================================================
% TABLE STYLING - Clean tables with crimson borders
% =============================================================================
% Professional table appearance with:
% - Clean white background (no colored rows)
% - Crimson-colored borders
% - Good spacing for readability
%
% Note: Headers are automatically bolded by Quarto when using **text** in source
\usepackage{booktabs}      % Professional table rules (\toprule, \midrule, \bottomrule)
\usepackage{colortbl}      % For colored borders (\arrayrulecolor)

% Global table styling - crimson borders
\setlength{\arrayrulewidth}{0.5pt}          % Thinner borders than default
%\arrayrulecolor{crimson}                    % Crimson borders matching brand

\setcounter{chapter}{0}

% =============================================================================
% DROP CAPS (Lettrine)
% =============================================================================
% Decorative large first letter at chapter openings, following the tradition
% of Hennessy & Patterson and other MIT Press textbooks.
% Usage in QMD: \lettrine{T}{he first sentence...}
\usepackage{lettrine}
\renewcommand{\LettrineFontHook}{\color{crimson}\bfseries}
\setcounter{DefaultLines}{3}          % Drop cap spans 3 lines
\renewcommand{\DefaultLoversize}{0.1} % Slight oversize for visual weight
\renewcommand{\DefaultLraise}{0}      % No vertical shift
\setlength{\DefaultNindent}{0.5em}    % Indent of continuation text
\setlength{\DefaultSlope}{0pt}        % No slope on continuation

% =============================================================================
% RUNNING HEADERS — Truncation Safety
% =============================================================================
% Long chapter/section titles can overflow the header. These marks truncate
% gracefully so headers stay within the text block.
\renewcommand{\chaptermark}[1]{%
  \markboth{\thechapter.\ #1}{}}
\renewcommand{\sectionmark}[1]{%
  \markright{\thesection\ #1}}

% =============================================================================
% EPIGRAPH ENVIRONMENT
% =============================================================================
% For chapter-opening quotations. Renders as right-aligned italic block
% with attribution in small caps below.
% Usage: \epigraph{Quote text}{Author Name, \textit{Source}}
\newcommand{\bookepigraph}[2]{%
  \vspace{1em}%
  \begin{flushright}%
    \begin{minipage}{0.75\textwidth}%
      \raggedleft\itshape\small #1\\[0.5em]%
      \normalfont\small --- #2%
    \end{minipage}%
  \end{flushright}%
  \vspace{1.5em}%
}

% =============================================================================
% THUMB INDEX TABS
% =============================================================================
% Colored tabs on the outer page edge for quick chapter navigation.
% Each Part gets a different vertical position; chapters within a Part
% share the same tab position. Visible when flipping through the book.
\newcounter{thumbindex}
\setcounter{thumbindex}{0}
\newlength{\thumbtabheight}
\setlength{\thumbtabheight}{16mm}     % Height of each tab
\newlength{\thumbtabwidth}
\setlength{\thumbtabwidth}{8mm}       % Width protruding from edge
\newlength{\thumbtabgap}
\setlength{\thumbtabgap}{1mm}         % Gap between tabs

% Advance to next thumb tab position (call at each \part)
\newcommand{\nextthumb}{%
  \stepcounter{thumbindex}%
}

% Draw the thumb tab on every page (placed in header via fancyhdr)
\newcommand{\drawthumb}{%
  \ifnum\value{thumbindex}>0%
    \begin{tikzpicture}[remember picture,overlay]
      \pgfmathsetmacro{\thumboffset}{%
        20 + (\value{thumbindex}-1) * (16 + 1)}  % mm from top
      \ifodd\value{page}%
        % Odd pages: tab on right edge
        \fill[crimson!80]
          ([yshift=-\thumboffset mm]current page.north east)
          rectangle +(-\thumbtabwidth, -\thumbtabheight);
        \node[white,font=\tiny\bfseries,rotate=90]
          at ([yshift=-\thumboffset mm - 0.5\thumbtabheight,
               xshift=-0.5\thumbtabwidth]current page.north east)
          {\Roman{thumbindex}};
      \else
        % Even pages: tab on left edge
        \fill[crimson!80]
          ([yshift=-\thumboffset mm]current page.north west)
          rectangle +(\thumbtabwidth, -\thumbtabheight);
        \node[white,font=\tiny\bfseries,rotate=-90]
          at ([yshift=-\thumboffset mm - 0.5\thumbtabheight,
               xshift=0.5\thumbtabwidth]current page.north west)
          {\Roman{thumbindex}};
      \fi
    \end{tikzpicture}%
  \fi
}

% Hook into fancyhdr to draw thumb on every content page
\AddToHook{shipout/foreground}{%
  \drawthumb%
}

% =============================================================================
% CROP / BLEED MARKS
% =============================================================================
% For final print submission, uncomment the line below to add crop marks.
% MIT Press production will advise on exact requirements.
% \usepackage[cam,center,width=7.5in,height=10.5in]{crop}

% =============================================================================
% PDF/A ARCHIVAL COMPLIANCE
% =============================================================================
% MIT Press increasingly requires PDF/A for long-term preservation.
% This embeds all fonts and removes transparency.
% Note: pdfx must be loaded early; if it conflicts with hyperref,
% MIT Press production can handle the conversion post-build.
% Uncomment when ready for final submission:
% \usepackage[a-3u]{pdfx}

% =============================================================================
% ENHANCED WIDOW / ORPHAN CONTROL
% =============================================================================
% Prevent single lines at top/bottom of pages and breaks before equations
\clubpenalty=10000          % No orphans (single first line at bottom)
\widowpenalty=10000         % No widows (single last line at top)
\displaywidowpenalty=10000  % No widow before display math
\predisplaypenalty=10000    % No page break just before display math
\postdisplaypenalty=0       % Allow break after display math (natural)
\usepackage{needspace}
\let\Needspace\needspace
\makeatletter
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{plain}
\@ifundefined{c@chapter}{\newfloat{vid}{h}{lovid}}{\newfloat{vid}{h}{lovid}[chapter]}
\floatname{vid}{Video}
\newcommand*\listofvids{\listof{vid}{List of Videos}}
\makeatother
\makeatletter
\@ifpackageloaded{tcolorbox}{}{\usepackage[skins,breakable]{tcolorbox}}
\@ifpackageloaded{fontawesome5}{}{\usepackage{fontawesome5}}
\definecolor{quarto-callout-color}{HTML}{909090}
\definecolor{quarto-callout-note-color}{HTML}{0758E5}
\definecolor{quarto-callout-important-color}{HTML}{CC1914}
\definecolor{quarto-callout-warning-color}{HTML}{EB9113}
\definecolor{quarto-callout-tip-color}{HTML}{00A047}
\definecolor{quarto-callout-caution-color}{HTML}{FC5300}
\definecolor{quarto-callout-color-frame}{HTML}{acacac}
\definecolor{quarto-callout-note-color-frame}{HTML}{4582ec}
\definecolor{quarto-callout-important-color-frame}{HTML}{d9534f}
\definecolor{quarto-callout-warning-color-frame}{HTML}{f0ad4e}
\definecolor{quarto-callout-tip-color-frame}{HTML}{02b875}
\definecolor{quarto-callout-caution-color-frame}{HTML}{fd7e14}
\makeatother
\makeatletter
\@ifpackageloaded{bookmark}{}{\usepackage{bookmark}}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\makeatletter
\@ifpackageloaded{sidenotes}{}{\usepackage{sidenotes}}
\@ifpackageloaded{marginnote}{}{\usepackage{marginnote}}
\makeatother
\newcommand{\fbxIconPath}{assets/images/icons/callouts}
\newcommand{\fbxIconFormat}{pdf}
\makeatletter
\@ifpackageloaded{tcolorbox}{}{\usepackage[many]{tcolorbox}}
\makeatother
%%%% ---foldboxy preamble ----- %%%%%

% Load xstring for string manipulation
\RequirePackage{xstring}

% Icon path and format configuration - can be overridden in filter-metadata
\providecommand{\fbxIconPath}{assets/images/icons/callouts}
\providecommand{\fbxIconFormat}{pdf}

% Helper command to include icon with hyphen-to-underscore conversion
% This ensures consistency: callout-quiz-question -> callout_quiz_question
% Using height= instead of width= ensures consistent header heights across all icons
% regardless of aspect ratio
\newcommand{\fbxIncludeIcon}[2]{%
  \StrSubstitute{#1}{-}{_}[\fbxIconName]%
  \includegraphics[height=#2]{\fbxIconPath/icon_\fbxIconName.\fbxIconFormat}%
}

% Legacy fallback colors (keep for compatibility)
\definecolor{fbx-default-color1}{HTML}{c7c7d0}
\definecolor{fbx-default-color2}{HTML}{a3a3aa}
\definecolor{fbox-color1}{HTML}{c7c7d0}
\definecolor{fbox-color2}{HTML}{a3a3aa}

% arguments: #1 typelabelnummer: #2 titel: #3
\newenvironment{fbx}[3]{%
\begin{tcolorbox}[
  enhanced,
  breakable,
  %fontupper=\fontsize{8pt}{10pt}\selectfont,  % 95% of body text (10pt -> 9.5pt)
  before skip=8pt,  % space above box (increased)
  after skip=8pt,   % space below box (increased)
  attach boxed title to top*={xshift=0pt},
  boxed title style={
  %fuzzy shadow={1pt}{-1pt}{0mm}{0.1mm}{gray},
  arc=1.5pt,
  rounded corners=north,
  sharp corners=south,
  top=6pt,          % Adjusted for ~40px equivalent height
  bottom=5pt,       % Adjusted for ~40px equivalent height
  overlay={
      \node [left,outer sep=0em, black,draw=none,anchor=west,
        rectangle,fill=none,inner sep=0pt]
        at ([xshift=4mm]frame.west) {\fbxIncludeIcon{#1}{4.2mm}};
    },
  },
  colframe=#1-color2,             % Border color (auto-generated from YAML)
  colbacktitle=#1-color1,         % Background color (auto-generated from YAML)
  colback=white,
  coltitle=black,
  titlerule=0mm,
  toprule=0.5pt,
  bottomrule=0.5pt,
  leftrule=2.2pt,
  rightrule=0.5pt,
  outer arc=1.5pt,
  arc=1.5pt,
  left=0.5em,       % increased left padding
  bottomtitle=1.5mm, % increased title bottom margin
  toptitle=1.5mm,    % increased title top margin
  title=\hspace{2.5em}\protect#2\hspace{0.5em}\protect#3, % Protect parameters
  extras middle and last={top=4pt} % increased continuation spacing
]}
{\end{tcolorbox}}


% boxed environment with right border
\newenvironment{fbxSimple}[3]{\begin{tcolorbox}[
  enhanced,
  breakable,
  %fontupper=\fontsize{8pt}{10pt}\selectfont,  % 95% of body text (10pt -> 9.5pt)
  before skip=8pt,  % space above box (increased)
  after skip=8pt,   % space below box (increased)
  attach boxed title to top*={xshift=0pt},
  boxed title style={
  %fuzzy shadow={1pt}{-1pt}{0mm}{0.1mm}{gray},
  arc=1.5pt,
  rounded corners=north,
  sharp corners=south,
  top=6pt,          % Adjusted for ~40px equivalent height
  bottom=5pt,       % Adjusted for ~40px equivalent height
  overlay={
      \node [left,outer sep=0em, black,draw=none,anchor=west,
        rectangle,fill=none,inner sep=0pt]
        at ([xshift=3mm]frame.west) {\fbxIncludeIcon{#1}{4.2mm}};
    },
  },
  colframe=#1-color2,             % Border color (auto-generated from YAML)
  colbacktitle=#1-color1,         % Background color (auto-generated from YAML)
  colback=white,
  coltitle=black,
  titlerule=0mm,
  toprule=0.5pt,
  bottomrule=0.5pt,
  leftrule=2.2pt,
  rightrule=0.5pt,
  outer arc=1.5pt,
  arc=1.5pt,
  left=0.5em,       % increased left padding
  bottomtitle=1.5mm, % increased title bottom margin
  toptitle=1.5mm,    % increased title top margin
  title=\hspace{2.5em}\protect#2\hspace{0.5em}\protect#3, % Protect parameters
  boxsep=1pt,
  extras first={bottom=0pt},
  extras last={top=0pt,bottom=-4pt},
  overlay first={
    \draw[line width=1pt,white] ([xshift=2.2pt]frame.south west)-- ([xshift=-0.5pt]frame.south east);
  },
  overlay last={
    \draw[line width=1pt,white] ([xshift=2.2pt]frame.north west)-- ([xshift=-0.5pt]frame.north east);
   }
]}
{\end{tcolorbox}}

%%%% --- end foldboxy preamble ----- %%%%%
%%==== colors from yaml ===%
\definecolor{callout-notebook-color1}{HTML}{F2F7FF}
\definecolor{callout-notebook-color2}{HTML}{2C5282}
\definecolor{callout-checkpoint-color1}{HTML}{FFF8E7}
\definecolor{callout-checkpoint-color2}{HTML}{D97706}
\definecolor{callout-chapter-connection-color1}{HTML}{EFF6FF}
\definecolor{callout-chapter-connection-color2}{HTML}{1E3A5F}
\definecolor{callout-perspective-color1}{HTML}{F7F8FA}
\definecolor{callout-perspective-color2}{HTML}{4A5568}
\definecolor{callout-resource-exercises-color1}{HTML}{E0F2F1}
\definecolor{callout-resource-exercises-color2}{HTML}{20B2AA}
\definecolor{callout-code-color1}{HTML}{F2F4F8}
\definecolor{callout-code-color2}{HTML}{D1D7E0}
\definecolor{callout-resource-slides-color1}{HTML}{E0F2F1}
\definecolor{callout-resource-slides-color2}{HTML}{20B2AA}
\definecolor{callout-example-color1}{HTML}{F0F8F6}
\definecolor{callout-example-color2}{HTML}{148F77}
\definecolor{callout-definition-color1}{HTML}{F0F4F8}
\definecolor{callout-definition-color2}{HTML}{1B4F72}
\definecolor{callout-takeaways-color1}{HTML}{FDF2F7}
\definecolor{callout-takeaways-color2}{HTML}{BE185D}
\definecolor{callout-lighthouse-color1}{HTML}{FDF8E6}
\definecolor{callout-lighthouse-color2}{HTML}{B8860B}
\definecolor{callout-colab-color1}{HTML}{FFF5E6}
\definecolor{callout-colab-color2}{HTML}{FF6B35}
\definecolor{callout-quiz-answer-color1}{HTML}{E8F2EA}
\definecolor{callout-quiz-answer-color2}{HTML}{4A7C59}
\definecolor{callout-principle-color1}{HTML}{F3F2FA}
\definecolor{callout-principle-color2}{HTML}{3D3B8E}
\definecolor{callout-theorem-color1}{HTML}{F5F0FA}
\definecolor{callout-theorem-color2}{HTML}{6B46C1}
\definecolor{callout-resource-videos-color1}{HTML}{E0F2F1}
\definecolor{callout-resource-videos-color2}{HTML}{20B2AA}
\definecolor{callout-quiz-question-color1}{HTML}{F0F0F8}
\definecolor{callout-quiz-question-color2}{HTML}{5B4B8A}
%=============%

\usepackage{hyphenat}
\usepackage{ifthen}
\usepackage{calc}
\usepackage{calculator}



\usepackage{graphicx}
\usepackage{geometry}
\usepackage{afterpage}
\usepackage{tikz}
\usetikzlibrary{calc}
\usetikzlibrary{fadings}
\usepackage[pagecolor=none]{pagecolor}


% Set the titlepage font families







% Set the coverpage font families

\usepackage{bookmark}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  pdftitle={Introduction to Machine Learning Systems},
  pdfauthor={Vijay Janapa Reddi},
  colorlinks=true,
  linkcolor={Maroon},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}


\title{Introduction to Machine Learning Systems}
\author{Vijay Janapa Reddi}
\date{February 2, 2026}
\begin{document}
%%%%% begin titlepage extension code

  \begin{frontmatter}

\begin{titlepage}
% This is a combination of Pandoc templating and LaTeX
% Pandoc templating https://pandoc.org/MANUAL.html#templates
% See the README for help

\thispagestyle{empty}

\newgeometry{top=-100in}

% Page color

\newcommand{\coverauthorstyle}[1]{{\fontsize{20}{24.0}\selectfont
{#1}}}

\begin{tikzpicture}[remember picture, overlay, inner sep=0pt, outer sep=0pt]

\tikzfading[name=fadeout, inner color=transparent!0,outer color=transparent!100]
\tikzfading[name=fadein, inner color=transparent!100,outer color=transparent!0]
\node[anchor=south west, rotate=0, opacity=1] at ($(current page.south west)+(0.225\paperwidth, 9)$) {
\includegraphics[width=\paperwidth, keepaspectratio]{assets/images/covers/cover-image-transparent-vol1.png}};

% Title
\newcommand{\titlelocationleft}{0.075\paperwidth}
\newcommand{\titlelocationbottom}{0.4\paperwidth}
\newcommand{\titlealign}{left}

\begin{scope}{%
\fontsize{52}{62.4}\selectfont
\node[anchor=north
west, align=left, rotate=0] (Title1) at ($(current page.south west)+(\titlelocationleft,\titlelocationbottom)$)  [text width = 0.9\paperwidth]  {{\nohyphens{Machine
Learning Systems}}};
}
\end{scope}

% Author
\newcommand{\authorlocationleft}{.925\paperwidth}
\newcommand{\authorlocationbottom}{0.175\paperwidth}
\newcommand{\authoralign}{right}

\begin{scope}
{%
\fontsize{20}{24.0}\selectfont
\node[anchor=north
east, align=right, rotate=0] (Author1) at ($(current page.south west)+(\authorlocationleft,\authorlocationbottom)$)  [text width = 6in]  {\coverauthorstyle{Vijay
Janapa Reddi\\}};
}
\end{scope}

% Footer
\newcommand{\footerlocationleft}{0.075\paperwidth}
\newcommand{\footerlocationbottom}{0.475\paperwidth}
\newcommand{\footerlocationalign}{left}

\begin{scope}
{%
\fontsize{25}{30.0}\selectfont
 \node[anchor=north west, align=left, rotate=0] (Footer1) at %
($(current page.south west)+(\footerlocationleft,\footerlocationbottom)$)  [text width = 0.9\paperwidth]  {{\nohyphens{Introduction
to}}};
}
\end{scope}

\end{tikzpicture}
\clearpage
\restoregeometry
%%% TITLE PAGE START

% Set up alignment commands
%Page
\newcommand{\titlepagepagealign}{
\ifthenelse{\equal{left}{right}}{\raggedleft}{}
\ifthenelse{\equal{left}{center}}{\centering}{}
\ifthenelse{\equal{left}{left}}{\raggedright}{}
}


\newcommand{\titleandsubtitle}{
% Title and subtitle
{{\huge{\bfseries{\nohyphens{Introduction to Machine Learning
Systems}}}}\par
}%
}
\newcommand{\titlepagetitleblock}{
\titleandsubtitle
}

\newcommand{\authorstyle}[1]{{\large{#1}}}

\newcommand{\affiliationstyle}[1]{{\large{#1}}}

\newcommand{\titlepageauthorblock}{
{\authorstyle{\nohyphens{Vijay Janapa
Reddi}{\textsuperscript{1}}\textsuperscript{,}{\textsuperscript{,*}}}}}

\newcommand{\titlepageaffiliationblock}{
\hangindent=1em
\hangafter=1
{\affiliationstyle{
{1}.~Harvard University


\vspace{1\baselineskip}
* \textit{Correspondence:}~Vijay Janapa Reddi~vj@eecs.harvard.edu
}}
}
\newcommand{\headerstyled}{%
{}
}
\newcommand{\footerstyled}{%
{\large{}}
}
\newcommand{\datestyled}{%
{February 2, 2026}
}


\newcommand{\titlepageheaderblock}{\headerstyled}

\newcommand{\titlepagefooterblock}{
\footerstyled
}

\newcommand{\titlepagedateblock}{
\datestyled
}

%set up blocks so user can specify order
\newcommand{\titleblock}{{

{\titlepagetitleblock}
}

\vspace{4\baselineskip}
}

\newcommand{\authorblock}{{\titlepageauthorblock}

\vspace{2\baselineskip}
}

\newcommand{\affiliationblock}{{\titlepageaffiliationblock}

\vspace{0pt}
}

\newcommand{\logoblock}{}

\newcommand{\footerblock}{}

\newcommand{\dateblock}{{\titlepagedateblock}

\vspace{0pt}
}

\newcommand{\headerblock}{}

\thispagestyle{empty} % no page numbers on titlepages


\newcommand{\vrulecode}{\textcolor{black}{\rule{\vrulewidth}{\textheight}}}
\newlength{\vrulewidth}
\setlength{\vrulewidth}{2pt}
\newlength{\B}
\setlength{\B}{\ifdim\vrulewidth > 0pt 0.05\textwidth\else 0pt\fi}
\newlength{\minipagewidth}
\ifthenelse{\equal{left}{left} \OR \equal{left}{right} }
{% True case
\setlength{\minipagewidth}{\textwidth - \vrulewidth - \B - 0.1\textwidth}
}{
\setlength{\minipagewidth}{\textwidth - 2\vrulewidth - 2\B - 0.1\textwidth}
}
\ifthenelse{\equal{left}{left} \OR \equal{left}{leftright}}
{% True case
\raggedleft % needed for the minipage to work
\vrulecode
\hspace{\B}
}{%
\raggedright % else it is right only and width is not 0
}
% [position of box][box height][inner position]{width}
% [s] means stretch out vertically; assuming there is a vfill
\begin{minipage}[b][\textheight][s]{\minipagewidth}
\titlepagepagealign
\titleblock

Prof.~Vijay Janapa Reddi

School of Engineering and Applied Sciences

Harvard University

\vspace{80mm}

With heartfelt gratitude to the community for their invaluable
contributions and steadfast support.

\vfill

February 2, 2026

\vfill
\par

\end{minipage}\ifthenelse{\equal{left}{right} \OR \equal{left}{leftright} }{
\hspace{\B}
\vrulecode}{}
\clearpage
%%% TITLE PAGE END
\end{titlepage}
\setcounter{page}{1}
\end{frontmatter}

%%%%% end titlepage extension code

% =============================================================================
% HALF-TITLE PAGE (Volume I)
% =============================================================================
% Standard academic book sequence: half-title -> blank -> title page -> copyright
% The half-title shows only the book title -- no author, no publisher, no date.
\thispagestyle{empty}
\begin{center}
\vspace*{0.3\textheight}
{\fontsize{24pt}{28pt}\selectfont\bfseries\color{crimson} Introduction to\\[0.4em] Machine Learning Systems}\\[2em]
{\large\itshape Volume~I}
\vfill
\end{center}
\clearpage
\thispagestyle{empty}\null\clearpage  % Blank verso (back of half-title)

\renewcommand*\contentsname{Table of contents}
{
\hypersetup{linkcolor=}
\setcounter{tocdepth}{2}
\tableofcontents
}
\listoffigures
\listoftables

\mainmatter
\bookmarksetup{startatroot}

\chapter*{Welcome to Volume I}\label{welcome-to-volume-i}
\addcontentsline{toc}{chapter}{Welcome to Volume I}

\markboth{Welcome to Volume I}{Welcome to Volume I}

\bookmarksetup{startatroot}

\chapter{Data Selection}\label{sec-data-selection}

\marginnote{\begin{footnotesize}

\emph{DALL·E 3 Prompt: A futuristic digital illustration depicting the
concept of data selection in machine learning. On one side of the image,
there is a sleek, powerful computing unit, symbolizing AI processing. On
the other side, streams of binary code (1s and 0s) flow into the
computer, but the data is represented with glowing golden elements,
signifying valuable, high-quality information. The background has a
high-tech, digital ambiance, emphasizing the role of refined, efficient
data in machine learning. No text, only a strong visual representation
of the relationship between computation and valuable data.}

\end{footnotesize}}

\noindent
\pandocbounded{\includegraphics[keepaspectratio]{contents/vol1/data_selection/images/png/cover_data_efficiency.png}}

\section*{Purpose}\label{purpose}
\addcontentsline{toc}{section}{Purpose}

\markright{Purpose}

\emph{Why can a carefully selected 10\% of your data match the accuracy
of 100\%?}

The optimization journey begins upstream, before a single gradient is
computed. While later chapters address how to compress models and
accelerate hardware, the most leveraged optimization operates on the
data itself. Naive scaling assumes data is homogeneous, that every
sample contributes equally to learning. Reality differs dramatically: in
large-scale datasets, a tiny fraction of examples provides the majority
of the gradient signal while the vast majority are redundant, noisy, or
misaligned with the target distribution. This heterogeneity is not a
statistical curiosity but a systems optimization opportunity. Data
selection reduces the fundamental workload required to train models
rather than merely speeding up its execution. This shifts the paradigm
from accumulating data as a massive liability to optimizing it as a
precise resource, where the savings compound through every subsequent
stage of the pipeline.

\begin{tcolorbox}[enhanced jigsaw, toprule=.15mm, arc=.35mm, titlerule=0mm, bottomrule=.15mm, rightrule=.15mm, colbacktitle=quarto-callout-tip-color!10!white, colframe=quarto-callout-tip-color-frame, breakable, leftrule=.75mm, bottomtitle=1mm, coltitle=black, toptitle=1mm, colback=white, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Learning Objectives}, left=2mm, opacityback=0, opacitybacktitle=0.6]

\begin{itemize}
\tightlist
\item
  Explain data selection as the third pillar of ML optimization
  alongside algorithms and systems
\item
  Apply the Information-Compute Ratio (ICR) framework to evaluate
  dataset value
\item
  Compare coreset and deduplication techniques for pre-training data
  reduction
\item
  Implement the three-stage optimization pipeline: static pruning,
  dynamic selection, and synthetic generation
\item
  Design curriculum learning and active learning strategies for
  training-time optimization
\item
  Analyze cost-benefit trade-offs and systems engineering challenges in
  data selection pipelines
\end{itemize}

\end{tcolorbox}

\section{Data Selection
Fundamentals}\label{sec-data-selection-fundamentals}

Data selection asks a simple question with profound engineering
consequences: given a clean, well-engineered dataset, which examples
contribute the most learning per unit of compute cost? The preceding
chapter on data engineering (\textbf{?@sec-data-engineering-ml})
established the infrastructure for collecting, cleaning, and preparing
data, producing pipelines that ingest raw signals and yield
well-governed, versioned datasets ready for training. That chapter
ensured data \emph{quality} through correct labels, consistent schemas,
and clean records. Data selection optimizes data \emph{value} by
extracting maximum learning from minimum samples. The distinction is
fundamental: quality asks \emph{whether} data is correct, while value
asks \emph{whether} correct data is worth the compute spent processing
it.

For decades, the dominant strategy was straightforward: more data,
better models. Scaling laws (\citeproc{ref-kaplan2020scaling}{Kaplan et
al. 2020}; \citeproc{ref-hoffmann2022training}{Hoffmann et al. 2022})
confirmed that model performance improves predictably with dataset size,
and teams responded rationally by scraping more web pages, labeling more
images, and generating more synthetic examples. A fundamental asymmetry
has since emerged. Hardware acceleration has outpaced the growth of
high-quality data. GPU compute capacity has increased faster than
traditional Moore's Law projections, with AI-specific workloads seeing
particularly rapid gains, while the supply of novel, high-quality
human-generated text and images grows at roughly 2\(\times\) per decade.
The internet has already been scraped. Domain experts cannot label
faster. This asymmetry, which researchers call the \textbf{Data
Wall}\sidenote{\textbf{Data Wall}: A term popularized by Epoch AI
researchers in 2022. Their analysis projected that high-quality language
data (books, academic papers, filtered web text) could be exhausted
within one to two decades at then-current scaling rates. The ``wall''
metaphor emphasizes that unlike compute (which can be purchased) or
algorithms (which can be improved), the stock of human-generated
training data grows slowly and may represent a fundamental constraint on
scaling. } (\citeproc{ref-villalobos2022will}{Villalobos et al. 2022}),
has inverted the optimization priority from ``get more data'' to ``get
more from existing data.''

Figure~\ref{fig-running-out-of-human-data} illustrates this trajectory:
foundation models are consuming the stock of human-generated text at an
accelerating rate, with projections suggesting exhaustion of
high-quality public data on a timeline measured in years, not decades.
This is not a distant concern. It shapes training strategies today.

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{contents/vol1/data_selection/images/png/running_out_of_data.png}}

}

\caption{\label{fig-running-out-of-human-data}\textbf{Dataset Growth
Approaching Limits}: Foundation models are increasingly trained on vast
datasets, approaching the total stock of human-generated text. Current
projections suggest that high-quality public text data faces exhaustion
on a near-term horizon, forcing a shift toward data selection, synthetic
generation, and multimodal learning. Source: Sevilla et al.
(\citeproc{ref-villalobos_ho_sevilla_besiroglu_heim_hobbhahn_2024}{2022}).}

\end{figure}%

The quantitative evidence for this scaling asymmetry is stark.
Table~\ref{tbl-scaling-asymmetry} compares growth rates across key
resources: compute budgets grow approximately 10\(\times\) every three
years, while high-quality text grows roughly 2\(\times\) every five
years. The gap between what compute can process and what quality data
exists is widening, making intelligent data selection increasingly
critical.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2421}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.1579}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.6000}}@{}}
\caption{\textbf{Scaling Asymmetry in ML Resources.} Compute grows
exponentially while high-quality data grows linearly or sub-linearly,
creating an increasing compute-to-data imbalance that makes data
selection essential.}\label{tbl-scaling-asymmetry}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Resource}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Growth Rate}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Implication}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Resource}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Growth Rate}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Implication}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{GPU Compute} & \textasciitilde10× / 3 years & Hardware vendors
deliver reliable exponential gains \\
\textbf{Training Data (Web)} & \textasciitilde2× / 5 years &
High-quality web text is finite; much already scraped \\
\textbf{Labeled Data} & \textasciitilde1.5× / 5 years & Human annotation
throughput is fundamentally bounded \\
\textbf{Synthetic Data} & Unbounded & But bounded by generator quality
(risk of model collapse) \\
\end{longtable}

\phantomsection\label{callout-notebookux2a-1.1}
\begin{fbxSimple}{callout-notebook}{AI Engineer’s Notebook:}{The Scaling Asymmetry}
\phantomsection\label{callout-notebook*-1.1}
\textbf{The Problem}: Compute scales exponentially. Data does not
(Table~\ref{tbl-scaling-asymmetry}).

\textbf{The Consequence}: Compute budgets now support training runs that
far exceed what available high-quality data can fill. The field has
become \emph{compute-rich and data-poor}.

\end{fbxSimple}

This asymmetry inverts the optimization priority. When data was abundant
and compute was scarce, the right strategy was algorithmic efficiency:
squeeze more accuracy from limited GPU cycles. Now that compute is
abundant and \emph{quality data} is scarce, the winning strategy is
\textbf{data selection}: squeeze more learning from each sample. Data
selection operates upstream of all other optimizations. By pruning
redundancy and selecting high-value samples, we reduce the workload
before it ever enters the model or hits the hardware. In the Iron Law of
Efficiency (\(T = \frac{D_{vol}}{BW} + \frac{O}{R_{peak} \cdot \eta}\)),
this is the only technique that shrinks the \emph{numerator} of the
first term. Companies training frontier models are no longer
bottlenecked by GPU access but by the quality and diversity of their
training corpora.

This chapter provides the engineering toolkit for intelligent data
selection, organized around Part III's \textbf{DAM Taxonomy}
(\textbf{?@sec-optimize-invariants}), which establishes a deliberate
optimization ordering: Data first, then Algorithm, then Machine. Data
selection puts the ``highest leverage first'' principle into practice by
addressing whether work is necessary before asking how to simplify or
accelerate it. The chapter follows a three-stage optimization pipeline
that structures the practical response to the Data Wall:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Static Data Pruning}: Removing low-value samples before
  training begins (coresets, deduplication).
\item
  \textbf{Dynamic Selection}: Selecting high-value samples during
  training (curriculum learning, active learning).
\item
  \textbf{Synthetic Generation}: Creating high-value samples on demand
  (augmentation, distillation).
\end{enumerate}

Each stage increases the \emph{information density} of the data that
reaches the model, and together they form a complementary toolkit:
pruning reduces \emph{what} you have, selection focuses \emph{how} you
use it, and synthesis expands \emph{what} you can access. The next two
sections formalize \emph{what} ``data selection'' means and \emph{why}
it is fundamentally a systems problem.

\section{Defining Data Selection}\label{sec-data-selection-defining}

The Data Wall establishes the urgency; a formal framework for measuring
data selection establishes the response. What exactly does ``efficient''
use of data mean?

\phantomsection\label{callout-definitionux2a-1.2}
\begin{fbxSimple}{callout-definition}{Definition:}{Data Selection}
\phantomsection\label{callout-definition*-1.2}
\textbf{\emph{Data Selection}} is the process of maximizing the
\textbf{Information-Compute Ratio}. It operates upstream of training to
identify the \textbf{Minimum Viable Subset} of data required to define
the decision boundary, reducing the \textbf{Total Operations} (\(O\))
term of the Iron Law by eliminating redundant, noisy, or non-informative
samples before they consume GPU cycles.

\[
\text{Selection Efficiency} = \frac{\Delta \text{Model Capability}}{\Delta \text{Data Cost}}
\]

where Data Cost encompasses:

\begin{itemize}
\tightlist
\item
  \textbf{Acquisition cost}: Time and money to collect or generate
  samples
\item
  \textbf{Labeling cost}: Human expert annotation effort
\item
  \textbf{Storage cost}: Bytes required to persist the dataset
\item
  \textbf{Compute cost}: FLOPs to process samples during training
\end{itemize}

A perfectly efficient dataset would contain only samples that contribute
unique information to the model's decision boundary: no redundancy, no
noise, no ``easy'' examples already mastered.

\end{fbxSimple}

To make this concrete, consider training a model in the
\textbf{GPT-2/Llama Lighthouse} family
(\textbf{?@sec-dnn-architectures}), a 70B parameter language model:

\begin{itemize}
\tightlist
\item
  \textbf{Compute available}: 10,000 H100 GPUs for 3 months represents
  tens of millions of dollars in compute budget, capable of processing
  over 10 trillion tokens
\item
  \textbf{High-quality data available}: \textasciitilde5 trillion tokens
  of deduplicated, filtered web text
\item
  \textbf{The gap}: 3× more compute than data can utilize
\end{itemize}

The team faces a choice: (1) train on the same data multiple epochs
(diminishing returns after epochs 2--3), (2) lower quality thresholds to
include more data (degrades model quality), or (3) invest in data
selection through better filtering, curriculum design, and synthetic
augmentation to extract more learning from each token. Option 3 is
increasingly the dominant approach.

This data selection imperative applies across model architectures,
though the bottlenecks differ. Unlike our compute-bound ResNet-50
Lighthouse, GPT-2/Llama models are \textbf{memory bandwidth-bound}
during inference but still benefit enormously from data selection during
training. Each token processed requires the same forward/backward pass
cost regardless of model bottleneck, so fewer tokens means fewer FLOPs.

\section{Data Selection as a Systems
Problem}\label{sec-data-selection-data-selection-systems-problem-d857}

The Data Wall establishes \emph{why} data selection matters; the
question is \emph{how} to approach it. Data selection is typically
framed as a machine learning problem: \emph{how do I achieve the same
accuracy with fewer samples?} This framing focuses on statistical sample
complexity and generalization theory. While valid, it misses the larger
picture.

In this textbook, we adopt a \textbf{systems framing}: \emph{how do I
reduce the total cost of achieving target performance across the entire
ML lifecycle?} This shifts attention from accuracy curves to resource
consumption, as Table~\ref{tbl-ml-vs-systems-framing} illustrates.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 2\tabcolsep) * \real{0.4625}}
  >{\raggedright\arraybackslash}p{(\linewidth - 2\tabcolsep) * \real{0.5375}}@{}}
\caption{\textbf{ML vs.~Systems Perspectives on Data Selection.} The ML
framing optimizes sample complexity; the systems framing optimizes total
resource cost across the
pipeline.}\label{tbl-ml-vs-systems-framing}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{ML Framing}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Systems Framing}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{ML Framing}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Systems Framing}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{``Fewer samples for same accuracy''} & ``Fewer FLOPs for same
accuracy'' \\
\textbf{``Better generalization''} & ``Lower training cost (time, money,
energy)'' \\
\textbf{``Sample complexity bounds''} & ``End-to-end resource
efficiency'' \\
\textbf{``Learning theory''} & ``Cost engineering'' \\
\end{longtable}

The systems framing reveals optimization opportunities invisible to the
ML framing. To see \emph{why}, consider \emph{how} data selection
interacts with the Iron Law introduced in
\textbf{?@sec-ml-system-architecture}.

\phantomsection\label{callout-perspectiveux2a-1.3}
\begin{fbxSimple}{callout-perspective}{Systems Perspective:}{Data Selection and the Iron Law}
\phantomsection\label{callout-perspective*-1.3}
In the \textbf{Iron Law of ML Systems}
(\(T = \frac{D_{vol}}{BW} + \frac{O}{R_{peak} \cdot \eta} + L_{lat}\)),
data selection is the only technique that reduces the \emph{Total
Operations} term at its source. Model compression reduces operations per
sample; hardware acceleration increases throughput per operation. But
data selection reduces the number of samples processed entirely.

\begin{itemize}
\tightlist
\item
  \textbf{Model compression}: Reduces \(O\) per forward/backward pass
\item
  \textbf{Hardware acceleration}: Increases \(R_{peak}\) (peak
  throughput) and \(\eta\) (utilization)
\item
  \textbf{Data selection}: Reduces the number of passes through the
  entire equation
\end{itemize}

This makes data selection multiplicatively valuable: a 2× reduction in
dataset size with 2× model compression and 2× hardware acceleration
yields 8× total cost reduction, not 6×.

\end{fbxSimple}

Consider training cost reduction: a 50\% reduction in dataset size does
not merely improve sample efficiency; it directly halves the number of
forward passes, backward passes, and gradient updates. For a \$100M
training run, this translates to \$50M in compute savings. The
relationship is linear and immediate.

These compute savings cascade into storage and I/O costs. Large datasets
consume petabytes of storage and saturate network bandwidth during
distributed training. Data selection techniques like deduplication
reduce storage costs and eliminate I/O bottlenecks that can idle
expensive GPU clusters.

Data selection also transforms labeling economics. Expert labeling costs
(\$5--100+ per sample in domains like medical imaging) often exceed
compute costs. Active learning and semi-supervised methods are not
merely algorithmic techniques but cost engineering tools that can reduce
labeling budgets by 10--100\(\times\).

The environmental implications are substantial. Training a large
language model can emit hundreds of tons of CO₂. Data selection is the
most direct lever for Green AI: halving the dataset halves training
energy, with no accuracy trade-off if done correctly.

Smaller, curated datasets also enable faster iteration velocity. A team
that can iterate in hours rather than days has a compounding advantage
in model development.

These cascading benefits illustrate a broader point about how systems
engineers think differently about data than ML researchers.

\phantomsection\label{callout-perspectiveux2a-1.4}
\begin{fbxSimple}{callout-perspective}{Systems Perspective:}{The Systems Engineer's View of Data}
\phantomsection\label{callout-perspective*-1.4}
\textbf{The ML Researcher asks:} ``What is the sample complexity of this
learning problem?''

\textbf{The Systems Engineer asks:} ``What is the
cost-per-accuracy-point across the entire pipeline, from data
acquisition through deployment?''

This chapter equips you with the systems engineer's toolkit: techniques
to minimize total cost, metrics to quantify efficiency gains, and
architectural patterns to implement data selection at scale.

\end{fbxSimple}

\section{The Information-Compute
Ratio}\label{sec-data-selection-informationcompute-ratio-8e9b}

The systems framing established above calls for a quantitative metric.
The Optimize Principles (\textbf{?@sec-optimize-invariants}) introduced
the \textbf{Pareto Frontier} as the boundary where improving one metric
necessarily degrades another, and identified three pillars of efficiency
following the DAM Taxonomy: Data, Algorithm (model compression,
\textbf{?@sec-model-compression}), and Machine (hardware acceleration,
\textbf{?@sec-ai-acceleration}). As the first pillar in the D-A-M
ordering, data selection addresses the most fundamental question: can we
reduce the work before it begins? We formalize this with a central
metric: the Information-Compute Ratio.

While model compression and hardware acceleration focus on the
\emph{execution} of the math, \emph{Data Selection} reduces the
\emph{amount} of math required by optimizing what enters the training
pipeline.

Data engineering (\textbf{?@sec-data-engineering-ml}) ensures that data
is clean, accessible, and correctly formatted. Data selection asks a
different question: \emph{how much information does each sample
contribute to the model's learning per unit of computation?}

In the optimization triad (Figure~\ref{fig-optimization-triad}), data
selection plays the role of \emph{Input Optimization}. While model
compression minimizes the math per parameter and hardware acceleration
maximizes the math per second, data selection minimizes the total math
required to reach convergence.

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/06fba62111d081d586596b9226c3c15b084ad956.pdf}}

}

\caption{\label{fig-optimization-triad}\textbf{The Optimization Triad}:
Machine learning performance relies on three pillars: Algorithms
(models), Systems (hardware/software), and Data Selection. While
algorithms and systems have traditionally received the most attention,
optimizing data selection (Input Optimization) offers a third, powerful
lever for scaling performance.}

\end{figure}%

We can formalize this as the \textbf{Information-Compute Ratio (ICR)}:

\[
\text{ICR} = \frac{\Delta \text{Model Performance}}{\Delta \text{FLOPs}}
\]

\textbf{The Iron Law Connection}: In the Iron Law of Training
Performance (\(T = \frac{O}{R_{peak} \cdot \eta}\)), the \textbf{Total
Operations (\(O\))} term is usually treated as a fixed constant
determined by model architecture and dataset size. \textbf{Data
Selection turns \(O\) into a variable.} By maximizing ICR, we reduce the
total FLOPs required to reach a target performance level, directly
shrinking the numerator of the Iron Law equation. A 2x improvement in
ICR is mathematically equivalent to a 2x improvement in hardware Peak
Throughput (\(R_{peak}\)), but often much cheaper to achieve.

A random batch of raw data often has low ICR because it contains
redundant examples, noisy samples, or ``easy'' examples the model has
already mastered. Training on such a batch wastes GPU cycles on
zero-information updates. High-efficiency data pipelines
(Figure~\ref{fig-data-selection-pipeline}) filter, order, and synthesize
data to maximize ICR, ensuring that every FLOP contributes to learning.
To illustrate, consider \emph{computing ICR} on a concrete coreset
selection task. Later in this chapter,
Section~\ref{sec-data-selection-measuring-data-selection-7957} provides
the complete measurement framework for evaluating these efficiency
gains, including the Data Roofline model that diagnoses whether a system
is data-bound or compute-bound.

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/46168e9b390a613ad07e8ad91a0cfa56994ae5b2.pdf}}

}

\caption{\label{fig-data-selection-pipeline}\textbf{The Data Selection
Pipeline}: A structured approach to increasing data value. Raw data is
first pruned to remove redundancy (Static Pruning), then dynamically
selected during training (Active Learning), and finally augmented to
increase diversity (Synthesis). Each stage increases the
Information-Compute Ratio (ICR).}

\end{figure}%

Before diving into calculation examples, ensure you have a solid grasp
of the core ICR concept.

\phantomsection\label{callout-checkpointux2a-1.5}
\begin{fbxSimple}{callout-checkpoint}{Checkpoint:}{Data Selection Efficiency}
\phantomsection\label{callout-checkpoint*-1.5}

The goal of data selection is to maximize the
\textbf{Information-Compute Ratio (ICR)}.

\textbf{Metrics}

\begin{itemize}
\tightlist
\item[$\square$]
  \textbf{ICR}: Can you define ICR as
  \(\Delta \text{Performance} / \Delta \text{FLOPs}\)?
\item[$\square$]
  \textbf{Data Efficiency}: Do you understand why a 50\% smaller dataset
  with 2x higher ICR yields the same model for half the training cost?
\end{itemize}

\textbf{The Pipeline}

\begin{itemize}
\tightlist
\item[$\square$]
  \textbf{The Three Stages}: Can you map Static Pruning, Dynamic
  Selection, and Synthetic Generation to the training lifecycle?
\end{itemize}

\end{fbxSimple}

To make the Information-Compute Ratio concrete, consider how coreset
selection improves training efficiency on a real workload.

\phantomsection\label{callout-exampleux2a-1.6}
\begin{fbxSimple}{callout-example}{Example:}{Computing ICR: Coresets}
\phantomsection\label{callout-example*-1.6}
\textbf{Scenario}: Training our \textbf{ResNet-50 Lighthouse model}
(\textbf{?@sec-dnn-architectures}) on ImageNet for one epoch. We compare
random batch selection versus EL2N-based coreset selection. ResNet-50's
compute-bound nature (high \textbf{arithmetic intensity}) makes it an
ideal candidate for data selection optimization: reducing dataset size
directly reduces training FLOPs with minimal I/O impact.

\textbf{Setup}:

\begin{itemize}
\tightlist
\item
  Dataset: ImageNet (1.28M images)
\item
  Model: ResNet-50 Lighthouse (\textasciitilde4.1 GFLOPs per forward
  pass, \textasciitilde8.2 GFLOPs forward + backward)
\item
  One epoch: 1.28M × 8.2 GFLOPs = \textbf{1.05e+16 FLOPs}
\item
  Accuracy improvement per epoch (early training): \textasciitilde5\%
  points
\end{itemize}

\textbf{Random Selection (baseline)}:

\begin{itemize}
\tightlist
\item
  Process all 1.28M samples uniformly
\item
  Accuracy gain: 5.0 percentage points
\item
  ICR\_random = 5.0 / (1.05e+16) = \textbf{4.8e-16 per FLOP}
\end{itemize}

\textbf{EL2N Coreset (50\% of data)}:

\begin{itemize}
\tightlist
\item
  Process 640K high-uncertainty samples selected by EL2N scoring
\item
  Coreset focuses on decision boundary samples
\item
  Accuracy gain: 4.5 percentage points (90\% of full data performance)
\item
  Compute: 640K × 8.2 GFLOPs = \textbf{5.2e+15 FLOPs}
\item
  ICR\_coreset = 4.5 / (5.2e+15) = \textbf{8.6e-16 per FLOP}
\end{itemize}

\textbf{Result}: The coreset achieves \textbf{1.8× higher ICR}, nearly
twice the learning per FLOP, by eliminating low-information ``easy''
samples that contribute little to the decision boundary. The 0.5
percentage point accuracy difference is often acceptable given the 50\%
compute savings.

\end{fbxSimple}

This chapter explores three strategies to maximize this ratio:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Static Data Pruning}: Removing low-value samples before
  training begins (Coresets, Deduplication).
\item
  \textbf{Dynamic Selection}: Selecting high-value samples during
  training (Curriculum Learning, Active Learning).
\item
  \textbf{Synthetic Generation}: Creating high-value samples on demand
  (Augmentation, Distillation).
\end{enumerate}

We begin with static pruning, the techniques that can reduce your
dataset by 30 to 50 percent before you even start training.

\section{Static Data Pruning: Pre-Training
Filtration}\label{sec-data-selection-static-data-pruning-pretraining-filtration-d6e6}

Before a single gradient is computed, significant efficiency gains are
available by removing low-value samples from the dataset. This
pre-training filtration reduces total computation without affecting, and
sometimes improving, final model accuracy. The techniques in this
section operate on the dataset itself, requiring no changes to the
training loop or model architecture.

\subsection{The Case for Smaller
Datasets}\label{sec-data-selection-case-smaller-datasets-0336}

The most counterintuitive finding in data selection is that training on
\emph{less} data often produces models just as accurate as training on
the full dataset. Practitioners have long assumed that more data yields
better performance, and while this holds in many scenarios, it obscures
a critical reality: typical large-scale datasets contain massive
redundancy. Empirical studies on coreset selection and data pruning have
consistently demonstrated this redundancy across standard benchmarks:

\begin{itemize}
\tightlist
\item
  \textbf{CIFAR-10}: Studies using gradient-based selection (EL2N,
  GraNd) (\citeproc{ref-paul2021deep}{Paul, Ganguli, and Dziugaite
  2021}) have shown that training on 50\% of CIFAR-10 with carefully
  selected samples achieves identical accuracy to the full dataset.
  Aggressive pruning can reach 10--30\% of samples while matching 90\%+
  of original performance.
\item
  \textbf{ImageNet-1K}: Pruning is harder on less redundant datasets.
  However, researchers have demonstrated that 20--30\% of ImageNet can
  be pruned with negligible loss, and up to 50\% reduction is possible
  with a small accuracy trade-off (\textasciitilde1\% point), yielding
  2\(\times\) fewer training FLOPs (\citeproc{ref-paul2021deep}{Paul,
  Ganguli, and Dziugaite 2021};
  \citeproc{ref-sorscher2022beyond}{\textbf{sorscher2022beyond?}}).
\item
  \textbf{Large Language Model Corpora}: Web-scraped datasets like The
  Pile and C4 contain substantial exact and near-duplicate content.
  Deduplication studies (\citeproc{ref-lee2022deduplicating}{Lee et al.
  2021}) report 10--30\% redundancy ratios, with deduplicated training
  yielding \emph{better} downstream performance (less memorization, more
  generalization).
\end{itemize}

These numbers are benchmark-specific. Gains from pruning depend on the
dataset's intrinsic redundancy, the selection algorithm, and the model
architecture; always validate on your specific task before deploying
aggressive pruning in production. The key insight remains: not all data
points provide equal value for training. The following analysis
quantifies what we call \emph{the data quality multiplier}.

\phantomsection\label{callout-notebookux2a-1.7}
\begin{fbxSimple}{callout-notebook}{AI Engineer’s Notebook:}{The Data Quality Multiplier}
\phantomsection\label{callout-notebook*-1.7}
\textbf{The Physics of Noise}: Why is one clean sample worth 100 noisy
ones?

\textbf{The Math}: Learning theory tells us that convergence rates
depend on label noise. 1. \textbf{Clean Data}: Convergence rate is
typically \(O(1/N)\). To halve the error, you need \textbf{2x} data. 2.
\textbf{Noisy Data}: Convergence rate drops to \(O(1/\sqrt{N})\). To
halve the error, you need \textbf{4x} data.

\textbf{The Multiplier}: To reach a target error \(\epsilon\):

\begin{itemize}
\tightlist
\item
  \(N_{clean} \propto 1/\epsilon\)
\item
  \(N_{noisy} \propto 1/\epsilon^2\)
\end{itemize}

\textbf{Example}: For target error \(\epsilon = 0.01\) (1\%):

\begin{itemize}
\tightlist
\item
  \(N_{clean} \approx 100\)
\item
  \(N_{noisy} \approx 10,000\)
\item
  \textbf{Ratio}: \(100\times\) more data required if noisy.
\end{itemize}

\textbf{The Systems Conclusion}: Cleaning your data (removing label
noise) is a \textbf{100x compute accelerator}.

\end{fbxSimple}

The practical question then becomes: \emph{how} do we identify which
samples to keep?

\subsection{Coreset Selection
Algorithms}\label{sec-data-selection-coreset-selection-algorithms-a520}

\textbf{Coreset selection}\sidenote{\textbf{Coreset}: The term
``coreset'' combines ``core'' and ``set,'' reflecting its purpose as a
core representative subset. The concept emerged from computational
geometry in the early 2000s, where researchers sought provably small
subsets that approximate solutions to geometric optimization problems.
For ML applications, coresets provide theoretical guarantees: a
well-constructed coreset of size independent of the original dataset can
approximate the full dataset's loss function within a factor of (1 + ε).
} answers this question by identifying a small subset of data that
preserves the statistical properties of the entire dataset.

The goal is to find a compact set of examples that allows a model to
generalize as well as it would if trained on the full dataset. Several
algorithmic families have proven effective, each with distinct
computational trade-offs.

Geometry-based methods select samples that cover the data distribution
without requiring any model training. The k-Center
algorithm\sidenote{\textbf{k-Center Algorithm}: Dorit Hochbaum and David
Shmoys established the modern approach to this problem in 1985
(\citeproc{ref-hochbaum1985best}{Hochbaum and Shmoys 1985}), proving
that their 2-approximation algorithm is ``best possible''---no
polynomial-time algorithm can achieve a better approximation factor
unless P=NP. The algorithm's origin in facility location (placing
warehouses to minimize maximum customer distance) explains why it
transfers well to coreset selection: both seek coverage of a space with
minimal representatives. } (also known as Facility Location) selects
samples that minimize the maximum distance from any point to its nearest
selected center, ensuring coverage of the entire data manifold.

Herding takes a different approach, iteratively selecting samples whose
features best approximate the mean of the full dataset, thereby
maintaining distributional fidelity. These methods are computationally
attractive because they operate purely on feature representations, but
they ignore label information entirely.

Gradient-based methods offer higher selection quality by using training
dynamics to identify important samples, though they require training a
proxy model first. GraNd (Gradient Normed) and EL2N (Error
L2-Norm)\sidenote{\textbf{EL2N and GraNd}: Introduced by Mansheej Paul
and colleagues at NeurIPS 2021 in their paper ``Deep Learning on a Data
Diet.'' These scores identify important examples using only information
from the first few training epochs, unlike forgetting-based methods that
require full training. The key insight: samples the model finds
uncertain early in training remain important throughout, and these
scores transfer across architectures---scores computed on ResNet-18
predict importance for ResNet-50. } score samples by gradient magnitude
or prediction error early in training; high-scoring samples lie near the
decision boundary and are most informative for learning. Forgetting
Events\sidenote{\textbf{Forgetting Events}: Coined by Mariya Toneva and
colleagues at ICLR 2019. A ``forgetting event'' occurs when a sample
transitions from correctly to incorrectly classified during
training---the opposite of a learning event. The surprising finding: a
large fraction of samples are never forgotten once learned, and these
``unforgettable'' examples can be safely pruned with minimal accuracy
impact. } tracks how often a sample is ``forgotten'' (correctly
classified, then later misclassified) during training, identifying
harder and more valuable examples.

These gradient-based approaches generally outperform geometry-based
methods in selection quality but incur the overhead of proxy model
training.

Table~\ref{tbl-coreset-comparison} quantifies the computational
trade-offs between these approaches:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.1443}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.1649}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2165}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2165}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2577}}@{}}
\caption{\textbf{Coreset Selection Algorithm Comparison.} N = dataset
size, K = coreset size. Gradient-based methods generally outperform
geometry-based methods but require proxy model
training.}\label{tbl-coreset-comparison}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Method}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Compute Cost}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Requires Training}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Best For}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Limitation}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Method}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Compute Cost}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Requires Training}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Best For}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Limitation}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{k-Center} & O(N²) or O(NK) & No & Coverage, exploration &
Ignores label information \\
\textbf{Herding} & O(NK) & No & Distribution matching & Assumes
Gaussian-like \\
\textbf{GraNd} & O(epochs × N) & Yes (few epochs) & Decision boundaries
& Requires proxy training \\
\textbf{Forgetting} & O(full training) & Yes (full) & Hard examples &
Expensive to compute \\
\textbf{EL2N} & O(epochs × N) & Yes (few epochs) & Uncertainty sampling
& Best with proxy model \\
\end{longtable}

Figure~\ref{fig-coreset-selection} illustrates the core insight behind
coreset methods: samples near the decision boundary (high uncertainty)
are more informative than samples deep within class regions (low
uncertainty). Random sampling wastes budget on redundant ``easy''
examples.

\begin{figure}[htb]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/a2d8f109476d2f0996c703d43d729aa4cc0f3460.pdf}}

}

\caption{\label{fig-coreset-selection}\textbf{Coreset Selection
Strategy}: Random sampling (left) selects uniformly, wasting budget on
easy samples far from the decision boundary. Coreset selection (right)
prioritizes samples near the boundary where the model is uncertain,
capturing more information per sample.}

\end{figure}%

Given these trade-offs, most practitioners find that EL2N with a small
proxy model offers the best balance of selection quality and
computational cost. The approach is straightforward: train a lightweight
model (for example, ResNet-18 instead of ResNet-50) for 5 to 10 epochs,
compute EL2N scores for all samples, then select the highest-scoring
subset. The proxy does not need to be accurate; it only needs to
identify which samples are hard. This upfront investment in proxy
training typically yields substantial returns when the coreset reduces
subsequent training by 50\% or more. The following example illustrates
this workflow in a concrete scenario.

\phantomsection\label{callout-exampleux2a-1.8}
\begin{fbxSimple}{callout-example}{Example:}{Coreset Selection in Practice}
\phantomsection\label{callout-example*-1.8}
\textbf{Scenario}: You have 1 million training images and want to reduce
to 100,000 (10\%) for faster experimentation.

\textbf{Naive Approach}: Random sampling loses rare classes and edge
cases.

\textbf{Coreset Approach}:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Train a small proxy model for 5 epochs
\item
  Compute EL2N scores for all samples
\item
  Select the 100,000 samples with highest uncertainty
\item
  Train your full model on this coreset
\end{enumerate}

\textbf{Result}: The coreset often achieves \textbf{higher accuracy}
than random sampling because it focuses on the decision boundary rather
than redundant ``easy'' examples.

\end{fbxSimple}

Listing~\ref{lst-el2n-coreset} demonstrates how to compute EL2N scores
and select a coreset using a lightweight proxy model.

\begin{codelisting}

\caption{\label{lst-el2n-coreset}\textbf{EL2N-Based Coreset Selection}:
Computing uncertainty scores with a proxy model enables 10x data
reduction while preserving accuracy. The \texttt{compute\_el2n\_scores}
function trains a small model for a few epochs, then measures prediction
confidence via L2 distance from one-hot labels. High scores indicate
uncertain samples near decision boundaries. The \texttt{select\_coreset}
function retains only these informative samples, discarding redundant
easy examples.}

\centering{

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ compute\_el2n\_scores(model, dataloader, num\_epochs}\OperatorTok{=}\DecValTok{5}\NormalTok{):}
    \CommentTok{"""Compute EL2N scores.}

\CommentTok{    Returns L2 norm of (prediction {-} one\_hot\_label).}
\CommentTok{    """}
    \CommentTok{\# Train proxy model for a few epochs to get meaningful predictions}
\NormalTok{    train\_proxy(model, dataloader, num\_epochs)}

\NormalTok{    scores }\OperatorTok{=}\NormalTok{ []}
\NormalTok{    model.}\BuiltInTok{eval}\NormalTok{()}
    \ControlFlowTok{for}\NormalTok{ x, y }\KeywordTok{in}\NormalTok{ dataloader:}
\NormalTok{        logits }\OperatorTok{=}\NormalTok{ model(x)}
\NormalTok{        probs }\OperatorTok{=}\NormalTok{ softmax(logits, dim}\OperatorTok{=}\DecValTok{1}\NormalTok{)}
        \CommentTok{\# One{-}hot encode labels}
\NormalTok{        one\_hot }\OperatorTok{=}\NormalTok{ zeros\_like(probs).scatter\_(}\DecValTok{1}\NormalTok{, y.unsqueeze(}\DecValTok{1}\NormalTok{), }\DecValTok{1}\NormalTok{)}
        \CommentTok{\# EL2N score = L2 distance from confident prediction}
\NormalTok{        el2n }\OperatorTok{=}\NormalTok{ (probs }\OperatorTok{{-}}\NormalTok{ one\_hot).norm(dim}\OperatorTok{=}\DecValTok{1}\NormalTok{)  }\CommentTok{\# High = uncertain}
\NormalTok{        scores.extend(el2n.tolist())}
    \ControlFlowTok{return}\NormalTok{ scores}


\KeywordTok{def}\NormalTok{ select\_coreset(scores, dataset, fraction}\OperatorTok{=}\FloatTok{0.1}\NormalTok{):}
    \CommentTok{"""Select top{-}k highest{-}scoring (most uncertain) samples."""}
\NormalTok{    k }\OperatorTok{=} \BuiltInTok{int}\NormalTok{(}\BuiltInTok{len}\NormalTok{(dataset) }\OperatorTok{*}\NormalTok{ fraction)}
    \CommentTok{\# Sort by score descending (highest uncertainty first)}
\NormalTok{    indices }\OperatorTok{=}\NormalTok{ argsort(scores, descending}\OperatorTok{=}\VariableTok{True}\NormalTok{)[:k]}
    \ControlFlowTok{return}\NormalTok{ Subset(dataset, indices)}


\CommentTok{\# Usage: 10x data reduction with minimal accuracy loss}
\NormalTok{scores }\OperatorTok{=}\NormalTok{ compute\_el2n\_scores(proxy\_model, full\_loader)}
\NormalTok{coreset }\OperatorTok{=}\NormalTok{ select\_coreset(scores, full\_dataset, fraction}\OperatorTok{=}\FloatTok{0.1}\NormalTok{)}
\NormalTok{train\_full\_model(model, coreset)  }\CommentTok{\# 10x faster training}
\end{Highlighting}
\end{Shaded}

}

\end{codelisting}%

\subsection{Data
Deduplication}\label{sec-data-selection-data-deduplication-9035}

While coreset selection identifies which samples to keep based on their
informativeness, a complementary approach targets what to remove: exact
and near-duplicates. Deduplication provides immediate efficiency gains
with no accuracy penalty and requires no model training. This makes it
the most accessible optimization in data selection, offering guaranteed
compute savings with zero risk of degrading model quality.

The simplest form of deduplication uses hash-based methods for exact
matches. By computing a cryptographic hash (MD5 or SHA-256) for each
sample and removing those with identical hashes, practitioners can
eliminate byte-for-byte duplicates that inevitably accumulate in large
web-scraped corpora. This process is computationally cheap, scaling
linearly with dataset size, and can be parallelized trivially.

Near-duplicate detection addresses the more subtle problem of
semantically redundant content that differs at the byte level. For text,
MinHash\sidenote{\textbf{MinHash}: Invented by Andrei Broder in 1997
(\citeproc{ref-broder1997resemblance}{Broder, n.d.}), originally to
detect duplicate web pages for the AltaVista search engine. The
algorithm uses random hash functions to create compact ``signatures''
that preserve set similarity---two documents with similar content
produce similar signatures with high probability. Broder received the
2012 ACM Kanellakis Award for this work, recognizing its foundational
impact on web-scale similarity detection. } with
\textbf{Locality-Sensitive Hashing} (LSH) approximates Jaccard
similarity efficiently, detecting paraphrased or lightly edited content.

For images, perceptual hashing produces signatures robust to minor
transformations like resizing and compression, identifying visually
identical images stored in different formats. Embedding-based similarity
offers the most powerful detection by computing dense representations
(CLIP for images, sentence transformers for text) and clustering similar
items, though this approach incurs higher computational overhead.

For foundation model pre-training, deduplication has become essential
rather than optional. Studies on GPT-3 and LLaMA training demonstrate
that deduplicated data improves both training efficiency and downstream
performance by preventing memorization of repeated content. The benefit
is twofold: fewer wasted FLOPs on redundant samples, and better
generalization because the model sees more diverse examples per training
token.

Deduplication benefits extend beyond text corpora. The DLRM lighthouse
presents a unique variant of this challenge centered on \emph{embedding
deduplication}.

\phantomsection\label{callout-lighthouseux2a-1.9}
\begin{fbxSimple}{callout-lighthouse}{Lighthouse:}{DLRM and Embedding Deduplication}
\phantomsection\label{callout-lighthouse*-1.9}
Our \textbf{DLRM Lighthouse model} (\textbf{?@sec-dnn-architectures})
presents a unique deduplication challenge. Recommendation systems are
memory capacity-bound, with embedding tables consuming terabytes of
storage for billions of user/item IDs. Much of this capacity is wasted
on \emph{cold embeddings}, IDs that appear rarely in training data.

Data selection for DLRM focuses on \textbf{interaction deduplication}
(removing redundant user-item pairs) and \textbf{embedding pruning}
(removing or sharing cold embeddings). A 20\% reduction in unique
interactions can reduce embedding table size by 30--40\%, directly
addressing DLRM's primary bottleneck: memory capacity rather than
compute.

\end{fbxSimple}

\subsection{Data Pruning by
Quality}\label{sec-data-selection-data-pruning-quality-72ea}

Deduplication removes redundant samples, but a third category of
problematic data remains: samples that actively harm learning.
Quality-based pruning eliminates samples that either contribute no
meaningful signal or introduce contradictory information that confuses
the optimization process.

Label error detection represents the most impactful form of quality
pruning. Tools like Cleanlab identify samples where the assigned label
is likely incorrect based on model confidence patterns across training.
A sample that the model consistently predicts as class A but is labeled
class B either represents a hard case near the decision boundary or,
more commonly, an annotation mistake. Removing or correcting these
mislabeled samples prevents the model from learning contradictory
signals that degrade its decision boundary.

Outlier removal addresses a different pathology: samples far from any
cluster center in feature space. While outliers might represent valuable
edge cases, they more often indicate noise, annotation errors, or data
corruption. The key is distinguishing between informative outliers (rare
but valid examples of a class) and noise (samples that do not belong to
any class). Conservative thresholds help avoid discarding genuinely rare
examples.

Low-information filtering applies domain-specific heuristics to remove
samples that lack sufficient signal for learning. For text corpora, this
means removing documents below a perplexity threshold or with low
semantic coherence, often indicative of machine-generated spam or
garbled content. For image datasets, filtering targets blurry,
corrupted, or near-uniform samples that provide little visual
information.

Together, these three static pruning techniques (coreset selection,
deduplication, and quality filtering) demonstrate that careful curation
before training yields significant efficiency gains. The compute savings
are multiplicative across the entire training process: a 50\% dataset
reduction means 50\% fewer forward passes, backward passes, and gradient
updates across all training epochs. For a model trained for 100 epochs,
this translates to 50 epochs worth of saved compute, yielding
substantial reductions in both training time and energy consumption.

\section{Dynamic Data Selection: Training-Time
Optimization}\label{sec-data-selection-dynamic-data-selection-trainingtime-optimization-cd62}

Static pruning commits to a fixed dataset before training begins, but
\emph{what if} the optimal training samples change as the model learns?
Early in training, the model benefits from diverse coverage to build
broad feature representations; later, it benefits from focusing on hard
examples near the decision boundary to refine its predictions. Dynamic
selection exploits this insight by optimizing which samples to use
\emph{during} training, adapting the data diet based on the model's
evolving state.

\subsection{Curriculum Learning: Easy to
Hard}\label{sec-data-selection-curriculum-learning-easy-hard-3428}

The first dynamic selection technique, \textbf{curriculum
learning}\sidenote{\textbf{Curriculum Learning}: Formalized by Yoshua
Bengio and colleagues at ICML 2009, drawing explicit inspiration from
human education where students master basics before advanced topics. The
paper's key insight was that curriculum learning acts as a
``continuation method'' for non-convex optimization: starting with easy
examples smooths the loss landscape, helping the optimizer find better
local minima. The paper has accumulated thousands of citations,
reflecting its influence on training methodology. }
(\citeproc{ref-bengio2009curriculum}{Bengio et al. 2009};
\citeproc{ref-soviany2022curriculum}{Soviany et al. 2022}), structures
the order in which data is presented to the model. Instead of random
shuffling, it starts with simpler examples and gradually introduces more
complex ones, mirroring how humans learn by mastering fundamentals
before advancing to harder material.

The effectiveness of curriculum learning stems from how neural networks
respond to gradient signals at different training stages. Easy examples
provide clear, consistent gradients that establish strong feature
representations early in training, when the loss landscape is highly
irregular. Hard examples introduced too early produce noisy gradient
signals that slow convergence or cause the model to memorize outliers
rather than learn general patterns. By sequencing examples from easy to
hard, curriculum learning smooths the optimization trajectory.

Implementing a curriculum requires two components: a difficulty scorer
that ranks samples, and a pacing function that controls how quickly hard
samples are introduced. A common choice is linear pacing:

\[
\text{samples}_t = \text{sort\_by\_difficulty}[:N \cdot \min(1, t/T_{warmup})]
\]

where \(t\) is the current epoch and \(T_{warmup}\) is the epoch at
which the full dataset becomes available. Early epochs train on the
easiest \(N \cdot (t/T_{warmup})\) fraction; after warmup, training
proceeds on the full dataset.

The difficulty scorer can be designed in several ways, each with
different computational requirements and applicability
(Table~\ref{tbl-difficulty-scoring}).

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2000}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3905}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4095}}@{}}
\caption{\textbf{Difficulty Scoring Strategies for Curriculum Learning.}
Loss-based and confidence-based methods require additional model
inference; domain heuristics are free but require expertise; self-paced
methods adapt dynamically during
training.}\label{tbl-difficulty-scoring}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Strategy}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Difficulty Score}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Best For}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Strategy}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Difficulty Score}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Best For}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Loss-Based} & Loss from probe model (low = easy) &
General-purpose; requires probe training \\
\textbf{Confidence-Based} & Teacher model confidence (high = easy) &
When teacher available; distillation setups \\
\textbf{Domain Heuristics} & Sentence length, image complexity & No
extra compute; domain knowledge required \\
\textbf{Self-Paced} & Current model's loss (updated each epoch) &
Adaptive; no probe needed \\
\end{longtable}

From a systems perspective, curriculum learning improves convergence by
reducing wasted gradient updates on samples the model cannot yet learn
from. The Information-Compute Ratio is higher in early training because
easy samples provide strong learning signal relative to their compute
cost. The efficiency gains manifest as faster convergence to target
accuracy, not higher final accuracy.

Table~\ref{tbl-curriculum-benchmarks} summarizes measured speedups from
curriculum learning across standard benchmarks:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.1625}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.1125}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2375}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.3125}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.1750}}@{}}
\caption{\textbf{Curriculum Learning Convergence Speedups.} Target
accuracy is 95\% of final baseline performance. Gains are larger on
redundant datasets (CIFAR-10) and noisy datasets (MentorNet removes
approximately 40\% noise). ImageNet shows smaller gains because the
dataset is less
redundant.}\label{tbl-curriculum-benchmarks}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Dataset}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Model}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Pacing Strategy}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Epochs to Target Acc.}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Speedup}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Dataset}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Model}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Pacing Strategy}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Epochs to Target Acc.}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Speedup}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{CIFAR-10} & ResNet-18 & Linear warmup & 115 vs.~150 baseline &
\textbf{23\%} faster \\
\textbf{CIFAR-100} & ResNet-32 & Self-paced & 180 vs.~220 baseline &
\textbf{18\%} faster \\
\textbf{ImageNet} & ResNet-50 & Loss-based & 80 vs.~90 baseline &
\textbf{11\%} faster \\
\textbf{ImageNet} & ResNet-50 & MentorNet (noisy) & 70 vs.~90 baseline &
\textbf{22\%} faster \\
\end{longtable}

The table reveals an important pattern: curriculum learning gains are
\textbf{inversely proportional to dataset quality}. On highly curated
datasets like ImageNet, the 11\% speedup is modest. On noisy or
redundant data, gains can exceed 20\%. The optimal ordering is also
task-dependent: \textbf{anti-curriculum} (hard examples first) can work
when the decision boundary is complex and easy examples contribute
little to defining it, while \textbf{self-paced learning} lets the model
dynamically adjust difficulty based on its current loss, eliminating the
need to pre-define a curriculum. Empirically, self-paced methods often
match or exceed hand-designed curricula.

\subsection{Active Learning:
Human-in-the-Loop}\label{sec-data-selection-active-learning-humanintheloop-a9fa}

Curriculum learning optimizes the order in which samples are presented
but assumes all samples are already labeled. This assumption breaks down
in specialized fields such as medical diagnosis, autonomous driving, and
scientific research, where labeling requires domain expertise and can
cost \$5--\$100 or more per sample. Rather than labeling everything
upfront, \textbf{active learning}\sidenote{\textbf{Active Learning}: The
concept traces to statistical experimental design, but Dana Angluin's
work on learning from queries (\citeproc{ref-angluin1988queries}{Angluin
1988}) established theoretical foundations for machine learning. The
term ``active'' contrasts with ``passive'' learning from pre-labeled
data---the learner actively queries an
oracle{[}\^{}fn-oracle-etymology{]} rather than passively receiving
examples. Early work in the 1990s demonstrated that active selection
could achieve the same accuracy as passive learning with exponentially
fewer labels in favorable cases. }
(\citeproc{ref-settles2009active}{Settles 2012};
\citeproc{ref-ren2021survey}{Ren et al. 2021}) shifts the optimization
target: instead of choosing which labeled samples to train on, it
chooses which unlabeled samples are worth labeling at all
(Figure~\ref{fig-active-learning-loop}).

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/9d49a4d2fde1ded3aa18fdc325c529d9d71a7c67.pdf}}

}

\caption{\label{fig-active-learning-loop}\textbf{Active Learning Loop}:
Instead of labeling all data, the model selects the most `confusing' or
informative samples from an unlabeled pool. These samples are sent to an
Oracle (human annotator) and added to the training set. The model is
retrained, and the cycle repeats, creating a feedback loop that
maximizes information gain per label.}

\end{figure}%

The effectiveness of active learning depends critically on the query
strategy used to select samples for annotation. The simplest approach,
uncertainty sampling, selects samples where the model is least
confident, such as predictions near 0.5 probability for binary
classification. This strategy is computationally cheap and effective in
practice. Query-by-committee extends this idea by training multiple
models and selecting samples where they disagree most, capturing
epistemic uncertainty that a single model might miss.

For practitioners willing to invest more compute, expected model change
selects samples that would cause the largest gradient update if labeled.
This approach provides a theoretically grounded but expensive
alternative. Diversity sampling complements uncertainty-based methods by
selecting samples dissimilar from currently labeled data, ensuring the
labeled set covers the full input space rather than clustering around
ambiguous regions.

Active learning is particularly valuable in domains where labeling
requires expertise. In medical imaging, for instance, an AI system
diagnosing diseases from X-rays may be confident on common conditions
but uncertain about rarer cases. By focusing human annotation on these
ambiguous cases, active learning optimizes the use of expensive expert
time while accelerating model improvement.

The economic implications are substantial. In production settings,
labeling costs often dwarf compute costs because a specialist's time is
far more expensive than GPU hours. The \emph{active learning ROI}
becomes dramatic in these settings.

\phantomsection\label{callout-notebookux2a-1.10}
\begin{fbxSimple}{callout-notebook}{AI Engineer’s Notebook:}{The Active Learning ROI}
\phantomsection\label{callout-notebook*-1.10}
\textbf{Problem}: You are building a medical diagnostic AI. You have a
pool of \textbf{1 Million unlabeled scans}. A specialist doctor charges
\textbf{\$5.00} to label one scan. You have a budget of
\textbf{\$500,000} and a deadline of \textbf{1 month}.

\textbf{Scenario A: Naive Labeling}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Cost}: Labeling all 1M scans would cost
  **\(5,000,000** (10\)\times\$ over budget).
\item
  \textbf{Time}: You can only afford to label 100,000 random scans.
\item
  \textbf{Result}: Your model misses rare pathologies because they
  weren't in the random 10\%.
\end{enumerate}

\textbf{Scenario B: Active Learning}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Strategy}: Use an uncertainty-based selection to pick the
  \textbf{50,000} ``hardest'' scans for the doctor to label.
\item
  \textbf{Cost}: \(50,000 \times 5.00 = \mathbf{\$250,000}\). (50\%
  under budget).
\item
  \textbf{Training Speed}: With 20\(\times\) less data, each training
  epoch is \textbf{20\(\times\) faster}.
\item
  \textbf{Result}: Research shows that these 50k ``high-information''
  samples often achieve higher accuracy than 500k random samples.
\end{enumerate}

\textbf{The Systems Conclusion}: Data Selection is not just a ``data
trick''; it is a \textbf{20\(\times\) compute accelerator} and a
\textbf{\$4.75 Million} cost-saving measure.

\end{fbxSimple}

Figure~\ref{fig-active-learning-multiplier} quantifies this advantage,
showing how Active Learning shifts the learning curve to the left,
achieving target accuracy with exponentially fewer samples than random
selection.

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{contents/vol1/data_selection/data_selection_files/figure-pdf/fig-active-learning-multiplier-output-1.pdf}}

}

\caption{\label{fig-active-learning-multiplier}\textbf{The Active
Learning Multiplier}: Model Accuracy vs.~Number of Labeled Samples (Log
Scale). Random sampling (gray dashed) yields linear improvements, often
requiring massive datasets to capture rare edge cases. Active Learning
(green solid) specifically targets informative samples, achieving the
same 90\% accuracy with 4x fewer labels. The green shaded region
represents the direct economic value (labeling cost saved) of
intelligent data selection.}

\end{figure}%

The benefits of active learning extend beyond cost savings: they enable
models to learn from precisely the examples that matter most. The Smart
Doorbell Lighthouse illustrates this principle in the context of hard
negative mining.

\phantomsection\label{callout-lighthouseux2a-1.11}
\begin{fbxSimple}{callout-lighthouse}{Lighthouse:}{Mining for Hard Negatives}
\phantomsection\label{callout-lighthouse*-1.11}
\textbf{The ``Hard Negative'' Problem}: Our \textbf{Smart Doorbell}
faces a classic data selection challenge. The vast majority of its video
feed is empty (easy negatives) or clearly people (easy positives). The
model fails on the 0.01\% of ``Hard Negatives'': statues, posters of
people, or laundry piles that cast human-like shadows.

Random sampling will miss these rare failures. Instead, the Wake Vision
team uses \textbf{Active Learning} to specifically query the Oracle
(human reviewers) on low-confidence predictions. If the model sees a
``statue'' and predicts ``Person (51\%)'', that sample is flagged for
labeling. This turns the feedback loop from a random walk into a guided
search for the decision boundary, reducing the data required to solve
the ``statue problem'' by orders of magnitude compared to random
collection.

\end{fbxSimple}

\subsection{Semi-Supervised Learning: Using Unlabeled
Data}\label{sec-data-selection-semisupervised-learning-leveraging-unlabeled-data-53b7}

Active learning optimizes which samples to label but still requires
human annotation for every selected example. A more aggressive approach
asks: \emph{can we} extract learning signal from unlabeled data
directly? \textbf{Semi-supervised learning} addresses this question. It
uses a small set of labeled examples to guide learning on a much larger
unlabeled pool, typically achieving 80--95\% of fully supervised
accuracy with only 10--20\% of the labels.

The core insight behind semi-supervised learning is that unlabeled data,
while it cannot directly teach the mapping from inputs to outputs,
contains structural information about the input distribution \(P(X)\)
that constrains the hypothesis space. A decision boundary that cuts
through dense regions of \(P(X)\) is unlikely to generalize well because
it would assign different labels to similar inputs. Semi-supervised
methods use unlabeled data to push decision boundaries toward
low-density regions, where class transitions are more likely to occur
naturally.

Three main techniques implement this insight. \textbf{Pseudo-labeling}
takes the most direct approach: train on labeled data, use the model to
generate ``pseudo-labels'' for high-confidence unlabeled predictions,
then retrain on both. The confidence threshold is critical: setting it
too low introduces label noise that degrades learning, while setting it
too high wastes potentially useful data.

\textbf{Consistency regularization} takes a different angle by enforcing
that the model produces similar predictions for augmented versions of
the same input. A robust classifier should be invariant to realistic
perturbations like cropping, rotation, or color shifts. Methods like
FixMatch combine both approaches, assigning pseudo-labels only to
samples where the unaugmented prediction is confident but training the
model to predict these labels on strongly augmented versions of the same
images.

Label propagation offers a third paradigm through graph-based reasoning:
construct a similarity graph over all samples and propagate labels from
labeled nodes to their neighbors. This approach works particularly well
when the feature space exhibits clear cluster structure.

The systems trade-off in semi-supervised learning is straightforward: it
typically achieves the same accuracy as fully supervised training with
5--10\(\times\) fewer labels but requires more compute because training
processes both labeled and unlabeled samples. Since labeling costs often
dominate compute costs in production settings, this trade-off is usually
favorable. The results of \emph{FixMatch on CIFAR-10} illustrate this
label efficiency concretely.

\phantomsection\label{callout-exampleux2a-1.12}
\begin{fbxSimple}{callout-example}{Example:}{FixMatch on CIFAR-10}
\phantomsection\label{callout-example*-1.12}
\textbf{FixMatch} (\citeproc{ref-sohn2020fixmatch}{Sohn et al. 2020})
combines pseudo-labeling with consistency regularization to achieve high
label efficiency (Table~\ref{tbl-fixmatch-cifar10}).

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2237}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2105}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1579}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.4079}}@{}}
\caption{\textbf{FixMatch Label Efficiency on CIFAR-10.} With 250 labels
(0.5\% of the dataset), FixMatch achieves within 1.2 points of full
supervision, demonstrating 200\(\times\) label
efficiency.}\label{tbl-fixmatch-cifar10}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Label Budget}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Method}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Accuracy}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Label Efficiency}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Label Budget}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Method}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Accuracy}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Label Efficiency}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{50,000 (100\%)} & Fully Supervised & 96.1\% & Baseline \\
\textbf{4,000 (8\%)} & FixMatch & 95.7\% & \textbf{12.5\(\times\) more
efficient} \\
\textbf{250 (0.5\%)} & FixMatch & 94.9\% & \textbf{200\(\times\) more
efficient} \\
\textbf{40 (0.08\%)} & FixMatch & 88.6\% & 1250\(\times\) more
efficient \\
\end{longtable}

With only 250 labeled samples (25 per class), FixMatch achieves 94.9\%
accuracy, within 1.2 points of full supervision using 200\(\times\)
fewer labels. The technique works by generating pseudo-labels on weakly
augmented unlabeled images (only when model confidence exceeds 0.95),
then training to predict these labels on strongly augmented versions of
the same images.

\textbf{The Systems Insight}: Semi-supervised learning trades labeled
data for unlabeled data and compute. On CIFAR-10, training FixMatch
requires \textasciitilde5\(\times\) more compute than supervised
training (processing 50K unlabeled samples per epoch). When labels cost
\$1 each and GPU hours cost \$0.50, the math favors semi-supervised:

\begin{itemize}
\tightlist
\item
  Supervised (4000 labels): \$4,000 labeling + \$50 compute =
  \textbf{\$4,050}
\item
  FixMatch (250 labels): \$250 labeling + \$250 compute = \textbf{\$500}
\end{itemize}

An 8\(\times\) cost reduction for \textless1\% accuracy loss.

\end{fbxSimple}

These gains are substantial, but semi-supervised learning is not
universally applicable. The technique assumes that unlabeled data comes
from the same distribution as labeled data, and it struggles when
unlabeled data contains out-of-distribution samples (the model
confidently mislabels them), when class imbalance is severe
(pseudo-labels amplify majority class bias), or when the labeled set
does not cover all classes (preventing label propagation for unseen
classes). Always validate on a held-out set with true labels to catch
distribution mismatch.

Despite these limitations, semi-supervised learning reduces label
requirements by 5--10\(\times\) while maintaining accuracy. Notice what
we have not yet questioned: the assumption that we need \emph{any}
task-specific labels at all. What if the structure of data itself (the
fact that cat images resemble other cat images, that coherent sentences
follow grammatical patterns) could provide the supervision signal?

The three-stage pipeline assumes labeled data, but a powerful
alternative eliminates labeling requirements entirely. Self-supervised
learning reframes the data selection problem: rather than selecting
which examples to label, it defines learning objectives that derive
supervision from the data itself. This approach transforms the
pipeline's annotation stage from a bottleneck into an opportunity.

\section{Self-Supervised Learning: Eliminating the Label
Bottleneck}\label{sec-data-selection-selfsupervised-learning-eliminating-label-bottleneck-1005}

Active learning reduces labeling cost by 10\(\times\). Semi-supervised
learning reduces it by another 5--10\(\times\). The most dramatic gain,
however, comes from \textbf{self-supervised
learning}\sidenote{\textbf{Self-Supervised Learning}: While
self-supervision ideas existed earlier, 2018 marked the paradigm's
breakthrough year. BERT (Google, October 2018) demonstrated that masked
language modeling could produce representations achieving
state-of-the-art results on 11 NLP tasks. GPT (OpenAI, June 2018) showed
that next-token prediction at scale yielded surprisingly general
language understanding. Together, they established pre-training on
unlabeled data as the dominant paradigm for NLP, later extended to
vision and multimodal domains. }, which removes the human annotation
bottleneck entirely by learning from data structure rather than human
labels. Self-supervised learning does not map neatly onto the
three-stage pipeline (static pruning, dynamic selection, synthetic
generation) introduced earlier. Rather than optimizing which labeled
samples to use, SSL eliminates the label bottleneck by redefining what
counts as supervision. It is best understood as a paradigm shift that
makes the entire pipeline more effective: pre-trained representations
improve the quality of coreset selection, active learning, and
downstream fine-tuning alike.

\subsection{The Paradigm Shift: Labels from
Structure}\label{sec-data-selection-paradigm-shift-labels-structure-e9cc}

Labels represent just one form of supervision. The structure of data
itself provides rich learning signals that require no human annotation,
as Table~\ref{tbl-self-supervised-tasks} summarizes.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.1829}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2927}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.5244}}@{}}
\caption{\textbf{Self-Supervised Pretext Tasks by Modality.} Each task
extracts supervision from data structure rather than human labels,
enabling pre-training on unlimited unlabeled
corpora.}\label{tbl-self-supervised-tasks}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Modality}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Self-Supervised Task}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Supervision Signal}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Modality}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Self-Supervised Task}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Supervision Signal}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Text} & Masked language modeling & Predict {[}MASK{]} from
context \\
\textbf{Text} & Next-token prediction & Predict next word in sequence \\
\textbf{Images} & Contrastive learning & Same image (augmented)
vs.~different images \\
\textbf{Images} & Masked autoencoding & Reconstruct masked patches \\
\textbf{Multi-modal} & CLIP-style alignment & Match image-text pairs \\
\end{longtable}

These \emph{pretext tasks} generate supervision signals automatically
from the data itself. A model that can predict masked words has
necessarily learned grammar, semantics, and world knowledge to make
accurate predictions. Similarly, a model that distinguishes augmented
views of the same image from different images has learned robust visual
features invariant to transformations.

The systems implication is direct: self-supervised pre-training moves
the data cost off the critical path. Instead of waiting for labels
before training begins, pre-training can start immediately on unlabeled
data, often web-scale corpora of billions of samples. This separation of
pre-training from task-specific labeling restructures the economics of
machine learning.

\subsection{The Economics of
Amortization}\label{sec-data-selection-economics-amortization-79e6}

Understanding \emph{why} self-supervised learning dominates modern ML
practice requires examining its economic structure. The shift translates
into concrete cost savings through \emph{cost amortization}, where
expensive pre-training is performed once and reused across many
applications (Table~\ref{tbl-cost-amortization}).

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3158}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2000}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2211}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2632}}@{}}
\caption{\textbf{Cost Amortization in Foundation Model Fine-Tuning.}
Pre-training costs are paid once; fine-tuning costs scale with task
count but remain small per
task.}\label{tbl-cost-amortization}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Approach}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Labels per Task}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Compute per Task}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Data Acquisition}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Approach}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Labels per Task}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Compute per Task}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Data Acquisition}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Train from scratch} & 100K--1M labeled & 100\% full training &
Task-specific collection \\
\textbf{Fine-tune foundation model} & 100--1K labeled & 1--5\% of full
training & Reuse pre-training corpus \\
\end{longtable}

To illustrate this economic transformation, consider a company building
ten specialized classifiers for tasks such as fraud detection, content
moderation, and medical diagnosis.

Training each classifier from scratch would require substantial
investment in both labeling and compute. With ten tasks each needing
100,000 labels at \$1 per label, the total labeling cost reaches
\textbf{\$1,000,000}. The compute burden amounts to 10,000 GPU-hours
across all tasks, with each requiring its own data collection effort.
From start to finish, each task takes 6--12 months to complete.

The fine-tuning approach restructures these costs. Pre-training requires
a one-time investment of 10,000 GPU-hours on unlabeled data, but this
cost is paid only once. Fine-tuning each task then requires just 1,000
labels (\$10,000 total across all ten tasks) and only 500 GPU-hours of
compute. Each task reaches deployment in 1--2 weeks after pre-training
completes.

The return on investment is substantial across every dimension: labeling
costs drop by \textbf{100\(\times\)} (from \$1M to \$10K), per-task
compute decreases by \textbf{10\(\times\)} when amortized across
applications, and time to deployment accelerates by
\textbf{20--50\(\times\)} per task.

This explains \emph{why} the fine-tuning paradigm dominates production
ML. The pre-training cost is high but amortized across many downstream
applications, while fine-tuning cost remains low on a per-task basis.

Figure~\ref{fig-amortization-comparison} visualizes this cost structure.
Training from scratch (left) incurs the full cost for each task
independently. The foundation model approach (right) pays a large
upfront pre-training cost but then fine-tunes each task at a fraction of
the per-task cost.

\begin{figure}[htb]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/b8348b0802ab6c3a8736db67eab3fb89122734e9.pdf}}

}

\caption{\label{fig-amortization-comparison}\textbf{Cost Amortization in
Foundation Models}: Training from scratch (left) requires 1,000
GPU-hours per task (10,000 total for 10 tasks). The foundation model
approach (right) pays 10,000 GPU-hours upfront for pre-training but
reduces each subsequent task to just 50 GPU-hours. At 10 tasks the
totals are comparable (10,000 vs 10,500), but the per-task marginal cost
drops by 20x, and the crossover favoring the foundation model occurs
around 11 tasks.}

\end{figure}%

\subsection{Trade-Offs Across Self-Supervised
Approaches}\label{sec-data-selection-tradeoffs-across-selfsupervised-approaches-b473}

The economics of amortization favor self-supervised learning broadly,
but not all self-supervised methods are equivalent. Different approaches
occupy different points on the efficiency frontier, trading off
pre-training cost, batch size requirements, and downstream data
efficiency (Table~\ref{tbl-ssl-tradeoffs}).

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1939}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2653}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2551}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2857}}@{}}
\caption{\textbf{Self-Supervised Learning Method Trade-Offs.}
Contrastive methods excel at downstream data efficiency but require
massive batches; masked modeling balances cost and efficiency;
generative methods scale best with unlimited
data.}\label{tbl-ssl-tradeoffs}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Method}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Batch Size Requirement}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Data Efficiency}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Best Use Case}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Method}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Batch Size Requirement}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Data Efficiency}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Best Use Case}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Contrastive} & Very large (4096+) & High (per labeled sample) &
Vision, small datasets \\
\textbf{(SimCLR, MoCo)} & & Low (pre-training cost) & \\
\textbf{Masked Modeling} & Moderate (256--1024) & Moderate & NLP,
balanced efficiency \\
\textbf{(BERT, MAE)} & & & \\
\textbf{Generative} & Large (512--2048) & Highest at scale & Foundation
models, unlimited \\
\textbf{(GPT)} & & & unlabeled data \\
\end{longtable}

Contrastive learning methods such as SimCLR
(\citeproc{ref-chen2020simclr}{T. Chen et al. 2020}) and MoCo
(\citeproc{ref-he2020momentum}{He et al. 2020})\sidenote{\textbf{SimCLR
and MoCo}: Both published in early 2020, these papers marked a turning
point for self-supervised vision. SimCLR (Ting Chen et al., Google)
achieved 76.5\% ImageNet accuracy with a linear classifier---matching
supervised ResNet-50---using only self-supervised pre-training. MoCo
(Kaiming He et al., Facebook AI) introduced the momentum encoder trick
that enabled contrastive learning without requiring enormous batch
sizes. Their combined impact closed the gap between supervised and
self-supervised visual representations. } require many negative examples
per batch to distinguish similar samples, making them compute-intensive
during pre-training.

The batch size requirement is substantial: these methods need batches of
4,096 or more samples to work effectively. This upfront investment
yields excellent downstream performance with minimal labeled data,
making contrastive learning particularly effective for vision
applications with small datasets.\sidenote{\textbf{Batch Size
Sensitivity}: The batch size sensitivity is substantial: SimCLR achieves
66.6\% ImageNet top-1 accuracy with batch size 8192 but drops to 61.9\%
with batch size 256, a 4.7 percentage point degradation
(\citeproc{ref-chen2020mocov2}{X. Chen et al. 2020}). This occurs
because contrastive learning treats all other samples in a batch as
negatives; with fewer negatives, the pretext task becomes easier and the
learned representations are weaker. }

Masked modeling approaches such as BERT and MAE occupy a middle ground
in this efficiency spectrum. These methods work with smaller batches
(256--1024 samples) but require more training iterations to converge.
The result is a balanced trade-off between pre-training cost and
downstream data efficiency that has made masked modeling the dominant
paradigm in natural language processing.

Generative pre-training, exemplified by the GPT family of models, scales
well with data volume. Performance improves log-linearly with dataset
size up to trillions of tokens, exhibiting no saturation within current
data availability. This scaling behavior makes generative pre-training
the preferred method for foundation models, where the substantial
pre-training cost can be amortized across thousands of downstream tasks.

\subsection{From 1000× Multiplier to Foundation Model
Paradigm}\label{sec-data-selection-1000-multiplier-foundation-model-paradigm-7866}

These trade-offs converge on a clear conclusion: from a data selection
perspective, self-supervised pre-training represents a
\textbf{1000\(\times\) or greater multiplier} on the value of labeled
data. Instead of labeling millions of task-specific examples,
practitioners fine-tune on hundreds or thousands of labeled samples
while inheriting knowledge distilled from billions of unlabeled tokens.

This multiplicative advantage created the \emph{foundation model
paradigm}\sidenote{\textbf{Foundation Model}: Term coined by Stanford's
Center for Research on Foundation Models in 2021 to describe models like
BERT, GPT-3, and DALL-E. The name emphasizes a critical property: these
models serve as a ``foundation'' for many downstream tasks, but this
creates dangerous homogenization---defects in the foundation model
propagate to all applications built upon it, making them single points
of failure that can ``radiate harms'' across an ecosystem. }
(\citeproc{ref-bommasani2021opportunities}{Bommasani et al. 2021}) that
defines modern ML systems.

The paradigm follows a three-step pattern. First, pre-train once on
massive unlabeled corpora comprising billions of tokens or images.
Second, fine-tune many times on small task-specific datasets containing
hundreds to thousands of samples. Third, amortize the pre-training cost
across all downstream applications. This structure explains \emph{why}
organizations invest millions of dollars in pre-training: the investment
pays dividends across every subsequent application built on that
foundation.

The architectural and training details for these methods appear in
\textbf{?@sec-ai-training}. From a data selection perspective,
self-supervised learning represents the current ceiling of what the
field has achieved, transforming data from the primary bottleneck into
an abundant resource. At scale, self-supervised pre-training requires
distributed infrastructure: gradient accumulation across mini-batches,
mixed precision to reduce memory footprint, and pipeline parallelism to
split models across devices are essential for pre-training
billion-parameter models. The data selection principles discussed here
(coreset selection, curriculum learning) apply regardless of scale, but
their implementation must account for distributed coordination overhead.

Self-supervised learning addresses the label bottleneck by learning from
data structure rather than human annotation. But what happens when the
data itself is scarce? When rare classes have too few examples, when
edge cases never appear in the wild, or when privacy constraints prevent
collecting real samples? The third stage of our data selection pipeline
addresses this: rather than selecting or curating existing data, we
create new data on demand.

\section{Synthetic Data Generation and
Augmentation}\label{sec-data-selection-synthetic-data-generation-augmentation-f3c5}

Static pruning removed redundancy before training began. Dynamic
selection focused compute on the most informative samples during
training. The third and final stage of the data selection pipeline takes
the opposite approach: rather than subtracting or selecting from
existing data, it creates new high-value samples when real data is
scarce, expensive, or lacks diversity. The strategy shifts from curation
to \textbf{creation}.

\subsection{Data Augmentation: Transformation-Based
Synthesis}\label{sec-data-selection-data-augmentation-transformationbased-synthesis-20a5}

Data augmentation expands a dataset by applying transformations to
existing samples. Because many transformations preserve label semantics
while creating novel inputs, augmentation effectively multiplies the
diversity of a training set without requiring additional data
collection.

For image data, augmentation techniques span a range of complexity.
Geometric transformations such as rotation, flipping, cropping, and
scaling introduce spatial variation that makes models robust to
viewpoint changes. Photometric transformations adjust brightness,
contrast, saturation, and hue to simulate different lighting conditions
and camera characteristics. More advanced techniques like Cutout (which
applies random rectangular masks), MixUp\sidenote{\textbf{MixUp}:
Introduced by Hongyi Zhang and colleagues at ICLR 2018. The elegantly
simple idea (train on linear interpolations of image pairs with
correspondingly interpolated labels) produces surprisingly strong
regularization. The paper showed MixUp reduces memorization of corrupt
labels, improves adversarial robustness, and stabilizes GAN training,
all from a technique requiring just two lines of code to implement. }
(\citeproc{ref-zhang2018mixup}{Zhang 2011}) (which blends two images and
their labels), and CutMix (which pastes patches between images) push
augmentation further by creating entirely synthetic training examples
that regularize learning.

Text augmentation presents different challenges because language is
discrete rather than continuous. Back-translation offers one solution:
translating text to another language and back generates paraphrases that
preserve meaning while varying surface form. Simpler approaches include
synonym replacement, which swaps words while preserving semantics, and
random insertion or deletion, which adds noise that makes models robust
to typos and informal input. Rather than hand-designing these
augmentation policies, \textbf{AutoAugment} uses reinforcement learning
to discover optimal augmentation strategies for specific datasets, while
RandAugment simplifies this by randomly sampling from a fixed set of
transformations, achieving similar performance with less computation.

These learned augmentation policies are particularly effective for
resource-constrained models, where overfitting risk is highest. The
MobileNet lighthouse illustrates this principle: when model capacity is
deliberately reduced for edge deployment, augmentation becomes the
primary defense against overfitting.

\phantomsection\label{callout-lighthouseux2a-1.13}
\begin{fbxSimple}{callout-lighthouse}{Lighthouse:}{MobileNet and Aggressive Augmentation}
\phantomsection\label{callout-lighthouse*-1.13}
Our \textbf{MobileNet Lighthouse model}
(\textbf{?@sec-dnn-architectures}) exemplifies how data augmentation
compensates for model capacity constraints. MobileNet's depthwise
separable convolutions reduce parameters by 8--9\(\times\) compared to
standard convolutions, but this efficiency comes at a cost: smaller
models are more prone to overfitting on limited data.

The solution is \textbf{aggressive augmentation}. MobileNet training
typically uses stronger augmentation than ResNet-50 training, including
RandAugment with higher magnitude, more aggressive cropping, and longer
training schedules. The augmentation effectively increases dataset
diversity without increasing model capacity, allowing MobileNet to
achieve near-ResNet accuracy at a fraction of the parameter count. For
edge deployment where both data collection and model size are
constrained, augmentation is essential rather than optional.

\end{fbxSimple}

\textbf{Generative Synthesis: Creating New Samples.} Augmentation
transforms existing samples; synthetic data generation goes further by
creating entirely new examples using generative models. This capability
becomes essential in three common scenarios: when real data is
privacy-sensitive (as with medical records or financial transactions),
when edge cases are rare (such as autonomous driving failure scenarios
that must be covered but seldom occur), or when data collection is
prohibitively expensive (as in robotics or scientific experiments where
each sample requires physical resources).

Three classes of generative approaches address these needs, each with
distinct cost and fidelity trade-offs. Generative Adversarial Networks
(GANs) train a generator against a discriminator in an adversarial
setup, producing realistic images through competition; StyleGAN, for
instance, generates photorealistic faces that have augmented facial
recognition datasets. Diffusion models use iterative denoising to
produce high-quality images; systems like Stable Diffusion enable
text-to-image synthesis, allowing you to generate targeted training
examples from natural language descriptions. Finally, simulation engines
such as CARLA for autonomous driving or Unity and Unreal for robotics
offer physics-based rendering that generates unlimited labeled data with
perfect ground-truth annotations, making them particularly valuable for
safety-critical applications where edge case coverage is essential.

\subsection{Bridging the Domain
Gap}\label{sec-data-selection-bridging-domain-gap-15f5}

Synthetic data's greatest limitation is the \textbf{domain gap}: the
statistical difference between generated and real-world data. A model
trained on perfectly rendered simulation images may fail on blurry,
poorly-lit real camera footage. This gap can negate the efficiency gains
of synthetic data if not addressed.

Figure~\ref{fig-domain-gap} illustrates the problem. Synthetic data
(left distribution) and real data (right distribution) occupy different
regions of feature space. A model trained only on synthetic data learns
a decision boundary that does not transfer to real deployment.

\begin{figure}[htb]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/0609a0767b08f3b6b18d2627a81aca33152b4377.pdf}}

}

\caption{\label{fig-domain-gap}\textbf{The Domain Gap Problem}:
Synthetic data (blue) and real data (orange) have different
distributions. A model trained on synthetic data alone learns a boundary
that fails on real data. Domain adaptation techniques aim to align these
distributions or learn domain-invariant features.}

\end{figure}%

Two complementary strategies address this distribution mismatch. Domain
randomization takes an aggressive approach: rather than trying to match
the real world precisely, it trains on wildly varied synthetic data by
randomizing lighting, textures, backgrounds, and camera parameters
during generation. If the model encounters sufficient variation during
training, the real world becomes ``just another variation'' within its
learned distribution. This strategy produces strong results for robotics
and autonomous driving, where simulation technology is mature enough to
generate physically plausible variations across a wide range.

Domain adaptation takes the opposite approach by explicitly aligning
synthetic and real distributions. Feature alignment methods train on
synthetic data while simultaneously minimizing the distance between
synthetic and real feature distributions, often using adversarial
training to learn domain-invariant representations. Fine-tuning offers a
simpler path: pre-train on abundant synthetic data to learn general
features, then fine-tune on a small real dataset to adapt to deployment
conditions. Self-training combines these ideas by using a
synthetic-trained model to pseudo-label real unlabeled data, then
retraining on the combined labeled set.

In practice, the best results often come from mixing synthetic and real
data rather than relying on either source alone.
Table~\ref{tbl-synthetic-mix} summarizes typical outcomes across
different mixing ratios.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 2\tabcolsep) * \real{0.4000}}
  >{\raggedright\arraybackslash}p{(\linewidth - 2\tabcolsep) * \real{0.6000}}@{}}
\caption{\textbf{Synthetic-to-Real Data Mixing Ratios.} Pure synthetic
data suffers from distribution shift; pure real data is expensive. The
optimal ratio varies by domain but typically falls in the 50--80\%
synthetic range when simulation fidelity is
high.}\label{tbl-synthetic-mix}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Synthetic Fraction}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Typical Outcome}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Synthetic Fraction}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Typical Outcome}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{100\% synthetic} & Poor real-world generalization \\
\textbf{80\% synthetic + 20\% real} & Good performance, significant cost
savings \\
\textbf{50\% synthetic + 50\% real} & Best performance in many
domains \\
\textbf{100\% real} & Baseline (expensive) \\
\end{longtable}

The optimal mix depends on simulation fidelity, domain complexity, and
the cost differential between synthetic and real data. One important
distinction applies when synthetic data comes from ML models rather than
simulators: there is a risk of \emph{model collapse}, where training on
model-generated data amplifies errors and reduces diversity over
generations. This concern is particularly acute for foundation models,
where synthetic data from earlier model generations may contaminate
future training corpora. With appropriate safeguards, synthetic data
generation remains a powerful tool. The following example illustrates
how to combine multiple data selection techniques (augmentation, noise
injection, and simulation) into a coherent strategy for a real
deployment scenario.

\phantomsection\label{callout-exampleux2a-1.14}
\begin{fbxSimple}{callout-example}{Example:}{KWS Data Selection}
\phantomsection\label{callout-example*-1.14}
\textbf{Scenario}: Our \textbf{Keyword Spotting Lighthouse model}
(\textbf{?@sec-dnn-architectures}), a DS-CNN with \textbf{200 K}
parameters, represents the extreme end of data selection challenges. You
are building a wake-word detector (``Hey Device'') for a microcontroller
with 256 KB SRAM (see
\textbf{?@sec-ml-system-architecture-tinyml-ubiquitous-sensing-scale-a67b}
for hardware constraints). The model must be tiny (\textasciitilde50 KB
quantized), but you need 10,000+ labeled audio samples to train it,
samples that do not yet exist.

\textbf{The Data Collection Problem}:

\begin{itemize}
\tightlist
\item
  Recording 10,000 real utterances requires 500+ speakers for diversity
\item
  Professional recording costs \$2--5 per sample (\$20--50K total)
\item
  Target deployment environment (noisy kitchen, car interior) differs
  from recording studio
\end{itemize}

\textbf{Data Selection Solution Stack}:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Seed Data (500 samples)}: Record 50 speakers × 10 utterances
  in controlled conditions
\item
  \textbf{Augmentation (5,000 samples)}: Apply pitch shift, time
  stretch, speed variation to 10\(\times\) the seed data
\item
  \textbf{Noise Injection (10,000 samples)}: Mix clean audio with
  environmental noise (kitchen appliances, HVAC, traffic) sampled from
  AudioSet
\item
  \textbf{Negative Mining}: Use acoustic similarity to find hard
  negatives (``Hey Siri'', ``Hey Google'') from public datasets
\item
  \textbf{Simulation (optional)}: Text-to-speech synthesis with diverse
  voice models
\end{enumerate}

\textbf{Result}: 500 real recordings → 10,000+ training samples at 5\%
of the cost. The noise injection serves as domain randomization,
improving deployment robustness.

\textbf{Key Insight for TinyML}: When the target model is tiny, the data
selection challenge shifts from ``reduce terabytes to gigabytes'' to
``create a useful dataset from almost nothing.'' Augmentation and
simulation become essential rather than optional.

\end{fbxSimple}

\subsection{Knowledge Distillation: Compressing
Information}\label{sec-data-selection-knowledge-distillation-compressing-information-40a5}

The techniques above create new input samples, but there is another form
of synthesis that creates enhanced labels. Knowledge
distillation\sidenote{\textbf{Knowledge Distillation}: Introduced by
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean in 2015. Hinton coined the
evocative term ``dark knowledge'' for the information in soft
probability distributions---the teacher reveals not just which class is
correct but which incorrect classes are most plausible. The temperature
parameter in the softmax function controls how much dark knowledge is
exposed: higher temperatures produce softer distributions that transfer
more nuanced inter-class relationships. }
(\citeproc{ref-hinton2015distilling}{Hinton, Vinyals, and Dean 2015}) is
a data selection technique where a smaller ``student'' model learns from
a larger ``teacher'' model's outputs rather than raw labels. This
section treats distillation as a \emph{data selection} technique, where
the teacher's outputs serve as enriched training data that carries more
information per sample than hard labels.
\textbf{?@sec-model-compression} examines the complementary perspective:
distillation as a model compression technique for producing smaller,
faster student models suitable for resource-constrained deployment.

The key insight is that the teacher's soft predictions contain more
information than hard labels: a teacher predicting {[}0.7, 0.2, 0.1{]}
for three classes reveals inter-class relationships (classes 1 and 2 are
more similar) that a hard label {[}1, 0, 0{]} obscures entirely.

This richer supervision signal enables student models to learn more
efficiently from the same data. From a systems perspective, distillation
is particularly powerful for creating synthetic labels at scale: run a
large model (such as GPT-4) on unlabeled data to generate high-quality
annotations, then train a smaller model on these synthetic labels. The
smaller model inherits much of the teacher's capability at a fraction of
the inference cost, amortizing the expensive teacher computation across
many student deployments.

Together, augmentation, generative synthesis, and distillation complete
the third stage of our data selection pipeline. Where static pruning
removes redundancy and dynamic selection focuses compute on high-value
samples, synthetic generation fills gaps by creating samples that never
existed. These three stages form a complementary toolkit: pruning
reduces what you have, selection focuses how you use it, and synthesis
expands what you can access.

Having examined selection techniques across all three pipeline stages,
we now consolidate their characteristics to guide practical application.

\section{Technique
Summary}\label{sec-data-selection-technique-summary-0ee8}

Table~\ref{tbl-data-selection} summarizes the three-stage optimization
pipeline introduced at the beginning of this chapter.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2030}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1203}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3985}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2782}}@{}}
\caption{\textbf{Three-Stage Data Selection Pipeline.} Each stage
increases ICR by different mechanisms: pruning removes low-value
samples, dynamic selection focuses compute on high-value samples, and
synthesis creates new high-value
samples.}\label{tbl-data-selection}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Stage}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{When Applied}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Techniques}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Typical Gains}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Stage}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{When Applied}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Techniques}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Typical Gains}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{1. Static Pruning} & Before training & Coreset Selection,
Deduplication, Quality Filtering & 30--50\% dataset reduction \\
\textbf{2. Dynamic Selection} & During training & Curriculum Learning,
Active Learning, Semi-Supervised & 10--30\% faster convergence \\
\textbf{3. Synthetic Generation} & On-demand & Augmentation, Generative
Models, Distillation & 2--10\(\times\) effective data expansion \\
\end{longtable}

Table~\ref{tbl-technique-selection} provides a decision guide for
selecting techniques based on your specific constraints.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2857}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2190}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4952}}@{}}
\caption{\textbf{Technique Selection Guide by Primary Constraint.}
Recommended data selection techniques mapped to the dominant resource
constraint in the ML
pipeline.}\label{tbl-technique-selection}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Constraint}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Best Technique}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Why}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Constraint}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Best Technique}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Why}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Limited labeling budget} & Active Learning & Maximizes label ROI
by selecting informative samples \\
\textbf{High redundancy in data} & Deduplication + Coreset & Removes
waste before training begins \\
\textbf{Rare classes or edge cases} & Synthetic Generation & Creates
samples that do not exist in raw data \\
\textbf{Slow convergence} & Curriculum Learning & Improves gradient
quality in early training \\
\textbf{Privacy requirements} & Synthetic Data & Train on generated
data, not real user data \\
\textbf{Large model, small dataset} & Knowledge Distillation & Use
teacher model's knowledge as ``data'' \\
\end{longtable}

\subsection{Decision Framework: Choosing the Right
Technique}\label{sec-data-selection-decision-framework-choosing-right-technique-c36a}

With many techniques available, practitioners need a systematic approach
to selection. Figure~\ref{fig-technique-decision-tree} provides a visual
flowchart that captures the key branching points:

\begin{figure}[htb]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/35e994fea419deec24f9fca9da84e211192ac318.pdf}}

}

\caption{\label{fig-technique-decision-tree}\textbf{Data Selection
Technique Selection Tree}: Start at the top by identifying your primary
bottleneck, then follow the branches to find the most appropriate
technique. Leaf nodes show recommended methods. Multiple paths may
apply; combine techniques as needed.}

\end{figure}%

The following text-based decision tree elaborates on each path, guiding
practitioners from initial bottleneck identification through
implementation.

\textbf{Step 1: Assess Your Bottleneck.} Identify which resource
constraint most severely limits your training pipeline. If labeling cost
dominates your budget, consider label efficiency techniques such as
Active Learning, Semi-Supervised, or Self-Supervised learning. These
methods maximize the value extracted from each human annotation. When
compute cost is the primary concern, prioritize dataset reduction
through Coreset selection, Deduplication, and Curriculum Learning, all
of which reduce the number of training iterations required. If data
scarcity is the fundamental problem, pursue data creation through
Augmentation, Synthesis, and Distillation to expand your effective
training set beyond what raw collection provides.

\textbf{Step 2: Check Prerequisites.} With the bottleneck identified,
verify that the corresponding techniques are feasible given your
infrastructure and data. Each approach carries specific requirements
that must be met before implementation can begin
(Table~\ref{tbl-technique-prerequisites}).

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 2\tabcolsep) * \real{0.2892}}
  >{\raggedright\arraybackslash}p{(\linewidth - 2\tabcolsep) * \real{0.7108}}@{}}
\caption{\textbf{Technique Prerequisites.} Required resources and
capabilities for each data selection technique. Verify these
requirements before committing to
implementation.}\label{tbl-technique-prerequisites}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Technique}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Prerequisites}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Technique}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Prerequisites}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Active Learning} & Access to oracle, unlabeled pool, retraining
infrastructure \\
\textbf{Coreset Selection} & Proxy model or embedding extractor, full
dataset accessible \\
\textbf{Curriculum Learning} & Difficulty scoring method, pacing
schedule \\
\textbf{Semi-Supervised} & Some labeled data, unlabeled data from same
distribution \\
\textbf{Self-Supervised} & Large unlabeled corpus, pre-training compute
budget \\
\textbf{Augmentation} & Domain knowledge of invariances, augmentation
library \\
\textbf{Synthetic Generation} & Generative model or simulator, domain
gap mitigation \\
\end{longtable}

\textbf{Step 3: Estimate ROI.} Meeting the prerequisites is necessary
but not sufficient. Before committing engineering resources, estimate
the return on investment for each candidate technique:

\[
\text{ROI} = \frac{\text{(Baseline Cost)} - \text{(Technique Cost + Implementation Cost)}}{\text{Technique Cost + Implementation Cost}}
\]

A technique with high theoretical gains but high implementation cost may
deliver lower ROI than a simpler approach. Deduplication, for example,
often achieves the highest ROI because implementation cost is minimal
and gains are immediate. Active Learning, by contrast, requires oracle
access, retraining infrastructure, and selection algorithm development,
so its ROI depends heavily on how many labeling cycles you expect to
amortize that investment across.

\textbf{Step 4: Combine Techniques Strategically.} The techniques in
this chapter are not mutually exclusive; in practice, the most effective
pipelines combine multiple approaches. A typical production workflow
begins by deduplicating the raw corpus for immediate gains at minimal
cost. This cleaned dataset then undergoes coreset selection to identify
the most informative samples. During training, curriculum learning
orders these samples to optimize gradient quality, while data
augmentation increases effective diversity at runtime. Finally, starting
from a self-supervised foundation model rather than random
initialization allows the pipeline to leverage knowledge learned from
massive unlabeled corpora.

Each stage compounds the efficiency gains of previous stages, turning
individual percentage improvements into multiplicative savings.

These techniques require infrastructure to operate at scale. The
following sections examine the engineering systems that enable efficient
data selection in production environments.

The preceding sections answer the \emph{what} of data selection: which
samples to prune, when to select dynamically, and how to synthesize new
data. Understanding these algorithmic choices is essential, but
algorithms alone do not translate into faster training. A perfectly
designed coreset algorithm that takes 10 hours to select samples for a
2-hour training run yields no practical benefit. The \emph{how} of
implementation matters as much as the \emph{what} of algorithm choice.

This gap between algorithmic elegance and practical value raises several
systems questions. How do you avoid selection overhead negating your
theoretical gains? How do you handle non-sequential I/O patterns that
confuse prefetching logic? How do you coordinate selection decisions
across distributed workers without introducing synchronization
bottlenecks? The following sections address these engineering
challenges, bridging the gap between data selection theory and
production reality.

\section{Engineering Data Selection
Systems}\label{sec-data-selection-engineering-data-selection-systems-7aef}

The strategies discussed so far (pruning, active learning, and
synthesis) are algorithmic interventions. Implementing them at scale
requires robust systems engineering. A naive active learning loop that
scans the entire dataset every epoch to select the ``best'' samples will
turn a compute-bound training job into an I/O-bound bottleneck. This
section examines the architectural patterns required to implement data
selection in production.

\subsection{The Selection
Bottleneck}\label{sec-data-selection-selection-bottleneck-f1d4}

Dynamic data selection introduces a new bottleneck: \textbf{selection
latency}. In standard training, the data loader reads the next batch
sequentially. In active learning or curriculum learning, the system must
evaluate a selection function \(f(x)\) over a large candidate pool to
determine the next batch.

For a selection strategy to be systems-efficient, it must satisfy the
\textbf{Selection Inequality}:

\[ T_{selection} + T_{train}(N_{subset}) < T_{train}(N_{total}) \]

Here \(T_{selection}\) is the time spent scoring the pool and
\(T_{train}\) is the compute time. If \(f(x)\) requires a forward pass
of a large model, the cost of selection can exceed the cost of training,
producing negative ROI. A concrete scenario illustrates this trade-off.

\phantomsection\label{callout-exampleux2a-1.15}
\begin{fbxSimple}{callout-example}{Example:}{Selection Inequality in Practice}
\phantomsection\label{callout-example*-1.15}
\textbf{Scenario}: You have 1 million training images and want to select
a 100k coreset (10\%) using EL2N scoring.

\textbf{Option A: Full Model Selection}

\begin{itemize}
\tightlist
\item
  Score all 1M images with your target ResNet-50: 1M × 0.01 s =
  \textbf{10,000 seconds} (2.8 hours)
\item
  Train on 100k coreset for 100 epochs: 100k × 100 × 0.01 s =
  \textbf{100,000 seconds} (27.8 hours)
\item
  \textbf{Total: 30.6 hours}
\end{itemize}

\textbf{Option B: Proxy Model Selection}

\begin{itemize}
\tightlist
\item
  Score all 1M images with a small proxy (ResNet-18): 1M × 0.002 s =
  \textbf{2,000 seconds} (0.6 hours)
\item
  Train on 100k coreset for 100 epochs: \textbf{100,000 seconds} (27.8
  hours)
\item
  \textbf{Total: 28.3 hours}
\end{itemize}

\textbf{Baseline: No Selection}

\begin{itemize}
\tightlist
\item
  Train on full 1M dataset for 100 epochs: 1M × 100 × 0.01 s =
  \textbf{1,000,000 seconds} (278 hours)
\end{itemize}

\textbf{Analysis}:

\begin{itemize}
\tightlist
\item
  Option A saves 247 hours vs.~baseline (89\% reduction) ✓
\item
  Option B saves 249 hours vs.~baseline (90\% reduction) ✓
\item
  Option B beats Option A by 2.2 hours. Proxy selection yields better
  ROI.
\end{itemize}

\textbf{The Trap}: If your selection required 50 hours (e.g., running a
7B parameter model), you would spend 77.8 hours total, still better than
baseline, but the selection overhead consumes 25\% of your savings.

\textbf{Rule of thumb}: Selection time should be \textless10\% of subset
training time for good ROI.

\end{fbxSimple}

The following analysis formalizes this heuristic, deriving what we call
\emph{the selection inequality}.

\phantomsection\label{callout-notebookux2a-1.16}
\begin{fbxSimple}{callout-notebook}{AI Engineer’s Notebook:}{The Selection Inequality}
\phantomsection\label{callout-notebook*-1.16}
\textbf{Problem}: You are using active learning to select the best 10\%
of samples for training. Your selection algorithm requires running the
full model on the unlabeled pool. Is this efficient?

\textbf{The Math}:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Full Training}: 100 epochs. Total cost =
  \(100 \times C_{epoch}\).
\item
  \textbf{Selection (Full Model)}: Scoring the full dataset is
  equivalent to \textbf{1 epoch} of training.
  \(T_{selection} = 1 \times C_{epoch}\).
\item
  \textbf{Subset Training}: 100 epochs on 10\% data =
  \(100 \times 0.1 \times C_{epoch} = 10 \times C_{epoch}\).
\item
  \textbf{Total Time}: \(1 + 10 = \mathbf{11 \times C_{epoch}}\).
\item
  \textbf{Speedup}: \(100 / 11 \approx \mathbf{9\times}\).
\end{enumerate}

\textbf{The Trap}: If your selection algorithm is iterative (e.g.,
repeating selection every epoch), \(T_{selection}\) becomes
\(100 \times 1 = 100 \times C_{epoch}\). Total time =
\(100 + 10 = 110 \times C_{epoch}\). You are now \textbf{slower} than
the baseline.

\textbf{The Failure Condition}: If the cost of selecting data exceeds
the cost of training on the discarded data, you have failed. The goal is
to spend compute to save \emph{more} compute.

\textbf{The Fix}: Use a \textbf{Proxy Model} (\(10\times\) smaller) for
selection. \(T_{selection} = 0.1 \times C_{epoch}\). Total time =
\(0.1 + 10 = 10.1\). You preserve the speedup, as
Figure~\ref{fig-selection-inequality} illustrates.

\end{fbxSimple}

\begin{figure}

\centering{

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{contents/vol1/data_selection/data_selection_files/figure-pdf/fig-selection-inequality-output-1.pdf}}

}

\subcaption{\label{fig-selection-inequality}\textbf{The Selection
Inequality}: Data selection only improves end-to-end efficiency if the
overhead of selection plus training on the subset is less than training
on the full dataset. A lightweight selection function (proxy model,
cached embeddings) keeps selection overhead low; an expensive selection
function (full model forward pass) can negate the savings.}

}

\caption{\label{fig-selection-inequality}\textbf{The Selection
Inequality}: Data selection only improves end-to-end efficiency if the
overhead of selection plus training on the subset is less than training
on the full dataset. A lightweight selection function (proxy model,
cached embeddings) keeps selection overhead low; an expensive selection
function (full model forward pass) can negate the savings.}

\end{figure}%

Figure~\ref{fig-selection-inequality} captures a critical point:
selection overhead can negate the benefits of training on a smaller
subset. Before examining hardware-aware optimizations, verify your
understanding of this trade-off.

\phantomsection\label{callout-checkpointux2a-1.17}
\begin{fbxSimple}{callout-checkpoint}{Checkpoint:}{The Selection Inequality}
\phantomsection\label{callout-checkpoint*-1.17}

Data selection is not free. It introduces a new term to the Iron Law.

\textbf{The Equation}

\begin{itemize}
\tightlist
\item[$\square$]
  \textbf{Selection Cost}: Do you understand why
  \(T_{selection} + T_{train}(subset)\) must be less than
  \(T_{train}(full)\) for the technique to be valid?
\item[$\square$]
  \textbf{Overhead Management}: How do proxy models and cached
  embeddings keep \(T_{selection}\) low?
\end{itemize}

\textbf{Systems Implications}

\begin{itemize}
\tightlist
\item[$\square$]
  \textbf{I/O patterns}: Why does random access (required for dynamic
  selection) kill data loader throughput compared to sequential reads?
\end{itemize}

\end{fbxSimple}

\subsection{Hardware Empathy: The Random Access
Penalty}\label{sec-data-selection-hardware-empathy-random-access-penalty-f9c1}

Data selection strategies like coresets or dynamic sampling often
require \textbf{random access} to samples across the dataset. Standard
training uses sequential reads that benefit from hardware readahead and
OS page caching; random access patterns devastate throughput, especially
on distributed filesystems or traditional hard drives.
Table~\ref{tbl-io-performance} quantifies this penalty across storage
tiers.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.1455}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2273}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.1909}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2727}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.1636}}@{}}
\caption{}\label{tbl-io-performance}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Storage Tier}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Sequential Throughput}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Random I/O (IOPS)}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Random Throughput (approx)}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Random Penalty}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Storage Tier}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Sequential Throughput}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Random I/O (IOPS)}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Random Throughput (approx)}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Random Penalty}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{HDD (7.2k)} & \textasciitilde150 MB/s & \textasciitilde80 &
\textasciitilde0.3 MB/s & \textbf{500x} \\
\textbf{SATA SSD} & \textasciitilde550 MB/s & \textasciitilde10k &
\textasciitilde40 MB/s & \textbf{14x} \\
\textbf{NVMe SSD} & \textasciitilde3,500 MB/s & \textasciitilde500k &
\textasciitilde2,000 MB/s & \textbf{1.75x} \\
\textbf{Cloud (S3)} & \textasciitilde100 MB/s (per conn) &
\textasciitilde10-50ms (lat) & Very Low (per conn) & \textbf{Extreme} \\
\end{longtable}

High-efficiency systems mitigate this penalty through two proxy
selection techniques. The first uses small proxy models: a distilled,
lightweight model (e.g., a 10M parameter ``student'') scores the data
pool on behalf of a 7B parameter ``teacher,'' reducing selection cost by
an order of magnitude while preserving most of the ranking quality. The
second leverages embedding indices: by pre-computing embeddings and
storing them in a vector search index (e.g., FAISS), selection
transforms from a \(O(N)\) linear scan into a \(O(\log N)\)
nearest-neighbor lookup. Both techniques share a common principle:
decoupling selection computation from training computation enables
independent optimization of each stage.

\subsection{Optimizing Data
Loaders}\label{sec-data-selection-optimizing-data-loaders-b6c2}

Data loaders themselves also require architectural adaptation. Data
selection strategies often produce non-sequential access patterns.
Standard training reads files sequentially, optimizing disk readahead,
but strategies like dynamic subset selection require random access to
specific high-value samples. Standard filesystems and object stores (S3)
suffer significant latency penalties under random access loads, as
Table~\ref{tbl-io-performance} demonstrates.

To maintain GPU utilization, data loaders must be architected for
sharded random access. Modern formats like WebDataset or FFCV group
thousands of samples into \texttt{tar} or \texttt{record} shards,
enabling efficient bulk reads even when the target samples are scattered
across the logical dataset. Complementing this, shuffle buffers provide
a practical approximation to true random access: the loader reads large
sequential shards into a memory buffer and samples randomly from within
the buffer. This design preserves the sequential I/O throughput that
storage hardware delivers best while still achieving the statistical
benefits of random sampling that many data selection algorithms require.

\subsection{Data Echoing: Amortizing I/O
Costs}\label{sec-data-selection-data-echoing-amortizing-io-costs-24e3}

\phantomsection\label{sec-data-selection-augmented-pipeline-parallelism-ee1b}{}The
optimizations discussed so far address I/O bandwidth, but modern data
selection pipelines introduce another bottleneck: CPU computation.
Synthetic data generation and heavy augmentation shift the constraint
from disk speed to augmentation throughput. Heavy augmentations like 3D
rotations and MixUp, or on-the-fly generative synthesis, can starve the
GPU if the CPU cannot keep pace with sample production. When the data
pipeline produces samples slower than the GPU can consume them, GPU
utilization drops and training time extends, negating the efficiency
gains from smarter data selection.

Data echoing (\citeproc{ref-choi2020dataechoing}{Choi et al. 2020})
offers an elegant solution to this CPU-GPU imbalance. The technique
reuses batches of data multiple times before fetching new samples,
effectively trading sample diversity for GPU utilization. When the data
pipeline (reading, decoding, augmenting) is slower than GPU processing,
the GPU idles waiting for data. Data echoing fills this gap by
``echoing'' (repeating) each batch \(e\) times, applying different
augmentations to each repetition so that the model still sees varied
inputs.

The optimal echo factor depends on the ratio \(R\) of upstream
processing time to downstream training time:

\[
R = \frac{T_{\text{data pipeline}}}{T_{\text{GPU training}}}
\]

If \(R > 1\) (data pipeline is the bottleneck), set echo factor
\(e \leq R\) to fully utilize GPU capacity. If \(R < 1\) (GPU is the
bottleneck), data echoing provides no benefit. The following worked
example calculates these trade-offs for a realistic scenario.

\phantomsection\label{callout-exampleux2a-1.18}
\begin{fbxSimple}{callout-example}{Example:}{Worked Example: Data Echoing ROI}
\phantomsection\label{callout-example*-1.18}
\textbf{Scenario}: Training ResNet-50 on ImageNet with heavy
augmentation (RandAugment + MixUp).

\textbf{Measurements}:

\begin{itemize}
\tightlist
\item
  Data pipeline throughput: 300 images/second (reading, decoding,
  augmenting on CPU)
\item
  GPU training throughput: 800 images/second (forward + backward pass)
\item
  Ratio \(R = 800/300 = 2\.67\) (GPU waiting 62\% of time)
\end{itemize}

\textbf{Without Echoing}:

\begin{itemize}
\tightlist
\item
  Effective throughput: 300 images/second (limited by data pipeline)
\item
  Training time for 90 epochs: 90 × 1.28M / 300 = \textbf{384,000
  seconds (107 hours)}
\item
  GPU utilization: \textasciitilde38\%
\end{itemize}

\textbf{With Echo Factor \(e = 2\)}:

\begin{itemize}
\tightlist
\item
  Each batch is processed twice with different augmentations
\item
  Effective throughput: 600 images/second (still below GPU capacity)
\item
  Unique images per second: 300 (unchanged)
\item
  Training time: 90 × 1.28M / 600 = \textbf{192,000 seconds (53 hours)}
  if echoed data is equally valuable
\end{itemize}

\textbf{But echoed data has diminishing returns}: Research shows echoed
samples provide approximately 70--90\% of the value of fresh samples,
depending on augmentation diversity. Empirically, Choi et al.~measured a
\textbf{3.25\(\times\) speedup} on ResNet-50 ImageNet training when
reading data over a network, with minimal accuracy degradation.

\textbf{The Trade-Off}: Data echoing trades sample diversity for GPU
utilization. It works best when:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Augmentation is diverse (each echo sees different transforms)
\item
  The dataset is already somewhat redundant
\item
  The echo factor \(e\) stays below the critical threshold
  (\textasciitilde{}\(4\times\) for ImageNet)
\end{enumerate}

Above this threshold, the model starts memorizing and accuracy degrades.

\end{fbxSimple}

Data echoing also interacts with batch normalization. When the same
image appears multiple times in a batch (or across nearby batches),
batch normalization statistics become less representative of the true
data distribution. This correlation violates the independence assumption
underlying batch normalization's effectiveness. Practitioners address
this by excluding consecutive echoes from the same batch or by
maintaining separate batch normalization statistics for echoed samples.

These engineering patterns provide production-ready implementations of
data selection principles. Proxy selection reduces the computational
cost of identifying valuable samples. Sharded formats and shuffle
buffers reconcile random access algorithms with sequential storage
hardware. Data echoing maximizes GPU utilization when the data pipeline
becomes the bottleneck. Together, they transform data selection from an
algorithmic idea into a deployable system. The question then becomes:
which techniques merit investment for a given workload?

\section{Cost Modeling and
Economics}\label{sec-data-selection-cost-modeling-economics-b702}

The systems framing of data selection demands quantitative answers:
\emph{Should I label 10,000 more samples or buy more GPU hours? When
does active learning pay for itself? What is the ROI of investing in
deduplication infrastructure?}

\subsection{Quantifying Data Costs and
ROI}\label{sec-data-selection-total-cost-training-data-92b9}

Answering these questions requires understanding what training data
actually costs. Total expense encompasses the full lifecycle of data
acquisition, preparation, and utilization, extending well beyond storage
fees:

\[
C_{\text{total}} = C_{\text{acquire}} + C_{\text{label}} + C_{\text{store}} + C_{\text{process}}
\]

where:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.1967}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4344}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3689}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Component}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Formula}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Typical Range}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{\(C_{\text{acquire}}\)} & \(N \times c_{\text{sample}}\) &
\$0.001--\$10/sample (web scrape vs.~licensed) \\
\textbf{\(C_{\text{label}}\)} &
\(N_{\text{labeled}} \times c_{\text{label}}\) & \$0.10--\$100/sample
(crowd vs.~expert) \\
\textbf{\(C_{\text{store}}\)} &
\(S_{\text{bytes}} \times c_{\text{storage}} \times T\) &
\$0.02--\$0.10/GB/month \\
\textbf{\(C_{\text{process}}\)} & \(N \times E \times c_{\text{FLOP}}\)
& Proportional to training FLOPs \\
\end{longtable}

For a concrete example, consider training a vision model:

\phantomsection\label{callout-exampleux2a-1.19}
\begin{fbxSimple}{callout-example}{Example:}{Cost Breakdown: ImageNet-Scale Training}
\phantomsection\label{callout-example*-1.19}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3443}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.1311}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.5246}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Cost Component}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Calculation}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Amount}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Raw data (1.2M images)} & Licensed dataset & \$50,000 \\
\textbf{Labels (1.2M × \$0.05)} & Crowd annotation & \$60,000 \\
\textbf{Storage (150 GB × 12 months)} & Cloud storage & \$200 \\
\textbf{Training (100 epochs × 8 A100s × 24 h)} & GPU compute &
\$25,000 \\
\textbf{Total} & & \textbf{\$135,200} \\
\textbf{Data vs.~Compute ratio} & & \textbf{82\% data, 18\% compute} \\
\end{longtable}

This ratio, where data costs dominate, is typical for supervised
learning. The ratio inverts for self-supervised learning on web-scraped
data, where compute dominates.

\end{fbxSimple}

\textbf{ROI Framework for Data Selection Techniques.} Understanding
total costs enables rational decisions about which efficiency techniques
merit investment. Every technique carries both a cost (implementation
effort, compute overhead) and a benefit (reduced data requirements,
faster training). Comparing these trade-offs requires a common
framework: \textbf{Return on Investment (ROI)}.

\[
\text{ROI} = \frac{\text{Savings} - \text{Investment}}{\text{Investment}} \times 100\%
\]

The challenge lies in quantifying both sides accurately. Different
techniques offer distinct cost-benefit profiles:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.1544}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4044}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4412}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Technique}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Investment (Cost)}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Savings (Benefit)}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Deduplication} & One-time compute for hashing + infrastructure &
Reduced storage, fewer epochs for same accuracy \\
\textbf{Coreset Selection} & Proxy model training + selection compute &
Train on 10--50\% of data with minimal accuracy loss \\
\textbf{Active Learning} & Inference on unlabeled pool +
human-in-the-loop latency & 2--10\(\times\) reduction in labeling budget
for same acc. \\
\textbf{Data Augmentation} & CPU/GPU cycles for transforms & Effective
dataset size increase without new data acquisition \\
\end{longtable}

\subsection{Break-Even
Analysis}\label{sec-data-selection-breakeven-analysis-ec3a}

ROI calculations assume that techniques deliver their promised benefits,
but actual outcomes vary. For any technique, there exists a
\textbf{break-even point} where investment equals savings. Below this
threshold, the technique costs more than it saves; above it, the
technique generates value. Identifying this threshold determines whether
a technique makes sense for a given project.

\textbf{Example: Active Learning Break-Even}

Suppose labeling costs \$10/sample and active learning requires:

\begin{itemize}
\tightlist
\item
  Initial labeled set: 1,000 samples (\$10,000)
\item
  Oracle queries per round: 100 samples
\item
  Inference cost per round: \$50 (scoring unlabeled pool)
\item
  Target accuracy achievable with 5,000 random samples
\end{itemize}

If active learning reaches target accuracy with only 2,000 labeled
samples:

\[
\text{Random labeling cost} = 5000 \times \$10 = \$50,000
\]

\[
\text{Active learning cost} = 2000 \times \$10 + 10 \text{ rounds} \times \$50 = \$20,500
\]

\[
\text{ROI} = \frac{\$50,000 - \$20,500}{\$20,500} \times 100\% = 144\%
\]

The break-even occurs when the labeling reduction equals the selection
overhead. If active learning only reduces labeling by 20\%, and
selection overhead is high, ROI may be negative.

\subsection{Amortization: The Time Value of Data
Selection}\label{sec-data-selection-amortization-time-value-data-selection-26c5}

Break-even analysis captures a snapshot in time, but many data selection
investments span multiple projects. Techniques with high upfront costs
yield significant returns when their benefits compound across repeated
training runs. \textbf{Amortized ROI} accounts for this temporal
dimension:

\[
\text{Amortized ROI} = \frac{N_{runs} \times \text{Per-Run Savings} - \text{One-Time Investment}}{\text{One-Time Investment}}
\]

\textbf{Example: Deduplication Infrastructure}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 2\tabcolsep) * \real{0.6029}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 2\tabcolsep) * \real{0.3971}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Component}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Cost}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Build deduplication pipeline} & \$50,000 (engineering time) \\
\textbf{Compute MinHash signatures (one-time)} & \$5,000 \\
\textbf{Per-run savings (20\% less data)} & \$10,000/run \\
\end{longtable}

\begin{longtable}[]{@{}lr@{}}
\toprule\noalign{}
\textbf{Number of Runs} & \textbf{Amortized ROI} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
1 run & -82\% (net loss) \\
5 runs & -9\% (near break-even) \\
10 runs & +82\% (positive) \\
50 runs & +809\% (highly profitable) \\
\end{longtable}

This pattern reveals which circumstances favor infrastructure
investment. Data selection investments deliver the highest returns under
three conditions: training runs repeat frequently (hyperparameter
search, model iterations, or scheduled retraining); datasets are shared
across multiple teams or model architectures; and the technique
generalizes broadly. Deduplication exemplifies a high-transfer
investment because it benefits all models trained on the cleaned
dataset. Task-specific coresets, by contrast, may not transfer across
architectures, limiting their amortization potential. For one-off
training runs, simple techniques like random sampling or basic
augmentation often yield better ROI than sophisticated methods requiring
substantial infrastructure investment. The following guidelines
summarize these considerations.

\phantomsection\label{callout-perspectiveux2a-1.20}
\begin{fbxSimple}{callout-perspective}{Systems Perspective:}{When to Invest in Data Selection}
\phantomsection\label{callout-perspective*-1.20}

\textbf{High ROI scenarios:}

\begin{itemize}
\tightlist
\item
  Labeling is expensive (medical, legal, scientific domains)
\item
  Dataset is large and redundant (web-scraped corpora)
\item
  Training runs are repeated frequently (hyperparameter search,
  retraining)
\item
  Iteration speed matters more than final accuracy
\end{itemize}

\textbf{Low ROI scenarios:}

\begin{itemize}
\tightlist
\item
  Labeling is cheap or already done
\item
  Dataset is small and curated
\item
  Single training run (one-time cost)
\item
  Accuracy matters more than efficiency
\end{itemize}

\end{fbxSimple}

\section{Distributed Data
Selection}\label{sec-data-selection-distributed-data-selection-25a8}

Everything discussed so far assumes a single-machine view: one process
can see the entire dataset, compute global statistics, and make
coordinated selection decisions. Production ML training breaks this
assumption. When data is sharded across hundreds of workers, each seeing
only a local slice, fundamental questions arise: How do you compute a
global coreset when no single node sees all samples? How do you maintain
consistent curriculum difficulty rankings when the model updates
asynchronously across workers?

The distributed training infrastructure that underlies these
challenges---including collective communication, fault tolerance, and
elastic scheduling---constitutes an advanced topic beyond this chapter's
scope. This section focuses specifically on how data selection
techniques adapt to distributed settings, and where they fail to do so.

\subsection{Strategies for Distributed
Selection}\label{sec-data-selection-strategies-distributed-selection-fcd6}

\phantomsection\label{sec-data-selection-distributed-selection-problem-24b5}{}In
standard distributed training, data parallelism is straightforward:
shard the dataset across workers, each processes its shard
independently. Data selection techniques, however, introduce
\textbf{selection dependencies}:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2347}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3163}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4490}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Technique}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Single-Node Assumption}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Distributed Challenge}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Coreset Selection} & Global view of dataset & Each worker sees
only its shard \\
\textbf{Active Learning} & Centralized uncertainty scoring & Scoring
requires model synchronization \\
\textbf{Curriculum Learning} & Global difficulty ordering & Workers may
have different ``hardest'' samples \\
\textbf{Deduplication} & Hash table fits in memory & Distributed hash
tables add latency \\
\end{longtable}

These selection dependencies admit several architectural solutions, each
navigating a different point in the consistency-scalability trade-off
space.

The most straightforward approach centralizes selection while
distributing training. A coordinator node performs selection on the full
dataset, then distributes selected indices to workers. This preserves
selection quality but introduces a single bottleneck:

\begin{verbatim}
Coordinator: score_all_samples() → selected_indices
Broadcast: selected_indices → all workers
Workers: train on subset(local_shard, selected_indices)
\end{verbatim}

The semantics remain clean, but the coordinator becomes a single point
of failure and a bandwidth bottleneck for large selections. For modest
cluster sizes, this overhead is acceptable; for thousand-node
deployments, it becomes prohibitive.

Hierarchical selection addresses this scalability limitation by
distributing the selection computation itself. Each worker performs
local selection on its shard, then a coordinator merges results:

\begin{verbatim}
Workers: local_selected = select_top_k(local_shard)
Coordinator: global_selected = merge_and_rerank(all local_selected)
Broadcast: final_indices → all workers
\end{verbatim}

This approach reduces coordinator load substantially, but introduces a
quality trade-off: the system may miss globally important samples that
appear unimportant within their local shard. A sample that is only
moderately difficult on one worker might be the hardest example in the
entire dataset when considered globally.

When even hierarchical approaches prove too expensive, approximate
global selection offers a fallback. These methods trade exactness for
scalability through distributed approximate algorithms. Distributed
MinHash enables deduplication by having each worker compute MinHash
signatures independently; signatures are then aggregated to find
near-duplicates across shards without requiring any single node to see
all the data. Similarly, federated uncertainty sampling allows workers
to compute local uncertainty scores, with a global threshold determined
by score distribution statistics rather than exact ranking.

\subsection{Consistency Challenges in Active
Learning}\label{sec-data-selection-consistency-challenges-active-learning-c071}

The approximate selection strategies above assume static selection
criteria, but active learning introduces an additional complication: the
model changes during selection. Consider what happens when Worker A
scores samples using the model at step \(t\) while Worker B
simultaneously updates the model to step \(t+1\). Worker A's scores are
now stale and may select samples that the updated model would rank
differently.

Several strategies mitigate this staleness problem, each with distinct
overhead characteristics. Synchronous scoring forces all workers to
pause training and score simultaneously, guaranteeing consistency but at
substantial cost in GPU utilization. Periodic score refresh offers a
middle ground by re-scoring every \(k\) epochs rather than every batch,
trading freshness for reduced overhead. The most robust approach selects
samples that exhibit high uncertainty under multiple model checkpoints,
ensuring that selection decisions remain valid even as the model
evolves. The following example demonstrates how these distributed
selection strategies combine in practice.

\phantomsection\label{callout-exampleux2a-1.21}
\begin{fbxSimple}{callout-example}{Example:}{Distributed Coreset Selection}
\phantomsection\label{callout-example*-1.21}
\textbf{Scenario}: Select a 10\% coreset from ImageNet (1.2M images)
using 8 workers with 4 GPUs each.

\textbf{Architecture}:

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/a948730bf8decf7906c57523950ce8c30bf40ed7.pdf}}

\textbf{Pipeline}:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Embedding phase} (parallel): Each worker computes ResNet-18
  embeddings for its shard → store in shared filesystem
\item
  \textbf{Deduplication phase} (distributed): Coordinator builds FAISS
  index, workers query for near-duplicates → remove 15\% duplicates
\item
  \textbf{Scoring phase} (parallel): Each worker computes EL2N scores on
  its deduplicated shard using proxy model
\item
  \textbf{Selection phase} (centralized): Coordinator collects top-20\%
  scores from each worker, re-ranks globally, selects final 10\%
\item
  \textbf{Broadcast}: Selected indices distributed to all workers for
  training
\end{enumerate}

\textbf{Performance} (measured on \(8\times\) A100 cluster):

\begin{itemize}
\tightlist
\item
  Embedding: 20 minutes (parallel)
\item
  Deduplication: 15 minutes (distributed hash join)
\item
  Scoring: 30 minutes (parallel, 5 epochs proxy training)
\item
  Selection: 2 minutes (centralized)
\item
  \textbf{Total overhead: 67 minutes} for \(10\times\) training speedup
\end{itemize}

\textbf{Key insight}: The 67-minute selection overhead pays for itself
if full training takes \textgreater12 hours. For ImageNet with modern
architectures, full training is \textasciitilde24 hours, so coreset
selection has clear positive ROI.

\end{fbxSimple}

This positive ROI can erode quickly when workers must coordinate
frequently during training. Distributed data selection always incurs a
\emph{coordination tax}: the overhead of maintaining consistent
selection across workers. This tax must be smaller than the efficiency
gains, or distributed selection yields negative ROI. As a rule of thumb,
if selection overhead exceeds 10\% of training time, simplify the
selection strategy or increase the selection interval.

The preceding sections treated data selection as an isolated
optimization: techniques to reduce dataset size, select better samples,
or generate synthetic data. But real ML systems combine multiple
optimizations simultaneously. A coreset-trained model will be quantized.
A curriculum-learning pipeline will run on specialized accelerators. How
do these optimizations interact?

\section{Interactions with Other
Optimizations}\label{sec-data-selection-interactions-optimizations-02bc}

Data selection does not exist in isolation. Its interactions with other
optimization techniques range from complementary to conflicting, and
understanding these interactions helps practitioners design end-to-end
efficient systems rather than optimizing components independently.

\textbf{Data Selection and Model Compression.} Model compression
(\textbf{?@sec-model-compression}) reduces the size of the trained model
through pruning, quantization, and distillation. The training dataset
directly affects how compressible the resulting model becomes. Perhaps
counterintuitively, models trained on smaller, higher-quality datasets
may be \emph{more} compressible than those trained on larger, noisier
ones.

The mechanism behind this effect relates to how models encode
information. A model trained on repetitive data learns redundant
features that pruning later removes. The training compute required to
learn those features was wasted, only to be discarded during
compression. By contrast, a model trained on diverse, informative
samples learns compact, non-redundant representations from the start,
making subsequent compression more effective.

Empirical evidence supports this relationship. In experiments on
ImageNet, models trained on 50\% coresets selected by EL2N compress to
4-bit precision with 2\% less accuracy loss than models trained on the
full dataset. The curated training led to cleaner weight distributions
that quantize more gracefully.

Data selection and model compression are therefore \emph{complementary}.
The techniques in this chapter can reduce both training cost \emph{and}
post-training compression effort. When planning an efficiency pipeline,
apply data selection first; the resulting model will be easier to
compress.

\textbf{Data Selection and Hardware Acceleration.} While model
compression affects what happens after training, hardware acceleration
determines how efficiently training itself proceeds. Hardware
acceleration (\textbf{?@sec-ai-acceleration}) increases throughput
through specialized accelerators, kernel optimization, and
parallelization. Data selection affects which hardware bottlenecks
dominate, and this relationship is more nuanced than simple speedup
calculations suggest.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2762}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3905}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Scenario}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Likely Bottleneck}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Hardware Optimization}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Large, sequential dataset} & Memory bandwidth & Larger batch
sizes, gradient accumulation \\
\textbf{Small, curated dataset} & Compute (GPU idle waiting for data) &
Faster data loaders, data echoing \\
\textbf{Dynamic selection} & Selection compute & Proxy models, cached
embeddings \\
\end{longtable}

Data selection can therefore shift the system from one bottleneck regime
to another. A technique that reduces dataset size by 80\% may move the
bottleneck from I/O to GPU compute, requiring different hardware
optimizations. Before applying aggressive data reduction, profile your
system to understand which bottleneck you're targeting.

\textbf{Data Selection and Distributed Training.} The hardware
bottleneck analysis above assumes single-machine training. The
interactions become more complex when scaling to multiple machines,
because data selection affects different parallelism strategies in
distinct ways.

Under strong scaling, where a fixed dataset is distributed across more
workers, data selection reduces communication overhead by reducing
gradient updates per epoch. Fewer samples means fewer synchronization
points, and communication costs often dominate at large worker counts.
Under weak scaling, where each worker processes more data as the cluster
grows, data selection techniques can maintain accuracy while adding
workers without proportionally increasing total data. This capability
proves essential when data collection rather than compute is the
bottleneck. Even within straightforward data parallelism, smaller
curated datasets reduce per-worker shard sizes, potentially improving
cache utilization and reducing I/O stalls on each node.

These benefits must be weighed against the distributed selection
challenges discussed in
Section~\ref{sec-data-selection-distributed-data-selection-25a8}. A
technique that works well on a single GPU may incur prohibitive
coordination overhead across 1,000 workers, negating its efficiency
gains.

\subsection{The Optimization
Stack}\label{sec-data-selection-optimization-stack-05ca}

The preceding sections examined pairwise interactions, but production
systems apply all these optimizations together. The full optimization
stack, from data to deployment, can be visualized as a pipeline where
each stage amplifies or attenuates the effects of others, as shown in
Figure~\ref{fig-optimization-stack}.

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/bb9ca5fab84d3d4ff917cf3c2cf530b1515e8fa2.pdf}}

}

\caption{\label{fig-optimization-stack}\textbf{The Optimization Stack}:
The complete pipeline from raw data to deployed system, showing how
optimizations at each stage propagate downstream. Data artifacts
(rounded boxes) flow through processing stages (rectangular boxes).
Optimizations early in the pipeline---particularly data selection---have
multiplicative effects because they reduce the workload for all
subsequent stages.}

\end{figure}%

The pipeline in Figure~\ref{fig-optimization-stack} reveals why data
selection occupies a strategic position: it sits at the head of the
optimization stack. Reducing the dataset by 50\% through intelligent
selection doesn't just halve data processing time---it halves the
training compute, which in turn produces a model that may require less
aggressive compression, which then demands less from hardware
acceleration. Each downstream stage inherits the efficiency gains (or
quality losses) from upstream decisions.

This \textbf{multiplicative effect} means that every FLOP saved in data
processing is a FLOP that never needs to be executed, compressed, or
accelerated. Conversely, poor data selection that degrades model quality
forces downstream stages to compensate---either through longer training,
less aggressive compression, or over-provisioned hardware.

How do we quantify this multiplicative effect? How do we know whether a
50\% dataset reduction actually delivers 50\% compute savings, or
whether it has inadvertently degraded model quality in ways that surface
only in production?

\section{Measuring Data
Selection}\label{sec-data-selection-measuring-data-selection-7957}

The techniques in this chapter (coreset selection, active learning,
augmentation) all claim to improve efficiency, and rigorous measurement
separates effective techniques from intuition.

\subsection{Core Metrics}\label{sec-data-selection-core-metrics-c0b5}

\textbf{Performance-Per-Data (PPD)}: The most direct metric measures
accuracy gain per sample:

\[
\text{PPD}(n) = \frac{\text{Accuracy}(n) - \text{Accuracy}(0)}{n}
\]

where \(n\) is the number of training samples. A higher PPD indicates
more efficient use of data. The key insight is that PPD exhibits
\textbf{diminishing returns}: the first 10,000 samples contribute far
more to model performance than the next 10,000.

\textbf{Area Under the Learning Curve (AULC)}: Rather than comparing at
a single point, AULC integrates performance across all dataset sizes:

\[
\text{AULC} = \int_0^N \text{Accuracy}(n) \, dn
\]

A data-efficient strategy has higher AULC because it achieves good
accuracy faster. This metric is particularly useful for comparing
coreset selection algorithms.

\textbf{Data Compression Ratio (DCR)}: For coreset methods, measure how
much data reduction is achieved at a target accuracy:

\[
\text{DCR} = \frac{N_{\text{full}}}{N_{\text{coreset}}} \text{ at } \text{Accuracy}_{\text{target}}
\]

A DCR of 5\(\times\) means the coreset achieves target accuracy with
20\% of the data.

\subsection{The Compute-Optimal
Frontier}\label{sec-data-selection-data-roofline-model-b400}

The metrics above measure individual techniques. But how do you diagnose
whether your \emph{overall} training strategy is data-limited or
compute-limited? Scaling laws provide the answer.

Research on neural scaling laws (\citeproc{ref-kaplan2020scaling}{Kaplan
et al. 2020}; \citeproc{ref-hoffmann2022training}{Hoffmann et al. 2022})
established that model performance follows predictable power laws with
respect to compute, data, and model size. The Chinchilla study
(\citeproc{ref-hoffmann2022training}{Hoffmann et al. 2022}) revealed a
key insight: for any fixed compute budget, there exists an
\textbf{optimal balance} between model size and training data. Train on
too little data relative to model size, and you waste compute on an
undertrained model. Train on too much data with too small a model, and
you waste data on a model that cannot absorb it.

This optimal balance defines a \textbf{compute-optimal frontier}: the
best achievable performance at each compute budget when data and model
size are properly balanced (Figure~\ref{fig-compute-optimal-frontier}).

\begin{figure}[htb]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/1e80115006883b4492d88be9bc7dcacbf915af83.pdf}}

}

\caption{\label{fig-compute-optimal-frontier}\textbf{The Compute-Optimal
Frontier}: For any training compute budget, there is a best achievable
performance when data and model size are optimally balanced (green
curve). Operating points below the frontier indicate inefficiency.
\textbf{Data-starved} systems (orange) have compute capacity but
insufficient quality data---the techniques in this chapter move them
toward the frontier. \textbf{Compute-starved} systems (red) have quality
data but insufficient training budget---hardware acceleration or
distributed training helps here. The goal is to operate \emph{on} the
frontier, extracting maximum performance from available resources.}

\end{figure}%

\textbf{Diagnosing your position}: The frontier provides a practical
diagnostic framework:

\begin{itemize}
\tightlist
\item
  \textbf{Data-starved} (orange): You have training compute available,
  but performance falls short of what the frontier predicts. The
  bottleneck is data quality or quantity. \emph{Solution}: Apply the
  techniques from this chapter---deduplication, coreset selection,
  curriculum learning, or synthetic augmentation---to extract more
  learning per sample.
\item
  \textbf{Compute-starved} (red): You have high-quality data, but
  insufficient compute to fully exploit it. Adding more data will not
  help. \emph{Solution}: Invest in hardware acceleration
  (\textbf{?@sec-ai-acceleration}), longer training runs, or distributed
  training.
\item
  \textbf{On the frontier} (purple): Data and compute are balanced. You
  are extracting maximum value from both resources. Further improvement
  requires increasing \emph{both} data quality and compute
  proportionally.
\end{itemize}

\textbf{The Chinchilla rule of thumb}: For compute-optimal training, the
number of training tokens should scale roughly as
\(D_{opt} \propto C^{0.5}\)---doubling your compute budget means you
should increase data by about 40\%, not 100\%. This explains why the
Data Wall is so constraining: as compute grows exponentially, the demand
for quality data grows with its square root, but even that slower growth
outpaces the supply of high-quality human-generated content.

\textbf{Applying the diagnostic}: If your training run underperforms
expectations, ask: \emph{Am I data-starved or compute-starved?} A simple
test: train for 2\(\times\) longer. If performance improves
substantially, you were compute-starved. If it plateaus quickly, you are
data-starved and need better data, not more training. The techniques in
this chapter address the data-starved regime; hardware acceleration and
distributed training address the compute-starved regime.

Figure~\ref{fig-ppd-curve} illustrates diminishing returns visually. A
data-efficient selection strategy (blue) reaches the performance plateau
much faster than random sampling (gray). The gap between the curves at
any dataset size represents the efficiency opportunity: compute that
could be saved by smarter data curation.

\begin{figure}

\centering{

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{contents/vol1/data_selection/data_selection_files/figure-pdf/fig-ppd-curve-output-1.pdf}}

}

\subcaption{\label{fig-ppd-curve}\textbf{Diminishing Returns of Data}:
Random sampling (gray) versus data-efficient selection (blue). The
efficient strategy achieves higher performance with less data, reaching
the convergence plateau much earlier. The red arrow shows the efficiency
gap at a fixed dataset size.}

}

\caption{\label{fig-ppd-curve}\textbf{Diminishing Returns of Data}:
Random sampling (gray) versus data-efficient selection (blue). The
efficient strategy achieves higher performance with less data, reaching
the convergence plateau much earlier. The red arrow shows the efficiency
gap at a fixed dataset size.}

\end{figure}%

\textbf{The practical question} for practitioners: at what point should
you stop collecting data and start curating it? When does adding more
samples waste compute rather than improve accuracy? These questions
require rigorous metrics: ways to quantify diminishing returns, compare
selection strategies, and evaluate the cost-effectiveness of different
data sources. Data selection techniques (coreset selection, active
learning, deduplication) all make implicit claims about the value of
different samples. Validating that a curated dataset actually preserves
model quality requires systematic benchmarking: coverage metrics
validate that coreset selection preserved representation across classes
and demographic groups; distribution alignment metrics detect whether
the curated training set drifted from the deployment distribution; and
label quality metrics (inter-annotator agreement, confident learning)
validate that active learning did not introduce systematic labeling
errors. A 50\% dataset reduction is only valuable if benchmarking
confirms the model maintains target accuracy, calibration, and
robustness.

For a comprehensive treatment of data selection metrics and benchmarking
methodologies, including how initiatives like DataPerf are standardizing
evaluation protocols, see \textbf{?@sec-benchmarking-ai}. That chapter
provides the measurement framework needed to quantify the ROI of the
techniques introduced here. Before concluding, let us revisit how data
selection applies to our Lighthouse Models.

\phantomsection\label{callout-lighthouseux2a-1.22}
\begin{fbxSimple}{callout-lighthouse}{Lighthouse:}{Lighthouse Data Selection}
\phantomsection\label{callout-lighthouse*-1.22}
This chapter has applied data selection principles to all five
Lighthouse Models, demonstrating that the techniques are universal but
the priorities differ by bottleneck:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.1639}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.1803}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.6557}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Lighthouse}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Primary Bottleneck}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Data Selection Priority}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{ResNet-50} & Compute & Coreset selection directly reduces
training FLOPs \\
\textbf{GPT-2/Llama} & Memory bandwidth & Deduplication reduces corpus
size; curriculum learning improves token efficiency \\
\textbf{MobileNet} & Latency/Power & Aggressive augmentation compensates
for reduced model capacity \\
\textbf{DLRM} & Memory capacity & Interaction deduplication and
embedding pruning reduce table size \\
\textbf{Keyword Spotting} & Extreme constraints & Augmentation and
synthesis create datasets from minimal seeds \\
\end{longtable}

The common thread: \textbf{data selection is not a single technique but
a systems optimization} tailored to whichever resource is most
constrained.

\end{fbxSimple}

\section{Fallacies and
Pitfalls}\label{sec-data-selection-fallacies-pitfalls-6285}

With metrics in hand, practitioners often rush to implement techniques
without recognizing the conceptual traps and implementation errors that
undermine their efforts. The following fallacies represent persistent
misconceptions about data selection, while the pitfalls capture
practical mistakes that sabotage otherwise sound strategies.
Understanding both is essential for translating theory into production
gains.

\paragraph*{\texorpdfstring{Fallacy: \emph{Data is the new oil, so more
is always
better.}}{Fallacy: Data is the new oil, so more is always better.}}\label{fallacy-data-is-the-new-oil-so-more-is-always-better.}
\addcontentsline{toc}{paragraph}{Fallacy: \emph{Data is the new oil, so
more is always better.}}

The ``Data is Oil'' metaphor fails to capture diminishing returns. A
saturation point exists where adding terabytes of data yields negligible
accuracy gains while exploding compute costs. Data is better understood
as fuel with specific energy density: high-quality, curated data (high
octane) powers models more efficiently than vast quantities of raw data
(crude oil).

\paragraph*{\texorpdfstring{Fallacy: \emph{Synthetic data can completely
replace real
data.}}{Fallacy: Synthetic data can completely replace real data.}}\label{fallacy-synthetic-data-can-completely-replace-real-data.}
\addcontentsline{toc}{paragraph}{Fallacy: \emph{Synthetic data can
completely replace real data.}}

While synthetic data addresses scarcity, it is bounded by the
generator's knowledge. A model trained purely on synthetic data from
another model risks ``Model Collapse,'' a degenerative feedback loop
where errors are amplified. Synthetic data augments, but rarely
replaces, the grounding provided by real-world distributions. It is best
used to fill gaps in the data manifold, not to define it.

\paragraph*{\texorpdfstring{Fallacy: \emph{Data selection is just data
cleaning.}}{Fallacy: Data selection is just data cleaning.}}\label{fallacy-data-selection-is-just-data-cleaning.}
\addcontentsline{toc}{paragraph}{Fallacy: \emph{Data selection is just
data cleaning.}}

Cleaning (removing errors) is necessary but insufficient. True data
selection involves \emph{selection} (finding the decision boundary) and
\emph{synthesis} (creating hard negatives). You can have a perfectly
clean dataset that is highly inefficient because it is filled with
redundant, easy examples. Efficiency requires optimizing the information
content, not just the hygiene.

\paragraph*{\texorpdfstring{Fallacy: \emph{Data selection is only for
resource-constrained
settings.}}{Fallacy: Data selection is only for resource-constrained settings.}}\label{fallacy-data-selection-is-only-for-resource-constrained-settings.}
\addcontentsline{toc}{paragraph}{Fallacy: \emph{Data selection is only
for resource-constrained settings.}}

Many practitioners view data selection as a corner-case optimization for
TinyML or budget-limited startups, irrelevant when training foundation
models with massive budgets. In reality, data selection is \emph{most}
valuable at scale. A 10\% efficiency gain on a \$100M training run saves
\$10M. The Data Wall affects frontier labs more acutely than anyone;
they have the compute but lack the data. The techniques in this chapter
are increasingly adopted by exactly those organizations with
``unlimited'' resources.

These conceptual misunderstandings often lead to flawed strategies.
Equally damaging are the implementation pitfalls that arise when correct
strategies meet messy engineering realities.

\paragraph*{\texorpdfstring{Pitfall: \emph{Optimizing selection without
measuring selection
overhead.}}{Pitfall: Optimizing selection without measuring selection overhead.}}\label{pitfall-optimizing-selection-without-measuring-selection-overhead.}
\addcontentsline{toc}{paragraph}{Pitfall: \emph{Optimizing selection
without measuring selection overhead.}}

A sophisticated coreset algorithm that takes 10 hours to select samples
for a 2-hour training run has negative ROI. Always measure the Selection
Inequality: \(T_{selection} + T_{train}(subset) < T_{train}(full)\). Use
lightweight proxy models or cached embeddings for selection, and profile
selection time alongside training time.

\paragraph*{\texorpdfstring{Pitfall: \emph{Pruning rare classes into
oblivion.}}{Pitfall: Pruning rare classes into oblivion.}}\label{pitfall-pruning-rare-classes-into-oblivion.}
\addcontentsline{toc}{paragraph}{Pitfall: \emph{Pruning rare classes
into oblivion.}}

Aggressive coreset selection often removes rare classes entirely because
they contribute little to average loss. The model then fails
catastrophically on these classes in production. Stratify selection by
class and set minimum samples per class before applying any pruning
algorithm.

\paragraph*{\texorpdfstring{Pitfall: \emph{Training on deduplicated data
while evaluating on duplicated test
sets.}}{Pitfall: Training on deduplicated data while evaluating on duplicated test sets.}}\label{pitfall-training-on-deduplicated-data-while-evaluating-on-duplicated-test-sets.}
\addcontentsline{toc}{paragraph}{Pitfall: \emph{Training on deduplicated
data while evaluating on duplicated test sets.}}

If your test set contains duplicates of training samples (common in
web-scraped data), deduplication appears to hurt performance when it
actually improves generalization. Deduplicate train and test sets
jointly, or use truly held-out evaluation data.

\paragraph*{\texorpdfstring{Pitfall: \emph{Active learning without
considering annotation
latency.}}{Pitfall: Active learning without considering annotation latency.}}\label{pitfall-active-learning-without-considering-annotation-latency.}
\addcontentsline{toc}{paragraph}{Pitfall: \emph{Active learning without
considering annotation latency.}}

Active learning theory assumes instant oracle responses. In practice,
getting expert labels takes days or weeks. By the time labels arrive,
the model has moved on, and the selected samples may no longer be
optimal. Select larger batches to amortize latency and use diversity
sampling to hedge against model drift.

A subtler class of errors emerges when practitioners assume that
benchmark results transfer directly to their specific domains and
deployment contexts.

\paragraph*{\texorpdfstring{Fallacy: \emph{If a technique works on
ImageNet, it will work on my
dataset.}}{Fallacy: If a technique works on ImageNet, it will work on my dataset.}}\label{fallacy-if-a-technique-works-on-imagenet-it-will-work-on-my-dataset.}
\addcontentsline{toc}{paragraph}{Fallacy: \emph{If a technique works on
ImageNet, it will work on my dataset.}}

Data selection results are highly dataset-dependent. CIFAR-10 is highly
redundant, so 50\% coresets work well. ImageNet has moderate redundancy.
Domain-specific datasets (medical imaging, satellite imagery, scientific
data) may have near-zero redundancy, where every sample captures unique
information. A coreset that preserves 95\% accuracy on ImageNet may fail
catastrophically on a well-curated radiology dataset. Always pilot data
selection techniques on your specific distribution. The ``free lunch''
ratios reported in benchmark papers (50\% pruning, 10\(\times\) label
reduction) rarely transfer directly. Start with conservative pruning
(20--30\%) and validate on held-out data before aggressive reduction.

\paragraph*{\texorpdfstring{Pitfall: \emph{Optimizing data selection
metrics instead of deployment
metrics.}}{Pitfall: Optimizing data selection metrics instead of deployment metrics.}}\label{pitfall-optimizing-data-selection-metrics-instead-of-deployment-metrics.}
\addcontentsline{toc}{paragraph}{Pitfall: \emph{Optimizing data
selection metrics instead of deployment metrics.}}

A team achieves excellent PPD (Performance-Per-Data) and DCR (Data
Compression Ratio) during development, having created a beautifully
efficient 10\% coreset. At deployment, the model fails on edge cases:
rare classes, unusual lighting conditions, demographic subgroups
underrepresented in the coreset. The efficiency metrics looked great;
the production metrics are catastrophic. Include deployment-relevant
evaluation in data selection optimization. If the task requires 99.9\%
reliability on edge cases, ensure the coreset \emph{oversamples} those
cases, even if it reduces average PPD. Stratify evaluation by subgroup,
not just overall accuracy. The goal is deployment success, not benchmark
efficiency.

\section{Summary}\label{sec-data-selection-summary-8b2a}

This chapter opened with a question: why do smaller, curated datasets
sometimes outperform massive ones? The answer lies in recognizing data
selection as a \emph{systems} problem rather than a purely statistical
one. Where traditional machine learning asks ``how few samples achieve
target accuracy?'', the systems perspective asks ``how do we minimize
total cost across the entire pipeline?''

This reframing transforms how practitioners approach the ML development
lifecycle. Rather than treating data as a static input to be collected
and labeled, data selection treats it as a dynamic resource to be
engineered. The goal is minimizing total cost across compute, storage,
labeling, energy, and time, not merely maximizing accuracy.

We explored the three-stage optimization pipeline: \textbf{Static
Pruning} removes redundancy before training through coreset selection
and deduplication; \textbf{Dynamic Selection} focuses compute on
informative examples during training through curriculum and active
learning; and \textbf{Synthetic Generation} creates data where none
exists through augmentation, simulation, and distillation. Together,
these strategies address the ``Data Wall,'' the fundamental asymmetry
between exponentially growing compute and slowly growing high-quality
data.

The self-supervised learning paradigm represents a ceiling of data
selection: by eliminating task-specific labels entirely, foundation
models achieve 1,000\(\times\) multipliers on downstream tasks through
cost amortization. This paradigm shift from ``train from scratch'' to
``pre-train once, fine-tune many'' has become the dominant approach in
production ML precisely because of its superior data economics.

\phantomsection\label{callout-takeawaysux2a-1.23}
\begin{fbxSimple}{callout-takeaways}{Takeaways:}{Key Takeaways}
\phantomsection\label{callout-takeaways*-1.23}

\begin{itemize}
\tightlist
\item
  \textbf{Data selection is a systems problem}: The goal is reduced cost
  across the entire pipeline (compute, storage, labeling, energy), not
  just ``fewer samples for same accuracy.''
\item
  \textbf{Start with deduplication}: Deduplication typically offers the
  highest return on investment: low cost, immediate gains, and no
  accuracy penalty. Deduplication should precede sophisticated selection
  methods.
\item
  \textbf{The Selection Inequality must hold}:
  \(T_{selection} + T_{train}(subset) < T_{train}(full)\). Selection
  overhead should be kept below 10\% of training time using proxy models
  or cached embeddings.
\item
  \textbf{Amortization determines ROI}: Data selection techniques are
  most effective when training repeats (hyperparameter search) or
  datasets are reused across multiple teams. For one-off training,
  simpler methods often outperform sophisticated approaches.
\item
  \textbf{Fine-tuning typically outperforms training from scratch}:
  Self-supervised pre-training amortizes cost across applications.
  Fine-tuning typically requires 100× fewer labels and 10× less per-task
  compute (when amortized) than from-scratch training.
\item
  \textbf{Avoid exclusive use of synthetic data}: Mixing 50--80\%
  synthetic with 20--50\% real data typically yields better results.
  Domain gap without mitigation can significantly degrade real-world
  performance.
\end{itemize}

\end{fbxSimple}

The techniques explored throughout this chapter (deduplication, coreset
selection, curriculum learning, active learning, and synthetic
generation) provide practitioners with a systematic toolkit for breaking
through the Data Wall. Organizations that master these techniques gain
compound advantages: reduced labeling budgets, faster iteration cycles,
lower storage costs, and models that generalize better because they
learn from higher-quality examples rather than redundant noise.

\phantomsection\label{callout-chapter-connectionux2a-1.24}
\begin{fbxSimple}{callout-chapter-connection}{What’s Next:}{From Data to Algorithms}
\phantomsection\label{callout-chapter-connection*-1.24}
With high-quality data in hand, we have optimized the source of the
system. But even the best data cannot make an inefficient model run fast
on constrained hardware. In \textbf{?@sec-model-compression}, we move
from optimizing \emph{what} the system learns to optimizing \emph{how}
it represents that knowledge, applying pruning, quantization, and
knowledge distillation to reduce the computational cost of the model
artifact itself.

\end{fbxSimple}

\phantomsection\label{refs}
\begin{CSLReferences}{1}{0}
\bibitem[\citeproctext]{ref-angluin1988queries}
Angluin, Dana. 1988. {``Queries and Concept Learning.''} \emph{Machine
Learning} 2 (4): 319--42. \url{https://doi.org/10.1023/a:1022821128753}.

\bibitem[\citeproctext]{ref-bengio2009curriculum}
Bengio, Yoshua, Jérôme Louradour, Ronan Collobert, and Jason Weston.
2009. {``Curriculum Learning.''} In \emph{Proceedings of the 26th Annual
International Conference on Machine Learning}, 41--48. ACM.
\url{https://doi.org/10.1145/1553374.1553380}.

\bibitem[\citeproctext]{ref-bommasani2021opportunities}
Bommasani, Rishi, Drew A. Hudson, Ehsan Adeli, Russ Altman, Simran
Arora, Sydney von Arx, Michael S. Bernstein, et al. 2021. {``On the
Opportunities and Risks of Foundation Models.''} \emph{arXiv Preprint
arXiv:2108.07258}, August. \url{http://arxiv.org/abs/2108.07258v3}.

\bibitem[\citeproctext]{ref-broder1997resemblance}
Broder, A. Z. n.d. {``On the Resemblance and Containment of
Documents.''} In \emph{Proceedings. Compression and Complexity of
SEQUENCES 1997 (Cat. No.97TB100171)}, 21--29. IEEE Comput. Soc.
\url{https://doi.org/10.1109/sequen.1997.666900}.

\bibitem[\citeproctext]{ref-chen2020simclr}
Chen, Ting, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton.
2020. {``A Simple Framework for Contrastive Learning of Visual
Representations.''} In \emph{International Conference on Machine
Learning}, 1597--607. PMLR.

\bibitem[\citeproctext]{ref-chen2020mocov2}
Chen, Xinlei, Haoqi Fan, Ross Girshick, and Kaiming He. 2020.
{``Improved Baselines with Momentum Contrastive Learning.''} \emph{arXiv
Preprint arXiv:2003.04297}, March.
\url{http://arxiv.org/abs/2003.04297v1}.

\bibitem[\citeproctext]{ref-choi2020dataechoing}
Choi, Dami, Christopher J Shallue, Zachary Nado, Jaehoon Lee, Chris J
Maddison, and George E Dahl. 2020. {``Data Echoing for Efficient
Training.''} In \emph{International Conference on Machine Learning}.

\bibitem[\citeproctext]{ref-he2020momentum}
He, Kaiming, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. 2020.
{``Momentum Contrast for Unsupervised Visual Representation Learning.''}
In \emph{2020 IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR)}, 9726--35. IEEE.
\url{https://doi.org/10.1109/cvpr42600.2020.00975}.

\bibitem[\citeproctext]{ref-hinton2015distilling}
Hinton, Geoffrey, Oriol Vinyals, and Jeff Dean. 2015. {``Distilling the
Knowledge in a Neural Network,''} March.
\url{https://doi.org/10.1002/0471743984.vse0673}.

\bibitem[\citeproctext]{ref-hochbaum1985best}
Hochbaum, Dorit S., and David B. Shmoys. 1985. {``A Best Possible
Heuristic for the k-Center Problem.''} \emph{Mathematics of Operations
Research} 10 (2): 180--84.

\bibitem[\citeproctext]{ref-hoffmann2022training}
Hoffmann, Jordan, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya,
Trevor Cai, Eliza Rutherford, Diego de Las Casas, et al. 2022.
{``Training Compute-Optimal Large Language Models.''} \emph{arXiv
Preprint arXiv:2203.15556}, March.
\url{http://arxiv.org/abs/2203.15556v1}.

\bibitem[\citeproctext]{ref-kaplan2020scaling}
Kaplan, Jared, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin
Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario
Amodei. 2020. {``Scaling Laws for Neural Language Models.''} \emph{arXiv
Preprint arXiv:2001.08361}, January.
\url{http://arxiv.org/abs/2001.08361v1}.

\bibitem[\citeproctext]{ref-lee2022deduplicating}
Lee, Katherine, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas
Eck, Chris Callison-Burch, and Nicholas Carlini. 2021. {``Deduplicating
Training Data Makes Language Models Better.''} \emph{arXiv Preprint
arXiv:2107.06499}, July. \url{http://arxiv.org/abs/2107.06499v2}.

\bibitem[\citeproctext]{ref-paul2021deep}
Paul, Mansheej, Surya Ganguli, and Gintare Karolina Dziugaite. 2021.
{``Deep Learning on a Data Diet: Finding Important Examples Early in
Training.''} In \emph{Advances in Neural Information Processing
Systems}, 34:20596--607.

\bibitem[\citeproctext]{ref-ren2021survey}
Ren, Pengzhen, Yun Xiao, Xiaojun Chang, Po-Yao Huang, Zhihui Li, Brij B.
Gupta, Xiaojiang Chen, and Xin Wang. 2021. {``A Survey of Deep Active
Learning.''} \emph{ACM Computing Surveys} 54 (9): 1--40.
\url{https://doi.org/10.1145/3472291}.

\bibitem[\citeproctext]{ref-settles2009active}
Settles, Burr. 2012. \emph{Active Learning}. \emph{University of
Wisconsin-Madison Department of Computer Sciences}. Vol. 1648. Springer
International Publishing.
\url{https://doi.org/10.1007/978-3-031-01560-1}.

\bibitem[\citeproctext]{ref-villalobos_ho_sevilla_besiroglu_heim_hobbhahn_2024}
Sevilla, Jaime, Lennart Heim, Anson Ho, Tamay Besiroglu, Marius
Hobbhahn, and Pablo Villalobos. 2022. {``Compute Trends Across Three
Eras of Machine Learning.''} In \emph{2022 International Joint
Conference on Neural Networks (IJCNN)}, 1--8. IEEE.
\url{https://doi.org/10.1109/ijcnn55064.2022.9891914}.

\bibitem[\citeproctext]{ref-sohn2020fixmatch}
Sohn, Kihyuk, David Berthelot, Nicholas Carlini, Zizhao Zhang, Han
Zhang, Colin A Raffel, Ekin Dogus Cubuk, Alexey Kurakin, and Chun-Liang
Li. 2020. {``FixMatch: Simplifying Semi-Supervised Learning with
Consistency and Confidence.''} In \emph{Advances in Neural Information
Processing Systems}, 33:596--608.

\bibitem[\citeproctext]{ref-soviany2022curriculum}
Soviany, Petru, Radu Tudor Ionescu, Paolo Rota, and Nicu Sebe. 2022.
{``Curriculum Learning: A Survey.''} \emph{International Journal of
Computer Vision} 130 (6): 1526--65.
\url{https://doi.org/10.1007/s11263-022-01611-x}.

\bibitem[\citeproctext]{ref-villalobos2022will}
Villalobos, Pablo, Anson Ho, Jaime Sevilla, Tamay Besiroglu, Lennart
Heim, and Marius Hobbhahn. 2022. {``Will We Run Out of Data? Limits of
LLM Scaling Based on Human-Generated Data.''} \emph{arXiv Preprint
arXiv:2211.04325}, October. \url{http://arxiv.org/abs/2211.04325v2}.

\bibitem[\citeproctext]{ref-zhang2018mixup}
Zhang, Xinhua. 2011. {``Empirical Risk Minimization.''} In
\emph{Encyclopedia of Machine Learning}, 312--12. Springer US.
\url{https://doi.org/10.1007/978-0-387-30164-8/_251}.

\end{CSLReferences}


\backmatter

\clearpage


\end{document}
