% Options for packages loaded elsewhere
% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode,linktoc=all,pdfwindowui,pdfpagemode=FullScreen,pdfpagelayout=TwoPageRight}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  9pt,
  letterpaper,
  abstract,
  titlepage]{scrbook}
\usepackage{xcolor}
\usepackage[left=1in,marginparwidth=2.0666666666667in,textwidth=4.1333333333333in,marginparsep=0.3in]{geometry}
\usepackage{amsmath,amssymb}
\setcounter{secnumdepth}{3}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
% Make \paragraph and \subparagraph free-standing
\makeatletter
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}{
    \@ifstar
      \xxxParagraphStar
      \xxxParagraphNoStar
  }
  \newcommand{\xxxParagraphStar}[1]{\oldparagraph*{#1}\mbox{}}
  \newcommand{\xxxParagraphNoStar}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}{
    \@ifstar
      \xxxSubParagraphStar
      \xxxSubParagraphNoStar
  }
  \newcommand{\xxxSubParagraphStar}[1]{\oldsubparagraph*{#1}\mbox{}}
  \newcommand{\xxxSubParagraphNoStar}[1]{\oldsubparagraph{#1}\mbox{}}
\fi
\makeatother

\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{242,244,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{#1}}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.82,0.10,0.26}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{#1}}
\newcommand{\BuiltInTok}[1]{\textcolor[rgb]{0.88,0.40,0.10}{#1}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.41,0.45,0.49}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.86,0.20,0.18}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.58,0.00,0.30}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{#1}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.75,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.88,0.40,0.10}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.42,0.37,0.78}{#1}}
\newcommand{\ImportTok}[1]{\textcolor[rgb]{0.00,0.45,0.70}{\textbf{#1}}}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.73,0.49,0.84}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.10,0.10,0.10}{#1}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{#1}}
\newcommand{\RegionMarkerTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.12,0.55,0.30}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{#1}}}}

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\newsavebox\pandoc@box
\newcommand*\pandocbounded[1]{% scales image to fit in text height/width
  \sbox\pandoc@box{#1}%
  \Gscale@div\@tempa{\textheight}{\dimexpr\ht\pandoc@box+\dp\pandoc@box\relax}%
  \Gscale@div\@tempb{\linewidth}{\wd\pandoc@box}%
  \ifdim\@tempb\p@<\@tempa\p@\let\@tempa\@tempb\fi% select the smaller of both
  \ifdim\@tempa\p@<\p@\scalebox{\@tempa}{\usebox\pandoc@box}%
  \else\usebox{\pandoc@box}%
  \fi%
}
% Set default figure placement to htbp
\def\fps@figure{htbp}
\makeatother
% definitions for citeproc citations
\NewDocumentCommand\citeproctext{}{}
\NewDocumentCommand\citeproc{mm}{%
  \begingroup\def\citeproctext{#2}\cite{#1}\endgroup}
\makeatletter
 % allow citations to break across lines
 \let\@cite@ofmt\@firstofone
 % avoid brackets around text for \cite:
 \def\@biblabel#1{}
 \def\@cite#1#2{{#1\if@tempswa , #2\fi}}
\makeatother
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newenvironment{CSLReferences}[2] % #1 hanging-indent, #2 entry-spacing
 {\begin{list}{}{%
  \setlength{\itemindent}{0pt}
  \setlength{\leftmargin}{0pt}
  \setlength{\parsep}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1
   \setlength{\leftmargin}{\cslhangindent}
   \setlength{\itemindent}{-1\cslhangindent}
  \fi
  % set entry spacing
  \setlength{\itemsep}{#2\baselineskip}}}
 {\end{list}}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{\hfill\break\parbox[t]{\linewidth}{\strut\ignorespaces#1\strut}}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}

% =============================================================================
% LATEX HEADER CONFIGURATION FOR MLSYSBOOK PDF
% =============================================================================
% This file contains all LaTeX package imports, custom commands, and styling
% definitions for the PDF output of the Machine Learning Systems textbook.
%
% Key Features:
% - Harvard crimson branding throughout
% - Custom part/chapter/section styling
% - Professional table formatting with colored headers
% - Margin notes with custom styling
% - TikZ-based part dividers
% - Page numbering (Roman for frontmatter, Arabic for mainmatter)
%
% Note: This file is included via _quarto-pdf.yml and affects PDF output only.
% HTML/EPUB styling is handled separately via CSS files.
% =============================================================================

% =============================================================================
% PACKAGE IMPORTS
% =============================================================================

% Layout and positioning
% \usepackage[outercaption, ragged]{sidecap}  % Commented out to make figure captions inline instead of in margin
\usepackage{adjustbox}      % Adjusting box dimensions
\usepackage{afterpage}      % Execute commands after page break
\usepackage{morefloats}     % Increase number of floats
\usepackage{array}          % Enhanced table column formatting
\usepackage{atbegshi}       % Insert content at page beginning
%\usepackage{changepage}     % Change page dimensions mid-document
\usepackage{emptypage}      % Clear headers/footers on empty pages

% Language and text
\usepackage[english]{babel} % English language support
\usepackage{microtype}      % Improved typography and hyphenation

% Captions and floats
\usepackage{caption}
% Caption styling configuration
%\captionsetup[table]{belowskip=5pt}
\captionsetup{format=plain}
\DeclareCaptionLabelFormat{mylabel}{#1
#2:\hspace{1.0ex}}
\DeclareCaptionFont{ninept}{\fontsize{7pt}{8}\selectfont #1}

% Figure captions: Small font, bold label, ragged right
\captionsetup[figure]{labelfont={bf,ninept},labelsep=space,
belowskip=2pt,aboveskip=6pt,labelformat=mylabel,
justification=raggedright,singlelinecheck=false,font={ninept}}

% Table captions: Small font, bold label, ragged right
\captionsetup[table]{belowskip=6pt,labelfont={bf,ninept},labelsep=none,
labelformat=mylabel,justification=raggedright,singlelinecheck=false,font={ninept}}

% Typography fine-tuning
\emergencystretch=5pt       % Allow extra stretch to avoid overfull boxes

% Utility packages
\usepackage{etoolbox}       % For patching commands and environments

% Page layout and headers
\usepackage{fancyhdr}       % Custom headers and footers
\usepackage{geometry}       % Page dimensions and margins

% Graphics and figures
\usepackage{graphicx}       % Include graphics
\usepackage{float}          % Improved float placement
\usepackage[skins,breakable]{tcolorbox} % Coloured and framed text boxes
\tcbset{before upper=\setlength{\parskip}{3pt}}

% Tables
\usepackage{longtable}      % Multi-page tables

% Fonts and typography
\usepackage{fontspec}       % Font selection for LuaLaTeX
\usepackage{mathptmx}       % Times-like math fonts
\usepackage{newpxtext}      % Palatino-like font for body text

% Colors and visual elements
\usepackage[dvipsnames]{xcolor}  % Extended color support
\usepackage{tikz}           % Programmatic graphics
\usetikzlibrary{positioning}
\usetikzlibrary{calc}
\usepackage{tikzpagenodes}  % TikZ positioning relative to page

% Code listings
\usepackage{listings}       % Code highlighting

% Hyperlinks
\usepackage{hyperref}       % Clickable links in PDF

% Conditional logic
\usepackage{ifthen}         % If-then-else commands

% Math symbols
\usepackage{amsmath}        % AMS math extensions
\usepackage{amssymb}        % AMS math symbols
\usepackage{latexsym}       % Additional LaTeX symbols
\usepackage{pifont}         % Zapf Dingbats symbols
\providecommand{\blacklozenge}{\ding{117}}  % Black diamond symbol

% Lists
\usepackage{enumitem}       % Customizable lists

% Margin notes and sidenotes
\usepackage{marginfix}      % Fixes margin note overflow
\usepackage{marginnote}     % Margin notes
\usepackage{sidenotes}      % Academic-style sidenotes
\renewcommand\raggedrightmarginnote{\sloppy}
\renewcommand\raggedleftmarginnote{\sloppy}

% Typography improvements
\usepackage{ragged2e}       % Better ragged text
\usepackage[all]{nowidow}   % Prevent widows and orphans
\usepackage{needspace}      % Ensure minimum space on page

% Section formatting
\usepackage[explicit]{titlesec}  % Custom section titles
\usepackage{tocloft}        % Table of contents formatting

% QR codes and icons
\usepackage{fontawesome5}   % Font Awesome icons
\usepackage{qrcode}         % QR code generation
\qrset{link, height=15mm}

% =============================================================================
% FLOAT CONFIGURATION
% =============================================================================
% Allow more floats per page to handle figure-heavy chapters
\extrafloats{200}
\setcounter{topnumber}{12}       % Max floats at top of page
\setcounter{bottomnumber}{12}    % Max floats at bottom of page
\setcounter{totalnumber}{24}     % Max floats per page
\setcounter{dbltopnumber}{8}     % Max floats at top of two-column page
\renewcommand{\topfraction}{.95}  % Max fraction of page for top floats
\renewcommand{\bottomfraction}{.95}
\renewcommand{\textfraction}{.05}  % Min fraction of page for text
\renewcommand{\floatpagefraction}{.7}  % Min fraction of float page
\renewcommand{\dbltopfraction}{.95}

% Prevent "Float(s) lost" errors by flushing floats more aggressively
\usepackage{placeins}  % Provides \FloatBarrier

% =============================================================================
% COLOR DEFINITIONS
% =============================================================================
% Harvard crimson - primary brand color used throughout
\definecolor{crimson}{HTML}{A51C30}

% Quiz element colors
\definecolor{quiz-question-color1}{RGB}{225,243,248}  % Light blue background
\definecolor{quiz-question-color2}{RGB}{17,158,199}   % Blue border
\definecolor{quiz-answer-color1}{RGB}{250,234,241}    % Light pink background
\definecolor{quiz-answer-color2}{RGB}{152,14,90}      % Magenta border

% =============================================================================
% LIST FORMATTING
% =============================================================================
% Tighter list spacing for academic style
\def\tightlist{}
\setlist{itemsep=1pt, parsep=1pt, topsep=0pt,after={\vspace{0.3\baselineskip}}}
\let\tightlist\relax

\makeatletter
\@ifpackageloaded{framed}{}{\usepackage{framed}}
\@ifpackageloaded{fancyvrb}{}{\usepackage{fancyvrb}}
\makeatother

\makeatletter
%New float "codelisting" has been updated
\AtBeginDocument{%
\floatstyle{ruled}
\newfloat{codelisting}{!htb}{lop}
\floatname{codelisting}{Listing}
\floatplacement{codelisting}{!htb}
\captionsetup[codelisting]{labelfont={bf,ninept},labelformat=mylabel,
  singlelinecheck=false,width=\linewidth,labelsep=none,font={ninept}}%
\renewenvironment{snugshade}{%
   \def\OuterFrameSep{3pt}%
   \def\FrameCommand{\fboxsep=5pt\colorbox{shadecolor}}%
   \MakeFramed{\advance\hsize-\width\FrameRestore}%
   \leftskip 0.5em \rightskip 0.5em%
   \small% decrease font size
   }{\endMakeFramed}%
}
\makeatother

%The space before and after the verbatim environment "Highlighting" has been reduced
\fvset{listparameters=\setlength{\topsep}{0pt}\setlength{\partopsep}{0pt}}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{framesep=0mm,commandchars=\\\{\}}

\makeatletter
\renewcommand\fs@ruled{\def\@fs@cfont{\bfseries}\let\@fs@capt\floatc@ruled
\def\@fs@pre{\hrule height.8pt depth0pt \kern2pt}%
\def\@fs@post{\kern2pt\hrule\relax}%
\def\@fs@mid{\kern2pt\hrule\kern1pt}%space between float and caption
\let\@fs@iftopcapt\iftrue}
\makeatother


% =============================================================================
% HYPHENATION RULES
% =============================================================================
% Explicit hyphenation points for technical terms to avoid bad breaks
\hyphenation{
  light-weight
  light-weight-ed
  de-vel-op-ment
  un-der-stand-ing
  mod-els
  prin-ci-ples
  ex-per-tise
  com-pli-cat-ed
  blue-print
  per‧for‧mance
  com-mu-ni-ca-tion
  par-a-digms
  hy-per-ten-sion
  a-chieved
}

% =============================================================================
% CODE LISTING CONFIGURATION
% =============================================================================
% Settings for code blocks using listings package
\lstset{
breaklines=true,              % Automatic line wrapping
breakatwhitespace=true,       % Break at whitespace only
basicstyle=\ttfamily,         % Monospace font
frame=none,                   % No frame around code
keepspaces=true,              % Preserve spaces
showspaces=false,             % Don't show space characters
showtabs=false,               % Don't show tab characters
columns=flexible,             % Flexible column width
belowskip=0pt,               % Minimal spacing
aboveskip=0pt
}

% =============================================================================
% PAGE GEOMETRY
% =============================================================================
% MIT Press trim size: 7" x 10" (per publisher specifications)
% This is a standard academic textbook format providing good readability
% for technical content with figures and code blocks.
% Wide outer margin accommodates sidenotes/margin notes.
\geometry{
  paperwidth=7in,
  paperheight=10in,
  top=0.875in,
  bottom=0.875in,
  inner=0.875in,              % Inner margin (binding side)
  outer=1.75in,               % Outer margin (includes space for sidenotes)
  footskip=30pt,
  marginparwidth=1.25in,      % Width for margin notes
  twoside                     % Different left/right pages
}

% =============================================================================
% SIDENOTE STYLING
% =============================================================================
% Custom sidenote design with crimson vertical bar
\renewcommand{\thefootnote}{\textcolor{crimson}{\arabic{footnote}}}

% Save original sidenote command
\makeatletter
\@ifundefined{oldsidenote}{
  \let\oldsidenote\sidenote%
}{}
\makeatother

% Redefine sidenote with vertical crimson bar
\renewcommand{\sidenote}[1]{%
  \oldsidenote{%
    \noindent
    \color{crimson!100}                        % Crimson vertical line
    \raisebox{0em}{%
      \rule{0.5pt}{1.5em}                      % Thin vertical line
    }
    \hspace{0.3em}                             % Space after line
    \color{black}                              % Reset text color
    \footnotesize #1                           % Sidenote content
  }%
}

% =============================================================================
% FLOAT HANDLING
% =============================================================================
% Patch LaTeX's output routine to handle float overflow gracefully
% The "Float(s) lost" error occurs in \@doclearpage when \@currlist is not empty
% This patch silently clears pending floats that can't be placed
\makeatletter
\let\orig@doclearpage\@doclearpage
\def\@doclearpage{%
  \ifx\@currlist\@empty\else
    \global\let\@currlist\@empty
    \typeout{Warning: Floats cleared to prevent overflow}%
  \fi
  \orig@doclearpage
}
\makeatother

% Additional safety for structural commands
\let\originalbackmatter\backmatter
\renewcommand{\backmatter}{%
  \clearpage%
  \originalbackmatter%
}

\let\originalfrontmatter\frontmatter
\renewcommand{\frontmatter}{%
  \clearpage%
  \originalfrontmatter%
}

\let\originalmainmatter\mainmatter
\renewcommand{\mainmatter}{%
  \clearpage%
  \originalmainmatter%
}

% =============================================================================
% PAGE HEADERS AND FOOTERS
% =============================================================================
% Ensure chapters use fancy page style (not plain)
\patchcmd{\chapter}{\thispagestyle{plain}}{\thispagestyle{fancy}}{}{}

% Main page style with crimson headers
\pagestyle{fancy}
\fancyhf{}                                              % Clear all
\fancyhead[LE]{\small\color{crimson}\nouppercase{\rightmark}}  % Left even: section
\fancyhead[RO]{\color{crimson}\thepage}                 % Right odd: page number
\fancyhead[LO]{\small\color{crimson}\nouppercase{\leftmark}}   % Left odd: chapter
\fancyhead[RE]{\color{crimson}\thepage}                 % Right even: page number
\renewcommand{\headrulewidth}{0.4pt}                    % Thin header line
\renewcommand{\footrulewidth}{0pt}                      % No footer line

% Plain page style (for chapter openings)
\fancypagestyle{plain}{
  \fancyhf{}
  \fancyfoot[C]{\color{crimson}\thepage}                % Centered page number
  \renewcommand{\headrulewidth}{0pt}
  \renewcommand{\footrulewidth}{0pt}
}

% =============================================================================
% KOMA-SCRIPT FONT ADJUSTMENTS
% =============================================================================
% Apply crimson color to all heading levels
\addtokomafont{disposition}{\rmfamily\color{crimson}}
\addtokomafont{chapter}{\color{crimson}}
\addtokomafont{section}{\color{crimson}}
\addtokomafont{subsection}{\color{crimson}}

% =============================================================================
% ABSTRACT ENVIRONMENT
% =============================================================================
\newenvironment{abstract}{
  \chapter*{\abstractname}
  \addcontentsline{toc}{chapter}{\abstractname}
  \small
}{
  \clearpage
}

% =============================================================================
% HYPERLINK CONFIGURATION
% =============================================================================
% Crimson-colored links throughout, two-page PDF layout
\hypersetup{
  linkcolor=crimson,
  citecolor=crimson,
  urlcolor=crimson,
  pdfpagelayout=TwoPageRight,   % Two-page spread view
  pdfstartview=Fit               % Initial zoom fits page
}

% =============================================================================
% PART SUMMARY SYSTEM
% =============================================================================
% Allows adding descriptive text below part titles
\newcommand{\partsummary}{}     % Empty by default
\newif\ifhaspartsummary%
\haspartsummaryfalse%

\newcommand{\setpartsummary}[1]{%
  \renewcommand{\partsummary}{#1}%
  \haspartsummarytrue%
}

% Additional colors for part page backgrounds
\definecolor{BrownLL}{RGB}{233,222,220}
\definecolor{BlueDD}{RGB}{62,100,125}
\colorlet{BlueDD}{magenta}

% ===============================================================================
% PART STYLING SYSTEM
% ===============================================================================
%
% This system provides three distinct visual styles for book organization:
%
% 1. NUMBERED PARTS (\part{title}) - For main book sections
%    - Roman numerals (I, II, III, etc.) in top right corner
%    - Crimson title with horizontal lines above/below
%    - "Part I" label in sidebar
%    - Used for: foundations, principles, optimization, deployment, etc.
%
% 2. UNNUMBERED PARTS (\part*{title}) - For special sections like "Labs"
%    - Division-style geometric background (left side)
%    - No Roman numerals
%    - Used for: labs section
%
% 3. DIVISIONS (\division{title}) - For major book divisions
%    - Clean geometric background with centered title
%    - Used for: frontmatter, main_content, backmatter
%
% The Lua filter (inject-parts.lua) automatically routes parts by {key:xxx} commands
% to the appropriate LaTeX command based on the key name.
% ===============================================================================

% NUMBERED PARTS: Roman numeral styling for main book sections
\titleformat{\part}[display]
{\thispagestyle{empty}}{}{20pt}{
\begin{tikzpicture}[remember picture,overlay]
%%%
%%
\node[crimson,align=flush right,
inner sep=0,outer sep=0mm,draw=none,%
anchor=east,minimum height=31mm, text width=1.2\textwidth,
yshift=-30mm,font={%
\fontsize{98pt}{104}\selectfont\bfseries}]  (BG) at (current page text area.north east){\thepart};
%
\node[black,inner sep=0mm,draw=none,
anchor=mid,text width=1.2\textwidth,
 minimum height=35mm, align=right,
node distance=7mm,below=of BG,
font={\fontsize{30pt}{34}\selectfont}]
(BGG)  {\hyphenchar\font=-1 \color{black}\MakeUppercase {#1}};
\draw [crimson,line width=3pt] ([yshift=0mm]BGG.north west) -- ([yshift=0mm]BGG.north east);
\draw [crimson,line width=2pt] ([yshift=0mm]BGG.south west) -- ([yshift=0mm]BGG.south east);
%
\node[fill=crimson,text=white,rotate=90,%
anchor=south west,minimum height=15mm,
minimum width=40mm,font={%
\fontsize{20pt}{20}\selectfont\bfseries}](BP)  at
(current page text area.south east)
{{\sffamily Part}~\thepart};
%
\path[red](BP.north west)-|coordinate(PS)(BGG.south west);
%
% Part summary box commented out for cleaner design
% \ifhaspartsummary
% \node[inner sep=4pt,text width=0.7\textwidth,draw=none,fill=BrownLL!40,
% align=justify,font={\fontsize{9pt}{12}\selectfont},anchor=south west]
% at (PS) {\partsummary};
% \fi
\end{tikzpicture}
}[]

\renewcommand{\thepart}{\Roman{part}}

% UNNUMBERED PARTS: Division-style background for special sections
\titleformat{name=\part,numberless}[display]
{\thispagestyle{empty}}{}{20pt}{
\begin{tikzpicture}[remember picture,overlay]
%%%
\coordinate(S1)at([yshift=-200mm]current page.north west);
\draw[draw=none,fill=BlueDD!7](S1)--++(45:16)coordinate(S2)-
|(S2|-current page.north west)--(current page.north west)coordinate(S3)--(S1);
%
\coordinate(E1)at([yshift=-98mm]current page.north west);
\draw[draw=none,fill=BlueDD!15](E1)--(current page.north west)coordinate(E2)
--++(0:98mm)coordinate(E3)--(E1);
%
\coordinate(D1)at([yshift=15mm]current page.south west);
\draw[draw=none,fill=BlueDD!40,opacity=0.5](D1)--++(45:5.5)coordinate(D2)
-|(D2|-current page.north west)--(current page.north west)coordinate(D3)--(D1);
%%%%
\path[red](S2)-|(S2-|current page.east)coordinate(SS2);
%PART
\node[crimson,align=flush right,inner sep=0,outer sep=0mm,draw=none,anchor=south,
font={\fontsize{48pt}{48}\selectfont\bfseries}]  (BG) at ($(S2)!0.5!(SS2)$){\hphantom{Part}};
%%%
\path[green]([yshift=15mm]D2)-|coordinate(TPD)(BG.south east);
\node[inner sep=0mm,draw=none,anchor=south east,%text width=0.9\textwidth,
align=right,font={\fontsize{40pt}{40}\selectfont}]
(BGG) at (TPD)  {\color{crimson}\MakeUppercase {#1}};%\MakeUppercase {}
\end{tikzpicture}
}

% Define \numberedpart command for numbered parts
\newcommand{\numberedpart}[1]{%
\FloatBarrier%  % Flush all pending floats before part break
\clearpage
\thispagestyle{empty}
\stepcounter{part}%
\begin{tikzpicture}[remember picture,overlay]
%%%
%%
\node[crimson,align=flush right,
inner sep=0,outer sep=0mm,draw=none,%
anchor=east,minimum height=31mm, text width=1.2\textwidth,
yshift=-30mm,font={%
\fontsize{98pt}{104}\selectfont\bfseries}]  (BG) at (current page text area.north east){\thepart};
%
\node[black,inner sep=0mm,draw=none,
anchor=mid,text width=1.2\textwidth,
 minimum height=35mm, align=right,
node distance=7mm,below=of BG,
font={\fontsize{30pt}{34}\selectfont}]
(BGG)  {\hyphenchar\font=-1 \color{black}\MakeUppercase {#1}};
\draw [crimson,line width=3pt] ([yshift=0mm]BGG.north west) -- ([yshift=0mm]BGG.north east);
\draw [crimson,line width=2pt] ([yshift=0mm]BGG.south west) -- ([yshift=0mm]BGG.south east);
%
\node[fill=crimson,text=white,rotate=90,%
anchor=south west,minimum height=15mm,
minimum width=40mm,font={%
\fontsize{20pt}{20}\selectfont\bfseries}](BP)  at
(current page text area.south east)
{{\sffamily Part}~\thepart};
%
\path[red](BP.north west)-|coordinate(PS)(BGG.south west);
%
% Part summary box commented out for cleaner design
% \ifhaspartsummary
% \node[inner sep=4pt,text width=0.7\textwidth,draw=none,fill=BrownLL!40,
% align=justify,font={\fontsize{9pt}{12}\selectfont},anchor=south west]
% at (PS) {\partsummary};
% \fi
\end{tikzpicture}
\clearpage
}



% DIVISIONS: Clean geometric styling with subtle tech elements
% Used for frontmatter, main_content, and backmatter divisions
\newcommand{\division}[1]{%
\FloatBarrier%  % Flush all pending floats before division break
\clearpage
\thispagestyle{empty}
\begin{tikzpicture}[remember picture,overlay]

% Clean geometric background (original design)
\coordinate(S1)at([yshift=-200mm]current page.north west);
\draw[draw=none,fill=BlueDD!7](S1)--++(45:16)coordinate(S2)-
|(S2|-current page.north west)--(current page.north west)coordinate(S3)--(S1);

\coordinate(E1)at([yshift=-98mm]current page.north west);
\draw[draw=none,fill=BlueDD!15](E1)--(current page.north west)coordinate(E2)
--++(0:98mm)coordinate(E3)--(E1);

\coordinate(D1)at([yshift=15mm]current page.south west);
\draw[draw=none,fill=BlueDD!40,opacity=0.5](D1)--++(45:5.5)coordinate(D2)
-|(D2|-current page.north west)--(current page.north west)coordinate(D3)--(D1);

% Subtle tech elements - positioned in white areas for better visibility
% Upper right white area - more visible
\draw[crimson!40, line width=0.8pt] ([xshift=140mm,yshift=-60mm]current page.north west) -- ++(40mm,0);
\draw[crimson!40, line width=0.8pt] ([xshift=150mm,yshift=-70mm]current page.north west) -- ++(30mm,0);
\draw[crimson!35, line width=0.7pt] ([xshift=160mm,yshift=-60mm]current page.north west) -- ++(0,-15mm);
\draw[crimson!35, line width=0.7pt] ([xshift=170mm,yshift=-70mm]current page.north west) -- ++(0,10mm);

% Circuit nodes - upper right
\fill[crimson!50] ([xshift=160mm,yshift=-60mm]current page.north west) circle (1.5mm);
\fill[white] ([xshift=160mm,yshift=-60mm]current page.north west) circle (0.8mm);
\fill[crimson!50] ([xshift=170mm,yshift=-70mm]current page.north west) circle (1.3mm);
\fill[white] ([xshift=170mm,yshift=-70mm]current page.north west) circle (0.6mm);

% Lower right white area - enhanced visibility
\draw[crimson!45, line width=0.9pt] ([xshift=140mm,yshift=-190mm]current page.north west) -- ++(45mm,0);
\draw[crimson!45, line width=0.9pt] ([xshift=150mm,yshift=-200mm]current page.north west) -- ++(35mm,0);
\draw[crimson!40, line width=0.8pt] ([xshift=160mm,yshift=-190mm]current page.north west) -- ++(0,-20mm);
\draw[crimson!40, line width=0.8pt] ([xshift=170mm,yshift=-200mm]current page.north west) -- ++(0,15mm);

% Additional connecting lines in lower right
\draw[crimson!35, line width=0.7pt] ([xshift=130mm,yshift=-180mm]current page.north west) -- ++(25mm,0);
\draw[crimson!35, line width=0.7pt] ([xshift=145mm,yshift=-180mm]current page.north west) -- ++(0,-25mm);

% Circuit nodes - lower right (more prominent)
\fill[crimson!55] ([xshift=160mm,yshift=-190mm]current page.north west) circle (1.6mm);
\fill[white] ([xshift=160mm,yshift=-190mm]current page.north west) circle (0.9mm);
\fill[crimson!55] ([xshift=170mm,yshift=-200mm]current page.north west) circle (1.4mm);
\fill[white] ([xshift=170mm,yshift=-200mm]current page.north west) circle (0.7mm);
\fill[crimson!50] ([xshift=145mm,yshift=-180mm]current page.north west) circle (1.2mm);
\fill[white] ([xshift=145mm,yshift=-180mm]current page.north west) circle (0.6mm);

% Title positioned in center - clean and readable
\node[inner sep=0mm,draw=none,anchor=center,text width=0.8\textwidth,
align=center,font={\fontsize{40pt}{40}\selectfont}]
(BGG) at (current page.center)  {\color{crimson}\MakeUppercase {#1}};

\end{tikzpicture}
\clearpage
}

% LAB DIVISIONS: Circuit-style neural network design for lab sections
% Used specifically for lab platform sections (arduino, xiao, grove, etc.)
\newcommand{\labdivision}[1]{%
\FloatBarrier%  % Flush all pending floats before lab division break
\clearpage
\thispagestyle{empty}
\begin{tikzpicture}[remember picture,overlay]
% Circuit background with subtle gradient
\coordinate(S1)at([yshift=-200mm]current page.north west);
\draw[draw=none,fill=BlueDD!5](S1)--++(45:16)coordinate(S2)-
|(S2|-current page.north west)--(current page.north west)coordinate(S3)--(S1);

% TOP AREA: Circuit lines in upper white space
\draw[crimson!50, line width=1.5pt] ([xshift=30mm,yshift=-40mm]current page.north west) -- ++(60mm,0);
\draw[crimson!40, line width=1pt] ([xshift=120mm,yshift=-50mm]current page.north west) -- ++(50mm,0);
\draw[crimson!50, line width=1.5pt] ([xshift=40mm,yshift=-70mm]current page.north west) -- ++(40mm,0);

% Connecting lines in top area
\draw[crimson!30, line width=1pt] ([xshift=60mm,yshift=-40mm]current page.north west) -- ++(0,-20mm);
\draw[crimson!30, line width=1pt] ([xshift=145mm,yshift=-50mm]current page.north west) -- ++(0,10mm);

% Neural nodes in top area
\fill[crimson!70] ([xshift=60mm,yshift=-40mm]current page.north west) circle (2.5mm);
\fill[white] ([xshift=60mm,yshift=-40mm]current page.north west) circle (1.5mm);
\fill[crimson!60] ([xshift=145mm,yshift=-50mm]current page.north west) circle (2mm);
\fill[white] ([xshift=145mm,yshift=-50mm]current page.north west) circle (1mm);
\fill[crimson!80] ([xshift=80mm,yshift=-70mm]current page.north west) circle (2mm);
\fill[white] ([xshift=80mm,yshift=-70mm]current page.north west) circle (1mm);

% BOTTOM AREA: Circuit lines in lower white space
\draw[crimson!50, line width=1.5pt] ([xshift=20mm,yshift=-200mm]current page.north west) -- ++(70mm,0);
\draw[crimson!40, line width=1pt] ([xshift=110mm,yshift=-210mm]current page.north west) -- ++(60mm,0);
\draw[crimson!50, line width=1.5pt] ([xshift=35mm,yshift=-230mm]current page.north west) -- ++(45mm,0);

% Connecting lines in bottom area
\draw[crimson!30, line width=1pt] ([xshift=55mm,yshift=-200mm]current page.north west) -- ++(0,-20mm);
\draw[crimson!30, line width=1pt] ([xshift=140mm,yshift=-210mm]current page.north west) -- ++(0,15mm);

% Neural nodes in bottom area
\fill[crimson!70] ([xshift=55mm,yshift=-200mm]current page.north west) circle (2.5mm);
\fill[white] ([xshift=55mm,yshift=-200mm]current page.north west) circle (1.5mm);
\fill[crimson!60] ([xshift=140mm,yshift=-210mm]current page.north west) circle (2mm);
\fill[white] ([xshift=140mm,yshift=-210mm]current page.north west) circle (1mm);
\fill[crimson!80] ([xshift=80mm,yshift=-230mm]current page.north west) circle (2mm);
\fill[white] ([xshift=80mm,yshift=-230mm]current page.north west) circle (1mm);

% SIDE AREAS: Subtle circuit elements on left and right edges
\draw[crimson!30, line width=1pt] ([xshift=15mm,yshift=-120mm]current page.north west) -- ++(20mm,0);
\draw[crimson!30, line width=1pt] ([xshift=175mm,yshift=-130mm]current page.north west) -- ++(15mm,0);
\fill[crimson!50] ([xshift=25mm,yshift=-120mm]current page.north west) circle (1.5mm);
\fill[white] ([xshift=25mm,yshift=-120mm]current page.north west) circle (0.8mm);
\fill[crimson!50] ([xshift=185mm,yshift=-130mm]current page.north west) circle (1.5mm);
\fill[white] ([xshift=185mm,yshift=-130mm]current page.north west) circle (0.8mm);

% Title positioned in center - CLEAN AREA
\node[inner sep=0mm,draw=none,anchor=center,text width=0.8\textwidth,
align=center,font={\fontsize{44pt}{44}\selectfont\bfseries}]
(BGG) at (current page.center)  {\color{crimson}\MakeUppercase {#1}};

\end{tikzpicture}
\clearpage
}

% Define \lab command for lab styling (different visual treatment)
\newcommand{\lab}[1]{%
\begin{tikzpicture}[remember picture,overlay]
%%%
% Different background pattern for labs
\coordinate(S1)at([yshift=-200mm]current page.north west);
\draw[draw=none,fill=BlueDD!15](S1)--++(45:16)coordinate(S2)-
|(S2|-current page.north west)--(current page.north west)coordinate(S3)--(S1);
%
\coordinate(E1)at([yshift=-98mm]current page.north west);
\draw[draw=none,fill=BlueDD!25](E1)--(current page.north west)coordinate(E2)
--++(0:98mm)coordinate(E3)--(E1);
%
\coordinate(D1)at([yshift=15mm]current page.south west);
\draw[draw=none,fill=BlueDD!60,opacity=0.7](D1)--++(45:5.5)coordinate(D2)
-|(D2|-current page.north west)--(current page.north west)coordinate(D3)--(D1);
%%%%
\path[red](S2)-|(S2-|current page.east)coordinate(SS2);
%LAB - Different styling
\node[crimson,align=flush right,inner sep=0,outer sep=0mm,draw=none,anchor=south,
font={\fontsize{48pt}{48}\selectfont\bfseries}]  (BG) at ($(S2)!0.5!(SS2)$){\hphantom{Workshop}};
%%%
\path[green]([yshift=15mm]D2)-|coordinate(TPD)(BG.south east);
\node[inner sep=0mm,draw=none,anchor=south east,%text width=0.9\textwidth,
align=right,font={\fontsize{40pt}{40}\selectfont}]
(BGG) at (TPD)  {\color{crimson}\MakeUppercase {#1}};%\MakeUppercase {}
\end{tikzpicture}
\thispagestyle{empty}
\clearpage
}

% =============================================================================
% SECTION FORMATTING
% =============================================================================
% All section levels use crimson color and are ragged right

% Section (Large, bold, crimson)
\titleformat{\section}
  {\normalfont\Large\bfseries\color{crimson}\raggedright}
  {\thesection}
  {0.5em}
  {#1}
\titlespacing*{\section}{0pc}{14pt plus 4pt minus 4pt}{6pt plus 2pt minus 2pt}[0pc]

% Subsection (large, bold, crimson)
\titleformat{\subsection}
  {\normalfont\large\bfseries\color{crimson}\raggedright}
  {\thesubsection}
  {0.5em}
  {#1}
\titlespacing*{\subsection}{0pc}{12pt plus 4pt minus 4pt}{5pt plus 1pt minus 2pt}[0pc]

% Subsubsection (normal size, bold, crimson)
\titleformat{\subsubsection}
  {\normalfont\normalsize\bfseries\color{crimson}\raggedright}
  {\thesubsubsection}
  {0.5em}
  {#1}
\titlespacing*{\subsubsection}{0pc}{12pt plus 4pt minus 4pt}{5pt plus 1pt minus 2pt}[0pc]

% Paragraph (run-in, bold, crimson)
\titleformat{\paragraph}[runin]
  {\normalfont\normalsize\bfseries\color{crimson}}
  {\theparagraph}
  {0.5em}
  {#1}
\titlespacing*{\paragraph}{0pc}{6pt plus 2pt minus 2pt}{0.5em}[0pc]

% Subparagraph (run-in, italic, crimson)
\titleformat{\subparagraph}[runin]
  {\normalfont\normalsize\itshape\color{crimson}}
  {\thesubparagraph}
  {0.5em}
  {#1}
\titlespacing*{\subparagraph}{0pc}{6pt plus 2pt minus 2pt}{0.5em}[0pc]

% =============================================================================
% CHAPTER FORMATTING
% =============================================================================
% Numbered chapters: "Chapter X" prefix, huge crimson title
\titleformat{\chapter}[display]
  {\normalfont\huge\bfseries\color{crimson}}
  {\chaptername\ \thechapter}
  {20pt}
  {\Huge #1}
  []

% Unnumbered chapters: no prefix, huge crimson title
\titleformat{name=\chapter,numberless}
  {\normalfont\huge\bfseries\color{crimson}}
  {}
  {0pt}
  {\Huge #1}
  []

\renewcommand{\chaptername}{Chapter}
% =============================================================================
% TABLE OF CONTENTS FORMATTING
% =============================================================================
\setcounter{tocdepth}{2}                      % Show chapters, sections, subsections

% TOC spacing adjustments for number widths and indentation
\setlength{\cftchapnumwidth}{2em}             % Chapter number width
\setlength{\cftsecnumwidth}{2.75em}           % Section number width
\setlength{\cftsubsecnumwidth}{3.25em}        % Subsection number width
\setlength{\cftsubsubsecnumwidth}{4em}        % Subsubsection number width
\setlength{\cftsubsecindent}{4.25em}          % Subsection indent
\setlength{\cftsubsubsecindent}{7.5em}        % Subsubsection indent

% Chapter entries in TOC: bold crimson with "Chapter" prefix
\renewcommand{\cftchapfont}{\bfseries\color{crimson}}
\renewcommand{\cftchappresnum}{\color{crimson}Chapter~}

% Custom formatting for division entries (styled like parts)
\newcommand{\divisionchapter}[1]{%
  \addvspace{12pt}%
  \noindent\hfil\bfseries\color{crimson}#1\hfil\par%
  \addvspace{6pt}%
}

% Adjust TOC spacing for "Chapter" prefix
\newlength{\xtraspace}
\settowidth{\xtraspace}{\cftchappresnum\cftchapaftersnum}
\addtolength{\cftchapnumwidth}{\xtraspace}

% Unnumbered chapters with TOC entry
\newcommand{\likechapter}[1]{%
    \chapter*{#1}
    \addcontentsline{toc}{chapter}{\textcolor{crimson}{#1}}
}

% =============================================================================
% PAGE NUMBERING SYSTEM
% =============================================================================
% Implements traditional book numbering:
% - Roman numerals (i, ii, iii...) for frontmatter
% - Arabic numerals (1, 2, 3...) for mainmatter
% Automatically switches at first numbered chapter
\makeatletter
\newif\if@firstnumbered%
\@firstnumberedtrue%
\newif\if@firstunnumbered%
\@firstunnumberedtrue%

\newcounter{lastRomanPage}
\setcounter{lastRomanPage}{1}

% Start document with Roman numerals (frontmatter)
\AtBeginDocument{
  \pagenumbering{roman}
  \renewcommand{\thepage}{\roman{page}}
}

% Intercept chapter command
\let\old@chapter\chapter%
\renewcommand{\chapter}{%
  \@ifstar{\unnumbered@chapter}{\numbered@chapter}%
}

% Numbered chapters: switch to Arabic on first occurrence
\newcommand{\numbered@chapter}[1]{%
  \if@firstnumbered%
    \cleardoublepage%
    \setcounter{lastRomanPage}{\value{page}}%
    \pagenumbering{arabic}%
    \@firstnumberedfalse%
  \else
    \setcounter{page}{\value{page}}%
  \fi
  \setcounter{sidenote}{1}                    % Reset footnote counter per chapter
  \old@chapter{#1}%
}

% Unnumbered chapters: stay in Roman numerals
\newcommand{\unnumbered@chapter}[1]{%
  \if@firstunnumbered%
    \clearpage
    \setcounter{lastRomanPage}{\value{page}}%
    \pagenumbering{roman}%
    \@firstunnumberedfalse%
  \fi
  \setcounter{sidenote}{1}
  \old@chapter*{#1}%
}
\makeatother

% =============================================================================
% TABLE SIZING AND SPACING
% =============================================================================
% Make tables slightly smaller to fit more content
\AtBeginEnvironment{longtable}{\scriptsize}

% Increase vertical spacing in table cells (default is 1.0)
\renewcommand{\arraystretch}{1.5}

% Prefer placing figures and tables at the top of pages
\makeatletter
\renewcommand{\fps@figure}{t}  % Default placement: top of page
\renewcommand{\fps@table}{t}   % Default placement: top of page
\makeatother

% =============================================================================
% LONGTABLE PAGE BREAKING FIXES (Windows compatibility)
% =============================================================================
% Prevent "Infinite glue shrinkage" errors on Windows LaTeX builds
% by giving longtable more flexibility in page breaking

% Allow more flexible page breaking (vs strict \flushbottom)
\raggedbottom

% Process more rows before attempting page break (default is 20)
\setcounter{LTchunksize}{50}

% Add extra stretch for longtable environments specifically
\AtBeginEnvironment{longtable}{%
  \setlength{\emergencystretch}{3em}%
  \setlength{\parskip}{0pt plus 1pt}%
}

% =============================================================================
% TABLE STYLING - Clean tables with crimson borders
% =============================================================================
% Professional table appearance with:
% - Clean white background (no colored rows)
% - Crimson-colored borders
% - Good spacing for readability
%
% Note: Headers are automatically bolded by Quarto when using **text** in source
\usepackage{booktabs}      % Professional table rules (\toprule, \midrule, \bottomrule)
\usepackage{colortbl}      % For colored borders (\arrayrulecolor)

% Global table styling - crimson borders
\setlength{\arrayrulewidth}{0.5pt}          % Thinner borders than default
%\arrayrulecolor{crimson}                    % Crimson borders matching brand

\setcounter{chapter}{0}

% =============================================================================
% DROP CAPS (Lettrine)
% =============================================================================
% Decorative large first letter at chapter openings, following the tradition
% of Hennessy & Patterson and other MIT Press textbooks.
% Usage in QMD: \lettrine{T}{he first sentence...}
\usepackage{lettrine}
\renewcommand{\LettrineFontHook}{\color{crimson}\bfseries}
\setcounter{DefaultLines}{3}          % Drop cap spans 3 lines
\renewcommand{\DefaultLoversize}{0.1} % Slight oversize for visual weight
\renewcommand{\DefaultLraise}{0}      % No vertical shift
\setlength{\DefaultNindent}{0.5em}    % Indent of continuation text
\setlength{\DefaultSlope}{0pt}        % No slope on continuation

% =============================================================================
% RUNNING HEADERS — Truncation Safety
% =============================================================================
% Long chapter/section titles can overflow the header. These marks truncate
% gracefully so headers stay within the text block.
\renewcommand{\chaptermark}[1]{%
  \markboth{\thechapter.\ #1}{}}
\renewcommand{\sectionmark}[1]{%
  \markright{\thesection\ #1}}

% =============================================================================
% EPIGRAPH ENVIRONMENT
% =============================================================================
% For chapter-opening quotations. Renders as right-aligned italic block
% with attribution in small caps below.
% Usage: \epigraph{Quote text}{Author Name, \textit{Source}}
\newcommand{\bookepigraph}[2]{%
  \vspace{1em}%
  \begin{flushright}%
    \begin{minipage}{0.75\textwidth}%
      \raggedleft\itshape\small #1\\[0.5em]%
      \normalfont\small --- #2%
    \end{minipage}%
  \end{flushright}%
  \vspace{1.5em}%
}

% =============================================================================
% THUMB INDEX TABS
% =============================================================================
% Colored tabs on the outer page edge for quick chapter navigation.
% Each Part gets a different vertical position; chapters within a Part
% share the same tab position. Visible when flipping through the book.
\newcounter{thumbindex}
\setcounter{thumbindex}{0}
\newlength{\thumbtabheight}
\setlength{\thumbtabheight}{16mm}     % Height of each tab
\newlength{\thumbtabwidth}
\setlength{\thumbtabwidth}{8mm}       % Width protruding from edge
\newlength{\thumbtabgap}
\setlength{\thumbtabgap}{1mm}         % Gap between tabs

% Advance to next thumb tab position (call at each \part)
\newcommand{\nextthumb}{%
  \stepcounter{thumbindex}%
}

% Draw the thumb tab on every page (placed in header via fancyhdr)
\newcommand{\drawthumb}{%
  \ifnum\value{thumbindex}>0%
    \begin{tikzpicture}[remember picture,overlay]
      \pgfmathsetmacro{\thumboffset}{%
        20 + (\value{thumbindex}-1) * (16 + 1)}  % mm from top
      \ifodd\value{page}%
        % Odd pages: tab on right edge
        \fill[crimson!80]
          ([yshift=-\thumboffset mm]current page.north east)
          rectangle +(-\thumbtabwidth, -\thumbtabheight);
        \node[white,font=\tiny\bfseries,rotate=90]
          at ([yshift=-\thumboffset mm - 0.5\thumbtabheight,
               xshift=-0.5\thumbtabwidth]current page.north east)
          {\Roman{thumbindex}};
      \else
        % Even pages: tab on left edge
        \fill[crimson!80]
          ([yshift=-\thumboffset mm]current page.north west)
          rectangle +(\thumbtabwidth, -\thumbtabheight);
        \node[white,font=\tiny\bfseries,rotate=-90]
          at ([yshift=-\thumboffset mm - 0.5\thumbtabheight,
               xshift=0.5\thumbtabwidth]current page.north west)
          {\Roman{thumbindex}};
      \fi
    \end{tikzpicture}%
  \fi
}

% Hook into fancyhdr to draw thumb on every content page
\AddToHook{shipout/foreground}{%
  \drawthumb%
}

% =============================================================================
% CROP / BLEED MARKS
% =============================================================================
% For final print submission, uncomment the line below to add crop marks.
% MIT Press production will advise on exact requirements.
% \usepackage[cam,center,width=7.5in,height=10.5in]{crop}

% =============================================================================
% PDF/A ARCHIVAL COMPLIANCE
% =============================================================================
% MIT Press increasingly requires PDF/A for long-term preservation.
% This embeds all fonts and removes transparency.
% Note: pdfx must be loaded early; if it conflicts with hyperref,
% MIT Press production can handle the conversion post-build.
% Uncomment when ready for final submission:
% \usepackage[a-3u]{pdfx}

% =============================================================================
% ENHANCED WIDOW / ORPHAN CONTROL
% =============================================================================
% Prevent single lines at top/bottom of pages and breaks before equations
\clubpenalty=10000          % No orphans (single first line at bottom)
\widowpenalty=10000         % No widows (single last line at top)
\displaywidowpenalty=10000  % No widow before display math
\predisplaypenalty=10000    % No page break just before display math
\postdisplaypenalty=0       % Allow break after display math (natural)
\usepackage{needspace}
\let\Needspace\needspace
\makeatletter
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{plain}
\@ifundefined{c@chapter}{\newfloat{vid}{h}{lovid}}{\newfloat{vid}{h}{lovid}[chapter]}
\floatname{vid}{Video}
\newcommand*\listofvids{\listof{vid}{List of Videos}}
\makeatother
\makeatletter
\@ifpackageloaded{tcolorbox}{}{\usepackage[skins,breakable]{tcolorbox}}
\@ifpackageloaded{fontawesome5}{}{\usepackage{fontawesome5}}
\definecolor{quarto-callout-color}{HTML}{909090}
\definecolor{quarto-callout-note-color}{HTML}{0758E5}
\definecolor{quarto-callout-important-color}{HTML}{CC1914}
\definecolor{quarto-callout-warning-color}{HTML}{EB9113}
\definecolor{quarto-callout-tip-color}{HTML}{00A047}
\definecolor{quarto-callout-caution-color}{HTML}{FC5300}
\definecolor{quarto-callout-color-frame}{HTML}{acacac}
\definecolor{quarto-callout-note-color-frame}{HTML}{4582ec}
\definecolor{quarto-callout-important-color-frame}{HTML}{d9534f}
\definecolor{quarto-callout-warning-color-frame}{HTML}{f0ad4e}
\definecolor{quarto-callout-tip-color-frame}{HTML}{02b875}
\definecolor{quarto-callout-caution-color-frame}{HTML}{fd7e14}
\makeatother
\makeatletter
\@ifpackageloaded{bookmark}{}{\usepackage{bookmark}}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\makeatletter
\@ifpackageloaded{sidenotes}{}{\usepackage{sidenotes}}
\@ifpackageloaded{marginnote}{}{\usepackage{marginnote}}
\makeatother
\newcommand{\fbxIconPath}{assets/images/icons/callouts}
\newcommand{\fbxIconFormat}{pdf}
\makeatletter
\@ifpackageloaded{tcolorbox}{}{\usepackage[many]{tcolorbox}}
\makeatother
%%%% ---foldboxy preamble ----- %%%%%

% Load xstring for string manipulation
\RequirePackage{xstring}

% Icon path and format configuration - can be overridden in filter-metadata
\providecommand{\fbxIconPath}{assets/images/icons/callouts}
\providecommand{\fbxIconFormat}{pdf}

% Helper command to include icon with hyphen-to-underscore conversion
% This ensures consistency: callout-quiz-question -> callout_quiz_question
\newcommand{\fbxIncludeIcon}[2]{%
  \StrSubstitute{#1}{-}{_}[\fbxIconName]%
  \includegraphics[width=#2]{\fbxIconPath/icon_\fbxIconName.\fbxIconFormat}%
}

% Legacy fallback colors (keep for compatibility)
\definecolor{fbx-default-color1}{HTML}{c7c7d0}
\definecolor{fbx-default-color2}{HTML}{a3a3aa}
\definecolor{fbox-color1}{HTML}{c7c7d0}
\definecolor{fbox-color2}{HTML}{a3a3aa}

% arguments: #1 typelabelnummer: #2 titel: #3
\newenvironment{fbx}[3]{%
\begin{tcolorbox}[
  enhanced,
  breakable,
  %fontupper=\fontsize{8pt}{10pt}\selectfont,  % 95% of body text (10pt -> 9.5pt)
  before skip=8pt,  % space above box (increased)
  after skip=8pt,   % space below box (increased)
  attach boxed title to top*={xshift=0pt},
  boxed title style={
  %fuzzy shadow={1pt}{-1pt}{0mm}{0.1mm}{gray},
  arc=1.5pt,
  rounded corners=north,
  sharp corners=south,
  top=6pt,          % Adjusted for ~40px equivalent height
  bottom=5pt,       % Adjusted for ~40px equivalent height
  overlay={
      \node [left,outer sep=0em, black,draw=none,anchor=west,
        rectangle,fill=none,inner sep=0pt]
        at ([xshift=4mm]frame.west) {\fbxIncludeIcon{#1}{4.2mm}};
    },
  },
  colframe=#1-color2,             % Border color (auto-generated from YAML)
  colbacktitle=#1-color1,         % Background color (auto-generated from YAML)
  colback=white,
  coltitle=black,
  titlerule=0mm,
  toprule=0.5pt,
  bottomrule=0.5pt,
  leftrule=2.2pt,
  rightrule=0.5pt,
  outer arc=1.5pt,
  arc=1.5pt,
  left=0.5em,       % increased left padding
  bottomtitle=1.5mm, % increased title bottom margin
  toptitle=1.5mm,    % increased title top margin
  title=\hspace{2.5em}\protect#2\hspace{0.5em}\protect#3, % Protect parameters
  extras middle and last={top=4pt} % increased continuation spacing
]}
{\end{tcolorbox}}


% boxed environment with right border
\newenvironment{fbxSimple}[3]{\begin{tcolorbox}[
  enhanced,
  breakable,
  %fontupper=\fontsize{8pt}{10pt}\selectfont,  % 95% of body text (10pt -> 9.5pt)
  before skip=8pt,  % space above box (increased)
  after skip=8pt,   % space below box (increased)
  attach boxed title to top*={xshift=0pt},
  boxed title style={
  %fuzzy shadow={1pt}{-1pt}{0mm}{0.1mm}{gray},
  arc=1.5pt,
  rounded corners=north,
  sharp corners=south,
  top=6pt,          % Adjusted for ~40px equivalent height
  bottom=5pt,       % Adjusted for ~40px equivalent height
  overlay={
      \node [left,outer sep=0em, black,draw=none,anchor=west,
        rectangle,fill=none,inner sep=0pt]
        at ([xshift=3mm]frame.west) {\fbxIncludeIcon{#1}{4.2mm}};
    },
  },
  colframe=#1-color2,             % Border color (auto-generated from YAML)
  colbacktitle=#1-color1,         % Background color (auto-generated from YAML)
  colback=white,
  coltitle=black,
  titlerule=0mm,
  toprule=0.5pt,
  bottomrule=0.5pt,
  leftrule=2.2pt,
  rightrule=0.5pt,
  outer arc=1.5pt,
  arc=1.5pt,
  left=0.5em,       % increased left padding
  bottomtitle=1.5mm, % increased title bottom margin
  toptitle=1.5mm,    % increased title top margin
  title=\hspace{2.5em}\protect#2\hspace{0.5em}\protect#3, % Protect parameters
  boxsep=1pt,
  extras first={bottom=0pt},
  extras last={top=0pt,bottom=-4pt},
  overlay first={
    \draw[line width=1pt,white] ([xshift=2.2pt]frame.south west)-- ([xshift=-0.5pt]frame.south east);
  },
  overlay last={
    \draw[line width=1pt,white] ([xshift=2.2pt]frame.north west)-- ([xshift=-0.5pt]frame.north east);
   }
]}
{\end{tcolorbox}}

%%%% --- end foldboxy preamble ----- %%%%%
%%==== colors from yaml ===%
\definecolor{callout-quiz-question-color1}{HTML}{F0F0F8}
\definecolor{callout-quiz-question-color2}{HTML}{5B4B8A}
\definecolor{callout-takeaways-color1}{HTML}{FDF2F7}
\definecolor{callout-takeaways-color2}{HTML}{BE185D}
\definecolor{callout-quiz-answer-color1}{HTML}{E8F2EA}
\definecolor{callout-quiz-answer-color2}{HTML}{4A7C59}
\definecolor{callout-chapter-connection-color1}{HTML}{FDF2F7}
\definecolor{callout-chapter-connection-color2}{HTML}{A51C30}
\definecolor{callout-principle-color1}{HTML}{F3F2FA}
\definecolor{callout-principle-color2}{HTML}{3D3B8E}
\definecolor{callout-perspective-color1}{HTML}{F7F8FA}
\definecolor{callout-perspective-color2}{HTML}{4A5568}
\definecolor{callout-definition-color1}{HTML}{F0F4F8}
\definecolor{callout-definition-color2}{HTML}{1B4F72}
\definecolor{callout-theorem-color1}{HTML}{F5F0FA}
\definecolor{callout-theorem-color2}{HTML}{6B46C1}
\definecolor{callout-resource-slides-color1}{HTML}{E0F2F1}
\definecolor{callout-resource-slides-color2}{HTML}{20B2AA}
\definecolor{callout-colab-color1}{HTML}{FFF5E6}
\definecolor{callout-colab-color2}{HTML}{FF6B35}
\definecolor{callout-example-color1}{HTML}{F0F8F6}
\definecolor{callout-example-color2}{HTML}{148F77}
\definecolor{callout-resource-videos-color1}{HTML}{E0F2F1}
\definecolor{callout-resource-videos-color2}{HTML}{20B2AA}
\definecolor{callout-resource-exercises-color1}{HTML}{E0F2F1}
\definecolor{callout-resource-exercises-color2}{HTML}{20B2AA}
\definecolor{callout-notebook-color1}{HTML}{F2F7FF}
\definecolor{callout-notebook-color2}{HTML}{2C5282}
\definecolor{callout-checkpoint-color1}{HTML}{FFF8E7}
\definecolor{callout-checkpoint-color2}{HTML}{D97706}
\definecolor{callout-code-color1}{HTML}{F2F4F8}
\definecolor{callout-code-color2}{HTML}{D1D7E0}
\definecolor{callout-lighthouse-color1}{HTML}{FDF8E6}
\definecolor{callout-lighthouse-color2}{HTML}{B8860B}
%=============%

\usepackage{hyphenat}
\usepackage{ifthen}
\usepackage{calc}
\usepackage{calculator}



\usepackage{graphicx}
\usepackage{geometry}
\usepackage{afterpage}
\usepackage{tikz}
\usetikzlibrary{calc}
\usetikzlibrary{fadings}
\usepackage[pagecolor=none]{pagecolor}


% Set the titlepage font families







% Set the coverpage font families

\usepackage{bookmark}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  pdftitle={Advanced Machine Learning Systems},
  pdfauthor={Vijay Janapa Reddi},
  colorlinks=true,
  linkcolor={Maroon},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}


\title{Advanced Machine Learning Systems}
\author{Vijay Janapa Reddi}
\date{February 1, 2026}
\begin{document}
%%%%% begin titlepage extension code

  \begin{frontmatter}

\begin{titlepage}
% This is a combination of Pandoc templating and LaTeX
% Pandoc templating https://pandoc.org/MANUAL.html#templates
% See the README for help

\thispagestyle{empty}

\newgeometry{top=-100in}

% Page color

\newcommand{\coverauthorstyle}[1]{{\fontsize{20}{24.0}\selectfont
{#1}}}

\begin{tikzpicture}[remember picture, overlay, inner sep=0pt, outer sep=0pt]

\tikzfading[name=fadeout, inner color=transparent!0,outer color=transparent!100]
\tikzfading[name=fadein, inner color=transparent!100,outer color=transparent!0]
\node[anchor=south west, rotate=0, opacity=1] at ($(current page.south west)+(0.225\paperwidth, 9)$) {
\includegraphics[width=\paperwidth, keepaspectratio]{assets/images/covers/cover-image-transparent-vol2.png}};

% Title
\newcommand{\titlelocationleft}{0.075\paperwidth}
\newcommand{\titlelocationbottom}{0.4\paperwidth}
\newcommand{\titlealign}{left}

\begin{scope}{%
\fontsize{52}{62.4}\selectfont
\node[anchor=north
west, align=left, rotate=0] (Title1) at ($(current page.south west)+(\titlelocationleft,\titlelocationbottom)$)  [text width = 0.9\paperwidth]  {{\nohyphens{Machine
Learning Systems}}};
}
\end{scope}

% Author
\newcommand{\authorlocationleft}{.925\paperwidth}
\newcommand{\authorlocationbottom}{0.225\paperwidth}
\newcommand{\authoralign}{right}

\begin{scope}
{%
\fontsize{20}{24.0}\selectfont
\node[anchor=north
east, align=right, rotate=0] (Author1) at ($(current page.south west)+(\authorlocationleft,\authorlocationbottom)$)  [text width = 6in]  {\coverauthorstyle{Vijay
Janapa Reddi\\}};
}
\end{scope}

% Footer
\newcommand{\footerlocationleft}{0.075\paperwidth}
\newcommand{\footerlocationbottom}{0.475\paperwidth}
\newcommand{\footerlocationalign}{left}

\begin{scope}
{%
\fontsize{25}{30.0}\selectfont
 \node[anchor=north west, align=left, rotate=0] (Footer1) at %
($(current page.south west)+(\footerlocationleft,\footerlocationbottom)$)  [text width = 0.9\paperwidth]  {{\nohyphens{Advanced}}};
}
\end{scope}

\end{tikzpicture}
\clearpage
\restoregeometry
%%% TITLE PAGE START

% Set up alignment commands
%Page
\newcommand{\titlepagepagealign}{
\ifthenelse{\equal{left}{right}}{\raggedleft}{}
\ifthenelse{\equal{left}{center}}{\centering}{}
\ifthenelse{\equal{left}{left}}{\raggedright}{}
}


\newcommand{\titleandsubtitle}{
% Title and subtitle
{{\huge{\bfseries{\nohyphens{Advanced Machine Learning Systems}}}}\par
}%
}
\newcommand{\titlepagetitleblock}{
\titleandsubtitle
}

\newcommand{\authorstyle}[1]{{\large{#1}}}

\newcommand{\affiliationstyle}[1]{{\large{#1}}}

\newcommand{\titlepageauthorblock}{
{\authorstyle{\nohyphens{Vijay Janapa
Reddi}{\textsuperscript{1}}\textsuperscript{,}{\textsuperscript{,*}}}}}

\newcommand{\titlepageaffiliationblock}{
\hangindent=1em
\hangafter=1
{\affiliationstyle{
{1}.~Harvard University


\vspace{1\baselineskip}
* \textit{Correspondence:}~Vijay Janapa Reddi~vj@eecs.harvard.edu
}}
}
\newcommand{\headerstyled}{%
{}
}
\newcommand{\footerstyled}{%
{\large{}}
}
\newcommand{\datestyled}{%
{February 1, 2026}
}


\newcommand{\titlepageheaderblock}{\headerstyled}

\newcommand{\titlepagefooterblock}{
\footerstyled
}

\newcommand{\titlepagedateblock}{
\datestyled
}

%set up blocks so user can specify order
\newcommand{\titleblock}{{

{\titlepagetitleblock}
}

\vspace{4\baselineskip}
}

\newcommand{\authorblock}{{\titlepageauthorblock}

\vspace{2\baselineskip}
}

\newcommand{\affiliationblock}{{\titlepageaffiliationblock}

\vspace{0pt}
}

\newcommand{\logoblock}{}

\newcommand{\footerblock}{}

\newcommand{\dateblock}{{\titlepagedateblock}

\vspace{0pt}
}

\newcommand{\headerblock}{}

\thispagestyle{empty} % no page numbers on titlepages


\newcommand{\vrulecode}{\textcolor{black}{\rule{\vrulewidth}{\textheight}}}
\newlength{\vrulewidth}
\setlength{\vrulewidth}{2pt}
\newlength{\B}
\setlength{\B}{\ifdim\vrulewidth > 0pt 0.05\textwidth\else 0pt\fi}
\newlength{\minipagewidth}
\ifthenelse{\equal{left}{left} \OR \equal{left}{right} }
{% True case
\setlength{\minipagewidth}{\textwidth - \vrulewidth - \B - 0.1\textwidth}
}{
\setlength{\minipagewidth}{\textwidth - 2\vrulewidth - 2\B - 0.1\textwidth}
}
\ifthenelse{\equal{left}{left} \OR \equal{left}{leftright}}
{% True case
\raggedleft % needed for the minipage to work
\vrulecode
\hspace{\B}
}{%
\raggedright % else it is right only and width is not 0
}
% [position of box][box height][inner position]{width}
% [s] means stretch out vertically; assuming there is a vfill
\begin{minipage}[b][\textheight][s]{\minipagewidth}
\titlepagepagealign
\titleblock

Prof.~Vijay Janapa Reddi

School of Engineering and Applied Sciences

Harvard University

\vspace{80mm}

With heartfelt gratitude to the community for their invaluable
contributions and steadfast support.

\vfill

February 1, 2026

\vfill
\par

\end{minipage}\ifthenelse{\equal{left}{right} \OR \equal{left}{leftright} }{
\hspace{\B}
\vrulecode}{}
\clearpage
%%% TITLE PAGE END
\end{titlepage}
\setcounter{page}{1}
\end{frontmatter}

%%%%% end titlepage extension code

% =============================================================================
% HALF-TITLE PAGE (Volume II)
% =============================================================================
% Standard academic book sequence: half-title -> blank -> title page -> copyright
% The half-title shows only the book title -- no author, no publisher, no date.
\thispagestyle{empty}
\begin{center}
\vspace*{0.3\textheight}
{\fontsize{24pt}{28pt}\selectfont\bfseries\color{crimson} Advanced\\[0.4em] Machine Learning Systems}\\[2em]
{\large\itshape Volume~II}
\vfill
\end{center}
\clearpage
\thispagestyle{empty}\null\clearpage  % Blank verso (back of half-title)

\renewcommand*\contentsname{Table of contents}
{
\hypersetup{linkcolor=}
\setcounter{tocdepth}{2}
\tableofcontents
}
\listoffigures
\listoftables

\mainmatter
\bookmarksetup{startatroot}

\chapter*{Welcome to Volume II}\label{welcome-to-volume-ii}
\addcontentsline{toc}{chapter}{Welcome to Volume II}

\markboth{Welcome to Volume II}{Welcome to Volume II}

\section*{What You Will Learn}\label{what-you-will-learn}
\addcontentsline{toc}{section}{What You Will Learn}

\markright{What You Will Learn}

Volume II extends the foundations into production-scale systems through
five parts:

\begin{itemize}
\tightlist
\item
  \textbf{Part I: Foundations of Scale} --- Master the algorithms of
  scale. Learn how to coordinate computation across thousands of devices
  using Data, Tensor, and Pipeline parallelism, and understand the
  collective communication primitives and fault tolerance mechanisms
  that synchronize them.
\item
  \textbf{Part II: Building the Machine Learning Fleet} --- Build the
  physical computer. Architect the datacenter infrastructure,
  accelerators, and high-performance storage systems required to support
  distributed workloads at scale.
\item
  \textbf{Part III: Deployment at Scale} --- Serve the world. Navigate
  the shift from training to inference, push intelligence to the edge,
  and manage the operational lifecycle of production fleets.
\item
  \textbf{Part IV: Production Concerns} --- Harden the system. Address
  the non-functional requirements of privacy, security, robustness, and
  environmental sustainability in large-scale operations.
\item
  \textbf{Part V: Responsible AI at Scale} --- Shape the future. Explore
  responsible AI governance, AI for social good, and the emerging
  frontiers of AGI systems.
\end{itemize}

\section*{Prerequisites}\label{prerequisites}
\addcontentsline{toc}{section}{Prerequisites}

\markright{Prerequisites}

This volume assumes:

\begin{itemize}
\tightlist
\item
  \textbf{Foundational or equivalent} background in single-machine ML
  systems
\item
  \textbf{Programming proficiency} in Python with familiarity in NumPy
\item
  \textbf{Mathematics foundations} in linear algebra, calculus, and
  probability
\item
  Familiarity with distributed systems concepts (networking,
  parallelism) is helpful for advanced topics
\end{itemize}

\section*{Support Our Mission}\label{support-our-mission}
\addcontentsline{toc}{section}{Support Our Mission}

\markright{Support Our Mission}

\section*{Listen to the AI Podcast}\label{listen-to-the-ai-podcast}
\addcontentsline{toc}{section}{Listen to the AI Podcast}

\markright{Listen to the AI Podcast}

\section*{Want to Help Out?}\label{want-to-help-out}
\addcontentsline{toc}{section}{Want to Help Out?}

\markright{Want to Help Out?}

This is a collaborative project, and your input matters. If you'd like
to contribute, check out our
\href{https://github.com/harvard-edge/cs249r_book/blob/dev/docs/contribute.md}{contribution
guidelines}. Feedback, corrections, and new ideas are welcome. Simply
file a GitHub
\href{https://github.com/harvard-edge/cs249r_book/issues}{issue}.

\bookmarksetup{startatroot}

\chapter{Edge Intelligence}\label{sec-edge-intelligence}

\marginnote{\begin{footnotesize}

\emph{Gemini Pro 3 Prompt: Drawing of a smartphone with its internal
components exposed, revealing diverse miniature engineers of different
genders and skin tones actively working on the ML model. The engineers,
including men, women, and non-binary individuals, are tuning parameters,
repairing connections, and enhancing the network on the fly. Data flows
into the ML model, being processed in real-time, and generating output
inferences. Rendered in the style of Nanobanana. High resolution,
rectangular image with golden ratio dimensions.}

\end{footnotesize}}

\noindent
\pandocbounded{\includegraphics[keepaspectratio]{contents/vol2/edge_intelligence/images/png/cover_ondevice_learning.png}}

\section*{Purpose}\label{purpose}
\addcontentsline{toc}{section}{Purpose}

\markright{Purpose}

\emph{Why must intelligence adapt where it operates rather than remain
frozen from distant training?}

A model trained in a datacenter encounters a world it has never seen
when deployed to a user's device. The user's vocabulary differs from
training data. Local conditions---lighting, acoustics, usage
patterns---diverge from the distributions that shaped the model's
parameters. Over time, the gap widens as user behavior evolves while the
model remains static. On-device learning exists because centralized
training cannot anticipate every context, personalize for every user, or
adapt to continuous change. It also exists because data cannot always
travel: privacy constraints, bandwidth limitations, and latency
requirements may prohibit sending raw observations to distant servers.
The alternative to on-device learning is accepting that deployed models
grow stale, that personalization requires privacy sacrifice, and that
disconnected operation means degraded intelligence. On-device learning
rejects these compromises by bringing the learning process to where the
data lives and where the adaptation matters.

\begin{tcolorbox}[enhanced jigsaw, rightrule=.15mm, bottomtitle=1mm, opacitybacktitle=0.6, breakable, leftrule=.75mm, bottomrule=.15mm, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Learning Objectives}, colframe=quarto-callout-tip-color-frame, coltitle=black, opacityback=0, left=2mm, titlerule=0mm, toptitle=1mm, arc=.35mm, toprule=.15mm, colbacktitle=quarto-callout-tip-color!10!white, colback=white]

\begin{itemize}
\tightlist
\item
  Contrast centralized cloud training, on-device learning, and federated
  learning by analyzing their data flow, privacy properties, and
  operational trade-offs.
\item
  Quantify training overhead on edge devices by calculating memory
  amplification (3-5\(\times\)), compute costs (2-3\(\times\)), and
  energy impacts relative to inference-only deployment.
\item
  Select model adaptation strategies (weight freezing, LoRA, sparse
  updates) by comparing memory footprints, expressivity, and convergence
  for specific device capabilities.
\item
  Apply data efficiency techniques (few-shot learning, experience
  replay, contrastive learning) to enable effective learning from
  limited local datasets while preventing catastrophic forgetting.
\item
  Design federated learning systems that aggregate privacy-preserving
  updates across heterogeneous devices while managing non-IID data,
  communication efficiency, and stragglers.
\item
  Integrate on-device learning into production MLOps by implementing
  device-aware deployment, distributed validation, and
  privacy-preserving monitoring across heterogeneous populations.
\end{itemize}

\end{tcolorbox}

\section{Distributed Learning Paradigm
Shift}\label{sec-edge-intelligence-distributed-learning-paradigm-shift-883d}

In the \textbf{Systems Sandwich} (\textbf{?@sec-vol2-introduction}),
Edge Intelligence represents the \textbf{Physical Layer} pushed to its
thermodynamic limits. While the datacenter (\textbf{?@sec-compute})
provides abundant power and cooling, the Edge provides almost none. The
physical constraints of battery density (mAh) and thermal dissipation
(Watts) here set the hard limits for what the \textbf{Operational Layer}
(federated learning algorithms) can achieve. If the model exceeds the
thermal envelope, the physics will throttle the logic.

Part II established the infrastructure for distributed ML at scale. We
examined datacenter architectures optimized for GPU clusters
(\textbf{?@sec-compute}), storage systems engineered for petabyte
datasets (\textbf{?@sec-storage}), distributed training across thousands
of accelerators (\textbf{?@sec-distributed-training-systems}),
communication patterns that coordinate gradient synchronization
(\textbf{?@sec-communication-collective-operations}), and fault
tolerance mechanisms that handle inevitable failures
(\textbf{?@sec-fault-tolerance-reliability}). The preceding chapter
(\textbf{?@sec-inference-scale}) then examined how to serve predictions
from this infrastructure, addressing load balancing, model sharding, and
autoscaling for centralized inference systems. These chapters share a
common assumption about controlled environments where computational
resources are abundant, network connectivity is reliable, and system
behavior is predictable.

A smartphone learning to predict user text input, a smart home device
adapting to household routines, or an autonomous vehicle updating its
perception models based on local driving conditions all demonstrate
scenarios where traditional centralized training approaches prove
inadequate. The smartphone encounters linguistic patterns unique to
individual users that were not present in global training data. The
smart home device must adapt to seasonal changes and family dynamics
that vary dramatically across households. The autonomous vehicle faces
local road conditions, weather patterns, and traffic behaviors that
differ from its original training environment.

These scenarios require on-device learning, where models must train and
adapt directly on the devices where they operate.\sidenote{\textbf{A11
Bionic Breakthrough}: Apple's A11 Bionic (2017) was the first mobile
chip with sufficient computational power for on-device training,
delivering 0.6 TOPS compared to the previous A10's 0.2 TOPS. This
\(3\times\) improvement, combined with 4.3 billion transistors and a
dual-core Neural Engine, allowed gradient computation for the first time
on mobile devices. Google's Pixel Visual Core achieved similar
capabilities with 8 custom Image Processing Units optimized for machine
learning workloads. } This shifts machine learning from centralized
training to distributed learning across millions of heterogeneous
devices, each operating under unique constraints and local conditions.
Within \emph{the Systems Sandwich}, this shift pushes the physical layer
to its thermodynamic limits.

The following callout situates edge intelligence within the broader
systems architecture framework introduced in
\textbf{?@sec-vol2-introduction}.

\begin{tcolorbox}[enhanced jigsaw, rightrule=.15mm, bottomtitle=1mm, opacitybacktitle=0.6, breakable, leftrule=.75mm, bottomrule=.15mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Connection: The Systems Sandwich}, colframe=quarto-callout-note-color-frame, coltitle=black, opacityback=0, left=2mm, titlerule=0mm, toptitle=1mm, arc=.35mm, toprule=.15mm, colbacktitle=quarto-callout-note-color!10!white, colback=white]

Edge Intelligence is the \textbf{Physical Layer} at its most extreme.
While the datacenter (\textbf{?@sec-compute}) provides abundant power
and cooling, the Edge provides none. The physical constraints of battery
density (mAh) and thermal dissipation (Watts) here set the hard limits
for what the \textbf{Operational Layer} (federated learning algorithms)
can achieve. If the model exceeds the thermal envelope, the physics will
throttle the logic.

\end{tcolorbox}

The transition to on-device learning introduces fundamental tension in
machine learning systems design. While cloud-based architectures
leverage abundant computational resources and controlled operational
environments, edge devices must function within severely constrained
resource envelopes. Memory is measured in megabytes rather than
gigabytes; power budgets are measured in milliwatts rather than watts;
and network connectivity may be intermittent or entirely absent.

Throughout this chapter, we trace these constraints through our
Archetype C lighthouse, where autonomy demands that learning happen
on-device under severe resource limits. The following callout previews
how these constraints manifest in practice.

\begin{tcolorbox}[enhanced jigsaw, rightrule=.15mm, bottomtitle=1mm, opacitybacktitle=0.6, breakable, leftrule=.75mm, bottomrule=.15mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Archetype C: Autonomy Constraints}, colframe=quarto-callout-note-color-frame, coltitle=black, opacityback=0, left=2mm, titlerule=0mm, toptitle=1mm, arc=.35mm, toprule=.15mm, colbacktitle=quarto-callout-note-color!10!white, colback=white]

\textbf{Archetype C (The Autonomous Health Monitor)} represents the
extreme end of this spectrum. Operating on milliwatt power budgets, it
cannot afford the energy cost of transmitting raw data to the cloud. For
Archetype C, \textbf{Data Locality} is not just a privacy feature but a
physical necessity imposed by the ``Power Wall.'' This chapter's
techniques, including quantized training, sparse updates, and federated
coordination, are the survival strategies that allow Archetype C to
exist.

\end{tcolorbox}

This chapter examines the foundations and methods necessary to navigate
this architectural tension. Building on computational efficiency
principles including quantization, pruning, and knowledge distillation,
we investigate the algorithmic techniques, design patterns, and system
principles that enable effective learning under extreme resource
constraints. The challenge extends beyond conventional optimization of
training algorithms. It requires rethinking the entire machine learning
pipeline for deployment environments where traditional computational
assumptions fail. The following definition formalizes this concept.

\phantomsection\label{callout-definitionux2a-1.1}
\begin{fbxSimple}{callout-definition}{Definition:}{On-Device Learning}
\phantomsection\label{callout-definition*-1.1}
\textbf{\emph{On-Device Learning}} is the local training or adaptation
of machine learning models directly on deployed hardware without server
connectivity, enabling \emph{personalization}, \emph{privacy
preservation}, and \emph{autonomous operation} under severe resource
constraints.

\end{fbxSimple}

The implications of this paradigm extend far beyond technical
optimization. They challenge core assumptions about machine learning
system development, deployment, and maintenance lifecycles. Models
transition from following predictable versioning patterns to exhibiting
continuous divergence and adaptation trajectories. Performance
evaluation shifts from centralized monitoring dashboards to distributed
assessment across heterogeneous user populations. Privacy preservation
evolves from a regulatory compliance consideration to a core
architectural requirement that shapes system design decisions.

Understanding these implications requires examining both the motivations
driving organizational adoption of on-device learning and the technical
challenges that must be addressed. This analysis establishes the
foundations and methods required to architect systems capable of
effective learning at the network edge while operating within stringent
constraints.

\section{Motivations and
Benefits}\label{sec-edge-intelligence-motivations-benefits-37c3}

Machine learning systems have traditionally relied on centralized
training pipelines. Models are developed and refined using large,
curated datasets and powerful cloud-based infrastructure
(\citeproc{ref-dean2012large}{Dean et al. 2012}). Once trained, these
models are deployed to client devices for inference, creating a clear
separation between the training and deployment phases. While this
architectural separation has served most use cases well, it imposes
significant limitations in modern applications where local data is
dynamic, private, or highly personalized.

On-device learning challenges this established model by enabling systems
to train or adapt directly on the device, without relying on constant
connectivity to the cloud. This shift represents more than a
technological advancement. It reflects changing application requirements
and user expectations that demand responsive, personalized, and
privacy-preserving machine learning systems.

Consider a smartphone keyboard adapting to a user's unique vocabulary
and typing patterns. To personalize predictions, the system must perform
gradient updates on a compact language model using locally observed text
input. A single gradient update for even a minimal language model
requires 50--100 MB of memory for activations and optimizer state.
Modern smartphones typically allocate 200--300 MB to background
applications like keyboards (varies by OS and device generation). This
razor-thin margin demonstrates the central engineering challenge of
on-device learning: a single training step can consume 25\% of available
memory. The system must achieve meaningful personalization while
operating within constraints so severe that traditional training
approaches become architecturally infeasible. This quantitative reality
drives the need for specialized techniques that make adaptation possible
within extreme resource limitations.

\subsection{On-Device Learning
Benefits}\label{sec-edge-intelligence-ondevice-learning-benefits-3256}

Understanding the driving forces behind on-device learning adoption
requires examining the inherent limitations of traditional centralized
approaches. Traditional machine learning systems rely on a clear
division of labor between model training and inference. Training is
performed in centralized environments with access to high-performance
compute resources and large-scale datasets. Once trained, models are
distributed to client devices, where they operate in a static
inference-only mode.

While this centralized paradigm has proven effective in many
deployments, it introduces fundamental limitations in scenarios where
data is user-specific, behavior is dynamic, or connectivity is
intermittent. These limitations become particularly acute as machine
learning moves beyond controlled environments into real-world
applications with diverse user populations and deployment contexts.

On-device learning addresses these limitations by enabling deployed
devices to perform model adaptation using locally available data.
On-device learning is not merely an efficiency optimization. It serves
as a cornerstone of building trustworthy AI systems. By keeping data
local, it provides a powerful foundation for privacy. By adapting to
individual users, it enhances fairness and utility. By enabling offline
operation, it improves robustness against network failures and
infrastructure dependencies. This chapter explores the engineering
required to build these trustworthy, adaptive systems.

This shift from centralized to decentralized learning is motivated by
four key considerations (\citeproc{ref-li2020federated}{Li et al.
2020}). These reflect both technological capabilities and changing
application requirements. The four considerations are personalization,
latency and availability, privacy, and infrastructure efficiency.

Personalization represents the most compelling motivation, as deployed
models often encounter usage patterns and data distributions that differ
from their training environments. Local adaptation allows models to
refine behavior in response to user-specific data, capturing linguistic
preferences, physiological baselines, sensor characteristics, or
environmental conditions. This capability is essential in applications
with high inter-user variability, where a single global model cannot
serve all users effectively.

Latency and availability constraints provide additional justification
for local learning. In edge computing scenarios, connectivity to
centralized infrastructure may be unreliable, delayed, or intentionally
limited to preserve bandwidth or reduce energy consumption. On-device
learning enables autonomous improvement of models even in fully offline
or delay-sensitive contexts, where round-trip updates to the cloud are
architecturally infeasible.

Privacy considerations provide a third compelling driver. Many modern
applications involve sensitive or regulated data including biometric
measurements, typed input, location traces, or health information. Local
learning mitigates privacy concerns by keeping raw data on the device
and operating within privacy-preserving boundaries. This potentially
aids adherence to regulations such as GDPR,\sidenote{\textbf{GDPR's ML
Impact}: When GDPR took effect in May 2018
(\citeproc{ref-gdpr2016regulation}{European Parliament and Council of
the European Union 2016}), it made centralized ML training illegal for
personal data without explicit consent. The ``right to be forgotten''
also meant models trained on personal data could be legally required to
``unlearn'' specific users, technically impossible with traditional
training. This drove massive investment in privacy-preserving ML
techniques. } HIPAA\sidenote{\textbf{HIPAA's ML Systems Impact}: The
Health Insurance Portability and Accountability Act (1996) imposes
strict requirements on ``protected health information'' (PHI) that
fundamentally shape health ML system architecture. Unlike GDPR's
consent-based framework, HIPAA requires physical, technical, and
administrative safeguards for any system processing PHI. For ML systems,
this means encrypted data at rest and in transit, audit trails for model
access, and Business Associate Agreements with any cloud providers.
On-device learning offers an architectural solution: by training locally
on wearables and medical devices, PHI never leaves the device,
simplifying compliance while enabling personalized health insights. }
(\citeproc{ref-hipaa1996health}{Tomes 1996}), or region-specific data
sovereignty laws.

Infrastructure efficiency provides economic motivation for distributed
learning approaches. Centralized training pipelines require substantial
backend infrastructure to collect, store, and process user data from
potentially millions of devices. By shifting learning to the edge,
systems reduce communication costs and distribute training workloads
across the deployment fleet, relieving pressure on centralized resources
while improving scalability.

\subsection{Alternative Approaches and Decision
Criteria}\label{sec-edge-intelligence-alternative-approaches-decision-criteria-bd36}

On-device learning represents a significant engineering investment with
inherent complexity that may not be justified by the benefits. Before
committing to this approach, teams should carefully evaluate whether
simpler alternatives can achieve comparable results with lower
operational overhead. Understanding when not to implement on-device
learning is as important as understanding its benefits, as premature
adoption can introduce unnecessary complexity without proportional
value.

Several alternative approaches often suffice for personalization and
adaptation requirements without local training complexity. Feature-based
personalization provides effective customization by storing user
preferences, interaction history, and behavioral features locally.
Rather than adapting model weights, the system feeds these stored
features into a static model to achieve personalization. News
recommendation systems exemplify this approach by storing user topic
preferences and reading patterns locally, then combining these features
with a centralized content model to provide personalized recommendations
without model updates.

Cloud-based fine-tuning with privacy controls enables personalization
through centralized adaptation with appropriate privacy safeguards. User
data is processed in batches during off-peak hours using
privacy-preserving techniques such as differential
privacy\sidenote{\textbf{Differential Privacy}: Mathematical framework
that provides quantifiable privacy guarantees by adding carefully
calibrated noise to computations. In federated learning, DP ensures that
individual user data cannot be inferred from model updates, even by
aggregators. Key parameter ε controls privacy-utility tradeoff: smaller
ε means stronger privacy but lower model accuracy. Typical deployments
use ε=1-8, requiring noise addition that can increase communication
overhead by 2-10\(\times\) and reduce model accuracy by 1-5\%. Essential
for regulatory compliance and user trust in distributed learning
systems. Differential privacy receives comprehensive treatment in
\textbf{?@sec-security-privacy}; this overview provides the context
needed for federated learning. } or federated analytics. This approach
often achieves superior accuracy compared to resource-constrained
on-device updates while maintaining acceptable privacy properties for
many applications.

User-specific lookup tables combine global models with personalized
retrieval mechanisms. The system maintains a lightweight, user-specific
lookup table for frequently accessed patterns while using a shared
global model for generalization. This hybrid approach provides
personalization benefits with minimal computational and storage
overhead.

The decision to implement on-device learning should be driven by
quantifiable requirements that preclude these simpler alternatives. True
data privacy constraints that legally prohibit cloud processing, genuine
network limitations that prevent reliable connectivity, quantitative
latency budgets that preclude cloud round-trips, or demonstrable
performance improvements that justify the operational complexity
represent legitimate drivers for on-device learning adoption.

For applications with critical timing requirements, network round-trip
times make cloud-based alternatives architecturally infeasible. Camera
processing under 33 ms, voice response under 500 ms, AR/VR
motion-to-photon latency under 20 ms, or safety-critical control under
10 ms all face network round-trip times typically ranging from 50 to 200
ms. In such scenarios, on-device learning becomes necessary regardless
of complexity considerations. Teams should thoroughly evaluate simpler
solutions before committing to the significant engineering investment
that on-device learning requires.

These motivations are grounded in the broader concept of knowledge
transfer, where a pretrained model transfers useful representations to a
new task or domain. This foundational principle makes on-device learning
both feasible and effective, enabling sophisticated adaptation with
minimal local resources. Figure~\ref{fig-transfer-conceptual}
illustrates how knowledge transfer occurs between closely related tasks
such as playing different board games or musical instruments, or across
domains that share structure such as from riding a bicycle to driving a
scooter. In the context of on-device learning, this means leveraging a
model pretrained in the cloud and adapting it efficiently to a new
context using only local data and limited updates, allowing fast
adaptation without relearning from scratch even when the new task
diverges in input modality or goal.

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{contents/vol2/edge_intelligence/images/png/ondevice_transfer_learning_apps.png}}

}

\caption{\label{fig-transfer-conceptual}Knowledge Transfer: Pretrained
models accelerate learning on new tasks by leveraging existing
representations, as seen by adapting skills between related board games
or musical instruments. This transfer extends across domains like
bicycle riding and scooter operation, where shared underlying structures
allow efficient adaptation with limited new data.}

\end{figure}%

This conceptual shift, enabled by transfer learning and adaptation,
enables real-world on-device applications. Whether adapting a language
model for personal typing preferences, adjusting gesture recognition to
individual movement patterns, or recalibrating a sensor model in
changing environments, on-device learning allows systems to remain
responsive, efficient, and user-aligned over time.

\subsection{Real-World Application
Domains}\label{sec-edge-intelligence-realworld-application-domains-c4e4}

Building on these established motivations, real-world deployments
demonstrate the practical impact of on-device learning across diverse
application domains. These domains span consumer technologies,
healthcare, industrial systems, and embedded applications. Each
showcases scenarios where personalization, latency, privacy, and
infrastructure efficiency become essential for effective machine
learning deployment.

Mobile input prediction represents the most mature and widely deployed
example of on-device learning. In systems such as smartphone keyboards,
predictive text and autocorrect features benefit substantially from
continuous local adaptation. User typing patterns are highly
personalized and evolve dynamically, making centralized static models
insufficient for optimal user experience. On-device learning allows
language models to fine-tune their predictions directly on the device,
achieving personalization while maintaining data locality.

For instance, Google's Gboard employs federated learning to improve
shared models across a large population of users while keeping raw data
local to each device (\citeproc{ref-hard2018federated}{Hard et al.
2018})\sidenote{\textbf{Gboard Federated Pioneer}: Gboard became the
first major commercial federated learning deployment in 2017, processing
updates from over 1 billion devices. The technical challenge was
immense: aggregating model updates while ensuring no individual user's
typing patterns could be inferred. Google's success with Gboard proved
federated learning could work at planetary scale, demonstrating 10-20\%
accuracy improvements over static models while maintaining strict
differential privacy guarantees. }.

Figure~\ref{fig-ondevice-gboard} demonstrates how different prediction
strategies enable local adaptation in real time. Next-word prediction
suggests likely continuations based on prior text, while Smart Compose
uses on-the-fly rescoring to offer dynamic completions. These techniques
demonstrate the sophistication of local inference mechanisms.

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{contents/vol2/edge_intelligence/images/png/ondevice_gboard_example.png}}

}

\caption{\label{fig-ondevice-gboard}On-Device Prediction Strategies:
Gboard employs both next-word prediction and smart compose with
on-the-fly rescoring to adapt to user typing patterns locally, enhancing
personalization and preserving privacy. These techniques demonstrate how
machine learning models can refine predictions in real time without
transmitting data to a central server, enabling efficient and private
mobile input experiences.}

\end{figure}%

Building on the consumer applications, wearable and health monitoring
devices present equally compelling use cases with additional regulatory
constraints. These systems rely on real-time data from accelerometers,
heart rate sensors, and electrodermal activity monitors to track user
health and fitness. Physiological baselines vary dramatically between
individuals, creating a personalization challenge that static models
cannot address effectively. On-device learning allows models to adapt to
these individual baselines over time, substantially improving the
accuracy of activity recognition, stress detection, and sleep staging
while meeting regulatory requirements for data localization.

Voice interaction technologies present another important application
domain with unique acoustic challenges. Wake-word
detection\sidenote{\textbf{Wake-Word Detection}: Always-listening
keyword spotting that activates voice assistants (``Hey Siri,'' ``OK
Google,'' ``Alexa''). These systems run continuously at approximately 1
mW power consumption, roughly 1000\(\times\) less than full speech
recognition. They use tiny neural networks (approximately 100 KB) with
specialized architectures optimized for sub-100 ms latency and minimal
false positive rates (\textless0.1 activations per hour). Modern systems
achieve 95\%+ accuracy while processing 16 kHz audio in real-time,
making on-device personalization (discussed in
Section~\ref{sec-edge-intelligence-federated-personalization-3c73})
critical for adapting to individual voice characteristics and reducing
false activations. } and voice interfaces in devices such as smart
speakers and earbuds must recognize voice commands quickly and
accurately even in noisy or dynamic acoustic environments.

These systems face strict latency requirements. Voice interfaces must
maintain end-to-end response times under 500 ms to preserve natural
conversation flow, with wake-word detection requiring sub-100 ms
response times to avoid user frustration. Local training allows models
to adapt to the user's unique voice profile and changing ambient
context, reducing false positives and missed detections while meeting
these demanding performance constraints. This adaptation is particularly
valuable in far-field audio settings, where microphone configurations
and room acoustics vary dramatically across deployments.

Beyond consumer applications, industrial IoT and remote monitoring
systems demonstrate the value of on-device learning in
resource-constrained environments. In applications such as agricultural
sensing, pipeline monitoring, or environmental surveillance,
connectivity to centralized infrastructure may be limited, expensive, or
entirely unavailable. On-device learning allows these systems to detect
anomalies, adjust thresholds, or adapt to seasonal trends without
continuous communication with the cloud. This capability is necessary
for maintaining autonomy and reliability in edge-deployed sensor
networks, where system downtime or missed detections can have
significant economic or safety consequences.

The most demanding applications emerge in embedded computer vision
systems including robotics, AR/VR, and smart cameras. These combine
complex visual processing with extreme timing constraints. Camera
applications must process frames within 33 ms to maintain 30 FPS
real-time performance. AR/VR systems demand motion-to-photon latencies
under 20 ms to prevent nausea and maintain immersion. Safety-critical
control systems require even tighter bounds, typically under 10 ms,
where delayed decisions can have severe consequences. These systems
operate in novel or rapidly changing environments that differ
substantially from their original training conditions. On-device
adaptation allows models to recalibrate to new lighting conditions,
object appearances, or motion patterns while meeting these critical
latency budgets that fundamentally drive the architectural decision
between on-device versus cloud-based processing.

Each domain reveals a common pattern. Deployment environments introduce
variation and context-specific requirements that cannot be anticipated
during centralized training. These applications demonstrate how the
motivational drivers manifest as concrete engineering constraints.
Mobile keyboards face memory limitations for storing user-specific
patterns. Wearable devices encounter energy budgets that restrict
training frequency. Voice interfaces must meet sub-100 ms latency
requirements that preclude cloud coordination. Industrial IoT systems
operate in network-constrained environments that demand autonomous
adaptation. This pattern illuminates the fundamental design requirement
shaping all subsequent technical decisions. Learning must be performed
efficiently, privately, and reliably under significant resource
constraints. Section~\ref{sec-edge-intelligence-design-constraints-c776}
analyzes these constraints systematically,
Section~\ref{sec-edge-intelligence-model-adaptation-6a82} presents
techniques for adapting models within tight resource envelopes, and
Section~\ref{sec-edge-intelligence-federated-learning-6e7e} establishes
protocols for privacy-preserving coordination across device populations.

\subsection{Architectural Trade-offs: Centralized vs.~Decentralized
Training}\label{sec-edge-intelligence-architectural-tradeoffs-centralized-vs-decentralized-training-d420}

These applications demonstrate the practical value of on-device learning
across diverse domains. Building on this foundation, we now examine how
on-device learning differs from traditional ML architectures, revealing
a complete reimagining of the training lifecycle that extends far beyond
simple deployment choices.

Understanding the shift that on-device learning represents requires
examining how traditional machine learning systems are structured and
where their limitations become apparent. Most machine learning systems
today follow a centralized learning paradigm that has served the field
well but increasingly shows strain under modern deployment requirements.
Models are trained in data centers using large-scale, curated datasets
aggregated from many sources. Once trained, these models are deployed to
client devices in a static form where they perform inference without
further modification. Updates to model parameters are handled
periodically through offline retraining, often using newly collected or
labeled data sent back from the field.

This established centralized model offers numerous proven advantages.
These include high-performance computing infrastructure, access to
diverse data distributions, and robust debugging and validation
pipelines. However, it also depends on several assumptions that may not
hold in modern deployment scenarios. These assumptions include reliable
data transfer, trust in data custodianship, and infrastructure capable
of managing global updates across device fleets. As machine learning is
deployed into increasingly diverse and distributed environments, the
limitations of this approach become more apparent and often prohibitive.

In contrast to this centralized approach, on-device learning embraces an
inherently decentralized paradigm that challenges many traditional
assumptions. Each device maintains its own copy of a model and adapts it
locally using data that is typically unavailable to centralized
infrastructure. Training occurs on-device, often asynchronously and
under varying resource conditions that change based on device usage
patterns, battery levels, and thermal states. Data never leaves the
device, reducing privacy exposure but also complicating coordination
between devices. Devices may differ dramatically in their hardware
capabilities, runtime environments, and patterns of use, making the
learning process heterogeneous and difficult to standardize. These
hardware variations create significant system design challenges.

This decentralized architecture introduces a new class of systems
challenges that extend well beyond traditional machine learning
concerns. Devices may operate with different versions of the model,
leading to inconsistencies in behavior across the deployment fleet.
Evaluation and validation become significantly more complex, as there is
no central point from which to measure performance across all devices
(\citeproc{ref-mcmahan2017communication}{McMahan et al. 2017}). Model
updates must be carefully managed to prevent degradation, and safety
guarantees become harder to enforce in the absence of centralized
testing and validation infrastructure.

Managing thousands of heterogeneous edge devices exceeds typical
distributed systems complexity. Device heterogeneity extends beyond
hardware differences to include varying operating system versions,
security patches, network configurations, and power management policies.
At any given time, 20 to 40\% of devices are offline
(\citeproc{ref-bonawitz2019towards}{Bonawitz et al. 2019}). Others have
been disconnected for weeks or months, creating persistent coordination
challenges.

When disconnected devices reconnect, they require state reconciliation
to avoid version conflicts. Update verification becomes critical as
devices can silently fail to apply updates or report success while
running outdated models. Robust systems implement multi-stage
verification. Cryptographic signatures confirm update integrity,
functional tests validate model behavior, and telemetry confirms
deployment success. Rollback strategies must handle partial deployments
where some devices received updates while others remain on previous
versions. This requires sophisticated orchestration to maintain system
consistency during failure recovery.

These challenges require different approaches to system design and
operational management compared to centralized ML systems, building on
distributed systems principles while introducing edge-specific
complexities.

Despite these challenges, decentralization introduces opportunities that
often justify the additional complexity. It allows for deep
personalization without centralized oversight, supports robust learning
in disconnected or bandwidth-limited environments, and reduces the
operational cost and infrastructure burden for model updates. Realizing
these benefits raises questions of how to effectively coordinate
learning across devices, whether through periodic synchronization,
federated aggregation, or hybrid approaches that balance local and
global objectives.

The move from centralized to decentralized learning represents more than
a shift in deployment architecture. It reshapes the entire design space
for machine learning systems, requiring new approaches to model
architecture, training algorithms, data management, and system
validation. In centralized training, data is aggregated from many
sources and processed in large-scale data centers. Models are trained,
validated, and then deployed in a static form to edge devices. In
contrast, on-device learning introduces a fundamentally different
paradigm. Models are updated directly on client devices using local
data, often asynchronously and under diverse hardware conditions. This
architectural transformation introduces coordination challenges while
enabling autonomous local adaptation. It requires careful consideration
of validation, system reliability, and update orchestration across
heterogeneous device populations.

On-device learning responds to the limitations of centralized machine
learning workflows. The transformation from centralized to decentralized
learning creates three distinct operational phases, each with different
characteristics and challenges.

The traditional centralized paradigm begins with cloud-based training on
aggregated data, followed by static model deployment to client devices.
This approach works well when data collection is feasible, network
connectivity is reliable, and a single global model can serve all users
effectively. However, it breaks down when data becomes personalized,
privacy-sensitive, or collected in environments with limited
connectivity.

Once deployed, local differences begin to emerge as each device
encounters its own unique data distribution. Devices collect data that
reflects individual user patterns, environmental conditions, and usage
contexts. This data is often non-IID\sidenote{\textbf{Non-IID
(Non-Independent and Identically Distributed)}: In machine learning,
data is IID when samples are drawn independently from the same
distribution. Non-IID violates this assumption, common in federated
learning where each device collects data from different users,
environments, or use cases. For example, smartphone keyboard data varies
dramatically between users (languages, writing styles, autocorrect
needs), making personalized model training essential but challenging for
convergence. } and noisy, requiring local model adaptation to maintain
performance. This transition marks the shift from global generalization
to local specialization.

The final phase introduces federated coordination, where devices
periodically synchronize their local adaptations through aggregated
model updates rather than raw data sharing. This enables
privacy-preserving global refinement while maintaining the benefits of
local personalization.

These three distinct phases represent an architectural evolution that
reshapes every aspect of the machine learning lifecycle.
Figure~\ref{fig-centralized-vs-decentralized} traces this evolution from
centralized training through local adaptation to federated coordination,
revealing how data flow, computational distribution, and coordination
mechanisms increase in complexity while enabling capabilities impossible
in purely centralized deployments. Understanding this progression helps
frame the challenges that on-device learning systems must address.

\begin{figure}[htb]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/db1b6235a1239f15b08cc6ed316f6c04e092210f.pdf}}

}

\caption{\label{fig-centralized-vs-decentralized}The evolution from
centralized cloud training (region A) through local device adaptation
(region B) to federated coordination (region C) represents a fundamental
shift in machine learning architecture. Each phase introduces distinct
operational characteristics, from uniform global models to personalized
local adaptations to coordinated distributed learning.}

\end{figure}%

\section{Design
Constraints}\label{sec-edge-intelligence-design-constraints-c776}

The architectural transformation from centralized to decentralized
learning, examined in the previous section, creates engineering
challenges that require systematic analysis. While cloud-based training
enjoys virtually unlimited computational budgets, edge devices must
achieve meaningful adaptation within severely constrained resource
envelopes. Understanding these constraints quantitatively enables
principled design decisions about what adaptation is feasible on which
devices.

Three efficiency dimensions define the on-device learning design space:
algorithmic efficiency (how many parameters must change), compute
efficiency (how many operations per update), and data efficiency (how
many examples are needed). These dimensions interact multiplicatively
rather than additively, creating a constrained optimization problem far
more challenging than inference-only deployment. Compression techniques
including quantization, pruning, and knowledge distillation (foundations
established in model optimization principles) enable deployment on
resource-constrained devices, but on-device learning amplifies these
constraints dramatically.

On-device learning operates under the same efficiency constraints as
inference but with training-specific amplifications that make
optimization far more demanding. Where inference requires a single
forward pass through the network, training demands forward propagation,
gradient computation through backpropagation, and weight updates,
increasing memory requirements by 3-5\(\times\) and computational costs
by 2-3\(\times\). The model compression techniques that enable efficient
inference become baseline requirements rather than optimizations, as
training within edge device constraints would be impossible without
aggressive compression.

Given the established motivations for on-device learning, we now examine
the fundamental engineering challenges that shape its implementation.
Enabling learning on the device requires completely rethinking
conventional assumptions about where and how machine learning systems
operate. In centralized environments, models are trained with access to
extensive compute infrastructure, large and curated datasets, and
generous memory and energy budgets. At the edge, none of these
assumptions hold, creating a fundamentally different design space.

On-device learning constraints fall into three critical dimensions that
define the efficiency framework. The first dimension is model
compression requirements, which determine algorithmic efficiency. The
second is sparse and non-uniform data characteristics, which determine
data efficiency. The third is severely limited computational resources,
which determine compute efficiency. These three dimensions form an
interconnected constraint space that defines the feasible region for
on-device learning systems. Each dimension imposes distinct limitations
that influence algorithmic choices, system architecture, and deployment
strategies.

\subsection{Quantifying Training Overhead on Edge
Devices}\label{sec-edge-intelligence-quantifying-training-overhead-edge-devices-3e4c}

The transition from inference-only deployment to on-device training
creates multiplicative rather than additive complexity. These
constraints interact and amplify each other in ways that reshape system
design requirements, building on resource optimization principles
including quantization, pruning, and knowledge distillation while
introducing new challenges specific to distributed learning
environments.

Standard efficiency constraints apply to both inference and training.
These constraints include quantization, pruning, and knowledge
distillation. However, training amplifies each constraint dimension by 3
to 10 times. Table~\ref{tbl-training-amplification} quantifies how
memory requirements increase 3-5\(\times\), compute operations grow
2-3\(\times\), and energy consumption can balloon 10-50\(\times\) when
transitioning from inference to on-device training.

\begin{tcolorbox}[enhanced jigsaw, rightrule=.15mm, bottomtitle=1mm, opacitybacktitle=0.6, breakable, leftrule=.75mm, bottomrule=.15mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Figure: Training Memory Amplification}, colframe=quarto-callout-note-color-frame, coltitle=black, opacityback=0, left=2mm, titlerule=0mm, toptitle=1mm, arc=.35mm, toprule=.15mm, colbacktitle=quarto-callout-note-color!10!white, colback=white]

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/99a20c856ec66a46313027dc5bdb889488a69d2d.pdf}}

\end{tcolorbox}

\textbf{Peak Memory Usage}

Memory consumption during training is not static. It fluctuates
dynamically, reaching a maximum during the backward pass when
activations, gradients, and optimizer states must coexist. This peak
memory usage determines whether a model can be trained on a device. A
model that averages 50 MB usage might spike to 200 MB during
backpropagation, causing an out-of-memory crash on a device with 128 MB
RAM. Techniques like gradient checkpointing mitigate this by discarding
intermediate activations during the forward pass and recomputing them
on-demand during the backward pass. This approach trades increased
computation (20 to 30\%) for a dramatic reduction in peak memory, often
achieving 3 to 4 times reduction.

These amplifications explain why simply applying standard optimization
techniques to training workloads is insufficient. Each constraint
category shapes on-device learning system design, requiring approaches
that build on but extend beyond inference-focused methods.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2500}}@{}}
\caption{\textbf{Training Amplifies Inference Constraints}: On-device
learning operates under the same efficiency constraints as inference but
with training-specific amplifications that make optimization
dramatically more challenging. This table quantifies how each constraint
dimension intensifies when transitioning from running pre-trained models
to adapting them locally. Amplification factors assume standard
backpropagation without optimizations like gradient
checkpointing.}\label{tbl-training-amplification}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Constraint Dimension}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Inference}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Training Amplification}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Impact on Design}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Constraint Dimension}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Inference}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Training Amplification}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Impact on Design}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Memory Footprint} & Model weights + single activation map &
Weights + full activation cache + gradients + optimizer state &
3-5\(\times\) increase; forces aggressive compression \\
\textbf{Compute Operations} & Forward pass only & Forward + backward +
weight update & 2-3\(\times\) increase; limits model complexity \\
\textbf{Memory Bandwidth} & Sequential weight reads & Bidirectional data
flow for gradients & 5-10\(\times\) increase; creates bottlenecks \\
\textbf{Energy per Sample} & Single inference operation & Multiple
gradient steps with convergence & 10-50\(\times\) increase; requires
opportunistic scheduling \\
\textbf{Data Requirements} & Pre-collected, curated datasets & Sparse,
noisy, streaming local data & Necessitates sample-efficient methods \\
\textbf{Hardware Utilization} & Optimized for forward passes & Different
access patterns for backprop & Inference accelerators may not help
training \\
\end{longtable}

Figure~\ref{fig-ondevice-pretraining} illustrates how the complete
training pipeline combines offline pre-training with online adaptive
learning on resource-constrained IoT devices. The system first undergoes
meta-training with generic data. During deployment, device-specific
constraints such as data availability, compute, and memory shape the
adaptation strategy by ranking and selecting layers and channels to
update. This allows efficient on-device learning within limited resource
envelopes.

\begin{figure}[htb]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/b1f51ff6e3ab976a5ea6b81f900a28f87e99f81c.pdf}}

}

\caption{\label{fig-ondevice-pretraining}Resource-constrained devices
use a two-stage learning process. Offline pre-training establishes
initial model weights. Online adaptation then selectively updates layers
based on available data, compute, and memory. This approach balances
model performance with the practical limitations of edge deployment,
enabling continuous learning in real-world environments.}

\end{figure}%

\subsection{Model
Constraints}\label{sec-edge-intelligence-model-constraints-9232}

The first dimension of on-device learning constraints centers on the
model itself. Its structure, size, and computational requirements
determine deployment feasibility. The structure and size of the machine
learning model directly influence whether on-device training is even
possible, let alone practical. Unlike cloud-deployed models that can
span billions of parameters and rely on multi-gigabyte memory budgets,
models intended for on-device learning must conform to tight constraints
on memory, storage, and computational complexity. These constraints
apply not only at inference time, but become even more restrictive
during training, where additional resources are needed for gradient
computation, parameter updates, and optimizer state management.

The scale of these constraints becomes apparent when examining specific
examples across the device spectrum. The MobileNetV2 architecture,
commonly used in mobile vision tasks, requires approximately 14 MB of
storage in its standard configuration. While this memory requirement is
entirely feasible for modern smartphones with gigabytes of available
RAM, it far exceeds the memory available on embedded microcontrollers
such as the Arduino Nano 33 BLE Sense\sidenote{\textbf{Arduino Edge
Computing Reality}: The Arduino Nano 33 BLE Sense represents typical
microcontroller constraints: 256 KB SRAM is roughly 65,000 times smaller
than a modern smartphone's 16 GB RAM (flagship devices). To put this in
perspective, storing just one 224×224×3 RGB image (150 KB) would consume
60\% of available memory. Training requires 3-5\(\times\) more memory
for gradients and activations, making even tiny models challenging. The
1 MB flash storage can hold only the smallest quantized models, forcing
designers to use 8-bit or even 4-bit representations. }, which provides
only 256 KB of SRAM and 1 MB of flash storage. This dramatic difference
in available resources necessitates aggressive model compression
techniques. In such severely constrained platforms, even a single layer
of a typical convolutional neural network may exceed available RAM
during training due to the need to store intermediate feature maps and
gradient information.

Beyond static storage requirements, the training process itself
dramatically expands the effective memory footprint, creating an
additional layer of constraint. Standard backpropagation requires
caching activations for each layer during the forward pass, which are
then reused during gradient computation in the backward pass. As
established in the amplification analysis above, this activation caching
multiplies memory requirements compared to inference-only deployment.
For a seemingly modest 10-layer convolutional model processing
\(64 \times 64\) images, the required memory may exceed 1 to 2 MB, well
beyond the SRAM capacity of most embedded systems and highlighting the
fundamental tension between model expressiveness and resource
availability.

Compounding these memory constraints, model complexity directly affects
runtime energy consumption and thermal limits, introducing additional
practical barriers to deployment. In systems such as smartwatches or
battery-powered wearables, sustained model training can rapidly deplete
energy reserves or trigger thermal throttling that degrades performance.
Training a full model using floating-point operations on these devices
is often infeasible from an energy perspective, even when memory
constraints are satisfied. These practical limitations have motivated
the development of ultra-lightweight model variants, such as MLPerf Tiny
benchmark networks (\citeproc{ref-banbury2021mlperf}{Banbury et al.
2021}), which fit within 100--200 KB and can be adapted using only
partial gradient updates. These specialized models employ aggressive
quantization and pruning strategies to achieve such compact
representations while maintaining sufficient expressiveness for
meaningful adaptation.

The practical implications of battery and thermal constraints extend
beyond just limiting training duration. Mobile devices must carefully
balance training opportunities with user experience. Aggressive
on-device training can cause noticeable device heating and rapid battery
drain, leading to user dissatisfaction and potential app uninstalls.
Modern smartphones typically limit sustained processing to 2-3 W for ML
workloads to prevent thermal discomfort, though they can burst to 5-10 W
for brief periods before thermal throttling kicks in. Training even
modest models can easily exceed these sustainable power limits. This
reality necessitates intelligent scheduling strategies: training during
charging periods when thermal dissipation is improved, utilizing
low-power cores for gradient computation when possible, and implementing
thermal-aware duty cycling that pauses training when temperature
thresholds are exceeded. Some systems even leverage device usage
patterns, scheduling intensive adaptation only during overnight charging
when the device is idle and connected to power.

Given these multifaceted constraints, the model architecture itself must
be fundamentally designed with on-device learning capabilities in mind
from the outset. Many conventional architectures, such as large
transformers or deep convolutional networks, are simply not viable for
on-device adaptation due to their inherent size and computational
complexity. Instead, specialized lightweight architectures such as
MobileNets\sidenote{\textbf{MobileNet Innovation}: Google's MobileNet
family revolutionized mobile AI by achieving 10--20× parameter reduction
compared to traditional CNNs. MobileNetV1 (2017) used depthwise
separable convolutions to reduce floating-point operations (FLOPs) by
8--9×, while MobileNetV2 (2018) added inverted residuals and linear
bottlenecks. The breakthrough allowed real-time inference on
smartphones: MobileNetV2 runs ImageNet classification in approximately
75 ms on a Pixel phone versus 1.8 seconds for ResNet-50
(\citeproc{ref-he2016deep}{He et al. 2015}). }, SqueezeNet
(\citeproc{ref-iandola2016squeezenet}{Iandola et al. 2016}), and
EfficientNet (\citeproc{ref-tan2019efficientnet}{Tan and Le 2019}) have
been developed specifically for resource-constrained environments. These
architectures leverage efficiency principles and architectural
optimizations, rethinking how neural networks can be structured. These
specialized models employ techniques such as depthwise separable
convolutions\sidenote{\textbf{Depthwise Separable Convolutions}: First
popularized in the Xception architecture
(\citeproc{ref-chollet2017xception}{Chollet 2017}), this technique
decomposes standard convolution into two operations: depthwise
convolution (applies single filter per input channel) and pointwise
convolution (1×1 convolution to combine channels). For a 3×3 convolution
with 512 input/output channels, standard convolution requires 2.4 M
parameters while depthwise separable needs only 13.8 K, a 174×
reduction. The computational savings are similarly dramatic, making
real-time inference possible on mobile CPUs. }, bottleneck layers, and
aggressive quantization to dramatically reduce memory and compute
requirements while maintaining sufficient performance for practical
applications.

These architectures are often designed to be modular, allowing for easy
adaptation and fine-tuning. For example, MobileNets
(\citeproc{ref-howard2017mobilenets}{Howard et al. 2017}) and
MobileNetV2 (\citeproc{ref-sandler2018mobilenetv2}{Sandler et al. 2018})
can be configured with different width multipliers and resolution
settings to balance performance and resource usage. Concretely,
MobileNetV2 with α=1.0 requires 3.4 M parameters (13.6 MB in FP32), but
with α=0.5 this drops to 0.7 M parameters (2.8 MB), enabling deployment
on devices with just 4 MB available RAM. This flexibility is important
for on-device learning, where the model must adapt to the specific
constraints of the deployment environment.

Given these architectural considerations, we turn to the complementary
challenge that shapes model adaptation: the characteristics of available
training data. While model architecture determines the memory and
computational baseline for on-device learning, data availability and
quality introduce equally fundamental limitations that shape every
aspect of the learning process.

\subsection{Data
Constraints}\label{sec-edge-intelligence-data-constraints-303e}

The second dimension of on-device learning constraints centers on data
availability and quality. The nature of data available to on-device ML
systems differs dramatically from the large, curated, and centrally
managed datasets used in cloud-based training. At the edge, data is
locally collected, temporally sparse, and often unstructured or
unlabeled, creating a different learning environment. These
characteristics introduce multifaceted challenges in volume, quality,
and statistical distribution, all of which directly affect the
reliability and generalizability of learning on the device.

Data volume represents the first major constraint, severely limited by
both storage constraints and the sporadic nature of user interaction.
For example, a smart fitness tracker may collect motion data only during
physical activity, generating relatively few labeled samples per day. If
a user wears the device for just 30 minutes of exercise, only a few
hundred data points might be available for training, compared to the
thousands or millions typically required for effective supervised
learning in controlled environments. This scarcity changes the learning
paradigm from data-rich to data-efficient algorithms.

Beyond volume limitations, on-device data is frequently non-IID
(non-independent and identically distributed)
(\citeproc{ref-zhao2018federated}{Zhao et al. 2018}), creating
statistical challenges that cloud-based systems rarely encounter. This
heterogeneity manifests across multiple dimensions: user behavior
patterns, environmental conditions, linguistic preferences, and usage
contexts. A voice assistant deployed across households encounters wide
variation in accents, languages, speaking styles, and command patterns.
Similarly, smartphone keyboards adapt to individual typing patterns,
autocorrect preferences, and multilingual usage that varies widely
between users. This data heterogeneity complicates both model
convergence and the design of update mechanisms that must generalize
across devices while maintaining personalization.

Compounding these distribution challenges, label scarcity presents an
additional critical obstacle that severely limits traditional learning
approaches. Most edge-collected data is unlabeled by default, requiring
systems to learn from weak or implicit supervision signals. In a
smartphone camera, for instance, the device may capture thousands of
images throughout the day, but only a few are associated with meaningful
user actions (e.g., tagging, favoriting, or sharing), which could serve
as implicit labels. In many applications, including detecting anomalies
in sensor data and adapting gesture recognition models, explicit labels
may be entirely unavailable, making traditional supervised learning
infeasible without developing alternative methods for weak supervision
or unsupervised adaptation.

Data quality issues add another layer of complexity to the on-device
learning challenge. Noise and variability further degrade the already
limited data available for training. Embedded systems such as
environmental sensors or automotive ECUs may experience fluctuations in
sensor calibration, environmental interference, or mechanical wear,
leading to corrupted or drifting input signals over time. Without
centralized validation systems to detect and filter these errors, they
may silently degrade learning performance, creating a reliability
challenge that cloud-based systems can more easily address through data
preprocessing pipelines.

Finally, data privacy and security concerns impose the most restrictive
constraints of all, often making data sharing architecturally impossible
rather than merely undesirable. Sensitive information, such as health
data, personal communications, or user behavioral patterns, must be
protected from unauthorized access under legal and ethical requirements.
This constraint often completely precludes the use of traditional
data-sharing methods, such as uploading raw data to a central server for
training. Instead, on-device learning must rely on sophisticated
techniques that enable local adaptation without ever exposing sensitive
information, changing how learning systems can be designed and
validated.

\subsection{Compute
Constraints}\label{sec-edge-intelligence-compute-constraints-4d6d}

The edge hardware landscape provides computational substrate for machine
learning, spanning from microcontrollers like STM32F4 and ESP32 at the
most constrained end, to mobile-class processors with dedicated AI
accelerators (Apple Neural Engine, Qualcomm Hexagon, Google Tensor) in
the middle, and high-capability edge devices at the upper end. While
these devices offer varying levels of inference capabilities
(computational throughput, memory bandwidth, and energy efficiency when
executing pre-trained models), training workloads exhibit fundamentally
different computational characteristics that reshape hardware
utilization patterns.

Building on this edge hardware landscape, from microcontrollers to
mobile AI accelerators, on-device learning must operate within severely
constrained computational envelopes that differ from cloud-based
training infrastructure by factors of hundreds or thousands in raw
computational capacity.

The key difference: backpropagation requires significantly higher memory
bandwidth than inference due to gradient computation and activation
caching, weight updates create write-heavy access patterns unlike
inference's read-only operations, and optimizer state management demands
additional memory allocation that inference never encounters. These
training-specific demands mean hardware perfectly adequate for inference
may prove entirely inadequate for adaptation, even when updating only a
small parameter subset.

At the most constrained end of the spectrum, devices such as the
STM32F4\sidenote{\textbf{STM32F4 Microcontroller Reality}: The STM32F4
represents the harsh reality of embedded computing: 192 KB SRAM (roughly
the size of a small JPEG image) and 1 MB flash storage, running at 168
MHz without floating-point hardware acceleration. Integer arithmetic is
10--100× slower than dedicated floating-point units found in mobile
chips. Power consumption is approximately 100 mW during active
processing, requiring careful duty-cycling to preserve battery life.
These constraints make even simple neural networks challenging: a
10-neuron hidden layer requires approximately 40 KB for weights alone in
FP32. } or ESP32\sidenote{\textbf{ESP32 Edge Computing}: The ESP32
provides 520 KB SRAM and dual-core processing at 240 MHz, making it more
capable than STM32F4 but still severely constrained. Its key advantage
is built-in WiFi and Bluetooth for federated learning scenarios.
However, the lack of hardware floating-point support means all ML
operations must use integer quantization. Real-world deployments show
8-bit quantized models can achieve 95\% of FP32 accuracy while fitting
in approximately 50 KB memory, enabling basic on-device training for
simple tasks like sensor anomaly detection. } microcontrollers offer
only a few hundred kilobytes of SRAM and completely lack hardware
support for floating-point operations
(\citeproc{ref-lai2020tinyml}{Warden and Situnayake 2020}). Libraries
like CMSIS-NN (\citeproc{ref-lai2018cmsis}{Lai, Suda, and Chandra 2018})
provide optimized neural network kernels specifically designed for Arm
Cortex-M processors, achieving 4.6x runtime improvement over baseline
implementations through fixed-point arithmetic and SIMD optimizations.
These extreme constraints represent the fundamental limitations of edge
hardware. Such severe limitations preclude the use of conventional deep
learning libraries and require models to be meticulously designed for
integer arithmetic and minimal runtime memory allocation. In these
environments, even apparently simple models require highly specialized
techniques, including quantization-aware
training\sidenote{\textbf{Quantization-Aware Training}: Unlike
post-training quantization which converts trained FP32 models to INT8,
quantization-aware training simulates low-precision arithmetic during
training itself. This allows the model to learn robust representations
despite reduced precision. Critical for edge devices where INT8
operations consume \(4\times\) less power and enable \(4\times\) faster
inference compared to FP32, while maintaining 95-99\% of original
accuracy. } and selective parameter updates, to execute training loops
without exceeding memory or power budgets.

The practical implications are stark: while the STM32F4 microcontroller
can run a simple linear regression model with a few hundred parameters,
training even a small convolutional neural network would immediately
exceed its memory capacity. In these severely constrained environments,
training is often limited to simple algorithms such as stochastic
gradient descent (SGD)\sidenote{\textbf{Stochastic Gradient Descent
(SGD)}: The fundamental optimization algorithm for neural networks,
updating parameters using gradients computed on small batches (or single
samples). Unlike full-batch gradient descent, SGD's randomness helps
escape local minima while requiring minimal memory, storing only current
parameters and gradients. This simplicity makes SGD ideal for
microcontrollers where advanced optimizers like Adam would exceed memory
budgets. } or \(k\)-means clustering, which can be implemented using
integer arithmetic and minimal memory overhead, representing a
fundamental departure from modern machine learning practice.

Moving up the computational hierarchy, mobile-class hardware represents
improvement but still operates under severe constraints. Platforms
including the Qualcomm Snapdragon, Apple Neural
Engine\sidenote{\textbf{Apple Neural Engine Evolution}: Apple's Neural
Engine has evolved dramatically since the A11 Bionic. The A17 Pro (2023)
features a 16-core Neural Engine delivering 35 TOPS, roughly equivalent
to an NVIDIA GTX 1080 Ti. This represents a 58× improvement over the
original A11. The Neural Engine specializes in matrix operations with
dedicated 8-bit and 16-bit arithmetic units, enabling efficient
on-device training. Real-world performance: fine-tuning a MobileNet
classifier takes approximately 2 seconds versus 45 seconds on CPU alone,
while consuming only approximately 500 mW additional power. }, and
Google Tensor SoC\sidenote{\textbf{Google Tensor SoC Architecture}:
Google's Tensor chips (starting with Pixel 6 in 2021) feature a custom
TPU v1-derived Edge TPU optimized for ML workloads. Unlike Apple's
Neural Engine, Tensor optimizes for Google's specific models (speech
recognition, computational photography). The TPU provides efficient
8-bit integer operations while consuming only 2 W, making it highly
efficient for federated learning scenarios where devices train locally
on speech or image data. } provide significantly more compute power than
microcontrollers, often featuring dedicated AI accelerators and
optimized support for 8-bit or
mixed-precision\sidenote{\textbf{Mixed-Precision Training}: Uses
different numerical precisions for different operations, typically FP16
for forward/backward passes and FP32 for parameter updates. This halves
memory usage and doubles throughput on modern hardware with Tensor
Cores, while maintaining training stability through automatic loss
scaling. Mobile implementations often use INT8 for inference and FP16
for gradient computation, balancing accuracy with hardware constraints.
} matrix operations. These accelerators offer dedicated matrix
multiplication units, on-chip memory hierarchies, and power management
features specifically designed for neural network inference and,
increasingly, training workloads. While these platforms can support more
sophisticated training routines, including full backpropagation over
compact models, they still fall far short of the computational
throughput and memory bandwidth available in centralized data centers.
For instance, training a lightweight
transformer\sidenote{\textbf{Lightweight Transformers}: Mobile-optimized
transformer architectures like MobileBERT
(\citeproc{ref-sun2020mobilebert}{Sun et al. 2020}) and DistilBERT
(\citeproc{ref-sanh2019distilbert}{Sanh et al. 2019}) achieve 4--6×
speedup over full models through techniques like knowledge distillation,
layer reduction, and attention head pruning. MobileBERT retains 97\% of
BERT-base accuracy while running inference in approximately 40 ms on
mobile CPUs versus 160 ms for full BERT. Key optimizations include
bottleneck attention mechanisms and specialized mobile-friendly layer
configurations. } on a smartphone is technically feasible but must be
tightly bounded in both time and energy consumption to avoid degrading
the user experience, highlighting the persistent tension between
learning capabilities and practical deployment constraints.

These computational limitations become especially acute in real-time or
battery-operated systems, as demonstrated in camera processing
requirements, where specific latency budgets create hard architectural
constraints. Camera applications processing at 30 FPS cannot exceed 33
ms per frame, voice interfaces require rapid response times for natural
interaction, AR/VR systems demand sub-20 ms motion-to-photon latency to
prevent user discomfort, and safety-critical control systems must
respond within 10 ms to ensure operational safety. These quantitative
constraints determine whether on-device learning is feasible or whether
cloud-based alternatives become architecturally necessary. In a
smartphone-based speech recognizer, on-device adaptation must seamlessly
coexist with primary inference workloads without interfering with
response latency or system responsiveness. Similarly, in wearable
medical monitors, training must occur opportunistically during carefully
managed windows (typically during periods of low activity or charging)
to preserve battery life and avoid thermal management issues.

Beyond raw computational capacity, the architectural implications of
these hardware constraints extend into fundamental system design
choices. Training operations exhibit fundamentally different memory
access patterns than inference workloads: backpropagation requires 3--5×
higher memory bandwidth due to gradient computation and activation
caching, creating bottlenecks that pure computational metrics don't
capture. Modern edge accelerators attempt to address these challenges
through increasingly specialized hardware features. Adaptive precision
datapaths allow dynamic switching between INT4 for forward passes and
FP16 for gradient computation, optimizing both accuracy and efficiency
within power budgets. Sparse computation units accelerate selective
parameter updates by skipping zero gradients, a capability critical for
efficient bias-only and LoRA adaptations. Near-memory compute
architectures\sidenote{\textbf{Near-Memory Computing}: Places processing
units directly adjacent to or within memory arrays, dramatically
reducing data movement costs. Traditional von Neumann architectures
spend 100-1000\(\times\) more energy moving data than computing on it.
Near-memory designs can perform matrix operations with 10-100\(\times\)
better energy efficiency by eliminating costly memory bus transfers.
Critical for edge training where gradient computations require intensive
memory access patterns that overwhelm traditional cache hierarchies. }
reduce data movement costs by performing gradient updates directly
adjacent to weight storage, addressing the memory bandwidth bottleneck.
However, most current edge accelerators remain fundamentally optimized
for inference workloads, creating significant hardware-software
co-design opportunities for future generations of on-device training
accelerators specifically designed to handle the unique demands of local
adaptation.

\subsubsection{The Mobile Memory
Wall}\label{sec-edge-intelligence-mobile-memory-wall-5ffe}

While mobile NPUs deliver impressive TOPS, the ``Memory Wall'' that
\textbf{?@sec-compute} examines becomes an impassable barrier for
large-scale models on edge devices. Recent analysis
(\citeproc{ref-ma2024challenges}{X. Ma and Patterson 2024}) highlights
that autoregressive ``Decode'' operations are strictly bandwidth-bound.

The quantitative disparity is stark:

\begin{itemize}
\tightlist
\item
  \textbf{Datacenter (H100)}: HBM3 provides 3,350 GB/s bandwidth.
\item
  \textbf{Flagship Mobile (A17/Snapdragon 8 Gen 3)}: LPDDR5X provides
  64--100 GB/s bandwidth.
\end{itemize}

This 30--50\(\times\) bandwidth gap means that even if a model fits in
mobile RAM, it will generate tokens 30--50\(\times\) slower than a
datacenter GPU. Consequently, on-device LLM serving requires aggressive
quantization (INT4 or even INT2) not merely for capacity, but as a
\textbf{bandwidth survival strategy}. By reducing model size by
\(8\times\), we effectively increase the relative bandwidth, making
interactive generation speeds possible on mobile hardware.

\subsection{Edge Hardware Integration
Challenges}\label{sec-edge-intelligence-edge-hardware-integration-challenges-a240}

Beyond the individual constraints of models, data, and computation,
on-device learning systems must navigate the complex interactions
between these elements and the underlying physics of mobile computing:
power dissipation, thermal limits, and energy budgets. These physical
constraints are not mere engineering details; they are fundamental
design drivers that determine the entire feasible space of on-device
learning algorithms. Understanding these quantitative constraints
enables informed design decisions that balance learning capabilities
with long-term system sustainability and user acceptance.

\subsubsection{Energy and Thermal Constraint
Analysis}\label{sec-edge-intelligence-energy-thermal-constraint-analysis-3133}

Energy and thermal management represent perhaps the most challenging
aspect of on-device learning system design, as they directly impact user
experience and device longevity. Mobile devices operate under strict
power budgets that fundamentally determine feasible model complexity and
training schedules. The thermal design power (TDP) of mobile processors
creates hard constraints that shape every aspect of on-device learning
strategies. Modern smartphones typically maintain sustained processing
at 2-3 W for ML workloads to prevent thermal discomfort, but can burst
to 5-10 W for brief periods before thermal throttling dramatically
reduces performance by 50\% or more. This thermal cycling behavior
forces training algorithms to operate in carefully managed burst modes,
utilizing peak performance for only 10-30 seconds before backing off to
sustainable power levels, a constraint that fundamentally changes how
training algorithms must be designed. The following worked example
illustrates the practical implications of these energy constraints.

\phantomsection\label{callout-notebookux2a-1.2}
\begin{fbxSimple}{callout-notebook}{AI Engineer’s Notebook:}{The Energy of Learning}
\phantomsection\label{callout-notebook*-1.2}
\textbf{Problem}: You want to fine-tune a small language model (1B
parameters) on a user's smartphone overnight. Is this feasible within a
\textbf{5\% battery budget}?

\textbf{The Math}:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Phone Battery}: Typical capacity is 15 Wh (Watt-hours)
  \(\approx\) 54,000 Joules.
\item
  \textbf{Budget}: 5\% of 54,000 J = \textbf{2,700 Joules}.
\item
  \textbf{Training Cost}:

  \begin{itemize}
  \tightlist
  \item
    Forward pass: \(\approx 2 \text{ nJ/param}\).
  \item
    Backward pass: \(\approx 4 \text{ nJ/param}\).
  \item
    Total per token:
    \(6 \text{ nJ/param} \times 10^9 \text{ params} = 6 \text{ Joules/token}\).
  \end{itemize}
\item
  \textbf{Capacity}:
  \(2,700 \text{ J} / 6 \text{ J/token} = \mathbf{450 \text{ tokens}}\).
\end{enumerate}

\textbf{The Systems Conclusion}: With a 1B parameter model, 5\% of a
phone battery buys you only \textbf{450 tokens} of training (a few
paragraphs). Full fine-tuning is impossible. You \emph{must} use
\textbf{PEFT (Parameter-Efficient Fine-Tuning)} or update only the last
layer to reduce the energy cost per token by 100×.

\end{fbxSimple}

The mobile power budget hierarchy reveals the tight constraints under
which on-device learning must operate. Smartphone sustained processing
is limited to 2--3 W to prevent user-noticeable heating and maintain
acceptable battery life throughout the day. Peak training burst mode can
reach 10 W, but this power level is sustainable for only 10--30 seconds
before thermal throttling kicks in to protect the hardware. Dedicated
neural processing units consume 0.5--2 W for AI workloads, offering
optimized power efficiency compared to general-purpose processors.
CPU-based AI processing requires 3--5 W and demands aggressive thermal
management with duty cycling to prevent overheating, making it the least
power-efficient option for sustained on-device learning.

The fundamental physics of energy consumption reveals why local
processing is almost always preferable to cloud offloading for on-device
learning. The following perspective examines this trade-off
quantitatively.

\phantomsection\label{callout-perspectiveux2a-1.3}
\begin{fbxSimple}{callout-perspective}{Systems Perspective:}{The Energy Hierarchy}
\phantomsection\label{callout-perspective*-1.3}
\textbf{The Trade-off}: Should you process data locally or send it to
the cloud? The physics of energy consumption provides a clear answer.

\textbf{Energy Cost per Operation (Approximate):}

\begin{itemize}
\tightlist
\item
  \textbf{32-bit Integer Add}: 0.1 pJ
\item
  \textbf{32-bit Float Mult}: 4.0 pJ
\item
  \textbf{32-bit DRAM Read}: 640 pJ
\item
  \textbf{Wireless Transmit (1 bit)}: 100,000 - 500,000 pJ
  (Bluetooth/WiFi)
\end{itemize}

\textbf{Ratio}: Transmitting a single bit of data costs roughly the same
energy as performing \textbf{100,000 to 500,000} compute operations.

\textbf{Conclusion}: If you can extract insight from data using fewer
than \textasciitilde100k operations per bit, \textbf{local processing is
strictly more energy efficient} than cloud offloading. This ratio drives
the architecture of Federated Learning: compute is cheap; radio
transmission is expensive.

\end{fbxSimple}

The power consumption characteristics of training workloads create
additional layers of constraint that extend beyond simple computational
capacity. Power consumption scales superlinearly with model size and
training complexity, with training operations consuming 10-50\(\times\)
more power than equivalent inference workloads due to the substantial
computational overhead of gradient computation (consuming 40-70\% of
training power), weight updates (20-30\%), and dramatically increased
data movement between memory hierarchies (10-30\%). To maintain
acceptable user experience, mobile devices typically budget only
500-1000 mW for sustained ML training, effectively limiting practical
training sessions to 10-100 minutes daily under normal usage patterns.
This severe power constraint fundamentally shifts the design priority
from maximizing computational throughput to optimizing power efficiency,
requiring careful co-optimization of algorithms and hardware utilization
patterns.

The thermal management challenges extend far beyond simple power limits,
creating complex dynamic constraints that vary with environmental
conditions and usage patterns. Training workloads generate localized
heat that can trigger protective throttling in specific processor cores
or accelerator units, often in unpredictable ways that depend on ambient
temperature and device design. Modern mobile SoCs implement
sophisticated thermal management systems, including dynamic voltage and
frequency scaling (DVFS)\sidenote{\textbf{Dynamic Voltage and Frequency
Scaling (DVFS)}: Modern mobile processors continuously adjust operating
voltage and clock frequency based on workload and thermal conditions.
During ML training, DVFS can reduce clock speeds by 30-50\% when
temperature exceeds 70°C, directly impacting training throughput.
Effective on-device learning systems monitor thermal state and
proactively reduce batch sizes or training intensity to maintain
consistent performance rather than experiencing sudden throttling
events. }, core migration between efficiency and performance clusters,
and selective shutdown of non-essential processing units. Successfully
deployed on-device learning systems must intimately integrate with these
thermal management frameworks, intelligently scheduling training bursts
during optimal thermal windows and gracefully degrading performance when
thermal limits are approached, rather than simply failing or causing
user-visible performance problems.

\subsubsection{Memory Hierarchy
Optimization}\label{sec-edge-intelligence-memory-hierarchy-optimization-8396}

Complementing the thermal and power challenges, memory hierarchy
constraints create another fundamental bottleneck that shapes on-device
learning system design. As established in the constraint amplification
analysis above, these limitations affect both static model storage and
the dynamic memory requirements during training, often pushing systems
beyond their practical limits.

The device memory hierarchy spans several orders of magnitude across
different device classes, each presenting distinct constraints for
on-device learning. The iPhone 15 Pro provides 8 GB total system memory,
but only approximately 2-4 GB remains available for application
workloads after accounting for operating system requirements and
background processes. Budget Android devices operate with 4 GB total
system memory, leaving just 1-2 GB available for ML workloads after OS
overhead consumes significant resources. IoT embedded systems provide 64
MB-1 GB total memory that must be shared between system tasks and
application data, creating severe constraints for any learning
algorithms. Microcontrollers offer only 256 KB-2 MB SRAM, requiring
extreme optimization and careful memory management that fundamentally
limits the complexity of models that can adapt on such platforms.

The memory expansion during training creates particularly acute
challenges that often determine system feasibility. Standard
backpropagation requires caching intermediate activations for each layer
during the forward pass, which are then reused during gradient
computation in the backward pass, creating substantial memory overhead.
A MobileNetV2 model requiring just 14 MB for inference balloons to 50-70
MB during training, often exceeding the available memory budget on many
mobile devices and making training impossible without aggressive
optimization. This dramatic expansion necessitates sophisticated model
compression techniques that must compound multiplicatively: INT8
quantization provides \(4\times\) memory reduction, structured pruning
achieves \(10\times\) parameter reduction, and knowledge distillation
enables \(5\times\) model size reduction while maintaining accuracy
within 2-5\% of the original model. These techniques must be carefully
combined to achieve the aggressive compression ratios required for
practical deployment.

Given these memory constraints, cache optimization becomes absolutely
critical for achieving acceptable performance with constrained memory
pools. Modern mobile SoCs feature complex memory hierarchies with L1
cache (32-64 KB), L2 cache (1-8 MB), and system memory (4-16 GB) that
exhibit 10-100\(\times\) latency differences between levels, creating
severe performance cliffs when working sets exceed cache capacity.
Training workloads that exceed cache capacity face dramatic performance
degradation due to memory bandwidth bottlenecks that can slow training
by orders of magnitude. Successful on-device learning systems must
carefully design data access patterns to maximize cache hit rates, often
requiring specialized memory layouts that group related parameters for
spatial locality, carefully sized mini-batches that fit entirely within
cache constraints, and sophisticated gradient accumulation strategies
that minimize expensive memory bus traffic.

The memory bandwidth limitations become particularly acute during
training. While inference workloads primarily read model weights
sequentially, training requires bidirectional data flow for gradient
computation and weight updates. This increased memory traffic can
saturate the memory subsystem, creating bottlenecks that limit training
throughput regardless of computational capacity. Advanced
implementations employ techniques such as gradient
checkpointing\sidenote{\textbf{Gradient Checkpointing}: A memory
optimization technique that trades computation for memory by recomputing
intermediate activations during the backward pass instead of storing
them (\citeproc{ref-chen2016training}{Chen et al. 2016}). This can
reduce memory requirements by 50-80\% at the cost of 20-30\% additional
computation. Particularly valuable for on-device training where memory
is more constrained than compute capacity, enabling training of larger
models within fixed memory budgets. } to trade computation for memory,
and mixed-precision training to reduce bandwidth requirements while
maintaining numerical stability.

\subsubsection{Mobile AI Accelerator
Optimization}\label{sec-edge-intelligence-mobile-ai-accelerator-optimization-67fe}

Different mobile platforms provide distinct acceleration capabilities
that determine not only achievable model complexity but also feasible
learning paradigms. The architectural differences between these
accelerators fundamentally shape the design space for on-device training
algorithms, influencing everything from numerical precision choices to
gradient computation strategies.

Current generation mobile accelerators demonstrate remarkable diversity
in their capabilities and optimization focus. Apple's Neural Engine in
the A17 Pro delivers 35 TOPS peak performance specialized for 8-bit and
16-bit operations, optimized primarily for CoreML inference patterns
with limited training support, making it ideal for inference-heavy
adaptation techniques. Qualcomm's Hexagon DSP in the Snapdragon 8 Gen 3
achieves 45 TOPS with flexible precision support and programmable vector
units, enabling mixed-precision training workflows that can adapt
precision dynamically based on training phase and memory constraints.
Google's Tensor TPU in the Pixel 8 is optimized specifically for
TensorFlow Lite operations with strong INT8 performance and tight
integration with federated learning frameworks, reflecting Google's
strategic focus on distributed learning scenarios. The energy efficiency
comparison reveals why dedicated neural processing units are essential:
NPUs achieve 1-5 TOPS per watt versus general-purpose CPUs at just
0.1-0.2 TOPS per watt, representing a 5-50\(\times\) efficiency
advantage that makes the difference between feasible and infeasible
on-device training.

These accelerators determine not just raw performance but feasible
learning paradigms and algorithmic approaches. Apple's Neural Engine
excels at fixed-precision inference workloads but provides limited
support for the dynamic precision requirements of gradient computation,
making it more suitable for inference-heavy adaptation techniques like
few-shot learning. Qualcomm's Hexagon DSP offers greater training
flexibility through its programmable vector units and support for
mixed-precision arithmetic, enabling more sophisticated on-device
training including full backpropagation on compact models. Google's
Tensor TPU integrates tightly with federated learning frameworks and
provides optimized communication primitives for distributed training
scenarios.

The architectural implications extend beyond computational throughput to
memory access patterns and data flow optimization. Training workloads
exhibit fundamentally different characteristics than inference: gradient
computation requires significantly higher memory bandwidth due to the
amplification effects discussed above, weight updates create write-heavy
access patterns, and optimizer state management demands additional
memory allocation. Modern edge accelerators are beginning to address
these challenges through specialized hardware features including
adaptive precision datapaths that dynamically switch between INT4 for
forward passes and FP16 for gradient computation, sparse computation
units that accelerate selective parameter updates by skipping zero
gradients, and near-memory compute architectures that reduce data
movement costs by performing gradient updates directly adjacent to
weight storage.

However, most current edge accelerators remain primarily optimized for
inference workloads, creating a significant hardware-software co-design
opportunity. Future on-device training accelerators will need to
efficiently handle the unique demands of local adaptation, including
support for dynamic precision scaling, efficient gradient accumulation,
and specialized memory hierarchies optimized for the bidirectional data
flow patterns characteristic of training workloads. Architecture
selection influences everything from model quantization strategies and
gradient computation approaches to federated communication protocols and
thermal management policies.

\subsection{Holistic Resource Management
Strategies}\label{sec-edge-intelligence-holistic-resource-management-strategies-9e4b}

The constraint analysis above reveals three fundamental challenge
categories that define the on-device learning design space. Each
constraint category directly drives a corresponding solution pillar,
creating a systematic engineering approach to this complex systems
problem. The constraint-to-solution mapping follows naturally from
understanding how specific limitations necessitate particular technical
responses.

The resource amplification effects, where training increases memory
requirements by 3--10×, computational costs by 2--3×, and energy
consumption proportionally, directly necessitate Model Adaptation
approaches. When traditional training becomes impossible due to resource
constraints, systems must fundamentally reduce the scope of parameter
updates while preserving learning capability.

The information scarcity constraints, including limited local datasets,
non-IID distributions, privacy restrictions on data sharing, and minimal
supervision, directly drive Data Efficiency solutions. When conventional
data-hungry approaches fail due to insufficient local information,
systems must extract maximum learning signal from minimal examples.

The coordination challenges, such as device heterogeneity, intermittent
connectivity, distributed validation complexity, and scalability
requirements, directly motivate Federated Coordination mechanisms. When
isolated on-device learning limits collective intelligence, systems must
enable privacy-preserving collaboration across device populations.

Table~\ref{tbl-constraint-solution-mapping} reveals how this
constraint-to-solution mapping creates a systematic engineering
framework: Model Adaptation addresses memory and compute limits through
selective parameter updates, Data Efficiency maximizes learning from
scarce private samples, and Federated Coordination enables
privacy-preserving collaboration. Rather than viewing these as
independent techniques, successful systems orchestrate all three
approaches to create coherent adaptive systems that operate effectively
within edge constraints.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3333}}@{}}
\caption{\textbf{Constraint-Solution Mapping}: The three fundamental
constraint categories in on-device learning each drive corresponding
solution approaches through direct
necessity.}\label{tbl-constraint-solution-mapping}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Constraint Category}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Key Challenges}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Solution Approach}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Constraint Category}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Key Challenges}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Solution Approach}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Resource Amplification} & •~Training workloads (3-10× memory)
•~Memory~limitations •~Power~constraints & \textbf{Model Adaptation}
•~Parameter-efficient updates •~Selective layer fine-tuning
•~Low-rank~adaptations \\
\textbf{Information Scarcity} & •~Limited local datasets
•~Non-IID~distributions & \textbf{Data Efficiency} \\
& •~Privacy~restrictions & •~Few-shot learning \\
& & •~Meta-learning •~Transfer learning \\
\end{longtable}

The subsequent sections examine each solution pillar systematically,
building on model optimization principles and distributed systems
frameworks. Each pillar provides essential capabilities that the others
cannot deliver alone, but their integration creates systems capable of
meaningful adaptation within the severe constraints of edge deployment
environments.

\section{Model
Adaptation}\label{sec-edge-intelligence-model-adaptation-6a82}

The computational and memory constraints outlined above create seemingly
impossible challenges for model training. Yet these constraints also
reveal solution pathways when approached systematically. Model
adaptation, the first pillar in our three-pillar framework
(Table~\ref{tbl-constraint-solution-mapping}), addresses resource
amplification by reducing the scope of parameter updates. Where full
training requires 3-5x inference memory, strategic adaptation can
achieve meaningful personalization within inference-comparable budgets.
This pillar answers a fundamental question: which parameters must change
to capture local patterns, and which can remain frozen?

The engineering challenge centers on navigating a fundamental trade-off
space: adaptation expressivity versus resource consumption. At one
extreme, updating all parameters provides maximum flexibility but
exceeds edge device capabilities. At the other extreme, no adaptation
preserves resources but fails to capture user-specific patterns.
Effective on-device learning systems must operate in the middle ground,
selecting adaptation strategies based on three key engineering criteria.

First, available memory, compute, and energy determine which adaptation
approaches are feasible. A smartwatch with 1 MB RAM requires
fundamentally different strategies than a smartphone with 8 GB. Second,
the degree of user-specific variation drives adaptation complexity
needs. Simple preference learning may require only bias updates, while
complex domain shifts demand more sophisticated approaches. Third,
adaptation techniques must integrate with existing inference pipelines,
federated coordination protocols, and operational monitoring systems for
model deployment and lifecycle management.

This systems perspective guides the selection and combination of
techniques starting with lightweight approaches
(Section~\ref{sec-edge-intelligence-weight-freezing-3407}) and
progressing to more sophisticated methods
(Section~\ref{sec-edge-intelligence-sparse-updates-879b}), moving from
lightweight bias-only approaches through progressively more expressive
but resource-intensive methods. Each technique represents a different
point in the engineering trade-off space rather than an isolated
algorithmic solution.

Building on compression techniques including quantization, pruning, and
knowledge distillation, on-device learning transforms compression from a
one-time optimization into an ongoing constraint. The central insight
driving all model adaptation approaches is that complete model
retraining is neither necessary nor feasible for on-device learning
scenarios. Instead, systems can strategically leverage pre-trained
representations and adapt only the minimal parameter subset required to
capture local variations, operating on the principle: preserve what
works globally, adapt what matters locally.

This section systematically examines three complementary adaptation
strategies, each specifically designed to address different device
constraint profiles and application requirements. Weight freezing
addresses severe memory limitations by updating only bias terms or final
layers, enabling learning even on severely constrained microcontrollers
that would otherwise lack the resources for any form of adaptation.
Structured updates use low-rank and residual adaptations to balance
model expressiveness with computational efficiency, enabling more
sophisticated learning than bias-only approaches while maintaining
manageable resource requirements. Sparse updates enable selective
parameter modification based on gradient importance or layer
criticality, concentrating learning capacity on the most impactful
parameters while leaving less important weights frozen.

These approaches build on established architectural principles while
strategically applying optimization strategies to the unique challenges
of edge deployment. Each technique represents a carefully considered
point in the fundamental accuracy-efficiency tradeoff space, enabling
practical deployment across the full spectrum of edge hardware
capabilities, from ultra-constrained microcontrollers to capable mobile
processors.

\subsection{Weight
Freezing}\label{sec-edge-intelligence-weight-freezing-3407}

The most straightforward approach to making on-device learning feasible
is to dramatically reduce the number of parameters that require
updating. One of the simplest and most effective strategies for
achieving this reduction is to freeze the majority of a model's
parameters and adapt only a carefully chosen minimal subset. The most
widely used approach within this family is bias-only adaptation, in
which all weights are held fixed and only the bias terms (typically
scalar offsets applied after linear or convolutional layers) are updated
during training. This simple constraint creates significant benefits: it
reduces the number of trainable parameters (often by
100-1000\(\times\)), simplifies memory management during
backpropagation, and helps mitigate overfitting when training data is
sparse or noisy.

Consider a standard neural network layer: \[
y = W x + b
\] where \(W \in \mathbb{R}^{m \times n}\) is the weight matrix,
\(b \in \mathbb{R}^m\) is the bias vector, and \(x \in \mathbb{R}^n\) is
the input. In full training, gradients are computed for both \(W\) and
\(b\). In bias-only adaptation, we constrain: \[
\frac{\partial \mathcal{L}}{\partial W} = 0, \quad \frac{\partial \mathcal{L}}{\partial b} \neq 0
\] so that only the bias is updated via gradient descent: \[
b \leftarrow b - \eta \frac{\partial \mathcal{L}}{\partial b}
\]

This reduces the number of stored gradients and optimizer states,
enabling training to proceed under memory-constrained conditions. On
embedded devices that lack floating-point units, this reduction enables
on-device learning.

Listing~\ref{lst-bias-adaptation} demonstrates the PyTorch
implementation of this bias-only approach, freezing all convolutional
and fully-connected weights while allowing only bias terms to adapt to
local data.

\begin{codelisting}

\caption{\label{lst-bias-adaptation}\textbf{Bias-Only Adaptation}:
Freezes model parameters except for biases to reduce memory usage and
allow on-device learning.}

\centering{

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Freeze all parameters}
\ControlFlowTok{for}\NormalTok{ name, param }\KeywordTok{in}\NormalTok{ model.named\_parameters():}
\NormalTok{    param.requires\_grad }\OperatorTok{=} \VariableTok{False}

\CommentTok{\# Enable gradients for bias parameters only}
\ControlFlowTok{for}\NormalTok{ name, param }\KeywordTok{in}\NormalTok{ model.named\_parameters():}
    \ControlFlowTok{if} \StringTok{"bias"} \KeywordTok{in}\NormalTok{ name:}
\NormalTok{        param.requires\_grad }\OperatorTok{=} \VariableTok{True}
\end{Highlighting}
\end{Shaded}

}

\end{codelisting}%

This pattern ensures that only bias terms participate in the backward
pass and optimizer update, simplifying the training process while
maintaining adaptation capability. This is valuable when adapting
pretrained models to user-specific or device-local data where the core
representations remain relevant but require calibration.

The practical effectiveness of this approach is demonstrated by TinyTL
(\citeproc{ref-cai2020tinytl}{Cai et al. 2020}), a framework explicitly
designed to enable efficient adaptation of deep neural networks on
microcontrollers and other severely memory-limited platforms. Rather
than updating all network parameters during training (impossible on such
constrained devices), TinyTL strategically freezes both the
convolutional weights and the batch normalization statistics, training
only the bias terms and, in some cases, lightweight residual components.
This architectural constraint creates a profound shift in memory
requirements during backpropagation, since the largest memory consumers
(intermediate activations) no longer need to be stored for gradient
computation across frozen layers.

Figure~\ref{fig-tinytl-architecture} visualizes this architectural
impact by contrasting standard training with the TinyTL approach. Where
conventional backpropagation requires storing activations across all
layers, TinyTL freezes backbone weights and batch normalization
statistics, training only bias terms and lightweight residual
components. This eliminates the need to store activations for frozen
layers, making adaptation possible within the severe memory constraints
established earlier.

\begin{figure}[htb]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/99b44e9208817959b50fa5c0480a6f8533b48508.pdf}}

}

\caption{\label{fig-tinytl-architecture}TinyTL reduces on-device
training costs by freezing convolutional weights and batch
normalization, updating only bias terms and lightweight residual
connections to minimize memory usage during backpropagation. This
approach allows deployment of deep neural networks on
resource-constrained edge devices with limited SRAM, facilitating
efficient model adaptation without requiring full parameter updates.}

\end{figure}%

In contrast, the TinyTL architecture freezes all weights and updates
only the bias terms inserted after convolutional layers. These bias
modules are lightweight and require minimal memory, enabling efficient
training with a drastically reduced memory footprint. The frozen
convolutional layers act as a fixed feature extractor, and only the
trainable bias components are involved in adaptation. By avoiding
storage of full activation maps and limiting the number of updated
parameters, TinyTL allows on-device training under severe resource
constraints.

Because the base model remains unchanged, TinyTL assumes that the
pretrained features are sufficiently expressive for downstream tasks.
The bias terms allow for minor but meaningful shifts in model behavior,
particularly for personalization tasks. When domain shift is more
significant, TinyTL can optionally incorporate small residual adapters
to improve expressivity, all while preserving the system's tight memory
and energy profile.

These design choices allow TinyTL to reduce training memory usage by
10×. For instance, adapting a MobileNetV2 model using TinyTL can reduce
the number of updated parameters from over 3 million to fewer than
50,000\sidenote{\textbf{TinyTL Memory Breakthrough}: TinyTL's 60×
parameter reduction (3.4 M to 50 K) translates to dramatic memory
savings. In FP32, MobileNetV2 requires approximately 12 MB for weights
plus approximately 8 MB for activation caching during training,
exceeding most microcontroller capabilities. TinyTL reduces this to
approximately 200 KB weights plus approximately 400 KB activations,
fitting comfortably within a 1 MB memory budget. Real deployments on
STM32H7 achieve 85\% of full fine-tuning accuracy while using 15× less
memory and completing updates in approximately 30 seconds versus 8
minutes for full training. }. Combined with quantization, this allows
local adaptation on devices with only a few hundred kilobytes of memory,
making on-device learning truly feasible in constrained environments.

\subsection{Structured Parameter
Updates}\label{sec-edge-intelligence-structured-parameter-updates-f1a1}

While weight freezing provides computational efficiency and clear memory
bounds, it severely limits model expressivity by constraining adaptation
to a small parameter subset. When bias-only updates prove insufficient
for capturing complex domain shifts or user-specific patterns, residual
and low-rank techniques provide increased adaptation capability while
maintaining computational tractability. These approaches represent a
middle ground between the extreme efficiency of weight freezing and the
full expressivity of unrestricted fine-tuning.

Rather than modifying existing parameters, these methods extend frozen
models by adding trainable components, such as residual adaptation
modules (\citeproc{ref-houlsby2019parameter}{Houlsby et al. 2019}) or
low-rank parameterizations (\citeproc{ref-hu2021lora}{Hu et al. 2021}),
that provide controlled increases in model capacity. This architectural
approach enables more sophisticated adaptation while preserving the
computational benefits that make on-device learning feasible.

These methods extend a frozen model by adding trainable layers, which
are typically small and computationally inexpensive, that allow the
network to respond to new data. The main body of the network remains
fixed, while only the added components are optimized. This modularity
makes the approach well-suited for on-device adaptation in constrained
settings, where small updates must deliver meaningful changes.

\subsubsection{Adapter-Based
Adaptation}\label{sec-edge-intelligence-adapterbased-adaptation-6a9a}

A common implementation involves inserting adapters, which are small
residual bottleneck layers, between existing layers in a pretrained
model. Consider a hidden representation \(h\) passed between layers. A
residual adapter introduces a transformation: \[
h' = h + A(h)
\] where \(A(\cdot)\) is a trainable function, typically composed of two
linear layers with a nonlinearity: \[
A(h) = W_2 \, \sigma(W_1 h)
\] with \(W_1 \in \mathbb{R}^{r \times d}\) and
\(W_2 \in \mathbb{R}^{d \times r}\), where \(r \ll d\). This bottleneck
design ensures that only a small number of parameters are introduced per
layer.

The adapters act as learnable perturbations on top of a frozen backbone.
Because they are small and sparsely applied, they add negligible memory
overhead, yet they allow the model to shift its predictions in response
to new inputs.

\subsubsection{Low-Rank
Techniques}\label{sec-edge-intelligence-lowrank-techniques-4570}

Another efficient strategy is to constrain weight updates themselves to
a low-rank structure. Rather than updating a full matrix \(W\), we
approximate the update as: \[
\Delta W \approx U V^\top
\] where \(U \in \mathbb{R}^{m \times r}\) and
\(V \in \mathbb{R}^{n \times r}\), with \(r \ll \min(m,n)\). This
reduces the number of trainable parameters from \(mn\) to \(r(m + n)\).

The mathematical intuition behind this decomposition connects to
fundamental linear algebra principles: any matrix can be expressed as a
sum of rank-one matrices through singular value decomposition. By
constraining our updates to low rank (typically \(r = 4\) to \(16\)), we
capture the most significant modes of variation while reducing
parameters. For a typical transformer layer with dimensions
\(768 \times 768\), full fine-tuning requires updating 589,824
parameters. With rank-4 decomposition, we update only
\(768 \times 4 \times 2 = 6,144\) parameters, a 96\% reduction, while
empirically retaining 85-90\% of the adaptation quality.

During adaptation, the new weight is computed as: \[
W_{\text{adapted}} = W_{\text{frozen}} + U V^\top
\]

This formulation is commonly used in LoRA (Low-Rank
Adaptation)\sidenote{\textbf{LoRA (Low-Rank Adaptation)}: Introduced by
Microsoft in 2021, LoRA enables efficient fine-tuning by learning
low-rank decomposition matrices rather than updating full weight
matrices. For a weight matrix W, LoRA learns rank-r matrices A and B
such that the update is BA (where r \textless\textless{} original
dimensions). This reduces trainable parameters by 100--10,000× while
maintaining 90--95\% adaptation quality. LoRA has become the standard
for parameter-efficient fine-tuning in large language models. }
techniques, originally developed for transformer models
(\citeproc{ref-hu2021lora}{Hu et al. 2021}) but broadly applicable
across architectures. From a systems engineering perspective, LoRA
addresses critical connectivity and resource trade-offs in on-device
learning deployment.

Consider a mobile deployment where a 7B parameter language model
requires 14 GB for full fine-tuning, which is impossible on typical
smartphones with 6--8 GB total memory. LoRA with rank-16 reduces this to
approximately 100 MB of trainable parameters (0.7\% of original),
enabling local adaptation within mobile memory constraints.

LoRA's efficiency becomes critical in intermittent connectivity
scenarios. A full model update over cellular networks would require 14
GB download (potential cost \$140+ in mobile data charges), while LoRA
adapter updates are typically 10--50 MB. For periodic federated
coordination, devices can synchronize LoRA adapters in under 30 seconds
on 3G networks, compared to hours for full model transfers. This enables
practical federated learning even with poor network conditions.

Systems typically deploy different LoRA configurations based on device
capabilities. Flagship phones use rank-32 adapters for higher
expressivity, mid-range devices use rank-16 for balanced performance,
and budget devices use rank-8 to stay within 2 GB memory limits.
Low-rank updates can be implemented efficiently on edge devices,
particularly when \(U\) and \(V\) are small and fixed-point
representations are supported. Listing~\ref{lst-lowrank-adapter}
implements this approach, showing how two small matrices \(A\) and \(B\)
with intermediate rank \(r\) replace the full weight update while
preserving adaptation expressiveness.

\begin{codelisting}

\caption{\label{lst-lowrank-adapter}\textbf{Low-Rank Adapter}: The code
implements a low-rank adapter module by approximating weight updates
using matrices (u) and (v), reducing parameter count while enabling
efficient model adaptation on edge devices.}

\centering{

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{class}\NormalTok{ Adapter(nn.Module):}
    \KeywordTok{def} \FunctionTok{\_\_init\_\_}\NormalTok{(}\VariableTok{self}\NormalTok{, dim, bottleneck\_dim):}
        \BuiltInTok{super}\NormalTok{().}\FunctionTok{\_\_init\_\_}\NormalTok{()}
        \CommentTok{\# Project from full dimension to bottleneck (e.g., 768 {-}\textgreater{} 16)}
        \VariableTok{self}\NormalTok{.down }\OperatorTok{=}\NormalTok{ nn.Linear(}
\NormalTok{            dim, bottleneck\_dim}
\NormalTok{        )  }\CommentTok{\# W1: learns compression}
        \CommentTok{\# Project back to original dimension (e.g., 16 {-}\textgreater{} 768)}
        \VariableTok{self}\NormalTok{.up }\OperatorTok{=}\NormalTok{ nn.Linear(}
\NormalTok{            bottleneck\_dim, dim}
\NormalTok{        )  }\CommentTok{\# W2: learns expansion}
        \VariableTok{self}\NormalTok{.activation }\OperatorTok{=}\NormalTok{ nn.ReLU()}

    \KeywordTok{def}\NormalTok{ forward(}\VariableTok{self}\NormalTok{, x):}
        \CommentTok{\# Residual connection: original + low{-}rank adaptation}
        \CommentTok{\# Only adapter params trained; base model frozen}
        \ControlFlowTok{return}\NormalTok{ x }\OperatorTok{+} \VariableTok{self}\NormalTok{.up(}\VariableTok{self}\NormalTok{.activation(}\VariableTok{self}\NormalTok{.down(x)))}
\end{Highlighting}
\end{Shaded}

}

\end{codelisting}%

This adapter adds a small residual transformation to a frozen layer.
When inserted into a larger model, only the adapter parameters are
trained.

\subsubsection{Edge
Personalization}\label{sec-edge-intelligence-edge-personalization-a511}

Adapters are useful when a global model is deployed to many devices and
must adapt to device-specific input distributions. In smartphone camera
pipelines, environmental lighting, user preferences, or lens distortion
vary between users (\citeproc{ref-rebuffi2017learning}{Rebuffi, Bilen,
and Vedaldi 2017}). A shared model can be frozen and fine-tuned
per-device using a few residual modules, allowing lightweight
personalization without risking catastrophic forgetting. In voice-based
systems, adapter modules have been shown to reduce word error rates in
personalized speech recognition without retraining the full acoustic
model. They also allow easy rollback or switching between user-specific
versions.

\subsubsection{Performance vs.~Resource
Trade-offs}\label{sec-edge-intelligence-performance-vs-resource-tradeoffs-2be2}

Residual and low-rank updates strike a balance between expressivity and
efficiency. Compared to bias-only learning, they can model more
substantial deviations from the pretrained task. However, they require
more memory and compute for training and inference.

When considering residual and low-rank updates for on-device learning,
several important tradeoffs emerge. First, these methods consistently
demonstrate superior adaptation quality compared to bias-only
approaches, particularly when deployed in scenarios involving
significant distribution shifts from the original training data
(\citeproc{ref-quinonero2009dataset}{Quiñonero-Candela et al. 2008}).
This improved adaptability stems from their increased parameter capacity
and ability to learn more complex transformations.

This enhanced adaptability comes at a cost. The introduction of
additional layers or parameters inevitably increases both memory
requirements and computational latency during forward and backward
passes. While these increases are modest compared to full model
training, they must be considered when deploying to resource-constrained
devices.

Implementing these adaptation techniques requires system-level support
for dynamic computation graphs and the ability to selectively inject
trainable parameters. Not all deployment environments or inference
engines support such capabilities out of the box.

Residual adaptation techniques have proven valuable in mobile and edge
computing scenarios where devices have sufficient computational
resources. Modern smartphones and tablets can accommodate these
adaptations while maintaining acceptable performance characteristics.
This makes residual adaptation a practical choice for applications
requiring personalization without the overhead of full model retraining.

\subsection{Sparse
Updates}\label{sec-edge-intelligence-sparse-updates-879b}

As we progress from bias-only updates through low-rank adaptations to
more sophisticated techniques, sparse updates represent the most
advanced approach in our model adaptation hierarchy. While the previous
techniques add new parameters or restrict updates to specific subsets,
sparse updates dynamically identify which existing parameters provide
the greatest adaptation benefit for each specific task or user. This
approach maximizes adaptation expressivity while maintaining the
computational efficiency essential for edge deployment.

Even when adaptation is restricted to a small number of parameters
through the techniques discussed above, training remains
resource-intensive on constrained devices. Sparse updates address this
challenge by selectively updating only task-relevant subsets of model
parameters, rather than modifying entire networks or introducing new
modules. This approach, known as task-adaptive sparse updating
(\citeproc{ref-zhang2020efficient}{Zhang, Song, and Tao 2020}),
represents the culmination of principled parameter selection strategies.

The key insight is that not all layers of a deep model contribute
equally to performance gains on a new task or dataset. If we can
identify a \emph{minimal subset of parameters} that are most impactful
for adaptation, we can train only those, reducing memory and compute
costs while still achieving meaningful personalization.

\subsubsection{Sparse Update
Design}\label{sec-edge-intelligence-sparse-update-design-ee7c}

Let a neural network be defined by parameters
\(\theta = \{\theta_1, \theta_2, \ldots, \theta_L\}\) across \(L\)
layers. In standard fine-tuning, we compute gradients and perform
updates on all parameters: \[
\theta_i \leftarrow \theta_i - \eta \frac{\partial \mathcal{L}}{\partial \theta_i}, \quad \text{for } i = 1, \ldots, L
\]

In task-adaptive sparse updates, we select a small subset
\(\mathcal{S} \subset \{1, \ldots, L\}\) such that only parameters in
\(\mathcal{S}\) are updated: \[
\theta_i \leftarrow
\begin{cases}
\theta_i - \eta \frac{\partial \mathcal{L}}{\partial \theta_i}, & \text{if } i \in \mathcal{S} \\
\theta_i, & \text{otherwise}
\end{cases}
\]

The challenge lies in selecting the optimal subset \(\mathcal{S}\) given
memory and compute constraints.

\subsubsection{Layer
Selection}\label{sec-edge-intelligence-layer-selection-ab3c}

A principled strategy for selecting \(\mathcal{S}\) is to use
contribution analysis, an empirical method that estimates how much each
layer contributes to downstream performance improvement. For example,
one can measure the marginal gain from updating each layer
independently:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Freeze the entire model.
\item
  Unfreeze one candidate layer.
\item
  Finetune briefly and evaluate improvement in validation accuracy.
\item
  Rank layers by performance gain per unit cost (e.g., per KB of
  trainable memory).
\end{enumerate}

This layer-wise profiling yields a ranking from which \(\mathcal{S}\)
can be constructed subject to a memory budget.

A concrete example is TinyTrain, a method designed to allow rapid
adaptation on-device (\citeproc{ref-deng2022tinytrain}{C. Deng, Zhang,
and Wu 2022}). TinyTrain pretrains a model along with meta-gradients
that capture which layers are most sensitive to new tasks. At runtime,
the system dynamically selects layers to update based on task
characteristics and available resources.

\subsubsection{Selective Layer Update
Implementation}\label{sec-edge-intelligence-selective-layer-update-implementation-eed2}

Listing~\ref{lst-selective-update} extends this pattern to
profiling-driven layer selection, demonstrating how hardware profiling
determines which layers to update based on contribution scores and
memory constraints.

\begin{codelisting}

\caption{\label{lst-selective-update}\textbf{Selective Layer Updating}:
This technique allows fine-tuning specific layers of a pre-trained model
while keeping others frozen, optimizing computational resources for
targeted improvements. \emph{Source: PyTorch Documentation}}

\centering{

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Selective layer update based on contribution analysis}
\CommentTok{\# Layers selected via profiling: conv2 and fc have highest}
\CommentTok{\# accuracy{-}per{-}KB impact for this task}

\ControlFlowTok{for}\NormalTok{ name, param }\KeywordTok{in}\NormalTok{ model.named\_parameters():}
    \ControlFlowTok{if} \StringTok{"conv2"} \KeywordTok{in}\NormalTok{ name }\KeywordTok{or} \StringTok{"fc"} \KeywordTok{in}\NormalTok{ name:}
\NormalTok{        param.requires\_grad }\OperatorTok{=} \VariableTok{True}  \CommentTok{\# Train high{-}impact layers}
    \ControlFlowTok{else}\NormalTok{:}
\NormalTok{        param.requires\_grad }\OperatorTok{=} \VariableTok{False}  \CommentTok{\# Freeze low{-}impact layers}

\CommentTok{\# Result: \textasciitilde{}10\% of params trainable, \textasciitilde{}60\% of adaptation quality}
\CommentTok{\# Memory savings: gradient storage only for selected layers}
\end{Highlighting}
\end{Shaded}

}

\end{codelisting}%

\subsubsection{TinyTrain
Personalization}\label{sec-edge-intelligence-tinytrain-personalization-66f2}

Consider a scenario where a user wears an augmented reality headset that
performs real-time object recognition. As lighting and environments
shift, the system must adapt to maintain accuracy, but training must
occur during brief idle periods or while charging.

TinyTrain allows this by using meta-training during offline preparation:
the model learns not only to perform the task, but also which parameters
are most important to adapt. Then, at deployment, the device performs
task-adaptive sparse updates, modifying only a few layers that are most
relevant for its current environment. This keeps adaptation fast,
energy-efficient, and memory-aware.

\subsubsection{Adaptation Strategy
Trade-offs}\label{sec-edge-intelligence-adaptation-strategy-tradeoffs-cb23}

Task-adaptive sparse updates introduce several important system-level
considerations that must be carefully balanced. First, the overhead of
contribution analysis, although primarily incurred during pretraining or
initial profiling, represents a non-trivial computational cost. This
overhead is typically acceptable since it occurs offline, but it must be
factored into the overall system design and deployment pipeline.

Second, the stability of the adaptation process becomes important when
working with sparse updates. If too few parameters are selected for
updating, the model may underfit the target distribution, failing to
capture important local variations. This suggests the need for careful
validation of the selected parameter subset before deployment,
potentially incorporating minimum thresholds for adaptation capacity.

Third, the selection of updatable parameters must account for
hardware-specific characteristics of the target platform. Beyond just
considering gradient magnitudes, the system must evaluate the actual
execution cost of updating specific layers on the deployed hardware.
Some parameters might show high contribution scores but prove expensive
to update on certain architectures, requiring a more nuanced selection
strategy that balances statistical utility with runtime efficiency.

Despite these tradeoffs, task-adaptive sparse updates provide a powerful
mechanism to scale adaptation to diverse deployment contexts, from
microcontrollers to mobile devices (\citeproc{ref-diao2023sparse}{Diao,
Ding, and Tarokh 2023}).

\subsubsection{Adaptation Strategy
Comparison}\label{sec-edge-intelligence-adaptation-strategy-comparison-1faf}

Each adaptation strategy for on-device learning offers a distinct
balance between expressivity, resource efficiency, and implementation
complexity. Understanding these tradeoffs is important when designing
systems for diverse deployment targets, from ultra-low-power
microcontrollers to feature-rich mobile processors.

Bias-only adaptation is the most lightweight approach, updating only
scalar offsets in each layer while freezing all other parameters. This
reduces memory requirements and computational burden, making it suitable
for devices with tight memory and energy budgets. However, its limited
expressivity means it is best suited to applications where the
pretrained model already captures most of the relevant task features and
only minor local calibration is required.

Residual adaptation, often implemented via adapter modules, introduces a
small number of trainable parameters into the frozen backbone of a
neural network. This allows for greater flexibility than bias-only
updates, while still maintaining control over the adaptation cost.
Because the backbone remains fixed, training can be performed
efficiently and safely under constrained conditions. This method
supports modular personalization across tasks and users, making it a
favorable choice for mobile settings where moderate adaptation capacity
is needed.

Task-adaptive sparse updates offer the greatest potential for
task-specific finetuning by selectively updating only a subset of layers
or parameters based on their contribution to downstream performance.
While this method allows expressive local adaptation, it requires a
mechanism for layer selection, through profiling, contribution analysis,
or meta-training, which introduces additional complexity. Nonetheless,
when deployed carefully, it allows for dynamic tradeoffs between
accuracy and efficiency, particularly in systems that experience large
domain shifts or evolving input conditions.

Table~\ref{tbl-adaptation-strategies} contrasts these three approaches
across memory footprint, storage requirements, and update granularity,
revealing how the optimal choice depends on application domain,
available hardware, latency constraints, and expected distribution
shift:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 10\tabcolsep) * \real{0.1667}}
  >{\raggedright\arraybackslash}p{(\linewidth - 10\tabcolsep) * \real{0.1667}}
  >{\raggedright\arraybackslash}p{(\linewidth - 10\tabcolsep) * \real{0.1667}}
  >{\raggedright\arraybackslash}p{(\linewidth - 10\tabcolsep) * \real{0.1667}}
  >{\raggedright\arraybackslash}p{(\linewidth - 10\tabcolsep) * \real{0.1667}}
  >{\raggedright\arraybackslash}p{(\linewidth - 10\tabcolsep) * \real{0.1667}}@{}}
\caption{\textbf{Adaptation Strategy Trade-Offs}: Table entries
characterize three approaches to model adaptation, bias-only updates,
selective layer updates, and full finetuning, by quantifying their
impact on trainable parameters, memory overhead, expressivity,
suitability for different use cases, and system requirements. These
characteristics reveal the inherent trade-offs between model
flexibility, computational cost, and performance when deploying machine
learning systems in dynamic
environments.}\label{tbl-adaptation-strategies}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Technique}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Trainable Parameters}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Memory Overhead}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Expressivity}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Use Case Suitability}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{System Requirements}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Technique}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Trainable Parameters}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Memory Overhead}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Expressivity}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Use Case Suitability}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{System Requirements}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Bias-Only Updates} & Bias terms only & Minimal & Low & Simple
personalization; low variance & Extreme memory/compute limits \\
\textbf{Residual Adapters} & Adapter modules & Moderate & Moderate to
High & User-specific tuning on mobile & Mobile-class SoCs with runtime
support \\
\textbf{Sparse Layer Updates} & Selective parameter subsets & Variable &
High (task-adaptive) & Real-time adaptation; domain shift & Requires
profiling or meta-training \\
\end{longtable}

\section{Data
Efficiency}\label{sec-edge-intelligence-data-efficiency-c701}

Having established resource-efficient adaptation through model
techniques in the first pillar
(Table~\ref{tbl-constraint-solution-mapping}), we now turn to the second
pillar: maximizing learning signal from severely constrained data. This
pillar addresses information scarcity, the fundamental shift from
data-abundant centralized environments to the information-scarce reality
of edge deployment where each local dataset represents only a narrow
slice of the full distribution.

The systems engineering challenge centers on a critical trade-off: data
collection cost versus adaptation quality. Edge devices face severe data
acquisition constraints that reshape learning system design in ways not
encountered in centralized training. Understanding and navigating these
constraints requires systematic analysis of four interconnected
engineering dimensions.

First, every data point has acquisition costs in terms of user friction,
energy consumption, storage overhead, and privacy risk. A voice
assistant learning from audio samples must balance improvement potential
against battery drain and user comfort with always-on recording. Second,
limited data collection capacity forces systems to choose between broad
coverage and deep examples. A mobile keyboard can collect many shallow
typing patterns or fewer detailed interaction sequences, each strategy
implying different learning approaches. Third, some applications demand
rapid learning from minimal examples (emergency response scenarios),
while others can accumulate data over time (user preference learning).
This temporal dimension drives fundamental architectural choices.
Fourth, data efficiency techniques must integrate with the model
adaptation approaches from
Section~\ref{sec-edge-intelligence-model-adaptation-6a82}, federated
coordination
(Section~\ref{sec-edge-intelligence-federated-learning-6e7e}), and
operational monitoring for model deployment and lifecycle management.

These engineering constraints create a systematic trade-off space where
different data efficiency approaches serve different combinations of
constraints. Rather than choosing a single technique, successful
on-device learning systems typically combine multiple approaches, each
addressing specific aspects of the data scarcity challenge.

This section examines four complementary data efficiency strategies that
address different facets of the data scarcity challenge. Few-shot
learning enables adaptation from minimal labeled examples, allowing
systems to personalize based on just a handful of user-provided samples
rather than requiring extensive training datasets. Streaming updates
accommodate data that arrives incrementally over time, enabling
continuous adaptation as devices encounter new patterns during normal
operation without needing to collect and store large batches. Experience
replay maximizes learning from limited data through intelligent reuse,
replaying important examples multiple times to extract maximum learning
signal from scarce training data. Data compression reduces memory
requirements while preserving learning signals, enabling systems to
maintain replay buffers and training histories within the tight memory
constraints of edge devices.

Each technique addresses different aspects of the data constraint
problem, enabling robust learning even when traditional supervised
learning would fail.

\subsection{Few-Shot Learning and Data
Streaming}\label{sec-edge-intelligence-fewshot-learning-data-streaming-566e}

In conventional machine learning workflows, effective training typically
requires large labeled datasets, carefully curated and preprocessed to
ensure sufficient diversity and balance. On-device learning, by
contrast, must often proceed from only a handful of local examples,
collected passively through user interaction or ambient sensing, and
rarely labeled in a supervised fashion. These constraints motivate two
complementary adaptation strategies: few-shot learning, in which models
generalize from a small, static set of examples, and streaming
adaptation, where updates occur continuously as data arrives.

Few-shot adaptation is particularly relevant when the device observes a
small number of labeled or weakly labeled instances for a new task or
user condition (\citeproc{ref-wang2020generalizing}{Y. Wang et al.
2020}). In such settings, it is often infeasible to perform full
finetuning of all model parameters without overfitting. Instead, methods
such as bias-only updates, adapter modules, or prototype-based
classification are employed to make use of limited data while minimizing
capacity for memorization. Let \(D = \{(x_i, y_i)\}_{i=1}^K\) denote a
\(K\)-shot dataset of labeled examples collected on-device. The goal is
to update the model parameters \(\theta\) to improve task performance
under constraints such as:

\begin{itemize}
\tightlist
\item
  Limited number of gradient steps: \(T \ll 100\)
\item
  Constrained memory footprint:
  \(\|\theta_{\text{updated}}\| \ll \|\theta\|\)
\item
  Preservation of prior task knowledge (to avoid catastrophic
  forgetting)
\end{itemize}

Keyword spotting (KWS) systems offer a concrete example of few-shot
adaptation in a real-world, on-device deployment
(\citeproc{ref-warden2018speech}{Warden 2018}). These models are used to
detect fixed phrases, including phrases like ``Hey
Siri''\sidenote{\textbf{``Hey Siri'' Technical Reality}: Apple's ``Hey
Siri'' system operates under extreme constraints, detection must
complete within 100 ms to feel responsive, while consuming less than 1
mW power when listening continuously. The always-on processor monitors
audio using a 192 KB model running at \textasciitilde0.5 TOPS. False
positive rate must be under 0.001\% of audio frames processed
(equivalent to \textless0.1 activations per hour, or less than once per
day under typical usage) while maintaining \textgreater95\% true
positive rate across accents, background noise, and speaking styles. The
system processes 16 kHz audio in 200 ms windows, extracting
Mel-frequency features for classification. } or ``OK Google'', with low
latency and high reliability. A typical KWS model consists of a
pretrained acoustic encoder (e.g., a small convolutional or recurrent
network that transforms input audio into an embedding space) followed by
a lightweight classifier. In commercial systems, the encoder is trained
centrally using thousands of hours of labeled speech across multiple
languages and speakers. However, supporting custom wake words (e.g.,
``Hey Jarvis'') or adapting to underrepresented accents and dialects is
often infeasible via centralized training due to data scarcity and
privacy concerns.

Few-shot adaptation solves this problem by finetuning only the output
classifier or a small subset of parameters, including bias terms, using
just a few example utterances collected directly on the device. For
example, a user might provide 5--10 recordings of their custom wake
word. These samples are then used to update the model locally, while the
main encoder remains frozen to preserve generalization and reduce memory
overhead. This allows personalization without requiring additional
labeled data or transmitting private audio to the cloud.

Such an approach is not only computationally efficient, but also aligned
with privacy-preserving design principles. Because only the output layer
is updated, often involving a simple gradient step or prototype
computation, the total memory footprint and runtime compute are
compatible with mobile-class devices or even microcontrollers. This
makes KWS a canonical case study for few-shot learning at the edge,
where the system must operate under tight constraints while delivering
user-specific performance.

Beyond static few-shot learning, many on-device scenarios benefit from
streaming adaptation, where models must learn incrementally as new data
arrives (\citeproc{ref-hayes2020remind}{Hayes et al. 2020}). Streaming
adaptation generalizes this idea to continuous, asynchronous settings
where data arrives incrementally over time. Let
\(\{x_t\}_{t=1}^{\infty}\) represent a stream of observations. In
streaming settings, the model must update itself after observing each
new input, typically without access to prior data, and under bounded
memory and compute. The model update can be written generically as: \[
\theta_{t+1} = \theta_t - \eta_t \nabla \mathcal{L}(x_t; \theta_t)
\] where \(\eta_t\) is the learning rate at time \(t\). This form of
adaptation is sensitive to noise and drift in the input distribution,
and thus often incorporates mechanisms such as learning rate decay,
meta-learned initialization, or update gating to improve stability.

Aside from KWS, practical examples of these strategies abound. In
wearable health devices, a model that classifies physical activities may
begin with a generic classifier and adapt to user-specific motion
patterns using only a few labeled activity segments. In smart
assistants, user voice profiles are fine-tuned over time using ongoing
speech input, even when explicit supervision is unavailable. In such
cases, local feedback, including correction, repetition, or downstream
task success, can serve as implicit signals to guide learning.

Few-shot and streaming adaptation highlight the shift from traditional
training pipelines to data-efficient, real-time learning under
uncertainty. They form a foundation for more advanced memory and replay
strategies, which we turn to next.

\subsection{Experience
Replay}\label{sec-edge-intelligence-experience-replay-737e}

Experience replay addresses the challenge of catastrophic forgetting,
where learning new tasks causes models to forget previously learned
information, in continuous learning scenarios by maintaining a buffer of
representative examples from previous learning episodes. This technique,
originally developed for reinforcement learning
(\citeproc{ref-mnih2015human}{Mnih et al. 2015}), proves essential in
on-device learning where sequential data streams can cause models to
overfit to recent examples. We introduce experience replay here to
address immediate stability needs; the deeper challenge of lifelong
adaptation without forgetting receives comprehensive treatment in the
bio-inspired learning section below
(Section~\ref{sec-edge-intelligence-lifelong-adaptation-without-forgetting-9200}).

Unlike server-side replay strategies that rely on large datasets and
extensive compute, on-device replay must operate with extremely limited
capacity, often with tens or hundreds of samples, and must avoid
interfering with user experience
(\citeproc{ref-rolnick2019experience}{Rolnick et al. 2019}). Buffers may
store only compressed features or distilled summaries, and updates must
occur opportunistically (e.g., during idle cycles or charging). These
system-level constraints reshape how replay is implemented and evaluated
in the context of embedded ML.

Let \(\mathcal{M}\) represent a memory buffer that retains a fixed-size
subset of training examples. At time step \(t\), the model receives a
new data point \((x_t, y_t)\) and appends it to \(\mathcal{M}\). A
replay-based update then samples a batch \(\{(x_i, y_i)\}_{i=1}^{k}\)
from \(\mathcal{M}\) and applies a gradient step: \[
\theta_{t+1} = \theta_t - \eta \nabla_\theta \left[ \frac{1}{k} \sum_{i=1}^{k} \mathcal{L}(x_i, y_i; \theta_t) \right]
\] where \(\theta_t\) are the model parameters, \(\eta\) is the learning
rate, and \(\mathcal{L}\) is the loss function. Over time, this replay
mechanism allows the model to reinforce prior knowledge while
incorporating new information.

A practical on-device implementation might use a ring buffer to store a
small set of compressed feature vectors rather than full input examples.
Listing~\ref{lst-replay-buffer} implements this minimal replay buffer
design, demonstrating how a circular storage mechanism enables efficient
memory management while balancing historical knowledge retention with
new information incorporation.

\begin{codelisting}

\caption{\label{lst-replay-buffer}\textbf{Replay Buffer}: Implements a
circular storage mechanism for efficient memory management in
constrained environments. This approach allows models to efficiently
retain and sample from recent data points, balancing the need to use
historical information while incorporating new insights.}

\centering{

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Ring buffer for experience replay on memory{-}constrained devices}
\CommentTok{\# Stores compressed features (not raw inputs) to minimize footprint}
\KeywordTok{class}\NormalTok{ ReplayBuffer:}
    \KeywordTok{def} \FunctionTok{\_\_init\_\_}\NormalTok{(}\VariableTok{self}\NormalTok{, capacity):}
        \VariableTok{self}\NormalTok{.capacity }\OperatorTok{=}\NormalTok{ capacity  }\CommentTok{\# Fixed size, e.g., 1000 examples}
        \VariableTok{self}\NormalTok{.}\BuiltInTok{buffer} \OperatorTok{=}\NormalTok{ []}
        \VariableTok{self}\NormalTok{.index }\OperatorTok{=} \DecValTok{0}  \CommentTok{\# Circular write pointer}

    \KeywordTok{def}\NormalTok{ store(}\VariableTok{self}\NormalTok{, feature\_vec, label):}
        \ControlFlowTok{if} \BuiltInTok{len}\NormalTok{(}\VariableTok{self}\NormalTok{.}\BuiltInTok{buffer}\NormalTok{) }\OperatorTok{\textless{}} \VariableTok{self}\NormalTok{.capacity:}
            \VariableTok{self}\NormalTok{.}\BuiltInTok{buffer}\NormalTok{.append((feature\_vec, label))  }\CommentTok{\# Fill phase}
        \ControlFlowTok{else}\NormalTok{:}
            \VariableTok{self}\NormalTok{.}\BuiltInTok{buffer}\NormalTok{[}\VariableTok{self}\NormalTok{.index] }\OperatorTok{=}\NormalTok{ (}
\NormalTok{                feature\_vec,}
\NormalTok{                label,}
\NormalTok{            )  }\CommentTok{\# Overwrite oldest}
        \VariableTok{self}\NormalTok{.index }\OperatorTok{=}\NormalTok{ (}\VariableTok{self}\NormalTok{.index }\OperatorTok{+} \DecValTok{1}\NormalTok{) }\OperatorTok{\%} \VariableTok{self}\NormalTok{.capacity  }\CommentTok{\# Wrap around}

    \KeywordTok{def}\NormalTok{ sample(}\VariableTok{self}\NormalTok{, k):}
        \CommentTok{\# Random sampling prevents recency bias during replay}
        \ControlFlowTok{return}\NormalTok{ random.sample(}\VariableTok{self}\NormalTok{.}\BuiltInTok{buffer}\NormalTok{, }\BuiltInTok{min}\NormalTok{(k, }\BuiltInTok{len}\NormalTok{(}\VariableTok{self}\NormalTok{.}\BuiltInTok{buffer}\NormalTok{)))}
\end{Highlighting}
\end{Shaded}

}

\end{codelisting}%

This implementation maintains a fixed-capacity cyclic buffer, storing
compressed representations (e.g., last-layer embeddings) and associated
labels. Such buffers are useful for replaying adaptation updates without
violating memory or energy budgets.

In TinyML applications\sidenote{\textbf{TinyML Market Reality}: The
TinyML market reached \$2.4 billion in 2023 and is projected to grow to
\$23.3 billion by 2030. Over 100 billion microcontrollers ship annually,
but fewer than 1\% currently support on-device learning due to memory
and power constraints. Successful TinyML deployments typically consume
\textless1 mW power, use \textless256 KB memory, and cost under \$1 per
chip. Applications include predictive maintenance (vibration sensors),
health monitoring (heart rate variability), and smart agriculture (soil
moisture prediction). }, experience replay has been applied to problems
such as gesture recognition, where devices must continuously improve
predictions while observing a small number of events per day. Instead of
training directly on the streaming data, the device stores
representative feature vectors from recent gestures and uses them to
finetune classification boundaries periodically. Similarly, in on-device
keyword spotting, replaying past utterances can improve wake-word
detection accuracy without the need to transmit audio data off-device.

While experience replay improves stability in data-sparse or
non-stationary environments, it introduces several tradeoffs. Storing
raw inputs may breach privacy constraints or exceed storage budgets,
especially in vision and audio applications. Replaying from feature
vectors reduces memory usage but may limit the richness of gradients for
upstream layers. Write cycles to persistent flash memory, which are
frequently necessary for long-term storage on embedded devices, can also
raise wear-leveling concerns. These constraints require careful
co-design of memory usage policies, replay frequency, and feature
selection strategies, particularly in continuous deployment scenarios.

\subsection{Data
Compression}\label{sec-edge-intelligence-data-compression-8b40}

In many on-device learning scenarios, the raw training data may be too
large, noisy, or redundant to store and process effectively. This
motivates the use of compressed data representations, where the original
inputs are transformed into lower-dimensional embeddings or compact
encodings that preserve salient information while minimizing memory and
compute costs.

Compressed representations serve two complementary goals. First, they
reduce the footprint of stored data, allowing devices to maintain longer
histories or replay buffers under tight memory budgets
(\citeproc{ref-sanh2019distilbert}{Sanh et al. 2019}). Second, they
simplify the learning task by projecting raw inputs into more structured
feature spaces, often learned via pretraining or meta-learning, in which
efficient adaptation is possible with minimal supervision.

One common approach is to encode data points using a pretrained feature
extractor and discard the original high-dimensional input. For example,
an image \(x_i\) might be passed through a CNN to produce an embedding
vector \(z_i = f(x_i)\), where \(f(\cdot)\) is a fixed feature encoder.
This embedding captures visual structure (e.g., shape, texture, or
spatial layout) in a compact representation, usually ranging from 64 to
512 dimensions, suitable for lightweight downstream adaptation.

Mathematically, training can proceed over compressed samples
\((z_i, y_i)\) using a lightweight decoder or projection head. Let
\(\theta\) represent the trainable parameters of this decoder model,
which is typically a small neural network that maps from compressed
representations to output predictions. As each example is presented, the
model parameters are updated using gradient descent: \[
\theta_{t+1} = \theta_t - \eta \nabla_\theta \mathcal{L}\big(g(z_i; \theta), y_i\big)
\] Here:

\begin{itemize}
\tightlist
\item
  \(z_i\) is the compressed representation of the \(i\)-th input,
\item
  \(y_i\) is the corresponding label or supervision signal,
\item
  \(g(z_i; \theta)\) is the decoder's prediction,
\item
  \(\mathcal{L}\) is the loss function measuring prediction error,
\item
  \(\eta\) is the learning rate, and
\item
  \(\nabla_\theta\) denotes the gradient with respect to the parameters
  \(\theta\).
\end{itemize}

This formulation highlights how only a compact decoder model, which has
the parameter set \(\theta\), needs to be trained, making the learning
process feasible even when memory and compute are limited.

Advanced approaches extend beyond fixed encoders by learning discrete or
sparse dictionaries that represent data using low-rank or sparse
coefficient matrices. A dataset of sensor traces can be factorized as
\(X \approx DC\), where \(D\) is a dictionary of basis patterns and
\(C\) is a block-sparse coefficient matrix indicating which patterns are
active in each example. By updating only a small number of dictionary
atoms or coefficients, the model adapts with minimal overhead.

Compressed representations prove useful in privacy-sensitive settings,
as they allow raw data to be discarded or obfuscated after encoding.
Compression acts as an implicit regularizer, smoothing the learning
process and mitigating overfitting when only a few training examples are
available.

In practice, these strategies have been applied in domains such as
keyword spotting, where raw audio signals are first transformed into
Mel-frequency cepstral coefficients
(MFCCs)\sidenote{\textbf{Mel-Frequency Cepstral Coefficients (MFCCs)}:
Audio features that mimic human auditory perception by applying
mel-scale frequency warping (emphasizing lower frequencies where speech
information concentrates) followed by cepstral analysis. A typical MFCC
extraction converts 16 kHz audio windows into 12-13 coefficients,
reducing a 320-sample window (20 ms) from 640 bytes to \textasciitilde50
bytes while preserving speech intelligibility. Widely used in speech
recognition since the 1980s due to robustness against noise and
computational efficiency. Instead of storing raw audio waveforms, which
are large and computationally expensive to process, devices store and
learn from these compressed feature vectors directly. Similarly, in
low-power computer vision systems, embeddings extracted from lightweight
CNNs are retained and reused for few-shot learning. These examples
illustrate how representation learning and compression serve as
foundational tools for scaling on-device learning to memory- and
bandwidth-constrained environments. }, a compact, lossy representation
of the power spectrum of speech. These MFCC vectors serve as compressed
inputs for downstream models, enabling local adaptation using only a few
kilobytes of memory.

\subsection{Data Efficiency Strategy
Comparison}\label{sec-edge-intelligence-data-efficiency-strategy-comparison-4f92}

The techniques introduced in this section (few-shot learning, experience
replay, and compressed data representations) offer strategies for
adapting models on-device when data is scarce or streaming. They operate
under different assumptions and constraints, and their effectiveness
depends on system-level factors such as memory capacity, data
availability, task structure, and privacy requirements.

Few-shot adaptation excels when a small but informative set of labeled
examples is available, particularly when personalization or rapid
task-specific tuning is required. It minimizes compute and data needs,
but its effectiveness depends on the quality of pretrained
representations and the alignment between the initial model and the
local task.

Experience replay addresses continual adaptation by mitigating
forgetting and improving stability, especially in non-stationary
environments. It allows reuse of past data, but requires memory to store
examples and compute cycles for periodic updates. Replay buffers may
also raise privacy or longevity concerns, especially on devices with
limited storage or flash write cycles.

Compressed data representations reduce the footprint of learning by
transforming raw data into compact feature spaces. This approach
supports longer retention of experience and efficient finetuning,
particularly when only lightweight heads are trainable. Compression can
introduce information loss, and fixed encoders may fail to capture
task-relevant variability if they are not well-aligned with deployment
conditions. Table~\ref{tbl-ondevice-techniques} summarizes the key
tradeoffs across data requirements, memory overhead, and use case
suitability for each technique:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2500}}@{}}
\caption{\textbf{On-Device Learning Trade-Offs}: Few-shot adaptation
balances data efficiency with model personalization by leveraging small
labeled datasets, but requires careful consideration of memory and
compute constraints for deployment on resource-limited devices. The
table summarizes key considerations for selecting appropriate on-device
learning techniques based on application requirements and available
resources.}\label{tbl-ondevice-techniques}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Technique}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Data Requirements}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Memory/Compute Overhead}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Use Case Fit}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Technique}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Data Requirements}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Memory/Compute Overhead}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Use Case Fit}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Few-Shot Adaptation} & Small labeled set (K-shots) & Low &
Personalization, quick on-device finetuning \\
\textbf{Experience Replay} & Streaming data & Moderate (buffer \&
update) & Non-stationary data, stability under drift \\
\textbf{Compressed} & Unlabeled or encoded & Low to Moderate &
Memory-limited devices, \\
\textbf{Representations} & data & & privacy-sensitive contexts \\
\end{longtable}

In practice, these methods are not mutually exclusive. Many real-world
systems combine them to achieve robust, efficient adaptation. For
example, a keyword spotting system may use compressed audio features
(e.g., MFCCs), finetune a few parameters from a small support set, and
maintain a replay buffer of past embeddings for continual refinement.

Together, these strategies embody the core challenge of on-device
learning: achieving reliable model improvement under persistent
constraints on data, compute, and memory.

\section{Federated
Learning}\label{sec-edge-intelligence-federated-learning-6e7e}

The model adaptation and data efficiency techniques examined above
enable each device to learn effectively from its local data. A keyboard
learns your unique vocabulary. A voice assistant masters your accent. A
health monitor calibrates to your resting heart rate. Yet consider the
fundamental limitation: that keyboard insight benefits only you. When
the voice assistant in your home masters your accent, devices in other
homes cannot learn from that adaptation. When millions of devices learn
in isolation, the collective intelligence that made centralized training
so powerful is lost.

This isolation creates the coordination challenge that motivates our
third pillar (Table~\ref{tbl-constraint-solution-mapping}). While the
first two pillars enable individual devices to learn effectively within
resource constraints, they cannot address the broader problem: how do we
share insights across a device population without sharing the private
data that generated those insights?

Consider a voice assistant deployed to 10 million homes. Each device
adapts locally to its user's voice, accent, and vocabulary. Device A
learns that ``data'' is pronounced /ˈdeɪtə/, Device B learns /ˈdætə/.
Device C encounters the rare phrase ``machine learning'' frequently
(tech household), while Device D never sees it (non-tech household).
After six months of local adaptation:

\begin{itemize}
\tightlist
\item
  Each device excels at its specific user's patterns but only its
  patterns
\item
  Rare vocabulary gets learned on some devices, forgotten on others
\item
  Local biases accumulate without correction from broader population
\item
  Valuable insights discovered on one device benefit no others
\end{itemize}

Individual on-device learning, while powerful, faces fundamental
limitations when devices operate in isolation. Each device observes only
a narrow slice of the full data distribution, limiting generalization.
Device capabilities vary dramatically, creating learning imbalances
across the population. Valuable insights learned on one device cannot
benefit others, reducing overall system intelligence. Without
coordination, models may diverge or degrade over time due to local
biases.

Federated learning emerges as the solution to distributed coordination
constraints. It enables privacy-preserving collaboration where devices
contribute to collective intelligence without sharing raw data. Rather
than viewing individual device learning and coordinated learning as
separate paradigms, federated learning represents the natural evolution
when on-device systems deploy at scale. This approach transforms the
constraint of data locality from a limitation into a privacy feature,
allowing systems to learn from population-scale data while keeping
individual information secure. The privacy requirements inherent in this
approach connect directly to the security and privacy principles
examined in \textbf{?@sec-security-privacy}, which become crucial in
production deployments at scale. The following definition formalizes
this paradigm.

\phantomsection\label{callout-definitionux2a-1.4}
\begin{fbxSimple}{callout-definition}{Definition:}{Federated Learning}
\phantomsection\label{callout-definition*-1.4}
\textbf{\emph{Federated Learning}} is a decentralized training approach
in which distributed devices collaboratively train a \emph{shared model}
using \emph{local data} while exchanging only \emph{model updates},
preserving \emph{privacy} through data localization.

\end{fbxSimple}

Figure~\ref{fig-learning-paradigms} contrasts federated learning with
other learning paradigms to clarify its unique position. In traditional
offline learning, all data is collected and processed centrally. The
model is trained in the cloud using curated datasets and is then
deployed to edge devices without further adaptation. In contrast,
on-device learning allows local model adaptation using data generated on
the device itself, supporting personalization but in isolation, without
sharing insights across users. Federated learning bridges these two
extremes by enabling localized training while coordinating updates
globally. It retains data privacy by keeping raw data local, yet
benefits from distributed model improvements by aggregating updates from
many devices.

\begin{figure}[htb]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/bb5ced3b1942ab0ec7bca35b41e41812483d2bde.pdf}}

}

\caption{\label{fig-learning-paradigms}Federated learning balances data
privacy with collective model improvement by coordinating local training
across distributed devices, unlike offline learning's centralized
approach or on-device learning's isolated adaptation. Each paradigm
handles data location and model update strategies differently, revealing
the trade-offs between personalization, data security, and global
knowledge sharing.}

\end{figure}%

This section explores the principles and practical considerations of
federated learning in the context of mobile and embedded systems. It
begins by outlining the canonical FL protocols and their system
implications. It then discusses device participation constraints,
communication-efficient update mechanisms, and strategies for
personalized learning. Throughout, the emphasis remains on how federated
methods can extend the reach of on-device learning by enabling
distributed model training across diverse and resource-constrained
hardware platforms.

\subsection{Privacy-Preserving Collaborative
Learning}\label{sec-edge-intelligence-privacypreserving-collaborative-learning-ecf9}

Federated learning (FL) is a decentralized paradigm for training machine
learning models across a population of devices without transferring raw
data to a central server
(\citeproc{ref-mcmahan2017communication}{McMahan et al. 2017}). Unlike
traditional centralized training pipelines, which require aggregating
all training data in a single location, federated learning distributes
the training process itself. Each participating device computes updates
based on its local data and contributes to a global model through an
aggregation protocol, typically coordinated by a central server. This
shift in training architecture aligns closely with the needs of mobile,
edge, and embedded systems, where privacy, communication cost, and
system heterogeneity impose significant constraints on centralized
approaches.

As demonstrated across the application domains discussed earlier, from
Gboard's keyboard personalization to wearable health monitoring to voice
interfaces, federated learning bridges the gap between model improvement
and the system-level constraints established throughout this chapter. It
enables the personalization, privacy, and connectivity benefits
motivating on-device learning while addressing the resource constraints
through coordinated but distributed training. However, these benefits
introduce new challenges including client variability, communication
efficiency, and non-IID data distributions that require specialized
protocols and coordination mechanisms.

Building on this foundation, the remainder of this section explores the
key techniques and tradeoffs that define federated learning in on-device
settings, examining the core learning protocols that govern coordination
across devices and investigating strategies for scheduling,
communication efficiency, and personalization.

\subsection{Learning
Protocols}\label{sec-edge-intelligence-learning-protocols-139a}

Having established federated learning's privacy-preserving foundation,
we now examine the protocols that make distributed coordination
practical at scale. These protocols must solve three engineering
challenges simultaneously: ensuring that local training produces
compatible updates despite non-IID data distributions, aggregating those
updates efficiently despite bandwidth constraints, and coordinating
timing across devices with heterogeneous availability patterns.

Federated learning protocols define the rules and mechanisms by which
devices collaborate to train a shared model. These protocols govern how
local updates are computed, aggregated, and communicated, as well as how
devices participate in the training process. The choice of protocol has
significant implications for system performance, communication overhead,
and model convergence.

\subsubsection{Local
Training}\label{sec-edge-intelligence-local-training-e73d}

Local training refers to the process by which individual devices compute
model updates based on their local data. This step is critical in
federated learning, as it allows devices to adapt the shared model to
their specific contexts without transferring raw data. The local
training process involves the following steps:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Model Initialization}: Each device initializes its local model
  parameters, often by downloading the latest global model from the
  server.
\item
  \textbf{Local Data Sampling}: The device samples a subset of its local
  data for training. This data may be non-IID, meaning that it may not
  be uniformly distributed across devices.
\item
  \textbf{Local Training}: The device performs a number of training
  iterations on its local data, updating the model parameters based on
  the computed gradients.
\item
  \textbf{Model Update}: After local training, the device computes a
  model update (e.g., the difference between the updated and initial
  parameters) and prepares to send it to the server.
\item
  \textbf{Communication}: The device transmits the model update to the
  server, typically using a secure communication channel to protect user
  privacy.
\item
  \textbf{Model Aggregation}: The server aggregates the updates from
  multiple devices to produce a new global model, which is then
  distributed back to the participating devices.
\end{enumerate}

This process is repeated iteratively, with devices periodically
downloading the latest global model and performing local training. The
frequency of these updates can vary based on system constraints, device
availability, and communication costs.

\subsubsection{Federated Aggregation
Protocols}\label{sec-edge-intelligence-federated-aggregation-protocols-5d37}

At the heart of federated learning is a coordination mechanism that
allows many devices, each having access to only a small, local dataset,
to collaboratively train a shared model. This is achieved through a
protocol where client devices perform local training and transmit model
updates to a central server. The server aggregates these updates to
refine a global model, which is then redistributed to clients for the
next training round. This cyclical procedure decouples the learning
process from centralized data collection, making it well-suited to the
mobile and edge environments characterized throughout this chapter where
user data is private, bandwidth is constrained, and device participation
is sporadic.

The most widely used baseline for this process is Federated Averaging
(FedAvg)\sidenote{\textbf{Federated Averaging (FedAvg)}: Introduced by
Google in 2017, FedAvg revolutionized distributed ML by averaging model
weights rather than gradients. Each client performs multiple local SGD
steps (typically 1-20) before sending weights to the server, reducing
communication by 10-100\(\times\) compared to distributed SGD. The key
insight: local updates contain richer information than single gradients,
enabling convergence with far fewer communication rounds. FedAvg powers
production systems like Gboard, processing billions of devices. After a
fixed number of local steps, each device sends its updated model
parameters to the server. The server computes a weighted average of
these parameters, which are weighted according to the number of data
samples on each device, and updates the global model accordingly. This
updated model is then sent back to the devices, completing one round of
training. }, which has become a canonical algorithm for federated
learning (\citeproc{ref-mcmahan2017communication}{McMahan et al. 2017}).
In FedAvg, each device trains its local copy of the model using
stochastic gradient descent (SGD) on its private data.

Formally, let \(\mathcal{D}_k\) denote the local dataset on client
\(k\), and let \(\theta_k^t\) be the parameters of the model on client
\(k\) at round \(t\). Each client performs \(E\) steps of SGD on its
local data, yielding an update \(\theta_k^{t+1}\). The central server
then aggregates these updates as: \[
\theta^{t+1} = \sum_{k=1}^{K} \frac{n_k}{n} \theta_k^{t+1}
\] where \(n_k = |\mathcal{D}_k|\) is the number of samples on device
\(k\), \(n = \sum_k n_k\) is the total number of samples across
participating clients, and \(K\) is the number of active devices in the
current round.

\subsubsection{Straggler
Mitigation}\label{sec-edge-intelligence-straggler-mitigation-b49d}

In a fleet of millions of heterogeneous devices, waiting for every
selected client to report its update is impractical. Network latency,
battery depletion, or background process contention can cause some
devices (``stragglers'') to take 10× longer than average.

To prevent the global model update from stalling, production FL systems
employ \textbf{Over-Selection}. The server selects a candidate pool size
\(K_{candidates}\) larger than the target number of updates
\(K_{target}\) (typically
\(K_{candidates} \approx 1.3 \times K_{target}\)). The server aggregates
updates from the first \(K_{target}\) responders and discards the rest.
This approach bounds the round duration by the speed of the
\(K_{target}\)-th fastest device rather than the absolute slowest,
dramatically accelerating convergence wall-clock time.

This cyclical coordination protocol forms the foundation of federated
learning. Figure~\ref{fig-federated-averaging-cycle} breaks down the
FedAvg process into four phases: client selection with over-selection to
handle stragglers, local training on private data, parameter upload with
optional compression, and weighted aggregation that produces the updated
global model:

\begin{figure}[htb]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/8a75b5e82c15ddd6fb625b650b0124cdad95c3b0.pdf}}

}

\caption{\label{fig-federated-averaging-cycle}}

\end{figure}%

This basic structure introduces a number of design choices and
tradeoffs. The number of local steps \(E\) impacts the balance between
computation and communication: larger \(E\) reduces communication
frequency but risks divergence if local data distributions vary too
much. The selection of participating clients affects convergence
stability and fairness. In real-world deployments, not all devices are
available at all times, and hardware capabilities may differ
substantially, requiring robust participation scheduling and failure
tolerance.

\subsubsection{Federated Learning Convergence
Analysis}\label{sec-edge-intelligence-federated-learning-convergence-analysis-c1fc}

Understanding when federated learning converges and how fast it reaches
acceptable accuracy is essential for system design. Unlike centralized
training where convergence depends primarily on learning rate and batch
size, federated learning convergence depends on the interplay between
communication rounds, local computation, client participation, and data
heterogeneity. These factors determine whether a federated deployment
will reach target accuracy in hours, days, or never.

\textbf{Convergence Rate Fundamentals.} The theoretical convergence rate
of FedAvg under standard assumptions (smooth, strongly convex
objectives) follows:

\[
\mathbb{E}[F(\theta^R) - F(\theta^*)] \leq \mathcal{O}\left(\frac{1}{\sqrt{CER}}\right)
\]

where \(C\) is the number of clients participating per round, \(E\) is
the number of local epochs each client performs, and \(R\) is the total
number of communication rounds
(\citeproc{ref-mcmahan2017communication}{McMahan et al. 2017}). This
result reveals that convergence improves with the square root of total
computation: doubling clients, local epochs, or rounds each provides
diminishing returns. To halve the optimality gap, the system must
quadruple total computation.

The product \(CER\) represents total client-epochs, the fundamental unit
of federated computation. A system with 10 clients performing 5 local
epochs over 100 rounds (\(CER = 5000\)) achieves similar convergence to
one with 50 clients performing 2 epochs over 50 rounds (\(CER = 5000\)),
assuming identical data distributions. This equivalence enables flexible
resource allocation: bandwidth-constrained deployments can compensate
with more local computation, while computation-constrained devices can
rely on more frequent communication.

\textbf{Non-IID Data Impact.} The convergence rate above assumes IID
data across clients, an assumption that rarely holds in practice. When
client data distributions differ, a heterogeneity penalty \(\beta\)
degrades convergence:

\[
\mathbb{E}[F(\theta^R) - F(\theta^*)] \leq \mathcal{O}\left(\frac{\beta}{\sqrt{CER}} + \frac{\beta^2 E^2}{R}\right)
\]

The heterogeneity factor \(\beta\) quantifies the divergence between
local and global optima. Formally,
\(\beta^2 = \frac{1}{K}\sum_{k=1}^{K} \|\nabla F_k(\theta^*) - \nabla F(\theta^*)\|^2\)
measures how much local gradients disagree at the global optimum
(\citeproc{ref-li2020federated}{Li et al. 2020};
\citeproc{ref-zhao2018federated}{Zhao et al. 2018}). When data is IID,
\(\beta = 0\) and the bound reduces to the standard rate. When data is
highly heterogeneous, \(\beta\) can exceed 1, significantly slowing
convergence.

The second term \(\frac{\beta^2 E^2}{R}\) reveals a critical
interaction: more local epochs \(E\) amplify the heterogeneity penalty.
Each additional local step moves the local model further from the global
optimum before aggregation, and these deviations compound across
heterogeneous clients. This creates a communication-computation
trade-off that depends on data distribution characteristics.

\textbf{Quantifying Heterogeneity in Practice.} Real-world federated
deployments exhibit \(\beta\) values that vary dramatically by
application domain. Keyboard prediction across users speaking the same
language typically shows \(\beta \approx 0.3\)-\(0.8\), as vocabulary
and typing patterns vary but share common linguistic structure.
Cross-language keyboard prediction increases to
\(\beta \approx 1.5\)-\(3.0\) due to fundamentally different character
distributions and word patterns. Health monitoring with diverse patient
populations can reach \(\beta \approx 2.0\)-\(5.0\) when physiological
baselines vary dramatically across age groups, fitness levels, and
medical conditions.

\textbf{Worked Example: Federated Learning with 100 Clients.} Consider a
production deployment with the following parameters: \(K = 100\) total
clients in the population, \(C = 10\) clients selected per round,
\(E = 5\) local epochs per round, target optimality gap
\(\epsilon = 0.01\), and two scenarios comparing IID data
(\(\beta = 0\)) versus moderately non-IID data (\(\beta = 1.5\)).

\emph{IID Case (\(\beta = 0\)):} Using the convergence bound
\(\epsilon \leq \frac{\sigma}{\sqrt{CER}}\) where \(\sigma\) captures
gradient variance (typically \(\sigma \approx 1\) for normalized
objectives), we solve for required rounds:

\[
R_{\text{IID}} \geq \frac{\sigma^2}{C \cdot E \cdot \epsilon^2} = \frac{1}{10 \cdot 5 \cdot 0.0001} = 200 \text{ rounds}
\]

With 10 clients per round and 5 local epochs, this represents
\(200 \times 10 \times 5 = 10,000\) total client-epochs of computation.

\emph{Non-IID Case (\(\beta = 1.5\)):} The heterogeneity penalty
requires satisfying both terms of the bound. The dominant constraint
becomes:

\[
\frac{\beta^2 E^2}{R} \leq \epsilon \implies R \geq \frac{\beta^2 E^2}{\epsilon} = \frac{2.25 \times 25}{0.01} = 5,625 \text{ rounds}
\]

This represents a \(28\times\) increase in communication rounds compared
to the IID case. The non-IID scenario requires
\(5,625 \times 10 \times 5 = 281,250\) client-epochs, demonstrating how
data heterogeneity dominates convergence costs.

\emph{Reducing Local Epochs:} The quadratic dependence on \(E\) suggests
reducing local computation when heterogeneity is high. With \(E = 2\)
instead of \(E = 5\):

\[
R \geq \frac{2.25 \times 4}{0.01} = 900 \text{ rounds}
\]

This reduces total rounds by \(6.25\times\) at the cost of \(2.5\times\)
more communication per unit of local computation. The total
client-epochs become \(900 \times 10 \times 2 = 18,000\), a
\(15.6\times\) reduction from the \(E = 5\) non-IID case. This
illustrates why adaptive local epoch selection based on estimated data
heterogeneity significantly improves federated learning efficiency.

\textbf{Communication-Computation Trade-off.} The interaction between
local epochs and communication rounds creates a fundamental design
trade-off. More local epochs reduce communication frequency but increase
client drift, while fewer local epochs maintain tighter synchronization
at higher communication cost.

\begin{tcolorbox}[enhanced jigsaw, rightrule=.15mm, bottomtitle=1mm, opacitybacktitle=0.6, breakable, leftrule=.75mm, bottomrule=.15mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Communication-Computation Tradeoff}, colframe=quarto-callout-note-color-frame, coltitle=black, opacityback=0, left=2mm, titlerule=0mm, toptitle=1mm, arc=.35mm, toprule=.15mm, colbacktitle=quarto-callout-note-color!10!white, colback=white]

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/a317bb146cb65fa9a8770f6ed70ea604a8bfdbc6.pdf}}

\textbf{Communication-Computation Trade-off}. For IID data, increasing
local epochs consistently reduces total communication with minimal
penalty. For non-IID data, client drift causes convergence degradation
beyond an optimal point (typically \(E = 2\)-\(5\)), making aggressive
local computation counterproductive. System designers must estimate data
heterogeneity to select appropriate operating points.

\end{tcolorbox}

The optimal operating point depends on deployment-specific factors.
Communication cost (bandwidth, energy) favors larger \(E\), while
convergence speed under heterogeneity favors smaller \(E\). Practical
systems often use adaptive strategies that start with small \(E\) and
increase as the model approaches convergence and client drift
diminishes.

\textbf{When Does Federated Learning Work?} Based on convergence
analysis and empirical validation across production deployments,
federated learning achieves practical convergence when several
conditions are met (\citeproc{ref-kairouz2021advances}{Kairouz and
McMahan 2021}). First, heterogeneity must remain bounded: \(\beta < 3\)
ensures the heterogeneity penalty does not dominate training time.
Applications with \(\beta > 5\) may require clustering clients into more
homogeneous groups or using personalization techniques rather than
training a single global model. Second, client participation must be
sufficient: at least 1\% of the total client population should be
available per round (\(C/K > 0.01\)) to ensure adequate gradient
diversity. With participation below this threshold, gradient estimates
become too noisy for stable convergence. Third, local data must be
adequate: each participating client needs sufficient local data for
meaningful gradient computation, typically at least 10-50 samples per
round. Clients with fewer samples contribute high-variance updates that
can destabilize training.

When these conditions are violated, alternative approaches become
necessary. Extremely heterogeneous data (\(\beta > 5\)) suggests
hierarchical federated learning with regional aggregation or fully
personalized models without global aggregation. Very low participation
rates indicate the need for asynchronous protocols that do not require
synchronized rounds. Clients with minimal local data benefit from
few-shot adaptation techniques rather than gradient-based training.

\subsubsection{Client
Scheduling}\label{sec-edge-intelligence-client-scheduling-f675}

Federated learning operates under the assumption that clients, devices,
which hold local data, periodically become available for participation
in training rounds. In real-world systems, client availability is
intermittent and variable. Devices may be turned off, disconnected from
power, lacking network access, or otherwise unable to participate at any
given time. As a result, client scheduling plays a central role in the
effectiveness and efficiency of distributed learning.

At a baseline level, federated ML systems define eligibility criteria
for participation. Devices must meet minimum requirements such as being
plugged in, connected to Wi-Fi, and idle, to avoid interfering with user
experience or depleting battery resources. These criteria determine
which subset of the total population is considered ``available'' for any
given training round.

Beyond these operational filters, devices also differ in their hardware
capabilities, data availability, and network conditions. Some
smartphones contain many recent examples relevant to the current task,
while others have outdated or irrelevant data. Network bandwidth and
upload speed may vary widely depending on geography and carrier
infrastructure. As a result, selecting clients at random can lead to
poor coverage of the underlying data distribution and unstable model
convergence.

Availability-driven selection introduces participation bias. Clients
with favorable conditions are more likely to participate repeatedly.
These favorable conditions include frequent charging, high-end hardware,
and consistent connectivity. Meanwhile, others are systematically
underrepresented. This can skew the resulting model toward behaviors and
preferences of a privileged subset of the population, raising both
fairness and generalization concerns.

The severity of participation bias becomes apparent when examining real
deployment statistics. Studies of federated learning deployments show
that the most active 10\% of devices can contribute to over 50\% of
training rounds, while the bottom 50\% of devices may never participate
at all. This creates a feedback loop: models become increasingly
optimized for users with high-end devices and stable connectivity,
potentially degrading performance for resource-constrained users who
need adaptation the most. A keyboard prediction model might become
biased toward the typing patterns of users with flagship phones who
charge overnight, missing important linguistic variations from users
with budget devices or irregular charging patterns.

To address these challenges, systems must balance scheduling efficiency
with client diversity. A key approach involves using stratified or
quota-based sampling to ensure representative client participation
across different groups. Some systems implement ``fairness budgets''
that track cumulative participation and actively prioritize
underrepresented devices when they become available. Others use
importance sampling techniques to reweight contributions based on
estimated population statistics rather than raw participation rates. For
instance, asynchronous buffer-based techniques allow participating
clients to contribute model updates independently, without requiring
synchronized coordination in every round (\citeproc{ref-fedbuff}{Nguyen
et al. 2021}). This model has been extended to incorporate staleness
awareness (\citeproc{ref-fedstale}{Rodio and Neglia 2024}) and fairness
mechanisms (\citeproc{ref-fedstaleweight}{J. Ma et al. 2024}),
preventing bias from over-active clients who might otherwise dominate
the training process.

To address these challenges, federated ML systems implement adaptive
client selection strategies. These include prioritizing clients with
underrepresented data types, targeting geographies or demographics that
are less frequently sampled, and using historical participation data to
enforce fairness constraints. Systems incorporate predictive modeling to
anticipate future client availability or success rates, improving
training throughput.

Selected clients perform one or more local training steps on their
private data and transmit their model updates to a central server. These
updates are aggregated to form a new global model. Typically, this
aggregation is weighted, where the contributions of each client are
scaled, for example, by the number of local examples used during
training, before averaging. This ensures that clients with more
representative or larger datasets exert proportional influence on the
global model.

These scheduling decisions directly impact system performance. They
affect convergence rate, model generalization, energy consumption, and
overall user experience. Poor scheduling can result in excessive
stragglers, overfitting to narrow client segments, or wasted
computation. As a result, client scheduling is not merely a logistical
concern; it is a core component of system design in federated learning,
demanding both algorithmic insight and infrastructure-level
coordination.

\subsubsection{Bandwidth-Aware Update
Compression}\label{sec-edge-intelligence-bandwidthaware-update-compression-7730}

One of the principal bottlenecks in federated ML systems is the cost of
communication between edge clients and the central server. Transmitting
full model weights or gradients after every training round can overwhelm
bandwidth and energy budgets, particularly for mobile or embedded
devices operating over constrained wireless
links\sidenote{\textbf{Wireless Communication Reality}: Mobile devices
face severe bandwidth and energy constraints for federated learning. LTE
uploads average 5-10 Mbps versus 50+ Mbps downloads, creating asymmetric
bottlenecks. Transmitting a 50 MB model update consumes approximately
100~mAh battery (2-3\% of typical capacity, varies by radio efficiency
and signal strength) and takes 40-80 seconds. WiFi improves throughput
but isn't always available. Low-power devices using LoRaWAN or NB-IoT
face even harsher limits, LoRaWAN maxes at 50~kbps with 1\% duty cycle
restrictions, making frequent updates impractical without aggressive
compression. }. To address this, a range of techniques have been
developed to reduce communication overhead while preserving learning
efficacy.

These techniques fall into three primary categories: model compression,
selective update sharing, and architectural partitioning.

Model compression methods aim to reduce the size of transmitted updates
through quantization\sidenote{\textbf{Gradient Quantization}: Reduces
communication by converting FP32 gradients to lower precision (INT8,
INT4, or even 1-bit). Advanced techniques like signSGD use only gradient
signs, achieving 32\(\times\) compression. Error compensation methods
accumulate quantization errors for later transmission, maintaining
convergence quality. Real deployments achieve 8-16\(\times\)
communication reduction with \textless1\% accuracy loss. },
sparsification, or subsampling. Instead of sending full-precision
gradients, a client transmits 8-bit quantized updates or communicates
only the top-\(k\) gradient elements\sidenote{\textbf{Gradient
Sparsification}: Transmits only the largest gradients by magnitude
(typically top 1-10\%), dramatically reducing communication. Gradient
accumulation stores untransmitted gradients locally until they become
large enough to send. This technique exploits the observation that most
gradients are small and contribute minimally to convergence, achieving
10-100\(\times\) compression ratios while maintaining training
effectiveness. These techniques reduce transmission size with limited
impact on convergence when applied carefully. } with highest magnitude.

Selective update sharing further reduces communication by transmitting
only subsets of model parameters or updates. In layer-wise selective
sharing, clients update only certain layers, typically the final
classifier or adapter modules, while keeping the majority of the
backbone frozen. This reduces both upload cost and the risk of
overfitting shared representations to non-representative client data.

Split models and architectural partitioning divide the model into a
shared global component and a private local component. Clients train and
maintain their private modules independently while synchronizing only
the shared parts with the server. This allows for user-specific
personalization with minimal communication and privacy leakage.

All of these approaches operate within the context of a federated
aggregation protocol. A standard baseline for aggregation is Federated
Averaging (FedAvg), in which the server updates the global model by
computing a weighted average of the client updates received in a given
round. Let \(\mathcal{K}_t\) denote the set of participating clients in
round \(t\), and let \(\theta_k^t\) represent the locally updated model
parameters from client \(k\). The server computes the new global model
\(\theta^{t+1}\) as: \[
\theta^{t+1} = \sum_{k \in \mathcal{K}_t} \frac{n_k}{n_{\mathcal{K}_t}} \theta_k^t
\]

Here, \(n_k\) is the number of local training examples at client \(k\),
and \(n_{\mathcal{K}_t} = \sum_{k \in \mathcal{K}_t} n_k\) is the total
number of training examples across all participating clients. This
data-weighted aggregation ensures that clients with more training data
exert a proportionally larger influence on the global model, while also
accounting for partial participation and heterogeneous data volumes.

However, communication-efficient updates can introduce tradeoffs.
Compression may degrade gradient fidelity, selective updates can limit
model capacity, and split architectures may complicate coordination. As
a result, effective federated learning requires careful balancing of
bandwidth constraints, privacy concerns, and convergence dynamics, a
balance that depends heavily on the capabilities and variability of the
client population.

\begin{tcolorbox}[enhanced jigsaw, rightrule=.15mm, bottomtitle=1mm, opacitybacktitle=0.6, breakable, leftrule=.75mm, bottomrule=.15mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Figure: Gradient Compression Techniques}, colframe=quarto-callout-note-color-frame, coltitle=black, opacityback=0, left=2mm, titlerule=0mm, toptitle=1mm, arc=.35mm, toprule=.15mm, colbacktitle=quarto-callout-note-color!10!white, colback=white]

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/fb5d9936ccdc561ebdec73217999fe1ad55479e1.pdf}}

\textbf{Gradient Compression Techniques}. \textbf{(a) Standard} updates
transmit full-precision values. \textbf{(b) Quantization} maps values to
low-precision buckets (e.g., FP32 to INT8), reducing bandwidth.
\textbf{(c) Sparsification} transmits only the most significant
(Top-\(k\)) gradients, exploiting the fact that many updates are
near-zero.

\end{tcolorbox}

\subsubsection{Federated
Personalization}\label{sec-edge-intelligence-federated-personalization-3c73}

While compression and communication strategies improve scalability, they
do not address a important limitation of the global federated learning
paradigm, its inability to capture user-specific variation. In
real-world deployments, devices often observe distinct and heterogeneous
data distributions. A one-size-fits-all global model may underperform
when applied uniformly across diverse users. This motivates the need for
personalized federated learning, where local models are adapted to
user-specific data without compromising the benefits of global
coordination.

Let \(\theta_k\) denote the model parameters on client \(k\), and
\(\theta_{\text{global}}\) the aggregated global model. Traditional FL
seeks to minimize a global objective: \[
\min_\theta \sum_{k=1}^K w_k \mathcal{L}_k(\theta)
\] where \(\mathcal{L}_k(\theta)\) is the local loss on client \(k\),
and \(w_k\) is a weighting factor (e.g., proportional to local dataset
size). However, this formulation assumes that a single model \(\theta\)
can serve all users well. In practice, local loss landscapes
\(\mathcal{L}_k\) often differ significantly across clients, reflecting
non-IID data distributions and varying task requirements.

Personalization modifies this objective to allow each client to maintain
its own adapted parameters \(\theta_k\), optimized with respect to both
the global model and local data: \[
\min_{\theta_1, \ldots, \theta_K} \sum_{k=1}^K \left( \mathcal{L}_k(\theta_k) + \lambda \cdot \mathcal{R}(\theta_k, \theta_{\text{global}}) \right)
\]

Here, \(\mathcal{R}\) is a regularization term that penalizes deviation
from the global model, and \(\lambda\) controls the strength of this
penalty. This formulation allows local models to deviate as needed,
while still benefiting from global coordination.

Real-world use cases illustrate the importance of this approach.
Consider a wearable health monitor that tracks physiological signals to
classify physical activities. While a global model may perform
reasonably well across the population, individual users exhibit unique
motion patterns, gait signatures, or sensor placements. Personalized
finetuning of the final classification layer or low-rank adapters allows
improved accuracy, particularly for rare or user-specific classes.

Several personalization strategies have emerged to address the tradeoffs
between compute overhead, privacy, and adaptation speed. One widely used
approach is local finetuning, in which each client downloads the latest
global model and performs a small number of gradient steps using its
private data. While this method is simple and preserves privacy, it may
yield suboptimal results when the global model is poorly aligned with
the client's data distribution or when the local dataset is extremely
limited.

Another effective technique involves personalization layers, where the
model is partitioned into a shared backbone and a lightweight,
client-specific head, typically the final classification layer
(\citeproc{ref-arivazhagan2019federated}{Arivazhagan et al. 2019}). Only
the head is updated on-device, reducing memory usage and training time.
This approach is particularly well-suited for scenarios in which the
primary variation across clients lies in output categories or decision
boundaries.

Clustered federated learning offers an alternative by grouping clients
according to similarities in their data or performance characteristics,
and training separate models for each cluster. This strategy can enhance
accuracy within homogeneous subpopulations but introduces additional
system complexity and may require exchanging metadata to determine group
membership.

Finally, meta-learning approaches, such as Model-Agnostic Meta-Learning
(MAML)\sidenote{\textbf{Model-Agnostic Meta-Learning (MAML)}: A
meta-learning algorithm (2017) that trains models not to perform well on
specific tasks, but to be easily adaptable to new tasks. MAML optimizes
for a model initialization from which a few gradient steps yield good
performance on any task. For on-device learning, this means pre-training
produces models that personalize quickly (1-5 gradient steps) from
minimal user data (5-20 examples). The computational cost is high during
meta-training (2-3x standard training) but minimal during deployment
adaptation, making it well-suited for edge scenarios where users provide
limited labeled examples. }, aim to produce a global model
initialization that can be quickly adapted to new tasks with just a few
local updates (\citeproc{ref-finn2017model}{Finn, Abbeel, and Levine
2017}). This technique is especially useful when clients have limited
data or operate in environments with frequent distributional shifts.

Each of these strategies reflects a different point in the tradeoff
space. Examine Table~\ref{tbl-personalization-strategies} to see how
compute overhead, privacy guarantees, and adaptation latency vary across
local finetuning, personalization layers, clustered federated learning,
and meta-learning approaches.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2000}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2000}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2000}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2000}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2000}}@{}}
\caption{\textbf{Personalization Trade-Offs}: Federated learning
strategies balance personalization with system costs, impacting compute
overhead, privacy preservation, and adaptation speed for diverse client
populations. This table summarizes how local finetuning, clustered
learning, and meta-learning each navigate this trade-off space, enabling
tailored models while considering practical deployment
constraints.}\label{tbl-personalization-strategies}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Strategy}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Personalization Mechanism}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Compute Overhead}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Privacy} \textbf{Preservation}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Adaptation Speed}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Strategy}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Personalization Mechanism}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Compute Overhead}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Privacy} \textbf{Preservation}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Adaptation Speed}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Local Finetuning} & Gradient descent on local loss
post-aggregation & Low to Moderate & High (no data sharing) & Fast (few
steps) \\
\textbf{Personalization Layers} & Split model: shared base +
user-specific head & Moderate & High & Fast (train small head) \\
\textbf{Clustered FL} & Group clients by data similarity, train per
group & Moderate to High & Medium (group metadata) & Medium \\
\textbf{Meta-Learning} & Train for fast adaptation across tasks/devices
& High (meta-objective) & High & Very Fast (few-shot) \\
\end{longtable}

Selecting the appropriate personalization method depends on deployment
constraints, data characteristics, and the desired balance between
accuracy, privacy, and computational efficiency.
Figure~\ref{fig-personalization-arch} contrasts three architectural
approaches: full fine-tuning offers maximum expressivity at high compute
cost, head-only adaptation provides low-cost but limited adaptation, and
adapter-based methods (PEFT) balance efficiency with deep representation
adaptation through small trainable modules.

\begin{figure}[htb]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/e2eeafd5cf493e277d43c95deecb7cfa737def18.pdf}}

}

\caption{\label{fig-personalization-arch}\textbf{Federated
Personalization Architectures}. Architectural strategies for adapting
global models to local data. \textbf{(a) Full Fine-tuning} updates all
model parameters, offering maximum expressivity but high compute cost.
\textbf{(b) Head-Only Adaptation} updates only the final classifier
layers while keeping the feature extractor frozen, suitable for
resource-constrained devices. \textbf{(c) Adapter-Based Learning} (e.g.,
LoRA) inserts small trainable modules into a frozen backbone, balancing
efficiency with the ability to adapt deep representations.}

\end{figure}%

\subsubsection{Federated
Privacy}\label{sec-edge-intelligence-federated-privacy-a1ed}

While federated learning is often motivated by privacy concerns, as it
involves keeping raw data localized instead of transmitting it to a
central server, the paradigm introduces its own set of security and
privacy risks. Although devices do not share their raw data, the
transmitted model updates (such as gradients or weight changes) can
inadvertently leak information about the underlying private data.
Techniques such as model inversion attacks\sidenote{\textbf{Model
Inversion Attacks}: Technique where adversaries reconstruct training
data by exploiting model outputs or gradients. Given access to a face
recognition model, attackers can generate recognizable images of
training subjects by optimizing inputs that maximize confidence scores.
In federated learning, gradient updates leak more information than model
outputs alone, a single gradient update can reveal whether specific data
points were in the training batch. Defense requires gradient
perturbation, secure aggregation, or differential privacy, each adding
10-50\% computational overhead. } and membership inference
attacks\sidenote{\textbf{Membership Inference Attacks}: Attacks that
determine whether specific data points were used to train a model,
exploiting the observation that models behave differently on training
data (higher confidence, lower loss) than unseen data. In healthcare,
this could reveal whether a patient's records were used to train a
disease prediction model, violating medical privacy. State-of-the-art
attacks achieve 60-90\% accuracy on overfitted models, making this a
serious concern for federated learning where local models train on
personal data. } demonstrate that adversaries may partially reconstruct
or infer properties of local datasets by analyzing these updates.

To mitigate such risks, modern federated ML systems commonly employ
protective measures. Secure Aggregation\sidenote{\textbf{Secure
Aggregation}: Cryptographic protocol enabling a server to compute the
sum of client updates without seeing individual contributions. Uses
secret sharing: each client pair generates a shared random mask that
cancels in the sum. If Alice adds +7 and Bob adds -7 to their respective
updates, the server sees the true sum without learning either client's
actual value. Google's implementation for Gboard handles millions of
clients with \textless10\% communication overhead but requires at least
100 participants per round for security guarantees. Enables
privacy-preserving federated learning without trusting the aggregation
server. } protocols ensure that individual model updates are encrypted
and aggregated in a way that the server only observes the combined
result, not any individual client's contribution. Differential
Privacy\sidenote{\textbf{Differential Privacy}: Mathematical framework
that provides quantifiable privacy guarantees by adding carefully
calibrated noise to computations. In federated learning, DP ensures that
individual user data cannot be inferred from model updates, even by
aggregators. Key parameter ε controls privacy-utility tradeoff: smaller
ε means stronger privacy but lower model accuracy. Typical deployments
use ε=1-8, requiring noise addition that can increase communication
overhead by 2-10\(\times\) and reduce model accuracy by 1-5\%. Essential
for regulatory compliance and user trust in distributed learning
systems. Differential privacy receives comprehensive treatment in
\textbf{?@sec-security-privacy}; this overview provides the context
needed for federated learning. } techniques inject carefully calibrated
noise into updates to mathematically bound the information that can be
inferred about any single client's data.

While these techniques enhance privacy, they introduce additional system
complexity and tradeoffs between model utility, communication cost, and
robustness. Figure~\ref{fig-secure-agg} illustrates the secure
aggregation protocol, showing how pairs of clients exchange shared
random masks that cancel out during server-side summation, revealing
only the aggregate gradient without exposing individual contributions. A
deeper exploration of these attacks, defenses, and their implications
requires dedicated coverage of security principles in distributed ML
systems.

\begin{figure}[htb]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/441d0e31ab74f475fc78262c8777cc385d2da0a1.pdf}}

}

\caption{\label{fig-secure-agg}\textbf{Secure Aggregation Protocol}. A
simplified view of how cryptographic masking protects individual
updates. Pairs of clients agree on shared random masks that are added by
one and subtracted by the other. The central server sums the masked
updates; the masks mathematically cancel out in the aggregate, revealing
the global update sum without ever exposing the raw value of any single
client's contribution.}

\end{figure}%

\subsection{Large-Scale Device
Orchestration}\label{sec-edge-intelligence-largescale-device-orchestration-1360}

Federated learning transforms machine learning into a massive
distributed systems challenge that extends far beyond traditional
algorithmic considerations. Coordinating thousands or millions of
heterogeneous devices with intermittent connectivity requires
sophisticated distributed systems protocols that handle Byzantine
failures, network partitions, and communication efficiency at
unprecedented scale. These challenges fundamentally differ from the
controlled environments of data center distributed training, where
high-bandwidth networks and reliable infrastructure enable
straightforward coordination protocols.

\subsubsection{Network and Bandwidth
Optimization}\label{sec-edge-intelligence-network-bandwidth-optimization-53da}

The communication bottleneck represents the primary scalability
constraint in federated learning systems. Understanding the quantitative
transfer requirements enables principled design decisions about model
architectures, update compression strategies, and client participation
policies that determine system viability.

The federated communication hierarchy reveals the severe bandwidth
constraints under which distributed learning must operate. Full model
synchronization requires 10-500 MB per training round for typical deep
learning models, prohibitive for mobile networks with limited upload
bandwidth that averages just 5-50 Mbps in practice. Gradient compression
achieves 10-100\(\times\) reduction through quantization (reducing FP32
to INT8), sparsification (transmitting only non-zero gradients), and
selective gradient transmission (sending only the most significant
updates). Practical deployments demand even more aggressive
100-1000\(\times\) compression ratios, reducing 100 MB models to
manageable 100 KB-1 MB updates that mobile devices can transmit within
reasonable timeframes and without exhausting data plans. Communication
frequency introduces a critical trade-off between model update
freshness, more frequent updates enable faster adaptation to changing
conditions, and network efficiency constraints that limit sustainable
bandwidth consumption.

Network infrastructure constraints directly impact participation rates
and overall system viability. Modern 4G networks typically provide
upload speeds ranging from 5-50 Mbps under optimal conditions (with
significant geographic and carrier variation), meaning an 8 MB model
update requires 1.3-13 seconds of sustained transmission. However,
real-world mobile networks exhibit extreme variability: rural areas may
experience 1 Mbps upload speeds while urban 5G deployments enable 100+
Mbps. This 100\(\times\) variance in network capability necessitates
adaptive communication strategies that optimize for
lowest-common-denominator connectivity while enabling high-capability
devices to contribute more effectively.

The relationship between communication requirements and participation
rates exhibits sharp threshold effects. Empirical studies demonstrate
that federated learning systems requiring model transfers exceeding 10
MB achieve less than 10\% sustained client participation, while systems
maintaining updates below 1 MB can sustain 40-60\% participation rates
across diverse mobile populations. This communication efficiency
directly translates to model quality improvements: higher participation
rates provide better statistical diversity and more robust gradient
estimates for global model updates.

Advanced compression techniques become essential for practical
deployment. Gradient quantization reduces precision from FP32 to INT8 or
even binary representations, achieving 4-32\(\times\) compression with
minimal accuracy loss. Sparsification techniques transmit only the
largest gradient components, leveraging the natural sparsity in neural
network updates. Top-k gradient selection further reduces communication
by transmitting only the most significant parameter updates, while error
accumulation ensures that small gradients are not permanently lost.

\subsubsection{Asynchronous Device
Synchronization}\label{sec-edge-intelligence-asynchronous-device-synchronization-348e}

Federated learning operates at the complex intersection of distributed
systems and machine learning, inheriting fundamental challenges from
both domains while introducing unique complications that arise from the
mobile, heterogeneous, and unreliable nature of edge devices.

Federated learning must contend with Byzantine fault tolerance
requirements that extend beyond typical distributed systems challenges.
Device failures occur frequently as clients crash, lose power, or
disconnect during training rounds due to battery depletion or network
connectivity issues, far more common than server failures in traditional
distributed training. Malicious updates present security concerns as
adversarial clients can provide corrupted gradients deliberately
designed to degrade global model performance or extract private
information from the aggregation process. Robust aggregation protocols
implementing Byzantine-resilient averaging ensure system reliability
despite the presence of compromised or unreliable participants, though
these protocols introduce significant computational overhead. Consensus
mechanisms must coordinate millions of unreliable participants without
the overhead of traditional distributed consensus protocols like Paxos
or Raft, which were designed for small clusters of reliable servers.

Network partitions pose particularly acute challenges for federated
coordination protocols. Unlike traditional distributed systems operating
within reliable data center networks, federated learning must gracefully
handle prolonged client disconnection events where devices may remain
offline for hours or days while traveling, in poor coverage areas, or
simply powered down. Asynchronous coordination protocols enable
continued training progress despite missing participants, but must
carefully balance staleness (accepting potentially outdated
contributions) against freshness (prioritizing recent but potentially
sparse updates).

Fault recovery and resilience strategies form an essential layer of
federated learning infrastructure. Checkpoint synchronization through
periodic global model snapshots enables recovery from server failures
and provides rollback points when corrupted training rounds are
detected, though checkpointing large models across millions of devices
introduces substantial storage and communication overhead. Partial
update handling ensures systems gracefully handle incomplete training
rounds when significant subsets of clients fail or disconnect
mid-training, requiring careful weighting strategies to prevent bias
toward more reliable device cohorts. State reconciliation protocols
enable clients rejoining after extended offline periods, potentially
days or weeks, to efficiently resynchronize with the current global
model while minimizing communication overhead that could overwhelm
bandwidth-constrained devices. Dynamic load balancing addresses uneven
client availability patterns that create computational hotspots,
requiring intelligent load redistribution across available participants
to maintain training throughput despite time-varying participation
rates.

The asynchronous nature of federated coordination introduces additional
complexity in maintaining training convergence guarantees. Traditional
synchronous training assumes all participants complete each round, but
federated systems must handle stragglers\sidenote{\textbf{Straggler
Problem in Federated Learning}: Slow or delayed clients that hold up
synchronous training rounds. In datacenter distributed training,
stragglers cause 10--30\% throughput reduction. In federated learning,
the problem is far worse: the slowest 10\% of mobile devices can be 100×
slower than the fastest, making synchronous protocols impractical.
Solutions include asynchronous aggregation (accept updates as they
arrive), bounded staleness (ignore updates older than k rounds), and
proactive client selection that preferentially samples faster devices.
However, aggressive straggler mitigation risks biasing models toward
users with high-end devices. Techniques such as FedAsync
(\citeproc{ref-xie2019fedasync}{Xie, Koyejo, and Gupta 2019}) enable
asynchronous aggregation where the server continuously updates the
global model as client updates arrive, while bounded staleness
mechanisms prevent extremely outdated updates from corrupting recent
progress. } and dropouts gracefully.

\subsubsection{Managing Million-Device
Heterogeneity}\label{sec-edge-intelligence-managing-milliondevice-heterogeneity-86c1}

Real-world federated learning deployments exhibit extreme heterogeneity
across multiple dimensions simultaneously: hardware capabilities,
network conditions, data distributions, and availability patterns. This
multi-dimensional heterogeneity fundamentally challenges traditional
distributed machine learning assumptions about homogeneous participants
operating under similar conditions.

Real-world federated learning deployments face multi-dimensional device
heterogeneity that creates extreme variation across every system
dimension. Computational variation spans 1000× differences in processing
power between flagship smartphones running at 35 TOPS and IoT
microcontrollers operating at just 0.03 TOPS, fundamentally limiting
what models can train on different device tiers. Memory constraints
exhibit even more dramatic 100--10,000× differences in available RAM
across device categories, ranging from 256 KB on microcontrollers to 16
GB on premium smartphones, determining whether devices can perform any
local training at all or must rely purely on inference. Energy
limitations force training sessions to be carefully scheduled around
charging patterns, thermal constraints, and battery preservation
requirements, with mobile devices typically limiting ML workloads to
500--1000 mW sustained power consumption. Network diversity introduces
orders-of-magnitude performance differences as WiFi, 4G, 5G, and
satellite connectivity exhibit vastly different bandwidth (ranging from
1 Mbps to 1 Gbps), latency (10 ms to 600 ms), and reliability
characteristics that determine feasible update frequencies and
compression requirements.

Adaptive coordination protocols address this heterogeneity through
sophisticated tiered participation strategies that optimize resource
utilization across the device spectrum. High-capability devices such as
flagship smartphones can perform complex local training with large batch
sizes and multiple epochs, while resource-constrained IoT devices
contribute through lightweight updates, specialized subtasks, or even
simple data aggregation. This creates a natural computational hierarchy
where powerful devices act as ``super-peers'' performing
disproportionate computation, while edge devices contribute specialized
local knowledge and coverage.

The scale challenges extend far beyond device heterogeneity to
fundamental coordination overhead limitations. Traditional distributed
consensus algorithms such as Raft or PBFT are designed for dozens of
nodes in controlled environments, but federated learning requires
coordination among millions of participants across unreliable networks.
This necessitates hierarchical coordination architectures where regional
aggregation servers reduce communication overhead by performing local
consensus before contributing to global aggregation. Edge computing
infrastructure provides natural hierarchical coordination points,
enabling federated learning systems to leverage existing content
delivery networks (CDNs) and mobile edge computing (MEC) deployments for
efficient gradient aggregation.

Modern federated systems implement sophisticated client selection
strategies that balance statistical diversity with practical
constraints. Random sampling ensures unbiased representation but may
select many low-capability devices, while capability-based selection
improves training efficiency but risks statistical bias. Hybrid
approaches use stratified sampling across device tiers, ensuring both
statistical representativeness and computational efficiency. These
selection strategies must also consider temporal patterns: office
workers' devices may be available during specific hours, while IoT
sensors provide continuous but limited computational resources.

\section{Production
Integration}\label{sec-edge-intelligence-production-integration-beb5}

The preceding sections examined individual techniques in isolation:
model adaptation strategies that reduce parameter update scope, data
efficiency methods that maximize learning from scarce examples, and
federated coordination protocols that enable privacy-preserving
collaboration. Production deployment requires orchestrating all these
techniques within real-world operational constraints that no single
algorithm addresses.

This section marks a shift from algorithmic understanding to systems
engineering. The challenges here are not about finding better adaptation
formulas or more efficient compression schemes. Instead, they concern
how to deploy, monitor, and maintain learning systems across millions of
heterogeneous devices with intermittent connectivity, varying
capabilities, and strict privacy requirements. Device heterogeneity,
network variability, regulatory compliance, and continuous monitoring
requirements create integration complexity that exceeds the sum of
individual techniques.

Real-world deployment introduces systemic challenges that emerge only
when techniques combine. Model adaptation, data efficiency, and
federated coordination must work together seamlessly rather than as
independent optimizations. Different learning strategies have varying
computational and memory profiles that must be coordinated within
overall device budgets. Training, inference, and communication must be
scheduled carefully to avoid interference with user experience and
system stability. Unlike centralized systems with observable training
loops, on-device learning requires distributed validation and failure
detection mechanisms that operate across heterogeneous device
populations.

This transition from theory to practice requires systematic engineering
approaches that balance competing constraints while maintaining system
reliability. Successful on-device learning deployments depend not on
individual algorithmic improvements but on holistic system designs that
orchestrate multiple techniques within operational constraints. The
subsequent sections examine how production systems address these
integration challenges through principled design patterns, operational
practices, and monitoring strategies that enable scalable, reliable
on-device learning deployment.

\subsection{MLOps Integration
Challenges}\label{sec-edge-intelligence-mlops-integration-challenges-bb4e}

Integrating on-device learning into existing MLOps workflows requires
extending operational frameworks to handle distributed training,
heterogeneous devices, and privacy-preserving coordination. Traditional
continuous integration pipelines, model versioning systems, and
monitoring infrastructure provide essential foundations, but must be
adapted to address unique edge deployment challenges. Standard MLOps
pipelines assume centralized data access, controlled deployment
environments, and unified monitoring capabilities that do not directly
apply to edge learning scenarios, requiring new approaches to technical
debt management and operational excellence.

\subsubsection{Deployment Pipeline
Transformations}\label{sec-edge-intelligence-deployment-pipeline-transformations-431d}

Traditional MLOps deployment pipelines follow a standardized CI/CD
process for model training, validation, staging, and production
deployment of a single model artifact to uniform infrastructure.
On-device learning requires device-aware deployment pipelines that
distribute different adaptation strategies across heterogeneous device
tiers. Microcontrollers receive bias-only updates, mid-range phones use
LoRA adapters, and flagship devices perform selective layer updates. The
deployment artifact evolves from a static model file to a collection of
adaptation policies, initial model weights, and device-specific
optimization configurations.

This architectural shift necessitates extending traditional deployment
pipelines with device capability detection, strategy selection logic,
and tiered deployment orchestration that maintains the reliability
guarantees of conventional MLOps while accommodating unprecedented
deployment diversity.

This transformation introduces new complexity in version management.
While centralized systems maintain a single model version, on-device
learning systems must simultaneously track multiple versioning
dimensions. The pre-trained backbone distributed to all devices
represents the base model version, which serves as the foundation for
all local adaptations. Different update mechanisms deployed per device
class constitute adaptation strategies, varying from simple bias
adjustments on microcontrollers to full layer fine-tuning on flagship
devices. Local model states naturally diverge from the base as devices
encounter unique data distributions, creating device-specific
checkpoints that reflect individual adaptation histories. Finally,
federated learning rounds that periodically synchronize device
populations establish aggregation epochs, marking discrete points where
distributed knowledge converges into updated global models. Successful
deployments implement tiered versioning schemes where base models evolve
slowly, typically through monthly updates, while local adaptations occur
continuously, creating a hierarchical version space rather than the
linear version history familiar from traditional deployments.

\subsubsection{Monitoring System
Evolution}\label{sec-edge-intelligence-monitoring-system-evolution-94c9}

Traditional monitoring practices aggregate metrics from centralized
inference servers. On-device learning monitoring must operate within
fundamentally different constraints that reshape how systems observe,
measure, and respond to model behavior across distributed device
populations.

Privacy-preserving telemetry represents the first fundamental departure
from traditional monitoring. Collecting performance metrics without
compromising user privacy requires federated analytics where devices
share only aggregate statistics or differentially private summaries.
Systems cannot simply log individual predictions or training samples as
centralized systems do. Instead, devices report distribution summaries
such as mean accuracy and confidence histograms rather than per-example
metrics. All reported statistics must include differential privacy
guarantees that bound information leakage through carefully calibrated
noise addition. Secure aggregation protocols prevent the server from
observing individual device contributions, ensuring that even the
aggregation process itself cannot reconstruct private information from
any single device's data.

Drift detection presents additional challenges without access to ground
truth labels. Traditional monitoring compares model predictions against
labeled validation sets maintained on centralized infrastructure.
On-device systems must detect drift using only local signals available
during deployment. Confidence calibration tracks whether predicted
probabilities match empirical frequencies, detecting degradation when
the model's confidence estimates become poorly calibrated to actual
outcomes. Input distribution monitoring detects when feature
distributions shift from training data through statistical techniques
that require no labels. Task performance proxies leverage implicit
feedback such as user corrections or task abandonment as quality signals
that indicate when the model fails to meet user needs. Shadow baseline
comparison runs a frozen base model alongside the adapted model to
measure divergence, flagging cases where local adaptation degrades
rather than improves performance relative to the known-good baseline.

Heterogeneous performance tracking addresses a third critical challenge:
global averages mask critical failures when device populations exhibit
high variance. Monitoring systems must segment performance across
multiple dimensions to identify systematic issues that affect specific
device cohorts. Capability-based performance gaps reveal when flagship
devices achieve substantially better results than budget devices,
indicating that adaptation strategies may need adjustment for
resource-constrained hardware. Regional bias issues surface when models
perform well in some geographic markets but poorly in others,
potentially reflecting data distribution shifts or cultural factors not
captured during initial training. Temporal patterns emerge when
performance degrades for devices running stale base models that have not
received recent updates from federated aggregation. Participation
inequality becomes visible when comparing devices that adapt frequently
against those that rarely participate in training, revealing potential
fairness issues in how learning benefits are distributed across the user
population.

\subsubsection{Continuous Training
Orchestration}\label{sec-edge-intelligence-continuous-training-orchestration-8974}

Traditional continuous training executes scheduled retraining jobs on
centralized infrastructure with predictable resource availability and
coordinated execution. On-device learning transforms this into
continuous distributed training where millions of devices train
independently without global synchronization, creating orchestration
challenges that require fundamentally different coordination strategies.

Asynchronous device coordination represents the first major departure
from centralized training. Millions of devices train independently on
their local data, but the orchestration system cannot rely on
synchronized participation. Only 20-40\% of devices are typically
available in any training round due to network connectivity limitations,
battery constraints, and varying usage patterns. The system must exhibit
straggler tolerance, ensuring that slow devices on limited hardware or
poor network connections cannot block faster devices from progressing
with their local adaptations. Devices often operate on different base
model versions simultaneously, creating version skew that the
aggregation protocol must handle gracefully without forcing all devices
to maintain identical model states. State reconciliation becomes
necessary when devices reconnect after extended offline periods,
potentially days or weeks, requiring the system to integrate their
accumulated local adaptations despite having missed multiple federated
aggregation rounds.

Resource-aware scheduling ensures that training respects both device
constraints and user experience. Orchestration policies implement
opportunistic training windows that execute adaptation only when the
device is idle, charging, and connected to WiFi, avoiding interference
with active user tasks or consuming metered cellular data. Thermal
budgets suspend training when device temperature exceeds
manufacturer-specified thresholds, preventing user discomfort and
hardware damage from sustained computational loads. Battery preservation
policies limit training energy consumption to less than 5\% of battery
capacity per day, ensuring that on-device learning does not noticeably
impact device runtime from the user's perspective. Network-aware
communication compresses model updates aggressively when devices must
use metered connections, trading computational overhead for reduced
bandwidth consumption to minimize user data charges.

Convergence assessment without global visibility poses the final
orchestration challenge. Traditional training monitors loss curves on
centralized validation sets, providing clear signals about training
progress and convergence. Distributed training must assess convergence
through indirect signals aggregated across the device population.
Federated evaluation aggregates validation metrics from devices that
maintain local held-out sets, providing approximate measures of global
model quality despite incomplete device participation. Update magnitude
tracking monitors how much local gradients change the global model in
each aggregation round, with diminishing update sizes signaling
potential convergence. Participation diversity ensures broad device
representation in aggregated updates, preventing convergence metrics
from reflecting only a narrow subset of the deployment environment.
Temporal consistency detects when model improvements plateau across
multiple aggregation rounds, indicating that the current adaptation
strategy has exhausted its potential gains and may require adjustment.

\subsubsection{Validation Strategy
Adaptation}\label{sec-edge-intelligence-validation-strategy-adaptation-ab3d}

Traditional validation approaches assume access to held-out test sets
and centralized evaluation infrastructure where model quality can be
measured directly against known ground truth. On-device learning
requires distributed validation that respects privacy and resource
constraints while still providing reliable quality signals across
heterogeneous device populations.

Shadow model evaluation provides the primary validation mechanism by
maintaining multiple model variants on each device and comparing their
behavior. Devices simultaneously run a baseline shadow model, a frozen
copy of the last known-good base model that provides a stable reference
point, alongside the current locally-adapted version that reflects
recent on-device training. Many systems also maintain the latest
federated aggregation result as a global model variant, enabling
comparison between individual device adaptations and the collective
knowledge aggregated from the entire device population. By comparing
predictions across these variants on incoming data streams, systems
detect when local adaptation degrades performance relative to
established baselines. This comparison occurs continuously during normal
operation, requiring no additional labeled validation data. When the
adapted model consistently underperforms the baseline shadow, the system
triggers automatic rollback to the known-good version, preventing
performance degradation from persisting in production.

Confidence-based quality gates provide an additional validation signal
when labeled validation data is unavailable. Without ground truth
labels, systems use prediction confidence as a quality proxy that
correlates with model performance. Well-calibrated models should exhibit
high confidence on in-distribution samples that resemble their training
data, with confidence scores that accurately reflect the probability of
correct predictions. Confidence drops indicate either distributional
shift, where input data no longer matches training distributions, or
model degradation from problematic local adaptations. Threshold-based
gating implements this validation mechanism by continuously monitoring
average prediction confidence and suspending adaptation when confidence
falls below baseline levels established during initial deployment. This
approach catches many failure modes without requiring labeled validation
data, though it cannot detect all performance issues since overconfident
but incorrect predictions can maintain high confidence scores.

Federated A/B testing enables validation of new adaptation strategies or
model architectures across distributed device populations. To validate
proposed changes, systems implement distributed experiments that
randomly assign devices to treatment and control groups while
maintaining statistical balance across device tiers and usage patterns.
Both groups collect federated metrics using privacy-preserving
aggregation protocols that prevent individual device data from being
exposed while enabling population-level comparisons. The system compares
adaptation success rates, measuring how frequently local adaptations
improve over baseline models, along with convergence speed that
indicates how quickly devices reach optimal performance, and final
performance metrics that reflect ultimate model quality after adaptation
completes. Successful strategies demonstrating clear improvements in
treatment groups are rolled out gradually across the device population,
starting with small percentages and expanding only after confirming that
benefits generalize beyond the experimental cohort.

These operational transformations necessitate new tooling and
infrastructure that systematically extends traditional MLOps practices.
The CI/CD pipelines, monitoring dashboards, A/B testing frameworks, and
incident response procedures established for centralized deployments
form the foundation for on-device learning operations. The federated
learning protocols presented in
Section~\ref{sec-edge-intelligence-federated-learning-6e7e} provide the
coordination mechanisms for distributed training, while
Section~\ref{sec-edge-intelligence-distributed-system-observability-2270}
addresses the observability gaps created by decentralized adaptation.

The shadow validation approach described above can be visualized as a
continuous comparison pipeline running on each device, where incoming
data flows through both a frozen baseline and the locally adapted model
before an arbiter decides whether to accept or roll back adaptations.

\begin{tcolorbox}[enhanced jigsaw, rightrule=.15mm, bottomtitle=1mm, opacitybacktitle=0.6, breakable, leftrule=.75mm, bottomrule=.15mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Figure: Shadow Mode Validation}, colframe=quarto-callout-note-color-frame, coltitle=black, opacityback=0, left=2mm, titlerule=0mm, toptitle=1mm, arc=.35mm, toprule=.15mm, colbacktitle=quarto-callout-note-color!10!white, colback=white]

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/b6715bc0d0f61de3544f12c3f4e73930336d959c.pdf}}

\textbf{On-Device Shadow Validation}. To detect model drift without
labels, a known-good ``Shadow Model'' runs in parallel with the ``Active
Model''. An on-device arbiter compares their predictions and confidence
scores. If the locally adapted model consistently shows lower confidence
than the frozen baseline, the system detects personalization drift and
can trigger an automatic rollback.

\end{tcolorbox}

Successful on-device learning deployments build upon proven MLOps
methodologies while adapting them to the unique challenges of
distributed, heterogeneous learning environments. This evolutionary
approach ensures operational reliability while enabling the benefits of
edge learning.

\subsection{Bio-Inspired Learning
Efficiency}\label{sec-edge-intelligence-bioinspired-learning-efficiency-55ad}

The operational patterns examined above, from opportunistic scheduling
to distributed validation, address immediate engineering requirements.
Yet these patterns gain deeper motivation from understanding how nature
solved similar problems. The human brain operates at just 20 watts while
continuously learning from limited supervision without catastrophic
forgetting. This remarkable efficiency, achieved through billions of
years of evolutionary optimization, provides both theoretical grounding
and practical inspiration for edge intelligence design.

Understanding biological solutions illuminates why certain engineering
approaches work. The sparse representations that enable efficient model
adaptation mirror how only 1-2\% of neurons activate during any
cognitive task. The experience replay buffers that stabilize learning
parallel how sleep consolidates memories through neural replay. The
federated coordination patterns that balance local and global learning
reflect how biological systems maintain individual adaptation while
benefiting from population-level evolution. These connections transform
production patterns from arbitrary design choices into principled
approaches grounded in proven optimization strategies.

\subsubsection{Learning from Biological Neural
Efficiency}\label{sec-edge-intelligence-learning-biological-neural-efficiency-74b5}

The human brain operates at approximately 20 watts while continuously
learning from limited supervision, precisely the efficiency target for
on-device learning systems\sidenote{\textbf{Brain Power Efficiency}: The
human brain's 20 W power consumption (equivalent to a bright LED bulb)
enables processing 10\^{}15 operations per second, roughly
50,000\(\times\) more efficient than current AI accelerators per
operation. This efficiency comes from \textasciitilde86 billion neurons
with only 1-2\% active simultaneously, massive parallelism with 10\^{}14
synapses, and adaptive precision where important computations use more
resources. Modern edge AI targets similar efficiency: sparse activation
patterns, adaptive precision (INT8 to FP16), and event-driven processing
that activates only when needed. }. This remarkable efficiency emerges
from several architectural principles that directly inform edge learning
design, demonstrating what is theoretically achievable with highly
optimized learning systems.

The brain's efficiency characteristics reveal multiple dimensions of
optimization that on-device systems should target. From a power
perspective, the brain consumes just 20 W total, with approximately 10 W
dedicated to active learning and memory consolidation, an energy budget
comparable to what mobile devices can sustainably allocate to on-device
learning during charging periods. Memory efficiency comes from sparse,
distributed representations where only 1-2\% of neurons activate
simultaneously during any cognitive task, dramatically reducing the
computational and storage requirements compared to dense neural
networks. Learning efficiency manifests through few-shot learning
capabilities that enable adaptation from single exposures, along with
continuous adaptation mechanisms that avoid catastrophic forgetting when
integrating new knowledge. Hierarchical processing organizes information
across multiple scales, from low-level sensory inputs to high-level
abstract reasoning, enabling efficient reuse of learned features across
different tasks and contexts.

Biological learning exhibits several features that on-device systems
must replicate to achieve similar efficiency. Sparse representations
ensure efficient use of limited neural resources, only a tiny fraction
of brain neurons fire during any cognitive task. This sparsity directly
parallels the selective parameter updates and pruned architectures
essential for mobile deployment. Event-driven processing minimizes
energy consumption by activating computation only when sensory input
changes, analogous to opportunistic training during device idle periods.

\subsubsection{Unlabeled Data Exploitation
Strategies}\label{sec-edge-intelligence-unlabeled-data-exploitation-strategies-cded}

Mobile devices continuously collect rich sensor streams ideal for
self-supervised learning: visual data from cameras, temporal patterns
from accelerometers, spatial patterns from GPS, and interaction patterns
from touchscreen usage. This abundant unlabeled data enables
sophisticated representation learning without external supervision.

The scale of sensor data generation on mobile devices creates
unprecedented opportunities for self-supervised learning. Visual streams
from cameras operating at 30 frames per second provide approximately 2.6
million frames daily, offering abundant data for contrastive learning
approaches that learn visual representations by comparing augmented
versions of the same image\sidenote{\textbf{Mobile Data Generation
Scale}: A typical smartphone generates \textasciitilde2-4 GB of sensor
data daily from cameras (1-2 GB), accelerometers (approximately 50 MB),
GPS traces (approximately 10 MB), and touch interactions (approximately
5 MB). This massive data stream offers unprecedented self-supervised
learning opportunities, modern contrastive learning can extract useful
representations from just 1\% of this data, making effective on-device
learning feasible without external labels or cloud processing. }. Motion
data from accelerometers sampling at 100 Hz generates 8.6 million data
points daily, capturing temporal patterns suitable for learning
representations of human activities and device movement. Location traces
from GPS sensors enable spatial representation learning and behavioral
prediction by capturing movement patterns and frequently visited
locations without requiring explicit labels. Interaction patterns from
touch events, typing dynamics, and app usage sequences create rich
behavioral embeddings that reveal user preferences and habits, enabling
personalized model adaptation without manual annotation.

Contrastive learning\sidenote{\textbf{Contrastive Learning}:
Self-supervised technique that learns representations by distinguishing
between similar (positive) and dissimilar (negative) pairs without
labels. The model learns embeddings where augmented versions of the same
image cluster together while different images separate. SimCLR (2020)
and MoCo achieve ImageNet accuracy within 1-5\% of supervised training
using only unlabeled data. For on-device learning, contrastive
pretraining on abundant sensor data creates transferable representations
that enable few-shot adaptation to downstream tasks, reducing labeled
data requirements by 10-100x. Audio streams from microphones enable
self-supervised speech representation learning through masking and
prediction tasks, where the model learns to predict masked portions of
audio spectrograms. Even device orientation and motion data can be used
for self-supervised pretraining of activity recognition models, learning
representations that capture the temporal structure of human movement
without requiring labeled activity annotations. } from temporal
correlations offers particularly promising opportunities for leveraging
this sensor data. Consecutive frames from mobile cameras naturally
provide positive pairs for visual representation learning, images
captured milliseconds apart typically show the same scene from slightly
different perspectives, while augmentation techniques such as color
jittering and random cropping create negative examples.

The biological inspiration extends to continual learning without
forgetting. Brains continuously integrate new experiences while
retaining decades of memories through mechanisms like synaptic
consolidation and replay. On-device systems must implement analogous
mechanisms: elastic weight consolidation\sidenote{\textbf{Elastic Weight
Consolidation (EWC)}: Continual learning technique inspired by synaptic
consolidation in the brain. EWC estimates which parameters are most
important for previously learned tasks (using Fisher Information Matrix)
and penalizes changes to those parameters during new learning. This
enables learning new tasks while preserving old knowledge with only
10-15\% additional computation per training step. On edge devices, EWC
requires storing per-parameter importance scores (4 bytes each), adding
\textasciitilde100\% memory overhead for weights but avoiding the need
to store entire replay buffers of past examples. }
(\citeproc{ref-kirkpatrick2017overcoming}{Kirkpatrick et al. 2017})
prevents catastrophic forgetting by protecting weights important for
previous tasks, experience replay maintains stability during adaptation
by interleaving new training with replayed examples from previous tasks,
and progressive neural architectures expand model capacity as new tasks
emerge rather than forcing all knowledge into fixed-capacity networks.

\subsubsection{Lifelong Adaptation Without
Forgetting}\label{sec-edge-intelligence-lifelong-adaptation-without-forgetting-9200}

Real-world on-device deployment demands continual adaptation to changing
environments, user behavior, and task requirements. This presents the
fundamental challenge of the stability-plasticity tradeoff: models must
remain stable enough to preserve existing knowledge while plastic enough
to learn new patterns.

Continual learning on edge devices faces several interconnected
challenges that compound the difficulty of distributed adaptation.
Catastrophic forgetting occurs when new learning overwrites previously
acquired knowledge, causing models to lose performance on earlier tasks
as they adapt to new ones, a particularly severe problem when devices
cannot access historical training data. Task interference emerges when
multiple learning objectives compete for limited model capacity, forcing
difficult tradeoffs between different capabilities that the model must
maintain simultaneously. Data distribution shift manifests as deployment
environments differ significantly from training conditions, requiring
models to adapt to new patterns while maintaining performance on the
original distribution. Resource constraints fundamentally limit the
available solutions, as limited memory prevents storing all historical
data for replay-based approaches that work well in centralized settings
but exceed edge device capabilities.

Meta-learning approaches address these challenges by learning learning
algorithms themselves rather than just learning specific tasks.
Model-Agnostic Meta-Learning (MAML) trains models to quickly adapt to
new tasks with minimal data, exactly the capability required for
personalized on-device adaptation where collecting large user-specific
datasets is impractical. Few-shot learning techniques enable rapid
specialization from small user-specific datasets, allowing models to
personalize based on just a handful of examples while maintaining
general capabilities learned during pretraining.

The theoretical foundation suggests that optimal on-device learning
systems will combine three elements. These are sparse representations,
self-supervised pretraining on sensor data, and meta-learning for rapid
adaptation. These principles directly influence practical system design.
Sparse model architectures reduce memory and compute requirements.
Self-supervised objectives utilize abundant unlabeled sensor data.
Meta-learning enables efficient personalization from limited user
interactions.

A key principle in building practical systems is to minimize the
adaptation footprint. Full-model fine-tuning is typically infeasible on
edge platforms, instead, localized update strategies, including
bias-only optimization, residual adapters, and lightweight task-specific
heads, should be prioritized. These approaches allow model
specialization under resource constraints while mitigating the risks of
overfitting or instability.

The feasibility of lightweight adaptation depends importantly on the
strength of offline pretraining
(\citeproc{ref-bommasani2021opportunities}{Bommasani et al. 2021}).
Pretrained models should encapsulate generalizable feature
representations that allow efficient adaptation from limited local data.
Shifting the burden of feature extraction to centralized training
reduces the complexity and energy cost of on-device updates, while
improving convergence stability in data-sparse environments.

Even when adaptation is lightweight, opportunistic scheduling remains
important to preserve system responsiveness and user experience. Local
updates should be deferred to periods when the device is idle, connected
to external power, and operating on a reliable network. Such policies
minimize the impact of background training on latency, battery
consumption, and thermal performance.

The sensitivity of local training artifacts necessitates careful data
security measures. Replay buffers, support sets, adaptation logs, and
model update metadata must be protected against unauthorized access or
tampering. Lightweight encryption or hardware-backed secure storage can
mitigate these risks without imposing prohibitive resource costs on edge
platforms.

However, security measures alone do not guarantee model robustness. As
models adapt locally, monitoring adaptation dynamics becomes important.
Lightweight validation techniques, including confidence scoring, drift
detection heuristics, and shadow model evaluation, can help identify
divergence early, enabling systems to trigger rollback mechanisms before
severe degradation occurs (\citeproc{ref-gama2014survey}{Gama et al.
2014}).

Robust rollback procedures depend on retaining trusted model
checkpoints. Every deployment should preserve a known-good baseline
version of the model that can be restored if adaptation leads to
unacceptable behavior. This principle is especially important in
safety-important and regulated domains, where failure recovery must be
provable and rapid.

In decentralized or federated learning contexts, communication
efficiency becomes a first-order design constraint. Compression
techniques such as quantized gradient updates, sparsified parameter
sets, and selective model transmission must be employed to allow
scalable coordination across large, heterogeneous fleets of devices
without overwhelming bandwidth or energy budgets
(\citeproc{ref-konevcny2016federated}{Konečný et al. 2016}).

When personalization is required, systems should aim for localized
adaptation wherever possible. Restricting updates to lightweight
components, including final classification heads or modular adapters,
constrains the risk of catastrophic forgetting, reduces memory overhead,
and accelerates adaptation without destabilizing core model
representations.

Finally, throughout the system lifecycle, privacy and compliance
requirements must be architected into adaptation pipelines. Mechanisms
to support user consent, data minimization, retention limits, and the
right to erasure must be considered core aspects of model design, not
post-hoc adjustments. Meeting regulatory obligations at scale demands
that on-device learning workflows align inherently with principles of
auditable autonomy.

Consider Figure~\ref{fig-odl-design-flow} for a systematic decision
framework: the flowchart guides practitioners through key decision
points about adaptation complexity, compute availability, and data
sharing requirements, mapping these choices to concrete implementation
strategies from bias-only updates to full federated learning with
privacy measures.

\begin{figure}[htb]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/9b8560f10e26452f3c75b8ec23d84946231faf97.pdf}}

}

\caption{\label{fig-odl-design-flow}This flowchart guides the systematic
development of practical on-device ML systems by outlining key decision
points related to data management, model selection, and privacy
considerations throughout the system lifecycle. Integrating privacy and
compliance requirements, such as user consent and data minimization,
into the design process ensures auditable autonomy and scalable
deployment of on-device intelligence.}

\end{figure}%

\section{Systems Integration for Production
Deployment}\label{sec-edge-intelligence-systems-integration-production-deployment-c6bb}

The theoretical foundations established above, including model
adaptation techniques, data efficiency strategies, and federated
coordination protocols, must now be synthesized into production-ready
systems. This section examines how the three pillars integrate under
real-world operational constraints, transforming individual techniques
into coherent system architectures.

Real-world on-device learning systems achieve effectiveness by
systematically combining all three solution pillars rather than relying
on isolated techniques. This integration requires careful systems
engineering to manage interactions, resolve conflicts, and optimize the
overall system performance within deployment constraints.

Consider a production voice assistant deployment across 50 million
heterogeneous devices. The system architecture demonstrates systematic
integration across three complementary layers that work together to
enable effective learning under diverse constraints.

The model adaptation layer stratifies techniques by device capability,
matching sophistication to available resources. Flagship phones
representing the top 20\% of the deployment use LoRA rank-32 adapters
that enable sophisticated voice pattern learning through
high-dimensional parameter updates. Mid-tier devices comprising 60\% of
the fleet employ rank-16 adapters that balance adaptation expressiveness
with the tighter memory constraints typical of mainstream smartphones.
Budget devices making up the remaining 20\% rely on bias-only updates
that stay comfortably within 1 GB memory limits while still enabling
basic personalization.

The data efficiency layer implements adaptive strategies across the
entire device population while respecting individual resource
constraints. All devices implement experience
replay\sidenote{\textbf{Experience Replay}: Technique borrowed from
reinforcement learning where past experiences (input-output pairs) are
stored in a buffer and periodically replayed during training. On edge
devices, this prevents catastrophic forgetting by interleaving new data
with stored examples from previous tasks. Memory-efficient
implementations store compressed embeddings (64-512 dimensions) rather
than raw inputs, reducing storage from MB to KB per example while
preserving gradient signal. Critical for continual learning where models
must adapt to new patterns without forgetting established behaviors. },
but with device-appropriate buffer sizes, 10 MB on budget devices versus
100 MB on flagship models, ensuring that memory-constrained devices can
still benefit from replay-based learning. Few-shot learning enables
rapid adaptation to new users within their first 10 interactions,
reducing the cold-start problem\sidenote{\textbf{Cold-Start Problem}:
The challenge that new users or devices have no historical data for
personalization, forcing models to rely on generic behavior until
sufficient local data accumulates. In recommendation systems, cold-start
reduces initial engagement by 30-50\% compared to personalized users.
Solutions include transferring learned representations from similar
users (collaborative filtering), using meta-learned initializations
(MAML), or starting with privacy-preserving population statistics. For
on-device learning, few-shot adaptation techniques can achieve
meaningful personalization from just 5-10 user interactions. } that
plagues systems requiring extensive training data. Streaming updates
accommodate continuous voice pattern evolution as users' speaking styles
naturally change over time or as they use the assistant in new acoustic
environments.

The federated coordination layer orchestrates privacy-preserving
collaboration across the device population. Devices participate in
federated training rounds opportunistically based on connectivity status
and battery level, ensuring that coordination does not degrade user
experience. LoRA adapters aggregate efficiently with just 50 MB per
update compared to 14 GB for full model synchronization, making
federated learning practical over mobile networks. Privacy-preserving
aggregation protocols ensure that individual voice patterns never leave
devices while still enabling population-scale improvements in accent
recognition and language understanding that benefit all users.

Effective systems integration requires adherence to key engineering
principles that ensure robust operation across heterogeneous device
populations:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Hierarchical Capability Matching}: Deploy more sophisticated
  techniques on capable devices while ensuring basic functionality
  across the device spectrum. Never assume uniform capabilities.
\item
  \textbf{Graceful Degradation}: Systems must operate effectively when
  individual components fail. Poor connectivity should not prevent local
  adaptation; low battery should trigger minimal adaptation modes.
\item
  \textbf{Conflict Resolution}: Model adaptation and data efficiency
  techniques can conflict (limited memory vs buffer size). Systematic
  resource allocation prevents these conflicts through predefined
  priority hierarchies.
\item
  \textbf{Performance Validation}: Integration creates emergent
  behaviors that individual techniques don't exhibit. Systems require
  comprehensive testing across device combinations and network
  conditions.
\end{enumerate}

This integrated approach transforms on-device learning from a collection
of techniques into a coherent systems capability that provides robust
personalization within real-world deployment constraints. A \emph{tiered
adaptation strategy} maps these techniques to device capabilities.

\begin{tcolorbox}[enhanced jigsaw, rightrule=.15mm, bottomtitle=1mm, opacitybacktitle=0.6, breakable, leftrule=.75mm, bottomrule=.15mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Figure: Tiered Adaptation Strategy}, colframe=quarto-callout-note-color-frame, coltitle=black, opacityback=0, left=2mm, titlerule=0mm, toptitle=1mm, arc=.35mm, toprule=.15mm, colbacktitle=quarto-callout-note-color!10!white, colback=white]

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/3476b21c361aad6b31cff92a6671626931c5a463.pdf}}

\end{tcolorbox}

\section{Persistent Technical and Operational
Challenges}\label{sec-edge-intelligence-persistent-technical-operational-challenges-8c12}

The solution techniques explored above, model adaptation, data
efficiency, and federated coordination, address many fundamental
constraints of on-device learning but also reveal persistent challenges
that emerge from their interaction in real-world deployments. These
challenges represent the current frontiers of on-device learning
research and highlight areas where the techniques discussed earlier
reach their limits or create new operational complexities. Understanding
these challenges provides critical context for evaluating when on-device
learning approaches are appropriate and where alternative strategies may
be necessary.

Unlike conventional centralized systems, where training occurs in
controlled environments with uniform hardware and curated datasets, edge
systems must contend with heterogeneity in devices, fragmentation in
data, and the absence of centralized validation infrastructure. These
factors give rise to new systems-level tradeoffs that test the
boundaries of the adaptation strategies, data efficiency methods, and
coordination mechanisms we have examined.

\subsection{Device and Data Heterogeneity
Management}\label{sec-edge-intelligence-device-data-heterogeneity-management-a789}

Federated and on-device ML systems must operate across a vast and
diverse ecosystem of devices, ranging from smartphones and wearables to
IoT sensors and microcontrollers. This heterogeneity spans multiple
dimensions: hardware capabilities, software stacks, network
connectivity, and power availability. Unlike cloud-based systems, where
environments can be standardized and controlled, edge deployments
encounter a wide distribution of system configurations and constraints.
These variations introduce significant complexity in algorithm design,
resource scheduling, and model deployment.

At the hardware level, devices differ in terms of memory capacity,
processor architecture (e.g., ARM Cortex-M
vs.~A-series)\sidenote{\textbf{ARM Cortex Architecture Spectrum}: The
ARM Cortex family spans 6 orders of magnitude in capabilities.
Cortex-M0+ (IoT sensors) runs at 48~MHz with 32 KB RAM and no
floating-point, consuming \textasciitilde10µW. Cortex-M7 (embedded
systems) reaches 400~MHz with 1 MB RAM and single-precision FPU,
consuming approximately 100 mW. Cortex-A78 (smartphones) delivers 3~GHz
performance with multi-core processing, NEON SIMD, and advanced branch
prediction, consuming 1-5 W. This diversity means federated learning
must adapt algorithms dynamically, quantized inference on M0+,
lightweight training on M7, and full backpropagation on A78. },
instruction set support (e.g., availability of SIMD or floating-point
units), and the presence or absence of AI accelerators. Some clients may
possess powerful NPUs capable of running small training loops, while
others may rely solely on low-frequency CPUs with minimal RAM. These
differences affect the feasible size of models, the choice of training
algorithm, and the frequency of updates.

Software heterogeneity compounds the challenge. Devices may run
different versions of operating systems, kernel-level drivers, and
runtime libraries. Some environments support optimized ML runtimes like
TensorFlow Lite\sidenote{\textbf{TensorFlow Lite}: Google's framework
for mobile and embedded ML inference, optimized for ARM processors and
mobile GPUs. TFLite reduces model size by 75\% through quantization and
pruning, while achieving \(3\times\) faster inference than full
TensorFlow. The framework supports 16-bit and 8-bit quantization, with
specialized kernels for mobile CPUs and GPUs. TFLite Micro targets
microcontrollers with \textless1 MB memory, enabling ML on Arduino and
other embedded platforms. } Micro or ONNX Runtime
Mobile\sidenote{\textbf{ONNX Runtime Mobile}: Microsoft's cross-platform
inference engine for the Open Neural Network Exchange format. ONNX
provides vendor-neutral model representation, enabling training in
PyTorch and deployment on iOS, Android, or embedded Linux without
framework-specific code. The mobile variant achieves 2-3x faster
inference than TensorFlow Lite on certain models through aggressive
operator fusion and platform-specific optimizations. Critical for
federated learning deployments where a single model must run identically
across heterogeneous devices with different ML frameworks. }, while
others rely on custom inference stacks or restricted APIs. These
discrepancies can lead to subtle inconsistencies in behavior, especially
when models are compiled differently or when floating-point precision
varies across platforms.

In addition to computational heterogeneity, devices exhibit variation in
connectivity and uptime. Some are intermittently connected, plugged in
only occasionally, or operate under strict bandwidth constraints. Others
may have continuous power and reliable networking, but still prioritize
user-facing responsiveness over background learning. These differences
complicate the orchestration of coordinated learning and the scheduling
of updates.

Finally, system fragmentation affects reproducibility and testing. With
such a wide range of execution environments, it is difficult to ensure
consistent model behavior or to debug failures reliably. This makes
monitoring, validation, and rollback mechanisms more important, but also
more difficult to implement uniformly across the fleet.

Consider a federated learning deployment for mobile keyboards. A
high-end smartphone might feature 8 GB of RAM, a dedicated AI
accelerator, and continuous Wi-Fi access. In contrast, a budget device
may have just 2 GB of RAM, no hardware acceleration, and rely on
intermittent mobile data. These disparities influence how long training
runs can proceed, how frequently models can be updated, and even whether
training is feasible at all. To support such a range, the system must
dynamically adjust training schedules, model formats, and compression
strategies, ensuring equitable model improvement across users while
respecting each device's limitations.

\subsection{Non-IID Data Distribution
Challenges}\label{sec-edge-intelligence-noniid-data-distribution-challenges-42cd}

In centralized machine learning, data can be aggregated, shuffled, and
curated to approximate independent and identically distributed (IID)
samples, a key assumption underlying many learning algorithms. On-device
and federated learning systems fundamentally challenge this assumption,
requiring algorithms that can handle highly fragmented and non-IID data
across diverse devices and contexts.

The statistical implications of this fragmentation create cascading
challenges throughout the learning process. Gradients computed on
different devices may conflict, slowing convergence or destabilizing
training. Local updates risk overfitting to individual client
idiosyncrasies, reducing performance when aggregated globally. The
diversity of data across clients also complicates evaluation, as no
single test set can represent the true deployment distribution.

These challenges necessitate robust algorithms that can handle
heterogeneity and imbalanced participation. Techniques such as
personalization layers, importance weighting, and adaptive aggregation
schemes provide partial solutions, but the optimal approach varies with
application context and the specific nature of data fragmentation.
Recall from Section~\ref{sec-edge-intelligence-data-constraints-303e}
that this statistical heterogeneity represents one of the core
challenges distinguishing on-device learning from traditional
centralized approaches, fundamentally reshaping assumptions about
gradient convergence and model generalization.

\subsection{Distributed System
Observability}\label{sec-edge-intelligence-distributed-system-observability-2270}

Monitoring and observability frameworks must be fundamentally reimagined
for distributed edge environments. Traditional centralized monitoring
approaches that rely on unified data collection and real-time visibility
become impractical when devices operate intermittently connected and
data cannot be centralized. The drift detection and performance
monitoring techniques established in MLOps provide conceptual
foundations, but require adaptation to handle the distributed,
privacy-preserving nature of on-device learning systems.

Unlike centralized machine learning systems, where model updates can be
continuously evaluated against held-out validation sets, on-device
learning introduces a core shift in visibility and observability. Once
deployed, models operate in highly diverse and often disconnected
environments, where internal updates may proceed without external
monitoring. This creates significant challenges for ensuring that model
adaptation is both beneficial and safe.

A core difficulty lies in the absence of centralized validation data. In
traditional workflows, models are trained and evaluated using curated
datasets that serve as proxies for deployment conditions. On-device
learners, by contrast, adapt in response to local inputs, which are
rarely labeled and may not be systematically collected. As a result, the
quality and direction of updates, whether they enhance generalization or
cause drift, are difficult to assess without interfering with the user
experience or violating privacy constraints.

The risk of model drift is especially pronounced in streaming settings,
where continual adaptation may cause a slow degradation in performance.
For instance, a voice recognition model that adapts too aggressively to
background noise may eventually overfit to transient acoustic
conditions, reducing accuracy on the target task. Without visibility
into the evolution of model parameters or outputs, such degradations can
remain undetected until they become severe.

Mitigating this problem requires mechanisms for on-device validation and
update gating. One approach is to interleave adaptation steps with
lightweight performance checks, using proxy objectives or
self-supervised signals to approximate model confidence
(\citeproc{ref-deng2021adaptive}{Y. Deng, Mokhtari, and Ozdaglar 2021}).
For example, a keyword spotting system might track detection confidence
across recent utterances and suspend updates if confidence consistently
drops below a threshold. Alternatively, shadow evaluation can be
employed, where multiple model variants are maintained on the device and
evaluated in parallel on incoming data streams, allowing the system to
compare the adapted model's behavior against a stable baseline.

Another strategy involves periodic checkpointing and rollback, where
snapshots of the model state are saved before adaptation. If subsequent
performance degrades, as determined by downstream metrics or user
feedback, the system can revert to a known good state. This approach has
been used in health monitoring devices, where incorrect predictions
could lead to user distrust or safety concerns. However, it introduces
storage and compute overhead, especially in memory-constrained
environments.

In some cases, federated validation offers a partial solution. Devices
can share anonymized model updates or summary statistics with a central
server, which aggregates them across users to identify global patterns
of drift or failure. While this preserves some degree of privacy, it
introduces communication overhead and may not capture rare or
user-specific failures.

Ultimately, update monitoring and validation in on-device learning
require a rethinking of traditional evaluation practices. Instead of
centralized test sets, systems must rely on implicit signals, runtime
feedback, and conservative adaptation policies to ensure robustness. The
absence of global observability is not merely a technical limitation, it
reflects a deeper systems challenge in aligning local adaptation with
global reliability.

\subsubsection{Performance Evaluation in Dynamic
Environments}\label{sec-edge-intelligence-performance-evaluation-dynamic-environments-82a9}

Systematic approaches for measuring ML system performance include
inference latency, throughput, energy efficiency, and accuracy metrics.
These benchmarking methodologies provide foundations for characterizing
model performance, but they were designed for static inference
workloads. On-device learning requires extending these metrics to
capture adaptation quality and training efficiency through
training-specific benchmarks.

Beyond traditional inference metrics, adaptive systems require
specialized training metrics that capture learning efficiency under edge
constraints. Adaptation efficiency measures accuracy improvement per
training sample consumed, quantified as the slope of the learning curve
under resource constraints, a system achieving 2\% accuracy gain per 100
training samples demonstrates higher adaptation efficiency than one
requiring 500 samples for the same improvement, directly translating to
faster personalization and reduced data collection requirements.
Memory-constrained convergence evaluates the validation loss achieved
within specified RAM budgets, such as ``convergence within 512 KB
training footprint,'' capturing how effectively systems learn given
fixed memory allocations, critical for comparing adaptation strategies
across device classes from microcontrollers to smartphones.
Energy-per-update quantifies millijoules consumed per gradient update, a
metric critical for battery-powered devices where training energy
directly impacts user experience, mobile devices typically budget
500-1000 mW for sustained ML workloads, translating to just 1.8-3.6
joules per hour of adaptation before noticeably affecting battery life.
Time-to-adaptation measures wall-clock time from receiving new data to
achieving measurable improvement, accounting for opportunistic
scheduling constraints that defer training to idle periods, this metric
captures real-world adaptation speed including waiting for device
idleness, charging status, and thermal headroom rather than just raw
computational throughput.

Evaluating whether local adaptation actually improves over global models
requires personalization gain metrics that justify the overhead of
on-device learning. Per-user performance delta measures accuracy
improvement for the adapted model versus the global baseline on
user-specific holdout data, systems should demonstrate statistically
significant improvements, typically exceeding 2\% accuracy gains, to
justify the computational overhead, energy consumption, and complexity
that adaptation introduces. Personalization-privacy tradeoff quantifies
accuracy gain per unit of local data exposure, measuring the value
extracted from privacy-sensitive information, this metric helps assess
whether adaptation benefits outweigh the privacy costs of retaining user
data locally, particularly important for applications handling sensitive
information like health data or personal communications. Catastrophic
forgetting rate measures degradation on the original task as the model
adapts to local distributions through retention testing, acceptable
forgetting rates depend on the application domain but typically should
remain below 5\% accuracy loss on original tasks to ensure that
personalization does not come at the expense of the model's general
capabilities.

When devices coordinate through the federated protocols examined in
Section~\ref{sec-edge-intelligence-federated-learning-6e7e}, federated
coordination cost metrics become critical for assessing system
viability. Communication efficiency measures model accuracy improvement
per byte transmitted, capturing the effectiveness of gradient
compression and selective update strategies, modern federated systems
achieve 10-100\(\times\) compression through quantization and
sparsification techniques while maintaining 95\% or more of uncompressed
accuracy, making the difference between practical and impractical mobile
deployment. Stragglers impact quantifies convergence delay caused by
slow or unreliable devices, measured as the difference in convergence
time with versus without participation filters, effective straggler
mitigation through asynchronous aggregation and selective participation
reduces convergence time by 30-50\% compared to synchronous approaches
that wait for all devices. Aggregation quality evaluates global model
performance as a function of device participation rate, revealing
minimum viable participation thresholds below which federated learning
fails to converge effectively, most federated systems require 10-20\%
device participation per round to maintain stable convergence,
establishing clear requirements for client selection and availability
management strategies.

These training-specific benchmarks complement inference metrics
including latency, throughput, and accuracy, creating complete
performance characterization for adaptive systems. Practical
benchmarking must measure both dimensions: a system that achieves fast
inference but slow adaptation, or efficient adaptation but poor final
accuracy, fails to meet real-world requirements. The integration of
inference and training benchmarks enables holistic evaluation of
on-device learning systems across their full operational lifecycle.

\subsection{Resource
Management}\label{sec-edge-intelligence-resource-management-691a}

On-device learning introduces resource contention modes absent in
conventional inference-only deployments. Many edge devices are
provisioned to run pretrained models efficiently but are rarely designed
with training workloads in mind. Local adaptation therefore competes for
scarce resources, including compute cycles, memory bandwidth, energy,
and thermal headroom, with other system processes and user-facing
applications.

The most direct constraint is compute availability. Training involves
additional forward and backward passes through the model, which can
exceed the cost of inference. Even when only a small subset of
parameters is updated, for instance, in bias-only or head-only
adaptation, backpropagation must still traverse the relevant layers,
triggering increased instruction counts and memory traffic. On devices
with shared compute units (e.g., mobile SoCs or embedded CPUs), this
demand can delay interactive tasks, reduce frame rates, or impair sensor
processing.

Energy consumption compounds this problem. Adaptation typically involves
sustained computation over multiple input samples, which taxes
battery-powered systems and may lead to rapid energy depletion. For
instance, performing a single epoch of adaptation on a
microcontroller-class device can consume several
millijoules\sidenote{\textbf{Microcontroller Power Budget Reality}: A
typical microcontroller consuming 10 mW during training exhausts 3.6
joules per hour, equivalent to a 1000~mAh battery in 2.8 hours. Energy
harvesting systems collect only 10-100 mW continuously (solar panels in
indoor light), making sustained training impossible. Real deployments
use duty cycling: train for 10 seconds every hour, consuming
\textasciitilde1 joule total. This constrains training to 100-1000
gradient steps maximum, requiring extremely efficient algorithms and
careful energy budgeting between sensing, computation, and
communication. }, an appreciable fraction of the energy budget for a
duty-cycled system operating on harvested power. This necessitates
careful scheduling, such that learning occurs only during idle periods,
when energy reserves are high and user latency constraints are relaxed.

From a memory perspective, training incurs higher peak usage than
inference, due to the need to cache intermediate
activations\sidenote{\textbf{Activation Caching}: During
backpropagation, forward pass activations must be stored to compute
gradients, dramatically increasing memory usage. For a typical CNN,
activation memory can be 3-5\(\times\) larger than model weights. Modern
techniques like gradient checkpointing trade computation for memory by
recomputing activations during backward pass, reducing memory by 80\% at
the cost of \textasciitilde30\% more compute time. Critical for training
on memory-constrained devices where activation storage often exceeds
available RAM. These requirements may exceed the static memory footprint
anticipated during model deployment, particularly when adaptation
involves multiple layers or gradient accumulation. In highly constrained
systems, for example, systems with less than 512 KB of RAM, this may
preclude certain types of adaptation altogether, unless additional
optimization techniques (e.g., checkpointing or low-rank updates) are
employed. }, gradients, and optimizer state
(\citeproc{ref-lin2020mcunet}{Lin et al. 2020}).

These resource demands must also be balanced against quality of service
(QoS)\sidenote{\textbf{Quality of Service (QoS) for Edge ML}: System
guarantees for latency, throughput, and reliability that on-device
learning must not violate. Typical QoS budgets include: voice assistants
require \textless300ms response time (users perceive \textgreater400ms
as laggy), video apps need 16--33 ms frame processing (30-60 fps), and
wearables must maintain \textless100ms touch response. Background
training that causes even 10\% latency increase triggers user
complaints. Systems implement priority scheduling where inference always
preempts training, and resource governors throttle learning when QoS
metrics approach thresholds. These system reliability concerns parallel
operational challenges in production ML deployment. As such, many
systems adopt opportunistic learning policies, where adaptation is
suspended during foreground activity and resumed only when system load
is low. } goals. Users expect edge devices to respond reliably and
consistently, regardless of whether learning is occurring in the
background. Any observable degradation, including dropped audio in a
wake-word detector or lag in a wearable display, can erode user trust.

In some deployments, adaptation is further gated by cost constraints
imposed by networked infrastructure. For instance, devices may offload
portions of the learning workload to nearby gateways or
cloudlets\sidenote{\textbf{Cloudlet}: Small-scale datacenter positioned
at the network edge, typically co-located with cellular base stations or
enterprise networks. Unlike cloud servers (50--200 ms away), cloudlets
provide 1--10 ms latency access to compute resources, enabling hybrid
architectures where time-sensitive inference runs on-device while
training offloads to nearby cloudlets. A typical cloudlet comprises 1-4
server-class machines with GPUs, serving 100-1000 connected devices
within a geographic area. This intermediate tier enables more
sophisticated on-device learning by providing nearby compute for
gradient aggregation or model updates without cloud round-trips. These
hybrid models raise additional questions of task placement and
scheduling: should the update occur locally, or be deferred until a
high-throughput link is available? }, introducing bandwidth and
communication trade-offs.

In summary, the cost of on-device learning is not solely measured in
FLOPs or memory usage. It manifests as a complex interplay of system
load, user experience, energy availability, and infrastructure capacity.
Addressing these challenges requires co-design across algorithmic,
runtime, and hardware layers, ensuring that adaptation remains
unobtrusive, efficient, and sustainable under real-world constraints.

\subsection{Identifying and Preventing System
Failures}\label{sec-edge-intelligence-identifying-preventing-system-failures-ee88}

Understanding potential failure modes in on-device learning helps
prevent costly deployment mistakes. Based on documented challenges in
federated learning research (\citeproc{ref-kairouz2021advances}{Kairouz
and McMahan 2021}) and known risks in adaptive systems, several
categories of failures warrant careful consideration.

The most fundamental risk in on-device learning is unbounded adaptation
drift, where continuous learning without constraints causes models to
gradually diverge from their intended behavior. Consider a hypothetical
keyboard prediction system that learns from all user inputs including
corrections, it might begin incorporating typos as valid suggestions,
leading to progressively degraded predictions. This risk becomes acute
in health monitoring applications where gradual changes in user
baselines could be learned as ``normal,'' potentially causing the system
to miss important anomalies that would have been detected by a static
model. The insidious nature of this drift is that it occurs slowly and
locally, making detection difficult without proper monitoring
infrastructure.

Beyond individual device drift, federated learning systems face the
challenge of participation bias amplification at the population level.
Devices with reliable power and connectivity participate more frequently
in federated rounds (\citeproc{ref-li2020federated}{Li et al. 2020}).
This uneven participation creates scenarios where models become
increasingly optimized for users with high-end devices while performance
degrades for those with limited resources. The resulting feedback loop
exacerbates digital inequality: better-served users receive increasingly
better models, while underserved populations experience declining
performance, reducing their engagement and further diminishing their
representation in training rounds (\citeproc{ref-wang2021field}{J. Wang
et al. 2021}). These fairness and bias amplification concerns highlight
the ethical implications of distributed learning systems.

These systematic biases interact with data quality issues to create
autocorrection feedback loops, particularly in text-based applications.
When systems cannot distinguish between intended inputs and corrections,
they may develop unexpected behaviors. Frequently corrected
domain-specific terminology might be incorrectly learned as errors,
leading to inappropriate suggestions in professional contexts. This
problem compounds the drift issue: not only do models adapt to
individual quirks, but they may also learn from their own mistakes when
users accept autocorrections without realizing the system is learning
from these interactions.

The interconnected nature of these failure modes, from individual drift
to population bias to data quality degradation, underscores the
importance of implementing comprehensive safety mechanisms. Successful
deployments require bounded adaptation ranges to prevent unbounded
drift, stratified sampling to address participation bias, careful data
filtering to avoid learning from corrections as ground truth, and shadow
evaluation against static baselines to detect degradation. While
specific production incidents are rarely publicized due to competitive
and privacy concerns, the research community has identified these
patterns as critical areas requiring systematic mitigation strategies
(\citeproc{ref-li2020federated}{Li et al. 2020};
\citeproc{ref-kairouz2021advances}{Kairouz and McMahan 2021}).

\subsection{Production Deployment Risk
Assessment}\label{sec-edge-intelligence-production-deployment-risk-assessment-db49}

The deployment of adaptive models on edge devices introduces challenges
that extend beyond technical feasibility. In domains where compliance,
auditability, and regulatory approval are necessary, including
healthcare, finance, and safety-important systems, on-device learning
poses a core tension between system autonomy and control.

In traditional machine learning pipelines, all model updates are
centrally managed, versioned, and validated. The training data, model
checkpoints, and evaluation metrics are typically recorded in
reproducible workflows that support traceability. When learning occurs
on the device itself, however, this visibility is lost. Each device may
independently evolve its model parameters, influenced by unique local
data streams that are never observed by the developer or system
maintainer.

This autonomy creates a validation gap. Without access to the input data
or the exact update trajectory, it becomes difficult to verify that the
learned model still adheres to its original specification or performance
guarantees. This is especially problematic in regulated industries,
where certification depends on demonstrating that a system behaves
consistently across defined operational boundaries. A device that
updates itself in response to real-world usage may drift outside those
bounds, triggering compliance violations without any external signal.

The lack of centralized oversight complicates rollback and failure
recovery. If a model update degrades performance, it may not be
immediately detectable, particularly in offline scenarios or systems
without telemetry. By the time failure is observed, the system's
internal state may have diverged significantly from any known
checkpoint, making diagnosis and recovery more complex than in static
deployments. This necessitates robust safety mechanisms, such as
conservative update thresholds, rollback caches, or dual-model
architectures that retain a verified baseline.

In addition to compliance challenges, on-device learning introduces new
security vulnerabilities. Because model adaptation occurs locally and
relies on device-specific, potentially untrusted data streams,
adversaries may attempt to manipulate the learning process by tampering
with stored data, such as replay buffers, or by injecting poisoned
examples during adaptation, to degrade model performance or introduce
vulnerabilities. Any locally stored adaptation data, such as feature
embeddings or few-shot examples, must be secured against unauthorized
access to prevent unintended information leakage.

Maintaining model integrity over time is particularly difficult in
decentralized settings, where central monitoring and validation are
limited. Autonomous updates could, without external visibility, cause
models to drift into unsafe or biased states. These risks are compounded
by compliance obligations such as the GDPR's right to erasure: if user
data subtly influences a model through adaptation, tracking and
reversing that influence becomes complex.

The security and integrity of self-adapting models, particularly at the
edge, pose important open challenges. A comprehensive treatment of these
threats and corresponding mitigation strategies requires specialized
security frameworks for distributed ML systems.

Privacy regulations also interact with on-device learning in nontrivial
ways. While local adaptation can reduce the need to transmit sensitive
data, it may still require storage and processing of personal
information, including sensor traces or behavioral logs, on the device
itself. These privacy considerations require careful attention to
security frameworks and regulatory compliance. Depending on
jurisdiction, this may invoke additional requirements for data
retention, user consent, and auditability. Systems must be designed to
satisfy these requirements without compromising adaptation
effectiveness, which often involves encrypting stored data, enforcing
retention limits, or implementing user-controlled reset mechanisms.

Lastly, the emergence of edge learning raises open questions about
accountability and liability
(\citeproc{ref-brakerski2022federated}{Brakerski et al. 2022}). When a
model adapts autonomously, who is responsible for its behavior? If an
adapted model makes a faulty decision, such as misdiagnosing a health
condition or misinterpreting a voice command, the root cause may lie in
local data drift, poor initialization, or insufficient safeguards.
Without standardized mechanisms for capturing and analyzing these
failure modes, responsibility may be difficult to assign, and regulatory
approval harder to obtain.

Addressing these deployment and compliance risks requires new tooling,
protocols, and design practices that support auditable autonomy, the
ability of a system to adapt in place while still satisfying external
requirements for traceability, reproducibility, and user protection. As
on-device learning becomes more prevalent, these challenges will become
central to both system architecture and governance frameworks.

\subsection{Engineering Challenge
Synthesis}\label{sec-edge-intelligence-engineering-challenge-synthesis-92d4}

The deployment risks, failure modes, and compliance concerns explored in
the preceding sections compound the technical challenges discussed
throughout this chapter. These interconnected issues, from drift and
bias amplification to validation gaps and accountability questions,
represent the full landscape of challenges that on-device learning
systems must navigate. Designing effective systems requires
understanding how these challenges interact across hardware
heterogeneity, data fragmentation, observability limitations, and
regulatory compliance requirements.

System heterogeneity complicates deployment and optimization by
introducing variation in compute, memory, and runtime environments.
Non-IID data distributions challenge learning stability and
generalization, especially when models are trained on-device without
access to global context. The absence of centralized monitoring makes it
difficult to validate updates or detect performance regressions, and
training activity must often compete with core device functionality for
energy and compute. Finally, post-deployment learning introduces
complications in model governance, from auditability and rollback to
privacy assurance.

These challenges are not isolated, they interact in ways that influence
the viability of different adaptation strategies.
Table~\ref{tbl-ondevice-challenges} synthesizes these interconnected
issues, mapping each challenge category to its root cause and
system-level implications for on-device learning deployments.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3333}}@{}}
\caption{\textbf{On-Device Learning Challenges}: System heterogeneity,
non-IID data, and limited resources introduce unique challenges for
deploying and adapting machine learning models on edge devices,
impacting portability, stability, and governance. The table details root
causes of these challenges and their system-level implications,
highlighting trade-offs between model performance and resource
constraints.}\label{tbl-ondevice-challenges}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Challenge}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Root Cause}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{System-Level Implications}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Challenge}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Root Cause}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{System-Level Implications}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{System Heterogeneity} & Diverse hardware, software, and
toolchains & Limits portability; requires platform-specific tuning \\
\textbf{Non-IID and Fragmented Data} & Localized, user-specific data
distributions & Hinders generalization; increases risk of drift \\
\textbf{Limited Observability and Feedback} & No centralized testing or
logging & Makes update validation and debugging difficult \\
\textbf{Resource Contention and Scheduling} & Competing demands for
memory, compute, and battery & Requires dynamic scheduling and
budget-aware learning \\
\textbf{Deployment and Compliance Risk} & Learning continues
post-deployment & Complicates model versioning, auditing, and
rollback \\
\end{longtable}

\subsection{Foundations for Robust AI
Systems}\label{sec-edge-intelligence-foundations-robust-ai-systems-3fde}

The operational challenges and failure modes explored in the preceding
sections reveal vulnerabilities that extend beyond deployment concerns
into fundamental system reliability. When models adapt autonomously
across millions of heterogeneous devices, three categories of threats
emerge that traditional centralized training never encounters.

First, unlike centralized systems where failures are localized and
observable, on-device learning creates scenarios where local failures
can propagate silently across device populations. A corrupted adaptation
on one device, if aggregated through federated learning, can poison the
global model. Hardware faults that would trigger errors in centralized
infrastructure may silently corrupt gradients on edge devices with
minimal error detection capabilities.

Second, the federated coordination mechanisms that enable collaborative
learning also create new attack surfaces. Adversarial clients can inject
poisoned gradients\sidenote{\textbf{Byzantine Fault Tolerance in FL}:
Distributed systems property that enables correct operation despite some
participants being malicious or faulty (named after the Byzantine
Generals Problem). In federated learning, up to f malicious clients can
be tolerated among n participants using algorithms like Krum or trimmed
mean aggregation, which requires n ≥ 3f + 1 total participants. These
robust aggregation methods increase communication costs by 2-5\(\times\)
and computational overhead by 3-10\(\times\), but prevent poisoning
attacks where malicious clients could degrade global model performance
by injecting adversarial gradients. } designed to degrade global model
performance. Model inversion attacks can extract private information
from shared updates despite aggregation. The distributed nature of
on-device learning makes these attacks both easier to execute
(compromising client devices) and harder to detect (no centralized
validation).

Third, on-device systems must handle distribution shifts and
environmental changes without access to labeled validation data. Models
may confidently drift into failure modes, adapting to local biases or
temporary anomalies. The non-IID data distributions across devices mean
that local drift on individual devices may not trigger global alarms,
allowing silent degradation.

These reliability threats demand systematic approaches that ensure
on-device learning systems remain robust despite autonomous adaptation,
malicious manipulation, and environmental uncertainty.
\textbf{?@sec-robust-ai} examines these challenges comprehensively,
establishing principles for fault-tolerant AI systems that can maintain
reliability despite hardware faults, adversarial attacks, and
distribution shifts. The techniques developed there, Byzantine-resilient
aggregation, adversarial training, and drift detection, become essential
components of production-ready on-device learning systems rather than
optional enhancements.

The privacy-preserving aspects of these robustness mechanisms, including
secure aggregation and differential privacy, connect directly to
\textbf{?@sec-security-privacy}, which establishes the cryptographic
foundations and privacy guarantees necessary for deploying self-learning
systems at scale while maintaining user trust and regulatory compliance.

\section{Fallacies and
Pitfalls}\label{sec-edge-intelligence-fallacies-pitfalls-6c6d}

On-device learning operates in a fundamentally different environment
from cloud-based training, with severe resource constraints and privacy
requirements that challenge traditional machine learning assumptions.
The appeal of local adaptation and privacy preservation can obscure the
significant technical limitations and implementation challenges that
determine whether on-device learning provides net benefits over simpler
alternatives.

\textbf{Fallacy:} \textbf{\emph{On-device learning provides the same
adaptation capabilities as cloud-based training.}}

Teams expect local learning to match centralized training's model
improvements, ignoring fundamental resource constraints. On-device
learning amplifies resource needs by 3-10× compared to inference-only
deployment due to activation caching, gradient storage, and
bidirectional memory traffic
(Section~\ref{sec-edge-intelligence-model-adaptation-6a82}). Local
datasets are typically small, biased, and non-representative, while
compute budgets remain orders of magnitude below cloud GPUs. A
smartphone with 8GB RAM and 5W power budget cannot replicate the
adaptation achieved by datacenter systems with terabytes of memory and
kilowatts of power. Effective on-device learning requires designing
adaptation strategies that provide meaningful improvements within these
constraints rather than attempting to replicate cloud-scale learning
capabilities.

\textbf{Pitfall:} \textbf{\emph{Assuming that federated learning
automatically preserves privacy without additional safeguards.}}

Practitioners believe that keeping data on local devices inherently
provides privacy protection, ignoring what can be inferred from model
updates. Gradient and parameter updates leak significant information
about local training data through various inference attacks, while
device participation patterns reveal sensitive information about users
and activities. True privacy preservation requires additional
mechanisms: differential privacy provides mathematical guarantees that
individual data points cannot be inferred, while secure aggregation
protocols prevent parameter inspection during coordination (as
Figure~\ref{fig-secure-agg} illustrates with cryptographic masking).
Data locality alone is insufficient for privacy protection.

\textbf{Fallacy:} \textbf{\emph{Resource-constrained adaptation always
produces better personalized models than generic models.}}

This belief assumes that any local adaptation is beneficial regardless
of the quality or quantity of local data available. On-device learning
with insufficient, noisy, or biased local data can actually degrade
model performance compared to well-trained generic models
(Section~\ref{sec-edge-intelligence-data-efficiency-c701}). Small
datasets may not provide enough signal for meaningful learning, while
adaptation to local noise can harm generalization. Effective on-device
learning systems must include mechanisms to detect when local adaptation
is beneficial and fall back to generic models when local data is
inadequate for reliable learning.

\textbf{Pitfall:} \textbf{\emph{Ignoring the heterogeneity challenges
across different device types and capabilities.}}

Teams design on-device learning systems assuming uniform hardware
capabilities across deployment devices. Real-world deployments span
diverse hardware with varying computational power, memory capacity,
energy constraints, and networking capabilities. Edge device
capabilities span 6+ orders of magnitude: from 32KB RAM microcontrollers
to 16GB smartphones, 48MHz ARM Cortex-M0+ (\textasciitilde10 MIPS) to
3GHz A-series processors (\textasciitilde100,000 MIPS), and 10μW sensor
nodes to 5W flagship phones. A learning algorithm that works well on
high-end smartphones may fail catastrophically on resource-constrained
IoT devices. Federated learning algorithms must dynamically adapt
through quantized inference on low-end devices, selective participation
based on capability, and tiered aggregation strategies that account for
10,000× performance differences within a single deployment.

\textbf{Pitfall:} \textbf{\emph{Underestimating the complexity of
orchestrating learning across distributed edge systems.}}

Many teams focus on individual device optimization without considering
the system-level challenges of coordinating learning across thousands or
millions of edge devices. Edge systems orchestration
(Section~\ref{sec-edge-intelligence-federated-learning-6e7e}) must
handle intermittent connectivity, varying power states, different time
zones, and unpredictable device availability patterns that create
complex scheduling and synchronization challenges. Device clustering,
federated rounds coordination, model versioning across diverse
deployment contexts, and handling partial participation from unreliable
devices require sophisticated infrastructure beyond simple aggregation
servers. Additionally, real-world edge deployments involve multiple
stakeholders with different incentives, security requirements, and
operational procedures that must be balanced against learning
objectives. Effective edge learning systems require robust orchestration
frameworks that can maintain system coherence despite constant device
churn, network partitions, and operational disruptions.

\section{Summary}\label{sec-edge-intelligence-summary-0af9}

Edge Intelligence represents the ``physics-limited frontier'' of the
Machine Learning Fleet. Throughout this volume, we have transitioned
from logical algorithms (Part I) to datacenter-scale machines (Part II)
and global cloud services (\textbf{?@sec-inference-scale}). This chapter
has explored what happens when those same principles are pushed to their
extreme physical limits: sensors with microwatt power budgets,
microcontrollers with kilobyte memory, and smartphones facing the
uncompromising ``Mobile Memory Wall.''

We established a three-pillar framework for navigating these
constraints: \textbf{Model Adaptation} (reducing the update footprint
via TinyTL or LoRA), \textbf{Data Efficiency} (learning from minimal
local samples via few-shot or streaming methods), and \textbf{Federated
Coordination} (sharing insights without sharing raw data). Success at
the edge requires more than algorithmic cleverness; it demands a deep
co-design between the software's mathematical requirements and the
hardware's thermal, energy, and bandwidth realities. The following key
takeaways summarize the essential principles.

\phantomsection\label{callout-takeawaysux2a-1.5}
\begin{fbxSimple}{callout-takeaways}{Takeaways:}{Key Takeaways}
\phantomsection\label{callout-takeaways*-1.5}

\begin{itemize}
\tightlist
\item
  \textbf{The Mobile Memory Wall}: While NPUs provide raw compute
  (TOPS), on-device LLMs are strictly bandwidth-bound. The
  30--50\(\times\) gap between mobile RAM and datacenter HBM makes
  quantization a mandatory ``bandwidth survival strategy'' for
  interactive decode speeds.
\item
  \textbf{Training is Not Inference}: On-device learning amplifies
  resource needs by 3--10\(\times\) compared to inference-only
  deployment, due to activation caching, gradient storage, and
  bidirectional memory traffic.
\item
  \textbf{The Three Pillars}: Resource-constrained learning depends on
  Model Adaptation (bias-only/sparse updates), Data Efficiency
  (few-shot/experience replay), and Federated Coordination
  (privacy-preserving aggregation).
\item
  \textbf{Heterogeneity is the Default}: Unlike uniform datacenter
  racks, the edge is a fragmented ecosystem of microcontrollers and
  SoCs. Federated learning must account for ``stragglers'' and
  participation bias generated by these physical disparities.
\end{itemize}

\end{fbxSimple}

Part III has now examined deployment at scale across the full spectrum:
from centralized inference systems serving millions of requests per
second (\textbf{?@sec-inference-scale}) to the advanced serving
optimization techniques covered there and the massively distributed edge
fleets explored here.

However, deploying a model, whether to a GPU cluster or a billion
smartphones, is only the beginning. Scaling these services creates
operational complexity that grows exponentially with every model
version, device type, and deployment region. How do we ensure a model
update does not silently fail on 10\% of budget Android phones? How do
we monitor drift across a million private, local datasets?

The final chapter of this part, \textbf{?@sec-ops-scale}, addresses the
\textbf{Management Layer}: the MLOps practices, CI/CD pipelines, and
observability frameworks required to sustain the Machine Learning Fleet
at production scale.

\phantomsection\label{refs}
\begin{CSLReferences}{1}{0}
\bibitem[\citeproctext]{ref-arivazhagan2019federated}
Arivazhagan, Manoj Ghuhan, Vinay Aggarwal, Aaditya Kumar Singh, and
Sunav Choudhary. 2019. {``Federated Learning with Personalization
Layers.''} \emph{CoRR} abs/1912.00818 (December).
\url{http://arxiv.org/abs/1912.00818v1}.

\bibitem[\citeproctext]{ref-banbury2021mlperf}
Banbury, Colby, Vijay Janapa Reddi, Peter Torelli, Jeremy Holleman, Nat
Jeffries, Csaba Kiraly, Pietro Montino, et al. 2021. {``MLPerf Tiny
Benchmark.''} \emph{arXiv Preprint arXiv:2106.07597}, June.
\url{http://arxiv.org/abs/2106.07597v4}.

\bibitem[\citeproctext]{ref-bommasani2021opportunities}
Bommasani, Rishi, Drew A. Hudson, Ehsan Adeli, Russ Altman, Simran
Arora, Sydney von Arx, Michael S. Bernstein, et al. 2021. {``On the
Opportunities and Risks of Foundation Models.''} \emph{arXiv Preprint
arXiv:2108.07258}, August. \url{http://arxiv.org/abs/2108.07258v3}.

\bibitem[\citeproctext]{ref-bonawitz2019towards}
Bonawitz, Keith, Hubert Eichner, Wolfgang Grieskamp, Dzmitry Huba, Alex
Ingerman, Vladimir Ivanov, Chloe Kiddon, et al. 2019. {``Towards
Federated Learning at Scale: System Design,''} February.
\url{http://arxiv.org/abs/1902.01046v2}.

\bibitem[\citeproctext]{ref-brakerski2022federated}
Brakerski, Zvika et al. 2022. {``Federated Learning and the Rise of Edge
Intelligence: Challenges and Opportunities.''} \emph{Communications of
the ACM} 65 (8): 54--63.

\bibitem[\citeproctext]{ref-cai2020tinytl}
Cai, Han, Chuang Gan, Ligeng Zhu, and Song Han. 2020. {``TinyTL: Reduce
Activations, Not Trainable Parameters for Efficient on-Device
Learning''} 33 (July): 11285--97.
\url{http://arxiv.org/abs/2007.11622v5}.

\bibitem[\citeproctext]{ref-chen2016training}
Chen, Tianqi, Bing Xu, Chiyuan Zhang, and Carlos Guestrin. 2016.
{``Training Deep Nets with Sublinear Memory Cost.''} \emph{arXiv
Preprint arXiv:1604.06174}. \url{https://arxiv.org/abs/1604.06174}.

\bibitem[\citeproctext]{ref-chollet2017xception}
Chollet, Francois. 2017. {``Xception: Deep Learning with Depthwise
Separable Convolutions.''} In \emph{2017 IEEE Conference on Computer
Vision and Pattern Recognition (CVPR)}, 1800--1807. IEEE.
\url{https://doi.org/10.1109/cvpr.2017.195}.

\bibitem[\citeproctext]{ref-dean2012large}
Dean, Jeffrey, Greg Corrado, Rajat Monga, Kai Chen 0010, Matthieu Devin,
Quoc V. Le, Mark Z. Mao, et al. 2012. {``Large Scale Distributed Deep
Networks.''} In \emph{Advances in Neural Information Processing
Systems}, 25:1232--40.
\url{https://proceedings.neurips.cc/paper/2012/hash/6aca97005c68f1206823815f66102863-Abstract.html}.

\bibitem[\citeproctext]{ref-deng2022tinytrain}
Deng, Chulin, Yujun Zhang, and Yanzhi Wu. 2022. {``TinyTrain: Learning
to Train Compact Neural Networks on the Edge.''} In \emph{Proceedings of
the 39th International Conference on Machine Learning (ICML)}.

\bibitem[\citeproctext]{ref-deng2021adaptive}
Deng, Yuzhe, Aryan Mokhtari, and Asuman Ozdaglar. 2021. {``Adaptive
Federated Optimization.''} In \emph{Proceedings of the 38th
International Conference on Machine Learning (ICML)}.

\bibitem[\citeproctext]{ref-diao2023sparse}
Diao, Enmao, Jie Ding, and Vahid Tarokh. 2023. {``Pruning and Sparse
Training for on-Device Neural Network Optimization.''} \emph{IEEE
Transactions on Mobile Computing} 22 (8): 4567--80.
\url{https://doi.org/10.1109/TMC.2022.3180000}.

\bibitem[\citeproctext]{ref-gdpr2016regulation}
European Parliament and Council of the European Union. 2016.
{``Regulation ({EU}) 2016/679 of the {European Parliament} and of the
{Council}.''} Official Journal of the European Union, L 119.
\url{https://eur-lex.europa.eu/eli/reg/2016/679/oj}.

\bibitem[\citeproctext]{ref-finn2017model}
Finn, Chelsea, Pieter Abbeel, and Sergey Levine. 2017. {``Model-Agnostic
Meta-Learning for Fast Adaptation of Deep Networks.''} In
\emph{Proceedings of the 34th International Conference on Machine
Learning (ICML)}.

\bibitem[\citeproctext]{ref-gama2014survey}
Gama, João, Indrė Žliobaitė, Albert Bifet, Mykola Pechenizkiy, and
Abdelhamid Bouchachia. 2014. {``A Survey on Concept Drift Adaptation.''}
\emph{ACM Computing Surveys} 46 (4): 1--37.
\url{https://doi.org/10.1145/2523813}.

\bibitem[\citeproctext]{ref-hard2018federated}
Hard, Andrew, Kanishka Rao, Rajiv Mathews, Swaroop Ramaswamy, Françoise
Beaufays, Sean Augenstein, Hubert Eichner, Chloé Kiddon, and Daniel
Ramage. 2018. {``Federated Learning for Mobile Keyboard Prediction.''}
\emph{arXiv Preprint arXiv:1811.03604}, November.
\url{http://arxiv.org/abs/1811.03604v2}.

\bibitem[\citeproctext]{ref-hayes2020remind}
Hayes, Tyler L., Kushal Kafle, Robik Shrestha, Manoj Acharya, and
Christopher Kanan. 2020. {``REMIND Your Neural Network to Prevent
Catastrophic Forgetting.''} In \emph{Computer Vision -- ECCV 2020},
466--83. Springer International Publishing.
\url{https://doi.org/10.1007/978-3-030-58598-3/_28}.

\bibitem[\citeproctext]{ref-he2016deep}
He, Kaiming, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2015. {``Deep
Residual Learning for Image Recognition,''} December, 770--78.
\url{https://doi.org/10.1109/CVPR.2016.90}.

\bibitem[\citeproctext]{ref-houlsby2019parameter}
Houlsby, Neil, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone,
Chloé de Laroussilhe, Andrea Gesmundo, Mohammad Attariyan, and Sylvain
Gelly. 2019. {``Parameter-Efficient Transfer Learning for NLP.''} In
\emph{International Conference on Machine Learning}, 2790--99. PMLR.

\bibitem[\citeproctext]{ref-howard2017mobilenets}
Howard, Andrew G., Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun
Wang, Tobias Weyand, Marco Andreetto, and Hartwig Adam. 2017.
{``MobileNets: Efficient Convolutional Neural Networks for Mobile Vision
Applications.''} \emph{CoRR} abs/1704.04861 (April).
\url{http://arxiv.org/abs/1704.04861v1}.

\bibitem[\citeproctext]{ref-hu2021lora}
Hu, Edward J., Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi
Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. {``LoRA: Low-Rank
Adaptation of Large Language Models.''} \emph{arXiv Preprint
arXiv:2106.09685}, June. \url{http://arxiv.org/abs/2106.09685v2}.

\bibitem[\citeproctext]{ref-iandola2016squeezenet}
Iandola, Forrest N., Song Han, Matthew W. Moskewicz, Khalid Ashraf,
William J. Dally, and Kurt Keutzer. 2016. {``SqueezeNet: AlexNet-Level
Accuracy with 50x Fewer Parameters and \textless0.5MB Model Size,''}
February. \url{http://arxiv.org/abs/1602.07360v4}.

\bibitem[\citeproctext]{ref-kairouz2021advances}
Kairouz, Peter, and H. Brendan McMahan. 2021. {``Advances and Open
Problems in Federated Learning.''} \emph{Foundations and Trends in
Machine Learning} 14 (1-2): 1--210.
\url{https://doi.org/10.1561/2200000083}.

\bibitem[\citeproctext]{ref-kirkpatrick2017overcoming}
Kirkpatrick, James, Razvan Pascanu, Neil Rabinowitz, Joel Veness,
Guillaume Desjardins, Andrei A. Rusu, Kieran Milan, et al. 2017.
{``Overcoming Catastrophic Forgetting in Neural Networks.''}
\emph{Proceedings of the National Academy of Sciences} 114 (13):
3521--26. \url{https://doi.org/10.1073/pnas.1611835114}.

\bibitem[\citeproctext]{ref-konevcny2016federated}
Konečný, Jakub, H. Brendan McMahan, Daniel Ramage, and Peter Richtárik.
2016. {``Federated Optimization: Distributed Machine Learning for
on-Device Intelligence.''} \emph{CoRR} abs/1610.02527 (October).
\url{http://arxiv.org/abs/1610.02527v1}.

\bibitem[\citeproctext]{ref-lai2018cmsis}
Lai, Liangzhen, Naveen Suda, and Vikas Chandra. 2018. {``CMSIS-NN:
Efficient Neural Network Kernels for Arm Cortex-m CPUs.''} \emph{arXiv
Preprint arXiv:1801.06601}, January.
\url{http://arxiv.org/abs/1801.06601v1}.

\bibitem[\citeproctext]{ref-li2020federated}
Li, Tian, Anit Kumar Sahu, Ameet Talwalkar, and Virginia Smith. 2020.
{``Federated Learning: Challenges, Methods, and Future Directions.''}
\emph{IEEE Signal Processing Magazine} 37 (3): 50--60.
\url{https://doi.org/10.1109/msp.2020.2975749}.

\bibitem[\citeproctext]{ref-lin2020mcunet}
Lin, Ji, Wei-Ming Chen, Yujun Lin, and Song Han. 2020. {``MCUNet: Tiny
Deep Learning on IoT Devices.''} In \emph{Advances in Neural Information
Processing Systems (NeurIPS)}.

\bibitem[\citeproctext]{ref-fedstaleweight}
Ma, Jeffrey, Alan Tu, Yiling Chen, and Vijay Janapa Reddi. 2024.
{``FedStaleWeight: Buffered Asynchronous Federated Learning with Fair
Aggregation via Staleness Reweighting,''} June.
\url{http://arxiv.org/abs/2406.02877v1}.

\bibitem[\citeproctext]{ref-ma2024challenges}
Ma, Xiaoyu, and David Patterson. 2024. {``Challenges and Research
Directions for Large Language Model Inference Hardware.''} \emph{arXiv
Preprint arXiv:2601.05047}.

\bibitem[\citeproctext]{ref-mcmahan2017communication}
McMahan, Brendan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise
Agüera y Arcas. 2017. {``Communication-Efficient Learning of Deep
Networks from Decentralized Data.''} In \emph{Proceedings of the 20th
International Conference on Artificial Intelligence and Statistics
(AISTATS)}, 54:1273--82. Proceedings of Machine Learning Research. PMLR.
\url{http://proceedings.mlr.press/v54/mcmahan17a.html}.

\bibitem[\citeproctext]{ref-mnih2015human}
Mnih, Volodymyr, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel
Veness, Marc G. Bellemare, Alex Graves, et al. 2015. {``Human-Level
Control Through Deep Reinforcement Learning.''} \emph{Nature} 518
(7540): 529--33. \url{https://doi.org/10.1038/nature14236}.

\bibitem[\citeproctext]{ref-fedbuff}
Nguyen, John, Kshitiz Malik, Hongyuan Zhan, Ashkan Yousefpour, Michael
Rabbat, Mani Malek, and Dzmitry Huba. 2021. {``Federated Learning with
Buffered Asynchronous Aggregation,''} June.
\url{http://arxiv.org/abs/2106.06639v4}.

\bibitem[\citeproctext]{ref-quinonero2009dataset}
Quiñonero-Candela, Joaquin, Masashi Sugiyama, Anton Schwaighofer, and
Neil D. Lawrence. 2008. {``Dataset Shift in Machine Learning.''}
\emph{The MIT Press}. The MIT Press.
\url{https://doi.org/10.7551/mitpress/7921.003.0002}.

\bibitem[\citeproctext]{ref-rebuffi2017learning}
Rebuffi, Sylvestre-Alvise, Hakan Bilen, and Andrea Vedaldi. 2017.
{``Learning Multiple Visual Domains with Residual Adapters.''} In
\emph{Advances in Neural Information Processing Systems}. Vol. 30.

\bibitem[\citeproctext]{ref-fedstale}
Rodio, Angelo, and Giovanni Neglia. 2024. {``FedStale: Leveraging Stale
Client Updates in Federated Learning,''} May.
\url{http://arxiv.org/abs/2405.04171v1}.

\bibitem[\citeproctext]{ref-rolnick2019experience}
Rolnick, David, Arun Ahuja, Jonathan Schwarz, Timothy Lillicrap, and
Greg Wayne. 2019. {``Experience Replay for Continual Learning.''} In
\emph{Advances in Neural Information Processing Systems (NeurIPS)}.

\bibitem[\citeproctext]{ref-sandler2018mobilenetv2}
Sandler, Mark, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and
Liang-Chieh Chen. 2018. {``MobileNetV2: Inverted Residuals and Linear
Bottlenecks.''} In \emph{2018 IEEE/CVF Conference on Computer Vision and
Pattern Recognition}, 4510--20. IEEE.
\url{https://doi.org/10.1109/cvpr.2018.00474}.

\bibitem[\citeproctext]{ref-sanh2019distilbert}
Sanh, Victor, Lysandre Debut, Julien Chaumond, and Thomas Wolf. 2019.
{``DistilBERT, a Distilled Version of BERT: Smaller, Faster, Cheaper and
Lighter,''} October. \url{http://arxiv.org/abs/1910.01108v4}.

\bibitem[\citeproctext]{ref-sun2020mobilebert}
Sun, Zhiqing, Hongkun Yu, Xiaodan Song, Renjie Liu, Yiming Yang, and
Denny Zhou. 2020. {``MobileBERT: A Compact Task-Agnostic BERT for
Resource-Limited Devices.''} In \emph{Proceedings of the 58th Annual
Meeting of the Association for Computational Linguistics}, 2158--70.
Association for Computational Linguistics.
\url{https://doi.org/10.18653/v1/2020.acl-main.195}.

\bibitem[\citeproctext]{ref-tan2019efficientnet}
Tan, Mingxing, and Quoc V. Le. 2019. {``EfficientNet: Rethinking Model
Scaling for Convolutional Neural Networks.''} In \emph{Proceedings of
the International Conference on Machine Learning (ICML)}, 6105--14.

\bibitem[\citeproctext]{ref-hipaa1996health}
Tomes, JP. 1996. {``The Health Insurance Portability and Accountability
Act of 1996: Understanding the Anti-Kickback Laws.''} \emph{Journal of
Health Care Finance} 25 (2): 55--62.
\url{https://www.hhs.gov/hipaa/index.html}.

\bibitem[\citeproctext]{ref-wang2021field}
Wang, Jianyu, Zachary Charles, Zheng Xu, Gauri Joshi, H. Brendan
McMahan, Blaise Aguera y Arcas, Maruan Al-Shedivat, et al. 2021. {``A
Field Guide to Federated Optimization,''} July.
\url{http://arxiv.org/abs/2107.06917v1}.

\bibitem[\citeproctext]{ref-wang2020generalizing}
Wang, Yaqing, Quanming Yao, James T. Kwok, and Lionel M. Ni. 2020.
{``Generalizing from a Few Examples: A Survey on Few-Shot Learning.''}
\emph{ACM Computing Surveys} 53 (3): 1--34.
\url{https://doi.org/10.1145/3386252}.

\bibitem[\citeproctext]{ref-warden2018speech}
Warden, Pete. 2018. {``Speech Commands: A Dataset for Limited-Vocabulary
Speech Recognition.''} \emph{arXiv Preprint arXiv:1804.03209}, April.
\url{http://arxiv.org/abs/1804.03209v1}.

\bibitem[\citeproctext]{ref-lai2020tinyml}
Warden, Pete, and Daniel Situnayake. 2020. \emph{TinyML: Machine
Learning with TensorFlow Lite on Arduino and Ultra-Low-Power
Microcontrollers}. O'Reilly Media.

\bibitem[\citeproctext]{ref-xie2019fedasync}
Xie, Cong, Sanmi Koyejo, and Indranil Gupta. 2019. {``Asynchronous
Federated Optimization.''} \emph{arXiv Preprint arXiv:1903.03934},
March. \url{http://arxiv.org/abs/1903.03934v5}.

\bibitem[\citeproctext]{ref-zhang2020efficient}
Zhang, Xitong, Jialin Song, and Dacheng Tao. 2020. {``Efficient
Task-Specific Adaptation for Deep Models.''} In \emph{International
Conference on Learning Representations (ICLR)}.

\bibitem[\citeproctext]{ref-zhao2018federated}
Zhao, Yue, Meng Li, Liangzhen Lai, Naveen Suda, Damon Civin, and Vikas
Chandra. 2018. {``Federated Learning with Non-IID Data.''} \emph{CoRR}
abs/1806.00582 (June). \url{http://arxiv.org/abs/1806.00582v2}.

\end{CSLReferences}


\backmatter

\clearpage


\end{document}
