% Options for packages loaded elsewhere
% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode,linktoc=all,pdfwindowui,pdfpagemode=FullScreen,pdfpagelayout=TwoPageRight}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  9pt,
  letterpaper,
  abstract,
  titlepage]{scrbook}
\usepackage{xcolor}
\usepackage[left=1in,marginparwidth=2.0666666666667in,textwidth=4.1333333333333in,marginparsep=0.3in]{geometry}
\usepackage{amsmath,amssymb}
\setcounter{secnumdepth}{3}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
% Make \paragraph and \subparagraph free-standing
\makeatletter
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}{
    \@ifstar
      \xxxParagraphStar
      \xxxParagraphNoStar
  }
  \newcommand{\xxxParagraphStar}[1]{\oldparagraph*{#1}\mbox{}}
  \newcommand{\xxxParagraphNoStar}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}{
    \@ifstar
      \xxxSubParagraphStar
      \xxxSubParagraphNoStar
  }
  \newcommand{\xxxSubParagraphStar}[1]{\oldsubparagraph*{#1}\mbox{}}
  \newcommand{\xxxSubParagraphNoStar}[1]{\oldsubparagraph{#1}\mbox{}}
\fi
\makeatother

\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{242,244,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{#1}}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.82,0.10,0.26}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{#1}}
\newcommand{\BuiltInTok}[1]{\textcolor[rgb]{0.88,0.40,0.10}{#1}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.41,0.45,0.49}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.86,0.20,0.18}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.58,0.00,0.30}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{#1}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.75,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.88,0.40,0.10}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.42,0.37,0.78}{#1}}
\newcommand{\ImportTok}[1]{\textcolor[rgb]{0.00,0.45,0.70}{\textbf{#1}}}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.73,0.49,0.84}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.10,0.10,0.10}{#1}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{#1}}
\newcommand{\RegionMarkerTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.12,0.55,0.30}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{#1}}}}

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\newsavebox\pandoc@box
\newcommand*\pandocbounded[1]{% scales image to fit in text height/width
  \sbox\pandoc@box{#1}%
  \Gscale@div\@tempa{\textheight}{\dimexpr\ht\pandoc@box+\dp\pandoc@box\relax}%
  \Gscale@div\@tempb{\linewidth}{\wd\pandoc@box}%
  \ifdim\@tempb\p@<\@tempa\p@\let\@tempa\@tempb\fi% select the smaller of both
  \ifdim\@tempa\p@<\p@\scalebox{\@tempa}{\usebox\pandoc@box}%
  \else\usebox{\pandoc@box}%
  \fi%
}
% Set default figure placement to htbp
\def\fps@figure{htbp}
\makeatother
% definitions for citeproc citations
\NewDocumentCommand\citeproctext{}{}
\NewDocumentCommand\citeproc{mm}{%
  \begingroup\def\citeproctext{#2}\cite{#1}\endgroup}
\makeatletter
 % allow citations to break across lines
 \let\@cite@ofmt\@firstofone
 % avoid brackets around text for \cite:
 \def\@biblabel#1{}
 \def\@cite#1#2{{#1\if@tempswa , #2\fi}}
\makeatother
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newenvironment{CSLReferences}[2] % #1 hanging-indent, #2 entry-spacing
 {\begin{list}{}{%
  \setlength{\itemindent}{0pt}
  \setlength{\leftmargin}{0pt}
  \setlength{\parsep}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1
   \setlength{\leftmargin}{\cslhangindent}
   \setlength{\itemindent}{-1\cslhangindent}
  \fi
  % set entry spacing
  \setlength{\itemsep}{#2\baselineskip}}}
 {\end{list}}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{\hfill\break\parbox[t]{\linewidth}{\strut\ignorespaces#1\strut}}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}

% =============================================================================
% LATEX HEADER CONFIGURATION FOR MLSYSBOOK PDF
% =============================================================================
% This file contains all LaTeX package imports, custom commands, and styling
% definitions for the PDF output of the Machine Learning Systems textbook.
%
% Key Features:
% - Harvard crimson branding throughout
% - Custom part/chapter/section styling
% - Professional table formatting with colored headers
% - Margin notes with custom styling
% - TikZ-based part dividers
% - Page numbering (Roman for frontmatter, Arabic for mainmatter)
%
% Note: This file is included via _quarto-pdf.yml and affects PDF output only.
% HTML/EPUB styling is handled separately via CSS files.
% =============================================================================

% =============================================================================
% PACKAGE IMPORTS
% =============================================================================

% Layout and positioning
% \usepackage[outercaption, ragged]{sidecap}  % Commented out to make figure captions inline instead of in margin
\usepackage{adjustbox}      % Adjusting box dimensions
\usepackage{afterpage}      % Execute commands after page break
\usepackage{morefloats}     % Increase number of floats
\usepackage{array}          % Enhanced table column formatting
\usepackage{atbegshi}       % Insert content at page beginning
%\usepackage{changepage}     % Change page dimensions mid-document
\usepackage{emptypage}      % Clear headers/footers on empty pages

% Language and text
\usepackage[english]{babel} % English language support
\usepackage{microtype}      % Improved typography and hyphenation

% Captions and floats
\usepackage{caption}
% Caption styling configuration
%\captionsetup[table]{belowskip=5pt}
\captionsetup{format=plain}
\DeclareCaptionLabelFormat{mylabel}{#1
#2:\hspace{1.0ex}}
\DeclareCaptionFont{ninept}{\fontsize{7pt}{8}\selectfont #1}

% Figure captions: Small font, bold label, ragged right
\captionsetup[figure]{labelfont={bf,ninept},labelsep=space,
belowskip=2pt,aboveskip=6pt,labelformat=mylabel,
justification=raggedright,singlelinecheck=false,font={ninept}}

% Table captions: Small font, bold label, ragged right
\captionsetup[table]{belowskip=6pt,labelfont={bf,ninept},labelsep=none,
labelformat=mylabel,justification=raggedright,singlelinecheck=false,font={ninept}}

% Typography fine-tuning
\emergencystretch=5pt       % Allow extra stretch to avoid overfull boxes

% Utility packages
\usepackage{etoolbox}       % For patching commands and environments

% Page layout and headers
\usepackage{fancyhdr}       % Custom headers and footers
\usepackage{geometry}       % Page dimensions and margins

% Graphics and figures
\usepackage{graphicx}       % Include graphics
\usepackage{float}          % Improved float placement
\usepackage[skins,breakable]{tcolorbox} % Coloured and framed text boxes
\tcbset{before upper=\setlength{\parskip}{3pt}}

% Tables
\usepackage{longtable}      % Multi-page tables

% Fonts and typography
\usepackage{fontspec}       % Font selection for LuaLaTeX
\usepackage{mathptmx}       % Times-like math fonts
\usepackage{newpxtext}      % Palatino-like font for body text

% Colors and visual elements
\usepackage[dvipsnames]{xcolor}  % Extended color support
\usepackage{tikz}           % Programmatic graphics
\usetikzlibrary{positioning}
\usetikzlibrary{calc}
\usepackage{tikzpagenodes}  % TikZ positioning relative to page

% Code listings
\usepackage{listings}       % Code highlighting

% Hyperlinks
\usepackage{hyperref}       % Clickable links in PDF

% Conditional logic
\usepackage{ifthen}         % If-then-else commands

% Math symbols
\usepackage{amsmath}        % AMS math extensions
\usepackage{amssymb}        % AMS math symbols
\usepackage{latexsym}       % Additional LaTeX symbols
\usepackage{pifont}         % Zapf Dingbats symbols
\providecommand{\blacklozenge}{\ding{117}}  % Black diamond symbol

% Lists
\usepackage{enumitem}       % Customizable lists

% Margin notes and sidenotes
\usepackage{marginfix}      % Fixes margin note overflow
\usepackage{marginnote}     % Margin notes
\usepackage{sidenotes}      % Academic-style sidenotes
\renewcommand\raggedrightmarginnote{\sloppy}
\renewcommand\raggedleftmarginnote{\sloppy}

% Typography improvements
\usepackage{ragged2e}       % Better ragged text
\usepackage[all]{nowidow}   % Prevent widows and orphans
\usepackage{needspace}      % Ensure minimum space on page

% Section formatting
\usepackage[explicit]{titlesec}  % Custom section titles
\usepackage{tocloft}        % Table of contents formatting

% QR codes and icons
\usepackage{fontawesome5}   % Font Awesome icons
\usepackage{qrcode}         % QR code generation
\qrset{link, height=15mm}

% =============================================================================
% FLOAT CONFIGURATION
% =============================================================================
% Allow more floats per page to handle figure-heavy chapters
\extrafloats{200}
\setcounter{topnumber}{12}       % Max floats at top of page
\setcounter{bottomnumber}{12}    % Max floats at bottom of page
\setcounter{totalnumber}{24}     % Max floats per page
\setcounter{dbltopnumber}{8}     % Max floats at top of two-column page
\renewcommand{\topfraction}{.95}  % Max fraction of page for top floats
\renewcommand{\bottomfraction}{.95}
\renewcommand{\textfraction}{.05}  % Min fraction of page for text
\renewcommand{\floatpagefraction}{.7}  % Min fraction of float page
\renewcommand{\dbltopfraction}{.95}

% Prevent "Float(s) lost" errors by flushing floats more aggressively
\usepackage{placeins}  % Provides \FloatBarrier

% =============================================================================
% COLOR DEFINITIONS
% =============================================================================
% Harvard crimson - primary brand color used throughout
\definecolor{crimson}{HTML}{A51C30}

% Quiz element colors
\definecolor{quiz-question-color1}{RGB}{225,243,248}  % Light blue background
\definecolor{quiz-question-color2}{RGB}{17,158,199}   % Blue border
\definecolor{quiz-answer-color1}{RGB}{250,234,241}    % Light pink background
\definecolor{quiz-answer-color2}{RGB}{152,14,90}      % Magenta border

% =============================================================================
% LIST FORMATTING
% =============================================================================
% Tighter list spacing for academic style
\def\tightlist{}
\setlist{itemsep=1pt, parsep=1pt, topsep=0pt,after={\vspace{0.3\baselineskip}}}
\let\tightlist\relax

\makeatletter
\@ifpackageloaded{framed}{}{\usepackage{framed}}
\@ifpackageloaded{fancyvrb}{}{\usepackage{fancyvrb}}
\makeatother

\makeatletter
%New float "codelisting" has been updated
\AtBeginDocument{%
\floatstyle{ruled}
\newfloat{codelisting}{!htb}{lop}
\floatname{codelisting}{Listing}
\floatplacement{codelisting}{!htb}
\captionsetup[codelisting]{labelfont={bf,ninept},labelformat=mylabel,
  singlelinecheck=false,width=\linewidth,labelsep=none,font={ninept}}%
\renewenvironment{snugshade}{%
   \def\OuterFrameSep{3pt}%
   \def\FrameCommand{\fboxsep=5pt\colorbox{shadecolor}}%
   \MakeFramed{\advance\hsize-\width\FrameRestore}%
   \leftskip 0.5em \rightskip 0.5em%
   \small% decrease font size
   }{\endMakeFramed}%
}
\makeatother

%The space before and after the verbatim environment "Highlighting" has been reduced
\fvset{listparameters=\setlength{\topsep}{0pt}\setlength{\partopsep}{0pt}}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{framesep=0mm,commandchars=\\\{\}}

\makeatletter
\renewcommand\fs@ruled{\def\@fs@cfont{\bfseries}\let\@fs@capt\floatc@ruled
\def\@fs@pre{\hrule height.8pt depth0pt \kern2pt}%
\def\@fs@post{\kern2pt\hrule\relax}%
\def\@fs@mid{\kern2pt\hrule\kern1pt}%space between float and caption
\let\@fs@iftopcapt\iftrue}
\makeatother


% =============================================================================
% HYPHENATION RULES
% =============================================================================
% Explicit hyphenation points for technical terms to avoid bad breaks
\hyphenation{
  light-weight
  light-weight-ed
  de-vel-op-ment
  un-der-stand-ing
  mod-els
  prin-ci-ples
  ex-per-tise
  com-pli-cat-ed
  blue-print
  per‧for‧mance
  com-mu-ni-ca-tion
  par-a-digms
  hy-per-ten-sion
  a-chieved
}

% =============================================================================
% CODE LISTING CONFIGURATION
% =============================================================================
% Settings for code blocks using listings package
\lstset{
breaklines=true,              % Automatic line wrapping
breakatwhitespace=true,       % Break at whitespace only
basicstyle=\ttfamily,         % Monospace font
frame=none,                   % No frame around code
keepspaces=true,              % Preserve spaces
showspaces=false,             % Don't show space characters
showtabs=false,               % Don't show tab characters
columns=flexible,             % Flexible column width
belowskip=0pt,               % Minimal spacing
aboveskip=0pt
}

% =============================================================================
% PAGE GEOMETRY
% =============================================================================
% MIT Press trim size: 7" x 10" (per publisher specifications)
% This is a standard academic textbook format providing good readability
% for technical content with figures and code blocks.
% Wide outer margin accommodates sidenotes/margin notes.
\geometry{
  paperwidth=7in,
  paperheight=10in,
  top=0.875in,
  bottom=0.875in,
  inner=0.875in,              % Inner margin (binding side)
  outer=1.75in,               % Outer margin (includes space for sidenotes)
  footskip=30pt,
  marginparwidth=1.25in,      % Width for margin notes
  twoside                     % Different left/right pages
}

% =============================================================================
% SIDENOTE STYLING
% =============================================================================
% Custom sidenote design with crimson vertical bar
\renewcommand{\thefootnote}{\textcolor{crimson}{\arabic{footnote}}}

% Save original sidenote command
\makeatletter
\@ifundefined{oldsidenote}{
  \let\oldsidenote\sidenote%
}{}
\makeatother

% Redefine sidenote with vertical crimson bar
\renewcommand{\sidenote}[1]{%
  \oldsidenote{%
    \noindent
    \color{crimson!100}                        % Crimson vertical line
    \raisebox{0em}{%
      \rule{0.5pt}{1.5em}                      % Thin vertical line
    }
    \hspace{0.3em}                             % Space after line
    \color{black}                              % Reset text color
    \footnotesize #1                           % Sidenote content
  }%
}

% =============================================================================
% FLOAT HANDLING
% =============================================================================
% Patch LaTeX's output routine to handle float overflow gracefully
% The "Float(s) lost" error occurs in \@doclearpage when \@currlist is not empty
% This patch silently clears pending floats that can't be placed
\makeatletter
\let\orig@doclearpage\@doclearpage
\def\@doclearpage{%
  \ifx\@currlist\@empty\else
    \global\let\@currlist\@empty
    \typeout{Warning: Floats cleared to prevent overflow}%
  \fi
  \orig@doclearpage
}
\makeatother

% Additional safety for structural commands
\let\originalbackmatter\backmatter
\renewcommand{\backmatter}{%
  \clearpage%
  \originalbackmatter%
}

\let\originalfrontmatter\frontmatter
\renewcommand{\frontmatter}{%
  \clearpage%
  \originalfrontmatter%
}

\let\originalmainmatter\mainmatter
\renewcommand{\mainmatter}{%
  \clearpage%
  \originalmainmatter%
}

% =============================================================================
% PAGE HEADERS AND FOOTERS
% =============================================================================
% Ensure chapters use fancy page style (not plain)
\patchcmd{\chapter}{\thispagestyle{plain}}{\thispagestyle{fancy}}{}{}

% Main page style with crimson headers
\pagestyle{fancy}
\fancyhf{}                                              % Clear all
\fancyhead[LE]{\small\color{crimson}\nouppercase{\rightmark}}  % Left even: section
\fancyhead[RO]{\color{crimson}\thepage}                 % Right odd: page number
\fancyhead[LO]{\small\color{crimson}\nouppercase{\leftmark}}   % Left odd: chapter
\fancyhead[RE]{\color{crimson}\thepage}                 % Right even: page number
\renewcommand{\headrulewidth}{0.4pt}                    % Thin header line
\renewcommand{\footrulewidth}{0pt}                      % No footer line

% Plain page style (for chapter openings)
\fancypagestyle{plain}{
  \fancyhf{}
  \fancyfoot[C]{\color{crimson}\thepage}                % Centered page number
  \renewcommand{\headrulewidth}{0pt}
  \renewcommand{\footrulewidth}{0pt}
}

% =============================================================================
% KOMA-SCRIPT FONT ADJUSTMENTS
% =============================================================================
% Apply crimson color to all heading levels
\addtokomafont{disposition}{\rmfamily\color{crimson}}
\addtokomafont{chapter}{\color{crimson}}
\addtokomafont{section}{\color{crimson}}
\addtokomafont{subsection}{\color{crimson}}

% =============================================================================
% ABSTRACT ENVIRONMENT
% =============================================================================
\newenvironment{abstract}{
  \chapter*{\abstractname}
  \addcontentsline{toc}{chapter}{\abstractname}
  \small
}{
  \clearpage
}

% =============================================================================
% HYPERLINK CONFIGURATION
% =============================================================================
% Crimson-colored links throughout, two-page PDF layout
\hypersetup{
  linkcolor=crimson,
  citecolor=crimson,
  urlcolor=crimson,
  pdfpagelayout=TwoPageRight,   % Two-page spread view
  pdfstartview=Fit               % Initial zoom fits page
}

% =============================================================================
% PART SUMMARY SYSTEM
% =============================================================================
% Allows adding descriptive text below part titles
\newcommand{\partsummary}{}     % Empty by default
\newif\ifhaspartsummary%
\haspartsummaryfalse%

\newcommand{\setpartsummary}[1]{%
  \renewcommand{\partsummary}{#1}%
  \haspartsummarytrue%
}

% Additional colors for part page backgrounds
\definecolor{BrownLL}{RGB}{233,222,220}
\definecolor{BlueDD}{RGB}{62,100,125}
\colorlet{BlueDD}{magenta}

% ===============================================================================
% PART STYLING SYSTEM
% ===============================================================================
%
% This system provides three distinct visual styles for book organization:
%
% 1. NUMBERED PARTS (\part{title}) - For main book sections
%    - Roman numerals (I, II, III, etc.) in top right corner
%    - Crimson title with horizontal lines above/below
%    - "Part I" label in sidebar
%    - Used for: foundations, principles, optimization, deployment, etc.
%
% 2. UNNUMBERED PARTS (\part*{title}) - For special sections like "Labs"
%    - Division-style geometric background (left side)
%    - No Roman numerals
%    - Used for: labs section
%
% 3. DIVISIONS (\division{title}) - For major book divisions
%    - Clean geometric background with centered title
%    - Used for: frontmatter, main_content, backmatter
%
% The Lua filter (inject-parts.lua) automatically routes parts by {key:xxx} commands
% to the appropriate LaTeX command based on the key name.
% ===============================================================================

% NUMBERED PARTS: Roman numeral styling for main book sections
\titleformat{\part}[display]
{\thispagestyle{empty}}{}{20pt}{
\begin{tikzpicture}[remember picture,overlay]
%%%
%%
\node[crimson,align=flush right,
inner sep=0,outer sep=0mm,draw=none,%
anchor=east,minimum height=31mm, text width=1.2\textwidth,
yshift=-30mm,font={%
\fontsize{98pt}{104}\selectfont\bfseries}]  (BG) at (current page text area.north east){\thepart};
%
\node[black,inner sep=0mm,draw=none,
anchor=mid,text width=1.2\textwidth,
 minimum height=35mm, align=right,
node distance=7mm,below=of BG,
font={\fontsize{30pt}{34}\selectfont}]
(BGG)  {\hyphenchar\font=-1 \color{black}\MakeUppercase {#1}};
\draw [crimson,line width=3pt] ([yshift=0mm]BGG.north west) -- ([yshift=0mm]BGG.north east);
\draw [crimson,line width=2pt] ([yshift=0mm]BGG.south west) -- ([yshift=0mm]BGG.south east);
%
\node[fill=crimson,text=white,rotate=90,%
anchor=south west,minimum height=15mm,
minimum width=40mm,font={%
\fontsize{20pt}{20}\selectfont\bfseries}](BP)  at
(current page text area.south east)
{{\sffamily Part}~\thepart};
%
\path[red](BP.north west)-|coordinate(PS)(BGG.south west);
%
% Part summary box commented out for cleaner design
% \ifhaspartsummary
% \node[inner sep=4pt,text width=0.7\textwidth,draw=none,fill=BrownLL!40,
% align=justify,font={\fontsize{9pt}{12}\selectfont},anchor=south west]
% at (PS) {\partsummary};
% \fi
\end{tikzpicture}
}[]

\renewcommand{\thepart}{\Roman{part}}

% UNNUMBERED PARTS: Division-style background for special sections
\titleformat{name=\part,numberless}[display]
{\thispagestyle{empty}}{}{20pt}{
\begin{tikzpicture}[remember picture,overlay]
%%%
\coordinate(S1)at([yshift=-200mm]current page.north west);
\draw[draw=none,fill=BlueDD!7](S1)--++(45:16)coordinate(S2)-
|(S2|-current page.north west)--(current page.north west)coordinate(S3)--(S1);
%
\coordinate(E1)at([yshift=-98mm]current page.north west);
\draw[draw=none,fill=BlueDD!15](E1)--(current page.north west)coordinate(E2)
--++(0:98mm)coordinate(E3)--(E1);
%
\coordinate(D1)at([yshift=15mm]current page.south west);
\draw[draw=none,fill=BlueDD!40,opacity=0.5](D1)--++(45:5.5)coordinate(D2)
-|(D2|-current page.north west)--(current page.north west)coordinate(D3)--(D1);
%%%%
\path[red](S2)-|(S2-|current page.east)coordinate(SS2);
%PART
\node[crimson,align=flush right,inner sep=0,outer sep=0mm,draw=none,anchor=south,
font={\fontsize{48pt}{48}\selectfont\bfseries}]  (BG) at ($(S2)!0.5!(SS2)$){\hphantom{Part}};
%%%
\path[green]([yshift=15mm]D2)-|coordinate(TPD)(BG.south east);
\node[inner sep=0mm,draw=none,anchor=south east,%text width=0.9\textwidth,
align=right,font={\fontsize{40pt}{40}\selectfont}]
(BGG) at (TPD)  {\color{crimson}\MakeUppercase {#1}};%\MakeUppercase {}
\end{tikzpicture}
}

% Define \numberedpart command for numbered parts
\newcommand{\numberedpart}[1]{%
\FloatBarrier%  % Flush all pending floats before part break
\clearpage
\thispagestyle{empty}
\stepcounter{part}%
\begin{tikzpicture}[remember picture,overlay]
%%%
%%
\node[crimson,align=flush right,
inner sep=0,outer sep=0mm,draw=none,%
anchor=east,minimum height=31mm, text width=1.2\textwidth,
yshift=-30mm,font={%
\fontsize{98pt}{104}\selectfont\bfseries}]  (BG) at (current page text area.north east){\thepart};
%
\node[black,inner sep=0mm,draw=none,
anchor=mid,text width=1.2\textwidth,
 minimum height=35mm, align=right,
node distance=7mm,below=of BG,
font={\fontsize{30pt}{34}\selectfont}]
(BGG)  {\hyphenchar\font=-1 \color{black}\MakeUppercase {#1}};
\draw [crimson,line width=3pt] ([yshift=0mm]BGG.north west) -- ([yshift=0mm]BGG.north east);
\draw [crimson,line width=2pt] ([yshift=0mm]BGG.south west) -- ([yshift=0mm]BGG.south east);
%
\node[fill=crimson,text=white,rotate=90,%
anchor=south west,minimum height=15mm,
minimum width=40mm,font={%
\fontsize{20pt}{20}\selectfont\bfseries}](BP)  at
(current page text area.south east)
{{\sffamily Part}~\thepart};
%
\path[red](BP.north west)-|coordinate(PS)(BGG.south west);
%
% Part summary box commented out for cleaner design
% \ifhaspartsummary
% \node[inner sep=4pt,text width=0.7\textwidth,draw=none,fill=BrownLL!40,
% align=justify,font={\fontsize{9pt}{12}\selectfont},anchor=south west]
% at (PS) {\partsummary};
% \fi
\end{tikzpicture}
\clearpage
}



% DIVISIONS: Clean geometric styling with subtle tech elements
% Used for frontmatter, main_content, and backmatter divisions
\newcommand{\division}[1]{%
\FloatBarrier%  % Flush all pending floats before division break
\clearpage
\thispagestyle{empty}
\begin{tikzpicture}[remember picture,overlay]

% Clean geometric background (original design)
\coordinate(S1)at([yshift=-200mm]current page.north west);
\draw[draw=none,fill=BlueDD!7](S1)--++(45:16)coordinate(S2)-
|(S2|-current page.north west)--(current page.north west)coordinate(S3)--(S1);

\coordinate(E1)at([yshift=-98mm]current page.north west);
\draw[draw=none,fill=BlueDD!15](E1)--(current page.north west)coordinate(E2)
--++(0:98mm)coordinate(E3)--(E1);

\coordinate(D1)at([yshift=15mm]current page.south west);
\draw[draw=none,fill=BlueDD!40,opacity=0.5](D1)--++(45:5.5)coordinate(D2)
-|(D2|-current page.north west)--(current page.north west)coordinate(D3)--(D1);

% Subtle tech elements - positioned in white areas for better visibility
% Upper right white area - more visible
\draw[crimson!40, line width=0.8pt] ([xshift=140mm,yshift=-60mm]current page.north west) -- ++(40mm,0);
\draw[crimson!40, line width=0.8pt] ([xshift=150mm,yshift=-70mm]current page.north west) -- ++(30mm,0);
\draw[crimson!35, line width=0.7pt] ([xshift=160mm,yshift=-60mm]current page.north west) -- ++(0,-15mm);
\draw[crimson!35, line width=0.7pt] ([xshift=170mm,yshift=-70mm]current page.north west) -- ++(0,10mm);

% Circuit nodes - upper right
\fill[crimson!50] ([xshift=160mm,yshift=-60mm]current page.north west) circle (1.5mm);
\fill[white] ([xshift=160mm,yshift=-60mm]current page.north west) circle (0.8mm);
\fill[crimson!50] ([xshift=170mm,yshift=-70mm]current page.north west) circle (1.3mm);
\fill[white] ([xshift=170mm,yshift=-70mm]current page.north west) circle (0.6mm);

% Lower right white area - enhanced visibility
\draw[crimson!45, line width=0.9pt] ([xshift=140mm,yshift=-190mm]current page.north west) -- ++(45mm,0);
\draw[crimson!45, line width=0.9pt] ([xshift=150mm,yshift=-200mm]current page.north west) -- ++(35mm,0);
\draw[crimson!40, line width=0.8pt] ([xshift=160mm,yshift=-190mm]current page.north west) -- ++(0,-20mm);
\draw[crimson!40, line width=0.8pt] ([xshift=170mm,yshift=-200mm]current page.north west) -- ++(0,15mm);

% Additional connecting lines in lower right
\draw[crimson!35, line width=0.7pt] ([xshift=130mm,yshift=-180mm]current page.north west) -- ++(25mm,0);
\draw[crimson!35, line width=0.7pt] ([xshift=145mm,yshift=-180mm]current page.north west) -- ++(0,-25mm);

% Circuit nodes - lower right (more prominent)
\fill[crimson!55] ([xshift=160mm,yshift=-190mm]current page.north west) circle (1.6mm);
\fill[white] ([xshift=160mm,yshift=-190mm]current page.north west) circle (0.9mm);
\fill[crimson!55] ([xshift=170mm,yshift=-200mm]current page.north west) circle (1.4mm);
\fill[white] ([xshift=170mm,yshift=-200mm]current page.north west) circle (0.7mm);
\fill[crimson!50] ([xshift=145mm,yshift=-180mm]current page.north west) circle (1.2mm);
\fill[white] ([xshift=145mm,yshift=-180mm]current page.north west) circle (0.6mm);

% Title positioned in center - clean and readable
\node[inner sep=0mm,draw=none,anchor=center,text width=0.8\textwidth,
align=center,font={\fontsize{40pt}{40}\selectfont}]
(BGG) at (current page.center)  {\color{crimson}\MakeUppercase {#1}};

\end{tikzpicture}
\clearpage
}

% LAB DIVISIONS: Circuit-style neural network design for lab sections
% Used specifically for lab platform sections (arduino, xiao, grove, etc.)
\newcommand{\labdivision}[1]{%
\FloatBarrier%  % Flush all pending floats before lab division break
\clearpage
\thispagestyle{empty}
\begin{tikzpicture}[remember picture,overlay]
% Circuit background with subtle gradient
\coordinate(S1)at([yshift=-200mm]current page.north west);
\draw[draw=none,fill=BlueDD!5](S1)--++(45:16)coordinate(S2)-
|(S2|-current page.north west)--(current page.north west)coordinate(S3)--(S1);

% TOP AREA: Circuit lines in upper white space
\draw[crimson!50, line width=1.5pt] ([xshift=30mm,yshift=-40mm]current page.north west) -- ++(60mm,0);
\draw[crimson!40, line width=1pt] ([xshift=120mm,yshift=-50mm]current page.north west) -- ++(50mm,0);
\draw[crimson!50, line width=1.5pt] ([xshift=40mm,yshift=-70mm]current page.north west) -- ++(40mm,0);

% Connecting lines in top area
\draw[crimson!30, line width=1pt] ([xshift=60mm,yshift=-40mm]current page.north west) -- ++(0,-20mm);
\draw[crimson!30, line width=1pt] ([xshift=145mm,yshift=-50mm]current page.north west) -- ++(0,10mm);

% Neural nodes in top area
\fill[crimson!70] ([xshift=60mm,yshift=-40mm]current page.north west) circle (2.5mm);
\fill[white] ([xshift=60mm,yshift=-40mm]current page.north west) circle (1.5mm);
\fill[crimson!60] ([xshift=145mm,yshift=-50mm]current page.north west) circle (2mm);
\fill[white] ([xshift=145mm,yshift=-50mm]current page.north west) circle (1mm);
\fill[crimson!80] ([xshift=80mm,yshift=-70mm]current page.north west) circle (2mm);
\fill[white] ([xshift=80mm,yshift=-70mm]current page.north west) circle (1mm);

% BOTTOM AREA: Circuit lines in lower white space
\draw[crimson!50, line width=1.5pt] ([xshift=20mm,yshift=-200mm]current page.north west) -- ++(70mm,0);
\draw[crimson!40, line width=1pt] ([xshift=110mm,yshift=-210mm]current page.north west) -- ++(60mm,0);
\draw[crimson!50, line width=1.5pt] ([xshift=35mm,yshift=-230mm]current page.north west) -- ++(45mm,0);

% Connecting lines in bottom area
\draw[crimson!30, line width=1pt] ([xshift=55mm,yshift=-200mm]current page.north west) -- ++(0,-20mm);
\draw[crimson!30, line width=1pt] ([xshift=140mm,yshift=-210mm]current page.north west) -- ++(0,15mm);

% Neural nodes in bottom area
\fill[crimson!70] ([xshift=55mm,yshift=-200mm]current page.north west) circle (2.5mm);
\fill[white] ([xshift=55mm,yshift=-200mm]current page.north west) circle (1.5mm);
\fill[crimson!60] ([xshift=140mm,yshift=-210mm]current page.north west) circle (2mm);
\fill[white] ([xshift=140mm,yshift=-210mm]current page.north west) circle (1mm);
\fill[crimson!80] ([xshift=80mm,yshift=-230mm]current page.north west) circle (2mm);
\fill[white] ([xshift=80mm,yshift=-230mm]current page.north west) circle (1mm);

% SIDE AREAS: Subtle circuit elements on left and right edges
\draw[crimson!30, line width=1pt] ([xshift=15mm,yshift=-120mm]current page.north west) -- ++(20mm,0);
\draw[crimson!30, line width=1pt] ([xshift=175mm,yshift=-130mm]current page.north west) -- ++(15mm,0);
\fill[crimson!50] ([xshift=25mm,yshift=-120mm]current page.north west) circle (1.5mm);
\fill[white] ([xshift=25mm,yshift=-120mm]current page.north west) circle (0.8mm);
\fill[crimson!50] ([xshift=185mm,yshift=-130mm]current page.north west) circle (1.5mm);
\fill[white] ([xshift=185mm,yshift=-130mm]current page.north west) circle (0.8mm);

% Title positioned in center - CLEAN AREA
\node[inner sep=0mm,draw=none,anchor=center,text width=0.8\textwidth,
align=center,font={\fontsize{44pt}{44}\selectfont\bfseries}]
(BGG) at (current page.center)  {\color{crimson}\MakeUppercase {#1}};

\end{tikzpicture}
\clearpage
}

% Define \lab command for lab styling (different visual treatment)
\newcommand{\lab}[1]{%
\begin{tikzpicture}[remember picture,overlay]
%%%
% Different background pattern for labs
\coordinate(S1)at([yshift=-200mm]current page.north west);
\draw[draw=none,fill=BlueDD!15](S1)--++(45:16)coordinate(S2)-
|(S2|-current page.north west)--(current page.north west)coordinate(S3)--(S1);
%
\coordinate(E1)at([yshift=-98mm]current page.north west);
\draw[draw=none,fill=BlueDD!25](E1)--(current page.north west)coordinate(E2)
--++(0:98mm)coordinate(E3)--(E1);
%
\coordinate(D1)at([yshift=15mm]current page.south west);
\draw[draw=none,fill=BlueDD!60,opacity=0.7](D1)--++(45:5.5)coordinate(D2)
-|(D2|-current page.north west)--(current page.north west)coordinate(D3)--(D1);
%%%%
\path[red](S2)-|(S2-|current page.east)coordinate(SS2);
%LAB - Different styling
\node[crimson,align=flush right,inner sep=0,outer sep=0mm,draw=none,anchor=south,
font={\fontsize{48pt}{48}\selectfont\bfseries}]  (BG) at ($(S2)!0.5!(SS2)$){\hphantom{Workshop}};
%%%
\path[green]([yshift=15mm]D2)-|coordinate(TPD)(BG.south east);
\node[inner sep=0mm,draw=none,anchor=south east,%text width=0.9\textwidth,
align=right,font={\fontsize{40pt}{40}\selectfont}]
(BGG) at (TPD)  {\color{crimson}\MakeUppercase {#1}};%\MakeUppercase {}
\end{tikzpicture}
\thispagestyle{empty}
\clearpage
}

% =============================================================================
% SECTION FORMATTING
% =============================================================================
% All section levels use crimson color and are ragged right

% Section (Large, bold, crimson)
\titleformat{\section}
  {\normalfont\Large\bfseries\color{crimson}\raggedright}
  {\thesection}
  {0.5em}
  {#1}
\titlespacing*{\section}{0pc}{14pt plus 4pt minus 4pt}{6pt plus 2pt minus 2pt}[0pc]

% Subsection (large, bold, crimson)
\titleformat{\subsection}
  {\normalfont\large\bfseries\color{crimson}\raggedright}
  {\thesubsection}
  {0.5em}
  {#1}
\titlespacing*{\subsection}{0pc}{12pt plus 4pt minus 4pt}{5pt plus 1pt minus 2pt}[0pc]

% Subsubsection (normal size, bold, crimson)
\titleformat{\subsubsection}
  {\normalfont\normalsize\bfseries\color{crimson}\raggedright}
  {\thesubsubsection}
  {0.5em}
  {#1}
\titlespacing*{\subsubsection}{0pc}{12pt plus 4pt minus 4pt}{5pt plus 1pt minus 2pt}[0pc]

% Paragraph (run-in, bold, crimson, ends with period)
\titleformat{\paragraph}[runin]
  {\normalfont\normalsize\bfseries\color{crimson}}
  {\theparagraph}
  {0.5em}
  {#1}
  [\textbf{.}]
  \titlespacing*{\paragraph}{0pc}{6pt plus 2pt minus 2pt}{0.5em}[0pc]

% Subparagraph (run-in, italic, crimson, ends with period)
\titleformat{\subparagraph}[runin]
  {\normalfont\normalsize\itshape\color{crimson}}
  {\thesubparagraph}
  {0.5em}
  {#1}
  [\textbf{.}]
  \titlespacing*{\subparagraph}{0pc}{6pt plus 2pt minus 2pt}{0.5em}[0pc]

% =============================================================================
% CHAPTER FORMATTING
% =============================================================================
% Numbered chapters: "Chapter X" prefix, huge crimson title
\titleformat{\chapter}[display]
  {\normalfont\huge\bfseries\color{crimson}}
  {\chaptername\ \thechapter}
  {20pt}
  {\Huge #1}
  []

% Unnumbered chapters: no prefix, huge crimson title
\titleformat{name=\chapter,numberless}
  {\normalfont\huge\bfseries\color{crimson}}
  {}
  {0pt}
  {\Huge #1}
  []

\renewcommand{\chaptername}{Chapter}
% =============================================================================
% TABLE OF CONTENTS FORMATTING
% =============================================================================
\setcounter{tocdepth}{2}                      % Show chapters, sections, subsections

% TOC spacing adjustments for number widths and indentation
\setlength{\cftchapnumwidth}{2em}             % Chapter number width
\setlength{\cftsecnumwidth}{2.75em}           % Section number width
\setlength{\cftsubsecnumwidth}{3.25em}        % Subsection number width
\setlength{\cftsubsubsecnumwidth}{4em}        % Subsubsection number width
\setlength{\cftsubsecindent}{4.25em}          % Subsection indent
\setlength{\cftsubsubsecindent}{7.5em}        % Subsubsection indent

% Chapter entries in TOC: bold crimson with "Chapter" prefix
\renewcommand{\cftchapfont}{\bfseries\color{crimson}}
\renewcommand{\cftchappresnum}{\color{crimson}Chapter~}

% Custom formatting for division entries (styled like parts)
\newcommand{\divisionchapter}[1]{%
  \addvspace{12pt}%
  \noindent\hfil\bfseries\color{crimson}#1\hfil\par%
  \addvspace{6pt}%
}

% Adjust TOC spacing for "Chapter" prefix
\newlength{\xtraspace}
\settowidth{\xtraspace}{\cftchappresnum\cftchapaftersnum}
\addtolength{\cftchapnumwidth}{\xtraspace}

% Unnumbered chapters with TOC entry
\newcommand{\likechapter}[1]{%
    \chapter*{#1}
    \addcontentsline{toc}{chapter}{\textcolor{crimson}{#1}}
}

% =============================================================================
% PAGE NUMBERING SYSTEM
% =============================================================================
% Implements traditional book numbering:
% - Roman numerals (i, ii, iii...) for frontmatter
% - Arabic numerals (1, 2, 3...) for mainmatter
% Automatically switches at first numbered chapter
\makeatletter
\newif\if@firstnumbered%
\@firstnumberedtrue%
\newif\if@firstunnumbered%
\@firstunnumberedtrue%

\newcounter{lastRomanPage}
\setcounter{lastRomanPage}{1}

% Start document with Roman numerals (frontmatter)
\AtBeginDocument{
  \pagenumbering{roman}
  \renewcommand{\thepage}{\roman{page}}
}

% Intercept chapter command
\let\old@chapter\chapter%
\renewcommand{\chapter}{%
  \@ifstar{\unnumbered@chapter}{\numbered@chapter}%
}

% Numbered chapters: switch to Arabic on first occurrence
\newcommand{\numbered@chapter}[1]{%
  \if@firstnumbered%
    \cleardoublepage%
    \setcounter{lastRomanPage}{\value{page}}%
    \pagenumbering{arabic}%
    \@firstnumberedfalse%
  \else
    \setcounter{page}{\value{page}}%
  \fi
  \setcounter{sidenote}{1}                    % Reset footnote counter per chapter
  \old@chapter{#1}%
}

% Unnumbered chapters: stay in Roman numerals
\newcommand{\unnumbered@chapter}[1]{%
  \if@firstunnumbered%
    \clearpage
    \setcounter{lastRomanPage}{\value{page}}%
    \pagenumbering{roman}%
    \@firstunnumberedfalse%
  \fi
  \setcounter{sidenote}{1}
  \old@chapter*{#1}%
}
\makeatother

% =============================================================================
% TABLE SIZING AND SPACING
% =============================================================================
% Make tables slightly smaller to fit more content
\AtBeginEnvironment{longtable}{\scriptsize}

% Increase vertical spacing in table cells (default is 1.0)
\renewcommand{\arraystretch}{1.3}

% Prefer placing tables at the top of pages
\makeatletter
\renewcommand{\fps@table}{t}  % Default placement: top of page
\makeatother

% =============================================================================
% LONGTABLE PAGE BREAKING FIXES (Windows compatibility)
% =============================================================================
% Prevent "Infinite glue shrinkage" errors on Windows LaTeX builds
% by giving longtable more flexibility in page breaking

% Allow more flexible page breaking (vs strict \flushbottom)
\raggedbottom

% Process more rows before attempting page break (default is 20)
\setcounter{LTchunksize}{50}

% Add extra stretch for longtable environments specifically
\AtBeginEnvironment{longtable}{%
  \setlength{\emergencystretch}{3em}%
  \setlength{\parskip}{0pt plus 1pt}%
}

% =============================================================================
% TABLE STYLING - Clean tables with crimson borders
% =============================================================================
% Professional table appearance with:
% - Clean white background (no colored rows)
% - Crimson-colored borders
% - Good spacing for readability
%
% Note: Headers are automatically bolded by Quarto when using **text** in source
\usepackage{booktabs}      % Professional table rules (\toprule, \midrule, \bottomrule)
\usepackage{colortbl}      % For colored borders (\arrayrulecolor)

% Global table styling - crimson borders
\setlength{\arrayrulewidth}{0.5pt}          % Thinner borders than default
%\arrayrulecolor{crimson}                    % Crimson borders matching brand

\setcounter{chapter}{0}
\usepackage{needspace}
\let\Needspace\needspace
\makeatletter
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{plain}
\@ifundefined{c@chapter}{\newfloat{vid}{h}{lovid}}{\newfloat{vid}{h}{lovid}[chapter]}
\floatname{vid}{Video}
\newcommand*\listofvids{\listof{vid}{List of Videos}}
\makeatother
\makeatletter
\@ifpackageloaded{tcolorbox}{}{\usepackage[skins,breakable]{tcolorbox}}
\@ifpackageloaded{fontawesome5}{}{\usepackage{fontawesome5}}
\definecolor{quarto-callout-color}{HTML}{909090}
\definecolor{quarto-callout-note-color}{HTML}{0758E5}
\definecolor{quarto-callout-important-color}{HTML}{CC1914}
\definecolor{quarto-callout-warning-color}{HTML}{EB9113}
\definecolor{quarto-callout-tip-color}{HTML}{00A047}
\definecolor{quarto-callout-caution-color}{HTML}{FC5300}
\definecolor{quarto-callout-color-frame}{HTML}{acacac}
\definecolor{quarto-callout-note-color-frame}{HTML}{4582ec}
\definecolor{quarto-callout-important-color-frame}{HTML}{d9534f}
\definecolor{quarto-callout-warning-color-frame}{HTML}{f0ad4e}
\definecolor{quarto-callout-tip-color-frame}{HTML}{02b875}
\definecolor{quarto-callout-caution-color-frame}{HTML}{fd7e14}
\makeatother
\makeatletter
\@ifpackageloaded{bookmark}{}{\usepackage{bookmark}}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\makeatletter
\@ifpackageloaded{sidenotes}{}{\usepackage{sidenotes}}
\@ifpackageloaded{marginnote}{}{\usepackage{marginnote}}
\makeatother
\newcommand{\fbxIconPath}{assets/images/icons/callouts}
\newcommand{\fbxIconFormat}{pdf}
\makeatletter
\@ifpackageloaded{tcolorbox}{}{\usepackage[many]{tcolorbox}}
\makeatother
%%%% ---foldboxy preamble ----- %%%%%

% Load xstring for string manipulation
\RequirePackage{xstring}

% Icon path and format configuration - can be overridden in filter-metadata
\providecommand{\fbxIconPath}{assets/images/icons/callouts}
\providecommand{\fbxIconFormat}{pdf}

% Helper command to include icon with hyphen-to-underscore conversion
% This ensures consistency: callout-quiz-question -> callout_quiz_question
\newcommand{\fbxIncludeIcon}[2]{%
  \StrSubstitute{#1}{-}{_}[\fbxIconName]%
  \includegraphics[width=#2]{\fbxIconPath/icon_\fbxIconName.\fbxIconFormat}%
}

% Legacy fallback colors (keep for compatibility)
\definecolor{fbx-default-color1}{HTML}{c7c7d0}
\definecolor{fbx-default-color2}{HTML}{a3a3aa}
\definecolor{fbox-color1}{HTML}{c7c7d0}
\definecolor{fbox-color2}{HTML}{a3a3aa}

% arguments: #1 typelabelnummer: #2 titel: #3
\newenvironment{fbx}[3]{%
\begin{tcolorbox}[
  enhanced,
  breakable,
  %fontupper=\fontsize{8pt}{10pt}\selectfont,  % 95% of body text (10pt -> 9.5pt)
  before skip=8pt,  % space above box (increased)
  after skip=8pt,   % space below box (increased)
  attach boxed title to top*={xshift=0pt},
  boxed title style={
  %fuzzy shadow={1pt}{-1pt}{0mm}{0.1mm}{gray},
  arc=1.5pt,
  rounded corners=north,
  sharp corners=south,
  top=6pt,          % Adjusted for ~40px equivalent height
  bottom=5pt,       % Adjusted for ~40px equivalent height
  overlay={
      \node [left,outer sep=0em, black,draw=none,anchor=west,
        rectangle,fill=none,inner sep=0pt]
        at ([xshift=4mm]frame.west) {\fbxIncludeIcon{#1}{4.2mm}};
    },
  },
  colframe=#1-color2,             % Border color (auto-generated from YAML)
  colbacktitle=#1-color1,         % Background color (auto-generated from YAML)
  colback=white,
  coltitle=black,
  titlerule=0mm,
  toprule=0.5pt,
  bottomrule=0.5pt,
  leftrule=2.2pt,
  rightrule=0.5pt,
  outer arc=1.5pt,
  arc=1.5pt,
  left=0.5em,       % increased left padding
  bottomtitle=1.5mm, % increased title bottom margin
  toptitle=1.5mm,    % increased title top margin
  title=\hspace{2.5em}\protect#2\hspace{0.2em}\protect#3, % Protect parameters
  extras middle and last={top=4pt} % increased continuation spacing
]}
{\end{tcolorbox}}


% boxed environment with right border
\newenvironment{fbxSimple}[3]{\begin{tcolorbox}[
  enhanced,
  breakable,
  %fontupper=\fontsize{8pt}{10pt}\selectfont,  % 95% of body text (10pt -> 9.5pt)
  before skip=8pt,  % space above box (increased)
  after skip=8pt,   % space below box (increased)
  attach boxed title to top*={xshift=0pt},
  boxed title style={
  %fuzzy shadow={1pt}{-1pt}{0mm}{0.1mm}{gray},
  arc=1.5pt,
  rounded corners=north,
  sharp corners=south,
  top=6pt,          % Adjusted for ~40px equivalent height
  bottom=5pt,       % Adjusted for ~40px equivalent height
  overlay={
      \node [left,outer sep=0em, black,draw=none,anchor=west,
        rectangle,fill=none,inner sep=0pt]
        at ([xshift=3mm]frame.west) {\fbxIncludeIcon{#1}{4.2mm}};
    },
  },
  colframe=#1-color2,             % Border color (auto-generated from YAML)
  colbacktitle=#1-color1,         % Background color (auto-generated from YAML)
  colback=white,
  coltitle=black,
  titlerule=0mm,
  toprule=0.5pt,
  bottomrule=0.5pt,
  leftrule=2.2pt,
  rightrule=0.5pt,
  outer arc=1.5pt,
  arc=1.5pt,
  left=0.5em,       % increased left padding
  bottomtitle=1.5mm, % increased title bottom margin
  toptitle=1.5mm,    % increased title top margin
  title=\hspace{2.5em}\protect#2\hspace{0.2em}\protect#3, % Protect parameters
  boxsep=1pt,
  extras first={bottom=0pt},
  extras last={top=0pt,bottom=-4pt},
  overlay first={
    \draw[line width=1pt,white] ([xshift=2.2pt]frame.south west)-- ([xshift=-0.5pt]frame.south east);
  },
  overlay last={
    \draw[line width=1pt,white] ([xshift=2.2pt]frame.north west)-- ([xshift=-0.5pt]frame.north east);
   }
]}
{\end{tcolorbox}}

%%%% --- end foldboxy preamble ----- %%%%%
%%==== colors from yaml ===%
\definecolor{callout-quiz-question-color1}{HTML}{F0F0F8}
\definecolor{callout-quiz-question-color2}{HTML}{5B4B8A}
\definecolor{callout-colab-color1}{HTML}{FFF5E6}
\definecolor{callout-colab-color2}{HTML}{FF6B35}
\definecolor{callout-notebook-color1}{HTML}{F2F7FF}
\definecolor{callout-notebook-color2}{HTML}{2C5282}
\definecolor{callout-perspective-color1}{HTML}{F7F8FA}
\definecolor{callout-perspective-color2}{HTML}{4A5568}
\definecolor{callout-definition-color1}{HTML}{F0F4F8}
\definecolor{callout-definition-color2}{HTML}{1B4F72}
\definecolor{callout-code-color1}{HTML}{F2F4F8}
\definecolor{callout-code-color2}{HTML}{D1D7E0}
\definecolor{callout-lighthouse-color1}{HTML}{FDF8E6}
\definecolor{callout-lighthouse-color2}{HTML}{B8860B}
\definecolor{callout-checkpoint-color1}{HTML}{E8F5E9}
\definecolor{callout-checkpoint-color2}{HTML}{2E7D32}
\definecolor{callout-example-color1}{HTML}{F0F8F6}
\definecolor{callout-example-color2}{HTML}{148F77}
\definecolor{callout-resource-slides-color1}{HTML}{E0F2F1}
\definecolor{callout-resource-slides-color2}{HTML}{20B2AA}
\definecolor{callout-chapter-connection-color1}{HTML}{FDF2F7}
\definecolor{callout-chapter-connection-color2}{HTML}{A51C30}
\definecolor{callout-quiz-answer-color1}{HTML}{E8F2EA}
\definecolor{callout-quiz-answer-color2}{HTML}{4a7c59}
\definecolor{callout-resource-exercises-color1}{HTML}{E0F2F1}
\definecolor{callout-resource-exercises-color2}{HTML}{20B2AA}
\definecolor{callout-resource-videos-color1}{HTML}{E0F2F1}
\definecolor{callout-resource-videos-color2}{HTML}{20B2AA}
%=============%

\usepackage{hyphenat}
\usepackage{ifthen}
\usepackage{calc}
\usepackage{calculator}



\usepackage{graphicx}
\usepackage{geometry}
\usepackage{afterpage}
\usepackage{tikz}
\usetikzlibrary{calc}
\usetikzlibrary{fadings}
\usepackage[pagecolor=none]{pagecolor}


% Set the titlepage font families







% Set the coverpage font families

\usepackage{bookmark}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  pdftitle={Machine Learning Systems},
  pdfauthor={Vijay Janapa Reddi},
  colorlinks=true,
  linkcolor={Maroon},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}


\title{Machine Learning Systems}
\usepackage{etoolbox}
\makeatletter
\providecommand{\subtitle}[1]{% add subtitle to \maketitle
  \apptocmd{\@title}{\par {\large #1 \par}}{}{}
}
\makeatother
\subtitle{Volume I: Introduction}
\author{Vijay Janapa Reddi}
\date{January 28, 2026}
\begin{document}
%%%%% begin titlepage extension code

  \begin{frontmatter}

\begin{titlepage}
% This is a combination of Pandoc templating and LaTeX
% Pandoc templating https://pandoc.org/MANUAL.html#templates
% See the README for help

\thispagestyle{empty}

\newgeometry{top=-100in}

% Page color

\newcommand{\coverauthorstyle}[1]{{\fontsize{20}{24.0}\selectfont
{#1}}}

\begin{tikzpicture}[remember picture, overlay, inner sep=0pt, outer sep=0pt]

\tikzfading[name=fadeout, inner color=transparent!0,outer color=transparent!100]
\tikzfading[name=fadein, inner color=transparent!100,outer color=transparent!0]
\node[anchor=south west, rotate=0, opacity=1] at ($(current page.south west)+(0.225\paperwidth, 9)$) {
\includegraphics[width=\paperwidth, keepaspectratio]{assets/images/covers/cover-image-transparent-vol1.png}};

% Title
\newcommand{\titlelocationleft}{0.075\paperwidth}
\newcommand{\titlelocationbottom}{0.4\paperwidth}
\newcommand{\titlealign}{left}

\begin{scope}{%
\fontsize{52}{62.4}\selectfont
\node[anchor=north
west, align=left, rotate=0] (Title1) at ($(current page.south west)+(\titlelocationleft,\titlelocationbottom)$)  [text width = 0.9\paperwidth]  {{\nohyphens{Machine
Learning Systems}}};
}
\end{scope}

% Author
\newcommand{\authorlocationleft}{.925\paperwidth}
\newcommand{\authorlocationbottom}{0.150\paperwidth}
\newcommand{\authoralign}{right}

\begin{scope}
{%
\fontsize{20}{24.0}\selectfont
\node[anchor=north
east, align=right, rotate=0] (Author1) at ($(current page.south west)+(\authorlocationleft,\authorlocationbottom)$)  [text width = 6in]  {\coverauthorstyle{Vijay\\Janapa
Reddi\\}};
}
\end{scope}

% Footer
\newcommand{\footerlocationleft}{0.075\paperwidth}
\newcommand{\footerlocationbottom}{0.475\paperwidth}
\newcommand{\footerlocationalign}{left}

\begin{scope}
{%
\fontsize{25}{30.0}\selectfont
 \node[anchor=north west, align=left, rotate=0] (Footer1) at %
($(current page.south west)+(\footerlocationleft,\footerlocationbottom)$)  [text width = 0.9\paperwidth]  {{\nohyphens{Volume
I: Introduction}}};
}
\end{scope}

\end{tikzpicture}
\clearpage
\restoregeometry
%%% TITLE PAGE START

% Set up alignment commands
%Page
\newcommand{\titlepagepagealign}{
\ifthenelse{\equal{left}{right}}{\raggedleft}{}
\ifthenelse{\equal{left}{center}}{\centering}{}
\ifthenelse{\equal{left}{left}}{\raggedright}{}
}


\newcommand{\titleandsubtitle}{
% Title and subtitle
{{\huge{\bfseries{\nohyphens{Machine Learning Systems}}}}\par
}%

\vspace{\betweentitlesubtitle}
{
{\large{\textit{\nohyphens{Volume I: Introduction}}}}\par
}}
\newcommand{\titlepagetitleblock}{
\titleandsubtitle
}

\newcommand{\authorstyle}[1]{{\large{#1}}}

\newcommand{\affiliationstyle}[1]{{\large{#1}}}

\newcommand{\titlepageauthorblock}{
{\authorstyle{\nohyphens{Vijay Janapa
Reddi}{\textsuperscript{1}}\textsuperscript{,}{\textsuperscript{,*}}}}}

\newcommand{\titlepageaffiliationblock}{
\hangindent=1em
\hangafter=1
{\affiliationstyle{
{1}.~Harvard University


\vspace{1\baselineskip}
* \textit{Correspondence:}~Vijay Janapa Reddi~vj@eecs.harvard.edu
}}
}
\newcommand{\headerstyled}{%
{}
}
\newcommand{\footerstyled}{%
{\large{}}
}
\newcommand{\datestyled}{%
{January 28, 2026}
}


\newcommand{\titlepageheaderblock}{\headerstyled}

\newcommand{\titlepagefooterblock}{
\footerstyled
}

\newcommand{\titlepagedateblock}{
\datestyled
}

%set up blocks so user can specify order
\newcommand{\titleblock}{\newlength{\betweentitlesubtitle}
\setlength{\betweentitlesubtitle}{0.05\textheight}
{

{\titlepagetitleblock}
}

\vspace{4\baselineskip}
}

\newcommand{\authorblock}{{\titlepageauthorblock}

\vspace{2\baselineskip}
}

\newcommand{\affiliationblock}{{\titlepageaffiliationblock}

\vspace{0pt}
}

\newcommand{\logoblock}{}

\newcommand{\footerblock}{}

\newcommand{\dateblock}{{\titlepagedateblock}

\vspace{0pt}
}

\newcommand{\headerblock}{}

\thispagestyle{empty} % no page numbers on titlepages


\newcommand{\vrulecode}{\textcolor{black}{\rule{\vrulewidth}{\textheight}}}
\newlength{\vrulewidth}
\setlength{\vrulewidth}{2pt}
\newlength{\B}
\setlength{\B}{\ifdim\vrulewidth > 0pt 0.05\textwidth\else 0pt\fi}
\newlength{\minipagewidth}
\ifthenelse{\equal{left}{left} \OR \equal{left}{right} }
{% True case
\setlength{\minipagewidth}{\textwidth - \vrulewidth - \B - 0.1\textwidth}
}{
\setlength{\minipagewidth}{\textwidth - 2\vrulewidth - 2\B - 0.1\textwidth}
}
\ifthenelse{\equal{left}{left} \OR \equal{left}{leftright}}
{% True case
\raggedleft % needed for the minipage to work
\vrulecode
\hspace{\B}
}{%
\raggedright % else it is right only and width is not 0
}
% [position of box][box height][inner position]{width}
% [s] means stretch out vertically; assuming there is a vfill
\begin{minipage}[b][\textheight][s]{\minipagewidth}
\titlepagepagealign
\titleblock

Prof.~Vijay Janapa Reddi

School of Engineering and Applied Sciences

Harvard University

\vspace{80mm}

With heartfelt gratitude to the community for their invaluable
contributions and steadfast support.

\vfill

January 28, 2026

\vfill
\par

\end{minipage}\ifthenelse{\equal{left}{right} \OR \equal{left}{leftright} }{
\hspace{\B}
\vrulecode}{}
\clearpage
%%% TITLE PAGE END
\end{titlepage}
\setcounter{page}{1}
\end{frontmatter}

%%%%% end titlepage extension code

\renewcommand*\contentsname{Table of contents}
{
\hypersetup{linkcolor=}
\setcounter{tocdepth}{2}
\tableofcontents
}

\mainmatter
\bookmarksetup{startatroot}

\chapter*{Welcome to Volume I}\label{welcome-to-volume-i}
\addcontentsline{toc}{chapter}{Welcome to Volume I}

\markboth{Welcome to Volume I}{Welcome to Volume I}

\section*{What You Will Learn}\label{what-you-will-learn}
\addcontentsline{toc}{section}{What You Will Learn}

\markright{What You Will Learn}

Volume I progresses through four stages:

\begin{itemize}
\tightlist
\item
  \textbf{Part I: Foundations} --- Build your conceptual foundation with
  mental models that underpin all effective systems work.
\item
  \textbf{Part II: Build} --- Engineer complete workflows from data
  pipelines through training infrastructure.
\item
  \textbf{Part III: Optimize} --- Transform theoretical understanding
  into systems that run efficiently in resource-constrained
  environments.
\item
  \textbf{Part IV: Deploy} --- Navigate serving, operations, and
  responsible engineering practices.
\end{itemize}

\section*{Prerequisites}\label{prerequisites}
\addcontentsline{toc}{section}{Prerequisites}

\markright{Prerequisites}

This volume assumes:

\begin{itemize}
\tightlist
\item
  \textbf{Programming proficiency} in Python with familiarity in NumPy
\item
  \textbf{Mathematics foundations} in linear algebra, calculus, and
  probability at the undergraduate level
\item
  Prior ML experience is helpful but not required;
  \textbf{?@sec-deep-learning-systems-foundations} provides essential
  background
\end{itemize}

\section*{Support Our Mission}\label{support-our-mission}
\addcontentsline{toc}{section}{Support Our Mission}

\markright{Support Our Mission}

\section*{Continue Your Journey}\label{continue-your-journey}
\addcontentsline{toc}{section}{Continue Your Journey}

\markright{Continue Your Journey}

\section*{Listen to the AI Podcast}\label{listen-to-the-ai-podcast}
\addcontentsline{toc}{section}{Listen to the AI Podcast}

\markright{Listen to the AI Podcast}

\section*{Want to Help Out?}\label{want-to-help-out}
\addcontentsline{toc}{section}{Want to Help Out?}

\markright{Want to Help Out?}

This is a collaborative project, and your input matters. If you'd like
to contribute, check out our
\href{https://github.com/harvard-edge/cs249r_book/blob/dev/docs/contribute.md}{contribution
guidelines}. Feedback, corrections, and new ideas are welcome. Simply
file a GitHub
\href{https://github.com/harvard-edge/cs249r_book/issues}{issue}.

\bookmarksetup{startatroot}

\chapter{Data Efficiency}\label{sec-data-efficiency}

\marginnote{\begin{footnotesize}

\emph{DALL·E 3 Prompt: A futuristic digital illustration depicting the
concept of data efficiency in machine learning. On one side of the
image, there is a sleek, powerful computing unit, symbolizing AI
processing. On the other side, streams of binary code (1s and 0s) flow
into the computer, but the data is represented with glowing golden
elements, signifying valuable, high-quality information. The background
has a high-tech, digital ambiance, emphasizing the role of refined,
efficient data in machine learning. No text, only a strong visual
representation of the relationship between computation and valuable
data.}

\end{footnotesize}}

\noindent
\pandocbounded{\includegraphics[keepaspectratio]{contents/vol1/data_efficiency/images/png/cover_data_efficiency.png}}

\section*{Purpose}\label{purpose}
\addcontentsline{toc}{section}{Purpose}

\markright{Purpose}

\emph{Why does collecting more data often produce smaller improvements
than expected, while smaller, curated datasets sometimes outperform
massive ones?}

Naive scaling assumes data is homogeneous, that each sample contributes
equally to learning. Reality differs dramatically: in typical
large-scale datasets, a small fraction of examples provides most of the
gradient signal, while the majority are redundant, noisy, or misaligned
with the target distribution. This heterogeneity explains both the
diminishing returns of brute-force data collection and the surprising
effectiveness of intelligent curation. A carefully selected 10\% subset
can often match the accuracy of the full dataset while reducing training
time by 90\%. This is not a statistical curiosity but a systems
optimization opportunity: data efficiency operates upstream of both
model compression and hardware acceleration, reducing the total
computation required to \emph{train} models rather than accelerating
predetermined workloads. The techniques in this chapter (coreset
selection, deduplication, active learning, and curriculum design)
transform data from a quantity to be accumulated into a resource to be
engineered.

\begin{tcolorbox}[enhanced jigsaw, breakable, colbacktitle=quarto-callout-tip-color!10!white, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Learning Objectives}, colback=white, bottomtitle=1mm, bottomrule=.15mm, coltitle=black, rightrule=.15mm, opacityback=0, opacitybacktitle=0.6, toptitle=1mm, toprule=.15mm, titlerule=0mm, arc=.35mm, colframe=quarto-callout-tip-color-frame, leftrule=.75mm, left=2mm]

\begin{itemize}
\item
  Understand data efficiency as the third pillar of ML optimization
  alongside algorithms and systems
\item
  Apply the Information-Compute Ratio (ICR) framework to evaluate
  dataset value
\item
  Implement the three-stage optimization pipeline: static pruning,
  dynamic selection, and synthetic generation
\item
  Select appropriate coreset and deduplication techniques for
  pre-training data reduction
\item
  Design active learning and curriculum learning strategies for
  training-time optimization
\item
  Analyze the cost-benefit trade-offs of data efficiency techniques
  using ROI frameworks
\item
  Address systems engineering challenges: selection bottleneck, I/O
  patterns, distributed selection, and data loader design
\end{itemize}

\end{tcolorbox}

Before exploring techniques to improve data efficiency, we need a formal
framework for measuring it. What exactly do we mean by ``efficient'' use
of data?

\phantomsection\label{callout-definitionux2a-1.1}
\begin{fbx}{callout-definition}{Definition: }{Data Efficiency}
\phantomsection\label{callout-definition*-1.1}
\textbf{Data Efficiency} refers to the ratio of \emph{model capability
gained} to \emph{data resources consumed}, encompassing acquisition,
labeling, storage, and compute costs. A data-efficient system maximizes
learning per sample, per byte stored, per label acquired, and per FLOP
expended on data processing. Formally:

\[
\text{Data Efficiency} = \frac{\Delta \text{Model Capability}}{\Delta \text{Data Cost}}
\]

where Data Cost encompasses:

\begin{itemize}
\tightlist
\item
  \textbf{Acquisition cost}: Time and money to collect or generate
  samples
\item
  \textbf{Labeling cost}: Human expert annotation effort
\item
  \textbf{Storage cost}: Bytes required to persist the dataset
\item
  \textbf{Compute cost}: FLOPs to process samples during training
\end{itemize}

A perfectly efficient dataset would contain only samples that contribute
unique information to the model's decision boundary---no redundancy, no
noise, no ``easy'' examples already mastered.

\end{fbx}

\section{The Data Wall: Why Efficiency Matters
Now}\label{sec-data-efficiency-data-wall-efficiency-matters-febc}

For decades, the dominant strategy in machine learning was simple:
\textbf{more data, better models}. This intuition, codified in scaling
laws (\citeproc{ref-kaplan2020scaling}{Kaplan et al. 2020};
\citeproc{ref-hoffmann2022training}{Hoffmann et al. 2022}), showed that
model performance improves predictably with dataset size. Teams
responded rationally by scraping more web pages, labeling more images,
and generating more synthetic examples. But this era is ending, and a
fundamental asymmetry has emerged: hardware acceleration has outpaced
data availability.

The machine learning field has hit what researchers call the
\textbf{Data Wall}\sidenote{\textbf{Data Wall}: A term popularized by
Epoch AI researchers in 2022. Their analysis projected that high-quality
language data (books, academic papers, filtered web text) could be
exhausted between 2026 and 2032 at then-current scaling rates, with
subsequent updates centering estimates around 2028. The ``wall''
metaphor emphasizes that unlike compute (which can be purchased) or
algorithms (which can be improved), the stock of human-generated
training data grows slowly and may represent a fundamental limit to
scaling. } (\citeproc{ref-villalobos2022will}{Villalobos et al. 2022}):
the empirical observation that high-quality training data is growing far
slower than compute capacity.

While GPU compute capacity has increased at rates faster than
traditional Moore's Law (with AI-specific workloads seeing particularly
rapid gains), the supply of novel, high-quality human-generated text and
images grows at perhaps 2\(\times\) per decade. The internet has already
been scraped. Domain experts cannot label faster.

Figure~\ref{fig-running-out-of-human-data} illustrates this trajectory:
foundation models are consuming the stock of human-generated text at an
accelerating rate, with projections suggesting exhaustion of
high-quality public data within the next few years. This is not a
distant concern. It shapes training strategies today.

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{contents/vol1/data_efficiency/images/png/running_out_of_data.png}}

}

\caption{\label{fig-running-out-of-human-data}\textbf{Dataset Growth
Approaching Limits}: Foundation models are increasingly trained on vast
datasets, approaching the total stock of human-generated text. Current
projections suggest that high-quality public text data may be exhausted
between 2026 and 2032, forcing a shift toward data efficiency, synthetic
generation, and multimodal learning. Source: Sevilla et al.
(\citeproc{ref-villalobos_ho_sevilla_besiroglu_heim_hobbhahn_2024}{2022}).}

\end{figure}%

The asymmetry is stark:

\phantomsection\label{callout-perspectiveux2a-1.2}
\begin{fbx}{callout-perspective}{Systems Perspective: }{The Scaling Asymmetry}
\phantomsection\label{callout-perspective*-1.2}
\textbf{The Problem}: Compute scales exponentially. Data does not.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2476}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.1714}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.5714}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Resource}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Growth Rate}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Implication}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{GPU Compute} & \textasciitilde10× / 3 years & Hardware vendors
deliver reliable exponential gains \\
\textbf{Training Data (Web)} & \textasciitilde2× / 5 years &
High-quality web text is finite; much already scraped \\
\textbf{Labeled Data} & \textasciitilde1.5× / 5 years & Human annotation
throughput is fundamentally bounded \\
\textbf{Synthetic Data} & Unbounded & But bounded by generator quality
(risk of model collapse) \\
\end{longtable}

\textbf{The Consequence}: In 2020, compute and data were roughly
balanced for frontier models. By 2025, compute budgets can support
training runs 10--100\(\times\) larger than available high-quality data
can fill. We are \textbf{compute-rich and data-poor}.

\end{fbx}

This asymmetry inverts the optimization priority. When data was abundant
and compute was scarce, the right strategy was algorithmic efficiency:
squeeze more accuracy from limited GPU cycles. Now that compute is
abundant and \emph{quality data} is scarce, the winning strategy is
\textbf{data efficiency}: squeeze more learning from each sample.

To make this concrete, consider training a model in the
\textbf{GPT-2/Llama Lighthouse} family
(\textbf{?@sec-dnn-architectures}), a 70B parameter language model:

\begin{itemize}
\tightlist
\item
  \textbf{Compute available}: 10,000 H100 GPUs for 3 months represents
  tens of millions of dollars in compute budget, capable of processing
  over 10 trillion tokens
\item
  \textbf{High-quality data available}: \textasciitilde5 trillion tokens
  of deduplicated, filtered web text
\item
  \textbf{The gap}: 3× more compute than data can utilize
\end{itemize}

The team faces a choice: (1) train on the same data multiple epochs
(diminishing returns after epochs 2--3), (2) lower quality thresholds to
include more data (degrades model quality), or (3) invest in data
efficiency through better filtering, curriculum design, and synthetic
augmentation to extract more learning from each token. Option 3 is
increasingly the dominant approach.

This data efficiency imperative applies across model architectures,
though the bottlenecks differ. Unlike our compute-bound ResNet-50
Lighthouse, GPT-2/Llama models are \textbf{memory bandwidth-bound}
during inference but still benefit enormously from data efficiency
during training. Each token processed requires the same forward/backward
pass cost regardless of model bottleneck, so fewer tokens means fewer
FLOPs.

The Data Wall explains why data efficiency has shifted from academic
curiosity to industrial necessity. Companies training frontier models
are no longer bottlenecked by GPU access. They are bottlenecked by the
quality and diversity of their training corpora. The differentiator is
not who has the most data but who extracts the most \emph{information}
from their data.

\section{Data Efficiency as a Systems
Problem}\label{sec-data-efficiency-data-efficiency-systems-problem-d857}

Understanding the Data Wall establishes \emph{why} data efficiency
matters; the question becomes \emph{how} to approach it. Data efficiency
is typically framed as a machine learning problem: \emph{how do I
achieve the same accuracy with fewer samples?} This framing focuses on
statistical sample complexity and generalization theory. While valid, it
misses the larger picture.

In this textbook, we adopt a \textbf{systems framing}: \emph{how do I
reduce the total cost of achieving target performance across the entire
ML lifecycle?} This shifts attention from accuracy curves to resource
consumption:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 2\tabcolsep) * \real{0.4138}}
  >{\raggedright\arraybackslash}p{(\linewidth - 2\tabcolsep) * \real{0.5862}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
ML Framing
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Systems Framing
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
``Fewer samples for same accuracy'' & ``Fewer FLOPs for same
accuracy'' \\
``Better generalization'' & ``Lower training cost (time, money,
energy)'' \\
``Sample complexity bounds'' & ``End-to-end resource efficiency'' \\
``Learning theory'' & ``Cost engineering'' \\
\end{longtable}

The systems framing reveals optimization opportunities invisible to the
ML framing.

\phantomsection\label{callout-perspectiveux2a-1.3}
\begin{fbx}{callout-perspective}{Systems Perspective: }{Data Efficiency and the Iron Law}
\phantomsection\label{callout-perspective*-1.3}
In the \textbf{Iron Law of ML Systems}
(\(L = \frac{D}{B} + \frac{Ops}{P \cdot \eta} + L_{fixed}\)), data
efficiency is the only technique that reduces the \textbf{Total
Operations} term at its source. Model compression reduces operations per
sample; hardware acceleration increases throughput per operation. But
data efficiency reduces the number of samples processed entirely.

\begin{itemize}
\tightlist
\item
  \textbf{Model compression}: Reduces \(Ops\) per forward/backward pass
\item
  \textbf{Hardware acceleration}: Increases \(P\) (peak throughput) and
  \(\eta\) (utilization)
\item
  \textbf{Data efficiency}: Reduces the number of passes through the
  entire equation
\end{itemize}

This makes data efficiency multiplicatively valuable: a 2× reduction in
dataset size with 2× model compression and 2× hardware acceleration
yields 8× total cost reduction, not 6×.

\end{fbx}

Consider training cost reduction: a 50\% reduction in dataset size does
not just improve sample efficiency but directly halves the number of
forward passes, backward passes, and gradient updates. For a \$100M
training run, this translates to \$50M in compute savings. The
relationship is linear and immediate.

These compute savings cascade into storage and I/O costs. Large datasets
consume petabytes of storage and saturate network bandwidth during
distributed training. Data efficiency techniques like deduplication
reduce storage costs and eliminate I/O bottlenecks that can idle
expensive GPU clusters.

Perhaps most significantly, data efficiency transforms labeling
economics. Expert labeling costs
(\(5–100+ per sample in domains like medical imaging) often exceed compute costs. Active learning and semi-supervised methods are not merely algorithmic techniques but cost engineering that can reduce labeling budgets by 10–100\)\times\$.

The environmental implications are equally significant. Training a large
language model can emit hundreds of tons of CO₂. Data efficiency is the
most direct lever for Green AI: halving the dataset halves training
energy, with no accuracy trade-off if done correctly.

Finally, smaller and curated datasets enable faster iteration velocity.
A team that can iterate in hours rather than days has a compounding
advantage in model development.

\phantomsection\label{callout-perspectiveux2a-1.4}
\begin{fbx}{callout-perspective}{Systems Perspective: }{The Systems Engineer's View of Data}
\phantomsection\label{callout-perspective*-1.4}
\textbf{The ML Researcher asks:} ``What is the sample complexity of this
learning problem?''

\textbf{The Systems Engineer asks:} ``What is the
cost-per-accuracy-point across the entire pipeline---from data
acquisition through deployment?''

This chapter equips you with the systems engineer's toolkit: techniques
to minimize total cost, metrics to quantify efficiency gains, and
architectural patterns to implement data efficiency at scale.

\end{fbx}

\section{The Information-Compute
Ratio}\label{sec-data-efficiency-informationcompute-ratio-8e9b}

With the systems framing established, we need a quantitative metric to
reason about data efficiency. The Optimize Principles introduced the
\textbf{Pareto Frontier} as the boundary where improving one metric
necessarily degrades another. We identified three pillars of efficiency:
model, hardware, and data. Having covered how to compress models
(\textbf{?@sec-model-compression}) and accelerate hardware
(\textbf{?@sec-ai-acceleration}), we now tackle the third pillar with a
central metric: the Information-Compute Ratio.

While model compression and hardware acceleration focus on the
\emph{execution} of the math, \textbf{Data Efficiency} reduces the
\emph{amount} of math required by optimizing what enters the training
pipeline.

Data engineering (\textbf{?@sec-data-engineering-ml}) ensures that data
is clean, accessible, and correctly formatted. Data efficiency asks a
different question: \emph{how much information does each sample
contribute to the model's learning per unit of computation?}

In the optimization triad (Figure~\ref{fig-optimization-triad}), data
efficiency plays the role of \textbf{Input Optimization}. While model
compression minimizes the math per parameter and hardware acceleration
maximizes the math per second, data efficiency minimizes the total math
required to reach convergence.

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/9393f588f2f78ee1129fac5513aca382371e1790.pdf}}

}

\caption{\label{fig-optimization-triad}\textbf{The Optimization Triad}:
Machine learning performance relies on three pillars: Algorithms
(models), Systems (hardware/software), and Data Efficiency. While
algorithms and systems have traditionally received the most attention,
optimizing data efficiency (Input Optimization) offers a third, powerful
lever for scaling performance.}

\end{figure}%

We can formalize this as the \textbf{Information-Compute Ratio (ICR)}:

\[
\text{ICR} = \frac{\Delta \text{Model Performance}}{\Delta \text{FLOPs}}
\]

A random batch of raw data often has low ICR: it contains redundant
examples, noisy samples, or ``easy'' examples the model has already
mastered. Training on such a batch wastes GPU cycles on zero-information
updates. High-efficiency data pipelines
(Figure~\ref{fig-data-efficiency-pipeline}) filter, order, and
synthesize data to maximize ICR, ensuring that every FLOP contributes to
learning. Later in this chapter,
Section~\ref{sec-data-efficiency-measuring-data-efficiency-7957}
provides the complete measurement framework for evaluating these
efficiency gains, including the Data Roofline model that diagnoses
whether a system is data-bound or compute-bound.

\phantomsection\label{callout-exampleux2a-1.5}
\begin{fbx}{callout-example}{Example: }{Worked Example: Computing ICR for Coreset vs. Random Selection}
\phantomsection\label{callout-example*-1.5}
\textbf{Scenario}: Training our \textbf{ResNet-50 Lighthouse model}
(\textbf{?@sec-dnn-architectures}) on ImageNet for one epoch. We compare
random batch selection versus EL2N-based coreset selection. ResNet-50's
compute-bound nature (high arithmetic intensity) makes it an ideal
candidate for data efficiency optimization---reducing dataset size
directly reduces training FLOPs with minimal I/O impact.

\textbf{Setup}:

\begin{itemize}
\tightlist
\item
  Dataset: ImageNet (1.28M images)
\item
  Model: ResNet-50 Lighthouse (\textasciitilde4 GFLOPs per forward pass,
  \textasciitilde8 GFLOPs forward + backward)
\item
  One epoch: 1.28M × 8 GFLOPs = \textbf{1.02 × 10\^{}16 FLOPs}
\item
  Accuracy improvement per epoch (early training): \textasciitilde5\%
  points
\end{itemize}

\textbf{Random Selection (baseline)}:

\begin{itemize}
\tightlist
\item
  Process all 1.28M samples uniformly
\item
  Accuracy gain: 5.0 percentage points
\item
  ICR\_random = 5.0 / (1.02 × 10\^{}16) = \textbf{4.9 × 10\^{}-16 per
  FLOP}
\end{itemize}

\textbf{EL2N Coreset (50\% of data)}:

\begin{itemize}
\tightlist
\item
  Process 640K high-uncertainty samples selected by EL2N scoring
\item
  Coreset focuses on decision boundary samples
\item
  Accuracy gain: 4.5 percentage points (90\% of full data performance)
\item
  Compute: 640K × 8 GFLOPs = \textbf{5.1 × 10\^{}15 FLOPs}
\item
  ICR\_coreset = 4.5 / (5.1 × 10\^{}15) = \textbf{8.8 × 10\^{}-16 per
  FLOP}
\end{itemize}

\textbf{Result}: The coreset achieves \textbf{1.8\(\times\) higher ICR},
nearly twice the learning per FLOP, by eliminating low-information
``easy'' samples that contribute little to the decision boundary. The
0.5 percentage point accuracy difference is often acceptable given the
50\% compute savings.

\end{fbx}

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/fd6a8622f838e8a9a8bac609f7b535a11a6c0b2a.pdf}}

}

\caption{\label{fig-data-efficiency-pipeline}\textbf{The Data Efficiency
Pipeline}: A structured approach to increasing data value. Raw data is
first pruned to remove redundancy (Static Pruning), then dynamically
selected during training (Active Learning), and finally augmented to
increase diversity (Synthesis). Each stage increases the
Information-Compute Ratio (ICR).}

\end{figure}%

This chapter explores three strategies to maximize this ratio:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Static Data Pruning}: Removing low-value samples before
  training begins (Coresets, Deduplication).
\item
  \textbf{Dynamic Selection}: Selecting high-value samples during
  training (Curriculum Learning, Active Learning).
\item
  \textbf{Synthetic Generation}: Creating high-value samples on demand
  (Augmentation, Distillation).
\end{enumerate}

We begin with static pruning, the techniques that can reduce your
dataset by 30 to 50 percent before you even start training.

\section{Static Data Pruning: Pre-Training
Filtration}\label{sec-data-efficiency-static-data-pruning-pretraining-filtration-d6e6}

Before a single gradient is computed, we can dramatically improve
efficiency by removing low-value samples from the dataset. This
pre-training filtration reduces the total computation required without
affecting, and sometimes improving, final model accuracy. The techniques
in this section operate on the dataset itself, requiring no changes to
the training loop or model architecture.

\subsection{The Case for Smaller
Datasets}\label{sec-data-efficiency-case-smaller-datasets-0336}

The most counterintuitive insight in data efficiency is that training on
\emph{less} data often produces models just as accurate as training on
the full dataset. Machine learning practitioners have long operated
under the assumption that more data yields better performance, and while
this holds in many scenarios, it obscures a critical reality: typical
large-scale datasets contain massive redundancy. Empirical studies on
coreset selection and data pruning have consistently demonstrated this
redundancy across standard benchmarks:

\begin{itemize}
\tightlist
\item
  \textbf{ImageNet-1K}: Studies using gradient-based selection (EL2N,
  GraNd) (\citeproc{ref-paul2021deep}{Paul, Ganguli, and Dziugaite
  2021}) have shown that training on 50\% of ImageNet with carefully
  selected samples achieves within 1\% of full-dataset accuracy. The
  savings: 50\% fewer training FLOPs.
\item
  \textbf{CIFAR-10}: Because CIFAR-10 is smaller and more redundant,
  aggressive pruning works even better. Experiments report that 10--30\%
  of samples (selected by forgetting scores
  (\citeproc{ref-toneva2019empirical}{Toneva et al. 2019}) or
  margin-based methods) can match 90\%+ of original accuracy, a
  3--10\(\times\) reduction.
\item
  \textbf{Large Language Model Corpora}: Web-scraped datasets like The
  Pile and C4 contain substantial exact and near-duplicate content.
  Deduplication studies (\citeproc{ref-lee2022deduplicating}{Lee et al.
  2022}) report 10--30\% redundancy ratios, with deduplicated training
  yielding \emph{better} downstream performance (less memorization, more
  generalization).
\end{itemize}

\begin{tcolorbox}[enhanced jigsaw, breakable, colbacktitle=quarto-callout-warning-color!10!white, title=\textcolor{quarto-callout-warning-color}{\faExclamationTriangle}\hspace{0.5em}{Caveat: Results Vary by Task and Model}, colback=white, bottomtitle=1mm, bottomrule=.15mm, coltitle=black, rightrule=.15mm, opacityback=0, opacitybacktitle=0.6, toptitle=1mm, toprule=.15mm, titlerule=0mm, arc=.35mm, colframe=quarto-callout-warning-color-frame, leftrule=.75mm, left=2mm]

These numbers are benchmark-specific. Gains from pruning depend on the
dataset's intrinsic redundancy, the selection algorithm, and the model
architecture. Always validate on your specific task before deploying
aggressive pruning in production.

\end{tcolorbox}

The key insight is that not all data points provide equal value for
training. Some samples are highly informative, capturing decision
boundaries or rare patterns, while others are repetitive or trivially
easy. Training on redundant data wastes compute without improving the
model. The practical question then becomes: how do we identify which
samples to keep?

\subsection{Coreset Selection
Algorithms}\label{sec-data-efficiency-coreset-selection-algorithms-a520}

Coreset selection\sidenote{\textbf{Coreset}: The term ``coreset''
combines ``core'' and ``set,'' reflecting its purpose as a core
representative subset. The concept emerged from computational geometry
in the early 2000s, where researchers sought provably small subsets that
approximate solutions to geometric optimization problems. For ML
applications, coresets provide theoretical guarantees: a
well-constructed coreset of size independent of the original dataset can
approximate the full dataset's loss function within a factor of (1 + ε).
} answers this question by identifying a small subset of data that
preserves the statistical properties of the entire dataset.

The goal is to find a compact set of examples that allows a model to
generalize as well as it would if trained on the full dataset. Several
algorithmic families have proven effective, each with distinct
computational trade-offs.

Geometry-based methods select samples that cover the data distribution
without requiring any model training. The k-Center
algorithm\sidenote{\textbf{k-Center Algorithm}: Dorit Hochbaum and David
Shmoys established the modern approach to this problem in 1985, proving
that their 2-approximation algorithm is ``best possible''---no
polynomial-time algorithm can achieve a better approximation factor
unless P=NP. The algorithm's origin in facility location (placing
warehouses to minimize maximum customer distance) explains why it
transfers well to coreset selection: both seek coverage of a space with
minimal representatives. } (also known as Facility Location) selects
samples that minimize the maximum distance from any point to its nearest
selected center, ensuring coverage of the entire data manifold.

Herding takes a different approach, iteratively selecting samples whose
features best approximate the mean of the full dataset, thereby
maintaining distributional fidelity. These methods are computationally
attractive because they operate purely on feature representations, but
they ignore label information entirely.

Gradient-based methods offer higher selection quality by using training
dynamics to identify important samples, though they require training a
proxy model first. GraNd (Gradient Normed) and EL2N (Error
L2-Norm)\sidenote{\textbf{EL2N and GraNd}: Introduced by Mansheej Paul
and colleagues at NeurIPS 2021 in their paper ``Deep Learning on a Data
Diet.'' These scores identify important examples using only information
from the first few training epochs, unlike forgetting-based methods that
require full training. The key insight: samples the model finds
uncertain early in training remain important throughout, and these
scores transfer across architectures---scores computed on ResNet-18
predict importance for ResNet-50. } score samples by gradient magnitude
or prediction error early in training; high-scoring samples lie near the
decision boundary and are most informative for learning. Forgetting
Events\sidenote{\textbf{Forgetting Events}: Coined by Mariya Toneva and
colleagues at ICLR 2019. A ``forgetting event'' occurs when a sample
transitions from correctly to incorrectly classified during
training---the opposite of a learning event. The surprising finding: a
large fraction of samples are never forgotten once learned, and these
``unforgettable'' examples can be safely pruned with minimal accuracy
impact. } tracks how often a sample is ``forgotten'' (correctly
classified, then later misclassified) during training, identifying
harder and more valuable examples.

These gradient-based approaches generally outperform geometry-based
methods in selection quality but incur the overhead of proxy model
training.

Table~\ref{tbl-coreset-comparison} quantifies the computational
trade-offs between these approaches:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.1478}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.1652}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2087}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2087}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2435}}@{}}
\caption{Comparison of coreset selection algorithms. N = dataset size, K
= coreset size. Gradient-based methods generally outperform
geometry-based methods but require proxy model
training.}\label{tbl-coreset-comparison}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Method}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Compute Cost}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Requires Training}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Best For}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Limitation}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Method}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Compute Cost}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Requires Training}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Best For}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Limitation}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{k-Center} & O(N²) or O(NK) & No & Coverage, exploration &
Ignores label information \\
\textbf{Herding} & O(NK) & No & Distribution matching & Assumes
Gaussian-like \\
\textbf{GraNd} & O(epochs × N) & Yes (few epochs) & Decision boundaries
& Requires proxy training \\
\textbf{Forgetting} & O(full training) & Yes (full) & Hard examples &
Expensive to compute \\
\textbf{EL2N} & O(epochs × N) & Yes (few epochs) & Uncertainty sampling
& Best with proxy model \\
\end{longtable}

Figure~\ref{fig-coreset-selection} illustrates the core insight behind
coreset methods: samples near the decision boundary (high uncertainty)
are more informative than samples deep within class regions (low
uncertainty). Random sampling wastes budget on redundant ``easy''
examples.

\begin{figure}[htb]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/bfe219306e5ede967f3149488c9ee13f76708565.pdf}}

}

\caption{\label{fig-coreset-selection}\textbf{Coreset Selection
Strategy}: Random sampling (left) selects uniformly, wasting budget on
easy samples far from the decision boundary. Coreset selection (right)
prioritizes samples near the boundary where the model is uncertain,
capturing more information per sample.}

\end{figure}%

Given these trade-offs, most practitioners find that EL2N with a small
proxy model offers the best balance of selection quality and
computational cost. The approach is straightforward: train a lightweight
model (for example, ResNet-18 instead of ResNet-50) for 5 to 10 epochs,
compute EL2N scores for all samples, then select the highest-scoring
subset. The proxy does not need to be accurate; it only needs to
identify which samples are hard. This upfront investment in proxy
training typically yields substantial returns when the coreset reduces
subsequent training by 50\% or more.

\phantomsection\label{callout-exampleux2a-1.6}
\begin{fbx}{callout-example}{Example: }{Coreset Selection in Practice}
\phantomsection\label{callout-example*-1.6}
\textbf{Scenario}: You have 1 million training images and want to reduce
to 100,000 (10\%) for faster experimentation.

\textbf{Naive Approach}: Random sampling loses rare classes and edge
cases.

\textbf{Coreset Approach}: 1. Train a small proxy model for 5 epochs 2.
Compute EL2N scores for all samples 3. Select the 100,000 samples with
highest uncertainty 4. Train your full model on this coreset

\textbf{Result}: The coreset often achieves \textbf{higher accuracy}
than random sampling because it focuses on the decision boundary rather
than redundant ``easy'' examples.

\end{fbx}

The following pseudocode shows how to compute EL2N scores and select a
coreset:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ compute\_el2n\_scores(model, dataloader, num\_epochs}\OperatorTok{=}\DecValTok{5}\NormalTok{):}
    \CommentTok{"""Compute EL2N scores: L2 norm of (prediction {-} one\_hot\_label)."""}
    \CommentTok{\# Train proxy model for a few epochs to get meaningful predictions}
\NormalTok{    train\_proxy(model, dataloader, num\_epochs)}

\NormalTok{    scores }\OperatorTok{=}\NormalTok{ []}
\NormalTok{    model.}\BuiltInTok{eval}\NormalTok{()}
    \ControlFlowTok{for}\NormalTok{ x, y }\KeywordTok{in}\NormalTok{ dataloader:}
\NormalTok{        logits }\OperatorTok{=}\NormalTok{ model(x)}
\NormalTok{        probs }\OperatorTok{=}\NormalTok{ softmax(logits, dim}\OperatorTok{=}\DecValTok{1}\NormalTok{)}
        \CommentTok{\# One{-}hot encode labels}
\NormalTok{        one\_hot }\OperatorTok{=}\NormalTok{ zeros\_like(probs).scatter\_(}\DecValTok{1}\NormalTok{, y.unsqueeze(}\DecValTok{1}\NormalTok{), }\DecValTok{1}\NormalTok{)}
        \CommentTok{\# EL2N score = L2 distance from confident prediction}
\NormalTok{        el2n }\OperatorTok{=}\NormalTok{ (probs }\OperatorTok{{-}}\NormalTok{ one\_hot).norm(dim}\OperatorTok{=}\DecValTok{1}\NormalTok{)  }\CommentTok{\# High = uncertain}
\NormalTok{        scores.extend(el2n.tolist())}
    \ControlFlowTok{return}\NormalTok{ scores}


\KeywordTok{def}\NormalTok{ select\_coreset(scores, dataset, fraction}\OperatorTok{=}\FloatTok{0.1}\NormalTok{):}
    \CommentTok{"""Select top{-}k highest{-}scoring (most uncertain) samples."""}
\NormalTok{    k }\OperatorTok{=} \BuiltInTok{int}\NormalTok{(}\BuiltInTok{len}\NormalTok{(dataset) }\OperatorTok{*}\NormalTok{ fraction)}
    \CommentTok{\# Sort by score descending (highest uncertainty first)}
\NormalTok{    indices }\OperatorTok{=}\NormalTok{ argsort(scores, descending}\OperatorTok{=}\VariableTok{True}\NormalTok{)[:k]}
    \ControlFlowTok{return}\NormalTok{ Subset(dataset, indices)}


\CommentTok{\# Usage: 10x data reduction with minimal accuracy loss}
\NormalTok{scores }\OperatorTok{=}\NormalTok{ compute\_el2n\_scores(proxy\_model, full\_loader)}
\NormalTok{coreset }\OperatorTok{=}\NormalTok{ select\_coreset(scores, full\_dataset, fraction}\OperatorTok{=}\FloatTok{0.1}\NormalTok{)}
\NormalTok{train\_full\_model(model, coreset)  }\CommentTok{\# 10x faster training}
\end{Highlighting}
\end{Shaded}

\subsection{Data
Deduplication}\label{sec-data-efficiency-data-deduplication-9035}

While coreset selection identifies which samples to keep based on their
informativeness, a complementary approach targets what to remove: exact
and near-duplicates. Unlike coreset selection, deduplication provides
immediate efficiency gains with no accuracy penalty and requires no
model training whatsoever. This makes deduplication the most accessible
optimization in data efficiency, offering guaranteed compute savings
with zero risk of degrading model quality.

The simplest form of deduplication uses hash-based methods for exact
matches. By computing a cryptographic hash (MD5 or SHA-256) for each
sample and removing those with identical hashes, practitioners can
eliminate byte-for-byte duplicates that inevitably accumulate in large
web-scraped corpora. This process is computationally cheap, scaling
linearly with dataset size, and can be parallelized trivially.

Near-duplicate detection addresses the more subtle problem of
semantically redundant content that differs at the byte level. For text,
MinHash\sidenote{\textbf{MinHash}: Invented by Andrei Broder in 1997,
originally to detect duplicate web pages for the AltaVista search
engine. The algorithm uses random hash functions to create compact
``signatures'' that preserve set similarity---two documents with similar
content produce similar signatures with high probability. Broder
received the 2012 ACM Kanellakis Award for this work, recognizing its
foundational impact on web-scale similarity detection. } with
Locality-Sensitive Hashing (LSH) approximates Jaccard similarity
efficiently, detecting paraphrased or lightly edited content.

For images, perceptual hashing produces signatures robust to minor
transformations like resizing and compression, identifying visually
identical images stored in different formats. Embedding-based similarity
offers the most powerful detection by computing dense representations
(CLIP for images, sentence transformers for text) and clustering similar
items, though this approach incurs higher computational overhead.

For foundation model pre-training, deduplication has become essential
rather than optional. Studies on GPT-3 and LLaMA training demonstrate
that deduplicated data improves both training efficiency and downstream
performance by preventing memorization of repeated content. The benefit
is twofold: fewer wasted FLOPs on redundant samples, and better
generalization because the model sees more diverse examples per training
token.

\begin{tcolorbox}[enhanced jigsaw, breakable, colbacktitle=quarto-callout-note-color!10!white, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Lighthouse Connection: DLRM and Embedding Deduplication}, colback=white, bottomtitle=1mm, bottomrule=.15mm, coltitle=black, rightrule=.15mm, opacityback=0, opacitybacktitle=0.6, toptitle=1mm, toprule=.15mm, titlerule=0mm, arc=.35mm, colframe=quarto-callout-note-color-frame, leftrule=.75mm, left=2mm]

Our \textbf{DLRM Lighthouse model} (\textbf{?@sec-dnn-architectures})
presents a unique deduplication challenge. Recommendation systems are
memory capacity-bound, with embedding tables consuming terabytes of
storage for billions of user/item IDs. Much of this capacity is wasted
on \textbf{cold embeddings}---IDs that appear rarely in training data.

Data efficiency for DLRM focuses on \textbf{interaction deduplication}
(removing redundant user-item pairs) and \textbf{embedding pruning}
(removing or sharing cold embeddings). A 20\% reduction in unique
interactions can reduce embedding table size by 30--40\%, directly
addressing DLRM's primary bottleneck: memory capacity rather than
compute.

\end{tcolorbox}

\subsection{Data Pruning by
Quality}\label{sec-data-efficiency-data-pruning-quality-72ea}

Deduplication removes redundant samples, but a third category of
problematic data remains: samples that are actively harmful to learning.
Quality-based pruning eliminates samples that either contribute no
meaningful signal or introduce contradictory information that confuses
the optimization process.

Label error detection represents the most impactful form of quality
pruning. Tools like Cleanlab identify samples where the assigned label
is likely incorrect based on model confidence patterns across training.
A sample that the model consistently predicts as class A but is labeled
class B either represents a hard case near the decision boundary or,
more commonly, an annotation mistake. Removing or correcting these
mislabeled samples prevents the model from learning contradictory
signals that degrade its decision boundary.

Outlier removal addresses a different pathology: samples far from any
cluster center in feature space. While outliers might represent valuable
edge cases, they more often indicate noise, annotation errors, or data
corruption. The key is distinguishing between informative outliers (rare
but valid examples of a class) and noise (samples that do not belong to
any class). Conservative thresholds help avoid discarding genuinely rare
examples.

Low-information filtering applies domain-specific heuristics to remove
samples that lack sufficient signal for learning. For text corpora, this
means removing documents below a perplexity threshold or with low
semantic coherence, often indicative of machine-generated spam or
garbled content. For image datasets, filtering targets blurry,
corrupted, or near-uniform samples that provide little visual
information.

Together, these three static pruning techniques (coreset selection,
deduplication, and quality filtering) demonstrate that careful curation
before training yields significant efficiency gains. The compute savings
are multiplicative across the entire training process: a 50\% dataset
reduction means 50\% fewer forward passes, backward passes, and gradient
updates across all training epochs. For a model trained for 100 epochs,
this translates to 50 epochs worth of saved compute, representing
substantial reductions in both training time and energy consumption.

\section{Dynamic Data Selection: Training-Time
Optimization}\label{sec-data-efficiency-dynamic-data-selection-trainingtime-optimization-cd62}

Static pruning techniques commit to a fixed dataset before training
begins, but this raises a fundamental question: what if the optimal
training samples change as the model learns? Early in training, the
model benefits from diverse coverage to build broad feature
representations; later, it benefits from focusing on hard examples near
the decision boundary to refine its predictions. Dynamic selection
exploits this insight by optimizing which samples to use \emph{during}
training, adapting the data diet based on the model's evolving state.

\subsection{Curriculum Learning: Easy to
Hard}\label{sec-data-efficiency-curriculum-learning-easy-hard-3428}

The first dynamic selection technique, \textbf{curriculum
learning}\sidenote{\textbf{Curriculum Learning}: Formalized by Yoshua
Bengio and colleagues at ICML 2009, drawing explicit inspiration from
human education where students master basics before advanced topics. The
paper's key insight was that curriculum learning acts as a
``continuation method'' for non-convex optimization: starting with easy
examples smooths the loss landscape, helping the optimizer find better
local minima. The paper has accumulated thousands of citations,
reflecting its influence on training methodology. }
(\citeproc{ref-bengio2009curriculum}{Bengio et al. 2009};
\citeproc{ref-soviany2022curriculum}{Soviany et al. 2022}), structures
the order in which data is presented to the model. Instead of random
shuffling, it starts with simpler examples and gradually introduces more
complex ones, mirroring how humans learn by mastering fundamentals
before advancing to harder material.

The effectiveness of curriculum learning stems from how neural networks
respond to gradient signals at different training stages. Easy examples
provide clear, consistent gradients that establish good feature
representations early in training when the loss landscape is highly
irregular. Hard examples introduced too early produce noisy gradient
signals that slow convergence or cause the model to memorize outliers
rather than learn general patterns. By sequencing examples from easy to
hard, curriculum learning smooths the optimization trajectory.

Implementing a curriculum requires two components: a difficulty scorer
that ranks samples, and a pacing function that controls how quickly hard
samples are introduced. A common choice is linear pacing:

\[
\text{samples}_t = \text{sort\_by\_difficulty}[:N \cdot \min(1, t/T_{warmup})]
\]

where \(t\) is the current epoch and \(T_{warmup}\) is the epoch at
which the full dataset becomes available. Early epochs train on the
easiest \(N \cdot (t/T_{warmup})\) fraction; after warmup, training
proceeds on the full dataset.

The difficulty scorer can be designed in several ways, each with
different computational requirements and applicability.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2087}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3826}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4000}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Strategy}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Difficulty Score}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Best For}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Loss-Based} & Loss from probe model (low = easy) &
General-purpose; requires probe training \\
\textbf{Confidence-Based} & Teacher model confidence (high = easy) &
When teacher available; distillation setups \\
\textbf{Domain Heuristics} & Sentence length, image complexity & No
extra compute; domain knowledge required \\
\textbf{Self-Paced} & Current model's loss (updated each epoch) &
Adaptive; no probe needed \\
\end{longtable}

From a systems perspective, curriculum learning improves convergence by
reducing wasted gradient updates on samples the model cannot yet learn
from. The Information-Compute Ratio is higher in early training because
easy samples provide strong learning signal relative to their compute
cost. The efficiency gains manifest as faster convergence to target
accuracy, not higher final accuracy.

Table~\ref{tbl-curriculum-benchmarks} summarizes measured speedups from
curriculum learning across standard benchmarks:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.1633}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.1224}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2245}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2857}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.1735}}@{}}
\caption{Curriculum learning convergence speedups on standard
benchmarks. Target accuracy is 95\% of final baseline performance. Gains
are larger on redundant datasets (CIFAR-10) and noisy datasets
(MentorNet removes \textasciitilde40\% noise). ImageNet shows smaller
gains because the dataset is less
redundant.}\label{tbl-curriculum-benchmarks}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Dataset}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Model}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Pacing Strategy}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Epochs to Target Acc.}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Speedup}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Dataset}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Model}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Pacing Strategy}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Epochs to Target Acc.}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Speedup}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{CIFAR-10} & ResNet-18 & Linear warmup & 115 vs.~150 baseline &
\textbf{23\%} faster \\
\textbf{CIFAR-100} & ResNet-32 & Self-paced & 180 vs.~220 baseline &
\textbf{18\%} faster \\
\textbf{ImageNet} & ResNet-50 & Loss-based & 80 vs.~90 baseline &
\textbf{11\%} faster \\
\textbf{ImageNet} & ResNet-50 & MentorNet (noisy) & 70 vs.~90 baseline &
\textbf{22\%} faster \\
\end{longtable}

The table reveals an important pattern: curriculum learning gains are
\textbf{inversely proportional to dataset quality}. On highly curated
datasets like ImageNet, the 11\% speedup is modest. On noisy or
redundant data, gains can exceed 20\%.

\begin{tcolorbox}[enhanced jigsaw, breakable, colbacktitle=quarto-callout-note-color!10!white, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Anti-Curriculum and Self-Paced Learning}, colback=white, bottomtitle=1mm, bottomrule=.15mm, coltitle=black, rightrule=.15mm, opacityback=0, opacitybacktitle=0.6, toptitle=1mm, toprule=.15mm, titlerule=0mm, arc=.35mm, colframe=quarto-callout-note-color-frame, leftrule=.75mm, left=2mm]

The optimal ordering is task-dependent. \textbf{Anti-curriculum} (hard
examples first) can work when the decision boundary is complex and easy
examples don't help define it. \textbf{Self-paced learning} lets the
model dynamically adjust difficulty based on its current loss, avoiding
the need to pre-define a curriculum. Empirically, self-paced methods
often match or exceed hand-designed curricula.

\end{tcolorbox}

\subsection{Active Learning:
Human-in-the-Loop}\label{sec-data-efficiency-active-learning-humanintheloop-a9fa}

Curriculum learning optimizes the order in which samples are presented,
but it assumes all samples are already labeled. This assumption breaks
down in specialized fields such as medical diagnosis, autonomous
driving, and scientific research, where labeling requires domain
expertise and can cost \$5--\$100 or more per sample. Rather than
labeling everything upfront, \textbf{active
learning}\sidenote{\textbf{Active Learning}: The concept traces to
statistical experimental design, but Donald Angluin's 1986 work on
``learning from queries'' established theoretical foundations for
machine learning. The term ``active'' contrasts with ``passive''
learning from pre-labeled data---the learner actively queries an oracle
rather than passively receiving examples. Early work in the 1990s
demonstrated that active selection could achieve the same accuracy as
passive learning with exponentially fewer labels in favorable cases. }
(\citeproc{ref-settles2009active}{Settles 2012};
\citeproc{ref-ren2021survey}{Ren et al. 2021}) shifts the optimization
target: instead of choosing which labeled samples to train on, it
chooses which unlabeled samples are worth labeling at all
(Figure~\ref{fig-active-learning-loop}).

The effectiveness of active learning depends critically on the query
strategy used to select samples for annotation. The simplest approach,
uncertainty sampling, selects samples where the model is least
confident, such as predictions near 0.5 probability for binary
classification. This strategy is computationally cheap and effective in
practice. Query-by-committee extends this idea by training multiple
models and selecting samples where they disagree most, capturing
epistemic uncertainty that a single model might miss.

For practitioners willing to invest more compute, expected model change
selects samples that would cause the largest gradient update if labeled.
This approach provides a theoretically grounded but expensive
alternative. Diversity sampling complements uncertainty-based methods by
selecting samples dissimilar from currently labeled data, ensuring the
labeled set covers the full input space rather than clustering around
ambiguous regions.

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/edf5de8972383f6deb47dc21dbe8ff9e55d46a63.pdf}}

}

\caption{\label{fig-active-learning-loop}\textbf{Active Learning Loop}:
Instead of labeling all data, the model selects the most `confusing' or
informative samples from an unlabeled pool. These samples are sent to an
Oracle (human annotator) and added to the training set. The model is
retrained, and the cycle repeats, creating a feedback loop that
maximizes data efficiency.}

\end{figure}%

Active learning is particularly valuable in domains where labeling
requires expertise. In medical imaging, for instance, an AI system
diagnosing diseases from X-rays may be confident on common conditions
but uncertain about rarer cases. By focusing human annotation on these
ambiguous cases, active learning optimizes the use of expensive expert
time while accelerating model improvement.

The economic implications are substantial. In production settings,
labeling costs often dwarf compute costs because a specialist's time is
far more expensive than GPU hours.

\phantomsection\label{callout-perspectiveux2a-1.7}
\begin{fbx}{callout-perspective}{Systems Perspective: }{Napkin Math: The Active Learning ROI}
\phantomsection\label{callout-perspective*-1.7}
\textbf{Problem}: You are building a medical diagnostic AI. You have a
pool of \textbf{1 Million unlabeled scans}. A specialist doctor charges
\textbf{\$5.00} to label one scan. You have a budget of
\textbf{\$500,000} and a deadline of \textbf{1 month}.

\textbf{Scenario A: Naive Labeling} 1. \textbf{Cost}: Labeling all 1M
scans would cost \textbf{\(5,000,000** (10\)\times\$ over budget). 2.
}Time\textbf{: You can only afford to label 100,000 random scans. 3.
}Result**: Your model misses rare pathologies because they weren't in
the random 10\%.

\textbf{Scenario B: Active Learning} 1. \textbf{Strategy}: Use an
uncertainty-based selection to pick the \textbf{50,000} ``hardest''
scans for the doctor to label. 2. \textbf{Cost}:
\(50,000 \times 5.00 = \mathbf{\$250,000}\). (50\% under budget). 3.
\textbf{Training Speed}: With 20\(\times\) less data, each training
epoch is \textbf{20\(\times\) faster}. 4. \textbf{Result}: Research
shows that these 50k ``high-information'' samples often achieve higher
accuracy than 500k random samples.

\textbf{The Systems Conclusion}: Data Efficiency is not just a ``data
trick''; it is a \textbf{20\(\times\) compute accelerator} and a
\textbf{\$4.75 Million} cost-saving measure.

\end{fbx}

\subsection{Semi-Supervised Learning: Using Unlabeled
Data}\label{sec-data-efficiency-semisupervised-learning-leveraging-unlabeled-data-53b7}

Active learning optimizes which samples to label but still requires
human annotation for every selected example. A more aggressive approach
asks: can we extract learning signal from unlabeled data directly? When
some labeled data is available but insufficient for fully supervised
learning, \textbf{semi-supervised learning} addresses this challenge. It
uses a small set of labeled examples to guide learning on a much larger
unlabeled pool, typically achieving 80--95\% of fully supervised
accuracy with only 10--20\% of the labels.

The core insight behind semi-supervised learning is that unlabeled data,
while it cannot directly teach the mapping from inputs to outputs,
contains structural information about the input distribution \(P(X)\)
that constrains the hypothesis space. A decision boundary that cuts
through dense regions of \(P(X)\) is unlikely to generalize well because
it would assign different labels to similar inputs. Semi-supervised
methods use unlabeled data to push decision boundaries toward
low-density regions, where class transitions are more likely to occur
naturally.

Three main techniques implement this insight. Pseudo-labeling takes the
most direct approach: train on labeled data, use the model to generate
``pseudo-labels'' for high-confidence unlabeled predictions, then train
on both. The confidence threshold is critical. Setting it too low
introduces label noise that degrades learning, while setting it too high
wastes potentially useful data.

Consistency regularization takes a different angle by enforcing that the
model produces similar predictions for augmented versions of the same
input. A robust classifier should be invariant to realistic
perturbations like cropping, rotation, or color shifts. Methods like
FixMatch combine both approaches, assigning pseudo-labels only to
samples where the unaugmented prediction is confident but training the
model to predict these labels on strongly augmented versions of the same
images.

Label propagation offers a third paradigm through graph-based reasoning:
construct a similarity graph over all samples and propagate labels from
labeled nodes to their neighbors. This approach works particularly well
when the feature space exhibits clear cluster structure.

The systems trade-off in semi-supervised learning is straightforward: it
typically achieves the same accuracy as fully supervised training with
5--10\(\times\) fewer labels but requires more compute because training
processes both labeled and unlabeled samples. Since labeling costs often
dominate compute costs in production settings, this trade-off is usually
favorable.

\phantomsection\label{callout-exampleux2a-1.8}
\begin{fbx}{callout-example}{Example: }{Quantitative Benchmark: FixMatch on CIFAR-10}
\phantomsection\label{callout-example*-1.8}
\textbf{FixMatch} (\citeproc{ref-sohn2020fixmatch}{Sohn et al. 2020})
combines pseudo-labeling with consistency regularization to achieve high
label efficiency:

\begin{longtable}[]{@{}llll@{}}
\toprule\noalign{}
Label Budget & Method & Accuracy & Label Efficiency \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
50,000 (100\%) & Fully Supervised & 96.1\% & Baseline \\
4,000 (8\%) & FixMatch & 95.7\% & \textbf{12.5\(\times\) more
efficient} \\
250 (0.5\%) & FixMatch & 94.9\% & \textbf{200\(\times\) more
efficient} \\
40 (0.08\%) & FixMatch & 88.6\% & 1250\(\times\) more efficient \\
\end{longtable}

With only 250 labeled samples (25 per class), FixMatch achieves 94.9\%
accuracy, within 1.2 points of full supervision using 200\(\times\)
fewer labels. The technique works by generating pseudo-labels on weakly
augmented unlabeled images (only when model confidence exceeds 0.95),
then training to predict these labels on strongly augmented versions of
the same images.

\textbf{The Systems Insight}: Semi-supervised learning trades labeled
data for unlabeled data and compute. On CIFAR-10, training FixMatch
requires \textasciitilde5\(\times\) more compute than supervised
training (processing 50K unlabeled samples per epoch). When labels cost
\$1 each and GPU hours cost \$0.50, the math favors semi-supervised:

\begin{itemize}
\tightlist
\item
  Supervised (4000 labels): \$4,000 labeling + \$50 compute =
  \textbf{\$4,050}
\item
  FixMatch (250 labels): \$250 labeling + \$250 compute = \textbf{\$500}
\end{itemize}

An 8\(\times\) cost reduction for \textless1\% accuracy loss.

\end{fbx}

These gains are substantial but semi-supervised learning is not
universally applicable. Understanding its failure modes is essential for
deployment decisions.

\begin{tcolorbox}[enhanced jigsaw, breakable, colbacktitle=quarto-callout-note-color!10!white, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{When Semi-Supervised Learning Works Best}, colback=white, bottomtitle=1mm, bottomrule=.15mm, coltitle=black, rightrule=.15mm, opacityback=0, opacitybacktitle=0.6, toptitle=1mm, toprule=.15mm, titlerule=0mm, arc=.35mm, colframe=quarto-callout-note-color-frame, leftrule=.75mm, left=2mm]

Semi-supervised learning assumes that the unlabeled data comes from the
same distribution as the labeled data. It struggles when:

\begin{itemize}
\tightlist
\item
  Unlabeled data contains out-of-distribution samples (the model
  confidently mislabels them)
\item
  Class imbalance is severe (pseudo-labels amplify majority class bias)
\item
  The labeled set doesn't cover all classes (can't propagate labels for
  unseen classes)
\end{itemize}

Always validate on a held-out set with true labels to catch distribution
mismatch.

\end{tcolorbox}

Semi-supervised learning reduces label requirements by 5--10\(\times\)
while maintaining accuracy. Notice what we have not questioned: the
assumption that we need \emph{any} task-specific labels at all. What if
the structure of data itself, the fact that cat images resemble other
cat images, that coherent sentences follow grammatical patterns, could
provide the supervision signal?

\section{Self-Supervised Learning: Eliminating the Label
Bottleneck}\label{sec-data-efficiency-selfsupervised-learning-eliminating-label-bottleneck-1005}

Active learning reduces labeling cost by 10\(\times\). Semi-supervised
learning reduces it by another 5--10\(\times\). But the most dramatic
gain comes from \textbf{self-supervised
learning}\sidenote{\textbf{Self-Supervised Learning}: While
self-supervision ideas existed earlier, 2018 marked the paradigm's
breakthrough year. BERT (Google, October 2018) demonstrated that masked
language modeling could produce representations achieving
state-of-the-art results on 11 NLP tasks. GPT (OpenAI, June 2018) showed
that next-token prediction at scale yielded surprisingly general
language understanding. Together, they established pre-training on
unlabeled data as the dominant paradigm for NLP, later extended to
vision and multimodal domains. }, which removes the human annotation
bottleneck entirely by learning from data structure rather than human
labels. This represents a fundamental paradigm shift in how we think
about supervision.

\subsection{The Paradigm Shift: Labels from
Structure}\label{sec-data-efficiency-paradigm-shift-labels-structure-e9cc}

Labels represent just one form of supervision. The structure of data
itself provides rich learning signals that require no human annotation:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2000}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4200}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3800}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Modality
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Self-Supervised Task
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Supervision Signal
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Text & Masked language modeling & Predict {[}MASK{]} from context \\
Text & Next-token prediction & Predict next word in sequence \\
Images & Contrastive learning & Same image (augmented) vs.~different
images \\
Images & Masked autoencoding & Reconstruct masked patches \\
Multi-modal & CLIP-style alignment & Match image-text pairs \\
\end{longtable}

These \emph{pretext tasks} generate supervision signals automatically
from the data itself. A model that can predict masked words has
necessarily learned grammar, semantics, and world knowledge to make
accurate predictions. Similarly, a model that distinguishes augmented
views of the same image from different images has learned robust visual
features invariant to transformations.

The systems implication is significant: self-supervised pre-training
moves the data cost from the critical path. Instead of waiting for
labels before training begins, pre-training can start immediately on
unlabeled data, often web-scale corpora of billions of samples. This
separation of pre-training from task-specific labeling restructures the
economics of machine learning.

\subsection{The Economics of
Amortization}\label{sec-data-efficiency-economics-amortization-79e6}

Understanding why self-supervised learning dominates modern ML practice
requires examining its economic structure. This shift translates into
concrete cost savings through \emph{cost amortization}, where expensive
pre-training is performed once and reused across many applications:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1613}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2581}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2903}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2903}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Approach
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Labels per Task
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Compute per Task
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Data Acquisition
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Train from scratch & 100K--1M labeled & 100\% full training &
Task-specific collection \\
Fine-tune foundation model & 100--1K labeled & 1--5\% of full training &
Reuse pre-training corpus \\
\end{longtable}

To illustrate this economic transformation, consider a company building
ten specialized classifiers for tasks such as fraud detection, content
moderation, and medical diagnosis.

Training each classifier from scratch would require substantial
investment in both labeling and compute. With ten tasks each needing
100,000 labels at \$1 per label, the total labeling cost reaches
\textbf{\$1,000,000}. The compute burden amounts to 10,000 GPU-hours
across all tasks, with each requiring its own data collection effort.
From start to finish, each task takes 6--12 months to complete.

The fine-tuning approach restructures these costs. Pre-training requires
a one-time investment of 10,000 GPU-hours on unlabeled data, but this
cost is paid only once. Fine-tuning each task then requires just 1,000
labels (\$10,000 total across all ten tasks) and only 500 GPU-hours of
compute. Each task reaches deployment in 1--2 weeks after pre-training
completes.

The return on investment is substantial across every dimension: labeling
costs drop by \textbf{100\(\times\)} (from \$1M to \$10K), per-task
compute decreases by \textbf{10\(\times\)} when amortized across
applications, and time-to-deployment accelerates by
\textbf{20--50\(\times\)} per task.

This explains why the fine-tuning paradigm dominates production ML. The
pre-training cost is high but amortized across many downstream
applications, while fine-tuning cost remains low on a per-task basis.

Figure~\ref{fig-amortization-comparison} visualizes this cost structure.
Training from scratch (left) incurs the full cost for each task
independently. The foundation model approach (right) pays a large
upfront pre-training cost but then fine-tunes each task at a fraction of
the per-task cost.

\begin{figure}[htb]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/17d6adec2401d1d104e2f3cf94edcd71578cfddb.pdf}}

}

\caption{\label{fig-amortization-comparison}\textbf{Cost Amortization in
Foundation Models}: Training from scratch (left) requires full cost for
each task. The foundation model approach (right) pays a large
pre-training cost once, then amortizes it across many low-cost
fine-tuning tasks. After 3--4 tasks, the foundation model approach
becomes more cost-effective; after 10+ tasks, the savings are dramatic.}

\end{figure}%

\subsection{Trade-Offs Across Self-Supervised
Approaches}\label{sec-data-efficiency-tradeoffs-across-selfsupervised-approaches-b473}

While the economics of amortization favor self-supervised learning
broadly, not all self-supervised methods are equivalent. Different
approaches occupy different points on the efficiency frontier, trading
off pre-training cost, batch size requirements, and downstream data
efficiency:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1964}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2589}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2768}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Method}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Batch Size Requirement}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Data Efficiency}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Best Use Case}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Contrastive} \textbf{(SimCLR, MoCo)} & Very large (4096+) & High
(per labeled sample) Low (pre-training cost) & Vision, small datasets \\
\textbf{Masked Modeling} \textbf{(BERT, MAE)} & Moderate (256--1024) &
Moderate & NLP, balanced efficiency \\
\textbf{Generative} \textbf{(GPT)} & Large (512--2048) & Highest at
scale & Foundation models, unlimited unlabeled data \\
\end{longtable}

Contrastive learning methods such as SimCLR and
MoCo\sidenote{\textbf{SimCLR and MoCo}: Both published in early 2020,
these papers marked a turning point for self-supervised vision. SimCLR
(Ting Chen et al., Google) achieved 76.5\% ImageNet accuracy with a
linear classifier---matching supervised ResNet-50---using only
self-supervised pre-training. MoCo (Kaiming He et al., Facebook AI)
introduced the momentum encoder trick that enabled contrastive learning
without requiring enormous batch sizes. Their combined impact closed the
gap between supervised and self-supervised visual representations. }
require many negative examples per batch to distinguish similar samples,
making them compute-intensive during pre-training.

The batch size requirement is substantial: these methods need batches of
4096 or more samples to work effectively. However, this upfront
investment yields excellent downstream performance with minimal labeled
data, making contrastive learning particularly effective for vision
applications with small datasets.\sidenote{\textbf{Batch Size
Sensitivity}: The batch size sensitivity is substantial: SimCLR achieves
66.6\% ImageNet top-1 accuracy with batch size 8192 but drops to 61.9\%
with batch size 256, a 4.7 percentage point degradation
(\citeproc{ref-chen2020mocov2}{Chen et al. 2020}). This occurs because
contrastive learning treats all other samples in a batch as negatives;
with fewer negatives, the pretext task becomes easier and the learned
representations are weaker. }

Masked modeling approaches such as BERT and MAE occupy a middle ground
in this efficiency landscape. These methods work with smaller batches
(256--1024 samples) but require more training iterations to converge.
The result is a balanced trade-off between pre-training cost and
downstream data efficiency that has made masked modeling the dominant
paradigm in natural language processing.

Generative pre-training, exemplified by the GPT family of models, scales
well with data volume. Performance improves log-linearly with dataset
size up to trillions of tokens, exhibiting no saturation within current
data availability. This scaling behavior makes generative pre-training
the method of choice for foundation models, where the substantial
pre-training cost can be amortized across thousands of downstream tasks.

\subsection{From 1000× Multiplier to Foundation Model
Paradigm}\label{sec-data-efficiency-1000-multiplier-foundation-model-paradigm-7866}

These trade-offs converge on a clear conclusion: from a data efficiency
perspective, self-supervised pre-training represents a
\textbf{1000\(\times\) or greater multiplier} on the value of labeled
data. Instead of labeling millions of task-specific examples,
practitioners fine-tune on hundreds or thousands of labeled samples
while inheriting knowledge distilled from billions of unlabeled tokens.

This multiplicative advantage created the \emph{foundation model
paradigm}\sidenote{\textbf{Foundation Model}: Term coined by Stanford's
Center for Research on Foundation Models in 2021 to describe models like
BERT, GPT-3, and DALL-E. The name emphasizes a critical property: these
models serve as a ``foundation'' for many downstream tasks, but this
creates dangerous homogenization---defects in the foundation model
propagate to all applications built upon it, making them single points
of failure that can ``radiate harms'' across an ecosystem. }
(\citeproc{ref-bommasani2021opportunities}{Bommasani et al. 2021}) that
defines modern ML systems.

The paradigm follows a three-step pattern. First, pre-train once on
massive unlabeled corpora comprising billions of tokens or images.
Second, fine-tune many times on small task-specific datasets containing
hundreds to thousands of samples. Third, amortize the pre-training cost
across all downstream applications. This structure explains why
organizations invest millions of dollars in pre-training: the investment
pays dividends across every subsequent application built on that
foundation.

The architectural and training details for these methods appear in
\textbf{?@sec-ai-training}. From a data efficiency perspective,
self-supervised learning represents the current ceiling of what the
field has achieved. This technique transformed data from the primary
bottleneck into an abundant resource.

\begin{tcolorbox}[enhanced jigsaw, breakable, colbacktitle=quarto-callout-note-color!10!white, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Scaling Self-Supervised Pre-training}, colback=white, bottomtitle=1mm, bottomrule=.15mm, coltitle=black, rightrule=.15mm, opacityback=0, opacitybacktitle=0.6, toptitle=1mm, toprule=.15mm, titlerule=0mm, arc=.35mm, colframe=quarto-callout-note-color-frame, leftrule=.75mm, left=2mm]

Self-supervised pre-training at scale requires distributed
infrastructure. While this chapter focuses on single-machine data
efficiency, the techniques that make foundation models possible
(gradient accumulation across mini-batches, mixed precision to reduce
memory footprint, and pipeline parallelism to split models across
devices) are essential for pre-training billion-parameter models. The
data efficiency principles here (coreset selection, curriculum learning)
apply regardless of scale, but their implementation must account for
distributed coordination overhead.

\end{tcolorbox}

Self-supervised learning addresses the label bottleneck by learning from
data structure rather than human annotation. But what happens when the
data itself is scarce? When rare classes have too few examples, when
edge cases never appear in the wild, or when privacy constraints prevent
collecting real samples? The third stage of our data efficiency pipeline
addresses this: rather than selecting or curating existing data, we
create new data on demand.

\section{Synthetic Data Generation and
Augmentation}\label{sec-data-efficiency-synthetic-data-generation-augmentation-f3c5}

The third strategy for maximizing ICR is to \textbf{create} high-value
samples when real data is scarce, expensive, or lacks diversity.

\subsection{Data Augmentation: Transformation-Based
Synthesis}\label{sec-data-efficiency-data-augmentation-transformationbased-synthesis-20a5}

Data augmentation artificially expands a dataset by applying
transformations to existing samples. Because many transformations
preserve label semantics while creating novel inputs, augmentation
effectively multiplies the diversity of a training set without requiring
additional data collection.

For image data, augmentation techniques span a range of complexity.
Geometric transformations such as rotation, flipping, cropping, and
scaling introduce spatial variation that makes models robust to
viewpoint changes. Photometric transformations adjust brightness,
contrast, saturation, and hue to simulate different lighting conditions
and camera characteristics. More advanced techniques like Cutout (which
applies random rectangular masks), MixUp\sidenote{\textbf{MixUp}:
Introduced by Hongyi Zhang and colleagues at ICLR 2018. The elegantly
simple idea---train on linear interpolations of image pairs with
correspondingly interpolated labels---produces surprisingly strong
regularization. The paper showed MixUp reduces memorization of corrupt
labels, improves adversarial robustness, and stabilizes GAN training,
all from a technique requiring just two lines of code to implement. }
(\citeproc{ref-zhang2018mixup}{Zhang et al. 2018}) (which blends two
images and their labels), and CutMix (which pastes patches between
images) push augmentation further by creating entirely synthetic
training examples that regularize learning.

Text augmentation presents different challenges since language is
discrete rather than continuous. Back-translation offers one solution:
translating text to another language and back generates paraphrases that
preserve meaning while varying surface form. Simpler approaches include
synonym replacement, which swaps words while preserving semantics, and
random insertion or deletion, which adds noise that makes models robust
to typos and informal input.

\begin{tcolorbox}[enhanced jigsaw, breakable, colbacktitle=quarto-callout-note-color!10!white, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{AutoAugment and Learned Policies}, colback=white, bottomtitle=1mm, bottomrule=.15mm, coltitle=black, rightrule=.15mm, opacityback=0, opacitybacktitle=0.6, toptitle=1mm, toprule=.15mm, titlerule=0mm, arc=.35mm, colframe=quarto-callout-note-color-frame, leftrule=.75mm, left=2mm]

Rather than hand-designing augmentation policies, \textbf{AutoAugment}
uses reinforcement learning to discover optimal augmentation strategies
for specific datasets. RandAugment simplifies this by randomly sampling
from a fixed set of transformations, achieving similar performance with
less computation.

\end{tcolorbox}

These learned augmentation policies are particularly effective for
resource-constrained models, where overfitting risk is highest.

\phantomsection\label{callout-lighthouseux2a-1.9}
\begin{fbx}{callout-lighthouse}{Lighthouse Example: }{MobileNet and Aggressive Augmentation}
\phantomsection\label{callout-lighthouse*-1.9}
Our \textbf{MobileNet Lighthouse model}
(\textbf{?@sec-dnn-architectures}) exemplifies how data augmentation
compensates for model capacity constraints. MobileNet's depthwise
separable convolutions reduce parameters by 8--9\(\times\) compared to
standard convolutions, but this efficiency comes at a cost: smaller
models are more prone to overfitting on limited data.

The solution is \textbf{aggressive augmentation}. MobileNet training
typically uses stronger augmentation than ResNet-50 training, including
RandAugment with higher magnitude, more aggressive cropping, and longer
training schedules. The augmentation effectively increases dataset
diversity without increasing model capacity, allowing MobileNet to
achieve near-ResNet accuracy at a fraction of the parameter count. For
edge deployment where both data collection and model size are
constrained, augmentation is essential rather than optional.

\end{fbx}

\subsection{Generative Synthesis: Creating New
Samples}\label{sec-data-efficiency-generative-synthesis-creating-new-samples-3873}

While augmentation transforms existing samples, synthetic data
generation extends this approach by creating entirely new examples using
generative models. This capability becomes essential in three common
scenarios: when real data is privacy-sensitive (as with medical records
or financial transactions), when edge cases are rare (such as autonomous
driving failure scenarios that must be covered but seldom occur), or
when data collection is prohibitively expensive (as in robotics or
scientific experiments where each sample requires physical resources).

Three classes of generative approaches address these needs, each with
distinct cost and fidelity trade-offs. Generative Adversarial Networks
(GANs) train a generator against a discriminator in an adversarial
setup, producing realistic images through competition; StyleGAN, for
instance, generates photorealistic faces that have augmented facial
recognition datasets. Diffusion models use iterative denoising to
produce high-quality images; systems like Stable Diffusion enable
text-to-image synthesis, allowing you to generate targeted training
examples from natural language descriptions. Finally, simulation engines
such as CARLA for autonomous driving or Unity and Unreal for robotics
offer physics-based rendering that generates unlimited labeled data with
perfect ground-truth annotations, making them particularly valuable for
safety-critical applications where edge case coverage is essential.

\subsection{Bridging the Domain
Gap}\label{sec-data-efficiency-bridging-domain-gap-15f5}

Synthetic data's greatest limitation is the \textbf{domain gap}: the
statistical difference between generated and real-world data. A model
trained on perfectly rendered simulation images may fail on blurry,
poorly-lit real camera footage. This gap can negate the efficiency gains
of synthetic data if not addressed.

Figure~\ref{fig-domain-gap} illustrates the problem. Synthetic data
(left distribution) and real data (right distribution) occupy different
regions of feature space. A model trained only on synthetic data learns
a decision boundary that doesn't transfer to real deployment.

\begin{figure}[htb]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/f741ca9984c3ad0b2f6ad2dfe98d17ac74472728.pdf}}

}

\caption{\label{fig-domain-gap}\textbf{The Domain Gap Problem}:
Synthetic data (blue) and real data (orange) have different
distributions. A model trained on synthetic data alone learns a boundary
that fails on real data. Domain adaptation techniques aim to align these
distributions or learn domain-invariant features.}

\end{figure}%

Two complementary strategies address this distribution mismatch. Domain
randomization takes an aggressive approach: rather than trying to match
the real world precisely, it trains on wildly varied synthetic data by
randomizing lighting, textures, backgrounds, and camera parameters
during generation. If the model encounters sufficient variation during
training, the real world becomes ``just another variation'' within its
learned distribution. This strategy demonstrates strong results for
robotics and autonomous driving, where simulation technology is mature
enough to generate physically plausible variations across a wide range.

Domain adaptation takes the opposite approach by explicitly aligning
synthetic and real distributions. Feature alignment methods train on
synthetic data while simultaneously minimizing the distance between
synthetic and real feature distributions, often using adversarial
training to learn domain-invariant representations. Fine-tuning offers a
simpler path: pre-train on abundant synthetic data to learn general
features, then fine-tune on a small real dataset to adapt to deployment
conditions. Self-training combines these ideas by using a
synthetic-trained model to pseudo-label real unlabeled data, then
retraining on the combined labeled set.

In practice, the best results often come from mixing synthetic and real
data rather than relying on either source alone:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 2\tabcolsep) * \real{0.5278}}
  >{\raggedright\arraybackslash}p{(\linewidth - 2\tabcolsep) * \real{0.4722}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Synthetic Fraction
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Typical Outcome
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
100\% synthetic & Poor real-world generalization \\
80\% synthetic + 20\% real & Good performance, significant cost
savings \\
50\% synthetic + 50\% real & Best performance in many domains \\
100\% real & Baseline (expensive) \\
\end{longtable}

The optimal mix depends on simulation fidelity, domain complexity, and
the cost differential between synthetic and real data.

\begin{tcolorbox}[enhanced jigsaw, breakable, colbacktitle=quarto-callout-warning-color!10!white, title=\textcolor{quarto-callout-warning-color}{\faExclamationTriangle}\hspace{0.5em}{Model Collapse Risk}, colback=white, bottomtitle=1mm, bottomrule=.15mm, coltitle=black, rightrule=.15mm, opacityback=0, opacitybacktitle=0.6, toptitle=1mm, toprule=.15mm, titlerule=0mm, arc=.35mm, colframe=quarto-callout-warning-color-frame, leftrule=.75mm, left=2mm]

When synthetic data is generated by ML models (not simulators), there's
a risk of \textbf{model collapse}: training on model-generated data
amplifies errors and reduces diversity over generations. This is
particularly concerning for foundation models where synthetic data from
earlier model generations may contaminate future training corpora.

\end{tcolorbox}

With appropriate safeguards, synthetic data generation remains a
powerful tool. The following example illustrates how to combine multiple
data efficiency techniques, including augmentation, noise injection, and
simulation, into a coherent strategy for a real deployment scenario.

\phantomsection\label{callout-exampleux2a-1.10}
\begin{fbx}{callout-example}{Example: }{Lighthouse Example: Keyword Spotting Data Efficiency}
\phantomsection\label{callout-example*-1.10}
\textbf{Scenario}: Our \textbf{Keyword Spotting Lighthouse model}
(\textbf{?@sec-dnn-architectures}), a DS-CNN with \textbf{200 K}
parameters, represents the extreme end of data efficiency challenges.
You are building a wake-word detector (``Hey Device'') for a
microcontroller with 256 KB SRAM (see
\textbf{?@sec-ml-system-architecture-tinyml-ubiquitous-sensing-scale-a67b}
for hardware constraints). The model must be tiny (\textasciitilde50 KB
quantized), but you need 10,000+ labeled audio samples to train it,
samples that do not yet exist.

\textbf{The Data Collection Problem}:

\begin{itemize}
\tightlist
\item
  Recording 10,000 real utterances requires 500+ speakers for diversity
\item
  Professional recording costs \$2--5 per sample (\$20--50K total)
\item
  Target deployment environment (noisy kitchen, car interior) differs
  from recording studio
\end{itemize}

\textbf{Data Efficiency Solution Stack}:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Seed Data (500 samples)}: Record 50 speakers × 10 utterances
  in controlled conditions
\item
  \textbf{Augmentation (5,000 samples)}: Apply pitch shift, time
  stretch, speed variation to 10\(\times\) the seed data
\item
  \textbf{Noise Injection (10,000 samples)}: Mix clean audio with
  environmental noise (kitchen appliances, HVAC, traffic) sampled from
  AudioSet
\item
  \textbf{Negative Mining}: Use acoustic similarity to find hard
  negatives (``Hey Siri'', ``Hey Google'') from public datasets
\item
  \textbf{Simulation (optional)}: Text-to-speech synthesis with diverse
  voice models
\end{enumerate}

\textbf{Result}: 500 real recordings → 10,000+ training samples at 5\%
of the cost. The noise injection serves as domain randomization,
improving deployment robustness.

\textbf{Key Insight for TinyML}: When the target model is tiny, the data
efficiency challenge shifts from ``reduce terabytes to gigabytes'' to
``create a useful dataset from almost nothing.'' Augmentation and
simulation become essential rather than optional.

\end{fbx}

\subsection{Knowledge Distillation: Compressing
Information}\label{sec-data-efficiency-knowledge-distillation-compressing-information-40a5}

The techniques above create new input samples, but there is another form
of synthesis that creates enhanced labels. Knowledge
distillation\sidenote{\textbf{Knowledge Distillation}: Introduced by
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean in 2015. Hinton coined the
evocative term ``dark knowledge'' for the information in soft
probability distributions---the teacher reveals not just which class is
correct but which incorrect classes are most plausible. The temperature
parameter in the softmax function controls how much dark knowledge is
exposed: higher temperatures produce softer distributions that transfer
more nuanced inter-class relationships. }
(\citeproc{ref-hinton2015distilling}{Hinton, Vinyals, and Dean 2015}) is
a data efficiency technique where a smaller ``student'' model learns
from a larger ``teacher'' model's outputs rather than raw labels.

The key insight is that the teacher's soft predictions contain more
information than hard labels: a teacher predicting {[}0.7, 0.2, 0.1{]}
for three classes reveals inter-class relationships (classes 1 and 2 are
more similar) that a hard label {[}1, 0, 0{]} obscures entirely.

This richer supervision signal enables student models to learn more
efficiently from the same data. From a systems perspective, distillation
is particularly powerful for creating synthetic labels at scale: run a
large model (such as GPT-4) on unlabeled data to generate high-quality
annotations, then train a smaller model on these synthetic labels. The
smaller model inherits much of the teacher's capability at a fraction of
the inference cost, amortizing the expensive teacher computation across
many student deployments.

Together, augmentation, generative synthesis, and distillation complete
the third stage of our data efficiency pipeline. Where static pruning
removes redundancy and dynamic selection focuses compute on high-value
samples, synthetic generation fills gaps by creating samples that never
existed. These three stages form a complementary toolkit: pruning
reduces what you have, selection focuses how you use it, and synthesis
expands what you can access.

\section{Technique
Summary}\label{sec-data-efficiency-technique-summary-0ee8}

Table~\ref{tbl-data-efficiency} summarizes the three-stage optimization
pipeline introduced at the beginning of this chapter.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2041}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1293}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3810}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2721}}@{}}
\caption{The three-stage data efficiency pipeline. Each stage increases
ICR by different mechanisms: pruning removes low-value samples, dynamic
selection focuses compute on high-value samples, and synthesis creates
new high-value samples.}\label{tbl-data-efficiency}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Stage}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{When Applied}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Techniques}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Typical Gains}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Stage}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{When Applied}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Techniques}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Typical Gains}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{1. Static Pruning} & Before training & Coreset Selection,
Deduplication, Quality Filtering & 30--50\% dataset reduction \\
\textbf{2. Dynamic Selection} & During training & Curriculum Learning,
Active Learning, Semi-Supervised & 10--30\% faster convergence \\
\textbf{3. Synthetic Generation} & On-demand & Augmentation, Generative
Models, Distillation & 2--10\(\times\) effective data expansion \\
\end{longtable}

Table~\ref{tbl-technique-selection} provides a decision guide for
selecting techniques based on your specific constraints.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2870}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2261}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4783}}@{}}
\caption{Technique selection guide based on primary
constraint.}\label{tbl-technique-selection}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Constraint}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Best Technique}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Why}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Constraint}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Best Technique}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Why}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Limited labeling budget} & Active Learning & Maximizes label ROI
by selecting informative samples \\
\textbf{High redundancy in data} & Deduplication + Coreset & Removes
waste before training begins \\
\textbf{Rare classes or edge cases} & Synthetic Generation & Creates
samples that don't exist in raw data \\
\textbf{Slow convergence} & Curriculum Learning & Improves gradient
quality in early training \\
\textbf{Privacy requirements} & Synthetic Data & Train on generated
data, not real user data \\
\textbf{Large model, small dataset} & Knowledge Distillation & Use
teacher model's knowledge as ``data'' \\
\end{longtable}

\subsection{Decision Framework: Choosing the Right
Technique}\label{sec-data-efficiency-decision-framework-choosing-right-technique-c36a}

With many techniques available, practitioners need a systematic approach
to selection. Figure~\ref{fig-technique-decision-tree} provides a visual
flowchart that captures the key branching points:

\begin{figure}[htb]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/c39f669c8787491f94cf96eff177c00785712a3f.pdf}}

}

\caption{\label{fig-technique-decision-tree}\textbf{Data Efficiency
Technique Selection Tree}: Start at the top by identifying your primary
bottleneck, then follow the branches to find the most appropriate
technique. Leaf nodes show recommended methods. Multiple paths may
apply---combine techniques as needed.}

\end{figure}%

The following text-based decision tree elaborates on each path, guiding
practitioners from initial bottleneck identification through
implementation.

\textbf{Step 1: Assess your bottleneck}

The first step is identifying which resource constraint most severely
limits your training pipeline. If labeling cost dominates your budget,
consider label efficiency techniques such as Active Learning,
Semi-Supervised, or Self-Supervised learning. These methods maximize the
value extracted from each human annotation. When compute cost is the
primary concern, prioritize dataset reduction through Coreset selection,
Deduplication, and Curriculum Learning, all of which reduce the number
of training iterations required. Finally, if data scarcity is the
fundamental problem, pursue data creation through Augmentation,
Synthesis, and Distillation to expand your effective training set beyond
what raw collection provides.

\textbf{Step 2: Check prerequisites}

Once you have identified your bottleneck, verify that the corresponding
techniques are feasible given your infrastructure and data. Each
approach carries specific requirements that must be met before
implementation can begin.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 2\tabcolsep) * \real{0.4231}}
  >{\raggedright\arraybackslash}p{(\linewidth - 2\tabcolsep) * \real{0.5769}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Technique
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Prerequisites
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Active Learning & Access to oracle, unlabeled pool, retraining
infrastructure \\
Coreset Selection & Proxy model or embedding extractor, full dataset
accessible \\
Curriculum Learning & Difficulty scoring method, pacing schedule \\
Semi-Supervised & Some labeled data, unlabeled data from same
distribution \\
Self-Supervised & Large unlabeled corpus, pre-training compute budget \\
Augmentation & Domain knowledge of invariances, augmentation library \\
Synthetic Generation & Generative model or simulator, domain gap
mitigation \\
\end{longtable}

\textbf{Step 3: Estimate ROI}

Meeting the prerequisites is necessary but not sufficient. Before
committing engineering resources, estimate the return on investment for
each candidate technique. The following formula captures the key
trade-off:

\[
\text{ROI} = \frac{\text{(Baseline Cost)} - \text{(Technique Cost + Implementation Cost)}}{\text{Technique Cost + Implementation Cost}}
\]

A technique with high theoretical gains but high implementation cost may
deliver lower ROI than a simpler approach. Deduplication, for example,
often achieves the highest ROI because implementation cost is minimal
and gains are immediate. Active Learning, by contrast, requires oracle
access, retraining infrastructure, and selection algorithm development,
so its ROI depends heavily on how many labeling cycles you expect to
amortize that investment across.

\textbf{Step 4: Combine techniques strategically}

The techniques in this chapter are not mutually exclusive; in practice,
the most effective pipelines combine multiple approaches. A typical
production workflow begins by deduplicating the raw corpus for immediate
gains at minimal cost. This cleaned dataset then undergoes coreset
selection to identify the most informative samples. During training,
curriculum learning orders these samples to optimize gradient quality,
while data augmentation increases effective diversity at runtime.
Finally, starting from a self-supervised foundation model rather than
random initialization allows the pipeline to leverage knowledge learned
from massive unlabeled corpora.

Each stage compounds the efficiency gains of previous stages, turning
individual percentage improvements into multiplicative savings.

The preceding sections answer the \emph{what} of data efficiency: which
samples to prune, when to select dynamically, and how to synthesize new
data. Understanding these algorithmic choices is essential, but
algorithms alone do not translate into faster training. A perfectly
designed coreset algorithm that takes 10 hours to select samples for a
2-hour training run yields no practical benefit. The \emph{how} of
implementation matters just as much as the \emph{what} of algorithm
choice.

This gap between algorithmic elegance and practical value motivates
several systems questions. How do you avoid selection overhead negating
your theoretical gains? How do you handle non-sequential I/O patterns
that confuse prefetching logic? How do you coordinate selection
decisions across distributed workers without introducing synchronization
bottlenecks? The following sections address these engineering
challenges, bridging the gap between data efficiency theory and
production reality.

\section{Engineering Data Efficiency
Systems}\label{sec-data-efficiency-engineering-data-efficiency-systems-7aef}

The strategies discussed so far, including pruning, active learning, and
synthesis, are algorithmic interventions. Implementing them at scale
requires robust systems engineering. A naive active learning loop that
scans the entire dataset every epoch to select the ``best'' samples will
turn a compute-bound training job into an I/O-bound bottleneck. This
section examines the architectural patterns required to implement data
efficiency in production.

\subsection{The Selection
Bottleneck}\label{sec-data-efficiency-selection-bottleneck-f1d4}

Dynamic data selection introduces a new bottleneck: \textbf{selection
latency}. In standard training, the data loader simply reads the next
batch. In active learning or curriculum learning, the system must
evaluate a selection function \(f(x)\) over a large candidate pool to
determine the next batch.

For a selection strategy to be systems-efficient, it must satisfy the
\textbf{Selection Inequality}:

\[ T_{selection} + T_{train}(N_{subset}) < T_{train}(N_{total}) \]

Where \(T_{selection}\) is the time spent scoring the pool and
\(T_{train}\) is the compute time. If \(f(x)\) requires a forward pass
of a large model, the cost of selection can exceed the cost of training,
leading to a negative ROI.

\phantomsection\label{callout-exampleux2a-1.11}
\begin{fbx}{callout-example}{Example: }{Worked Example: Selection Inequality in Practice}
\phantomsection\label{callout-example*-1.11}
\textbf{Scenario}: You have 1 million training images and want to select
a 100k coreset (10\%) using EL2N scoring.

\textbf{Option A: Full Model Selection} - Score all 1M images with your
target ResNet-50: 1M × 0.01 s = \textbf{10,000 seconds} (2.8 hours) -
Train on 100k coreset for 100 epochs: 100k × 100 × 0.01 s =
\textbf{100,000 seconds} (27.8 hours) - \textbf{Total: 30.6 hours}

\textbf{Option B: Proxy Model Selection} - Score all 1M images with a
small proxy (ResNet-18): 1M × 0.002 s = \textbf{2,000 seconds} (0.6
hours) - Train on 100k coreset for 100 epochs: \textbf{100,000 seconds}
(27.8 hours) - \textbf{Total: 28.4 hours}

\textbf{Baseline: No Selection} - Train on full 1M dataset for 100
epochs: 1M × 100 × 0.01 s = \textbf{1,000,000 seconds} (278 hours)

\textbf{Analysis}:

\begin{itemize}
\tightlist
\item
  Option A saves 247 hours vs.~baseline (89\% reduction) ✓
\item
  Option B saves 250 hours vs.~baseline (90\% reduction) ✓
\item
  Option B beats Option A by 2.2 hours. Proxy selection yields better
  ROI.
\end{itemize}

\textbf{The Trap}: If your selection required 50 hours (e.g., running a
7B parameter model), you'd spend 77.8 hours total---still better than
baseline, but the selection overhead consumes 64\% of your savings.

\textbf{Rule of thumb}: Selection time should be \textless10\% of subset
training time for good ROI.

\end{fbx}

The following analysis formalizes this heuristic, deriving the general
condition under which data selection provides positive return on
investment.

\phantomsection\label{callout-perspectiveux2a-1.12}
\begin{fbx}{callout-perspective}{Systems Perspective: }{Napkin Math: The Selection Inequality}
\phantomsection\label{callout-perspective*-1.12}
\textbf{Problem}: You are using active learning to select the best 10\%
of samples for training. Your selection algorithm requires running the
full model on the unlabeled pool. Is this efficient?

\textbf{The Math}: 1. \textbf{Full Training}: 100 epochs. Total cost =
\(100 \times C_{epoch}\). 2. \textbf{Selection (Full Model)}: Scoring
the full dataset is equivalent to \textbf{1 epoch} of training.
\(T_{selection} = 1 \times C_{epoch}\). 3. \textbf{Subset Training}: 100
epochs on 10\% data =
\(100 \times 0.1 \times C_{epoch} = 10 \times C_{epoch}\). 4.
\textbf{Total Time}: \(1 + 10 = \mathbf{11 \times C_{epoch}}\). 5.
\textbf{Speedup}: \(100 / 11 \approx \mathbf{9\times}\).

\textbf{The Trap}: If your selection algorithm is iterative (e.g.,
repeating selection every epoch), \(T_{selection}\) becomes
\(100 \times 1 = 100 \times C_{epoch}\). Total time =
\(100 + 10 = 110 \times C_{epoch}\). You are now \textbf{slower} than
the baseline.

\textbf{The Fix}: Use a \textbf{Proxy Model} (\(10\times\) smaller) for
selection. \(T_{selection} = 0.1 \times C_{epoch}\). Total time =
\(0.1 + 10 = 10.1\). You preserve the speedup.
Figure~\ref{fig-selection-inequality} visualizes this trade-off.

\end{fbx}

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/fc416a2646c9e068a4a6bb72670427409fc196d2.pdf}}

}

\caption{\label{fig-selection-inequality}\textbf{The Selection
Inequality}: Data selection only improves end-to-end efficiency if the
overhead of selection plus training on the subset is less than training
on the full dataset. A lightweight selection function (proxy model,
cached embeddings) keeps selection overhead low; an expensive selection
function (full model forward pass) can negate the savings.}

\end{figure}%

\subsection{Hardware Empathy: The Random Access
Penalty}\label{sec-data-efficiency-hardware-empathy-random-access-penalty-f9c1}

Data efficiency strategies like coresets or dynamic sampling often
require \textbf{random access} to samples across the dataset. While
standard training uses sequential reads (benefiting from hardware
readahead and OS page caching), random access patterns devastate
throughput, especially on distributed filesystems or traditional hard
drives. Table~\ref{tbl-io-performance} quantifies this penalty across
storage tiers.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.1484}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2188}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.1875}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2578}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.1641}}@{}}
\caption{}\label{tbl-io-performance}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Storage Tier}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Sequential Throughput}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Random I/O (IOPS)}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Random Throughput (approx)}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Random Penalty}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Storage Tier}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Sequential Throughput}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Random I/O (IOPS)}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Random Throughput (approx)}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Random Penalty}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{HDD (7.2k)} & \textasciitilde150 MB/s & \textasciitilde80 &
\textasciitilde0.3 MB/s & \textbf{500x} \\
\textbf{SATA SSD} & \textasciitilde550 MB/s & \textasciitilde10k &
\textasciitilde40 MB/s & \textbf{14x} \\
\textbf{NVMe SSD} & \textasciitilde3,500 MB/s & \textasciitilde500k &
\textasciitilde2,000 MB/s & \textbf{1.75x} \\
\textbf{Cloud (S3)} & \textasciitilde100 MB/s (per conn) &
\textasciitilde10-50ms (lat) & Very Low (per conn) & \textbf{Extreme} \\
\end{longtable}

To mitigate this, high-efficiency systems employ proxy selection
techniques. The first approach uses small proxy models: a distilled,
lightweight model (e.g., a 10M parameter ``student'') scores the data
pool on behalf of a 7B parameter ``teacher,'' reducing selection cost by
an order of magnitude while preserving most of the ranking quality. The
second approach leverages embedding indices. By pre-computing embeddings
and storing them in a vector search index (e.g., FAISS), selection
transforms from a \(O(N)\) linear scan into a \(O(\log N)\)
nearest-neighbor lookup. Both techniques share a common principle:
decoupling selection computation from training computation enables
independent optimization of each stage.

\subsection{Optimizing Data
Loaders}\label{sec-data-efficiency-optimizing-data-loaders-b6c2}

Beyond selection algorithms, data loaders themselves require
architectural adaptation. Data efficiency strategies often lead to
non-sequential access patterns. While standard training reads files
sequentially (optimizing disk readahead), strategies like dynamic subset
selection require random access to specific high-value samples. Standard
filesystems and object stores (S3) suffer significant latency penalties
under random access loads, as Table~\ref{tbl-io-performance}
demonstrates.

To maintain GPU utilization, data loaders must be architected for
sharded random access. Modern formats like WebDataset or FFCV group
thousands of samples into \texttt{tar} or \texttt{record} shards,
enabling efficient bulk reads even when the target samples are scattered
across the logical dataset. Complementing this, shuffle buffers provide
a practical approximation to true random access: the loader reads large
sequential shards into a memory buffer and samples randomly from within
the buffer. This design preserves the sequential I/O throughput that
storage hardware delivers best while still achieving the statistical
benefits of random sampling that many data efficiency algorithms
require.

\subsection{Augmented Pipeline
Parallelism}\label{sec-data-efficiency-augmented-pipeline-parallelism-ee1b}

The optimizations discussed so far address I/O bandwidth, but modern
data efficiency pipelines introduce another bottleneck: CPU computation.
Synthetic data generation and heavy augmentation shift the constraint
from disk speed to augmentation throughput. Heavy augmentations like 3D
rotations and MixUp, or on-the-fly generative synthesis, can starve the
GPU if the CPU cannot keep pace with sample production. When the data
pipeline cannot produce samples as fast as the GPU can consume them, GPU
utilization drops and training time extends, negating the efficiency
gains from smarter data selection.

\subsection{Data Echoing: Amortizing I/O
Costs}\label{sec-data-efficiency-data-echoing-amortizing-io-costs-24e3}

Data echoing (\citeproc{ref-choi2020dataechoing}{Choi et al. 2020})
offers an elegant solution to this CPU-GPU imbalance. The technique
reuses batches of data multiple times before fetching new samples,
effectively trading sample diversity for GPU utilization. When the data
pipeline (reading, decoding, augmenting) is slower than GPU processing,
the GPU idles waiting for data. Data echoing fills this gap by
``echoing'' (repeating) each batch \(e\) times, applying different
augmentations to each repetition so that the model still sees varied
inputs.

The optimal echo factor depends on the ratio \(R\) of upstream
processing time to downstream training time:

\[
R = \frac{T_{\text{data pipeline}}}{T_{\text{GPU training}}}
\]

If \(R > 1\) (data pipeline is the bottleneck), set echo factor
\(e \leq R\) to fully utilize GPU capacity. If \(R < 1\) (GPU is the
bottleneck), data echoing provides no benefit.

\phantomsection\label{callout-exampleux2a-1.13}
\begin{fbx}{callout-example}{Example: }{Worked Example: Data Echoing ROI}
\phantomsection\label{callout-example*-1.13}
\textbf{Scenario}: Training ResNet-50 on ImageNet with heavy
augmentation (RandAugment + MixUp).

\textbf{Measurements}:

\begin{itemize}
\tightlist
\item
  Data pipeline throughput: 300 images/second (reading, decoding,
  augmenting on CPU)
\item
  GPU training throughput: 800 images/second (forward + backward pass)
\item
  Ratio \(R = 800/300 = 2.67\) (GPU waiting 63\% of time)
\end{itemize}

\textbf{Without Echoing}:

\begin{itemize}
\tightlist
\item
  Effective throughput: 300 images/second (limited by data pipeline)
\item
  Training time for 90 epochs: 90 × 1.28M / 300 = \textbf{384,000
  seconds (106 hours)}
\item
  GPU utilization: \textasciitilde38\%
\end{itemize}

\textbf{With Echo Factor \(e = 2\)}:

\begin{itemize}
\tightlist
\item
  Each batch is processed twice with different augmentations
\item
  Effective throughput: 600 images/second (still below GPU capacity)
\item
  Unique images per second: 300 (unchanged)
\item
  Training time: 90 × 1.28M / 600 = \textbf{192,000 seconds (53 hours)}
  if echoed data is equally valuable
\end{itemize}

\textbf{But echoed data has diminishing returns}: Research shows echoed
samples provide approximately 70--90\% of the value of fresh samples,
depending on augmentation diversity. Empirically, Choi et al.~measured a
\textbf{3.25\(\times\) speedup} on ResNet-50 ImageNet training when
reading data over a network, with minimal accuracy degradation.

\textbf{The Trade-Off}: Data echoing trades sample diversity for GPU
utilization. It works best when:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Augmentation is diverse (each echo sees different transforms)
\item
  The dataset is already somewhat redundant
\item
  The echo factor \(e\) stays below the critical threshold
  (\textasciitilde{}\(4\times\) for ImageNet)
\end{enumerate}

Above this threshold, the model starts memorizing and accuracy degrades.

\end{fbx}

Data echoing also interacts subtly with batch normalization. When the
same image appears multiple times in a batch (or across nearby batches),
batch normalization statistics become less representative of the true
data distribution. This correlation violates the independence assumption
underlying batch normalization's effectiveness. Practitioners address
this by excluding consecutive echoes from the same batch or by
maintaining separate batch normalization statistics for echoed samples.

These engineering patterns provide production-ready implementations of
data efficiency principles. Proxy selection reduces the computational
cost of identifying valuable samples. Sharded formats and shuffle
buffers reconcile random access algorithms with sequential storage
hardware. Data echoing maximizes GPU utilization when the data pipeline
becomes the bottleneck. Together, they transform data efficiency from an
algorithmic idea into a deployable system. The question then becomes:
which techniques merit investment for a given workload?

\section{Cost Modeling and
Economics}\label{sec-data-efficiency-cost-modeling-economics-b702}

The systems framing of data efficiency demands quantitative answers:
\emph{Should I label 10,000 more samples or buy more GPU hours? When
does active learning pay for itself? What's the ROI of investing in
deduplication infrastructure?}

\subsection{The Total Cost of Training
Data}\label{sec-data-efficiency-total-cost-training-data-92b9}

To answer these questions, practitioners must first understand what
training data actually costs. Total expense encompasses the full
lifecycle of data acquisition, preparation, and utilization, extending
well beyond storage fees:

\[
C_{\text{total}} = C_{\text{acquire}} + C_{\text{label}} + C_{\text{store}} + C_{\text{process}}
\]

where:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3143}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2571}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4286}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Component
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Formula
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Typical Range
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\(C_{\text{acquire}}\) & \(N \times c_{\text{sample}}\) &
\$0.001--\$10/sample (web scrape vs.~licensed) \\
\(C_{\text{label}}\) & \(N_{\text{labeled}} \times c_{\text{label}}\) &
\$0.10--\$100/sample (crowd vs.~expert) \\
\(C_{\text{store}}\) &
\(S_{\text{bytes}} \times c_{\text{storage}} \times T\) &
\$0.02--\$0.10/GB/month \\
\(C_{\text{process}}\) & \(N \times E \times c_{\text{FLOP}}\) &
Proportional to training FLOPs \\
\end{longtable}

For a concrete example, consider training a vision model:

\phantomsection\label{callout-exampleux2a-1.14}
\begin{fbx}{callout-example}{Example: }{Cost Breakdown: ImageNet-Scale Training}
\phantomsection\label{callout-example*-1.14}

\begin{longtable}[]{@{}lll@{}}
\toprule\noalign{}
Cost Component & Calculation & Amount \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Raw data (1.2M images) & Licensed dataset & \$50,000 \\
Labels (1.2M × \$0.05) & Crowd annotation & \$60,000 \\
Storage (150 GB × 12 months) & Cloud storage & \$200 \\
Training (100 epochs × 8 A100s × 24 h) & GPU compute & \$25,000 \\
\textbf{Total} & & \textbf{\$135,200} \\
\textbf{Data vs.~Compute ratio} & & \textbf{81\% data, 19\% compute} \\
\end{longtable}

This ratio, where data costs dominate, is typical for supervised
learning. The ratio inverts for self-supervised learning on web-scraped
data, where compute dominates.

\end{fbx}

\subsection{ROI Framework for Data Efficiency
Techniques}\label{sec-data-efficiency-roi-framework-data-efficiency-techniques-03cd}

Understanding total costs enables rational decisions about which
efficiency techniques merit investment. Every technique carries both a
cost (implementation effort, compute overhead) and a benefit (reduced
data requirements, faster training). Comparing these trade-offs requires
a common framework: \textbf{Return on Investment} (ROI).

\[
\text{ROI} = \frac{\text{Savings} - \text{Investment}}{\text{Investment}} \times 100\%
\]

The challenge lies in quantifying both sides accurately. Different
techniques offer distinct cost-benefit profiles:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2637}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3516}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3736}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Technique}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Investment (Cost)}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Savings (Benefit)}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Deduplication} & One-time compute for hashing + infrastructure &
Reduced storage, fewer epochs for same accuracy \\
\textbf{Coreset Selection} & Proxy model training + selection compute &
Train on 10--50\% of data with minimal accuracy loss \\
\textbf{Active Learning} & Inference on unlabeled pool +
human-in-the-loop latency & 2--10\(\times\) reduction in labeling budget
for same acc. \\
\textbf{Data Augmentation} & CPU/GPU cycles for transforms & Effective
dataset size increase without new data acquisition \\
\end{longtable}

\subsection{Break-Even
Analysis}\label{sec-data-efficiency-breakeven-analysis-ec3a}

ROI calculations assume that techniques deliver their promised benefits,
but actual outcomes vary considerably. For any technique, there exists a
\textbf{break-even point} where investment equals savings. Below this
threshold, the technique costs more than it saves; above it, the
technique generates value. Identifying this threshold determines whether
a technique makes sense for a given project.

\textbf{Example: Active Learning Break-Even}

Suppose labeling costs \$10/sample and active learning requires:

\begin{itemize}
\tightlist
\item
  Initial labeled set: 1,000 samples (\$10,000)
\item
  Oracle queries per round: 100 samples
\item
  Inference cost per round: \$50 (scoring unlabeled pool)
\item
  Target accuracy achievable with 5,000 random samples
\end{itemize}

If active learning reaches target accuracy with only 2,000 labeled
samples:

\[
\text{Random labeling cost} = 5000 \times \$10 = \$50,000
\]

\[
\text{Active learning cost} = 2000 \times \$10 + 10 \text{ rounds} \times \$50 = \$20,500
\]

\[
\text{ROI} = \frac{\$50,000 - \$20,500}{\$20,500} \times 100\% = 144\%
\]

The break-even occurs when the labeling reduction equals the selection
overhead. If active learning only reduces labeling by 20\%, and
selection overhead is high, ROI may be negative.

\subsection{Amortization: The Time Value of Data
Efficiency}\label{sec-data-efficiency-amortization-time-value-data-efficiency-26c5}

Break-even analysis captures a snapshot in time, but many data
efficiency investments span multiple projects. Techniques with high
upfront costs yield significant returns when their benefits compound
across repeated training runs. The \textbf{amortized ROI} accounts for
this temporal dimension:

\[
\text{Amortized ROI} = \frac{N_{runs} \times \text{Per-Run Savings} - \text{One-Time Investment}}{\text{One-Time Investment}}
\]

\textbf{Example: Deduplication Infrastructure}

\begin{longtable}[]{@{}ll@{}}
\toprule\noalign{}
Component & Cost \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Build deduplication pipeline & \$50,000 (engineering time) \\
Compute MinHash signatures (one-time) & \$5,000 \\
Per-run savings (20\% less data) & \$10,000/run \\
\end{longtable}

\begin{longtable}[]{@{}ll@{}}
\toprule\noalign{}
Number of Runs & Amortized ROI \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
1 run & -82\% (net loss) \\
5 runs & -9\% (near break-even) \\
10 runs & +82\% (positive) \\
50 runs & +809\% (highly profitable) \\
\end{longtable}

This pattern reveals which circumstances favor infrastructure
investment. Data efficiency investments deliver the highest returns
under three conditions: training runs are repeated frequently during
hyperparameter search, model iterations, or scheduled retraining;
datasets are shared across multiple teams or model architectures; and
the technique generalizes broadly. Deduplication exemplifies a
high-transfer investment: it benefits all models trained on the cleaned
dataset. Task-specific coresets, by contrast, may not transfer across
architectures, limiting their amortization potential. For one-off
training runs, simple techniques like random sampling or basic
augmentation often yield better ROI than sophisticated methods requiring
substantial infrastructure investment.

\begin{tcolorbox}[enhanced jigsaw, breakable, colbacktitle=quarto-callout-tip-color!10!white, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{When to Invest in Data Efficiency}, colback=white, bottomtitle=1mm, bottomrule=.15mm, coltitle=black, rightrule=.15mm, opacityback=0, opacitybacktitle=0.6, toptitle=1mm, toprule=.15mm, titlerule=0mm, arc=.35mm, colframe=quarto-callout-tip-color-frame, leftrule=.75mm, left=2mm]

\textbf{High ROI scenarios:}

\begin{itemize}
\tightlist
\item
  Labeling is expensive (medical, legal, scientific domains)
\item
  Dataset is large and redundant (web-scraped corpora)
\item
  Training runs are repeated frequently (hyperparameter search,
  retraining)
\item
  Iteration speed matters more than final accuracy
\end{itemize}

\textbf{Low ROI scenarios:}

\begin{itemize}
\tightlist
\item
  Labeling is cheap or already done
\item
  Dataset is small and curated
\item
  Single training run (one-time cost)
\item
  Accuracy matters more than efficiency
\end{itemize}

\end{tcolorbox}

\section{Distributed Data
Efficiency}\label{sec-data-efficiency-distributed-data-efficiency-25a8}

Everything we have discussed so far assumes a single-machine view: one
process can see the entire dataset, compute global statistics, and make
coordinated selection decisions. Production ML training breaks this
assumption. When data is sharded across hundreds of workers, each seeing
only a local slice, fundamental questions arise: How do you compute a
global coreset when no single node sees all samples? How do you maintain
consistent curriculum difficulty rankings when the model updates
asynchronously across workers?

This section examines how data efficiency techniques scale in
distributed settings, and where they fail to do so.

\subsection{The Distributed Selection
Problem}\label{sec-data-efficiency-distributed-selection-problem-24b5}

In standard distributed training, data parallelism is straightforward:
shard the dataset across workers, each processes its shard
independently. Data efficiency techniques, however, introduce
\textbf{selection dependencies}:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.1930}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4211}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3860}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Technique
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Single-Node Assumption
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Distributed Challenge
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Coreset Selection & Global view of dataset & Each worker sees only its
shard \\
Active Learning & Centralized uncertainty scoring & Scoring requires
model synchronization \\
Curriculum Learning & Global difficulty ordering & Workers may have
different ``hardest'' samples \\
Deduplication & Hash table fits in memory & Distributed hash tables add
latency \\
\end{longtable}

\subsection{Strategies for Distributed
Selection}\label{sec-data-efficiency-strategies-distributed-selection-fcd6}

These selection dependencies admit several architectural solutions, each
navigating a different point in the consistency-scalability trade-off
space.

The most straightforward approach centralizes selection while
distributing training. A coordinator node performs selection on the full
dataset, then distributes selected indices to workers. This preserves
selection quality but introduces a bottleneck:

\begin{verbatim}
Coordinator: score_all_samples() → selected_indices
Broadcast: selected_indices → all workers
Workers: train on subset(local_shard, selected_indices)
\end{verbatim}

The semantics remain clean, but the coordinator becomes a single point
of failure and a bandwidth bottleneck for large selections. For modest
cluster sizes, this overhead is acceptable; for thousand-node
deployments, it becomes prohibitive.

Hierarchical selection addresses this scalability limitation by
distributing the selection computation itself. Each worker performs
local selection on its shard, then a coordinator merges results:

\begin{verbatim}
Workers: local_selected = select_top_k(local_shard)
Coordinator: global_selected = merge_and_rerank(all local_selected)
Broadcast: final_indices → all workers
\end{verbatim}

This approach reduces coordinator load substantially, but introduces a
quality trade-off: the system may miss globally important samples that
appear unimportant within their local shard. A sample that is only
moderately difficult on one worker might be the hardest example in the
entire dataset when considered globally.

When even hierarchical approaches prove too expensive, approximate
global selection offers a fallback. These methods trade exactness for
scalability through distributed approximate algorithms. Distributed
MinHash enables deduplication by having each worker compute MinHash
signatures independently; signatures are then aggregated to find
near-duplicates across shards without requiring any single node to see
all the data. Similarly, federated uncertainty sampling allows workers
to compute local uncertainty scores, with a global threshold determined
by score distribution statistics rather than exact ranking.

\subsection{Consistency Challenges in Active
Learning}\label{sec-data-efficiency-consistency-challenges-active-learning-c071}

The approximate selection strategies above assume static selection
criteria, but active learning introduces an additional complication: the
model changes during selection. Consider what happens when Worker A
scores samples using the model at step \(t\), while Worker B
simultaneously updates the model to step \(t+1\). Worker A's scores are
now stale, potentially selecting samples that the updated model would
rank differently.

Several strategies mitigate this staleness problem, each with distinct
overhead characteristics. Synchronous scoring forces all workers to
pause training and score simultaneously, guaranteeing consistency but at
substantial cost in GPU utilization. Periodic score refresh offers a
middle ground by re-scoring every \(k\) epochs rather than every batch,
trading freshness for reduced overhead. The most robust approach selects
samples that exhibit high uncertainty under multiple model checkpoints,
ensuring that selection decisions remain valid even as the model
evolves.

\phantomsection\label{callout-exampleux2a-1.15}
\begin{fbx}{callout-example}{Example: }{End-to-End: Distributed Coreset Selection for ImageNet}
\phantomsection\label{callout-example*-1.15}
\textbf{Scenario}: Select a 10\% coreset from ImageNet (1.2M images)
using 8 workers with 4 GPUs each.

\textbf{Architecture}:

\begin{verbatim}
┌─────────────────────────────────────────────────────────┐
│                    Coordinator Node                      │
│  • Maintains global embedding index (FAISS)             │
│  • Merges local selections                              │
│  • Broadcasts final coreset indices                     │
└─────────────────────────────────────────────────────────┘
        ↑                    ↑                    ↑
        │ local_scores       │ local_scores       │
┌───────┴───────┐    ┌───────┴───────┐    ┌───────┴───────┐
│   Worker 0    │    │   Worker 1    │    │   Worker N    │
│ 150K images   │    │ 150K images   │    │ 150K images   │
│ Local EL2N    │    │ Local EL2N    │    │ Local EL2N    │
└───────────────┘    └───────────────┘    └───────────────┘
\end{verbatim}

\textbf{Pipeline}: 1. \textbf{Embedding phase} (parallel): Each worker
computes ResNet-18 embeddings for its shard → store in shared filesystem
2. \textbf{Deduplication phase} (distributed): Coordinator builds FAISS
index, workers query for near-duplicates → remove 15\% duplicates 3.
\textbf{Scoring phase} (parallel): Each worker computes EL2N scores on
its deduplicated shard using proxy model 4. \textbf{Selection phase}
(centralized): Coordinator collects top-20\% scores from each worker,
re-ranks globally, selects final 10\% 5. \textbf{Broadcast}: Selected
indices distributed to all workers for training

\textbf{Performance} (measured on \(8\times\) A100 cluster):

\begin{itemize}
\tightlist
\item
  Embedding: 20 minutes (parallel)
\item
  Deduplication: 15 minutes (distributed hash join)
\item
  Scoring: 30 minutes (parallel, 5 epochs proxy training)
\item
  Selection: 2 minutes (centralized)
\item
  \textbf{Total overhead: 67 minutes} for \(10\times\) training speedup
\end{itemize}

\textbf{Key insight}: The 67-minute selection overhead pays for itself
if full training takes \textgreater12 hours. For ImageNet with modern
architectures, full training is \textasciitilde24 hours, so coreset
selection has clear positive ROI.

\end{fbx}

However, this positive ROI can erode quickly when workers must
coordinate frequently during training.

\begin{tcolorbox}[enhanced jigsaw, breakable, colbacktitle=quarto-callout-warning-color!10!white, title=\textcolor{quarto-callout-warning-color}{\faExclamationTriangle}\hspace{0.5em}{The Coordination Tax}, colback=white, bottomtitle=1mm, bottomrule=.15mm, coltitle=black, rightrule=.15mm, opacityback=0, opacitybacktitle=0.6, toptitle=1mm, toprule=.15mm, titlerule=0mm, arc=.35mm, colframe=quarto-callout-warning-color-frame, leftrule=.75mm, left=2mm]

Distributed data efficiency always incurs a \textbf{coordination tax}:
the overhead of maintaining consistent selection across workers. This
tax must be smaller than the efficiency gains, or distributed selection
has negative ROI.

\textbf{Rule of thumb:} If selection overhead exceeds 10\% of training
time, simplify the selection strategy or increase selection interval.

\end{tcolorbox}

The preceding sections treated data efficiency as an isolated
optimization: techniques to reduce dataset size, select better samples,
or generate synthetic data. But real ML systems combine multiple
optimizations simultaneously. A coreset-trained model will be quantized.
A curriculum-learning pipeline will run on specialized accelerators. How
do these optimizations interact?

\section{Interactions with Other
Optimizations}\label{sec-data-efficiency-interactions-optimizations-02bc}

Data efficiency does not exist in isolation. Its interactions with other
optimization techniques range from complementary to conflicting.
Understanding these interactions helps practitioners design end-to-end
efficient systems rather than optimizing components in isolation.

\subsection{Data Efficiency × Model
Compression}\label{sec-data-efficiency-data-efficiency-model-compression-3d6d}

Model compression (\textbf{?@sec-model-compression}) reduces the size of
the trained model through pruning, quantization, and distillation. The
training dataset directly affects how compressible the resulting model
becomes, and perhaps counterintuitively, models trained on smaller,
higher-quality datasets may be \emph{more} compressible than those
trained on larger, noisier ones.

The mechanism behind this effect relates to how models encode
information. A model trained on repetitive data learns redundant
features that pruning later removes. The training compute required to
learn those features was wasted, only to be discarded during
compression. By contrast, a model trained on diverse, informative
samples learns compact, non-redundant representations from the start,
making subsequent compression more effective.

Empirical evidence supports this relationship. In experiments on
ImageNet, models trained on 50\% coresets selected by EL2N compress to
4-bit precision with 2\% less accuracy loss than models trained on the
full dataset. The curated training led to cleaner weight distributions
that quantize more gracefully.

Data efficiency and model compression are therefore
\emph{complementary}. The techniques in this chapter can reduce both
training cost \emph{and} post-training compression effort. When planning
an efficiency pipeline, apply data efficiency first; the resulting model
will be easier to compress.

\subsection{Data Efficiency × Hardware
Acceleration}\label{sec-data-efficiency-data-efficiency-hardware-acceleration-be6d}

While model compression affects what happens after training, hardware
acceleration determines how efficiently training itself proceeds.
Hardware acceleration (\textbf{?@sec-ai-acceleration}) increases
throughput through specialized accelerators, kernel optimization, and
parallelization. Data efficiency affects which hardware bottlenecks
dominate, and this relationship is more nuanced than simple speedup
calculations suggest:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.1961}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3725}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4314}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Scenario
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Likely Bottleneck
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Hardware Optimization
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Large, sequential dataset & Memory bandwidth & Larger batch sizes,
gradient accumulation \\
Small, curated dataset & Compute (GPU idle waiting for data) & Faster
data loaders, data echoing \\
Dynamic selection & Selection compute & Proxy models, cached
embeddings \\
\end{longtable}

Data efficiency can therefore shift the system from one bottleneck
regime to another. A technique that reduces dataset size by 80\% may
move the bottleneck from I/O to GPU compute, requiring different
hardware optimizations. Before applying aggressive data reduction,
profile your system to understand which bottleneck you're targeting.

\subsection{Data Efficiency × Distributed
Training}\label{sec-data-efficiency-data-efficiency-distributed-training-b34a}

The hardware bottleneck analysis above assumes single-machine training,
but the interactions become more complex when scaling to multiple
machines. Data efficiency affects different parallelism strategies in
distinct ways.

Under strong scaling, where a fixed dataset is distributed across more
workers, data efficiency reduces communication overhead by reducing
gradient updates per epoch. Fewer samples means fewer synchronization
points, and communication costs often dominate at large worker counts.
Under weak scaling, where each worker processes more data as the cluster
grows, data efficiency techniques can maintain accuracy while adding
workers without proportionally increasing total data. This capability
proves essential when data collection rather than compute is the
bottleneck. Even within straightforward data parallelism, smaller
curated datasets reduce the per-worker shard size, potentially improving
cache utilization and reducing I/O stalls on each node.

These benefits must be weighed against the distributed selection
challenges discussed in
Section~\ref{sec-data-efficiency-distributed-data-efficiency-25a8}. A
technique that works well on a single GPU may have prohibitive
coordination overhead across 1000 workers, negating any efficiency
gains.

\subsection{The Optimization
Stack}\label{sec-data-efficiency-optimization-stack-05ca}

The preceding sections examined pairwise interactions, but production
systems apply all these optimizations together. The full optimization
stack, from data to deployment, can be visualized as a pipeline where
each stage amplifies or attenuates the effects of others:

\begin{verbatim}
Raw Data → [Data Efficiency] → Curated Data → [Training] → Model
         → [Compression] → Compact Model → [Hardware] → Deployed System
\end{verbatim}

Optimizing early in the pipeline (data efficiency) has a
\textbf{multiplicative effect}: every FLOP saved in data processing is a
FLOP that never needs to be executed, compressed, or accelerated.

But how do we quantify this multiplicative effect? How do we know
whether a 50\% dataset reduction actually delivers 50\% compute savings,
or whether we've inadvertently degraded model quality in ways that will
surface only in production?

\section{Measuring Data
Efficiency}\label{sec-data-efficiency-measuring-data-efficiency-7957}

The techniques in this chapter---coreset selection, active learning,
augmentation---all claim to improve efficiency. Rigorous measurement
separates effective techniques from intuition.

\subsection{Core Metrics}\label{sec-data-efficiency-core-metrics-c0b5}

\textbf{Performance-Per-Data (PPD)}: The most direct metric measures
accuracy gain per sample:

\[
\text{PPD}(n) = \frac{\text{Accuracy}(n) - \text{Accuracy}(0)}{n}
\]

where \(n\) is the number of training samples. A higher PPD indicates
more efficient use of data. The key insight is that PPD exhibits
\textbf{diminishing returns}: the first 10,000 samples contribute far
more to model performance than the next 10,000.

\textbf{Area Under the Learning Curve (AULC)}: Rather than comparing at
a single point, AULC integrates performance across all dataset sizes:

\[
\text{AULC} = \int_0^N \text{Accuracy}(n) \, dn
\]

A data-efficient strategy has higher AULC because it achieves good
accuracy faster. This metric is particularly useful for comparing
coreset selection algorithms.

\textbf{Data Compression Ratio (DCR)}: For coreset methods, measure how
much data reduction is achieved at a target accuracy:

\[
\text{DCR} = \frac{N_{\text{full}}}{N_{\text{coreset}}} \text{ at } \text{Accuracy}_{\text{target}}
\]

A DCR of 5\(\times\) means the coreset achieves target accuracy with
20\% of the data.

\subsection{The Data Roofline
Model}\label{sec-data-efficiency-data-roofline-model-b400}

Just as the compute Roofline model diagnoses whether a system is
compute-bound or memory-bound, we can construct a \textbf{Data Roofline}
to diagnose whether training is \textbf{data-bound} or
\textbf{compute-bound} (Figure~\ref{fig-data-roofline}).

\begin{figure}[htb]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/404f2e60b493c4e17e9351952099a210c4c5f9b0.pdf}}

}

\caption{\label{fig-data-roofline}\textbf{The Data Roofline Model}:
Analogous to the compute Roofline, this diagnostic tool shows two
regimes. Below the diagonal (data-bound), adding more data improves
performance---invest in data collection. Above the diagonal
(compute-bound), more data won't help without more training
compute---invest in GPUs. The optimal operating point is at the knee
where data and compute are balanced. Data efficiency techniques move you
along the diagonal by extracting more value per sample.}

\end{figure}%

\textbf{Reading the Data Roofline}:

\begin{itemize}
\tightlist
\item
  \textbf{Below the diagonal (data-bound)}: Your system is limited by
  data quality, not compute. More FLOPs won't help---invest in better
  data curation, deduplication, or coreset selection to increase ICR.
\item
  \textbf{Above the diagonal (compute-bound)}: You have high-quality
  data but insufficient compute to exploit it. Adding more training time
  or GPUs will improve performance.
\item
  \textbf{At the knee}: Data quality and compute are balanced. This is
  the optimal operating point where both resources are fully utilized.
\end{itemize}

\textbf{Using the Roofline for diagnosis}: If your training run is
performing below expectations, compute your effective ICR (performance
gain per training FLOP) and plot your position. If you're in the
data-bound region, the techniques in this chapter (coreset selection,
curriculum learning, deduplication) will move you right along the
diagonal. If you're compute-bound, focus on hardware acceleration or
distributed training instead.

Figure~\ref{fig-ppd-curve} illustrates diminishing returns visually. A
data-efficient selection strategy (blue) reaches the performance plateau
much faster than random sampling (gray). The gap between the curves at
any dataset size represents the efficiency opportunity---compute that
could be saved by smarter data curation.

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/60961e95051446fe0845982b620d61a54b59530b.pdf}}

}

\caption{\label{fig-ppd-curve}\textbf{Diminishing Returns of Data}:
Random sampling (gray) versus data-efficient selection (blue). The
efficient strategy achieves higher performance with less data, reaching
the convergence plateau much earlier. The red arrow shows the efficiency
gap at a fixed dataset size.}

\end{figure}%

\textbf{The practical question} for practitioners: At what point should
you stop collecting data and start curating it? When does adding more
samples waste compute rather than improve accuracy? These questions
require rigorous metrics: ways to quantify diminishing returns, compare
selection strategies, and evaluate the cost-effectiveness of different
data sources.

\begin{tcolorbox}[enhanced jigsaw, breakable, colbacktitle=quarto-callout-note-color!10!white, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Validating Data Efficiency: The Benchmarking Connection}, colback=white, bottomtitle=1mm, bottomrule=.15mm, coltitle=black, rightrule=.15mm, opacityback=0, opacitybacktitle=0.6, toptitle=1mm, toprule=.15mm, titlerule=0mm, arc=.35mm, colframe=quarto-callout-note-color-frame, leftrule=.75mm, left=2mm]

Data efficiency techniques, including coreset selection, active
learning, and deduplication, all make implicit claims about the value of
different samples. But how do you know if your curated dataset actually
preserves model quality? The answer lies in systematic benchmarking.

\textbf{?@sec-benchmarking-ai} provides the measurement framework for
validating data efficiency investments. Specifically:

\begin{itemize}
\tightlist
\item
  \textbf{Coverage metrics} validate that coreset selection preserved
  representation across classes and demographic groups
\item
  \textbf{Distribution alignment metrics} detect if your curated
  training set drifted from the deployment distribution
\item
  \textbf{Label quality metrics} (inter-annotator agreement, confident
  learning) validate that active learning didn't introduce systematic
  labeling errors
\end{itemize}

The key insight is that data efficiency optimizations must be validated
just like model compression or hardware acceleration. A 50\% dataset
reduction is only valuable if benchmarking confirms the model maintains
target accuracy, calibration, and robustness. Otherwise, you've saved
compute at the cost of deployment failures.

\end{tcolorbox}

For a comprehensive treatment of data efficiency metrics and
benchmarking methodologies, including how initiatives like DataPerf are
standardizing evaluation protocols, see \textbf{?@sec-benchmarking-ai}.
That chapter provides the measurement framework needed to quantify the
ROI of the techniques introduced here.

\section{Fallacies and
Pitfalls}\label{sec-data-efficiency-fallacies-pitfalls-6285}

With metrics in hand to measure data efficiency, practitioners often
rush to implement techniques without recognizing the conceptual traps
and implementation errors that undermine their efforts. The following
fallacies represent persistent misconceptions about data efficiency,
while the pitfalls capture practical mistakes that sabotage otherwise
sound strategies. Understanding both is essential for translating theory
into production gains.

\textbf{Fallacy: ``Data is the new oil, so more is always better.''} The
``Data is Oil'' metaphor fails to capture the diminishing returns of
information. There is a saturation point where adding terabytes of data
yields negligible accuracy gains while exploding compute costs.
\textbf{Reality:} Data is like fuel: it has a specific energy density.
High-quality, curated data (high octane) powers models more efficiently
than vast quantities of raw data (crude oil).

\textbf{Fallacy: ``Synthetic data can completely replace real data.''}
While synthetic data addresses scarcity, it is bounded by the
generator's knowledge. A model trained purely on synthetic data from
another model risks ``Model Collapse,'' a degenerative feedback loop
where errors are amplified. \textbf{Reality:} Synthetic data augments,
but rarely replaces, the grounding provided by real-world distributions.
It is best used to fill gaps in the data manifold, not to define it.

\textbf{Fallacy: ``Data efficiency is just data cleaning.''} Cleaning
(removing errors) is necessary but insufficient. True data efficiency
involves \emph{selection} (finding the decision boundary) and
\emph{synthesis} (creating hard negatives). \textbf{Reality:} You can
have a perfectly clean dataset that is highly inefficient because it is
filled with redundant, easy examples. Efficiency requires optimizing the
information content, not just the hygiene.

\textbf{Fallacy: ``Data efficiency is only for resource-constrained
settings.''} Many practitioners view data efficiency as a corner-case
optimization for TinyML or budget-limited startups, irrelevant when
training foundation models with massive budgets. \textbf{Reality:} Data
efficiency is \emph{most} valuable at scale. A 10\% efficiency gain on a
\$100M training run saves \$10M. The Data Wall affects frontier labs
more than anyone; they have the compute but lack the data. The
techniques in this chapter are increasingly adopted by exactly those
organizations with ``unlimited'' resources.

These conceptual misunderstandings often lead to flawed strategies.
Equally damaging are the implementation pitfalls that arise when correct
strategies meet messy engineering realities.

\textbf{Pitfall: Optimizing selection without measuring selection
overhead.} A sophisticated coreset algorithm that takes 10 hours to
select samples for a 2-hour training run has negative ROI. Always
measure the Selection Inequality:
\(T_{selection} + T_{train}(subset) < T_{train}(full)\). \textbf{Fix:}
Use lightweight proxy models or cached embeddings for selection. Profile
selection time alongside training time.

\textbf{Pitfall: Pruning rare classes into oblivion.} Aggressive coreset
selection often removes rare classes entirely because they contribute
little to average loss. The model then fails catastrophically on these
classes in production. \textbf{Fix:} Stratify selection by class. Set
minimum samples per class before applying any pruning algorithm.

\textbf{Pitfall: Training on deduplicated data, evaluating on duplicated
test sets.} If your test set contains duplicates of training samples
(common in web-scraped data), deduplication appears to hurt performance
when it actually improves generalization. \textbf{Fix:} Deduplicate
train and test sets jointly, or use truly held-out evaluation data.

\textbf{Pitfall: Active learning without considering annotation
latency.} Active learning theory assumes instant oracle responses. In
practice, getting expert labels takes days or weeks. By the time labels
arrive, the model has moved on, and the selected samples may no longer
be optimal. \textbf{Fix:} Select larger batches to amortize latency. Use
diversity sampling to hedge against model drift.

Beyond these general implementation traps, a subtle but damaging class
of errors emerges when practitioners assume that benchmark results
transfer directly to their specific domains and deployment contexts.

\textbf{Fallacy: ``If a technique works on ImageNet, it will work on my
dataset.''} Data efficiency results are highly dataset-dependent.
CIFAR-10 is highly redundant, and 50\% coresets work well. ImageNet has
moderate redundancy. But domain-specific datasets (medical imaging,
satellite imagery, scientific data) may have near-zero redundancy where
every sample captures unique information. A coreset that preserves 95\%
accuracy on ImageNet may catastrophically fail on a well-curated
radiology dataset. \textbf{Reality:} Always pilot data efficiency
techniques on your specific distribution. The ``free lunch'' ratios
reported in benchmark papers (50\% pruning, 10\(\times\) label
reduction) rarely transfer directly. Start with conservative pruning
(20--30\%) and validate on held-out data before aggressive reduction.

\textbf{Pitfall: Optimizing data efficiency metrics instead of
deployment metrics.} A team achieves excellent PPD
(Performance-Per-Data) and DCR (Data Compression Ratio) during
development, having created a beautifully efficient 10\% coreset. But at
deployment, the model fails on edge cases: rare classes, unusual
lighting conditions, demographic subgroups underrepresented in the
coreset. The efficiency metrics looked great; the production metrics are
catastrophic. \textbf{Fix:} Include deployment-relevant evaluation in
data efficiency optimization. If the task requires 99.9\% reliability on
edge cases, ensure the coreset \emph{oversamples} those cases, even if
it reduces average PPD. Stratify evaluation by subgroup, not just
overall accuracy. The goal is deployment success, not benchmark
efficiency.

The exercises that follow provide opportunities to apply these
principles quantitatively, working through the ROI calculations and
trade-offs that separate theoretical understanding from practical
mastery.

\section{Exercises}\label{sec-data-efficiency-exercises-7ec0}

\begin{tcolorbox}[enhanced jigsaw, breakable, colbacktitle=quarto-callout-note-color!10!white, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Quantitative Exercises}, colback=white, bottomtitle=1mm, bottomrule=.15mm, coltitle=black, rightrule=.15mm, opacityback=0, opacitybacktitle=0.6, toptitle=1mm, toprule=.15mm, titlerule=0mm, arc=.35mm, colframe=quarto-callout-note-color-frame, leftrule=.75mm, left=2mm]

\textbf{Exercise 5.1 (Selection Inequality)}: You have a dataset of 2
million images. Training on the full dataset takes 48 hours on your GPU
cluster. You're considering EL2N coreset selection that would reduce the
dataset to 400,000 images (20\%).

\begin{enumerate}
\def\labelenumi{\alph{enumi})}
\tightlist
\item
  If EL2N scoring requires 0.002 seconds per image using a proxy model,
  what is the total selection overhead?
\item
  Assuming training time scales linearly with dataset size, what is the
  total time for the coreset approach (selection + training)?
\item
  Does the Selection Inequality hold? What is the speedup ratio?
\item
  If you used the full model (0.01 seconds per image) for scoring
  instead of a proxy, would the Selection Inequality still hold?
\end{enumerate}

\textbf{Exercise 5.2 (Deduplication ROI)}: A company is building a
deduplication pipeline for their 10TB training corpus. The estimated
costs are:

\begin{itemize}
\tightlist
\item
  Engineering time to build pipeline: \$80,000 (one-time)
\item
  Compute for MinHash signatures: \$3,000 (one-time)
\item
  Expected deduplication: 25\% of data removed
\item
  Training cost per run: \$50,000 on full dataset
\item
  Planned training runs: 20 (hyperparameter search + model iterations)
\end{itemize}

\begin{enumerate}
\def\labelenumi{\alph{enumi})}
\tightlist
\item
  Calculate the per-run savings from deduplication (assume linear cost
  scaling).
\item
  Calculate the break-even point (number of runs needed to recover
  investment).
\item
  Calculate the total ROI over all 20 planned runs.
\item
  If the company only planned 3 training runs, would deduplication be
  worth the investment?
\end{enumerate}

\textbf{Exercise 5.3 (Active Learning Budget)}: A medical imaging
company has a pool of 500,000 unlabeled X-rays. Expert radiologist
labeling costs \$25 per image. They have a budget of \$200,000 for
labeling.

\begin{enumerate}
\def\labelenumi{\alph{enumi})}
\tightlist
\item
  How many images can they label with naive random sampling?
\item
  Research shows active learning achieves the same accuracy as random
  sampling with 4× fewer labels. How many images would active learning
  require to match the accuracy of 8,000 randomly labeled images?
\item
  If each active learning round requires \$500 in compute (for
  uncertainty scoring), and each round selects 500 images, how many
  rounds can they afford while staying within budget?
\item
  What is the total number of labeled images they can acquire through
  active learning, and how does this compare to naive labeling?
\end{enumerate}

\textbf{Exercise 5.4 (ICR Computation)}: You're training a language
model and comparing two data selection strategies:

\textbf{Strategy A (Random)}:

\begin{itemize}
\tightlist
\item
  Dataset: 100B tokens, uniformly sampled
\item
  Training: 1 epoch = 3 × 10\^{}21 FLOPs
\item
  Final perplexity improvement: 15 points (from 50 to 35)
\end{itemize}

\textbf{Strategy B (Quality Filtered)}:

\begin{itemize}
\tightlist
\item
  Dataset: 30B tokens, filtered by perplexity and deduplication
\item
  Training: 1 epoch = 9 × 10\^{}20 FLOPs
\item
  Final perplexity improvement: 13 points (from 50 to 37)
\end{itemize}

\begin{enumerate}
\def\labelenumi{\alph{enumi})}
\tightlist
\item
  Compute the ICR (perplexity improvement per FLOP) for each strategy.
\item
  Which strategy is more data-efficient?
\item
  If you had a fixed compute budget of 5 × 10\^{}21 FLOPs, how many
  epochs could you train with each strategy?
\item
  Assuming diminishing returns (each subsequent epoch provides 60\% of
  the previous epoch's improvement), estimate final perplexity for each
  strategy under the fixed compute budget.
\end{enumerate}

\textbf{Exercise 5.5 (Data Echoing)}: Your training pipeline has the
following characteristics:

\begin{itemize}
\tightlist
\item
  Data loading + augmentation: 400 images/second
\item
  GPU training throughput: 1200 images/second
\item
  Dataset: 1 million images
\item
  Target: 100 epochs
\end{itemize}

\begin{enumerate}
\def\labelenumi{\alph{enumi})}
\tightlist
\item
  Calculate \(R\), the ratio of GPU throughput to data pipeline
  throughput.
\item
  What is the effective training throughput without data echoing?
\item
  If you use echo factor \(e = 2\), what is the new effective throughput
  (assuming echoed samples are processed at GPU speed)?
\item
  Calculate training time with and without echoing. What is the speedup?
\item
  If echoed samples provide only 80\% of the learning value of fresh
  samples, how does this affect the effective number of
  ``information-equivalent'' epochs?
\end{enumerate}

\textbf{Exercise 5.6 (Synthetic Data Mix)}: A robotics team is training
a grasping model. They have:

\begin{itemize}
\tightlist
\item
  10,000 real-world grasping attempts (\$20 each to collect = \$200,000)
\item
  Access to a physics simulator that generates grasps for \$0.01 each
\item
  A budget of \$250,000 total
\end{itemize}

Research shows that for their domain:

\begin{itemize}
\tightlist
\item
  100\% synthetic: 65\% success rate
\item
  80\% synthetic + 20\% real: 82\% success rate
\item
  50\% synthetic + 50\% real: 88\% success rate
\item
  100\% real: 90\% success rate
\end{itemize}

\begin{enumerate}
\def\labelenumi{\alph{enumi})}
\tightlist
\item
  They have \$50,000 remaining budget. How many synthetic samples can
  they generate?
\item
  If they want to maximize total training data, what mix should they
  use? Calculate the total samples.
\item
  If they want to maximize expected success rate within budget, what mix
  should they use?
\item
  Calculate the cost-per-accuracy-point for each strategy (\$/\% success
  rate).
\end{enumerate}

\end{tcolorbox}

\section*{Closing}\label{closing}
\addcontentsline{toc}{section}{Closing}

\markright{Closing}

This chapter opened with a question: why do smaller, curated datasets
sometimes outperform massive ones? The answer lies in recognizing data
efficiency as a \emph{systems} problem rather than a purely statistical
one. Where traditional machine learning asks ``how few samples achieve
target accuracy?'', the systems perspective asks ``how do we minimize
total cost across the entire pipeline?''

This reframing transforms how we approach the ML development lifecycle.
Rather than treating data as a static input to be collected and labeled,
data efficiency treats it as a dynamic resource to be engineered. The
goal is minimizing total cost across compute, storage, labeling, energy,
and time, not just maximizing accuracy.

We explored the three-stage optimization pipeline: \textbf{Static
Pruning} removes redundancy before training through coreset selection
and deduplication; \textbf{Dynamic Selection} focuses compute on
informative examples during training through curriculum and active
learning; and \textbf{Synthetic Generation} creates data where none
exists through augmentation, simulation, and distillation. Together,
these strategies address the ``Data Wall,'' the fundamental asymmetry
between exponentially growing compute and slowly growing high-quality
data.

The self-supervised learning paradigm represents a ceiling of data
efficiency: by eliminating task-specific labels entirely, foundation
models achieve 1000\(\times\) multipliers on downstream tasks through
cost amortization. This paradigm shift from ``train from scratch'' to
``pre-train once, fine-tune many'' has become the dominant approach in
production ML precisely because of its superior data economics.

\begin{tcolorbox}[enhanced jigsaw, breakable, colbacktitle=quarto-callout-important-color!10!white, title=\textcolor{quarto-callout-important-color}{\faExclamation}\hspace{0.5em}{Key Takeaways}, colback=white, bottomtitle=1mm, bottomrule=.15mm, coltitle=black, rightrule=.15mm, opacityback=0, opacitybacktitle=0.6, toptitle=1mm, toprule=.15mm, titlerule=0mm, arc=.35mm, colframe=quarto-callout-important-color-frame, leftrule=.75mm, left=2mm]

\begin{itemize}
\item
  \textbf{Data efficiency is a systems problem}: The goal is reduced
  cost across the entire pipeline---compute, storage, labeling,
  energy---not just ``fewer samples for same accuracy.''
\item
  \textbf{Start with deduplication}: Deduplication typically offers the
  highest return on investment: low cost, immediate gains, and no
  accuracy penalty. Deduplication should precede sophisticated selection
  methods.
\item
  \textbf{The Selection Inequality must hold}:
  \(T_{selection} + T_{train}(subset) < T_{train}(full)\). Selection
  overhead should be kept below 10\% of training time using proxy models
  or cached embeddings.
\item
  \textbf{Amortization determines ROI}: Data efficiency techniques are
  most effective when training repeats (hyperparameter search) or
  datasets are reused across multiple teams. For one-off training,
  simpler methods often outperform sophisticated approaches.
\item
  \textbf{Fine-tuning typically outperforms training from scratch}:
  Self-supervised pre-training amortizes cost across applications.
  Fine-tuning typically requires 100× fewer labels and 10× less per-task
  compute (when amortized) than from-scratch training.
\item
  \textbf{Avoid exclusive use of synthetic data}: Mixing 50--80\%
  synthetic with 20--50\% real data typically yields better results.
  Domain gap without mitigation can significantly degrade real-world
  performance.
\end{itemize}

\end{tcolorbox}

Throughout this chapter, we applied these principles to each Lighthouse
model, revealing how the same fundamental concepts manifest differently
depending on system constraints.

\phantomsection\label{callout-lighthouseux2a-1.16}
\begin{fbx}{callout-lighthouse}{Lighthouse Example: }{Data Efficiency Across the Lighthouse Spectrum}
\phantomsection\label{callout-lighthouse*-1.16}
This chapter has applied data efficiency principles to all five
Lighthouse models, demonstrating that the techniques are universal but
the priorities differ by bottleneck:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2143}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3393}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4464}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Lighthouse
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Primary Bottleneck
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Data Efficiency Priority
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{ResNet-50} & Compute & Coreset selection directly reduces
training FLOPs \\
\textbf{GPT-2/Llama} & Memory bandwidth & Deduplication reduces corpus
size; curriculum learning improves token efficiency \\
\textbf{MobileNet} & Latency/Power & Aggressive augmentation compensates
for reduced model capacity \\
\textbf{DLRM} & Memory capacity & Interaction deduplication and
embedding pruning reduce table size \\
\textbf{Keyword Spotting} & Extreme constraints & Augmentation and
synthesis create datasets from minimal seeds \\
\end{longtable}

The common thread: \textbf{data efficiency is not a single technique but
a systems optimization} tailored to whichever resource is most
constrained.

\end{fbx}

With data efficiency, we complete the optimization trilogy: model
compression (\textbf{?@sec-model-compression}) reduced the ``math
required to represent'' through pruning, quantization, and distillation;
hardware acceleration (\textbf{?@sec-ai-acceleration}) maximized the
``math per second'' through specialized silicon; and now data efficiency
has minimized the ``math required to learn'' by extracting more from
every sample.

But optimization without measurement is guesswork. How do you know
whether a 30\% dataset reduction actually translates to 30\% training
speedup, or whether I/O bottlenecks consume the gains? How do you
compare the efficiency of different coreset algorithms, or validate that
synthetic data maintains real-world accuracy? \textbf{Benchmarking}
(\textbf{?@sec-benchmarking-ai}) provides the measurement discipline
that transforms these optimization techniques from theoretical claims
into validated engineering decisions.

\phantomsection\label{refs}
\begin{CSLReferences}{1}{0}
\bibitem[\citeproctext]{ref-bengio2009curriculum}
Bengio, Yoshua, Jérôme Louradour, Ronan Collobert, and Jason Weston.
2009. {``Curriculum Learning.''} In \emph{Proceedings of the 26th Annual
International Conference on Machine Learning}, 41--48.

\bibitem[\citeproctext]{ref-bommasani2021opportunities}
Bommasani, Rishi, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arber,
Sydney von Arx, Michael S Bernstein, et al. 2021. {``On the
Opportunities and Risks of Foundation Models.''} \emph{arXiv Preprint
arXiv:2108.07258}.

\bibitem[\citeproctext]{ref-chen2020mocov2}
Chen, Xinlei, Haoqi Fan, Ross Girshick, and Kaiming He. 2020.
{``Improved Baselines with Momentum Contrastive Learning.''} \emph{arXiv
Preprint arXiv:2003.04297}.

\bibitem[\citeproctext]{ref-choi2020dataechoing}
Choi, Dami, Christopher J Shallue, Zachary Nado, Jaehoon Lee, Chris J
Maddison, and George E Dahl. 2020. {``Data Echoing for Efficient
Training.''} In \emph{International Conference on Machine Learning}.

\bibitem[\citeproctext]{ref-hinton2015distilling}
Hinton, Geoffrey, Oriol Vinyals, and Jeff Dean. 2015. {``Distilling the
Knowledge in a Neural Network,''} March.
\url{https://doi.org/10.1002/0471743984.vse0673}.

\bibitem[\citeproctext]{ref-hoffmann2022training}
Hoffmann, Jordan, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya,
Trevor Cai, Eliza Rutherford, Diego de Las Casas, et al. 2022.
{``Training Compute-Optimal Large Language Models.''} \emph{arXiv
Preprint arXiv:2203.15556}, March.
\url{http://arxiv.org/abs/2203.15556v1}.

\bibitem[\citeproctext]{ref-kaplan2020scaling}
Kaplan, Jared, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin
Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario
Amodei. 2020. {``Scaling Laws for Neural Language Models.''} \emph{arXiv
Preprint arXiv:2001.08361}, January.
\url{http://arxiv.org/abs/2001.08361v1}.

\bibitem[\citeproctext]{ref-lee2022deduplicating}
Lee, Katherine, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas
Eck, Chris Callison-Burch, and Nicholas Carlini. 2022. {``Deduplicating
Training Data Makes Language Models Better.''} \emph{arXiv Preprint
arXiv:2107.06499}.

\bibitem[\citeproctext]{ref-paul2021deep}
Paul, Mansheej, Surya Ganguli, and Gintare Karolina Dziugaite. 2021.
{``Deep Learning on a Data Diet: Finding Important Examples Early in
Training.''} In \emph{Advances in Neural Information Processing
Systems}, 34:20596--607.

\bibitem[\citeproctext]{ref-ren2021survey}
Ren, Pengzhen, Yun Xiao, Xiaojun Chang, Po-Yao Huang, Zhihui Li, Brij B
Gupta, Xiaojiang Chen, and Xin Wang. 2021. {``A Survey of Deep Active
Learning.''} \emph{ACM Computing Surveys} 54 (9): 1--40.

\bibitem[\citeproctext]{ref-settles2009active}
Settles, Burr. 2012. \emph{Active Learning}. \emph{University of
Wisconsin-Madison Department of Computer Sciences}. Vol. 1648. Springer
International Publishing.
\url{https://doi.org/10.1007/978-3-031-01560-1}.

\bibitem[\citeproctext]{ref-villalobos_ho_sevilla_besiroglu_heim_hobbhahn_2024}
Sevilla, Jaime, Lennart Heim, Anson Ho, Tamay Besiroglu, Marius
Hobbhahn, and Pablo Villalobos. 2022. {``Compute Trends Across Three
Eras of Machine Learning.''} In \emph{2022 International Joint
Conference on Neural Networks (IJCNN)}, 1--8. IEEE.
\url{https://doi.org/10.1109/ijcnn55064.2022.9891914}.

\bibitem[\citeproctext]{ref-sohn2020fixmatch}
Sohn, Kihyuk, David Berthelot, Nicholas Carlini, Zizhao Zhang, Han
Zhang, Colin A Raffel, Ekin Dogus Cubuk, Alexey Kurakin, and Chun-Liang
Li. 2020. {``{FixMatch}: Simplifying Semi-Supervised Learning with
Consistency and Confidence.''} In \emph{Advances in Neural Information
Processing Systems}, 33:596--608.

\bibitem[\citeproctext]{ref-soviany2022curriculum}
Soviany, Petru, Radu Tudor Ionescu, Paolo Rota, and Nicu Sebe. 2022.
{``Curriculum Learning: A Survey.''} \emph{International Journal of
Computer Vision} 130 (6): 1526--65.

\bibitem[\citeproctext]{ref-toneva2019empirical}
Toneva, Mariya, Alessandro Sordoni, Remi Tachet des Combes, Adam
Trischler, Yoshua Bengio, and Geoffrey J Gordon. 2019. {``An Empirical
Study of Example Forgetting During Deep Neural Network Learning.''} In
\emph{International Conference on Learning Representations}.

\bibitem[\citeproctext]{ref-villalobos2022will}
Villalobos, Pablo, Jaime Sevilla, Lennart Heim, Tamay Besiroglu, Marius
Hobbhahn, and Anson Ho. 2022. {``Will We Run Out of Data? An Analysis of
the Limits of Scaling Datasets in Machine Learning.''} \emph{arXiv
Preprint arXiv:2211.04325}.

\bibitem[\citeproctext]{ref-zhang2018mixup}
Zhang, Hongyi, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz.
2018. {``Mixup: Beyond Empirical Risk Minimization.''} In
\emph{International Conference on Learning Representations}.

\end{CSLReferences}


\backmatter


\end{document}
