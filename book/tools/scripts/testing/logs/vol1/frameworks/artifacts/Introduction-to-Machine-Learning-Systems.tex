% Options for packages loaded elsewhere
% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode,linktoc=all,pdfwindowui,pdfpagemode=FullScreen,pdfpagelayout=TwoPageRight}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  9pt,
  letterpaper,
  abstract,
  titlepage]{scrbook}
\usepackage{xcolor}
\usepackage[left=1in,marginparwidth=2.0666666666667in,textwidth=4.1333333333333in,marginparsep=0.3in]{geometry}
\usepackage{amsmath,amssymb}
\setcounter{secnumdepth}{3}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
% Make \paragraph and \subparagraph free-standing
\makeatletter
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}{
    \@ifstar
      \xxxParagraphStar
      \xxxParagraphNoStar
  }
  \newcommand{\xxxParagraphStar}[1]{\oldparagraph*{#1}\mbox{}}
  \newcommand{\xxxParagraphNoStar}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}{
    \@ifstar
      \xxxSubParagraphStar
      \xxxSubParagraphNoStar
  }
  \newcommand{\xxxSubParagraphStar}[1]{\oldsubparagraph*{#1}\mbox{}}
  \newcommand{\xxxSubParagraphNoStar}[1]{\oldsubparagraph{#1}\mbox{}}
\fi
\makeatother

\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{242,244,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{#1}}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.82,0.10,0.26}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{#1}}
\newcommand{\BuiltInTok}[1]{\textcolor[rgb]{0.88,0.40,0.10}{#1}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.41,0.45,0.49}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.86,0.20,0.18}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.58,0.00,0.30}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{#1}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.75,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.88,0.40,0.10}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.42,0.37,0.78}{#1}}
\newcommand{\ImportTok}[1]{\textcolor[rgb]{0.00,0.45,0.70}{\textbf{#1}}}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.73,0.49,0.84}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.10,0.10,0.10}{#1}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{#1}}
\newcommand{\RegionMarkerTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.12,0.55,0.30}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{#1}}}}

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\newsavebox\pandoc@box
\newcommand*\pandocbounded[1]{% scales image to fit in text height/width
  \sbox\pandoc@box{#1}%
  \Gscale@div\@tempa{\textheight}{\dimexpr\ht\pandoc@box+\dp\pandoc@box\relax}%
  \Gscale@div\@tempb{\linewidth}{\wd\pandoc@box}%
  \ifdim\@tempb\p@<\@tempa\p@\let\@tempa\@tempb\fi% select the smaller of both
  \ifdim\@tempa\p@<\p@\scalebox{\@tempa}{\usebox\pandoc@box}%
  \else\usebox{\pandoc@box}%
  \fi%
}
% Set default figure placement to htbp
\def\fps@figure{htbp}
\makeatother
% definitions for citeproc citations
\NewDocumentCommand\citeproctext{}{}
\NewDocumentCommand\citeproc{mm}{%
  \begingroup\def\citeproctext{#2}\cite{#1}\endgroup}
\makeatletter
 % allow citations to break across lines
 \let\@cite@ofmt\@firstofone
 % avoid brackets around text for \cite:
 \def\@biblabel#1{}
 \def\@cite#1#2{{#1\if@tempswa , #2\fi}}
\makeatother
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newenvironment{CSLReferences}[2] % #1 hanging-indent, #2 entry-spacing
 {\begin{list}{}{%
  \setlength{\itemindent}{0pt}
  \setlength{\leftmargin}{0pt}
  \setlength{\parsep}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1
   \setlength{\leftmargin}{\cslhangindent}
   \setlength{\itemindent}{-1\cslhangindent}
  \fi
  % set entry spacing
  \setlength{\itemsep}{#2\baselineskip}}}
 {\end{list}}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{\hfill\break\parbox[t]{\linewidth}{\strut\ignorespaces#1\strut}}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}

% =============================================================================
% LATEX HEADER CONFIGURATION FOR MLSYSBOOK PDF
% =============================================================================
% This file contains all LaTeX package imports, custom commands, and styling
% definitions for the PDF output of the Machine Learning Systems textbook.
%
% Key Features:
% - Harvard crimson branding throughout
% - Custom part/chapter/section styling
% - Professional table formatting with colored headers
% - Margin notes with custom styling
% - TikZ-based part dividers
% - Page numbering (Roman for frontmatter, Arabic for mainmatter)
%
% Note: This file is included via _quarto-pdf.yml and affects PDF output only.
% HTML/EPUB styling is handled separately via CSS files.
% =============================================================================

% =============================================================================
% PACKAGE IMPORTS
% =============================================================================

% Layout and positioning
% \usepackage[outercaption, ragged]{sidecap}  % Commented out to make figure captions inline instead of in margin
\usepackage{adjustbox}      % Adjusting box dimensions
\usepackage{afterpage}      % Execute commands after page break
\usepackage{morefloats}     % Increase number of floats
\usepackage{array}          % Enhanced table column formatting
\usepackage{atbegshi}       % Insert content at page beginning
%\usepackage{changepage}     % Change page dimensions mid-document
\usepackage{emptypage}      % Clear headers/footers on empty pages

% Language and text
\usepackage[english]{babel} % English language support
\usepackage{microtype}      % Improved typography and hyphenation

% Captions and floats
\usepackage{caption}
% Caption styling configuration
%\captionsetup[table]{belowskip=5pt}
\captionsetup{format=plain}
\DeclareCaptionLabelFormat{mylabel}{#1
#2:\hspace{1.0ex}}
\DeclareCaptionFont{ninept}{\fontsize{7pt}{8}\selectfont #1}

% Figure captions: Small font, bold label, ragged right
\captionsetup[figure]{labelfont={bf,ninept},labelsep=space,
belowskip=2pt,aboveskip=6pt,labelformat=mylabel,
justification=raggedright,singlelinecheck=false,font={ninept}}

% Table captions: Small font, bold label, ragged right
\captionsetup[table]{belowskip=6pt,labelfont={bf,ninept},labelsep=none,
labelformat=mylabel,justification=raggedright,singlelinecheck=false,font={ninept}}

% Typography fine-tuning
\emergencystretch=5pt       % Allow extra stretch to avoid overfull boxes

% Utility packages
\usepackage{etoolbox}       % For patching commands and environments

% Page layout and headers
\usepackage{fancyhdr}       % Custom headers and footers
\usepackage{geometry}       % Page dimensions and margins

% Graphics and figures
\usepackage{graphicx}       % Include graphics
\usepackage{float}          % Improved float placement
\usepackage[skins,breakable]{tcolorbox} % Coloured and framed text boxes
\tcbset{before upper=\setlength{\parskip}{3pt}}

% Tables
\usepackage{longtable}      % Multi-page tables

% Fonts and typography
\usepackage{fontspec}       % Font selection for LuaLaTeX
\usepackage{mathptmx}       % Times-like math fonts
\usepackage{newpxtext}      % Palatino-like font for body text

% Colors and visual elements
\usepackage[dvipsnames]{xcolor}  % Extended color support
\usepackage{tikz}           % Programmatic graphics
\usetikzlibrary{positioning}
\usetikzlibrary{calc}
\usepackage{tikzpagenodes}  % TikZ positioning relative to page

% Code listings
\usepackage{listings}       % Code highlighting

% Hyperlinks
\usepackage{hyperref}       % Clickable links in PDF

% Conditional logic
\usepackage{ifthen}         % If-then-else commands

% Math symbols
\usepackage{amsmath}        % AMS math extensions
\usepackage{amssymb}        % AMS math symbols
\usepackage{latexsym}       % Additional LaTeX symbols
\usepackage{pifont}         % Zapf Dingbats symbols
\providecommand{\blacklozenge}{\ding{117}}  % Black diamond symbol

% Lists
\usepackage{enumitem}       % Customizable lists

% Margin notes and sidenotes
\usepackage{marginfix}      % Fixes margin note overflow
\usepackage{marginnote}     % Margin notes
\usepackage{sidenotes}      % Academic-style sidenotes
\renewcommand\raggedrightmarginnote{\sloppy}
\renewcommand\raggedleftmarginnote{\sloppy}

% Typography improvements
\usepackage{ragged2e}       % Better ragged text
\usepackage[all]{nowidow}   % Prevent widows and orphans
\usepackage{needspace}      % Ensure minimum space on page

% Section formatting
\usepackage[explicit]{titlesec}  % Custom section titles
\usepackage{tocloft}        % Table of contents formatting

% QR codes and icons
\usepackage{fontawesome5}   % Font Awesome icons
\usepackage{qrcode}         % QR code generation
\qrset{link, height=15mm}

% =============================================================================
% FLOAT CONFIGURATION
% =============================================================================
% Allow more floats per page to handle figure-heavy chapters
\extrafloats{200}
\setcounter{topnumber}{12}       % Max floats at top of page
\setcounter{bottomnumber}{12}    % Max floats at bottom of page
\setcounter{totalnumber}{24}     % Max floats per page
\setcounter{dbltopnumber}{8}     % Max floats at top of two-column page
\renewcommand{\topfraction}{.95}  % Max fraction of page for top floats
\renewcommand{\bottomfraction}{.95}
\renewcommand{\textfraction}{.05}  % Min fraction of page for text
\renewcommand{\floatpagefraction}{.7}  % Min fraction of float page
\renewcommand{\dbltopfraction}{.95}

% Prevent "Float(s) lost" errors by flushing floats more aggressively
\usepackage{placeins}  % Provides \FloatBarrier

% =============================================================================
% COLOR DEFINITIONS
% =============================================================================
% Harvard crimson - primary brand color used throughout
\definecolor{crimson}{HTML}{A51C30}

% Quiz element colors
\definecolor{quiz-question-color1}{RGB}{225,243,248}  % Light blue background
\definecolor{quiz-question-color2}{RGB}{17,158,199}   % Blue border
\definecolor{quiz-answer-color1}{RGB}{250,234,241}    % Light pink background
\definecolor{quiz-answer-color2}{RGB}{152,14,90}      % Magenta border

% =============================================================================
% LIST FORMATTING
% =============================================================================
% Tighter list spacing for academic style
\def\tightlist{}
\setlist{itemsep=1pt, parsep=1pt, topsep=0pt,after={\vspace{0.3\baselineskip}}}
\let\tightlist\relax

\makeatletter
\@ifpackageloaded{framed}{}{\usepackage{framed}}
\@ifpackageloaded{fancyvrb}{}{\usepackage{fancyvrb}}
\makeatother

\makeatletter
%New float "codelisting" has been updated
\AtBeginDocument{%
\floatstyle{ruled}
\newfloat{codelisting}{!htb}{lop}
\floatname{codelisting}{Listing}
\floatplacement{codelisting}{!htb}
\captionsetup[codelisting]{labelfont={bf,ninept},labelformat=mylabel,
  singlelinecheck=false,width=\linewidth,labelsep=none,font={ninept}}%
\renewenvironment{snugshade}{%
   \def\OuterFrameSep{3pt}%
   \def\FrameCommand{\fboxsep=5pt\colorbox{shadecolor}}%
   \MakeFramed{\advance\hsize-\width\FrameRestore}%
   \leftskip 0.5em \rightskip 0.5em%
   \small% decrease font size
   }{\endMakeFramed}%
}
\makeatother

%The space before and after the verbatim environment "Highlighting" has been reduced
\fvset{listparameters=\setlength{\topsep}{0pt}\setlength{\partopsep}{0pt}}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{framesep=0mm,commandchars=\\\{\}}

\makeatletter
\renewcommand\fs@ruled{\def\@fs@cfont{\bfseries}\let\@fs@capt\floatc@ruled
\def\@fs@pre{\hrule height.8pt depth0pt \kern2pt}%
\def\@fs@post{\kern2pt\hrule\relax}%
\def\@fs@mid{\kern2pt\hrule\kern1pt}%space between float and caption
\let\@fs@iftopcapt\iftrue}
\makeatother


% =============================================================================
% HYPHENATION RULES
% =============================================================================
% Explicit hyphenation points for technical terms to avoid bad breaks
\hyphenation{
  light-weight
  light-weight-ed
  de-vel-op-ment
  un-der-stand-ing
  mod-els
  prin-ci-ples
  ex-per-tise
  com-pli-cat-ed
  blue-print
  per‧for‧mance
  com-mu-ni-ca-tion
  par-a-digms
  hy-per-ten-sion
  a-chieved
}

% =============================================================================
% CODE LISTING CONFIGURATION
% =============================================================================
% Settings for code blocks using listings package
\lstset{
breaklines=true,              % Automatic line wrapping
breakatwhitespace=true,       % Break at whitespace only
basicstyle=\ttfamily,         % Monospace font
frame=none,                   % No frame around code
keepspaces=true,              % Preserve spaces
showspaces=false,             % Don't show space characters
showtabs=false,               % Don't show tab characters
columns=flexible,             % Flexible column width
belowskip=0pt,               % Minimal spacing
aboveskip=0pt
}

% =============================================================================
% PAGE GEOMETRY
% =============================================================================
% MIT Press trim size: 7" x 10" (per publisher specifications)
% This is a standard academic textbook format providing good readability
% for technical content with figures and code blocks.
% Wide outer margin accommodates sidenotes/margin notes.
\geometry{
  paperwidth=7in,
  paperheight=10in,
  top=0.875in,
  bottom=0.875in,
  inner=0.875in,              % Inner margin (binding side)
  outer=1.75in,               % Outer margin (includes space for sidenotes)
  footskip=30pt,
  marginparwidth=1.25in,      % Width for margin notes
  twoside                     % Different left/right pages
}

% =============================================================================
% SIDENOTE STYLING
% =============================================================================
% Custom sidenote design with crimson vertical bar
\renewcommand{\thefootnote}{\textcolor{crimson}{\arabic{footnote}}}

% Save original sidenote command
\makeatletter
\@ifundefined{oldsidenote}{
  \let\oldsidenote\sidenote%
}{}
\makeatother

% Redefine sidenote with vertical crimson bar
\renewcommand{\sidenote}[1]{%
  \oldsidenote{%
    \noindent
    \color{crimson!100}                        % Crimson vertical line
    \raisebox{0em}{%
      \rule{0.5pt}{1.5em}                      % Thin vertical line
    }
    \hspace{0.3em}                             % Space after line
    \color{black}                              % Reset text color
    \footnotesize #1                           % Sidenote content
  }%
}

% =============================================================================
% FLOAT HANDLING
% =============================================================================
% Patch LaTeX's output routine to handle float overflow gracefully
% The "Float(s) lost" error occurs in \@doclearpage when \@currlist is not empty
% This patch silently clears pending floats that can't be placed
\makeatletter
\let\orig@doclearpage\@doclearpage
\def\@doclearpage{%
  \ifx\@currlist\@empty\else
    \global\let\@currlist\@empty
    \typeout{Warning: Floats cleared to prevent overflow}%
  \fi
  \orig@doclearpage
}
\makeatother

% Additional safety for structural commands
\let\originalbackmatter\backmatter
\renewcommand{\backmatter}{%
  \clearpage%
  \originalbackmatter%
}

\let\originalfrontmatter\frontmatter
\renewcommand{\frontmatter}{%
  \clearpage%
  \originalfrontmatter%
}

\let\originalmainmatter\mainmatter
\renewcommand{\mainmatter}{%
  \clearpage%
  \originalmainmatter%
}

% =============================================================================
% PAGE HEADERS AND FOOTERS
% =============================================================================
% Ensure chapters use fancy page style (not plain)
\patchcmd{\chapter}{\thispagestyle{plain}}{\thispagestyle{fancy}}{}{}

% Main page style with crimson headers
\pagestyle{fancy}
\fancyhf{}                                              % Clear all
\fancyhead[LE]{\small\color{crimson}\nouppercase{\rightmark}}  % Left even: section
\fancyhead[RO]{\color{crimson}\thepage}                 % Right odd: page number
\fancyhead[LO]{\small\color{crimson}\nouppercase{\leftmark}}   % Left odd: chapter
\fancyhead[RE]{\color{crimson}\thepage}                 % Right even: page number
\renewcommand{\headrulewidth}{0.4pt}                    % Thin header line
\renewcommand{\footrulewidth}{0pt}                      % No footer line

% Plain page style (for chapter openings)
\fancypagestyle{plain}{
  \fancyhf{}
  \fancyfoot[C]{\color{crimson}\thepage}                % Centered page number
  \renewcommand{\headrulewidth}{0pt}
  \renewcommand{\footrulewidth}{0pt}
}

% =============================================================================
% KOMA-SCRIPT FONT ADJUSTMENTS
% =============================================================================
% Apply crimson color to all heading levels
\addtokomafont{disposition}{\rmfamily\color{crimson}}
\addtokomafont{chapter}{\color{crimson}}
\addtokomafont{section}{\color{crimson}}
\addtokomafont{subsection}{\color{crimson}}

% =============================================================================
% ABSTRACT ENVIRONMENT
% =============================================================================
\newenvironment{abstract}{
  \chapter*{\abstractname}
  \addcontentsline{toc}{chapter}{\abstractname}
  \small
}{
  \clearpage
}

% =============================================================================
% HYPERLINK CONFIGURATION
% =============================================================================
% Crimson-colored links throughout, two-page PDF layout
\hypersetup{
  linkcolor=crimson,
  citecolor=crimson,
  urlcolor=crimson,
  pdfpagelayout=TwoPageRight,   % Two-page spread view
  pdfstartview=Fit               % Initial zoom fits page
}

% =============================================================================
% PART SUMMARY SYSTEM
% =============================================================================
% Allows adding descriptive text below part titles
\newcommand{\partsummary}{}     % Empty by default
\newif\ifhaspartsummary%
\haspartsummaryfalse%

\newcommand{\setpartsummary}[1]{%
  \renewcommand{\partsummary}{#1}%
  \haspartsummarytrue%
}

% Additional colors for part page backgrounds
\definecolor{BrownLL}{RGB}{233,222,220}
\definecolor{BlueDD}{RGB}{62,100,125}
\colorlet{BlueDD}{magenta}

% ===============================================================================
% PART STYLING SYSTEM
% ===============================================================================
%
% This system provides three distinct visual styles for book organization:
%
% 1. NUMBERED PARTS (\part{title}) - For main book sections
%    - Roman numerals (I, II, III, etc.) in top right corner
%    - Crimson title with horizontal lines above/below
%    - "Part I" label in sidebar
%    - Used for: foundations, principles, optimization, deployment, etc.
%
% 2. UNNUMBERED PARTS (\part*{title}) - For special sections like "Labs"
%    - Division-style geometric background (left side)
%    - No Roman numerals
%    - Used for: labs section
%
% 3. DIVISIONS (\division{title}) - For major book divisions
%    - Clean geometric background with centered title
%    - Used for: frontmatter, main_content, backmatter
%
% The Lua filter (inject-parts.lua) automatically routes parts by {key:xxx} commands
% to the appropriate LaTeX command based on the key name.
% ===============================================================================

% NUMBERED PARTS: Roman numeral styling for main book sections
\titleformat{\part}[display]
{\thispagestyle{empty}}{}{20pt}{
\begin{tikzpicture}[remember picture,overlay]
%%%
%%
\node[crimson,align=flush right,
inner sep=0,outer sep=0mm,draw=none,%
anchor=east,minimum height=31mm, text width=1.2\textwidth,
yshift=-30mm,font={%
\fontsize{98pt}{104}\selectfont\bfseries}]  (BG) at (current page text area.north east){\thepart};
%
\node[black,inner sep=0mm,draw=none,
anchor=mid,text width=1.2\textwidth,
 minimum height=35mm, align=right,
node distance=7mm,below=of BG,
font={\fontsize{30pt}{34}\selectfont}]
(BGG)  {\hyphenchar\font=-1 \color{black}\MakeUppercase {#1}};
\draw [crimson,line width=3pt] ([yshift=0mm]BGG.north west) -- ([yshift=0mm]BGG.north east);
\draw [crimson,line width=2pt] ([yshift=0mm]BGG.south west) -- ([yshift=0mm]BGG.south east);
%
\node[fill=crimson,text=white,rotate=90,%
anchor=south west,minimum height=15mm,
minimum width=40mm,font={%
\fontsize{20pt}{20}\selectfont\bfseries}](BP)  at
(current page text area.south east)
{{\sffamily Part}~\thepart};
%
\path[red](BP.north west)-|coordinate(PS)(BGG.south west);
%
% Part summary box commented out for cleaner design
% \ifhaspartsummary
% \node[inner sep=4pt,text width=0.7\textwidth,draw=none,fill=BrownLL!40,
% align=justify,font={\fontsize{9pt}{12}\selectfont},anchor=south west]
% at (PS) {\partsummary};
% \fi
\end{tikzpicture}
}[]

\renewcommand{\thepart}{\Roman{part}}

% UNNUMBERED PARTS: Division-style background for special sections
\titleformat{name=\part,numberless}[display]
{\thispagestyle{empty}}{}{20pt}{
\begin{tikzpicture}[remember picture,overlay]
%%%
\coordinate(S1)at([yshift=-200mm]current page.north west);
\draw[draw=none,fill=BlueDD!7](S1)--++(45:16)coordinate(S2)-
|(S2|-current page.north west)--(current page.north west)coordinate(S3)--(S1);
%
\coordinate(E1)at([yshift=-98mm]current page.north west);
\draw[draw=none,fill=BlueDD!15](E1)--(current page.north west)coordinate(E2)
--++(0:98mm)coordinate(E3)--(E1);
%
\coordinate(D1)at([yshift=15mm]current page.south west);
\draw[draw=none,fill=BlueDD!40,opacity=0.5](D1)--++(45:5.5)coordinate(D2)
-|(D2|-current page.north west)--(current page.north west)coordinate(D3)--(D1);
%%%%
\path[red](S2)-|(S2-|current page.east)coordinate(SS2);
%PART
\node[crimson,align=flush right,inner sep=0,outer sep=0mm,draw=none,anchor=south,
font={\fontsize{48pt}{48}\selectfont\bfseries}]  (BG) at ($(S2)!0.5!(SS2)$){\hphantom{Part}};
%%%
\path[green]([yshift=15mm]D2)-|coordinate(TPD)(BG.south east);
\node[inner sep=0mm,draw=none,anchor=south east,%text width=0.9\textwidth,
align=right,font={\fontsize{40pt}{40}\selectfont}]
(BGG) at (TPD)  {\color{crimson}\MakeUppercase {#1}};%\MakeUppercase {}
\end{tikzpicture}
}

% Define \numberedpart command for numbered parts
\newcommand{\numberedpart}[1]{%
\FloatBarrier%  % Flush all pending floats before part break
\clearpage
\thispagestyle{empty}
\stepcounter{part}%
\begin{tikzpicture}[remember picture,overlay]
%%%
%%
\node[crimson,align=flush right,
inner sep=0,outer sep=0mm,draw=none,%
anchor=east,minimum height=31mm, text width=1.2\textwidth,
yshift=-30mm,font={%
\fontsize{98pt}{104}\selectfont\bfseries}]  (BG) at (current page text area.north east){\thepart};
%
\node[black,inner sep=0mm,draw=none,
anchor=mid,text width=1.2\textwidth,
 minimum height=35mm, align=right,
node distance=7mm,below=of BG,
font={\fontsize{30pt}{34}\selectfont}]
(BGG)  {\hyphenchar\font=-1 \color{black}\MakeUppercase {#1}};
\draw [crimson,line width=3pt] ([yshift=0mm]BGG.north west) -- ([yshift=0mm]BGG.north east);
\draw [crimson,line width=2pt] ([yshift=0mm]BGG.south west) -- ([yshift=0mm]BGG.south east);
%
\node[fill=crimson,text=white,rotate=90,%
anchor=south west,minimum height=15mm,
minimum width=40mm,font={%
\fontsize{20pt}{20}\selectfont\bfseries}](BP)  at
(current page text area.south east)
{{\sffamily Part}~\thepart};
%
\path[red](BP.north west)-|coordinate(PS)(BGG.south west);
%
% Part summary box commented out for cleaner design
% \ifhaspartsummary
% \node[inner sep=4pt,text width=0.7\textwidth,draw=none,fill=BrownLL!40,
% align=justify,font={\fontsize{9pt}{12}\selectfont},anchor=south west]
% at (PS) {\partsummary};
% \fi
\end{tikzpicture}
\clearpage
}



% DIVISIONS: Clean geometric styling with subtle tech elements
% Used for frontmatter, main_content, and backmatter divisions
\newcommand{\division}[1]{%
\FloatBarrier%  % Flush all pending floats before division break
\clearpage
\thispagestyle{empty}
\begin{tikzpicture}[remember picture,overlay]

% Clean geometric background (original design)
\coordinate(S1)at([yshift=-200mm]current page.north west);
\draw[draw=none,fill=BlueDD!7](S1)--++(45:16)coordinate(S2)-
|(S2|-current page.north west)--(current page.north west)coordinate(S3)--(S1);

\coordinate(E1)at([yshift=-98mm]current page.north west);
\draw[draw=none,fill=BlueDD!15](E1)--(current page.north west)coordinate(E2)
--++(0:98mm)coordinate(E3)--(E1);

\coordinate(D1)at([yshift=15mm]current page.south west);
\draw[draw=none,fill=BlueDD!40,opacity=0.5](D1)--++(45:5.5)coordinate(D2)
-|(D2|-current page.north west)--(current page.north west)coordinate(D3)--(D1);

% Subtle tech elements - positioned in white areas for better visibility
% Upper right white area - more visible
\draw[crimson!40, line width=0.8pt] ([xshift=140mm,yshift=-60mm]current page.north west) -- ++(40mm,0);
\draw[crimson!40, line width=0.8pt] ([xshift=150mm,yshift=-70mm]current page.north west) -- ++(30mm,0);
\draw[crimson!35, line width=0.7pt] ([xshift=160mm,yshift=-60mm]current page.north west) -- ++(0,-15mm);
\draw[crimson!35, line width=0.7pt] ([xshift=170mm,yshift=-70mm]current page.north west) -- ++(0,10mm);

% Circuit nodes - upper right
\fill[crimson!50] ([xshift=160mm,yshift=-60mm]current page.north west) circle (1.5mm);
\fill[white] ([xshift=160mm,yshift=-60mm]current page.north west) circle (0.8mm);
\fill[crimson!50] ([xshift=170mm,yshift=-70mm]current page.north west) circle (1.3mm);
\fill[white] ([xshift=170mm,yshift=-70mm]current page.north west) circle (0.6mm);

% Lower right white area - enhanced visibility
\draw[crimson!45, line width=0.9pt] ([xshift=140mm,yshift=-190mm]current page.north west) -- ++(45mm,0);
\draw[crimson!45, line width=0.9pt] ([xshift=150mm,yshift=-200mm]current page.north west) -- ++(35mm,0);
\draw[crimson!40, line width=0.8pt] ([xshift=160mm,yshift=-190mm]current page.north west) -- ++(0,-20mm);
\draw[crimson!40, line width=0.8pt] ([xshift=170mm,yshift=-200mm]current page.north west) -- ++(0,15mm);

% Additional connecting lines in lower right
\draw[crimson!35, line width=0.7pt] ([xshift=130mm,yshift=-180mm]current page.north west) -- ++(25mm,0);
\draw[crimson!35, line width=0.7pt] ([xshift=145mm,yshift=-180mm]current page.north west) -- ++(0,-25mm);

% Circuit nodes - lower right (more prominent)
\fill[crimson!55] ([xshift=160mm,yshift=-190mm]current page.north west) circle (1.6mm);
\fill[white] ([xshift=160mm,yshift=-190mm]current page.north west) circle (0.9mm);
\fill[crimson!55] ([xshift=170mm,yshift=-200mm]current page.north west) circle (1.4mm);
\fill[white] ([xshift=170mm,yshift=-200mm]current page.north west) circle (0.7mm);
\fill[crimson!50] ([xshift=145mm,yshift=-180mm]current page.north west) circle (1.2mm);
\fill[white] ([xshift=145mm,yshift=-180mm]current page.north west) circle (0.6mm);

% Title positioned in center - clean and readable
\node[inner sep=0mm,draw=none,anchor=center,text width=0.8\textwidth,
align=center,font={\fontsize{40pt}{40}\selectfont}]
(BGG) at (current page.center)  {\color{crimson}\MakeUppercase {#1}};

\end{tikzpicture}
\clearpage
}

% LAB DIVISIONS: Circuit-style neural network design for lab sections
% Used specifically for lab platform sections (arduino, xiao, grove, etc.)
\newcommand{\labdivision}[1]{%
\FloatBarrier%  % Flush all pending floats before lab division break
\clearpage
\thispagestyle{empty}
\begin{tikzpicture}[remember picture,overlay]
% Circuit background with subtle gradient
\coordinate(S1)at([yshift=-200mm]current page.north west);
\draw[draw=none,fill=BlueDD!5](S1)--++(45:16)coordinate(S2)-
|(S2|-current page.north west)--(current page.north west)coordinate(S3)--(S1);

% TOP AREA: Circuit lines in upper white space
\draw[crimson!50, line width=1.5pt] ([xshift=30mm,yshift=-40mm]current page.north west) -- ++(60mm,0);
\draw[crimson!40, line width=1pt] ([xshift=120mm,yshift=-50mm]current page.north west) -- ++(50mm,0);
\draw[crimson!50, line width=1.5pt] ([xshift=40mm,yshift=-70mm]current page.north west) -- ++(40mm,0);

% Connecting lines in top area
\draw[crimson!30, line width=1pt] ([xshift=60mm,yshift=-40mm]current page.north west) -- ++(0,-20mm);
\draw[crimson!30, line width=1pt] ([xshift=145mm,yshift=-50mm]current page.north west) -- ++(0,10mm);

% Neural nodes in top area
\fill[crimson!70] ([xshift=60mm,yshift=-40mm]current page.north west) circle (2.5mm);
\fill[white] ([xshift=60mm,yshift=-40mm]current page.north west) circle (1.5mm);
\fill[crimson!60] ([xshift=145mm,yshift=-50mm]current page.north west) circle (2mm);
\fill[white] ([xshift=145mm,yshift=-50mm]current page.north west) circle (1mm);
\fill[crimson!80] ([xshift=80mm,yshift=-70mm]current page.north west) circle (2mm);
\fill[white] ([xshift=80mm,yshift=-70mm]current page.north west) circle (1mm);

% BOTTOM AREA: Circuit lines in lower white space
\draw[crimson!50, line width=1.5pt] ([xshift=20mm,yshift=-200mm]current page.north west) -- ++(70mm,0);
\draw[crimson!40, line width=1pt] ([xshift=110mm,yshift=-210mm]current page.north west) -- ++(60mm,0);
\draw[crimson!50, line width=1.5pt] ([xshift=35mm,yshift=-230mm]current page.north west) -- ++(45mm,0);

% Connecting lines in bottom area
\draw[crimson!30, line width=1pt] ([xshift=55mm,yshift=-200mm]current page.north west) -- ++(0,-20mm);
\draw[crimson!30, line width=1pt] ([xshift=140mm,yshift=-210mm]current page.north west) -- ++(0,15mm);

% Neural nodes in bottom area
\fill[crimson!70] ([xshift=55mm,yshift=-200mm]current page.north west) circle (2.5mm);
\fill[white] ([xshift=55mm,yshift=-200mm]current page.north west) circle (1.5mm);
\fill[crimson!60] ([xshift=140mm,yshift=-210mm]current page.north west) circle (2mm);
\fill[white] ([xshift=140mm,yshift=-210mm]current page.north west) circle (1mm);
\fill[crimson!80] ([xshift=80mm,yshift=-230mm]current page.north west) circle (2mm);
\fill[white] ([xshift=80mm,yshift=-230mm]current page.north west) circle (1mm);

% SIDE AREAS: Subtle circuit elements on left and right edges
\draw[crimson!30, line width=1pt] ([xshift=15mm,yshift=-120mm]current page.north west) -- ++(20mm,0);
\draw[crimson!30, line width=1pt] ([xshift=175mm,yshift=-130mm]current page.north west) -- ++(15mm,0);
\fill[crimson!50] ([xshift=25mm,yshift=-120mm]current page.north west) circle (1.5mm);
\fill[white] ([xshift=25mm,yshift=-120mm]current page.north west) circle (0.8mm);
\fill[crimson!50] ([xshift=185mm,yshift=-130mm]current page.north west) circle (1.5mm);
\fill[white] ([xshift=185mm,yshift=-130mm]current page.north west) circle (0.8mm);

% Title positioned in center - CLEAN AREA
\node[inner sep=0mm,draw=none,anchor=center,text width=0.8\textwidth,
align=center,font={\fontsize{44pt}{44}\selectfont\bfseries}]
(BGG) at (current page.center)  {\color{crimson}\MakeUppercase {#1}};

\end{tikzpicture}
\clearpage
}

% Define \lab command for lab styling (different visual treatment)
\newcommand{\lab}[1]{%
\begin{tikzpicture}[remember picture,overlay]
%%%
% Different background pattern for labs
\coordinate(S1)at([yshift=-200mm]current page.north west);
\draw[draw=none,fill=BlueDD!15](S1)--++(45:16)coordinate(S2)-
|(S2|-current page.north west)--(current page.north west)coordinate(S3)--(S1);
%
\coordinate(E1)at([yshift=-98mm]current page.north west);
\draw[draw=none,fill=BlueDD!25](E1)--(current page.north west)coordinate(E2)
--++(0:98mm)coordinate(E3)--(E1);
%
\coordinate(D1)at([yshift=15mm]current page.south west);
\draw[draw=none,fill=BlueDD!60,opacity=0.7](D1)--++(45:5.5)coordinate(D2)
-|(D2|-current page.north west)--(current page.north west)coordinate(D3)--(D1);
%%%%
\path[red](S2)-|(S2-|current page.east)coordinate(SS2);
%LAB - Different styling
\node[crimson,align=flush right,inner sep=0,outer sep=0mm,draw=none,anchor=south,
font={\fontsize{48pt}{48}\selectfont\bfseries}]  (BG) at ($(S2)!0.5!(SS2)$){\hphantom{Workshop}};
%%%
\path[green]([yshift=15mm]D2)-|coordinate(TPD)(BG.south east);
\node[inner sep=0mm,draw=none,anchor=south east,%text width=0.9\textwidth,
align=right,font={\fontsize{40pt}{40}\selectfont}]
(BGG) at (TPD)  {\color{crimson}\MakeUppercase {#1}};%\MakeUppercase {}
\end{tikzpicture}
\thispagestyle{empty}
\clearpage
}

% =============================================================================
% SECTION FORMATTING
% =============================================================================
% All section levels use crimson color and are ragged right

% Section (Large, bold, crimson)
\titleformat{\section}
  {\normalfont\Large\bfseries\color{crimson}\raggedright}
  {\thesection}
  {0.5em}
  {#1}
\titlespacing*{\section}{0pc}{14pt plus 4pt minus 4pt}{6pt plus 2pt minus 2pt}[0pc]

% Subsection (large, bold, crimson)
\titleformat{\subsection}
  {\normalfont\large\bfseries\color{crimson}\raggedright}
  {\thesubsection}
  {0.5em}
  {#1}
\titlespacing*{\subsection}{0pc}{12pt plus 4pt minus 4pt}{5pt plus 1pt minus 2pt}[0pc]

% Subsubsection (normal size, bold, crimson)
\titleformat{\subsubsection}
  {\normalfont\normalsize\bfseries\color{crimson}\raggedright}
  {\thesubsubsection}
  {0.5em}
  {#1}
\titlespacing*{\subsubsection}{0pc}{12pt plus 4pt minus 4pt}{5pt plus 1pt minus 2pt}[0pc]

% Paragraph (run-in, bold, crimson, ends with period)
\titleformat{\paragraph}[runin]
  {\normalfont\normalsize\bfseries\color{crimson}}
  {\theparagraph}
  {0.5em}
  {#1}
  [\textbf{.}]
  \titlespacing*{\paragraph}{0pc}{6pt plus 2pt minus 2pt}{0.5em}[0pc]

% Subparagraph (run-in, italic, crimson, ends with period)
\titleformat{\subparagraph}[runin]
  {\normalfont\normalsize\itshape\color{crimson}}
  {\thesubparagraph}
  {0.5em}
  {#1}
  [\textbf{.}]
  \titlespacing*{\subparagraph}{0pc}{6pt plus 2pt minus 2pt}{0.5em}[0pc]

% =============================================================================
% CHAPTER FORMATTING
% =============================================================================
% Numbered chapters: "Chapter X" prefix, huge crimson title
\titleformat{\chapter}[display]
  {\normalfont\huge\bfseries\color{crimson}}
  {\chaptername\ \thechapter}
  {20pt}
  {\Huge #1}
  []

% Unnumbered chapters: no prefix, huge crimson title
\titleformat{name=\chapter,numberless}
  {\normalfont\huge\bfseries\color{crimson}}
  {}
  {0pt}
  {\Huge #1}
  []

\renewcommand{\chaptername}{Chapter}
% =============================================================================
% TABLE OF CONTENTS FORMATTING
% =============================================================================
\setcounter{tocdepth}{2}                      % Show chapters, sections, subsections

% TOC spacing adjustments for number widths and indentation
\setlength{\cftchapnumwidth}{2em}             % Chapter number width
\setlength{\cftsecnumwidth}{2.75em}           % Section number width
\setlength{\cftsubsecnumwidth}{3.25em}        % Subsection number width
\setlength{\cftsubsubsecnumwidth}{4em}        % Subsubsection number width
\setlength{\cftsubsecindent}{4.25em}          % Subsection indent
\setlength{\cftsubsubsecindent}{7.5em}        % Subsubsection indent

% Chapter entries in TOC: bold crimson with "Chapter" prefix
\renewcommand{\cftchapfont}{\bfseries\color{crimson}}
\renewcommand{\cftchappresnum}{\color{crimson}Chapter~}

% Custom formatting for division entries (styled like parts)
\newcommand{\divisionchapter}[1]{%
  \addvspace{12pt}%
  \noindent\hfil\bfseries\color{crimson}#1\hfil\par%
  \addvspace{6pt}%
}

% Adjust TOC spacing for "Chapter" prefix
\newlength{\xtraspace}
\settowidth{\xtraspace}{\cftchappresnum\cftchapaftersnum}
\addtolength{\cftchapnumwidth}{\xtraspace}

% Unnumbered chapters with TOC entry
\newcommand{\likechapter}[1]{%
    \chapter*{#1}
    \addcontentsline{toc}{chapter}{\textcolor{crimson}{#1}}
}

% =============================================================================
% PAGE NUMBERING SYSTEM
% =============================================================================
% Implements traditional book numbering:
% - Roman numerals (i, ii, iii...) for frontmatter
% - Arabic numerals (1, 2, 3...) for mainmatter
% Automatically switches at first numbered chapter
\makeatletter
\newif\if@firstnumbered%
\@firstnumberedtrue%
\newif\if@firstunnumbered%
\@firstunnumberedtrue%

\newcounter{lastRomanPage}
\setcounter{lastRomanPage}{1}

% Start document with Roman numerals (frontmatter)
\AtBeginDocument{
  \pagenumbering{roman}
  \renewcommand{\thepage}{\roman{page}}
}

% Intercept chapter command
\let\old@chapter\chapter%
\renewcommand{\chapter}{%
  \@ifstar{\unnumbered@chapter}{\numbered@chapter}%
}

% Numbered chapters: switch to Arabic on first occurrence
\newcommand{\numbered@chapter}[1]{%
  \if@firstnumbered%
    \cleardoublepage%
    \setcounter{lastRomanPage}{\value{page}}%
    \pagenumbering{arabic}%
    \@firstnumberedfalse%
  \else
    \setcounter{page}{\value{page}}%
  \fi
  \setcounter{sidenote}{1}                    % Reset footnote counter per chapter
  \old@chapter{#1}%
}

% Unnumbered chapters: stay in Roman numerals
\newcommand{\unnumbered@chapter}[1]{%
  \if@firstunnumbered%
    \clearpage
    \setcounter{lastRomanPage}{\value{page}}%
    \pagenumbering{roman}%
    \@firstunnumberedfalse%
  \fi
  \setcounter{sidenote}{1}
  \old@chapter*{#1}%
}
\makeatother

% =============================================================================
% TABLE SIZING AND SPACING
% =============================================================================
% Make tables slightly smaller to fit more content
\AtBeginEnvironment{longtable}{\scriptsize}

% Increase vertical spacing in table cells (default is 1.0)
\renewcommand{\arraystretch}{1.5}

% Prefer placing figures and tables at the top of pages
\makeatletter
\renewcommand{\fps@figure}{t}  % Default placement: top of page
\renewcommand{\fps@table}{t}   % Default placement: top of page
\makeatother

% =============================================================================
% LONGTABLE PAGE BREAKING FIXES (Windows compatibility)
% =============================================================================
% Prevent "Infinite glue shrinkage" errors on Windows LaTeX builds
% by giving longtable more flexibility in page breaking

% Allow more flexible page breaking (vs strict \flushbottom)
\raggedbottom

% Process more rows before attempting page break (default is 20)
\setcounter{LTchunksize}{50}

% Add extra stretch for longtable environments specifically
\AtBeginEnvironment{longtable}{%
  \setlength{\emergencystretch}{3em}%
  \setlength{\parskip}{0pt plus 1pt}%
}

% =============================================================================
% TABLE STYLING - Clean tables with crimson borders
% =============================================================================
% Professional table appearance with:
% - Clean white background (no colored rows)
% - Crimson-colored borders
% - Good spacing for readability
%
% Note: Headers are automatically bolded by Quarto when using **text** in source
\usepackage{booktabs}      % Professional table rules (\toprule, \midrule, \bottomrule)
\usepackage{colortbl}      % For colored borders (\arrayrulecolor)

% Global table styling - crimson borders
\setlength{\arrayrulewidth}{0.5pt}          % Thinner borders than default
%\arrayrulecolor{crimson}                    % Crimson borders matching brand

\setcounter{chapter}{0}

% =============================================================================
% DROP CAPS (Lettrine)
% =============================================================================
% Decorative large first letter at chapter openings, following the tradition
% of Hennessy & Patterson and other MIT Press textbooks.
% Usage in QMD: \lettrine{T}{he first sentence...}
\usepackage{lettrine}
\renewcommand{\LettrineFontHook}{\color{crimson}\bfseries}
\setcounter{DefaultLines}{3}          % Drop cap spans 3 lines
\renewcommand{\DefaultLoversize}{0.1} % Slight oversize for visual weight
\renewcommand{\DefaultLraise}{0}      % No vertical shift
\setlength{\DefaultNindent}{0.5em}    % Indent of continuation text
\setlength{\DefaultSlope}{0pt}        % No slope on continuation

% =============================================================================
% RUNNING HEADERS — Truncation Safety
% =============================================================================
% Long chapter/section titles can overflow the header. These marks truncate
% gracefully so headers stay within the text block.
\renewcommand{\chaptermark}[1]{%
  \markboth{\thechapter.\ #1}{}}
\renewcommand{\sectionmark}[1]{%
  \markright{\thesection\ #1}}

% =============================================================================
% EPIGRAPH ENVIRONMENT
% =============================================================================
% For chapter-opening quotations. Renders as right-aligned italic block
% with attribution in small caps below.
% Usage: \epigraph{Quote text}{Author Name, \textit{Source}}
\newcommand{\bookepigraph}[2]{%
  \vspace{1em}%
  \begin{flushright}%
    \begin{minipage}{0.75\textwidth}%
      \raggedleft\itshape\small #1\\[0.5em]%
      \normalfont\small --- #2%
    \end{minipage}%
  \end{flushright}%
  \vspace{1.5em}%
}

% =============================================================================
% THUMB INDEX TABS
% =============================================================================
% Colored tabs on the outer page edge for quick chapter navigation.
% Each Part gets a different vertical position; chapters within a Part
% share the same tab position. Visible when flipping through the book.
\newcounter{thumbindex}
\setcounter{thumbindex}{0}
\newlength{\thumbtabheight}
\setlength{\thumbtabheight}{16mm}     % Height of each tab
\newlength{\thumbtabwidth}
\setlength{\thumbtabwidth}{8mm}       % Width protruding from edge
\newlength{\thumbtabgap}
\setlength{\thumbtabgap}{1mm}         % Gap between tabs

% Advance to next thumb tab position (call at each \part)
\newcommand{\nextthumb}{%
  \stepcounter{thumbindex}%
}

% Draw the thumb tab on every page (placed in header via fancyhdr)
\newcommand{\drawthumb}{%
  \ifnum\value{thumbindex}>0%
    \begin{tikzpicture}[remember picture,overlay]
      \pgfmathsetmacro{\thumboffset}{%
        20 + (\value{thumbindex}-1) * (16 + 1)}  % mm from top
      \ifodd\value{page}%
        % Odd pages: tab on right edge
        \fill[crimson!80]
          ([yshift=-\thumboffset mm]current page.north east)
          rectangle +(-\thumbtabwidth, -\thumbtabheight);
        \node[white,font=\tiny\bfseries,rotate=90]
          at ([yshift=-\thumboffset mm - 0.5\thumbtabheight,
               xshift=-0.5\thumbtabwidth]current page.north east)
          {\Roman{thumbindex}};
      \else
        % Even pages: tab on left edge
        \fill[crimson!80]
          ([yshift=-\thumboffset mm]current page.north west)
          rectangle +(\thumbtabwidth, -\thumbtabheight);
        \node[white,font=\tiny\bfseries,rotate=-90]
          at ([yshift=-\thumboffset mm - 0.5\thumbtabheight,
               xshift=0.5\thumbtabwidth]current page.north west)
          {\Roman{thumbindex}};
      \fi
    \end{tikzpicture}%
  \fi
}

% Hook into fancyhdr to draw thumb on every content page
\AddToHook{shipout/foreground}{%
  \drawthumb%
}

% =============================================================================
% CROP / BLEED MARKS
% =============================================================================
% For final print submission, uncomment the line below to add crop marks.
% MIT Press production will advise on exact requirements.
% \usepackage[cam,center,width=7.5in,height=10.5in]{crop}

% =============================================================================
% PDF/A ARCHIVAL COMPLIANCE
% =============================================================================
% MIT Press increasingly requires PDF/A for long-term preservation.
% This embeds all fonts and removes transparency.
% Note: pdfx must be loaded early; if it conflicts with hyperref,
% MIT Press production can handle the conversion post-build.
% Uncomment when ready for final submission:
% \usepackage[a-3u]{pdfx}

% =============================================================================
% ENHANCED WIDOW / ORPHAN CONTROL
% =============================================================================
% Prevent single lines at top/bottom of pages and breaks before equations
\clubpenalty=10000          % No orphans (single first line at bottom)
\widowpenalty=10000         % No widows (single last line at top)
\displaywidowpenalty=10000  % No widow before display math
\predisplaypenalty=10000    % No page break just before display math
\postdisplaypenalty=0       % Allow break after display math (natural)
\usepackage{needspace}
\let\Needspace\needspace
\makeatletter
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{plain}
\@ifundefined{c@chapter}{\newfloat{vid}{h}{lovid}}{\newfloat{vid}{h}{lovid}[chapter]}
\floatname{vid}{Video}
\newcommand*\listofvids{\listof{vid}{List of Videos}}
\makeatother
\makeatletter
\@ifpackageloaded{tcolorbox}{}{\usepackage[skins,breakable]{tcolorbox}}
\@ifpackageloaded{fontawesome5}{}{\usepackage{fontawesome5}}
\definecolor{quarto-callout-color}{HTML}{909090}
\definecolor{quarto-callout-note-color}{HTML}{0758E5}
\definecolor{quarto-callout-important-color}{HTML}{CC1914}
\definecolor{quarto-callout-warning-color}{HTML}{EB9113}
\definecolor{quarto-callout-tip-color}{HTML}{00A047}
\definecolor{quarto-callout-caution-color}{HTML}{FC5300}
\definecolor{quarto-callout-color-frame}{HTML}{acacac}
\definecolor{quarto-callout-note-color-frame}{HTML}{4582ec}
\definecolor{quarto-callout-important-color-frame}{HTML}{d9534f}
\definecolor{quarto-callout-warning-color-frame}{HTML}{f0ad4e}
\definecolor{quarto-callout-tip-color-frame}{HTML}{02b875}
\definecolor{quarto-callout-caution-color-frame}{HTML}{fd7e14}
\makeatother
\makeatletter
\@ifpackageloaded{bookmark}{}{\usepackage{bookmark}}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\makeatletter
\@ifpackageloaded{sidenotes}{}{\usepackage{sidenotes}}
\@ifpackageloaded{marginnote}{}{\usepackage{marginnote}}
\makeatother
\newcommand{\fbxIconPath}{assets/images/icons/callouts}
\newcommand{\fbxIconFormat}{pdf}
\makeatletter
\@ifpackageloaded{tcolorbox}{}{\usepackage[many]{tcolorbox}}
\makeatother
%%%% ---foldboxy preamble ----- %%%%%

% Load xstring for string manipulation
\RequirePackage{xstring}

% Icon path and format configuration - can be overridden in filter-metadata
\providecommand{\fbxIconPath}{assets/images/icons/callouts}
\providecommand{\fbxIconFormat}{pdf}

% Helper command to include icon with hyphen-to-underscore conversion
% This ensures consistency: callout-quiz-question -> callout_quiz_question
\newcommand{\fbxIncludeIcon}[2]{%
  \StrSubstitute{#1}{-}{_}[\fbxIconName]%
  \includegraphics[width=#2]{\fbxIconPath/icon_\fbxIconName.\fbxIconFormat}%
}

% Legacy fallback colors (keep for compatibility)
\definecolor{fbx-default-color1}{HTML}{c7c7d0}
\definecolor{fbx-default-color2}{HTML}{a3a3aa}
\definecolor{fbox-color1}{HTML}{c7c7d0}
\definecolor{fbox-color2}{HTML}{a3a3aa}

% arguments: #1 typelabelnummer: #2 titel: #3
\newenvironment{fbx}[3]{%
\begin{tcolorbox}[
  enhanced,
  breakable,
  %fontupper=\fontsize{8pt}{10pt}\selectfont,  % 95% of body text (10pt -> 9.5pt)
  before skip=8pt,  % space above box (increased)
  after skip=8pt,   % space below box (increased)
  attach boxed title to top*={xshift=0pt},
  boxed title style={
  %fuzzy shadow={1pt}{-1pt}{0mm}{0.1mm}{gray},
  arc=1.5pt,
  rounded corners=north,
  sharp corners=south,
  top=6pt,          % Adjusted for ~40px equivalent height
  bottom=5pt,       % Adjusted for ~40px equivalent height
  overlay={
      \node [left,outer sep=0em, black,draw=none,anchor=west,
        rectangle,fill=none,inner sep=0pt]
        at ([xshift=4mm]frame.west) {\fbxIncludeIcon{#1}{4.2mm}};
    },
  },
  colframe=#1-color2,             % Border color (auto-generated from YAML)
  colbacktitle=#1-color1,         % Background color (auto-generated from YAML)
  colback=white,
  coltitle=black,
  titlerule=0mm,
  toprule=0.5pt,
  bottomrule=0.5pt,
  leftrule=2.2pt,
  rightrule=0.5pt,
  outer arc=1.5pt,
  arc=1.5pt,
  left=0.5em,       % increased left padding
  bottomtitle=1.5mm, % increased title bottom margin
  toptitle=1.5mm,    % increased title top margin
  title=\hspace{2.5em}\protect#2\hspace{0.5em}\protect#3, % Protect parameters
  extras middle and last={top=4pt} % increased continuation spacing
]}
{\end{tcolorbox}}


% boxed environment with right border
\newenvironment{fbxSimple}[3]{\begin{tcolorbox}[
  enhanced,
  breakable,
  %fontupper=\fontsize{8pt}{10pt}\selectfont,  % 95% of body text (10pt -> 9.5pt)
  before skip=8pt,  % space above box (increased)
  after skip=8pt,   % space below box (increased)
  attach boxed title to top*={xshift=0pt},
  boxed title style={
  %fuzzy shadow={1pt}{-1pt}{0mm}{0.1mm}{gray},
  arc=1.5pt,
  rounded corners=north,
  sharp corners=south,
  top=6pt,          % Adjusted for ~40px equivalent height
  bottom=5pt,       % Adjusted for ~40px equivalent height
  overlay={
      \node [left,outer sep=0em, black,draw=none,anchor=west,
        rectangle,fill=none,inner sep=0pt]
        at ([xshift=3mm]frame.west) {\fbxIncludeIcon{#1}{4.2mm}};
    },
  },
  colframe=#1-color2,             % Border color (auto-generated from YAML)
  colbacktitle=#1-color1,         % Background color (auto-generated from YAML)
  colback=white,
  coltitle=black,
  titlerule=0mm,
  toprule=0.5pt,
  bottomrule=0.5pt,
  leftrule=2.2pt,
  rightrule=0.5pt,
  outer arc=1.5pt,
  arc=1.5pt,
  left=0.5em,       % increased left padding
  bottomtitle=1.5mm, % increased title bottom margin
  toptitle=1.5mm,    % increased title top margin
  title=\hspace{2.5em}\protect#2\hspace{0.5em}\protect#3, % Protect parameters
  boxsep=1pt,
  extras first={bottom=0pt},
  extras last={top=0pt,bottom=-4pt},
  overlay first={
    \draw[line width=1pt,white] ([xshift=2.2pt]frame.south west)-- ([xshift=-0.5pt]frame.south east);
  },
  overlay last={
    \draw[line width=1pt,white] ([xshift=2.2pt]frame.north west)-- ([xshift=-0.5pt]frame.north east);
   }
]}
{\end{tcolorbox}}

%%%% --- end foldboxy preamble ----- %%%%%
%%==== colors from yaml ===%
\definecolor{callout-chapter-connection-color1}{HTML}{FDF2F7}
\definecolor{callout-chapter-connection-color2}{HTML}{A51C30}
\definecolor{callout-code-color1}{HTML}{F2F4F8}
\definecolor{callout-code-color2}{HTML}{D1D7E0}
\definecolor{callout-principle-color1}{HTML}{F3F2FA}
\definecolor{callout-principle-color2}{HTML}{3D3B8E}
\definecolor{callout-colab-color1}{HTML}{FFF5E6}
\definecolor{callout-colab-color2}{HTML}{FF6B35}
\definecolor{callout-resource-slides-color1}{HTML}{E0F2F1}
\definecolor{callout-resource-slides-color2}{HTML}{20B2AA}
\definecolor{callout-definition-color1}{HTML}{F0F4F8}
\definecolor{callout-definition-color2}{HTML}{1B4F72}
\definecolor{callout-resource-videos-color1}{HTML}{E0F2F1}
\definecolor{callout-resource-videos-color2}{HTML}{20B2AA}
\definecolor{callout-theorem-color1}{HTML}{F5F0FF}
\definecolor{callout-theorem-color2}{HTML}{6B46C1}
\definecolor{callout-example-color1}{HTML}{F0F8F6}
\definecolor{callout-example-color2}{HTML}{148F77}
\definecolor{callout-quiz-question-color1}{HTML}{F0F0F8}
\definecolor{callout-quiz-question-color2}{HTML}{5B4B8A}
\definecolor{callout-checkpoint-color1}{HTML}{E8F5E9}
\definecolor{callout-checkpoint-color2}{HTML}{2E7D32}
\definecolor{callout-takeaways-color1}{HTML}{FDF2F7}
\definecolor{callout-takeaways-color2}{HTML}{BE185D}
\definecolor{callout-perspective-color1}{HTML}{F7F8FA}
\definecolor{callout-perspective-color2}{HTML}{4A5568}
\definecolor{callout-quiz-answer-color1}{HTML}{E8F2EA}
\definecolor{callout-quiz-answer-color2}{HTML}{4a7c59}
\definecolor{callout-notebook-color1}{HTML}{F2F7FF}
\definecolor{callout-notebook-color2}{HTML}{2C5282}
\definecolor{callout-resource-exercises-color1}{HTML}{E0F2F1}
\definecolor{callout-resource-exercises-color2}{HTML}{20B2AA}
\definecolor{callout-lighthouse-color1}{HTML}{FDF8E6}
\definecolor{callout-lighthouse-color2}{HTML}{B8860B}
%=============%

\usepackage{hyphenat}
\usepackage{ifthen}
\usepackage{calc}
\usepackage{calculator}



\usepackage{graphicx}
\usepackage{geometry}
\usepackage{afterpage}
\usepackage{tikz}
\usetikzlibrary{calc}
\usetikzlibrary{fadings}
\usepackage[pagecolor=none]{pagecolor}


% Set the titlepage font families







% Set the coverpage font families

\usepackage{bookmark}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  pdftitle={Introduction to Machine Learning Systems},
  pdfauthor={Vijay Janapa Reddi},
  colorlinks=true,
  linkcolor={Maroon},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}


\title{Introduction to Machine Learning Systems}
\author{Vijay Janapa Reddi}
\date{February 1, 2026}
\begin{document}
%%%%% begin titlepage extension code

  \begin{frontmatter}

\begin{titlepage}
% This is a combination of Pandoc templating and LaTeX
% Pandoc templating https://pandoc.org/MANUAL.html#templates
% See the README for help

\thispagestyle{empty}

\newgeometry{top=-100in}

% Page color

\newcommand{\coverauthorstyle}[1]{{\fontsize{20}{24.0}\selectfont
{#1}}}

\begin{tikzpicture}[remember picture, overlay, inner sep=0pt, outer sep=0pt]

\tikzfading[name=fadeout, inner color=transparent!0,outer color=transparent!100]
\tikzfading[name=fadein, inner color=transparent!100,outer color=transparent!0]
\node[anchor=south west, rotate=0, opacity=1] at ($(current page.south west)+(0.225\paperwidth, 9)$) {
\includegraphics[width=\paperwidth, keepaspectratio]{assets/images/covers/cover-image-transparent-vol1.png}};

% Title
\newcommand{\titlelocationleft}{0.075\paperwidth}
\newcommand{\titlelocationbottom}{0.4\paperwidth}
\newcommand{\titlealign}{left}

\begin{scope}{%
\fontsize{52}{62.4}\selectfont
\node[anchor=north
west, align=left, rotate=0] (Title1) at ($(current page.south west)+(\titlelocationleft,\titlelocationbottom)$)  [text width = 0.9\paperwidth]  {{\nohyphens{Machine
Learning Systems}}};
}
\end{scope}

% Author
\newcommand{\authorlocationleft}{.925\paperwidth}
\newcommand{\authorlocationbottom}{0.175\paperwidth}
\newcommand{\authoralign}{right}

\begin{scope}
{%
\fontsize{20}{24.0}\selectfont
\node[anchor=north
east, align=right, rotate=0] (Author1) at ($(current page.south west)+(\authorlocationleft,\authorlocationbottom)$)  [text width = 6in]  {\coverauthorstyle{Vijay
Janapa Reddi\\}};
}
\end{scope}

% Footer
\newcommand{\footerlocationleft}{0.075\paperwidth}
\newcommand{\footerlocationbottom}{0.475\paperwidth}
\newcommand{\footerlocationalign}{left}

\begin{scope}
{%
\fontsize{25}{30.0}\selectfont
 \node[anchor=north west, align=left, rotate=0] (Footer1) at %
($(current page.south west)+(\footerlocationleft,\footerlocationbottom)$)  [text width = 0.9\paperwidth]  {{\nohyphens{Introduction
to}}};
}
\end{scope}

\end{tikzpicture}
\clearpage
\restoregeometry
%%% TITLE PAGE START

% Set up alignment commands
%Page
\newcommand{\titlepagepagealign}{
\ifthenelse{\equal{left}{right}}{\raggedleft}{}
\ifthenelse{\equal{left}{center}}{\centering}{}
\ifthenelse{\equal{left}{left}}{\raggedright}{}
}


\newcommand{\titleandsubtitle}{
% Title and subtitle
{{\huge{\bfseries{\nohyphens{Introduction to Machine Learning
Systems}}}}\par
}%
}
\newcommand{\titlepagetitleblock}{
\titleandsubtitle
}

\newcommand{\authorstyle}[1]{{\large{#1}}}

\newcommand{\affiliationstyle}[1]{{\large{#1}}}

\newcommand{\titlepageauthorblock}{
{\authorstyle{\nohyphens{Vijay Janapa
Reddi}{\textsuperscript{1}}\textsuperscript{,}{\textsuperscript{,*}}}}}

\newcommand{\titlepageaffiliationblock}{
\hangindent=1em
\hangafter=1
{\affiliationstyle{
{1}.~Harvard University


\vspace{1\baselineskip}
* \textit{Correspondence:}~Vijay Janapa Reddi~vj@eecs.harvard.edu
}}
}
\newcommand{\headerstyled}{%
{}
}
\newcommand{\footerstyled}{%
{\large{}}
}
\newcommand{\datestyled}{%
{February 1, 2026}
}


\newcommand{\titlepageheaderblock}{\headerstyled}

\newcommand{\titlepagefooterblock}{
\footerstyled
}

\newcommand{\titlepagedateblock}{
\datestyled
}

%set up blocks so user can specify order
\newcommand{\titleblock}{{

{\titlepagetitleblock}
}

\vspace{4\baselineskip}
}

\newcommand{\authorblock}{{\titlepageauthorblock}

\vspace{2\baselineskip}
}

\newcommand{\affiliationblock}{{\titlepageaffiliationblock}

\vspace{0pt}
}

\newcommand{\logoblock}{}

\newcommand{\footerblock}{}

\newcommand{\dateblock}{{\titlepagedateblock}

\vspace{0pt}
}

\newcommand{\headerblock}{}

\thispagestyle{empty} % no page numbers on titlepages


\newcommand{\vrulecode}{\textcolor{black}{\rule{\vrulewidth}{\textheight}}}
\newlength{\vrulewidth}
\setlength{\vrulewidth}{2pt}
\newlength{\B}
\setlength{\B}{\ifdim\vrulewidth > 0pt 0.05\textwidth\else 0pt\fi}
\newlength{\minipagewidth}
\ifthenelse{\equal{left}{left} \OR \equal{left}{right} }
{% True case
\setlength{\minipagewidth}{\textwidth - \vrulewidth - \B - 0.1\textwidth}
}{
\setlength{\minipagewidth}{\textwidth - 2\vrulewidth - 2\B - 0.1\textwidth}
}
\ifthenelse{\equal{left}{left} \OR \equal{left}{leftright}}
{% True case
\raggedleft % needed for the minipage to work
\vrulecode
\hspace{\B}
}{%
\raggedright % else it is right only and width is not 0
}
% [position of box][box height][inner position]{width}
% [s] means stretch out vertically; assuming there is a vfill
\begin{minipage}[b][\textheight][s]{\minipagewidth}
\titlepagepagealign
\titleblock

Prof.~Vijay Janapa Reddi

School of Engineering and Applied Sciences

Harvard University

\vspace{80mm}

With heartfelt gratitude to the community for their invaluable
contributions and steadfast support.

\vfill

February 1, 2026

\vfill
\par

\end{minipage}\ifthenelse{\equal{left}{right} \OR \equal{left}{leftright} }{
\hspace{\B}
\vrulecode}{}
\clearpage
%%% TITLE PAGE END
\end{titlepage}
\setcounter{page}{1}
\end{frontmatter}

%%%%% end titlepage extension code

% =============================================================================
% HALF-TITLE PAGE (Volume I)
% =============================================================================
% Standard academic book sequence: half-title -> blank -> title page -> copyright
% The half-title shows only the book title -- no author, no publisher, no date.
\thispagestyle{empty}
\begin{center}
\vspace*{0.3\textheight}
{\fontsize{24pt}{28pt}\selectfont\bfseries\color{crimson} Introduction to\\[0.4em] Machine Learning Systems}\\[2em]
{\large\itshape Volume~I}
\vfill
\end{center}
\clearpage
\thispagestyle{empty}\null\clearpage  % Blank verso (back of half-title)

\renewcommand*\contentsname{Table of contents}
{
\hypersetup{linkcolor=}
\setcounter{tocdepth}{2}
\tableofcontents
}
\listoffigures
\listoftables

\mainmatter
\bookmarksetup{startatroot}

\chapter*{Welcome to Volume I}\label{welcome-to-volume-i}
\addcontentsline{toc}{chapter}{Welcome to Volume I}

\markboth{Welcome to Volume I}{Welcome to Volume I}

\bookmarksetup{startatroot}

\chapter{ML Frameworks}\label{sec-ai-frameworks}

\marginnote{\begin{footnotesize}

\emph{DALL·E 3 Prompt: Illustration in a rectangular format, designed
for a professional textbook, where the content spans the entire width.
The vibrant chart represents training and inference frameworks for ML.
Icons for TensorFlow, Keras, PyTorch, ONNX, and TensorRT are spread out,
filling the entire horizontal space, and aligned vertically. Each icon
is accompanied by brief annotations detailing their features. The lively
colors like blues, greens, and oranges highlight the icons and sections
against a soft gradient background. The distinction between training and
inference frameworks is accentuated through color-coded sections, with
clean lines and modern typography maintaining clarity and focus.}

\end{footnotesize}}

\noindent
\pandocbounded{\includegraphics[keepaspectratio]{contents/vol1/frameworks/images/png/cover_ml_frameworks.png}}

\section*{Purpose}\label{purpose}
\addcontentsline{toc}{section}{Purpose}

\markright{Purpose}

\emph{Why does your choice of ML framework constrain your system's
performance, deployment targets, and hardware compatibility far more
than the model architecture itself?}

Architectures define \emph{what} computations neural networks must
perform, but knowing what to compute differs fundamentally from knowing
\emph{how} to compute it efficiently across diverse hardware. Framework
selection determines hardware compatibility, deployment targets, and
available optimization strategies. Migrating between frameworks is
rarely a simple refactor; it invalidates data pipelines, serving
infrastructure, and team expertise, typically requiring three to six
engineer-months for production systems. This lock-in explains why
framework selection carries engineering weight disproportionate to its
apparent simplicity. Organizations that treat frameworks as
interchangeable tools discover \emph{too late} that their inference
pipeline requires a graph representation their eager-mode framework
cannot provide, or that their target hardware lacks support in the
ecosystem they chose.

\begin{tcolorbox}[enhanced jigsaw, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Learning Objectives}, arc=.35mm, opacitybacktitle=0.6, colframe=quarto-callout-tip-color-frame, left=2mm, breakable, bottomtitle=1mm, rightrule=.15mm, toptitle=1mm, opacityback=0, titlerule=0mm, leftrule=.75mm, colback=white, coltitle=black, bottomrule=.15mm, toprule=.15mm, colbacktitle=quarto-callout-tip-color!10!white]

\begin{itemize}
\tightlist
\item
  Explain how ML frameworks solve three core problems: execution
  (computational graphs), differentiation (automatic differentiation),
  and abstraction (hardware-optimized operations)
\item
  Compare static and dynamic computational graph execution models in
  terms of debugging capability, optimization potential, and deployment
  efficiency
\item
  Describe the nn.Module abstraction pattern for hierarchical
  composition, automatic parameter discovery, and mode-dependent
  behavior
\item
  Analyze how memory bandwidth constraints drive framework optimization
  strategies including kernel fusion and operation scheduling
\item
  Evaluate major framework architectures (TensorFlow, PyTorch, JAX)
  based on their design philosophies and performance trade-offs
\item
  Apply systematic framework selection methodology by matching model
  requirements, hardware constraints, and deployment contexts
\end{itemize}

\end{tcolorbox}

\section{The Three Problems Every Framework Must
Solve}\label{sec-ai-frameworks-three-problems-every-framework-must-solve-317d}

Two lines of code: \texttt{model\ =\ Transformer(...)} followed by
\texttt{loss.backward()}. Between them, invisible to the programmer, the
framework orchestrates billions of floating-point operations across
memory hierarchies, computes exact gradients through millions of
parameters using \textbf{automatic differentiation}, schedules thousands
of GPU kernel launches, and manages gigabytes of intermediate state. The
simplicity is an illusion. Those two lines trigger machinery as complex
as a compiler, because that is exactly what a modern ML framework is.

The architectural foundations established in
\textbf{?@sec-dnn-architectures} defined \emph{what} computations neural
networks perform. Knowing \emph{what} to compute, however, is
fundamentally different from knowing \emph{how} to compute it
efficiently. A Transformer's attention mechanism requires coordinating
computation across memory hierarchies and accelerator cores in patterns
that naive implementations would execute 100× slower than optimized
ones. Implementing these operations from scratch for every model would
make deep learning economically infeasible. ML frameworks exist to
bridge this gap by translating high-level model definitions into
hardware-specific execution plans that extract maximum performance from
silicon.

A framework is to machine learning \emph{what} a compiler is to
traditional programming. A C compiler translates human-readable code
into optimized machine instructions, managing register allocation,
instruction scheduling, and memory layout. An ML framework translates
high-level model definitions into hardware-specific execution plans,
managing operator fusion, memory reuse, and device placement. This
analogy is more than metaphor: modern frameworks literally include
compilers, as we will see throughout this chapter.

Every ML framework, regardless of API or design philosophy, must solve
three fundamental problems. First, the \emph{execution problem}: when
and how should computation happen? Should operations execute immediately
as written (\textbf{eager execution}), or should the framework build a
complete description first and optimize before executing (graph
execution)? This choice shapes debugging capability, optimization
potential, and deployment flexibility. Second, the \emph{differentiation
problem}: how should the framework compute gradients automatically? As
established in \textbf{?@sec-deep-learning-systems-foundations},
training requires derivatives of a loss function with respect to
millions or billions of parameters, and manual differentiation is
error-prone at this scale. Frameworks must implement \textbf{automatic
differentiation} systems that compute exact gradients for arbitrary
compositions of operations while managing the memory overhead of storing
intermediate values. Third, the \emph{hardware abstraction problem}: how
should the framework target diverse hardware from a single interface?
The same model definition should run on CPUs, GPUs, TPUs, and mobile
devices, each with different memory constraints and optimal execution
patterns.

These three problems are deeply interconnected. The execution model
determines \emph{when} differentiation occurs and \emph{what}
optimizations are possible. The abstraction layer must support both
execution styles across all hardware targets. Solving any one problem in
isolation leads to frameworks that excel in narrow contexts but fail in
broader deployment. This chapter examines \emph{how} frameworks evolved
to solve these problems and the trade-offs each solution entails,
tracing the history from early libraries through modern frameworks
before examining each problem in depth.

This tension between debuggability and performance shapes \emph{how}
frameworks should be evaluated more broadly. The following perspective
reframes \emph{what} a framework actually does.

\phantomsection\label{callout-perspectiveux2a-1.1}
\begin{fbx}{callout-perspective}{Systems Perspective:}{The ML Compiler}
\phantomsection\label{callout-perspective*-1.1}
In the context of the \textbf{Iron Law}, a framework is fundamentally a
\textbf{Compiler for the Silicon Contract}.

Your ``Source Code'' is the model architecture (the \textbf{\(Ops\)}
term). The framework's job is to take this high-level math and compile
it into a series of hardware-specific kernel launches that:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Minimize \textbf{Data Movement (\(D\))} through techniques like kernel
  fusion.
\item
  Maximize \textbf{Utilization (\(\eta\))} by matching operations to
  specialized hardware units like Tensor Cores.
\item
  Minimize \textbf{Overhead (\(L_{lat}\))} through efficient
  asynchronous dispatch and graph capture.
\end{enumerate}

Choosing a framework means choosing the compiler that determines
\emph{how} efficiently your model utilizes hardware.

\end{fbx}

With these three problems in mind, we can now define \emph{what} a
machine learning framework fundamentally is.

\phantomsection\label{callout-definitionux2a-1.2}
\begin{fbx}{callout-definition}{Definition:}{Machine Learning Frameworks}
\phantomsection\label{callout-definition*-1.2}
\textbf{\emph{Machine Learning Frameworks}} are the \textbf{Compilers}
for the Silicon Contract. They translate high-level mathematical
definitions into hardware-specific execution plans, managing the
\textbf{Abstraction Gap} between algorithmic logic and physical silicon
constraints (memory layout, kernel dispatch, differentiation).

\end{fbx}

The complexity becomes apparent when considering specific implementation
challenges. Implementing backpropagation for a simple 3-layer multilayer
perceptron manually requires hundreds of lines of careful calculus and
matrix manipulation code. A modern framework accomplishes this in a
single line: \texttt{loss.backward()}. But that single line triggers
sophisticated machinery: operation recording, memory allocation for
gradients, reverse-order traversal of the computation graph, and
hardware-optimized kernel dispatch. Training a contemporary language
model further involves orchestrating billions of floating-point
operations across distributed hardware, requiring precise coordination
of memory hierarchies, communication protocols, and numerical precision
management. Implementing these systems from basic computational
primitives would be economically prohibitive for most organizations.

\section{How Frameworks
Evolved}\label{sec-ai-frameworks-frameworks-evolved-ac68}

Understanding \emph{why} modern frameworks are designed as they are
requires tracing their evolution through decades of incremental
abstraction, each generation solving problems that made the previous
generation impractical. This evolution progressed through three distinct
levels of abstraction:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Hardware Primitives (1979--1992)}: The \textbf{Basic Linear
  Algebra Subprograms (BLAS)}\sidenote{\textbf{BLAS (Basic Linear
  Algebra Subprograms)}: A standardized specification (Level 1: vector
  operations; Level 2: matrix-vector; Level 3: matrix-matrix like GEMM)
  published in 1979 that defines portable APIs for dense linear algebra.
  Hardware vendors implement optimized BLAS libraries: Intel MKL
  achieves near-peak FLOPS on x86 CPUs through AVX-512 vectorization,
  while NVIDIA cuBLAS uses Tensor Cores for up to 312 TFLOPS
  (FP16/BF16/TF32) on A100 GPUs, or 624 TFLOPS with structured sparsity.
  This 45-year-old interface remains the performance foundation of
  modern ML frameworks. } established standardized, hardware-optimized
  implementations of operations like matrix multiplication
  (\texttt{GEMM}) (\citeproc{ref-lawson1979blas}{Lawson et al. 1979}).
  \textbf{LAPACK} extended this with higher-level solvers. These
  libraries remain the hidden foundation of every modern ML system;
  vendors like Intel (MKL) and NVIDIA (cuBLAS) provide highly tuned
  versions for their silicon.
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  \textbf{Vectorized Productivity (2006)}:
  \textbf{NumPy}\sidenote{\textbf{NumPy}: Contraction of ``Numerical
  Python,'' created by Travis Oliphant in 2005 by merging two earlier
  projects (Numeric and Numarray). Released publicly in 2006, NumPy
  established the n-dimensional array as Python's standard numerical
  container. Its array-oriented computing model, borrowed from APL and
  MATLAB, remains the conceptual foundation for PyTorch tensors and
  TensorFlow arrays. } made Python viable for numerical computing by
  delegating heavy computation to underlying C and Fortran BLAS
  libraries. This ``vectorization'' approach, writing code in high-level
  Python but executing it in low-level C, became the dominant pattern,
  drastically reducing the gap between research ideas and execution
  speed.
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  \textbf{Automatic Differentiation (2015--present)}: While NumPy
  required engineers to manually derive and code gradients, modern
  frameworks like \textbf{TensorFlow}
  (\citeproc{ref-abadi2016tensorflow}{Abadi et al. 2016}) and
  \textbf{PyTorch} automated this through the \textbf{computational
  graph}. This architectural shift, separating the \emph{definition} of
  the model from the \emph{computation} of its derivatives, enabled the
  scaling of deep learning.
\end{enumerate}

This evolution highlights a critical engineering lesson: scaling ML
development required turning the mathematical chain rule into a software
primitive. The transition from manual gradients to static graphs (Theano
(\citeproc{ref-al2016theano}{Team et al. 2016}), TensorFlow 1.x), and
eventually to dynamic graphs (PyTorch), reflects the industry's search
for the optimal balance between performance and developer velocity.
Figure~\ref{fig-mlfm-timeline} visualizes this progression from
foundational libraries to modern frameworks.

\begin{figure}[htb]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/719d87c10def4cc24114ac401a7396bf2db93359.pdf}}

}

\caption{\label{fig-mlfm-timeline}\textbf{Computational Library
Evolution}: Modern machine learning frameworks build upon decades of
numerical computing advancements, transitioning from low-level routines
like BLAS and LAPACK to high-level abstractions in NumPy,
SciPy,{[}\^{}fn-scipy-date{]} and finally to deep learning frameworks
such as Theano (\citeproc{ref-bergstra2010theano}{Bergstra et al.
2010}), TensorFlow, and PyTorch.}

\end{figure}%

Each generation in this evolution addressed specific limitations of its
predecessor, but all modern frameworks converge on the same three
fundamental problems: \emph{how} to execute computation, \emph{how} to
differentiate it, and \emph{how} to abstract across hardware. The
sections that follow examine each problem in turn.

\section{The Execution
Problem}\label{sec-ai-frameworks-execution-problem-e1e1}

The first fundamental problem every framework must solve is deciding
\emph{when} and \emph{how} computation should happen. This seemingly
simple question, ``Should operations execute immediately as written, or
be recorded for later execution?'', creates a cascade of engineering
trade-offs that shape every aspect of framework behavior.

\subsection{Why Execution Strategy Matters: The Memory
Wall}\label{sec-ai-frameworks-execution-strategy-matters-memory-wall-1ce8}

To understand why execution strategy matters so much, consider the
\textbf{Memory Wall}, the growing gap between processor computational
speed and memory bandwidth. Modern GPUs can perform arithmetic far
faster than they can fetch data. On an A100 GPU, element-wise operations
like ReLU achieve less than 1\% of peak compute capacity, not because
the hardware is slow, but because they spend nearly all their time
waiting for data.

This creates a fundamental classification: operations are either
\textbf{compute-bound} (limited by arithmetic throughput, like large
matrix multiplications) or \textbf{memory-bound} (limited by data
movement, like activation functions and normalization). Most neural
network operations are memory-bound.

The key optimization for memory-bound operations is \textbf{kernel
fusion}, combining multiple operations into a single GPU function
(called a \emph{kernel})\sidenote{\textbf{Kernel}: From German ``Kern''
(core/nucleus), borrowed from operating systems where it denotes the
core program with full hardware access. In GPU programming, a kernel is
the function that executes in parallel across thousands of threads. The
metaphor extends: just as an OS kernel mediates between software and
hardware, a GPU kernel is the fundamental unit where algorithms meet
silicon. } to avoid intermediate memory traffic. Fusing
\texttt{LayerNorm\ →\ Dropout\ →\ ReLU} into one kernel can yield 5×
speedup. FlashAttention\sidenote{FlashAttention (introduced in
\textbf{?@sec-dnn-architectures}) exemplifies kernel fusion taken to its
logical extreme. By fusing the entire attention computation into a
single kernel that tiles data to fit in SRAM, it reduces HBM traffic by
orders of magnitude. The 10-20x speedup demonstrates that frameworks
enabling such fusions can transform memory-bound operations into
compute-bound ones. } fuses the entire attention computation for 10-20×
speedup.

\textbf{The critical constraint}: a framework can only fuse operations
it can see together. If operations execute immediately one at a time
(eager execution), the framework cannot fuse them. If operations are
recorded first into a graph (deferred execution), the framework can
analyze and optimize the entire computation. This is why execution
strategy matters so much: it determines what optimizations are even
possible.

\subsection{The Computational
Graph}\label{sec-ai-frameworks-computational-graph-00f7}

At the heart of this problem is a representation called the
\textbf{computational graph}, a directed acyclic graph (DAG) where nodes
represent operations and edges represent data dependencies. This graph
is the framework's internal model of your computation.
Figure~\ref{fig-comp-graph} illustrates a simple example: computing
\(z = x \times y\) requires two input nodes (\(x\) and \(y\)), one
operation node (multiplication), and one output node (\(z\)). The
execution problem asks: \emph{when} is this graph constructed, and
\emph{when} is it executed?

\begin{figure}[htb]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/10115043f9f4d00600404b3424214d2cae8c8a59.pdf}}

}

\caption{\label{fig-comp-graph}\textbf{Simple Computational Graph.} A
directed acyclic graph representing the computation \(z = x \times y\),
where nodes define operations and edges specify the flow of data between
them.}

\end{figure}%

Real machine learning models require much more complex graph structures.
Figure~\ref{fig-mlfm-comp-graph} shows \emph{how} a neural network
computation graph involves interconnected operation nodes with
system-level interactions including memory management and device
placement, demonstrating \emph{how} the graph representation enables
pre-execution analysis and resource allocation.

\begin{figure}[htb]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/af180559b4fa75e5c7cbcd6162bd5fe421af8e6e.pdf}}

}

\caption{\label{fig-mlfm-comp-graph}\textbf{Computation Graph with
System Interactions.} A neural network represented as a directed acyclic
graph (left), with system components including memory management and
device placement (right) that interact with the graph to optimize
resource allocation before execution.}

\end{figure}%

This graph representation is more than a visualization; it is the data
structure that enables both efficient execution and automatic
differentiation. The answer to \emph{when} this graph is constructed has
profound implications:

\begin{itemize}
\tightlist
\item
  \textbf{For debugging}: Can you print intermediate values? Step
  through code with a debugger?
\item
  \textbf{For optimization}: Can the framework see multiple operations
  at once to fuse them?
\item
  \textbf{For deployment}: Can the model run without a Python
  interpreter?
\item
  \textbf{For flexibility}: Can control flow depend on computed tensor
  values?
\end{itemize}

No single execution model optimizes all these dimensions. Frameworks
must choose their position in this trade-off space, and practitioners
must understand these trade-offs to select appropriate tools and write
efficient code. The following sections examine \emph{how} different
execution strategies navigate these constraints.

\subsection{Three Execution
Strategies}\label{sec-ai-frameworks-three-execution-strategies-5934}

Consider writing \texttt{y\ =\ x\ *\ 2} in code. Two fundamentally
different approaches exist:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Immediate execution}: Perform the multiplication right now,
  storing the result in \texttt{y}. Natural and debuggable, but the
  framework sees only one operation at a time.
\item
  \textbf{Deferred execution}: Record the intention to multiply,
  building a graph of operations. Execute later when explicitly
  requested. Less intuitive, but the framework sees the complete
  computation, enabling optimization.
\end{enumerate}

Neither approach dominates; each embodies different trade-offs between
\textbf{flexibility} and \textbf{optimization potential}. Modern
frameworks have explored three primary execution strategies, which we
examine through their systems implications.

\subsubsection{Eager Execution with Dynamic
Graphs}\label{sec-ai-frameworks-eager-execution-dynamic-graphs-29c2}

\phantomsection\label{callout-exampleux2a-1.3}
\begin{fbx}{callout-example}{Example:}{Eager vs. Graph Execution Code Comparison}
\phantomsection\label{callout-example*-1.3}

\textbf{PyTorch (Eager Execution)}:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ torch}

\NormalTok{x }\OperatorTok{=}\NormalTok{ torch.tensor([}\FloatTok{1.0}\NormalTok{, }\FloatTok{2.0}\NormalTok{])}
\NormalTok{y }\OperatorTok{=}\NormalTok{ x }\OperatorTok{*} \DecValTok{2}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Intermediate value: }\SpecialCharTok{\{}\NormalTok{y}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)  }\CommentTok{\# Works immediately}
\NormalTok{z }\OperatorTok{=}\NormalTok{ y.}\BuiltInTok{sum}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  \textbf{Pros}: Intuitive debugging, dynamic control flow.
\item
  \textbf{Cons}: No global optimization, Python overhead per op.
\end{itemize}

\textbf{TensorFlow 1.x (Static Graph)}:

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ tensorflow }\ImportTok{as}\NormalTok{ tf}

\NormalTok{x }\OperatorTok{=}\NormalTok{ tf.placeholder(tf.float32)}
\NormalTok{y }\OperatorTok{=}\NormalTok{ x }\OperatorTok{*} \DecValTok{2}
\CommentTok{\# print(y) {-}\textgreater{} Prints Tensor("mul:0"...), not value!}
\NormalTok{z }\OperatorTok{=}\NormalTok{ tf.reduce\_sum(y)}

\ControlFlowTok{with}\NormalTok{ tf.Session() }\ImportTok{as}\NormalTok{ sess:}
\NormalTok{    result }\OperatorTok{=}\NormalTok{ sess.run(z, feed\_dict}\OperatorTok{=}\NormalTok{\{x: [}\FloatTok{1.0}\NormalTok{, }\FloatTok{2.0}\NormalTok{]\})}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  \textbf{Pros}: Whole-graph optimization (fusion), portability.
\item
  \textbf{Cons}: ``Define-then-run'' friction, hard to debug.
\end{itemize}

\end{fbx}

\textbf{The Approach}: Execute operations immediately as encountered,
building the computation graph dynamically during execution. When you
write \texttt{y\ =\ x\ *\ 2}, the multiplication happens instantly and
the result is available for immediate use.

This provides the flexibility of normal programming: you can print
intermediate values, use conditionals based on computed results, and
debug with standard tools. The framework records operations as they
happen, constructing a \textbf{dynamic graph} that reflects the actual
execution path taken.

For gradient computation, the framework records a history of operations
in what's called an \textbf{autograd tape}\sidenote{\textbf{Autograd
Tape}: A dynamically constructed data structure recording operations
during forward pass execution. Each operation adds a node to the tape
containing: (1) the operation type, (2) references to input tensors, (3)
saved intermediate values needed for gradient computation, and (4) the
backward function implementing chain rule application. The tape is
destroyed after backward pass to free memory. }, a transient data
structure built during execution. Each tensor operation creates a node
that records: the operation performed, references to input tensors, and
how to compute gradients. These nodes form a directed acyclic graph
(DAG) of operations built \textbf{during} forward pass execution, not
before.

Consider this example using PyTorch, which implements eager execution as
its default mode. Listing~\ref{lst-autograd-tape-example} shows
\emph{how} operations are recorded as they execute.

\begin{codelisting}

\caption{\label{lst-autograd-tape-example}\textbf{Autograd Tape
Construction}: Each operation executes immediately while recording a
backward node to the autograd tape for later gradient computation.}

\centering{

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ torch}

\NormalTok{x }\OperatorTok{=}\NormalTok{ torch.tensor([}\FloatTok{1.0}\NormalTok{], requires\_grad}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\NormalTok{y }\OperatorTok{=}\NormalTok{ x }\OperatorTok{*} \DecValTok{2}  \CommentTok{\# Executes immediately; records MulBackward node}
\NormalTok{z }\OperatorTok{=}\NormalTok{ y }\OperatorTok{+} \DecValTok{1}  \CommentTok{\# Executes immediately; records AddBackward node}
\CommentTok{\# The autograd tape exists NOW, built during execution}
\end{Highlighting}
\end{Shaded}

}

\end{codelisting}%

After these two operations, the framework has constructed an autograd
tape with two nodes: one for the multiplication and one for the
addition. The tape records that \texttt{z} depends on \texttt{y}, and
\texttt{y} depends on \texttt{x}.

Calling \texttt{z.backward()} traverses this tape in reverse topological
order, applying the chain rule at each node:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Compute \(\frac{\partial z}{\partial z} = 1\) (seed gradient)
\item
  Call \texttt{AddBackward0.backward()}
  \(\rightarrow \frac{\partial z}{\partial y} = 1\)
\item
  Call \texttt{MulBackward0.backward()}
  \(\rightarrow \frac{\partial z}{\partial x} = 2\)
\item
  Accumulate gradient in \texttt{x.grad}
\end{enumerate}

After \texttt{backward()} completes, the autograd tape is
\textbf{destroyed} to free memory. The next forward pass builds a
completely new tape. This design enables memory-efficient training: you
only pay for gradient computation storage during the backward pass.

Figure~\ref{fig-mlfm-dynamic-graph-flow} illustrates this
``define-by-run'' execution model: each operation is defined, executed,
and completed before moving on to define the next operation. This
contrasts sharply with static graphs, where all operations must be
defined upfront. When an operation is defined, it is immediately
executed, and its results become available for subsequent operations or
for inspection during debugging.

\begin{figure}[htb]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/a7957a0e515b670e9a04be6e7c7c9d3f7d41c192.pdf}}

}

\caption{\label{fig-mlfm-dynamic-graph-flow}\textbf{Dynamic Graph
Execution Flow}: In eager execution, each operation is defined and
immediately executed before the next operation begins. This
define-by-run model enables natural debugging and data-dependent control
flow at the cost of optimization opportunities.}

\end{figure}%

\textbf{Systems Implications: Flexibility.} The dynamic autograd tape
enables capabilities impossible with static graphs:

\begin{itemize}
\tightlist
\item
  \textbf{Data-dependent control flow}: Conditionals and loops can
  depend on tensor values computed during execution. For example, you
  can implement beam search, dynamic RNN lengths, or adaptive
  computation based on intermediate results.
\item
  \textbf{Variable-length sequences}: Different iterations can process
  tensors of different sizes without redefining the computation. This
  enables natural language processing where sentence lengths vary.
\item
  \textbf{Debugging}: You can print tensors, inspect values, and use
  standard Python debuggers (\texttt{pdb}, breakpoints) because
  operations execute immediately in standard Python execution.
\end{itemize}

\textbf{Systems Implications: Overhead.} The flexibility of eager
execution with autograd tape comes with performance costs. Each overhead
maps to a specific Iron Law term, revealing why eager execution creates
a performance ceiling for small models:

\begin{itemize}
\tightlist
\item
  \textbf{Graph construction overhead} (\(L_{\text{fixed}}\)): Each
  forward pass rebuilds the autograd tape from scratch, adding Python
  object creation, reference counting, and node linking cost to every
  iteration.
\item
  \textbf{Python interpreter overhead} (\(L_{\text{fixed}}\)): Every
  operation goes through Python dispatch, including function lookup,
  argument parsing, and type checking. At \textasciitilde1μs per
  operation, this becomes significant for models with thousands of
  operations.
\item
  \textbf{Limited optimization opportunities} (\(Ops\), \(D\)): Because
  the graph is built during execution, the framework cannot optimize
  across operations. Each operation launches its own GPU kernel,
  preventing kernel fusion that would reduce both operation count and
  memory traffic.
\item
  \textbf{Memory overhead} (\(D\)): The autograd tape stores references
  to all intermediate tensors and Function nodes, increasing memory
  consumption by 2--3\(\times\) compared to forward-only execution and
  adding pressure to the memory hierarchy.
\end{itemize}

For a typical ResNet-50 forward pass, eager execution overhead adds
approximately 5-10ms compared to an optimized compiled version, with the
majority spent in Python dispatch and tape construction rather than
actual computation.

\subsubsection{Static Computation
Graphs}\label{sec-ai-frameworks-static-computation-graphs-e100}

\textbf{The Approach}: Define the complete computational graph as a
symbolic representation first, then execute it separately. This
``define-then-run'' execution model means the graph exists
\textbf{before} any computation occurs, enabling aggressive
ahead-of-time optimization.

The key insight is that if the framework sees the entire computation
before running it, the framework can analyze, transform, and optimize
the graph globally. This visibility is impossible when operations
execute immediately one at a time.

\textbf{Two-Phase Execution.} Static graphs implement a clear separation
between graph construction and execution.
Listing~\ref{lst-tf-static-graph} illustrates the two phases using
TensorFlow 1.x, which pioneered this approach: symbolic definition
creates placeholders and operations without computation, while explicit
execution triggers actual arithmetic:

\begin{codelisting}

\caption{\label{lst-tf-static-graph}\textbf{Static Graph Two-Phase
Execution}: Graph construction (symbolic definition) is separated from
execution (actual computation), enabling ahead-of-time optimization.}

\centering{

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Phase 1: Graph Construction (symbolic, no computation)}
\ImportTok{import}\NormalTok{ tensorflow.compat.v1 }\ImportTok{as}\NormalTok{ tf}

\NormalTok{tf.disable\_v2\_behavior()}

\CommentTok{\# Define graph symbolically}
\NormalTok{x }\OperatorTok{=}\NormalTok{ tf.placeholder(tf.float32, shape}\OperatorTok{=}\NormalTok{[}\DecValTok{1}\NormalTok{])  }\CommentTok{\# Just a placeholder}
\NormalTok{y }\OperatorTok{=}\NormalTok{ x }\OperatorTok{*} \DecValTok{2}  \CommentTok{\# Not executed, just recorded}
\NormalTok{z }\OperatorTok{=}\NormalTok{ y }\OperatorTok{+} \DecValTok{1}  \CommentTok{\# Still no execution}
\CommentTok{\# At this point, nothing has been computed}

\CommentTok{\# Phase 2: Graph Execution (actual computation)}
\ControlFlowTok{with}\NormalTok{ tf.Session() }\ImportTok{as}\NormalTok{ sess:}
\NormalTok{    result }\OperatorTok{=}\NormalTok{ sess.run(z, feed\_dict}\OperatorTok{=}\NormalTok{\{x: [}\FloatTok{1.0}\NormalTok{]\})}
    \CommentTok{\# Now computation happens: result = [3.0]}
\end{Highlighting}
\end{Shaded}

}

\end{codelisting}%

Figure~\ref{fig-mlfm-static-graph} illustrates this two-phase approach:
first, the complete computational graph is constructed and optimized;
then, during the execution phase, actual data flows through the graph to
produce results. This separation enables the framework to perform
thorough analysis and optimization of the entire computation before any
execution begins.

\begin{figure}[htb]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/c7375276bfdc1705b32c5d8a42cae57d8cfd8e63.pdf}}

}

\caption{\label{fig-mlfm-static-graph}\textbf{Static Graph: Define then
Execute.} The two phases of static graph execution. The definition phase
(left) declares operations and builds the graph. The execution phase
(right) loads data, runs the optimized graph, and produces results.}

\end{figure}%

During construction, \texttt{x}, \texttt{y}, and \texttt{z} are not
tensors containing values but rather symbolic nodes in a graph.
Operations like \texttt{*} and \texttt{+} add nodes to the graph
definition without performing any arithmetic. Execution is triggered
explicitly, at which point the framework analyzes the complete graph,
optimizes it, and then executes the optimized version with the provided
input data.

\textbf{Ahead-of-Time Optimization.} Because the framework has the
complete graph before execution, it can perform optimizations impossible
in eager mode:

\begin{itemize}
\tightlist
\item
  \textbf{Operation fusion}: Combine \texttt{y\ =\ x\ *\ 2} and
  \texttt{z\ =\ y\ +\ 1} into a single fused kernel
  \texttt{z\ =\ x\ *\ 2\ +\ 1}, eliminating the intermediate \texttt{y}
  and reducing memory traffic by 50\%.
\item
  \textbf{Memory pre-allocation}: Calculate exact memory requirements
  for all tensors before execution, allocating memory in a single pass
  and reusing buffers where possible.
\item
  \textbf{Data layout optimization}: Transform tensor layouts (e.g.,
  NCHW to NHWC) to match hardware preferences without copying.
\item
  \textbf{Dead code elimination}: Remove operations not needed to
  compute the requested outputs.
\item
  \textbf{Constant folding}: Pre-compute operations on constant values
  at graph construction time.
\end{itemize}

These optimizations map directly to Iron Law terms: operation fusion
reduces \(D\) by eliminating intermediate memory writes, constant
folding reduces \(Ops\) by computing values once, memory pre-allocation
reduces \(L_{\text{fixed}}\) by avoiding runtime allocation overhead,
and dead code elimination reduces both \(Ops\) and \(D\).

Compilation frameworks like XLA\sidenote{\textbf{XLA (Accelerated Linear
Algebra)}: Google's domain-specific compiler released in March 2017,
optimizing tensor operations across CPUs, GPUs, and TPUs. The name
emphasizes that linear algebra operations (matrix multiplies,
convolutions) dominate ML computation. Achieves 3-10x speedups through
operation fusion and hardware-specific codegen. Now part of OpenXLA
(2022), a cross-industry effort including Google, Meta, NVIDIA, and
Apple. } (Accelerated Linear Algebra) (\citeproc{ref-GoogleXLA}{Google
2025}) take this further, compiling the TensorFlow graph to optimized
machine code for specific hardware. For a transformer encoder block, XLA
can achieve 1.5--2\(\times\) speedup over unoptimized execution through
aggressive fusion and hardware-specific code generation.

\textbf{Systems Implications.} Static graphs achieve high performance
through ahead-of-time optimization. Kernel fusion reduces memory
bandwidth requirements (often the bottleneck for ML workloads), and
hardware-specific compilation enables near-peak utilization.

The cost of this performance is reduced flexibility. Static graphs
require fixed control flow: you cannot have conditionals or loops that
depend on computed values. While TensorFlow provides \texttt{tf.cond}
and \texttt{tf.while\_loop}, these require static unrolling or special
handling. Debugging is difficult because stack traces point to graph
construction code, not execution code. Error messages often reference
symbolic node names rather than the actual operations that failed.

\subsubsection{Hybrid Approaches: JIT
Compilation}\label{sec-ai-frameworks-hybrid-approaches-jit-compilation-8954}

The fundamental trade-off in JIT compilation is \emph{fidelity versus
generality}. Tracing captures the exact execution path taken during a
sample run, producing high fidelity to that specific input but low
generality because it misses branches not taken. Source-level
compilation (scripting) analyzes the full program structure, achieving
higher generality by preserving all control flow branches, but requiring
a restricted language subset that excludes much of Python's dynamic
behavior. Both approaches produce an intermediate representation (IR)
that enables the same ahead-of-time optimizations available to static
graphs: operator fusion, constant folding, dead code elimination, and
buffer reuse.

This trade-off has a direct Iron Law consequence. JIT compilation
amortizes the \(L_{\text{fixed}}\) (dispatch overhead) across the
compiled region. Longer compiled regions mean more overhead amortized
per operation, which explains why graph breaks are performance-critical:
each break forces a return to eager dispatch, resetting the
amortization.

PyTorch's TorchScript exemplifies both strategies. Tracing executes a
function once with example inputs and records every tensor operation
into a static computation graph. Listing~\ref{lst-torchscript-trace}
demonstrates the approach: the traced module becomes a compiled artifact
that can be serialized, optimized, and executed independently of the
Python interpreter:

\begin{codelisting}

\caption{\label{lst-torchscript-trace}\textbf{TorchScript Tracing}:
Captures tensor operations by executing a function with example inputs
and recording the execution path into a static computation graph.}

\centering{

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ torch}


\KeywordTok{def}\NormalTok{ forward(x):}
\NormalTok{    y }\OperatorTok{=}\NormalTok{ x }\OperatorTok{*} \DecValTok{2}
\NormalTok{    z }\OperatorTok{=}\NormalTok{ y }\OperatorTok{+} \DecValTok{1}
    \ControlFlowTok{return}\NormalTok{ z}


\CommentTok{\# Trace the function by running it once}
\NormalTok{x\_example }\OperatorTok{=}\NormalTok{ torch.tensor([}\FloatTok{1.0}\NormalTok{])}
\NormalTok{traced }\OperatorTok{=}\NormalTok{ torch.jit.trace(forward, x\_example)}

\CommentTok{\# traced is now a compiled TorchScript module}
\CommentTok{\# Can serialize: torch.jit.save(traced, "model.pt")}
\CommentTok{\# Can optimize: fusion, constant folding}
\CommentTok{\# Can run without Python interpreter}
\end{Highlighting}
\end{Shaded}

}

\end{codelisting}%

The critical limitation of tracing reveals the fidelity-generality
trade-off concretely. Because tracing records a single execution path,
it cannot handle data-dependent control flow.
Listing~\ref{lst-tracing-silent-failure} illustrates a silent
correctness failure.

\begin{codelisting}

\caption{\label{lst-tracing-silent-failure}\textbf{Tracing Silent
Failure}: Tracing records only the execution path taken by the example
input, silently ignoring all other branches of data-dependent control
flow.}

\centering{

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ conditional\_forward(x):}
    \ControlFlowTok{if}\NormalTok{ x.}\BuiltInTok{sum}\NormalTok{() }\OperatorTok{\textgreater{}} \DecValTok{0}\NormalTok{:  }\CommentTok{\# Data{-}dependent condition}
        \ControlFlowTok{return}\NormalTok{ x }\OperatorTok{*} \DecValTok{2}
    \ControlFlowTok{else}\NormalTok{:}
        \ControlFlowTok{return}\NormalTok{ x }\OperatorTok{*} \DecValTok{3}


\NormalTok{traced }\OperatorTok{=}\NormalTok{ torch.jit.trace(conditional\_forward, torch.tensor([}\FloatTok{1.0}\NormalTok{]))}
\CommentTok{\# Tracing captures ONLY the x.sum() \textgreater{} 0 branch}
\CommentTok{\# If input later has sum \textless{}= 0, traced version}
\CommentTok{\# still executes x * 2 branch}
\end{Highlighting}
\end{Shaded}

}

\end{codelisting}%

Tracing records whichever branch executed during the example input.
Subsequent executions always follow the traced path regardless of input
values, silently producing incorrect results for inputs that would have
taken the other branch. This failure mode is particularly dangerous
because it produces no error, only wrong outputs.

The alternative, scripting, achieves generality by analyzing Python
source code directly and compiling it to TorchScript IR without
executing. The scripting compiler parses the abstract syntax tree (AST),
converts supported operations to IR operations, and preserves the
branching structure so that both branches of a conditional exist in the
compiled representation. The cost of this generality is a restricted
Python subset: type annotations are required where inference fails,
arbitrary Python objects and standard library modules are excluded, and
dynamic metaprogramming is forbidden.

Tracing suits feed-forward models without conditionals (ResNet, VGG,
Vision Transformer) and models where control flow depends only on
hyperparameters fixed at trace time. Scripting suits models with
data-dependent control flow (RNN variants, recursive networks, adaptive
computation) and deployment to environments without a Python
interpreter. The following examples demonstrate scripting syntax
(Listing~\ref{lst-torchscript-script}), control flow preservation
(Listing~\ref{lst-torchscript-conditional}), language restrictions
(Listing~\ref{lst-torchscript-restrictions}), and IR inspection
(Listing~\ref{lst-torchscript-ir}).

\begin{codelisting}

\caption{\label{lst-torchscript-script}\textbf{TorchScript Scripting}:
Compiles Python source code directly to TorchScript IR by parsing the
AST, preserving control flow structure without requiring example
inputs.}

\centering{

\begin{Shaded}
\begin{Highlighting}[]
\AttributeTok{@torch.jit.script}
\KeywordTok{def}\NormalTok{ forward(x):}
\NormalTok{    y }\OperatorTok{=}\NormalTok{ x }\OperatorTok{*} \DecValTok{2}
\NormalTok{    z }\OperatorTok{=}\NormalTok{ y }\OperatorTok{+} \DecValTok{1}
    \ControlFlowTok{return}\NormalTok{ z}


\CommentTok{\# Compiles Python source code to TorchScript IR}
\CommentTok{\# No example inputs needed}
\CommentTok{\# Preserves control flow structure}
\end{Highlighting}
\end{Shaded}

}

\end{codelisting}%

\begin{codelisting}

\caption{\label{lst-torchscript-conditional}\textbf{Scripted Control
Flow}: Unlike tracing, scripting preserves both branches of conditionals
in the IR, enabling correct execution based on runtime input values.}

\centering{

\begin{Shaded}
\begin{Highlighting}[]
\AttributeTok{@torch.jit.script}
\KeywordTok{def}\NormalTok{ conditional\_forward(x: torch.Tensor) }\OperatorTok{{-}\textgreater{}}\NormalTok{ torch.Tensor:}
    \ControlFlowTok{if}\NormalTok{ x.}\BuiltInTok{sum}\NormalTok{() }\OperatorTok{\textgreater{}} \DecValTok{0}\NormalTok{:}
        \ControlFlowTok{return}\NormalTok{ x }\OperatorTok{*} \DecValTok{2}
    \ControlFlowTok{else}\NormalTok{:}
        \ControlFlowTok{return}\NormalTok{ x }\OperatorTok{*} \DecValTok{3}


\CommentTok{\# Both branches preserved in IR}
\CommentTok{\# Correct branch executes based on runtime input values}
\end{Highlighting}
\end{Shaded}

}

\end{codelisting}%

Scripting requires a restricted Python subset because TorchScript cannot
support all Python language features:

\begin{itemize}
\tightlist
\item
  \textbf{Type annotations required}: TorchScript needs explicit types
  for function signatures and variables when type inference fails
\item
  \textbf{No arbitrary Python objects}: Only tensor operations, numeric
  types, lists, dicts, tuples, and TorchScript classes
\item
  \textbf{Limited standard library}: Cannot use most Python standard
  library modules (no \texttt{os}, \texttt{sys}, arbitrary imports)
\item
  \textbf{Restricted dynamic behavior}: Cannot dynamically modify class
  structure or use Python metaprogramming
\end{itemize}

\begin{codelisting}

\caption{\label{lst-torchscript-restrictions}\textbf{TorchScript
Restrictions}: Scripting requires a restricted Python subset. Common
unsupported features include arbitrary imports, NumPy operations, and
f-strings.}

\centering{

\begin{Shaded}
\begin{Highlighting}[]
\AttributeTok{@torch.jit.script}
\KeywordTok{def}\NormalTok{ invalid\_script(x):}
    \ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np  }\CommentTok{\# ERROR: Cannot import arbitrary modules}

\NormalTok{    result }\OperatorTok{=}\NormalTok{ np.array([}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{])  }\CommentTok{\# ERROR: NumPy not supported}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Debug: }\SpecialCharTok{\{}\NormalTok{x}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)  }\CommentTok{\# ERROR: f{-}strings not supported}
    \ControlFlowTok{return}\NormalTok{ result}


\CommentTok{\# Valid alternative:}
\AttributeTok{@torch.jit.script}
\KeywordTok{def}\NormalTok{ valid\_script(x: torch.Tensor) }\OperatorTok{{-}\textgreater{}}\NormalTok{ torch.Tensor:}
    \CommentTok{\# Use TorchScript{-}compatible operations}
\NormalTok{    result }\OperatorTok{=}\NormalTok{ torch.tensor([}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{], dtype}\OperatorTok{=}\NormalTok{x.dtype, device}\OperatorTok{=}\NormalTok{x.device)}
    \ControlFlowTok{return}\NormalTok{ result}
\end{Highlighting}
\end{Shaded}

}

\end{codelisting}%

\begin{codelisting}

\caption{\label{lst-torchscript-ir}\textbf{TorchScript IR Inspection}:
The generated intermediate representation shows primitive operations and
constants, useful for debugging and understanding compilation results.}

\centering{

\begin{Shaded}
\begin{Highlighting}[]
\AttributeTok{@torch.jit.script}
\KeywordTok{def}\NormalTok{ example(x: torch.Tensor) }\OperatorTok{{-}\textgreater{}}\NormalTok{ torch.Tensor:}
    \ControlFlowTok{return}\NormalTok{ x }\OperatorTok{*} \DecValTok{2} \OperatorTok{+} \DecValTok{1}


\CommentTok{\# Inspect generated IR:}
\BuiltInTok{print}\NormalTok{(example.graph)}
\CommentTok{\# graph(\%x : Tensor):}
\CommentTok{\#   \%1 : int = prim::Constant[value=2]()}
\CommentTok{\#   \%2 : Tensor = aten::mul(\%x, \%1)}
\CommentTok{\#   \%3 : int = prim::Constant[value=1]()}
\CommentTok{\#   \%4 : Tensor = aten::add(\%2, \%3, \%3)}
\CommentTok{\#   return (\%4)}
\end{Highlighting}
\end{Shaded}

}

\end{codelisting}%

The TorchScript IR represents operations using the \texttt{aten}
namespace for core tensor operations, the \texttt{prim} namespace for
primitives and control flow, static types for every value, and Single
Static Assignment (SSA) form. This IR enables optimizations independent
of Python: operator fusion combines adjacent operations into single
kernels, constant folding evaluates constant expressions at compile
time, dead code elimination removes unused operations, and memory
optimization reuses buffers when possible.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3333}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Aspect}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Tracing}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Scripting}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Input requirement} & Example inputs needed & No inputs needed \\
\textbf{Control flow} & Cannot handle data-dependent & Supports
data-dependent \\
\textbf{Conversion ease} & Simpler (just run function) & Harder
(restricted Python) \\
\textbf{Type annotations} & Not required & Required when inference
fails \\
\textbf{Error detection} & Runtime (wrong results) & Compile time
(syntax errors) \\
\textbf{Best for} & Feed-forward models & Models with conditionals \\
\end{longtable}

\subsubsection{Modern Compilation:
torch.compile}\label{sec-ai-frameworks-modern-compilation-torchcompile-d025}

\textbf{The Problem}: The previous approaches force a choice: write
flexible code (eager execution) or fast code (static graphs). Modern JIT
compilation attempts to eliminate this trade-off by automatically
compiling eager code into optimized graphs with minimal developer
intervention.

PyTorch 2.0's \texttt{torch.compile}
(\citeproc{ref-ansel2024pytorch2}{Ansel et al. 2024}) represents this
approach: developers write natural Python code that executes eagerly
during development, but the framework automatically captures and
compiles hot paths into optimized kernels for production.
Listing~\ref{lst-torch-compile-intro} shows the basic usage pattern:

\begin{codelisting}

\caption{\label{lst-torch-compile-intro}\textbf{torch.compile}: PyTorch
2.0's compiler captures execution on first call, compiles an optimized
kernel, then reuses compiled code for subsequent calls with matching
shapes.}

\centering{

\begin{Shaded}
\begin{Highlighting}[]
\AttributeTok{@torch.compile}
\KeywordTok{def}\NormalTok{ forward(x):}
    \ControlFlowTok{return}\NormalTok{ x }\OperatorTok{*} \DecValTok{2} \OperatorTok{+} \DecValTok{1}


\CommentTok{\# First call: captures execution, compiles optimized kernel (\textasciitilde{}100ms)}
\NormalTok{result1 }\OperatorTok{=}\NormalTok{ forward(torch.tensor([}\FloatTok{1.0}\NormalTok{]))}

\CommentTok{\# Reuse compiled code}
\NormalTok{model(torch.randn(}\DecValTok{10}\NormalTok{, }\DecValTok{10}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

}

\end{codelisting}%

The compilation overhead in these examples (milliseconds to compile,
microseconds to reuse) illustrates a broader principle. Software
dispatch costs that seem negligible for a single operation compound
dramatically across the thousands of operations in a forward pass. The
following analysis quantifies \emph{the physics of software overhead}.

\phantomsection\label{callout-notebookux2a-1.4}
\begin{fbx}{callout-notebook}{AI Engineer’s Notebook:}{The Physics of Software Overhead}
\phantomsection\label{callout-notebook*-1.4}
\textbf{The Iron Law Connection:} The \textbf{Latency Term}
(\(\text{Latency}_{\text{fixed}}\)) in the Iron Law is dominated by
software overhead: dispatching instructions from Python to the GPU.

\textbf{The Constants of Latency:}

\begin{itemize}
\tightlist
\item
  \textbf{Python Dispatch:} \textasciitilde10 \(\mu\)s per operation.
\item
  \textbf{Kernel Launch:} \textasciitilde5 \(\mu\)s per operation.
\item
  \textbf{Memory Access (VRAM):} \textasciitilde1 \(\mu\)s.
\end{itemize}

\textbf{Scenario 1: Eager Mode (The ``Tiny Op'' Trap)} Consider a simple
activation block: \texttt{y\ =\ relu(x\ +\ bias)}.

\begin{itemize}
\tightlist
\item
  \textbf{Operations:} 2 (Add, ReLU).
\item
  \textbf{Execution:}

  \begin{enumerate}
  \def\labelenumi{\arabic{enumi}.}
  \tightlist
  \item
    Launch \texttt{Add} Kernel: 15 \(\mu\)s overhead.
  \item
    Read/Write Memory: \(2N\) bytes.
  \item
    Launch \texttt{ReLU} Kernel: 15 \(\mu\)s overhead.
  \item
    Read/Write Memory: \(2N\) bytes.
  \end{enumerate}
\item
  \textbf{Total Overhead:} 30 \(\mu\)s.
\item
  \textbf{Total Memory Traffic:} \(4N\) bytes.
\end{itemize}

\textbf{Scenario 2: Compiled Mode (Fusion)} The compiler fuses this into
one kernel: \texttt{FusedAddRelu}.

\begin{itemize}
\tightlist
\item
  \textbf{Execution:}

  \begin{enumerate}
  \def\labelenumi{\arabic{enumi}.}
  \tightlist
  \item
    Launch \texttt{Fused} Kernel: 15 \(\mu\)s overhead.
  \item
    Read/Write Memory: \(2N\) bytes (intermediate result stays in
    registers).
  \end{enumerate}
\item
  \textbf{Total Overhead:} 15 \(\mu\)s (\textbf{2x speedup}).
\item
  \textbf{Total Memory Traffic:} \(2N\) bytes (\textbf{2x bandwidth
  efficiency}).
\end{itemize}

\textbf{The Conclusion:} Compilation is not magic; it is
\textbf{overhead amortization}. For small, element-wise operations (like
LayerNorm, GELU, Add), overhead often exceeds compute time by 10-100x.
Fusing them is the only way to utilize the hardware.

\end{fbx}

Figure~\ref{fig-python-tax} demonstrates this tax in action. Eager
execution (top) suffers from ``gaps'' where the GPU sits idle while
Python dispatches the next kernel. Compilation (bottom) fuses these
operations into a single kernel launch, eliminating the gaps.

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{contents/vol1/frameworks/frameworks_files/figure-pdf/fig-python-tax-output-1.pdf}}

}

\caption{\label{fig-python-tax}\textbf{The Python Tax}: Execution
timeline for a sequence of small operations (e.g., LayerNorm). In Eager
Mode (top), the GPU (blue) finishes processing each op in microseconds
but must sit idle while the Python interpreter (red) dispatches the next
kernel launch. Compilation (bottom) fuses these operations into a single
kernel, effectively hiding the dispatch latency and maximizing GPU
utilization.}

\end{figure}%

To achieve this fusion automatically without requiring users to write
custom CUDA kernels, PyTorch 2.0 introduces a compiler stack designed to
capture and optimize graphs dynamically.

\textbf{Architecture: Three-Stage Compilation Pipeline.} torch.compile
consists of three coordinated components:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{TorchDynamo} (graph capture): Intercepts Python bytecode
  execution using CPython's PEP 523 frame evaluation API. Unlike
  tracing, which requires executing code with sample inputs and records
  only one execution path, TorchDynamo hooks into the interpreter's
  frame evaluation mechanism, observing each bytecode instruction as it
  executes.

  This bytecode-level capture enables TorchDynamo to record operations
  without manual tracing. When it encounters unsupported operations
  (print statements, arbitrary Python code), it creates graph breaks:
  the current graph is finalized for compilation, unsupported code
  executes eagerly, and a new graph begins after.
\item
  \textbf{FX Graph} (intermediate representation): Operations captured
  by TorchDynamo are converted to FX graph format, PyTorch's node-based
  directed acyclic graph where each node represents an operation (matrix
  multiplication, ReLU activation, addition) with explicit inputs and
  outputs.

  The FX graph serves as PyTorch's analog to LLVM IR: a standardized
  representation that separates frontend (Python code capture) from
  backend (hardware-specific code generation). This design allows
  different backends (TorchInductor, ONNX Runtime, TensorRT) to consume
  FX graphs and generate optimized code for their target platforms. The
  graph structure enables optimization passes such as dead code
  elimination, constant folding, operation reordering, and pattern
  matching for fusion opportunities.
\item
  \textbf{TorchInductor} (code generation): The default backend that
  compiles FX graphs to optimized machine code. For CUDA GPUs,
  TorchInductor generates Triton\sidenote{\textbf{Triton}: Named after
  the Greek god of the sea (son of Poseidon), evoking mastery over waves
  of parallel threads. OpenAI released this GPU programming language in
  2021 to enable writing custom kernels in Python-like syntax without
  low-level CUDA knowledge. Triton compiles to PTX (NVIDIA's
  intermediate assembly), handling memory coalescing and thread
  synchronization automatically. Achieves 80-95\% of hand-tuned CUDA
  performance while reducing development time from weeks to hours. }
  kernels, a Python-based GPU kernel language that compiles to PTX. For
  CPUs, it generates C++ code with vectorization instructions (AVX2,
  AVX-512).

  TorchInductor applies three key optimizations. Kernel fusion combines
  multiple operations into single kernels to reduce memory traffic; for
  example, \texttt{(x\ *\ 2).relu()} becomes one fused kernel instead of
  two separate kernels. Memory layout optimization chooses optimal
  tensor layouts to minimize memory access overhead. Autotuning measures
  actual performance across multiple implementation strategies and
  selects the fastest variant.
\end{enumerate}

The generated code is cached on disk (in
\texttt{\textasciitilde{}/.triton/cache/} for Triton kernels).
Subsequent runs with the same input shapes can skip compilation and
directly execute cached code.

\textbf{Execution Flow.} The first execution follows a multi-step
process: TorchDynamo intercepts bytecode and records operations into FX
graph, FX graph is passed to TorchInductor for compilation (5-30 seconds
for transformer models), and compiled code is cached and executed.
Subsequent executions with the same input shapes dispatch directly to
compiled code with microseconds overhead. If input shapes change,
TorchInductor must recompile for the new shapes (shape specialization).
PyTorch maintains separate compiled versions for each unique shape
configuration.

\textbf{Graph Breaks: Causes and Detection.} Graph breaks occur when
torch.compile encounters code it cannot compile, forcing execution to
fall back to eager mode. Understanding graph break causes provides the
foundation for achieving good performance.

Data-dependent control flow requires tensor values unavailable at
compile time, as shown in Listing~\ref{lst-graph-break-control-flow}.

\begin{codelisting}

\caption{\label{lst-graph-break-control-flow}\textbf{Graph Break from
Control Flow}: Data-dependent conditionals force a graph break because
tensor values are unavailable at compile time, splitting execution into
separate compiled regions.}

\centering{

\begin{Shaded}
\begin{Highlighting}[]
\AttributeTok{@torch.compile}
\KeywordTok{def}\NormalTok{ conditional\_compute(x):}
    \ControlFlowTok{if}\NormalTok{ x.}\BuiltInTok{sum}\NormalTok{() }\OperatorTok{\textgreater{}} \DecValTok{0}\NormalTok{:  }\CommentTok{\# Graph break: tensor value needed}
        \ControlFlowTok{return}\NormalTok{ x }\OperatorTok{*} \DecValTok{2}
    \ControlFlowTok{else}\NormalTok{:}
        \ControlFlowTok{return}\NormalTok{ x }\OperatorTok{*} \DecValTok{3}


\CommentTok{\# Creates two compiled regions: operations before}
\CommentTok{\# and after the if statement}
\CommentTok{\# The if statement itself executes eagerly}
\end{Highlighting}
\end{Shaded}

}

\end{codelisting}%

TorchDynamo creates a graph break: operations before the if statement
are compiled, the if statement executes eagerly (evaluating which branch
to take), and the chosen branch is compiled as a separate region.

Unsupported operations also cause graph breaks, as
Listing~\ref{lst-graph-break-io} demonstrates.

\begin{codelisting}

\caption{\label{lst-graph-break-io}\textbf{Graph Break from I/O}:
Unsupported operations like \texttt{print} force a graph break,
splitting compiled code into two regions with eager execution in
between.}

\centering{

\begin{Shaded}
\begin{Highlighting}[]
\AttributeTok{@torch.compile}
\KeywordTok{def}\NormalTok{ debug\_compute(x):}
\NormalTok{    y }\OperatorTok{=}\NormalTok{ x }\OperatorTok{*} \DecValTok{2}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"y = }\SpecialCharTok{\{}\NormalTok{y}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)  }\CommentTok{\# Graph break: I/O operation}
\NormalTok{    z }\OperatorTok{=}\NormalTok{ y }\OperatorTok{+} \DecValTok{1}
    \ControlFlowTok{return}\NormalTok{ z}


\CommentTok{\# Creates two compiled regions: before and after print}
\end{Highlighting}
\end{Shaded}

}

\end{codelisting}%

Common unsupported operations include I/O (\texttt{print}, file
operations), custom Python objects, and calls to non-PyTorch libraries.
Each graph break incurs overhead: tensors must be marshalled from
compiled code back to Python (possibly copying from GPU to CPU), the
eager operation executes, and results are marshalled into the next
compiled region.

Shape changes prevent compiled code reuse, as
Listing~\ref{lst-graph-break-shapes} illustrates.

\begin{codelisting}

\caption{\label{lst-graph-break-shapes}\textbf{Recompilation from Shape
Changes}: Each unique input shape triggers a separate compilation,
causing significant overhead when shapes vary frequently.}

\centering{

\begin{Shaded}
\begin{Highlighting}[]
\AttributeTok{@torch.compile}
\KeywordTok{def}\NormalTok{ variable\_length(x, length):}
    \ControlFlowTok{return}\NormalTok{ x[:, :length]  }\CommentTok{\# Shape changes each call}


\CommentTok{\# Each unique length triggers recompilation}
\ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\DecValTok{10}\NormalTok{):}
\NormalTok{    result }\OperatorTok{=}\NormalTok{ variable\_length(x, i)  }\CommentTok{\# 10 recompilations}
\end{Highlighting}
\end{Shaded}

}

\end{codelisting}%

Detect graph breaks using Listing~\ref{lst-graph-break-detect}.

\begin{codelisting}

\caption{\label{lst-graph-break-detect}\textbf{Detecting Graph Breaks}:
Setting \texttt{TORCH\_LOGS} to \texttt{graph\_breaks} prints each break
location and reason during execution.}

\centering{

\begin{Shaded}
\begin{Highlighting}[]
\VariableTok{TORCH\_LOGS}\OperatorTok{=}\StringTok{"graph\_breaks"} \ExtensionTok{python}\NormalTok{ train.py}
\end{Highlighting}
\end{Shaded}

}

\end{codelisting}%

This prints each break location and reason:
\texttt{Graph\ break\ in\ user\ code\ at\ file.py:15\ /\ Reason:\ call\ to\ unsupported\ function\ print}.
Minimizing graph breaks is key to performance: move unsupported
operations outside compiled regions, replace data-dependent control flow
with conditional execution (\texttt{torch.where}), or accept eager
execution for inherently dynamic sections.

\textbf{Compilation Modes.} torch.compile supports three modes balancing
compilation time against runtime performance:

\begin{itemize}
\tightlist
\item
  \textbf{mode=`default'}: Moderate optimization with fast compilation
  (5-30 seconds for transformer models). Suitable for development and
  training where compilation overhead is amortized over many iterations.
\item
  \textbf{mode=`reduce-overhead'}: Minimizes Python interpreter overhead
  by aggressively capturing operations and enabling CUDA graphs (batch
  kernel launches to reduce \textasciitilde5-10 microseconds launch
  overhead per kernel). Optimized for inference with fixed shapes.
  Improves throughput by 20--40\% over default mode for inference
  servers.
\item
  \textbf{mode=`max-autotune'}: Extensive autotuning generates and
  benchmarks multiple implementation variants for operations.
  Compilation time increases (minutes to hours for large models) but
  runtime performance improves by 10-30\% over default mode. Best for
  production training where compilation is performed once and amortized
  over days of training.
\end{itemize}

\textbf{Backend Options.} While TorchInductor is the default backend,
torch.compile supports multiple backends:

\begin{itemize}
\tightlist
\item
  \textbf{backend=`inductor'} (default): Generates Triton kernels for
  CUDA and C++ for CPU. Best general-purpose performance for both
  training and inference.
\item
  \textbf{backend=`onnxrt'}: Exports FX graph to ONNX format and
  executes using ONNX Runtime. Enables cross-platform deployment (CPU,
  GPU, mobile, edge devices) but may cause more graph breaks due to
  limited ONNX operation coverage.
\item
  \textbf{backend=`tensorrt'}: Compiles to NVIDIA TensorRT inference
  engine with aggressive optimizations (int8 quantization, layer fusion,
  kernel autotuning). Inference-only (no backward pass), NVIDIA GPUs
  only, often achieves 2--5\(\times\) speedup over TorchInductor for
  inference.
\end{itemize}

\textbf{Practical Example: Measuring Speedup.}
Listing~\ref{lst-torch-compile-benchmark} implements correct GPU
benchmarking methodology, incorporating CUDA synchronization, warmup
iterations to exclude compilation time, and sufficient iterations to
amortize measurement overhead:

\begin{codelisting}

\caption{\label{lst-torch-compile-benchmark}\textbf{Benchmarking
torch.compile}: Properly measuring speedup requires CUDA
synchronization, warmup to exclude compilation time, and sufficient
iterations to amortize measurement overhead.}

\centering{

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ torch}
\ImportTok{import}\NormalTok{ time}


\KeywordTok{def}\NormalTok{ forward(x, w):}
    \ControlFlowTok{return}\NormalTok{ torch.matmul(x, w).relu()}


\NormalTok{x }\OperatorTok{=}\NormalTok{ torch.randn(}\DecValTok{1024}\NormalTok{, }\DecValTok{1024}\NormalTok{, device}\OperatorTok{=}\StringTok{"cuda"}\NormalTok{)}
\NormalTok{w }\OperatorTok{=}\NormalTok{ torch.randn(}\DecValTok{1024}\NormalTok{, }\DecValTok{512}\NormalTok{, device}\OperatorTok{=}\StringTok{"cuda"}\NormalTok{)}

\CommentTok{\# Eager mode benchmark}
\NormalTok{torch.cuda.synchronize()  }\CommentTok{\# Ensure GPU operations complete}
\NormalTok{start }\OperatorTok{=}\NormalTok{ time.time()}
\ControlFlowTok{for}\NormalTok{ \_ }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\DecValTok{100}\NormalTok{):}
\NormalTok{    y }\OperatorTok{=}\NormalTok{ forward(x, w)}
\NormalTok{    torch.cuda.synchronize()  }\CommentTok{\# Wait for GPU kernel completion}
\NormalTok{eager\_time }\OperatorTok{=}\NormalTok{ time.time() }\OperatorTok{{-}}\NormalTok{ start}

\CommentTok{\# Compiled mode benchmark}
\NormalTok{forward\_compiled }\OperatorTok{=}\NormalTok{ torch.}\BuiltInTok{compile}\NormalTok{(forward)}
\NormalTok{forward\_compiled(x, w)  }\CommentTok{\# Warmup: trigger compilation}
\NormalTok{torch.cuda.synchronize()}

\NormalTok{start }\OperatorTok{=}\NormalTok{ time.time()}
\ControlFlowTok{for}\NormalTok{ \_ }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\DecValTok{100}\NormalTok{):}
\NormalTok{    y }\OperatorTok{=}\NormalTok{ forward\_compiled(x, w)}
\NormalTok{    torch.cuda.synchronize()}
\NormalTok{compiled\_time }\OperatorTok{=}\NormalTok{ time.time() }\OperatorTok{{-}}\NormalTok{ start}

\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Speedup: }\SpecialCharTok{\{}\NormalTok{eager\_time}\OperatorTok{/}\NormalTok{compiled\_time}\SpecialCharTok{:.2f\}}\SpecialStringTok{×"}\NormalTok{)}
\CommentTok{\# Typical: 2{-}{-}5$\textbackslash{}times$ for matrix operations}
\end{Highlighting}
\end{Shaded}

}

\end{codelisting}%

Critical benchmarking details: (1) Use \texttt{torch.cuda.synchronize()}
because CUDA operations are asynchronous; without synchronization,
timing measures only kernel launch time, not execution time. (2) Warmup
compilation by calling once before timing to exclude compilation from
measurements. (3) Run 100+ iterations to amortize measurement overhead.

\textbf{Systems Implications.} First execution includes compilation
time: 5--10 s for small models, 30--60 s for BERT-base transformers,
5--10 min for GPT-3 scale models. This overhead is amortized across
training (compile once, train for thousands of iterations) but impacts
development iteration time. Compiled kernels are cached on disk;
subsequent runs skip compilation.

Compilation adds overhead: 100--500 MB for FX graph construction, 500
MB--2 GB peak during Triton compilation, 10--100 MB per compiled graph
for storage. Runtime memory usage is similar to eager mode (kernel
fusion can reduce intermediate tensors but compiled code may allocate
temporary buffers). Compiled models typically use 90--110\% of eager
mode memory.

Errors in compiled code produce stack traces pointing to generated code,
not source Python code. Print statements inside compiled regions cause
graph breaks (executed eagerly, not compiled). For debugging, remove
\texttt{@torch.compile} to revert to eager execution, fix bugs, then
re-enable compilation. Use \texttt{TORCH\_COMPILE\_DEBUG=1} for verbose
compilation logs.

\textbf{When to Use torch.compile.} Use torch.compile for:

\begin{itemize}
\tightlist
\item
  \textbf{Training}: Long training runs (hundreds of iterations)
  amortize compilation overhead. Stable model architectures with fixed
  control flow minimize graph breaks.
\item
  \textbf{Inference}: Deployed models compile once at startup and serve
  thousands of requests. Use
  \texttt{mode=\textquotesingle{}reduce-overhead\textquotesingle{}} to
  minimize per-request overhead.
\end{itemize}

Avoid torch.compile for:

\begin{itemize}
\tightlist
\item
  \textbf{Rapid prototyping}: Compilation overhead slows iteration time.
  Defer until model architecture stabilizes.
\item
  \textbf{Highly dynamic models}: Frequent graph breaks or shape changes
  prevent effective compilation.
\item
  \textbf{Debugging}: Compiled code obscures error locations. Use eager
  mode to identify bugs.
\end{itemize}

\textbf{Comparison of Execution Models.}
Table~\ref{tbl-framework-execution-models} contrasts the three execution
models across six dimensions, revealing that hybrid JIT compilation
achieves most of static graph performance while preserving much of eager
execution's flexibility:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2500}}@{}}
\caption{\textbf{Execution Model Trade-Offs.} Comparison of static
graph, eager execution, and hybrid JIT compilation across six dimensions
including performance, debugging, and deployment
flexibility.}\label{tbl-framework-execution-models}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Aspect}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Eager + Autograd Tape} \textbf{(PyTorch default)}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Static Graph} \textbf{(TensorFlow 1.x)}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{JIT Compilation} \textbf{(torch.compile)}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Aspect}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Eager + Autograd Tape} \textbf{(PyTorch default)}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Static Graph} \textbf{(TensorFlow 1.x)}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{JIT Compilation} \textbf{(torch.compile)}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Execution Model} & Immediate & Deferred & Hybrid \\
\textbf{Graph Construction} & During forward pass & Before execution &
First execution (cached) \\
\textbf{Optimization} & None (per-operation) & Ahead-of-time & JIT
compilation \\
\textbf{Dynamic Control Flow} & Full support & Limited (static unroll) &
Partial (graph breaks) \\
\textbf{Debugging} & Easy (standard Python) & Difficult (symbolic) &
Moderate (mixed) \\
\textbf{Performance} & Baseline & High (optimized) & High (compiled
regions) \\
\end{longtable}

The trade-offs between static and dynamic graphs extend beyond the
dimensions shown above. Table~\ref{tbl-mlfm-graphs} provides deeper
analysis of how these architectures influence optimization potential,
debugging workflows, scalability, and deployment complexity:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3333}}@{}}
\caption{\textbf{Graph Computation Modes.} Static graphs define the
entire computation upfront for optimization, while dynamic graphs
construct the computation on the fly for flexibility with
variable-length inputs and control flow. The choice affects both
execution efficiency and the ease of model development and
debugging.}\label{tbl-mlfm-graphs}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Aspect}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Static Graphs}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Dynamic Graphs}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Aspect}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Static Graphs}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Dynamic Graphs}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Memory Management} & Precise allocation planning, optimized
memory usage & Flexible but potentially less efficient \\
\textbf{Optimization Potential} & Comprehensive graph-level
optimizations possible & Limited to local optimizations due to
runtime \\
\textbf{Hardware Utilization} & Can generate highly optimized
hardware-specific code & May sacrifice hardware-specific
optimizations \\
\textbf{Development Experience} & Requires more upfront planning, harder
to debug & Better debugging, faster iteration cycles \\
\textbf{Debugging Workflow} & Framework-specific tools, disconnected
stack traces & Standard Python debugging (pdb, print, inspect) \\
\textbf{Error Reporting} & Execution-time errors disconnected from
definition & Intuitive stack traces pointing to exact lines \\
\textbf{Research Velocity} & Slower iteration due to define-then-run
requirement & Faster prototyping and model experimentation \\
\textbf{Runtime Flexibility} & Fixed computation structure & Can adapt
to runtime conditions \\
\textbf{Production Performance} & Generally better performance at scale
& May have overhead from graph construction \\
\textbf{Integration with Legacy Code} & More separation between
definition and execution & Natural integration with imperative code \\
\end{longtable}

These trade-offs are not binary choices. Modern frameworks offer a
spectrum of options, which raises the quantitative question: where on
this spectrum should your project operate?

\subsection{Quantitative Principles of
Execution}\label{sec-ai-frameworks-compilation-continuum-principle-c106}

\textbf{The Compilation Continuum Principle.} The Execution Problem
demands a quantitative principle: \textbf{when should you compile?}

The execution models form a continuum from maximum flexibility to
maximum optimization:

\[
\text{Eager} \xrightarrow{\text{tracing}} \text{JIT} \xrightarrow{\text{AOT}} \text{Static Graph} \xrightarrow{\text{synthesis}} \text{Custom Hardware}
\]

Each step rightward sacrifices flexibility for performance. The
fundamental question is: \emph{where} on this continuum should your
project operate? The optimal compilation strategy depends on the ratio
of \textbf{development iterations} to \textbf{production executions}
(Equation~\ref{eq-compilation-benefit}):

\begin{equation}\phantomsection\label{eq-compilation-benefit}{
\text{Compilation Benefit} = \frac{N_{\text{prod}} \cdot (T_{\text{eager}} - T_{\text{compiled}})}{T_{\text{compile}} + N_{\text{dev}} \cdot T_{\text{compile}}}
}\end{equation}

Where:

\begin{itemize}
\tightlist
\item
  \(N_{\text{prod}}\) = number of production executions (inference
  requests, training steps)
\item
  \(N_{\text{dev}}\) = number of development iterations requiring
  recompilation
\item
  \(T_{\text{eager}}\) = time per execution in eager mode
\item
  \(T_{\text{compiled}}\) = time per execution in compiled mode
\item
  \(T_{\text{compile}}\) = one-time compilation cost
\end{itemize}

\textbf{Decision Rule}: Compile when \(\text{Compilation Benefit} > 1\).

Table~\ref{tbl-training-benchmark} provides representative throughput
data across execution modes and model architectures:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2000}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2000}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2000}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2000}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2000}}@{}}
\caption{\textbf{Training and Inference Throughput.} Representative
throughput comparison across execution modes for common model
architectures on NVIDIA A100 GPU with batch size 32. torch.compile
typically provides 1.4 to 1.5\(\times\) speedup over eager mode, while
TensorRT provides 2 to 3\(\times\) speedup but requires longer
compilation and is inference only. Compile times vary based on model
complexity and optimization
level.}\label{tbl-training-benchmark}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Model}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Eager} \textbf{(img/sec)}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{torch.compile} \textbf{(img/sec)}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{TensorRT} \textbf{(img/sec)}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Compile Time} \textbf{(seconds)}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Model}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Eager} \textbf{(img/sec)}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{torch.compile} \textbf{(img/sec)}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{TensorRT} \textbf{(img/sec)}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Compile Time} \textbf{(seconds)}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{ResNet-50} & 1,450 & 2,150 & 3,800 & 15-30 \\
\textbf{BERT-Base} & 380 & 520 & 890 & 30-60 \\
\textbf{ViT-B/16} & 620 & 950 & 1,650 & 25-45 \\
\textbf{GPT-2 (124M)} & 180 & 260 & 420 & 45-90 \\
\end{longtable}

These throughput differences across execution modes raise a practical
question: which \emph{framework execution strategy} best serves each
workload archetype?

\phantomsection\label{callout-lighthouseux2a-1.5}
\begin{fbx}{callout-lighthouse}{Lighthouse:}{Framework Strategy by Archetype}
\phantomsection\label{callout-lighthouse*-1.5}
The optimal framework execution strategy depends on which Iron Law term
dominates your workload. Table~\ref{tbl-framework-archetype-strategy}
aligns each archetype to its recommended execution strategy:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2500}}@{}}
\caption{\textbf{Framework Execution Strategy by Workload.} Recommended
execution strategy for each workload archetype, aligned to the dominant
Iron Law term. Compute-bound workloads benefit most from compilation,
while irregular access patterns favor eager
execution.}\label{tbl-framework-archetype-strategy}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Archetype}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Dominant Iron Law Term}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Optimal Framework Strategy}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Rationale}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Archetype}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Dominant Iron Law Term}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Optimal Framework Strategy}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Rationale}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{ResNet-50} & \(\frac{O}{R_{peak} \cdot \eta}\) (Compute) &
\textbf{TensorRT} (inference) & Kernel fusion maximizes MFU;
compute-bound \\
\textbf{(Compute Beast)} & & \textbf{torch.compile} (training) &
workloads benefit most from optimization \\
\textbf{GPT-2} & \(\frac{D_{vol}}{BW}\) (Memory Bandwidth) &
\textbf{torch.compile} & Kernel fusion reduces HBM round-trips; \\
\textbf{(Bandwidth Hog)} & & & keeps data in cache to mitigate
bandwidth \\
\textbf{DLRM} & \(\frac{D_{vol}}{BW}\) (Random Access) + &
\textbf{Eager} with specialized kernels & Embedding lookups are
inherently irregular \\
\textbf{(Sparse Scatter)} & \(T_{network}\) & (FBGEMM) & and dynamic;
compilation gains are small \\
\textbf{DS-CNN} & \(L_{lat}\) (Overhead) & \textbf{AOT compilation}
(TFLite, ONNX) & Sub-ms inference; every microsecond of \\
\textbf{(Tiny Constraint)} & & & Python overhead is unacceptable \\
\end{longtable}

\textbf{Key insight}: Compilation benefits scale with how much of your
workload is \emph{optimizable}. Compute Beasts
(Table~\ref{tbl-training-benchmark}: ResNet-50 sees 2.6× speedup from
TensorRT) benefit most. Sparse Scatter workloads gain little because
their bottleneck (embedding lookups) is inherently irregular.

\end{fbx}

This principle has concrete implications:

\textbf{Research prototyping} (\(N_{\text{dev}} \gg N_{\text{prod}}\)):
Stay eager. If you change model architecture every few minutes,
compilation overhead dominates. A 30-second compile time with 10
iterations/hour means 5 minutes lost to compilation per hour, often more
than the runtime savings.

\textbf{Training runs} (\(N_{\text{prod}} \gg N_{\text{dev}}\)):
Compile. A typical training run executes millions of forward/backward
passes. Even 60 seconds of compilation amortizes to microseconds per
step. From Table~\ref{tbl-training-benchmark}, torch.compile provides
\textasciitilde48\% speedup on ResNet-50 (2,150 vs 1,450 img/sec); this
pays off after:

\[
N_{\text{breakeven}} = \frac{T_{\text{compile}}}{T_{\text{eager}} - T_{\text{compiled}}} = \frac{30\text{s}}{(1/1450 - 1/2150)\text{s/img}} \approx 140{,}000 \text{ images}
\]

For ImageNet (1.28M training images), compilation pays off within the
first epoch.

\textbf{Production inference} (\(N_{\text{dev}} \approx 0\),
\(N_{\text{prod}} \rightarrow \infty\)): Maximize compilation. With no
development iterations and potentially millions of requests, every
optimization matters. Use
\texttt{mode=\textquotesingle{}max-autotune\textquotesingle{}} despite
hour-long compilation; the cost is amortized over the deployment
lifetime.

Figure~\ref{fig-compilation-continuum} visualizes the decision space:

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/07d9ffe12dcd2da25ea86acd4d29a51f6fae18a0.pdf}}

}

\caption{\label{fig-compilation-continuum}\textbf{The Compilation
Continuum}: Optimal execution strategy depends on
development-to-production ratio. Left region (high dev iterations):
eager mode dominates. Right region (high prod executions): compilation
dominates. The crossover point depends on compilation cost and
per-execution speedup.}

\end{figure}%

\textbf{The Dispatch Overhead Law.} A second principle emerges from the
Dispatch Overhead Equation (Equation~\ref{eq-dispatch-overhead}): when
does framework overhead, rather than compute or memory, dominate
execution time? Framework overhead dominates when operations are small
relative to dispatch cost:

\begin{equation}\phantomsection\label{eq-dispatch-overhead}{
\text{Overhead Ratio} = \frac{N_{\text{ops}} \cdot t_{\text{dispatch}}}{T_{\text{compute}} + T_{\text{memory}}}
}\end{equation}

When Overhead Ratio \(> 1\), the model is \textbf{overhead-bound}.
Compilation provides maximum benefit for overhead-bound workloads
because it eliminates per-operation dispatch.

From the case study in
Section~\ref{sec-ai-frameworks-putting-together-anatomy-training-step-c7f1},
we can quantify this effect.

This cumulative latency creates what is effectively \emph{a dispatch
tax} on execution.

\phantomsection\label{callout-notebookux2a-1.6}
\begin{fbx}{callout-notebook}{AI Engineer’s Notebook:}{The Dispatch Tax}
\phantomsection\label{callout-notebook*-1.6}

\textbf{Problem}: When does Python overhead kill performance?

\textbf{Scenario 1: Small MLP (Overhead Bound)}

\begin{itemize}
\tightlist
\item
  \textbf{Compute}: 6 small matrix/element-wise operations.
\item
  \textbf{Hardware Time}: \(T_{hw} \approx 2.6 \mu s\) (mostly memory
  latency).
\item
  \textbf{Software Overhead}:
  \(T_{sw} \approx 6 \text{ ops} \times 5 \mu s/\text{op} = 30 \mu s\).
\item
  \textbf{Ratio}: \(\frac{30}{2.6} \approx \mathbf{11\.5}\).
\item
  \textbf{Conclusion}: You spend 92\% of time waiting for Python.
  Compilation yields \textbf{13x speedup}.
\end{itemize}

\textbf{Scenario 2: GPT-3 Layer (Compute Bound)}

\begin{itemize}
\tightlist
\item
  \textbf{Compute}: Huge matrix multiplications.
\item
  \textbf{Hardware Time}:
  \(T_{hw} \approx 100 \text{ ms} = 100,000 \mu s\).
\item
  \textbf{Software Overhead}: \(T_{sw} \approx 50 \mu s\).
\item
  \textbf{Ratio}: \(\frac{50}{100,000} \approx \mathbf{0\.0005}\).
\item
  \textbf{Conclusion}: Python overhead is negligible. Compilation helps
  only via kernel fusion (memory bandwidth), not dispatch elimination.
\end{itemize}

\end{fbx}

\textbf{The Principle's Implication}: Small models benefit
\emph{disproportionately} from compilation. A 100-parameter toy model
might see 10× speedup from torch.compile, while a 175B-parameter model
sees only 1.3×. This explains why compilation matters most for efficient
inference on smaller, deployed models.

The execution problem determines \emph{when} computation happens. But
neural network training requires a capability that no amount of clever
scheduling can provide: the ability to compute gradients automatically.
A framework that executes efficiently but cannot differentiate is
useless for training.

\subsection{Frameworks for the Edge: TinyML and
Micro-Runtimes}\label{sec-ai-frameworks-tinyml-micro-runtimes-2a1b}

The compilation continuum reaches its extreme at the far edge. While
cloud frameworks like PyTorch and TensorFlow 2.x prioritize flexibility
through eager execution, \textbf{TinyML} systems operating on
microcontrollers (MCUs) with kilobytes of memory cannot afford the
overhead of a Python interpreter or a dynamic runtime.

\phantomsection\label{callout-lighthouseux2a-1.7}
\begin{fbx}{callout-lighthouse}{Lighthouse:}{Lighthouse Example: TinyML Frameworks (KWS)}
\phantomsection\label{callout-lighthouse*-1.7}
\textbf{The Scenario}: Deploying our \textbf{Keyword Spotting (KWS)}
model to an ARM Cortex-M4 microcontroller with 256 KB of RAM and 1 MB of
Flash.

\textbf{The Constraint}: A standard PyTorch runtime occupies
\textasciitilde500 MB. The Python interpreter itself occupies
\textasciitilde20 MB. Both are orders of magnitude larger than the
entire device.

\textbf{The Framework Solution}: Micro-frameworks like
\textbf{TensorFlow Lite Micro (TFLM)} and \textbf{PyTorch ExecuTorch}
solve this through \textbf{Extreme AOT Compilation}:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Static Memory Planning}: The framework calculates the exact
  memory address for every tensor \emph{at compile time}. There is no
  dynamic \texttt{malloc()} or garbage collection.
\item
  \textbf{Kernel Specialization}: Only the specific kernels used by the
  model (e.g., Conv2D, DepthwiseConv) are compiled into the binary.
  Unused code is stripped away.
\item
  \textbf{No-Interpreter Execution}: The model is converted into a flat
  sequence of function calls or a simple ``Command Buffer'' that the MCU
  executes directly in C/C++.
\end{enumerate}

\textbf{The Silicon Contract}: On TinyML devices, the contract is
strictly \textbf{Memory-Bound}. The framework's primary job is to ensure
the model's intermediate activations (the ``working set'') fit within
the MCU's tiny SRAM.

\end{fbx}

These micro-runtimes represent the ``Pure AOT'' endpoint of the
continuum. By sacrificing all dynamic flexibility, they enable machine
learning to run on devices consuming milliwatts of power, fulfilling the
\textbf{Energy-Movement Invariant} by keeping all data movement local to
the chip.

This spectrum of execution strategies---from dynamic eager execution to
static graph compilation and specialized micro-runtimes---requires
developers to make deliberate trade-offs.

\phantomsection\label{callout-checkpointux2a-1.8}
\begin{fbx}{callout-checkpoint}{Checkpoint:}{Execution Models}
\phantomsection\label{callout-checkpoint*-1.8}

The choice of execution mode determines both developer velocity and
model performance.

\textbf{Debuggability vs.~Speed}

\begin{itemize}
\tightlist
\item[$\square$]
  \textbf{Eager Mode (Python-First)}: Why does executing ops one-by-one
  make debugging easy but optimization hard? (Hint: The compiler cannot
  see the ``future'' ops to fuse them).
\item[$\square$]
  \textbf{Graph Mode (Compiler-First)}: Why does building a static graph
  enable \textbf{Kernel Fusion}? (Merging Conv+ReLU saves memory
  bandwidth).
\end{itemize}

\textbf{The Modern Compromise}

\begin{itemize}
\tightlist
\item[$\square$]
  \textbf{JIT Compilation}: How does \texttt{torch.compile} bridge the
  gap? (It captures the graph \emph{just in time} to optimize, while
  falling back to Python for dynamic parts).
\end{itemize}

\end{fbx}

\section{The Differentiation
Problem}\label{sec-ai-frameworks-differentiation-problem-8b8a}

The second fundamental problem is computing
gradients\sidenote{\textbf{Gradient}: From Latin ``gradiens''
(stepping/walking), related to ``gradus'' (step). The gradient points in
the direction of steepest ascent, as if climbing steps up a hill. The
term was introduced by Sylvester in 1854 for the vector of partial
derivatives. In ML, we descend this slope toward lower loss, hence
``gradient descent'' as the algorithm that takes steps downhill. }
automatically. Neural network training requires derivatives of a scalar
loss \(L\) with respect to millions or billions of parameters, making
manual differentiation impractical. Because a single scalar loss depends
on all parameters, reverse-mode automatic differentiation
(AD)\sidenote{\textbf{Automatic Differentiation}: Technique computing
exact derivatives by applying chain rule to elementary operations,
formalized by Wengert (1964). Reverse-mode autodiff (backpropagation)
computes all gradients in O(1) passes regardless of parameter count,
making billion-parameter training feasible. Modern implementations like
JAX's grad and PyTorch's autograd support higher-order derivatives and
custom gradient rules. } is the optimal strategy: one backward pass
computes all parameter gradients simultaneously, while forward mode
would require a separate pass for each parameter. All major ML
frameworks therefore implement reverse-mode AD by default
(\citeproc{ref-baydin2018}{Baydin et al. 2017}).

Building on the backpropagation algorithm introduced in
\textbf{?@sec-deep-learning-systems-foundations}, this section shifts
focus from the mathematics of the chain rule to the systems engineering
of differentiation: how frameworks represent computation graphs, manage
memory for intermediate values, and orchestrate the backward pass
efficiently across accelerators. The framework's role is not to perform
calculus but to manage the bookkeeping at scale, which is required for
the training algorithms detailed in \textbf{?@sec-ai-training}.
Listing~\ref{lst-auto_diff_intro} illustrates the core idea with a
simple three-operation function:

\begin{codelisting}

\caption{\label{lst-auto_diff_intro}\textbf{Automatic Differentiation}:
AD decomposes complex functions into elementary operations with known
derivatives, enabling gradient computation through arbitrarily deep
compositions in O(n) time where n is the number of operations.}

\centering{

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ f(x):}
\NormalTok{    a }\OperatorTok{=}\NormalTok{ x }\OperatorTok{*}\NormalTok{ x  }\CommentTok{\# Square}
\NormalTok{    b }\OperatorTok{=}\NormalTok{ sin(x)  }\CommentTok{\# Sine}
    \ControlFlowTok{return}\NormalTok{ a }\OperatorTok{*}\NormalTok{ b  }\CommentTok{\# Product}
\end{Highlighting}
\end{Shaded}

}

\end{codelisting}%

Frameworks decompose this function into elementary operations, each with
a known local derivative, and then combine these local derivatives via
the chain rule to compute gradients through arbitrary compositions. The
systems challenge is implementing this efficiently: the framework must
record the computation graph during the forward pass, store intermediate
values, and execute the backward pass with minimal memory overhead. The
remainder of this section examines how production frameworks solve each
of these problems.

\subsection{Forward and Reverse Mode
Differentiation}\label{sec-ai-frameworks-forward-reverse-mode-differentiation-f70a}

Automatic differentiation can be implemented in two ways: propagating
derivatives forward from inputs to outputs, or backward from outputs to
inputs. The choice between these modes determines whether gradient
computation scales with the number of inputs or the number of outputs, a
distinction that explains why neural network training universally uses
one mode over the other.

\subsubsection{Forward Mode}\label{sec-ai-frameworks-forward-mode-c3ff}

Forward mode automatic differentiation computes derivatives alongside
the original computation, tracking how changes propagate from input to
output. This approach mirrors manual derivative computation, making it
intuitive to understand and implement.

The Iron Law consequence of forward mode is direct: forward mode doubles
the Ops term for each input parameter whose derivative is requested. For
a model with \(N\) parameters, forward mode multiplies total computation
by \(N\), because each parameter requires a separate forward pass.
Reverse mode, by contrast, adds a constant factor of approximately 2 to
3x regardless of \(N\). This asymmetry explains why forward mode is
never used for training neural networks, where \(N\) ranges from
millions to hundreds of billions.

Forward mode's memory requirements, however, are its strength. The
method stores only the original value, a single derivative value, and
temporary results. Memory usage stays constant regardless of computation
depth, making forward mode particularly suitable for embedded systems,
real-time applications, and memory-bandwidth-limited systems. This
combination of computational scaling with input count but constant
memory creates a specific niche: forward mode excels in scenarios with
few inputs but many outputs, such as sensitivity analysis, feature
importance computation, and online learning with single-example updates.

To see the mechanism concretely, consider computing both the value and
derivative of \(f(x) = x^2 \sin(x)\). Listing~\ref{lst-forward_mode_ad}
shows how forward mode propagates derivative computations alongside
every operation, applying the chain rule and product rule at each step:

\begin{codelisting}

\caption{\label{lst-forward_mode_ad}\textbf{Forward Mode AD}: Propagates
derivatives forward through the computation graph, computing one
directional derivative per forward pass with 2x computational overhead.}

\centering{

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ f(x):  }\CommentTok{\# Computing both value and derivative}
    \CommentTok{\# Step 1: x {-}\textgreater{} x²}
\NormalTok{    a }\OperatorTok{=}\NormalTok{ x }\OperatorTok{*}\NormalTok{ x  }\CommentTok{\# Value: x²}
\NormalTok{    da }\OperatorTok{=} \DecValTok{2} \OperatorTok{*}\NormalTok{ x  }\CommentTok{\# Derivative: 2x}

    \CommentTok{\# Step 2: x {-}\textgreater{} sin(x)}
\NormalTok{    b }\OperatorTok{=}\NormalTok{ sin(x)  }\CommentTok{\# Value: sin(x)}
\NormalTok{    db }\OperatorTok{=}\NormalTok{ cos(x)  }\CommentTok{\# Derivative: cos(x)}

    \CommentTok{\# Step 3: Combine using product rule}
\NormalTok{    result }\OperatorTok{=}\NormalTok{ a }\OperatorTok{*}\NormalTok{ b  }\CommentTok{\# Value: x² * sin(x)}
\NormalTok{    dresult }\OperatorTok{=}\NormalTok{ a }\OperatorTok{*}\NormalTok{ db }\OperatorTok{+}\NormalTok{ b }\OperatorTok{*}\NormalTok{ da  }\CommentTok{\# Derivative: x²*cos(x) + sin(x)*2x}

    \ControlFlowTok{return}\NormalTok{ result, dresult}
\end{Highlighting}
\end{Shaded}

}

\end{codelisting}%

Forward mode achieves this systematic derivative computation by
augmenting each number with its derivative value, creating what
mathematicians call a ``dual number.''
Listing~\ref{lst-forward_mode_dual} traces a concrete execution with x =
2.0, revealing how each intermediate result carries both its value and
derivative through the computation:

\begin{codelisting}

\caption{\label{lst-forward_mode_dual}\textbf{Dual Number Computation}:
Forward mode augments each value with its derivative, doubling memory
per intermediate but enabling single-pass gradient computation.}

\centering{

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x }\OperatorTok{=} \FloatTok{2.0}  \CommentTok{\# Initial value}
\NormalTok{dx }\OperatorTok{=} \FloatTok{1.0}  \CommentTok{\# We\textquotesingle{}re tracking derivative with respect to x}

\CommentTok{\# Step 1: x²}
\NormalTok{a }\OperatorTok{=} \FloatTok{4.0}  \CommentTok{\# (2.0)²}
\NormalTok{da }\OperatorTok{=} \FloatTok{4.0}  \CommentTok{\# 2 * 2.0}

\CommentTok{\# Step 2: sin(x)}
\NormalTok{b }\OperatorTok{=} \FloatTok{0.909}  \CommentTok{\# sin(2.0)}
\NormalTok{db }\OperatorTok{=} \OperatorTok{{-}}\FloatTok{0.416}  \CommentTok{\# cos(2.0)}

\CommentTok{\# Final result}
\NormalTok{result }\OperatorTok{=} \FloatTok{3.637}  \CommentTok{\# 4.0 * 0.909}
\NormalTok{dresult }\OperatorTok{=} \FloatTok{2.805}  \CommentTok{\# 4.0 * ({-}0.416) + 0.909 * 4.0}
\end{Highlighting}
\end{Shaded}

}

\end{codelisting}%

The dual number trace demonstrates the 2x computational overhead per
input: every arithmetic operation (multiply, sine, product rule
combination) is performed twice, once for the value and once for the
derivative. For this single-input function, the overhead is acceptable.
For a neural network with \(N = 100{,}000{,}000\) parameters, computing
all gradients would require 100 million such passes, which is why
forward mode is restricted to the few-input applications described
above.

Forward mode's strength in single-input analysis becomes its fatal
weakness for training. A neural network has one scalar loss but millions
of parameters, and forward mode would require a separate pass for each
one. The following examples provide additional perspectives on forward
mode AD: the bare computation structure
(Listing~\ref{lst-forward_structure}), dual-number tuple representation
(Listing~\ref{lst-dual_tracking}), and application to sensitivity
analysis (Listing~\ref{lst-image_sensitivity}) and feature importance
(Listing~\ref{lst-feature_importance}).

\begin{codelisting}

\caption{\label{lst-forward_structure}\textbf{Forward Mode AD
Structure}: Each operation tracks values and derivatives simultaneously,
highlighting how computations are structured in forward mode automatic
differentiation.}

\centering{

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ f(x):}
\NormalTok{    a }\OperatorTok{=}\NormalTok{ x }\OperatorTok{*}\NormalTok{ x}
\NormalTok{    b }\OperatorTok{=}\NormalTok{ sin(x)}
    \ControlFlowTok{return}\NormalTok{ a }\OperatorTok{*}\NormalTok{ b}
\end{Highlighting}
\end{Shaded}

}

\end{codelisting}%

When a framework executes this function in forward mode, it augments
each computation to carry two pieces of information: the value itself
and how that value changes with respect to the input.

\begin{codelisting}

\caption{\label{lst-dual_tracking}\textbf{Dual Tracking}: Forward mode
AD augments each value with its derivative, propagating both through the
computation.}

\centering{

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Conceptually, each computation tracks (value, derivative)}
\NormalTok{x }\OperatorTok{=}\NormalTok{ (}\FloatTok{2.0}\NormalTok{, }\FloatTok{1.0}\NormalTok{)  }\CommentTok{\# Input value and its derivative}
\NormalTok{a }\OperatorTok{=}\NormalTok{ (}\FloatTok{4.0}\NormalTok{, }\FloatTok{4.0}\NormalTok{)  }\CommentTok{\# x² and its derivative 2x}
\NormalTok{b }\OperatorTok{=}\NormalTok{ (}\FloatTok{0.909}\NormalTok{, }\OperatorTok{{-}}\FloatTok{0.416}\NormalTok{)  }\CommentTok{\# sin(x) and its derivative cos(x)}
\NormalTok{result }\OperatorTok{=}\NormalTok{ (}\FloatTok{3.637}\NormalTok{, }\FloatTok{2.805}\NormalTok{)  }\CommentTok{\# Final value and derivative}
\end{Highlighting}
\end{Shaded}

}

\end{codelisting}%

Sensitivity analysis measures how changing a single pixel or feature
affects the model's output. Forward mode tracks input perturbations
through each layer to the final prediction.

\begin{codelisting}

\caption{\label{lst-image_sensitivity}\textbf{Sensitivity Analysis}:
Forward mode AD tracks how input perturbations propagate through the
network to affect predictions.}

\centering{

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ analyze\_image\_sensitivity(model, image):}
    \CommentTok{\# Forward mode tracks how changing one pixel}
    \CommentTok{\# affects the final classification}
\NormalTok{    layer1 }\OperatorTok{=}\NormalTok{ relu(W1 }\OperatorTok{@}\NormalTok{ image }\OperatorTok{+}\NormalTok{ b1)}
\NormalTok{    layer2 }\OperatorTok{=}\NormalTok{ relu(W2 }\OperatorTok{@}\NormalTok{ layer1 }\OperatorTok{+}\NormalTok{ b2)}
\NormalTok{    predictions }\OperatorTok{=}\NormalTok{ softmax(W3 }\OperatorTok{@}\NormalTok{ layer2 }\OperatorTok{+}\NormalTok{ b3)}
    \ControlFlowTok{return}\NormalTok{ predictions}
\end{Highlighting}
\end{Shaded}

}

\end{codelisting}%

Neural network interpretation presents another application. Forward mode
efficiently computes feature importance by tracking input perturbations
through each network layer to the output logits.

\begin{codelisting}

\caption{\label{lst-feature_importance}\textbf{Forward Mode AD}:
Efficiently computes feature importance by tracking input perturbations
through network operations.}

\centering{

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ compute\_feature\_importance(model, input\_features):}
    \CommentTok{\# Track influence of each input feature}
    \CommentTok{\# through the network\textquotesingle{}s computation}
\NormalTok{    hidden }\OperatorTok{=}\NormalTok{ tanh(W1 }\OperatorTok{@}\NormalTok{ input\_features }\OperatorTok{+}\NormalTok{ b1)}
\NormalTok{    logits }\OperatorTok{=}\NormalTok{ W2 }\OperatorTok{@}\NormalTok{ hidden }\OperatorTok{+}\NormalTok{ b2}
    \CommentTok{\# Forward mode efficiently computes d(logits)/d(input)}
    \ControlFlowTok{return}\NormalTok{ logits}
\end{Highlighting}
\end{Shaded}

}

\end{codelisting}%

\subsubsection{Reverse Mode}\label{sec-ai-frameworks-reverse-mode-d328}

Why does every modern ML framework default to reverse mode for training?
The answer is computational asymmetry. A neural network has one scalar
loss but millions of parameters. Forward mode computes one parameter's
gradient per pass, requiring \(n\) passes for \(n\) parameters. Reverse
mode computes all \(n\) gradients in a single backward pass. For a model
with 100 million parameters, that is the difference between 100 million
forward passes and exactly one backward pass, a speedup proportional to
the parameter count.

This asymmetry makes reverse mode the only viable option for training.
Consider a function where \(x\) influences the output through two
distinct paths. Listing~\ref{lst-reverse_simple} defines such a
function, and Listing~\ref{lst-reverse_forward} traces its forward and
backward computation for a concrete input.

\begin{codelisting}

\caption{\label{lst-reverse_simple}Basic example of reverse mode
automatic differentiation}

\centering{

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ f(x):}
\NormalTok{    a }\OperatorTok{=}\NormalTok{ x }\OperatorTok{*}\NormalTok{ x  }\CommentTok{\# First operation: square x}
\NormalTok{    b }\OperatorTok{=}\NormalTok{ sin(x)  }\CommentTok{\# Second operation: sine of x}
\NormalTok{    c }\OperatorTok{=}\NormalTok{ a }\OperatorTok{*}\NormalTok{ b  }\CommentTok{\# Third operation: multiply results}
    \ControlFlowTok{return}\NormalTok{ c}
\end{Highlighting}
\end{Shaded}

}

\end{codelisting}%

\begin{codelisting}

\caption{\label{lst-reverse_forward}\textbf{Forward and Backward Pass}:
The forward pass stores intermediate values; the backward pass
propagates gradients from output to input, accumulating contributions
from all paths.}

\centering{

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# {-}{-}{-} Forward pass: compute and store values {-}{-}{-}}
\NormalTok{x }\OperatorTok{=} \FloatTok{2.0}             \CommentTok{\# Input value}
\NormalTok{a }\OperatorTok{=} \FloatTok{4.0}             \CommentTok{\# x * x = 2.0 * 2.0 = 4.0}
\NormalTok{b }\OperatorTok{=} \FloatTok{0.909}           \CommentTok{\# sin(2.0) ≈ 0.909}
\NormalTok{c }\OperatorTok{=} \FloatTok{3.637}           \CommentTok{\# a * b = 4.0 * 0.909 ≈ 3.637}

\CommentTok{\# {-}{-}{-} Backward pass: propagate gradients from output {-}{-}{-}}
\NormalTok{dc}\OperatorTok{/}\NormalTok{dc }\OperatorTok{=} \FloatTok{1.0}         \CommentTok{\# Seed gradient}

\CommentTok{\# Through multiplication c = a * b}
\NormalTok{dc}\OperatorTok{/}\NormalTok{da }\OperatorTok{=}\NormalTok{ b            }\CommentTok{\# ∂(a*b)/∂a = b = 0.909}
\NormalTok{dc}\OperatorTok{/}\NormalTok{db }\OperatorTok{=}\NormalTok{ a            }\CommentTok{\# ∂(a*b)/∂b = a = 4.0}

\CommentTok{\# Combine contributions from both paths through x}
\CommentTok{\# Path 1: x {-}\textgreater{} x² {-}\textgreater{} c    contribution: 2x * dc/da}
\CommentTok{\# Path 2: x {-}\textgreater{} sin(x) {-}\textgreater{} c contribution: cos(x) * dc/db}
\NormalTok{dc}\OperatorTok{/}\NormalTok{dx }\OperatorTok{=}\NormalTok{ (}\DecValTok{2} \OperatorTok{*}\NormalTok{ x }\OperatorTok{*}\NormalTok{ dc}\OperatorTok{/}\NormalTok{da) }\OperatorTok{+}\NormalTok{ (cos(x) }\OperatorTok{*}\NormalTok{ dc}\OperatorTok{/}\NormalTok{db)}
      \OperatorTok{=}\NormalTok{ (}\DecValTok{2} \OperatorTok{*} \FloatTok{2.0} \OperatorTok{*} \FloatTok{0.909}\NormalTok{) }\OperatorTok{+}\NormalTok{ (cos(}\FloatTok{2.0}\NormalTok{) }\OperatorTok{*} \FloatTok{4.0}\NormalTok{)}
      \OperatorTok{=} \FloatTok{3.636} \OperatorTok{+}\NormalTok{ (}\OperatorTok{{-}}\FloatTok{0.416} \OperatorTok{*} \FloatTok{4.0}\NormalTok{)}
      \OperatorTok{=} \FloatTok{2.805}
\end{Highlighting}
\end{Shaded}

}

\end{codelisting}%

The critical observation is that this single backward pass computed
dc/dx regardless of how many paths connect \(x\) to \(c\). In a neural
network, each weight can affect the loss through thousands of paths
across layers, and reverse mode handles them all in one traversal. This
is why training a 175B parameter model like GPT-3 is feasible at all:
reverse mode's \(O(1)\) backward passes (relative to parameter count)
keeps gradient computation tractable.

\textbf{Implementation Structure.} Translating this mathematical
elegance into a working system requires solving a concrete engineering
problem: the backward pass needs values computed during the forward
pass, so the framework must decide what to store, when to store it, and
when to free it. Modern frameworks accomplish this through computational
graphs and automatic gradient accumulation\sidenote{\textbf{Gradient
Accumulation}: A technique for simulating larger batch sizes by summing
gradients over multiple mini-batches before parameter updates. Covered
in detail in \textbf{?@sec-ai-training}. }.

Listing~\ref{lst-reverse_simple_nn} illustrates this with a two-layer
network, showing both the forward computation that stores intermediate
values and the backward pass that consumes them to produce gradients for
every parameter simultaneously.

\begin{codelisting}

\caption{\label{lst-reverse_simple_nn}\textbf{Reverse Mode in a Neural
Network}: The forward pass computes and stores intermediate values; the
backward pass walks the computation in reverse to produce gradients for
every parameter.}

\centering{

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ simple\_network(x, w1, w2):}
\NormalTok{    hidden }\OperatorTok{=}\NormalTok{ x }\OperatorTok{*}\NormalTok{ w1  }\CommentTok{\# First layer}
\NormalTok{    activated }\OperatorTok{=} \BuiltInTok{max}\NormalTok{(}\DecValTok{0}\NormalTok{, hidden)  }\CommentTok{\# ReLU activation}
\NormalTok{    output }\OperatorTok{=}\NormalTok{ activated }\OperatorTok{*}\NormalTok{ w2  }\CommentTok{\# Second layer}
    \ControlFlowTok{return}\NormalTok{ output}


\CommentTok{\# {-}{-}{-} Forward pass stores intermediates {-}{-}{-}}
\CommentTok{\# x=1.0, w1=2.0, w2=3.0}
\CommentTok{\# hidden=2.0, activated=2.0, output=6.0}

\CommentTok{\# {-}{-}{-} Backward pass consumes them {-}{-}{-}}
\NormalTok{d\_output }\OperatorTok{=} \FloatTok{1.0}  \CommentTok{\# Seed gradient}
\NormalTok{d\_w2 }\OperatorTok{=}\NormalTok{ activated  }\CommentTok{\# = 2.0}
\NormalTok{d\_activated }\OperatorTok{=}\NormalTok{ w2  }\CommentTok{\# = 3.0}
\NormalTok{d\_hidden }\OperatorTok{=}\NormalTok{ d\_activated }\OperatorTok{*}\NormalTok{ (}\DecValTok{1} \ControlFlowTok{if}\NormalTok{ hidden }\OperatorTok{\textgreater{}} \DecValTok{0} \ControlFlowTok{else} \DecValTok{0}\NormalTok{)  }\CommentTok{\# ReLU gate: 3.0}
\NormalTok{d\_w1 }\OperatorTok{=}\NormalTok{ x }\OperatorTok{*}\NormalTok{ d\_hidden  }\CommentTok{\# = 3.0}
\NormalTok{d\_x }\OperatorTok{=}\NormalTok{ w1 }\OperatorTok{*}\NormalTok{ d\_hidden  }\CommentTok{\# = 6.0}
\end{Highlighting}
\end{Shaded}

}

\end{codelisting}%

Three implementation requirements emerge from this example. First, the
framework must track dependencies between operations to determine the
correct reverse traversal order. Second, intermediate values (hidden,
activated) must persist in memory until the backward pass consumes them.
Third, every operation needs both a forward implementation and a
corresponding backward rule. These requirements define the engineering
surface of any AD system, and the second requirement, memory
persistence, turns out to be the dominant cost.

\textbf{Memory Management Strategies.}
\phantomsection\label{sec-ai-frameworks-memory-management-strategies-b008}{}
A 175B parameter model in FP16 requires 350 GB just for weights, far
exceeding any single GPU's memory. But weights are only the beginning:
reverse mode AD also stores every intermediate activation from the
forward pass for use during the backward pass. For a 100-layer network
processing a batch of 64 images, these stored activations can consume 8
to 12 GB on top of the model weights, gradients, and optimizer state.
Memory, not compute, is the binding constraint on what models a
framework can train.

The problem scales linearly with depth. Listing~\ref{lst-reverse_memory}
shows how each layer in a deeper network adds another activation tensor
that must persist until the backward pass reaches that layer.

\begin{codelisting}

\caption{\label{lst-reverse_memory}\textbf{Reverse Mode Memory
Management}: Stores intermediate values for gradient computation during
backpropagation.}

\centering{

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ deep\_network(x, w1, w2, w3):}
    \CommentTok{\# Forward pass {-} must store intermediates}
\NormalTok{    hidden1 }\OperatorTok{=}\NormalTok{ x }\OperatorTok{*}\NormalTok{ w1}
\NormalTok{    activated1 }\OperatorTok{=} \BuiltInTok{max}\NormalTok{(}\DecValTok{0}\NormalTok{, hidden1)  }\CommentTok{\# Store for backward}
\NormalTok{    hidden2 }\OperatorTok{=}\NormalTok{ activated1 }\OperatorTok{*}\NormalTok{ w2}
\NormalTok{    activated2 }\OperatorTok{=} \BuiltInTok{max}\NormalTok{(}\DecValTok{0}\NormalTok{, hidden2)  }\CommentTok{\# Store for backward}
\NormalTok{    output }\OperatorTok{=}\NormalTok{ activated2 }\OperatorTok{*}\NormalTok{ w3}
    \ControlFlowTok{return}\NormalTok{ output}
\end{Highlighting}
\end{Shaded}

}

\end{codelisting}%

Frameworks attack this memory wall with two primary strategies. The
first is \emph{activation checkpointing} (also called gradient
checkpointing): rather than storing every activation, the framework
stores only selected checkpoints and recomputes the intermediate
activations during the backward pass. This trades roughly 2x
recomputation cost for a 50 to 90\% reduction in activation memory, and
\textbf{?@sec-ai-training} covers the implementation details.
Listing~\ref{lst-checkpoint_scheme} shows the pattern: save activations
at checkpoint boundaries, recompute everything between them.

\begin{codelisting}

\caption{\label{lst-checkpoint_scheme}\textbf{Checkpointing}: Reduces
memory usage by selectively storing intermediate activations during
forward passes. Frameworks balance storage needs with computational
efficiency to optimize model training.}

\centering{

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Conceptual representation of checkpointing}
\NormalTok{checkpoint1 }\OperatorTok{=}\NormalTok{ save\_for\_backward(activation1)}
\CommentTok{\# Intermediate activations can be recomputed}
\NormalTok{checkpoint2 }\OperatorTok{=}\NormalTok{ save\_for\_backward(activation4)}
\CommentTok{\# Framework balances storage vs recomputation}
\end{Highlighting}
\end{Shaded}

}

\end{codelisting}%

The second strategy is \emph{operation
fusion}\sidenote{\textbf{Operation Fusion}: Compiler optimization that
combines multiple sequential operations into a single kernel to reduce
memory bandwidth and latency. For example, fusing matrix multiplication,
bias addition, and ReLU activation can eliminate intermediate memory
allocations and achieve 2--3\(\times\) speedup on modern GPUs. }. Rather
than executing matrix multiplication, bias addition, and ReLU as three
separate operations, each writing intermediate results to memory,
frameworks fuse them into a single kernel. This eliminates intermediate
memory allocations entirely and achieves 2 to 3x speedup on modern GPUs
by keeping data in registers and caches.

The backward pass itself benefits from hardware-specific optimization.
Rather than directly translating the mathematical definition of a
convolution gradient into code, frameworks implement specialized
backward kernels that exploit memory access patterns and hardware
capabilities of modern accelerators
(\citeproc{ref-chetlur2014cudnn}{Chetlur et al. 2014}). These
optimizations, checkpointing, fusion, and specialized kernels, work
together to make training practical for architectures that would
otherwise exhaust GPU memory in a single forward pass.

\subsection{Framework Implementation of Automatic
Differentiation}\label{sec-ai-frameworks-framework-implementation-automatic-differentiation-1407}

Checkpointing, fusion, and specialized kernels solve the systems
problems of AD. But practitioners never interact with these mechanisms
directly. Instead, frameworks expose AD through high-level APIs that
hide the underlying machinery behind simple method calls.

Frameworks present AD to users through various interfaces.
Listing~\ref{lst-ad_interface} demonstrates PyTorch's approach: the
training loop appears straightforward, but \texttt{loss.backward()}
triggers the full autograd machinery that tracks operations, builds the
computation graph, and computes all parameter gradients.

\begin{codelisting}

\caption{\label{lst-ad_interface}\textbf{Automatic Differentiation
Interface}: PyTorch transparently tracks operations during neural
network execution to enable efficient backpropagation. Training requires
careful management of gradients and model parameters, highlighting the
importance of automatic differentiation in achieving optimal
performance.}

\centering{

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# PyTorch{-}style automatic differentiation}
\KeywordTok{def}\NormalTok{ neural\_network(x):}
    \CommentTok{\# Framework transparently tracks operations}
\NormalTok{    layer1 }\OperatorTok{=}\NormalTok{ nn.Linear(}\DecValTok{784}\NormalTok{, }\DecValTok{256}\NormalTok{)}
\NormalTok{    layer2 }\OperatorTok{=}\NormalTok{ nn.Linear(}\DecValTok{256}\NormalTok{, }\DecValTok{10}\NormalTok{)}

    \CommentTok{\# Each operation is automatically tracked}
\NormalTok{    hidden }\OperatorTok{=}\NormalTok{ torch.relu(layer1(x))}
\NormalTok{    output }\OperatorTok{=}\NormalTok{ layer2(hidden)}
    \ControlFlowTok{return}\NormalTok{ output}


\CommentTok{\# Training loop showing AD integration}
\ControlFlowTok{for}\NormalTok{ batch\_x, batch\_y }\KeywordTok{in}\NormalTok{ data\_loader:}
\NormalTok{    optimizer.zero\_grad()  }\CommentTok{\# Clear previous gradients}
\NormalTok{    output }\OperatorTok{=}\NormalTok{ neural\_network(batch\_x)}
\NormalTok{    loss }\OperatorTok{=}\NormalTok{ loss\_function(output, batch\_y)}

    \CommentTok{\# Framework handles all AD machinery}
\NormalTok{    loss.backward()  }\CommentTok{\# Automatic backward pass}
\NormalTok{    optimizer.step()  }\CommentTok{\# Parameter updates}
\end{Highlighting}
\end{Shaded}

}

\end{codelisting}%

While this code appears straightforward, it masks considerable
complexity. The framework must track all operations during the forward
pass, build and maintain the computational graph, manage memory for
intermediate values, schedule gradient computations efficiently, and
interface with hardware accelerators. This integration extends beyond
basic training to include complex scenarios like higher-order gradients
and mixed-precision training. Listing~\ref{lst-higher_order} illustrates
computing second-order derivatives using nested
\texttt{torch.autograd.grad} calls, enabling advanced optimization
techniques like natural gradient descent.

\begin{codelisting}

\caption{\label{lst-higher_order}\textbf{Higher-Order Gradients}:
Second-order gradients reveal how changes in model parameters affect
first-order gradients, required for advanced optimization techniques.}

\centering{

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Computing higher{-}order gradients}
\ControlFlowTok{with}\NormalTok{ torch.set\_grad\_enabled(}\VariableTok{True}\NormalTok{):}
    \CommentTok{\# First{-}order gradient computation}
\NormalTok{    output }\OperatorTok{=}\NormalTok{ model(}\BuiltInTok{input}\NormalTok{)}
\NormalTok{    grad\_output }\OperatorTok{=}\NormalTok{ torch.autograd.grad(output, model.parameters())}

    \CommentTok{\# Second{-}order gradient computation}
\NormalTok{    grad2\_output }\OperatorTok{=}\NormalTok{ torch.autograd.grad(}
\NormalTok{        grad\_output, model.parameters()}
\NormalTok{    )}
\end{Highlighting}
\end{Shaded}

}

\end{codelisting}%

\subsubsection{PyTorch Autograd
Internals}\label{sec-ai-frameworks-pytorch-autograd-internals-4fa0}

The autograd system is the framework component that solves the
differentiation problem described in
Section~\ref{sec-ai-frameworks-three-problems-every-framework-must-solve-317d}.
Three systems principles govern its design: the data structure that
enables efficient gradient computation, the memory cost of maintaining
that data structure, and the control mechanisms that production systems
require. Understanding these principles explains why training consumes
100x more memory than inference for the same model, and why frameworks
provide specific mechanisms to manage that cost.

\paragraph*{Principle 1: The Reverse-Linked Graph
Structure}\label{sec-frameworks-autograd-principle-graph}
\addcontentsline{toc}{paragraph}{Principle 1: The Reverse-Linked Graph
Structure}

During the forward pass, the autograd system constructs a reverse-linked
graph of \texttt{Function} nodes. Each node records the operation
performed and stores references to the tensors it needs for gradient
computation. This graph is the data structure that makes reverse-mode
automatic differentiation possible: regardless of how many parameters a
model has, a single backward pass through this graph computes all
gradients. For a model with \(N\) parameters, reverse-mode AD requires
\(O(1)\) backward passes (compared to \(O(N)\) for forward-mode), which
is why every major framework implements this approach.

Concretely, every tensor produced by a differentiable operation stores a
\texttt{grad\_fn} attribute pointing to the \texttt{Function} that
created it. Each \texttt{Function} links to its inputs through
\texttt{next\_functions}, forming a chain from the loss back to the leaf
parameters. Listing~\ref{lst-grad-fn-chain} illustrates this structure
for a simple computation:

\begin{codelisting}

\caption{\label{lst-grad-fn-chain}\textbf{Reverse-Linked Graph
Structure}: Each tensor's \texttt{grad\_fn} links to the
\texttt{Function} that created it, forming a reverse chain from output
to leaf parameters that enables O(1) backward passes.}

\centering{

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ torch}

\NormalTok{x }\OperatorTok{=}\NormalTok{ torch.tensor([}\FloatTok{2.0}\NormalTok{], requires\_grad}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\NormalTok{y }\OperatorTok{=}\NormalTok{ x }\OperatorTok{*} \DecValTok{3}
\NormalTok{z }\OperatorTok{=}\NormalTok{ y.}\BuiltInTok{pow}\NormalTok{(}\DecValTok{2}\NormalTok{)}

\CommentTok{\# Traverse the reverse{-}linked graph}
\BuiltInTok{print}\NormalTok{(z.grad\_fn)  }\CommentTok{\# PowBackward0}
\BuiltInTok{print}\NormalTok{(z.grad\_fn.next\_functions)  }\CommentTok{\# {-}\textgreater{} MulBackward0}
\BuiltInTok{print}\NormalTok{(}
\NormalTok{    z.grad\_fn.next\_functions[}\DecValTok{0}\NormalTok{][}\DecValTok{0}\NormalTok{].next\_functions}
\NormalTok{)  }\CommentTok{\# {-}\textgreater{} AccumulateGrad (leaf)}
\end{Highlighting}
\end{Shaded}

}

\end{codelisting}%

The traversal reveals the chain: \texttt{PowBackward0} (for
\texttt{z\ =\ y**2}) links to \texttt{MulBackward0} (for
\texttt{y\ =\ x\ *\ 3}), which terminates at \texttt{AccumulateGrad} for
the leaf tensor \texttt{x}. Leaf tensors are the endpoints of the graph
where gradients accumulate into the \texttt{.grad} attribute rather than
propagating further. The tuple format \texttt{(Function,\ index)} tracks
which output of a multi-output operation each connection corresponds to.

This reverse-linked structure has a critical systems implication: the
entire graph must remain in memory from the time a tensor is created
until the backward pass consumes it. The graph itself is lightweight
(pointers and metadata), but the tensors it references are not, as the
next principle quantifies.

\paragraph*{Principle 2: The Memory-Compute
Trade-off}\label{sec-frameworks-autograd-principle-memory}
\addcontentsline{toc}{paragraph}{Principle 2: The Memory-Compute
Trade-off}

Every activation saved for the backward pass persists in memory until
consumed by gradient computation. This is the fundamental reason
training memory dwarfs inference memory. Computing the gradient of most
operations requires values from the forward pass: multiplication needs
both inputs (\(\frac{\partial}{\partial x}(x \cdot y) = y\)),
exponentiation needs the base
(\(\frac{\partial}{\partial x}(x^2) = 2x\)), and softmax needs its
output values. The autograd system stores these tensors in each
\texttt{Function} node's \texttt{saved\_tensors} attribute.

For a network with \(L\) layers, the system must save approximately
\(L\) activation tensors, one per layer, for the entire batch. Consider
a concrete example: ResNet-50 has 25M parameters (\textasciitilde100 MB
in FP32) and processes batch size 64 with 224x224 images. The memory
breakdown reveals the scale of this trade-off:

\begin{itemize}
\tightlist
\item
  \textbf{Forward activations}: \textasciitilde8-12 GB (varies by
  implementation and checkpointing)
\item
  \textbf{Parameter gradients}: \textasciitilde100 MB (same size as
  parameters)
\item
  \textbf{Optimizer state (Adam)}: \textasciitilde200 MB (two momentum
  buffers per parameter)
\item
  \textbf{Total training footprint}: \textasciitilde10-15 GB versus
  \textasciitilde100 MB for inference
\end{itemize}

This 100x ratio between training and inference memory quantifies why the
Data Movement (\(D\)) term dominates training latency in the Iron Law.
During training, the framework must write all activations to memory
during the forward pass and read them back during the backward pass,
doubling the memory traffic compared to inference alone.

Frameworks provide two primary mechanisms to manage this trade-off.
\textbf{Gradient checkpointing} (\citeproc{ref-chen2016training}{Chen et
al. 2016}) trades recomputation for memory: instead of saving all
activations, the framework saves only a subset and recomputes the rest
during the backward pass. This typically reduces activation memory by
50\% at the cost of approximately 2x slower backward computation. In
Iron Law terms, checkpointing increases the \(Ops\) term (recomputation)
to reduce the \(D\) term (memory traffic). \textbf{Tensor detachment}
provides a complementary mechanism: calling \texttt{.detach()} on a
tensor removes it from the computation graph entirely, preventing the
framework from saving activations through that path. This is essential
for transfer learning, where pretrained layers should not accumulate
gradients, and reduces the \(D\) term by eliminating unnecessary
activation storage.

Mixed-precision training offers a third approach: storing activations in
FP16 rather than FP32 halves the activation memory while modern Tensor
Cores execute FP16 matrix multiplications at 2x the throughput of FP32.
The net effect is both reduced \(D\) (smaller activations) and increased
hardware utilization (\(\eta\)).

\paragraph*{Principle 3: Extensibility and
Control}\label{sec-frameworks-autograd-principle-extensibility}
\addcontentsline{toc}{paragraph}{Principle 3: Extensibility and Control}

Production training systems require fine-grained control over gradient
flow that goes beyond the default backward pass. Three categories of
control arise in practice. First, \textbf{selective gradient
computation}: transfer learning and fine-tuning require freezing subsets
of parameters, which the framework supports through
\texttt{requires\_grad=False} flags and the \texttt{.detach()} mechanism
described above. Second, \textbf{gradient inspection and modification}:
debugging vanishing or exploding gradients, implementing per-tensor
gradient clipping, and logging gradient statistics all require
intercepting gradients mid-computation, which frameworks expose through
hook APIs. Third, \textbf{custom differentiation rules}: operations not
in the framework's built-in library (custom CUDA kernels, novel
activation functions, domain-specific operations) require user-defined
forward and backward implementations.

These control mechanisms share a common systems design: they are
callback-based extensions that the autograd engine invokes at specific
points during graph traversal, without modifying the core
differentiation algorithm. This extensibility pattern allows the
framework to maintain a single optimized backward pass while supporting
arbitrarily complex gradient manipulation. The following examples
demonstrate how to inspect and control PyTorch's autograd system:
gradient accumulation (Listing~\ref{lst-gradient-accumulation}), custom
autograd functions (Listing~\ref{lst-custom-autograd-function}),
gradient hooks (Listing~\ref{lst-gradient-hooks}), and safe gradient
detachment (Listing~\ref{lst-detach-vs-data}).

\textbf{Gradient Accumulation Behavior.} Gradients accumulate across
backward passes by default. Without calling \texttt{zero\_grad()},
successive backward passes sum their gradients:

\begin{codelisting}

\caption{\label{lst-gradient-accumulation}\textbf{Gradient Accumulation
Behavior}: Gradients accumulate across backward passes by default. Use
zero\_grad() to reset gradients before each optimization step.}

\centering{

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x }\OperatorTok{=}\NormalTok{ torch.tensor([}\FloatTok{1.0}\NormalTok{], requires\_grad}\OperatorTok{=}\VariableTok{True}\NormalTok{)}

\CommentTok{\# First backward pass}
\NormalTok{y }\OperatorTok{=}\NormalTok{ x }\OperatorTok{*} \DecValTok{2}
\NormalTok{y.backward()}
\BuiltInTok{print}\NormalTok{(x.grad)  }\CommentTok{\# tensor([2.])}

\CommentTok{\# Second backward pass (without zero\_grad)}
\NormalTok{y }\OperatorTok{=}\NormalTok{ x }\OperatorTok{*} \DecValTok{3}
\NormalTok{y.backward()}
\BuiltInTok{print}\NormalTok{(x.grad)  }\CommentTok{\# tensor([5.]) = 2 + 3 (accumulated!)}

\CommentTok{\# Reset gradients}
\NormalTok{x.grad.zero\_()}
\NormalTok{y }\OperatorTok{=}\NormalTok{ x }\OperatorTok{*} \DecValTok{3}
\NormalTok{y.backward()}
\BuiltInTok{print}\NormalTok{(x.grad)  }\CommentTok{\# tensor([3.])}
\end{Highlighting}
\end{Shaded}

}

\end{codelisting}%

\textbf{Custom Autograd Functions.} When implementing custom operations,
you explicitly specify what to save for the backward pass and how to
compute gradients:

\begin{codelisting}

\caption{\label{lst-custom-autograd-function}\textbf{Custom Autograd
Function}: Implement forward and backward methods to define custom
differentiable operations, explicitly specifying tensors to save for
gradient computation.}

\centering{

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{class}\NormalTok{ MultiplyAdd(torch.autograd.Function):}
    \AttributeTok{@staticmethod}
    \KeywordTok{def}\NormalTok{ forward(ctx, x, y, z):}
        \CommentTok{\# Save tensors needed for backward}
\NormalTok{        ctx.save\_for\_backward(x, y)}
        \ControlFlowTok{return}\NormalTok{ x }\OperatorTok{*}\NormalTok{ y }\OperatorTok{+}\NormalTok{ z}

    \AttributeTok{@staticmethod}
    \KeywordTok{def}\NormalTok{ backward(ctx, grad\_output):}
        \CommentTok{\# Retrieve saved tensors}
\NormalTok{        x, y }\OperatorTok{=}\NormalTok{ ctx.saved\_tensors}

        \CommentTok{\# Compute gradients using chain rule}
\NormalTok{        grad\_x }\OperatorTok{=}\NormalTok{ grad\_output }\OperatorTok{*}\NormalTok{ y  }\CommentTok{\# ∂L/∂x = ∂L/∂out * ∂out/∂x}
\NormalTok{        grad\_y }\OperatorTok{=}\NormalTok{ grad\_output }\OperatorTok{*}\NormalTok{ x  }\CommentTok{\# ∂L/∂y = ∂L/∂out * ∂out/∂y}
\NormalTok{        grad\_z }\OperatorTok{=}\NormalTok{ grad\_output  }\CommentTok{\# ∂L/∂z = ∂L/∂out * 1}

        \ControlFlowTok{return}\NormalTok{ grad\_x, grad\_y, grad\_z}


\CommentTok{\# Usage}
\NormalTok{x }\OperatorTok{=}\NormalTok{ torch.tensor([}\FloatTok{2.0}\NormalTok{], requires\_grad}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\NormalTok{y }\OperatorTok{=}\NormalTok{ torch.tensor([}\FloatTok{3.0}\NormalTok{], requires\_grad}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\NormalTok{z }\OperatorTok{=}\NormalTok{ torch.tensor([}\FloatTok{1.0}\NormalTok{], requires\_grad}\OperatorTok{=}\VariableTok{True}\NormalTok{)}

\NormalTok{output }\OperatorTok{=}\NormalTok{ MultiplyAdd.}\BuiltInTok{apply}\NormalTok{(x, y, z)}
\NormalTok{output.backward()}

\BuiltInTok{print}\NormalTok{(}
\NormalTok{    x.grad, y.grad, z.grad}
\NormalTok{)  }\CommentTok{\# tensor([3.]), tensor([2.]), tensor([1.])}
\end{Highlighting}
\end{Shaded}

}

\end{codelisting}%

\textbf{Retaining the Computation Graph.} By default,
\texttt{backward()} frees the graph after use. To run multiple backward
passes (for multi-loss optimization or higher-order derivatives), use
\texttt{retain\_graph=True} at the cost of doubled memory, as shown in
Listing~\ref{lst-retain-graph}.

\begin{codelisting}

\caption{\label{lst-retain-graph}\textbf{Retaining Computation Graph}:
Use retain\_graph=True to run multiple backward passes on the same
graph, useful for multi-loss optimization or higher-order derivatives.}

\centering{

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x }\OperatorTok{=}\NormalTok{ torch.tensor([}\FloatTok{1.0}\NormalTok{], requires\_grad}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\NormalTok{y }\OperatorTok{=}\NormalTok{ x }\OperatorTok{*} \DecValTok{2}
\NormalTok{loss }\OperatorTok{=}\NormalTok{ y.}\BuiltInTok{pow}\NormalTok{(}\DecValTok{2}\NormalTok{)}

\CommentTok{\# First backward (graph retained)}
\NormalTok{loss.backward(retain\_graph}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(x.grad)  }\CommentTok{\# tensor([8.])}

\CommentTok{\# Second backward on same graph}
\NormalTok{x.grad.zero\_()}
\NormalTok{loss.backward()}
\BuiltInTok{print}\NormalTok{(x.grad)  }\CommentTok{\# tensor([8.])}
\end{Highlighting}
\end{Shaded}

}

\end{codelisting}%

\textbf{Gradient Hooks.} Register hooks on tensors to inspect or modify
gradients during backpropagation:

\begin{codelisting}

\caption{\label{lst-gradient-hooks}\textbf{Gradient Hooks}: Register
hooks on tensors to inspect or modify gradients during backpropagation,
useful for debugging, gradient clipping, or custom gradient
manipulation.}

\centering{

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ gradient\_hook(grad):}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Gradient: }\SpecialCharTok{\{}\NormalTok{grad}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
    \CommentTok{\# Modify gradient (e.g., gradient clipping)}
    \ControlFlowTok{return}\NormalTok{ grad.clamp(}\OperatorTok{{-}}\FloatTok{1.0}\NormalTok{, }\FloatTok{1.0}\NormalTok{)}


\NormalTok{x }\OperatorTok{=}\NormalTok{ torch.tensor([}\FloatTok{2.0}\NormalTok{], requires\_grad}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\NormalTok{x.register\_hook(gradient\_hook)}

\NormalTok{y }\OperatorTok{=}\NormalTok{ x }\OperatorTok{*} \DecValTok{10}
\NormalTok{y.backward()}
\CommentTok{\# Prints: Gradient: tensor([10.])}
\CommentTok{\# x.grad contains clamped value: tensor([1.])}
\end{Highlighting}
\end{Shaded}

}

\end{codelisting}%

\textbf{Detach vs.~Data.} Use \texttt{.detach()} to safely break
gradient flow. The deprecated \texttt{.data} attribute can silently
corrupt gradient computation through in-place operations:

\begin{codelisting}

\caption{\label{lst-detach-vs-data}\textbf{Detach vs Data}: Use
.detach() to safely break gradient flow. Avoid .data which can silently
break gradient computation with in-place operations.}

\centering{

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Using .detach() (recommended)}
\NormalTok{x }\OperatorTok{=}\NormalTok{ torch.tensor([}\FloatTok{1.0}\NormalTok{], requires\_grad}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\NormalTok{y }\OperatorTok{=}\NormalTok{ x.detach()}

\CommentTok{\# y shares storage with x but requires\_grad=False}
\CommentTok{\# Gradients don\textquotesingle{}t flow through y}
\NormalTok{z }\OperatorTok{=}\NormalTok{ y }\OperatorTok{*} \DecValTok{2}
\NormalTok{z.backward()  }\CommentTok{\# Error: z doesn\textquotesingle{}t require grad}

\CommentTok{\# Using .data (deprecated, dangerous)}
\NormalTok{x }\OperatorTok{=}\NormalTok{ torch.tensor([}\FloatTok{1.0}\NormalTok{], requires\_grad}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\NormalTok{y }\OperatorTok{=}\NormalTok{ x.data}

\CommentTok{\# DANGEROUS: In{-}place operations on y affect x but break autograd}
\NormalTok{y.mul\_(}\DecValTok{2}\NormalTok{)  }\CommentTok{\# x is now [2.0] but autograd doesn\textquotesingle{}t know!}
\NormalTok{z }\OperatorTok{=}\NormalTok{ x }\OperatorTok{+} \DecValTok{1}
\NormalTok{z.backward()  }\CommentTok{\# Computes wrong gradient!}
\end{Highlighting}
\end{Shaded}

}

\end{codelisting}%

These three principles connect directly to the framework's role as a
compiler for the Silicon Contract. The reverse-linked graph determines
which operations the backward pass must execute (the \(Ops\) term). The
memory-compute trade-off governs how much data the framework must move
through the memory hierarchy (the \(D\) term). And the extensibility
mechanisms allow engineers to tune both terms for their specific
workload. The interaction between autograd memory management and
numerical precision leads naturally to mixed-precision training, which
further reduces the \(D\) term.

\textbf{Mixed-Precision Training Support.} Mixed precision exploits a
hardware asymmetry to improve two Iron Law terms simultaneously: Tensor
Cores execute FP16 matrix multiplications at 2x the throughput of FP32
(increasing effective \(O/R_{peak}\)), while FP16 activations halve the
memory footprint (reducing \(D_{vol}\)). Improving both terms
simultaneously is rare; most optimizations improve one at the expense of
the other.

Frameworks exploit this through automatic mixed-precision APIs that
select reduced precision for compute-intensive operations while
maintaining FP32 where numerical stability demands it. Inside these
APIs, frameworks automatically apply precision rules: matrix
multiplications and convolutions use FP16 for bandwidth efficiency,
while numerically sensitive operations like softmax and layer
normalization remain in FP32. This selective precision maintains
accuracy while achieving speedups on modern GPUs with specialized
hardware units. Because FP16 has a narrower dynamic range than FP32,
gradients can underflow to zero during backpropagation. Loss scaling
addresses this by multiplying the loss by a large factor before the
backward pass, then dividing gradients by the same factor afterward.

Frameworks also support multiple precision formats including FP16, BF16,
and TF32, each with different trade-offs between range and precision.
BF16 maintains FP32's dynamic range, simplifying training by eliminating
most gradient underflow issues and removing the need for loss scaling
entirely. \textbf{?@sec-ai-training} examines the mechanics of
mixed-precision training in detail, including loss scaling algorithms,
memory savings analysis, and numerical stability considerations.
Listing~\ref{lst-autocast-usage} demonstrates PyTorch's mixed precision
API: the \texttt{autocast} context manager automatically selects FP16
for compute-intensive operations while \texttt{GradScaler} prevents
gradient underflow by dynamically scaling loss values.

\begin{codelisting}

\caption{\label{lst-autocast-usage}\textbf{Mixed-Precision API}: Modern
frameworks provide automatic mixed-precision support through context
managers that handle precision selection and numerical stability.}

\centering{

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ torch}
\ImportTok{from}\NormalTok{ torch.amp }\ImportTok{import}\NormalTok{ autocast, GradScaler}

\NormalTok{model }\OperatorTok{=}\NormalTok{ MyModel().cuda()}
\NormalTok{optimizer }\OperatorTok{=}\NormalTok{ torch.optim.Adam(model.parameters())}
\NormalTok{scaler }\OperatorTok{=}\NormalTok{ GradScaler(}\StringTok{"cuda"}\NormalTok{)}

\ControlFlowTok{for}\NormalTok{ inputs, targets }\KeywordTok{in}\NormalTok{ dataloader:}
\NormalTok{    inputs, targets }\OperatorTok{=}\NormalTok{ inputs.cuda(), targets.cuda()}
\NormalTok{    optimizer.zero\_grad()}

    \CommentTok{\# Framework automatically selects precision per operation}
    \ControlFlowTok{with}\NormalTok{ autocast(device\_type}\OperatorTok{=}\StringTok{"cuda"}\NormalTok{, dtype}\OperatorTok{=}\NormalTok{torch.float16):}
\NormalTok{        outputs }\OperatorTok{=}\NormalTok{ model(inputs)}
\NormalTok{        loss }\OperatorTok{=}\NormalTok{ criterion(outputs, targets)}

    \CommentTok{\# GradScaler handles gradient scaling for numerical stability}
\NormalTok{    scaler.scale(loss).backward()}
\NormalTok{    scaler.step(optimizer)}
\NormalTok{    scaler.update()}
\end{Highlighting}
\end{Shaded}

}

\end{codelisting}%

BF16 training typically does not require loss scaling, as
Listing~\ref{lst-bf16-training} demonstrates.

\begin{codelisting}

\caption{\label{lst-bf16-training}\textbf{BF16 Training}: BF16 maintains
FP32's dynamic range, eliminating the need for loss scaling that FP16
requires.}

\centering{

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# BF16 training typically does not require loss scaling}
\ControlFlowTok{with}\NormalTok{ torch.autocast(device\_type}\OperatorTok{=}\StringTok{"cuda"}\NormalTok{, dtype}\OperatorTok{=}\NormalTok{torch.bfloat16):}
\NormalTok{    outputs }\OperatorTok{=}\NormalTok{ model(inputs)}
\NormalTok{    loss }\OperatorTok{=}\NormalTok{ criterion(outputs, targets)}
\NormalTok{loss.backward()  }\CommentTok{\# No GradScaler needed}
\NormalTok{optimizer.step()}
\end{Highlighting}
\end{Shaded}

}

\end{codelisting}%

\textbf{Optimizer State and Checkpointing.} Resuming training after
interruption requires restoring not just model weights but optimizer
state: momentum buffers, adaptive learning rates, and gradient
statistics. For Adam, optimizer state typically quintuples the memory
footprint beyond weights alone (since two FP32 states are stored for
each FP16 parameter), meaning a 7B-parameter model requires
approximately 70 GB total (14 GB weights + 56 GB optimizer state).
Checkpoint size therefore bounds recovery speed after failure,
connecting fault tolerance directly to the Iron Law's \(D\) term.

\textbf{?@sec-ai-training} covers optimizer memory requirements and
optimization strategies for large-scale training, where checkpoint size
becomes a binding constraint. Frameworks provide the
\texttt{state\_dict()} interface to access optimizer state for
serialization (Listing~\ref{lst-state-dict-interface}), and resuming
training requires loading both model parameters and optimizer state
(Listing~\ref{lst-checkpoint-save-load}).

\begin{codelisting}

\caption{\label{lst-state-dict-interface}\textbf{State Dictionary
Interface}: Optimizers expose internal state through state\_dict(),
enabling serialization of momentum buffers and adaptive learning rate
estimates for checkpointing.}

\centering{

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ torch}
\ImportTok{import}\NormalTok{ torch.nn }\ImportTok{as}\NormalTok{ nn}
\ImportTok{import}\NormalTok{ torch.optim }\ImportTok{as}\NormalTok{ optim}

\NormalTok{model }\OperatorTok{=}\NormalTok{ nn.Linear(}\DecValTok{10}\NormalTok{, }\DecValTok{5}\NormalTok{)}
\NormalTok{optimizer }\OperatorTok{=}\NormalTok{ optim.Adam(model.parameters(), lr}\OperatorTok{=}\FloatTok{0.001}\NormalTok{)}

\CommentTok{\# After training steps, optimizer accumulates state}
\NormalTok{loss }\OperatorTok{=}\NormalTok{ model(torch.randn(}\DecValTok{3}\NormalTok{, }\DecValTok{10}\NormalTok{)).}\BuiltInTok{sum}\NormalTok{()}
\NormalTok{loss.backward()}
\NormalTok{optimizer.step()}

\CommentTok{\# Access state for checkpointing}
\NormalTok{state }\OperatorTok{=}\NormalTok{ optimizer.state\_dict()}
\CommentTok{\# Contains: \{\textquotesingle{}state\textquotesingle{}: \{...\}, \textquotesingle{}param\_groups\textquotesingle{}: [\{\textquotesingle{}lr\textquotesingle{}: 0.001, ...\}]\}}
\end{Highlighting}
\end{Shaded}

}

\end{codelisting}%

\begin{codelisting}

\caption{\label{lst-checkpoint-save-load}\textbf{Checkpoint Save and
Load}: Save both model parameters and optimizer state to properly resume
training with correct momentum and adaptive learning rate values.}

\centering{

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Saving checkpoint}
\NormalTok{checkpoint }\OperatorTok{=}\NormalTok{ \{}
    \StringTok{"epoch"}\NormalTok{: epoch,}
    \StringTok{"model\_state\_dict"}\NormalTok{: model.state\_dict(),}
    \StringTok{"optimizer\_state\_dict"}\NormalTok{: optimizer.state\_dict(),}
\NormalTok{\}}
\NormalTok{torch.save(checkpoint, }\StringTok{"checkpoint.pt"}\NormalTok{)}

\CommentTok{\# Resuming training}
\NormalTok{checkpoint }\OperatorTok{=}\NormalTok{ torch.load(}\StringTok{"checkpoint.pt"}\NormalTok{)}
\NormalTok{model.load\_state\_dict(checkpoint[}\StringTok{"model\_state\_dict"}\NormalTok{])}
\NormalTok{optimizer.load\_state\_dict(checkpoint[}\StringTok{"optimizer\_state\_dict"}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

}

\end{codelisting}%

The mathematics of automatic differentiation were established decades
before deep learning's resurgence. What changed was the systems
engineering. Before framework automation, implementing gradient
computation for a single fully connected layer meant writing separate
forward and backward functions, manually tracking intermediate values,
and verifying mathematical correctness across dozens of operations. A
modern Transformer involves hundreds of operations with complex
dependencies; manual gradient derivation for attention, layer
normalization, and residual connections would require months of careful
work per architecture variant.

The breakthrough was turning this manual process into software
infrastructure. A single matrix multiplication requires different
gradient computations depending on which inputs require gradients,
tensor shapes, hardware capabilities, and memory constraints. Autograd
systems handle these variations transparently, which is why the rate of
architectural innovation accelerated after frameworks matured. The
mathematics did not change; software engineering made the mathematics
practical to apply at scale.

\textbf{Memory Management in Gradient Computation.} The memory
strategies from Section~\ref{sec-ai-frameworks-reverse-mode-d328}
(checkpointing, gradient accumulation) exist because of a fundamental
constraint: reverse-mode differentiation requires preserving
computational history. Unlike traditional programs that can discard
intermediate results as soon as they are used, AD systems must carefully
preserve this history to compute gradients during the backward pass.
Listing~\ref{lst-forward_trace} illustrates this necessity.

\begin{codelisting}

\caption{\label{lst-forward_trace}\textbf{Forward Pass}: Neural networks
compute values sequentially, storing intermediate results for
backpropagation to calculate gradients accurately.}

\centering{

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ neural\_network(x):}
    \CommentTok{\# Each operation creates values that must be remembered}
\NormalTok{    a }\OperatorTok{=}\NormalTok{ layer1(x)  }\CommentTok{\# Must store for backward pass}
\NormalTok{    b }\OperatorTok{=}\NormalTok{ relu(a)  }\CommentTok{\# Must store input to relu}
\NormalTok{    c }\OperatorTok{=}\NormalTok{ layer2(b)  }\CommentTok{\# Must store for backward pass}
    \ControlFlowTok{return}\NormalTok{ c}
\end{Highlighting}
\end{Shaded}

}

\end{codelisting}%

When this network processes data, each operation creates not just its
output, but also a memory obligation. The multiplication in layer1 needs
to remember its inputs because computing its gradient later will require
them. Even the seemingly simple relu function must track which inputs
were negative to correctly propagate gradients. As networks grow deeper,
these memory requirements accumulate, as Listing~\ref{lst-deep_memory}
demonstrates.

\begin{codelisting}

\caption{\label{lst-deep_memory}\textbf{Memory Accumulation}: Each layer
in a deep neural network retains information needed for backpropagation,
highlighting the growing memory demands as networks deepen.}

\centering{

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# A deeper network shows the accumulating memory needs}
\NormalTok{hidden1 }\OperatorTok{=}\NormalTok{ large\_matrix\_multiply(}\BuiltInTok{input}\NormalTok{, weights1)}
\NormalTok{activated1 }\OperatorTok{=}\NormalTok{ relu(hidden1)}
\NormalTok{hidden2 }\OperatorTok{=}\NormalTok{ large\_matrix\_multiply(activated1, weights2)}
\NormalTok{activated2 }\OperatorTok{=}\NormalTok{ relu(hidden2)}
\NormalTok{output }\OperatorTok{=}\NormalTok{ large\_matrix\_multiply(activated2, weights3)}
\end{Highlighting}
\end{Shaded}

}

\end{codelisting}%

Each layer's computation adds to the memory burden. The framework must
keep hidden1 in memory until gradients are computed through hidden2,
after which it can be safely discarded. This creates a wave of memory
usage that peaks when we start the backward pass and gradually recedes
as we compute gradients.

Modern frameworks track the lifetime of each intermediate value
automatically, freeing memory as soon as it is no longer needed for
gradient computation. But even with precise lifetime tracking, a deeper
problem remains: the cost of acquiring memory from the GPU in the first
place.

This observation provides a fundamental engineering lesson: production
systems require \textbf{Memory Abstraction}. In a production
environment, requesting memory directly from a GPU is a high-latency
operation that can synchronize the entire device, creating a massive
``Allocation Bottleneck'' that stalls computation. To solve this, modern
frameworks implement \textbf{Caching Allocators}. Instead of
communicating with the hardware for every new tensor, the framework
requests large blocks of memory upfront and manages its own internal
pool. This abstraction is critical because it prevents \textbf{memory
fragmentation}, the scenario where free memory is available but
scattered in pieces too small to hold a large tensor, allowing models to
push the physical limits of the hardware without constant system-level
overhead.

\phantomsection\label{callout-perspectiveux2a-1.9}
\begin{fbx}{callout-perspective}{Systems Perspective:}{Caching Allocator and Utilization}
\phantomsection\label{callout-perspective*-1.9}
The \textbf{Caching Allocator} is the framework's primary mechanism for
maximizing the \textbf{Utilization} term in the Iron Law
(\(\frac{1}{\text{Utilization}}\)). Without it, two factors degrade
performance significantly:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Allocation Latency}: \texttt{cudaMalloc} is a synchronous
  operation that costs 10--100 \(\mu\)s. In a training loop with
  thousands of operations per second, this latency would dominate
  execution time. The caching allocator pays this cost once, then serves
  subsequent requests in nanoseconds from its pool.
\item
  \textbf{Fragmentation}: A ``Swiss cheese'' memory pattern reduces
  \textbf{Effective Capacity}. If you have 10 GB free but the largest
  contiguous block is 1 GB, you cannot allocate a 2 GB tensor. By
  binning allocations into standard sizes (powers of 2), the allocator
  ensures that freed memory can be reused for future requests, keeping
  \textbf{Utilization} high.
\end{enumerate}

When you see ``OOM'' (Out of Memory) errors despite \texttt{nvidia-smi}
showing free memory, \textbf{fragmentation} is often the culprit. The
allocator cannot find a contiguous block large enough for the requested
tensor.

\end{fbx}

\textbf{Production System Integration Challenges.} A training iteration
that takes 300 ms in profiling may take 500 ms in production because the
AD system must coordinate with the memory allocator, the device manager,
the operation scheduler, and the optimizer on every single step. Each
gradient computation can trigger data movement between CPU and GPU,
memory allocation for intermediate tensors, and kernel launches on
accelerators. These system interactions dominate wall-clock time for
small models and remain significant even at scale.

Listing~\ref{lst-train_loop} reveals the gap between what the programmer
writes and what the system executes.

\begin{codelisting}

\caption{\label{lst-train_loop}\textbf{Training Loop}: A typical
training iteration coordinates data movement, forward pass, gradient
computation, and parameter updates.}

\centering{

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ train\_epoch(model, data\_loader):}
    \ControlFlowTok{for}\NormalTok{ batch\_x, batch\_y }\KeywordTok{in}\NormalTok{ data\_loader:}
        \CommentTok{\# Moving data between CPU and accelerator}
\NormalTok{        batch\_x }\OperatorTok{=}\NormalTok{ batch\_x.to(device)}
\NormalTok{        batch\_y }\OperatorTok{=}\NormalTok{ batch\_y.to(device)}

        \CommentTok{\# Forward pass builds computational graph}
\NormalTok{        outputs }\OperatorTok{=}\NormalTok{ model(batch\_x)}
\NormalTok{        loss }\OperatorTok{=}\NormalTok{ criterion(outputs, batch\_y)}

        \CommentTok{\# Backward pass computes gradients}
\NormalTok{        loss.backward()}
\NormalTok{        optimizer.step()}
\NormalTok{        optimizer.zero\_grad()}
\end{Highlighting}
\end{Shaded}

}

\end{codelisting}%

Beyond this sequential overhead, modern networks frequently contain
independent branches whose gradients can be computed concurrently.
Listing~\ref{lst-parallel_ad} illustrates a branching architecture where
two convolutional paths process the same input independently before
merging. On a GPU with sufficient resources, the framework's scheduler
can execute both branch backward passes on separate CUDA streams,
reducing backward pass time by up to 30 to 40\% for architectures with
significant branch parallelism (such as Inception-style networks).

\begin{codelisting}

\caption{\label{lst-parallel_ad}\textbf{Parallel Computation}:
Independent branches can execute concurrently, requiring synchronization
only when results are combined.}

\centering{

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ parallel\_network(x):}
    \CommentTok{\# These operations could run concurrently}
\NormalTok{    branch1 }\OperatorTok{=}\NormalTok{ conv\_layer1(x)}
\NormalTok{    branch2 }\OperatorTok{=}\NormalTok{ conv\_layer2(x)}

    \CommentTok{\# Must synchronize for combination}
\NormalTok{    combined }\OperatorTok{=}\NormalTok{ branch1 }\OperatorTok{+}\NormalTok{ branch2}
    \ControlFlowTok{return}\NormalTok{ final\_layer(combined)}
\end{Highlighting}
\end{Shaded}

}

\end{codelisting}%

The AD system must therefore track dependencies for two purposes:
correctness (computing the right gradients) and performance (scheduling
independent computations concurrently). Frameworks hide this complexity
behind \texttt{loss.backward()}, but the scheduling, memory allocation,
and data movement decisions behind that call determine whether training
runs at 40\% or 80\% of peak hardware utilization.

The way frameworks implement automatic differentiation varies
significantly, with consequences for both \emph{optimization potential}
and developer experience. The distinction between \emph{tape-based and
transform-based autodiff} captures this divergence.

\phantomsection\label{callout-perspectiveux2a-1.10}
\begin{fbx}{callout-perspective}{Systems Perspective:}{Tape-based vs. Transform-based Autodiff}
\phantomsection\label{callout-perspective*-1.10}
\textbf{PyTorch (Tape-based)}: Records operations on a dynamic ``tape''
during the forward pass. This is flexible and easy to debug but makes it
hard for a compiler to see the whole graph at once for global
optimization.

\textbf{JAX (Transform-based)}: Treats automatic differentiation as a
high-level function transformation (\texttt{grad(f)}). Because JAX sees
the mathematical function before execution, it can easily chain other
transformations like \texttt{jit(grad(f))} or \texttt{vmap(grad(f))},
producing highly optimized, compiled kernels that often outperform
dynamic frameworks on specialized hardware like TPUs.

\end{fbx}

\textbf{How Different Frameworks Implement AD.} The execution models
covered in Section~\ref{sec-ai-frameworks-execution-problem-e1e1},
namely eager, static graph, and hybrid, directly shape how each
framework implements automatic differentiation:

\begin{itemize}
\tightlist
\item
  \textbf{PyTorch} (\citeproc{ref-paszke2019pytorch}{Paszke et al.
  2019}) builds its autograd tape dynamically during forward execution,
  providing immediate debugging at the cost of graph-level optimization.
  The \texttt{grad\_fn} chain mechanism detailed in
  Section~\ref{sec-ai-frameworks-pytorch-autograd-internals-4fa0}
  enables flexible control flow but requires storing the complete graph
  until backward pass completion.
\item
  \textbf{TensorFlow} (in its 1.x incarnation) performed symbolic
  differentiation during graph construction, enabling ahead-of-time
  optimization. Modern TensorFlow 2.x uses eager execution by default
  but provides \texttt{tf.function} for graph compilation when
  performance matters.
\item
  \textbf{JAX}
  (\citeproc{ref-frostig2018compiling}{\textbf{frostig2018compiling?}})
  transforms functions rather than tracking operations. The
  \texttt{jax.grad()} transformation returns a new function that
  computes gradients, enabling composition with \texttt{jax.vmap()} for
  vectorization and \texttt{jax.jit()} for compilation. This
  fundamentally different approach requires pure functions but enables
  powerful program transformations.
\end{itemize}

These implementation differences have direct practical consequences for
framework selection, which
Section~\ref{sec-ai-frameworks-major-framework-platform-analysis-fe96}
examines in detail.

A recurring tension runs through every AD design decision: mathematical
correctness demands storing computational history, but hardware imposes
strict memory limits. Every framework resolves this tension differently,
choosing which activations to checkpoint, which operations to fuse, and
how aggressively to trade recomputation for memory. These choices
determine which models can train on which hardware, making AD system
design one of the most consequential engineering decisions in any
framework.

\phantomsection\label{callout-checkpointux2a-1.11}
\begin{fbx}{callout-checkpoint}{Checkpoint:}{The Systems Cost of Gradients}
\phantomsection\label{callout-checkpoint*-1.11}

Training is \emph{fundamentally more expensive} than inference because
of Automatic Differentiation.

\textbf{Computational Reality}

\begin{itemize}
\tightlist
\item[$\square$]
  \textbf{Reverse Mode AD}: Why is this the only viable method for
  neural networks? (Because we have 1 loss scalar and \(10^9\)
  parameters. Forward mode would require \(10^9\) passes).
\item[$\square$]
  \textbf{The Activation Tax}: Do you understand why training memory
  scales linearly with depth? (We must stash forward activations to
  compute backward gradients).
\end{itemize}

\textbf{Optimization Mechanics}

\begin{itemize}
\tightlist
\item[$\square$]
  \textbf{Gradient Checkpointing}: How does re-computing activations
  save memory? (We discard the stash and regenerate it on demand).
\end{itemize}

\end{fbx}

The execution and differentiation problems together enable the training
loop. But both assume that the same code can run across diverse
hardware: CPUs, GPUs, TPUs, mobile devices, and microcontrollers. This
hardware diversity creates the third fundamental problem.

\section{The Abstraction
Problem}\label{sec-ai-frameworks-abstraction-problem-37a5}

The third fundamental problem is targeting diverse hardware from a
single programming interface. The same model definition should run on
CPUs, GPUs, TPUs, mobile devices, and microcontrollers, each with
radically different capabilities, memory constraints, and optimal
execution patterns. Frameworks must provide abstractions that hide this
hardware complexity while enabling efficient utilization.

This abstraction problem has two dimensions:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Data representation}: How should frameworks represent tensors,
  parameters, and computational state in ways that work across hardware?
\item
  \textbf{Execution mapping}: How should high-level operations translate
  to hardware-specific implementations?
\end{enumerate}

The challenge is that these are not independent concerns. The way data
is represented (memory layout, precision, device placement) directly
affects what execution strategies are possible. A tensor stored in
row-major format on a GPU requires different kernels than one in
column-major format on a CPU. A model quantized to INT8 enables entirely
different execution paths than FP32.

Solving the abstraction problem requires sophisticated software
infrastructure: tensor representations that encode both mathematical
semantics and hardware constraints, intermediate representations that
enable hardware-specific compilation, and runtime systems that manage
data movement across the memory hierarchy.

We examine this problem through three lenses. First, we explore how
frameworks represent data through tensor abstractions and the data
structures that organize computation. Second, we investigate how
frameworks manage diverse hardware through explicit device placement,
memory hierarchies, and concurrent execution via streams. Third, we
examine how frameworks organize the operations themselves into efficient
computational primitives that map to hardware capabilities. Together,
these three concerns form a cohesive solution to the abstraction
problem.

\subsection{Data Structures and Tensor
Abstractions}\label{sec-ai-frameworks-data-structures-tensor-abstractions-9cbf}

A ResNet-50 forward pass touches 25.6 million parameters, produces
intermediate activations at every layer, and must coordinate memory
across CPU and GPU address spaces. How do frameworks organize all of
this data so that a single Python call like \texttt{model(input)}
executes millions of operations without the programmer managing a single
pointer?

The answer lies in a hierarchy of specialized data structures.
Computational graphs specify the logical flow of operations, but data
structures determine how those operations access and manipulate data in
physical memory. This distinction matters because the same mathematical
operation can differ by an order of magnitude in throughput depending on
whether data is contiguous in cache, pinned for DMA transfer, or
scattered across pages.

Framework data structures must therefore satisfy three concrete
requirements: sustaining memory bandwidth (hundreds of GB/s on modern
GPUs), accommodating architectures from 1D sequences to 5D video
tensors, and hiding device management behind clean APIs. We examine
tensor abstractions first as the core building blocks, then turn to
parameter management, dataset handling, and execution control.

\subsubsection{Tensors}\label{sec-ai-frameworks-tensors-1cb7}

At the foundation of every framework's data representation lies a single
abstraction: the \textbf{tensor}.

\phantomsection\label{callout-definitionux2a-1.12}
\begin{fbx}{callout-definition}{Definition:}{Tensor}
\phantomsection\label{callout-definition*-1.12}
\textbf{\emph{Tensors}} are the fundamental unit of \textbf{Data
Parallelism}. By abstracting n-dimensional arrays into a unified data
structure with defined \textbf{Strides} and \textbf{Types}, they enable
frameworks to map mathematical operations onto vectorized hardware
instructions without exposing memory layout complexity to the user.

\end{fbx}

Every computation in a neural network operates on
tensors.\sidenote{\textbf{Tensor}: From Latin ``tendere'' (to stretch),
originally describing stress distributions in elastic materials.
Mathematicians Ricci and Levi-Civita formalized tensor calculus in 1900
for Einstein's general relativity, where tensors describe how spacetime
curves. In ML, the term emphasizes that these arrays transform
predictably under coordinate changes, though practitioners primarily use
them as n-dimensional arrays with hardware-optimized operations. }
Training batches, activation maps, parameter gradients, and optimizer
states are all tensors. This unified representation lets frameworks
optimize a single data structure for hardware rather than managing
separate containers for each role.

But how much memory does this single abstraction actually consume? The
answer is far more than the model weights alone suggest, because every
tensor carrying weights has shadow tensors for gradients, optimizer
momentum, and stored activations. The following notebook quantifies this
hidden overhead, which we call the \emph{administrative tax}.

\phantomsection\label{callout-notebookux2a-1.13}
\begin{fbx}{callout-notebook}{AI Engineer’s Notebook:}{The Administrative Tax}
\phantomsection\label{callout-notebook*-1.13}
\textbf{Problem}: Why does your GPU utilization drop when training small
models?

\textbf{The Math (The Hidden Tax)}:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Model Weights}: 2 GB.
\item
  \textbf{Gradients}: 2 GB (same size as weights).
\item
  \textbf{Optimizer States (Adam)}: 8 GB (\(2 \times \text{weights}\)
  for momentum and velocity in FP32).
\item
  \textbf{Activations}: For a batch size of 100 and a 100-layer network,
  you must store every intermediate layer output for the backward pass.

  \[ \text{Activations} \approx \text{Batch} \times \text{Layers} \times \text{Width} \times 2 \text{ bytes} \]
  For a 1024-width model:
  \(100 \times 100 \times 1024 \times 2 \approx \mathbf{20 \text{ GB}}\).
\end{enumerate}

\textbf{The Systems Conclusion}: Your 2 GB model has an
\textbf{``Administrative Tax''} of \textasciitilde30 GB before you even
process the first image. During training, \textbf{Data Movement}
includes saving and retrieving these activations, which is why training
is often 3-4x slower than pure inference.

\end{fbx}

\textbf{Tensor Structure and Dimensions.} A tensor generalizes scalars,
vectors, and matrices to arbitrary dimensions. The hierarchy is
straightforward: a scalar is a rank-0 tensor (single value), a vector is
rank-1 (sequence of values), and a matrix is rank-2 (rows and columns).
Higher ranks extend this pattern through nesting, so a rank-3 tensor is
a stack of matrices, as Figure~\ref{fig-tensor-data-structure-a}
illustrates.

\begin{figure}[htb]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/d5ee2b0ec23d05a040f66bdbc44e07c92114ac76.pdf}}

}

\caption{\label{fig-tensor-data-structure-a}\textbf{Tensor Rank
Hierarchy.} Four shapes illustrating tensor ranks from left to right: a
single value (rank 0, scalar), a column of values (rank 1, vector), a
grid of values (rank 2, matrix), and a cube of values (rank 3,
three-dimensional tensor).}

\end{figure}%

This rank hierarchy maps directly onto ML data. A color image is a
rank-3 tensor: height x width x 3 channels (red, green, blue), as
Figure~\ref{fig-tensor-data-structure-b} illustrates. Stacking a batch
of \(N\) images adds a fourth dimension, producing a rank-4 tensor of
shape \([N, 3, H, W]\). Every convolutional layer in a vision model
consumes and produces tensors of exactly this shape, which is why the
tensor abstraction is so central to framework design.

\begin{figure}[htb]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/2ecf31393ed5f109ff31d8a4765ba86ee0b5fc97.pdf}}

}

\caption{\label{fig-tensor-data-structure-b}\textbf{Image as RGB
Tensor.} Three stacked grids representing the red, green, and blue color
channels of an image, with dimension labels showing width, height, and
channel depth forming a rank-3 tensor. \emph{Credit: Niklas Lang
\url{https://towardsdatascience.com/what-are-tensors-in-machine-learning-5671814646ff}}.}

\end{figure}%

Framework tensors carry more than raw numbers. Each tensor stores
metadata that the runtime uses to validate operations and select fast
execution paths: a \emph{shape} tuple (e.g.,
\texttt{{[}64,\ 3,\ 224,\ 224{]}} for a batch of images), a \emph{dtype}
(float32, float16, int8), and a \emph{device} tag (CPU, cuda:0). A
matrix multiplication, for instance, checks shape compatibility at
dispatch time and uses the dtype to route to the correct hardware
kernel, whether a standard FP32 GEMM or a Tensor Core FP16 path.

Memory layout implementation introduces distinct challenges in tensor
design. While tensors provide an abstraction of multi-dimensional data,
physical computer memory remains linear. Stride patterns address this
disparity by creating mappings between multi-dimensional tensor indices
and linear memory addresses. These patterns significantly impact
computational performance by determining memory access patterns during
tensor operations. Figure~\ref{fig-tensor-memory-layout} demonstrates
this concept using a 2×3 tensor, showing both row-major and column-major
memory layouts with their corresponding stride calculations.

\begin{figure}[htb]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/979816f9beec5e5890152b3506b855f519416115.pdf}}

}

\caption{\label{fig-tensor-memory-layout}\textbf{Tensor Memory Layout}:
A 2×3 tensor can be stored in linear memory using either row-major
(C-style) or column-major (Fortran-style) ordering. Strides define the
number of elements to skip in each dimension when moving through memory,
enabling frameworks to calculate memory addresses for tensor{[}i,j{]} as
base\_address + i×stride{[}0{]} + j×stride{[}1{]}. The choice of memory
layout significantly impacts cache performance and computational
efficiency.}

\end{figure}%

These memory layout patterns are crucial for framework performance
optimization. Row-major layout (used by NumPy, PyTorch) stores elements
row by row, making row-wise operations more cache-friendly. Column-major
layout (used by some BLAS libraries) stores elements column by column,
optimizing column-wise access patterns. The stride values encode this
layout information: in row-major layout for a 2×3 tensor, moving to the
next row requires skipping 3 elements (stride{[}0{]}=3), while moving to
the next column requires skipping 1 element (stride{[}1{]}=1).

Careful alignment of stride patterns with hardware memory hierarchies
maximizes cache efficiency and memory throughput, with optimal layouts
achieving 80-90\% of theoretical memory bandwidth (typically 100-500GB/s
on modern GPUs) compared to suboptimal patterns that may achieve only
20-30\% utilization.

\textbf{Type Systems and Precision.} Tensor implementations use type
systems to control numerical precision and memory consumption. The
standard choice in machine learning has been 32-bit floating-point
numbers (\texttt{float32}), offering a balance of precision and
efficiency. Modern frameworks extend this with multiple numeric types
for different needs. Integer types support indexing and embedding
operations. Reduced-precision types like 16-bit floating-point numbers
enable efficient mobile deployment. 8-bit integers allow fast inference
on specialized hardware.

The choice of numeric type affects both model behavior and computational
efficiency. Neural network training typically requires float32 precision
to maintain stable gradient computations. Inference tasks can often use
lower precision (\texttt{int8} or even \texttt{int4}), reducing memory
usage and increasing processing speed. Mixed-precision training
approaches combine these benefits by using float32 for critical
accumulations while performing most computations at lower precision.

Type conversions between different numeric representations require
careful management. Operating on tensors with different types demands
explicit conversion rules to preserve numerical correctness. These
conversions introduce computational costs and risk precision loss.
Frameworks provide type casting capabilities but rely on developers to
maintain numerical precision across operations.

\subsubsection{Device and Memory
Management}\label{sec-ai-frameworks-device-memory-management-9404}

Every tensor resides on a specific device, and cross-device operations
incur transfer costs that can dominate execution time. PCIe 4.0 delivers
32 GB/s between CPU and GPU, while HBM2e provides over 2 TB/s within the
GPU, a bandwidth gap exceeding 60x. This asymmetry means a single
misplaced tensor transfer can erase the entire speedup from GPU
acceleration. The systems engineer must therefore minimize cross-device
transfers and, when transfers are unavoidable, overlap them with
computation.

Three systems principles govern effective device and memory management:
understanding the bandwidth hierarchy that constrains data movement,
overlapping computation with communication to hide transfer latency, and
using fine-grained synchronization to maintain correctness without
sacrificing concurrency. The remainder of this section develops each
principle, with quantitative analysis grounded in the Iron Law's data
movement term.

The systems principles below build production-level GPU programming
skills. These techniques become critical when optimizing large-scale
training pipelines, implementing custom data transfer strategies, or
diagnosing performance bottlenecks where data transfer latency dominates
execution time. A collapsible notebook at the end of this section
provides the PyTorch API patterns that implement these principles.

\paragraph*{Principle 1: The Device Bandwidth
Hierarchy}\label{sec-frameworks-device-principle-bandwidth}
\addcontentsline{toc}{paragraph}{Principle 1: The Device Bandwidth
Hierarchy}

The cost of moving data between devices varies by orders of magnitude
depending on the interconnect.\sidenote{\textbf{NVLink}: NVIDIA's
high-bandwidth interconnect for GPU-to-GPU communication, providing 600
GB/s bidirectional bandwidth (NVLink 3.0 on A100) compared to 64 GB/s
for PCIe 4.0 x16. Critical for multi-GPU training where gradient
synchronization requires moving gigabytes per iteration. NVSwitch
extends NVLink to connect 8 GPUs in a fully-connected topology (DGX
systems), enabling all-to-all communication without bottlenecks. The 10x
bandwidth advantage over PCIe determines whether tensor parallelism is
practical for a given model size. }
Table~\ref{tbl-device-transfer-overhead} quantifies these costs for a
1000x1000 float32 tensor (4 MB), the size of a typical activation tensor
in a moderately sized model:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2500}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2500}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2500}}@{}}
\caption{\textbf{Device Transfer Overhead.} Transfer time for a 4 MB
tensor across different interconnects. PCIe bandwidth shown is
unidirectional (typical for GPU transfers), with full-duplex operation
providing 2x total bandwidth. NVLink bandwidth is bidirectional (300
GB/s per direction). Transfer times dominate for small operations,
making device placement critical for
performance.}\label{tbl-device-transfer-overhead}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Interconnect}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Bandwidth}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Transfer Time}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Relative to Compute}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Interconnect}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Bandwidth}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Transfer Time}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Relative to Compute}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{PCIe 3.0 x16} & 16 GB/s & 0.25 ms & 10x slower than GPU
compute \\
\textbf{PCIe 4.0 x16} & 32 GB/s & 0.125 ms & 5x slower than GPU
compute \\
\textbf{NVLink 3.0} & 600 GB/s bidirectional & 0.007 ms & Comparable to
GPU compute \\
\textbf{GPU Memory} & 2000 GB/s & 0.002 ms & Optimal \\
\end{longtable}

These numbers connect directly to the Iron Law of performance. Every
cross-device transfer inflates the data movement term (\(D_{vol}/BW\))
at a fraction of the available on-device bandwidth. A PCIe 4.0 transfer
at 32 GB/s means moving a 1 GB activation tensor adds approximately 31
ms to the data movement cost, equivalent to roughly 9.4 trillion
operations on a GPU delivering 300 TFLOPS. For a model forward pass
taking 0.5 ms on GPU, transferring inputs and outputs over PCIe 3.0
doubles the total latency. When batches are small or models are
lightweight, transfer overhead can exceed computation time entirely.

The systems implication is clear: every tensor should reside on the
device where it will be consumed, and transfers should occur only when
unavoidable. Frameworks track device placement for every tensor and
raise errors when operations attempt to combine tensors from different
devices, enforcing this discipline at the API level.

\paragraph*{Principle 2: Overlapping Computation and
Communication}\label{sec-frameworks-device-principle-overlap}
\addcontentsline{toc}{paragraph}{Principle 2: Overlapping Computation
and Communication}

When transfers are unavoidable, the next optimization is to hide their
latency by executing them concurrently with computation. Modern GPUs
contain independent hardware units for computation (SM clusters) and
data transfer (copy engines), enabling true simultaneous execution. The
framework abstraction that exposes this hardware parallelism is the
\emph{CUDA stream}: an independent execution queue where operations
execute sequentially within a stream but concurrently across streams.

Without explicit concurrency control, the GPU serializes all operations
on a single default stream, leaving execution units idle while data
transfers complete. By placing data transfers on one stream and
computation on another, the effective latency approaches the theoretical
minimum of \(\max(\text{compute\_time}, \text{transfer\_time})\) rather
than their sum. Stream-based overlap effectively hides the
\(D_{vol}/BW\) penalty when computation is the longer operation (see
Listing~\ref{lst-overlap-compute-transfer}):

\begin{codelisting}

\caption{\label{lst-overlap-compute-transfer}\textbf{Overlapping
Computation and Transfer}: Use separate streams for data transfer and
computation to hide transfer latency. Pinned memory enables truly
asynchronous non-blocking transfers.}

\centering{

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{compute\_stream }\OperatorTok{=}\NormalTok{ torch.cuda.Stream()}
\NormalTok{transfer\_stream }\OperatorTok{=}\NormalTok{ torch.cuda.Stream()}

\CommentTok{\# Transfer next batch while computing current batch}
\ControlFlowTok{with}\NormalTok{ torch.cuda.stream(transfer\_stream):}
\NormalTok{    next\_batch }\OperatorTok{=}\NormalTok{ next\_batch\_cpu.to(}\StringTok{"cuda"}\NormalTok{, non\_blocking}\OperatorTok{=}\VariableTok{True}\NormalTok{)}

\ControlFlowTok{with}\NormalTok{ torch.cuda.stream(compute\_stream):}
\NormalTok{    output }\OperatorTok{=}\NormalTok{ model(current\_batch)}
\NormalTok{    loss }\OperatorTok{=}\NormalTok{ criterion(output, labels)}

\CommentTok{\# Pinned memory enables non\_blocking transfers}
\NormalTok{x\_pinned }\OperatorTok{=}\NormalTok{ torch.randn(}\DecValTok{1000}\NormalTok{, }\DecValTok{1000}\NormalTok{).pin\_memory()}
\NormalTok{x\_gpu }\OperatorTok{=}\NormalTok{ x\_pinned.to(}\StringTok{"cuda"}\NormalTok{, non\_blocking}\OperatorTok{=}\VariableTok{True}\NormalTok{)  }\CommentTok{\# Asynchronous}

\CommentTok{\# Regular memory requires blocking transfer}
\NormalTok{y\_regular }\OperatorTok{=}\NormalTok{ torch.randn(}\DecValTok{1000}\NormalTok{, }\DecValTok{1000}\NormalTok{)}
\NormalTok{y\_gpu }\OperatorTok{=}\NormalTok{ y\_regular.to(}\StringTok{"cuda"}\NormalTok{, non\_blocking}\OperatorTok{=}\VariableTok{True}\NormalTok{)  }\CommentTok{\# Still blocks}
\end{Highlighting}
\end{Shaded}

}

\end{codelisting}%

The \texttt{non\_blocking=True} flag enables asynchronous transfers that
return immediately without waiting for completion. This works only when
the source tensor uses \emph{pinned memory} (page-locked memory that
enables DMA transfers). Without pinned memory, the transfer blocks even
when \texttt{non\_blocking=True} is specified, because the GPU's copy
engine cannot initiate a DMA transfer from pageable host memory.

This overlap principle extends naturally to pipeline parallelism within
a single node. Different model stages on separate GPUs can process
different microbatches concurrently, with each stage's computation
overlapping the next stage's data reception (see
Listing~\ref{lst-pipeline-parallelism-streams}):

\begin{codelisting}

\caption{\label{lst-pipeline-parallelism-streams}\textbf{Pipeline
Parallelism with Streams}: Overlap multiple model stages across
microbatches using streams and events for inter-stage synchronization.}

\centering{

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Pipeline parallelism: overlap stages across microbatches}
\NormalTok{stages }\OperatorTok{=}\NormalTok{ [Stage1().cuda(), Stage2().cuda(), Stage3().cuda()]}
\NormalTok{streams }\OperatorTok{=}\NormalTok{ [torch.cuda.Stream() }\ControlFlowTok{for}\NormalTok{ \_ }\KeywordTok{in}\NormalTok{ stages]}
\NormalTok{events }\OperatorTok{=}\NormalTok{ [}
\NormalTok{    [torch.cuda.Event() }\ControlFlowTok{for}\NormalTok{ \_ }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(num\_microbatches)]}
    \ControlFlowTok{for}\NormalTok{ \_ }\KeywordTok{in}\NormalTok{ stages}
\NormalTok{]}

\ControlFlowTok{for}\NormalTok{ mb }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(num\_microbatches):}
    \ControlFlowTok{for}\NormalTok{ stage\_idx, (stage, stream) }\KeywordTok{in} \BuiltInTok{enumerate}\NormalTok{(}\BuiltInTok{zip}\NormalTok{(stages, streams)):}
        \ControlFlowTok{with}\NormalTok{ torch.cuda.stream(stream):}
            \ControlFlowTok{if}\NormalTok{ stage\_idx }\OperatorTok{\textgreater{}} \DecValTok{0}\NormalTok{:}
                \CommentTok{\# Wait for previous stage to complete this microbatch}
\NormalTok{                events[stage\_idx }\OperatorTok{{-}} \DecValTok{1}\NormalTok{][mb].wait()}

\NormalTok{            output }\OperatorTok{=}\NormalTok{ stage(inputs[stage\_idx][mb])}
\NormalTok{            events[stage\_idx][mb].record()}
\end{Highlighting}
\end{Shaded}

}

\end{codelisting}%

Extending this pattern across multiple machines requires distributed
training techniques covered in \emph{Distributed Training Systems}
(Volume II), but the single-node implementation above illustrates the
core synchronization principles that underlie all pipeline-parallel
systems.

\paragraph*{Principle 3: Synchronization and
Correctness}\label{sec-frameworks-device-principle-sync}
\addcontentsline{toc}{paragraph}{Principle 3: Synchronization and
Correctness}

\phantomsection\label{sec-ai-frameworks-stream-events}{} Concurrent
execution introduces ordering constraints. When one stream's output
becomes another stream's input, the system must enforce a happens-before
relationship without unnecessarily serializing independent work. Two
synchronization mechanisms exist, with dramatically different
performance implications.

Full device synchronization (\texttt{torch.cuda.synchronize()}) blocks
all streams and the CPU until every queued operation completes. This
creates a global serialization point that eliminates all overlap
benefits. CUDA events provide the alternative: fine-grained
synchronization that blocks only the dependent stream, allowing other
streams and the CPU to continue execution (see
Listing~\ref{lst-cuda-events}):

\begin{codelisting}

\caption{\label{lst-cuda-events}\textbf{CUDA Events for
Synchronization}: Events enable fine-grained producer-consumer patterns
between streams without blocking the entire device.}

\centering{

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Create streams and event}
\NormalTok{stream1 }\OperatorTok{=}\NormalTok{ torch.cuda.Stream()}
\NormalTok{stream2 }\OperatorTok{=}\NormalTok{ torch.cuda.Stream()}
\NormalTok{event }\OperatorTok{=}\NormalTok{ torch.cuda.Event()}

\CommentTok{\# Stream 1: producer}
\ControlFlowTok{with}\NormalTok{ torch.cuda.stream(stream1):}
\NormalTok{    result1 }\OperatorTok{=}\NormalTok{ expensive\_computation(data1)}
\NormalTok{    event.record()  }\CommentTok{\# Mark completion point}

\CommentTok{\# Stream 2: consumer (waits only for stream1\textquotesingle{}s event)}
\ControlFlowTok{with}\NormalTok{ torch.cuda.stream(stream2):}
\NormalTok{    event.wait()  }\CommentTok{\# Block stream2 until event is recorded}
\NormalTok{    result2 }\OperatorTok{=}\NormalTok{ dependent\_computation(result1)  }\CommentTok{\# Safe to use result1}
\end{Highlighting}
\end{Shaded}

}

\end{codelisting}%

The performance difference between these approaches is not incremental
but categorical. Full synchronization after every operation converts a
concurrent pipeline into a sequential one, entirely negating the
hardware parallelism that streams expose. Event-based synchronization
preserves the concurrent execution model while enforcing only the
dependencies that correctness requires. A common mistake in production
code is inserting \texttt{torch.cuda.synchronize()} calls for debugging
and forgetting to remove them, silently converting an overlapped
pipeline into a serialized one. The following examples demonstrate
PyTorch's device management API: device placement
(Listing~\ref{lst-tensor-device-placement}), CUDA contexts
(Listing~\ref{lst-cuda-device-context}), error handling
(Listing~\ref{lst-device-mismatch-error}), and profiling
(Listing~\ref{lst-pytorch-profiler}).

\textbf{Device Placement with .to().} Every tensor has a device
attribute. The \texttt{.to()} method moves tensors between devices with
copy-on-write semantics:

\begin{codelisting}

\caption{\label{lst-tensor-device-placement}\textbf{Tensor Device
Placement}: The .to() method moves tensors between CPU and GPU devices,
with copy-on-write semantics that avoid unnecessary copies when the
tensor is already on the target device.}

\centering{

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ torch}

\CommentTok{\# Create tensor on CPU (default)}
\NormalTok{x }\OperatorTok{=}\NormalTok{ torch.randn(}\DecValTok{1024}\NormalTok{, }\DecValTok{1024}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(x.device)  }\CommentTok{\# cpu}

\CommentTok{\# Move to GPU}
\NormalTok{x\_gpu }\OperatorTok{=}\NormalTok{ x.to(}\StringTok{"cuda"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(x\_gpu.device)  }\CommentTok{\# cuda:0}

\CommentTok{\# Move to specific GPU}
\NormalTok{x\_gpu1 }\OperatorTok{=}\NormalTok{ x.to(}\StringTok{"cuda:1"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(x\_gpu1.device)  }\CommentTok{\# cuda:1}

\CommentTok{\# Copy{-}on{-}write: no copy if already on target device}
\NormalTok{x }\OperatorTok{=}\NormalTok{ torch.randn(}\DecValTok{100}\NormalTok{, device}\OperatorTok{=}\StringTok{"cuda"}\NormalTok{)}
\NormalTok{y }\OperatorTok{=}\NormalTok{ x.to(}\StringTok{"cuda"}\NormalTok{)  }\CommentTok{\# No copy, returns x}
\ControlFlowTok{assert}\NormalTok{ y.data\_ptr() }\OperatorTok{==}\NormalTok{ x.data\_ptr()  }\CommentTok{\# Same underlying memory}

\CommentTok{\# But dtype changes always create copies}
\NormalTok{x }\OperatorTok{=}\NormalTok{ torch.randn(}\DecValTok{100}\NormalTok{, device}\OperatorTok{=}\StringTok{"cuda"}\NormalTok{, dtype}\OperatorTok{=}\NormalTok{torch.float32)}
\NormalTok{y }\OperatorTok{=}\NormalTok{ x.to(device}\OperatorTok{=}\StringTok{"cuda"}\NormalTok{, dtype}\OperatorTok{=}\NormalTok{torch.float16)  }\CommentTok{\# Creates copy}
\ControlFlowTok{assert}\NormalTok{ y.data\_ptr() }\OperatorTok{!=}\NormalTok{ x.data\_ptr()  }\CommentTok{\# Different memory}
\end{Highlighting}
\end{Shaded}

}

\end{codelisting}%

Calling \texttt{.to(device)} multiple times is safe and efficient due to
the copy-on-write behavior.

\textbf{CUDA Contexts and Current Device.} Each host thread has an
associated current device for GPU allocations:

\begin{codelisting}

\caption{\label{lst-cuda-device-context}\textbf{CUDA Device Context}:
Each thread has a current device for GPU allocations. Use set\_device()
for global changes or context managers for scoped changes that
automatically restore the previous device.}

\centering{

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Check current device}
\BuiltInTok{print}\NormalTok{(torch.cuda.current\_device())  }\CommentTok{\# 0}

\CommentTok{\# Allocate on current device}
\NormalTok{x }\OperatorTok{=}\NormalTok{ torch.randn(}\DecValTok{100}\NormalTok{, device}\OperatorTok{=}\StringTok{"cuda"}\NormalTok{)  }\CommentTok{\# Uses cuda:0}
\BuiltInTok{print}\NormalTok{(x.device)  }\CommentTok{\# cuda:0}

\CommentTok{\# Change current device}
\NormalTok{torch.cuda.set\_device(}\DecValTok{1}\NormalTok{)}
\NormalTok{y }\OperatorTok{=}\NormalTok{ torch.randn(}\DecValTok{100}\NormalTok{, device}\OperatorTok{=}\StringTok{"cuda"}\NormalTok{)  }\CommentTok{\# Uses cuda:1}
\BuiltInTok{print}\NormalTok{(y.device)  }\CommentTok{\# cuda:1}

\CommentTok{\# Context manager for scoped device changes}
\BuiltInTok{print}\NormalTok{(torch.cuda.current\_device())  }\CommentTok{\# 0}

\ControlFlowTok{with}\NormalTok{ torch.cuda.device(}\DecValTok{1}\NormalTok{):}
\NormalTok{    x }\OperatorTok{=}\NormalTok{ torch.randn(}\DecValTok{100}\NormalTok{, device}\OperatorTok{=}\StringTok{"cuda"}\NormalTok{)}
    \BuiltInTok{print}\NormalTok{(x.device)  }\CommentTok{\# cuda:1}
    \BuiltInTok{print}\NormalTok{(torch.cuda.current\_device())  }\CommentTok{\# 1}

\BuiltInTok{print}\NormalTok{(torch.cuda.current\_device())  }\CommentTok{\# 0 (restored)}
\end{Highlighting}
\end{Shaded}

}

\end{codelisting}%

\textbf{Device Mismatch Errors.} Operations on tensors from different
devices raise errors, enforcing the placement discipline described in
Principle 1:

\begin{codelisting}

\caption{\label{lst-device-mismatch-error}\textbf{Device Mismatch
Error}: Operations on tensors from different devices raise RuntimeError.
Always ensure tensors are on the same device before performing
operations.}

\centering{

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{torch.cuda.set\_device(}\DecValTok{0}\NormalTok{)}
\NormalTok{x }\OperatorTok{=}\NormalTok{ torch.randn(}\DecValTok{100}\NormalTok{, device}\OperatorTok{=}\StringTok{"cuda:0"}\NormalTok{)}
\NormalTok{y }\OperatorTok{=}\NormalTok{ torch.randn(}\DecValTok{100}\NormalTok{, device}\OperatorTok{=}\StringTok{"cuda:1"}\NormalTok{)}

\CommentTok{\# This raises an error {-} tensors on different devices}
\ControlFlowTok{try}\NormalTok{:}
\NormalTok{    z }\OperatorTok{=}\NormalTok{ x }\OperatorTok{+}\NormalTok{ y}
\ControlFlowTok{except} \PreprocessorTok{RuntimeError} \ImportTok{as}\NormalTok{ e:}
    \BuiltInTok{print}\NormalTok{(e)  }\CommentTok{\# Expected all tensors to be on the same device}
\end{Highlighting}
\end{Shaded}

}

\end{codelisting}%

\textbf{Device Placement Patterns.} Minimizing transfers requires
consistent device placement and memory reuse.
Listing~\ref{lst-device-placement-reuse} shows the difference between
repeated transfers and memory reuse.

\begin{codelisting}

\caption{\label{lst-device-placement-reuse}\textbf{Device Placement and
Memory Reuse}: Reusing GPU memory avoids costly per-iteration transfers
that can bottleneck training loops.}

\centering{

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Bad: repeated transfers}
\ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\DecValTok{1000}\NormalTok{):}
\NormalTok{    x\_cpu }\OperatorTok{=}\NormalTok{ torch.randn(}\DecValTok{100}\NormalTok{, }\DecValTok{100}\NormalTok{)}
\NormalTok{    x\_gpu }\OperatorTok{=}\NormalTok{ x\_cpu.to(}\StringTok{"cuda"}\NormalTok{)  }\CommentTok{\# Transfer every iteration}
\NormalTok{    y }\OperatorTok{=}\NormalTok{ model(x\_gpu)}
\NormalTok{    y\_cpu }\OperatorTok{=}\NormalTok{ y.to(}\StringTok{"cpu"}\NormalTok{)  }\CommentTok{\# Transfer every iteration}

\CommentTok{\# Good: reuse GPU memory}
\NormalTok{x\_gpu }\OperatorTok{=}\NormalTok{ torch.empty(}\DecValTok{100}\NormalTok{, }\DecValTok{100}\NormalTok{, device}\OperatorTok{=}\StringTok{"cuda"}\NormalTok{)}
\ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\DecValTok{1000}\NormalTok{):}
\NormalTok{    x\_gpu.copy\_(generate\_batch())  }\CommentTok{\# Reuse allocated memory}
\NormalTok{    y }\OperatorTok{=}\NormalTok{ model(x\_gpu)}
\end{Highlighting}
\end{Shaded}

}

\end{codelisting}%

Colocating all tensors on the same device prevents implicit transfers,
as Listing~\ref{lst-consistent-device-placement} demonstrates.

\begin{codelisting}

\caption{\label{lst-consistent-device-placement}\textbf{Consistent
Device Placement}: Ensuring all tensors are on the same device
eliminates implicit transfers that degrade performance.}

\centering{

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Bad: mixed device placement}
\NormalTok{model }\OperatorTok{=}\NormalTok{ Model().to(}\StringTok{"cuda"}\NormalTok{)}
\NormalTok{inputs }\OperatorTok{=}\NormalTok{ torch.randn(}\DecValTok{32}\NormalTok{, }\DecValTok{784}\NormalTok{, device}\OperatorTok{=}\StringTok{"cpu"}\NormalTok{)}
\NormalTok{labels }\OperatorTok{=}\NormalTok{ torch.randn(}\DecValTok{32}\NormalTok{, }\DecValTok{10}\NormalTok{, device}\OperatorTok{=}\StringTok{"cuda"}\NormalTok{)}

\NormalTok{outputs }\OperatorTok{=}\NormalTok{ model(inputs)  }\CommentTok{\# Implicit transfer of inputs}
\NormalTok{loss }\OperatorTok{=}\NormalTok{ criterion(outputs, labels)}

\CommentTok{\# Good: consistent device placement}
\NormalTok{model }\OperatorTok{=}\NormalTok{ Model().to(}\StringTok{"cuda"}\NormalTok{)}
\NormalTok{inputs }\OperatorTok{=}\NormalTok{ torch.randn(}\DecValTok{32}\NormalTok{, }\DecValTok{784}\NormalTok{, device}\OperatorTok{=}\StringTok{"cuda"}\NormalTok{)}
\NormalTok{labels }\OperatorTok{=}\NormalTok{ torch.randn(}\DecValTok{32}\NormalTok{, }\DecValTok{10}\NormalTok{, device}\OperatorTok{=}\StringTok{"cuda"}\NormalTok{)}

\NormalTok{outputs }\OperatorTok{=}\NormalTok{ model(inputs)  }\CommentTok{\# No transfers}
\NormalTok{loss }\OperatorTok{=}\NormalTok{ criterion(outputs, labels)}
\end{Highlighting}
\end{Shaded}

}

\end{codelisting}%

Module \texttt{.to()} recursively moves all parameters and buffers, as
Listing~\ref{lst-module-to-recursive} shows.

\begin{codelisting}

\caption{\label{lst-module-to-recursive}\textbf{Recursive Module
Transfer}: The .to() method recursively moves all parameters and buffers
in a module hierarchy to the target device.}

\centering{

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model }\OperatorTok{=}\NormalTok{ Model()}
\BuiltInTok{print}\NormalTok{(}\BuiltInTok{next}\NormalTok{(model.parameters()).device)  }\CommentTok{\# cpu}

\NormalTok{model }\OperatorTok{=}\NormalTok{ model.to(}\StringTok{"cuda"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\BuiltInTok{next}\NormalTok{(model.parameters()).device)  }\CommentTok{\# cuda:0}
\end{Highlighting}
\end{Shaded}

}

\end{codelisting}%

\textbf{Synchronization Patterns.} Avoid full synchronization when
events suffice. Listing~\ref{lst-sync-patterns} shows the performance
difference between full synchronization and event-based coordination.

\begin{codelisting}

\caption{\label{lst-sync-patterns}\textbf{Synchronization Patterns}:
Event-based synchronization preserves parallelism while full
synchronization serializes all computation.}

\centering{

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Bad: serializes all computation}
\ControlFlowTok{with}\NormalTok{ torch.cuda.stream(stream1):}
\NormalTok{    result1 }\OperatorTok{=}\NormalTok{ computation1(data)}
\NormalTok{torch.cuda.synchronize()  }\CommentTok{\# Blocks everything}

\ControlFlowTok{with}\NormalTok{ torch.cuda.stream(stream2):}
\NormalTok{    result2 }\OperatorTok{=}\NormalTok{ computation2(result1)}
\NormalTok{torch.cuda.synchronize()  }\CommentTok{\# Blocks everything again}

\CommentTok{\# Good: allows overlap where possible}
\ControlFlowTok{with}\NormalTok{ torch.cuda.stream(stream1):}
\NormalTok{    result1 }\OperatorTok{=}\NormalTok{ computation1(data)}
\NormalTok{    event.record()}

\ControlFlowTok{with}\NormalTok{ torch.cuda.stream(stream2):}
\NormalTok{    event.wait()  }\CommentTok{\# Only blocks stream2}
\NormalTok{    result2 }\OperatorTok{=}\NormalTok{ computation2(result1)}
\CommentTok{\# Other streams and CPU can continue working}
\end{Highlighting}
\end{Shaded}

}

\end{codelisting}%

\textbf{Profiling Transfer Bottlenecks.} PyTorch's built-in profiler
captures CPU-GPU transfers as Chrome traces:

\begin{codelisting}

\caption{\label{lst-pytorch-profiler}\textbf{PyTorch Profiler}: Capture
CPU-GPU transfers and operations as a Chrome trace for visualization and
analysis of transfer bottlenecks.}

\centering{

\begin{Shaded}
\begin{Highlighting}[]
\ControlFlowTok{with}\NormalTok{ torch.profiler.profile(}
\NormalTok{    activities}\OperatorTok{=}\NormalTok{[}
\NormalTok{        torch.profiler.ProfilerActivity.CPU,}
\NormalTok{        torch.profiler.ProfilerActivity.CUDA,}
\NormalTok{    ],}
\NormalTok{    with\_stack}\OperatorTok{=}\VariableTok{True}\NormalTok{,}
\NormalTok{) }\ImportTok{as}\NormalTok{ prof:}
\NormalTok{    x }\OperatorTok{=}\NormalTok{ torch.randn(}\DecValTok{1000}\NormalTok{, }\DecValTok{1000}\NormalTok{, device}\OperatorTok{=}\StringTok{"cpu"}\NormalTok{)}
\NormalTok{    x\_gpu }\OperatorTok{=}\NormalTok{ x.to(}\StringTok{"cuda"}\NormalTok{)}
\NormalTok{    y }\OperatorTok{=}\NormalTok{ x\_gpu }\OperatorTok{@}\NormalTok{ x\_gpu.T}

\NormalTok{prof.export\_chrome\_trace(}\StringTok{"trace.json"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

}

\end{codelisting}%

\textbf{GPU-Level Profiling with NVIDIA Nsight.} For hardware-level
bottleneck diagnosis, NVIDIA provides two complementary tools. Nsight
Systems captures system-wide timelines correlating CPU activity, GPU
kernel execution, and memory transfers. Listing~\ref{lst-nsight-systems}
shows common profiling commands.

\begin{codelisting}

\caption{\label{lst-nsight-systems}\textbf{Nsight Systems Profiling}:
Capture system-wide timelines correlating CPU activity, GPU kernel
execution, and memory transfers.}

\centering{

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Profile entire training script}
\ExtensionTok{nsys}\NormalTok{ profile }\AttributeTok{{-}o}\NormalTok{ training\_profile python train.py}

\CommentTok{\# Common options for ML workloads}
\ExtensionTok{nsys}\NormalTok{ profile }\DataTypeTok{\textbackslash{}}
    \AttributeTok{{-}{-}trace}\OperatorTok{=}\NormalTok{cuda,nvtx,osrt }\DataTypeTok{\textbackslash{}}
    \AttributeTok{{-}{-}cuda{-}memory{-}usage}\OperatorTok{=}\NormalTok{true }\DataTypeTok{\textbackslash{}}
    \AttributeTok{{-}{-}output}\OperatorTok{=}\NormalTok{profile\_output }\DataTypeTok{\textbackslash{}}
\NormalTok{    python train.py}
\end{Highlighting}
\end{Shaded}

}

\end{codelisting}%

Nsight Compute provides kernel-level analysis with hardware counters, as
Listing~\ref{lst-nsight-compute} demonstrates.
Table~\ref{tbl-nsight-metrics} lists the key metrics to examine when
optimizing ML kernels.

\begin{codelisting}

\caption{\label{lst-nsight-compute}\textbf{Nsight Compute Profiling}:
Profile specific kernels with detailed hardware metrics for optimization
analysis.}

\centering{

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Profile specific kernels with detailed metrics}
\ExtensionTok{ncu} \AttributeTok{{-}{-}target{-}processes}\NormalTok{ all }\DataTypeTok{\textbackslash{}}
    \AttributeTok{{-}{-}set}\NormalTok{ full }\DataTypeTok{\textbackslash{}}
    \AttributeTok{{-}{-}output}\NormalTok{ kernel\_analysis }\DataTypeTok{\textbackslash{}}
\NormalTok{    python inference.py}
\end{Highlighting}
\end{Shaded}

}

\end{codelisting}%

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3333}}@{}}
\caption{\textbf{Nsight Compute Metrics.} Key metrics for ML kernel
optimization. Low values indicate specific optimization opportunities.
Nsight Systems identifies which kernels dominate runtime, and Nsight
Compute reveals why those kernels
underperform.}\label{tbl-nsight-metrics}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Metric}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Meaning}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Optimization Target}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Metric}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Meaning}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Optimization Target}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{SM Occupancy} & Active warps / maximum warps & Increase
parallelism if low \\
\textbf{Memory Throughput} & Achieved / peak bandwidth & Optimize memory
access patterns \\
\textbf{Compute Throughput} & Achieved / peak FLOPS & Reduce memory
bottlenecks \\
\textbf{Tensor Core Active} & Time in Tensor Core ops & Verify
mixed-precision utilization \\
\end{longtable}

\subsubsection{Domain-Specific Data
Organizations}\label{sec-ai-frameworks-domainspecific-data-organizations-48d9}

The device bandwidth hierarchy and overlap techniques above govern how
tensors move between devices. With these hardware-level constraints in
place, we turn to the higher-level structures that organize data for
machine learning workflows. The core systems principle is
straightforward: the data pipeline must sustain the accelerator's
consumption rate. A GPU processing 1,000 images per second at 224x224
resolution requires approximately 150 MB/s of sustained data throughput.
If the pipeline cannot maintain this rate, the accelerator idles and the
effective utilization term in the Iron Law drops below 1.

Frameworks address this throughput requirement through three mechanisms.
The first is \emph{parallel worker processes}: the DataLoader spawns
multiple CPU processes, each independently loading and preprocessing
samples. Because data loading involves disk I/O and CPU-bound
transformations (decoding, augmentation, normalization), a single
process cannot saturate a modern GPU. Multiple workers overlap I/O wait
times with preprocessing computation, collectively sustaining throughput
that no single process could achieve. When
\texttt{num\_workers\ \textgreater{}\ 0}, the DataLoader distributes
sample indices across workers through a shared queue, and workers push
completed samples to a data queue that the main process assembles into
batches.

The second mechanism is \emph{prefetching}. The
\texttt{prefetch\_factor} parameter (default 2) controls how many
batches each worker prepares in advance. With 4 workers and
\texttt{prefetch\_factor=2}, the pipeline maintains 8 batches in flight,
ensuring the GPU never stalls waiting for data. While the model
processes batch \(N\) on the GPU, workers simultaneously load and
preprocess batch \(N+1\) through \(N+8\) on CPUs, effectively hiding
data loading latency behind computation. The cost is memory consumption
proportional to batch size times prefetch depth.

The third mechanism is \emph{pinned memory for DMA transfers}. The
\texttt{pin\_memory=True} option allocates batch data in page-locked
(pinned) host memory rather than pageable memory. Pageable memory can be
swapped to disk by the operating system, forcing the CUDA runtime to
first copy data to a temporary pinned buffer before initiating the GPU
transfer. Pinned memory bypasses this intermediate copy, enabling direct
memory access (DMA) transfers where the GPU's memory controller reads
directly from host memory while the CPU continues other work. For a
batch of 64 images at 224x224x3 resolution (37 MB), pinned memory
transfer takes approximately 0.5 ms over PCIe 4.0 x16 (31.5 GB/s)
compared to 1.5 ms with pageable memory, a 2 to 3x speedup. The cost is
reduced available system memory, as pinned pages cannot be swapped.

These three mechanisms appear together in the DataLoader configuration.
Listing~\ref{lst-dataloader-throughput} shows a typical setup where
\texttt{num\_workers} enables parallel loading,
\texttt{prefetch\_factor} controls pipeline depth, and
\texttt{pin\_memory} enables DMA transfers. Each parameter maps directly
to one of the throughput principles above:

\begin{codelisting}

\caption{\label{lst-dataloader-throughput}\textbf{DataLoader Throughput
Configuration}: Each parameter addresses a specific throughput
bottleneck. num\_workers parallelizes I/O and preprocessing across CPU
cores, prefetch\_factor controls pipeline depth, and pin\_memory enables
DMA transfers to the GPU.}

\centering{

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ torch.utils.data }\ImportTok{import}\NormalTok{ DataLoader}

\NormalTok{loader }\OperatorTok{=}\NormalTok{ DataLoader(}
\NormalTok{    dataset,}
\NormalTok{    batch\_size}\OperatorTok{=}\DecValTok{64}\NormalTok{,}
\NormalTok{    shuffle}\OperatorTok{=}\VariableTok{True}\NormalTok{,}
\NormalTok{    num\_workers}\OperatorTok{=}\DecValTok{4}\NormalTok{,  }\CommentTok{\# Parallel worker processes (mechanism 1)}
\NormalTok{    prefetch\_factor}\OperatorTok{=}\DecValTok{2}\NormalTok{,  }\CommentTok{\# Batches prepared ahead per worker (mechanism 2)}
\NormalTok{    pin\_memory}\OperatorTok{=}\VariableTok{True}\NormalTok{,  }\CommentTok{\# Page{-}locked memory for DMA (mechanism 3)}
\NormalTok{    worker\_init\_fn}\OperatorTok{=}\NormalTok{seed\_worker,  }\CommentTok{\# Reproducible augmentation per worker}
\NormalTok{)}

\CommentTok{\# Pipeline effect: while GPU processes batch N,}
\CommentTok{\# 4 workers load batches N+1..N+8 into pinned memory,}
\CommentTok{\# ready for DMA transfer when the GPU finishes.}
\end{Highlighting}
\end{Shaded}

}

\end{codelisting}%

A practical starting point is setting \texttt{num\_workers} equal to the
number of available CPU cores. The optimal value depends on whether
loading is I/O-bound or CPU-bound. For I/O-bound workloads such as
reading images from network storage, more workers overlap disk latency
and improve throughput. For CPU-bound workloads involving heavy
augmentation, the benefit saturates once all cores are utilized. Too
many workers waste memory, since each maintains a copy of the Dataset
object.

Worker process management introduces several subtle issues. Because
workers are separate processes, random number generators used in data
augmentation must be explicitly seeded per worker via
\texttt{worker\_init\_fn} to ensure reproducibility. Without proper
seeding, workers may produce identical augmentation sequences, reducing
effective data diversity. Shared state between workers presents a
separate challenge: each worker has its own memory space, so
modifications to global variables in one worker do not propagate to
others or to the main process. For large datasets where caching matters,
memory-mapped files or shared memory regions that persist across
processes are the standard solution. The following examples demonstrate
Dataset and DataLoader patterns: map-style datasets
(Listing~\ref{lst-map-style-dataset}), iterable datasets
(Listing~\ref{lst-iterable-dataset}), and custom collation
(Listing~\ref{lst-custom-collate-fn}).

The DataLoader wraps a Dataset object that defines how individual
samples are accessed. PyTorch supports two dataset paradigms. Map-style
datasets implement \texttt{\_\_len\_\_} and \texttt{\_\_getitem\_\_},
enabling random access to samples by index. This pattern works well for
datasets that fit in memory or support efficient random access on disk.
Iterable-style datasets implement \texttt{\_\_iter\_\_} instead,
yielding samples sequentially for streaming data sources where random
access is impractical.

\begin{codelisting}

\caption{\label{lst-map-style-dataset}\textbf{Map-Style Dataset}:
Implement \textbf{len} and \textbf{getitem} for random access to samples
by index, enabling shuffling and efficient batching.}

\centering{

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ torch.utils.data }\ImportTok{import}\NormalTok{ Dataset}


\KeywordTok{class}\NormalTok{ ImageDataset(Dataset):}
    \KeywordTok{def} \FunctionTok{\_\_init\_\_}\NormalTok{(}\VariableTok{self}\NormalTok{, image\_paths, labels, transform}\OperatorTok{=}\VariableTok{None}\NormalTok{):}
        \VariableTok{self}\NormalTok{.image\_paths }\OperatorTok{=}\NormalTok{ image\_paths}
        \VariableTok{self}\NormalTok{.labels }\OperatorTok{=}\NormalTok{ labels}
        \VariableTok{self}\NormalTok{.transform }\OperatorTok{=}\NormalTok{ transform}

    \KeywordTok{def} \FunctionTok{\_\_len\_\_}\NormalTok{(}\VariableTok{self}\NormalTok{):}
        \ControlFlowTok{return} \BuiltInTok{len}\NormalTok{(}\VariableTok{self}\NormalTok{.image\_paths)  }\CommentTok{\# Total number of samples}

    \KeywordTok{def} \FunctionTok{\_\_getitem\_\_}\NormalTok{(}\VariableTok{self}\NormalTok{, idx):}
\NormalTok{        image }\OperatorTok{=}\NormalTok{ Image.}\BuiltInTok{open}\NormalTok{(}\VariableTok{self}\NormalTok{.image\_paths[idx])}
\NormalTok{        label }\OperatorTok{=} \VariableTok{self}\NormalTok{.labels[idx]}
        \ControlFlowTok{if} \VariableTok{self}\NormalTok{.transform:}
\NormalTok{            image }\OperatorTok{=} \VariableTok{self}\NormalTok{.transform(image)}
        \ControlFlowTok{return}\NormalTok{ image, label}
\end{Highlighting}
\end{Shaded}

}

\end{codelisting}%

\begin{codelisting}

\caption{\label{lst-iterable-dataset}\textbf{Iterable Dataset}:
Implement \textbf{iter} for streaming data sources where random access
is impractical, enabling processing of arbitrarily large datasets.}

\centering{

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ torch.utils.data }\ImportTok{import}\NormalTok{ IterableDataset}


\KeywordTok{class}\NormalTok{ StreamingDataset(IterableDataset):}
    \KeywordTok{def} \FunctionTok{\_\_init\_\_}\NormalTok{(}\VariableTok{self}\NormalTok{, file\_path):}
        \VariableTok{self}\NormalTok{.file\_path }\OperatorTok{=}\NormalTok{ file\_path}

    \KeywordTok{def} \FunctionTok{\_\_iter\_\_}\NormalTok{(}\VariableTok{self}\NormalTok{):}
        \ControlFlowTok{with} \BuiltInTok{open}\NormalTok{(}\VariableTok{self}\NormalTok{.file\_path, }\StringTok{"r"}\NormalTok{) }\ImportTok{as}\NormalTok{ f:}
            \ControlFlowTok{for}\NormalTok{ line }\KeywordTok{in}\NormalTok{ f:}
                \ControlFlowTok{yield}\NormalTok{ parse\_line(line)  }\CommentTok{\# Stream samples one at a time}
\end{Highlighting}
\end{Shaded}

}

\end{codelisting}%

The \texttt{collate\_fn} parameter determines how individual samples are
combined into batches. The default collation stacks tensors along a new
batch dimension, which works well when all samples have identical
shapes. For variable-length data such as text sequences or audio clips,
custom collation functions handle padding, sorting by length, or
creating attention masks. Efficient collation minimizes wasted
computation on padding tokens and can significantly impact both memory
usage and training throughput.

\begin{codelisting}

\caption{\label{lst-custom-collate-fn}\textbf{Custom Collate Function}:
Handle variable-length data by implementing custom padding and batch
assembly logic.}

\centering{

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ collate\_variable\_length(batch):}
\NormalTok{    sequences, labels }\OperatorTok{=} \BuiltInTok{zip}\NormalTok{(}\OperatorTok{*}\NormalTok{batch)}
\NormalTok{    lengths }\OperatorTok{=}\NormalTok{ [}\BuiltInTok{len}\NormalTok{(seq) }\ControlFlowTok{for}\NormalTok{ seq }\KeywordTok{in}\NormalTok{ sequences]}
    \CommentTok{\# Pad sequences to maximum length in batch}
\NormalTok{    padded }\OperatorTok{=}\NormalTok{ pad\_sequence(sequences, batch\_first}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
    \ControlFlowTok{return}\NormalTok{ padded, torch.tensor(labels), torch.tensor(lengths)}


\NormalTok{loader }\OperatorTok{=}\NormalTok{ DataLoader(dataset, collate\_fn}\OperatorTok{=}\NormalTok{collate\_variable\_length)}
\end{Highlighting}
\end{Shaded}

}

\end{codelisting}%

\textbf{Parameter Structures.} A GPT-3 scale model stores 175 billion
parameters, occupying 350 GB in FP16. Managing these parameters across
devices, keeping gradients synchronized, and maintaining optimizer state
(which can triple the memory footprint, as the Administrative Tax
notebook showed) is a core framework responsibility.

Unlike dataset samples, which flow through the pipeline transiently,
parameters persist throughout training and inference. Frameworks
organize them into compact structures that minimize memory while
enabling fast read and write access
(\citeproc{ref-li2014communication}{Li et al. 2014}). During multi-GPU
training, frameworks may replicate parameters across devices for
parallel computation while keeping a synchronized master copy.
Synchronizing multi-billion parameter models can require transferring
tens of GB of gradients per step, which is why frameworks implement
gradient compression and efficient communication patterns like ring
all-reduce.

Parameter structures must also adapt to varying precision requirements.
Training typically uses FP32 for gradient stability, but inference and
large-scale training increasingly use FP16 or INT8. Frameworks implement
type casting and mixed-precision management to enable these
optimizations without compromising numerical accuracy.

\textbf{Distributed Execution Contexts.} The computational graph defines
\emph{what} to compute, but \emph{where} and \emph{how} that computation
runs across devices is the job of execution contexts. On a single node,
execution contexts manage CUDA streams and events (discussed in
\textbf{?@sec-frameworks-device-principle-sync}) to overlap computation
and data transfer across GPUs.

When training scales beyond a single machine, these same abstractions
extend to manage process groups and communication primitives. Frameworks
use constructs like \texttt{ProcessGroup} (PyTorch) or \texttt{Mesh}
(JAX) to define how devices communicate, maintaining state for
collective operations such as AllReduce that synchronize gradients
across thousands of GPUs. This includes partitioning computational
graphs, synchronizing gradients, and redistributing data as needed.

We introduce these concepts here because they shape framework API design
even for single-node code. The implementation details of distributed
training, including gradient compression, communication topologies, and
fault tolerance, are covered in Volume II (\emph{Distributed Training
Systems}).

Figure~\ref{fig-3d-parallelism} visualizes how large-scale training
distributes computation. The grid layout illustrates three orthogonal
scaling axes: \textbf{Data Parallelism} (replicating the model across
columns), \textbf{Pipeline Parallelism} (splitting layers across rows),
and \textbf{Model Parallelism} (sharding tensors within each cluster).
This ``3D'' approach allows frameworks to scale beyond the memory limits
of any single device
(\citeproc{ref-mcmahan2023communicationefficient}{McMahan et al. 2017}).

\begin{figure}[htb]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/46709c04770a517e410cf38e45346963db386179.pdf}}

}

\caption{\label{fig-3d-parallelism}\textbf{3D Parallelism.} A grid of
eight accelerator clusters arranged in two rows and four columns, each
containing stacked computational units. Distinct colors encode the three
parallelism dimensions: data parallelism across columns, pipeline
parallelism across rows, and model parallelism within each cluster.}

\end{figure}%

\subsection{Core
Operations}\label{sec-ai-frameworks-core-operations-914f}

When you write \texttt{y\ =\ torch.matmul(x,\ w)}, what actually happens
between Python and the GPU? The answer involves three distinct layers
working in coordination. Figure~\ref{fig-mlfm-core-ops} illustrates how
hardware abstraction operations manage computing platform complexity,
basic numerical operations implement mathematical computations, and
system-level operations coordinate resources and execution.

\begin{figure}[htb]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/895676f22c8cace42b91b8e678c916e6b6af83a5.pdf}}

}

\caption{\label{fig-mlfm-core-ops}\textbf{Core Operations Stack.} Three
grouped layers showing how frameworks bridge Python code to hardware.
The top layer contains system-level operations (scheduling, memory
management, resource optimization), the middle layer holds numerical
operations (GEMM, BLAS, element-wise), and the bottom layer provides
hardware abstraction (kernel management, memory abstraction, execution
control).}

\end{figure}%

\textbf{Hardware Abstraction Operations.} The hardware abstraction layer
isolates framework code from platform-specific details. It solves three
concrete problems: selecting the right compute kernel, moving data
through the memory hierarchy, and coordinating execution across
processing units.

\textbf{Compute Kernel Management.} The kernel manager dispatches each
operation to the fastest available implementation for the current
hardware. When a framework encounters a matrix multiplication, it
selects among AVX-512 vector instructions on modern CPUs,
\href{https://developer.nvidia.com/cublas}{cuBLAS} on NVIDIA GPUs, or
dedicated tensor processing instructions on AI accelerators. The
dispatch decision depends on input dimensions, data layout, and hardware
capabilities. A 4096x4096 GEMM on an A100 GPU routes to cuBLAS Tensor
Core kernels that sustain over 300 TFLOPS in FP16, while the same
operation on a CPU falls back to an AVX-512 path at roughly 2 TFLOPS.
When no specialized kernel exists, the manager falls back to a generic
implementation rather than failing.

\textbf{Memory System Abstraction.} The memory abstraction layer moves
tensors between device types (CPU registered memory, GPU pinned memory,
unified memory) and transforms data layouts to match hardware
preferences. A convolutional layer, for example, may store activations
in NCHW format (batch, channels, height, width) on NVIDIA GPUs but
convert to NHWC for Apple's Metal backend. Alignment requirements vary
from 4 bytes on CPUs to 128 bytes on some accelerators, and misaligned
access can halve effective memory bandwidth. The layer also enforces
cache coherency when multiple execution units read and write the same
tensor, preventing silent data corruption during concurrent operations.

\textbf{Execution Control.} The execution controller coordinates work
across multiple processing units and memory spaces. On a modern GPU,
this means managing dozens of concurrent CUDA streams: when two
independent convolutions are both ready to execute, the controller
launches them on separate streams so they overlap on the GPU's streaming
multiprocessors, improving utilization from as low as 40\% (sequential)
to over 80\% (concurrent). The controller inserts synchronization
barriers only where true data dependencies exist, tracks event
completions to trigger dependent operations, and routes hardware errors
(ECC failures, timeout watchdogs) to the framework's error handling
path.

\textbf{Basic Numerical Operations.} With hardware abstraction managing
the platform-specific details, frameworks build a layer of mathematical
operations on top. General Matrix Multiply (GEMM) dominates ML
computation. The operation C = \(\alpha\)AB + \(\beta\)C accounts for
the vast majority of arithmetic in neural networks: a single ResNet-50
forward pass performs approximately 4 billion multiply-accumulate
operations, nearly all of which reduce to GEMM. Frameworks optimize GEMM
through cache-aware tiling (splitting matrices into blocks that fit in
L1/L2 cache), loop unrolling for instruction-level parallelism, and
shape-specific kernels. Fully connected layers use standard dense GEMM,
while convolutional layers use im2col transformations that reshape input
patches into matrix columns, converting convolution into GEMM.

Beyond GEMM, frameworks implement BLAS operations (AXPY for vector
addition, GEMV for matrix-vector products) and element-wise operations
(activation functions, normalization). Element-wise operations are
individually cheap but collectively expensive due to memory bandwidth.
Each operation reads and writes the full tensor, so a sequence of five
element-wise operations on a 100 MB tensor moves 1 GB of data. Fusing
those five operations into a single kernel reduces memory traffic to 200
MB, a 5x bandwidth savings that directly translates to faster execution.

Numerical precision adds another dimension. Training in FP32 uses 4
bytes per parameter; quantizing to INT8 reduces this to 1 byte, cutting
memory by 4x and enabling 2-4x throughput improvements on hardware with
INT8 acceleration. Training typically requires FP32 for gradient
stability, while inference runs at FP16 or INT8 with minimal accuracy
loss. Frameworks maintain separate kernel implementations for each
precision format and handle mixed-precision workflows where different
layers operate at different bit widths within a single forward pass.

\textbf{System-Level Operations.} Hardware abstraction and numerical
operations provide the building blocks; system-level operations
orchestrate them. The system layer ties scheduling, memory management,
and resource optimization into a coherent execution engine.

The operation scheduler analyzes the computational graph to find
parallelism while respecting data dependencies. In a static graph, the
scheduler sees the full dependency structure before execution begins and
can plan an optimal ordering. In a dynamic graph, dependencies emerge at
runtime, forcing the scheduler to make greedy decisions. Concretely,
when a ResNet block produces two independent branch outputs, the
scheduler launches both branches simultaneously rather than serializing
them, reducing idle cycles on the GPU's streaming multiprocessors.

The memory manager allocates and reclaims GPU memory across the
computational graph's lifetime. Model parameters (a 7B-parameter model
consumes approximately 14 GB in FP16) persist for the entire training
run, while activation tensors live only until the backward pass consumes
them. PyTorch's caching allocator maintains a memory pool, subdividing
and reusing freed blocks without returning them to CUDA, which avoids
the 1 ms overhead of \texttt{cudaMalloc} calls. For models that exceed
GPU memory, the manager applies gradient checkpointing: discarding
selected activations during the forward pass and recomputing them during
the backward pass, trading roughly 30\% additional compute for 60\% or
more memory savings.

The resource optimizer integrates these scheduling and memory decisions.
When two matrix multiplications with different shapes are ready to
execute, it selects the algorithm variant (Winograd, Strassen, or
standard tiled GEMM) that best fits each shape and the current memory
pressure. A poorly scheduled graph wastes compute; a poorly managed
memory pool triggers out-of-memory errors on hardware that theoretically
has capacity to spare.

Every framework must solve these same three problems, yet real
frameworks differ dramatically in how they prioritize among them.

\section{The nn.Module
Abstraction}\label{sec-ai-frameworks-nnmodule-abstraction-2622}

Before comparing frameworks broadly, we examine one abstraction in
depth. PyTorch's \texttt{nn.Module} provides an instructive case study
because its design patterns recur across frameworks: Keras uses similar
layer abstractions, JAX's Flax employs analogous module structures, and
even TensorFlow's functional API shares conceptual parallels. By
understanding how one framework solves the abstraction problem
concretely, we develop vocabulary for comparing alternatives.

The \texttt{nn.Module} class is PyTorch's answer to a concrete question:
how should a framework organize the parameters, state, and computation
of a neural network into a single programmable unit? Rather than catalog
its API, we extract three enduring design principles that every
framework must address regardless of its syntax or programming paradigm.

\subsection*{Principle 1: Automatic Parameter
Discovery}\label{sec-nnmodule-param-discovery}
\addcontentsline{toc}{subsection}{Principle 1: Automatic Parameter
Discovery}

A modern neural network may contain millions of trainable parameters
spread across dozens of layers. Without automation, a programmer would
need to manually enumerate every parameter tensor and pass it to the
optimizer, an error-prone process that scales poorly with model
complexity. Frameworks solve this through \emph{automatic parameter
discovery}: the system walks the module tree, collecting every parameter
tensor so the optimizer can update them in a single call.

This is fundamentally a graph traversal problem. When a developer
assigns an \texttt{nn.Parameter} as a class attribute, the framework's
metaclass machinery intercepts the assignment and registers the tensor
in an internal dictionary. A call to \texttt{.parameters()} then
performs a recursive depth-first traversal of the module tree, yielding
every registered parameter. The same pattern appears in every major
framework: Keras layers maintain a \texttt{trainable\_weights} list,
JAX's Flax modules use \texttt{init()} to return a nested parameter
dictionary, and TensorFlow's \texttt{tf.Module} provides
\texttt{trainable\_variables}. The mechanism differs but the principle
is universal.

The systems consequence is significant. Automatic parameter discovery
enables \texttt{optimizer.step()} to update millions of parameters in a
single vectorized operation, keeping the operations-per-parameter term
efficient by avoiding per-parameter Python dispatch. Without this
abstraction, each parameter update would require a separate Python
function call, and the interpreter overhead alone would dominate
training time for large models. Listing~\ref{lst-parameter_registration}
demonstrates the core mechanism: attribute assignment triggers
registration, and \texttt{.parameters()} returns all discovered tensors.

\begin{codelisting}

\caption{\label{lst-parameter_registration}\textbf{Parameter
Registration}: Automatic parameter tracking through attribute assignment
enables optimizer access to all trainable weights without manual
enumeration.}

\centering{

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ torch}
\ImportTok{import}\NormalTok{ torch.nn }\ImportTok{as}\NormalTok{ nn}


\KeywordTok{class}\NormalTok{ CustomLayer(nn.Module):}
    \KeywordTok{def} \FunctionTok{\_\_init\_\_}\NormalTok{(}\VariableTok{self}\NormalTok{, input\_size, output\_size):}
        \BuiltInTok{super}\NormalTok{().}\FunctionTok{\_\_init\_\_}\NormalTok{()}
        \VariableTok{self}\NormalTok{.weight }\OperatorTok{=}\NormalTok{ nn.Parameter(}
\NormalTok{            torch.randn(output\_size, input\_size)}
\NormalTok{        )}
        \VariableTok{self}\NormalTok{.bias }\OperatorTok{=}\NormalTok{ nn.Parameter(torch.randn(output\_size))}
        \VariableTok{self}\NormalTok{.register\_buffer(}\StringTok{"running\_mean"}\NormalTok{, torch.zeros(output\_size))}

    \KeywordTok{def}\NormalTok{ forward(}\VariableTok{self}\NormalTok{, x):}
        \ControlFlowTok{return}\NormalTok{ torch.matmul(x, }\VariableTok{self}\NormalTok{.weight.t()) }\OperatorTok{+} \VariableTok{self}\NormalTok{.bias}


\NormalTok{layer }\OperatorTok{=}\NormalTok{ CustomLayer(}\DecValTok{10}\NormalTok{, }\DecValTok{20}\NormalTok{)}
\CommentTok{\# Framework discovers both parameters automatically:}
\ControlFlowTok{for}\NormalTok{ name, param }\KeywordTok{in}\NormalTok{ layer.named\_parameters():}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"}\SpecialCharTok{\{}\NormalTok{name}\SpecialCharTok{\}}\SpecialStringTok{: shape }\SpecialCharTok{\{}\NormalTok{param}\SpecialCharTok{.}\NormalTok{shape}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

}

\end{codelisting}%

The distinction between \emph{parameters} and \emph{buffers} illustrates
a subtlety of the discovery mechanism. Parameters carry
\texttt{requires\_grad=True} and participate in gradient computation.
Buffers, registered through \texttt{register\_buffer()}, travel with the
model during device transfers but remain excluded from gradient updates.
This separation is essential for normalization layers, where running
statistics must persist across batches but must not receive gradients.
The same dual-track design appears in Keras (via
\texttt{non\_trainable\_weights}) and Flax (via \texttt{state} versus
\texttt{params}).

\subsection*{Principle 2: Mode-Dependent
Behavior}\label{sec-nnmodule-mode-behavior}
\addcontentsline{toc}{subsection}{Principle 2: Mode-Dependent Behavior}

Training and inference require different computational behavior from the
same model graph. During training, dropout layers randomly zero elements
with probability \(p\) to regularize the network, while during inference
those same layers must perform identity mapping to produce deterministic
outputs. Batch normalization uses per-batch statistics during training
but switches to accumulated running statistics during inference. If
these behavioral changes are left to the programmer, forgetting a single
mode switch produces silently incorrect predictions in production.

Frameworks solve this with a \emph{state flag} that propagates through
the module hierarchy. A single call to \texttt{.eval()} on the root
module recursively sets \texttt{self.training\ =\ False} on every
descendant, and each layer queries this flag to select its behavior.
This is an instance of a broader systems principle: the same computation
graph must produce different execution behavior depending on context.
Compilers face the same challenge when the same source code must produce
debug builds (with bounds checking and symbol tables) versus release
builds (with aggressive optimization). The flag-propagation pattern
ensures correctness by centralizing the mode decision at the root rather
than requiring per-layer coordination.

This principle extends to parameter freezing for transfer learning.
Setting \texttt{requires\_grad=False} on specific parameters excludes
them from gradient computation, effectively creating a third behavioral
mode where some parameters train while others remain fixed. Selective
freezing achieves computational savings by pruning the backward pass
graph: frozen parameters need no gradient storage, reducing memory
consumption proportionally.

\subsection*{Principle 3: Hierarchical Composition and
Serialization}\label{sec-nnmodule-composition}
\addcontentsline{toc}{subsection}{Principle 3: Hierarchical Composition
and Serialization}

Complex models compose from reusable submodules, creating a tree
structure. A ResNet is not implemented as a monolithic block of
operations but as a hierarchy: the root module contains a sequence of
residual blocks, each block contains convolution layers and
normalization layers, and each layer contains parameter tensors. This
hierarchical composition must support two critical operations:
\emph{recursive parameter collection} for training and \emph{state
serialization} for checkpointing and deployment.

Hierarchical composition mirrors the hardware memory hierarchy in a
systems-relevant way: each submodule's parameters can be loaded
independently, enabling model parallelism across devices. When a model
is too large for a single GPU, the framework can assign different
subtrees of the module hierarchy to different devices, with the tree
structure providing natural partition boundaries.

The state dictionary mechanism provides the serialization half of this
principle. The \texttt{state\_dict()} method produces a flat key-value
mapping of the full module tree, where dotted path names (e.g.,
\texttt{blocks.0.conv1.weight}) encode the hierarchy. This flat
structure enables efficient serialization: a 7B-parameter model's
approximately 14 GB FP16 checkpoint can be written as a sequential byte
stream, maximizing storage bandwidth utilization. The inverse operation,
\texttt{load\_state\_dict()}, reconstructs the hierarchy from the flat
mapping, enabling checkpoint recovery and cross-framework model exchange
via formats like ONNX. Listing~\ref{lst-nested_modules} demonstrates how
the module tree enables both recursive parameter access and hierarchical
state serialization.

\begin{codelisting}

\caption{\label{lst-nested_modules}\textbf{Nested Module Composition}:
Hierarchical module composition enables recursive parameter collection
and flat state serialization across the module tree.}

\centering{

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ torch}
\ImportTok{import}\NormalTok{ torch.nn }\ImportTok{as}\NormalTok{ nn}


\KeywordTok{class}\NormalTok{ ResidualBlock(nn.Module):}
    \KeywordTok{def} \FunctionTok{\_\_init\_\_}\NormalTok{(}\VariableTok{self}\NormalTok{, channels):}
        \BuiltInTok{super}\NormalTok{().}\FunctionTok{\_\_init\_\_}\NormalTok{()}
        \VariableTok{self}\NormalTok{.conv1 }\OperatorTok{=}\NormalTok{ nn.Conv2d(channels, channels, }\DecValTok{3}\NormalTok{, padding}\OperatorTok{=}\DecValTok{1}\NormalTok{)}
        \VariableTok{self}\NormalTok{.bn1 }\OperatorTok{=}\NormalTok{ nn.BatchNorm2d(channels)}
        \VariableTok{self}\NormalTok{.conv2 }\OperatorTok{=}\NormalTok{ nn.Conv2d(channels, channels, }\DecValTok{3}\NormalTok{, padding}\OperatorTok{=}\DecValTok{1}\NormalTok{)}
        \VariableTok{self}\NormalTok{.bn2 }\OperatorTok{=}\NormalTok{ nn.BatchNorm2d(channels)}

    \KeywordTok{def}\NormalTok{ forward(}\VariableTok{self}\NormalTok{, x):}
\NormalTok{        residual }\OperatorTok{=}\NormalTok{ x}
\NormalTok{        x }\OperatorTok{=}\NormalTok{ torch.relu(}\VariableTok{self}\NormalTok{.bn1(}\VariableTok{self}\NormalTok{.conv1(x)))}
\NormalTok{        x }\OperatorTok{=} \VariableTok{self}\NormalTok{.bn2(}\VariableTok{self}\NormalTok{.conv2(x))}
        \ControlFlowTok{return}\NormalTok{ torch.relu(x }\OperatorTok{+}\NormalTok{ residual)}


\KeywordTok{class}\NormalTok{ ResNet(nn.Module):}
    \KeywordTok{def} \FunctionTok{\_\_init\_\_}\NormalTok{(}\VariableTok{self}\NormalTok{, num\_blocks, channels}\OperatorTok{=}\DecValTok{64}\NormalTok{):}
        \BuiltInTok{super}\NormalTok{().}\FunctionTok{\_\_init\_\_}\NormalTok{()}
        \VariableTok{self}\NormalTok{.conv\_in }\OperatorTok{=}\NormalTok{ nn.Conv2d(}\DecValTok{3}\NormalTok{, channels, }\DecValTok{7}\NormalTok{, padding}\OperatorTok{=}\DecValTok{3}\NormalTok{)}
        \VariableTok{self}\NormalTok{.blocks }\OperatorTok{=}\NormalTok{ nn.ModuleList(}
\NormalTok{            [ResidualBlock(channels) }\ControlFlowTok{for}\NormalTok{ \_ }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(num\_blocks)]}
\NormalTok{        )}
        \VariableTok{self}\NormalTok{.fc }\OperatorTok{=}\NormalTok{ nn.Linear(channels, }\DecValTok{10}\NormalTok{)}

    \KeywordTok{def}\NormalTok{ forward(}\VariableTok{self}\NormalTok{, x):}
\NormalTok{        x }\OperatorTok{=} \VariableTok{self}\NormalTok{.conv\_in(x)}
        \ControlFlowTok{for}\NormalTok{ block }\KeywordTok{in} \VariableTok{self}\NormalTok{.blocks:}
\NormalTok{            x }\OperatorTok{=}\NormalTok{ block(x)}
\NormalTok{        x }\OperatorTok{=}\NormalTok{ x.mean(dim}\OperatorTok{=}\NormalTok{[}\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{])  }\CommentTok{\# Global average pooling}
        \ControlFlowTok{return} \VariableTok{self}\NormalTok{.fc(x)}


\NormalTok{model }\OperatorTok{=}\NormalTok{ ResNet(num\_blocks}\OperatorTok{=}\DecValTok{4}\NormalTok{)}
\NormalTok{total }\OperatorTok{=} \BuiltInTok{sum}\NormalTok{(p.numel() }\ControlFlowTok{for}\NormalTok{ p }\KeywordTok{in}\NormalTok{ model.parameters())}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Total parameters: }\SpecialCharTok{\{}\NormalTok{total}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\CommentTok{\# state\_dict() flattens the tree: \textquotesingle{}blocks.0.conv1.weight\textquotesingle{}, etc.}
\BuiltInTok{print}\NormalTok{(}\BuiltInTok{list}\NormalTok{(model.state\_dict().keys())[:}\DecValTok{4}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

}

\end{codelisting}%

The hierarchical structure also enables module-level traversal for
systematic operations. Methods like \texttt{.named\_modules()} iterate
the entire tree, supporting bulk transformations such as replacing all
BatchNorm layers with GroupNorm or applying Xavier initialization to
every Linear layer. These traversal operations depend on the same tree
structure that enables parameter discovery, illustrating how a single
design decision propagates benefits across multiple use cases.

These three principles, automatic parameter discovery, mode-dependent
behavior, and hierarchical composition with serialization, are not
PyTorch-specific. Every framework must solve them. Keras layers, JAX's
Flax modules, and even functional approaches all address the same
fundamental problems of parameter management, state tracking, and
compositional design. The differences lie not in \emph{what} problems
they solve but in \emph{how} they prioritize among competing solutions.
The following examples demonstrate PyTorch's \texttt{nn.Module} API
patterns: module state management (Listing~\ref{lst-module_state}),
parameter freezing (Listing~\ref{lst-parameter_freezing}), module hooks
(Listing~\ref{lst-module_hooks}), and state dictionary serialization
(Listing~\ref{lst-state_dict}).

\textbf{Parameter Registration and Buffer Management.}
Listing~\ref{lst-parameter_registration} above shows the core mechanism.
The full pattern includes \texttt{register\_buffer()} for non-trainable
state that moves with the model during device transfers, essential for
normalization layer statistics.

\textbf{Module State and Training Modes.} The \texttt{.train()} and
\texttt{.eval()} methods toggle behavioral flags across the entire
module hierarchy. Listing~\ref{lst-module_state} shows how Dropout and
BatchNormalization respond to mode changes.

\begin{codelisting}

\caption{\label{lst-module_state}\textbf{Module State Management}:
Illustrates how training and evaluation modes affect layer behavior,
particularly for Dropout and BatchNormalization.}

\centering{

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ torch}
\ImportTok{import}\NormalTok{ torch.nn }\ImportTok{as}\NormalTok{ nn}


\KeywordTok{class}\NormalTok{ Net(nn.Module):}
    \KeywordTok{def} \FunctionTok{\_\_init\_\_}\NormalTok{(}\VariableTok{self}\NormalTok{):}
        \BuiltInTok{super}\NormalTok{().}\FunctionTok{\_\_init\_\_}\NormalTok{()}
        \VariableTok{self}\NormalTok{.dropout }\OperatorTok{=}\NormalTok{ nn.Dropout(p}\OperatorTok{=}\FloatTok{0.5}\NormalTok{)}
        \VariableTok{self}\NormalTok{.bn }\OperatorTok{=}\NormalTok{ nn.BatchNorm1d(}\DecValTok{10}\NormalTok{)}
        \VariableTok{self}\NormalTok{.fc }\OperatorTok{=}\NormalTok{ nn.Linear(}\DecValTok{10}\NormalTok{, }\DecValTok{5}\NormalTok{)}

    \KeywordTok{def}\NormalTok{ forward(}\VariableTok{self}\NormalTok{, x):}
\NormalTok{        x }\OperatorTok{=} \VariableTok{self}\NormalTok{.dropout(x)  }\CommentTok{\# Behavior depends on self.training}
\NormalTok{        x }\OperatorTok{=} \VariableTok{self}\NormalTok{.bn(x)  }\CommentTok{\# Updates running stats if training}
        \ControlFlowTok{return} \VariableTok{self}\NormalTok{.fc(x)}


\NormalTok{model }\OperatorTok{=}\NormalTok{ Net()}
\NormalTok{model.train()  }\CommentTok{\# Sets self.training = True for all modules}
\CommentTok{\# Dropout active, BatchNorm updates statistics}

\NormalTok{model.}\BuiltInTok{eval}\NormalTok{()  }\CommentTok{\# Sets self.training = False for all modules}
\CommentTok{\# Dropout disabled, BatchNorm uses frozen statistics}
\end{Highlighting}
\end{Shaded}

}

\end{codelisting}%

\textbf{Parameter Freezing for Transfer Learning.} Selective gradient
exclusion enables transfer learning workflows where pretrained layers
remain fixed. Listing~\ref{lst-parameter_freezing} demonstrates freezing
and replacing layers.

\begin{codelisting}

\caption{\label{lst-parameter_freezing}\textbf{Parameter Freezing}:
Demonstrates selective parameter freezing for transfer learning, where
pretrained layers remain fixed while new layers train.}

\centering{

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Freeze all parameters in a pretrained model}
\NormalTok{pretrained\_model }\OperatorTok{=}\NormalTok{ torch.hub.load(}
    \StringTok{"pytorch/vision"}\NormalTok{, }\StringTok{"resnet18"}\NormalTok{, pretrained}\OperatorTok{=}\VariableTok{True}
\NormalTok{)}

\ControlFlowTok{for}\NormalTok{ param }\KeywordTok{in}\NormalTok{ pretrained\_model.parameters():}
\NormalTok{    param.requires\_grad }\OperatorTok{=} \VariableTok{False}

\CommentTok{\# Replace final layer with trainable parameters}
\NormalTok{pretrained\_model.fc }\OperatorTok{=}\NormalTok{ nn.Linear(}\DecValTok{512}\NormalTok{, }\DecValTok{10}\NormalTok{)  }\CommentTok{\# New layer is trainable}

\CommentTok{\# Only fc.parameters() will receive gradients during training}
\NormalTok{optimizer }\OperatorTok{=}\NormalTok{ torch.optim.Adam(}
    \BuiltInTok{filter}\NormalTok{(}\KeywordTok{lambda}\NormalTok{ p: p.requires\_grad, pretrained\_model.parameters()),}
\NormalTok{    lr}\OperatorTok{=}\FloatTok{0.001}\NormalTok{,}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

}

\end{codelisting}%

\textbf{Module Hooks for Inspection and Debugging.} Forward and backward
hooks intercept intermediate computations without modifying model code,
enabling gradient flow diagnosis and activation monitoring.
Listing~\ref{lst-module_hooks} illustrates both hook types.

\begin{codelisting}

\caption{\label{lst-module_hooks}\textbf{Module Hooks}: Shows forward
and backward hooks for inspecting activations and gradients during
training.}

\centering{

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ torch}
\ImportTok{import}\NormalTok{ torch.nn }\ImportTok{as}\NormalTok{ nn}

\NormalTok{model }\OperatorTok{=}\NormalTok{ nn.Sequential(nn.Linear(}\DecValTok{10}\NormalTok{, }\DecValTok{20}\NormalTok{), nn.ReLU(), nn.Linear(}\DecValTok{20}\NormalTok{, }\DecValTok{5}\NormalTok{))}


\CommentTok{\# Forward hook to inspect activations}
\KeywordTok{def}\NormalTok{ forward\_hook(module, }\BuiltInTok{input}\NormalTok{, output):}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Layer output shape: }\SpecialCharTok{\{}\NormalTok{output}\SpecialCharTok{.}\NormalTok{shape}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
    \BuiltInTok{print}\NormalTok{(}
        \SpecialStringTok{f"Output statistics: mean=}\SpecialCharTok{\{}\NormalTok{output}\SpecialCharTok{.}\NormalTok{mean()}\SpecialCharTok{:.3f\}}\SpecialStringTok{, "}
        \SpecialStringTok{f"std=}\SpecialCharTok{\{}\NormalTok{output}\SpecialCharTok{.}\NormalTok{std()}\SpecialCharTok{:.3f\}}\SpecialStringTok{"}
\NormalTok{    )}


\CommentTok{\# Backward hook to inspect gradients}
\KeywordTok{def}\NormalTok{ backward\_hook(module, grad\_input, grad\_output):}
    \BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Gradient norm: }\SpecialCharTok{\{}\NormalTok{grad\_output[}\DecValTok{0}\NormalTok{]}\SpecialCharTok{.}\NormalTok{norm()}\SpecialCharTok{:.3f\}}\SpecialStringTok{"}\NormalTok{)}


\CommentTok{\# Register hooks on specific layer}
\NormalTok{handle\_fwd }\OperatorTok{=}\NormalTok{ model[}\DecValTok{0}\NormalTok{].register\_forward\_hook(forward\_hook)}
\NormalTok{handle\_bwd }\OperatorTok{=}\NormalTok{ model[}\DecValTok{0}\NormalTok{].register\_full\_backward\_hook(backward\_hook)}

\CommentTok{\# Execute forward and backward pass}
\NormalTok{x }\OperatorTok{=}\NormalTok{ torch.randn(}\DecValTok{32}\NormalTok{, }\DecValTok{10}\NormalTok{)}
\NormalTok{y }\OperatorTok{=}\NormalTok{ model(x)}
\NormalTok{loss }\OperatorTok{=}\NormalTok{ y.}\BuiltInTok{sum}\NormalTok{()}
\NormalTok{loss.backward()}

\CommentTok{\# Remove hooks when done}
\NormalTok{handle\_fwd.remove()}
\NormalTok{handle\_bwd.remove()}
\end{Highlighting}
\end{Shaded}

}

\end{codelisting}%

\textbf{State Dictionary and Model Serialization.} The
\texttt{state\_dict()} mechanism provides checkpoint management, and
\texttt{load\_state\_dict()} restores model state. Using
\texttt{strict=False} enables partial loading for architecture
modifications during fine-tuning. Listing~\ref{lst-state_dict}
demonstrates the save/load cycle.

\begin{codelisting}

\caption{\label{lst-state_dict}\textbf{State Dictionary}: Demonstrates
model serialization and loading for checkpoint management.}

\centering{

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ torch}
\ImportTok{import}\NormalTok{ torch.nn }\ImportTok{as}\NormalTok{ nn}

\CommentTok{\# Save model checkpoint}
\NormalTok{model }\OperatorTok{=}\NormalTok{ nn.Sequential(nn.Linear(}\DecValTok{10}\NormalTok{, }\DecValTok{20}\NormalTok{), nn.ReLU(), nn.Linear(}\DecValTok{20}\NormalTok{, }\DecValTok{5}\NormalTok{))}

\NormalTok{checkpoint }\OperatorTok{=}\NormalTok{ \{}
    \StringTok{"model\_state\_dict"}\NormalTok{: model.state\_dict(),}
    \StringTok{"epoch"}\NormalTok{: }\DecValTok{42}\NormalTok{,}
    \StringTok{"optimizer\_state\_dict"}\NormalTok{: optimizer.state\_dict(),}
\NormalTok{\}}
\NormalTok{torch.save(checkpoint, }\StringTok{"checkpoint.pt"}\NormalTok{)}

\CommentTok{\# Load checkpoint}
\NormalTok{loaded\_checkpoint }\OperatorTok{=}\NormalTok{ torch.load(}\StringTok{"checkpoint.pt"}\NormalTok{)}
\NormalTok{model.load\_state\_dict(loaded\_checkpoint[}\StringTok{"model\_state\_dict"}\NormalTok{])}
\NormalTok{optimizer.load\_state\_dict(loaded\_checkpoint[}\StringTok{"optimizer\_state\_dict"}\NormalTok{])}
\NormalTok{start\_epoch }\OperatorTok{=}\NormalTok{ loaded\_checkpoint[}\StringTok{"epoch"}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

}

\end{codelisting}%

\section{Major Framework Platform
Analysis}\label{sec-ai-frameworks-major-framework-platform-analysis-fe96}

Each major framework represents a distinct point in the design space
defined by the three fundamental problems: TensorFlow prioritizes the
\textbf{Abstraction Problem} through its comprehensive deployment
ecosystem, PyTorch prioritizes the \textbf{Execution Problem} through
its dynamic graph approach, and JAX reframes the \textbf{Differentiation
Problem} through composable function transformations. These are not just
API differences but fundamental capability trade-offs that determine
what each framework can and cannot do well.

\subsection{TensorFlow: The Graph-First Production
Machine}\label{sec-ai-frameworks-tensorflow-ecosystem-063c}

TensorFlow's architecture reflects a comprehensive solution to the
\textbf{Abstraction Problem}: how do you target diverse hardware---from
cloud TPUs to microcontrollers---using a single interface? Its design
philosophy is built on the \textbf{Static Graph} (or ``Define-and-Run'')
principle. By requiring the model to be represented as a complete
computational graph before execution, TensorFlow enables ahead-of-time
(AOT) compilation and optimization.

This approach prioritizes the \textbf{Deployment Spectrum}. Because the
framework sees the entire graph, it can perform aggressive optimizations
like constant folding, operator fusion, and memory layout optimization
before the first byte of data is processed. This is why TensorFlow
remains the standard for complex production ecosystems, as illustrated
in Figure~\ref{fig-tensorflow-architecture}.

\begin{figure}[htb]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/19c6f5d3052f44f583040947186fa3db9127e046.pdf}}

}

\caption{\label{fig-tensorflow-architecture}\textbf{TensorFlow
Training-to-Deployment Pipeline.} Two-column diagram showing the
training path (left) from data preprocessing through tf.keras and
distribution strategy across CPU, GPU, and TPU, and the deployment path
(right) from SavedModel export to TensorFlow Serving, Lite, JS, and
language bindings. Source:
\href{https://blog.tensorflow.org/2019/01/whats-coming-in-tensorflow-2-0.html}{TensorFlow.}.}

\end{figure}%

While TensorFlow 2.0 introduced eager execution to bridge the gap
between research and production, its core strength remains the robust,
compiled path from research to global-scale deployment. These
optimizations are addressed systematically in \textbf{?@sec-ai-training}
and \textbf{?@sec-model-serving-systems}.

\subsection{PyTorch: The Eager Research
Standard}\label{sec-ai-frameworks-pytorch-85cd}

In contrast, PyTorch's architecture represents a fundamentally different
answer to the \textbf{Execution Problem}. Its design philosophy is built
on \textbf{Dynamic Graphs} (or ``Define-by-Run''). Instead of building a
blueprint before execution, PyTorch builds the computational graph
on-the-fly as the code runs.

This approach won the research community because it treats deep learning
as standard Python programming. You can use Python loops, conditionals,
and debuggers (like \texttt{pdb}) directly within your model's forward
pass. This ``Eager Execution'' model enables rapid iteration and
intuitive model design, which is essential for the trial-and-error
nature of frontier AI research.

The trade-off is a more fragmented deployment path. Because the graph is
dynamic, the framework cannot easily perform global optimizations before
execution. To bridge this ``Research-to-Production gap,'' PyTorch
introduced \textbf{TorchScript} and \textbf{PyTorch 2.0 (with
\texttt{torch.compile})}, which allow developers to ``capture'' a
dynamic model and turn it into an optimized, static representation for
deployment. This evolution shows PyTorch moving toward the
``Production'' end of the spectrum while preserving the ``Eager''
experience that made it famous.

\subsection{JAX: The Functional Transformation
Engine}\label{sec-ai-frameworks-jax}

Where PyTorch and TensorFlow evolved from imperative programming
traditions, JAX represents a fundamentally different approach to ML
frameworks, one built on functional programming principles and
composable program transformations rather than computational graphs
(\citeproc{ref-jax2018github}{Bradbury et al. 2018}). Developed by
Google Research, JAX has gained significant traction in research
settings, particularly for work requiring custom differentiation,
advanced optimization research, and large-scale distributed training.

JAX's architecture reframes the \textbf{Differentiation Problem}
entirely. Rather than implementing automatic differentiation as a
tape-based system (PyTorch) or a graph transformation pass (TensorFlow),
JAX treats differentiation as one of several \emph{composable function
transformations}. The \texttt{jax.grad} function does not compute
gradients directly; it returns a \emph{new function} that computes
gradients. This subtle distinction enables powerful compositions: you
can differentiate a differentiated function (higher-order derivatives),
vectorize a gradient computation (\texttt{vmap(grad(f))}), or compile a
vectorized gradient to XLA (\texttt{jit(vmap(grad(f)))}).

JAX's functional paradigm requires a genuine mental shift from
``tracking state through objects'' to ``transforming pure functions.''
The conceptual introduction here covers JAX's core design;
transformation composition, pytree handling, and XLA tracing mechanics
each warrant dedicated study for production use.

\textbf{Transformations over State.} While PyTorch and TensorFlow build
computational graphs (dynamically or statically), JAX transforms
functions. The core insight is that automatic differentiation,
vectorization, and JIT compilation are all \emph{program
transformations} that can compose. Listing~\ref{lst-jax-transformations}
demonstrates this composable approach.

\begin{codelisting}

\caption{\label{lst-jax-transformations}\textbf{JAX Function
Transformations}: JAX treats differentiation, vectorization, and
compilation as composable function transformations rather than graph
operations.}

\centering{

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ jax}
\ImportTok{import}\NormalTok{ jax.numpy }\ImportTok{as}\NormalTok{ jnp}


\KeywordTok{def}\NormalTok{ loss\_fn(params, x, y):}
\NormalTok{    pred }\OperatorTok{=}\NormalTok{ jnp.dot(x, params[}\StringTok{"w"}\NormalTok{]) }\OperatorTok{+}\NormalTok{ params[}\StringTok{"b"}\NormalTok{]}
    \ControlFlowTok{return}\NormalTok{ jnp.mean((pred }\OperatorTok{{-}}\NormalTok{ y) }\OperatorTok{**} \DecValTok{2}\NormalTok{)}


\CommentTok{\# Transform: compute gradients}
\NormalTok{grad\_fn }\OperatorTok{=}\NormalTok{ jax.grad(loss\_fn)}

\CommentTok{\# Transform: vectorize over batch dimension}
\NormalTok{batched\_grad }\OperatorTok{=}\NormalTok{ jax.vmap(grad\_fn, in\_axes}\OperatorTok{=}\NormalTok{(}\VariableTok{None}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{))}

\CommentTok{\# Transform: compile to XLA}
\NormalTok{fast\_batched\_grad }\OperatorTok{=}\NormalTok{ jax.jit(batched\_grad)}

\CommentTok{\# Compose all three: fast, batched gradient computation}
\end{Highlighting}
\end{Shaded}

}

\end{codelisting}%

This functional approach requires \textbf{pure functions} (no side
effects) and \textbf{immutable data} (arrays cannot be modified in
place). These constraints enable powerful guarantees: the compiler can
safely reorder, fuse, and parallelize operations because function
outputs depend only on inputs.

\textbf{Key Transformations.} JAX provides four core transformations
that compose arbitrarily:

\begin{itemize}
\tightlist
\item
  \textbf{\texttt{jax.grad}}: Automatic differentiation. Unlike
  PyTorch's tape-based autograd, \texttt{grad} returns a \emph{new
  function} that computes gradients. Supports both forward-mode
  (\texttt{jacfwd}) and reverse-mode (\texttt{jacrev}) differentiation.
\item
  \textbf{\texttt{jax.jit}}: Just-in-time compilation to XLA. Traces the
  function once, compiles to optimized machine code, caches the result.
  Subsequent calls execute the compiled version without Python overhead.
\item
  \textbf{\texttt{jax.vmap}}: Automatic vectorization. Transforms a
  function operating on single examples into one operating on batches,
  without manual batching code.
\item
  \textbf{\texttt{jax.pmap}}: Parallel execution across devices. Maps a
  function over data distributed across multiple GPUs/TPUs,
  automatically handling communication.
\end{itemize}

\textbf{Ecosystem and Libraries.} JAX's minimalist core delegates neural
network abstractions to companion libraries (Flax, Haiku, Equinox) and
optimization to Optax. This separation reflects the functional
philosophy: the core provides transformations, while libraries build
conventional abstractions on top. The ecosystem is younger and smaller
than PyTorch's or TensorFlow's, which affects the availability of
pre-built components for production use.

\textbf{Trade-offs and Use Cases.} JAX excels in scenarios requiring:

\begin{itemize}
\tightlist
\item
  Custom differentiation (higher-order gradients, custom VJP/JVP rules)
\item
  Research on optimization algorithms
\item
  Large-scale distributed training (particularly on TPUs)
\item
  Scientific computing with AD requirements
\end{itemize}

JAX requires more upfront investment than PyTorch: the functional
paradigm has a learning curve, state management requires explicit
patterns, and debugging compiled code is harder than eager execution.
The ecosystem is also younger, with fewer pre-built components than
PyTorch or TensorFlow.

\subsection{Quantitative Platform Performance
Analysis}\label{sec-ai-frameworks-quantitative-platform-performance-analysis-816d}

Design philosophy claims are only meaningful when backed by measurement.
Table~\ref{tbl-mlfm-comparison} quantifies how the architectural choices
of TensorFlow, PyTorch, and JAX translate to system characteristics
across execution model, differentiation approach, and hardware
utilization.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2500}}@{}}
\caption{\textbf{Framework Characteristics.} Comparison of TensorFlow,
PyTorch, and JAX across graph construction, data mutability, automatic
differentiation, GPU utilization, and distributed scalability. GPU
utilization varies by model architecture, batch size, and operation mix.
JAX/XLA achieves higher utilization for TPU workloads through aggressive
fusion, while PyTorch and TensorFlow perform similarly for most deep
learning workloads. Students should profile their specific workloads
rather than relying on framework-level
generalizations.}\label{tbl-mlfm-comparison}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Aspect}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{TensorFlow}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{PyTorch}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{JAX}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Aspect}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{TensorFlow}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{PyTorch}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{JAX}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Graph Type} & Static (1.x), Dynamic (2.x) & Dynamic & Functional
transformations \\
\textbf{Programming Model} & Imperative (2.x), Symbolic (1.x) &
Imperative & Functional \\
\textbf{Core Data Structure} & Tensor (mutable) & Tensor (mutable) &
Array (immutable) \\
\textbf{Execution Mode} & Eager (2.x default), Graph & Eager &
Just-in-time compilation \\
\textbf{Automatic Differentiation} & Reverse mode & Reverse mode &
Forward and Reverse mode \\
\textbf{Hardware Acceleration} & CPU, GPU, TPU & CPU, GPU & CPU, GPU,
TPU \\
\textbf{Compilation Optimization} & XLA: 3-10x speedup & TorchScript: 2x
& XLA: 3-10x speedup \\
\textbf{Memory Efficiency} & 70-90\% (workload dependent) & 70-90\%
(varies) & 75-95\% (with XLA fusion) \\
\textbf{Distributed Scalability} & High (1024+ GPUs) & High & Very High
(1024+ GPUs) \\
\end{longtable}

How do these architectural differences look in practice?
Listing~\ref{lst-framework-hello-world} implements the same neural
network (a single linear layer mapping 10 inputs to 1 output) across all
three frameworks, revealing how design philosophy shapes even the
simplest code:

\begin{codelisting}

\caption{\label{lst-framework-hello-world}\textbf{Framework Comparison:
Hello World}: The same simple neural network implemented in PyTorch
(object-oriented), TensorFlow/Keras (declarative), and JAX (functional),
illustrating each framework's distinct design philosophy.}

\centering{

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# PyTorch {-} Dynamic, Pythonic}
\ImportTok{import}\NormalTok{ torch.nn }\ImportTok{as}\NormalTok{ nn}


\KeywordTok{class}\NormalTok{ SimpleNet(nn.Module):}
    \KeywordTok{def} \FunctionTok{\_\_init\_\_}\NormalTok{(}\VariableTok{self}\NormalTok{):}
        \BuiltInTok{super}\NormalTok{().}\FunctionTok{\_\_init\_\_}\NormalTok{()}
        \VariableTok{self}\NormalTok{.fc }\OperatorTok{=}\NormalTok{ nn.Linear(}\DecValTok{10}\NormalTok{, }\DecValTok{1}\NormalTok{)}

    \KeywordTok{def}\NormalTok{ forward(}\VariableTok{self}\NormalTok{, x):}
        \ControlFlowTok{return} \VariableTok{self}\NormalTok{.fc(x)}


\CommentTok{\# TensorFlow/Keras {-} High{-}level API}
\ImportTok{import}\NormalTok{ tensorflow }\ImportTok{as}\NormalTok{ tf}

\NormalTok{model }\OperatorTok{=}\NormalTok{ tf.keras.Sequential(}
\NormalTok{    [tf.keras.layers.Dense(}\DecValTok{1}\NormalTok{, input\_shape}\OperatorTok{=}\NormalTok{(}\DecValTok{10}\NormalTok{,))]}
\NormalTok{)}

\CommentTok{\# JAX {-} Functional approach}
\ImportTok{import}\NormalTok{ jax.numpy }\ImportTok{as}\NormalTok{ jnp}
\ImportTok{from}\NormalTok{ jax }\ImportTok{import}\NormalTok{ random}


\KeywordTok{def}\NormalTok{ simple\_net(params, x):}
    \ControlFlowTok{return}\NormalTok{ jnp.dot(x, params[}\StringTok{"w"}\NormalTok{]) }\OperatorTok{+}\NormalTok{ params[}\StringTok{"b"}\NormalTok{]}


\NormalTok{key }\OperatorTok{=}\NormalTok{ random.PRNGKey(}\DecValTok{0}\NormalTok{)}
\NormalTok{params }\OperatorTok{=}\NormalTok{ \{}
    \StringTok{"w"}\NormalTok{: random.normal(key, (}\DecValTok{10}\NormalTok{, }\DecValTok{1}\NormalTok{)),}
    \StringTok{"b"}\NormalTok{: random.normal(key, (}\DecValTok{1}\NormalTok{,)),}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

}

\end{codelisting}%

These three implementations solve the same mathematical problem but
reveal fundamentally different answers to the Three Problems. PyTorch's
class inheritance (\texttt{nn.Module}) binds state and computation
together, solving the Execution Problem through eager evaluation: the
graph builds as Python runs, making standard debuggers and control flow
work naturally. The cost is that no optimizer sees the full computation
before execution begins.

TensorFlow/Keras inverts this priority. The \texttt{Sequential} API
declares structure without executing it, solving the Abstraction Problem
first: the same declaration compiles to server GPUs, mobile NPUs, or
browser WebGL backends. Eager mode (default in TensorFlow 2.x) recovers
some of PyTorch's debugging flexibility, but production deployment still
relies on graph capture for optimization.

JAX makes the most radical trade-off. The model is a pure
function\sidenote{\textbf{Pure Function}: Has no side effects and always
returns the same output for the same inputs. Pure functions enable
mathematical reasoning about code behavior and safe program
transformations. } with immutable data\sidenote{\textbf{Immutable Data
Structures}: Cannot be modified after creation. Any operation that
appears to change the data actually creates a new copy, ensuring that
the original data remains unchanged. This prevents accidental
modifications and enables safe parallel processing. } and no internal
state\sidenote{\textbf{Stateless Function}: Produces the same output for
the same inputs every time, without relying on or modifying any external
state. This predictability enables mathematical optimization and
parallel execution. }. This functional purity solves the Differentiation
Problem most elegantly: \texttt{grad}, \texttt{vmap} (automatic
vectorization\sidenote{\textbf{Automatic Vectorization}: Transforms
operations on single data points into operations on entire arrays or
batches, improving computational efficiency by using SIMD (Single
Instruction, Multiple Data) processor capabilities. }), and \texttt{jit}
(just-in-time compilation\sidenote{\textbf{Just-in-Time (JIT)
Compilation}: Translates high-level code into optimized machine code at
runtime, enabling performance optimizations based on actual data shapes
and hardware characteristics. }) are composable transformations on
stateless functions, not infrastructure bolted onto an object system.
The cost is explicit parameter management and a programming model
unfamiliar to most engineers.

No framework optimizes all three problems simultaneously; each makes
deliberate trade-offs that shape everything from API design to
performance characteristics.

\textbf{PyTorch: Research-First.} PyTorch prioritizes developer
experience over optimization potential. Its answer to the
\textbf{Execution Problem} is eager execution with dynamic graphs:
operations run immediately, standard Python debuggers work unchanged,
and data-dependent control flow requires no special syntax. The autograd
tape
(Section~\ref{sec-ai-frameworks-eager-execution-dynamic-graphs-29c2})
embodies this philosophy by rebuilding the computation graph each
forward pass, keeping intermediate values accessible for inspection. The
trade-off is concrete: without explicit compilation
(\texttt{torch.compile}), the framework cannot fuse operations across
kernel boundaries, leaving optimization opportunities on the table.

\textbf{TensorFlow: Deployment-Optimized.} TensorFlow prioritizes
production deployment and scalability, reflecting Google's experience
operating ML at massive scale. Its strongest answer is to the
\textbf{Abstraction Problem}: a unified SavedModel format compiles to
optimized runtimes across cloud servers, mobile devices, browsers, and
microcontrollers. XLA compilation addresses the \textbf{Execution
Problem} with 3--10\(\times\) speedups through operation fusion and
hardware-specific code generation, at the cost of harder debugging in
graph mode. The complete production toolchain (TensorFlow Serving, TFX,
TensorBoard) reflects a philosophy where backward compatibility and
deployment reliability take precedence over research flexibility.

\textbf{JAX: Mathematical Composability.} JAX redefines the
\textbf{Differentiation Problem} entirely: rather than implementing
autodiff as infrastructure (tape or graph), it treats differentiation as
one composable transformation among many. The \texttt{grad}
transformation returns a new function that can itself be transformed by
\texttt{vmap}, \texttt{jit}, or another \texttt{grad}. Functional purity
also enables an aggressive answer to the \textbf{Execution Problem}:
because functions have no side effects, the XLA compiler can freely
reorder, fuse, and parallelize operations without correctness concerns.
The cost is a steeper learning curve and a programming model that
requires explicit state management unfamiliar to most ML practitioners.

Exploratory research favors PyTorch's debugging immediacy, production
deployment favors TensorFlow's optimization depth, and algorithmic
research favors JAX's composable transformations. Each philosophy shapes
not just code syntax but team workflows, debugging practices, and
deployment pipelines, which is why framework migration costs are
measured in engineer-months rather than engineer-days.

\subsection{Quantitative Framework Efficiency
Comparison}\label{sec-ai-frameworks-quantitative-framework-efficiency-comparison-3b77}

How large are these differences in practice?
Table~\ref{tbl-framework-efficiency-matrix} compares major frameworks
across efficiency dimensions using benchmark workloads representative of
production deployment scenarios.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 10\tabcolsep) * \real{0.1667}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 10\tabcolsep) * \real{0.1667}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 10\tabcolsep) * \real{0.1667}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 10\tabcolsep) * \real{0.1667}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 10\tabcolsep) * \real{0.1667}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 10\tabcolsep) * \real{0.1667}}@{}}
\caption{\textbf{Framework Efficiency Comparison.} Quantitative
comparison of major ML frameworks across efficiency dimensions using
ResNet-50 inference on representative hardware (NVIDIA A100 GPU for
server frameworks, ARM Cortex-A78 for mobile). Metrics reflect
production workloads with accuracy maintained within 1\% of baseline.
Hardware utilization represents percentage of theoretical peak
performance on typical
operations.}\label{tbl-framework-efficiency-matrix}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Framework}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Inference} \textbf{Latency (ms)}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Memory} \textbf{Usage (MB)}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Energy} \textbf{(mJ/inference)}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Model Size} \textbf{Reduction}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Hardware} \textbf{Utilization (\%)}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Framework}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Inference} \textbf{Latency (ms)}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Memory} \textbf{Usage (MB)}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Energy} \textbf{(mJ/inference)}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Model Size} \textbf{Reduction}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Hardware} \textbf{Utilization (\%)}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{TensorFlow} & 45 & 2,100 & 850 & None & 35 \\
\textbf{TensorFlow Lite} & 12 & 180 & 120 & 4x (quantized) & 65 \\
\textbf{TensorFlow Lite Micro} & 8 & 32 & 45 & 8x (pruned+quant) & 75 \\
\textbf{PyTorch} & 52 & 1,800 & 920 & None & 32 \\
\textbf{PyTorch Mobile} & 18 & 220 & 180 & 3x (quantized) & 58 \\
\textbf{ONNX Runtime} & 15 & 340 & 210 & 2x (optimized) & 72 \\
\textbf{TensorRT} & 3 & 450 & 65 & 2x (precision opt) & 88 \\
\textbf{Apache TVM} & 6 & 280 & 95 & 3x (compiled) & 82 \\
\end{longtable}

The efficiency data reveals several important patterns. First,
specialized inference frameworks (TensorRT, Apache TVM) achieve
10--15\(\times\) lower latency than general-purpose training frameworks
(PyTorch, TensorFlow) on identical hardware, demonstrating that
framework selection has quantitative performance implications beyond
qualitative design preferences. Second, mobile-optimized variants (TF
Lite, PyTorch Mobile) reduce memory requirements by 10\(\times\)
compared to their full counterparts while maintaining accuracy within
1\% through quantization and graph optimization. Third, hardware
utilization varies dramatically: TensorRT achieves 88\% GPU utilization
through aggressive kernel fusion while vanilla PyTorch achieves only
32\%, a 2.75\(\times\) efficiency gap that directly translates to cost
differences in production deployment.

These gaps widen further when we move beyond the server room.

\section{Deployment
Targets}\label{sec-ai-frameworks-deployment-targets-13f1}

On resource-constrained devices, the efficiency differences measured
above become hard constraints that determine whether deployment is
feasible at all. Execution strategy shifts from ``eager vs.~graph'' to
``can we execute at all?'' The differentiation problem often disappears
entirely (inference-only). The abstraction problem intensifies:
targeting ARM vs.~x86, mobile NPUs vs.~edge TPUs, microcontrollers with
kilobytes of memory.

Table~\ref{tbl-deployment-frameworks} summarizes framework choices by
deployment target:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2500}}@{}}
\caption{\textbf{Framework Selection by Deployment Target.} Recommended
frameworks, optimization techniques, and key constraints for each
deployment tier, from cloud servers to
microcontrollers.}\label{tbl-deployment-frameworks}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Environment}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Primary Frameworks}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Key Optimizations}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Typical Constraints}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Environment}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Primary Frameworks}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Key Optimizations}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Typical Constraints}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Cloud/Server} & PyTorch, TensorFlow, JAX & Distributed training,
mixed precision & Throughput, cost \\
\textbf{Edge} & TensorFlow Lite, ONNX Runtime & Quantization (INT8),
static graphs & Latency \textless10ms, limited memory \\
\textbf{Mobile} & TF Lite, Core ML, PyTorch Mobile & NPU acceleration,
model compression & Battery, thermal, app size limits \\
\textbf{Microcontroller} & TF Lite Micro, & 4-bit quantization, &
\textless256KB RAM, \\
\textbf{(TinyML)} & uTensor & static allocation & no dynamic memory \\
\end{longtable}

The table above reveals a fragmented landscape: different deployment
targets favor different frameworks. This fragmentation creates a
practical problem when organizations train in one framework but deploy
on a target best served by another. \textbf{Interoperability through
ONNX.} The Open Neural Network Exchange (ONNX)\sidenote{\textbf{ONNX
(Open Neural Network Exchange)} (\citeproc{ref-bai2019onnx}{Flint et al.
2019}): Launched September 2017 by Facebook and Microsoft to solve
framework fragmentation. Originally named ``Toffee'' internally at
Facebook, the name change emphasized its role as an exchange format.
Became a Linux Foundation project in 2019. Enables training in PyTorch
and deploying via TensorFlow Lite or TensorRT without manual conversion.
} format addresses this by enabling model portability across frameworks:
train in PyTorch, deploy via TensorFlow Lite or ONNX Runtime. This
standardization eliminates manual conversion when moving between
development and production environments. Figure~\ref{fig-onnx}
illustrates this hub-and-spoke interoperability model. Detailed
deployment optimization (quantization, pruning, hardware-specific
compilation) appears in \textbf{?@sec-model-compression} and
\textbf{?@sec-model-serving-systems}.

\begin{figure}[htb]

\centering{

\includegraphics[width=0.7\linewidth,height=\textheight,keepaspectratio]{contents/vol1/frameworks/images/jpeg/onnx_new.jpg}

}

\caption{\label{fig-onnx}\textbf{Framework Interoperability}: ONNX
enables model portability across frameworks, allowing training in one
framework and deployment in another.}

\end{figure}%

\section{Selecting a
Framework}\label{sec-ai-frameworks-selecting-framework-2949}

How should an engineer choose among frameworks when no single option
dominates across all criteria? Framework selection is a constrained
optimization problem across technical capabilities, operational
requirements, and organizational factors.

\subsection{The Framework Selection Trade-off
Space}\label{sec-ai-frameworks-framework-selection-tradeoff-space-c76e}

Framework selection can be characterized along three primary axes that
define the trade-off space:

\textbf{Axis 1: Development Velocity vs.~Production Performance.} Eager
execution (PyTorch) prioritizes iteration speed; graph compilation
(TensorFlow/XLA, JAX/JIT) prioritizes runtime optimization. The optimal
point depends on lifecycle stage: research weights velocity, production
weights performance.

\textbf{Axis 2: Flexibility vs.~Optimization Depth.} Dynamic graphs
enable arbitrary control flow but limit compiler scope. Static graphs
constrain expressiveness but enable aggressive fusion and
hardware-specific code generation. As Table~\ref{tbl-mlfm-graphs}
demonstrated, this trade-off manifests across memory management,
utilization, and debugging workflows.

\textbf{Axis 3: Ecosystem Breadth vs.~Specialization.} General-purpose
frameworks cover broad operation sets but underperform specialized
runtimes. TensorRT achieves 88\% GPU utilization versus PyTorch's 32\%
(Table~\ref{tbl-framework-efficiency-matrix}) precisely because it
optimizes for a narrower problem. ONNX bridges this gap through
standardized interchange.

\phantomsection\label{callout-perspectiveux2a-1.14}
\begin{fbx}{callout-perspective}{Systems Perspective:}{Framework Selection Constraints}
\phantomsection\label{callout-perspective*-1.14}
Rather than seeking the ``best'' framework, effective selection
identifies the framework that satisfies hard constraints (deployment
target, required operations, team expertise) while optimizing soft
preferences (performance, development speed, ecosystem). Hard
constraints eliminate options; soft preferences rank remaining
candidates.

\end{fbx}

The TensorFlow ecosystem illustrates how these axes interact concretely.
Its three variants (TensorFlow, TensorFlow Lite, TensorFlow Lite Micro)
trace a single design philosophy across progressively tighter
constraints, a pattern that generalizes to any framework family.
Table~\ref{tbl-tf-comparison} quantifies the trade-offs.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2500}}@{}}
\caption{\textbf{TensorFlow Variant Software Comparison.} Design
trade-offs across TensorFlow, TensorFlow Lite, and TensorFlow Lite
Micro, balancing model expressiveness, binary size, and resource
constraints. Supported operations decrease from approximately 1,400 in
full TensorFlow to approximately 50 in TensorFlow Lite Micro, reflecting
a shift from training capability to efficient edge inference. Native
quantization tooling enables further optimization for constrained
environments.}\label{tbl-tf-comparison}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{TensorFlow}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{TensorFlow Lite}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{TensorFlow Lite for Microcontrollers}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{TensorFlow}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{TensorFlow Lite}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{TensorFlow Lite for Microcontrollers}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Training} & Yes & No & No \\
\textbf{Inference} & Yes (\emph{but inefficient on edge}) & Yes
(\emph{and efficient}) & Yes (\emph{and even more efficient}) \\
\textbf{How Many Ops} & \textasciitilde1400 & \textasciitilde130 &
\textasciitilde50 \\
\textbf{Native Quantization Tooling} & No & Yes & Yes \\
\end{longtable}

The principle is progressive constraint leading to progressive
optimization: fewer supported operations enable smaller binaries,
tighter memory budgets, and native quantization. Three dimensions
structure this analysis: model requirements (what operations must the
framework support?), software dependencies (what runtime environment is
available?), and hardware constraints (what are the physical limits?).

\subsection{Framework Selection
Criteria}\label{sec-ai-frameworks-model-requirements-2e01}

\textbf{Model Requirements.} The first question is whether a framework
can express the models your project requires. As
Table~\ref{tbl-tf-comparison} shows, operator count drops from
approximately \(10^3\) (full TensorFlow) to \(10^2\) (TensorFlow Lite)
to \(10^1\) (TensorFlow Lite Micro). Each reduction eliminates training
capability and general-purpose operations while adding native
quantization tooling. The engineering principle is that expressiveness
and efficiency trade against each other: fewer supported operations
enable tighter code generation, smaller binaries, and hardware-specific
optimization paths. This progressive constraint model applies to any
framework family, not just TensorFlow. The choice between \emph{dynamic
and static computational graphs} further shapes which optimizations each
constraint level permits.

\phantomsection\label{callout-perspectiveux2a-1.15}
\begin{fbx}{callout-perspective}{Systems Perspective:}{Dynamic vs Static Computational Graphs}
\phantomsection\label{callout-perspective*-1.15}
The static-versus-dynamic graph distinction (examined in
Section~\ref{sec-ai-frameworks-execution-problem-e1e1}) has direct
implications for model requirements analysis. Static graphs constrain
which operations are expressible but enable ahead-of-time optimization
for deployment. Dynamic graphs support arbitrary Python control flow but
require explicit compilation steps (e.g., \texttt{torch.compile},
\texttt{tf.function}) to recover optimization potential.

\end{fbx}

\textbf{Software Dependencies.} Once model requirements are satisfied,
the framework must integrate with the target software environment.
Table~\ref{tbl-tf-sw-comparison} reveals how operating system
requirements, memory management, and accelerator support vary across
TensorFlow variants.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2500}}@{}}
\caption{\textbf{TensorFlow Variant Capability Comparison.} Capabilities
of TensorFlow, TensorFlow Lite, and TensorFlow Lite Micro regarding
operating system dependence, memory management, and hardware
acceleration. Progressive constraint across variants enables selection
by deployment context, from full-scale servers to resource-constrained
edge devices.}\label{tbl-tf-sw-comparison}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{TensorFlow}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{TensorFlow Lite}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{TensorFlow Lite for Microcontrollers}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{TensorFlow}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{TensorFlow Lite}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{TensorFlow Lite for Microcontrollers}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Needs an OS} & Yes & Yes & No \\
\textbf{Memory Mapping of Models} & No & Yes & Yes \\
\textbf{Delegation to accelerators} & Yes & Yes & No \\
\end{longtable}

The key distinctions follow the same progressive constraint pattern.
TensorFlow Lite Micro eliminates the OS requirement entirely, enabling
bare-metal execution on microcontrollers (though it integrates with
RTOSes like FreeRTOS and Zephyr when available). Both Lite variants
support memory-mapped model access from flash storage, avoiding the RAM
overhead of loading full models. Accelerator delegation drops out at the
microcontroller tier, where specialized hardware is rarely available.
Each software dependency removed is a deployment target gained.

\textbf{Hardware Constraints.} Software compatibility alone does not
guarantee deployment; the framework must fit within physical hardware
limits. Table~\ref{tbl-tf-hw-comparison} quantifies this final
constraint dimension.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2500}}@{}}
\caption{\textbf{TensorFlow Hardware Optimization.} Resource
requirements (binary size and memory footprint) decrease across
TensorFlow variants as they target increasingly constrained hardware,
from servers to microcontrollers. Optimized architectures shift from
general-purpose CPUs and GPUs to ARM Cortex-M processors and digital
signal processors for resource-limited
environments.}\label{tbl-tf-hw-comparison}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{TensorFlow}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{TensorFlow Lite}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{TensorFlow Lite for Microcontrollers}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{TensorFlow}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{TensorFlow Lite}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{TensorFlow Lite for Microcontrollers}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Base Binary Size} & A few MB (varies by platform and build
configuration) & Tens to hundreds of KB & On the order of 10 KB \\
\textbf{Base Memory Footprint} & Several MB (minimum runtime overhead) &
Hundreds of KB & Tens of KB \\
\textbf{Optimized Architectures} & X86, TPUs, GPUs & Arm Cortex A, x86 &
Arm Cortex M, DSPs, MCUs \\
\end{longtable}

Binary size spans three orders of magnitude: from MB (full TensorFlow)
to tens of KB (TensorFlow Lite Micro). Memory footprint follows the same
pattern. Processor architecture support shifts correspondingly from
x86/GPU/TPU (data center) through Arm Cortex-A (mobile/edge) to Arm
Cortex-M, DSPs, and MCUs (embedded). The engineering lesson generalizes
beyond TensorFlow: every framework family that spans deployment tiers
makes analogous trade-offs between capability and resource footprint.

\textbf{Production-Ready Evaluation Factors.} The engineering principle
underlying production evaluation is that expressiveness and efficiency
trade against each other: fewer supported operations enable tighter code
generation, smaller binaries, and hardware-specific optimization paths.
Technical specifications establish necessary but not sufficient
conditions for selection. Production deployments also require evaluating
operational factors: migration cost (typically 3-6 engineer-months for
production systems, as noted in the Fallacies section), maintenance
burden, and deployment success rates.

\textbf{Performance Optimization.} Embedded performance spans four
coupled dimensions: inference latency (tens of milliseconds for mobile
image classification, sub-millisecond for industrial control), memory
footprint (MB for full TensorFlow down to tens of KB for TensorFlow Lite
Micro), power consumption (INT8 inference consuming several-fold less
energy than FP32 on mobile hardware), and hardware utilization (operator
fusion improving FLOPS utilization from 10--20\% to 60--80\% of peak).
These metrics are not independent: quantization simultaneously reduces
memory, latency, and energy at the cost of precision. Framework
selection determines which optimization levers are available.

\textbf{Deployment Scalability.} Scalability spans device scaling
(consistent deployment from microcontrollers to servers), operational
scaling (prototype to production transition), and version management
(model updates across deployed fleets). The three-dimension methodology
illustrated here (model requirements, software dependencies, hardware
constraints) applies to any framework ecosystem, not just TensorFlow.

\subsection{Development Support and Long-term Viability
Assessment}\label{sec-ai-frameworks-development-support-longterm-viability-assessment-d1d7}

What determines whether a framework remains viable five years into a
production deployment? Technical capabilities are necessary but not
sufficient; community health, ecosystem breadth, and organizational
alignment determine long-term success.

\textbf{Developer Resources.} Community composition shapes framework
evolution in measurable ways. PyTorch's academic community drives
research-oriented features and reproducibility tools, though production
tooling (PyTorch Lightning, TorchServe) has historically lagged.
TensorFlow's enterprise community emphasizes production reliability
through TFX pipelines, TensorBoard visualization, and TensorFlow Model
Analysis. JAX's smaller community concentrates on mathematical rigor and
program transformations, producing powerful research tools but with a
steeper onboarding curve.

\textbf{Supporting Infrastructure.} A framework's practical utility
often depends more on its ecosystem than its core capabilities. Hugging
Face provides consistent model APIs across all three major frameworks,
making pretrained model availability a near-commodity. Cross-framework
tools (Weights \& Biases, MLflow for experiment tracking; ONNX Runtime
for serving) reduce lock-in, while framework-native tools (XLA,
TorchScript, TensorFlow Serving) offer deeper optimization at the cost
of portability. Cloud ML services (SageMaker, Google AI Platform, Azure
ML) provide native integration for specific frameworks, creating
operational advantages that compound over time.

\textbf{Long-term Viability.} Framework viability depends on measurable
indicators: contributor diversity (single-company dependence is a risk),
backward compatibility track record, and hiring pool alignment with
organizational needs. Integration with existing CI/CD pipelines,
monitoring infrastructure, and cloud providers creates compounding
operational advantages that resist migration. The mitigation strategy is
defensive: use standardized formats (ONNX), maintain framework-agnostic
data pipelines, and document framework-specific customizations to
preserve future flexibility.

\section{Putting It All Together: Anatomy of a Training
Step}\label{sec-ai-frameworks-putting-together-anatomy-training-step-c7f1}

The preceding sections have examined framework selection criteria and
deployment considerations in the abstract. To solidify understanding of
how frameworks solve the three fundamental problems in practice, we
trace a single training step through the PyTorch stack. This case study
reveals how abstract Python code triggers concrete system interactions
across the memory hierarchy and accelerator.

Listing~\ref{lst-training-step-anatomy} presents a minimal training
iteration for a two-layer multilayer perceptron. Though only eight lines
of Python, this code exercises the entire framework stack: tensor
allocation, kernel dispatch, autograd recording, gradient computation,
and parameter updates. We will trace each phase to see the three
problems in action.

\begin{codelisting}

\caption{\label{lst-training-step-anatomy}\textbf{Training Step
Anatomy}: A minimal training iteration for a two-layer MLP, exercising
tensor allocation, kernel dispatch, autograd recording, gradient
computation, and parameter updates.}

\centering{

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Single training step for a 2{-}layer MLP}
\NormalTok{x }\OperatorTok{=}\NormalTok{ torch.randn(}\DecValTok{32}\NormalTok{, }\DecValTok{784}\NormalTok{, device}\OperatorTok{=}\StringTok{"cuda"}\NormalTok{)  }\CommentTok{\# Input batch}
\NormalTok{y }\OperatorTok{=}\NormalTok{ torch.randint(}\DecValTok{0}\NormalTok{, }\DecValTok{10}\NormalTok{, (}\DecValTok{32}\NormalTok{,), device}\OperatorTok{=}\StringTok{"cuda"}\NormalTok{)  }\CommentTok{\# Labels}

\CommentTok{\# Forward pass}
\NormalTok{h }\OperatorTok{=}\NormalTok{ torch.relu(x }\OperatorTok{@}\NormalTok{ W1 }\OperatorTok{+}\NormalTok{ b1)  }\CommentTok{\# Hidden layer}
\NormalTok{logits }\OperatorTok{=}\NormalTok{ h }\OperatorTok{@}\NormalTok{ W2 }\OperatorTok{+}\NormalTok{ b2  }\CommentTok{\# Output layer}
\NormalTok{loss }\OperatorTok{=}\NormalTok{ F.cross\_entropy(logits, y)}

\CommentTok{\# Backward pass}
\NormalTok{loss.backward()}

\CommentTok{\# Parameter update}
\NormalTok{optimizer.step()}
\end{Highlighting}
\end{Shaded}

}

\end{codelisting}%

\textbf{Phase 1: Forward Pass (Solving the Execution Problem).} When
\texttt{h\ =\ torch.relu(x\ @\ W1\ +\ b1)} executes, PyTorch's eager
execution triggers immediate computation:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Python Dispatch} (\textasciitilde1μs): Python interpreter
  calls \texttt{torch.matmul}, which routes through PyTorch's dispatcher
  to select the CUDA backend.
\item
  \textbf{Kernel Selection} (\textasciitilde0.5μs): cuBLAS selects an
  optimized GEMM kernel based on matrix dimensions (32×784 × 784×256).
  For these dimensions, it might choose a tiled algorithm optimized for
  L2 cache.
\item
  \textbf{Kernel Launch} (\textasciitilde5μs): The selected kernel is
  queued to the GPU's command buffer. The CPU continues immediately
  (asynchronous execution).
\item
  \textbf{GPU Execution} (\textasciitilde15μs):

  \begin{itemize}
  \tightlist
  \item
    Load W1 from HBM\sidenote{HBM (introduced in
    \textbf{?@sec-dnn-architectures}) provides 2-3 TB/s bandwidth on
    modern GPUs. For framework execution, HBM bandwidth determines
    whether operations are memory-bound or compute-bound. The 80 GB
    capacity on an A100 sets practical limits on model size, as weights,
    activations, and gradients must all fit in HBM during execution. }
    to L2 cache (\textasciitilde200GB/s effective bandwidth)
  \item
    Perform matrix multiply in tensor cores (if available)
  \item
    Write result to HBM
  \end{itemize}
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{4}
\tightlist
\item
  \textbf{Autograd Recording}: Simultaneously, PyTorch's autograd engine
  records a \texttt{MmBackward} node on the tape, storing references to
  \texttt{x} and \texttt{W1} for gradient computation.
\end{enumerate}

The bias addition and ReLU follow similar patterns, each adding a node
to the autograd tape.

\textbf{Phase 2: Backward Pass (Solving the Differentiation Problem).}
When \texttt{loss.backward()} executes:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Tape Traversal}: The autograd engine traverses the recorded
  graph in reverse topological order.
\item
  \textbf{Gradient Computation}: For each node, it calls the registered
  backward function:

  \begin{itemize}
  \tightlist
  \item
    \texttt{CrossEntropyBackward}: Computes
    \(\frac{\partial L}{\partial \text{logits}}\) using softmax
    derivative
  \item
    \texttt{MmBackward} (W2): Computes
    \(\frac{\partial L}{\partial W_2} = h^T \cdot \frac{\partial L}{\partial \text{logits}}\)
    and \(\frac{\partial L}{\partial h}\)
  \item
    \texttt{ReluBackward}: Applies ReLU derivative mask (zero where h ≤
    0)
  \item
    \texttt{MmBackward} (W1): Computes
    \(\frac{\partial L}{\partial W_1}\) and
    \(\frac{\partial L}{\partial x}\)
  \end{itemize}
\item
  \textbf{Gradient Accumulation}: Gradients are accumulated into
  \texttt{.grad} attributes of leaf tensors.
\item
  \textbf{Memory Management}: After each backward node completes, its
  saved tensors are freed, allowing memory reuse.
\end{enumerate}

\textbf{Phase 3: Memory Traffic Analysis (The Physics at Work).}
Applying the Dispatch Overhead Equation
(Equation~\ref{eq-dispatch-overhead}) to this step:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2500}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2500}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2500}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2500}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Component}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{FLOPs}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Memory Traffic}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Arithmetic Intensity}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{MatMul (x @ W1)} & 2×32×784×256 = 12.8M & \textasciitilde1.6 MB
& 8.0 \\
\textbf{ReLU} & 32×256 = 8K & 64 KB & 0.125 \\
\textbf{MatMul (h @ W2)} & 2×32×256×10 = 164K & \textasciitilde40 KB &
4.1 \\
\textbf{Cross-entropy} & \textasciitilde10K & \textasciitilde1 KB &
10.0 \\
\textbf{Backward (2× forward)} & \textasciitilde26M & \textasciitilde3.2
MB & \textasciitilde8.0 \\
\end{longtable}

Total: \textasciitilde40M FLOPs, \textasciitilde5MB memory traffic. On
an A100:

\begin{itemize}
\tightlist
\item
  \(T_{\text{compute}} \approx 40\text{M} / 312\text{T} \approx 0\.1\mu\text{s}\)
\item
  \(T_{\text{memory}} \approx 5\text{MB} / 2\text{TB/s} \approx 2\.5\mu\text{s}\)
\item
  \(T_{\text{overhead}} \approx 6\text{ ops} \times 5\mu\text{s} \approx 30\mu\text{s}\)
\end{itemize}

\textbf{The training step is overhead-bound!} For small models, Python
dispatch and kernel launch dominate. This explains why:

\begin{itemize}
\tightlist
\item
  \texttt{torch.compile} provides 2-3× speedup by fusing operations and
  reducing kernel launches
\item
  Batch size increases help amortize per-batch overhead
\item
  Production training uses much larger models where compute dominates
\end{itemize}

\textbf{Phase 4: Hardware Abstraction (Solving the Abstraction
Problem).} The same Python code runs on different hardware through
abstraction layers:

\begin{itemize}
\tightlist
\item
  \textbf{CUDA GPU}: cuBLAS GEMM kernels, CUDA streams for async
  execution
\item
  \textbf{CPU}: Intel MKL or OpenBLAS, OpenMP for parallelism
\item
  \textbf{TPU}: XLA compilation to TPU-specific HLO operations
\item
  \textbf{Apple Silicon}: Metal Performance Shaders via MPS backend
\end{itemize}

Each backend implements the same tensor operations with
hardware-specific optimizations. The framework's abstraction layer
(Section~\ref{sec-ai-frameworks-abstraction-problem-37a5}) ensures
identical numerical results (within floating-point tolerance) across
platforms.

\phantomsection\label{callout-perspectiveux2a-1.16}
\begin{fbx}{callout-perspective}{Systems Perspective:}{The Three Problems in Action}
\phantomsection\label{callout-perspective*-1.16}
This trace reveals the three problems in concrete terms:

\begin{itemize}
\tightlist
\item
  \textbf{Execution}: Eager mode enables line-by-line debugging but
  incurs dispatch overhead
\item
  \textbf{Differentiation}: Autograd tape records operations during
  forward, replays in reverse during backward
\item
  \textbf{Abstraction}: Same code runs on GPU/CPU/TPU through
  backend-specific kernel implementations
\end{itemize}

Understanding this flow enables informed optimization: fuse operations
to reduce overhead, use appropriate batch sizes, and match model scale
to hardware capabilities.

\end{fbx}

\section{Fallacies and
Pitfalls}\label{sec-ai-frameworks-fallacies-pitfalls-61ef}

The training step analysis above demonstrates how abstract framework
concepts translate to concrete system behavior. With this foundation, we
can identify common reasoning errors, each accompanied by quantitative
evidence from earlier sections.

\paragraph*{\texorpdfstring{Fallacy: \emph{``All frameworks provide
equivalent performance for the same model
architecture.''}}{Fallacy: ``All frameworks provide equivalent performance for the same model architecture.''}}\label{fallacy-all-frameworks-provide-equivalent-performance-for-the-same-model-architecture.}
\addcontentsline{toc}{paragraph}{Fallacy: \emph{``All frameworks provide
equivalent performance for the same model architecture.''}}

Engineers assume that since ResNet-50 is mathematically identical across
frameworks, performance must be equivalent.
Table~\ref{tbl-framework-efficiency-matrix} disproves this: PyTorch
achieves 52ms inference at 32\% hardware utilization, while TensorRT
delivers 3ms at 88\% utilization, a \textbf{17x performance gap} on
identical hardware. The difference arises from kernel fusion, graph
optimization depth, and memory access patterns. Organizations assuming
framework equivalence routinely miss production targets by 5-10×.

\paragraph*{\texorpdfstring{Pitfall: \emph{Choosing frameworks based on
popularity rather than project
requirements.}}{Pitfall: Choosing frameworks based on popularity rather than project requirements.}}\label{pitfall-choosing-frameworks-based-on-popularity-rather-than-project-requirements.}
\addcontentsline{toc}{paragraph}{Pitfall: \emph{Choosing frameworks
based on popularity rather than project requirements.}}

Table~\ref{tbl-framework-efficiency-matrix} shows the deployment
spectrum: PyTorch Mobile requires 220MB memory, TensorFlow Lite needs
180MB, and TensorFlow Lite Micro runs in 32KB, a \textbf{7040x
difference}. Teams building edge applications with PyTorch face either
massive memory overhead or costly framework migration. Match framework
capabilities to deployment constraints \emph{before} development begins.

\paragraph*{\texorpdfstring{Fallacy: \emph{``Framework abstractions
eliminate the need for systems
knowledge.''}}{Fallacy: ``Framework abstractions eliminate the need for systems knowledge.''}}\label{fallacy-framework-abstractions-eliminate-the-need-for-systems-knowledge.}
\addcontentsline{toc}{paragraph}{Fallacy: \emph{``Framework abstractions
eliminate the need for systems knowledge.''}}

The Roofline Model disproves this fallacy. Element-wise operations like
ReLU achieve arithmetic intensity of 0.125 FLOPS/byte, utilizing
\textbf{under 0.1\%} of an A100's peak compute regardless of framework.
Understanding when operations are memory-bound vs.~compute-bound and
which sequences can be fused separates efficient implementations from
those leaving 80-90\% of hardware capacity unused. No framework can
optimize away fundamental physics.

\paragraph*{\texorpdfstring{Pitfall: \emph{Ignoring vendor lock-in from
framework-specific
formats.}}{Pitfall: Ignoring vendor lock-in from framework-specific formats.}}\label{pitfall-ignoring-vendor-lock-in-from-framework-specific-formats.}
\addcontentsline{toc}{paragraph}{Pitfall: \emph{Ignoring vendor lock-in
from framework-specific formats.}}

Converting TensorFlow SavedModel to PyTorch requires: rewriting custom
operations, validating numerical equivalence across 10,000+ test cases,
and retraining when operations lack exact equivalents, typically
\textbf{3-6 engineer-months} for production systems. ONNX provides
portability but supports only 80-85\% of operations. Design for
portability from the start by avoiding framework-specific features where
standard alternatives exist.

\paragraph*{\texorpdfstring{Pitfall: \emph{Selecting development
frameworks without evaluating production
infrastructure.}}{Pitfall: Selecting development frameworks without evaluating production infrastructure.}}\label{pitfall-selecting-development-frameworks-without-evaluating-production-infrastructure.}
\addcontentsline{toc}{paragraph}{Pitfall: \emph{Selecting development
frameworks without evaluating production infrastructure.}}

Framework-infrastructure mismatches impose substantial operational
overhead. TensorFlow Serving provides atomic model swaps with zero
downtime; PyTorch deployments often require container restarts imposing
30-60 second outages. TensorFlow integrates natively with
Prometheus/OpenTelemetry; PyTorch requires custom instrumentation adding
2-4 weeks of development. Evaluate the \emph{complete deployment stack},
not just training APIs.

\paragraph*{\texorpdfstring{Fallacy: \emph{``Increasing batch size is a
free throughput optimization within framework memory
limits.''}}{Fallacy: ``Increasing batch size is a free throughput optimization within framework memory limits.''}}\label{fallacy-increasing-batch-size-is-a-free-throughput-optimization-within-framework-memory-limits.}
\addcontentsline{toc}{paragraph}{Fallacy: \emph{``Increasing batch size
is a free throughput optimization within framework memory limits.''}}

The Dispatch Overhead Equation (Equation~\ref{eq-dispatch-overhead})
reveals hidden costs that framework memory reporting obscures. A 7B
parameter model in FP16 consumes 14GB, leaving 66GB on an A100-80GB.
Frameworks report sufficient free memory for batch size 32, but
increasing from 8 to 32 quadruples activation memory for transformers
(due to attention's \(O(S^2)\) scaling), potentially triggering
memory-saving recomputation strategies (trading extra computation to
reduce memory usage) that \textbf{reduce throughput by 20-30\%} despite
the larger batch. Eager-mode frameworks compound this with per-operation
allocation overhead that scales with batch size, while graph-mode
frameworks may silently insert memory-management operations that add
latency. The optimal batch size balances compute saturation (70-80\%
utilization), activation memory, and framework allocator overhead.
Blindly maximizing batch size based on reported memory availability
often achieves \emph{lower} throughput than smaller batches that avoid
triggering these framework-level memory management pathways.

\paragraph*{\texorpdfstring{Pitfall: \emph{Treating compilation overhead
as
negligible.}}{Pitfall: Treating compilation overhead as negligible.}}\label{pitfall-treating-compilation-overhead-as-negligible.}
\addcontentsline{toc}{paragraph}{Pitfall: \emph{Treating compilation
overhead as negligible.}}

Table~\ref{tbl-training-benchmark} shows torch.compile achieves 48\%
higher ResNet-50 throughput than eager PyTorch, but incurs 15-60 seconds
compilation overhead. For a 10,000-image experiment requiring 10
recompilations due to code changes:

\begin{itemize}
\tightlist
\item
  Eager: \(10{,}000 / 1{,}450 = 6\.9\) seconds
\item
  Compiled: \(10{,}000 / 2{,}150 + 10 \times 30 = 304\.7\) seconds
\end{itemize}

Compilation pays off only when amortized over long training runs. For
rapid prototyping with frequent changes, eager execution is often faster
\emph{end-to-end}.

\section{Summary}\label{sec-ai-frameworks-summary-07f0}

Machine learning frameworks exist to solve three fundamental problems
that would otherwise make deep learning impractical:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{The Execution Problem}: When and how should computation
  happen? Frameworks navigate the trade-off between eager execution
  (immediate, debuggable, flexible) and graph execution (deferred,
  optimizable, deployable). Modern hybrid approaches like
  \texttt{torch.compile} attempt to provide both flexibility during
  development and optimization for production.
\item
  \textbf{The Differentiation Problem}: How do we compute gradients
  automatically? Frameworks implement reverse-mode automatic
  differentiation that computes exact gradients for arbitrary operation
  compositions. This transforms the mathematical chain rule into a
  software primitive, enabling training on billions of parameters with a
  single \texttt{loss.backward()} call.
\item
  \textbf{The Abstraction Problem}: How do we target diverse hardware
  from a single interface? Frameworks provide tensor abstractions,
  intermediate representations, and runtime systems that hide hardware
  complexity while enabling efficient utilization across CPUs, GPUs,
  TPUs, and specialized accelerators.
\end{enumerate}

These problems are interconnected and constrained by physics,
particularly the memory wall that makes data movement often more
expensive than computation. Understanding these constraints explains why
frameworks invest in kernel fusion, memory-efficient attention, and
compilation pipelines.

\phantomsection\label{callout-takeawaysux2a-1.17}
\begin{fbx}{callout-takeaways}{Takeaways:}{Key Takeaways}
\phantomsection\label{callout-takeaways*-1.17}

\begin{itemize}
\tightlist
\item
  \textbf{Three problems define every framework}: Execution (how to
  run), differentiation (how to train), and abstraction (how to
  express). Design choices in each area determine framework behavior.
\item
  \textbf{The memory wall drives optimization}: Compute has grown
  approximately 1000× faster than memory bandwidth. Kernel fusion,
  operator scheduling, and data layout optimizations are designed to
  mitigate memory latency.
\item
  \textbf{Eager vs.~graph is a fundamental trade-off}: Eager execution
  enables debugging and flexibility; graph execution enables
  optimization and deployment. Hybrid approaches (``write eager, deploy
  compiled'') offer both.
\item
  \textbf{Framework choice constrains deployment}: TensorFlow Lite for
  mobile, CoreML for Apple, ONNX for portability. The training framework
  may differ from the deployment framework.
\item
  \textbf{Inadequate framework understanding results in 5--10×
  performance degradation}: Treating frameworks as black boxes, ignoring
  deployment constraints, or assuming equivalence leads to
  order-of-magnitude inefficiencies.
\end{itemize}

\end{fbx}

Understanding framework internals transforms how practitioners approach
performance debugging and optimization. When a training job runs slower
than expected, engineers who understand execution graphs can identify
whether the bottleneck lies in eager-mode overhead, insufficient kernel
fusion, or suboptimal memory layout. When deployment fails on target
hardware, understanding the compilation pipeline reveals whether the
issue is operator support, quantization compatibility, or runtime
configuration. Understanding framework internals is therefore essential
for diagnosing and resolving performance issues in production systems.

\phantomsection\label{callout-chapter-connectionux2a-1.18}
\begin{fbxSimple}{callout-chapter-connection}{What’s Next:}{From Control Room to Power Plant}
\phantomsection\label{callout-chapter-connection*-1.18}
We have established the software substrate of ML: the frameworks that
translate abstract architectures into executable kernels. But a control
room without a source of energy is just a room with glowing lights. We
turn next to \textbf{?@sec-ai-training}, where we scale these frameworks
across massive datasets and hardware to build the power plant of modern
AI.

\end{fbxSimple}

\phantomsection\label{refs}
\begin{CSLReferences}{1}{0}
\bibitem[\citeproctext]{ref-abadi2016tensorflow}
Abadi, Martín, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen,
Craig Citro, Greg S. Corrado, et al. 2016. {``TensorFlow: Large-Scale
Machine Learning on Heterogeneous Distributed Systems.''} \emph{arXiv
Preprint arXiv:1603.04467}, March.
\url{http://arxiv.org/abs/1603.04467v2}.

\bibitem[\citeproctext]{ref-ansel2024pytorch2}
Ansel, Jason, Edward Yang, Horace He, Natalia Gimelshein, Animesh Jain,
Michael Voznesensky, Bin Bao, et al. 2024. {``PyTorch 2: Faster Machine
Learning Through Dynamic Python Bytecode Transformation and Graph
Compilation.''} In \emph{Proceedings of the 29th ACM International
Conference on Architectural Support for Programming Languages and
Operating Systems, Volume 2}, 929--47. ACM.
\url{https://doi.org/10.1145/3620665.3640366}.

\bibitem[\citeproctext]{ref-baydin2018}
Baydin, Atilim Gunes, Barak A. Pearlmutter, Alexey Andreyevich Radul,
and Jeffrey Mark Siskind. 2017. {``Automatic Differentiation in Machine
Learning: A Survey.''} \emph{J. Mach. Learn. Res.} 18 (153): 153:1--43.
\url{https://jmlr.org/papers/v18/17-468.html}.

\bibitem[\citeproctext]{ref-bergstra2010theano}
Bergstra, James, Olivier Breuleux, Frédéric Bastien, Pascal Lamblin,
Razvan Pascanu, Guillaume Desjardins, Joseph Turian, David Warde-Farley,
and Yoshua Bengio. 2010. {``Theano: A CPU and GPU Math Compiler in
Python.''} In \emph{Proceedings of the 9th Python in Science
Conference}, 4:18--24. 1. SciPy.
\url{https://doi.org/10.25080/majora-92bf1922-003}.

\bibitem[\citeproctext]{ref-jax2018github}
Bradbury, James, Roy Frostig, Peter Hawkins, Matthew James Johnson,
Chris Leary, Dougal Maclaurin, George Necula, et al. 2018. {``JAX:
Composable Transformations of Python+NumPy Programs.''}
\url{http://github.com/google/jax}.

\bibitem[\citeproctext]{ref-chen2016training}
Chen, Tianqi, Bing Xu, Chiyuan Zhang, and Carlos Guestrin. 2016.
{``Training Deep Nets with Sublinear Memory Cost.''} \emph{arXiv
Preprint arXiv:1604.06174}, April.
\url{http://arxiv.org/abs/1604.06174v2}.

\bibitem[\citeproctext]{ref-chetlur2014cudnn}
Chetlur, Sharan, Cliff Woolley, Philippe Vandermersch, Jonathan Cohen,
John Tran, Bryan Catanzaro, and Evan Shelhamer. 2014. {``cuDNN:
Efficient Primitives for Deep Learning.''} \emph{arXiv Preprint
arXiv:1410.0759}, October. \url{http://arxiv.org/abs/1410.0759v3}.

\bibitem[\citeproctext]{ref-bai2019onnx}
Flint, Claas, Micah Cearns, Nils Opel, Ronny Redlich, David M. A.
Mehler, Daniel Emden, Nils R. Winter, et al. 2019. {``Systematic
Misestimation of Machine Learning Performance in Neuroimaging Studies of
Depression.''} \emph{arXiv Preprint arXiv:1912.06686}, December.
\url{http://arxiv.org/abs/1912.06686v2}.

\bibitem[\citeproctext]{ref-GoogleXLA}
Google. 2025. {``XLA: Optimizing Compiler for Machine Learning.''}
\url{https://tensorflow.org/xla}.

\bibitem[\citeproctext]{ref-lawson1979blas}
Lawson, C. L., R. J. Hanson, D. R. Kincaid, and F. T. Krogh. 1979.
{``Basic Linear Algebra Subprograms for Fortran Usage.''} \emph{ACM
Transactions on Mathematical Software} 5 (3): 308--23.
\url{https://doi.org/10.1145/355841.355847}.

\bibitem[\citeproctext]{ref-li2014communication}
Li, Mu, David G. Andersen, Jun Woo Park, Alexander J. Smola, Amr Ahmed,
Vanja Josifovski, James Long, Eugene J. Shekita, and Bor-Yiing Su. 2014.
{``Communication Efficient Distributed Machine Learning with the
Parameter Server.''} In \emph{Advances in Neural Information Processing
Systems}, 27:19--27.
\url{https://proceedings.neurips.cc/paper/2014/hash/a49e9411d64ff53eccfdd09ad10a15b3-Abstract.html}.

\bibitem[\citeproctext]{ref-mcmahan2023communicationefficient}
McMahan, Brendan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise
Agüera y Arcas. 2017. {``Communication-Efficient Learning of Deep
Networks from Decentralized Data.''} In \emph{Proceedings of the 20th
International Conference on Artificial Intelligence and Statistics,
AISTATS 2017, 20-22 April 2017, Fort Lauderdale, FL, USA}, edited by
Aarti Singh and Xiaojin (Jerry) Zhu, 54:1273--82. Proceedings of Machine
Learning Research. PMLR.
\url{http://proceedings.mlr.press/v54/mcmahan17a.html}.

\bibitem[\citeproctext]{ref-paszke2019pytorch}
Paszke, Adam, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury,
Gregory Chanan, Trevor Killeen, et al. 2019. {``PyTorch: An Imperative
Style, High-Performance Deep Learning Library.''} In \emph{Advances in
Neural Information Processing Systems}, 32:8024--35.
\url{https://proceedings.neurips.cc/paper/2019/hash/bdbca288fee7f92f2bfa9f7012727740-Abstract.html}.

\bibitem[\citeproctext]{ref-al2016theano}
Team, The Theano Development, Rami Al-Rfou, Guillaume Alain, Amjad
Almahairi, Christof Angermueller, Dzmitry Bahdanau, Nicolas Ballas, et
al. 2016. {``Theano: A Python Framework for Fast Computation of
Mathematical Expressions,''} May.
\url{http://arxiv.org/abs/1605.02688v1}.

\end{CSLReferences}


\backmatter

\clearpage


\end{document}
