% Options for packages loaded elsewhere
% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode,linktoc=all,pdfwindowui,pdfpagemode=FullScreen,pdfpagelayout=TwoPageRight}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  9pt,
  letterpaper,
  abstract,
  titlepage]{scrbook}
\usepackage{xcolor}
\usepackage[left=1in,marginparwidth=2.0666666666667in,textwidth=4.1333333333333in,marginparsep=0.3in]{geometry}
\usepackage{amsmath,amssymb}
\setcounter{secnumdepth}{3}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
% Make \paragraph and \subparagraph free-standing
\makeatletter
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}{
    \@ifstar
      \xxxParagraphStar
      \xxxParagraphNoStar
  }
  \newcommand{\xxxParagraphStar}[1]{\oldparagraph*{#1}\mbox{}}
  \newcommand{\xxxParagraphNoStar}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}{
    \@ifstar
      \xxxSubParagraphStar
      \xxxSubParagraphNoStar
  }
  \newcommand{\xxxSubParagraphStar}[1]{\oldsubparagraph*{#1}\mbox{}}
  \newcommand{\xxxSubParagraphNoStar}[1]{\oldsubparagraph{#1}\mbox{}}
\fi
\makeatother

\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{242,244,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{#1}}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.82,0.10,0.26}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{#1}}
\newcommand{\BuiltInTok}[1]{\textcolor[rgb]{0.88,0.40,0.10}{#1}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.41,0.45,0.49}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.86,0.20,0.18}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.58,0.00,0.30}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{#1}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.75,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.88,0.40,0.10}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.42,0.37,0.78}{#1}}
\newcommand{\ImportTok}[1]{\textcolor[rgb]{0.00,0.45,0.70}{\textbf{#1}}}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.73,0.49,0.84}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.10,0.10,0.10}{#1}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{#1}}
\newcommand{\RegionMarkerTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.12,0.55,0.30}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{#1}}}}

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\newsavebox\pandoc@box
\newcommand*\pandocbounded[1]{% scales image to fit in text height/width
  \sbox\pandoc@box{#1}%
  \Gscale@div\@tempa{\textheight}{\dimexpr\ht\pandoc@box+\dp\pandoc@box\relax}%
  \Gscale@div\@tempb{\linewidth}{\wd\pandoc@box}%
  \ifdim\@tempb\p@<\@tempa\p@\let\@tempa\@tempb\fi% select the smaller of both
  \ifdim\@tempa\p@<\p@\scalebox{\@tempa}{\usebox\pandoc@box}%
  \else\usebox{\pandoc@box}%
  \fi%
}
% Set default figure placement to htbp
\def\fps@figure{htbp}
\makeatother
% definitions for citeproc citations
\NewDocumentCommand\citeproctext{}{}
\NewDocumentCommand\citeproc{mm}{%
  \begingroup\def\citeproctext{#2}\cite{#1}\endgroup}
\makeatletter
 % allow citations to break across lines
 \let\@cite@ofmt\@firstofone
 % avoid brackets around text for \cite:
 \def\@biblabel#1{}
 \def\@cite#1#2{{#1\if@tempswa , #2\fi}}
\makeatother
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newenvironment{CSLReferences}[2] % #1 hanging-indent, #2 entry-spacing
 {\begin{list}{}{%
  \setlength{\itemindent}{0pt}
  \setlength{\leftmargin}{0pt}
  \setlength{\parsep}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1
   \setlength{\leftmargin}{\cslhangindent}
   \setlength{\itemindent}{-1\cslhangindent}
  \fi
  % set entry spacing
  \setlength{\itemsep}{#2\baselineskip}}}
 {\end{list}}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{\hfill\break\parbox[t]{\linewidth}{\strut\ignorespaces#1\strut}}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}

% =============================================================================
% LATEX HEADER CONFIGURATION FOR MLSYSBOOK PDF
% =============================================================================
% This file contains all LaTeX package imports, custom commands, and styling
% definitions for the PDF output of the Machine Learning Systems textbook.
%
% Key Features:
% - Harvard crimson branding throughout
% - Custom part/chapter/section styling
% - Professional table formatting with colored headers
% - Margin notes with custom styling
% - TikZ-based part dividers
% - Page numbering (Roman for frontmatter, Arabic for mainmatter)
%
% Note: This file is included via _quarto-pdf.yml and affects PDF output only.
% HTML/EPUB styling is handled separately via CSS files.
% =============================================================================

% =============================================================================
% PACKAGE IMPORTS
% =============================================================================

% Layout and positioning
% \usepackage[outercaption, ragged]{sidecap}  % Commented out to make figure captions inline instead of in margin
\usepackage{adjustbox}      % Adjusting box dimensions
\usepackage{afterpage}      % Execute commands after page break
\usepackage{morefloats}     % Increase number of floats
\usepackage{array}          % Enhanced table column formatting
\usepackage{atbegshi}       % Insert content at page beginning
%\usepackage{changepage}     % Change page dimensions mid-document
\usepackage{emptypage}      % Clear headers/footers on empty pages

% Language and text
\usepackage[english]{babel} % English language support
\usepackage{microtype}      % Improved typography and hyphenation

% Captions and floats
\usepackage{caption}
% Caption styling configuration
%\captionsetup[table]{belowskip=5pt}
\captionsetup{format=plain}
\DeclareCaptionLabelFormat{mylabel}{#1
#2:\hspace{1.0ex}}
\DeclareCaptionFont{ninept}{\fontsize{7pt}{8}\selectfont #1}

% Figure captions: Small font, bold label, ragged right
\captionsetup[figure]{labelfont={bf,ninept},labelsep=space,
belowskip=2pt,aboveskip=6pt,labelformat=mylabel,
justification=raggedright,singlelinecheck=false,font={ninept}}

% Table captions: Small font, bold label, ragged right
\captionsetup[table]{belowskip=6pt,labelfont={bf,ninept},labelsep=none,
labelformat=mylabel,justification=raggedright,singlelinecheck=false,font={ninept}}

% Typography fine-tuning
\emergencystretch=5pt       % Allow extra stretch to avoid overfull boxes

% Utility packages
\usepackage{etoolbox}       % For patching commands and environments

% Page layout and headers
\usepackage{fancyhdr}       % Custom headers and footers
\usepackage{geometry}       % Page dimensions and margins

% Graphics and figures
\usepackage{graphicx}       % Include graphics
\usepackage{float}          % Improved float placement
\usepackage[skins,breakable]{tcolorbox} % Coloured and framed text boxes
\tcbset{before upper=\setlength{\parskip}{3pt}}

% Tables
\usepackage{longtable}      % Multi-page tables

% Fonts and typography
\usepackage{fontspec}       % Font selection for LuaLaTeX
\usepackage{mathptmx}       % Times-like math fonts
\usepackage{newpxtext}      % Palatino-like font for body text

% Colors and visual elements
\usepackage[dvipsnames]{xcolor}  % Extended color support
\usepackage{tikz}           % Programmatic graphics
\usetikzlibrary{positioning}
\usetikzlibrary{calc}
\usepackage{tikzpagenodes}  % TikZ positioning relative to page

% Code listings
\usepackage{listings}       % Code highlighting

% Hyperlinks
\usepackage{hyperref}       % Clickable links in PDF

% Conditional logic
\usepackage{ifthen}         % If-then-else commands

% Math symbols
\usepackage{amsmath}        % AMS math extensions
\usepackage{amssymb}        % AMS math symbols
\usepackage{latexsym}       % Additional LaTeX symbols
\usepackage{pifont}         % Zapf Dingbats symbols
\providecommand{\blacklozenge}{\ding{117}}  % Black diamond symbol

% Lists
\usepackage{enumitem}       % Customizable lists

% Margin notes and sidenotes
\usepackage{marginfix}      % Fixes margin note overflow
\usepackage{marginnote}     % Margin notes
\usepackage{sidenotes}      % Academic-style sidenotes
\renewcommand\raggedrightmarginnote{\sloppy}
\renewcommand\raggedleftmarginnote{\sloppy}

% Typography improvements
\usepackage{ragged2e}       % Better ragged text
\usepackage[all]{nowidow}   % Prevent widows and orphans
\usepackage{needspace}      % Ensure minimum space on page

% Section formatting
\usepackage[explicit]{titlesec}  % Custom section titles
\usepackage{tocloft}        % Table of contents formatting

% QR codes and icons
\usepackage{fontawesome5}   % Font Awesome icons
\usepackage{qrcode}         % QR code generation
\qrset{link, height=15mm}

% =============================================================================
% FLOAT CONFIGURATION
% =============================================================================
% Allow more floats per page to handle figure-heavy chapters
\extrafloats{200}
\setcounter{topnumber}{12}       % Max floats at top of page
\setcounter{bottomnumber}{12}    % Max floats at bottom of page
\setcounter{totalnumber}{24}     % Max floats per page
\setcounter{dbltopnumber}{8}     % Max floats at top of two-column page
\renewcommand{\topfraction}{.95}  % Max fraction of page for top floats
\renewcommand{\bottomfraction}{.95}
\renewcommand{\textfraction}{.05}  % Min fraction of page for text
\renewcommand{\floatpagefraction}{.7}  % Min fraction of float page
\renewcommand{\dbltopfraction}{.95}

% Prevent "Float(s) lost" errors by flushing floats more aggressively
\usepackage{placeins}  % Provides \FloatBarrier

% =============================================================================
% COLOR DEFINITIONS
% =============================================================================
% NOTE: TikZ colors (BlueLine, GreenLine, RedLine, OrangeLine, etc.) are defined
% in the YAML config files under format > pdf > tikz > include-headers.
% Only colors specific to LaTeX packages (not TikZ) are defined here.

% Harvard crimson - primary brand color used throughout
\definecolor{crimson}{HTML}{A51C30}

% Quiz element colors
\definecolor{quiz-question-color1}{RGB}{225,243,248}  % Light blue background
\definecolor{quiz-question-color2}{RGB}{17,158,199}   % Blue border
\definecolor{quiz-answer-color1}{RGB}{250,234,241}    % Light pink background
\definecolor{quiz-answer-color2}{RGB}{152,14,90}      % Magenta border

% =============================================================================
% LIST FORMATTING
% =============================================================================
% Tighter list spacing for academic style
\def\tightlist{}
\setlist{itemsep=1pt, parsep=1pt, topsep=0pt,after={\vspace{0.3\baselineskip}}}
\let\tightlist\relax

\makeatletter
\@ifpackageloaded{framed}{}{\usepackage{framed}}
\@ifpackageloaded{fancyvrb}{}{\usepackage{fancyvrb}}
\makeatother

\makeatletter
%New float "codelisting" has been updated
\AtBeginDocument{%
\floatstyle{ruled}
\newfloat{codelisting}{!htb}{lop}
\floatname{codelisting}{Listing}
\floatplacement{codelisting}{!htb}
\captionsetup[codelisting]{labelfont={bf,ninept},labelformat=mylabel,
  singlelinecheck=false,width=\linewidth,labelsep=none,font={ninept}}%
\renewenvironment{snugshade}{%
   \def\OuterFrameSep{3pt}%
   \def\FrameCommand{\fboxsep=5pt\colorbox{shadecolor}}%
   \MakeFramed{\advance\hsize-\width\FrameRestore}%
   \leftskip 0.5em \rightskip 0.5em%
   \small% decrease font size
   }{\endMakeFramed}%
}
\makeatother

%The space before and after the verbatim environment "Highlighting" has been reduced
\fvset{listparameters=\setlength{\topsep}{0pt}\setlength{\partopsep}{0pt}}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{framesep=0mm,commandchars=\\\{\}}

\makeatletter
\renewcommand\fs@ruled{\def\@fs@cfont{\bfseries}\let\@fs@capt\floatc@ruled
\def\@fs@pre{\hrule height.8pt depth0pt \kern2pt}%
\def\@fs@post{\kern2pt\hrule\relax}%
\def\@fs@mid{\kern2pt\hrule\kern1pt}%space between float and caption
\let\@fs@iftopcapt\iftrue}
\makeatother


% =============================================================================
% HYPHENATION RULES
% =============================================================================
% Explicit hyphenation points for technical terms to avoid bad breaks
\hyphenation{
  light-weight
  light-weight-ed
  de-vel-op-ment
  un-der-stand-ing
  mod-els
  prin-ci-ples
  ex-per-tise
  com-pli-cat-ed
  blue-print
  per‧for‧mance
  com-mu-ni-ca-tion
  par-a-digms
  hy-per-ten-sion
  a-chieved
}

% =============================================================================
% CODE LISTING CONFIGURATION
% =============================================================================
% Settings for code blocks using listings package
\lstset{
breaklines=true,              % Automatic line wrapping
breakatwhitespace=true,       % Break at whitespace only
basicstyle=\ttfamily,         % Monospace font
frame=none,                   % No frame around code
keepspaces=true,              % Preserve spaces
showspaces=false,             % Don't show space characters
showtabs=false,               % Don't show tab characters
columns=flexible,             % Flexible column width
belowskip=0pt,               % Minimal spacing
aboveskip=0pt
}

% =============================================================================
% PAGE GEOMETRY
% =============================================================================
% MIT Press trim size: 7" x 10" (per publisher specifications)
% This is a standard academic textbook format providing good readability
% for technical content with figures and code blocks.
% Wide outer margin accommodates sidenotes/margin notes.
\geometry{
  paperwidth=7in,
  paperheight=10in,
  top=0.875in,
  bottom=0.875in,
  inner=0.875in,              % Inner margin (binding side)
  outer=1.75in,               % Outer margin (includes space for sidenotes)
  footskip=30pt,
  marginparwidth=1.25in,      % Width for margin notes
  twoside                     % Different left/right pages
}

% =============================================================================
% SIDENOTE STYLING
% =============================================================================
% Custom sidenote design with crimson vertical bar
\renewcommand{\thefootnote}{\textcolor{crimson}{\arabic{footnote}}}

% Save original sidenote command
\makeatletter
\@ifundefined{oldsidenote}{
  \let\oldsidenote\sidenote%
}{}
\makeatother

% Redefine sidenote with vertical crimson bar
\renewcommand{\sidenote}[1]{%
  \oldsidenote{%
    \noindent
    \color{crimson!100}                        % Crimson vertical line
    \raisebox{0em}{%
      \rule{0.5pt}{1.5em}                      % Thin vertical line
    }
    \hspace{0.3em}                             % Space after line
    \color{black}                              % Reset text color
    \footnotesize #1                           % Sidenote content
  }%
}

% =============================================================================
% FLOAT HANDLING
% =============================================================================
% Patch LaTeX's output routine to handle float overflow gracefully
% The "Float(s) lost" error occurs in \@doclearpage when \@currlist is not empty
% This patch silently clears pending floats that can't be placed
\makeatletter
\let\orig@doclearpage\@doclearpage
\def\@doclearpage{%
  \ifx\@currlist\@empty\else
    \global\let\@currlist\@empty
    \typeout{Warning: Floats cleared to prevent overflow}%
  \fi
  \orig@doclearpage
}
\makeatother

% Additional safety for structural commands
\let\originalbackmatter\backmatter
\renewcommand{\backmatter}{%
  \clearpage%
  \originalbackmatter%
}

\let\originalfrontmatter\frontmatter
\renewcommand{\frontmatter}{%
  \clearpage%
  \originalfrontmatter%
}

\let\originalmainmatter\mainmatter
\renewcommand{\mainmatter}{%
  \clearpage%
  \originalmainmatter%
}

% =============================================================================
% PAGE HEADERS AND FOOTERS
% =============================================================================
% Ensure chapters use fancy page style (not plain)
\patchcmd{\chapter}{\thispagestyle{plain}}{\thispagestyle{fancy}}{}{}

% Main page style with crimson headers
\pagestyle{fancy}
\fancyhf{}                                              % Clear all
\fancyhead[LE]{\small\color{crimson}\nouppercase{\rightmark}}  % Left even: section
\fancyhead[RO]{\color{crimson}\thepage}                 % Right odd: page number
\fancyhead[LO]{\small\color{crimson}\nouppercase{\leftmark}}   % Left odd: chapter
\fancyhead[RE]{\color{crimson}\thepage}                 % Right even: page number
\renewcommand{\headrulewidth}{0.4pt}                    % Thin header line
\renewcommand{\footrulewidth}{0pt}                      % No footer line

% Plain page style (for chapter openings)
\fancypagestyle{plain}{
  \fancyhf{}
  \fancyfoot[C]{\color{crimson}\thepage}                % Centered page number
  \renewcommand{\headrulewidth}{0pt}
  \renewcommand{\footrulewidth}{0pt}
}

% =============================================================================
% KOMA-SCRIPT FONT ADJUSTMENTS
% =============================================================================
% Apply crimson color to all heading levels
\addtokomafont{disposition}{\rmfamily\color{crimson}}
\addtokomafont{chapter}{\color{crimson}}
\addtokomafont{section}{\color{crimson}}
\addtokomafont{subsection}{\color{crimson}}

% =============================================================================
% ABSTRACT ENVIRONMENT
% =============================================================================
\newenvironment{abstract}{
  \chapter*{\abstractname}
  \addcontentsline{toc}{chapter}{\abstractname}
  \small
}{
  \clearpage
}

% =============================================================================
% HYPERLINK CONFIGURATION
% =============================================================================
% Crimson-colored links throughout, two-page PDF layout
\hypersetup{
  linkcolor=crimson,
  citecolor=crimson,
  urlcolor=crimson,
  pdfpagelayout=TwoPageRight,   % Two-page spread view
  pdfstartview=Fit               % Initial zoom fits page
}

% =============================================================================
% PART SUMMARY SYSTEM
% =============================================================================
% Allows adding descriptive text below part titles
\newcommand{\partsummary}{}     % Empty by default
\newif\ifhaspartsummary%
\haspartsummaryfalse%

\newcommand{\setpartsummary}[1]{%
  \renewcommand{\partsummary}{#1}%
  \haspartsummarytrue%
}

% Additional colors for part page backgrounds
\definecolor{BrownLL}{RGB}{233,222,220}
\definecolor{BlueDD}{RGB}{62,100,125}
\colorlet{BlueDD}{magenta}

% ===============================================================================
% PART STYLING SYSTEM
% ===============================================================================
%
% This system provides three distinct visual styles for book organization:
%
% 1. NUMBERED PARTS (\part{title}) - For main book sections
%    - Roman numerals (I, II, III, etc.) in top right corner
%    - Crimson title with horizontal lines above/below
%    - "Part I" label in sidebar
%    - Used for: foundations, principles, optimization, deployment, etc.
%
% 2. UNNUMBERED PARTS (\part*{title}) - For special sections like "Labs"
%    - Division-style geometric background (left side)
%    - No Roman numerals
%    - Used for: labs section
%
% 3. DIVISIONS (\division{title}) - For major book divisions
%    - Clean geometric background with centered title
%    - Used for: frontmatter, main_content, backmatter
%
% The Lua filter (inject-parts.lua) automatically routes parts by {key:xxx} commands
% to the appropriate LaTeX command based on the key name.
% ===============================================================================

% NUMBERED PARTS: Roman numeral styling for main book sections
\titleformat{\part}[display]
{\thispagestyle{empty}}{}{20pt}{
\begin{tikzpicture}[remember picture,overlay]
%%%
%%
\node[crimson,align=flush right,
inner sep=0,outer sep=0mm,draw=none,%
anchor=east,minimum height=31mm, text width=1.2\textwidth,
yshift=-30mm,font={%
\fontsize{98pt}{104}\selectfont\bfseries}]  (BG) at (current page text area.north east){\thepart};
%
\node[black,inner sep=0mm,draw=none,
anchor=mid,text width=1.2\textwidth,
 minimum height=35mm, align=right,
node distance=7mm,below=of BG,
font={\fontsize{30pt}{34}\selectfont}]
(BGG)  {\hyphenchar\font=-1 \color{black}\MakeUppercase {#1}};
\draw [crimson,line width=3pt] ([yshift=0mm]BGG.north west) -- ([yshift=0mm]BGG.north east);
\draw [crimson,line width=2pt] ([yshift=0mm]BGG.south west) -- ([yshift=0mm]BGG.south east);
%
\node[fill=crimson,text=white,rotate=90,%
anchor=south west,minimum height=15mm,
minimum width=40mm,font={%
\fontsize{20pt}{20}\selectfont\bfseries}](BP)  at
(current page text area.south east)
{{\sffamily Part}~\thepart};
%
\path[red](BP.north west)-|coordinate(PS)(BGG.south west);
%
% Part summary box commented out for cleaner design
% \ifhaspartsummary
% \node[inner sep=4pt,text width=0.7\textwidth,draw=none,fill=BrownLL!40,
% align=justify,font={\fontsize{9pt}{12}\selectfont},anchor=south west]
% at (PS) {\partsummary};
% \fi
\end{tikzpicture}
}[]

\renewcommand{\thepart}{\Roman{part}}

% UNNUMBERED PARTS: Division-style background for special sections
\titleformat{name=\part,numberless}[display]
{\thispagestyle{empty}}{}{20pt}{
\begin{tikzpicture}[remember picture,overlay]
%%%
\coordinate(S1)at([yshift=-200mm]current page.north west);
\draw[draw=none,fill=BlueDD!7](S1)--++(45:16)coordinate(S2)-
|(S2|-current page.north west)--(current page.north west)coordinate(S3)--(S1);
%
\coordinate(E1)at([yshift=-98mm]current page.north west);
\draw[draw=none,fill=BlueDD!15](E1)--(current page.north west)coordinate(E2)
--++(0:98mm)coordinate(E3)--(E1);
%
\coordinate(D1)at([yshift=15mm]current page.south west);
\draw[draw=none,fill=BlueDD!40,opacity=0.5](D1)--++(45:5.5)coordinate(D2)
-|(D2|-current page.north west)--(current page.north west)coordinate(D3)--(D1);
%%%%
\path[red](S2)-|(S2-|current page.east)coordinate(SS2);
%PART
\node[crimson,align=flush right,inner sep=0,outer sep=0mm,draw=none,anchor=south,
font={\fontsize{48pt}{48}\selectfont\bfseries}]  (BG) at ($(S2)!0.5!(SS2)$){\hphantom{Part}};
%%%
\path[green]([yshift=15mm]D2)-|coordinate(TPD)(BG.south east);
\node[inner sep=0mm,draw=none,anchor=south east,%text width=0.9\textwidth,
align=right,font={\fontsize{40pt}{40}\selectfont}]
(BGG) at (TPD)  {\color{crimson}\MakeUppercase {#1}};%\MakeUppercase {}
\end{tikzpicture}
}

% Define \numberedpart command for numbered parts
\newcommand{\numberedpart}[1]{%
\FloatBarrier%  % Flush all pending floats before part break
\clearpage
\thispagestyle{empty}
\stepcounter{part}%
\begin{tikzpicture}[remember picture,overlay]
%%%
%%
\node[crimson,align=flush right,
inner sep=0,outer sep=0mm,draw=none,%
anchor=east,minimum height=31mm, text width=1.2\textwidth,
yshift=-30mm,font={%
\fontsize{98pt}{104}\selectfont\bfseries}]  (BG) at (current page text area.north east){\thepart};
%
\node[black,inner sep=0mm,draw=none,
anchor=mid,text width=1.2\textwidth,
 minimum height=35mm, align=right,
node distance=7mm,below=of BG,
font={\fontsize{30pt}{34}\selectfont}]
(BGG)  {\hyphenchar\font=-1 \color{black}\MakeUppercase {#1}};
\draw [crimson,line width=3pt] ([yshift=0mm]BGG.north west) -- ([yshift=0mm]BGG.north east);
\draw [crimson,line width=2pt] ([yshift=0mm]BGG.south west) -- ([yshift=0mm]BGG.south east);
%
\node[fill=crimson,text=white,rotate=90,%
anchor=south west,minimum height=15mm,
minimum width=40mm,font={%
\fontsize{20pt}{20}\selectfont\bfseries}](BP)  at
(current page text area.south east)
{{\sffamily Part}~\thepart};
%
\path[red](BP.north west)-|coordinate(PS)(BGG.south west);
%
% Part summary box commented out for cleaner design
% \ifhaspartsummary
% \node[inner sep=4pt,text width=0.7\textwidth,draw=none,fill=BrownLL!40,
% align=justify,font={\fontsize{9pt}{12}\selectfont},anchor=south west]
% at (PS) {\partsummary};
% \fi
\end{tikzpicture}
\clearpage
}



% DIVISIONS: Clean geometric styling with subtle tech elements
% Used for frontmatter, main_content, and backmatter divisions
\newcommand{\division}[1]{%
\FloatBarrier%  % Flush all pending floats before division break
\clearpage
\thispagestyle{empty}
\begin{tikzpicture}[remember picture,overlay]

% Clean geometric background (original design)
\coordinate(S1)at([yshift=-200mm]current page.north west);
\draw[draw=none,fill=BlueDD!7](S1)--++(45:16)coordinate(S2)-
|(S2|-current page.north west)--(current page.north west)coordinate(S3)--(S1);

\coordinate(E1)at([yshift=-98mm]current page.north west);
\draw[draw=none,fill=BlueDD!15](E1)--(current page.north west)coordinate(E2)
--++(0:98mm)coordinate(E3)--(E1);

\coordinate(D1)at([yshift=15mm]current page.south west);
\draw[draw=none,fill=BlueDD!40,opacity=0.5](D1)--++(45:5.5)coordinate(D2)
-|(D2|-current page.north west)--(current page.north west)coordinate(D3)--(D1);

% Subtle tech elements - positioned in white areas for better visibility
% Upper right white area - more visible
\draw[crimson!40, line width=0.8pt] ([xshift=140mm,yshift=-60mm]current page.north west) -- ++(40mm,0);
\draw[crimson!40, line width=0.8pt] ([xshift=150mm,yshift=-70mm]current page.north west) -- ++(30mm,0);
\draw[crimson!35, line width=0.7pt] ([xshift=160mm,yshift=-60mm]current page.north west) -- ++(0,-15mm);
\draw[crimson!35, line width=0.7pt] ([xshift=170mm,yshift=-70mm]current page.north west) -- ++(0,10mm);

% Circuit nodes - upper right
\fill[crimson!50] ([xshift=160mm,yshift=-60mm]current page.north west) circle (1.5mm);
\fill[white] ([xshift=160mm,yshift=-60mm]current page.north west) circle (0.8mm);
\fill[crimson!50] ([xshift=170mm,yshift=-70mm]current page.north west) circle (1.3mm);
\fill[white] ([xshift=170mm,yshift=-70mm]current page.north west) circle (0.6mm);

% Lower right white area - enhanced visibility
\draw[crimson!45, line width=0.9pt] ([xshift=140mm,yshift=-190mm]current page.north west) -- ++(45mm,0);
\draw[crimson!45, line width=0.9pt] ([xshift=150mm,yshift=-200mm]current page.north west) -- ++(35mm,0);
\draw[crimson!40, line width=0.8pt] ([xshift=160mm,yshift=-190mm]current page.north west) -- ++(0,-20mm);
\draw[crimson!40, line width=0.8pt] ([xshift=170mm,yshift=-200mm]current page.north west) -- ++(0,15mm);

% Additional connecting lines in lower right
\draw[crimson!35, line width=0.7pt] ([xshift=130mm,yshift=-180mm]current page.north west) -- ++(25mm,0);
\draw[crimson!35, line width=0.7pt] ([xshift=145mm,yshift=-180mm]current page.north west) -- ++(0,-25mm);

% Circuit nodes - lower right (more prominent)
\fill[crimson!55] ([xshift=160mm,yshift=-190mm]current page.north west) circle (1.6mm);
\fill[white] ([xshift=160mm,yshift=-190mm]current page.north west) circle (0.9mm);
\fill[crimson!55] ([xshift=170mm,yshift=-200mm]current page.north west) circle (1.4mm);
\fill[white] ([xshift=170mm,yshift=-200mm]current page.north west) circle (0.7mm);
\fill[crimson!50] ([xshift=145mm,yshift=-180mm]current page.north west) circle (1.2mm);
\fill[white] ([xshift=145mm,yshift=-180mm]current page.north west) circle (0.6mm);

% Title positioned in center - clean and readable
\node[inner sep=0mm,draw=none,anchor=center,text width=0.8\textwidth,
align=center,font={\fontsize{40pt}{40}\selectfont}]
(BGG) at (current page.center)  {\color{crimson}\MakeUppercase {#1}};

\end{tikzpicture}
\clearpage
}

% LAB DIVISIONS: Circuit-style neural network design for lab sections
% Used specifically for lab platform sections (arduino, xiao, grove, etc.)
\newcommand{\labdivision}[1]{%
\FloatBarrier%  % Flush all pending floats before lab division break
\clearpage
\thispagestyle{empty}
\begin{tikzpicture}[remember picture,overlay]
% Circuit background with subtle gradient
\coordinate(S1)at([yshift=-200mm]current page.north west);
\draw[draw=none,fill=BlueDD!5](S1)--++(45:16)coordinate(S2)-
|(S2|-current page.north west)--(current page.north west)coordinate(S3)--(S1);

% TOP AREA: Circuit lines in upper white space
\draw[crimson!50, line width=1.5pt] ([xshift=30mm,yshift=-40mm]current page.north west) -- ++(60mm,0);
\draw[crimson!40, line width=1pt] ([xshift=120mm,yshift=-50mm]current page.north west) -- ++(50mm,0);
\draw[crimson!50, line width=1.5pt] ([xshift=40mm,yshift=-70mm]current page.north west) -- ++(40mm,0);

% Connecting lines in top area
\draw[crimson!30, line width=1pt] ([xshift=60mm,yshift=-40mm]current page.north west) -- ++(0,-20mm);
\draw[crimson!30, line width=1pt] ([xshift=145mm,yshift=-50mm]current page.north west) -- ++(0,10mm);

% Neural nodes in top area
\fill[crimson!70] ([xshift=60mm,yshift=-40mm]current page.north west) circle (2.5mm);
\fill[white] ([xshift=60mm,yshift=-40mm]current page.north west) circle (1.5mm);
\fill[crimson!60] ([xshift=145mm,yshift=-50mm]current page.north west) circle (2mm);
\fill[white] ([xshift=145mm,yshift=-50mm]current page.north west) circle (1mm);
\fill[crimson!80] ([xshift=80mm,yshift=-70mm]current page.north west) circle (2mm);
\fill[white] ([xshift=80mm,yshift=-70mm]current page.north west) circle (1mm);

% BOTTOM AREA: Circuit lines in lower white space
\draw[crimson!50, line width=1.5pt] ([xshift=20mm,yshift=-200mm]current page.north west) -- ++(70mm,0);
\draw[crimson!40, line width=1pt] ([xshift=110mm,yshift=-210mm]current page.north west) -- ++(60mm,0);
\draw[crimson!50, line width=1.5pt] ([xshift=35mm,yshift=-230mm]current page.north west) -- ++(45mm,0);

% Connecting lines in bottom area
\draw[crimson!30, line width=1pt] ([xshift=55mm,yshift=-200mm]current page.north west) -- ++(0,-20mm);
\draw[crimson!30, line width=1pt] ([xshift=140mm,yshift=-210mm]current page.north west) -- ++(0,15mm);

% Neural nodes in bottom area
\fill[crimson!70] ([xshift=55mm,yshift=-200mm]current page.north west) circle (2.5mm);
\fill[white] ([xshift=55mm,yshift=-200mm]current page.north west) circle (1.5mm);
\fill[crimson!60] ([xshift=140mm,yshift=-210mm]current page.north west) circle (2mm);
\fill[white] ([xshift=140mm,yshift=-210mm]current page.north west) circle (1mm);
\fill[crimson!80] ([xshift=80mm,yshift=-230mm]current page.north west) circle (2mm);
\fill[white] ([xshift=80mm,yshift=-230mm]current page.north west) circle (1mm);

% SIDE AREAS: Subtle circuit elements on left and right edges
\draw[crimson!30, line width=1pt] ([xshift=15mm,yshift=-120mm]current page.north west) -- ++(20mm,0);
\draw[crimson!30, line width=1pt] ([xshift=175mm,yshift=-130mm]current page.north west) -- ++(15mm,0);
\fill[crimson!50] ([xshift=25mm,yshift=-120mm]current page.north west) circle (1.5mm);
\fill[white] ([xshift=25mm,yshift=-120mm]current page.north west) circle (0.8mm);
\fill[crimson!50] ([xshift=185mm,yshift=-130mm]current page.north west) circle (1.5mm);
\fill[white] ([xshift=185mm,yshift=-130mm]current page.north west) circle (0.8mm);

% Title positioned in center - CLEAN AREA
\node[inner sep=0mm,draw=none,anchor=center,text width=0.8\textwidth,
align=center,font={\fontsize{44pt}{44}\selectfont\bfseries}]
(BGG) at (current page.center)  {\color{crimson}\MakeUppercase {#1}};

\end{tikzpicture}
\clearpage
}

% Define \lab command for lab styling (different visual treatment)
\newcommand{\lab}[1]{%
\begin{tikzpicture}[remember picture,overlay]
%%%
% Different background pattern for labs
\coordinate(S1)at([yshift=-200mm]current page.north west);
\draw[draw=none,fill=BlueDD!15](S1)--++(45:16)coordinate(S2)-
|(S2|-current page.north west)--(current page.north west)coordinate(S3)--(S1);
%
\coordinate(E1)at([yshift=-98mm]current page.north west);
\draw[draw=none,fill=BlueDD!25](E1)--(current page.north west)coordinate(E2)
--++(0:98mm)coordinate(E3)--(E1);
%
\coordinate(D1)at([yshift=15mm]current page.south west);
\draw[draw=none,fill=BlueDD!60,opacity=0.7](D1)--++(45:5.5)coordinate(D2)
-|(D2|-current page.north west)--(current page.north west)coordinate(D3)--(D1);
%%%%
\path[red](S2)-|(S2-|current page.east)coordinate(SS2);
%LAB - Different styling
\node[crimson,align=flush right,inner sep=0,outer sep=0mm,draw=none,anchor=south,
font={\fontsize{48pt}{48}\selectfont\bfseries}]  (BG) at ($(S2)!0.5!(SS2)$){\hphantom{Workshop}};
%%%
\path[green]([yshift=15mm]D2)-|coordinate(TPD)(BG.south east);
\node[inner sep=0mm,draw=none,anchor=south east,%text width=0.9\textwidth,
align=right,font={\fontsize{40pt}{40}\selectfont}]
(BGG) at (TPD)  {\color{crimson}\MakeUppercase {#1}};%\MakeUppercase {}
\end{tikzpicture}
\thispagestyle{empty}
\clearpage
}

% =============================================================================
% SECTION FORMATTING
% =============================================================================
% All section levels use crimson color and are ragged right

% Section (Large, bold, crimson)
\titleformat{\section}
  {\normalfont\Large\bfseries\color{crimson}\raggedright}
  {\thesection}
  {0.5em}
  {#1}
\titlespacing*{\section}{0pc}{14pt plus 4pt minus 4pt}{6pt plus 2pt minus 2pt}[0pc]

% Subsection (large, bold, crimson)
\titleformat{\subsection}
  {\normalfont\large\bfseries\color{crimson}\raggedright}
  {\thesubsection}
  {0.5em}
  {#1}
\titlespacing*{\subsection}{0pc}{12pt plus 4pt minus 4pt}{5pt plus 1pt minus 2pt}[0pc]

% Subsubsection (normal size, bold, crimson)
\titleformat{\subsubsection}
  {\normalfont\normalsize\bfseries\color{crimson}\raggedright}
  {\thesubsubsection}
  {0.5em}
  {#1}
\titlespacing*{\subsubsection}{0pc}{12pt plus 4pt minus 4pt}{5pt plus 1pt minus 2pt}[0pc]

% Paragraph (run-in, bold, crimson)
\titleformat{\paragraph}[runin]
  {\normalfont\normalsize\bfseries\color{crimson}}
  {\theparagraph}
  {0.5em}
  {#1}
\titlespacing*{\paragraph}{0pc}{6pt plus 2pt minus 2pt}{0.5em}[0pc]

% Subparagraph (run-in, italic, crimson)
\titleformat{\subparagraph}[runin]
  {\normalfont\normalsize\itshape\color{crimson}}
  {\thesubparagraph}
  {0.5em}
  {#1}
\titlespacing*{\subparagraph}{0pc}{6pt plus 2pt minus 2pt}{0.5em}[0pc]

% =============================================================================
% CHAPTER FORMATTING
% =============================================================================
% Numbered chapters: "Chapter X" prefix, huge crimson title
\titleformat{\chapter}[display]
  {\normalfont\huge\bfseries\color{crimson}}
  {\chaptername\ \thechapter}
  {20pt}
  {\Huge #1}
  []

% Unnumbered chapters: no prefix, huge crimson title
\titleformat{name=\chapter,numberless}
  {\normalfont\huge\bfseries\color{crimson}}
  {}
  {0pt}
  {\Huge #1}
  []

\renewcommand{\chaptername}{Chapter}
% =============================================================================
% TABLE OF CONTENTS FORMATTING
% =============================================================================
\setcounter{tocdepth}{2}                      % Show chapters, sections, subsections

% TOC spacing adjustments for number widths and indentation
\setlength{\cftchapnumwidth}{2em}             % Chapter number width
\setlength{\cftsecnumwidth}{2.75em}           % Section number width
\setlength{\cftsubsecnumwidth}{3.25em}        % Subsection number width
\setlength{\cftsubsubsecnumwidth}{4em}        % Subsubsection number width
\setlength{\cftsubsecindent}{4.25em}          % Subsection indent
\setlength{\cftsubsubsecindent}{7.5em}        % Subsubsection indent

% Chapter entries in TOC: bold crimson with "Chapter" prefix
\renewcommand{\cftchapfont}{\bfseries\color{crimson}}
\renewcommand{\cftchappresnum}{\color{crimson}Chapter~}

% Custom formatting for division entries (styled like parts)
\newcommand{\divisionchapter}[1]{%
  \addvspace{12pt}%
  \noindent\hfil\bfseries\color{crimson}#1\hfil\par%
  \addvspace{6pt}%
}

% Adjust TOC spacing for "Chapter" prefix
\newlength{\xtraspace}
\settowidth{\xtraspace}{\cftchappresnum\cftchapaftersnum}
\addtolength{\cftchapnumwidth}{\xtraspace}

% Unnumbered chapters with TOC entry
\newcommand{\likechapter}[1]{%
    \chapter*{#1}
    \addcontentsline{toc}{chapter}{\textcolor{crimson}{#1}}
}

% =============================================================================
% PAGE NUMBERING SYSTEM
% =============================================================================
% Implements traditional book numbering:
% - Roman numerals (i, ii, iii...) for frontmatter
% - Arabic numerals (1, 2, 3...) for mainmatter
% Automatically switches at first numbered chapter
\makeatletter
\newif\if@firstnumbered%
\@firstnumberedtrue%
\newif\if@firstunnumbered%
\@firstunnumberedtrue%

\newcounter{lastRomanPage}
\setcounter{lastRomanPage}{1}

% Start document with Roman numerals (frontmatter)
\AtBeginDocument{
  \pagenumbering{roman}
  \renewcommand{\thepage}{\roman{page}}
}

% Intercept chapter command
\let\old@chapter\chapter%
\renewcommand{\chapter}{%
  \@ifstar{\unnumbered@chapter}{\numbered@chapter}%
}

% Numbered chapters: switch to Arabic on first occurrence
\newcommand{\numbered@chapter}[1]{%
  \if@firstnumbered%
    \cleardoublepage%
    \setcounter{lastRomanPage}{\value{page}}%
    \pagenumbering{arabic}%
    \@firstnumberedfalse%
  \else
    \setcounter{page}{\value{page}}%
  \fi
  \setcounter{sidenote}{1}                    % Reset footnote counter per chapter
  \old@chapter{#1}%
}

% Unnumbered chapters: stay in Roman numerals
\newcommand{\unnumbered@chapter}[1]{%
  \if@firstunnumbered%
    \clearpage
    \setcounter{lastRomanPage}{\value{page}}%
    \pagenumbering{roman}%
    \@firstunnumberedfalse%
  \fi
  \setcounter{sidenote}{1}
  \old@chapter*{#1}%
}
\makeatother

% =============================================================================
% TABLE SIZING AND SPACING
% =============================================================================
% Make tables slightly smaller to fit more content
\AtBeginEnvironment{longtable}{\scriptsize}

% Increase vertical spacing in table cells (default is 1.0)
\renewcommand{\arraystretch}{1.5}

% Prefer placing figures and tables at the top of pages
\makeatletter
\renewcommand{\fps@figure}{t}  % Default placement: top of page
\renewcommand{\fps@table}{t}   % Default placement: top of page
\makeatother

% =============================================================================
% LONGTABLE PAGE BREAKING FIXES (Windows compatibility)
% =============================================================================
% Prevent "Infinite glue shrinkage" errors on Windows LaTeX builds
% by giving longtable more flexibility in page breaking

% Allow more flexible page breaking (vs strict \flushbottom)
\raggedbottom

% Process more rows before attempting page break (default is 20)
\setcounter{LTchunksize}{50}

% Add extra stretch for longtable environments specifically
\AtBeginEnvironment{longtable}{%
  \setlength{\emergencystretch}{3em}%
  \setlength{\parskip}{0pt plus 1pt}%
}

% =============================================================================
% TABLE STYLING - Clean tables with crimson borders
% =============================================================================
% Professional table appearance with:
% - Clean white background (no colored rows)
% - Crimson-colored borders
% - Good spacing for readability
%
% Note: Headers are automatically bolded by Quarto when using **text** in source
\usepackage{booktabs}      % Professional table rules (\toprule, \midrule, \bottomrule)
\usepackage{colortbl}      % For colored borders (\arrayrulecolor)

% Global table styling - crimson borders
\setlength{\arrayrulewidth}{0.5pt}          % Thinner borders than default
%\arrayrulecolor{crimson}                    % Crimson borders matching brand

\setcounter{chapter}{0}

% =============================================================================
% DROP CAPS (Lettrine)
% =============================================================================
% Decorative large first letter at chapter openings, following the tradition
% of Hennessy & Patterson and other MIT Press textbooks.
% Usage in QMD: \lettrine{T}{he first sentence...}
\usepackage{lettrine}
\renewcommand{\LettrineFontHook}{\color{crimson}\bfseries}
\setcounter{DefaultLines}{3}          % Drop cap spans 3 lines
\renewcommand{\DefaultLoversize}{0.1} % Slight oversize for visual weight
\renewcommand{\DefaultLraise}{0}      % No vertical shift
\setlength{\DefaultNindent}{0.5em}    % Indent of continuation text
\setlength{\DefaultSlope}{0pt}        % No slope on continuation

% =============================================================================
% RUNNING HEADERS — Truncation Safety
% =============================================================================
% Long chapter/section titles can overflow the header. These marks truncate
% gracefully so headers stay within the text block.
\renewcommand{\chaptermark}[1]{%
  \markboth{\thechapter.\ #1}{}}
\renewcommand{\sectionmark}[1]{%
  \markright{\thesection\ #1}}

% =============================================================================
% EPIGRAPH ENVIRONMENT
% =============================================================================
% For chapter-opening quotations. Renders as right-aligned italic block
% with attribution in small caps below.
% Usage: \epigraph{Quote text}{Author Name, \textit{Source}}
\newcommand{\bookepigraph}[2]{%
  \vspace{1em}%
  \begin{flushright}%
    \begin{minipage}{0.75\textwidth}%
      \raggedleft\itshape\small #1\\[0.5em]%
      \normalfont\small --- #2%
    \end{minipage}%
  \end{flushright}%
  \vspace{1.5em}%
}

% =============================================================================
% THUMB INDEX TABS
% =============================================================================
% Colored tabs on the outer page edge for quick chapter navigation.
% Each Part gets a different vertical position; chapters within a Part
% share the same tab position. Visible when flipping through the book.
\newcounter{thumbindex}
\setcounter{thumbindex}{0}
\newlength{\thumbtabheight}
\setlength{\thumbtabheight}{16mm}     % Height of each tab
\newlength{\thumbtabwidth}
\setlength{\thumbtabwidth}{8mm}       % Width protruding from edge
\newlength{\thumbtabgap}
\setlength{\thumbtabgap}{1mm}         % Gap between tabs

% Advance to next thumb tab position (call at each \part)
\newcommand{\nextthumb}{%
  \stepcounter{thumbindex}%
}

% Draw the thumb tab on every page (placed in header via fancyhdr)
\newcommand{\drawthumb}{%
  \ifnum\value{thumbindex}>0%
    \begin{tikzpicture}[remember picture,overlay]
      \pgfmathsetmacro{\thumboffset}{%
        20 + (\value{thumbindex}-1) * (16 + 1)}  % mm from top
      \ifodd\value{page}%
        % Odd pages: tab on right edge
        \fill[crimson!80]
          ([yshift=-\thumboffset mm]current page.north east)
          rectangle +(-\thumbtabwidth, -\thumbtabheight);
        \node[white,font=\tiny\bfseries,rotate=90]
          at ([yshift=-\thumboffset mm - 0.5\thumbtabheight,
               xshift=-0.5\thumbtabwidth]current page.north east)
          {\Roman{thumbindex}};
      \else
        % Even pages: tab on left edge
        \fill[crimson!80]
          ([yshift=-\thumboffset mm]current page.north west)
          rectangle +(\thumbtabwidth, -\thumbtabheight);
        \node[white,font=\tiny\bfseries,rotate=-90]
          at ([yshift=-\thumboffset mm - 0.5\thumbtabheight,
               xshift=0.5\thumbtabwidth]current page.north west)
          {\Roman{thumbindex}};
      \fi
    \end{tikzpicture}%
  \fi
}

% Hook into fancyhdr to draw thumb on every content page
\AddToHook{shipout/foreground}{%
  \drawthumb%
}

% =============================================================================
% CROP / BLEED MARKS
% =============================================================================
% For final print submission, uncomment the line below to add crop marks.
% MIT Press production will advise on exact requirements.
% \usepackage[cam,center,width=7.5in,height=10.5in]{crop}

% =============================================================================
% PDF/A ARCHIVAL COMPLIANCE
% =============================================================================
% MIT Press increasingly requires PDF/A for long-term preservation.
% This embeds all fonts and removes transparency.
% Note: pdfx must be loaded early; if it conflicts with hyperref,
% MIT Press production can handle the conversion post-build.
% Uncomment when ready for final submission:
% \usepackage[a-3u]{pdfx}

% =============================================================================
% ENHANCED WIDOW / ORPHAN CONTROL
% =============================================================================
% Prevent single lines at top/bottom of pages and breaks before equations
\clubpenalty=10000          % No orphans (single first line at bottom)
\widowpenalty=10000         % No widows (single last line at top)
\displaywidowpenalty=10000  % No widow before display math
\predisplaypenalty=10000    % No page break just before display math
\postdisplaypenalty=0       % Allow break after display math (natural)
\usepackage{needspace}
\let\Needspace\needspace
\makeatletter
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{plain}
\@ifundefined{c@chapter}{\newfloat{vid}{h}{lovid}}{\newfloat{vid}{h}{lovid}[chapter]}
\floatname{vid}{Video}
\newcommand*\listofvids{\listof{vid}{List of Videos}}
\makeatother
\makeatletter
\@ifpackageloaded{tcolorbox}{}{\usepackage[skins,breakable]{tcolorbox}}
\@ifpackageloaded{fontawesome5}{}{\usepackage{fontawesome5}}
\definecolor{quarto-callout-color}{HTML}{909090}
\definecolor{quarto-callout-note-color}{HTML}{0758E5}
\definecolor{quarto-callout-important-color}{HTML}{CC1914}
\definecolor{quarto-callout-warning-color}{HTML}{EB9113}
\definecolor{quarto-callout-tip-color}{HTML}{00A047}
\definecolor{quarto-callout-caution-color}{HTML}{FC5300}
\definecolor{quarto-callout-color-frame}{HTML}{acacac}
\definecolor{quarto-callout-note-color-frame}{HTML}{4582ec}
\definecolor{quarto-callout-important-color-frame}{HTML}{d9534f}
\definecolor{quarto-callout-warning-color-frame}{HTML}{f0ad4e}
\definecolor{quarto-callout-tip-color-frame}{HTML}{02b875}
\definecolor{quarto-callout-caution-color-frame}{HTML}{fd7e14}
\makeatother
\makeatletter
\@ifpackageloaded{bookmark}{}{\usepackage{bookmark}}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\makeatletter
\@ifpackageloaded{sidenotes}{}{\usepackage{sidenotes}}
\@ifpackageloaded{marginnote}{}{\usepackage{marginnote}}
\makeatother
\newcommand{\fbxIconPath}{assets/images/icons/callouts}
\newcommand{\fbxIconFormat}{pdf}
\makeatletter
\@ifpackageloaded{tcolorbox}{}{\usepackage[many]{tcolorbox}}
\makeatother
%%%% ---foldboxy preamble ----- %%%%%

% Load xstring for string manipulation
\RequirePackage{xstring}

% Icon path and format configuration - can be overridden in filter-metadata
\providecommand{\fbxIconPath}{assets/images/icons/callouts}
\providecommand{\fbxIconFormat}{pdf}

% Helper command to include icon with hyphen-to-underscore conversion
% This ensures consistency: callout-quiz-question -> callout_quiz_question
% Using height= instead of width= ensures consistent header heights across all icons
% regardless of aspect ratio
\newcommand{\fbxIncludeIcon}[2]{%
  \StrSubstitute{#1}{-}{_}[\fbxIconName]%
  \includegraphics[height=#2]{\fbxIconPath/icon_\fbxIconName.\fbxIconFormat}%
}

% Legacy fallback colors (keep for compatibility)
\definecolor{fbx-default-color1}{HTML}{c7c7d0}
\definecolor{fbx-default-color2}{HTML}{a3a3aa}
\definecolor{fbox-color1}{HTML}{c7c7d0}
\definecolor{fbox-color2}{HTML}{a3a3aa}

% arguments: #1 typelabelnummer: #2 titel: #3
\newenvironment{fbx}[3]{%
\begin{tcolorbox}[
  enhanced,
  breakable,
  %fontupper=\fontsize{8pt}{10pt}\selectfont,  % 95% of body text (10pt -> 9.5pt)
  before skip=8pt,  % space above box (increased)
  after skip=8pt,   % space below box (increased)
  attach boxed title to top*={xshift=0pt},
  boxed title style={
  %fuzzy shadow={1pt}{-1pt}{0mm}{0.1mm}{gray},
  arc=1.5pt,
  rounded corners=north,
  sharp corners=south,
  top=6pt,          % Adjusted for ~40px equivalent height
  bottom=5pt,       % Adjusted for ~40px equivalent height
  overlay={
      \node [left,outer sep=0em, black,draw=none,anchor=west,
        rectangle,fill=none,inner sep=0pt]
        at ([xshift=4mm]frame.west) {\fbxIncludeIcon{#1}{4.2mm}};
    },
  },
  colframe=#1-color2,             % Border color (auto-generated from YAML)
  colbacktitle=#1-color1,         % Background color (auto-generated from YAML)
  colback=white,
  coltitle=black,
  titlerule=0mm,
  toprule=0.5pt,
  bottomrule=0.5pt,
  leftrule=2.2pt,
  rightrule=0.5pt,
  outer arc=1.5pt,
  arc=1.5pt,
  left=0.5em,       % increased left padding
  bottomtitle=1.5mm, % increased title bottom margin
  toptitle=1.5mm,    % increased title top margin
  title=\hspace{2.5em}\protect#2\hspace{0.5em}\protect#3, % Protect parameters
  extras middle and last={top=4pt} % increased continuation spacing
]}
{\end{tcolorbox}}


% boxed environment with right border
\newenvironment{fbxSimple}[3]{\begin{tcolorbox}[
  enhanced,
  breakable,
  %fontupper=\fontsize{8pt}{10pt}\selectfont,  % 95% of body text (10pt -> 9.5pt)
  before skip=8pt,  % space above box (increased)
  after skip=8pt,   % space below box (increased)
  attach boxed title to top*={xshift=0pt},
  boxed title style={
  %fuzzy shadow={1pt}{-1pt}{0mm}{0.1mm}{gray},
  arc=1.5pt,
  rounded corners=north,
  sharp corners=south,
  top=6pt,          % Adjusted for ~40px equivalent height
  bottom=5pt,       % Adjusted for ~40px equivalent height
  overlay={
      \node [left,outer sep=0em, black,draw=none,anchor=west,
        rectangle,fill=none,inner sep=0pt]
        at ([xshift=3mm]frame.west) {\fbxIncludeIcon{#1}{4.2mm}};
    },
  },
  colframe=#1-color2,             % Border color (auto-generated from YAML)
  colbacktitle=#1-color1,         % Background color (auto-generated from YAML)
  colback=white,
  coltitle=black,
  titlerule=0mm,
  toprule=0.5pt,
  bottomrule=0.5pt,
  leftrule=2.2pt,
  rightrule=0.5pt,
  outer arc=1.5pt,
  arc=1.5pt,
  left=0.5em,       % increased left padding
  bottomtitle=1.5mm, % increased title bottom margin
  toptitle=1.5mm,    % increased title top margin
  title=\hspace{2.5em}\protect#2\hspace{0.5em}\protect#3, % Protect parameters
  boxsep=1pt,
  extras first={bottom=0pt},
  extras last={top=0pt,bottom=-4pt},
  overlay first={
    \draw[line width=1pt,white] ([xshift=2.2pt]frame.south west)-- ([xshift=-0.5pt]frame.south east);
  },
  overlay last={
    \draw[line width=1pt,white] ([xshift=2.2pt]frame.north west)-- ([xshift=-0.5pt]frame.north east);
   }
]}
{\end{tcolorbox}}

%%%% --- end foldboxy preamble ----- %%%%%
%%==== colors from yaml ===%
\definecolor{callout-code-color1}{HTML}{F2F4F8}
\definecolor{callout-code-color2}{HTML}{D1D7E0}
\definecolor{callout-colab-color1}{HTML}{FFF5E6}
\definecolor{callout-colab-color2}{HTML}{FF6B35}
\definecolor{callout-quiz-question-color1}{HTML}{F0F0F8}
\definecolor{callout-quiz-question-color2}{HTML}{5B4B8A}
\definecolor{callout-theorem-color1}{HTML}{F5F0FA}
\definecolor{callout-theorem-color2}{HTML}{6B46C1}
\definecolor{callout-resource-exercises-color1}{HTML}{E0F2F1}
\definecolor{callout-resource-exercises-color2}{HTML}{20B2AA}
\definecolor{callout-perspective-color1}{HTML}{F7F8FA}
\definecolor{callout-perspective-color2}{HTML}{4A5568}
\definecolor{callout-example-color1}{HTML}{F0F8F6}
\definecolor{callout-example-color2}{HTML}{148F77}
\definecolor{callout-definition-color1}{HTML}{F0F4F8}
\definecolor{callout-definition-color2}{HTML}{1B4F72}
\definecolor{callout-resource-slides-color1}{HTML}{E0F2F1}
\definecolor{callout-resource-slides-color2}{HTML}{20B2AA}
\definecolor{callout-chapter-connection-color1}{HTML}{EFF6FF}
\definecolor{callout-chapter-connection-color2}{HTML}{1E3A5F}
\definecolor{callout-principle-color1}{HTML}{F3F2FA}
\definecolor{callout-principle-color2}{HTML}{3D3B8E}
\definecolor{callout-notebook-color1}{HTML}{F2F7FF}
\definecolor{callout-notebook-color2}{HTML}{2C5282}
\definecolor{callout-lighthouse-color1}{HTML}{FDF8E6}
\definecolor{callout-lighthouse-color2}{HTML}{B8860B}
\definecolor{callout-resource-videos-color1}{HTML}{E0F2F1}
\definecolor{callout-resource-videos-color2}{HTML}{20B2AA}
\definecolor{callout-takeaways-color1}{HTML}{FDF2F7}
\definecolor{callout-takeaways-color2}{HTML}{BE185D}
\definecolor{callout-checkpoint-color1}{HTML}{FFF8E7}
\definecolor{callout-checkpoint-color2}{HTML}{D97706}
\definecolor{callout-quiz-answer-color1}{HTML}{E8F2EA}
\definecolor{callout-quiz-answer-color2}{HTML}{4A7C59}
%=============%

\usepackage{hyphenat}
\usepackage{ifthen}
\usepackage{calc}
\usepackage{calculator}



\usepackage{graphicx}
\usepackage{geometry}
\usepackage{afterpage}
\usepackage{tikz}
\usetikzlibrary{calc}
\usetikzlibrary{fadings}
\usepackage[pagecolor=none]{pagecolor}


% Set the titlepage font families







% Set the coverpage font families

\usepackage{bookmark}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  pdftitle={Introduction to Machine Learning Systems},
  pdfauthor={Vijay Janapa Reddi},
  colorlinks=true,
  linkcolor={Maroon},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}


\title{Introduction to Machine Learning Systems}
\author{Vijay Janapa Reddi}
\date{February 2, 2026}
\begin{document}
%%%%% begin titlepage extension code

  \begin{frontmatter}

\begin{titlepage}
% This is a combination of Pandoc templating and LaTeX
% Pandoc templating https://pandoc.org/MANUAL.html#templates
% See the README for help

\thispagestyle{empty}

\newgeometry{top=-100in}

% Page color

\newcommand{\coverauthorstyle}[1]{{\fontsize{20}{24.0}\selectfont
{#1}}}

\begin{tikzpicture}[remember picture, overlay, inner sep=0pt, outer sep=0pt]

\tikzfading[name=fadeout, inner color=transparent!0,outer color=transparent!100]
\tikzfading[name=fadein, inner color=transparent!100,outer color=transparent!0]
\node[anchor=south west, rotate=0, opacity=1] at ($(current page.south west)+(0.225\paperwidth, 9)$) {
\includegraphics[width=\paperwidth, keepaspectratio]{assets/images/covers/cover-image-transparent-vol1.png}};

% Title
\newcommand{\titlelocationleft}{0.075\paperwidth}
\newcommand{\titlelocationbottom}{0.4\paperwidth}
\newcommand{\titlealign}{left}

\begin{scope}{%
\fontsize{52}{62.4}\selectfont
\node[anchor=north
west, align=left, rotate=0] (Title1) at ($(current page.south west)+(\titlelocationleft,\titlelocationbottom)$)  [text width = 0.9\paperwidth]  {{\nohyphens{Machine
Learning Systems}}};
}
\end{scope}

% Author
\newcommand{\authorlocationleft}{.925\paperwidth}
\newcommand{\authorlocationbottom}{0.175\paperwidth}
\newcommand{\authoralign}{right}

\begin{scope}
{%
\fontsize{20}{24.0}\selectfont
\node[anchor=north
east, align=right, rotate=0] (Author1) at ($(current page.south west)+(\authorlocationleft,\authorlocationbottom)$)  [text width = 6in]  {\coverauthorstyle{Vijay
Janapa Reddi\\}};
}
\end{scope}

% Footer
\newcommand{\footerlocationleft}{0.075\paperwidth}
\newcommand{\footerlocationbottom}{0.475\paperwidth}
\newcommand{\footerlocationalign}{left}

\begin{scope}
{%
\fontsize{25}{30.0}\selectfont
 \node[anchor=north west, align=left, rotate=0] (Footer1) at %
($(current page.south west)+(\footerlocationleft,\footerlocationbottom)$)  [text width = 0.9\paperwidth]  {{\nohyphens{Introduction
to}}};
}
\end{scope}

\end{tikzpicture}
\clearpage
\restoregeometry
%%% TITLE PAGE START

% Set up alignment commands
%Page
\newcommand{\titlepagepagealign}{
\ifthenelse{\equal{left}{right}}{\raggedleft}{}
\ifthenelse{\equal{left}{center}}{\centering}{}
\ifthenelse{\equal{left}{left}}{\raggedright}{}
}


\newcommand{\titleandsubtitle}{
% Title and subtitle
{{\huge{\bfseries{\nohyphens{Introduction to Machine Learning
Systems}}}}\par
}%
}
\newcommand{\titlepagetitleblock}{
\titleandsubtitle
}

\newcommand{\authorstyle}[1]{{\large{#1}}}

\newcommand{\affiliationstyle}[1]{{\large{#1}}}

\newcommand{\titlepageauthorblock}{
{\authorstyle{\nohyphens{Vijay Janapa
Reddi}{\textsuperscript{1}}\textsuperscript{,}{\textsuperscript{,*}}}}}

\newcommand{\titlepageaffiliationblock}{
\hangindent=1em
\hangafter=1
{\affiliationstyle{
{1}.~Harvard University


\vspace{1\baselineskip}
* \textit{Correspondence:}~Vijay Janapa Reddi~vj@eecs.harvard.edu
}}
}
\newcommand{\headerstyled}{%
{}
}
\newcommand{\footerstyled}{%
{\large{}}
}
\newcommand{\datestyled}{%
{February 2, 2026}
}


\newcommand{\titlepageheaderblock}{\headerstyled}

\newcommand{\titlepagefooterblock}{
\footerstyled
}

\newcommand{\titlepagedateblock}{
\datestyled
}

%set up blocks so user can specify order
\newcommand{\titleblock}{{

{\titlepagetitleblock}
}

\vspace{4\baselineskip}
}

\newcommand{\authorblock}{{\titlepageauthorblock}

\vspace{2\baselineskip}
}

\newcommand{\affiliationblock}{{\titlepageaffiliationblock}

\vspace{0pt}
}

\newcommand{\logoblock}{}

\newcommand{\footerblock}{}

\newcommand{\dateblock}{{\titlepagedateblock}

\vspace{0pt}
}

\newcommand{\headerblock}{}

\thispagestyle{empty} % no page numbers on titlepages


\newcommand{\vrulecode}{\textcolor{black}{\rule{\vrulewidth}{\textheight}}}
\newlength{\vrulewidth}
\setlength{\vrulewidth}{2pt}
\newlength{\B}
\setlength{\B}{\ifdim\vrulewidth > 0pt 0.05\textwidth\else 0pt\fi}
\newlength{\minipagewidth}
\ifthenelse{\equal{left}{left} \OR \equal{left}{right} }
{% True case
\setlength{\minipagewidth}{\textwidth - \vrulewidth - \B - 0.1\textwidth}
}{
\setlength{\minipagewidth}{\textwidth - 2\vrulewidth - 2\B - 0.1\textwidth}
}
\ifthenelse{\equal{left}{left} \OR \equal{left}{leftright}}
{% True case
\raggedleft % needed for the minipage to work
\vrulecode
\hspace{\B}
}{%
\raggedright % else it is right only and width is not 0
}
% [position of box][box height][inner position]{width}
% [s] means stretch out vertically; assuming there is a vfill
\begin{minipage}[b][\textheight][s]{\minipagewidth}
\titlepagepagealign
\titleblock

Prof.~Vijay Janapa Reddi

School of Engineering and Applied Sciences

Harvard University

\vspace{80mm}

With heartfelt gratitude to the community for their invaluable
contributions and steadfast support.

\vfill

February 2, 2026

\vfill
\par

\end{minipage}\ifthenelse{\equal{left}{right} \OR \equal{left}{leftright} }{
\hspace{\B}
\vrulecode}{}
\clearpage
%%% TITLE PAGE END
\end{titlepage}
\setcounter{page}{1}
\end{frontmatter}

%%%%% end titlepage extension code

% =============================================================================
% HALF-TITLE PAGE (Volume I)
% =============================================================================
% Standard academic book sequence: half-title -> blank -> title page -> copyright
% The half-title shows only the book title -- no author, no publisher, no date.
\thispagestyle{empty}
\begin{center}
\vspace*{0.3\textheight}
{\fontsize{24pt}{28pt}\selectfont\bfseries\color{crimson} Introduction to\\[0.4em] Machine Learning Systems}\\[2em]
{\large\itshape Volume~I}
\vfill
\end{center}
\clearpage
\thispagestyle{empty}\null\clearpage  % Blank verso (back of half-title)

\renewcommand*\contentsname{Table of contents}
{
\hypersetup{linkcolor=}
\setcounter{tocdepth}{2}
\tableofcontents
}
\listoffigures
\listoftables

\mainmatter
\bookmarksetup{startatroot}

\chapter*{Welcome to Volume I}\label{welcome-to-volume-i}
\addcontentsline{toc}{chapter}{Welcome to Volume I}

\markboth{Welcome to Volume I}{Welcome to Volume I}

\bookmarksetup{startatroot}

\chapter{ML Operations}\label{sec-machine-learning-operations-mlops}

\marginnote{\begin{footnotesize}

\emph{DALL·E 3 Prompt: Create a detailed, wide rectangular illustration
of an AI workflow. The image should showcase the process across six
stages, with a flow from left to right: 1. Data collection, with diverse
individuals of different genders and descents using a variety of devices
like laptops, smartphones, and sensors to gather data. 2. Data
processing, displaying a data center with active servers and databases
with glowing lights. 3. Model training, represented by a computer screen
with code, neural network diagrams, and progress indicators. 4. Model
evaluation, featuring people examining data analytics on large monitors.
5. Deployment, where the AI is integrated into robotics, mobile apps,
and industrial equipment. 6. Monitoring, showing professionals tracking
AI performance metrics on dashboards to check for accuracy and concept
drift over time. Each stage should be distinctly marked and the style
should be clean, sleek, and modern with a dynamic and informative color
scheme.}

\end{footnotesize}}

\noindent
\pandocbounded{\includegraphics[keepaspectratio]{contents/vol1/ops/images/png/cover_ml_ops.png}}

\section*{Purpose}\label{purpose}
\addcontentsline{toc}{section}{Purpose}

\markright{Purpose}

\emph{Why do machine learning prototypes that work perfectly in
development often fail catastrophically when deployed to production?}

Serving infrastructure answers requests in milliseconds---but deployment
is not the finish line. Traditional software fails loudly: a null
pointer exception crashes the server and monitoring dashboards turn red.
Machine learning systems fail silently: a model experiencing data drift
continues serving predictions with full confidence while accuracy
degrades significantly, triggering no alerts. This fundamental
difference---probabilistic systems that decay rather than
crash---explains \emph{why} ML requires operational practices distinct
from traditional DevOps. The gap between development and production is
not a deployment hurdle to be cleared once but a continuous condition of
entropy to be managed indefinitely: the world changes, distributions
drift, and models that were correct yesterday become wrong tomorrow.
Machine learning operations exists because uptime is not enough---the
system can be perfectly available while being perfectly wrong.

\begin{tcolorbox}[enhanced jigsaw, left=2mm, colframe=quarto-callout-tip-color-frame, coltitle=black, opacityback=0, breakable, colback=white, rightrule=.15mm, toptitle=1mm, colbacktitle=quarto-callout-tip-color!10!white, arc=.35mm, leftrule=.75mm, opacitybacktitle=0.6, bottomtitle=1mm, titlerule=0mm, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Learning Objectives}, bottomrule=.15mm, toprule=.15mm]

\begin{itemize}
\tightlist
\item
  Explain why silent failures distinguish ML systems from traditional
  software and necessitate specialized operational practices
\item
  Compare deployment patterns including canary testing, blue-green
  strategies, and shadow deployments for different risk profiles
\item
  Analyze technical debt patterns in ML systems, including boundary
  erosion, correction cascades, and data dependencies
\item
  Apply cost-aware automation principles to make quantitative decisions
  about model retraining and resource allocation
\item
  Design feature stores and CI/CD pipelines that ensure consistency
  between training and serving environments
\item
  Implement monitoring strategies that detect data drift, model
  degradation, and training-serving skew before production impact
\item
  Evaluate organizational MLOps maturity levels and their architectural
  implications for infrastructure and automation
\end{itemize}

\end{tcolorbox}

\section{Introduction to Machine Learning
Operations}\label{sec-machine-learning-operations-mlops-introduction-machine-learning-operations-04c6}

The preceding chapters taught you to build, optimize, benchmark, and
serve ML systems. Benchmarking (\textbf{?@sec-benchmarking-ai}) told you
how a model performs at a point in time; serving infrastructure
(\textbf{?@sec-model-serving-systems}) showed how to answer requests in
milliseconds. You deploy to production, and week one looks excellent.
But what happens next? Data distributions shift, user behavior changes,
and the world moves on from the conditions under which the model was
trained. This chapter addresses what happens \emph{over time}: how to
ensure that a model's measured performance is maintained as the
environment evolves around it.

The urgency of this problem is difficult to overstate. Organizations
without systematic operational practices routinely see deployment cycles
stretch from weeks to months, and a large fraction of ML models that
succeed in development never reach sustained production
use{[}\^{}fn-mlops-business-impact{]}. The root cause is what we call
\emph{the operational mismatch} between how traditional software fails
and how ML systems degrade:

\phantomsection\label{callout-perspectiveux2a-1.1}
\begin{fbxSimple}{callout-perspective}{Systems Perspective:}{The Operational Mismatch}
\phantomsection\label{callout-perspective*-1.1}
Traditional monitoring answers: \emph{Is} the server running? \emph{Is}
latency acceptable? \emph{Are} requests succeeding? These questions
suffice for deterministic software where correctness is binary.

ML monitoring must answer: \emph{Is the model still accurate? Have input
distributions shifted? Are predictions degrading for specific user
segments?} These are statistical questions with no obvious error
signals. A 94\% accurate model degrading to 81\% throws no exceptions,
triggers no alerts, and maintains perfect uptime while actively harming
users.

The operational discipline of MLOps exists to close this observability
gap, transforming statistical health into actionable signals before
business metrics reveal the damage.

\end{fbxSimple}

\textbf{Machine Learning Operations (MLOps)}\sidenote{\textbf{MLOps
Emergence}: While machine learning operations challenges were identified
earlier by D. Sculley and colleagues at Google in their influential 2015
paper ``Hidden Technical Debt in Machine Learning Systems''
(\citeproc{ref-sculley2015hidden}{Sculley et al. 2021}), the term
``MLOps'' itself was coined around 2018 as the discipline matured. The
field emerged as organizations faced the ``last mile'' problem where a
large fraction of ML models fail to reach sustained production use due
to operational challenges such as monitoring gaps, data and feature
drift, brittle pipelines, and unclear ownership. } is the engineering
discipline that \emph{makes these invisible failures visible}. It
synthesizes monitoring, automation, and governance into production
architectures that detect degradation, trigger retraining, and maintain
system health over a model's operational lifetime. Where traditional
DevOps\sidenote{\textbf{DevOps Origins}: The ``wall of confusion''
between development and operations teams was so notorious that Patrick
Debois called his 2009 conference ``DevOpsDays'' specifically to bridge
this gap. The movement emerged from the frustrations of the ``throw it
over the wall'' mentality where developers built software in isolation
from operations teams who had to deploy and maintain it. } assumes
deterministic software in which the same code with the same inputs
produces the same outputs, MLOps addresses systems whose correctness
depends on training data distributions, learned parameters, and
environmental conditions that shift continuously.

A concrete example illustrates \emph{why} this discipline is necessary.
Consider deploying a demand prediction system for a ridesharing service.
Your benchmark results (\textbf{?@sec-benchmarking-ai}) show 94\%
accuracy, 15ms P99 latency, and strong performance across test segments.
You deploy. Week one: excellent. Week four: accuracy has dropped to
88\%, but your infrastructure metrics show nothing wrong. Week eight: a
product manager notices driver dispatch is inefficient; investigation
reveals the model has not adapted to a competitor's new promotion that
shifted user behavior. The model needed retraining six weeks ago, but no
system was watching for this degradation. MLOps provides the framework
to detect such drift, trigger retraining, and validate new models before
users experience the impact.

This framing connects directly to the book's analytical foundations. If
benchmarking provides the \emph{sensors} for our system, MLOps is the
complete \emph{control system}. It closes the \textbf{Verification Gap}
by continuously recalibrating against a changing world, ensuring that
model performance does not silently erode between evaluation cycles.
MLOps operationalizes the \textbf{Degradation Equation} introduced in
\textbf{?@sec-introduction}: accuracy decay is not a failure of the
code, but an inevitable consequence of the distributional divergence
between the world we trained on and the world we serve. MLOps also
formalizes interfaces and responsibilities across traditionally isolated
domains: data science, machine learning engineering, and systems
operations (\citeproc{ref-amershi2019software}{Amershi et al. 2019}).
Mature implementations provide continuous retraining as new data
arrives, A/B evaluation against production baselines, graduated rollout
strategies, and real-time performance assessment without service
disruption. Standardized tracking of model versions, data lineage, and
configuration parameters establishes reproducible, auditable artifact
trails essential for regulated domains where provenance constitutes a
compliance requirement.

This chapter focuses on what we term \textbf{single-model operations}:
the practices required to deploy, monitor, and maintain one ML system in
production. We define the operational unit for this scope as the
\emph{ML Node}, a complete system comprising data pipelines, feature
computation, model training, serving infrastructure, and monitoring for
a single machine learning application. The chapter covers operating one
ML system end-to-end through training, deployment, monitoring, and
retraining; technical debt patterns that emerge within a single model's
lifecycle; feature stores and CI/CD pipelines for individual model
workflows; monitoring and drift detection for a single production model;
and deployment strategies for different risk profiles. Platform
operations at larger scale---managing hundreds of models, cross-model
dependencies, multi-region coordination, and organization-wide ML
platform engineering---constitute advanced topics that build on these
single-model foundations.

The single-model operational challenge can be decomposed into three
distinct interface problems. Each interface has its own failure modes,
its own tooling ecosystem, and its own organizational responsibilities.
Understanding \emph{the three critical interfaces} helps identify where
operational investments will have the highest impact.

\phantomsection\label{callout-perspectiveux2a-1.2}
\begin{fbxSimple}{callout-perspective}{Systems Perspective:}{The Three Critical Interfaces}
\phantomsection\label{callout-perspective*-1.2}
Operationalizing machine learning requires coordinating three distinct
system boundaries, each with unique constraints:

\textbf{Data-Model Interface}: The handoff between data infrastructure
and model training. The goal is \textbf{feature consistency}: if
training and serving pipelines compute features differently, model
behavior becomes unpredictable. Feature stores
(Section~\ref{sec-machine-learning-operations-mlops-feature-stores-c01c})
address this by providing a single source of truth.

\textbf{Model-Infrastructure Interface}: The transition from trained
weights to scalable service. The challenge is \textbf{environment
parity}: a model working in a notebook may fail in production due to
version mismatches. Model registries and containerization
(Section~\ref{sec-machine-learning-operations-mlops-model-deployment-serving-63f2})
package models with their operational context.

\textbf{Production-Monitoring Interface}: The feedback loop enabling
self-correction. Because ML systems fail silently through drift rather
than crashes, monitoring must provide statistical telemetry back to
training. This interface determines \textbf{retraining cadence}: when
has the world changed enough to require a new model?

The infrastructure components, production operations, and maturity
frameworks that follow address these three interfaces systematically.

\end{fbxSimple}

Telemetry\sidenote{\textbf{Telemetry}: From Greek \emph{tele} (far,
remote) and \emph{metron} (measure), literally ``remote measurement.''
The term originated in the 1930s for transmitting measurements from
remote locations via radio. In ML systems, telemetry refers to the
automated collection and transmission of model performance metrics,
feature distributions, and system health indicators from production
environments back to development teams. } forms the foundation of the
production-monitoring interface, enabling teams to detect drift before
it impacts users.

\subsection{MLOps}\label{sec-machine-learning-operations-mlops-mlops-3ea3}

MLOps builds on DevOps but addresses the specific demands of ML system
development and deployment. DevOps achieved remarkable success for
traditional software by assuming deterministic behavior: the same code
with the same inputs produces the same outputs. Machine learning systems
violate this assumption fundamentally because they depend on training
data distributions, learned parameters, and environmental conditions
that shift over time.

Where DevOps focuses on integrating and delivering deterministic
software, MLOps must manage non-deterministic, data-dependent workflows
spanning data acquisition, preprocessing, model training, evaluation,
deployment, and continuous monitoring through an iterative cycle
connecting design, model development, and operations
(Figure~\ref{fig-mlops-diagram}). The following definition captures this
discipline's scope:

\begin{figure}[htb]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/1eac252ead31f6c15bf79d0b380969288a3069a3.pdf}}

}

\caption{\label{fig-mlops-diagram}\textbf{Iterative MLOps Loop.} MLOps
extends DevOps principles to manage the unique challenges of machine
learning systems, including data versioning, model retraining, and
continuous monitoring. The iterative workflow encompasses data
engineering, model development, and reliable deployment for sustained
performance in production.}

\end{figure}%

\phantomsection\label{callout-definitionux2a-1.3}
\begin{fbxSimple}{callout-definition}{Definition:}{MLOps}
\phantomsection\label{callout-definition*-1.3}
\textbf{\emph{Machine Learning Operations (MLOps)}} is the discipline of
managing \textbf{Stochastic Systems} in production. It bridges the gap
between deterministic code and probabilistic data by implementing
\textbf{Continuous Training} and \textbf{Monitoring} loops, ensuring
that the system's predictive performance is maintained despite the
inevitable \textbf{Entropy} of changing data distributions.

\end{fbxSimple}

The operational complexity and business risk of deploying machine
learning without systematic engineering practices becomes clear when
examining real-world failures. Consider a retail company that deployed a
recommendation model initially boosting sales by 15\%. Due to silent
data drift, the model's accuracy degraded over six months, eventually
reducing sales by 5\% compared to the original system. The problem went
undetected because monitoring focused on system uptime rather than model
performance metrics. The company lost an estimated \$10 million in
revenue before the issue was discovered during routine quarterly
analysis. This scenario, common in early ML deployments, illustrates
\emph{why} MLOps is not merely an engineering best practice but a
business necessity for organizations depending on machine learning
systems for critical operations.

\subsection{Foundational
Principles}\label{sec-machine-learning-operations-mlops-foundational-principles-44e6}

The retail company example illustrates a pattern: without systematic
operational practices, even accurate models fail in production. Before
examining specific tools and practices, we establish the enduring
principles that underpin all MLOps implementations. These principles
remain constant even as specific tools evolve, providing a framework for
evaluating any MLOps solution.

\paragraph*{Principle 1: Reproducibility Through
Versioning}\label{sec-ops-principle-reproducibility}
\addcontentsline{toc}{paragraph}{Principle 1: Reproducibility Through
Versioning}

Every artifact\sidenote{\textbf{Artifact}: From Latin \emph{arte factum}
(``made by skill''), originally referring to objects crafted by human
workmanship, especially in archaeology. In software engineering, the
term was adopted to describe any tangible byproduct of development:
compiled binaries, documentation, or test results. In ML, artifacts
include trained model weights, preprocessing configurations, feature
definitions, and evaluation metrics, all products of the training
``craft'' that must be preserved for reproducibility. } that influences
model behavior must be versioned and traceable. This principle extends
beyond code versioning to encompass data, configurations, and
environments:

\[\text{Model Output} = f(\text{Code}_v, \text{Data}_v, \text{Config}_v, \text{Environment}_v)\]

where each subscript \(v\) denotes a specific version. A model cannot be
reproduced unless all four components are captured. Tools that implement
this principle vary in implementation but share the common goal of
enabling complete reproducibility. These include version control
systems, data versioning platforms, and configuration managers.

\paragraph*{Principle 2: Separation of
Concerns}\label{sec-ops-principle-separation}
\addcontentsline{toc}{paragraph}{Principle 2: Separation of Concerns}

Table~\ref{tbl-mlops-layers} decomposes MLOps systems into distinct
functional layers that can evolve independently:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2000}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4600}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3400}}@{}}
\caption{\textbf{MLOps Separation of Concerns.} Each layer addresses a
distinct responsibility and evolves at different rates, from stable
hardware foundations through model-level components that change with
each experiment. This separation enables independent scaling and
updates, reducing blast radius when changes are
required.}\label{tbl-mlops-layers}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Layer}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Responsibility}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Stability}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Layer}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Responsibility}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Stability}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Data Layer} & Feature computation, storage, serving & Changes
with data schema evolution \\
\textbf{Training Layer} & Model development, hyperparameter optimization
& Changes with algorithm research \\
\textbf{Serving Layer} & Inference, scaling, latency management &
Changes with traffic patterns \\
\textbf{Monitoring Layer} & Drift detection, performance tracking &
Changes with business requirements \\
\end{longtable}

This separation enables teams to update serving infrastructure without
retraining models, modify monitoring thresholds without redeploying, and
evolve data pipelines while maintaining model compatibility.

\paragraph*{Principle 3: The Consistency
Imperative}\label{sec-ops-principle-consistency}
\addcontentsline{toc}{paragraph}{Principle 3: The Consistency
Imperative}

Training and serving environments must process data identically. The
cost of inconsistency grows with system scale:

\[\text{Skew Cost} = \text{Base Error Rate} \times \text{Query Volume} \times \text{Error Impact}\]

For a system serving one million queries daily with 1\% skew-induced
errors costing \$0.10 each, annual skew cost reaches \$365,000. This
quantifies why consistency mechanisms represent investments with
measurable returns. These mechanisms include feature stores, shared
preprocessing code, and validation checks.

\paragraph*{Principle 4: Observable
Degradation}\label{sec-ops-principle-observable}
\addcontentsline{toc}{paragraph}{Principle 4: Observable Degradation}

ML systems must make silent failures visible through continuous
measurement. Model performance degrades along a continuum rather than
failing discretely, requiring the detection mechanisms and response
strategies summarized in Table~\ref{tbl-degradation-types}:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3380}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3239}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3380}}@{}}
\caption{\textbf{Degradation Detection Strategies.} Different failure
modes require different monitoring approaches and response strategies.
Statistical tests detect distribution shifts before performance degrades
visibly, while performance monitoring catches issues that evade
statistical detection. Adaptive thresholds prevent false alarms while
maintaining sensitivity to genuine
degradation.}\label{tbl-degradation-types}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Degradation Type}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Detection Mechanism}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Response Strategy}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Degradation Type}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Detection Mechanism}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Response Strategy}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Sudden accuracy drop} & Threshold alerts & Immediate rollback \\
\textbf{Gradual drift} & Trend analysis & Scheduled retraining \\
\textbf{Subgroup degradation} & Cohort monitoring & Targeted data
collection \\
\textbf{Latency increase} & Percentile tracking & Infrastructure
scaling \\
\end{longtable}

\paragraph*{Principle 5: Cost-Aware
Automation}\label{sec-ops-principle-cost}
\addcontentsline{toc}{paragraph}{Principle 5: Cost-Aware Automation}

Automation decisions should balance computational costs against accuracy
improvements. The decision to retrain can be modeled as:

\[\text{Retrain if: } \Delta\text{Accuracy} \times \text{Value per Point} > \text{Training Cost} + \text{Deployment Risk}\]

This principle guides the design of retraining triggers, validation
thresholds, and deployment strategies examined throughout this chapter.
The specific values vary by domain, but the framework for making
principled tradeoff decisions remains constant.
Section~\ref{sec-machine-learning-operations-mlops-quantitative-retraining-economics-1579}
derives the complete economic model with worked examples showing
\emph{how} to calculate optimal retraining intervals.

These five principles form the evaluation framework for all MLOps
tooling and practices. Table~\ref{tbl-mlops-principles-summary} provides
a quick reference for each principle, its core insight, and key metric.
When assessing any tool or approach, ask: Does it enable
reproducibility? Does it respect separation of concerns? Does it ensure
consistency? Does it make degradation observable? Does it support
cost-aware decisions?

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3718}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3462}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2821}}@{}}
\caption{\textbf{MLOps Principles Summary.} Quick reference for the five
foundational principles that guide all MLOps tooling and practice
decisions.}\label{tbl-mlops-principles-summary}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Principle}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Core Insight}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Key Metric}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Principle}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Core Insight}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Key Metric}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{1. Reproducibility} & Version all artifacts & Complete artifact
hash \\
\textbf{2. Separation of Concerns} & Independent layer evolution & Layer
coupling score \\
\textbf{3. Consistency} & Training equals Serving & Feature skew rate \\
\textbf{4. Observable Degradation} & Make failures visible & Time to
detection \\
\textbf{5. Cost-Aware Automation} & Optimize total cost & Net retraining
value \\
\end{longtable}

\emph{How} these principles manifest in practice depends on the
workload. A recommendation system drifts daily as user preferences
shift; a TinyML model deployed on embedded hardware may run unchanged
for months. The monitoring strategy must match the archetype.

\phantomsection\label{callout-lighthouseux2a-1.4}
\begin{fbxSimple}{callout-lighthouse}{Lighthouse:}{Monitoring Strategy by Archetype}
\phantomsection\label{callout-lighthouse*-1.4}
The dominant failure modes and monitoring priorities differ
fundamentally across workload archetypes.
Table~\ref{tbl-monitoring-archetype-strategy} details each archetype's
drift pattern, monitoring metric, and retraining trigger:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1556}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2963}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2370}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3111}}@{}}
\caption{\textbf{Monitoring Strategy by Workload Archetype.} Monitoring
strategy varies by workload archetype's dominant failure mode, requiring
tailored metrics and response thresholds for each deployment
context.}\label{tbl-monitoring-archetype-strategy}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Archetype}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Dominant Drift Pattern}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Primary Monitoring Metric}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Typical Retraining Trigger}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Archetype}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Dominant Drift Pattern}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Primary Monitoring Metric}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Typical Retraining Trigger}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{ResNet-50} & Visual distribution shift & \textbf{Accuracy on
holdout set} & Accuracy drops \textgreater{} 2\% from baseline \\
\textbf{(Compute Beast)} & (lighting, camera, new object classes) &
(ground truth available) & (\(\sim\)monthly for stable domains) \\
\textbf{GPT-2} & Vocabulary drift, topic shift, & \textbf{Perplexity on
live traffic} & Perplexity increases \textgreater{} 10\%; new
vocabulary \\
\textbf{(Bandwidth Hog)} & emerging entities & (no ground truth needed)
& detected (\(\sim\)weekly for news domains) \\
\textbf{DLRM} & User behavior shift, item catalog churn, &
\textbf{CTR/CVR delta} vs.~historical & Engagement drops \textgreater{}
5\%; catalog refresh \\
\textbf{(Sparse Scatter)} & cold-start items & cohorts & (\(\sim\)daily
for e-commerce) \\
\textbf{DS-CNN} & Acoustic environment change & \textbf{Duty cycle}
(wakeups/hour) + & False wake rate \textgreater{} 1\%; battery drain \\
\textbf{(Tiny Constraint)} & (noise floor shift) & \textbf{false
positive rate} & exceeds spec (\(\sim\)quarterly OTA update) \\
\end{longtable}

\textbf{Key insight}: Ground truth availability determines monitoring
strategy. ResNet-50 (image classification) can use explicit labels;
GPT-2 relies on proxy metrics (perplexity); DLRM uses implicit feedback
(clicks); DS-CNN monitors operational metrics (energy, false positives).
The retraining frequency spans 4 orders of magnitude: daily for
recommendation systems to quarterly for embedded devices.

\end{fbxSimple}

These principles respond to recurring challenges that distinguish ML
from traditional software deployment. \textbf{Data
drift}\sidenote{\textbf{Data Drift Discovery}: The concept was first
formalized by researchers studying spam detection systems in the early
2000s, who noticed that spam patterns evolved so rapidly that models
became obsolete within weeks. This led to the realization that ML
systems face a different challenge than traditional software: their
environment actively adapts to defeat them. }, the shift in input data
distributions over time, degrades model accuracy and requires continuous
monitoring and automated retraining procedures.
Reproducibility\sidenote{\textbf{ML Reproducibility Crisis}: A 2016
study by Collberg and Proebsting found that only 54\% of computer
systems research papers could be reproduced even when authors were
available to assist (\citeproc{ref-collberg2016repeatability}{Collberg
and Proebsting 2016}). This reproducibility challenge is even more acute
in ML research, though the situation has improved with initiatives like
Papers with Code and requirements for code submission at major ML
conferences. } presents another challenge: ML workflows lack
standardized mechanisms to track code, datasets, configurations, and
environments, making it difficult to reproduce past experiments
(\citeproc{ref-schelter2018automating}{Schelter et al. 2018}). The lack
of explainability in complex models has driven demand for tools that
increase model transparency and interpretability, especially in
regulated domains.

Beyond these foundational challenges, organizations face additional
operational complexities: difficulty detecting silent failures in
post-deployment monitoring, manual overhead in retraining and
redeploying models, and complex infrastructure configuration. These
challenges collectively motivate MLOps practices that emphasize
automation, collaboration, and lifecycle management.

The divergence of MLOps from traditional DevOps is driven by the silent
failure problem introduced at the chapter's opening: system health
cannot be measured by uptime or latency alone. Operational discipline in
ML requires monitoring the statistical properties of data distributions
and model outputs, shifting the focus from ``is the server running?'' to
``is the system still intelligent?''

In response to these distinct challenges, the field developed
specialized tools and workflows tailored to the ML lifecycle. Building
on DevOps foundations while addressing ML-specific requirements, MLOps
coordinates a broader stakeholder ecosystem and introduces specialized
practices such as data versioning\sidenote{\textbf{DVC Creation Story}:
Data Version Control was born from the frustration of Dmitry Petrov, who
spent weeks trying to reproduce an experiment only to discover the
training data had been quietly updated. He created DVC in 2017 to bring
Git-like versioning to data science, solving what he called ``the
biggest unsolved problem in machine learning.'' }, model versioning, and
model monitoring that extend beyond traditional DevOps scope.
Table~\ref{tbl-mlops} contrasts the objectives, methodologies, primary
tools, and typical outcomes of DevOps and MLOps, illustrating \emph{how}
ML workflows demand fundamentally different operational practices:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.0797}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4263}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4940}}@{}}
\caption{\textbf{MLOps vs.~DevOps.} MLOps extends DevOps principles to
address the unique requirements of machine learning systems, including
data and model versioning, and continuous monitoring for model
performance and data drift. MLOps coordinates a broader range of
stakeholders and emphasizes reproducibility and scalability beyond
traditional software development
workflows.}\label{tbl-mlops}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Aspect}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{DevOps}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{MLOps}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Aspect}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{DevOps}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{MLOps}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Objective} & Streamlining software development and operations
processes & Optimizing the lifecycle of machine learning models \\
\textbf{Methodology} & Continuous Integration and Continuous Delivery
(CI/CD) for software development & Similar to CI/CD but focuses on
machine learning workflows \\
\textbf{Primary Tools} & Version control (Git), CI/CD tools (Jenkins,
Travis CI), Configuration management (Ansible, Puppet) & Data versioning
tools, Model training and deployment tools, CI/CD pipelines tailored for
ML \\
\textbf{Primary Concerns} & Code integration, Testing, Release
management, Automation, Infrastructure as code & Data management, Model
versioning, Experiment tracking, Model deployment, Scalability of ML
workflows \\
\textbf{Typical Outcomes} & Faster and more reliable software releases,
Improved collaboration between development and operations teams &
Efficient management and deployment of machine learning models, Enhanced
collaboration between data scientists and engineers \\
\end{longtable}

This expanded scope fundamentally changes the development cycle. Verify
your understanding of the non-linear nature of MLOps.

\phantomsection\label{callout-checkpointux2a-1.5}
\begin{fbxSimple}{callout-checkpoint}{Checkpoint:}{The MLOps Loop}
\phantomsection\label{callout-checkpoint*-1.5}

MLOps is not linear; it is circular.

\textbf{The Feedback Cycle}

\begin{itemize}
\tightlist
\item[$\square$]
  \textbf{Closing the Loop}: How do production metrics (e.g., drift
  alerts) trigger new training cycles?
\item[$\square$]
  \textbf{Automated Retraining}: Is your pipeline robust enough to
  retrain and deploy a model without human intervention?
\end{itemize}

\textbf{The Artifacts}

\begin{itemize}
\tightlist
\item[$\square$]
  \textbf{Versioning}: Are you versioning Data + Code + Model +
  Environment together? (If you miss one, you cannot reproduce the
  system).
\end{itemize}

\end{fbxSimple}

The evolution from DevOps to MLOps reflects a fundamental truth: machine
learning systems fail differently than traditional software. Where
DevOps addresses deployment and scaling challenges for deterministic
code, MLOps must contend with systems that accumulate hidden complexity
through data dependencies, model interactions, and evolving
requirements. Understanding these unique failure modes, collectively
termed technical debt, explains \emph{why} MLOps requires specialized
infrastructure and practices beyond traditional DevOps approaches.

Having established \emph{how} MLOps diverges from DevOps, we now examine
the specific ways ML systems accumulate complexity over time. These
technical debt patterns are not merely theoretical; they are the
concrete failure modes that motivated every infrastructure component we
will examine later. Understanding boundary erosion explains \emph{why}
modular pipeline design is necessary. Recognizing correction cascades
reveals \emph{why} versioning and rollback are essential. Identifying
undeclared consumers justifies strict interface contracts. The patterns
below form a diagnostic vocabulary for recognizing problems before they
become production incidents.

\section{Technical Debt and System
Complexity}\label{sec-machine-learning-operations-mlops-technical-debt-system-complexity-2762}

Technical debt in machine learning systems extends beyond traditional
software engineering concerns to include ``hidden'' costs unique to
statistical modeling and data dependencies
(\citeproc{ref-sculley2015hidden}{Sculley et al. 2021}). Systematic
evaluation rubrics, such as the ML Test Score
(\citeproc{ref-breck2020ml}{Breck et al. 2017}), provide frameworks for
quantifying this debt and assessing production readiness across data,
model, and infrastructure components. Understanding these debt patterns
is essential for building systems that can evolve without becoming
brittle.

The silent failure modes established earlier manifest concretely as
technical debt: data changes, model interactions, and evolving
requirements cause gradual degradation that compounds over time. Unlike
code bugs that trigger stack traces, these failures accumulate invisibly
across multiple system components, demanding engineering approaches
designed specifically for probabilistic systems.

This complexity manifests as machine learning systems mature and scale,
where they accumulate technical debt: the long-term cost of expedient
design decisions made during development. Originally proposed in
software engineering in the 1990s\sidenote{\textbf{Technical Debt
Origins}: Ward Cunningham coined the term in 1992, comparing rushed
coding decisions to financial debt: ``A little debt speeds development
so long as it is paid back promptly with a rewrite.'' He later regretted
the metaphor became an excuse for bad code rather than a tool for
communicating tradeoffs. }, this metaphor compares shortcuts in
implementation to financial debt. Such shortcuts may enable short-term
velocity but require ongoing interest payments in the form of
maintenance, refactoring, and systemic risk.

In the ML context, this concept takes on a specific meaning:

\phantomsection\label{callout-definitionux2a-1.6}
\begin{fbxSimple}{callout-definition}{Definition:}{Technical Debt in ML}
\phantomsection\label{callout-definition*-1.6}
\textbf{\emph{Technical Debt in Machine Learning}} is the high interest
rate paid on \textbf{System Complexity} and \textbf{Implicit
Dependencies}. It arises because ML systems have all the maintenance
problems of traditional code plus a new set of ML-specific debt drivers:
\textbf{Entanglement}, \textbf{Correction Cascades}, and
\textbf{Undeclared Consumers}, where improvements in one component can
catastrophically degrade another.

\end{fbxSimple}

The abstract notion of technical debt becomes concrete when we examine
the cost dynamics of manual versus automated ML operations. Teams often
resist automation investment because manual processes seem faster in the
short term. The following analysis of \emph{the compound cost of manual
operations} reveals why this intuition is systematically wrong.

\phantomsection\label{callout-notebookux2a-1.7}
\begin{fbxSimple}{callout-notebook}{AI Engineer’s Notebook:}{The Compound Cost of Manual Operations}
\phantomsection\label{callout-notebook*-1.7}
\textbf{The Problem}: Why build automated pipelines when manual
retraining is faster?

\textbf{The Physics}: Manual work accumulates \textbf{Compound
Interest}.

\begin{itemize}
\tightlist
\item
  \textbf{Manual Retrain}: 4 engineering hours per week.
\item
  \textbf{Pipeline Build}: 80 engineering hours (one-time).
\end{itemize}

\textbf{The Calculation}:

\begin{itemize}
\tightlist
\item
  \textbf{Break-even Point}: 20 weeks.
\item
  \textbf{The Trap}: This assumes the model never changes.
\item
  \textbf{Reality}: Every new feature adds manual complexity. If feature
  count doubles, manual time doubles.
\item
  \textbf{Result}: After 1 year, manual teams spend \textbf{100\% of
  time} on maintenance. Pipeline teams spend \textbf{0\%}.
\end{itemize}

\textbf{The Broader Pattern}: A fundamental law of systems engineering
is that the cost of \emph{maintaining} a system over its lifetime dwarfs
the cost of \emph{building} it. Dave Patterson frequently emphasizes
that ``measuring everything'' is the only way to manage this complexity.
In ML, technical debt is especially dangerous because it is often
\textbf{data-driven} rather than \textbf{code-driven}: a perfect piece
of code can still fail if the data it processes shifts.

\textbf{The Systems Conclusion}: Automation is not just about speed; it
is about \textbf{Capacity Cap}. A manual team hits a ceiling where they
cannot deploy new models because they are drowning in the maintenance of
old ones. MLOps is the engineering response: it replaces the manual
``craft'' of model maintenance with a systematic ``factory'' of
observability and automation. If you don't build the monitoring
infrastructure to make silent failures visible, you aren't just taking
on debt; you are creating a system that is unmanageable by design.

\end{fbxSimple}

Figure~\ref{fig-technical-debt} illustrates how the ML code itself
represents only a small fraction of a production ML system's complexity.

\begin{figure}[htb]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/82e41076ef9a59e8f0f4bba990f7ba54e89a3206.pdf}}

}

\caption{\label{fig-technical-debt}\textbf{Hidden Infrastructure of ML
Systems.} Most engineering effort in a typical machine learning system
concentrates on components surrounding the model itself: data
collection, feature engineering, and system configuration rather than
the model code. The distribution reveals the operational challenges and
potential for technical debt arising from these often-overlooked
surrounding components. Source:
(\citeproc{ref-sculley2015hidden}{Sculley et al. 2021}).}

\end{figure}%

The calculation above reveals a fundamental truth: manual operations hit
a capacity ceiling. But the cost of automation extends beyond
engineering time to include the hidden complexity that accumulates in ML
systems. These operational challenges manifest in several distinct
patterns. Rather than cataloging every debt pattern, we focus on
representative examples that illustrate MLOps engineering approaches.
Each challenge emerges from ML's unique characteristics: reliance on
data rather than deterministic logic, statistical rather than exact
behavior, and implicit dependencies through data flows rather than
explicit interfaces.

Figure~\ref{fig-technical-debt-taxonomy} captures these technical debt
patterns, demonstrating \emph{why} traditional DevOps practices require
extension for ML systems and motivating the infrastructure solutions
presented in subsequent sections.

\begin{figure}[htb]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/a63c17b26d6223acbc1c7799f50c1097c9e59a07.pdf}}

}

\caption{\label{fig-technical-debt-taxonomy}\textbf{ML Technical Debt
Taxonomy.} Machine learning systems accumulate distinct forms of
technical debt from data dependencies, model interactions, and evolving
requirements. Six primary debt patterns radiate from a central hub:
boundary erosion undermines modularity, correction cascades propagate
fixes through dependencies, feedback loops create hidden coupling, while
data, configuration, and pipeline debt reflect poorly managed artifacts
and workflows.}

\end{figure}%

\subsection{Boundary
Erosion}\label{sec-machine-learning-operations-mlops-boundary-erosion-8ef5}

The first and often most insidious debt pattern involves the dissolution
of system boundaries. In traditional software, modularity and
abstraction provide clear boundaries between components, allowing
changes to be isolated and behavior to remain predictable. Machine
learning systems blur these boundaries through tightly coupled
interactions between data pipelines, feature engineering, model
training, and downstream consumption.

This erosion makes ML systems particularly vulnerable to cascading
effects from even minor changes. A seemingly small update to a
preprocessing step or feature transformation can propagate through the
system in unexpected ways, breaking assumptions made elsewhere in the
pipeline. This lack of encapsulation increases the risk of entanglement,
where dependencies between components become so intertwined that local
modifications require global understanding and coordination.

One manifestation of this problem is CACHE: Change Anything Changes
Everything. When systems are built without strong boundaries, adjusting
a feature encoding, model hyperparameter, or data selection criterion
can affect downstream behavior in unpredictable ways. This inhibits
iteration and makes testing and validation more complex. For example,
changing the binning strategy of a numerical feature may cause a
previously tuned model to underperform, triggering retraining and
downstream evaluation changes.

To mitigate boundary erosion, teams should prioritize architectural
practices that support modularity and encapsulation. Designing
components with well-defined interfaces allows teams to isolate faults,
reason about changes, and reduce the risk of system-wide regressions.
For instance, clearly separating data ingestion from feature engineering
and feature engineering from modeling logic introduces layers that can
be independently validated, monitored, and maintained.

Boundary erosion is often invisible in early development but becomes a
significant burden as systems scale. The root cause is that ML systems
operate with statistical rather than logical guarantees, creating
implicit couplings through data flows that bypass explicit interfaces.
Proactive design decisions that preserve abstraction, systematic
testing, and interface documentation provide practical defenses against
this creeping complexity.

\subsection{Correction
Cascades}\label{sec-machine-learning-operations-mlops-correction-cascades-e309}

If boundary erosion describes \emph{how} ML systems lose their
structural integrity, correction cascades describe \emph{what} happens
when teams attempt repairs.

A correction cascade occurs when fixing one component introduces
problems elsewhere, requiring additional fixes that themselves cause
further problems. In ML systems, these cascades are particularly severe
because changes propagate through statistical dependencies rather than
explicit code paths. Retraining a model to fix one failure mode may
degrade performance on previously working cases. Adjusting thresholds to
reduce false positives may increase false negatives. Adding features to
address edge cases may introduce correlations that destabilize the
entire system. Each correction triggers the need for more corrections,
creating a cascade that can consume engineering resources far exceeding
the original fix.

Figure~\ref{fig-correction-cascades-flowchart} visualizes how these
cascading effects propagate through ML system development.

\begin{figure}[htb]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/8e7f62824c7edf75c0b33466f042a8c707c511d9.pdf}}

}

\caption{\label{fig-correction-cascades-flowchart}\textbf{Correction
Cascades}: Iterative refinements in ML systems often trigger dependent
fixes across the workflow, propagating from initial adjustments through
data, model, and deployment stages. Color-coded arcs represent
corrective actions stemming from sources of instability, while red
arrows and the dotted line indicate escalating revisions, potentially
requiring a full system restart.}

\end{figure}%

The diagram illustrates \emph{how} cascades emerge across different
stages of the ML lifecycle, from problem definition and data collection
to model development and deployment. Each arc represents a corrective
action, and the colors indicate different sources of instability:
inadequate domain expertise, brittle real-world interfaces, misaligned
incentives, and insufficient documentation. Red arrows represent
cascading revisions, while the dotted arrow at the bottom highlights a
full system restart, a drastic but sometimes necessary outcome.

One common source of correction cascades is sequential model
development: reusing or fine-tuning existing models to accelerate
development for new tasks. While this strategy is often efficient, it
introduces hidden dependencies that are difficult to unwind later.
Assumptions embedded in earlier models become implicit constraints for
future models, limiting flexibility and increasing the cost of
downstream corrections.

Consider a team that fine-tunes a customer churn prediction model for a
new product. The original model may embed product-specific behaviors or
feature encodings that do not transfer to the new setting. As
performance issues emerge, teams may attempt to patch the model, only to
discover that the true problem lies several layers upstream in the
original feature selection or labeling criteria.

To mitigate correction cascades, teams must balance reuse against
redesign. For small, static datasets, fine-tuning may be appropriate;
for large or rapidly evolving datasets, retraining from scratch provides
greater control. Fine-tuning requires fewer computational resources but
modifying foundational components later becomes extremely costly due to
cascading effects.

The underlying mechanism is that when model A's outputs influence model
B's training data, implicit dependencies emerge through data flows
rather than explicit code interfaces. These dependencies are invisible
to traditional dependency analysis tools. Preventing cascades requires
architectural decisions that preserve system modularity: keeping models
loosely coupled, maintaining clear version boundaries, and designing for
independent evolution even when reusing components.

\subsection{Interface and Dependency
Challenges}\label{sec-machine-learning-operations-mlops-interface-dependency-challenges-7711}

Boundary erosion and correction cascades both stem from a deeper
structural problem: ML systems develop dependencies that bypass explicit
interfaces. Unlike traditional software where component interactions
occur through explicit APIs, ML systems often develop implicit
dependencies through data flows and shared outputs. Two critical
patterns illustrate these challenges:

\textbf{Undeclared Consumers}: Model outputs frequently serve downstream
components without formal tracking or interface contracts. When models
evolve, these hidden dependencies can break silently. For example, a
credit scoring model's outputs might feed an eligibility engine that
influences future applicant pools and training data, creating untracked
feedback loops that bias model behavior over time.

\textbf{Data Dependency Debt}: ML pipelines accumulate unstable and
underutilized data dependencies that become difficult to trace or
validate. Feature engineering scripts, data joins, and labeling
conventions lack the dependency analysis tools available in traditional
software development. When data sources change structure or
distribution, downstream models can fail unexpectedly.

\textbf{Engineering Solutions}: These challenges require systematic
approaches including strict access controls for model outputs, formal
interface contracts with documented schemas, data versioning and lineage
tracking systems, and comprehensive monitoring of prediction usage
patterns. The MLOps infrastructure patterns presented in subsequent
sections provide concrete implementations of these solutions.

\subsection{System Evolution
Challenges}\label{sec-machine-learning-operations-mlops-system-evolution-challenges-ea51}

The dependency patterns above describe debt that accumulates through
poor design. But even well-designed ML systems face evolution challenges
that differ fundamentally from traditional software:

\textbf{Feedback Loops}: Models influence their own future behavior
through the data they generate. Recommendation systems exemplify this:
suggested items shape user clicks, which become training data,
potentially creating self-reinforcing biases. These loops undermine data
independence assumptions and can mask performance degradation for
months.

\textbf{Pipeline and Configuration Debt}: ML workflows often evolve into
``pipeline jungles'' of ad hoc scripts and fragmented configurations.
Without modular interfaces, teams build duplicate pipelines rather than
refactor brittle ones, leading to inconsistent processing and
maintenance burden.

\textbf{Early-Stage Shortcuts}: Rapid prototyping encourages embedding
business logic in training code and undocumented configuration changes.
While necessary for innovation, these shortcuts become liabilities as
systems scale across teams.

\textbf{Engineering Solutions}: Managing evolution requires
architectural discipline including cohort-based monitoring for loop
detection, modular pipeline design with workflow orchestration tools,
and treating configuration as a first-class system component with
versioning and validation.

\subsection{Code and Architecture
Debt}\label{sec-machine-learning-operations-mlops-code-architecture-debt-9140}

The preceding sections examined debt arising from data dependencies and
system evolution. But ML systems also accumulate code-level debt
patterns that differ from traditional software debt. The Sculley et
al.~technical debt paper (\citeproc{ref-sculley2015hidden}{Sculley et
al. 2021}) identifies several patterns that deserve explicit attention:

\textbf{Glue Code}: ML systems often require substantial ``glue code''
to connect general-purpose ML packages to specific data pipelines and
serving systems. This glue code can constitute 95\% of the codebase
while the actual ML code represents only 5\%. Glue code is particularly
problematic because it creates tight coupling between the ML package's
API and the surrounding system. When packages update their interfaces,
all glue code must be rewritten. Mitigation requires wrapping ML
packages in stable internal APIs and treating external dependencies as
substitutable components.

\textbf{Dead Experimental Codepaths}: ML development involves extensive
experimentation, leaving behind conditional branches for abandoned
approaches. Unlike traditional dead code that can be detected
statically, experimental ML codepaths often remain ``live'' because they
are controlled by configuration flags rather than compile-time
conditions. Over time, these paths accumulate, increasing testing burden
and creating confusion about which code actually runs in production.
Regular code audits with explicit deprecation timelines and feature flag
hygiene help manage this debt.

\textbf{Abstraction Debt}: Traditional software engineering relies on
well-defined abstractions like functions, classes, and modules. ML
systems lack mature abstractions for key concepts: What is the right
interface for a ``feature''? How should ``model behavior'' be
encapsulated? This absence forces teams to reinvent abstractions or,
worse, avoid abstraction entirely. The ML community has begun addressing
this through emerging patterns like feature stores (abstracting feature
computation), model registries (abstracting model versioning), and
prediction services (abstracting inference). Adopting these emerging
standards reduces per-project abstraction debt.

\textbf{Common Smells Pattern}: Sculley et al.~identify warning signs
that indicate accumulating debt:

\begin{itemize}
\tightlist
\item
  \emph{Plain-Old-Data Type Smell}: Using generic types (strings,
  floats) instead of semantic types that encode meaning and constraints
\item
  \emph{Multiple-Language Smell}: Systems spanning Python, SQL, C++, and
  shell scripts with inconsistent conventions
\item
  \emph{Prototype Smell}: ``Temporary'' research code that becomes
  permanent infrastructure without refactoring
\end{itemize}

Teams should track these smells in code reviews and allocate explicit
time for debt reduction, treating technical debt paydown as a
first-class engineering activity rather than an afterthought.

\subsection{Real-World Technical Debt
Examples}\label{sec-machine-learning-operations-mlops-realworld-technical-debt-examples-8480}

The debt patterns described above are not theoretical constructs. They
have played a critical role in shaping real-world machine learning
systems. The following examples illustrate \emph{how} unseen
dependencies and misaligned assumptions accumulate quietly, only to
become major liabilities over time:

\textbf{YouTube: Feedback Loop Debt.} YouTube's recommendation engine
has faced repeated criticism for promoting sensational or polarizing
content\sidenote{\textbf{YouTube Recommendation Impact}: The
recommendation system drives the majority of watch time on the platform,
processing over 1 billion hours of video daily. Algorithmic changes
around 2016, which incorporated deep neural networks, significantly
increased watch time while inadvertently promoting conspiracy content.
Fixing these feedback loops required over 2 years of engineering work
and new evaluation frameworks. }. Much of this stems from feedback loop
debt: recommendations influence user behavior, which in turn becomes
training data. Over time, this led to unintended content amplification.
Mitigating this required substantial architectural overhauls, including
cohort-based evaluation, delayed labeling, and more explicit
disentanglement between engagement metrics and ranking logic.

\textbf{Zillow: Correction Cascade Failure.} Zillow's home valuation
model (Zestimate) faced significant correction cascades during its
iBuying venture\sidenote{\textbf{Zillow iBuying Failure}: Zillow
reported losses exceeding \$500 million in Q3 2021 due to multiple
factors including ML model failures. The Zestimate algorithm reportedly
overvalued homes, leading to systematic purchasing errors. The company
laid off approximately 2,000 employees (25\% of workforce) and took a
\$304 million inventory write-down when shutting down Zillow Offers. }.
When initial valuation errors propagated into purchasing decisions,
retroactive corrections triggered systemic instability that required
data revalidation, model redesign, and eventually a full system
rollback. The company shut down the iBuying arm in 2021, citing model
unpredictability and data feedback effects as core challenges.

\textbf{Tesla: Undeclared Consumer Debt.} In early deployments, Tesla's
Autopilot made driving decisions based on models whose outputs were
repurposed across subsystems without clear boundaries. Over-the-air
updates occasionally introduced silent behavior changes that affected
multiple subsystems (lane centering, braking) in unpredictable ways.
This entanglement illustrates undeclared consumer debt and the risks of
skipping strict interface governance in ML-enabled safety-critical
systems.

\textbf{Facebook: Configuration Debt.} Facebook's News Feed algorithm
has undergone numerous iterations, often driven by rapid
experimentation. The lack of consistent configuration management led to
opaque settings that influenced content ranking without clear
documentation. As a result, changes to the algorithm's behavior were
difficult to trace, and unintended consequences emerged from misaligned
configurations. This example highlights the importance of treating
configuration as a first-class citizen in ML systems.

These real-world examples demonstrate the pervasive nature of technical
debt in ML systems. The patterns examined above manifest as concrete
operational failures costing hundreds of millions of dollars and years
of engineering remediation.

\textbf{The key insight}: each debt pattern has a corresponding
infrastructure solution. Feature stores address data dependency debt by
centralizing feature computation. Versioning systems prevent
configuration debt by tracking all artifacts. CI/CD pipelines reduce
pipeline debt through standardized workflows. Monitoring systems make
feedback loops visible. The infrastructure components that follow are
not arbitrary tooling choices; they are engineering responses to the
specific failure modes we have examined. Having diagnosed the disease,
we now turn to the treatment.

\section{Development Infrastructure and
Automation}\label{sec-machine-learning-operations-mlops-development-infrastructure-automation-de41}

Having established \emph{why} ML systems accumulate unique forms of
technical debt, we now examine \emph{how} infrastructure components
prevent and mitigate these patterns. Each component maps directly to the
foundational principles
(Section~\ref{sec-machine-learning-operations-mlops-foundational-principles-44e6}):

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2667}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3238}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4095}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Infrastructure Component}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Principle Implemented}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Debt Pattern Addressed}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Feature stores} & Consistency Imperative & Data dependency debt,
training-serving skew \\
\textbf{Versioning systems} & Reproducibility Through Versioning &
Configuration debt, correction cascades \\
\textbf{CI/CD pipelines} & Cost-Aware Automation & Pipeline debt,
boundary erosion \\
\textbf{Monitoring systems} & Observable Degradation & Feedback loops,
silent failures \\
\end{longtable}

Figure~\ref{fig-ops-layers} visualizes how these components form a
layered architecture spanning ML models, frameworks, orchestration,
infrastructure, and hardware. Understanding how these components
interact enables practitioners to design systems that systematically
address the technical debt patterns identified earlier while maintaining
operational sustainability.

\begin{figure}[htb]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/58279ef513017eed407e2327bfe2dcf12b28fd79.pdf}}

}

\caption{\label{fig-ops-layers}\textbf{MLOps Stack Layers.} Five tiers
organize the ML system stack: ML Models at the top, followed by
Frameworks, Orchestration, Infrastructure, and Hardware. MLOps spans
orchestration tasks (data management through model serving) and
infrastructure tasks (job scheduling through monitoring), enabling
automation, reproducibility, and scalable deployment.}

\end{figure}%

\subsection{Data Infrastructure and
Preparation}\label{sec-machine-learning-operations-mlops-data-infrastructure-preparation-284a}

Reliable machine learning systems depend on structured, scalable, and
repeatable data handling. From the moment data is ingested to the point
where it informs predictions, each stage must preserve quality,
consistency, and traceability. In operational settings, data
infrastructure supports initial development, continual retraining,
auditing, and serving. These requirements demand systems that formalize
the transformation and versioning of data throughout the ML lifecycle.

\subsubsection{Data
Management}\label{sec-machine-learning-operations-mlops-data-management-93ed}

The technical debt patterns we examined stem largely from poor data
management: unversioned datasets create boundary erosion, inconsistent
feature computation causes correction cascades, and undocumented data
dependencies breed hidden consumers. Data management infrastructure
directly addresses these root causes. Building on the data engineering
foundations from \textbf{?@sec-data-engineering-ml}, data collection,
preprocessing, and feature transformation become formalized operational
processes. Where data engineering focuses on single-pipeline
correctness, MLOps data management emphasizes cross-pipeline
consistency, ensuring that training and serving compute identical
features. Data management thus extends beyond initial preparation to
encompass the continuous handling of data artifacts throughout the ML
system lifecycle.

Central to this foundation is dataset versioning that enables
reproducible model development by tracking data evolution.
Section~\ref{sec-machine-learning-operations-mlops-versioning-lineage-b1cf}
examines implementation details including Git integration, metadata
tracking, and lineage preservation. Tools such as DVC
(\citeproc{ref-dvc}{Iterative 2024}) enable teams to version large
datasets alongside code repositories managed by Git
(\citeproc{ref-git}{Torvalds and Hamano 2024}), ensuring data lineage is
preserved and experiments are reproducible.

This versioning foundation enables more sophisticated capabilities.
Supervised learning pipelines, for instance, require consistent
annotation workflows. Labeling tools such as Label Studio
(\citeproc{ref-label_studio}{Borghaei et al. 2015}) support scalable,
team-based annotation with integrated audit trails and version
histories. These capabilities are essential in production settings,
where labeling conventions evolve over time or require refinement across
multiple iterations of a project.

Beyond annotation workflows, operational environments require data
storage supporting secure, scalable, and collaborative access.
Cloud-based object storage systems such as
\href{https://aws.amazon.com/s3/}{Amazon S3} and
\href{https://cloud.google.com/storage}{Google Cloud Storage} offer
durability and fine-grained access control for managing both raw and
processed data artifacts.

Building on this storage foundation, MLOps teams construct automated
data pipelines to transition from raw data to analysis- or
inference-ready formats. These pipelines perform structured tasks such
as data ingestion, schema validation, deduplication, transformation, and
loading. Orchestration tools including Apache Airflow
(\citeproc{ref-apache_airflow}{Cejudo et al. 2025}), Prefect
(\citeproc{ref-prefect}{Wang et al. 2024}), and dbt
(\citeproc{ref-dbt}{Naghavi et al. 2025}) are commonly used to define
and manage these workflows. When managed as code, pipelines support
versioning, modularity, and integration with CI/CD systems.

As these automated pipelines scale across organizations, they encounter
the challenge of feature management at scale. An increasingly important
element of modern data infrastructure is the \textbf{feature store}, a
concept pioneered by Uber's Michelangelo platform team in 2017. The team
coined the term after realizing that feature engineering was duplicated
across hundreds of ML models. Their solution, a centralized ``feature
store,'' became the template that inspired Feast, Tecton, and dozens of
other platforms.

Feature stores centralize engineered features for reuse across models
and teams.
Section~\ref{sec-machine-learning-operations-mlops-feature-stores-c01c}
details implementation patterns for training-serving consistency.

To illustrate these concepts in practice, consider a predictive
maintenance application in an industrial setting. A continuous stream of
sensor data is ingested and joined with historical maintenance logs
through a scheduled pipeline managed in Airflow. The resulting features,
including rolling averages and statistical aggregates, are stored in a
feature store for both retraining and low-latency inference. This
pipeline is versioned, monitored, and integrated with the model
registry, enabling full traceability from data to deployed model
predictions.

Data management establishes the operational backbone enabling model
reproducibility, auditability, and sustained deployment at scale, making
feature stores a critical infrastructure component.

\subsubsection{Feature
Stores}\label{sec-machine-learning-operations-mlops-feature-stores-c01c}

The data dependency debt and training-serving skew patterns described in
Section~\ref{sec-machine-learning-operations-mlops-technical-debt-system-complexity-2762}
share a common root cause: inconsistent feature computation across
pipeline stages. Feature stores\sidenote{\textbf{Feature Store Scale}:
Large-scale feature stores at companies like Uber and Airbnb serve
millions of features per second with P99 latencies under 10ms using
optimized, co-located serving infrastructure. These systems support
thousands of ML models with automated feature validation that
significantly reduces training-serving skew issues. } address this
challenge by providing an abstraction layer between data engineering and
machine learning, implementing Principle 3 (The Consistency Imperative)
through a single source of truth for feature values. In conventional
pipelines, feature engineering logic is duplicated or diverges across
environments, introducing risks of \textbf{Training-Serving Skew}, data
leakage, and model drift.

Feature stores manage both offline (batch) and online (real-time)
feature access through a centralized repository. During training,
features are computed and stored in a batch environment alongside
historical labels. At inference time, the same transformation logic is
applied to fresh data in an online serving system. This architecture
ensures models consume identical features in both contexts, a property
that becomes critical when deploying the optimized models discussed in
\textbf{?@sec-model-compression}.

Beyond consistency, feature stores support versioning, metadata
management, and feature reuse across teams. A fraud detection model and
a credit scoring model may rely on overlapping transaction features that
can be centrally maintained, validated, and shared. Integration with
data pipelines and model registries enables lineage tracking: when a
feature is updated or deprecated, dependent models are identified and
retrained accordingly.

\textbf{Training-Serving Skew: Diagnosis and Prevention.}
Training-serving skew occurs when the model sees different features
during inference than during training, causing silent accuracy
degradation. The model continues to produce predictions without errors,
but those predictions are simply less accurate.

Table~\ref{tbl-training-serving-skew} summarizes common causes of
training-serving skew:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2269}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3782}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3950}}@{}}
\caption{\textbf{Training-Serving Skew Categories.} Each category
requires different detection and prevention strategies. Schema and
preprocessing skew emerge from code divergence and require feature store
unification, while data distribution skew requires statistical
monitoring against training baselines. Timing skew demands careful
analysis of feature freshness between training and serving
contexts.}\label{tbl-training-serving-skew}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Skew Type}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Example}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Detection Method}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Skew Type}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Example}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Detection Method}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Feature preprocessing} & Normalization uses different statistics
& Statistical comparison of feature distributions \\
\textbf{Missing data handling} & Training fills NaN with mean; serving
uses 0 & Schema validation with explicit null handling \\
\textbf{Time-dependent features} & Features computed with different time
cutoffs & Timestamp validation in feature pipelines \\
\textbf{Library version drift} & NumPy or Pandas version differences &
Environment hash comparison \\
\end{longtable}

\textbf{Training-Serving Skew Case Study.} A practical example
illustrates how training-serving skew manifests in production systems.
Consider a recommendation system that shows 8\% accuracy degradation one
month after deployment with no code changes. Feature distribution
comparison reveals that \texttt{user\_session\_length} has a mean of 45
minutes in serving versus 12 minutes in training. The root cause is that
training data excluded mobile sessions, which are typically shorter,
while serving data includes all sessions. As a result, the model learned
patterns specific to desktop users that fail for mobile users.

Feature stores address this problem by computing features once and
serving them consistently to both training and serving pipelines.
Listing~\ref{lst-feature-store-consistency} demonstrates how Feast
enables unified feature retrieval for both historical training data and
online serving, eliminating the divergent code paths that cause skew.

\begin{codelisting}

\caption{\label{lst-feature-store-consistency}\textbf{Feature Store
Consistency}: Unified feature retrieval eliminates training-serving skew
by ensuring both pipelines access identical feature computations,
reducing accuracy degradation by 5-15\% in production systems.}

\centering{

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ feast }\ImportTok{import}\NormalTok{ FeatureStore}

\NormalTok{fs }\OperatorTok{=}\NormalTok{ FeatureStore(}
\NormalTok{    repo\_path}\OperatorTok{=}\StringTok{"."}
\NormalTok{)  }\CommentTok{\# Initialize feature store connection}

\CommentTok{\# Training: pull historical features with point{-}in{-}time correctness}
\NormalTok{training\_df }\OperatorTok{=}\NormalTok{ fs.get\_historical\_features(}
\NormalTok{    entity\_df}\OperatorTok{=}\NormalTok{training\_entities,  }\CommentTok{\# Contains entity keys and timestamps}
\NormalTok{    features}\OperatorTok{=}\NormalTok{[}\StringTok{"user:session\_length"}\NormalTok{, }\StringTok{"user:purchase\_history"}\NormalTok{],}
\NormalTok{).to\_df()}

\CommentTok{\# Serving: pull online features using same feature definitions}
\CommentTok{\# Guarantees identical computation logic as training}
\NormalTok{online\_features }\OperatorTok{=}\NormalTok{ fs.get\_online\_features(}
\NormalTok{    entity\_rows}\OperatorTok{=}\NormalTok{[\{}\StringTok{"user\_id"}\NormalTok{: }\DecValTok{12345}\NormalTok{\}],}
\NormalTok{    features}\OperatorTok{=}\NormalTok{[}\StringTok{"user:session\_length"}\NormalTok{, }\StringTok{"user:purchase\_history"}\NormalTok{],}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

}

\end{codelisting}%

By computing \texttt{session\_length} once in the feature pipeline,
training and serving see identical values. Organizations report
significant accuracy improvements after eliminating skew through
centralized feature stores, with documented cases showing improvements
ranging from single-digit to double-digit percentages depending on the
severity of the original skew.

The consistency guarantee can be quantified economically. For a model
serving one million daily predictions, even a 1\% skew-induced error
rate at \$0.10 per error represents \$36,500 in annual cost. Feature
stores transform this continuous leakage into a one-time infrastructure
investment with measurable returns. The Uber case study below
demonstrates these economics at scale.

\phantomsection\label{callout-lighthouseux2a-1.8}
\begin{fbxSimple}{callout-lighthouse}{Lighthouse:}{Uber Michelangelo Feature Store}
\phantomsection\label{callout-lighthouse*-1.8}
Uber's Michelangelo platform pioneered the feature store concept in 2017
(\citeproc{ref-hermann2017meet}{Hermann and Balso 2017}), addressing
training-serving skew across thousands of ML models powering ride
pricing, ETA prediction, and fraud detection.

\textbf{The Problem}: Data scientists computed features in Spark for
training, while engineers reimplemented the same logic in Java for
serving. Feature definitions diverged, contributing to a significant
percentage of production incidents.

\textbf{The Solution}: Michelangelo's feature store computes features
once and serves them to both training (via Hive) and production (via
Cassandra). Feature definitions are written in DSL, automatically
generating both batch and online implementations.

\textbf{Key Design Decisions}:

\begin{itemize}
\tightlist
\item
  Point-in-time correctness for historical features prevents data
  leakage
\item
  Feature versioning enables safe iteration without breaking dependent
  models
\item
  Centralized feature catalog enables discovery and reuse across 5,000+
  features
\end{itemize}

\textbf{Results}: Significant reduction in feature engineering time,
near-elimination of skew-related incidents, and standardized feature
quality across 100+ ML teams.

\emph{Reference: Hermann \& Del Balso
(\citeproc{ref-uber2017michelangelo}{Hermann and Del Balso 2017})}

\end{fbxSimple}

\textbf{Skew Detection in CI/CD}:

Automated pipelines should validate feature consistency before
deployment. Listing~\ref{lst-ops-validate-skew} shows a function that
compares training and serving feature distributions using the
Kolmogorov-Smirnov test, rejecting deployment when any feature diverges
beyond a threshold.

\begin{codelisting}

\caption{\label{lst-ops-validate-skew}\textbf{Feature Skew Validation}:
This function compares training and serving feature distributions using
the Kolmogorov-Smirnov test, rejecting deployment when any feature
diverges beyond a configurable threshold.}

\centering{

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ validate\_no\_skew(}
\NormalTok{    training\_features, serving\_features, threshold}\OperatorTok{=}\FloatTok{0.1}
\NormalTok{):}
    \CommentTok{"""Reject deployment if feature distributions diverge."""}
    \ControlFlowTok{for}\NormalTok{ feature }\KeywordTok{in}\NormalTok{ training\_features.columns:}
\NormalTok{        ks\_stat }\OperatorTok{=}\NormalTok{ ks\_2samp(}
\NormalTok{            training\_features[feature], serving\_features[feature]}
\NormalTok{        )}
        \ControlFlowTok{if}\NormalTok{ ks\_stat.statistic }\OperatorTok{\textgreater{}}\NormalTok{ threshold:}
            \ControlFlowTok{raise}\NormalTok{ SkewDetectedError(}
                \SpecialStringTok{f"}\SpecialCharTok{\{}\NormalTok{feature}\SpecialCharTok{\}}\SpecialStringTok{: KS=}\SpecialCharTok{\{}\NormalTok{ks\_stat}\SpecialCharTok{.}\NormalTok{statistic}\SpecialCharTok{:.3f\}}\SpecialStringTok{"}
\NormalTok{            )}
\end{Highlighting}
\end{Shaded}

}

\end{codelisting}%

\subsubsection{Versioning and
Lineage}\label{sec-machine-learning-operations-mlops-versioning-lineage-b1cf}

Versioning implements \textbf{Principle 1: Reproducibility Through
Versioning}
(Section~\ref{sec-machine-learning-operations-mlops-foundational-principles-44e6}),
which requires all artifacts influencing model behavior to be versioned.
Unlike traditional software, ML models depend on multiple changing
artifacts: training data, feature engineering logic, trained model
parameters, and configuration settings. MLOps practices enforce tracking
of versions across all pipeline components to manage this complexity.

Data versioning allows teams to snapshot datasets at specific points in
time and associate them with particular model runs, including both raw
data and processed artifacts. Model versioning registers trained models
as immutable artifacts alongside metadata such as training parameters,
evaluation metrics, and environment specifications. Model registries
provide structured interfaces for promoting, deploying, and rolling back
model versions, with some supporting lineage visualization tracing the
full dependency graph from raw data to deployed prediction.

These complementary practices form the lineage layer of an ML system.
This layer enables introspection, experimentation, and governance. When
a deployed model underperforms, lineage tools help teams answer
questions such as:

\begin{itemize}
\tightlist
\item
  Was the input distribution consistent with training data?
\item
  Did the feature definitions change?
\item
  Is the model version aligned with the serving infrastructure?
\end{itemize}

By elevating versioning and lineage to first-class citizens in the
system design, MLOps enables teams to build and maintain reliable,
auditable, and evolvable ML workflows at scale.

\subsection{Continuous Pipelines and
Automation}\label{sec-machine-learning-operations-mlops-continuous-pipelines-automation-27a4}

The feature stores and versioning systems examined above address data
consistency statically. Automation enables these systems to evolve
continuously in response to new data, shifting objectives, and
operational constraints. Rather than treating development and deployment
as isolated phases, automated pipelines synchronize workflows that
integrate data preprocessing, training, evaluation, and release.

\subsubsection{CI/CD
Pipelines}\label{sec-machine-learning-operations-mlops-cicd-pipelines-a9de}

Feature stores and versioning systems address the \emph{data} side of
the consistency problem; CI/CD pipelines address the \emph{process}
side, ensuring that changes flow through validated stages rather than ad
hoc deployments. While conventional software systems rely on CI/CD
pipelines for efficient code testing and deployment, machine learning
systems require significant adaptations. ML CI/CD pipelines must handle
additional complexity from data dependencies, model training workflows,
and artifact versioning.

A typical ML CI/CD pipeline consists of coordinated stages: checking out
updated code, preprocessing input data, training a candidate model,
validating performance, packaging the model, and deploying to a serving
environment. In some cases, pipelines also include triggers for
automatic retraining based on data drift or performance degradation. By
codifying these steps, CI/CD pipelines\sidenote{\textbf{Idempotency in
ML Systems}: Property where repeated operations produce identical
results, crucial for reliable MLOps pipelines. Unlike traditional
software where rerunning deployments is often identical, ML training
introduces randomness through data shuffling, weight initialization, and
hardware variations. Production MLOps aims for idempotency through fixed
random seeds, deterministic data ordering, and consistent compute
environments. Without idempotency, debugging becomes difficult when
pipeline reruns produce different model artifacts. } reduce manual
intervention, enforce quality checks, and support continuous improvement
of deployed systems.

To support these complex workflows, a wide range of tools is available
for implementing ML-focused CI/CD workflows. General-purpose CI/CD
orchestrators such as Jenkins (\citeproc{ref-jenkins}{Soni, Gallivan,
and Jenkins 1999}), CircleCI (\citeproc{ref-circleci}{Denecker et al.
2019}), and GitHub Actions (\citeproc{ref-github_actions}{Jin, Plikus,
and Nie 2024})\sidenote{\textbf{GitHub Actions for ML}: GitHub Actions
has become a dominant CI/CD choice for ML teams, with typical ML
pipelines taking 15-45 minutes to run (vs.~2-5 minutes for traditional
software). Large streaming services reportedly run thousands of ML
pipeline executions weekly, with high first-run success rates when
pipelines are properly configured. } manage version control events and
execution logic. These tools integrate with domain-specific platforms
such as Kubeflow (\citeproc{ref-kubeflow}{Authors
2024})\sidenote{\textbf{Kubeflow Production Scale}: Large technology
companies run hundreds of thousands of ML jobs monthly across many
clusters, with auto-scaling providing significant cost reductions.
Kubeflow's pipeline abstraction enables reproducible experiments while
Katib handles hyperparameter optimization at scale. Organizations like
Spotify have reported orchestrating thousands of concurrent training
jobs with fault tolerance. }, Metaflow (\citeproc{ref-metaflow}{Netflix
2024}), and Prefect (\citeproc{ref-prefect}{Wang et al. 2024}), which
offer higher-level abstractions for managing ML tasks and workflows.

Figure~\ref{fig-ops-cicd} illustrates a representative CI/CD pipeline
for machine learning systems, beginning with a dataset and feature
repository, from which data is ingested and validated. Validated data is
then transformed for model training. A retraining trigger, such as a
scheduled job or performance threshold, initiates this process
automatically. Once training and hyperparameter tuning are complete, the
resulting model undergoes evaluation against predefined criteria. If the
model satisfies the required thresholds, it is registered in a model
repository along with metadata, performance metrics, and lineage
information. Finally, the model is deployed back into the production
system, closing the loop and enabling continuous delivery of updated
models.

\begin{figure}[htb]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/8081f18b9d902db6dc0e4f3168bf65b20e66308a.pdf}}

}

\caption{\label{fig-ops-cicd}\textbf{ML CI/CD Pipeline.} The pipeline
begins with dataset and feature repositories, flows through data
validation, transformation, training, evaluation, and model registration
stages, then deploys to production. Retraining triggers initiate the
cycle automatically, while metadata and artifact repositories ensure
reproducibility and governance. Source: HarvardX.}

\end{figure}%

To illustrate these concepts in practice, consider an image
classification model under active development. When a data scientist
commits changes to a GitHub (\citeproc{ref-github}{Lima et al. 2023})
repository, a Jenkins pipeline is triggered. The pipeline fetches the
latest data, performs preprocessing, and initiates model training.
Experiments are tracked using MLflow
(\citeproc{ref-mlflow_website}{NOGARE et al. 2025}) which logs metrics
and stores model artifacts. After passing automated evaluation tests,
the model is containerized and deployed to a staging environment using
Kubernetes (\citeproc{ref-kubernetes}{Rahman, Dozier, and Zhang 2025}).
If the model meets validation criteria in staging, the pipeline
orchestrates controlled deployment strategies such as canary testing
(detailed in
Section~\ref{sec-machine-learning-operations-mlops-model-validation-a891}),
gradually routing production traffic to the new model while monitoring
key metrics for anomalies. In case of performance regressions, the
system can automatically revert to a previous model version.

CI/CD pipelines play a central role in enabling scalable, repeatable,
and safe ML deployment. In mature MLOps environments, CI/CD is not
optional but foundational, transforming ad hoc experimentation into
structured, operationally sound development. Google's \emph{TFX
(TensorFlow Extended)} platform exemplifies how these CI/CD principles
scale to production.

\phantomsection\label{callout-lighthouseux2a-1.9}
\begin{fbxSimple}{callout-lighthouse}{Lighthouse:}{Google TFX Production ML Pipelines}
\phantomsection\label{callout-lighthouse*-1.9}
TensorFlow Extended (TFX) emerged from Google's internal ML
infrastructure, productionizing the same pipeline patterns that power
Search, Ads, and YouTube recommendations.

\textbf{The Origin}: Before TFX, Google teams built bespoke pipelines
for each ML project. Common problems (data validation, schema
enforcement, model validation) were solved repeatedly with inconsistent
approaches.

\textbf{Core Components}:

\begin{itemize}
\tightlist
\item
  \textbf{ExampleGen}: Ingests and splits data with consistent shuffling
\item
  \textbf{StatisticsGen + SchemaGen}: Automatically infer data
  statistics and schema
\item
  \textbf{ExampleValidator}: Detects training-serving skew and data
  anomalies
\item
  \textbf{Transform}: Consistent feature engineering for training and
  serving
\item
  \textbf{Trainer}: Standardized model training with hyperparameter
  support
\item
  \textbf{Evaluator}: Model validation against baseline with sliced
  metrics
\item
  \textbf{Pusher}: Conditional deployment based on validation gates
\end{itemize}

\textbf{Key Insight}: TFX enforces that every pipeline step produces
artifacts with metadata, enabling full lineage tracking from raw data to
deployed model. When a production issue occurs, engineers trace back
through the exact data, code, and configuration that produced the
problematic model.

\textbf{Impact}: Open-sourced in 2019, TFX patterns influenced Kubeflow
Pipelines, MLflow, and Vertex AI Pipelines. The ``transform once, serve
everywhere'' pattern became industry standard for eliminating
training-serving skew.

\emph{Reference: Baylor et al. (\citeproc{ref-baylor2017tfx}{Baylor et
al. 2017})}

\end{fbxSimple}

\subsubsection{Training
Pipelines}\label{sec-machine-learning-operations-mlops-training-pipelines-2dcf}

CI/CD pipelines orchestrate the overall workflow, but training itself
requires specialized infrastructure. Model training, where algorithms
are optimized to learn patterns from data, builds on the distributed
training concepts covered in \textbf{?@sec-ai-training}. Within MLOps,
training activities become part of a reproducible, scalable, and
automated pipeline supporting continual experimentation and reliable
production deployment.

Modern machine learning frameworks such as TensorFlow
(\citeproc{ref-tensorflow}{Coy et al. 2019a}), PyTorch
(\citeproc{ref-pytorch}{Taylor and Kriegeskorte 2023}), and Keras
(\citeproc{ref-keras}{Chollet et al. 2024}) provide modular components
for building and training models. The framework selection principles
from \textbf{?@sec-ai-frameworks} become essential for production
training pipelines requiring reliable scaling.

Reproducibility is a key objective. Training scripts and configurations
are version-controlled using tools like Git (\citeproc{ref-git}{Torvalds
and Hamano 2024}) and hosted on platforms such as GitHub
(\citeproc{ref-github}{Lima et al. 2023}). Interactive development
environments, including Jupyter (\citeproc{ref-jupyter}{Dombrowski,
Gniady, and Kloster 2020}) notebooks, encapsulate data ingestion,
feature engineering, training routines, and evaluation logic in a
unified format. These notebooks integrate into automated pipelines,
allowing the same logic used for local experimentation to be reused for
scheduled retraining in production systems.

\paragraph{Notebooks in
Production}\label{sec-machine-learning-operations-mlops-notebooks-production-7016}

While notebooks excel for exploration and prototyping, using them
directly in production pipelines introduces operational risks that
require mitigation. These considerations are essential for teams
transitioning from experimental workflows to production systems.

Reproducibility presents the first challenge. Notebook cells can be
executed out of order, creating hidden state dependencies that make
results non-reproducible. A common failure mode occurs when a data
scientist runs cells 1, 3, 2 during development and the resulting model
works, but a production pipeline running cells 1, 2, 3 fails.

Testing difficulties compound this challenge. Traditional unit testing
frameworks do not integrate naturally with notebook structure.
Cell-level testing is possible but rarely practiced, leaving notebooks
less tested than equivalent Python modules.

Several mitigation strategies address these operational concerns.
Papermill enables parameterization and programmatic execution of
notebooks, treating them as configurable pipeline stages. The nbconvert
tool converts validated notebooks to Python scripts for production
execution. Cell execution order enforcement tools execute all cells
top-to-bottom, rejecting out-of-order dependencies.

The recommended practice is to use notebooks for exploration and rapid
iteration, then refactor validated logic into tested Python modules for
production pipelines. The overhead of refactoring pays off in
maintainability and reliability. Failing to robustify these pipelines
can lead to silent failures with massive economic consequences. The
following analysis quantifies \emph{the cost of silent failures}.

\phantomsection\label{callout-notebookux2a-1.10}
\begin{fbxSimple}{callout-notebook}{AI Engineer’s Notebook:}{The Cost of Silent Failures}
\phantomsection\label{callout-notebook*-1.10}
\textbf{Problem}: Is building an automated drift detection system worth
the engineering effort?

\textbf{Scenario}: You run a product recommendation engine generating
\textbf{\$100M/year} in revenue. \textbf{The Failure}: A deployment bug
causes \textbf{Training-Serving Skew}, dropping recommendation quality
by \textbf{2\%}. This degrades conversion rate proportionally.

\textbf{Cost Analysis}:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Manual Ops (Monthly Review)}:

  \begin{itemize}
  \tightlist
  \item
    Detection Time: \textasciitilde4 weeks (28 days).
  \item
    Revenue Loss:
    \(\$100\text{M} \times 0.02 \times \frac{28}{365} \approx \mathbf{\$153\,425}\).
  \end{itemize}
\item
  \textbf{Automated MLOps (Daily Checks)}:

  \begin{itemize}
  \tightlist
  \item
    Detection Time: 1 day.
  \item
    Revenue Loss:
    \(\$100\text{M} \times 0.02 \times \frac{1}{365} \approx \mathbf{\$5\,479}\).
  \end{itemize}
\end{enumerate}

\textbf{The Systems Conclusion}: A single silent failure costs
\textbf{\$147,945} more without MLOps. If you have 5 such incidents a
year, MLOps saves nearly \textbf{\$739,726}. The ``expensive''
infrastructure pays for itself by reducing the \textbf{Time-to-Detection
(TTD)} for the silent failures defined in the \textbf{Verification Gap}.

\end{fbxSimple}

Beyond reproducibility, automation enhances model training by reducing
manual effort and standardizing critical steps. MLOps workflows
incorporate techniques such as hyperparameter tuning
(\citeproc{ref-hyperparameter_tuning_website}{Ranjit et al. 2019}),
neural architecture search
(\citeproc{ref-neural_architecture_search_paper}{Elsken, Metzen, and
Hutter 2018}), and automatic feature selection
(\citeproc{ref-scikit_learn_feature_selection}{Blum et al. 2021}) to
explore the design space efficiently. These tasks are orchestrated using
CI/CD pipelines, which automate data preprocessing, model training,
evaluation, registration, and deployment. For instance, a Jenkins
pipeline triggers a retraining job when new labeled data becomes
available. The resulting model is evaluated against baseline metrics,
and if performance thresholds are met, it is deployed automatically.

The increasing availability of cloud-based infrastructure has expanded
the reach of model training. This connects to the workflow orchestration
patterns explored in \textbf{?@sec-ai-development-workflow}, which
provide the foundation for managing complex, multi-stage training
processes across distributed systems. Cloud providers offer managed
services that provision high-performance computing resources, including
GPU and TPU accelerators, on demand\sidenote{\textbf{Cloud ML Training
Economics}: Training GPT-3 was estimated to cost at least \$4.6 million
according to Lambda Labs calculations based on V100 GPU-hours; actual
costs would likely be higher due to distributed training overhead.
Official training costs were not disclosed by OpenAI, while fine-tuning
typically costs \$100-\$10,000. TPU v4 offers performance-per-dollar
improvements ranging from 1.2-1.7\(\times\) over A100 GPUs for general
workloads, with some large-scale language model training achieving
greater savings. Organizations report 60-90\% cost savings through spot
instances and preemptible VMs compared to on-demand pricing. }.
Depending on the platform, teams construct their own training workflows
or rely on fully managed services such as Vertex AI Fine Tuning
(\citeproc{ref-vertex_ai_fine_tuning}{Cloud 2024a}), which support
automated adaptation of foundation models to new tasks. Hardware
availability, regional access restrictions, and cost constraints remain
important considerations when designing cloud-based training systems.

To illustrate these integrated practices, consider a data scientist
developing a neural network for image classification using a PyTorch
notebook. The fastai (\citeproc{ref-fastai}{Howard and Gugger 2024})
library is used to simplify model construction and training. The
notebook trains the model on a labeled dataset, computes performance
metrics, and tunes model configuration parameters. Once validated, the
training script is version-controlled and incorporated into a retraining
pipeline that is periodically triggered based on data updates or model
performance monitoring.

Through standardized workflows, versioned environments, and automated
orchestration, MLOps transitions model training from ad hoc
experimentation to robust, repeatable systems meeting production
standards for reliability, traceability, and performance.

\paragraph{Retraining Decision
Framework}\label{sec-machine-learning-operations-mlops-retraining-decision-framework-e33d}

Automated training pipelines raise a critical question: \emph{how often}
should they run? Deciding when to retrain a model requires balancing
accuracy maintenance against computational costs. Three common
strategies exist, each with distinct tradeoffs.
Table~\ref{tbl-retraining-schedules} provides typical schedules across
domains, from daily retraining for rapidly shifting ad click prediction
to quarterly updates for stable medical imaging applications:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2949}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2564}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4487}}@{}}
\caption{\textbf{Typical Retraining Schedules by Domain.} These
represent starting points; teams should calibrate based on observed
drift rates and business
impact.}\label{tbl-retraining-schedules}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Domain}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Typical Schedule}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Rationale}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Domain}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Typical Schedule}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Rationale}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Ad click prediction} & Daily & User interests shift rapidly \\
\textbf{Fraud detection} & Weekly & Attack patterns evolve
continuously \\
\textbf{Demand forecasting} & Monthly & Seasonal patterns change
slowly \\
\textbf{Medical imaging} & Quarterly & Disease presentations are
stable \\
\end{longtable}

\textbf{Scheduled Retraining}

Retrain on a fixed schedule (daily, weekly, monthly) regardless of
performance metrics. This approach is simple to implement and ensures
models incorporate recent data. However, it may retrain unnecessarily
when data is stable or fail to retrain quickly enough during rapid
distribution shifts.

\textbf{Triggered Retraining}

Retrain when monitoring detects performance degradation or drift beyond
thresholds. This optimizes compute costs by retraining only when
necessary but requires robust monitoring infrastructure and careful
threshold calibration to avoid false positives or missed degradation.
Listing~\ref{lst-ops-triggered-retraining} illustrates a configuration
that initiates retraining based on accuracy drops, feature drift, or
prediction distribution shifts.

\begin{codelisting}

\caption{\label{lst-ops-triggered-retraining}\textbf{Triggered
Retraining Configuration}: This example defines three trigger conditions
for automatic retraining---accuracy degradation, feature drift measured
by PSI, and prediction distribution shift---each with configurable
thresholds and time windows.}

\centering{

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Example triggered retraining configuration}
\FunctionTok{triggers}\KeywordTok{:}
\AttributeTok{  }\KeywordTok{{-}}\AttributeTok{ }\FunctionTok{metric}\KeywordTok{:}\AttributeTok{ accuracy}
\AttributeTok{    }\FunctionTok{threshold}\KeywordTok{:}\AttributeTok{ }\FloatTok{0.05}\CommentTok{  \# 5\% accuracy drop}
\AttributeTok{    }\FunctionTok{window}\KeywordTok{:}\AttributeTok{ 7d}
\AttributeTok{  }\KeywordTok{{-}}\AttributeTok{ }\FunctionTok{metric}\KeywordTok{:}\AttributeTok{ feature\_drift\_psi}
\AttributeTok{    }\FunctionTok{threshold}\KeywordTok{:}\AttributeTok{ }\FloatTok{0.2}
\AttributeTok{    }\FunctionTok{features}\KeywordTok{:}\AttributeTok{ }\KeywordTok{[}\AttributeTok{user\_age\_bucket}\KeywordTok{,}\AttributeTok{ purchase\_amount\_bin}\KeywordTok{]}
\AttributeTok{  }\KeywordTok{{-}}\AttributeTok{ }\FunctionTok{metric}\KeywordTok{:}\AttributeTok{ prediction\_distribution\_shift}
\AttributeTok{    }\FunctionTok{threshold}\KeywordTok{:}\AttributeTok{ }\FloatTok{0.1}
\AttributeTok{    }\FunctionTok{window}\KeywordTok{:}\AttributeTok{ 24h}
\end{Highlighting}
\end{Shaded}

}

\end{codelisting}%

\textbf{Continuous Retraining}

Incrementally update models as new labeled data arrives using online
learning or periodic micro-updates. This keeps models current with
minimal latency but requires careful validation to prevent model
degradation from noisy labels or adversarial data.

\textbf{Retraining Decision Factors}:

\begin{itemize}
\tightlist
\item
  \textbf{Compute cost}: Large models may cost tens of thousands of
  dollars to retrain
\item
  \textbf{Validation infrastructure}: Sufficient testing to ensure new
  model outperforms baseline
\item
  \textbf{Rollback capability}: Ability to revert if new model degrades
\item
  \textbf{Label availability}: Triggered retraining requires ground
  truth labels to detect degradation
\end{itemize}

The choice among these strategies depends on domain characteristics:
scheduled retraining suits stable domains, triggered retraining
addresses gradual drift, and continuous retraining handles rapidly
evolving data distributions.

\paragraph{Quantitative Retraining
Economics}\label{sec-machine-learning-operations-mlops-quantitative-retraining-economics-1579}

The decision to retrain a model is not a matter of intuition but an
engineering optimization that balances the cost of \textbf{System
Entropy}\sidenote{\textbf{Entropy}: From Greek \emph{entropia} (turning
toward), coined by Clausius in 1865 for thermodynamics. Originally
quantifying how energy disperses and becomes unavailable for work,
Shannon borrowed the term in 1948 for information theory, measuring
uncertainty in messages. In ML operations, ``system entropy'' extends
the metaphor: like heat dissipating into the environment, model accuracy
gradually disperses as the gap between training and production data
widens. } (accuracy decay) against the cost of \textbf{Infrastructure}
(retraining expense). In production, a model behaves like a radioactive
isotope: it has a measurable \textbf{Half-Life} after which its
predictive value becomes toxic to the business.

The following calculation demonstrates how to determine optimal
retraining frequency by estimating the \emph{half-life of a model}.

\phantomsection\label{callout-notebookux2a-1.11}
\begin{fbxSimple}{callout-notebook}{AI Engineer’s Notebook:}{The Half-Life of a Model}
\phantomsection\label{callout-notebook*-1.11}
\textbf{The Problem}: How often should you retrain your model to
maximize profit?

\textbf{The Physics}: Model accuracy \(A(t)\) decays at rate \(\lambda\)
due to \textbf{Data Drift}.

\begin{itemize}
\tightlist
\item
  \(Q\): Daily Query Volume (Traffic).
\item
  \(V\): Financial Value of 1\% Accuracy (Utility).
\item
  \(C\): Fixed Cost of a Retraining Run (Compute + Ops).
\end{itemize}

\textbf{The Equation}: The optimal retraining interval (\(T^*\))
minimizes the sum of staleness losses and training costs:
\[ T^* \approx \sqrt{\frac{2 \cdot C}{Q \cdot V \cdot A_0 \cdot \lambda}} \]

\textbf{The Calculation}: Consider a \textbf{Lighthouse Fraud Model}
(\(A_0=0.95\)):

\begin{itemize}
\tightlist
\item
  \textbf{Traffic (\(Q\))}: 1 Million transactions/day.
\item
  \textbf{Utility (\(V\))}: \$0.50 per accuracy point.
\item
  \textbf{Retraining Cost (\(C\))}: \$5,000.
\item
  \textbf{Drift Rate (\(\lambda\))}: 2\% per day.
\end{itemize}

\[ T^* \approx \sqrt{\frac{2 \times 5000}{1,000,000 \times 0.50 \times 0.95 \times 0.02}} \approx \mathbf{1 \text{ Day}} \]

\textbf{The Systems Conclusion}: If your traffic is high and your
accuracy is valuable, you cannot afford to wait. You must automate the
pipeline. If \(T^*\) is less than your manual deployment time, your
system is in a state of \textbf{Permanent Technical Debt}.

\end{fbxSimple}

The following framework formalizes the derivation used in the
calculation above, providing the tools to calibrate your monitoring
thresholds based on measurable business impact. This quantitative
framework transforms retraining from an ad hoc decision into an
engineering optimization, implementing \textbf{Principle 5: Cost-Aware
Automation}
(Section~\ref{sec-machine-learning-operations-mlops-foundational-principles-44e6}).

\textbf{The Staleness Cost Function}

Model accuracy typically degrades over time due to distribution drift.
Let \(A(t)\) represent accuracy at time \(t\) since last training, and
\(A_0\) represent initial accuracy. The degradation rate \(\lambda\)
depends on domain volatility:

\[A(t) = A_0 \cdot e^{-\lambda t}\]

The cost of staleness accumulates based on query volume \(Q\) per time
period and the value impact \(V\) of each accuracy point:

\[\text{Staleness Cost}(T) = \int_0^T Q \cdot V \cdot (A_0 - A(t)) \, dt = Q \cdot V \cdot A_0 \cdot \left(T - \frac{1-e^{-\lambda T}}{\lambda}\right)\]

\subparagraph*{The Retraining Cost
Function}\label{the-retraining-cost-function}
\addcontentsline{toc}{subparagraph}{The Retraining Cost Function}

Each retraining incurs fixed costs including compute, validation, and
deployment overhead:

\[\text{Retraining Cost} = C_{\text{compute}} + C_{\text{validation}} + C_{\text{deployment}} + C_{\text{risk}}\]

where \(C_{\text{risk}}\) represents the expected cost of potential
regression from the new model.

\subparagraph*{Optimal Retraining
Interval}\label{optimal-retraining-interval}
\addcontentsline{toc}{subparagraph}{Optimal Retraining Interval}

The optimal retraining interval \(T^*\) minimizes total cost per unit
time:

\[T^* = \arg\min_T \frac{\text{Staleness Cost}(T) + \text{Retraining Cost}}{T}\]

For exponential decay, this yields the square-root law used in our
Napkin Math calculation above. A \emph{fraud detection retraining}
scenario demonstrates how these formulas guide real scheduling
decisions.

\phantomsection\label{callout-exampleux2a-1.12}
\begin{fbxSimple}{callout-example}{Example:}{Fraud Detection Retraining}
\phantomsection\label{callout-example*-1.12}
Consider a fraud detection model with the parameters in
Table~\ref{tbl-retraining-parameters} that captures the high query
volume and rapid drift rate characteristic of financial fraud detection:

\begin{longtable}[]{@{}lrl@{}}
\caption{\textbf{Retraining Decision Parameters.} Example values for a
fraud detection system processing 50,000 queries daily. The 5\% per week
drift rate reflects observed fraud evolution in financial services,
while the \$50 per point accuracy value captures the cost of missed
fraud cases. Actual parameters vary by domain and should be calibrated
from production
observations.}\label{tbl-retraining-parameters}\tabularnewline
\toprule\noalign{}
\textbf{Parameter} & \textbf{Value} & \textbf{Description} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\textbf{Parameter} & \textbf{Value} & \textbf{Description} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{\(Q\)} & 1,000,000 & Transactions per day \\
\textbf{\(V\)} & \$0.50 & Value per accuracy point \\
\textbf{\(A_0\)} & 0.95 & Initial accuracy \\
\textbf{\(\lambda\)} & 0.02 & Daily decay rate (2\% per day) \\
\textbf{Retraining Cost} & \$5,000 & Total retraining expense \\
\end{longtable}

\textbf{Applying the formula:}

\[T^* \approx \sqrt{\frac{2 \times 5\,000}{1\,000\,000 \times 0\.5 \times 0\.95 \times 0\.02}} \approx \sqrt{\frac{10\,000}{9\,500}} \approx 1\.03 \text{ days}\]

This analysis suggests daily retraining is economically optimal for this
high-volume, high-stakes fraud detection scenario.

\end{fbxSimple}

\subparagraph*{Sensitivity Analysis}\label{sensitivity-analysis}
\addcontentsline{toc}{subparagraph}{Sensitivity Analysis}

Table~\ref{tbl-retraining-sensitivity} shows how the optimal interval
scales with the square root of costs and inversely with the square root
of value and decay rate:

\begin{longtable}[]{@{}lr@{}}
\caption{\textbf{Retraining Interval Sensitivity.} How parameter changes
affect optimal retraining frequency. Doubling query volume halves the
optimal interval because degradation costs scale linearly with traffic.
Halving retraining costs similarly reduces the interval, while lower
drift rates extend it. Systems with high traffic and high per-query
value benefit most from frequent retraining
automation.}\label{tbl-retraining-sensitivity}\tabularnewline
\toprule\noalign{}
\textbf{Change} & \textbf{Effect on \(T^*\)} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\textbf{Change} & \textbf{Effect on \(T^*\)} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{4x retraining cost} & 2x longer interval \\
\textbf{4x query volume} & 2x shorter interval \\
\textbf{4x decay rate} & 2x shorter interval \\
\end{longtable}

\subparagraph*{Model Limitations}\label{model-limitations}
\addcontentsline{toc}{subparagraph}{Model Limitations}

This framework provides a first-order approximation that enables
principled decision-making, but practitioners should be aware of its
assumptions:

\begin{itemize}
\tightlist
\item
  \textbf{Predictable drift}: The exponential decay model assumes drift
  occurs gradually at a known rate. Sudden distribution shifts (concept
  drift) require different detection and response mechanisms.
\item
  \textbf{Known value function}: The model assumes each accuracy point
  has a quantifiable business value. In practice, this value may be
  nonlinear or context-dependent.
\item
  \textbf{Independent retraining cycles}: The model treats each
  retraining decision independently, ignoring potential benefits from
  continuous learning or transfer across retraining cycles.
\item
  \textbf{Linear cost scaling}: Retraining costs are assumed fixed. In
  practice, infrastructure costs may vary with compute availability and
  pricing dynamics.
\end{itemize}

Despite these limitations, the framework provides a principled starting
point for retraining decisions. Teams should calibrate parameters using
historical data and refine the model as they accumulate operational
experience. By making cost-benefit tradeoffs explicit and quantifiable,
this framework implements \textbf{Principle 5: Cost-Aware Automation}
(Section~\ref{sec-machine-learning-operations-mlops-foundational-principles-44e6}),
enabling teams to justify infrastructure investments and calibrate
monitoring thresholds based on measurable business impact.

\subsubsection{Model
Validation}\label{sec-machine-learning-operations-mlops-model-validation-a891}

Training pipelines produce model candidates; validation determines which
candidates merit production deployment. Before deployment, a machine
learning model must undergo rigorous evaluation to ensure it meets
predefined performance, robustness, and reliability criteria. MLOps
reframes evaluation as a structured, repeatable process for validating
operational readiness, incorporating pre-deployment assessment,
post-deployment monitoring, and automated regression testing.

The evaluation process begins with performance testing against a holdout
test set sampled from the same distribution as production data. Core
metrics such as accuracy, AUC, precision, recall, and F1 score
(\citeproc{ref-rainio2024evaluation}{Rainio, Teuho, and Klén 2024}) are
computed and tracked longitudinally to detect degradation from data
drift (\citeproc{ref-ibm_data_drift}{Rı́os et al. 2019}).
Figure~\ref{fig-data-drift} demonstrates this degradation pattern,
showing how changes in feature distributions correlate with declining
model quality.

\begin{figure}[htb]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/01063bfc40c1f17c8aab2ad46a42d6ebf2fc7c79.pdf}}

}

\caption{\label{fig-data-drift}\textbf{Data Drift Impact}: Declining
model performance over time results from data drift, where the
characteristics of production data diverge from the training dataset.
Monitoring key metrics longitudinally allows MLOps engineers to detect
this drift and trigger model retraining or data pipeline adjustments to
maintain accuracy.}

\end{figure}%

Beyond static evaluation, MLOps encourages controlled deployment
strategies that simulate production conditions while minimizing risk.
One widely adopted method is
\href{https://martinfowler.com/bliki/CanaryRelease.html}{canary
testing}, in which a new model is deployed to a small fraction of users
or queries. During this limited rollout, live performance metrics are
monitored to assess system stability and user impact. For instance, an
e-commerce platform deploys a new recommendation model to 5\% of web
traffic and observes metrics such as click-through rate, latency, and
prediction accuracy. Only after the model demonstrates consistent and
reliable performance is it promoted to full production.

Cloud-based ML platforms further support model evaluation by enabling
experiment logging, request replay, and synthetic test case generation.
These capabilities allow teams to evaluate different models under
identical conditions, facilitating comparisons and root-cause analysis.
Tools such as \href{https://wandb.ai/}{Weights and Biases} automate
aspects of this process by capturing training artifacts, recording
hyperparameter configurations, and visualizing performance metrics
across experiments. These tools integrate directly into training and
deployment pipelines, improving transparency and traceability.

While automation is central to MLOps evaluation, human oversight remains
essential. Automated tests may fail to capture nuanced performance
issues such as poor generalization on rare subpopulations or shifts in
user behavior. Teams combine quantitative evaluation with qualitative
review, particularly for models deployed in high-stakes or regulated
environments.

This multi-stage evaluation process bridges offline testing and live
system monitoring, ensuring models behave predictably under real-world
conditions and completing the development infrastructure foundation
necessary for production deployment.

\subsection{Infrastructure Integration
Summary}\label{sec-machine-learning-operations-mlops-infrastructure-integration-summary-cb2f}

The infrastructure and development components examined in this section
establish the foundation for reliable machine learning operations. These
systems transform ad hoc experimentation into structured workflows that
support reproducibility, collaboration, and continuous improvement.

\textbf{Data infrastructure} provides the foundation through feature
stores that enable feature reuse across projects, versioning systems
that track data lineage and evolution, and validation frameworks that
ensure data quality throughout the pipeline. Building on the data
management foundations from \textbf{?@sec-data-engineering-ml}, these
components extend basic capabilities to production contexts where
multiple teams and models depend on shared data assets.

\textbf{Continuous pipelines} automate the ML lifecycle through CI/CD
systems adapted for machine learning workflows. Unlike traditional
software CI/CD that focuses solely on code, ML pipelines orchestrate
data validation, feature transformation, model training, and evaluation
in integrated workflows. Training pipelines manage the computationally
intensive process of model development, coordinating resource
allocation, hyperparameter optimization, and experiment tracking. These
automated workflows enable teams to iterate rapidly while maintaining
reproducibility and quality standards.

\textbf{Model validation} bridges development and production through
systematic evaluation that extends beyond offline metrics. Validation
strategies combine performance benchmarking on held-out datasets with
canary testing in production environments, allowing teams to detect
issues before full deployment. This multi-stage validation recognizes
that models must perform not just on static test sets but under dynamic
real-world conditions where data distributions shift and user behavior
evolves.

These infrastructure components directly address the operational
challenges identified earlier through systematic engineering
capabilities:

\begin{itemize}
\tightlist
\item
  Feature stores and data versioning solve data dependency debt by
  ensuring consistent, tracked feature access across training and
  serving
\item
  CI/CD pipelines and model registries prevent correction cascades
  through controlled deployment and rollback mechanisms
\item
  Automated workflows and lineage tracking eliminate undeclared consumer
  risks via explicit dependency management
\item
  Modular pipeline architectures avoid pipeline debt through reusable,
  well-defined component interfaces
\end{itemize}

The infrastructure components establish reliable development and
deployment workflows, addressing the Data-Model and Model-Infrastructure
interfaces introduced earlier. Feature stores ensure training-serving
consistency, and CI/CD pipelines automate the transition from trained
weights to containerized services. These represent only half the
operational challenge, however. The third critical interface,
Production-Monitoring, requires a different set of practices focused not
on building models but on keeping them healthy over time.

\section{Production
Operations}\label{sec-machine-learning-operations-mlops-production-operations-b76d}

Production operations transform validated models into reliable services
that maintain performance under real-world conditions. This operational
layer implements the Production-Monitoring Interface, making the silent
failure problem
(Section~\ref{sec-machine-learning-operations-mlops-introduction-machine-learning-operations-04c6})
visible and manageable through monitoring, governance, and deployment
strategies.

Deployed systems must handle variable loads, maintain consistent
latency, recover gracefully from failures, and adapt to evolving data
distributions without disrupting service. These requirements demand
operational practices implementing Principle 4 (Observable Degradation)
at runtime, transforming silent model drift into actionable alerts
before users experience degradation.

\subsection{Model Deployment and
Serving}\label{sec-machine-learning-operations-mlops-model-deployment-serving-63f2}

Once trained and validated, a model must be integrated into a production
environment that delivers predictions at scale. Deployment transforms a
static artifact into a live system component, and serving ensures
accessibility, reliability, and efficiency in responding to inference
requests. Together, these components bridge model development and
real-world impact.

\subsubsection{Model
Deployment}\label{sec-machine-learning-operations-mlops-model-deployment-b9b9}

Validated models must transition from artifacts to services. Teams must
properly package, test, and track ML models for reliable production
deployment. One common approach involves containerizing models using
container technologies\sidenote{\textbf{Containerization and
Orchestration}: Docker (2013), named after dockworkers who load shipping
containers, pioneered application containerization by packaging code
with dependencies into portable units. Kubernetes (2014), from Greek
\emph{kubernetes} (helmsman, pilot), emerged from Google's internal Borg
system. The nautical metaphor reflects its role steering containers
across clusters, automating deployment, scaling, and networking. },
ensuring portability across environments.

Production deployment requires frameworks that handle model packaging,
versioning, and integration with serving infrastructure. Tools like
MLflow and model registries manage these deployment artifacts, while
serving-specific frameworks (detailed in the Inference Serving section)
handle the runtime optimization and scaling requirements.

Before full-scale rollout, teams deploy updated models to staging or QA
environments\sidenote{\textbf{TensorFlow Serving Origins}: Born from
Google's internal serving system that handled billions of predictions
per day for products like Gmail spam detection and YouTube
recommendations. Google open-sourced it in 2016 when they realized that
productionizing ML models was the bottleneck preventing widespread AI
adoption. } to rigorously test performance.

Techniques such as shadow deployments, canary
testing\sidenote{\textbf{Canary Deployment History}: Named after the
canaries miners used to detect toxic gases; if the bird died, miners
knew to evacuate immediately. Netflix pioneered this technique for
software in 2011, and it became essential for ML where model failures
can be subtle and catastrophic. }, and blue-green
deployment\sidenote{\textbf{Blue-Green Deployment}: Zero-downtime
deployment strategy maintaining two identical production environments.
One serves traffic (blue) while the other receives updates (green).
After validation, traffic switches instantly to green. For ML systems,
this enables risk-free model updates since rollback takes \textless10
seconds vs.~hours for model retraining. Spotify uses blue-green
deployment for their recommendation models, serving 400+ million users
with 99.95\% uptime during model updates. } validate new models
incrementally. These controlled deployment strategies enable safe model
validation in production. Robust rollback procedures are essential to
handle unexpected issues, reverting systems to the previous stable model
version to ensure minimal disruption.

When canary deployments reveal problems at partial traffic levels
(issues appearing at 30\% traffic but not at 5\%), teams need systematic
debugging strategies. Effective diagnosis requires correlating multiple
signals: performance metrics from \textbf{?@sec-benchmarking-ai}, data
distribution analysis to detect drift, and feature importance shifts
that might explain degradation. Teams maintain debug toolkits including
A/B test\sidenote{\textbf{A/B Testing for ML}: Statistical method to
compare model performance by splitting traffic between model versions.
Netflix runs 1,000+ A/B tests annually on recommendation algorithms,
while Uber tests ride pricing models on millions of trips daily to
optimize both user experience and revenue. Rollback decisions must
balance the severity of degradation against business impact: a 2\%
accuracy drop might be acceptable during feature launches but
unacceptable for safety-critical applications. } analysis frameworks,
feature attribution tools, and data slice analyzers that identify which
subpopulations are experiencing degraded performance.

Integration with CI/CD pipelines further automates the deployment and
rollback process, enabling efficient iteration cycles.

\paragraph{Rollback Strategies and Safety
Mechanisms}\label{sec-machine-learning-operations-mlops-rollback-strategies-safety-mechanisms-7707}

Rollback\sidenote{\textbf{Rollback}: From database transaction
management, where a failed transaction is ``rolled back'' to its
previous state, undoing partial changes. The term evokes physically
rolling back a tape or scroll to an earlier position. In ML deployment,
rollback extends this concept to model versions: reverting production
traffic to a previous model when the new version degrades performance,
preserving the core guarantee that incomplete or failed changes should
leave no trace. } capability is the safety net that enables confident
deployment. Without reliable rollback, teams become deployment-averse
and slow their iteration velocity. Effective rollback requires planning
for three distinct scenarios:

\textbf{Immediate Rollback (\textless{} 1 minute)}: For critical
failures detected immediately after deployment, including serving
errors, latency spikes, or obvious prediction failures. Implementation
requires maintaining the previous model version loaded and warm in
serving infrastructure, enabling instant traffic switching without
cold-start delays.

\textbf{Rapid Rollback (\textless{} 15 minutes)}: For performance
degradation detected through canary metrics within the first hour. This
requires model registry integration where previous versions remain
deployable with minimal configuration changes.

\textbf{Delayed Rollback (\textless{} 4 hours)}: For subtle issues
detected through business metrics or user feedback after full
deployment. This requires state management for any model-dependent data
(e.g., personalization state, cached embeddings) that accumulated during
the new model's operation.

Table~\ref{tbl-rollback-patterns} summarizes implementation patterns for
each rollback type:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1619}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2381}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3048}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2952}}@{}}
\caption{\textbf{Rollback Patterns by Scenario.} Each rollback type
requires different infrastructure support and state handling strategies.
Immediate rollback demands always-warm standbys; delayed rollback may
require data migration
procedures.}\label{tbl-rollback-patterns}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Rollback Type}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Trigger}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Implementation}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{State Handling}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Rollback Type}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Trigger}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Implementation}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{State Handling}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Immediate} & Serving errors, crashes & Hot standby with instant
switch & Stateless---no special handling \\
\textbf{Rapid} & Canary metric degradation & Registry-based redeployment
& Clear caches, restart sessions \\
\textbf{Delayed} & Business metric decline & Full redeployment with
migration & Migrate state, replay if needed \\
\end{longtable}

\textbf{Rollback Testing}

Rollback procedures that have never been tested will fail when needed.
Teams should:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Regularly test rollbacks}: Schedule monthly ``fire drills''
  where teams practice rolling back to previous versions
\item
  \textbf{Automate rollback triggers}: Define clear thresholds (e.g.,
  ``if P99 latency exceeds 2× baseline for 5 minutes, automatically
  rollback'')
\item
  \textbf{Validate rollback state}: Ensure rolled-back models produce
  consistent behavior, not corrupted predictions from stale caches
\item
  \textbf{Document rollback runbooks}: Maintain step-by-step procedures
  that work at 3 AM under stress
\end{enumerate}

\textbf{Stateful vs.~Stateless Rollback}

ML systems vary in statefulness, affecting rollback complexity:

\begin{itemize}
\tightlist
\item
  \textbf{Stateless models} (classification, regression): Rollback
  involves only switching model weights. Each prediction is independent.
\item
  \textbf{Stateful models} (sequential recommendations, conversational
  AI): Rollback must consider accumulated user state. May require
  session resets or state migration.
\item
  \textbf{Models with feedback loops}: Rollback may not restore previous
  behavior if training data was contaminated during the problematic
  deployment window.
\end{itemize}

For stateful systems, implement ``rollback checkpoints'' that capture
consistent state snapshots at deployment boundaries, enabling clean
restoration without user-visible disruption.

\paragraph{A/B Testing for Model
Validation}\label{sec-machine-learning-operations-mlops-ab-testing-model-validation-e480}

A/B testing provides the statistical foundation for deployment decisions
by comparing model versions under controlled conditions. Unlike canary
deployments (which validate operational stability), A/B tests measure
whether a new model improves business outcomes with statistical
confidence.

\textbf{Experimental Design}

A valid A/B test requires:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Randomization Unit}: Define what gets randomly assigned to
  treatment vs.~control. User-level randomization ensures consistent
  experience but requires larger sample sizes. Request-level
  randomization enables faster experiments but can confuse users seeing
  different results.
\item
  \textbf{Sample Size Calculation}: Determine required traffic before
  launch:
\end{enumerate}

\[n = \frac{2(z_{\alpha/2} + z_{\beta})^2 \sigma^2}{\delta^2}\]

where \(\delta\) is the minimum detectable effect, \(\sigma\) is outcome
variance, and \(z\) values depend on desired confidence (typically 95\%)
and power (typically 80\%). For a 2\% lift detection with 5\% baseline
conversion and 80\% power, expect \textasciitilde25,000 users per
variant.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\item
  \textbf{Guardrail Metrics}: Define metrics that must not degrade even
  if primary metric improves. A recommendation model improving
  click-through rate by 10\% while increasing page load time by 500ms
  may fail guardrail checks.
\item
  \textbf{Runtime}: Run tests until reaching statistical significance,
  typically 1-2 weeks minimum to capture weekly patterns. Avoid
  ``peeking'' at results and stopping early, as this inflates false
  positive rates.
\end{enumerate}

\textbf{ML-Specific Challenges}

A/B testing ML models introduces challenges absent from traditional
software experiments:

\begin{itemize}
\tightlist
\item
  \textbf{Delayed feedback}: Conversion events may occur days after
  prediction. A recommendation shown Monday might drive a purchase
  Friday. Tests must account for this attribution window.
\item
  \textbf{Novelty effects}: New models may show inflated initial
  performance as users engage with fresh recommendations. Include
  ``burn-in'' periods before measuring.
\item
  \textbf{Interference effects}: In recommendation and ranking systems,
  showing item A to user 1 affects availability for user 2. This
  violates the independence assumption underlying standard A/B analysis.
  Advanced treatments cover techniques for handling interference at
  platform scale.
\item
  \textbf{Segment heterogeneity}: Overall neutral results may mask
  strong positive effects for some users and negative effects for
  others. Always analyze by key segments.
\end{itemize}

\textbf{Decision Framework}

Table~\ref{tbl-ab-test-decisions} guides interpretation of A/B test
results:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2784}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.1443}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.5773}}@{}}
\caption{\textbf{A/B Test Decision Matrix.} Deployment decisions should
consider both primary metrics and guardrails. Improvements that come at
the cost of guardrail violations require careful tradeoff analysis
rather than automatic
deployment.}\label{tbl-ab-test-decisions}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Primary Metric}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Guardrails}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Decision}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Primary Metric}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Guardrails}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Decision}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Significant improvement} & All pass & Ship new model \\
\textbf{Significant improvement} & Some fail & Investigate tradeoffs,
may need model iteration \\
\textbf{No significant change} & All pass & New model adds no value;
keep current unless simplifying \\
\textbf{Significant degradation} & N/A & Do not ship; investigate root
cause \\
\end{longtable}

\textbf{Practical Guidelines}

\begin{itemize}
\tightlist
\item
  \textbf{Pre-register hypotheses}: Document expected effects before
  running tests to avoid p-hacking.
\item
  \textbf{Use sequential testing}: Methods like CUPED
  (Controlled-experiment Using Pre-Experiment Data) reduce variance and
  enable faster decisions.
\item
  \textbf{Archive all experiments}: Failed experiments provide valuable
  information for future model development.
\item
  \textbf{Automate analysis pipelines}: Manual analysis introduces
  errors and delays decisions.
\end{itemize}

Model registries, such as Vertex AI's model registry
(\citeproc{ref-vertex_ai_model_registry}{Cloud 2024b}), act as
centralized repositories for storing and managing trained models. These
registries facilitate version comparisons and often include access to
base models that may be open source, proprietary, or hybrid, such as
LLAMA (\citeproc{ref-llama_meta}{McMurray et al. 2014}). Deploying a
model from the registry to an inference endpoint is streamlined,
handling resource provisioning, model weight downloads, and hosting.

Inference endpoints typically expose the deployed model via REST APIs
for real-time predictions. Depending on performance requirements, teams
can configure resources, such as GPU accelerators, to meet latency and
throughput targets. Some providers also offer flexible options like
serverless\sidenote{\textbf{Serverless Computing for ML}: Infrastructure
that automatically scales from zero to thousands of instances based on
demand, with sub-second cold start times. AWS Lambda can handle 10,000+
concurrent ML inference requests, while Google Cloud Functions supports
models up to 32~GB, charging only for actual compute time used. For
example, AWS SageMaker Inference (\citeproc{ref-aws_sagemaker}{Ye, Hu,
and Enev 2020}) supports such configurations. } or batch inference,
eliminating the need for persistent endpoints and enabling
cost-efficient, scalable deployments.

To maintain lineage and auditability, teams track model artifacts,
including scripts, weights, logs, and metrics, using tools like
\href{https://mlflow.org/}{MLflow}\sidenote{\textbf{MLflow's Creation}:
Built by the team at Databricks who were frustrated watching their
customers struggle with ML experiment tracking. They noticed that data
scientists were keeping model results in spreadsheets and could never
reproduce their best experiments, a problem that inspired MLflow's
``model registry'' concept. }.

These tools and practices, along with distributed orchestration
frameworks like Ray\sidenote{\textbf{Ray and Model Orchestration}: Ray
is an open-source distributed computing framework created at UC
Berkeley's RISELab in 2017. Originally designed for reinforcement
learning, it evolved into a general-purpose system for scaling Python
applications across clusters. Ray Train and Ray Serve provide
ML-specific capabilities for distributed training and model serving,
while libraries like Ray Tune enable hyperparameter optimization across
thousands of concurrent experiments. Companies like Uber, OpenAI, and
Ant Group use Ray to orchestrate ML workloads at scale. }, enable teams
to deploy ML models resiliently, ensuring smooth transitions between
versions, maintaining production stability, and optimizing performance
across diverse use cases.

\subsubsection{Model Format
Optimization}\label{sec-machine-learning-operations-mlops-model-format-optimization-c9d6}

Deployment strategies determine how models reach production; format
optimization determines how efficiently they run once deployed. Before
deployment, models typically undergo format conversion and optimization
to maximize serving performance. The gap between research frameworks
(PyTorch, TensorFlow) and production serving can be substantial, and
optimized formats often achieve 2-10x latency improvements over naive
deployment approaches.

\textbf{Optimization Frameworks}

Table~\ref{tbl-model-optimization-frameworks} summarizes the major model
optimization tools and their characteristics:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1221}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2366}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2214}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.4198}}@{}}
\caption{\textbf{Model Optimization Frameworks.} Different optimization
tools target different deployment scenarios. TensorRT provides maximum
performance on NVIDIA GPUs but locks you into that hardware. ONNX
Runtime offers broader compatibility across hardware targets. OpenVINO
optimizes for Intel hardware
ecosystems.}\label{tbl-model-optimization-frameworks}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Framework}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Source Formats}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Target Hardware}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Key Optimizations}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Framework}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Source Formats}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Target Hardware}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Key Optimizations}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{ONNX Runtime} & PyTorch, TF, Keras, scikit & CPU, GPU, NPU &
Graph optimization, operator fusion, quantization \\
\textbf{TensorRT} & ONNX, TF, PyTorch & NVIDIA GPU only & Kernel
auto-tuning, precision calibration, layer fusion \\
\textbf{OpenVINO} & ONNX, TF, PyTorch, Caffe, MXNet & Intel CPU, GPU,
VPU, FPGA & Model compression, async execution, caching \\
\textbf{TF-TRT} & TensorFlow & NVIDIA GPU & TensorRT integration within
TensorFlow graph \\
\textbf{Core ML} & ONNX, TF, PyTorch & Apple Neural Engine, GPU, CPU &
Unified format for Apple devices, on-device inference \\
\textbf{TFLite} & TensorFlow, Keras & Mobile CPU, GPU, Edge TPU &
Quantization, delegate support, model compression \\
\end{longtable}

\textbf{ONNX as Interchange Format}

ONNX (Open Neural Network Exchange) has emerged as the standard
interchange format for model portability. The typical optimization
workflow is:

\begin{verbatim}
PyTorch Model → ONNX Export → ONNX Optimization → Target Runtime
                    │              │
                    │              ├── Graph optimization (constant folding,
                    │              │   dead code elimination)
                    │              ├── Operator fusion (Conv+BN+ReLU → single op)
                    │              └── Quantization (FP32 → INT8)
                    │
                    └── Validate numerical equivalence
\end{verbatim}

\textbf{Quantization Strategies}

Quantization reduces model size and increases throughput by using
lower-precision arithmetic:

\begin{itemize}
\tightlist
\item
  \textbf{Post-Training Quantization (PTQ)}: Convert trained FP32 model
  to INT8 without retraining. Typically achieves 2-4× speedup with
  \textless1\% accuracy loss for well-behaved models. Requires
  calibration dataset (1000-10000 samples).
\item
  \textbf{Quantization-Aware Training (QAT)}: Simulate quantization
  during training. Recovers accuracy lost in PTQ, essential for
  sensitive models. Adds 10-20\% training time overhead.
\item
  \textbf{Mixed Precision}: Keep sensitive layers (first, last,
  attention) in FP16/FP32 while quantizing others to INT8. Balances
  accuracy and performance.
\end{itemize}

\textbf{Optimization Checklist}

Before production deployment, verify:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Numerical equivalence}: Compare optimized model outputs
  against original on test set. Maximum acceptable divergence depends on
  application (typically \textless0.1\% for classification).
\item
  \textbf{Edge case behavior}: Test on out-of-distribution inputs.
  Optimizations can introduce different failure modes than the original
  model.
\item
  \textbf{Memory footprint}: Measure peak memory during inference, not
  just model size. Some optimizations increase runtime memory for speed.
\item
  \textbf{Warm-up requirements}: Many optimized runtimes require warm-up
  inference passes to compile kernels. Factor this into deployment
  procedures.
\item
  \textbf{Version compatibility}: Lock runtime versions in deployment
  configuration. Minor version changes can significantly affect
  performance or correctness.
\end{enumerate}

\subsubsection{Inference
Serving}\label{sec-machine-learning-operations-mlops-inference-serving-3f9a}

Optimized models need runtime infrastructure to handle production
traffic. Serving infrastructure provides the interface between trained
models and real-world systems, enabling predictions to be delivered
reliably and efficiently. In large-scale settings, serving systems
process tens of trillions of inference queries per day
(\citeproc{ref-wu2019machine}{Wu et al. 2019}). The measurement
frameworks established in \textbf{?@sec-benchmarking-ai} become
essential for validating performance claims. Meeting such demand
requires careful design that balances latency, scalability, and
robustness.

To address these challenges, production-grade serving frameworks have
emerged. Tools such as TensorFlow Serving
(\citeproc{ref-tensorflow_serving}{Coy et al.
2019b})\sidenote{\textbf{TensorFlow Serving}: Google's production-grade
ML serving system can handle approximately 100,000 queries per second
per CPU core for lightweight models, excluding actual inference time
(benchmarked on 16 vCPU Intel Xeon E5). End-to-end throughput depends on
model complexity and batch configuration. Originally built to serve
YouTube's recommendation system, processing over 1 billion hours of
video watched daily. }, NVIDIA Triton Inference Server
(\citeproc{ref-nvidia_triton}{NVIDIA 2025})\sidenote{\textbf{NVIDIA
Triton Inference Server}: Can achieve tens of thousands of inferences
per second on a single A100 GPU for optimized BERT-base models with
dynamic batching, reducing latency by up to 10\(\times\) compared to
naive serving approaches. Performance varies significantly with model
size and batch configuration. Supports concurrent execution of up to 100
different model types. }, and KServe (\citeproc{ref-kserve}{Aly et al.
2025})\sidenote{\textbf{KServe (formerly KFServing)}: Kubernetes-native
model serving framework supporting scale-to-zero and rapid autoscaling
for individual models. For a single ML Node, KServe provides
standardized inference protocol, model versioning, and canary
deployments with minimal configuration. At platform scale, organizations
like Bloomberg use KServe to serve thousands of models. } provide
standardized mechanisms for deploying, versioning, and scaling machine
learning models across heterogeneous infrastructure. These frameworks
abstract many of the lower-level concerns, allowing teams to focus on
system behavior, integration, and performance targets.

Model serving architectures are typically designed around three broad
paradigms:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Online Serving provides low-latency, real-time predictions for
  interactive systems such as recommendation engines or fraud detection.
\item
  Offline Serving processes large batches of data asynchronously,
  typically in scheduled jobs used for reporting or model retraining.
\item
  Near-Online (Semi-Synchronous) Serving offers a balance between
  latency and throughput, appropriate for scenarios like chatbots or
  semi-interactive analytics.
\end{enumerate}

Each of these approaches introduces different constraints in terms of
availability, responsiveness, and throughput. The efficiency techniques
from \textbf{?@sec-introduction} become crucial for meeting these
performance requirements, particularly when serving models at scale.
Serving systems are therefore constructed to meet specific Service Level
Agreements (SLAs)\sidenote{\textbf{Service Level Agreements (SLAs)}:
Production ML systems typically target 99.9\% uptime (8.77 hours
downtime/year) for critical services, with penalties of 10-25\% monthly
service credits for each 0.1\% below target. Major cloud providers offer
99.5\% to 99.95\% uptime SLAs for ML services, though recovery time
objectives vary by service tier and configuration. } and Service Level
Objectives (SLOs)\sidenote{\textbf{Service Level Objectives (SLOs)}:
Real-world ML serving SLOs often specify P95 latency \textless100~ms for
online inference, P99 \textless500~ms, and error rates \textless0.1\%.
Large-scale streaming services with hundreds of millions of users
typically target P99 latencies under 200~ms for their recommendation
systems, though actual performance depends on infrastructure and model
complexity. }, which quantify acceptable performance boundaries along
dimensions such as latency, error rates, and uptime. Achieving these
goals requires a range of optimizations in request handling, scheduling,
and resource allocation. Decomposing \emph{the latency budget} reveals
that model inference is often a minority of end-to-end response time.

\phantomsection\label{callout-perspectiveux2a-1.13}
\begin{fbxSimple}{callout-perspective}{Systems Perspective:}{The Latency Budget}
\phantomsection\label{callout-perspective*-1.13}
\textbf{The Problem}: A model achieves 15ms inference time on an A100
GPU, yet end-to-end P99 latency is 180ms. Where does the time go?

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2524}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1262}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1942}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.4272}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Component}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Typical \%}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{P99 Contribution}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Optimization Lever}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Network RTT} & 10-25\% & 15-45ms & Edge deployment, connection
pooling \\
\textbf{Feature retrieval} & 15-35\% & 25-65ms & Feature caching,
precomputation \\
\textbf{Request parsing} & 3-8\% & 5-15ms & Binary protocols (gRPC),
schema optimization \\
\textbf{Model inference} & 25-45\% & 45-80ms & Quantization, batching,
model distillation \\
\textbf{Post-processing} & 5-12\% & 10-20ms & Async processing, result
caching \\
\textbf{Response serialization} & 3-8\% & 5-15ms & Efficient formats
(Protobuf, MessagePack) \\
\end{longtable}

\textbf{The Engineering Insight}: Model optimization alone often
captures less than 50\% of the latency opportunity. A model that runs 2×
faster provides only 1.3× end-to-end improvement if inference is 40\% of
total latency.

Systems thinking demands end-to-end analysis. Apply the \textbf{DAM
Taxonomy} to diagnose the root cause:

\begin{itemize}
\tightlist
\item
  Is it \textbf{Logic} (Algorithm)? (Too many layers, unoptimized graph)
\item
  Is it \textbf{Physics} (Machine)? (Memory bandwidth saturation,
  thermal throttling)
\item
  Is it \textbf{Information} (Data)? (Feature extraction overhead,
  serialization cost)
\end{itemize}

Dave Patterson's principle applies: ``Measure everything, optimize the
bottleneck.''

\textbf{Worked Example}: Given a P99 SLO of 100ms, allocate the budget:

\begin{itemize}
\tightlist
\item
  Network: 15ms (use regional deployment)
\item
  Feature fetch: 25ms (require feature store P99 \textless{} 25ms)
\item
  Inference: 45ms (sets model complexity ceiling)
\item
  Post-processing: 15ms (allows light business logic)
\end{itemize}

If feature retrieval exceeds its budget, no amount of model optimization
will achieve the SLO.

\end{fbxSimple}

Serving system design strategies include:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Request scheduling and batching}: Aggregates inference
  requests to improve throughput. Clipper
  (\citeproc{ref-crankshaw2017clipper}{Crankshaw et al. 2017}) applies
  batching and caching for low-latency online prediction.
\item
  \textbf{Model instance selection and routing}: Dynamically assigns
  requests to model variants based on load or constraints. INFaaS
  (\citeproc{ref-romero2021infaas}{Romero et al. 2021}) optimizes
  accuracy-latency trade-offs across variants.
\item
  \textbf{Load balancing}: Distributes workloads evenly across
  instances. MArk (\citeproc{ref-zhang2019mark}{C. Zhang et al. 2019})
  demonstrates effective techniques for ML serving.
\item
  \textbf{Model instance autoscaling}: Dynamically adjusts capacity
  based on demand. Both INFaaS and MArk incorporate autoscaling for
  workload fluctuations.
\item
  \textbf{Model orchestration}: Coordinates execution of multi-stage
  models. AlpaServe (\citeproc{ref-li2023alpaserve}{Li et al. 2023})
  enables efficient serving of large foundation models through
  coordinated resource allocation.
\item
  \textbf{Execution time prediction}: Anticipates latency for individual
  requests. Clockwork (\citeproc{ref-gujarati2020serving}{Gujarati et
  al. 2020}) reduces tail latency by predicting inference times.
\end{enumerate}

Table~\ref{tbl-serving-techniques} summarizes these techniques alongside
representative systems.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2797}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.5678}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.1525}}@{}}
\caption{\textbf{Serving System Techniques.} Scalable ML-as-a-service
infrastructure relies on techniques like request scheduling and instance
selection to optimize resource utilization and reduce latency under high
load. Key strategies and representative systems illustrate approaches
for efficient deployment of machine learning
models.}\label{tbl-serving-techniques}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Technique}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Description}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Example System}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Technique}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Description}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Example System}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Request Scheduling \& Batching} & Groups inference requests to
improve throughput and reduce overhead & Clipper \\
\textbf{Instance Selection \& Routing} & Dynamically assigns requests to
model variants based on constraints & INFaaS \\
\textbf{Load Balancing} & Distributes traffic across replicas to prevent
bottlenecks & MArk \\
\textbf{Autoscaling} & Adjusts model instances to match workload demands
& INFaaS, MArk \\
\textbf{Model Orchestration} & Coordinates execution across model
components or pipelines & AlpaServe \\
\textbf{Execution Time Prediction} & Forecasts latency to optimize
request scheduling & Clockwork \\
\end{longtable}

Together, these strategies form the foundation of robust model serving
systems. While cloud-based serving infrastructure handles many
production scenarios, an increasing proportion of ML inference occurs at
the edge, where different operational constraints apply.

\subsubsection{Edge AI Deployment
Patterns}\label{sec-machine-learning-operations-mlops-edge-ai-deployment-patterns-dc1d}

Cloud-based serving handles many production scenarios, but an increasing
proportion of ML inference occurs at the edge. Edge AI represents a
shift where machine learning inference occurs at or near the data source
rather than in centralized cloud infrastructure. This shift introduces
three categories of operational challenges: resource constraints
(memory, power, compute limitations), update mechanisms (maintaining
models when direct access is unavailable), and monitoring approaches
(observing systems with limited connectivity).

According to industry analyses, a rapidly growing proportion of ML
inference by query count now occurs at the edge, making edge deployment
patterns essential knowledge for MLOps practitioners
(\citeproc{ref-reddi2023mlperf}{Reddi et al. 2019}). This paradigm
addresses critical constraints including latency requirements, bandwidth
limitations, privacy concerns, and connectivity constraints.

Edge deployment introduces unique operational challenges that
distinguish it from traditional cloud-centric MLOps. Resource
constraints on edge devices require the aggressive model optimization
techniques established in \textbf{?@sec-model-compression}
(quantization, pruning, and knowledge distillation) to meet very small
memory footprints (often sub-megabyte in microcontroller-class
deployments) while maintaining acceptable accuracy. Power budgets for
edge devices span a wide range, from milliwatts for small IoT sensors to
tens of watts in automotive systems, demanding power-aware inference
scheduling and thermal management strategies. Real-time requirements for
safety-critical applications can impose deterministic inference timing
targets, with worst-case response-time requirements often on the order
of milliseconds for collision avoidance and on the order of tens to
hundreds of milliseconds for interactive robotics.

The operational architecture for edge AI systems typically follows
hierarchical deployment patterns that distribute intelligence across
multiple tiers. Sensor-level processing handles immediate data filtering
and feature extraction with microcontroller-class devices consuming
1-100~mW. Edge gateway processing performs intermediate inference tasks
using application processors with 1-10~W power budgets. Cloud
coordination manages model distribution, aggregated learning, and
complex reasoning tasks requiring GPU-class computational resources.
This hierarchy enables system-wide optimization where computationally
expensive operations migrate to higher tiers while latency-critical
decisions remain local.

The most resource-constrained edge AI scenarios involve TinyML
deployment patterns, targeting microcontroller-based inference with
memory constraints under 1~MB and power consumption measured in
milliwatts. TinyML deployment requires specialized inference engines
such as TensorFlow Lite Micro, CMSIS-NN, and hardware-specific optimized
libraries that eliminate dynamic memory allocation and minimize
computational overhead. Model architectures must be co-designed with
hardware constraints, favoring depthwise convolutions, binary neural
networks, and pruned models that achieve 90\%+ sparsity while
maintaining task-specific accuracy requirements.

Mobile AI operations extend this edge deployment paradigm to smartphones
and tablets with moderate computational capabilities and strict power
efficiency requirements. Mobile deployment leverages hardware
acceleration through Neural Processing Units (NPUs), GPU compute
shaders, and specialized instruction sets to achieve inference
performance targets of 5-50~ms latency with power consumption under
500~mW. Mobile AI operations require sophisticated power management
including dynamic frequency scaling, thermal throttling coordination,
and background inference scheduling that balances performance against
battery life and user experience constraints.

Critical operational capabilities for deployed edge systems include
over-the-air model updates, which enable maintenance for systems that
cannot be physically accessed. OTA update pipelines must implement
secure, verified model distribution that prevents malicious model
injection while ensuring update integrity through cryptographic
signatures and rollback mechanisms. Edge devices require differential
compression techniques that minimize bandwidth usage by transmitting
only model parameter changes rather than complete model artifacts.
Update scheduling must account for device connectivity patterns, power
availability, and operational criticality to prevent update-induced
service disruptions.

Production edge AI systems implement real-time constraint management
through systematic approaches to deadline analysis and resource
allocation. Worst-case execution time (WCET) analysis provides evidence
that inference operations can complete within specified timing bounds
even under adverse conditions including thermal throttling, memory
contention, and interrupt service routines. Resource reservation
mechanisms can provide predictable computational bandwidth for
safety-critical inference tasks while enabling best-effort execution of
non-critical workloads. Graceful degradation strategies enable systems
to maintain essential functionality when resources become constrained by
reducing model complexity, inference frequency, or feature completeness.

Edge-cloud coordination patterns enable hybrid deployment architectures
that optimize the distribution of inference workloads across
computational tiers. Adaptive offloading strategies dynamically route
inference requests between edge and cloud resources based on current
system load, network conditions, and latency requirements. Feature
caching at edge gateways reduces redundant computation by storing
frequently accessed intermediate representations while maintaining data
freshness through cache invalidation policies. Federated learning
coordination enables edge devices to contribute to model improvement
without transmitting raw data, addressing privacy constraints while
maintaining system-wide learning capabilities.

The operational complexity of edge AI deployment requires specialized
monitoring and debugging approaches adapted to resource-constrained
environments. Lightweight telemetry systems capture essential
performance metrics including inference latency, power consumption, and
accuracy indicators while minimizing overhead on edge devices. Remote
debugging capabilities enable engineers to diagnose deployed systems
through secure channels that preserve privacy while providing sufficient
visibility into system behavior. Health monitoring systems track
device-level conditions including thermal status, battery levels, and
connectivity quality to predict maintenance requirements and prevent
catastrophic failures.

Resource constraint analysis underpins successful edge AI deployment by
systematically modeling the trade-offs between computational capability,
power consumption, memory utilization, and inference accuracy. Power
budgeting frameworks establish operational envelopes that define
sustainable workload configurations under varying environmental
conditions and usage patterns. Memory optimization hierarchies guide the
selection of model compression techniques, from parameter reduction
through structural simplification to architectural modifications that
reduce computational requirements.

Edge AI deployment represents the operational frontier where MLOps
practices must adapt to physical constraints and distributed complexity.
Success requires technical expertise in model optimization and embedded
systems combined with systematic approaches to distributed system
management, security, and reliability engineering.

The deployment and serving patterns above address how models reach
production. Deployment is only half the operational challenge, however.
The silent failure modes discussed earlier mean that a successfully
deployed model can still degrade through drift or data quality issues
without triggering any alerts. The monitoring, incident response, and
on-call practices that follow address the Production-Monitoring
Interface, ensuring deployed models remain healthy throughout their
operational lifetime.

\subsection{Resource Management and Performance
Monitoring}\label{sec-machine-learning-operations-mlops-resource-management-performance-monitoring-afe7}

The operational stability of a machine learning system depends on robust
underlying infrastructure. Compute, storage, and networking resources
must be provisioned, configured, and scaled to accommodate training
workloads, deployment pipelines, and real-time inference. Effective
observability practices ensure system behavior can be monitored,
interpreted, and acted upon as conditions change.

\subsubsection{Infrastructure
Management}\label{sec-machine-learning-operations-mlops-infrastructure-management-bf8f}

Deployment and serving patterns address how models reach users, but both
depend on underlying infrastructure. Scalable, resilient infrastructure
is foundational for operationalizing ML systems. As models move from
experimentation to production, computational resources must support
continuous integration, large-scale training, automated deployment, and
real-time inference. Meeting these requirements demands managing
infrastructure as a dynamic, programmable, versioned system.

Teams adopt Infrastructure as Code (IaC), treating infrastructure
configuration as software that is version-controlled, reviewed, and
automatically executed. Rather than manually configuring servers through
graphical interfaces or command-line tools, IaC describes the desired
state of infrastructure resources in declarative files. This approach
brings software engineering best practices to infrastructure management:
changes are tracked, configurations can be tested before deployment, and
environments can be reliably reproduced.

Tools such as Terraform (\citeproc{ref-terraform}{McElwain et al.
2024}), AWS CloudFormation (\citeproc{ref-aws_cloudformation}{Services
2020}), and Ansible (\citeproc{ref-ansible}{Hatcher 2024}) support this
paradigm by enabling teams to version infrastructure definitions
alongside application code. In MLOps settings, Terraform is widely used
to provision and manage resources across public cloud platforms such as
AWS (\citeproc{ref-aws}{Amazon 2025}), Google Cloud Platform
(\citeproc{ref-google_cloud}{Qin et al. 2024}), and Microsoft Azure
(\citeproc{ref-azure}{Rosa et al. 2021}).

Infrastructure management spans the full ML lifecycle. During training,
IaC scripts allocate compute instances with GPU or TPU accelerators,
configure distributed storage, and deploy container clusters. Because
infrastructure definitions are stored as code, they can be audited,
reused, and integrated into CI/CD pipelines ensuring consistency across
environments.

Containerization plays a critical role in making ML workloads portable.
Tools like \href{https://www.docker.com/}{Docker} encapsulate models and
their dependencies into isolated units, while orchestration systems such
as \href{https://kubernetes.io/}{Kubernetes} manage containerized
workloads across clusters, enabling rapid deployment, resource
allocation, and scaling.

To handle changes in workload intensity, including spikes during
hyperparameter tuning and surges in prediction traffic, teams rely on
cloud elasticity and autoscaling\sidenote{\textbf{ML Autoscaling}: For a
single model, Kubernetes-based serving can scale from 1 to dozens of
replicas in under 60 seconds based on traffic, reducing infrastructure
costs by 35-50\% through right-sizing. Autoscaling a single model
handles traffic bursts without over-provisioning; platform-scale
autoscaling across model portfolios introduces additional coordination
challenges. }. Cloud platforms support on-demand provisioning and
horizontal scaling of infrastructure resources.
\href{https://aws.amazon.com/autoscaling/}{Autoscaling mechanisms}
automatically adjust compute capacity based on usage metrics, enabling
teams to optimize for both performance and cost-efficiency.

Infrastructure in MLOps is not limited to the cloud. Many deployments
span on-premises, cloud, and edge environments, depending on latency,
privacy, or regulatory constraints. A robust infrastructure management
strategy must accommodate this diversity by offering flexible deployment
targets and consistent configuration management across environments.

To illustrate, consider a scenario in which a team uses Terraform to
deploy a Kubernetes cluster on Google Cloud Platform. The cluster is
configured to host containerized TensorFlow models that serve
predictions via HTTP APIs. As user demand increases, Kubernetes
automatically scales the number of pods to handle the load. Meanwhile,
CI/CD pipelines update the model containers based on retraining cycles,
and monitoring tools track cluster performance, latency, and resource
utilization. All infrastructure components, ranging from network
configurations to compute quotas, are managed as version-controlled
code, ensuring reproducibility and auditability.

By adopting Infrastructure as Code, cloud-native orchestration, and
automated scaling, MLOps teams can provision and maintain resources
required for machine learning at production scale.

Infrastructure as Code addresses how to provision resources; the
challenge remains deciding when and how much. ML workloads exhibit
fundamentally different resource consumption patterns than stateless web
applications: training jobs burst from zero to dozens of GPUs then
return to minimal consumption, while inference maintains steady
utilization under variable traffic. Training workloads demonstrate
bursty requirements that create tension between resource utilization
efficiency and time-to-insight. Inference workloads present steadier
consumption patterns but with strict latency requirements under variable
traffic.

The optimization challenge intensifies when considering the
interdependencies between training frequency, model complexity, and
serving infrastructure costs. Effective resource management requires
holistic approaches that model the entire system rather than optimizing
individual components in isolation. Such approaches must account for
data pipeline throughput, model retraining schedules, and serving
capacity planning.

Hardware-aware resource optimization emerges as a critical operational
discipline that bridges infrastructure efficiency with model
performance. Production MLOps teams must establish utilization targets
that balance cost efficiency against operational reliability: GPU
utilization should consistently exceed 80\% for batch training workloads
to justify hardware costs, while serving workloads require sustained
utilization above 60\% to maintain economically viable inference
operations. Memory bandwidth utilization patterns become equally
important, as underutilized memory interfaces indicate suboptimal data
pipeline configurations that can degrade training throughput by 30-50\%.

Operational resource allocation extends beyond simple utilization
metrics to encompass power budget management across mixed workloads.
Production deployments typically allocate 60-70\% of power budgets to
training operations during development cycles, reserving 30-40\% for
sustained inference workloads. This allocation shifts dynamically based
on business priorities: recommendation systems might reallocate power
toward inference during peak traffic periods, while research
environments prioritize training resource availability. Thermal
management considerations become operational constraints rather than
hardware design concerns, as sustained high-utilization workloads must
be scheduled with cooling capacity limitations and thermal throttling
thresholds that can impact SLA compliance.

\paragraph{Hardware Utilization
Patterns}\label{sec-machine-learning-operations-mlops-hardware-utilization-patterns-5c10}

Understanding hardware utilization patterns is essential for
cost-effective ML operations. Unlike traditional web services where CPU
utilization directly correlates with throughput, ML inference exhibits
complex relationships between hardware metrics and actual performance.

\textbf{GPU Utilization Reality}

GPU utilization metrics can mislead operators. A GPU reporting 90\%
utilization might be:

\begin{itemize}
\tightlist
\item
  \textbf{Compute-bound}: Actively executing tensor operations (ideal)
\item
  \textbf{Memory-bound}: Waiting for data transfers from GPU memory
\item
  \textbf{I/O-bound}: Stalled waiting for input data from CPU/network
\end{itemize}

Table~\ref{tbl-gpu-utilization-patterns} distinguishes these patterns
and their optimization strategies:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1667}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1569}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1765}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.5000}}@{}}
\caption{\textbf{GPU Utilization Patterns.} Different utilization
signatures require different optimizations. High GPU utilization with
low memory bandwidth suggests compute-bound workloads that benefit from
parallelism. High memory bandwidth with moderate GPU utilization
indicates memory-bound workloads requiring model
optimization.}\label{tbl-gpu-utilization-patterns}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Pattern}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{GPU Util}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Memory BW Util}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Optimization Strategy}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Pattern}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{GPU Util}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Memory BW Util}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Optimization Strategy}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Compute-bound} & \textgreater85\% & \textless70\% & Larger batch
sizes, tensor parallelism within node \\
\textbf{Memory-bound} & 50-85\% & \textgreater85\% & Reduce model size,
quantize, optimize memory access \\
\textbf{I/O-bound} & \textless50\% & \textless50\% & Improve data
pipeline, prefetch inputs, use SSDs \\
\textbf{Batch-starved} & Variable (spiky) & Variable & Dynamic batching,
request queuing on single server \\
\end{longtable}

\paragraph*{Utilization Targets by
Workload}\label{utilization-targets-by-workload}
\addcontentsline{toc}{paragraph}{Utilization Targets by Workload}

Production systems should establish utilization targets based on
workload characteristics:

\begin{itemize}
\tightlist
\item
  \textbf{Batch training}: Target \textgreater80\% GPU utilization.
  Lower utilization indicates data pipeline bottlenecks or suboptimal
  batch sizes. Monitor \texttt{gpu\_util},
  \texttt{memory\_bandwidth\_util}, and \texttt{data\_load\_time}.
\item
  \textbf{Online inference}: Target 50-70\% GPU utilization at P50 load.
  Reserve headroom (30-50\%) for traffic spikes. Higher sustained
  utilization risks latency SLA violations during bursts.
\item
  \textbf{Batch inference}: Target \textgreater85\% utilization. Unlike
  online serving, batch jobs can tolerate queuing delays, enabling
  maximum hardware efficiency.
\end{itemize}

\paragraph*{Memory Hierarchy Effects}\label{memory-hierarchy-effects}
\addcontentsline{toc}{paragraph}{Memory Hierarchy Effects}

Model serving performance depends critically on memory hierarchy
utilization, as Table~\ref{tbl-gpu-memory-hierarchy} quantifies:

\begin{longtable}[]{@{}lrl@{}}
\caption{GPU Memory Hierarchy and Bandwidth
Characteristics}\label{tbl-gpu-memory-hierarchy}\tabularnewline
\toprule\noalign{}
\textbf{Memory Level} & \textbf{Bandwidth} & \textbf{Typical
Contents} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\textbf{Memory Level} & \textbf{Bandwidth} & \textbf{Typical
Contents} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{L2 Cache (40 MB on A100)} & \textasciitilde3 TB/s & Hot
weights \\
\textbf{HBM2e GPU Memory (80 GB)} & \textasciitilde2 TB/s & Model \\
\textbf{PCIe/NVLink to CPU} & \textasciitilde64 GB/s & Activations \\
\textbf{System RAM (512 GB)} & \textasciitilde200 GB/s & Batched
inputs \\
\textbf{NVMe SSD} & \textasciitilde7 GB/s & Model swap \\
\end{longtable}

For LLM serving on a single GPU or server, the KV-cache (storing
attention keys and values for each token) often becomes the memory
bottleneck. A 70B parameter model may require 2-4 GB of KV-cache per
active sequence, limiting how many requests a single node can batch
together. Monitoring KV-cache utilization on each serving node enables
capacity planning: when KV-cache approaches GPU memory limits,
additional requests queue rather than batch, degrading latency.

\paragraph*{Cost-Per-Inference
Tracking}\label{cost-per-inference-tracking}
\addcontentsline{toc}{paragraph}{Cost-Per-Inference Tracking}

Translate hardware metrics into business-relevant cost metrics:

\[\text{Cost per 1K inferences} = \frac{\text{Hourly GPU cost} \times 1000}{\text{Inferences per hour}}\]

For a \$3/hour A100 instance processing 50,000 inferences/hour, cost is
\$0.06 per 1K inferences. Track this metric over time; increases
indicate efficiency degradation requiring investigation.

\subsubsection{Model and Infrastructure
Monitoring}\label{sec-machine-learning-operations-mlops-model-infrastructure-monitoring-3988}

Infrastructure management provisions resources; monitoring observes
their behavior. This distinction is critical because the
\textbf{Verification Gap} (\textbf{?@sec-deploy-invariants}) means ML
systems cannot be proven correct through unit tests; they can only be
bounded statistically. Monitoring implements \textbf{Principle 4:
Observable Degradation}
(Section~\ref{sec-machine-learning-operations-mlops-foundational-principles-44e6}),
transforming the Verification Gap from a theoretical limitation into
operational practice through actionable signals. Once a model is live,
it becomes exposed to real-world inputs, evolving data distributions,
and shifting user behavior. Without continuous monitoring, and the
deeper observability\sidenote{\textbf{Observability}: A term from
Control Theory introduced by Rudolf Kálmán in 1960. It measures how well
the internal states of a system can be inferred from knowledge of its
external outputs. In MLOps, ``Monitoring'' tells you the system is
broken (high error rate); ``Observability'' allows you to ask \emph{why}
(by inferring the internal state of feature distributions or neuron
activations from the outputs). } it enables, detecting performance
degradation, data quality issues, or system failures becomes difficult.

Effective monitoring spans both model behavior and infrastructure
performance. On the model side, teams track metrics such as accuracy,
precision, recall, and the confusion matrix
(\citeproc{ref-scikit_learn_confusion_matrix}{Liu et al. 2018}) using
live or sampled predictions to detect whether performance remains stable
or begins to drift. A critical constraint is \emph{the drift detection
delay}, which determines how quickly statistical monitoring can confirm
that degradation has occurred.

\phantomsection\label{callout-notebookux2a-1.14}
\begin{fbxSimple}{callout-notebook}{AI Engineer’s Notebook:}{The Drift Detection Delay}
\phantomsection\label{callout-notebook*-1.14}
\textbf{Problem}: You have a model with \textbf{95\% baseline accuracy}.
You want to detect a \textbf{5\% drop} (to 90\%) with 95\% statistical
confidence. Your system handles \textbf{1 request per second (1 QPS)}.
How long will it take to ``prove'' the model has drifted?

\textbf{The Math}:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Required Samples}: To distinguish 95\% from 90\% with high
  confidence, you need \(\approx \mathbf{1,000}\) labeled samples.
\item
  \textbf{Detection Latency}:
  \(1,000 \text{ samples} / 1 \text{ QPS} = \mathbf{1,000 \text{ seconds}} \approx \mathbf{17 \text{ minutes}}\).
\item
  \textbf{Low Traffic Case}: If the model only processes \textbf{100
  requests per day}, detecting the same 5\% drift takes \textbf{10
  days}.
\end{enumerate}

\textbf{The Systems Conclusion}: The ``Sample Rate'' of your monitoring
is physically limited by your traffic volume. For low-traffic,
high-stakes models (like medical diagnosis), drift detection can take
\emph{days or weeks}, leaving the system in a long-term ``Silent
Failure'' state. This is why high-stakes systems must supplement
statistical monitoring with proactive \textbf{Model Audits}.

\end{fbxSimple}

Production ML systems face model drift\ldots{}\sidenote{\textbf{Drift in
Practice}: Data Drift occurs when the input distribution \(P(X)\) shifts
(e.g., ``everyone bought webcams in 2020,'' changing the frequency of
webcam images). Concept Drift occurs when the relationship \(P(Y|X)\)
shifts (e.g., ``users stopped clicking on clickbait titles they used to
like,'' changing the predicted outcome for the same input). }

\begin{itemize}
\tightlist
\item
  \textbf{Concept drift}\sidenote{\textbf{COVID-19 ML Impact}: COVID-era
  behavior changes provide a salient example of abrupt concept drift.
  Many systems experienced rapid shifts in demand patterns and user
  behavior, requiring accelerated retraining, feature updates, and
  revised capacity planning. The exact magnitude varied widely by
  application, region, and time period. } occurs when the underlying
  relationship between features and targets evolves. For example, during
  the COVID-19 pandemic, purchasing behavior shifted dramatically,
  invalidating many previously accurate recommendation models.
\end{itemize}

\begin{itemize}
\tightlist
\item
  Data drift\sidenote{\textbf{Drift}: Borrowed from meteorology and
  navigation, where ``drift'' describes the gradual movement of
  something from its intended course due to external forces (wind,
  current). The statistical concept of ``covariate shift'' was
  formalized by Shimodaira in 2000, but the more intuitive term
  ``drift'' gained adoption in ML operations because it captures the
  essential insight: production data gradually moves away from training
  distributions, like a ship drifting off course. } refers to shifts in
  the input data distribution itself. In applications such as
  self-driving cars, this may result from seasonal changes in weather,
  lighting, or road conditions, all of which affect the model's inputs.
\end{itemize}

This phenomenon is formally defined as follows:

\phantomsection\label{callout-definitionux2a-1.15}
\begin{fbxSimple}{callout-definition}{Definition:}{Data Drift}
\phantomsection\label{callout-definition*-1.15}
\textbf{\emph{Data Drift}} is the divergence between the \textbf{Source
Distribution} (\(P_{train}(X)\)) and the \textbf{Target Distribution}
(\(P_{serve}(X)\)). It represents a violation of the \textbf{I.I.D.
Assumption} (Independent and Identically Distributed) that underpins
statistical learning theory, necessitating \textbf{Continuous
Monitoring} and \textbf{Retraining} to prevent performance decay.

\end{fbxSimple}

Because of drift, a deployed model behaves less like software (which
doesn't break unless changed) and more like \textbf{inventory} (which
decays over time). This is the \textbf{Statistical Drift Invariant} at
work: the Degradation Equation
(\(\text{Accuracy}(t) \approx \text{Accuracy}_0 - \lambda \cdot D(P_t \| P_0)\))
introduced in \textbf{?@sec-deploy-invariants} predicts that accuracy
erodes in proportion to the distributional divergence \(D\), regardless
of code quality. Every monitoring strategy in this chapter exists to
detect \(D\) before it compounds into business impact. The following
``Rotting Asset'' plot (Figure~\ref{fig-rotting-asset-curve}) visualizes
this entropy and compares two maintenance strategies: scheduled
``Sawtooth'' retraining versus trigger-based ``Flatline'' retraining.

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{contents/vol1/ops/ops_files/figure-pdf/fig-rotting-asset-curve-output-1.pdf}}

}

\caption{\label{fig-rotting-asset-curve}\textbf{The Rotting Asset
Curve}: Model Accuracy vs.~Time (Days) showing the impact of statistical
drift. Unlike traditional software that remains static unless modified,
ML models decay as the world changes. Periodic retraining (sawtooth) and
triggered retraining (green) are the primary engineering responses to
prevent this silent failure. Monitoring is the mechanism that transforms
model `decay' into a manageable maintenance schedule.}

\end{figure}%

\subsection{Quantifying Drift: The Physics of
PSI}\label{sec-ops-quantifying-drift-physics-psi-4d92}

Beyond these recognized drift patterns lies a more insidious challenge:
gradual long-term degradation that evades standard detection thresholds.
Unlike sudden distribution shifts that trigger immediate alerts, some
models experience performance erosion over months through imperceptible
daily changes. Small day-to-day changes (on the order of basis points of
a quality metric) can compound into material degradation over a year
without tripping coarse monthly alerts. Seasonal patterns compound this
complexity: a model trained in summer may perform well through autumn
but fail in winter conditions it never observed. Detecting such gradual
degradation requires specialized monitoring approaches, including
establishing performance baselines across multiple time horizons (daily,
weekly, quarterly), implementing sliding window comparisons that detect
slow trends, and maintaining seasonal performance profiles that account
for cyclical patterns. Teams often discover these degradations only
through periodic reviews when cumulative impact becomes visible,
emphasizing the need for multi-timescale monitoring strategies.

Infrastructure-level monitoring tracks indicators such as CPU and GPU
utilization, memory and disk consumption, network latency, and service
availability. Hardware-aware monitoring extends these metrics to capture
resource efficiency patterns: GPU memory bandwidth utilization, power
consumption relative to computational output, and thermal envelope
adherence across sustained workloads. GPU utilization monitoring should
distinguish between compute-bound and memory-bound operations, as
identical 90\% utilization metrics can represent vastly different
operational efficiency depending on bottleneck location. Memory
bandwidth monitoring becomes essential for detecting suboptimal data
loading patterns that manifest as high GPU utilization with low
computational throughput. Power efficiency metrics, measured as
operations per watt, enable teams to optimize mixed workload scheduling
for both cost and environmental impact.

Thermal monitoring integrates into operational scheduling decisions,
particularly for sustained high-utilization deployments where thermal
throttling can degrade performance unpredictably. Modern MLOps
monitoring dashboards incorporate thermal headroom metrics that guide
workload distribution across available hardware, preventing
thermal-induced performance degradation that can violate inference
latency SLAs. Tools such as Prometheus
(\citeproc{ref-prometheus}{Rinella et al.
2023})\sidenote{\textbf{Prometheus}: Named after the Greek Titan who
stole fire from the gods to give to humanity, the monitoring system
embodies the metaphor of bringing visibility (fire/light) to hidden
system states. Created at SoundCloud in 2012 and open-sourced in 2016,
Prometheus pioneered the pull-based metrics model that became standard
for cloud-native monitoring. The system can be operated at large scale
with sharding and federation, with achievable scale depending on scrape
intervals, label cardinality, and retention requirements. }, Grafana
(\citeproc{ref-grafana}{Labs 2024}), and Elastic
(\citeproc{ref-elastic}{Ryu et al. 2024}) are widely used to collect,
aggregate, and visualize these operational metrics. These tools often
integrate into dashboards that offer real-time and historical views of
system behavior.

Collecting all of these signals at production scale introduces its own
cost constraints. \emph{The economics of observability} force
engineering teams to make deliberate trade-offs between monitoring
granularity and infrastructure expense.

\phantomsection\label{callout-notebookux2a-1.16}
\begin{fbxSimple}{callout-notebook}{AI Engineer’s Notebook:}{The Economics of Observability}
\phantomsection\label{callout-notebook*-1.16}
\textbf{The Monitoring Trade-off}: ``Measure everything'' is physically
impossible at scale. The cost of observability scales linearly with
sampling frequency and metric cardinality.

\[ \text{Cost} \approx \text{Frequency} \times \text{Metrics} \times (\text{Ingest Cost} + \text{Storage Cost}) \]

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1364}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1705}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3182}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3750}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Sampling}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Granularity}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Data Volume (1M req/sec)}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Cost Impact}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{1 sec} & Micro-bursts & \textasciitilde1 GB/sec & High (Requires
dedicated cluster) \\
\textbf{60 sec} & Trends & \textasciitilde16 MB/sec & Low (Standard
sidecar) \\
\end{longtable}

\textbf{Engineering Decision}: Use \textbf{dynamic sampling}. Sample 1\%
of successful requests but 100\% of errors. Use high-frequency (1s)
monitoring only for aggregate counters (like error rate), but
low-frequency (60s) for high-cardinality data (like user-level
distribution sketches).
Section~\ref{sec-machine-learning-operations-mlops-monitoring-cost-model-7fe3}
provides worked examples for budgeting monitoring infrastructure.

\end{fbxSimple}

Proactive alerting mechanisms notify teams when anomalies or threshold
violations occur\sidenote{\textbf{Production Alert Thresholds}:
Production alert thresholds are workload-specific and should be tuned to
minimize alert fatigue while catching meaningful degradation early.
Common patterns include alerts on sustained resource saturation,
elevated error rates, and latency regressions relative to a baseline,
alongside ML-specific alerts on data drift indicators and model quality
metrics. Hardware-aware alerting can extend these thresholds to include
sustained underutilization (waste), bandwidth bottlenecks, power and
thermal budget violations, and throttling events. }. A sustained drop in
model accuracy may trigger drift investigation; infrastructure alerts
can signal memory saturation or degraded network performance. Robust
monitoring enables teams to detect problems before they escalate and
maintain ML system reliability.

Netflix's monitoring system illustrates these alerting principles at
extreme scale, where hundreds of models serve billions of predictions
daily.

\phantomsection\label{callout-lighthouseux2a-1.17}
\begin{fbxSimple}{callout-lighthouse}{Lighthouse:}{Netflix ML Monitoring at Scale}
\phantomsection\label{callout-lighthouse*-1.17}
Netflix operates hundreds of ML models powering recommendations, content
optimization, and infrastructure management, processing billions of
predictions daily across 200+ million subscribers.

\textbf{The Challenge}: Traditional monitoring detected only 40\% of ML
issues before user impact. Models degraded silently as viewing patterns
shifted, content libraries changed, and user bases evolved across
regions.

\textbf{The Solution}: Netflix developed a multi-layer monitoring
approach:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Statistical Process Control}: Treats model metrics as time
  series, applying control charts to detect anomalous deviation patterns
  rather than fixed thresholds
\item
  \textbf{Cohort-Based Monitoring}: Segments performance by user tenure,
  device type, and region to catch localized degradation masked by
  global averages
\item
  \textbf{Counterfactual Evaluation}: Maintains holdout groups to
  continuously measure model lift against baseline, detecting when
  models stop adding value
\end{enumerate}

\textbf{Key Innovation}: ``Interleaving'' experiments run new and old
models simultaneously on the same users, using preference ranking to
detect subtle quality differences undetectable through aggregate
metrics.

\textbf{Results}: 85\% of ML issues now detected before user impact.
Mean time to detection reduced from 4 hours to 15 minutes. False
positive rate kept below 5\% through adaptive thresholds.

\emph{Reference: Steck et al. (\citeproc{ref-steck2021netflix}{Steck et
al. 2021})}

\end{fbxSimple}

\subsubsection{Data Quality
Monitoring}\label{sec-machine-learning-operations-mlops-data-quality-monitoring-c6b6}

Model and infrastructure monitoring tracks outputs. By the time output
metrics degrade, however, the underlying problem may have existed for
days or weeks. Data quality monitoring catches issues before they
propagate through the system. In production ML, monitoring inputs is
often more important than monitoring outputs because data issues cause
the majority of model degradation.

\textbf{Input Data Validation}

Schema\sidenote{\textbf{Schema}: From Greek \emph{skhema} (shape, form,
figure), originally used in philosophy to describe the form or outline
of an argument. Kant used ``schema'' to describe mental templates that
organize experience. In databases (1970s), the term came to mean the
formal structure defining data organization: tables, columns, types, and
constraints. In ML operations, schema validation enforces that incoming
data matches the expected structure before processing. } validation
catches structural problems before they reach the model.
Listing~\ref{lst-ops-schema-validation} demonstrates common validation
rules using Great Expectations, including column existence checks, type
enforcement, null detection, and statistical bounds.

\begin{codelisting}

\caption{\label{lst-ops-schema-validation}\textbf{Input Data Validation
with Great Expectations}: Schema validation rules check column
existence, data types, null values, and statistical bounds to catch data
quality issues before they propagate to model inference.}

\centering{

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Example using Great Expectations}
\NormalTok{expect\_column\_to\_exist(column}\OperatorTok{=}\StringTok{"user\_id"}\NormalTok{)}
\NormalTok{expect\_column\_values\_to\_be\_of\_type(}
\NormalTok{    column}\OperatorTok{=}\StringTok{"timestamp"}\NormalTok{, type\_}\OperatorTok{=}\StringTok{"datetime"}
\NormalTok{)}
\NormalTok{expect\_column\_values\_to\_not\_be\_null(column}\OperatorTok{=}\StringTok{"feature\_a"}\NormalTok{)}

\CommentTok{\# Statistical bounds catch value anomalies}
\NormalTok{expect\_column\_values\_to\_be\_between(}
\NormalTok{    column}\OperatorTok{=}\StringTok{"age"}\NormalTok{, min\_value}\OperatorTok{=}\DecValTok{0}\NormalTok{, max\_value}\OperatorTok{=}\DecValTok{120}
\NormalTok{)}
\NormalTok{expect\_column\_mean\_to\_be\_between(}
\NormalTok{    column}\OperatorTok{=}\StringTok{"purchase\_amount"}\NormalTok{, min\_value}\OperatorTok{=}\DecValTok{10}\NormalTok{, max\_value}\OperatorTok{=}\DecValTok{1000}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

}

\end{codelisting}%

\textbf{Feature Distribution Monitoring}

Track feature distributions against training baselines using statistical
distance measures. Table~\ref{tbl-feature-distribution-thresholds}
specifies alert thresholds for three common metrics, with PSI suited for
categorical features and KS statistics for continuous distributions:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4138}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2184}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3678}}@{}}
\caption{\textbf{Feature Distribution Thresholds.} Starting points for
drift detection; teams should calibrate based on feature sensitivity and
business impact. PSI values above 0.2 indicate significant distribution
shift, while KS statistics exceeding 0.1 suggest statistically
significant divergence from training distributions. Higher thresholds
reduce alert fatigue but risk missing gradual
drift.}\label{tbl-feature-distribution-thresholds}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Metric}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Alert Threshold}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Use Case}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Metric}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Alert Threshold}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Use Case}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Population Stability Index (PSI)} & PSI \textgreater{} 0.2 &
Categorical and binned features \\
\textbf{Kolmogorov-Smirnov statistic} & KS \textgreater{} 0.1 &
Continuous feature distributions \\
\textbf{Jensen-Shannon divergence} & JS \textgreater{} 0.1 & Probability
distributions \\
\end{longtable}

\textbf{Quantifying Drift: The Physics of PSI}

Understanding \emph{why} we use these thresholds requires looking at the
math. The Population Stability Index (PSI) quantifies distributional
shift by comparing expected (training) vs.~actual (serving) frequencies
across bins:

\[ \text{PSI} = \sum_{i=1}^{n} (\text{actual}_i - \text{expected}_i) \times \ln\left(\frac{\text{actual}_i}{\text{expected}_i}\right) \]

For continuous distributions, Kullback-Leibler (KL) divergence offers a
more sensitive alternative, though PSI's symmetric properties often make
it preferred for drift alerting:

\[ D_{\text{KL}}(P \parallel Q) = \sum_{x} P(x) \log\left(\frac{P(x)}{Q(x)}\right) \]

To see this in practice, consider a recommendation system monitoring
user age. A shift from ``younger'' to ``older'' demographics might look
subtle on a histogram but generates a clear PSI signal:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 10\tabcolsep) * \real{0.1196}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 10\tabcolsep) * \real{0.1522}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 10\tabcolsep) * \real{0.1413}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 10\tabcolsep) * \real{0.1522}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 10\tabcolsep) * \real{0.2609}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 10\tabcolsep) * \real{0.1739}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Age Bin}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Training \%}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Serving \%}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Difference}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{ln(Serving/Training)}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Contribution}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{18--25} & 15.0 & 12.0 & -0.03 & -0.223 & 0.0067 \\
\textbf{26--35} & 25.0 & 22.0 & -0.03 & -0.128 & 0.0038 \\
\textbf{36--45} & 20.0 & 18.0 & -0.02 & -0.105 & 0.0021 \\
\textbf{46--55} & 18.0 & 20.0 & +0.02 & +0.105 & 0.0021 \\
\textbf{56--65} & 12.0 & 15.0 & +0.03 & +0.223 & 0.0067 \\
\textbf{66+} & 10.0 & 13.0 & +0.03 & +0.262 & 0.0079 \\
\end{longtable}

\textbf{Total PSI}: \(0.029\) (Stable). Even though specific bins
shifted by 3\%, the aggregate drift is well below the 0.1 warning
threshold. This calculation prevents false alarms from minor
fluctuations while remaining sensitive to systematic shifts.

\textbf{Data Freshness Monitoring}

Feature stores and data pipelines can become stale without triggering
obvious errors. Listing~\ref{lst-ops-freshness-alert} shows a
configuration that monitors feature freshness and triggers fallback
behavior when data becomes stale.

\begin{codelisting}

\caption{\label{lst-ops-freshness-alert}\textbf{Data Freshness Alert
Configuration}: This configuration monitors the
\texttt{user\_purchase\_history} feature for staleness, alerting
operations teams via PagerDuty and Slack and falling back to default
values when the feature exceeds the maximum allowed age.}

\centering{

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Example freshness alert configuration}
\FunctionTok{feature}\KeywordTok{:}\AttributeTok{ user\_purchase\_history}
\FunctionTok{max\_staleness}\KeywordTok{:}\AttributeTok{ 6h}
\FunctionTok{alert\_channels}\KeywordTok{:}\AttributeTok{ }\KeywordTok{[}\AttributeTok{pagerduty}\KeywordTok{,}\AttributeTok{ slack}\KeywordTok{]}
\FunctionTok{on\_stale}\KeywordTok{:}
\AttributeTok{  }\FunctionTok{action}\KeywordTok{:}\AttributeTok{ fallback\_to\_default}
\AttributeTok{  }\FunctionTok{default\_value}\KeywordTok{:}\AttributeTok{ }\KeywordTok{[]}
\end{Highlighting}
\end{Shaded}

}

\end{codelisting}%

Effective monitoring requires observing the system at multiple levels of
abstraction. Use this checklist to ensure your observability stack is
complete.

\phantomsection\label{callout-checkpointux2a-1.18}
\begin{fbxSimple}{callout-checkpoint}{Checkpoint:}{The Monitoring Stack}
\phantomsection\label{callout-checkpoint*-1.18}

ML monitoring is layered, not monolithic.

\textbf{Layer 1: Infrastructure}

\begin{itemize}
\tightlist
\item[$\square$]
  \textbf{Utilization}: Are GPUs at 0\% or 100\%? (Both are bad).
\item[$\square$]
  \textbf{Throughput}: Is QPS steady?
\end{itemize}

\textbf{Layer 2: Data}

\begin{itemize}
\tightlist
\item[$\square$]
  \textbf{Freshness}: Is the data stale? (A common silent killer).
\item[$\square$]
  \textbf{Drift}: Has the input distribution shifted? (Use PSI or KL
  Divergence).
\end{itemize}

\textbf{Layer 3: Model}

\begin{itemize}
\tightlist
\item[$\square$]
  \textbf{Accuracy}: Is the model still right? (Requires ground truth
  labels).
\item[$\square$]
  \textbf{Bias}: Is the model failing for specific subgroups? (Requires
  cohort analysis).
\end{itemize}

\end{fbxSimple}

\textbf{Upstream Dependency Health}

Monitor the health of data sources that feed the ML system: database
replication lag, API endpoint availability, and ETL job completion
status. A recommendation system that detected a 15\% shift in
\texttt{user\_lifetime\_value} distribution within 48 hours traced the
issue to a database migration that changed aggregation logic. Without
data quality monitoring, this would have degraded recommendations for
weeks before accuracy metrics detected the problem.

\paragraph{Monitoring Cost
Model}\label{sec-machine-learning-operations-mlops-monitoring-cost-model-7fe3}

Observability infrastructure incurs costs that scale with monitoring
granularity. Understanding these costs enables rational decisions about
monitoring depth versus budget constraints.

\textbf{Cost Components}

Monitoring costs break down into four categories:

\[\text{Monitoring Cost} = C_{\text{ingest}} + C_{\text{storage}} + C_{\text{compute}} + C_{\text{alert}}\]

Table~\ref{tbl-monitoring-cost-components} provides typical unit costs
for each component:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2128}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3723}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4149}}@{}}
\caption{\textbf{Monitoring Cost Components.} Costs scale differently
across components. Metric ingestion scales with cardinality (number of
unique metric series), while storage scales with retention. Query costs
scale with dashboard usage
patterns.}\label{tbl-monitoring-cost-components}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Component}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Typical Unit Cost}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Scaling Factor}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Component}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Typical Unit Cost}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Scaling Factor}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Metric Ingestion} & \$0.10-0.50 per million data points & Number
of metrics × sample rate \\
\textbf{Log Storage} & \$0.50-2.00 per GB/month & Log verbosity ×
retention period \\
\textbf{Query Compute} & \$0.01-0.05 per query & Dashboard refresh rate
× users \\
\textbf{Alert Evaluation} & \$0.001-0.01 per evaluation & Number of
alert rules × check frequency \\
\end{longtable}

Translating these unit costs into a concrete budget estimate clarifies
the real expense of monitoring even a single production model.

\phantomsection\label{callout-exampleux2a-1.19}
\begin{fbxSimple}{callout-example}{Example:}{Single-Model Monitoring Budget}
\phantomsection\label{callout-example*-1.19}
Consider monitoring a single ML Node (one production model) with:

\begin{itemize}
\tightlist
\item
  1 model with 3 deployment variants (production, canary, staging), each
  emitting 50 metrics
\item
  Metrics sampled every 15 seconds
\item
  30-day retention requirement
\item
  2 dashboards (model health, infrastructure), 3 team members, 5-minute
  refresh
\end{itemize}

\textbf{Metric ingestion:}

\begin{itemize}
\tightlist
\item
  Data points per month: 3 × 50 × (4/min × 60 × 24 × 30) = 26 million
\item
  Cost at \$0.30/million: \textbf{\$8/month}
\end{itemize}

\textbf{Storage:}

\begin{itemize}
\tightlist
\item
  At 8 bytes/point compressed: 26M × 8 = 0.2 GB
\item
  Cost at \$1.00/GB: \textbf{\$0.21/month}
\end{itemize}

\textbf{Query compute:}

\begin{itemize}
\tightlist
\item
  Queries per month: 2 dashboards × 3 users × (12/hr × 8 hours × 22
  days) = 12,672
\item
  Cost at \$0.02/query: \textbf{\$253/month}
\end{itemize}

\textbf{Total:} \textasciitilde\$261/month for a single ML Node

This scales linearly. Platform teams managing 50+ models face additional
constraints where query cost optimization becomes critical.

\end{fbxSimple}

\textbf{Cost Optimization Strategies}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Reduce metric cardinality}: High-cardinality labels (user\_id,
  request\_id) explode storage costs. Use sampling or aggregation for
  high-cardinality dimensions.
\item
  \textbf{Implement metric tiering}: Store high-resolution data (15s)
  for 24 hours, downsample to 1-minute for 7 days, 5-minute for 30 days.
\item
  \textbf{Optimize dashboard refresh}: Default to 5-minute refresh for
  non-critical dashboards. Auto-pause dashboards when tabs are inactive.
\item
  \textbf{Alert strategically}: Consolidate related alerts. Use
  multi-condition alerts rather than multiple single-condition alerts.
\end{enumerate}

\textbf{Cost-Benefit Framework}

Justify monitoring investments against incident costs:

\[\text{Monitoring ROI} = \frac{\text{Incidents Prevented} \times \text{Avg Incident Cost}}{\text{Annual Monitoring Cost}}\]

If average incident costs \$50,000 (downtime + engineering time +
reputation) and monitoring prevents 5 incidents annually at \$50,000
monitoring cost:

\[\text{ROI} = \frac{5 \times \$50\,000}{\$50\,000} = 5\times\]

This framework helps justify monitoring investments and prioritize which
metrics deserve fine-grained observation versus coarse sampling.

The monitoring systems themselves require resilience planning to prevent
operational blind spots. When primary monitoring infrastructure fails
(Prometheus experiencing downtime or Grafana becoming unavailable),
teams risk operating blind during critical periods. Production-grade
MLOps implementations therefore maintain redundant monitoring pathways:
secondary metric collectors that activate during primary system
failures, local logging that persists when centralized systems fail, and
heartbeat checks that detect monitoring system outages.

Some organizations implement cross-monitoring where separate
infrastructure monitors the monitoring systems themselves, ensuring that
observation failures trigger immediate alerts through alternative
channels such as PagerDuty or direct notifications. This
defense-in-depth approach prevents the catastrophic scenario where both
models and their monitoring systems fail simultaneously without
detection. Multi-region and distributed ML deployments introduce
additional monitoring coordination challenges, including consensus-based
alerting, distributed circuit breakers, and cross-region metric
aggregation, that go beyond single-node scope.

Distributed circuit breakers\sidenote{\textbf{Circuit Breaker Pattern}:
Automatic failure detection mechanism that prevents cascade failures by
``opening'' when error rates exceed thresholds (typically 50\% over 10
seconds), routing traffic away from failing services. Originally
inspired by electrical circuit breakers, the pattern prevents one
failing ML model from overwhelming downstream services. Netflix's
Hystrix processes 20+ billion requests daily using circuit breakers,
with typical recovery times of 30-60 seconds. } automatically prevent
cascade failures by routing traffic away from failing services when
error rates exceed thresholds.

\subsubsection{Incident Response for ML
Systems}\label{sec-machine-learning-operations-mlops-incident-response-ml-systems-c637}

Monitoring detects problems; incident response resolves them. When
monitoring detects anomalies, structured response processes guide
resolution. ML incidents differ from traditional software incidents
because symptoms often manifest as accuracy degradation rather than
explicit errors. This distinction requires specialized response
frameworks that account for the probabilistic nature of ML systems.

Severity classification provides the foundation for prioritizing
incident response. Table~\ref{tbl-incident-severity} defines four
priority levels with associated response times, from P0 complete
failures requiring 15-minute response to P3 minor anomalies allowing
24-hour investigation.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.0947}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.4105}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1789}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3158}}@{}}
\caption{\textbf{Incident Severity Classification for ML Systems.}
Response times reflect the urgency and potential business impact of each
severity level.}\label{tbl-incident-severity}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Level}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Criteria}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Response Time}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Example}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Level}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Criteria}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Response Time}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Example}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{P0} & Complete model failure, serving errors & 15 minutes &
Model returns null predictions \\
\textbf{P1} & Significant accuracy degradation (\textgreater10\%) & 1
hour & Recommendation CTR drops 15\% \\
\textbf{P2} & Moderate drift, localized impact & 4 hours & One feature
shows PSI \textgreater{} 0.3 \\
\textbf{P3} & Minor anomalies, no user impact & 24 hours & Training
pipeline delay \\
\end{longtable}

The incident response process follows a structured checklist. First,
detection determines which monitoring signal triggered the alert.
Second, impact assessment quantifies what percentage of traffic is
affected. Third, responders review recent changes to identify whether
any models, features, or data pipelines were deployed. Fourth,
mitigation options are evaluated, including rollback, fallback
enablement, or traffic reduction. Finally, root cause analysis
determines whether the issue stems from the model, data, or
infrastructure.

For P0 and P1 incidents, postmortem documentation is required. These
postmortems must include timeline, root cause, user impact, and
preventive measures. ML-specific elements include identifying which
monitoring gap allowed the issue to reach production and what validation
would have caught it earlier.

\subsubsection{Model Debugging: From Detection to
Diagnosis}\label{sec-machine-learning-operations-mlops-model-debugging-detection-diagnosis-4992}

Incident response triages and mitigates; debugging identifies root
causes. Monitoring detects that something is wrong; debugging determines
\emph{why}. ML debugging differs fundamentally from traditional software
debugging because failures are probabilistic rather than deterministic.
A model producing incorrect predictions does not throw exceptions or
generate stack traces, making systematic debugging approaches essential
for resolving ML incidents efficiently.

\textbf{The Debugging Decision Tree}

When model performance degrades, work through these diagnostic questions
in order:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Is it the data?} Check for upstream data pipeline failures,
  schema changes, missing values, or distribution shifts. Most
  production ML issues (60-80\%) originate in data.
\item
  \textbf{Is it training-serving skew?} Compare feature distributions
  between training and production. Use the KS statistic or PSI to
  identify divergent features.
\item
  \textbf{Is it a specific subpopulation?} Slice performance by key
  dimensions (geography, device type, user segment). Degradation
  localized to one slice suggests a data coverage or labeling issue.
\item
  \textbf{Is it temporal?} Plot performance over time. Sudden drops
  indicate deployment or data issues; gradual decline suggests concept
  drift.
\item
  \textbf{Is it the model?} Only after eliminating data issues, examine
  model behavior through prediction analysis and feature attribution.
\end{enumerate}

\textbf{Slice Analysis}

Performance metrics aggregated across all traffic can mask significant
problems in subpopulations. Table~\ref{tbl-slice-analysis-example}
illustrates how overall accuracy can hide severe degradation in specific
segments:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2899}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1884}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1739}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3478}}@{}}
\caption{\textbf{Slice Analysis Example.} Overall accuracy of 91\%
appears acceptable, but tablet users (5\% of traffic) experience 62\%
accuracy, a severe degradation masked by aggregation. Effective
debugging requires systematic slice analysis across key
dimensions.}\label{tbl-slice-analysis-example}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{User Segment}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Traffic \%}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Accuracy}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Impact}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{User Segment}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Traffic \%}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Accuracy}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Impact}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Desktop users} & 45\% & 94\% & Nominal \\
\textbf{Mobile (iOS)} & 30\% & 92\% & Nominal \\
\textbf{Mobile (Android)} & 20\% & 88\% & Minor degradation \\
\textbf{Tablet users} & \textbf{5\%} & \textbf{62\%} &
\textbf{Severe---investigate} \\
\textbf{Overall} & \textbf{100\%} & \textbf{91\%} & \textbf{Masks tablet
problem} \\
\end{longtable}

\textbf{Feature Attribution for Debugging}

When slice analysis identifies a problematic segment, feature
attribution techniques help identify \emph{which} features drive
incorrect predictions. Listing~\ref{lst-ops-shap-debugging} demonstrates
a workflow that uses SHAP values to analyze mispredictions within a
specific slice.

\begin{codelisting}

\caption{\label{lst-ops-shap-debugging}\textbf{SHAP-Based Debugging
Workflow}: This code filters mispredicted examples from a problematic
slice (tablet users), computes SHAP values to explain model decisions,
and generates a summary plot revealing which features contribute most to
the errors.}

\centering{

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# SHAP{-}based debugging workflow}
\ImportTok{import}\NormalTok{ shap}

\CommentTok{\# Select mispredicted examples from problematic slice}
\NormalTok{errors }\OperatorTok{=}\NormalTok{ predictions[}
\NormalTok{    (predictions.actual }\OperatorTok{!=}\NormalTok{ predictions.predicted)}
    \OperatorTok{\&}\NormalTok{ (predictions.device\_type }\OperatorTok{==} \StringTok{"tablet"}\NormalTok{)}
\NormalTok{]}

\CommentTok{\# Compute SHAP values for error cases}
\NormalTok{explainer }\OperatorTok{=}\NormalTok{ shap.Explainer(model)}
\NormalTok{shap\_values }\OperatorTok{=}\NormalTok{ explainer(errors[feature\_columns])}

\CommentTok{\# Identify features with high attribution for errors}
\NormalTok{shap.summary\_plot(shap\_values, errors[feature\_columns])}
\end{Highlighting}
\end{Shaded}

}

\end{codelisting}%

Common findings from feature attribution debugging include: stale
features (feature store not updating for specific segments), missing
feature coverage (features undefined for edge cases), and feature
distribution shift (feature semantics changed in production).

Beyond aggregate feature importance, individual predictions sometimes
require deeper investigation.

\textbf{Counterfactual Analysis}

For individual mispredictions, counterfactual analysis identifies
minimal changes that would flip the prediction:

\begin{itemize}
\tightlist
\item
  ``If \texttt{session\_duration} were 45 seconds instead of 12 seconds,
  the model would predict `engaged' instead of `churned'.''
\end{itemize}

This reveals which feature boundaries drive decisions and whether those
boundaries make semantic sense. Counterfactuals that require implausible
changes (``user age would need to be -5 years'') often indicate feature
engineering problems.

These techniques (decision trees, slice analysis, feature attribution,
and counterfactuals) form a debugging toolkit. To apply them
consistently, teams codify the process.

\textbf{Debugging Checklist}

For systematic debugging, maintain a runbook with these steps:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Reproduce}: Can you reproduce the failure on held-out data? If
  not, the issue may be data-dependent.
\item
  \textbf{Isolate}: Identify the minimal input set that triggers the
  failure.
\item
  \textbf{Bisect}: If recent deployments, identify which change
  introduced the regression.
\item
  \textbf{Attribute}: Use feature importance to identify contributing
  factors.
\item
  \textbf{Validate}: Confirm the root cause by demonstrating that fixing
  it resolves the issue.
\item
  \textbf{Prevent}: Add monitoring or validation to catch similar issues
  earlier.
\end{enumerate}

Debugging ML systems requires both systematic methodology and domain
expertise. The most effective debugging often comes from engineers who
understand both the model architecture and the business context of the
predictions.

\subsubsection{On-Call Practices for ML
Systems}\label{sec-machine-learning-operations-mlops-oncall-practices-ml-systems-5191}

Debugging resolves individual incidents; on-call practices sustain
operational health over time. On-call rotation for ML systems requires
specialized practices beyond traditional software operations, since ML
incidents often manifest as gradual degradation rather than hard
failures. A traditional software engineer responding to an alert can
typically trace a stack trace to a root cause within minutes. An ML
engineer facing a 3\% accuracy drop must first determine whether the
change represents statistical noise, legitimate concept drift, or a
critical failure requiring immediate rollback---a distinction that
demands statistical context rather than simple log analysis.

This ambiguity compounds with delayed impact visibility. Unlike latency
spikes that surface immediately in dashboards, ML degradation may take
hours or days to manifest in business metrics. A recommendation model
that began serving slightly worse suggestions on Monday might not
produce measurable revenue impact until Friday, by which time the window
for easy diagnosis has closed. Cross-system dependencies further
complicate response: ML issues often originate in upstream data systems
owned by different teams, requiring coordination across organizational
boundaries during incident response. Perhaps most critically, effective
response demands understanding model behavior, not just infrastructure
health. A database administrator can restart a crashed service without
understanding its business logic, but an ML engineer cannot meaningfully
debug accuracy degradation without understanding the model's feature
dependencies and expected behavior patterns.

These challenges motivate tiered escalation structures that match
expertise to incident complexity. Table~\ref{tbl-oncall-structure}
illustrates a recommended on-call structure for ML teams, where primary
responders handle routine issues using standardized runbooks while
escalation paths connect to specialists capable of deeper investigation.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2222}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2778}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.5000}}@{}}
\caption{\textbf{ML On-Call Structure.} Tiered escalation with parallel
data on-call enables efficient incident response. Tier 1 handles routine
issues using runbooks; Tier 2 addresses complex debugging; Tier 3
manages critical incidents requiring architectural
decisions.}\label{tbl-oncall-structure}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Tier}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Responder}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Responsibility}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Tier}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Responder}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Responsibility}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Tier 1} & ML Engineer & Initial triage, standard runbooks, \\
\textbf{(Primary)} & & escalation decisions \\
\textbf{Tier 2} & Senior ML Engineer / & Complex debugging,
cross-system \\
\textbf{(Escalation)} & Data Scientist & investigation, model-specific
issues \\
\textbf{Tier 3} & ML Platform Lead & Architecture decisions, major \\
\textbf{(Critical)} & & incidents, vendor escalation \\
\textbf{Data On-Call} & Data Engineer & Data pipeline issues, feature
store \\
\textbf{(Parallel)} & & problems, upstream dependencies \\
\end{longtable}

The parallel data on-call role deserves particular attention. Since data
issues cause the majority of ML incidents, having a data engineer
available alongside the ML on-call dramatically reduces
time-to-resolution for upstream problems. Without this parallel
structure, ML engineers waste hours investigating model behavior only to
discover that the root cause lies in a data pipeline they cannot access
or modify.

Effective on-call depends heavily on runbook quality. Every production
ML model should have documentation covering the model's purpose,
ownership, and business criticality alongside its normal operating
parameters---expected latency, throughput, and accuracy ranges that
define healthy behavior. Historical incidents and their resolutions
provide templates for common failure patterns, while diagnostic commands
enable rapid health assessment: how to check recent predictions, feature
distributions, and model confidence scores. Critically, runbooks must
specify escalation criteria (when to wake up Tier 2 versus when to
rollback without approval) and rollback procedures with step-by-step
instructions and expected recovery times. Runbooks written during calm
periods save critical minutes during 3 AM incidents.

Even well-designed monitoring can generate excessive alerts that erode
on-call effectiveness. Alert fatigue---the tendency to ignore or dismiss
alerts after experiencing too many false positives---represents a
significant operational risk. Teams combat fatigue through
consolidation, grouping related alerts so that multiple features
drifting simultaneously generate a single notification rather than
dozens. Adaptive thresholds that account for weekly and seasonal
patterns prevent predictable variations from triggering unnecessary
pages. Measuring alert actionability provides empirical guidance: alerts
acted upon less than 10\% of the time should be retired or recalibrated.
When temporary silencing is necessary, accountability
mechanisms---requiring a follow-up ticket before snoozing---prevent
alerts from being permanently ignored.

Shift handoffs represent another critical practice that distinguishes
mature operations. Incoming on-call engineers need context about active
incidents and their current status, recent deployments that might cause
delayed issues, upcoming scheduled changes such as data migrations or
model updates, and any alerts that were suppressed along with the
reasoning. Without structured handoffs, context is lost between shifts,
and incoming engineers waste time rediscovering information their
predecessors already gathered.

Finally, sustainable on-call practices must address burnout. ML on-call
carries particular stress due to incident ambiguity---the uncertainty of
not knowing whether an alert represents a real problem demands constant
vigilance. Organizations mitigate burnout by limiting consecutive
on-call days (typically three to four), providing compensatory time off
after high-severity incidents, conducting regular rotation reviews to
balance load across team members, and investing in automation that
reduces toil. The goal is not merely to staff on-call rotations but to
make them sustainable over years of operation.

The monitoring, incident response, and debugging practices examined
above form the technical backbone of production ML operations. Technical
capabilities alone do not ensure operational success, however. The most
sophisticated monitoring dashboards fail if no one is responsible for
acting on alerts, and the most detailed runbooks languish if team
structures do not support their use. Production ML operations require
organizational infrastructure that parallels the technical
infrastructure: clear governance frameworks, defined roles and
responsibilities, and communication patterns that enable
cross-functional coordination.

\subsection{Model Governance and Team
Coordination}\label{sec-machine-learning-operations-mlops-model-governance-team-coordination-d86f}

Successful MLOps implementation requires robust governance frameworks
and effective collaboration across diverse teams and stakeholders. This
section examines model governance principles that ensure transparency
and accountability, cross-functional collaboration strategies that
bridge technical and business teams, and stakeholder communication
approaches that align expectations and facilitate decision-making.

\subsubsection{Model
Governance}\label{sec-machine-learning-operations-mlops-model-governance-363c}

On-call practices address operational emergencies, but production ML
also requires proactive governance. Governance encompasses the policies,
practices, and tools ensuring that ML models operate transparently,
fairly, and in compliance with ethical and regulatory standards. Without
proper governance, deployed models may produce biased or opaque
decisions, creating significant legal, reputational, and societal risks.

Governance begins during model development, where teams implement
techniques to increase transparency and explainability. Methods such as
SHAP (\citeproc{ref-shap_github}{Biecek and Burzykowski
2021})\sidenote{\textbf{SHAP (SHapley Additive exPlanations)}: Named
after Lloyd Shapley, who developed the game-theoretic concept of fair
value distribution in 1953 (earning a Nobel Prize in 2012). Shapley
values calculate each player's ``fair'' contribution to a coalition's
total payoff. Lundberg and Lee adapted this framework for ML in 2017,
treating features as ``players'' and prediction impact as the
``payoff.'' SHAP explanations add 10-500 ms latency per prediction, but
40\% of enterprise ML teams use them, with Microsoft reporting \$2M in
bias-related risk identification in hiring models. } and LIME
(\citeproc{ref-lime_github}{Ribeiro, Singh, and Guestrin
2016})\sidenote{\textbf{LIME (Local Interpretable Model-agnostic
Explanations)}: Perturbation-based explanation method introduced by
Ribeiro, Singh, and Guestrin in 2016. Unlike SHAP's gradient-based
approach, LIME generates explanations by sampling perturbed inputs
around a prediction and fitting a local linear model. This makes LIME
faster than SHAP for complex models (typically 50-200~ms vs.~100-500~ms)
but produces less stable explanations. For production systems, LIME's
computational efficiency makes it suitable for real-time explanations,
while SHAP's theoretical consistency is preferred for compliance audits.
} offer post hoc explanations of model predictions by identifying which
input features were most influential in a particular decision. These
interpretability techniques complement security measures that address
how to protect both model integrity and data privacy in production
environments. They allow auditors, developers, and non-technical
stakeholders to better understand how and why a model behaves the way it
does.

In addition to interpretability, fairness is a central concern in
governance. Governance encompasses fairness and bias monitoring to
ensure equitable treatment across user groups. The specific fairness
metrics and bias detection techniques are examined in
\textbf{?@sec-responsible-engineering}; MLOps provides the
infrastructure to implement these checks throughout the deployment
lifecycle, including pre-deployment audits that evaluate fairness,
robustness, and overall model behavior before a system is put into
production.

Governance also extends into the post-deployment phase. As introduced in
the previous section on monitoring, teams must track for concept drift,
where the statistical relationships between features and labels evolve
over time. Such drift can undermine the fairness or accuracy of a model,
particularly if the shift disproportionately affects a specific
subgroup. By analyzing logs and user feedback, teams can identify
recurring failure modes, unexplained model outputs, or emerging
disparities in treatment across user segments.

Supporting this lifecycle approach to governance are platforms and
toolkits that integrate governance functions into the broader MLOps
stack. For example, Watson OpenScale
(\citeproc{ref-watson_openscale}{IBM 2024}) provides built-in modules
for explainability, bias detection, and monitoring. These tools allow
governance policies to be encoded as part of automated pipelines,
ensuring that checks are consistently applied throughout development,
evaluation, and production.

Governance focuses on three core objectives: transparency
(interpretable, auditable models), fairness (equitable treatment across
user groups), and compliance (alignment with legal and organizational
policies). Embedding governance throughout the MLOps lifecycle
transforms machine learning into a trustworthy system serving societal
and organizational goals.

\subsubsection{Cross-Functional
Collaboration}\label{sec-machine-learning-operations-mlops-crossfunctional-collaboration-9f0d}

Governance frameworks establish policies; cross-functional collaboration
implements them. Machine learning systems are developed and maintained
by multidisciplinary teams: data scientists, ML engineers, software
developers, infrastructure specialists, product managers, and compliance
officers. MLOps fosters cross-functional integration by introducing
shared tools, processes, and artifacts that promote transparency and
coordination across the ML lifecycle.

Collaboration begins with consistent tracking of experiments, model
versions, and metadata. Tools such as \href{https://mlflow.org/}{MLflow}
(\citeproc{ref-zaharia2018accelerating}{A. Chen et al. 2020}) provide a
structured environment for logging experiments, capturing parameters,
recording evaluation metrics, and managing trained models through a
centralized registry. This registry serves as a shared reference point
for all team members, enabling reproducibility and easing handoff
between roles. Integration with version control systems such as GitHub
(\citeproc{ref-github}{Lima et al. 2023}) and GitLab
(\citeproc{ref-gitlab}{0001, Boer, and Kurnatowski 2021}) further
streamlines collaboration by linking code changes with model updates and
pipeline triggers.

In addition to tracking infrastructure, teams benefit from platforms
that support exploratory collaboration. Weights \& Biases
(\citeproc{ref-wandb}{Selvan and Potdar 2024}) is one such platform that
allows data scientists to visualize experiment metrics, compare training
runs, and share insights with peers. Features such as live dashboards
and experiment timelines facilitate discussion and decision-making
around model improvements, hyperparameter tuning, or dataset
refinements. These collaborative environments reduce friction in model
development by making results interpretable and reproducible across the
team.

Beyond model tracking, collaboration also depends on shared
understanding of data semantics and usage. Establishing common data
contexts, by means of glossaries, data dictionaries, schema references,
and lineage documentation, ensures that all stakeholders interpret
features, labels, and statistics consistently. This is particularly
important in large organizations, where data pipelines may evolve
independently across teams or departments.

For example, a data scientist working on an anomaly detection model may
use Weights \& Biases to log experiment results and visualize
performance trends. These insights are shared with the broader team to
inform feature engineering decisions. Once the model reaches an
acceptable performance threshold, it is registered in MLflow along with
its metadata and training lineage. This allows an ML engineer to pick up
the model for deployment without ambiguity about its provenance or
configuration.

By integrating collaborative tools, standardized documentation, and
transparent experiment tracking, MLOps removes communication barriers
that have traditionally slowed down ML workflows. It enables distributed
teams to operate cohesively, accelerating iteration cycles and improving
the reliability of deployed systems.

\textbf{ML Team Roles and Responsibilities.} Effective MLOps requires
clear role definitions that align expertise with responsibilities. While
titles vary across organizations, five core roles emerge consistently in
ML teams. Table~\ref{tbl-ml-roles-matrix} maps these roles to their
primary responsibilities:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.0950}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2941}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2670}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3439}}@{}}
\caption{\textbf{ML Team Roles Matrix.} Clear role boundaries prevent
gaps and overlaps. Data Scientists focus on model quality while ML
Engineers handle productionization. Data Engineers own data pipelines
while Platform Engineers own MLOps tooling. SREs ensure overall system
reliability.}\label{tbl-ml-roles-matrix}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Role}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Primary Focus}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Key Deliverables}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Collaboration Points}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Role}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Primary Focus}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Key Deliverables}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Collaboration Points}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Data Scientist} & Model development, experimentation, algorithm
selection & Trained models, experiment results, performance benchmarks &
Hands off to ML Engineer for productionization \\
\textbf{ML Engineer} & Production ML systems, training pipelines,
serving infrastructure & Deployed models, training pipelines, serving
systems & Receives from Data Scientist; works with Platform Engineer on
infrastructure \\
\textbf{Data Engineer} & Data pipelines, feature engineering, data
quality & Feature pipelines, data quality systems, feature stores &
Provides data to Data Scientist; maintains feature store for ML
Engineer \\
\textbf{Platform Engineer} & MLOps infrastructure, tooling, automation &
CI/CD pipelines, monitoring systems, compute infrastructure & Enables ML
Engineer; maintains shared infrastructure \\
\textbf{DevOps/SRE} & Reliability, incident response, system health &
SLOs/SLAs, on-call procedures, runbooks & Supports all roles; owns
production health \\
\end{longtable}

Clear role definitions matter most at handoff points, where work
transitions between specialists. The most failure-prone handoff occurs
between Data Scientists and ML Engineers---the infamous ``notebook to
production'' gap. A model that performs well in a Jupyter notebook may
fail in production due to undocumented preprocessing steps, hardcoded
file paths, or dependencies on the Data Scientist's local environment.
Organizations mitigate this gap through standardized model interfaces
that define expected inputs and outputs, required documentation that
captures feature engineering logic and training procedures, and
reproducibility requirements that must be verified before handoff.
Without these guardrails, ML Engineers spend weeks reverse-engineering
notebooks rather than productionizing models.

The handoff from ML Engineers to SREs presents different challenges.
Before SREs accept operational responsibility for a model, production
readiness reviews should verify that monitoring dashboards exist,
alerting rules are configured with appropriate thresholds, runbooks
document common failure scenarios and their resolutions, and rollback
procedures have been tested. SREs cannot effectively support systems
they do not understand, and ML Engineers cannot provide 24/7 coverage
indefinitely. Formal handoff criteria protect both teams.

Data Engineers hand off to the broader ML team through feature
contracts---formal specifications of schema, freshness SLOs, and quality
guarantees. When a Data Engineer needs to modify a feature pipeline, the
contract identifies which downstream models depend on the affected
features and what coordination is required. Breaking changes demand
explicit communication rather than silent updates that surface as
mysterious model degradation weeks later.

Some organizations, particularly startups, attempt to avoid these
handoff complexities by expecting individuals to span all roles---the
``full-stack ML engineer'' who handles everything from data pipelines to
model serving. While appealing for small teams seeking to minimize
coordination overhead, this approach creates expertise gaps in critical
areas like security and reliability, burnout from constant
context-switching between fundamentally different types of work, and
insufficient depth in any single area to handle complex problems. A
single engineer can prototype across roles during early experimentation,
but production systems benefit from specialization. Small teams that
cannot afford full specialization should explicitly acknowledge their
coverage gaps and prioritize based on risk---perhaps accepting weaker
data infrastructure while investing heavily in serving reliability for
customer-facing applications.

Team structure naturally evolves with organizational maturity and model
portfolio size. Organizations operating one to three models typically
rely on generalist ML Engineers who cover most responsibilities; Data
Scientists may even deploy their own models with minimal handoff. As the
portfolio grows to three to ten models, specialization becomes
necessary: dedicated Data Engineers own feature pipelines while ML
Engineers focus on serving infrastructure. Beyond ten models, a platform
team typically emerges to provide shared infrastructure---experiment
tracking, model registries, deployment pipelines---that individual
product teams consume. Large organizations often embed ML Engineers
within product teams while maintaining a central platform that provides
common tooling and best practices.

Effective MLOps extends beyond internal team coordination, however, to
encompass the broader communication challenges that arise when technical
teams interface with business stakeholders.

\subsubsection{Stakeholder
Communication}\label{sec-machine-learning-operations-mlops-stakeholder-communication-215c}

Cross-functional collaboration addresses coordination within technical
teams; stakeholder communication bridges technical and business domains.
Effective MLOps extends beyond technical implementation to encompass
strategic communication when translating machine learning realities into
business language. Unlike deterministic software, machine learning
systems exhibit probabilistic performance, data dependencies, and
degradation patterns that stakeholders often find counterintuitive.

The most common communication challenge emerges from oversimplified
improvement requests. Product managers frequently propose ``make the
model more accurate'' without understanding underlying trade-offs.
Effective communication reframes such requests by presenting concrete
options: improving accuracy from 85\% to 87\% might require collecting
four times more training data over three weeks while doubling inference
latency from 50 to 120 ms. Articulating specific constraints transforms
vague requests into informed business decisions.

Translating technical metrics into business impact requires consistent
frameworks connecting model performance to operational outcomes. A 5\%
accuracy improvement appears modest in isolation, but contextualizing
this as ``reducing false fraud alerts from 1,000 to 800 daily customer
friction incidents'' provides actionable business context.

This connection is not linear. As Figure~\ref{fig-business-cost-curve}
illustrates, the optimal operating point for a model is rarely the point
of highest accuracy. It is the point where the combined cost of False
Positives (e.g., blocking a legitimate user) and False Negatives (e.g.,
missing fraud) is minimized.

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{contents/vol1/ops/ops_files/figure-pdf/fig-business-cost-curve-output-1.pdf}}

}

\caption{\label{fig-business-cost-curve}\textbf{The Business Cost
Curve.} Expected Cost vs.~Classification Threshold. Technical metrics
like ROC curves hide the economic reality: errors have different costs.
In this fraud detection scenario, a False Negative (missed fraud) costs
\$1000, while a False Positive (blocked user) costs
\(10. The optimal threshold (\)T=0.85\$) is shifted far to the right to
minimize total cost, even if it reduces aggregate accuracy. MLOps is the
discipline of tuning this threshold dynamically as costs change.}

\end{figure}%

Incident communication presents another critical challenge. When models
degrade or require rollbacks, maintaining stakeholder trust depends on
clear categorization: temporary performance fluctuations as normal
variation, data drift as planned maintenance requirements, and system
failures demanding immediate rollback. Regular performance reporting
cadences preemptively address reliability concerns.

Resource justification requires translating technical requirements into
business value. Rather than requesting ``8 A100 GPUs for model
training,'' effective communication frames investments as
``infrastructure to reduce experiment cycle time from 2 weeks to 3 days,
enabling 4x faster feature iteration.'' Timeline estimation must account
for realistic proportions: data preparation typically consumes 60\% of
project duration, model development 25\%, and deployment monitoring
15\%.

Consider a fraud detection team implementing model improvements. When
stakeholders request enhanced accuracy, the team responds with a
structured proposal: increasing detection rates from 92\% to 94\%
requires integrating external data sources, extending training duration
by two weeks, and accepting 30\% higher infrastructure costs, but would
prevent an estimated \$2 million in annual fraud losses while reducing
false positive alerts affecting 50,000 customers monthly.

Through disciplined stakeholder communication, MLOps practitioners
maintain organizational support while establishing realistic
expectations about system capabilities. This communication competency is
as essential as technical expertise for sustaining successful ML
operations.

Before examining system design and maturity frameworks,
Table~\ref{tbl-technical-debt-summary} consolidates the debt patterns
discussed throughout this chapter, providing a reference for the
assessment rubric that follows.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1159}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2609}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2947}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3285}}@{}}
\caption{\textbf{Technical Debt Patterns.} Machine learning systems
accumulate distinct forms of technical debt from data dependencies,
model interactions, and evolving operational contexts. Primary debt
patterns, their causes, symptoms, and recommended mitigation strategies
guide practitioners in recognizing and addressing these challenges
systematically.}\label{tbl-technical-debt-summary}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Debt Pattern}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Primary Cause}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Key Symptoms}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Mitigation Strategies}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Debt Pattern}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Primary Cause}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Key Symptoms}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Mitigation Strategies}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Boundary Erosion} & Tightly coupled components, unclear
interfaces & Changes cascade unpredictably, CACHE principle violations &
Enforce modular interfaces, design for encapsulation \\
\textbf{Correction Cascades} & Sequential model dependencies, inherited
assumptions & Upstream fixes break downstream systems, escalating
revisions & Careful reuse vs.~redesign tradeoffs, clear versioning \\
\textbf{Undeclared Consumers} & Informal output sharing, untracked
dependencies & Silent breakage from model updates, hidden feedback loops
& Strict access controls, formal interface contracts, usage
monitoring \\
\textbf{Data Dependency Debt} & Unstable or underutilized data inputs &
Model failures from data changes, brittle feature pipelines & Data
versioning, lineage tracking, leave-one-out analysis \\
\textbf{Feedback Loops} & Model outputs influence future training data &
Self-reinforcing behavior, hidden performance degradation & Cohort-based
monitoring, canary deployments, architectural isolation \\
\textbf{Pipeline Debt} & Ad hoc workflows, lack of standard interfaces &
Fragile execution, duplication, maintenance burden & Modular design,
workflow orchestration tools, shared libraries \\
\textbf{Configuration Debt} & Fragmented settings, poor versioning &
Irreproducible results, silent failures, tuning opacity & Version
control, validation, structured formats, automation \\
\textbf{Early-Stage Debt} & Rapid prototyping shortcuts, tight
code-logic coupling & Inflexibility as systems scale, difficult team
collaboration & Flexible foundations, intentional debt tracking, planned
refactoring \\
\end{longtable}

\subsection{Assessing Technical Debt: The ML Test
Score}\label{sec-machine-learning-operations-mlops-assessing-technical-debt-ml-test-score-0099}

Table~\ref{tbl-technical-debt-summary} consolidates the debt patterns
examined throughout this chapter. Managing this debt requires not just
awareness but systematic assessment. The ML Test Score provides a
production readiness rubric that transforms subjective ``is this system
ready?'' conversations into quantifiable evaluations.

\subsubsection{ML Test Score: A Production Readiness
Rubric}\label{sec-machine-learning-operations-mlops-ml-test-score-production-readiness-rubric-72b1}

The ML Test Score (\citeproc{ref-breck2020ml}{Breck et al. 2017})
provides a systematic rubric for evaluating production readiness across
four categories. Organizations score each test (0 = not implemented, 0.5
= partially implemented, 1 = fully automated), with total scores
indicating maturity: 0-5 (ad hoc), 5-10 (developing), 10-15 (mature),
15+ (production-grade). Table~\ref{tbl-ml-test-score} summarizes the key
tests practitioners should implement:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.1846}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4462}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3692}}@{}}
\caption{\textbf{ML Test Score Checklist.} A practical rubric for
assessing ML system production readiness. Each test scores 0 (not
implemented), 0.5 (partially implemented), or 1 (fully automated).
Systems scoring below 5 require significant investment before production
deployment; scores above 10 indicate mature operational practices. Based
on Breck et al. (\citeproc{ref-breck2020ml}{Breck et al.
2017}).}\label{tbl-ml-test-score}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Category}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Test}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Implementation Example}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Category}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Test}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Implementation Example}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Data Tests} & Feature expectations are captured in schema &
Great Expectations, TFX Data Validation \\
& All features are beneficial (no unused features) & Feature importance
analysis, ablation studies \\
& No feature's cost exceeds its benefit & Latency/accuracy tradeoff
analysis \\
& Data pipeline has appropriate privacy controls & PII detection, access
logging \\
\textbf{Model Tests} & Model spec is reviewed and checked into version
control & Git-tracked model configs \\
& Offline and online metrics are correlated & A/B test validation of
offline improvements \\
& All hyperparameters are tuned & Automated HPO with tracked results \\
& Model staleness is measured and bounded & Performance decay
monitoring \\
\textbf{Infrastructure Tests} & Training is reproducible & Fixed seeds,
versioned data, locked dependencies \\
& Model can be rolled back to previous version & Model registry with
versioning \\
& Training and serving code paths are tested for consistency & Feature
store integration tests \\
& Model quality is validated before serving & Automated validation gates
in CI/CD \\
\textbf{Monitoring Tests} & Dependency changes result in alerts & Data
schema monitoring \\
& Data invariants hold in training and serving & Distribution comparison
tests \\
& Training and serving features are not skewed & Training-serving skew
detection \\
& Model staleness triggers retraining & Automated retraining
pipelines \\
\end{longtable}

\textbf{Interpreting Your Score}:

\begin{itemize}
\tightlist
\item
  \textbf{0-5}: High-risk deployment. Critical gaps in reproducibility,
  monitoring, or validation. Expect frequent incidents and difficulty
  debugging production issues.
\item
  \textbf{5-10}: Developing practices. Basic automation exists but gaps
  remain. Suitable for low-stakes internal applications with active
  engineering support.
\item
  \textbf{10-15}: Mature operations. Most best practices implemented.
  Suitable for customer-facing applications with moderate risk
  tolerance.
\item
  \textbf{15+}: Production-grade. Comprehensive automation, monitoring,
  and validation. Suitable for safety-critical or high-stakes
  applications.
\end{itemize}

Teams should audit their systems quarterly against this rubric,
prioritizing tests that address their most frequent incident types.

The ML Test Score provides a systematic rubric for evaluating whether
individual practices are in place. But production readiness involves
more than checking boxes: it requires understanding how practices
integrate into a coherent system, how teams are organized to execute
them, and how organizations evolve their capabilities over time.
Operational maturity captures this systems-level perspective, describing
not just what practices exist but how well they work together.

\section{System Design and Maturity
Framework}\label{sec-machine-learning-operations-mlops-system-design-maturity-framework-9901}

Operational maturity refers to the degree to which ML workflows are
automated, reproducible, monitored, and aligned with engineering and
governance practices. Early-stage efforts may rely on ad hoc scripts,
but production-scale systems require deliberate design choices that
support long-term sustainability. This section examines how operational
maturity influences system architecture, infrastructure design, and
organizational structure (\citeproc{ref-kreuzberger2022machine}{Paleyes,
Urma, and Lawrence 2022}).

\subsection{Operational
Maturity}\label{sec-machine-learning-operations-mlops-operational-maturity-3d14}

The ML Test Score provides a checklist for assessing individual
practices. Operational maturity captures something broader: the systemic
integration of those practices into a coherent whole, examining how well
a team combines infrastructure, automation, monitoring, governance, and
collaboration across the ML lifecycle.

Low-maturity environments rely on manual workflows and ad hoc
experimentation, suitable for early-stage research but brittle at scale.
High-maturity environments implement modular, versioned, and automated
workflows allowing models to be developed, validated, and deployed in a
controlled fashion. Data lineage is preserved, model behavior is
continuously monitored, and infrastructure is managed as code
(\citeproc{ref-zaharia2018accelerating}{A. Chen et al. 2020}).

Operational maturity centers not on tool adoption but on system
integration: how teams collaborate through shared interfaces,
standardized workflows, and automated handoffs. This integration
distinguishes mature ML systems from loosely connected artifacts.

\subsection{Maturity
Levels}\label{sec-machine-learning-operations-mlops-maturity-levels-71f8}

Operational maturity describes capabilities; maturity levels describe
stages of evolution. Although operational maturity exists on a
continuum, distinguishing broad stages helps illustrate how ML systems
evolve from research prototypes to production-grade infrastructure.

At the lowest level, ML workflows are ad hoc: experiments run manually,
models train on local machines, and deployment involves hand-crafted
scripts. As maturity increases, workflows become structured: teams adopt
version control, automated training pipelines, and centralized model
storage. At the highest levels, systems are fully integrated with
infrastructure-as-code, continuous delivery pipelines, and automated
monitoring that support large-scale deployment and rapid
experimentation.

Table~\ref{tbl-maturity-levels} captures this progression, offering a
system-level framework for analyzing ML operational practices that
emphasizes architectural cohesion and lifecycle integration over tool
selection, guiding the design of scalable and maintainable learning
systems.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.1132}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.5472}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3396}}@{}}
\caption{\textbf{Maturity Progression.} Machine learning operational
practices evolve from manual, fragile workflows toward fully integrated,
automated systems, impacting reproducibility and scalability. Key
characteristics and outcomes at each maturity level emphasize
architectural cohesion and lifecycle integration for building
maintainable learning
systems.}\label{tbl-maturity-levels}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Maturity Level}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{System Characteristics}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Typical Outcomes}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Maturity Level}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{System Characteristics}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Typical Outcomes}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Ad Hoc} & Manual data processing, local training, no version
control, unclear ownership & Fragile workflows, difficult to reproduce
or debug \\
\textbf{Repeatable} & Automated training pipelines, basic CI/CD,
centralized model storage, some monitoring & Improved reproducibility,
limited scalability \\
\textbf{Scalable} & Fully automated workflows, integrated observability,
infrastructure-as-code, governance & High reliability, rapid iteration,
production-grade ML \\
\end{longtable}

\textbf{Concrete Example: Fraud Detection System Across Maturity Levels}

Consider how a fraud detection system evolves across these maturity
levels:

\begin{itemize}
\tightlist
\item
  \textbf{Ad Hoc}: A data scientist trains a model in a Jupyter
  notebook, exports it as a pickle file, and hands it to an engineer who
  deploys it to a single server. When accuracy drops, the data scientist
  retrains manually by running the notebook again with fresh data.
  Debugging requires the original data scientist because no one else
  understands the preprocessing steps.
\item
  \textbf{Repeatable}: The training script is version-controlled, with a
  scheduled Jenkins job that retrains monthly. Features are computed in
  a SQL script that engineering maintains separately. The model is
  deployed via container, with basic accuracy monitoring. When the
  feature SQL changes, the data scientist must manually verify the model
  still works.
\item
  \textbf{Scalable}: Training and serving use the same feature store,
  eliminating skew. A CI/CD pipeline automatically retrains when drift
  exceeds PSI \textgreater{} 0.2, validates the new model against the
  baseline, and deploys via canary release. Monitoring tracks
  per-merchant accuracy, triggering investigation when specific segments
  degrade. The entire lineage from raw data to production prediction is
  auditable.
\end{itemize}

The investment required to move between levels is substantial (typically
3-6 months of engineering effort per transition), but the reduction in
incident frequency and debugging time justifies the cost for
production-critical systems.

These maturity levels provide a systems lens through which to evaluate
ML operations, not in terms of specific tools adopted, but in how
reliably and cohesively a system supports the full machine learning
lifecycle. Understanding this progression prepares practitioners to
identify design bottlenecks and prioritize investments that support
long-term system sustainability.

\subsection{System Design
Implications}\label{sec-machine-learning-operations-mlops-system-design-implications-05a1}

Maturity levels describe organizational stages; system design
implications describe the architectural consequences. As machine
learning operations mature, the underlying system architecture evolves
in response. Operational maturity has direct consequences for how ML
systems are structured, deployed, and maintained. Each level of maturity
introduces new expectations around modularity, automation, monitoring,
and fault tolerance, shaping the design space in both technical and
procedural terms.

In low-maturity environments, ML systems are often constructed around
monolithic scripts and tightly coupled components. Data processing logic
may be embedded directly within model code, and configurations are
managed informally. These architectures, while expedient for rapid
experimentation, lack the separation of concerns needed for
maintainability, version control, or safe iteration. As a result, teams
frequently encounter regressions, silent failures, and inconsistent
performance across environments.

As maturity increases, modular abstractions begin to emerge. Feature
engineering is decoupled from model logic, pipelines are defined
declaratively, and system boundaries are enforced through APIs and
orchestration frameworks. These changes support reproducibility and
enable teams to scale development across multiple contributors or
applications. Infrastructure becomes programmable through configuration
files, and model artifacts are promoted through standardized deployment
stages. This architectural discipline allows systems to evolve
predictably, even as requirements shift or data distributions change.

At high levels of maturity, ML systems exhibit properties commonly found
in production-grade software systems: stateless services,
contract-driven interfaces, environment isolation, and observable
execution. Design patterns such as feature stores, model registries, and
infrastructure-as-code become foundational. System behavior is not
inferred from static assumptions but monitored in real time and adapted
as needed. This enables feedback-driven development and supports
closed-loop systems where data, models, and infrastructure co-evolve.

In each case, operational maturity functions as an architectural force:
it governs how complexity is managed, how change is absorbed, and how
the system scales in the face of threats to service uptime.
Figure~\ref{fig-uptime-iceberg} depicts this dependency stack as an
iceberg, with visible uptime floating above hidden threats including
data drift, concept drift, broken pipelines, schema changes, model bias,
and underperforming segments. Design decisions that disregard these
constraints may function under ideal conditions, but fail under
real-world pressures such as latency requirements, drift, outages, or
regulatory audits. Understanding this relationship between maturity and
design is essential for building resilient machine learning systems that
sustain performance over time.

\begin{figure}[htb]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/2bed64ea684b5546e530624831e0ea4729707cfa.pdf}}

}

\caption{\label{fig-uptime-iceberg}\textbf{Uptime Dependency Stack.} An
iceberg visualization where visible service uptime floats above the
waterline, supported by hidden threats below: model accuracy
degradation, data drift, concept drift, broken pipelines, schema
changes, model bias, data outages, and underperforming segments. Labels
group these threats into data health, model health, and service health
categories.}

\end{figure}%

\subsection{Design Patterns and
Anti-Patterns}\label{sec-machine-learning-operations-mlops-design-patterns-antipatterns-82ad}

System design implications describe technical architecture; design
patterns describe organizational structure. The structure of teams
building and maintaining ML systems significantly determines operational
outcomes. As systems grow in complexity, organizational patterns must
evolve to reflect the interdependence of data, modeling, infrastructure,
and governance.

In mature environments, organizational design emphasizes clear ownership
and interface discipline. Platform teams may take responsibility for
shared infrastructure and CI/CD pipelines while domain teams focus on
model development and business alignment. Interfaces between teams
(feature definitions, data schemas, and deployment targets) are
well-defined and versioned.

One effective pattern is a centralized MLOps team providing shared
services to multiple model development groups. Such structures promote
consistency and reduce duplicated effort. Alternatively, some
organizations adopt a federated model, embedding MLOps engineers within
product teams while maintaining a central architectural function for
system-wide integration.

Anti-patterns emerge when responsibilities are fragmented. The
tool-first approach (adopting infrastructure tools without first
defining processes and roles) results in fragile pipelines and unclear
handoffs. Siloed experimentation, where data scientists operate in
isolation from production engineers, leads to models that are difficult
to deploy or retrain effectively.

Organizational drift presents another challenge: as teams scale,
undocumented workflows become entrenched and coordination costs
increase. Organizational maturity must co-evolve with system complexity
through communication patterns, role definitions, and accountability
structures that reinforce modularity, automation, and observability.

These organizational patterns must be supported by technical
architectures handling the unique reliability challenges of ML systems.
MLOps inherits distributed systems challenges but adds complications
through learning components requiring adaptations for probabilistic
behavior.

Circuit breaker patterns must account for model-specific failure modes,
where prediction accuracy degradation requires different thresholds than
service availability failures. Bulkhead
patterns\sidenote{\textbf{Bulkhead Pattern}: Fault isolation technique
borrowed from ship design, where watertight compartments prevent a hull
breach from sinking the entire vessel. Popularized for software by
Michael Nygard in ``Release It!'' (2007), bulkheads partition system
resources so failures in one component cannot exhaust resources needed
by others. In ML systems, bulkheads isolate experimental models from
production traffic, typically allocating 10-20\% of compute capacity to
canary deployments while reserving the remainder for stable models. }
become critical when isolating experimental model versions from
production traffic. These patterns require resource partitioning
strategies that prevent resource exhaustion in one model from affecting
others. The Byzantine fault tolerance\sidenote{\textbf{Byzantine Fault
Tolerance}: Formalized by Lamport, Shostak, and Pease in 1982, Byzantine
faults occur when system components fail in arbitrary ways, potentially
providing different information to different parts of the system. The
classic result requires 3f+1 nodes to tolerate f Byzantine failures. In
ML systems, ``Byzantine'' behavior manifests as models returning
plausible but incorrect predictions, harder to detect than crashes since
outputs appear valid. Ensemble serving with voting mechanisms provides
partial Byzantine resilience. } problem takes on new characteristics in
MLOps environments, where ``Byzantine'' behavior includes models
producing plausible but incorrect outputs rather than obvious failures.

Traditional consensus algorithms focus on agreement among correct nodes,
but ML systems require consensus about model correctness when ground
truth may be delayed or unavailable. These reliability patterns form the
theoretical foundation distinguishing robust MLOps implementations from
fragile ones.

\subsection{Contextualizing
MLOps}\label{sec-machine-learning-operations-mlops-contextualizing-mlops-194a}

The operational maturity of a machine learning system is realized in
concrete systems with physical, organizational, and regulatory
constraints. The preceding sections outlined best practices for mature
MLOps, but these practices are rarely deployed in pristine environments.
Every ML system operates within a specific context that shapes how
workflows are implemented and adapted.

System constraints may arise from the physical environment: compute,
memory, or power limitations common in edge and embedded systems, or
connectivity limitations that complicate model updates and telemetry
collection. In high-assurance domains such as healthcare, finance, and
industrial control, governance and fail-safety may take precedence over
throughput or latency.

A standard CI/CD pipeline may be infeasible in environments without
direct model host access. Teams must implement alternatives such as
over-the-air updates accounting for reliability and rollback capability.
Monitoring practices may need reimagining using indirect signals or
on-device anomaly detection. Even collecting training data may be
limited by privacy concerns or legal restrictions.

These adaptations are expressions of maturity under constraint. A
well-engineered ML system accounts for the realities of its operating
environment. The chapters ahead on on-device learning, privacy
preservation, safety, and sustainability each introduce system-level
constraints that reshape how ML is practiced at scale.

\subsection{Future Operational
Considerations}\label{sec-machine-learning-operations-mlops-future-operational-considerations-ebd6}

Machine learning deployment requires more than technical correctness; it
demands architectural coherence, organizational alignment, and
operational maturity. Early-stage systems benefit from process
discipline and modular abstraction, while mature systems require
automation, governance, and resilience.

Edge computing, adversarial robustness, and privacy-preserving
deployment each require adaptations of the foundational MLOps principles
established here. Operational maturity is not the end of the ML
lifecycle but rather the foundation for production-grade, responsible,
and adaptive systems.

\textbf{Enterprise-Scale ML Systems.} At the highest levels of
operational maturity, the single-model practices established in this
chapter become building blocks for larger organizational capabilities.
When organizations operate many ML Nodes simultaneously, they often
consolidate into platform architectures (sometimes called ``AI
factories'') that provide shared infrastructure, centralized governance,
and economies of scale. The transition from operating individual ML
Nodes to managing platform-scale ML infrastructure introduces
qualitatively different challenges: cross-model resource allocation,
system-level observability correlating performance across models, and
fault tolerance for interdependent AI systems. These challenges extend
beyond our single-model scope.

For single-model operations, the key insight is that solid ML Node
practices are prerequisite to platform success. Organizations that
attempt to build platforms before mastering individual model operations
typically encounter compounded complexity: every gap in single-model
monitoring, testing, or deployment becomes multiplied across the model
portfolio.

\subsection{Investment and Return on
Investment}\label{sec-machine-learning-operations-mlops-investment-return-investment-8571}

The operational benefits of MLOps are substantial, but implementing
mature practices requires organizational investment. Understanding costs
and returns helps teams make informed decisions about MLOps adoption for
their ML Node.

\textbf{Single-Model MLOps Investment}

For a single production ML system, establishing solid MLOps practices
typically requires:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2874}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2299}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4828}}@{}}
\caption{\textbf{Single-Model MLOps Investment.} Costs for
operationalizing one production ML system. Open-source tooling (MLflow,
Feast) can reduce software costs; cloud-managed services trade higher
unit costs for reduced engineering overhead.}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Component}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Typical Cost}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Justification}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Component}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Typical Cost}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Justification}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{CI/CD pipeline setup} & \$10-30K one-time & Reduces deployment
time from days to hours \\
\textbf{Monitoring \& alerting} & \$2-10K/year & Catches degradation
before user impact \\
\textbf{Feature store (basic)} & \$5-20K/year & Eliminates
training-serving skew \\
\textbf{Model registry} & \$0-5K/year & Enables rollback, audit
trails \\
\textbf{Engineering time} & 1-2 FTE-months setup & Initial automation
and integration \\
\end{longtable}

\textbf{Single-Model ROI Calculation}

The ROI for a single ML Node depends on model criticality:

\[\text{Annual ROI} = \frac{\text{Incidents Avoided} \times \text{Avg Incident Cost} + \text{Time Savings} \times \text{Hourly Cost}}{\text{Annual MLOps Investment}}\]

For a model generating \$1M annual revenue with:

\begin{itemize}
\tightlist
\item
  4 incidents/year avoided (at \$25K each) = \$100K saved
\item
  20 hours/month deployment time saved (at \$150/hr) = \$36K saved
\item
  MLOps investment of \$30K/year
\end{itemize}

\[\text{ROI} = \frac{\$100K + \$36K}{\$30K} = 4\.5\times\]

\textbf{When to Invest More}

The returns from single-model MLOps practices compound when teams add
additional models. The transition from operating several independent ML
Nodes to building a centralized platform involves different economics
entirely, including shared infrastructure amortization, platform team
overhead, and cross-model coordination costs. These platform-scale
economics are covered in specialized coverage of large-scale
infrastructure.

For single-model operations, the key insight is: invest in MLOps
proportional to model criticality. A model driving \$10M in annual
revenue justifies more operational rigor than an internal analytics
model. Start with monitoring and CI/CD (highest ROI), then add feature
stores and automated retraining as the model matures.

The strategic value of MLOps extends beyond operational efficiency to
enable organizational capabilities that would be impossible without
systematic engineering practices. Mature MLOps platforms support rapid
experimentation, controlled A/B testing of model variations, and
real-time adaptation to changing conditions. Organizations should view
MLOps not merely as an operational necessity but as foundational
infrastructure enabling sustained innovation in machine learning
applications.

Having established both the technical infrastructure and the economic
framework for evaluating MLOps investments, we now examine how these
elements combine in production systems. The Oura Ring and ClinAIOps case
studies that follow are not merely examples; they are demonstrations of
how the technical debt patterns
(Section~\ref{sec-machine-learning-operations-mlops-technical-debt-system-complexity-2762}),
infrastructure components
(Section~\ref{sec-machine-learning-operations-mlops-development-infrastructure-automation-de41}),
and production operations
(Section~\ref{sec-machine-learning-operations-mlops-production-operations-b76d})
manifest in real systems. As you read each case, look for specific
implementations of the five foundational principles: Where does
reproducibility appear? How is observable degradation achieved? What
triggers automation? The Principle Mapping Guide below provides a
reading framework for extracting maximum insight from these examples.

\section{Case
Studies}\label{sec-machine-learning-operations-mlops-case-studies-641d}

The operational design principles, technical debt patterns, and maturity
frameworks examined throughout this chapter converge in real-world
implementations. These case studies illustrate how infrastructure
components, monitoring strategies, and cross-functional roles combine to
address challenges ranging from data dependency debt to feedback loops.

We examine two cases representing distinct deployment contexts. The Oura
Ring demonstrates how pipeline debt and configuration management
challenges manifest in resource-constrained edge environments. The
ClinAIOps case study shows how feedback loops and governance
requirements drive specialized operational frameworks in healthcare. The
following \emph{principle mapping guide} structures this comparison.

\phantomsection\label{callout-exampleux2a-1.20}
\begin{fbxSimple}{callout-example}{Example:}{Principle Mapping Guide}
\phantomsection\label{callout-example*-1.20}
As you read these case studies, look for how each implements the five
foundational MLOps principles:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.1436}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4257}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4307}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Principle}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Oura Ring}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{ClinAIOps}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{1. Reproducibility} & Versioned data pipelines, Edge Impulse
lineage & Audit trails, decision provenance \\
\textbf{2. Separation of Concerns} & Independent data, training, and
serving layers with edge-specific deployment pipeline & Distinct
clinical validation and deployment stages with regulatory compliance
isolation \\
\textbf{3. Consistency} & PSG-aligned preprocessing across training and
on-device inference & Standardized clinical data pipelines ensuring
training-serving parity \\
\textbf{4. Observable Degradation} & On-device anomaly detection,
limited telemetry & Cohort-specific monitoring, outcome tracking \\
\textbf{5. Cost-Aware Automation} & Battery-aware retraining triggers,
CI/CD for edge balancing accuracy and resource cost & Automated model
updates with human-in-loop gates balancing update cost and patient
risk \\
\end{longtable}

Each case study demonstrates that domain constraints (edge hardware,
clinical regulation) reshape \emph{how} principles are implemented
without changing \emph{which} principles matter.

\end{fbxSimple}

\subsection{Oura Ring Case
Study}\label{sec-machine-learning-operations-mlops-oura-ring-case-study-2444}

The Oura Ring exemplifies MLOps practices applied to consumer wearable
devices, where embedded ML must operate under strict resource
constraints while delivering accurate health insights.

\subsubsection{Context and
Motivation}\label{sec-machine-learning-operations-mlops-context-motivation-df0a}

The Oura Ring is a consumer-grade wearable monitoring sleep, activity,
and physiological recovery through embedded sensing and computation. By
measuring motion, heart rate, and body temperature, the device estimates
sleep stages and delivers personalized feedback. Unlike traditional
cloud-based systems, much of the data processing and inference occurs
directly on the device.

The central objective was improving sleep stage classification accuracy
to align more closely with polysomnography
(PSG)\sidenote{\textbf{Polysomnography (PSG)}: Multi-parameter sleep
study that records brain waves, eye movements, muscle activity, heart
rhythm, breathing, and blood oxygen levels simultaneously. First
developed by Alrick Hertzman in 1936 and formalized by researchers at
Harvard and University of Chicago in the 1930s-1950s, PSG requires
patients to sleep overnight in specialized labs with 20+ electrodes
attached. Modern sleep centers conduct over 2.8 million PSG studies
annually in the US, with each study costing \$1,000-\$3,000 and
requiring 6-8 hours of monitoring. }, the clinical gold standard.
Initial evaluations revealed 62\% correlation with PSG labels, compared
with 82 to 83\% correlation between expert human scorers. This
discrepancy prompted an effort to re-evaluate data collection,
preprocessing, and model development workflows.

\subsubsection{Data Acquisition and
Preprocessing}\label{sec-machine-learning-operations-mlops-data-acquisition-preprocessing-7fe6}

To overcome performance limitations, the Oura team constructed a diverse
dataset grounded in clinical standards through a study involving 106
participants from three continents. Each participant wore the Oura Ring
while simultaneously undergoing PSG, enabling high-fidelity labeled data
that aligned wearable sensor data with validated sleep annotations.

The study yielded 440 nights of data and over 3,400 hours of
time-synchronized recordings, capturing physiological diversity and
variability in environmental and behavioral factors critical for
generalizing across a real-world user base.

The team implemented automated data pipelines for ingestion, cleaning,
and preprocessing. Leveraging the Edge Impulse
platform\sidenote{\textbf{Edge Impulse Platform}: End-to-end development
platform for machine learning on edge devices, founded in 2019 by Jan
Jongboom and Zach Shelby (former ARM executives). The platform enables
developers to collect data, train models, and deploy to microcontrollers
and edge devices with automated model optimization. Over 70,000
developers use Edge Impulse for embedded ML projects, with the platform
supporting 80+ hardware targets and providing automatic model
compression achieving 100\(\times\) size reduction while maintaining
accuracy. }, they consolidated raw inputs from multiple sources,
resolved temporal misalignments, and structured data for downstream
development. These workflows address \textbf{data dependency debt}
patterns by implementing robust versioning and lineage tracking,
avoiding unstable dependencies that commonly plague embedded ML systems.

\subsubsection{Model Development and
Evaluation}\label{sec-machine-learning-operations-mlops-model-development-evaluation-102e}

With high-quality data in place, the team developed models classifying
sleep stages. Recognizing operational constraints, model design
prioritized efficiency alongside predictive accuracy, selecting
architectures that could operate within the ring's limited memory and
compute budget.

Two model configurations were explored: one using only accelerometer
data for minimal energy consumption, and another incorporating heart
rate variability and body temperature to capture autonomic nervous
system activity and circadian rhythms.

Through five-fold cross-validation\sidenote{\textbf{Five-Fold
Cross-Validation}: Statistical method that divides data into 5 equal
subsets, training on 4 folds and testing on 1, repeating 5 times with
each fold used exactly once for testing. Developed from early
statistical resampling work in the 1930s, k-fold cross-validation (with
k=5 or k=10) became standard in machine learning for model evaluation.
This approach reduces overfitting bias compared to single train/test
splits and provides more robust performance estimates by averaging
results across multiple iterations. } against PSG annotations and
iterative tuning, the enhanced models achieved 79\% correlation
accuracy, a significant improvement from baseline toward the clinical
benchmark.

These gains reflect the broader impact of an MLOps approach integrating
data collection, reproducible training pipelines, and disciplined
evaluation. Structured documentation and version control of model
parameters avoided the fragmented settings that often undermine embedded
ML deployments, while requiring close collaboration among data
scientists, ML engineers, and DevOps engineers.

\subsubsection{Deployment and
Iteration}\label{sec-machine-learning-operations-mlops-deployment-iteration-f128}

Following validation, the team deployed models onto the ring's embedded
hardware with careful accommodation of memory, compute, and power
constraints. The lightweight accelerometer-only model enabled real-time
inference with minimal energy, while the more complex model using heart
rate variability and temperature was deployed selectively where
resources permitted.

The team developed a modular toolchain for converting models into
optimized formats through quantization and pruning, then deployed using
over-the-air (OTA)\sidenote{\textbf{Over-the-Air (OTA) Updates}: Remote
software deployment method that wirelessly delivers updates to devices
without physical access. Originally developed for mobile networks in the
1990s, OTA technology now enables critical functionality for IoT and
edge devices. Tesla delivers over 2 GB software updates to vehicles via
OTA, while smartphone manufacturers push security patches to billions of
devices monthly. For ML models, OTA enables rapid deployment of
retrained models with differential compression reducing update sizes by
80-95\%. } update mechanisms ensuring consistency across devices in the
field.

\subsubsection{Key Operational
Insights}\label{sec-machine-learning-operations-mlops-key-operational-insights-ba35}

The Oura Ring case demonstrates how operational challenges manifest in
edge environments. The team's modular tiered architectures with clear
interfaces avoided the ``pipeline jungle'' problem while enabling
runtime tradeoffs between accuracy and efficiency. The transition from
62\% to 79\% accuracy required systematic configuration management
across data collection, model architectures, and deployment targets.
Success emerged from coordinated collaboration across data engineers, ML
researchers, embedded systems developers, and operations personnel. The
following summary captures how the five foundational MLOps principles
manifested in this edge deployment:

\phantomsection\label{callout-lighthouseux2a-1.21}
\begin{fbxSimple}{callout-lighthouse}{Lighthouse:}{Oura Ring: Principles Summary}
\phantomsection\label{callout-lighthouse*-1.21}
\textbf{Principle 1 (Reproducibility)}: Edge Impulse platform provided
versioned data pipelines with full lineage tracking. Every model can be
traced to its exact training data, preprocessing code, and
hyperparameters.

\textbf{Principle 2 (Automation)}: Automated conversion from training
frameworks to embedded formats (quantization, pruning). OTA update
infrastructure enables push-button deployment to millions of devices.

\textbf{Principle 3 (Testing)}: Five-fold cross-validation against PSG
gold standard. Continuous benchmarking ensures model updates don't
regress below 79\% correlation threshold.

\textbf{Principle 4 (Observable Degradation)}: Battery and compute
monitoring detect when models underperform due to resource constraints.
Limited telemetry (privacy-preserving) tracks aggregate accuracy across
device population.

\textbf{Principle 5 (Graceful Degradation)}: Tiered model architecture
enables fallback from complex (heart rate + temperature) to simple
(accelerometer-only) model when resources constrained.

\textbf{Key Adaptation}: Edge constraints forced \emph{proactive}
graceful degradation design rather than reactive incident response.

\end{fbxSimple}

This case exemplifies how MLOps principles adapt to domain-specific
constraints. When machine learning moves into clinical applications,
additional complexity emerges, requiring frameworks that address
regulatory compliance, patient safety, and clinical decision-making.

\subsection{ClinAIOps Case
Study}\label{sec-machine-learning-operations-mlops-clinaiops-case-study-5d5d}

Healthcare ML deployment presents challenges extending beyond resource
constraints. Traditional MLOps frameworks often fall short in domains
requiring extensive human oversight, domain-specific evaluation, and
ethical governance. Continuous therapeutic monitoring
(CTM)\sidenote{\textbf{Continuous Therapeutic Monitoring (CTM)}:
Healthcare approach using wearable sensors to collect real-time
physiological and behavioral data for personalized treatment
adjustments. Wearable device adoption in healthcare reached 36.4\% in
2022, with the global healthcare wearables market valued at \$33.85
billion in 2023. CTM applications include automated insulin dosing for
diabetes, blood thinner adjustments for atrial fibrillation, and early
mobility interventions for older adults, shifting from reactive to
proactive, personalized care. } exemplifies a domain where MLOps must
evolve to meet clinical integration demands.

CTM leverages wearable sensors to collect real-time physiological and
behavioral data from patients. AI systems must be integrated into
clinical workflows, aligned with regulatory requirements, and designed
to augment rather than replace human decision-making. The traditional
MLOps paradigm does not adequately account for patient safety, clinician
judgment, and ethical constraints.

ClinAIOps (\citeproc{ref-chen2023framework}{E. Chen et al. 2023}), a
framework for operationalizing AI in clinical environments, shows how
MLOps principles must evolve for regulatory and human-centered
requirements. Unlike conventional MLOps, ClinAIOps directly addresses
\textbf{feedback loop} challenges by designing them into the system
architecture. The framework's structured coordination between patients,
clinicians, and AI systems represents practical implementation of
\textbf{governance and collaboration} principles.

Standard MLOps falls short in clinical environments because healthcare
requires coordination among diverse human actors, clinical
decision-making hinges on personalized care and shared accountability,
and health data must comply with strict privacy regulations. ClinAIOps
presents a framework that balances technical rigor with clinical utility
and operational reliability with ethical responsibility.

\subsubsection{Feedback
Loops}\label{sec-machine-learning-operations-mlops-feedback-loops-3cdd}

Three interlocking feedback loops enable safe, adaptive integration of
machine learning into clinical practice. Figure~\ref{fig-clinaiops}
visualizes these as a cyclical framework where patients contribute
monitoring data, clinicians provide therapy regimens and approval
limits, and AI systems generate alerts and recommendations.

\begin{figure}[htb]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/d72f1c72901fa447c3f99fa1ed7c09f5700d8f40.pdf}}

}

\caption{\label{fig-clinaiops}\textbf{ClinAIOps Feedback Loops}: The
cyclical framework coordinates data flow between patients, clinicians,
and AI systems to support continuous model improvement and safe clinical
integration. These interconnected loops enable iterative refinement of
AI models based on real-world performance and clinical feedback,
fostering trust and accountability in healthcare applications. Source:
(\citeproc{ref-chen2023framework}{E. Chen et al. 2023}).}

\end{figure}%

Each feedback loop plays a distinct yet interconnected role:

\begin{itemize}
\tightlist
\item
  The \textbf{patient-AI loop} captures real-time physiological data and
  generates tailored treatment suggestions.
\item
  The \textbf{clinician-AI loop} ensures recommendations are reviewed
  and refined under professional supervision.
\item
  The \textbf{patient-clinician loop} supports shared decision-making
  for collaborative goal-setting.
\end{itemize}

Together, these loops enable adaptive personalization, maintain
clinician control, and promote continuous model improvement based on
real-world feedback.

\textbf{Patient-AI Loop.} The patient-AI loop enables personalized
therapy optimization through continuous physiological data from wearable
devices. Patients wear sensors such as continuous glucose monitors or
ECG-enabled wearables that passively capture health signals.

The AI system analyzes these data streams alongside clinical context
from electronic medical records, generating individualized
recommendations for treatment adjustments. Treatment suggestions are
tiered: minor adjustments within clinician-defined safety thresholds may
be acted upon directly by the patient, while significant changes require
clinician approval. This structure maintains human oversight while
enabling high-frequency, data-driven adaptation.

\textbf{Clinician-AI Loop.} The clinician-AI loop introduces human
oversight into AI-assisted decision-making. The AI generates treatment
recommendations with interpretable summaries of patient data including
longitudinal trends and sensor-derived metrics.

For example, an AI model might recommend reducing antihypertensive
medication for a patient with consistently below-target blood pressure.
The clinician reviews the recommendation in context and may accept,
reject, or modify it, and this feedback refines model alignment with
clinical practice. Clinicians also define operational boundaries that
ensure only low-risk adjustments are automated, preserving clinical
accountability while integrating machine intelligence.

\textbf{Patient-Clinician Loop.} The patient-clinician loop shifts
clinical interactions from routine data collection to higher-level
interpretation and shared decision-making. With AI handling data
aggregation and trend analysis, clinicians engage more meaningfully:
reviewing patterns, contextualizing insights, and setting personalized
health goals.

For example, in diabetes management, a clinician may use AI-summarized
data to guide discussions on dietary habits and physical activity. Visit
frequency adjusts dynamically based on patient progress rather than
fixed intervals. This positions the clinician as coach and advisor,
interpreting data through the lens of patient preferences and clinical
judgment.

\subsubsection{Hypertension Case
Example}\label{sec-machine-learning-operations-mlops-hypertension-case-example-7b1f}

Hypertension management illustrates how the three ClinAIOps loops work
in practice. Affecting nearly half of US adults (119.9 million
individuals), hypertension requires individualized, ongoing therapy
adjustments. This makes it an ideal candidate for continuous therapeutic
monitoring.

\textbf{Data Infrastructure}: Wrist-worn devices with
photoplethysmography (PPG)\sidenote{\textbf{Photoplethysmography (PPG)}:
Optical technique detecting blood volume changes by measuring light
absorption variations. Modern smartwatches use PPG sensors with green
LEDs to measure heart rate, with Apple Watch collecting billions of
measurements monthly for heart rhythm analysis and atrial fibrillation
detection. } and ECG sensors provide noninvasive blood pressure
estimates (\citeproc{ref-zhang2017highly}{Q. Zhang, Zhou, and Zeng
2017}), augmented by accelerometer data for activity context and
self-reported medication adherence logs. This multimodal data stream,
integrated with electronic health records, forms the foundation for
personalized AI recommendations.

\textbf{Loop Implementation}: Figure~\ref{fig-interactive-loop} shows
how the three feedback loops manifest in hypertension management. The
patient-AI loop enables bounded self-management: minor dosage
adjustments within clinician-defined safety thresholds can be acted upon
directly, while significant changes require approval. The clinician-AI
loop provides oversight through longitudinal trend summaries and
generates alerts for clinical risk (persistent hypotension, hypertensive
crisis). The patient-clinician loop shifts appointments from data
collection to higher-level discussions of lifestyle factors (diet,
activity, stress), with visit frequency adapting to patient stability
rather than fixed intervals.

\begin{figure}[htb]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/12a89c4aabd66f0cf0610c5958de62d8c44ffb41.pdf}}

}

\caption{\label{fig-interactive-loop}\textbf{Hypertension Management
Loops.} Three feedback loops operate in parallel: the patient-AI loop
enables bounded self-management through blood pressure monitoring and
titration recommendations; the clinician-AI loop provides oversight via
trend summaries and clinical risk alerts; and the patient-clinician loop
shifts appointments toward therapy trends and lifestyle modifiers.
Source: (\citeproc{ref-chen2023framework}{E. Chen et al. 2023}).}

\end{figure}%

\subsubsection{MLOps vs ClinAIOps
Comparison}\label{sec-machine-learning-operations-mlops-mlops-vs-clinaiops-comparison-94be}

The hypertension case illustrates why traditional MLOps frameworks are
often insufficient for high-stakes clinical domains. Conventional MLOps
excels at technical lifecycle management but lacks constructs for
coordinating human decision-making and ensuring ethical accountability.

ClinAIOps extends beyond technical infrastructure to support complex
sociotechnical systems, embedding machine learning into contexts where
clinicians, patients, and stakeholders collaboratively shape treatment
decisions. Table~\ref{tbl-clinical_ops} contrasts these approaches
across eight dimensions.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2170}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3585}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4245}}@{}}
\caption{\textbf{Clinical AI Operations.} Traditional MLOps focuses on
model performance, while ClinAIOps integrates technical systems with
clinical workflows, ethical considerations, and ongoing feedback loops
to ensure safe, trustworthy AI assistance in healthcare settings.
ClinAIOps prioritizes human oversight and accountability alongside
automation, addressing unique challenges in clinical decision-making
that standard MLOps pipelines often
overlook.}\label{tbl-clinical_ops}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Traditional MLOps}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{ClinAIOps}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Traditional MLOps}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{ClinAIOps}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Focus} & ML model development and deployment & Coordinating
human and AI decision-making \\
\textbf{Stakeholders} & Data scientists, IT engineers & Patients,
clinicians, AI developers \\
\textbf{Feedback loops} & Model retraining, monitoring & Patient-AI,
clinician-AI, patient-clinician \\
\textbf{Objective} & Operationalize ML deployments & Optimize patient
health outcomes \\
\textbf{Processes} & Automated pipelines and infrastructure & Integrates
clinical workflows and oversight \\
\textbf{Data considerations} & Building training datasets & Privacy,
ethics, protected health information \\
\textbf{Model validation} & Testing model performance metrics & Clinical
evaluation of recommendations \\
\textbf{Implementation} & Focuses on technical integration & Aligns
incentives of human stakeholders \\
\end{longtable}

Successfully deploying AI in healthcare requires aligning systems with
clinical workflows, human expertise, and patient needs. Technical
performance alone is insufficient; deployment must account for ethical
oversight and continuous adaptation to dynamic clinical contexts.

The ClinAIOps framework specifically addresses the operational
challenges identified earlier, demonstrating how they manifest in
healthcare contexts. Rather than treating feedback loops as technical
debt, ClinAIOps architects them as beneficial system features.
Patient-AI, clinician-AI, and patient-clinician loops create intentional
feedback mechanisms that improve care quality while maintaining safety
through human oversight. The structured interface between AI
recommendations and clinical decision-making eliminates hidden
dependencies, ensuring clinicians maintain explicit control over AI
outputs and preventing silent breakage when model updates unexpectedly
affect downstream systems. Clear delineation of AI responsibilities
(monitoring and recommendations) versus human responsibilities
(diagnosis and treatment decisions) prevents the gradual erosion of
system boundaries that undermines reliability in complex ML systems. The
framework's emphasis on regulatory compliance, ethical oversight, and
clinical validation prevents the ad hoc practices that accumulate
governance debt in healthcare AI systems. By embedding AI within
collaborative clinical ecosystems, ClinAIOps demonstrates how
operational challenges can be transformed from liabilities into
systematic design opportunities, reframing AI as a component of a
broader sociotechnical system designed to advance health outcomes while
maintaining engineering rigor. The following summary captures how the
five foundational MLOps principles adapted to clinical constraints:

\phantomsection\label{callout-lighthouseux2a-1.22}
\begin{fbxSimple}{callout-lighthouse}{Lighthouse:}{ClinAIOps: Principles Summary}
\phantomsection\label{callout-lighthouse*-1.22}
\textbf{Principle 1 (Reproducibility)}: Every AI recommendation is
logged with complete provenance: input data, model version, confidence
scores, and clinician decision. Audit trails enable regulatory review
and outcome analysis.

\textbf{Principle 2 (Automation)}: Automated data collection from
wearables, routine titration within clinician-approved bounds. Human
gates at critical decision points (diagnosis changes, medication
starts/stops).

\textbf{Principle 3 (Testing)}: Clinical validation protocols beyond
statistical metrics. Prospective trials, cohort analysis, comparison
against standard-of-care outcomes. Regulatory approval requirements
drive testing rigor.

\textbf{Principle 4 (Observable Degradation)}: Outcome tracking (blood
pressure control, adverse events) enables detection of model
degradation. Cohort-specific monitoring catches failures that affect
subpopulations differently.

\textbf{Principle 5 (Graceful Degradation)}: Clinician override always
available. Uncertainty flagging triggers human review. Conservative
recommendations when confidence is low. System designed to augment,
never replace, clinical judgment.

\textbf{Key Adaptation}: Regulatory requirements transformed graceful
degradation from ``nice to have'' into mandatory design constraint, with
human-in-the-loop as the primary safety mechanism.

\end{fbxSimple}

The Oura Ring and ClinAIOps cases demonstrate MLOps principles applied
successfully under domain-specific constraints. Production ML systems
more commonly fail, however, and the failure modes often stem from
predictable misconceptions: intuitions that work for traditional
software but break down for probabilistic systems.

\section{Fallacies and
Pitfalls}\label{sec-machine-learning-operations-mlops-fallacies-pitfalls-e4e0}

The following fallacies and pitfalls capture common errors that waste
engineering resources, trigger production incidents, and cause silent
accuracy degradation. Each connects to specific sections detailing the
underlying mechanisms and solutions.

\paragraph*{\texorpdfstring{Fallacy: \emph{MLOps is just applying
traditional DevOps practices to machine learning
models.}}{Fallacy: MLOps is just applying traditional DevOps practices to machine learning models.}}\label{fallacy-mlops-is-just-applying-traditional-devops-practices-to-machine-learning-models.}
\addcontentsline{toc}{paragraph}{Fallacy: \emph{MLOps is just applying
traditional DevOps practices to machine learning models.}}

Engineers assume standard CI/CD pipelines transfer directly to ML, but
production ML requires fundamentally different infrastructure.
Section~\ref{sec-machine-learning-operations-mlops-cicd-pipelines-a9de}
establishes that ML pipelines execute in 15 to 45 minutes versus 2 to 5
minutes for traditional software, with distinct stages for data
validation (20-30\%), model training (40-60\%), and performance
evaluation (15-25\%). Traditional DevOps achieves daily or hourly
deployments; ML systems without specialized tooling deploy weekly to
monthly. Standard CI/CD tools cannot handle feature stores, model
registries, or drift detection. A recommendation system deployed using
conventional DevOps experienced 8\% accuracy loss because the pipeline
lacked training-serving consistency checks. Organizations that adopt
DevOps without ML adaptations encounter silent model degradation,
training-serving skew, and data quality failures that evade conventional
testing.

\paragraph*{\texorpdfstring{Pitfall: \emph{Treating model deployment as
a one-time event rather than an ongoing
process.}}{Pitfall: Treating model deployment as a one-time event rather than an ongoing process.}}\label{pitfall-treating-model-deployment-as-a-one-time-event-rather-than-an-ongoing-process.}
\addcontentsline{toc}{paragraph}{Pitfall: \emph{Treating model
deployment as a one-time event rather than an ongoing process.}}

Teams view deployment as a terminal milestone analogous to shipping
software releases, but models degrade continuously due to data drift and
distribution shift.
Section~\ref{sec-machine-learning-operations-mlops-data-quality-monitoring-c6b6}
establishes that PSI values exceeding 0.2 indicate significant
distribution shift requiring investigation. A fraud detection model with
PSI = 0.18 at deployment reached PSI = 0.31 within three months, causing
accuracy to drop from 94\% to 87\%. The optimal retraining interval
follows \(T^* \approx \sqrt{\frac{2C}{QVA_0\lambda}}\) from
Section~\ref{sec-machine-learning-operations-mlops-quantitative-retraining-economics-1579},
where high-volume systems require daily retraining while low-drift
domains sustain monthly intervals. Without automated retraining
pipelines, Mean Time To Recovery (MTTR) for accuracy degradation
averages 5 to 14 days; with automation, MTTR drops to 4 to 24 hours.
Production ML requires continuous monitoring of feature distributions,
performance metrics, and automated retraining triggers throughout the
operational lifecycle.

\paragraph*{\texorpdfstring{Fallacy: \emph{Automated retraining ensures
optimal model performance without human
oversight.}}{Fallacy: Automated retraining ensures optimal model performance without human oversight.}}\label{fallacy-automated-retraining-ensures-optimal-model-performance-without-human-oversight.}
\addcontentsline{toc}{paragraph}{Fallacy: \emph{Automated retraining
ensures optimal model performance without human oversight.}}

Engineers assume automated pipelines handle all maintenance scenarios,
yet automation cannot detect all failure modes. Automated retraining
perpetuates biases in corrupted training data, triggers updates during
peak traffic, or deploys models that pass validation but degrade edge
cases.
Section~\ref{sec-machine-learning-operations-mlops-incident-response-ml-systems-c637}
documents that 15 to 25 percent of P1 incidents (accuracy drops
exceeding 10\%) originate from automated retraining propagating upstream
data quality issues. A news recommendation system retrained on weekend
data exhibited 22\% lower engagement because user behavior differs
fundamentally across weekday versus weekend contexts. Systems with human
checkpoints at validation boundaries experience 40-60\% fewer production
incidents than fully automated pipelines. Effective MLOps requires
escalation protocols for anomalous validation results, manual approval
for unusual metric patterns, and override capabilities when automation
produces questionable outcomes.

\paragraph*{\texorpdfstring{Pitfall: \emph{Focusing on technical
infrastructure while neglecting organizational and process
alignment.}}{Pitfall: Focusing on technical infrastructure while neglecting organizational and process alignment.}}\label{pitfall-focusing-on-technical-infrastructure-while-neglecting-organizational-and-process-alignment.}
\addcontentsline{toc}{paragraph}{Pitfall: \emph{Focusing on technical
infrastructure while neglecting organizational and process alignment.}}

Organizations invest in MLOps platforms expecting tooling to solve
deployment problems, but sophisticated infrastructure fails without
cultural transformation. MLOps demands coordination between data
scientists optimizing for accuracy, engineers prioritizing latency, and
business stakeholders focused on impact. A retail company deployed
feature stores and model registries but maintained quarterly deployment
frequency because data scientists and engineers operated in isolation.
Industry data shows unified ML teams achieve 8 to 15 deployments per
month versus 1 to 2 per quarter for siloed teams. Time-to-production for
new models averages 45 to 90 days in fragmented organizations but drops
to 7 to 21 days with integrated teams. Successful MLOps requires
cross-functional teams with unified objectives, shared on-call rotations
building empathy across roles, and incentive structures rewarding
production reliability alongside model performance.

\paragraph*{\texorpdfstring{Fallacy: \emph{Training and serving
environments automatically remain consistent once pipelines are
established.}}{Fallacy: Training and serving environments automatically remain consistent once pipelines are established.}}\label{fallacy-training-and-serving-environments-automatically-remain-consistent-once-pipelines-are-established.}
\addcontentsline{toc}{paragraph}{Fallacy: \emph{Training and serving
environments automatically remain consistent once pipelines are
established.}}

Teams assume that feature computation produces identical values across
training and serving after initial pipeline setup, but training-serving
skew emerges from subtle inconsistencies in preprocessing logic,
timezone handling, or dependency versions.
Section~\ref{sec-machine-learning-operations-mlops-feature-stores-c01c}
demonstrates that skew causes 5 to 15 percent accuracy degradation even
when models perform well in offline validation. An e-commerce ranking
model computed \texttt{session\_length} using wall-clock time in
training but processing time in serving, creating 12\% accuracy loss
that persisted for six weeks before detection. Google reported that
eliminating training-serving skew in ad prediction improved performance
by 8\%, worth millions in annual revenue. Without centralized feature
stores and automated consistency validation, skew detection requires 3
to 8 weeks as degradation gradually becomes visible in aggregate
metrics. Organizations using feature stores with built-in consistency
checks detect skew within 1 to 3 days through automated validation that
compares feature distributions across environments.

\paragraph*{\texorpdfstring{Pitfall: \emph{Assuming comprehensive
monitoring prevents all production
incidents.}}{Pitfall: Assuming comprehensive monitoring prevents all production incidents.}}\label{pitfall-assuming-comprehensive-monitoring-prevents-all-production-incidents.}
\addcontentsline{toc}{paragraph}{Pitfall: \emph{Assuming comprehensive
monitoring prevents all production incidents.}}

Engineers believe sufficient metrics and dashboards eliminate surprise
failures, but monitoring creates blind spots when teams track outputs
without validating inputs.
Section~\ref{sec-machine-learning-operations-mlops-data-quality-monitoring-c6b6}
establishes that input validation detects issues before they degrade
predictions, yet 60-75\% of ML systems monitor only accuracy and
latency. A recommendation system tracked click-through rate but ignored
feature staleness, missing that user embeddings were 18 hours stale due
to database replication lag. This created 9\% engagement degradation for
three days before accuracy monitoring triggered alerts. Systems
monitoring only outputs exhibit 48 to 96 hour Mean Time To Detection
(MTTD); adding data quality monitoring reduces MTTD to 15 minutes to 4
hours. Production ML requires layered monitoring with distinct SLAs:
data freshness (15 minutes), schema validation (1 hour), feature
distributions (4 hours), model outputs (1 hour), and business metrics
(24 hours). Monitoring infrastructure itself needs redundancy to prevent
blind operation during platform failures.

\section{Summary}\label{sec-machine-learning-operations-mlops-summary-ac43}

MLOps exists because machine learning systems fail differently than
traditional software. Where a crashed server throws exceptions and turns
dashboards red, a degrading model continues serving predictions with
full confidence while accuracy erodes invisibly. This fundamental
difference---probabilistic systems that decay rather than
crash---explains \emph{why} the operational practices developed for
deterministic software prove insufficient for ML, and \emph{why} the
discipline of machine learning operations emerged to close this
observability gap.

The five foundational principles introduced at the chapter's opening
(Section~\ref{sec-machine-learning-operations-mlops-foundational-principles-44e6})
provide an evaluation framework that applies regardless of scale or
domain. Reproducibility through versioning addresses the root cause of
most production incidents: untracked artifacts including data versions,
configuration changes, and environment drift that make debugging
impossible and rollbacks unreliable. Separation of concerns contains the
blast radius when changes are required, preventing the boundary erosion
and correction cascades that transform local fixes into system-wide
regressions. The consistency imperative targets training-serving skew,
the silent accuracy killer that caused 5-15\% degradation in our
examples when feature computation diverged between pipelines; feature
stores implement this principle by computing features once and serving
them everywhere. Observable degradation transforms the abstract ``silent
failure'' problem into actionable alerts through layered monitoring that
tracks data freshness, feature distributions, model outputs, and
business metrics. Cost-aware automation replaces arbitrary retraining
schedules with principled economics, using the staleness cost function
(\(T^* \approx \sqrt{\frac{2C}{QVA_0\lambda}}\)) to quantify when
accuracy decay justifies retraining expense.

The infrastructure components examined throughout the chapter directly
implement these principles. CI/CD pipelines enforce reproducibility by
tracking code, data, and configuration together. Feature stores ensure
consistency by eliminating divergent feature computation paths.
Monitoring systems make degradation observable before users notice
impact. The retraining decision framework enables cost-aware automation
by connecting drift detection to economic thresholds. The case studies
demonstrated that domain constraints reshape \emph{how} principles are
implemented without changing \emph{which} principles matter: Oura Ring
showed how edge constraints force proactive graceful degradation design,
with the 62\% to 79\% accuracy improvement coming from systematic data
management and configuration control rather than algorithmic innovation.
ClinAIOps showed how regulatory requirements transform graceful
degradation from optional to mandatory, with human-in-the-loop
governance serving as the primary safety mechanism and the three
feedback loops (patient-AI, clinician-AI, patient-clinician) functioning
as architectural patterns rather than operational overhead.

\phantomsection\label{callout-takeawaysux2a-1.23}
\begin{fbxSimple}{callout-takeaways}{Takeaways:}{Key Takeaways}
\phantomsection\label{callout-takeaways*-1.23}

\begin{itemize}
\tightlist
\item
  \textbf{ML systems fail silently}: Unlike software that crashes, ML
  degrades gradually. A model can maintain 100\% uptime while accuracy
  drops 15\% over weeks. Outcome monitoring is essential, not uptime
  tracking alone.
\item
  \textbf{Start with monitoring and CI/CD}: These typically provide the
  highest return on investment. Feature stores should be added when
  training-serving skew becomes measurable. Investment should be
  proportional to model criticality: a \$10M model justifies more rigor
  than internal analytics.
\item
  \textbf{The Five Principles apply universally}: Reproducibility
  (version everything), Separation of Concerns (modular layers),
  Consistency (feature stores), Observable Degradation (layered
  monitoring), Cost-Aware Automation (retraining economics).
\item
  \textbf{Master single-model operations before fleet operations}:
  Managing one model differs qualitatively from managing many. The
  principles scale, but complexity grows superlinearly with fleet size.
\item
  \textbf{Technical infrastructure alone cannot solve deployment}: MLOps
  requires cross-functional coordination. Shared on-call rotations and
  unified incentives are as critical as tooling.
\end{itemize}

\end{fbxSimple}

MLOps discipline separates teams that ship production ML from those that
ship demos. The practitioners who internalize these principles can
diagnose a degrading model and immediately identify whether the problem
is data drift (check feature distributions), training-serving skew
(compare preprocessing paths), configuration debt (audit recent
changes), or feedback loop contamination (analyze temporal patterns).
Those who treat production ML as ``deploy and forget'' discover their
models have been silently wrong for months, eroding user trust and
business value while dashboards showed green. As ML systems become
critical infrastructure powering decisions from loan approvals to
medical diagnoses, this operational discipline determines whether
organizations can deploy AI responsibly at scale.

\phantomsection\label{callout-chapter-connectionux2a-1.24}
\begin{fbxSimple}{callout-chapter-connection}{What’s Next:}{From Reliability to Responsibility}
\phantomsection\label{callout-chapter-connection*-1.24}
We have built a system that is efficient, scalable, and reliable. But a
system can achieve 99.9\% uptime and sub-10ms latency while still
causing harm by amplifying bias or leaking data. In
\textbf{?@sec-responsible-engineering}, we face the final and most
difficult constraint: aligning our technical optimization with human
values, ensuring that what we build serves the world we want to live in.

\end{fbxSimple}

\phantomsection\label{refs}
\begin{CSLReferences}{1}{0}
\bibitem[\citeproctext]{ref-gitlab}
0001, Andreas Schreiber, Claas de Boer, and Lynn von Kurnatowski. 2021.
{``GitLab2PROV - Provenance of Software Projects Hosted on GitLab.''} In
\emph{TaPP}.
\url{https://www.usenix.org/conference/tapp2021/presentation/schreiber}.

\bibitem[\citeproctext]{ref-kserve}
Aly, Abdelrahman, Ahmed M. Hamad, Mirvat Al-Qutt, and Mahmoud Fayez.
2025. {``Real-Time Multi-Class Threat Detection and Adaptive Deception
in Kubernetes Environments.''} \emph{Scientific Reports} 15 (1): 8924.
\url{https://doi.org/10.1038/s41598-025-91606-8}.

\bibitem[\citeproctext]{ref-aws}
Amazon. 2025. {``Amazon Web Services (AWS) \&Amp; Cloud Dominance.''} In
\emph{Corporate Case Studies}. San International Scientific
Publications. \url{https://doi.org/10.59646/csc5/327}.

\bibitem[\citeproctext]{ref-amershi2019software}
Amershi, Saleema, Andrew Begel, Christian Bird, Robert DeLine, Harald
Gall, Ece Kamar, Nachiappan Nagappan, Besmira Nushi, and Thomas
Zimmermann. 2019. {``Software Engineering for Machine Learning: A Case
Study.''} In \emph{2019 IEEE/ACM 41st International Conference on
Software Engineering: Software Engineering in Practice (ICSE-SEIP)},
291--300. IEEE. \url{https://doi.org/10.1109/icse-seip.2019.00042}.

\bibitem[\citeproctext]{ref-kubeflow}
Authors, Kubeflow. 2024. {``Kubeflow.''}
\url{https://www.kubeflow.org/}.

\bibitem[\citeproctext]{ref-baylor2017tfx}
Baylor, Denis, Eric Breck, Heng-Tze Cheng, Noah Fiedel, Chuan Yu Foo,
Zakaria Haque, Salem Haykal, et al. 2017. {``TFX: A TensorFlow-Based
Production-Scale Machine Learning Platform.''} In \emph{Proceedings of
the 23rd ACM SIGKDD International Conference on Knowledge Discovery and
Data Mining}, 1387--95. ACM.
\url{https://doi.org/10.1145/3097983.3098021}.

\bibitem[\citeproctext]{ref-shap_github}
Biecek, Przemyslaw, and Tomasz Burzykowski. 2021. {``SHAP (SHapley
Additive exPlanations).''} In \emph{Explanatory Model Analysis},
95--106. Chapman; Hall/CRC.
\url{https://doi.org/10.1201/9780429027192-10}.

\bibitem[\citeproctext]{ref-scikit_learn_feature_selection}
Blum, Travis R., Hao Liu, Michael S. Packer, Xiaozhe Xiong, Pyung-Gang
Lee, Sicai Zhang, Michelle Richter, et al. 2021. {``Phage-Assisted
Evolution of Botulinum Neurotoxin Proteases with Reprogrammed
Specificity.''} \emph{Science} 371 (6531): 803--10.
\url{https://doi.org/10.1126/science.abf5972}.

\bibitem[\citeproctext]{ref-label_studio}
Borghaei, Hossein, Luis Paz-Ares, Leora Horn, David R. Spigel, Martin
Steins, Neal E. Ready, Laura Q. Chow, et al. 2015. {``Nivolumab Versus
Docetaxel in Advanced Nonsquamous Non--Small-Cell Lung Cancer.''}
\emph{New England Journal of Medicine} 373 (17): 1627--39.
\url{https://doi.org/10.1056/nejmoa1507643}.

\bibitem[\citeproctext]{ref-breck2020ml}
Breck, Eric, Shanqing Cai, Eric Nielsen, Michael Salib, and D. Sculley.
2017. {``The ML Test Score: A Rubric for ML Production Readiness and
Technical Debt Reduction.''} In \emph{2017 IEEE International Conference
on Big Data (Big Data)}, 6:1123--32. 2. IEEE.
\url{https://doi.org/10.1109/bigdata.2017.8258038}.

\bibitem[\citeproctext]{ref-apache_airflow}
Cejudo, Ander, Yone Tellechea, Amaia Calvo, Aitor Almeida, Cristina
Martı́n, and Andoni Beristain. 2025. {``Scalable Big Data Platform with
End-to-End Traceability for Health Data Monitoring in Older Adults:
Development and Performance Evaluation.''} \emph{JMIR Medical
Informatics} 13 (December): e81701--1.
\url{https://doi.org/10.2196/81701}.

\bibitem[\citeproctext]{ref-zaharia2018accelerating}
Chen, Andrew, Andy Chow, Aaron Davidson, Arjun DCunha, Ali Ghodsi, Sue
Ann Hong, Andy Konwinski, et al. 2020. {``Developments in MLflow: A
System to Accelerate the Machine Learning Lifecycle.''} In
\emph{Proceedings of the Fourth International Workshop on Data
Management for End-to-End Machine Learning}, 41:1--4. 4. ACM.
\url{https://doi.org/10.1145/3399579.3399867}.

\bibitem[\citeproctext]{ref-chen2023framework}
Chen, Emma, Shvetank Prakash, Vijay Janapa Reddi, David Kim, and Pranav
Rajpurkar. 2023. {``A Framework for Integrating Artificial Intelligence
for Clinical Care with Continuous Therapeutic Monitoring.''}
\emph{Nature Biomedical Engineering} 9 (4): 445--54.
\url{https://doi.org/10.1038/s41551-023-01115-0}.

\bibitem[\citeproctext]{ref-keras}
Chollet, François et al. 2024. {``Keras.''} \url{https://keras.io/}.

\bibitem[\citeproctext]{ref-vertex_ai_fine_tuning}
Cloud, Google. 2024a. {``Tune Models Overview.''}
\url{https://cloud.google.com/vertex-ai/docs/generative-ai/models/tune-models}.

\bibitem[\citeproctext]{ref-vertex_ai_model_registry}
---------. 2024b. {``Vertex AI Model Registry.''}
\url{https://cloud.google.com/vertex-ai/docs/model-registry/introduction}.

\bibitem[\citeproctext]{ref-collberg2016repeatability}
Collberg, Christian, and Todd A. Proebsting. 2016. {``Repeatability in
Computer Systems Research.''} \emph{Communications of the ACM} 59 (3):
62--69. \url{https://doi.org/10.1145/2812803}.

\bibitem[\citeproctext]{ref-tensorflow}
Coy, Heidi, Kevin Hsieh, Willie Wu, Mahesh B. Nagarajan, Jonathan R.
Young, Michael L. Douek, Matthew S. Brown, Fabien Scalzo, and Steven S.
Raman. 2019a. {``Deep Learning and Radiomics: The Utility of Google
TensorFlow\texttrademark{} Inception in Classifying Clear Cell Renal
Cell Carcinoma and Oncocytoma on Multiphasic CT.''} \emph{Abdominal
Radiology} 44 (6): 2009--20.
\url{https://doi.org/10.1007/s00261-019-01929-0}.

\bibitem[\citeproctext]{ref-tensorflow_serving}
---------. 2019b. {``Deep Learning and Radiomics: The Utility of Google
TensorFlow\texttrademark{} Inception in Classifying Clear Cell Renal
Cell Carcinoma and Oncocytoma on Multiphasic CT.''} \emph{Abdominal
Radiology} 44 (6): 2009--20.
\url{https://doi.org/10.1007/s00261-019-01929-0}.

\bibitem[\citeproctext]{ref-crankshaw2017clipper}
Crankshaw, Daniel, Xin Wang, Guilio Zhou, Michael J. Franklin, Joseph E.
Gonzalez, and Ion Stoica. 2017. {``Clipper: A Low-Latency Online
Prediction Serving System.''} In \emph{14th USENIX Symposium on
Networked Systems Design and Implementation (NSDI 17)}, 613--27. USENIX
Association.

\bibitem[\citeproctext]{ref-circleci}
Denecker, Thomas, William Durand, Julien Maupetit, Charles Hébert,
Jean-Michel Camadro, Pierre Poulain, and Gaëlle Lelandais. 2019.
{``Pixel: A Content Management Platform for Quantitative Omics Data.''}
\emph{PeerJ} 7 (March): e6623. \url{https://doi.org/10.7717/peerj.6623}.

\bibitem[\citeproctext]{ref-jupyter}
Dombrowski, Quinn, Tassie Gniady, and David Kloster. 2020.
{``Introduction Aux Carnets Jupyter.''} \emph{Programming Historian En
Français}, no. 2 (October). \url{https://doi.org/10.46430/phfr0014}.

\bibitem[\citeproctext]{ref-neural_architecture_search_paper}
Elsken, Thomas, Jan Hendrik Metzen, and Frank Hutter. 2018. {``Neural
Architecture Search: A Survey.''} \emph{Journal of Machine Learning
Research} 20 (55): 1--21. \url{http://arxiv.org/abs/1808.05377v3}.

\bibitem[\citeproctext]{ref-gujarati2020serving}
Gujarati, Arpan, Reza Karber, Safraz Musaev, Weiyang Liu, Anurag
Narayanan, Shi Quan Zhong, Mahmut Kandemir, Vyas Sekar, and Alexander
Zadorozhny. 2020. {``Serving DNNs Like Clockwork: Performance
Predictability from the Bottom Up.''} In \emph{14th USENIX Symposium on
Operating Systems Design and Implementation (OSDI 20)}, 443--62. USENIX
Association.

\bibitem[\citeproctext]{ref-ansible}
Hatcher, Blake Douglas. 2024. {``Automating Server Deployments with
Ansible: Utilizing Automation in DevOps.''} \emph{J. Comput. Sci. Coll.}
40 (3): 42--43. \url{https://doi.org/10.5555/3722479.3722494}.

\bibitem[\citeproctext]{ref-hermann2017meet}
Hermann, Jeremy, and Mike Del Balso. 2017. {``Meet Michelangelo: Uber's
Machine Learning Platform.''}
\textless https://eng.uber.com/michelangelo-machine-learning-platform/\textgreater.

\bibitem[\citeproctext]{ref-uber2017michelangelo}
Hermann, Jeremy, and Mike Del Balso. 2017. {``Michelangelo: Uber's
Machine Learning Platform.''} In \emph{Data Engineering Bulletin},
40:8--21. 4.

\bibitem[\citeproctext]{ref-fastai}
Howard, Jeremy, and Sylvain Gugger. 2024. {``Fast.ai.''}
\url{https://www.fast.ai/}.

\bibitem[\citeproctext]{ref-watson_openscale}
IBM. 2024. {``IBM Watson OpenScale.''}
\url{https://www.ibm.com/cloud/watson-openscale}.

\bibitem[\citeproctext]{ref-dvc}
Iterative. 2024. {``Data Version Control (DVC).''}
\url{https://dvc.org/}.

\bibitem[\citeproctext]{ref-github_actions}
Jin, Suoqin, Maksim V. Plikus, and Qing Nie. 2024. {``CellChat for
Systematic Analysis of Cell--Cell Communication from Single-Cell
Transcriptomics.''} \emph{Nature Protocols} 20 (1): 180--219.
\url{https://doi.org/10.1038/s41596-024-01045-4}.

\bibitem[\citeproctext]{ref-grafana}
Labs, Grafana. 2024. {``Grafana.''} \url{https://grafana.com/}.

\bibitem[\citeproctext]{ref-li2023alpaserve}
Li, Zhuohan, Lianmin Zheng, Yinmin Zhong, Vincent Liu, Ying Sheng, Xin
Jin, Yanping Huang, et al. 2023. {``\(\{\)AlpaServe\(\}\): Statistical
Multiplexing with Model Parallelism for Deep Learning Serving.''} In
\emph{17th USENIX Symposium on Operating Systems Design and
Implementation (OSDI 23)}, 663--79.

\bibitem[\citeproctext]{ref-github}
Lima, Marcia, Igor Steinmacher, Denae Ford, Evangeline Liu, Grace
Vorreuter, Tayana Conte, and Bruno Gadelha. 2023. {``Looking for Related
Posts on GitHub Discussions.''} \emph{PeerJ Computer Science} 9
(November): e1567. \url{https://doi.org/10.7717/peerj-cs.1567}.

\bibitem[\citeproctext]{ref-scikit_learn_confusion_matrix}
Liu, Shixia, Jiannan Xiao, Junlin Liu, Xiting Wang, Jing Wu, and Jun
Zhu. 2018. {``Visual Diagnosis of Tree Boosting Methods.''} \emph{IEEE
Transactions on Visualization and Computer Graphics} 24 (1): 163--73.
\url{https://doi.org/10.1109/tvcg.2017.2744378}.

\bibitem[\citeproctext]{ref-terraform}
McElwain, Jennifer C., William J. Matthaeus, Catarina Barbosa, Christos
Chondrogiannis, Katie O' Dea, Bea Jackson, Antonietta B. Knetge, et al.
2024. {``Functional Traits of Fossil Plants.''} \emph{New Phytologist}
242 (2): 392--423. \url{https://doi.org/10.1111/nph.19622}.

\bibitem[\citeproctext]{ref-llama_meta}
McMurray, John J. V., Milton Packer, Akshay S. Desai, Jianjian Gong,
Martin P. Lefkowitz, Adel R. Rizkala, Jean L. Rouleau, et al. 2014.
{``Angiotensin--Neprilysin Inhibition Versus Enalapril in Heart
Failure.''} \emph{New England Journal of Medicine} 371 (11): 993--1004.
\url{https://doi.org/10.1056/nejmoa1409077}.

\bibitem[\citeproctext]{ref-dbt}
Naghavi, Mohsen, Hmwe Hmwe Kyu, Bhoomadevi A, Mohammad Amin Aalipour,
Hasan Aalruz, Hazim S Ababneh, Bedru J Abafita, et al. 2025. {``Global
Burden of 292 Causes of Death in 204 Countries and Territories and 660
Subnational Locations, 1990--2023: A Systematic Analysis for the Global
Burden of Disease Study 2023.''} \emph{The Lancet} 406 (10513):
1811--72. \url{https://doi.org/10.1016/s0140-6736(25)01917-8}.

\bibitem[\citeproctext]{ref-metaflow}
Netflix. 2024. {``Metaflow.''} \url{https://metaflow.org/}.

\bibitem[\citeproctext]{ref-mlflow_website}
NOGARE, DIEGO, ISMAR F. SILVEIRA, RENATO BANZAI, and MAÍNA C. ALEXANDRE.
2025. {``Make or Buy Strategy for Machine Learning Operations --
MLOps.''} \emph{Anais Da Academia Brasileira de Ciências} 97 (2):
e20240924. \url{https://doi.org/10.1590/0001-3765202520240924}.

\bibitem[\citeproctext]{ref-nvidia_triton}
NVIDIA. 2025. {``Productionizing GPU Inference on EKS with KServe and
NVIDIA Triton.''} \emph{American International Journal of Computer
Science and Technology} 7 (6).
\url{https://doi.org/10.63282/3117-5481/aijcst-v7i6p104}.

\bibitem[\citeproctext]{ref-kreuzberger2022machine}
Paleyes, Andrei, Raoul-Gabriel Urma, and Neil D. Lawrence. 2022.
{``Challenges in Deploying Machine Learning: A Survey of Case
Studies.''} \emph{ACM Computing Surveys} 55 (6): 1--29.
\url{https://doi.org/10.1145/3533378}.

\bibitem[\citeproctext]{ref-google_cloud}
Qin, Yujia, Angela Maggio, Dale Hawkins, Laura Beaudry, Allen Kim,
Daniel Pan, Ting Gong, Yuanyuan Fu, Hua Yang, and Youping Deng. 2024.
{``Whole-Genome Bisulfite Sequencing Data Analysis Learning Module on
Google Cloud Platform.''} \emph{Briefings in Bioinformatics} 25
(Supplement\_1). \url{https://doi.org/10.1093/bib/bbae236}.

\bibitem[\citeproctext]{ref-kubernetes}
Rahman, Akond, Gerry Dozier, and Yue Zhang. 2025. {``Authorship of Minor
Contributors in Kubernetes Configuration Scripts: An Exploratory
Study.''} In \emph{Proceedings of the 33rd ACM International Conference
on the Foundations of Software Engineering}, 1424--27. ACM.
\url{https://doi.org/10.1145/3696630.3731618}.

\bibitem[\citeproctext]{ref-rainio2024evaluation}
Rainio, Oona, Jarmo Teuho, and Riku Klén. 2024. {``Evaluation Metrics
and Statistical Tests for Machine Learning.''} \emph{Scientific Reports}
14 (1): 6086. \url{https://doi.org/10.1038/s41598-024-56706-x}.

\bibitem[\citeproctext]{ref-hyperparameter_tuning_website}
Ranjit, Mercy Prasanna, Gopinath Ganapathy, Kalaivani Sridhar, and
Vikram Arumugham. 2019. {``Efficient Deep Learning Hyperparameter Tuning
Using Cloud Infrastructure: Intelligent Distributed Hyperparameter
Tuning with Bayesian Optimization in the Cloud.''} In \emph{2019 IEEE
12th International Conference on Cloud Computing (CLOUD)}, 520--22.
IEEE. \url{https://doi.org/10.1109/cloud.2019.00097}.

\bibitem[\citeproctext]{ref-reddi2023mlperf}
Reddi, Vijay Janapa, Christine Cheng, David Kanter, Peter Mattson,
Guenther Schmuelling, Carole-Jean Wu, Brian Anderson, et al. 2019.
{``MLPerf Inference Benchmark.''} \emph{arXiv Preprint
arXiv:1911.02549}, November. \url{http://arxiv.org/abs/1911.02549v2}.

\bibitem[\citeproctext]{ref-lime_github}
Ribeiro, Marco Tulio, Sameer Singh, and Carlos Guestrin. 2016. {``LIME
(Local Interpretable Model-Agnostic Explanations).''}
\url{https://github.com/marcotcr/lime}.

\bibitem[\citeproctext]{ref-prometheus}
Rinella, Mary E., Brent A. Neuschwander-Tetri, Mohammad Shadab Siddiqui,
Manal F. Abdelmalek, Stephen Caldwell, Diana Barb, David E. Kleiner, and
Rohit Loomba. 2023. {``AASLD Practice Guidance on the Clinical
Assessment and Management of Nonalcoholic Fatty Liver Disease.''}
\emph{Hepatology} 77 (5): 1797--1835.
\url{https://doi.org/10.1097/hep.0000000000000323}.

\bibitem[\citeproctext]{ref-ibm_data_drift}
Rı́os, Carlos, Nathan Youngblood, Zengguang Cheng, Manuel Le Gallo,
Wolfram H. P. Pernice, C. David Wright, Abu Sebastian, and Harish
Bhaskaran. 2019. {``In-Memory Computing on a Photonic Platform.''}
\emph{Science Advances} 5 (2): eaau5759.
\url{https://doi.org/10.1126/sciadv.aau5759}.

\bibitem[\citeproctext]{ref-romero2021infaas}
Romero, Francisco, Qian Li 0027, Neeraja J. Yadwadkar, and Christos
Kozyrakis. 2021. {``INFaaS: Automated Model-Less Inference Serving.''}
In \emph{2021 USENIX Annual Technical Conference (USENIX ATC 21)},
397--411. USENIX Association.
\url{https://www.usenix.org/conference/atc21/presentation/romero}.

\bibitem[\citeproctext]{ref-azure}
Rosa, Benedetta, Filippo Colombo Zefinetti, Andrea Vitali, and Daniele
Regazzoni. 2021. {``RGB-d Sensors as Marker-Less MOCAP Systems: A
Comparison Between Microsoft Kinect V2 and the New Microsoft Kinect
Azure.''} In \emph{Advances in Simulation and Digital Human Modeling},
359--67. Springer International Publishing.
\url{https://doi.org/10.1007/978-3-030-79763-8/_43}.

\bibitem[\citeproctext]{ref-elastic}
Ryu, Jaehyeon, Yi Qiang, Longtu Chen, Gen Li, Xun Han, Eric Woon, Tianyu
Bai, et al. 2024. {``Multifunctional Nanomesh Enables
Cellular-Resolution, Elastic Neuroelectronics.''} \emph{Advanced
Materials} 36 (36): e2403141.
\url{https://doi.org/10.1002/adma.202403141}.

\bibitem[\citeproctext]{ref-schelter2018automating}
Schelter, Sebastian, Matthias Boehm, Johannes Kirschnick, Kostas
Tzoumas, and Gunnar Ratsch. 2018. {``Automating Large-Scale Machine
Learning Model Management.''} In \emph{Proceedings of the 2018 IEEE
International Conference on Data Engineering (ICDE)}, 137--48. IEEE.

\bibitem[\citeproctext]{ref-sculley2015hidden}
Sculley, D., Gary Holt, Daniel Golovin, Eugene Davydov, Todd Phillips,
Dietmar Ebner, Vinay Chaudhary, Michael Young, Jean-François Crespo, and
Dan Dennison. 2021. {``Technical Debt in Machine Learning Systems.''} In
\emph{Technical Debt in Practice}, 28:177--92. The MIT Press.
\url{https://doi.org/10.7551/mitpress/12440.003.0011}.

\bibitem[\citeproctext]{ref-wandb}
Selvan, Jerin Paul, and Girish Pandurang Potdar. 2024. {``Calculation of
Neural Network Weights and Biases Using Particle Swarm Optimization.''}
In \emph{RAiSE-2023}, 190. MDPI.
\url{https://doi.org/10.3390/engproc2023059190}.

\bibitem[\citeproctext]{ref-aws_cloudformation}
Services, Amazon Web. 2020. {``AWS Certified SysOps Administrator Study
Guide, 2E.''} Wiley. \url{https://doi.org/10.1002/9781119561569.ch18}.

\bibitem[\citeproctext]{ref-jenkins}
Soni, R. A., K. A. Gallivan, and W. K. Jenkins. 1999. {``Affine
Projection Methods in Fault Tolerant Adaptive Filtering.''} In
\emph{1999 IEEE International Conference on Acoustics, Speech, and
Signal Processing. Proceedings. ICASSP99 (Cat. No.99CH36258)}, 1685--88.
IEEE. \url{https://doi.org/10.1109/icassp.1999.756317}.

\bibitem[\citeproctext]{ref-steck2021netflix}
Steck, Harald, Linas Baltrunas, Ehtsham Elahi, Dawen Liang, Yves
Raimond, and Justin Basilico. 2021. {``Deep Learning for Recommender
Systems: A Netflix Case Study.''} \emph{AI Magazine} 42 (3): 7--18.
\url{https://doi.org/10.1609/aimag.v42i3.18140}.

\bibitem[\citeproctext]{ref-pytorch}
Taylor, JohnMark, and Nikolaus Kriegeskorte. 2023. {``Extracting and
Visualizing Hidden Activations and Computational Graphs of PyTorch
Models with TorchLens.''} \emph{Scientific Reports} 13 (1): 14375.
\url{https://doi.org/10.1038/s41598-023-40807-0}.

\bibitem[\citeproctext]{ref-git}
Torvalds, Linus, and Junio Hamano. 2024. {``Git.''}
\url{https://git-scm.com/}.

\bibitem[\citeproctext]{ref-prefect}
Wang, Qian, Zhonghao Li, Dengyun Nie, Xinru Mu, Yuxuan Wang, Yongwei
Jiang, Yongchen Zhang, and Zhigang Lu. 2024. {``Low-Frequency
Electroacupuncture Exerts Antinociceptive Effects Through Activation of
POMC Neural Circuit Induced Endorphinergic Input to the Periaqueductal
Gray from the Arcuate Nucleus.''} \emph{Molecular Pain} 20 (January):
17448069241254201. \url{https://doi.org/10.1177/17448069241254201}.

\bibitem[\citeproctext]{ref-wu2019machine}
Wu, Carole-Jean, David Brooks, Kevin Chen, Douglas Chen, Sy Choudhury,
Marat Dukhan, Kim Hazelwood, et al. 2019. {``Machine Learning at
Facebook: Understanding Inference at the Edge.''} In \emph{2019 IEEE
International Symposium on High Performance Computer Architecture
(HPCA)}, 331--44. IEEE. \url{https://doi.org/10.1109/hpca.2019.00048}.

\bibitem[\citeproctext]{ref-aws_sagemaker}
Ye, Wenming, Rachel Hu, and Miro Enev. 2020. {``Put Deep Learning to
Work: Accelerate Deep Learning Through Amazon SageMaker and ML
Services.''} In \emph{Proceedings of the 26th ACM SIGKDD International
Conference on Knowledge Discovery \&Amp; Data Mining}, 3496--96. ACM.
\url{https://doi.org/10.1145/3394486.3406698}.

\bibitem[\citeproctext]{ref-zhang2019mark}
Zhang, Chengliang, Minchen Yu, Wei Wang 0030, and Feng Yan 0001. 2019.
{``MArk: Exploiting Cloud Services for Cost-Effective, SLO-Aware Machine
Learning Inference Serving.''} In \emph{2019 USENIX Annual Technical
Conference (USENIX ATC 19)}, 1049--62. USENIX Association.
\url{https://www.usenix.org/conference/atc19/presentation/zhang-chengliang}.

\bibitem[\citeproctext]{ref-zhang2017highly}
Zhang, Qingxue, Dian Zhou, and Xuan Zeng. 2017. {``Highly Wearable
Cuff-Less Blood Pressure and Heart Rate Monitoring with Single-Arm
Electrocardiogram and Photoplethysmogram Signals.''} \emph{BioMedical
Engineering OnLine} 16 (1): 23.
\url{https://doi.org/10.1186/s12938-017-0317-z}.

\end{CSLReferences}


\backmatter

\clearpage


\end{document}
