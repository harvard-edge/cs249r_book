% Options for packages loaded elsewhere
% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode,linktoc=all,pdfwindowui,pdfpagemode=FullScreen,pdfpagelayout=TwoPageRight}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  9pt,
  letterpaper,
  abstract,
  titlepage]{scrbook}
\usepackage{xcolor}
\usepackage[left=1in,marginparwidth=2.0666666666667in,textwidth=4.1333333333333in,marginparsep=0.3in]{geometry}
\usepackage{amsmath,amssymb}
\setcounter{secnumdepth}{3}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
% Make \paragraph and \subparagraph free-standing
\makeatletter
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}{
    \@ifstar
      \xxxParagraphStar
      \xxxParagraphNoStar
  }
  \newcommand{\xxxParagraphStar}[1]{\oldparagraph*{#1}\mbox{}}
  \newcommand{\xxxParagraphNoStar}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}{
    \@ifstar
      \xxxSubParagraphStar
      \xxxSubParagraphNoStar
  }
  \newcommand{\xxxSubParagraphStar}[1]{\oldsubparagraph*{#1}\mbox{}}
  \newcommand{\xxxSubParagraphNoStar}[1]{\oldsubparagraph{#1}\mbox{}}
\fi
\makeatother

\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{242,244,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{#1}}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.82,0.10,0.26}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{#1}}
\newcommand{\BuiltInTok}[1]{\textcolor[rgb]{0.88,0.40,0.10}{#1}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.41,0.45,0.49}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.86,0.20,0.18}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.58,0.00,0.30}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{#1}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.75,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.88,0.40,0.10}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.42,0.37,0.78}{#1}}
\newcommand{\ImportTok}[1]{\textcolor[rgb]{0.00,0.45,0.70}{\textbf{#1}}}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.73,0.49,0.84}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.10,0.10,0.10}{#1}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{#1}}
\newcommand{\RegionMarkerTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.12,0.55,0.30}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{#1}}}}

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\newsavebox\pandoc@box
\newcommand*\pandocbounded[1]{% scales image to fit in text height/width
  \sbox\pandoc@box{#1}%
  \Gscale@div\@tempa{\textheight}{\dimexpr\ht\pandoc@box+\dp\pandoc@box\relax}%
  \Gscale@div\@tempb{\linewidth}{\wd\pandoc@box}%
  \ifdim\@tempb\p@<\@tempa\p@\let\@tempa\@tempb\fi% select the smaller of both
  \ifdim\@tempa\p@<\p@\scalebox{\@tempa}{\usebox\pandoc@box}%
  \else\usebox{\pandoc@box}%
  \fi%
}
% Set default figure placement to htbp
\def\fps@figure{htbp}
\makeatother
% definitions for citeproc citations
\NewDocumentCommand\citeproctext{}{}
\NewDocumentCommand\citeproc{mm}{%
  \begingroup\def\citeproctext{#2}\cite{#1}\endgroup}
\makeatletter
 % allow citations to break across lines
 \let\@cite@ofmt\@firstofone
 % avoid brackets around text for \cite:
 \def\@biblabel#1{}
 \def\@cite#1#2{{#1\if@tempswa , #2\fi}}
\makeatother
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newenvironment{CSLReferences}[2] % #1 hanging-indent, #2 entry-spacing
 {\begin{list}{}{%
  \setlength{\itemindent}{0pt}
  \setlength{\leftmargin}{0pt}
  \setlength{\parsep}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1
   \setlength{\leftmargin}{\cslhangindent}
   \setlength{\itemindent}{-1\cslhangindent}
  \fi
  % set entry spacing
  \setlength{\itemsep}{#2\baselineskip}}}
 {\end{list}}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{\hfill\break\parbox[t]{\linewidth}{\strut\ignorespaces#1\strut}}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}

% =============================================================================
% LATEX HEADER CONFIGURATION FOR MLSYSBOOK PDF
% =============================================================================
% This file contains all LaTeX package imports, custom commands, and styling
% definitions for the PDF output of the Machine Learning Systems textbook.
%
% Key Features:
% - Harvard crimson branding throughout
% - Custom part/chapter/section styling
% - Professional table formatting with colored headers
% - Margin notes with custom styling
% - TikZ-based part dividers
% - Page numbering (Roman for frontmatter, Arabic for mainmatter)
%
% Note: This file is included via _quarto-pdf.yml and affects PDF output only.
% HTML/EPUB styling is handled separately via CSS files.
% =============================================================================

% =============================================================================
% PACKAGE IMPORTS
% =============================================================================

% Layout and positioning
% \usepackage[outercaption, ragged]{sidecap}  % Commented out to make figure captions inline instead of in margin
\usepackage{adjustbox}      % Adjusting box dimensions
\usepackage{afterpage}      % Execute commands after page break
\usepackage{morefloats}     % Increase number of floats
\usepackage{array}          % Enhanced table column formatting
\usepackage{atbegshi}       % Insert content at page beginning
%\usepackage{changepage}     % Change page dimensions mid-document
\usepackage{emptypage}      % Clear headers/footers on empty pages

% Language and text
\usepackage[english]{babel} % English language support
\usepackage{microtype}      % Improved typography and hyphenation

% Captions and floats
\usepackage{caption}
% Caption styling configuration
%\captionsetup[table]{belowskip=5pt}
\captionsetup{format=plain}
\DeclareCaptionLabelFormat{mylabel}{#1
#2:\hspace{1.0ex}}
\DeclareCaptionFont{ninept}{\fontsize{7pt}{8}\selectfont #1}

% Figure captions: Small font, bold label, ragged right
\captionsetup[figure]{labelfont={bf,ninept},labelsep=space,
belowskip=2pt,aboveskip=6pt,labelformat=mylabel,
justification=raggedright,singlelinecheck=false,font={ninept}}

% Table captions: Small font, bold label, ragged right
\captionsetup[table]{belowskip=6pt,labelfont={bf,ninept},labelsep=none,
labelformat=mylabel,justification=raggedright,singlelinecheck=false,font={ninept}}

% Typography fine-tuning
\emergencystretch=5pt       % Allow extra stretch to avoid overfull boxes

% Utility packages
\usepackage{etoolbox}       % For patching commands and environments

% Page layout and headers
\usepackage{fancyhdr}       % Custom headers and footers
\usepackage{geometry}       % Page dimensions and margins

% Graphics and figures
\usepackage{graphicx}       % Include graphics
\usepackage{float}          % Improved float placement
\usepackage[skins,breakable]{tcolorbox} % Coloured and framed text boxes
\tcbset{before upper=\setlength{\parskip}{3pt}}

% Tables
\usepackage{longtable}      % Multi-page tables

% Fonts and typography
\usepackage{fontspec}       % Font selection for LuaLaTeX
\usepackage{mathptmx}       % Times-like math fonts
\usepackage{newpxtext}      % Palatino-like font for body text

% Colors and visual elements
\usepackage[dvipsnames]{xcolor}  % Extended color support
\usepackage{tikz}           % Programmatic graphics
\usetikzlibrary{positioning}
\usetikzlibrary{calc}
\usepackage{tikzpagenodes}  % TikZ positioning relative to page

% Code listings
\usepackage{listings}       % Code highlighting

% Hyperlinks
\usepackage{hyperref}       % Clickable links in PDF

% Conditional logic
\usepackage{ifthen}         % If-then-else commands

% Math symbols
\usepackage{amsmath}        % AMS math extensions
\usepackage{amssymb}        % AMS math symbols
\usepackage{latexsym}       % Additional LaTeX symbols
\usepackage{pifont}         % Zapf Dingbats symbols
\providecommand{\blacklozenge}{\ding{117}}  % Black diamond symbol

% Lists
\usepackage{enumitem}       % Customizable lists

% Margin notes and sidenotes
\usepackage{marginfix}      % Fixes margin note overflow
\usepackage{marginnote}     % Margin notes
\usepackage{sidenotes}      % Academic-style sidenotes
\renewcommand\raggedrightmarginnote{\sloppy}
\renewcommand\raggedleftmarginnote{\sloppy}

% Typography improvements
\usepackage{ragged2e}       % Better ragged text
\usepackage[all]{nowidow}   % Prevent widows and orphans
\usepackage{needspace}      % Ensure minimum space on page

% Section formatting
\usepackage[explicit]{titlesec}  % Custom section titles
\usepackage{tocloft}        % Table of contents formatting

% QR codes and icons
\usepackage{fontawesome5}   % Font Awesome icons
\usepackage{qrcode}         % QR code generation
\qrset{link, height=15mm}

% =============================================================================
% FLOAT CONFIGURATION
% =============================================================================
% Allow more floats per page to handle figure-heavy chapters
\extrafloats{200}
\setcounter{topnumber}{12}       % Max floats at top of page
\setcounter{bottomnumber}{12}    % Max floats at bottom of page
\setcounter{totalnumber}{24}     % Max floats per page
\setcounter{dbltopnumber}{8}     % Max floats at top of two-column page
\renewcommand{\topfraction}{.95}  % Max fraction of page for top floats
\renewcommand{\bottomfraction}{.95}
\renewcommand{\textfraction}{.05}  % Min fraction of page for text
\renewcommand{\floatpagefraction}{.7}  % Min fraction of float page
\renewcommand{\dbltopfraction}{.95}

% Prevent "Float(s) lost" errors by flushing floats more aggressively
\usepackage{placeins}  % Provides \FloatBarrier

% =============================================================================
% COLOR DEFINITIONS
% =============================================================================
% Harvard crimson - primary brand color used throughout
\definecolor{crimson}{HTML}{A51C30}

% Quiz element colors
\definecolor{quiz-question-color1}{RGB}{225,243,248}  % Light blue background
\definecolor{quiz-question-color2}{RGB}{17,158,199}   % Blue border
\definecolor{quiz-answer-color1}{RGB}{250,234,241}    % Light pink background
\definecolor{quiz-answer-color2}{RGB}{152,14,90}      % Magenta border

% =============================================================================
% LIST FORMATTING
% =============================================================================
% Tighter list spacing for academic style
\def\tightlist{}
\setlist{itemsep=1pt, parsep=1pt, topsep=0pt,after={\vspace{0.3\baselineskip}}}
\let\tightlist\relax

\makeatletter
\@ifpackageloaded{framed}{}{\usepackage{framed}}
\@ifpackageloaded{fancyvrb}{}{\usepackage{fancyvrb}}
\makeatother

\makeatletter
%New float "codelisting" has been updated
\AtBeginDocument{%
\floatstyle{ruled}
\newfloat{codelisting}{!htb}{lop}
\floatname{codelisting}{Listing}
\floatplacement{codelisting}{!htb}
\captionsetup[codelisting]{labelfont={bf,ninept},labelformat=mylabel,
  singlelinecheck=false,width=\linewidth,labelsep=none,font={ninept}}%
\renewenvironment{snugshade}{%
   \def\OuterFrameSep{3pt}%
   \def\FrameCommand{\fboxsep=5pt\colorbox{shadecolor}}%
   \MakeFramed{\advance\hsize-\width\FrameRestore}%
   \leftskip 0.5em \rightskip 0.5em%
   \small% decrease font size
   }{\endMakeFramed}%
}
\makeatother

%The space before and after the verbatim environment "Highlighting" has been reduced
\fvset{listparameters=\setlength{\topsep}{0pt}\setlength{\partopsep}{0pt}}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{framesep=0mm,commandchars=\\\{\}}

\makeatletter
\renewcommand\fs@ruled{\def\@fs@cfont{\bfseries}\let\@fs@capt\floatc@ruled
\def\@fs@pre{\hrule height.8pt depth0pt \kern2pt}%
\def\@fs@post{\kern2pt\hrule\relax}%
\def\@fs@mid{\kern2pt\hrule\kern1pt}%space between float and caption
\let\@fs@iftopcapt\iftrue}
\makeatother


% =============================================================================
% HYPHENATION RULES
% =============================================================================
% Explicit hyphenation points for technical terms to avoid bad breaks
\hyphenation{
  light-weight
  light-weight-ed
  de-vel-op-ment
  un-der-stand-ing
  mod-els
  prin-ci-ples
  ex-per-tise
  com-pli-cat-ed
  blue-print
  per‧for‧mance
  com-mu-ni-ca-tion
  par-a-digms
  hy-per-ten-sion
  a-chieved
}

% =============================================================================
% CODE LISTING CONFIGURATION
% =============================================================================
% Settings for code blocks using listings package
\lstset{
breaklines=true,              % Automatic line wrapping
breakatwhitespace=true,       % Break at whitespace only
basicstyle=\ttfamily,         % Monospace font
frame=none,                   % No frame around code
keepspaces=true,              % Preserve spaces
showspaces=false,             % Don't show space characters
showtabs=false,               % Don't show tab characters
columns=flexible,             % Flexible column width
belowskip=0pt,               % Minimal spacing
aboveskip=0pt
}

% =============================================================================
% PAGE GEOMETRY
% =============================================================================
% MIT Press trim size: 7" x 10" (per publisher specifications)
% This is a standard academic textbook format providing good readability
% for technical content with figures and code blocks.
% Wide outer margin accommodates sidenotes/margin notes.
\geometry{
  paperwidth=7in,
  paperheight=10in,
  top=0.875in,
  bottom=0.875in,
  inner=0.875in,              % Inner margin (binding side)
  outer=1.75in,               % Outer margin (includes space for sidenotes)
  footskip=30pt,
  marginparwidth=1.25in,      % Width for margin notes
  twoside                     % Different left/right pages
}

% =============================================================================
% SIDENOTE STYLING
% =============================================================================
% Custom sidenote design with crimson vertical bar
\renewcommand{\thefootnote}{\textcolor{crimson}{\arabic{footnote}}}

% Save original sidenote command
\makeatletter
\@ifundefined{oldsidenote}{
  \let\oldsidenote\sidenote%
}{}
\makeatother

% Redefine sidenote with vertical crimson bar
\renewcommand{\sidenote}[1]{%
  \oldsidenote{%
    \noindent
    \color{crimson!100}                        % Crimson vertical line
    \raisebox{0em}{%
      \rule{0.5pt}{1.5em}                      % Thin vertical line
    }
    \hspace{0.3em}                             % Space after line
    \color{black}                              % Reset text color
    \footnotesize #1                           % Sidenote content
  }%
}

% =============================================================================
% FLOAT HANDLING
% =============================================================================
% Patch LaTeX's output routine to handle float overflow gracefully
% The "Float(s) lost" error occurs in \@doclearpage when \@currlist is not empty
% This patch silently clears pending floats that can't be placed
\makeatletter
\let\orig@doclearpage\@doclearpage
\def\@doclearpage{%
  \ifx\@currlist\@empty\else
    \global\let\@currlist\@empty
    \typeout{Warning: Floats cleared to prevent overflow}%
  \fi
  \orig@doclearpage
}
\makeatother

% Additional safety for structural commands
\let\originalbackmatter\backmatter
\renewcommand{\backmatter}{%
  \clearpage%
  \originalbackmatter%
}

\let\originalfrontmatter\frontmatter
\renewcommand{\frontmatter}{%
  \clearpage%
  \originalfrontmatter%
}

\let\originalmainmatter\mainmatter
\renewcommand{\mainmatter}{%
  \clearpage%
  \originalmainmatter%
}

% =============================================================================
% PAGE HEADERS AND FOOTERS
% =============================================================================
% Ensure chapters use fancy page style (not plain)
\patchcmd{\chapter}{\thispagestyle{plain}}{\thispagestyle{fancy}}{}{}

% Main page style with crimson headers
\pagestyle{fancy}
\fancyhf{}                                              % Clear all
\fancyhead[LE]{\small\color{crimson}\nouppercase{\rightmark}}  % Left even: section
\fancyhead[RO]{\color{crimson}\thepage}                 % Right odd: page number
\fancyhead[LO]{\small\color{crimson}\nouppercase{\leftmark}}   % Left odd: chapter
\fancyhead[RE]{\color{crimson}\thepage}                 % Right even: page number
\renewcommand{\headrulewidth}{0.4pt}                    % Thin header line
\renewcommand{\footrulewidth}{0pt}                      % No footer line

% Plain page style (for chapter openings)
\fancypagestyle{plain}{
  \fancyhf{}
  \fancyfoot[C]{\color{crimson}\thepage}                % Centered page number
  \renewcommand{\headrulewidth}{0pt}
  \renewcommand{\footrulewidth}{0pt}
}

% =============================================================================
% KOMA-SCRIPT FONT ADJUSTMENTS
% =============================================================================
% Apply crimson color to all heading levels
\addtokomafont{disposition}{\rmfamily\color{crimson}}
\addtokomafont{chapter}{\color{crimson}}
\addtokomafont{section}{\color{crimson}}
\addtokomafont{subsection}{\color{crimson}}

% =============================================================================
% ABSTRACT ENVIRONMENT
% =============================================================================
\newenvironment{abstract}{
  \chapter*{\abstractname}
  \addcontentsline{toc}{chapter}{\abstractname}
  \small
}{
  \clearpage
}

% =============================================================================
% HYPERLINK CONFIGURATION
% =============================================================================
% Crimson-colored links throughout, two-page PDF layout
\hypersetup{
  linkcolor=crimson,
  citecolor=crimson,
  urlcolor=crimson,
  pdfpagelayout=TwoPageRight,   % Two-page spread view
  pdfstartview=Fit               % Initial zoom fits page
}

% =============================================================================
% PART SUMMARY SYSTEM
% =============================================================================
% Allows adding descriptive text below part titles
\newcommand{\partsummary}{}     % Empty by default
\newif\ifhaspartsummary%
\haspartsummaryfalse%

\newcommand{\setpartsummary}[1]{%
  \renewcommand{\partsummary}{#1}%
  \haspartsummarytrue%
}

% Additional colors for part page backgrounds
\definecolor{BrownLL}{RGB}{233,222,220}
\definecolor{BlueDD}{RGB}{62,100,125}
\colorlet{BlueDD}{magenta}

% ===============================================================================
% PART STYLING SYSTEM
% ===============================================================================
%
% This system provides three distinct visual styles for book organization:
%
% 1. NUMBERED PARTS (\part{title}) - For main book sections
%    - Roman numerals (I, II, III, etc.) in top right corner
%    - Crimson title with horizontal lines above/below
%    - "Part I" label in sidebar
%    - Used for: foundations, principles, optimization, deployment, etc.
%
% 2. UNNUMBERED PARTS (\part*{title}) - For special sections like "Labs"
%    - Division-style geometric background (left side)
%    - No Roman numerals
%    - Used for: labs section
%
% 3. DIVISIONS (\division{title}) - For major book divisions
%    - Clean geometric background with centered title
%    - Used for: frontmatter, main_content, backmatter
%
% The Lua filter (inject-parts.lua) automatically routes parts by {key:xxx} commands
% to the appropriate LaTeX command based on the key name.
% ===============================================================================

% NUMBERED PARTS: Roman numeral styling for main book sections
\titleformat{\part}[display]
{\thispagestyle{empty}}{}{20pt}{
\begin{tikzpicture}[remember picture,overlay]
%%%
%%
\node[crimson,align=flush right,
inner sep=0,outer sep=0mm,draw=none,%
anchor=east,minimum height=31mm, text width=1.2\textwidth,
yshift=-30mm,font={%
\fontsize{98pt}{104}\selectfont\bfseries}]  (BG) at (current page text area.north east){\thepart};
%
\node[black,inner sep=0mm,draw=none,
anchor=mid,text width=1.2\textwidth,
 minimum height=35mm, align=right,
node distance=7mm,below=of BG,
font={\fontsize{30pt}{34}\selectfont}]
(BGG)  {\hyphenchar\font=-1 \color{black}\MakeUppercase {#1}};
\draw [crimson,line width=3pt] ([yshift=0mm]BGG.north west) -- ([yshift=0mm]BGG.north east);
\draw [crimson,line width=2pt] ([yshift=0mm]BGG.south west) -- ([yshift=0mm]BGG.south east);
%
\node[fill=crimson,text=white,rotate=90,%
anchor=south west,minimum height=15mm,
minimum width=40mm,font={%
\fontsize{20pt}{20}\selectfont\bfseries}](BP)  at
(current page text area.south east)
{{\sffamily Part}~\thepart};
%
\path[red](BP.north west)-|coordinate(PS)(BGG.south west);
%
% Part summary box commented out for cleaner design
% \ifhaspartsummary
% \node[inner sep=4pt,text width=0.7\textwidth,draw=none,fill=BrownLL!40,
% align=justify,font={\fontsize{9pt}{12}\selectfont},anchor=south west]
% at (PS) {\partsummary};
% \fi
\end{tikzpicture}
}[]

\renewcommand{\thepart}{\Roman{part}}

% UNNUMBERED PARTS: Division-style background for special sections
\titleformat{name=\part,numberless}[display]
{\thispagestyle{empty}}{}{20pt}{
\begin{tikzpicture}[remember picture,overlay]
%%%
\coordinate(S1)at([yshift=-200mm]current page.north west);
\draw[draw=none,fill=BlueDD!7](S1)--++(45:16)coordinate(S2)-
|(S2|-current page.north west)--(current page.north west)coordinate(S3)--(S1);
%
\coordinate(E1)at([yshift=-98mm]current page.north west);
\draw[draw=none,fill=BlueDD!15](E1)--(current page.north west)coordinate(E2)
--++(0:98mm)coordinate(E3)--(E1);
%
\coordinate(D1)at([yshift=15mm]current page.south west);
\draw[draw=none,fill=BlueDD!40,opacity=0.5](D1)--++(45:5.5)coordinate(D2)
-|(D2|-current page.north west)--(current page.north west)coordinate(D3)--(D1);
%%%%
\path[red](S2)-|(S2-|current page.east)coordinate(SS2);
%PART
\node[crimson,align=flush right,inner sep=0,outer sep=0mm,draw=none,anchor=south,
font={\fontsize{48pt}{48}\selectfont\bfseries}]  (BG) at ($(S2)!0.5!(SS2)$){\hphantom{Part}};
%%%
\path[green]([yshift=15mm]D2)-|coordinate(TPD)(BG.south east);
\node[inner sep=0mm,draw=none,anchor=south east,%text width=0.9\textwidth,
align=right,font={\fontsize{40pt}{40}\selectfont}]
(BGG) at (TPD)  {\color{crimson}\MakeUppercase {#1}};%\MakeUppercase {}
\end{tikzpicture}
}

% Define \numberedpart command for numbered parts
\newcommand{\numberedpart}[1]{%
\FloatBarrier%  % Flush all pending floats before part break
\clearpage
\thispagestyle{empty}
\stepcounter{part}%
\begin{tikzpicture}[remember picture,overlay]
%%%
%%
\node[crimson,align=flush right,
inner sep=0,outer sep=0mm,draw=none,%
anchor=east,minimum height=31mm, text width=1.2\textwidth,
yshift=-30mm,font={%
\fontsize{98pt}{104}\selectfont\bfseries}]  (BG) at (current page text area.north east){\thepart};
%
\node[black,inner sep=0mm,draw=none,
anchor=mid,text width=1.2\textwidth,
 minimum height=35mm, align=right,
node distance=7mm,below=of BG,
font={\fontsize{30pt}{34}\selectfont}]
(BGG)  {\hyphenchar\font=-1 \color{black}\MakeUppercase {#1}};
\draw [crimson,line width=3pt] ([yshift=0mm]BGG.north west) -- ([yshift=0mm]BGG.north east);
\draw [crimson,line width=2pt] ([yshift=0mm]BGG.south west) -- ([yshift=0mm]BGG.south east);
%
\node[fill=crimson,text=white,rotate=90,%
anchor=south west,minimum height=15mm,
minimum width=40mm,font={%
\fontsize{20pt}{20}\selectfont\bfseries}](BP)  at
(current page text area.south east)
{{\sffamily Part}~\thepart};
%
\path[red](BP.north west)-|coordinate(PS)(BGG.south west);
%
% Part summary box commented out for cleaner design
% \ifhaspartsummary
% \node[inner sep=4pt,text width=0.7\textwidth,draw=none,fill=BrownLL!40,
% align=justify,font={\fontsize{9pt}{12}\selectfont},anchor=south west]
% at (PS) {\partsummary};
% \fi
\end{tikzpicture}
\clearpage
}



% DIVISIONS: Clean geometric styling with subtle tech elements
% Used for frontmatter, main_content, and backmatter divisions
\newcommand{\division}[1]{%
\FloatBarrier%  % Flush all pending floats before division break
\clearpage
\thispagestyle{empty}
\begin{tikzpicture}[remember picture,overlay]

% Clean geometric background (original design)
\coordinate(S1)at([yshift=-200mm]current page.north west);
\draw[draw=none,fill=BlueDD!7](S1)--++(45:16)coordinate(S2)-
|(S2|-current page.north west)--(current page.north west)coordinate(S3)--(S1);

\coordinate(E1)at([yshift=-98mm]current page.north west);
\draw[draw=none,fill=BlueDD!15](E1)--(current page.north west)coordinate(E2)
--++(0:98mm)coordinate(E3)--(E1);

\coordinate(D1)at([yshift=15mm]current page.south west);
\draw[draw=none,fill=BlueDD!40,opacity=0.5](D1)--++(45:5.5)coordinate(D2)
-|(D2|-current page.north west)--(current page.north west)coordinate(D3)--(D1);

% Subtle tech elements - positioned in white areas for better visibility
% Upper right white area - more visible
\draw[crimson!40, line width=0.8pt] ([xshift=140mm,yshift=-60mm]current page.north west) -- ++(40mm,0);
\draw[crimson!40, line width=0.8pt] ([xshift=150mm,yshift=-70mm]current page.north west) -- ++(30mm,0);
\draw[crimson!35, line width=0.7pt] ([xshift=160mm,yshift=-60mm]current page.north west) -- ++(0,-15mm);
\draw[crimson!35, line width=0.7pt] ([xshift=170mm,yshift=-70mm]current page.north west) -- ++(0,10mm);

% Circuit nodes - upper right
\fill[crimson!50] ([xshift=160mm,yshift=-60mm]current page.north west) circle (1.5mm);
\fill[white] ([xshift=160mm,yshift=-60mm]current page.north west) circle (0.8mm);
\fill[crimson!50] ([xshift=170mm,yshift=-70mm]current page.north west) circle (1.3mm);
\fill[white] ([xshift=170mm,yshift=-70mm]current page.north west) circle (0.6mm);

% Lower right white area - enhanced visibility
\draw[crimson!45, line width=0.9pt] ([xshift=140mm,yshift=-190mm]current page.north west) -- ++(45mm,0);
\draw[crimson!45, line width=0.9pt] ([xshift=150mm,yshift=-200mm]current page.north west) -- ++(35mm,0);
\draw[crimson!40, line width=0.8pt] ([xshift=160mm,yshift=-190mm]current page.north west) -- ++(0,-20mm);
\draw[crimson!40, line width=0.8pt] ([xshift=170mm,yshift=-200mm]current page.north west) -- ++(0,15mm);

% Additional connecting lines in lower right
\draw[crimson!35, line width=0.7pt] ([xshift=130mm,yshift=-180mm]current page.north west) -- ++(25mm,0);
\draw[crimson!35, line width=0.7pt] ([xshift=145mm,yshift=-180mm]current page.north west) -- ++(0,-25mm);

% Circuit nodes - lower right (more prominent)
\fill[crimson!55] ([xshift=160mm,yshift=-190mm]current page.north west) circle (1.6mm);
\fill[white] ([xshift=160mm,yshift=-190mm]current page.north west) circle (0.9mm);
\fill[crimson!55] ([xshift=170mm,yshift=-200mm]current page.north west) circle (1.4mm);
\fill[white] ([xshift=170mm,yshift=-200mm]current page.north west) circle (0.7mm);
\fill[crimson!50] ([xshift=145mm,yshift=-180mm]current page.north west) circle (1.2mm);
\fill[white] ([xshift=145mm,yshift=-180mm]current page.north west) circle (0.6mm);

% Title positioned in center - clean and readable
\node[inner sep=0mm,draw=none,anchor=center,text width=0.8\textwidth,
align=center,font={\fontsize{40pt}{40}\selectfont}]
(BGG) at (current page.center)  {\color{crimson}\MakeUppercase {#1}};

\end{tikzpicture}
\clearpage
}

% LAB DIVISIONS: Circuit-style neural network design for lab sections
% Used specifically for lab platform sections (arduino, xiao, grove, etc.)
\newcommand{\labdivision}[1]{%
\FloatBarrier%  % Flush all pending floats before lab division break
\clearpage
\thispagestyle{empty}
\begin{tikzpicture}[remember picture,overlay]
% Circuit background with subtle gradient
\coordinate(S1)at([yshift=-200mm]current page.north west);
\draw[draw=none,fill=BlueDD!5](S1)--++(45:16)coordinate(S2)-
|(S2|-current page.north west)--(current page.north west)coordinate(S3)--(S1);

% TOP AREA: Circuit lines in upper white space
\draw[crimson!50, line width=1.5pt] ([xshift=30mm,yshift=-40mm]current page.north west) -- ++(60mm,0);
\draw[crimson!40, line width=1pt] ([xshift=120mm,yshift=-50mm]current page.north west) -- ++(50mm,0);
\draw[crimson!50, line width=1.5pt] ([xshift=40mm,yshift=-70mm]current page.north west) -- ++(40mm,0);

% Connecting lines in top area
\draw[crimson!30, line width=1pt] ([xshift=60mm,yshift=-40mm]current page.north west) -- ++(0,-20mm);
\draw[crimson!30, line width=1pt] ([xshift=145mm,yshift=-50mm]current page.north west) -- ++(0,10mm);

% Neural nodes in top area
\fill[crimson!70] ([xshift=60mm,yshift=-40mm]current page.north west) circle (2.5mm);
\fill[white] ([xshift=60mm,yshift=-40mm]current page.north west) circle (1.5mm);
\fill[crimson!60] ([xshift=145mm,yshift=-50mm]current page.north west) circle (2mm);
\fill[white] ([xshift=145mm,yshift=-50mm]current page.north west) circle (1mm);
\fill[crimson!80] ([xshift=80mm,yshift=-70mm]current page.north west) circle (2mm);
\fill[white] ([xshift=80mm,yshift=-70mm]current page.north west) circle (1mm);

% BOTTOM AREA: Circuit lines in lower white space
\draw[crimson!50, line width=1.5pt] ([xshift=20mm,yshift=-200mm]current page.north west) -- ++(70mm,0);
\draw[crimson!40, line width=1pt] ([xshift=110mm,yshift=-210mm]current page.north west) -- ++(60mm,0);
\draw[crimson!50, line width=1.5pt] ([xshift=35mm,yshift=-230mm]current page.north west) -- ++(45mm,0);

% Connecting lines in bottom area
\draw[crimson!30, line width=1pt] ([xshift=55mm,yshift=-200mm]current page.north west) -- ++(0,-20mm);
\draw[crimson!30, line width=1pt] ([xshift=140mm,yshift=-210mm]current page.north west) -- ++(0,15mm);

% Neural nodes in bottom area
\fill[crimson!70] ([xshift=55mm,yshift=-200mm]current page.north west) circle (2.5mm);
\fill[white] ([xshift=55mm,yshift=-200mm]current page.north west) circle (1.5mm);
\fill[crimson!60] ([xshift=140mm,yshift=-210mm]current page.north west) circle (2mm);
\fill[white] ([xshift=140mm,yshift=-210mm]current page.north west) circle (1mm);
\fill[crimson!80] ([xshift=80mm,yshift=-230mm]current page.north west) circle (2mm);
\fill[white] ([xshift=80mm,yshift=-230mm]current page.north west) circle (1mm);

% SIDE AREAS: Subtle circuit elements on left and right edges
\draw[crimson!30, line width=1pt] ([xshift=15mm,yshift=-120mm]current page.north west) -- ++(20mm,0);
\draw[crimson!30, line width=1pt] ([xshift=175mm,yshift=-130mm]current page.north west) -- ++(15mm,0);
\fill[crimson!50] ([xshift=25mm,yshift=-120mm]current page.north west) circle (1.5mm);
\fill[white] ([xshift=25mm,yshift=-120mm]current page.north west) circle (0.8mm);
\fill[crimson!50] ([xshift=185mm,yshift=-130mm]current page.north west) circle (1.5mm);
\fill[white] ([xshift=185mm,yshift=-130mm]current page.north west) circle (0.8mm);

% Title positioned in center - CLEAN AREA
\node[inner sep=0mm,draw=none,anchor=center,text width=0.8\textwidth,
align=center,font={\fontsize{44pt}{44}\selectfont\bfseries}]
(BGG) at (current page.center)  {\color{crimson}\MakeUppercase {#1}};

\end{tikzpicture}
\clearpage
}

% Define \lab command for lab styling (different visual treatment)
\newcommand{\lab}[1]{%
\begin{tikzpicture}[remember picture,overlay]
%%%
% Different background pattern for labs
\coordinate(S1)at([yshift=-200mm]current page.north west);
\draw[draw=none,fill=BlueDD!15](S1)--++(45:16)coordinate(S2)-
|(S2|-current page.north west)--(current page.north west)coordinate(S3)--(S1);
%
\coordinate(E1)at([yshift=-98mm]current page.north west);
\draw[draw=none,fill=BlueDD!25](E1)--(current page.north west)coordinate(E2)
--++(0:98mm)coordinate(E3)--(E1);
%
\coordinate(D1)at([yshift=15mm]current page.south west);
\draw[draw=none,fill=BlueDD!60,opacity=0.7](D1)--++(45:5.5)coordinate(D2)
-|(D2|-current page.north west)--(current page.north west)coordinate(D3)--(D1);
%%%%
\path[red](S2)-|(S2-|current page.east)coordinate(SS2);
%LAB - Different styling
\node[crimson,align=flush right,inner sep=0,outer sep=0mm,draw=none,anchor=south,
font={\fontsize{48pt}{48}\selectfont\bfseries}]  (BG) at ($(S2)!0.5!(SS2)$){\hphantom{Workshop}};
%%%
\path[green]([yshift=15mm]D2)-|coordinate(TPD)(BG.south east);
\node[inner sep=0mm,draw=none,anchor=south east,%text width=0.9\textwidth,
align=right,font={\fontsize{40pt}{40}\selectfont}]
(BGG) at (TPD)  {\color{crimson}\MakeUppercase {#1}};%\MakeUppercase {}
\end{tikzpicture}
\thispagestyle{empty}
\clearpage
}

% =============================================================================
% SECTION FORMATTING
% =============================================================================
% All section levels use crimson color and are ragged right

% Section (Large, bold, crimson)
\titleformat{\section}
  {\normalfont\Large\bfseries\color{crimson}\raggedright}
  {\thesection}
  {0.5em}
  {#1}
\titlespacing*{\section}{0pc}{14pt plus 4pt minus 4pt}{6pt plus 2pt minus 2pt}[0pc]

% Subsection (large, bold, crimson)
\titleformat{\subsection}
  {\normalfont\large\bfseries\color{crimson}\raggedright}
  {\thesubsection}
  {0.5em}
  {#1}
\titlespacing*{\subsection}{0pc}{12pt plus 4pt minus 4pt}{5pt plus 1pt minus 2pt}[0pc]

% Subsubsection (normal size, bold, crimson)
\titleformat{\subsubsection}
  {\normalfont\normalsize\bfseries\color{crimson}\raggedright}
  {\thesubsubsection}
  {0.5em}
  {#1}
\titlespacing*{\subsubsection}{0pc}{12pt plus 4pt minus 4pt}{5pt plus 1pt minus 2pt}[0pc]

% Paragraph (run-in, bold, crimson, ends with period)
\titleformat{\paragraph}[runin]
  {\normalfont\normalsize\bfseries\color{crimson}}
  {\theparagraph}
  {0.5em}
  {#1}
  [\textbf{.}]
  \titlespacing*{\paragraph}{0pc}{6pt plus 2pt minus 2pt}{0.5em}[0pc]

% Subparagraph (run-in, italic, crimson, ends with period)
\titleformat{\subparagraph}[runin]
  {\normalfont\normalsize\itshape\color{crimson}}
  {\thesubparagraph}
  {0.5em}
  {#1}
  [\textbf{.}]
  \titlespacing*{\subparagraph}{0pc}{6pt plus 2pt minus 2pt}{0.5em}[0pc]

% =============================================================================
% CHAPTER FORMATTING
% =============================================================================
% Numbered chapters: "Chapter X" prefix, huge crimson title
\titleformat{\chapter}[display]
  {\normalfont\huge\bfseries\color{crimson}}
  {\chaptername\ \thechapter}
  {20pt}
  {\Huge #1}
  []

% Unnumbered chapters: no prefix, huge crimson title
\titleformat{name=\chapter,numberless}
  {\normalfont\huge\bfseries\color{crimson}}
  {}
  {0pt}
  {\Huge #1}
  []

\renewcommand{\chaptername}{Chapter}
% =============================================================================
% TABLE OF CONTENTS FORMATTING
% =============================================================================
\setcounter{tocdepth}{2}                      % Show chapters, sections, subsections

% TOC spacing adjustments for number widths and indentation
\setlength{\cftchapnumwidth}{2em}             % Chapter number width
\setlength{\cftsecnumwidth}{2.75em}           % Section number width
\setlength{\cftsubsecnumwidth}{3.25em}        % Subsection number width
\setlength{\cftsubsubsecnumwidth}{4em}        % Subsubsection number width
\setlength{\cftsubsecindent}{4.25em}          % Subsection indent
\setlength{\cftsubsubsecindent}{7.5em}        % Subsubsection indent

% Chapter entries in TOC: bold crimson with "Chapter" prefix
\renewcommand{\cftchapfont}{\bfseries\color{crimson}}
\renewcommand{\cftchappresnum}{\color{crimson}Chapter~}

% Custom formatting for division entries (styled like parts)
\newcommand{\divisionchapter}[1]{%
  \addvspace{12pt}%
  \noindent\hfil\bfseries\color{crimson}#1\hfil\par%
  \addvspace{6pt}%
}

% Adjust TOC spacing for "Chapter" prefix
\newlength{\xtraspace}
\settowidth{\xtraspace}{\cftchappresnum\cftchapaftersnum}
\addtolength{\cftchapnumwidth}{\xtraspace}

% Unnumbered chapters with TOC entry
\newcommand{\likechapter}[1]{%
    \chapter*{#1}
    \addcontentsline{toc}{chapter}{\textcolor{crimson}{#1}}
}

% =============================================================================
% PAGE NUMBERING SYSTEM
% =============================================================================
% Implements traditional book numbering:
% - Roman numerals (i, ii, iii...) for frontmatter
% - Arabic numerals (1, 2, 3...) for mainmatter
% Automatically switches at first numbered chapter
\makeatletter
\newif\if@firstnumbered%
\@firstnumberedtrue%
\newif\if@firstunnumbered%
\@firstunnumberedtrue%

\newcounter{lastRomanPage}
\setcounter{lastRomanPage}{1}

% Start document with Roman numerals (frontmatter)
\AtBeginDocument{
  \pagenumbering{roman}
  \renewcommand{\thepage}{\roman{page}}
}

% Intercept chapter command
\let\old@chapter\chapter%
\renewcommand{\chapter}{%
  \@ifstar{\unnumbered@chapter}{\numbered@chapter}%
}

% Numbered chapters: switch to Arabic on first occurrence
\newcommand{\numbered@chapter}[1]{%
  \if@firstnumbered%
    \cleardoublepage%
    \setcounter{lastRomanPage}{\value{page}}%
    \pagenumbering{arabic}%
    \@firstnumberedfalse%
  \else
    \setcounter{page}{\value{page}}%
  \fi
  \setcounter{sidenote}{1}                    % Reset footnote counter per chapter
  \old@chapter{#1}%
}

% Unnumbered chapters: stay in Roman numerals
\newcommand{\unnumbered@chapter}[1]{%
  \if@firstunnumbered%
    \clearpage
    \setcounter{lastRomanPage}{\value{page}}%
    \pagenumbering{roman}%
    \@firstunnumberedfalse%
  \fi
  \setcounter{sidenote}{1}
  \old@chapter*{#1}%
}
\makeatother

% =============================================================================
% TABLE SIZING AND SPACING
% =============================================================================
% Make tables slightly smaller to fit more content
\AtBeginEnvironment{longtable}{\scriptsize}

% Increase vertical spacing in table cells (default is 1.0)
\renewcommand{\arraystretch}{1.5}

% Prefer placing figures and tables at the top of pages
\makeatletter
\renewcommand{\fps@figure}{t}  % Default placement: top of page
\renewcommand{\fps@table}{t}   % Default placement: top of page
\makeatother

% =============================================================================
% LONGTABLE PAGE BREAKING FIXES (Windows compatibility)
% =============================================================================
% Prevent "Infinite glue shrinkage" errors on Windows LaTeX builds
% by giving longtable more flexibility in page breaking

% Allow more flexible page breaking (vs strict \flushbottom)
\raggedbottom

% Process more rows before attempting page break (default is 20)
\setcounter{LTchunksize}{50}

% Add extra stretch for longtable environments specifically
\AtBeginEnvironment{longtable}{%
  \setlength{\emergencystretch}{3em}%
  \setlength{\parskip}{0pt plus 1pt}%
}

% =============================================================================
% TABLE STYLING - Clean tables with crimson borders
% =============================================================================
% Professional table appearance with:
% - Clean white background (no colored rows)
% - Crimson-colored borders
% - Good spacing for readability
%
% Note: Headers are automatically bolded by Quarto when using **text** in source
\usepackage{booktabs}      % Professional table rules (\toprule, \midrule, \bottomrule)
\usepackage{colortbl}      % For colored borders (\arrayrulecolor)

% Global table styling - crimson borders
\setlength{\arrayrulewidth}{0.5pt}          % Thinner borders than default
%\arrayrulecolor{crimson}                    % Crimson borders matching brand

\setcounter{chapter}{0}

% =============================================================================
% DROP CAPS (Lettrine)
% =============================================================================
% Decorative large first letter at chapter openings, following the tradition
% of Hennessy & Patterson and other MIT Press textbooks.
% Usage in QMD: \lettrine{T}{he first sentence...}
\usepackage{lettrine}
\renewcommand{\LettrineFontHook}{\color{crimson}\bfseries}
\setcounter{DefaultLines}{3}          % Drop cap spans 3 lines
\renewcommand{\DefaultLoversize}{0.1} % Slight oversize for visual weight
\renewcommand{\DefaultLraise}{0}      % No vertical shift
\setlength{\DefaultNindent}{0.5em}    % Indent of continuation text
\setlength{\DefaultSlope}{0pt}        % No slope on continuation

% =============================================================================
% RUNNING HEADERS — Truncation Safety
% =============================================================================
% Long chapter/section titles can overflow the header. These marks truncate
% gracefully so headers stay within the text block.
\renewcommand{\chaptermark}[1]{%
  \markboth{\thechapter.\ #1}{}}
\renewcommand{\sectionmark}[1]{%
  \markright{\thesection\ #1}}

% =============================================================================
% EPIGRAPH ENVIRONMENT
% =============================================================================
% For chapter-opening quotations. Renders as right-aligned italic block
% with attribution in small caps below.
% Usage: \epigraph{Quote text}{Author Name, \textit{Source}}
\newcommand{\bookepigraph}[2]{%
  \vspace{1em}%
  \begin{flushright}%
    \begin{minipage}{0.75\textwidth}%
      \raggedleft\itshape\small #1\\[0.5em]%
      \normalfont\small --- #2%
    \end{minipage}%
  \end{flushright}%
  \vspace{1.5em}%
}

% =============================================================================
% THUMB INDEX TABS
% =============================================================================
% Colored tabs on the outer page edge for quick chapter navigation.
% Each Part gets a different vertical position; chapters within a Part
% share the same tab position. Visible when flipping through the book.
\newcounter{thumbindex}
\setcounter{thumbindex}{0}
\newlength{\thumbtabheight}
\setlength{\thumbtabheight}{16mm}     % Height of each tab
\newlength{\thumbtabwidth}
\setlength{\thumbtabwidth}{8mm}       % Width protruding from edge
\newlength{\thumbtabgap}
\setlength{\thumbtabgap}{1mm}         % Gap between tabs

% Advance to next thumb tab position (call at each \part)
\newcommand{\nextthumb}{%
  \stepcounter{thumbindex}%
}

% Draw the thumb tab on every page (placed in header via fancyhdr)
\newcommand{\drawthumb}{%
  \ifnum\value{thumbindex}>0%
    \begin{tikzpicture}[remember picture,overlay]
      \pgfmathsetmacro{\thumboffset}{%
        20 + (\value{thumbindex}-1) * (16 + 1)}  % mm from top
      \ifodd\value{page}%
        % Odd pages: tab on right edge
        \fill[crimson!80]
          ([yshift=-\thumboffset mm]current page.north east)
          rectangle +(-\thumbtabwidth, -\thumbtabheight);
        \node[white,font=\tiny\bfseries,rotate=90]
          at ([yshift=-\thumboffset mm - 0.5\thumbtabheight,
               xshift=-0.5\thumbtabwidth]current page.north east)
          {\Roman{thumbindex}};
      \else
        % Even pages: tab on left edge
        \fill[crimson!80]
          ([yshift=-\thumboffset mm]current page.north west)
          rectangle +(\thumbtabwidth, -\thumbtabheight);
        \node[white,font=\tiny\bfseries,rotate=-90]
          at ([yshift=-\thumboffset mm - 0.5\thumbtabheight,
               xshift=0.5\thumbtabwidth]current page.north west)
          {\Roman{thumbindex}};
      \fi
    \end{tikzpicture}%
  \fi
}

% Hook into fancyhdr to draw thumb on every content page
\AddToHook{shipout/foreground}{%
  \drawthumb%
}

% =============================================================================
% CROP / BLEED MARKS
% =============================================================================
% For final print submission, uncomment the line below to add crop marks.
% MIT Press production will advise on exact requirements.
% \usepackage[cam,center,width=7.5in,height=10.5in]{crop}

% =============================================================================
% PDF/A ARCHIVAL COMPLIANCE
% =============================================================================
% MIT Press increasingly requires PDF/A for long-term preservation.
% This embeds all fonts and removes transparency.
% Note: pdfx must be loaded early; if it conflicts with hyperref,
% MIT Press production can handle the conversion post-build.
% Uncomment when ready for final submission:
% \usepackage[a-3u]{pdfx}

% =============================================================================
% ENHANCED WIDOW / ORPHAN CONTROL
% =============================================================================
% Prevent single lines at top/bottom of pages and breaks before equations
\clubpenalty=10000          % No orphans (single first line at bottom)
\widowpenalty=10000         % No widows (single last line at top)
\displaywidowpenalty=10000  % No widow before display math
\predisplaypenalty=10000    % No page break just before display math
\postdisplaypenalty=0       % Allow break after display math (natural)
\usepackage{needspace}
\let\Needspace\needspace
\makeatletter
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{plain}
\@ifundefined{c@chapter}{\newfloat{vid}{h}{lovid}}{\newfloat{vid}{h}{lovid}[chapter]}
\floatname{vid}{Video}
\newcommand*\listofvids{\listof{vid}{List of Videos}}
\makeatother
\makeatletter
\@ifpackageloaded{tcolorbox}{}{\usepackage[skins,breakable]{tcolorbox}}
\@ifpackageloaded{fontawesome5}{}{\usepackage{fontawesome5}}
\definecolor{quarto-callout-color}{HTML}{909090}
\definecolor{quarto-callout-note-color}{HTML}{0758E5}
\definecolor{quarto-callout-important-color}{HTML}{CC1914}
\definecolor{quarto-callout-warning-color}{HTML}{EB9113}
\definecolor{quarto-callout-tip-color}{HTML}{00A047}
\definecolor{quarto-callout-caution-color}{HTML}{FC5300}
\definecolor{quarto-callout-color-frame}{HTML}{acacac}
\definecolor{quarto-callout-note-color-frame}{HTML}{4582ec}
\definecolor{quarto-callout-important-color-frame}{HTML}{d9534f}
\definecolor{quarto-callout-warning-color-frame}{HTML}{f0ad4e}
\definecolor{quarto-callout-tip-color-frame}{HTML}{02b875}
\definecolor{quarto-callout-caution-color-frame}{HTML}{fd7e14}
\makeatother
\makeatletter
\@ifpackageloaded{bookmark}{}{\usepackage{bookmark}}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\makeatletter
\@ifpackageloaded{sidenotes}{}{\usepackage{sidenotes}}
\@ifpackageloaded{marginnote}{}{\usepackage{marginnote}}
\makeatother
\newcommand{\fbxIconPath}{assets/images/icons/callouts}
\newcommand{\fbxIconFormat}{pdf}
\makeatletter
\@ifpackageloaded{tcolorbox}{}{\usepackage[many]{tcolorbox}}
\makeatother
%%%% ---foldboxy preamble ----- %%%%%

% Load xstring for string manipulation
\RequirePackage{xstring}

% Icon path and format configuration - can be overridden in filter-metadata
\providecommand{\fbxIconPath}{assets/images/icons/callouts}
\providecommand{\fbxIconFormat}{pdf}

% Helper command to include icon with hyphen-to-underscore conversion
% This ensures consistency: callout-quiz-question -> callout_quiz_question
\newcommand{\fbxIncludeIcon}[2]{%
  \StrSubstitute{#1}{-}{_}[\fbxIconName]%
  \includegraphics[width=#2]{\fbxIconPath/icon_\fbxIconName.\fbxIconFormat}%
}

% Legacy fallback colors (keep for compatibility)
\definecolor{fbx-default-color1}{HTML}{c7c7d0}
\definecolor{fbx-default-color2}{HTML}{a3a3aa}
\definecolor{fbox-color1}{HTML}{c7c7d0}
\definecolor{fbox-color2}{HTML}{a3a3aa}

% arguments: #1 typelabelnummer: #2 titel: #3
\newenvironment{fbx}[3]{%
\begin{tcolorbox}[
  enhanced,
  breakable,
  %fontupper=\fontsize{8pt}{10pt}\selectfont,  % 95% of body text (10pt -> 9.5pt)
  before skip=8pt,  % space above box (increased)
  after skip=8pt,   % space below box (increased)
  attach boxed title to top*={xshift=0pt},
  boxed title style={
  %fuzzy shadow={1pt}{-1pt}{0mm}{0.1mm}{gray},
  arc=1.5pt,
  rounded corners=north,
  sharp corners=south,
  top=6pt,          % Adjusted for ~40px equivalent height
  bottom=5pt,       % Adjusted for ~40px equivalent height
  overlay={
      \node [left,outer sep=0em, black,draw=none,anchor=west,
        rectangle,fill=none,inner sep=0pt]
        at ([xshift=4mm]frame.west) {\fbxIncludeIcon{#1}{4.2mm}};
    },
  },
  colframe=#1-color2,             % Border color (auto-generated from YAML)
  colbacktitle=#1-color1,         % Background color (auto-generated from YAML)
  colback=white,
  coltitle=black,
  titlerule=0mm,
  toprule=0.5pt,
  bottomrule=0.5pt,
  leftrule=2.2pt,
  rightrule=0.5pt,
  outer arc=1.5pt,
  arc=1.5pt,
  left=0.5em,       % increased left padding
  bottomtitle=1.5mm, % increased title bottom margin
  toptitle=1.5mm,    % increased title top margin
  title=\hspace{2.5em}\protect#2\hspace{0.5em}\protect#3, % Protect parameters
  extras middle and last={top=4pt} % increased continuation spacing
]}
{\end{tcolorbox}}


% boxed environment with right border
\newenvironment{fbxSimple}[3]{\begin{tcolorbox}[
  enhanced,
  breakable,
  %fontupper=\fontsize{8pt}{10pt}\selectfont,  % 95% of body text (10pt -> 9.5pt)
  before skip=8pt,  % space above box (increased)
  after skip=8pt,   % space below box (increased)
  attach boxed title to top*={xshift=0pt},
  boxed title style={
  %fuzzy shadow={1pt}{-1pt}{0mm}{0.1mm}{gray},
  arc=1.5pt,
  rounded corners=north,
  sharp corners=south,
  top=6pt,          % Adjusted for ~40px equivalent height
  bottom=5pt,       % Adjusted for ~40px equivalent height
  overlay={
      \node [left,outer sep=0em, black,draw=none,anchor=west,
        rectangle,fill=none,inner sep=0pt]
        at ([xshift=3mm]frame.west) {\fbxIncludeIcon{#1}{4.2mm}};
    },
  },
  colframe=#1-color2,             % Border color (auto-generated from YAML)
  colbacktitle=#1-color1,         % Background color (auto-generated from YAML)
  colback=white,
  coltitle=black,
  titlerule=0mm,
  toprule=0.5pt,
  bottomrule=0.5pt,
  leftrule=2.2pt,
  rightrule=0.5pt,
  outer arc=1.5pt,
  arc=1.5pt,
  left=0.5em,       % increased left padding
  bottomtitle=1.5mm, % increased title bottom margin
  toptitle=1.5mm,    % increased title top margin
  title=\hspace{2.5em}\protect#2\hspace{0.5em}\protect#3, % Protect parameters
  boxsep=1pt,
  extras first={bottom=0pt},
  extras last={top=0pt,bottom=-4pt},
  overlay first={
    \draw[line width=1pt,white] ([xshift=2.2pt]frame.south west)-- ([xshift=-0.5pt]frame.south east);
  },
  overlay last={
    \draw[line width=1pt,white] ([xshift=2.2pt]frame.north west)-- ([xshift=-0.5pt]frame.north east);
   }
]}
{\end{tcolorbox}}

%%%% --- end foldboxy preamble ----- %%%%%
%%==== colors from yaml ===%
\definecolor{callout-notebook-color1}{HTML}{F2F7FF}
\definecolor{callout-notebook-color2}{HTML}{2C5282}
\definecolor{callout-lighthouse-color1}{HTML}{FDF8E6}
\definecolor{callout-lighthouse-color2}{HTML}{B8860B}
\definecolor{callout-takeaways-color1}{HTML}{FDF2F7}
\definecolor{callout-takeaways-color2}{HTML}{BE185D}
\definecolor{callout-perspective-color1}{HTML}{F7F8FA}
\definecolor{callout-perspective-color2}{HTML}{4A5568}
\definecolor{callout-chapter-connection-color1}{HTML}{FDF2F7}
\definecolor{callout-chapter-connection-color2}{HTML}{A51C30}
\definecolor{callout-theorem-color1}{HTML}{F5F0FF}
\definecolor{callout-theorem-color2}{HTML}{6B46C1}
\definecolor{callout-colab-color1}{HTML}{FFF5E6}
\definecolor{callout-colab-color2}{HTML}{FF6B35}
\definecolor{callout-code-color1}{HTML}{F2F4F8}
\definecolor{callout-code-color2}{HTML}{D1D7E0}
\definecolor{callout-resource-exercises-color1}{HTML}{E0F2F1}
\definecolor{callout-resource-exercises-color2}{HTML}{20B2AA}
\definecolor{callout-resource-videos-color1}{HTML}{E0F2F1}
\definecolor{callout-resource-videos-color2}{HTML}{20B2AA}
\definecolor{callout-quiz-question-color1}{HTML}{F0F0F8}
\definecolor{callout-quiz-question-color2}{HTML}{5B4B8A}
\definecolor{callout-resource-slides-color1}{HTML}{E0F2F1}
\definecolor{callout-resource-slides-color2}{HTML}{20B2AA}
\definecolor{callout-principle-color1}{HTML}{F3F2FA}
\definecolor{callout-principle-color2}{HTML}{3D3B8E}
\definecolor{callout-checkpoint-color1}{HTML}{E8F5E9}
\definecolor{callout-checkpoint-color2}{HTML}{2E7D32}
\definecolor{callout-definition-color1}{HTML}{F0F4F8}
\definecolor{callout-definition-color2}{HTML}{1B4F72}
\definecolor{callout-example-color1}{HTML}{F0F8F6}
\definecolor{callout-example-color2}{HTML}{148F77}
\definecolor{callout-quiz-answer-color1}{HTML}{E8F2EA}
\definecolor{callout-quiz-answer-color2}{HTML}{4a7c59}
%=============%

\usepackage{hyphenat}
\usepackage{ifthen}
\usepackage{calc}
\usepackage{calculator}



\usepackage{graphicx}
\usepackage{geometry}
\usepackage{afterpage}
\usepackage{tikz}
\usetikzlibrary{calc}
\usetikzlibrary{fadings}
\usepackage[pagecolor=none]{pagecolor}


% Set the titlepage font families







% Set the coverpage font families

\usepackage{bookmark}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  pdftitle={Introduction to Machine Learning Systems},
  pdfauthor={Vijay Janapa Reddi},
  colorlinks=true,
  linkcolor={Maroon},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}


\title{Introduction to Machine Learning Systems}
\author{Vijay Janapa Reddi}
\date{February 1, 2026}
\begin{document}
%%%%% begin titlepage extension code

  \begin{frontmatter}

\begin{titlepage}
% This is a combination of Pandoc templating and LaTeX
% Pandoc templating https://pandoc.org/MANUAL.html#templates
% See the README for help

\thispagestyle{empty}

\newgeometry{top=-100in}

% Page color

\newcommand{\coverauthorstyle}[1]{{\fontsize{20}{24.0}\selectfont
{#1}}}

\begin{tikzpicture}[remember picture, overlay, inner sep=0pt, outer sep=0pt]

\tikzfading[name=fadeout, inner color=transparent!0,outer color=transparent!100]
\tikzfading[name=fadein, inner color=transparent!100,outer color=transparent!0]
\node[anchor=south west, rotate=0, opacity=1] at ($(current page.south west)+(0.225\paperwidth, 9)$) {
\includegraphics[width=\paperwidth, keepaspectratio]{assets/images/covers/cover-image-transparent-vol1.png}};

% Title
\newcommand{\titlelocationleft}{0.075\paperwidth}
\newcommand{\titlelocationbottom}{0.4\paperwidth}
\newcommand{\titlealign}{left}

\begin{scope}{%
\fontsize{52}{62.4}\selectfont
\node[anchor=north
west, align=left, rotate=0] (Title1) at ($(current page.south west)+(\titlelocationleft,\titlelocationbottom)$)  [text width = 0.9\paperwidth]  {{\nohyphens{Machine
Learning Systems}}};
}
\end{scope}

% Author
\newcommand{\authorlocationleft}{.925\paperwidth}
\newcommand{\authorlocationbottom}{0.175\paperwidth}
\newcommand{\authoralign}{right}

\begin{scope}
{%
\fontsize{20}{24.0}\selectfont
\node[anchor=north
east, align=right, rotate=0] (Author1) at ($(current page.south west)+(\authorlocationleft,\authorlocationbottom)$)  [text width = 6in]  {\coverauthorstyle{Vijay
Janapa Reddi\\}};
}
\end{scope}

% Footer
\newcommand{\footerlocationleft}{0.075\paperwidth}
\newcommand{\footerlocationbottom}{0.475\paperwidth}
\newcommand{\footerlocationalign}{left}

\begin{scope}
{%
\fontsize{25}{30.0}\selectfont
 \node[anchor=north west, align=left, rotate=0] (Footer1) at %
($(current page.south west)+(\footerlocationleft,\footerlocationbottom)$)  [text width = 0.9\paperwidth]  {{\nohyphens{Introduction
to}}};
}
\end{scope}

\end{tikzpicture}
\clearpage
\restoregeometry
%%% TITLE PAGE START

% Set up alignment commands
%Page
\newcommand{\titlepagepagealign}{
\ifthenelse{\equal{left}{right}}{\raggedleft}{}
\ifthenelse{\equal{left}{center}}{\centering}{}
\ifthenelse{\equal{left}{left}}{\raggedright}{}
}


\newcommand{\titleandsubtitle}{
% Title and subtitle
{{\huge{\bfseries{\nohyphens{Introduction to Machine Learning
Systems}}}}\par
}%
}
\newcommand{\titlepagetitleblock}{
\titleandsubtitle
}

\newcommand{\authorstyle}[1]{{\large{#1}}}

\newcommand{\affiliationstyle}[1]{{\large{#1}}}

\newcommand{\titlepageauthorblock}{
{\authorstyle{\nohyphens{Vijay Janapa
Reddi}{\textsuperscript{1}}\textsuperscript{,}{\textsuperscript{,*}}}}}

\newcommand{\titlepageaffiliationblock}{
\hangindent=1em
\hangafter=1
{\affiliationstyle{
{1}.~Harvard University


\vspace{1\baselineskip}
* \textit{Correspondence:}~Vijay Janapa Reddi~vj@eecs.harvard.edu
}}
}
\newcommand{\headerstyled}{%
{}
}
\newcommand{\footerstyled}{%
{\large{}}
}
\newcommand{\datestyled}{%
{February 1, 2026}
}


\newcommand{\titlepageheaderblock}{\headerstyled}

\newcommand{\titlepagefooterblock}{
\footerstyled
}

\newcommand{\titlepagedateblock}{
\datestyled
}

%set up blocks so user can specify order
\newcommand{\titleblock}{{

{\titlepagetitleblock}
}

\vspace{4\baselineskip}
}

\newcommand{\authorblock}{{\titlepageauthorblock}

\vspace{2\baselineskip}
}

\newcommand{\affiliationblock}{{\titlepageaffiliationblock}

\vspace{0pt}
}

\newcommand{\logoblock}{}

\newcommand{\footerblock}{}

\newcommand{\dateblock}{{\titlepagedateblock}

\vspace{0pt}
}

\newcommand{\headerblock}{}

\thispagestyle{empty} % no page numbers on titlepages


\newcommand{\vrulecode}{\textcolor{black}{\rule{\vrulewidth}{\textheight}}}
\newlength{\vrulewidth}
\setlength{\vrulewidth}{2pt}
\newlength{\B}
\setlength{\B}{\ifdim\vrulewidth > 0pt 0.05\textwidth\else 0pt\fi}
\newlength{\minipagewidth}
\ifthenelse{\equal{left}{left} \OR \equal{left}{right} }
{% True case
\setlength{\minipagewidth}{\textwidth - \vrulewidth - \B - 0.1\textwidth}
}{
\setlength{\minipagewidth}{\textwidth - 2\vrulewidth - 2\B - 0.1\textwidth}
}
\ifthenelse{\equal{left}{left} \OR \equal{left}{leftright}}
{% True case
\raggedleft % needed for the minipage to work
\vrulecode
\hspace{\B}
}{%
\raggedright % else it is right only and width is not 0
}
% [position of box][box height][inner position]{width}
% [s] means stretch out vertically; assuming there is a vfill
\begin{minipage}[b][\textheight][s]{\minipagewidth}
\titlepagepagealign
\titleblock

Prof.~Vijay Janapa Reddi

School of Engineering and Applied Sciences

Harvard University

\vspace{80mm}

With heartfelt gratitude to the community for their invaluable
contributions and steadfast support.

\vfill

February 1, 2026

\vfill
\par

\end{minipage}\ifthenelse{\equal{left}{right} \OR \equal{left}{leftright} }{
\hspace{\B}
\vrulecode}{}
\clearpage
%%% TITLE PAGE END
\end{titlepage}
\setcounter{page}{1}
\end{frontmatter}

%%%%% end titlepage extension code

% =============================================================================
% HALF-TITLE PAGE (Volume I)
% =============================================================================
% Standard academic book sequence: half-title -> blank -> title page -> copyright
% The half-title shows only the book title -- no author, no publisher, no date.
\thispagestyle{empty}
\begin{center}
\vspace*{0.3\textheight}
{\fontsize{24pt}{28pt}\selectfont\bfseries\color{crimson} Introduction to\\[0.4em] Machine Learning Systems}\\[2em]
{\large\itshape Volume~I}
\vfill
\end{center}
\clearpage
\thispagestyle{empty}\null\clearpage  % Blank verso (back of half-title)

\renewcommand*\contentsname{Table of contents}
{
\hypersetup{linkcolor=}
\setcounter{tocdepth}{2}
\tableofcontents
}
\listoffigures
\listoftables

\mainmatter
\bookmarksetup{startatroot}

\chapter*{Welcome to Volume I}\label{welcome-to-volume-i}
\addcontentsline{toc}{chapter}{Welcome to Volume I}

\markboth{Welcome to Volume I}{Welcome to Volume I}

\bookmarksetup{startatroot}

\chapter{Data Engineering}\label{sec-data-engineering-ml}

\marginnote{\begin{footnotesize}

\emph{DALL·E 3 Prompt: Create a rectangular illustration visualizing the
concept of data engineering. Include elements such as raw data sources,
data processing pipelines, storage systems, and refined datasets. Show
how raw data is transformed through cleaning, processing, and storage to
become valuable information that can be analyzed and used for
decision-making.}

\end{footnotesize}}

\noindent
\pandocbounded{\includegraphics[keepaspectratio]{contents/vol1/data_engineering/images/png/cover_data_engineering.png}}

\section*{Purpose}\label{purpose}
\addcontentsline{toc}{section}{Purpose}

\markright{Purpose}

\emph{Why does data represent the actual source code of machine learning
systems while traditional code merely describes how to compile it?}

In conventional software, programmers write logic that computers
execute. In machine learning, programmers write optimization procedures
that extract operational logic from data. This inversion makes data the
true source code: change the data and you change what the system does,
regardless of whether a single line of traditional code has been
modified. A dataset with subtle labeling inconsistencies produces a
model with subtle behavioral inconsistencies. A dataset missing edge
cases produces a model that fails on edge cases. A dataset reflecting
historical biases produces a model that perpetuates those biases. No
architecture, hyperparameter, or training trick can recover information
that was never present or correct errors that were baked in from the
start. Data engineering is therefore \emph{not preprocessing---it is
programming in a different language}, one where quality control
determines whether the compiled system works.

\begin{tcolorbox}[enhanced jigsaw, titlerule=0mm, colframe=quarto-callout-tip-color-frame, leftrule=.75mm, bottomrule=.15mm, toptitle=1mm, colback=white, opacityback=0, opacitybacktitle=0.6, arc=.35mm, bottomtitle=1mm, breakable, coltitle=black, toprule=.15mm, rightrule=.15mm, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Learning Objectives}, left=2mm, colbacktitle=quarto-callout-tip-color!10!white]

\begin{itemize}
\tightlist
\item
  Explain the Four Pillars framework for evaluating data engineering
  trade-offs across quality, reliability, scalability, and governance
\item
  Design data acquisition strategies combining datasets, crowdsourcing,
  and synthetic generation based on cost-quality trade-offs
\item
  Architect data pipelines with validation, monitoring, and graceful
  degradation for operational reliability
\item
  Implement training-serving consistency through idempotent
  transformations and drift detection
\item
  Build data labeling systems balancing accuracy, throughput, and cost
  with quality control mechanisms
\item
  Evaluate storage and governance architectures for ML workloads
  including lineage tracking and regulatory compliance
\end{itemize}

\end{tcolorbox}

\section{Data Engineering as Dataset
Compilation}\label{sec-data-engineering-ml-data-engineering-dataset-compilation-0496}

The methodologies examined in \textbf{?@sec-ai-development-workflow}
establish the \emph{when} and \emph{why} of data preparation, showing
that data work consumes 60 to 80 percent of ML project effort and
represents the foundational ``D'' in the \textbf{AI Triad} introduced in
\textbf{?@sec-introduction}. However, executing those stages at scale
requires dedicated infrastructure. If the workflow is the plan, data
engineering is the factory floor. This chapter examines the
infrastructure, processes, and engineering principles that turn raw
observations into training-ready datasets.

The stakes of this infrastructure are quantifiable. The
\textbf{Degradation Equation} from \textbf{?@sec-introduction} captures
\emph{why} data quality matters so directly to model performance:

\[\text{Accuracy}(t) \approx \text{Accuracy}_0 - \lambda \cdot D(P_t \| P_0)\]

Model performance erodes as the serving distribution \(P_t\) diverges
from the training distribution \(P_0\). Every data quality issue,
whether labeling errors, sampling bias, or schema drift, increases this
distributional divergence \(D\), directly translating to accuracy loss.
Data engineering provides the infrastructure to minimize \(D\) through
systematic acquisition, validation, and monitoring. The metrics
introduced later in this chapter (PSI, KL divergence) operationalize
this equation for production systems.

\phantomsection\label{callout-definitionux2a-1.1}
\begin{fbx}{callout-definition}{Definition:}{Data Engineering}
\phantomsection\label{callout-definition*-1.1}
\textbf{\emph{Data Engineering}} is the infrastructure layer that
manages the lifecycle of data from source to model---acquisition,
transformation, storage, and governance. In ML systems, its critical
function is ensuring \textbf{Training-Serving Consistency}: decoupling
the model from the volatility of raw data sources so that the
distribution at inference time (\(P_t\)) remains statistically aligned
with training (\(P_{t_0}\)), preventing \textbf{Silent Degradation}.

\end{fbx}

This definition highlights a fundamental difference between traditional
software and machine learning: in traditional software, computational
logic is defined by code, while in machine learning, system behavior is
defined by data. This is the \emph{Data as Code Invariant} established
in \textbf{?@sec-foundations-invariants}: data is the source code of the
ML system, and a change in the training dataset (\(\Delta D\)) is
functionally equivalent to a change in the executable logic
(\(\Delta P\)). We therefore reframe data engineering not as ``data
cleaning,'' but as \emph{Dataset Compilation}:

\[ \text{System Behavior} \approx f(\text{Data}) \]

Just as a compiler transforms human-readable source code into an
optimized binary executable, a data pipeline transforms raw, noisy
observations into a clean, optimized training set that the model
consumes. This analogy extends to specific operations, where standard
compiler optimizations map directly to data engineering tasks:

\begin{itemize}
\tightlist
\item
  \textbf{Dead Code Elimination → Filtering:} Removing corrupted
  records, outliers, or irrelevant features that contribute nothing to
  the learned representation.
\item
  \textbf{Loop Unrolling → Augmentation:} Synthetically expanding
  limited examples (e.g., rotating images, pitch-shifting audio) to
  expose the model to more variations of the underlying pattern.
\item
  \textbf{Common Subexpression Elimination → Deduplication:} Identifying
  and merging duplicate records to prevent bias and wasted compute.
\item
  \textbf{Type Checking → Schema Validation:} Enforcing strict types and
  ranges to ensure the ``runtime'' (model training) does not crash.
\end{itemize}

The engineering implication is direct: datasets must be
\textbf{versioned} (like git), \textbf{unit-tested} (data quality
checks), and \textbf{debugged}. \emph{Deleting a row of training data is
the engineering equivalent of deleting a line of code, and retraining a
model is simply recompiling the binary.}

The compilation metaphor establishes the engineering mindset that runs
through this chapter. The quantitative foundations that follow address
the statistical ceiling on model performance, the economics of data
quality improvements, and the transition from model-centric to
data-centric development. The first of these foundations is a constraint
that even the most rigorously engineered dataset cannot escape: the
\emph{statistical ceiling} imposed by label noise and data quality.

\phantomsection\label{callout-perspectiveux2a-1.2}
\begin{fbx}{callout-perspective}{Systems Perspective:}{The Statistical Ceiling}
\phantomsection\label{callout-perspective*-1.2}
A fundamental law of data-centric engineering is the \emph{Statistical
Ceiling}: The maximum achievable accuracy of any model is strictly
bounded by the quality and noise floor of its training data.

If your labels are only 90\% accurate, your model's ceiling is 90\%
accuracy, regardless of whether you use a simple linear regression or a
trillion-parameter transformer. In the context of the \textbf{Iron Law},
data engineering is the dual process of lowering the \textbf{Data
(\(D_{vol}\))} term (through filtering and deduplication) while
simultaneously raising this \textbf{Performance Ceiling} (through
cleaning and labeling consistency). \emph{No amount of compute (\(O\))
can overcome a low statistical ceiling.}

\end{fbx}

The implications of this ceiling are visualized in
Figure~\ref{fig-data-quality-multiplier}, which contrasts the scaling
laws of clean versus noisy data. While clean data allows accuracy to
improve with a power law, noisy data causes performance to plateau,
making data cleaning a higher-leverage activity than mere data
accumulation.

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{contents/vol1/data_engineering/data_engineering_files/figure-pdf/fig-data-quality-multiplier-output-1.pdf}}

}

\caption{\label{fig-data-quality-multiplier}\textbf{The Data Quality
Multiplier}: Model Accuracy vs.~Dataset Size (Log Scale) for Clean
vs.~Noisy Data. High-quality data (Green) follows a steeper power law,
reaching higher accuracy with fewer samples. Low-quality data (Red) hits
a `Statistical Ceiling' earlier, where adding more data yields
diminishing returns due to irreducible label noise. This gap illustrates
why data cleaning is often a higher-leverage optimization than model
scaling.}

\end{figure}%

The statistical ceiling establishes the theoretical bound on model
performance. But how do we quantify the practical returns from investing
in data quality? The following analysis demonstrates the compounding
economics of one common data quality improvement, a phenomenon we call
the \emph{deduplication dividend}.

\phantomsection\label{callout-notebookux2a-1.3}
\begin{fbx}{callout-notebook}{AI Engineer’s Notebook:}{The Deduplication Dividend}
\phantomsection\label{callout-notebook*-1.3}
\textbf{Problem}: You have a 10TB dataset of web-scraped images.
Training takes 2 weeks on your GPU cluster (\$5,000). You suspect 30\%
of the images are near-duplicates. Is it worth the engineering time to
remove them?

\textbf{The Math}: If duplicates contribute little training signal,
removing 30\% reduces training time to \textasciitilde10 days, saving
\$1,500 per run. Over 10 training iterations (hyperparameter tuning,
architecture search), that's \$15,000 saved. If deduplication takes 2
engineer-days (\$2,000), the ROI is 7.5×.

\textbf{The Systems Lesson}: Data cleaning has compounding returns
because cleaned data benefits every downstream experiment. This is
\emph{why} production teams prioritize data quality tooling over model
complexity.

\end{fbx}

The deduplication dividend illustrates a broader shift in how ML
practitioners approach system development. This shift begins by
contrasting the traditional approach, which we formalize as
Model-Centric AI, against the emerging Data-Centric paradigm.

\phantomsection\label{callout-definitionux2a-1.4}
\begin{fbx}{callout-definition}{Definition:}{Model-Centric AI}
\phantomsection\label{callout-definition*-1.4}
\textbf{\emph{Model-Centric AI}} is the research paradigm where the
\textbf{Dataset is Fixed} and engineering effort focuses on optimizing
\textbf{Model Architecture} and hyperparameters. This approach enables
fair algorithmic comparison (e.g., ImageNet benchmarks) but hits
diminishing returns in production, where data quality---not
architecture---is often the binding constraint.

\end{fbx}

Research benchmarks reinforce this pattern by providing static datasets
where progress is measured purely through algorithmic innovation.
Production systems, however, face a different reality: datasets
continuously evolve, data quality varies across sources and time, and
model improvements often plateau while data improvements continue
yielding gains. This realization has catalyzed what Andrew Ng termed the
shift to \textbf{Data-Centric AI} (\citeproc{ref-ng2021datacentric}{Ng
2021}).

\phantomsection\label{callout-definitionux2a-1.5}
\begin{fbx}{callout-definition}{Definition:}{Data-Centric AI}
\phantomsection\label{callout-definition*-1.5}
\textbf{\emph{Data-Centric AI}} is the engineering paradigm of holding
\textbf{Model Architecture} constant to iteratively optimize the
\textbf{Training Dataset}. It addresses the diminishing returns of
algorithmic complexity by treating data quality as the primary variable
for improving system performance, effectively shifting the bottleneck
from code optimization to \textbf{Signal-to-Noise Maximization}.

\end{fbx}

These two terms---Data-Centric Computing and Data-Centric AI---sound
similar but operate at different levels of the stack.

\begin{tcolorbox}[enhanced jigsaw, titlerule=0mm, colframe=quarto-callout-caution-color-frame, leftrule=.75mm, bottomrule=.15mm, toptitle=1mm, colback=white, opacityback=0, opacitybacktitle=0.6, arc=.35mm, bottomtitle=1mm, breakable, coltitle=black, toprule=.15mm, rightrule=.15mm, title=\textcolor{quarto-callout-caution-color}{\faFire}\hspace{0.5em}{Data-Centric Computing vs.~Data-Centric AI}, left=2mm, colbacktitle=quarto-callout-caution-color!10!white]

As you proceed through this text, distinguish between
\textbf{Data-Centric Computing} (the systems paradigm defined here) and
\textbf{Data-Centric AI} (the development methodology discussed in this
chapter).

\begin{itemize}
\tightlist
\item
  \textbf{Data-Centric Computing} concerns the \textbf{machine}:
  optimizing hardware and software architectures to handle massive data
  throughput rather than complex instruction pipelines.
\item
  \textbf{Data-Centric AI} concerns the \textbf{workflow}:
  systematically improving data quality while holding model
  architectures fixed to boost performance.
\end{itemize}

One is about how we \emph{build the computer}; the other is about how we
\emph{teach the model}.

\end{tcolorbox}

While the definitions establish the conceptual goals,
Table~\ref{tbl-model-vs-data-centric} contrasts how these paradigms
diverge across five key operational dimensions, from iteration targets
to handling label noise.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3333}}@{}}
\caption{\textbf{Model-Centric vs.~Data-Centric Development}: The shift
in ML development philosophy. Production experience shows that data
improvements often outperform algorithmic innovation: a dataset with
10,000 consistently labeled examples often outperforms 100,000 examples
with noisy labels.}\label{tbl-model-vs-data-centric}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Aspect}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Model-Centric Approach}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Data-Centric Approach}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Aspect}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Model-Centric Approach}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Data-Centric Approach}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Primary iteration target} & Model architecture, hyperparameters
& Data quality, labeling consistency \\
\textbf{Response to poor performance} & Try different models, add layers
& Analyze errors, improve data for failure cases \\
\textbf{Benchmark philosophy} & Fixed dataset, compete on algorithms &
Improve dataset systematically \\
\textbf{Label disagreement handling} & Majority vote, accept noise &
Resolve inconsistencies, create clear guidelines \\
\textbf{Data augmentation role} & Increase training set size & Target
specific failure modes \\
\end{longtable}

The data-centric approach proves particularly valuable when models have
reached architectural maturity. For many production tasks, the
difference between model architectures is small compared to the impact
of data quality improvements. This mindset, treating data as the primary
improvement lever, motivates \emph{why} data engineering demands the
same rigor we apply to code.

This rigorous treatment of data becomes even more critical when we
consider how data quality issues propagate differently than software
bugs. Traditional software produces predictable errors when encountering
malformed input, enabling immediate correction. ML systems degrade
silently: data quality deficiencies manifest as subtle performance
losses that accumulate through the pipeline and often go undetected
until production failures occur. A single mislabeled training instance
may appear inconsequential, but systematic labeling inconsistencies
compound into model corruption across entire feature spaces. Gradual
distribution shifts can silently erode accuracy until complete
retraining becomes necessary.

These failure modes require systematic engineering approaches. To
understand \emph{why}, we first examine \emph{how} data quality failures
propagate through ML systems.

\subsection{The Physics of Data: Entropy and
Gravity}\label{sec-data-engineering-ml-physics-of-data}

To engineer data systems effectively, we must move beyond the ``data as
code'' metaphor and treat data as a physical substance with measurable
properties. Just as diverse materials have density and viscosity,
datasets have \textbf{Information Entropy} and \textbf{Data Gravity}.

\textbf{Data Gravity} is the cost of movement. It is a function of
volume (\(V\)) and network bandwidth (\(BW\)). The time to move a
petabyte dataset across a 10 Gbps link is fixed by physics
(\(T = V/BW \approx 10 \text{ days}\)). This gravity dictates
architecture: because moving 1PB to the compute is slow and expensive,
we must move the compute to the data. This explains the rise of ``Data
Lakehouse'' architectures (\citeproc{ref-armbrust2021lakehouse}{Zaharia
et al. 2021}) where processing engines (Spark, Presto) run directly on
storage nodes. In contrast, \textbf{Data Mesh}
(\citeproc{ref-dehghani2022data}{Dehghani 2022}) proposes decentralizing
ownership to manage this scale organizationally, treating data as a
product owned by domain teams.

\textbf{Information Entropy} is the density of signal. A dataset of 1
million identical images has high gravity (TB of storage) but zero
entropy (1 image worth of information). A dataset of 10,000 diverse
edge-cases has low gravity but high entropy.

\[ \text{Data Selection Gain} \propto \frac{\text{Information Entropy}}{\text{Data Gravity}} \]

Underlying these physical properties is a fundamental constraint we call
the \emph{energy-movement invariant}: moving data always dominates the
energy budget.

\phantomsection\label{callout-perspectiveux2a-1.6}
\begin{fbx}{callout-perspective}{Systems Perspective:}{The Energy-Movement Invariant}
\phantomsection\label{callout-perspective*-1.6}
The fundamental ``Iron Law'' of data engineering is that \textbf{moving
a bit costs 100--1,000× more energy than computing on it.} While
\textbf{?@sec-model-compression} examines the energy cost inside the
processor, we must also consider the cost of the information flow from
the external world.

\begin{longtable}[]{@{}lrr@{}}
\toprule\noalign{}
\textbf{Operation} & \textbf{Energy (pJ)} & \textbf{Relative Cost} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{32-bit Floating Point Add} & 3.7 pJ & 1× \\
\textbf{DRAM Memory Access (32-bit)} & \textbf{640 pJ} &
\textbf{172×} \\
\textbf{Local SSD Access (per bit)} & \textasciitilde10,000 pJ &
100,000× \\
\textbf{Network Transfer (Data Center)} & \textasciitilde50,000 pJ &
500,000× \\
\end{longtable}

\textbf{Systems Implication}: Data has physical mass. If you prune 50\%
of your training data through deduplication, you are not just saving
disk space; you are eliminating the most energy-intensive stages of the
training lifecycle. This is \emph{why} \textbf{Data Selection} is the
highest-leverage tool in the systems engineer's toolkit: it addresses
the problem at the most expensive source.

\end{fbx}

Effective data engineering maximizes this ratio. ``Data Cleaning'' is
not just hygiene; it is \textbf{Signal-to-Noise Engineering}.
Deduplication removes mass without reducing entropy. Active learning
adds high-entropy examples (edge cases) while ignoring low-entropy ones
(common cases). We optimize this ratio to ensure our storage and compute
budgets are spent on signal, not noise. To see how data gravity
constrains real engineering decisions, consider the \emph{physics of
data gravity} in a concrete scenario.

\phantomsection\label{callout-notebookux2a-1.7}
\begin{fbx}{callout-notebook}{AI Engineer’s Notebook:}{The Physics of Data Gravity}
\phantomsection\label{callout-notebook*-1.7}
\textbf{Problem}: You have a \textbf{1 PB} training dataset in a US East
data center. You want to train a model using a TPU pod in US West. Is it
faster to move the data or the compute?

\textbf{The Physics}:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Network Bandwidth}: A dedicated 100 Gbps line = 12.5 GB/s.
\item
  \textbf{Transfer Time}:
  \(1\,000\,000 \text{{ GB}} / 12\.5 \text{{ GB/s}} = 80\,000 \text{{ seconds}} \approx \mathbf{22 \text{{ hours}}}\).
\item
  \textbf{Cost}: At \$0.01/GB egress, moving 1 PB costs
  \textbf{\$10,000}.
\end{enumerate}

\textbf{The Engineering Conclusion}: If training takes \textless{} 22
hours, you spend more time moving data than training. If training costs
\textless{} \$10,000 (approx. 2,500 TPUv4-hours), you spend more on
bandwidth than compute.

\textbf{Rule of Thumb}: For petabyte-scale data, \textbf{Code moves to
Data}. For gigabyte-scale data, \textbf{Data moves to Code}.

\end{fbx}

These physical constraints govern every decision in production data
pipelines. Before proceeding, verify your intuition about these
fundamentals.

\phantomsection\label{callout-checkpointux2a-1.8}
\begin{fbx}{callout-checkpoint}{Checkpoint:}{The Physics of Data}
\phantomsection\label{callout-checkpoint*-1.8}

Data engineering is governed by physical costs. Check your intuition:

\begin{itemize}
\tightlist
\item[$\square$]
  Do you understand \textbf{Data Gravity}: why petabyte-scale datasets
  force compute to move to the data?
\item[$\square$]
  Can you explain the \textbf{Energy-Movement Invariant}: why moving a
  byte of data costs orders of magnitude more energy than processing it?
\item[$\square$]
  Can you define \textbf{Information Entropy} in this context: why a
  smaller, diverse dataset can be more valuable than a massive,
  redundant one?
\end{itemize}

\end{fbx}

These physical properties of data impose hard engineering constraints on
every pipeline decision: where to store data, how to transform it, and
when to move computation rather than bytes. Translating these
constraints into reliable engineering practice requires a systematic
framework that organizes decisions around these physical realities.

\section{Four Pillars
Framework}\label{sec-data-engineering-ml-four-pillars-framework-4ef1}

Data engineering decisions span an enormous design space, from schema
validation rules to storage tier selection to privacy compliance
mechanisms. Without a unifying structure, teams make these decisions ad
hoc, often discovering costly interactions between choices only after
deployment failures. This section introduces the \textbf{Four Pillars
Framework}, which organizes data engineering concerns into four
interdependent dimensions: \textbf{Quality}, \textbf{Reliability},
\textbf{Scalability}, and \textbf{Governance}. We begin by examining the
cascading failure patterns that motivate such a framework, then derive
the pillars from first principles and demonstrate how they interact
across every stage of the data lifecycle.

\subsection{Data Cascades: Why Systematic Foundations
Matter}\label{sec-data-engineering-ml-data-cascades-systematic-foundations-matter-2efe}

Before presenting the pillars themselves, we examine why such a
framework is necessary by studying the cascading failure patterns that
arise when data engineering lacks systematic foundations.

Machine learning systems face a unique failure pattern called ``Data
Cascades,''\sidenote{\textbf{Data Cascades}: A systems failure pattern
unique to ML where poor data quality in early stages amplifies
throughout the entire pipeline, causing downstream model failures,
project termination, and potential user harm. Unlike traditional
software where bad inputs typically produce immediate errors, ML systems
degrade silently until quality issues become severe enough to
necessitate complete system rebuilds. } where poor data quality in early
stages amplifies throughout the entire pipeline, causing downstream
model failures, project termination, and potential user harm
(\citeproc{ref-sambasivan2021everyone}{Sambasivan et al. 2021}).
Traditional software produces immediate errors when encountering bad
inputs. ML systems degrade silently until quality issues become severe
enough to necessitate complete system rebuilds.

Figure~\ref{fig-cascades} reveals how data quality failures cascade
through every pipeline stage, with data collection errors proving
especially problematic. Lapses in this initial stage become apparent
during model evaluation and deployment, potentially requiring abandoning
the entire model and restarting. This cascading nature of failures
motivates a systematic framework for data engineering decisions.

\begin{figure}[htb]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/f6e6503d9e8055606ed541942b012cb6753050b3.pdf}}

}

\caption{\label{fig-cascades}\textbf{Data Quality Cascades}: Errors
introduced early in the machine learning workflow amplify across
subsequent stages, increasing costs and potentially leading to flawed
predictions or harmful outcomes. Source:
(\citeproc{ref-sambasivan2021everyone}{Sambasivan et al. 2021}).}

\end{figure}%

Data cascades occur when teams skip establishing clear quality criteria,
reliability requirements, and governance principles before beginning
data collection and processing. Preventing these cascades requires a
systematic framework that guides technical choices from data acquisition
through production deployment. To see how quickly a single upstream
change can cascade into a production failure, consider what happens when
a schema change propagates through a \emph{pipeline jungle} without
validation.

\phantomsection\label{callout-exampleux2a-1.9}
\begin{fbx}{callout-example}{Example:}{The Pipeline Jungle}
\phantomsection\label{callout-example*-1.9}
\textbf{The Failure}: A credit scoring model suddenly started rejecting
all applicants from a specific region.

\textbf{The Root Cause}: An upstream team changed the schema of the
\texttt{zip\_code} field from \texttt{integer} to \texttt{string} to
handle international codes. * The data pipeline silently cast ``02139''
(string) to 2139 (integer). * The leading zero was lost. * The model,
treating \texttt{zip\_code} as a categorical feature, saw ``2139'' as a
completely new, unknown category and defaulted to ``high risk''
behavior.

\textbf{The Systems Lesson}: This is a \textbf{Pipeline Jungle} failure.
Without explicit \textbf{Data Contracts} and schema validation at the
ingestion interface, changes in one system (``we need string zip
codes'') cause catastrophic, silent failures in downstream systems. Data
engineering is the defense against this entropy.

\end{fbx}

Having established why cascades occur, we can now ground the four
pillars in the fundamental constraints that make each one necessary.
Understanding why these pillars emerge from first principles, rather
than accepting them as arbitrary categories, helps practitioners apply
them consistently even in novel situations.

\subsection{Deriving the Four Pillars from First
Principles}\label{sec-data-engineering-ml-deriving-four-pillars-first-principles-c833}

\textbf{Quality emerges from statistical learning theory.} Machine
learning assumes that training data and production data are drawn from
the same distribution: \(P_{train}(X, Y) \approx P_{serve}(X, Y)\). When
this assumption breaks---through labeling errors, sampling bias, or data
corruption---the learned function \(f(x)\) fails to generalize. Quality
is not a preference but a \emph{mathematical prerequisite} for learning.
The cascading failures documented in
Section~\ref{sec-data-engineering-ml-data-cascades-systematic-foundations-matter-2efe}
are direct consequences of violating this distributional assumption.

\textbf{Reliability emerges from distributed systems reality.} Any
system with more than one component experiences partial failures.
Networks partition. Disks fail. Services crash. The CAP theorem
(\citeproc{ref-brewer2000towards}{Brewer 2000}) formalizes this:
distributed systems cannot simultaneously guarantee Consistency,
Availability, and Partition tolerance. Data pipelines spanning multiple
services, storage systems, and processing stages must assume failures
will occur. Reliability is not about preventing failures but
\emph{surviving them gracefully}---through redundancy, idempotent
operations, and graceful degradation.

\textbf{Scalability emerges from the exponential growth of data.} Data
volumes grow exponentially while human attention remains constant. From
2010 to 2020, global data creation grew from 2 zettabytes to 64
zettabytes---a 32x increase. No amount of manual effort can keep pace.
Systems must handle 10x growth without 10x engineering effort.
Scalability is not about big data \emph{per se} but about
\emph{sublinear effort scaling}---ensuring that doubling data volume
does not require doubling infrastructure complexity or operational
burden.

\textbf{Governance emerges from the principal-agent problem.} When data
involves humans---whether as subjects (whose data is collected), as
annotators (who label data), or as consumers (who trust model
predictions)---interests diverge. Users want privacy; models want
features. Annotators want fair compensation; platforms want low costs.
Regulators demand transparency; proprietary systems resist disclosure.
Governance is not bureaucratic overhead but the \emph{coordination
mechanism} that aligns these divergent interests through access
controls, audit trails, and accountability structures.

\subsection{The Four Foundational
Pillars}\label{sec-data-engineering-ml-four-foundational-pillars-c119}

Every data engineering decision, from choosing storage formats to
designing ingestion pipelines, should be evaluated against four
principles. Figure~\ref{fig-four-pillars} illustrates how these pillars
interact, with each contributing to system success through systematic
decision-making.

Data quality provides the foundation for system success. Quality issues
compound throughout the ML lifecycle through ``Data Cascades''
(Section~\ref{sec-data-engineering-ml-data-cascades-systematic-foundations-matter-2efe}),
where early failures propagate and amplify downstream. Quality includes
accuracy, completeness, consistency, and fitness for the intended ML
task. The mathematical foundations of this relationship appear in
\textbf{?@sec-deep-learning-systems-foundations} and
\textbf{?@sec-dnn-architectures}.

ML systems require consistent, predictable data processing that handles
failures gracefully. Reliability means building systems that continue
operating despite component failures, data anomalies, or unexpected load
patterns. This includes error handling, monitoring, and recovery
mechanisms throughout the data pipeline.

Scalability addresses the challenge of growth. As ML systems grow from
prototypes to production services, data volumes and processing
requirements increase dramatically. Systems must handle growing data
volumes, user bases, and computational demands without complete
redesigns. Scalability must be cost-effective: raw capacity means little
if infrastructure costs grow faster than business value. Cost
effectiveness spans resource efficiency (compute proportional to
workload), storage optimization (balancing access speed against
retention costs), and operational sustainability (avoiding technical
debt that compounds maintenance burden). Our recommendation lighthouse
illustrates this scalability challenge at its most extreme.

\phantomsection\label{callout-lighthouseux2a-1.10}
\begin{fbx}{callout-lighthouse}{Lighthouse:}{DLRM (Recommendation Lighthouse)}
\phantomsection\label{callout-lighthouse*-1.10}
\textbf{Why it matters:} Recommendation systems like DLRM exemplify the
\textbf{scalability} challenge of modern data engineering. They rely on
high-cardinality categorical features (like User IDs or Product IDs)
that must be mapped to dense vectors via embedding tables.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3333}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Property}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Value}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{System Implication}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Data Scale} & Billion+ users/items & Embedding tables grow to
TB/PB scale. \\
\textbf{Constraint} & Memory Capacity & Tables exceed single-GPU memory;
requires sharding. \\
\textbf{Bottleneck} & Sparse Access & Random lookups stress memory
bandwidth more than compute. \\
\end{longtable}

Unlike ResNet (compute-bound) or GPT-2 (bandwidth-bound), DLRM is
limited by \textbf{memory capacity} and the sheer logistics of storing
and accessing massive lookup tables efficiently.

\end{fbx}

Governance provides the framework within which quality, reliability, and
scalability operate. Data governance ensures systems operate within
legal, ethical, and business constraints while maintaining transparency
and accountability. This includes privacy protection, bias mitigation,
regulatory compliance, and clear data ownership and access controls.

\begin{figure}[htb]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/7d87a284545b42c71989f497beb3edc15e3ec069.pdf}}

}

\caption{\label{fig-four-pillars}\textbf{The Four Pillars of Data
Engineering}: Quality, Reliability, Scalability, and Governance form the
foundational framework for ML data systems. Each pillar contributes
essential capabilities (solid arrows), while trade-offs between pillars
(dashed lines) require careful balancing: validation overhead affects
throughput, consistency constraints limit distributed scale, privacy
requirements impact performance, and bias mitigation may reduce
available training data.}

\end{figure}%

When ML systems exhibit failures,
Table~\ref{tbl-four-pillars-diagnostic} helps teams identify which
pillar to investigate first based on observed symptoms:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3333}}@{}}
\caption{\textbf{Four Pillars Diagnostic Guide}: When ML systems exhibit
failures, this mapping helps teams identify which pillar to investigate
first based on observed
symptoms.}\label{tbl-four-pillars-diagnostic}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Symptom}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Likely Pillar}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Investigation}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Symptom}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Likely Pillar}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Investigation}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Model accuracy drops gradually} & Quality & Check for data
drift, label degradation \\
\textbf{Pipeline fails intermittently} & Reliability & Review error
handling, retry logic \\
\textbf{Training takes too long} & Scalability & Profile bottlenecks,
check parallelization \\
\textbf{Audit finds compliance gaps} & Governance & Review lineage
tracking, access controls \\
\textbf{Features differ train vs.~serve} & Quality and Reliability &
Check consistency contracts \\
\end{longtable}

\subsection{Integrating the Pillars Through Systems
Thinking}\label{sec-data-engineering-ml-integrating-pillars-systems-thinking-a32e}

Having examined how each pillar addresses specific concerns and how they
manifest across pipeline stages, we now turn to how they function
together as a system. Understanding each pillar individually is
necessary but not sufficient; effective data engineering requires
recognizing their interconnections. These four pillars are not
independent components but interconnected aspects of a unified system
where decisions in one area affect all others. Quality improvements must
account for scalability constraints, reliability requirements influence
governance implementations, and governance policies shape quality
metrics. This systems perspective guides our exploration of data
engineering, examining how each technical topic supports and balances
these principles while managing their tensions.

The practical stakes of this integrated framework are substantial.
According to industry surveys, data scientists spend an estimated
60--80\% of their time on data preparation tasks\sidenote{\textbf{Data
Quality Reality}: The famous ``garbage in, garbage out'' principle was
first coined by IBM computer programmer George Fuechsel in the 1960s,
describing how flawed input data produces nonsense output. This
principle remains critically relevant in modern ML systems
(\citeproc{ref-crowdflower2016data}{CrowdFlower, n.d.}). }, with data
cleaning alone consuming up to 60\% of practitioners' effort (see
\textbf{?@fig-ds-time} in \textbf{?@sec-ai-development-workflow}). This
imbalance reflects ad-hoc rather than systematic data engineering
practices. Applying the four-pillar framework consistently can reduce
data preparation time while producing more reliable and maintainable
systems.

This time allocation motivates knowing the key constants that govern
data engineering costs and timelines.

\phantomsection\label{callout-perspectiveux2a-1.11}
\begin{fbx}{callout-perspective}{Systems Perspective:}{Key Data Engineering Numbers}
\phantomsection\label{callout-perspective*-1.11}
Just as systems engineers memorize latency numbers, ML engineers should
internalize these data engineering constants:

\textbf{Costs (2024 estimates)}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3333}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Operation}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Cost}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Notes}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Crowdsourced image label} & \$0.01--0.05 & Simple
classification \\
\textbf{Bounding box annotation} & \$0.05--0.20 & Per box, simple
scenes \\
\textbf{Expert medical label} & \$50--200 & Per study, radiologist \\
\textbf{S3 storage (Standard)} & \$23/TB/month & Hot storage \\
\textbf{S3 retrieval (Glacier)} & \$0.01/GB & Standard: 3-5
hours\sidenote{\textbf{S3 Glacier Retrieval Tiers}: AWS S3 Glacier
offers multiple retrieval options with different cost and latency
trade-offs: Standard (\$0.01/GB, 3-5 hours), Expedited (\$0.03/GB, 1-5
minutes), and Bulk (free, 5-12 hours). Glacier Deep Archive has longer
retrieval times (up to 12 hours for Standard). Pricing as of 2024; see
aws.amazon.com/s3/pricing for current rates. } \\
\textbf{GPU training hour (A100)} & \$2--4 & Cloud spot pricing \\
\textbf{Human review hour} & \$15--50 & Depending on expertise \\
\end{longtable}

\textbf{Time Constants}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3333}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Operation}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Duration}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Bottleneck}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Label 1M images (crowdsourced)} & 2--4 weeks & Annotation
throughput \\
\textbf{Train ResNet-50 on ImageNet} & 4--6 hours & Compute (8× A100,
optimized) \\
\textbf{Feature store lookup} & 1--10 ms & Network + cache \\
\end{longtable}

The contrast matters: \textbf{weeks} for human labeling, \textbf{hours}
for GPU training, \textbf{milliseconds} for serving. Labeling is the
bottleneck.

\textbf{The 1000× Rule}: Labeling typically costs \textbf{1,000--3,000×}
more than the compute to train on that data. A \$100K labeling budget
buys data that trains on \$30--100 of GPU time.

\textbf{The 80/20 Split}: 80\% of data engineering effort goes to 20\%
of features: the ``long tail'' of edge cases, rare categories, and
quality exceptions.

\end{fbx}

\subsection{Framework Application Across Data
Lifecycle}\label{sec-data-engineering-ml-framework-application-across-data-lifecycle-c512}

This four-pillar framework guides our exploration from problem
definition through production operations. Establishing clear problem
definitions and governance principles shapes all subsequent technical
decisions. The framework guides acquisition strategies, where quality
and reliability requirements determine how we source and validate data.
Processing and storage decisions follow from scalability and governance
constraints, while operational practices maintain all four pillars
throughout the system lifecycle.

Subsequent sections examine how these pillars manifest in specific
technical decisions: sourcing techniques that balance quality with
scalability, storage architectures that support performance within
governance constraints, and processing pipelines that maintain
reliability while handling massive scale.

Table~\ref{tbl-four-pillars-matrix} shows how each pillar manifests
across major stages of the data pipeline, providing both a planning tool
for system design and a reference for troubleshooting when issues arise
at different pipeline stages.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2000}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2000}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2000}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2000}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2000}}@{}}
\caption{\textbf{Four Pillars Applied Across Data Pipeline Stages}:
Quality, Reliability, Scalability, and Governance principles manifest
differently at each major stage of the data engineering pipeline.
Specific techniques and practices implement each pillar at every stage,
providing a comprehensive framework for systematic decision-making and
troubleshooting.}\label{tbl-four-pillars-matrix}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Stage}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Quality}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Reliability}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Scalability}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Governance}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Stage}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Quality}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Reliability}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Scalability}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Governance}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Acquisition} & Representative sampling, bias detection & Diverse
sources, redundant collection strategies & Web scraping, synthetic data
generation & Consent, anonymization, ethical sourcing \\
\textbf{Ingestion} & Schema validation, data profiling & Dead letter
queues, graceful degradation & Batch vs stream processing, autoscaling
pipelines & Access controls, audit logs, data lineage \\
\textbf{Processing} & Consistency validation, training-serving parity &
Idempotent transformations, retry mechanisms & Distributed frameworks,
horizontal scaling & Lineage tracking, privacy preservation, bias
monitoring \\
\textbf{Storage} & Data validation checks, freshness monitoring &
Backups, replication, disaster recovery & Tiered storage, partitioning,
compression optimization & Access audits, encryption, retention
policies \\
\end{longtable}

\section{Applying the
Framework}\label{sec-data-engineering-ml-applying-framework-3af9}

The Four Pillars provide a conceptual lens for reasoning about data
engineering decisions, but principles only become useful when they guide
concrete action. This section bridges that gap by demonstrating how to
apply the framework through structured problem definition and a detailed
case study in keyword spotting (KWS) for TinyML. The case study will
recur throughout the chapter, grounding each subsequent topic in a
single, realistic engineering scenario where quality, reliability,
scalability, and governance concerns interact under tight resource
constraints.

\subsection{Structured Approach to Problem
Definition}\label{sec-data-engineering-ml-structured-approach-problem-definition-f7ad}

Translating the Four Pillars into practice begins with structured
problem definition. Governance principles (privacy protection, bias
mitigation, regulatory compliance, and documentation) must be
established before any data collection begins, not retrofitted later.
The full implementation of governance infrastructure is examined in
Section~\ref{sec-data-engineering-ml-governance-observability-2c05};
here we focus on how governance shapes initial problem definition.

ML systems require problem framing that goes beyond traditional software
development (\citeproc{ref-sculley2015hidden}{Sculley et al. 2021}).
Whether developing recommendation engines processing millions of user
interactions, computer vision systems analyzing medical images, or
natural language models handling diverse text data, each system brings
unique challenges requiring careful consideration within the governance
and technical framework.

Clear objectives provide unified direction from data collection
strategies through deployment operations. These objectives must balance
technical performance with governance requirements, creating measurable
outcomes that include both accuracy metrics and fairness criteria. Key
steps must precede any data collection effort:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Identify and clearly state the problem definition
\item
  Set clear objectives to meet
\item
  Establish success benchmarks
\item
  Understand end-user engagement/use
\item
  Understand the constraints and limitations of deployment
\item
  Perform data collection.
\item
  Iterate and refine.
\end{enumerate}

Our TinyML lighthouse demonstrates why these steps are especially
consequential under extreme resource constraints.

\phantomsection\label{callout-lighthouseux2a-1.12}
\begin{fbx}{callout-lighthouse}{Lighthouse:}{KWS (TinyML Lighthouse)}
\phantomsection\label{callout-lighthouse*-1.12}
\textbf{Why it matters:} Keyword Spotting represents the \textbf{Tiny
Constraint} archetype from \textbf{?@sec-introduction}, our fifth
lighthouse example. With approximately 200K parameters and an 800KB
footprint, KWS systems must achieve 98\% accuracy within sub-milliwatt
power budgets on always-on embedded devices (like Smart Doorbells).

This extreme constraint makes data engineering decisions, not model
architecture, the primary lever for system performance. Unlike
cloud-deployed models where additional compute can partially compensate
for data issues, TinyML systems have no such headroom. Every labeling
error, distribution gap, or drift event directly impacts the user
experience. The KWS case study demonstrates why data quality determines
success when computational resources cannot paper over data
deficiencies.

\end{fbx}

\subsection{Framework Application Through Keyword Spotting Case
Study}\label{sec-data-engineering-ml-framework-application-keyword-spotting-case-study-c8e9}

Keyword Spotting (KWS) systems provide an ideal case study for applying
our four-pillar framework to real-world data engineering challenges.
These systems power voice-activated devices like smartphones and smart
speakers, detecting specific wake words such as ``OK, Google'' or
``Alexa'' within continuous audio streams while operating under strict
resource constraints.\sidenote{\textbf{Voice Match Personalization}:
When you set up ``OK Google'' on an Android phone, the device asks you
to say the wake phrase several times. This enrollment process collects
speaker-specific audio samples that fine-tune the on-device KWS model to
your voice, reducing false activations from other speakers while
improving detection accuracy for your speech patterns. This
personalization step illustrates the data engineering challenge at
individual scale: even a single user requires careful data collection
(multiple utterances), quality control (re-record if ambient noise is
too high), and privacy governance (voice prints stored locally
on-device, not uploaded to the cloud). }

Figure~\ref{fig-keywords} depicts a KWS system operating as a
lightweight, always-on front-end that triggers more complex voice
processing systems. These systems demonstrate interconnected challenges
across all four pillars: Quality (accuracy across diverse environments),
Reliability (consistent battery-powered operation), Scalability (severe
memory constraints), and Governance (privacy protection). These
constraints explain why many KWS systems support only a limited number
of languages: collecting high-quality, representative voice data for
smaller linguistic populations proves prohibitively difficult given
governance and scalability challenges. All four pillars must work
together to achieve successful deployment.

\begin{figure}

\centering{

\includegraphics[width=0.55\linewidth,height=\textheight,keepaspectratio]{contents/vol1/data_engineering/images/png/data_engineering_kws.png}

}

\caption{\label{fig-keywords}\textbf{Keyword Spotting System}: A
voice-activated device uses a lightweight, always-on wake word detector
that listens continuously and triggers the main voice assistant upon
keyword detection.}

\end{figure}%

With this understanding established, we apply the problem definition
approach to the KWS example, demonstrating how the four pillars guide
practical engineering decisions:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Identifying the Problem}: KWS detects specific keywords amidst
  ambient sounds and other spoken words. The primary problem is to
  design a system that can recognize these keywords with high accuracy,
  low latency, and minimal false positives or negatives, especially when
  deployed on devices with limited computational resources. A
  well-specified problem definition for developing a new KWS model
  should identify the desired keywords along with the envisioned
  application and deployment scenario.
\item
  \textbf{Setting Clear Objectives}: The objectives for a KWS system
  must balance multiple competing requirements. Performance targets
  include achieving high accuracy rates (98\% accuracy in keyword
  detection) while ensuring low latency (keyword detection and response
  within 200 milliseconds). Resource constraints demand minimizing power
  consumption to extend battery life on embedded devices and ensuring
  the model size is optimized for available memory on the device.
\item
  \textbf{Benchmarks for Success}: Establish clear metrics to measure
  the success of the KWS system. Key performance indicators include true
  positive rate (the percentage of correctly identified keywords
  relative to all spoken keywords) and false positive rate (the
  percentage of non-keywords including silence, background noise, and
  out-of-vocabulary words incorrectly identified as keywords).
  Detection/error tradeoff curves evaluate KWS on streaming audio
  representative of real-world deployment scenarios by comparing false
  accepts per hour (false positives over total evaluation audio
  duration) against false rejection rate (missed keywords relative to
  spoken keywords in evaluation audio), as demonstrated by Nayak et al.
  (\citeproc{ref-nayak2022improving}{2022}).
\end{enumerate}

\phantomsection\label{callout-notebookux2a-1.13}
\begin{fbx}{callout-notebook}{AI Engineer’s Notebook:}{False Positive Targets}
\phantomsection\label{callout-notebook*-1.13}
\textbf{Constraint}: User tolerance is max 1 false wake-up per month.

\textbf{Operational Parameters}

\begin{itemize}
\tightlist
\item
  \textbf{Duty Cycle}: Always-on (24 hours/day).
\item
  \textbf{Window Size}: 1 second classification windows.
\item
  \textbf{Windows per Month}:
  \(60 \times 60 \times 24 \times 30 = 2\,592\,000\) windows.
\end{itemize}

\textbf{Required Accuracy}

\begin{itemize}
\tightlist
\item
  \textbf{False Positive Rate (FPR)}:
  \(\frac{1}{2\,592\,000} \approx 3\.9e\-07\)
\item
  \textbf{Precision Requirement}: 99.99996\% rejection of non-keywords.
\end{itemize}

\textbf{Implication}: Standard accuracy metrics (e.g., ``99\%
accuracy'') are meaningless here. We must evaluate specifically on
\textbf{False Accepts per Hour (FA/Hr)}.

\end{fbx}

Operational metrics track response time (keyword utterance to system
response) and power consumption (average power used during keyword
detection).

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\item
  \textbf{Stakeholder Engagement and Understanding}: Engage with
  stakeholders, which include device manufacturers, hardware and
  software developers, and end-users. Understand their needs,
  capabilities, and constraints. Different stakeholders bring competing
  priorities: device manufacturers might prioritize low power
  consumption, software developers might emphasize ease of integration,
  and end-users would prioritize accuracy and responsiveness. Balancing
  these competing requirements shapes system architecture decisions
  throughout development.
\item
  \textbf{Understanding the Constraints and Limitations of Embedded
  Systems}: Embedded devices come with their own set of challenges that
  shape KWS system design. Memory limitations require extremely
  lightweight models, often in the tens-of-kilobytes range, to fit in
  the always-on island of the SoC\sidenote{\textbf{System on Chip
  (SoC)}: An integrated circuit that combines all essential computer
  components (processor, memory, I/O interfaces) on a single chip.
  Modern SoCs include specialized ``always-on'' low-power domains that
  continuously monitor for triggers like wake words while the main
  processor sleeps, often achieving sub-milliwatt power consumption for
  continuous listening workloads (exact power depends on implementation
  and duty cycle). }; this constraint covers only model weights while
  preprocessing code must also fit within tight memory bounds.
  Processing power constraints from limited computational capabilities
  (often a few hundred MHz of clock speed) demand aggressive model
  optimization for efficiency. Power consumption becomes critical since
  most embedded devices run on batteries, so KWS systems often target
  sub-milliwatt power consumption during continuous listening.
  Environmental challenges add another layer of complexity, as devices
  must function effectively across diverse deployment scenarios ranging
  from quiet bedrooms to noisy industrial settings.
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{5}
\item
  \textbf{Data Collection and Analysis}: For a KWS system, data quality
  and diversity determine success. The dataset must capture demographic
  diversity by including speakers with various accents across age and
  gender to ensure wide-ranging recognition support. Keyword variations
  require attention since people pronounce wake words differently,
  requiring the dataset to capture these pronunciation nuances and
  slight variations. Background noise diversity proves essential,
  necessitating data samples that include or are augmented with
  different ambient noises to train the model for real-world scenarios
  ranging from quiet environments to noisy conditions.
\item
  \textbf{Iterative Feedback and Refinement}: Finally, once a prototype
  KWS system is developed, teams must ensure the system remains aligned
  with the defined problem and objectives as deployment scenarios change
  over time and use-cases evolve. This requires testing in real-world
  scenarios, gathering feedback about whether some users or deployment
  scenarios encounter underperformance relative to others, and
  iteratively refining both the dataset and model based on observed
  failure patterns.
\end{enumerate}

\textbf{The KWS Design Space}: These requirements create a
multi-dimensional design space where data engineering choices cascade
through system performance. Table~\ref{tbl-kws-design-space} quantifies
key trade-offs, enabling principled decisions rather than ad-hoc
selection.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2000}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2000}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2000}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2000}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2000}}@{}}
\caption{\textbf{KWS Data Engineering Design Space}: Each design choice
creates quantifiable trade-offs across the four pillars. Higher sampling
rates improve quality but double storage and processing (scalability
impact). More training data improves accuracy but multiplies labeling
costs (governance/cost impact). Local inference eliminates latency but
requires aggressive quantization (quality/reliability trade-off). This
design space analysis guides systematic optimization rather than
intuition-based decisions.}\label{tbl-kws-design-space}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Design Choice}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Quality Impact}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Latency Impact}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Cost Impact}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Memory Impact}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Design Choice}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Quality Impact}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Latency Impact}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Cost Impact}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Memory Impact}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{16kHz vs 8kHz sampling} & +2--4\% accuracy & 2× storage & 2×
processing & 2× feature size \\
\textbf{13 vs 40 MFCC coefficients} & +3--5\% accuracy & 3× feature
compute & Minimal & 3× feature memory \\
\textbf{1M vs 10M training examples} & +5--8\% accuracy & 10× training
time & 10× labeling cost & 10× storage \\
\textbf{Clean vs noisy training data} & +10--15\% real-world & Minimal &
3× collection cost & Minimal \\
\textbf{Local vs cloud inference} & −2\% accuracy (quant) & 10ms vs
100ms & \$0 vs \$0.001/query & 16KB vs unlimited \\
\textbf{Synthetic vs real augmentation} & +3--5\% robustness & Minimal &
10× cheaper & Minimal \\
\end{longtable}

The following worked example demonstrates how to apply this design space
analysis to a concrete engineering scenario.

\phantomsection\label{callout-exampleux2a-1.14}
\begin{fbx}{callout-example}{Example:}{Optimizing the KWS Design Space}
\phantomsection\label{callout-example*-1.14}
\textbf{Scenario}: You're building a KWS system for a smart speaker with
these constraints:

\begin{itemize}
\tightlist
\item
  \textbf{Target}: 98\% accuracy, \textless1 false wake per month
\item
  \textbf{Budget}: \$150K total data engineering budget
\item
  \textbf{Memory}: 64KB model size limit (always-on island)
\item
  \textbf{Timeline}: 6 months to production
\end{itemize}

\subsubsection*{Step 1: Apply Constraints to Eliminate
Options}\label{step-1-apply-constraints-to-eliminate-options}
\addcontentsline{toc}{subsubsection}{Step 1: Apply Constraints to
Eliminate Options}

From Table~\ref{tbl-kws-design-space}, the 64KB memory limit eliminates:

\begin{itemize}
\tightlist
\item
  40 MFCC coefficients (3× memory) → Must use 13 MFCCs
\item
  Cloud inference (requires network stack) → Must use local inference
\end{itemize}

\subsubsection*{Step 2: Calculate Budget
Allocation}\label{step-2-calculate-budget-allocation}
\addcontentsline{toc}{subsubsection}{Step 2: Calculate Budget
Allocation}

Using the TCDO model with \$150K budget:

\begin{itemize}
\tightlist
\item
  \textbf{Labeling} (\textasciitilde60\%): \$90K available
\item
  \textbf{Storage/Processing} (\textasciitilde25\%): \$37.5K
\item
  \textbf{Governance/Other} (\textasciitilde15\%): \$22.5K
\end{itemize}

At \$0.10/label with 20\% review overhead: \$90K ÷ \$0.12 = \textbf{750K
labeled examples}

This falls between 1M and 10M in our design space---closer to 1M,
suggesting +5--6\% accuracy contribution from data volume.

\subsubsection*{Step 3: Maximize Remaining
Accuracy}\label{step-3-maximize-remaining-accuracy}
\addcontentsline{toc}{subsubsection}{Step 3: Maximize Remaining
Accuracy}

Current accuracy budget:

\begin{itemize}
\tightlist
\item
  Base model: \textasciitilde90\% (minimal data)
\item
  +5--6\% from 750K examples
\item
  Need: +2--3\% more to reach 98\%
\end{itemize}

Options from design space:

\begin{itemize}
\tightlist
\item
  16kHz sampling: +2--4\% accuracy, 2× storage cost ✓ (fits budget)
\item
  Noisy training data: +10--15\% real-world accuracy, 3× collection cost
\item
  Synthetic augmentation: +3--5\% robustness, 10× cheaper than real data
  ✓
\end{itemize}

\subsubsection*{Step 4: Final
Configuration}\label{step-4-final-configuration}
\addcontentsline{toc}{subsubsection}{Step 4: Final Configuration}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3333}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Choice}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Selection}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Rationale}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Sampling rate} & 16kHz & +3\% accuracy worth 2× storage within
budget \\
\textbf{MFCC coefficients} & 13 & Memory-constrained, non-negotiable \\
\textbf{Training examples} & 750K real + 2M synthetic & Budget-optimal
mix \\
\textbf{Data diversity} & Noisy + clean mix & Critical for real-world
deployment \\
\textbf{Inference} & Local, 8-bit quantized & Memory-constrained \\
\textbf{Augmentation} & Heavy synthetic & 10× cost efficiency \\
\end{longtable}

\textbf{Projected Outcome}: 97--99\% accuracy (meeting target), \$145K
spend (under budget), 48KB model (under limit).

\textbf{The Engineering Lesson}: Systematic design space analysis
transformed intuition (``we need more data'') into quantified decisions
(``750K real + 2M synthetic maximizes accuracy per dollar given memory
constraints'').

\end{fbx}

With optimal parameters selected from our design space, implementation
requires combining multiple data collection approaches. Our KWS system
demonstrates how these approaches work together across the project
lifecycle. Pre-existing datasets like Google's Speech Commands
(\citeproc{ref-warden2018speech}{Warden 2018}) provide a foundation for
initial development, offering carefully curated voice samples for common
wake words. For multilingual coverage, the Multilingual Spoken Words
Corpus (MSWC) (\citeproc{ref-mazumder2021multilingual}{Mazumder et al.
2021}) extends this foundation to 50 languages with over 23 million
examples. However, even these large-scale datasets often lack diversity
in accents, environments, and recording conditions, necessitating
additional strategies.

To address coverage gaps, web scraping supplements baseline datasets by
gathering diverse voice samples from video platforms and speech
databases, capturing natural speech patterns and wake word variations.
Crowdsourcing platforms like Amazon Mechanical
Turk\sidenote{\textbf{Mechanical Turk Origins}: Amazon's crowdsourcing
platform (2005) named after the 18th-century chess ``automaton'' that
secretly concealed a human player. MTurk enables distributed human
computation: ImageNet's 14M labels came from 49,000 MTurk workers. The
platform ironically reverses the original Turk's deception---presenting
human intelligence as AI then, ML training leverages human intelligence
now. } enable targeted collection of wake word samples across different
demographics and environments. This approach is particularly valuable
for underrepresented languages or specific acoustic conditions.

Finally, synthetic data generation fills remaining gaps through speech
synthesis (\citeproc{ref-werchniak2021exploring}{Werchniak et al. 2021})
and audio augmentation, creating unlimited wake word variations across
acoustic environments, speaker characteristics, and background
conditions. This comprehensive approach enables KWS systems that perform
well across diverse real-world conditions while demonstrating how
systematic problem definition guides data strategy throughout the
project lifecycle.

The KWS case study previews how multiple data sources work together:
curated datasets provide foundations, web scraping captures natural
variations, crowdsourcing addresses coverage gaps, and synthetic
generation enables systematic exploration of edge cases. These
complementary strategies, each with distinct trade-offs across quality,
cost, and scale, require systematic evaluation rather than ad-hoc
selection. Before examining pipeline architecture that processes this
heterogeneous data, we must first understand how to strategically
acquire it.

\section{Strategic Data
Acquisition}\label{sec-data-engineering-ml-strategic-data-acquisition-418f}

Data acquisition is a strategic decision that determines a system's
capabilities and limitations. Each sourcing strategy (existing datasets,
web scraping, crowdsourcing, synthetic generation) offers different
trade-offs across quality, cost, scale, and ethical considerations. No
single approach satisfies all requirements; successful ML systems
typically combine multiple strategies, balancing complementary strengths
against competing constraints.

Our KWS system illustrates these interconnected requirements. Achieving
98\% accuracy across diverse acoustic environments requires
representative data spanning accents, ages, and recording conditions
(quality). Maintaining consistent detection despite device variations
demands data from varied hardware (reliability). Supporting millions of
concurrent users requires data volumes that manual collection cannot
economically provide (scalability). Protecting user privacy in
always-listening systems constrains collection methods and requires
careful anonymization (governance). These interconnected requirements
demonstrate why acquisition strategy must be evaluated systematically
rather than through ad-hoc source selection.

\subsection{Data Source Evaluation and
Selection}\label{sec-data-engineering-ml-data-source-evaluation-selection-cd87}

Having established the strategic importance of data acquisition, quality
serves as the primary driver. When quality requirements dominate
acquisition decisions, the choice between curated datasets, expert
crowdsourcing, and controlled web scraping depends on the accuracy
targets, domain expertise needed, and benchmark requirements that guide
model development. Achieving quality requires understanding not just
that data appears correct but that it accurately represents the
deployment environment and provides sufficient coverage of edge cases
that might cause failures.

Pre-existing datasets from repositories such as Kaggle
(\citeproc{ref-kaggle_website}{Smith 2025}), UCI
(\citeproc{ref-uci_repo}{Srinivasan et al. 2023}), and ImageNet
(\citeproc{ref-imagenet_website}{Blaivas and Blaivas 2020}) offer
cost-efficient starting points with established benchmarks for
consistent performance comparison. However, quality assurance remains a
systems concern: ImageNet's validation set contains 3.4\% label errors
(\citeproc{ref-northcutt2021pervasive}{Northcutt, Athalye, and Mueller
2021}), and most datasets remain ``untended gardens'' where undocumented
quality issues propagate silently into downstream models. As Gebru et
al. (\citeproc{ref-gebru2018datasheets}{2021}) argued, datasets without
proper documentation invite misuse and bias amplification.

Documentation quality directly affects reproducibility, an ongoing
crisis in machine learning research
(\citeproc{ref-pineau2021improving}{Pineau et al. 2021};
\citeproc{ref-henderson2018deep}{Henderson et al. 2018}). Good
documentation captures collection methodology, variable definitions, and
baseline performance, enabling validation and replication. At scale,
volume and variety compound these quality challenges
(\citeproc{ref-gudivada2017data}{Gudivada, Rao, et al. 2017}), requiring
systematic validation pipelines rather than ad-hoc inspection.

Context matters as much as content. Popular benchmarks like ImageNet
invite overfitting that inflates performance metrics
(\citeproc{ref-beyer2020we}{Beyer et al. 2020}), and curated datasets
frequently fail to reflect real-world deployment distributions
(\citeproc{ref-venturebeat_datasets}{VentureBeat 2024}).

Central to these contextual concerns, a key consideration for ML systems
is how well pre-existing datasets reflect real-world deployment
conditions. Relying on standard datasets can create a concerning
disconnect between training and production environments.
Figure~\ref{fig-misalignment} visualizes this risk: when multiple ML
systems train on identical datasets, they propagate shared biases and
limitations throughout an entire ecosystem of deployed models.

\begin{figure}[htb]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/06a409f058b7b9585f8c817f1513cb81215ea55f.pdf}}

}

\caption{\label{fig-misalignment}\textbf{Shared Dataset Bias
Propagation}: Five models (A through E) all train on a single central
dataset repository. Arrows show how shared limitations, biases, and
blind spots propagate from the common dataset to every downstream model,
leading to correlated failures across the ecosystem.}

\end{figure}%

For our KWS lighthouse (and its broader Smart Doorbell application
context), pre-existing datasets provide essential starting points. For
the audio component, Google's Speech Commands
(\citeproc{ref-warden2018speech}{Warden 2018}) offers curated voice
samples for common wake words, while the Multilingual Spoken Words
Corpus (MSWC) (\citeproc{ref-mazumder2021multilingual}{Mazumder et al.
2021}) provides broader language coverage. For the visual component
(Wake Vision), the Wake Vision dataset
(\citeproc{ref-banbury2024wakevisiontailoreddataset}{Banbury et al.
2024}) serves as the standard benchmark for person detection on
microcontrollers. These datasets enable rapid prototyping and establish
baseline performance metrics. However, evaluating them against our
quality requirements immediately reveals coverage gaps: limited accent
diversity in audio, lack of low-light scenarios in vision, and
predominantly clean recording environments for both. Quality-driven
acquisition strategy recognizes these limitations and plans
complementary approaches to address them, demonstrating how
framework-based thinking guides source selection beyond simply choosing
available datasets.

\subsection{Scalability and Cost
Optimization}\label{sec-data-engineering-ml-scalability-cost-optimization-b9b3}

While quality-focused approaches excel at creating accurate,
well-curated datasets, they face inherent scaling limitations. When
scale requirements dominate---needing millions or billions of examples
that manual curation cannot economically provide---web scraping and
synthetic generation offer paths to massive datasets. Scalability
requires understanding the economic models underlying different
acquisition strategies: cost per labeled example, throughput
limitations, and how these scale with data volume. What proves
cost-effective at thousand-example scale often becomes prohibitive at
million-example scale, while approaches that require high setup costs
amortize favorably across large volumes.

Web scraping enables dataset construction at scales that manual curation
cannot match. Major vision datasets like ImageNet
(\citeproc{ref-imagenet_website}{Blaivas and Blaivas 2020}) and
OpenImages (\citeproc{ref-openimages_website}{Huang et al. 2022}) were
built through systematic scraping, and large language models depend on
web-scale text corpora (\citeproc{ref-groeneveld2024olmo}{Groeneveld et
al. 2024}). Targeted scraping of domain-specific sources, such as code
repositories (\citeproc{ref-chen2021evaluating}{Chen et al. 2021}),
further demonstrates the approach's versatility. However, production
systems that rely on continuous scraping face pipeline reliability
challenges: website structure changes break extractors, rate limiting
throttles collection throughput, and dynamic content introduces
inconsistencies that degrade model performance. Scraped data can also
contain unexpected noise, such as historical images appearing in
contemporary searches (Figure~\ref{fig-traffic-light}), requiring
systematic validation and cleaning stages.

Legal and ethical constraints further bound what scraping can achieve.
Not all websites permit scraping, and ongoing litigation around training
data usage illustrates the consequences of non-compliance
(\citeproc{ref-harvard_law_chatgpt}{School 2024}). Teams must document
data provenance, ensure compliance with terms of service and copyright
law, and apply anonymization procedures when scraping user-generated
content.

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{contents/vol1/data_engineering/images/jpg/1914_traffic.jpeg}}

}

\caption{\label{fig-traffic-light}\textbf{Data Source Noise}: A
black-and-white photograph from 1914 showing early manual semaphore
traffic signals, illustrating how historical images can appear in modern
web scraping results for contemporary queries. Such anachronistic
content requires systematic validation and filtering to prevent spurious
correlations in training data. Source: Vox.}

\end{figure}%

Crowdsourcing distributes annotation tasks across a global workforce,
enabling rapid labeling at scales that in-house teams cannot match.
Platforms like Amazon Mechanical Turk
(\citeproc{ref-mturk_website}{Liebetrau, Nowak, and Schneider 2014})
demonstrated this at landmark scale with ImageNet, where distributed
contributors categorized millions of images into thousands of classes
(\citeproc{ref-krizhevsky2012imagenet}{Krizhevsky, Sutskever, and Hinton
2017}). The approach's primary systems advantage is twofold: scalability
through parallel microtask distribution, and diversity through the range
of perspectives, cultural contexts, and linguistic variations that a
global contributor pool introduces. This diversity directly improves
model generalization across populations. Tasks can also be adjusted
dynamically based on initial results, enabling iterative refinement of
collection strategies as quality gaps emerge.

Moving beyond human-generated data entirely, synthetic data generation
represents the ultimate scalability solution, creating unlimited
training examples through algorithmic generation rather than manual
collection. This approach changes the economics of data acquisition by
removing human labor from the equation. Figure~\ref{fig-synthetic-data}
shows how synthetic data combines with historical datasets to create
larger, more diverse training sets that would be impractical to collect
manually.

\begin{figure}[htb]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/b042f8aea3bb24d819eca7b4e27c6a8a43313403.pdf}}

}

\caption{\label{fig-synthetic-data}\textbf{Synthetic Data Augmentation}:
A four-node pipeline where historical data and simulation outputs feed
into a synthetic data generation process, producing an expanded combined
training dataset with greater size and diversity than either source
alone. Source: AnyLogic (\citeproc{ref-anylogic_synthetic}{AnyLogic
2024}).}

\end{figure}%

Synthetic data is particularly valuable for rare event coverage and data
augmentation. Simulation environments enable controlled generation of
edge cases that are impractical to collect naturally
(\citeproc{ref-nvidia_simulation}{Crewe et al. 2023}), while
augmentation techniques like SpecAugment
(\citeproc{ref-park2019specaugment}{Park et al. 2019}) introduce noise,
pitch shifts, and temporal variations that improve model generalization
across deployment conditions (\citeproc{ref-shorten2019survey}{Shorten
and Khoshgoftaar 2019}).

For our KWS system, the scalability pillar drove the need for 23 million
training examples across 50 languages, a volume that manual collection
cannot economically provide. Web scraping supplements baseline datasets
with diverse voice samples from video platforms. Crowdsourcing enables
targeted collection for underrepresented languages. Synthetic data
generation fills remaining gaps through speech synthesis
(\citeproc{ref-werchniak2021exploring}{Werchniak et al. 2021}) and audio
augmentation, creating unlimited wake word variations across acoustic
environments, speaker characteristics, and background conditions. This
multi-source strategy demonstrates how scalability requirements shape
acquisition decisions, with each approach contributing specific
capabilities to the overall data ecosystem.

\subsection{Reliability Across Diverse
Conditions}\label{sec-data-engineering-ml-reliability-across-diverse-conditions-d63c}

Beyond quality and scale considerations, the reliability pillar
addresses a critical question: will our collected data enable models
that perform consistently across the deployment environment's full range
of conditions? A dataset might achieve high quality by established
metrics yet fail to support reliable production systems if it does not
capture the diversity encountered during deployment. Coverage
requirements for reliable models extend beyond simple volume to
encompass geographic diversity, demographic representation, temporal
variation, and edge case inclusion that stress-test model behavior.

Understanding coverage requirements requires examining potential failure
modes. Geographic bias occurs when training data comes predominantly
from specific regions, causing models to underperform in other areas. A
study of image datasets found significant geographic skew, with image
recognition systems trained on predominantly Western imagery performing
poorly on images from other regions
(\citeproc{ref-wang2019balanced}{Wang et al. 2019}). Demographic bias
emerges when training data does not represent the full user population,
potentially causing discriminatory outcomes. Hidden
stratification---where subpopulations are underrepresented or exhibit
different patterns---can cause systematic failures even in models that
perform well on aggregate metrics
(\citeproc{ref-oakden2020hidden}{Oakden-Rayner et al. 2020}). Temporal
variation matters when phenomena change over time: a fraud detection
model trained only on historical data may fail against new fraud
patterns. Edge case collection proves particularly challenging yet
critical, as rare scenarios often represent high-stakes situations where
failures cause the most harm.

The challenge of edge case collection becomes apparent in autonomous
vehicle development. While normal driving conditions are easy to capture
through test fleet operation, near-accidents, unusual pedestrian
behavior, or rare weather conditions occur infrequently. Synthetic data
generation helps address this by simulating rare scenarios, but
validating that synthetic examples accurately represent real edge cases
requires careful engineering. Some organizations employ targeted data
collection where test drivers deliberately create edge cases or where
engineers identify scenarios from incident reports that need better
coverage.

Dataset convergence represents another reliability challenge.
Figure~\ref{fig-misalignment} illustrates how multiple systems training
on identical datasets inherit identical blind spots and biases. An
entire ecosystem of models may fail on the same edge cases because all
trained on data with the same coverage gaps. This systemic risk
motivates diverse data sourcing strategies where each organization
collects supplementary data beyond common benchmarks, ensuring their
models develop different strengths and weaknesses rather than shared
failure modes.

For our KWS system, reliability manifests as consistent wake word
detection across acoustic environments from quiet bedrooms to noisy
streets, across accents from various geographic regions, and across age
ranges from children to elderly speakers. The data sourcing strategy
explicitly addresses these diversity requirements: web scraping captures
natural speech variation from diverse video sources, crowdsourcing
targets underrepresented demographics and environments, and synthetic
data systematically explores the parameter space of acoustic conditions.
Without this deliberate diversity in sourcing, the system might achieve
high accuracy on test sets while failing unreliably in production
deployment.

\subsection{Governance and Ethics in
Sourcing}\label{sec-data-engineering-ml-governance-ethics-sourcing-2d7f}

The governance pillar in data acquisition encompasses legal compliance,
ethical treatment of data contributors, privacy protection, and
transparency about data origins and limitations. Unlike the other
pillars that focus on system capabilities, governance ensures data
sourcing occurs within appropriate legal and ethical boundaries. The
consequences of governance failures extend beyond system performance to
reputational damage, legal liability, and potential harm to individuals
whose data was improperly collected or used.

Legal constraints significantly limit data collection methods across
different jurisdictions and domains. Not all websites permit scraping,
and violating these restrictions can have serious consequences, as
ongoing litigation around training data for large language models
demonstrates. Copyright law governs what publicly available content can
be used for training, with different standards emerging across
jurisdictions. Terms of service agreements may prohibit using data for
ML training even when technically accessible. Privacy regulations like
GDPR in Europe and CCPA in California impose strict requirements on
personal data collection, requiring consent, enabling deletion requests,
and sometimes demanding explanations of algorithmic decisions
(\citeproc{ref-wachter2017counterfactual}{Wachter, Mittelstadt, and
Russell 2017}). Healthcare data falls under additional regulations like
HIPAA in the United States, requiring specific safeguards for patient
information. Organizations must carefully navigate these legal
frameworks, documenting data sources and ensuring compliance throughout
the acquisition process.

Beyond legal compliance, ethical sourcing requires fair treatment of
human contributors. The crowdsourcing example we examined
earlier---where OpenAI outsourced data annotation to workers in Kenya
(\citeproc{ref-time_openai_kenya}{Perrigo 2023}) paying as little as
\$1.32 per hour for reviewing traumatic content---highlights governance
failures that can occur when economic pressures override ethical
considerations. Many workers reportedly suffered psychological harm from
exposure to disturbing material without adequate mental health support.
This case underscores power imbalances that can emerge when outsourcing
data work to economically disadvantaged regions. The lack of fair
compensation, inadequate support for workers dealing with traumatic
content, and insufficient transparency about working conditions
represent governance failures that affect human welfare beyond just
system performance.

Industry-wide standards for ethical crowdsourcing have begun emerging in
response to such concerns. Fair compensation means paying at least local
minimum wages, ideally benchmarked against comparable work in workers'
regions. Worker wellbeing requires providing mental health resources for
those dealing with sensitive content, limiting exposure to traumatic
material, and ensuring reasonable working conditions. Transparency
demands clear communication about task purposes, how contributions will
be used, and worker rights. Organizations like the Partnership on AI
have published guidelines for ethical crowdwork, establishing baselines
for acceptable practices.

While quality, scalability, and reliability focus on system
capabilities, the governance pillar ensures our data acquisition occurs
within appropriate ethical and legal boundaries. Privacy protection
forms another critical governance concern, particularly when sourcing
data involving individuals who did not explicitly consent to ML training
use. Anonymization emerges as a critical capability when handling
sensitive data. From a systems engineering perspective, anonymization
represents more than regulatory compliance; it constitutes a core design
constraint affecting data pipeline architecture, storage strategies, and
processing efficiency. ML systems must handle sensitive data throughout
their lifecycle: during collection, storage, transformation, model
training, and even in error logs and debugging outputs. A single privacy
breach can compromise not just individual records but entire datasets,
making the system unusable for future development.

Anonymization techniques form a spectrum from simple obfuscation to
formal mathematical guarantees, each trading data utility for privacy
protection. At the simple end, masking replaces sensitive values with
dummy characters, and generalization aggregates precise attributes into
broader categories (e.g., exact age to age range). Pseudonymization
replaces direct identifiers with artificial tokens, preserving record
linkage without exposing identity. More formally, k-anonymity ensures
each record is indistinguishable from at least (k-1) others under chosen
quasi-identifiers, though it remains vulnerable to homogeneity and
background knowledge attacks. At the strongest end, differential privacy
(\citeproc{ref-dwork2008differential}{Dwork, n.d.}) adds calibrated
noise controlled by the \(\epsilon\) parameter, providing mathematical
guarantees that outputs remain stable to the inclusion or exclusion of
any single individual's data. The core systems trade-off is consistent:
stronger privacy protection requires greater data distortion, which
directly affects model performance.
Table~\ref{tbl-anonymization-comparison} summarizes these trade-offs.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2000}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2000}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2000}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2000}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2000}}@{}}
\caption{\textbf{Anonymization Techniques Comparison}: Privacy-utility
trade-offs across anonymization methods. Masking preserves utility for
display but offers minimal protection; differential privacy provides
mathematical guarantees but reduces data accuracy. Practitioners must
select techniques based on their specific regulatory requirements, data
sensitivity, and analytical
needs.}\label{tbl-anonymization-comparison}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Technique}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Data Utility}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Privacy Level}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Implementation}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Best Use Case}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Technique}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Data Utility}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Privacy Level}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Implementation}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Best Use Case}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Masking} & High & Low-Medium & Simple & Displaying sensitive
data \\
\textbf{Generalization} & Medium & Medium & Moderate & Age ranges,
location bucketing \\
\textbf{Pseudonymization} & High & Medium & Moderate & Individual
tracking needed \\
\textbf{K-anonymity} & Low-Medium & High & Complex & Formal
indistinguishability (k-anonymity) \\
\textbf{Differential Privacy} & Medium & Very High & Complex &
Statistical guarantees \\
\end{longtable}

For our KWS system, governance constraints shape acquisition throughout.
Voice data inherently contains biometric information requiring privacy
protection, driving decisions about anonymization, consent requirements,
and data retention policies. Multilingual support raises equity
concerns: will the system work only for commercially valuable languages
or also serve smaller linguistic communities? Fair crowdsourcing
practices ensure that annotators providing voice samples or labeling
receive appropriate compensation and understand how their contributions
will be used.

\subsection{Integrated Acquisition
Strategy}\label{sec-data-engineering-ml-integrated-acquisition-strategy-9821}

Having examined how each pillar shapes acquisition choices, we now see
why real-world ML systems rarely use a single acquisition method in
isolation. Instead, they combine approaches strategically to balance
competing pillar requirements, recognizing that each method contributes
complementary strengths. The art of data acquisition lies in
understanding how these sources work together to create datasets that
satisfy quality, scalability, reliability, and governance constraints
simultaneously.

Our KWS system exemplifies this integrated approach. Google's Speech
Commands dataset provides a quality-assured baseline enabling rapid
prototyping and establishing performance benchmarks. However, evaluating
this against our requirements reveals gaps: limited accent diversity,
coverage of only major languages, predominantly clean recording
environments. Web scraping addresses some gaps by gathering diverse
voice samples from video platforms and speech databases, capturing
natural speech patterns across varied acoustic conditions. This scales
beyond what manual collection could provide while maintaining reasonable
quality through automated filtering.

Crowdsourcing fills targeted gaps that neither existing datasets nor web
scraping adequately address: underrepresented accents, specific
demographic groups, or particular acoustic environments identified as
weak points. By carefully designing crowdsourcing tasks with clear
guidelines and quality control, the system balances scale with quality
while ensuring ethical treatment of contributors. Synthetic data
generation completes the picture by systematically exploring the
parameter space: varying background noise levels, speaker ages,
microphone characteristics, and wake word pronunciations. This addresses
the long tail of rare conditions that are impractical to collect
naturally while enabling controlled experiments about which acoustic
variations most affect model performance.

The synthesis of these approaches demonstrates how our framework guides
strategy. Quality requirements drive use of curated datasets and expert
review. Scalability needs motivate synthetic generation and web
scraping. Reliability demands mandate diverse sourcing across
demographics and environments. Governance constraints shape consent
requirements, anonymization practices, and fair compensation policies.
Rather than selecting sources based on convenience, the integrated
strategy systematically addresses each pillar's requirements through
complementary methods.

The diversity achieved through multi-source acquisition---crowdsourced
audio with varying quality, synthetic data with perfect consistency,
web-scraped content with unpredictable formats---creates specific
challenges at the boundary where external data enters our controlled
pipeline environment.

With our strategic data acquisition framework established, we now
examine the infrastructure that receives, validates, and routes this
heterogeneous data. The pipeline architecture transforms the diverse
data from our multi-source acquisition strategy into reliable ML
training inputs.

\section{Data Pipeline
Architecture}\label{sec-data-engineering-ml-data-pipeline-architecture-b527}

Data pipelines implement our four-pillar framework, transforming raw
data into ML-ready formats while maintaining quality, reliability,
scalability, and governance standards. The heterogeneity resulting from
multi-source acquisition (audio files from crowdsourcing platforms,
synthetic waveforms from generation systems, and real-world captures
from deployed devices) requires pipelines that can normalize, validate,
and route data while enforcing consistent standards. Pipeline
architecture translates framework principles into operational reality,
where each pillar manifests as concrete engineering decisions about
validation strategies, error handling mechanisms, throughput
optimization, and observability infrastructure.

Our KWS system pipeline architecture must handle continuous audio
streams, maintain low-latency processing for real-time keyword
detection, and ensure privacy-preserving data handling. The pipeline
must scale from development environments processing sample audio files
to production deployments handling millions of concurrent audio streams
while maintaining strict quality and governance standards.

\begin{figure}[htb]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/422283e68a38835bb86bbc89d1b482c743c7d9ab.pdf}}

}

\caption{\label{fig-pipeline-flow}\textbf{Three-Stage Pipeline Flow}:
Raw data sources and APIs feed into batch and stream ingestion at the
middle layer, then flow to data warehouse and storage destinations at
the bottom. Each stage scales independently, enabling modular quality
control across the pipeline.}

\end{figure}%

Figure~\ref{fig-pipeline-flow} breaks down ML data pipelines into
several distinct layers: data sources, ingestion, processing, labeling,
storage, and ML training. Each layer plays a specific role in the data
preparation workflow. Selecting appropriate technologies requires
understanding how our four framework pillars manifest at each stage.
Quality requirements at one stage affect scalability constraints at
another, reliability needs shape governance implementations, and the
pillars interact to determine overall system effectiveness.

Data pipeline design is constrained by storage hierarchies and I/O
bandwidth limitations rather than CPU capacity. Understanding these
constraints enables building efficient systems for modern ML workloads.
Storage hierarchy trade-offs, ranging from high-latency object storage
(ideal for archival) to low-latency in-memory stores (essential for
real-time serving), and bandwidth limitations (spinning disks at 100-200
MB/s versus RAM at 50-200 GB/s) shape every pipeline decision.
Section~\ref{sec-data-engineering-ml-strategic-storage-architecture-1a6b}
covers detailed storage architecture considerations.

Design decisions should align with specific requirements. For streaming
data, consider message durability (ability to replay failed processing),
ordering properties (what ordering is provided, under what conditions),
and geographic distribution. For batch processing, key decision factors
include data volume relative to memory, processing complexity, and
whether computation must be distributed. Single-machine tools suffice
for gigabyte-scale data, but terabyte-scale processing often benefits
from distributed frameworks that partition work across clusters. The
interactions between these layers, viewed through our four-pillar lens,
determine system effectiveness and guide the specific engineering
decisions we examine in the following subsections.

\subsection{Quality Through Validation and
Monitoring}\label{sec-data-engineering-ml-quality-validation-monitoring-498f}

Quality represents the foundation of reliable ML systems. Pipelines
implement quality through systematic validation and monitoring at every
stage. Production experience shows data pipeline issues represent a
major source of ML failures. Schema changes breaking downstream
processing, distribution drift degrading model accuracy, and data
corruption silently introducing errors account for a substantial
fraction of production incidents
(\citeproc{ref-sculley2015hidden}{Sculley et al. 2021}). These failures
are insidious because they often do not cause obvious system crashes but
slowly degrade model performance in ways that become apparent only after
affecting users. Achieving quality therefore demands proactive
monitoring and validation that catches issues before they cascade into
model failures.

Production teams implement monitoring at scale through severity-based
alerting systems where different failure types trigger different
response protocols. The most critical alerts indicate complete system
failure: the pipeline has stopped processing entirely, showing zero
throughput for more than 5 minutes, or a primary data source has become
unavailable. These situations demand immediate attention because they
halt all downstream model training or serving. More subtle degradation
patterns require different detection strategies. When throughput drops
to 80\% of baseline levels, error rates climb above 5\%, or quality
metrics drift more than 2 standard deviations from training data
characteristics, the system signals degradation requiring urgent but not
immediate attention. These gradual failures often prove more dangerous
than complete outages because they can persist undetected for hours or
days, silently corrupting model inputs and degrading prediction quality.

Consider how these principles apply to a recommendation system
processing user interaction events. With a baseline throughput of 50,000
records per second, the monitoring system tracks several interdependent
signals. Instantaneous throughput alerts fire if processing drops below
40,000 records per second for more than 10 minutes, accounting for
normal traffic variation while catching genuine capacity or processing
problems. Each feature in the data stream has its own quality profile:
if a feature like user\_age shows null values in more than 5\% of
records when the training data contained less than 1\% nulls, something
has likely broken in the upstream data source. Duplicate detection runs
on sampled data, watching for the same event appearing multiple
times---a pattern that might indicate retry logic gone wrong or a
database query accidentally returning the same records repeatedly.

These monitoring dimensions become particularly important when
considering end-to-end latency. The system must track not just whether
data arrives, but how long it takes to flow through the entire pipeline
from the moment an event occurs to when the resulting features become
available for model inference. When 95th
percentile\sidenote{\textbf{95th percentile}: A statistical measure
indicating that 95\% of values fall below this threshold, commonly used
in performance monitoring to capture typical worst-case behavior while
excluding outliers. For latency monitoring, the 95th percentile provides
more stable insights than maximum values (which may be anomalies) while
revealing performance degradation that averages would hide. } latency
exceeds 30 seconds in a system with a 10-second service level agreement,
the monitoring system needs to pinpoint which pipeline stage introduced
the delay: ingestion, transformation, validation, or storage.

Quality monitoring extends beyond simple schema validation to
statistical properties that capture whether serving data resembles
training data. Rather than just checking that values fall within valid
ranges, production systems track rolling statistics over 24-hour
windows. For numerical features like transaction\_amount or
session\_duration, the system computes means and standard deviations
continuously, then applies statistical tests like the Kolmogorov-Smirnov
test\sidenote{\textbf{Kolmogorov-Smirnov test}: Named after Soviet
mathematicians Andrey Kolmogorov (1933) and Nikolai Smirnov (1939) who
independently developed this non-parametric test quantifying whether two
datasets come from the same distribution. The test measures maximum
distance between cumulative distribution functions, requiring no
assumptions about underlying distributions. In ML systems, K-S tests
detect data drift by comparing serving data against training baselines;
p-values below 0.05 indicate statistically significant distribution
shifts requiring investigation. } to compare serving distributions
against training distributions.

\phantomsection\label{callout-exampleux2a-1.15}
\begin{fbx}{callout-example}{Example:}{Detecting Drift with K-S Test}
\phantomsection\label{callout-example*-1.15}

\textbf{Scenario}: Monitoring \texttt{session\_duration} distribution
stability between training (\(P\)) and serving (\(Q\)).

\textbf{Methodology}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Compute CDFs}: Calculate Cumulative Distribution Functions for
  both datasets.
\item
  \textbf{Calculate Statistic (\(D_{KS}\))}: Find the maximum absolute
  difference between the CDFs. \[ D_{KS} = \max_x | F_P(x) - F_Q(x) | \]
\item
  \textbf{Determine Significance}: Compare \(D_{KS}\) to critical value
  \(D_{crit}\) based on sample size (\(n\)) and confidence level
  (\(\alpha=0.05\)). \[ D_{crit} \approx \frac{1.36}{\sqrt{n}} \]
\end{enumerate}

\textbf{Example Calculation}

\begin{itemize}
\tightlist
\item
  Sample size \(n=1000\).
\item
  Critical value \(D_{crit} \approx 1.36 / \sqrt{1000} \approx 0\.043\).
\item
  If observed max difference \(D_{KS} = 0.08\):
\item
  \textbf{Result}: \(0.08 > 0\.043\) \(\rightarrow\) \textbf{Reject Null
  Hypothesis}. Significant drift detected. Trigger retraining or
  investigation.
\end{itemize}

\end{fbx}

Categorical features require different statistical approaches. Instead
of comparing means and variances, monitoring systems track category
frequency distributions. When new categories appear that never existed
in training data, or when existing categories shift substantially in
relative frequency---say, the proportion of ``mobile'' versus
``desktop'' traffic changes by more than 20\%, the system flags
potential data quality issues or genuine distribution shifts. This
statistical vigilance catches subtle problems that simple schema
validation misses entirely: imagine if age values remain in the valid
range of 18-95, but the distribution shifts from primarily 25-45 year
olds to primarily 65+ year olds, indicating the data source has changed
in ways that will affect model performance.

Validation at the pipeline level encompasses multiple strategies working
together. Schema validation executes synchronously as data enters the
pipeline, rejecting malformed records immediately before they can
propagate downstream. Modern tools like TensorFlow Data Validation
(TFDV)\sidenote{\textbf{TensorFlow Data Validation (TFDV)}: A
production-grade library for analyzing and validating ML data that
automatically infers schemas, detects anomalies, and identifies
training-serving skew. TFDV computes descriptive statistics, identifies
data drift through distribution comparisons, and generates
human-readable validation reports, integrating with TFX pipelines for
automated data quality monitoring. For a feature vector containing user
demographics, the inferred schema might specify that user\_age must be a
64-bit integer between 18 and 95 and cannot be null, user\_country must
be a string from a specific set of country codes, and session\_duration
must be a floating-point number between 0 and 7200 seconds but is
optional. During serving, the validator checks each incoming record
against these specifications, rejecting records with null required
fields, out-of-range values, or type mismatches before they reach
feature computation logic. } (\citeproc{ref-polyzotis2019data}{Breck et
al. 2019}) automatically infer schemas from training data, capturing
expected data types, value ranges, and presence requirements.

This synchronous validation necessarily remains simple and fast,
checking properties that can be evaluated on individual records in
microseconds. More sophisticated validation that requires comparing
serving data against training data distributions or aggregating
statistics across many records must run asynchronously to avoid blocking
the ingestion pipeline. Statistical validation systems typically sample
1-10\% of serving traffic---enough to detect meaningful shifts while
avoiding the computational cost of analyzing every record. These samples
accumulate in rolling windows, commonly 1 hour, 24 hours, and 7 days,
with different windows revealing different patterns. Hourly windows
detect sudden shifts like a data source failing over to a backup with
different characteristics, while weekly windows reveal gradual drift in
user populations or behavior.

\hyperref[sec-data-engineering-ml-detecting-trainingserving-skew-production-2998]{}
Perhaps the most insidious validation challenge arises from
training-serving skew\sidenote{\textbf{Training-Serving Skew}: A ML
systems failure where identical features are computed differently during
training versus serving, causing silent model degradation. Occurs when
training uses batch processing with one implementation while serving
uses real-time processing with different libraries, creating subtle
differences that compound to degrade accuracy significantly without
obvious errors. }, where the same features get computed differently in
training versus serving environments. This typically happens when
training pipelines process data in batch using one set of libraries or
logic, while serving systems compute features in real-time using
different implementations. A recommendation system might compute
``user\_lifetime\_purchases'' in training by joining user profiles
against complete transaction histories, while the serving system
inadvertently uses a cached materialized
view\sidenote{\textbf{Materialized view}: A database optimization that
pre-computes and stores query results as physical tables, trading
storage space for query performance. Unlike standard views that compute
results on-demand, materialized views cache expensive join and
aggregation operations but require refresh strategies to maintain data
freshness, creating potential training-serving skew when refresh
schedules differ between environments. The resulting 15\% discrepancy
between training and serving features directly explains seemingly
mysterious 12\% accuracy drops observed in production A/B tests.
Detecting training-serving skew requires infrastructure that can
recompute training features on serving data for comparison. Production
systems implement periodic validation where they sample raw serving
data, process it through both training and serving feature pipelines,
and measure discrepancies. } updated only weekly. The data consistency
patterns we establish here provide the foundation for the comprehensive
treatment of training-serving skew in
\textbf{?@sec-machine-learning-operations-mlops}, which examines this
challenge from the operational monitoring perspective.

\subsection{Data Quality as
Code}\label{sec-data-engineering-ml-data-quality-code-1cca}

Just as unit tests protect software systems, data expectation tests
protect ML pipelines. Using libraries like Great Expectations or
Pandera, teams codify quality expectations as executable assertions
(Listing~\ref{lst-data-expectations}) that run on every pipeline
execution.

\phantomsection\label{callout-perspectiveux2a-1.16}
\begin{fbx}{callout-perspective}{Systems Perspective:}{Mechanical vs. Semantic Quality}
\phantomsection\label{callout-perspective*-1.16}
\textbf{Why Data Validation feels different from Unit Testing:}

In traditional software, quality is \textbf{Mechanical}. A null pointer
is always a bug. An integer overflow is always a crash. These are
binary, deterministic failures.

In ML systems, data quality has a second, softer dimension:
\textbf{Semantic Quality}. * \textbf{Mechanical Check}: ``Is
\texttt{age} an integer?'' (Yes/No). * \textbf{Semantic Check}: ``Is the
\texttt{age} distribution shifting?'' (Probabilistic).

A dataset can be mechanically perfect (no nulls, correct types) but
semantically broken (e.g., all users are suddenly 25 years old due to a
default value change). Robust ML systems must validate both the
\textbf{Container} (Mechanical) and the \textbf{Content} (Semantic).

\end{fbx}

\begin{codelisting}

\caption{\label{lst-data-expectations}\textbf{Data Quality Assertions}:
Executable data contracts catch schema violations, missing values, and
invalid entries before training begins. Production systems using this
pattern detect approximately 60\% of data issues at pipeline execution
time, preventing cascading failures that would otherwise propagate to
model training.}

\centering{

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ great\_expectations }\ImportTok{as}\NormalTok{ gx}

\CommentTok{\# Create a data context and load training data}
\NormalTok{context }\OperatorTok{=}\NormalTok{ gx.get\_context()}
\NormalTok{batch }\OperatorTok{=}\NormalTok{ context.get\_batch(}\StringTok{"training\_users"}\NormalTok{)}

\CommentTok{\# Define an expectation suite as executable quality contract}
\NormalTok{suite }\OperatorTok{=}\NormalTok{ context.create\_expectation\_suite(}\StringTok{"user\_data\_quality"}\NormalTok{)}

\CommentTok{\# Range validation: prevents physiologically impossible values}
\NormalTok{batch.expect\_column\_values\_to\_be\_between(}
\NormalTok{    column}\OperatorTok{=}\StringTok{"age"}\NormalTok{, min\_value}\OperatorTok{=}\DecValTok{0}\NormalTok{, max\_value}\OperatorTok{=}\DecValTok{120}
\NormalTok{)}

\CommentTok{\# Null detection: ensures primary key integrity for joins}
\NormalTok{batch.expect\_column\_values\_to\_not\_be\_null(column}\OperatorTok{=}\StringTok{"user\_id"}\NormalTok{)}

\CommentTok{\# Uniqueness: prevents duplicate training examples}
\NormalTok{batch.expect\_column\_values\_to\_be\_unique(column}\OperatorTok{=}\StringTok{"user\_id"}\NormalTok{)}

\CommentTok{\# Categorical validation: detects unexpected values}
\CommentTok{\# from upstream changes}
\NormalTok{batch.expect\_column\_distinct\_values\_to\_be\_in\_set(}
\NormalTok{    column}\OperatorTok{=}\StringTok{"country\_code"}\NormalTok{, value\_set}\OperatorTok{=}\NormalTok{[}\StringTok{"US"}\NormalTok{, }\StringTok{"CA"}\NormalTok{, }\StringTok{"UK"}\NormalTok{, }\StringTok{"DE"}\NormalTok{, }\StringTok{"FR"}\NormalTok{]}
\NormalTok{)}

\CommentTok{\# Run validation and fail pipeline if expectations not met}
\NormalTok{results }\OperatorTok{=}\NormalTok{ context.run\_validation\_operator(}
    \StringTok{"action\_list\_operator"}\NormalTok{, assets\_to\_validate}\OperatorTok{=}\NormalTok{[batch]}
\NormalTok{)}
\ControlFlowTok{if} \KeywordTok{not}\NormalTok{ results[}\StringTok{"success"}\NormalTok{]:}
    \ControlFlowTok{raise} \PreprocessorTok{ValueError}\NormalTok{(}\SpecialStringTok{f"Data quality check failed: }\SpecialCharTok{\{}\NormalTok{results}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

}

\end{codelisting}%

CI/CD integration runs expectations in the deployment pipeline.
Expectation violations fail deployments before bad data reaches
training. A pipeline structured as data ingestion followed by data
validation followed by training blocks deployment when validation
detects anomalies like age values of 150, triggering alerts for
investigation.

Expectation suites as artifacts version alongside training code. When
training code changes, expectation updates help keep data contracts
evolving together. This coupling reduces the risk of silent divergence
where code assumes data properties that the upstream pipeline no longer
provides.

This pattern catches approximately 60\% of production data issues before
they reach training, based on industry experience with tools like Great
Expectations, Pandera, and Pydantic. The remaining issues require the
runtime monitoring discussed in the previous sections, as some quality
problems only emerge in the full production data stream.

\subsection{Detecting and Responding to Data
Drift}\label{sec-data-engineering-ml-detecting-responding-data-drift-509a}

ML models fundamentally assume that production data resembles training
data. When this assumption breaks, model performance degrades silently
without obvious errors or system failures. Data drift detection provides
early warning before accuracy drops become severe, enabling proactive
response rather than reactive recovery after users experience degraded
service. Unlike the validation and monitoring techniques we have
examined that catch immediate data quality issues, drift detection
identifies gradual statistical changes in data distributions that
compound over time to undermine model effectiveness.

The \textbf{Degradation Equation} from \textbf{?@sec-introduction}
formalizes this relationship:

\[
\text{Accuracy}(t) \approx \text{Accuracy}_0 - \lambda \cdot D(P_t \| P_0)
\]

The distributional divergence term \(D(P_t \| P_0)\) directly measures
drift---as serving data \(P_t\) diverges from training data \(P_0\),
accuracy erodes proportionally. The detection metrics examined below
(PSI, KL divergence, statistical tests) operationalize this divergence
measurement, transforming the theoretical equation into actionable
monitoring infrastructure. When PSI exceeds 0.2 or KL divergence crosses
threshold, the system signals that \(D\) has grown large enough to
materially impact \(\text{Accuracy}(t)\).

This subsection provides comprehensive coverage of drift detection
because production experience reveals that drift detection and response
consume 30--40\% of ongoing ML operations effort, making this a core
data engineering responsibility rather than an optional advanced topic.
We examine drift types, quantitative detection metrics, monitoring
infrastructure, and response strategies here;
\textbf{?@sec-machine-learning-operations-mlops} builds on this
foundation to address operational response orchestration and automated
retraining pipelines at scale. The insidious nature of drift-induced
failures motivates systematic detection: model accuracy degrades
gradually over weeks or months rather than failing catastrophically,
making problems difficult to attribute to specific causes without
quantitative distribution monitoring.

\subsubsection*{Types of Distribution
Shift}\label{types-of-distribution-shift}
\addcontentsline{toc}{subsubsection}{Types of Distribution Shift}

Understanding the three core types of drift enables targeted detection
and response strategies. Each type manifests differently in production
systems and requires distinct monitoring approaches.

Covariate shift\sidenote{\textbf{Covariate Shift}: A type of data drift
where the distribution of input features \(P(X)\) changes between
training and serving while the conditional relationship \(P(Y|X)\)
remains constant. Common in production ML when data collection contexts
change (new sensors, different user populations, seasonal effects) but
core relationships persist. Detectable through feature distribution
monitoring without requiring ground truth labels. } occurs when input
feature distributions change while the relationship between features and
labels remains constant: \(P(X)\) changes but \(P(Y|X)\) stays the same.
A medical imaging system trained on one camera model might see
production data from a different camera manufacturer. The disease-image
relationship remains unchanged (same pathologies produce same visual
indicators), but pixel value distributions shift due to different sensor
characteristics, color calibration, or image processing pipelines.
Detection focuses on monitoring feature distributions using statistical
metrics like PSI or KL divergence applied to input features.

Label shift occurs when the output label distribution changes while the
relationship between labels and features remains constant: \(P(Y)\)
changes but \(P(X|Y)\) stays the same. Disease prevalence might change
seasonally (flu cases spike in winter) while symptoms remain consistent
predictors of each disease. A recommendation system might see label
shift when new product categories launch, changing the distribution of
user preferences without altering what makes products appealing within
each category. Detection monitors prediction distributions for shifts in
relative frequencies of predicted classes, which can be done without
ground truth labels by tracking model output distributions.

Concept drift\sidenote{\textbf{Concept Drift}: A type of data drift
(\citeproc{ref-gama2014survey}{Gama et al. 2014}) where the relationship
between features and labels \(P(Y|X)\) changes over time, substantially
altering what the model must learn. Most challenging drift type to
detect and respond to, requiring ground truth labels and potentially
model retraining with recent data emphasizing new patterns. May indicate
need for architectural changes if relationships have substantially
altered. } represents the most challenging case: the relationship
between features and labels changes, meaning \(P(Y|X)\) evolves over
time (\citeproc{ref-gama2014survey}{Gama et al. 2014}). Medical
treatment protocols change, altering disease outcomes for given
symptoms. User preferences shift as social trends evolve, changing what
product features drive purchases. Fraud patterns evolve as attackers
adapt to detection systems. Concept drift requires ground truth labels
for detection since we must monitor whether the feature-to-label
relationship has changed, making it inherently more difficult and
delayed than detecting covariate or label shift.

Label quality drift\sidenote{\textbf{Label Quality Drift}: Degradation
in annotation reliability over time, distinct from changes in label
distributions. Sources include annotator fatigue or turnover, evolving
labeling guidelines, changing domain knowledge, and adversarial label
noise. Detection requires monitoring inter-annotator agreement,
comparing automated labels against periodic human audits, and tracking
proxy metrics indicating labeling consistency. } represents a meta-level
shift distinct from the three distribution shifts above: the reliability
of ground truth labels degrades over time even when the underlying data
distributions remain stable. This drift type proves particularly
insidious because standard feature distribution monitoring fails to
detect it. Crowdsourced labels may degrade as annotator pools change,
training materials become outdated, or labeling guidelines evolve
without corresponding model updates. Automated labeling systems
accumulate errors as the models powering them drift from their original
operating conditions. A recommendation system using click feedback as
implicit labels may see label quality degrade as user behavior becomes
more exploratory, as bot traffic patterns change, or as interface
modifications alter how users interact with content.

Detection requires monitoring annotation consistency rather than feature
distributions. Inter-annotator agreement metrics like Cohen's
kappa\sidenote{\textbf{Cohen's kappa}: Named after psychologist Jacob
Cohen who introduced it in 1960, this statistic measures inter-rater
agreement for categorical items while accounting for agreement occurring
by chance. Cohen developed the metric while studying psychological
assessment reliability. Values range from -1 to 1: above 0.8 indicates
almost perfect agreement, 0.6-0.8 substantial, 0.4-0.6 moderate, and
below 0.4 suggests guidelines need clarification or genuine ambiguity
exists. } (\(\kappa\)) provide quantitative assessment:

\[
\kappa = \frac{p_o - p_e}{1 - p_e}
\]

where \(p_o\) represents observed agreement between annotators and
\(p_e\) represents agreement expected by chance. Monitoring \(\kappa\)
over time windows reveals degradation trends. A medical imaging
annotation project might establish a baseline \(\kappa = 0.85\)
(substantial agreement) during initial data collection, then observe
decline to \(\kappa = 0.72\) (moderate agreement) after six months as
new annotators join without receiving equivalent domain training.

For systems with calibrated model probabilities, label confidence
entropy provides an alternative detection signal:

\[
H_{\text{label}} = -\sum_i p_i \log p_i
\]

Rising entropy in model confidence distributions suggests increasing
ambiguity or mislabeling in training data, as the model learns from
inconsistent supervision.

Mitigation strategies depend on root cause analysis. Annotator
retraining addresses systematic errors from unclear guidelines at low
cost with high effectiveness. Multi-annotator voting with majority or
consensus rules provides very high accuracy for high-stakes domains but
significantly increases annotation costs. Model-assisted labeling
reduces annotator fatigue but risks introducing bias if the assisting
model has its own systematic errors. Expert review sampling, where
domain specialists audit a random sample of annotations, enables root
cause analysis when quality decline is detected but provides medium
coverage of the overall annotation stream.

\subsubsection*{Operationalizing Drift
Detection}\label{operationalizing-drift-detection}
\addcontentsline{toc}{subsubsection}{Operationalizing Drift Detection}

Detecting these shifts requires quantitative metrics that compare
serving distributions against training baselines. Common approaches
include the \textbf{Population Stability Index (PSI)} for categorical
features and \textbf{Kullback-Leibler (KL) Divergence} for continuous
distributions. These metrics provide interpretable scores (such as a PSI
above 0.2 indicating significant drift) that trigger automated alerts or
retraining workflows.

While data engineering is responsible for \emph{preventing} drift
through robust pipeline design and \emph{defining} the quality
constraints, the \emph{operational infrastructure} for monitoring,
alerting, and automated retraining is a core concern of MLOps.
\textbf{?@sec-machine-learning-operations-mlops} provides comprehensive
coverage of these operational practices, including tiered alerting
strategies, cold start monitoring, and automated response orchestration.

Drift detection represents one dimension of quality monitoring, focused
on identifying statistical changes in data distributions over time.
However, detecting issues is only half the challenge; the other half is
ensuring systems continue operating effectively even when problems are
detected. This leads us from quality monitoring to the reliability
pillar, which addresses how pipelines maintain service continuity under
adverse conditions.

\subsection{Reliability Through Graceful
Degradation}\label{sec-data-engineering-ml-reliability-graceful-degradation-f83d}

While quality monitoring detects issues, reliability ensures systems
continue operating effectively when problems occur. Pipelines face
constant challenges: data sources become temporarily unavailable,
network partitions separate components, upstream schema changes break
parsing logic, or unexpected load spikes exhaust resources. Robust
systems handle these failures gracefully through systematic failure
analysis, intelligent error handling, and automated recovery strategies
that maintain service continuity even under adverse conditions.

Systematic failure mode analysis for ML data pipelines reveals
predictable patterns that require specific engineering countermeasures.
Data corruption failures occur when upstream systems introduce subtle
format changes, encoding issues, or field value modifications that pass
basic validation but corrupt model inputs. A date field switching from
``YYYY-MM-DD'' to ``MM/DD/YYYY'' format might not trigger schema
validation but will break any date-based feature computation. Schema
evolution\sidenote{\textbf{Schema Evolution}: The challenge of managing
changes to data structure over time as source systems add fields, rename
columns, or modify data types. Critical for ML systems because model
training expects consistent feature schemas, and schema changes can
silently break feature computation or introduce training-serving skew. }
failures happen when source systems add fields, rename columns, or
change data types without coordination, breaking downstream processing
assumptions that expected specific field names or types. Resource
exhaustion manifests as gradually degrading performance when data volume
growth outpaces capacity planning, eventually causing pipeline failures
during peak load periods.

Building on this failure analysis, effective error handling strategies
ensure problems are contained and recovered from systematically.
Implementing intelligent retry logic for transient errors, such as
network interruptions or temporary service outages, requires exponential
backoff strategies to avoid overwhelming recovering services. A simple
linear retry that attempts reconnection every second would flood a
struggling service with connection attempts, potentially preventing its
recovery. Exponential backoff---retrying after 1 second, then 2 seconds,
then 4 seconds, doubling with each attempt---gives services breathing
room to recover while still maintaining persistence. Many ML systems
employ the concept of dead letter queues\sidenote{\textbf{Dead Letter
Queue}: Borrowed from postal terminology where ``dead letters'' are
undeliverable mail held at a Dead Letter Office for investigation (the
first US Dead Letter Office opened in 1825). In computing, DLQs store
messages that fail processing after exhausting retry attempts, enabling
later analysis without blocking the main pipeline. For ML systems, DLQs
are essential where data loss is unacceptable: malformed training
examples can be fixed and reprocessed. }, using separate storage for
data that fails processing after multiple retry attempts. This allows
for later analysis and potential reprocessing of problematic data
without blocking the main pipeline
(\citeproc{ref-kleppmann2017designing}{Kleppmann 2016}). A pipeline
processing financial transactions that encounters malformed data can
route it to a dead letter queue rather than losing critical records or
halting all processing.

In ML systems, dead letter queues serve dual purposes beyond failure
analysis. Production teams implement systematic review of DLQ contents
to identify: (1) schema violations indicating upstream changes, (2) edge
case patterns the model should handle, and (3) data quality issues
requiring source system fixes. For example, a fraud detection system's
DLQ revealed transactions from a new payment type the model had never
seen, prompting targeted data collection and retraining rather than
simply logging the failures. This transforms DLQs from passive error
storage into active sources for identifying model blind spots and
driving improvement.

Moving beyond ad-hoc error handling, cascade failure prevention requires
circuit breaker\sidenote{\textbf{Circuit Breaker}: Named after the
electrical safety device invented by Thomas Edison (1879) that
interrupts current flow when overload is detected. Michael Nygard
popularized the software pattern in ``Release It!'' (2007), applying the
same principle: automatically stop calling a failing service to prevent
cascade failures. The pattern has three states mirroring electrical
behavior: closed (normal flow), open (circuit broken, service blocked),
and half-open (testing if service recovered). } patterns and bulkhead
isolation to prevent single component failures from propagating
throughout the system. When a feature computation service fails, the
circuit breaker pattern stops calling that service after detecting
repeated failures, preventing the caller from waiting on timeouts that
would cascade into its own failure.

Automated recovery engineering implements sophisticated strategies
beyond simple retry logic. Progressive timeout increases prevent
overwhelming struggling services while maintaining rapid recovery for
transient issues---initial requests timeout after 1 second, but after
detecting service degradation, timeouts extend to 5 seconds, then 30
seconds, giving the service time to stabilize. Multi-tier fallback
systems provide degraded service when primary data sources fail: serving
slightly stale cached features when real-time computation fails, or
using approximate features when exact computation times out. A
recommendation system unable to compute user preferences from the past
30 days might fall back to preferences from the past 90 days, providing
somewhat less accurate but still useful recommendations rather than
failing entirely. Comprehensive alerting and escalation procedures
ensure human intervention occurs when automated recovery fails, with
sufficient diagnostic information captured during the failure to enable
rapid debugging.

These concepts become concrete when considering a financial ML system
ingesting market data. Error handling might involve falling back to
slightly delayed data sources if real-time feeds fail, while
simultaneously alerting the operations team to the issue. Dead letter
queues capture malformed price updates for investigation rather than
dropping them silently. Circuit breakers prevent the system from
overwhelming a struggling market data provider during recovery. This
comprehensive approach to error management ensures that downstream
processes have access to reliable, high-quality data for training and
inference tasks, even in the face of the inevitable failures that occur
in distributed systems at scale.

\subsection{Scalability
Patterns}\label{sec-data-engineering-ml-scalability-patterns-b9b1}

While quality and reliability ensure correct system operation,
scalability addresses a different challenge: how systems evolve as data
volumes grow and ML systems mature from prototypes to production
services. Pipelines that work effectively at gigabyte scale often break
at terabyte scale without architectural changes that enable distributed
processing. Scalability involves designing systems that handle growing
data volumes, user bases, and computational demands without requiring
complete redesigns.

At the ingestion layer, systems choose between batch processing
(collecting data in groups before processing) and stream processing
(processing data in real-time as it arrives). Batch processing enables
efficient resource utilization by amortizing costs across large volumes;
stream processing enables real-time responsiveness but at significantly
higher cost---often 10× or more per byte processed. Most production ML
systems employ hybrid approaches. The detailed trade-offs between these
patterns are examined in
Section~\ref{sec-data-engineering-ml-data-ingestion-8efc}.

Beyond ingestion patterns, distributed processing becomes necessary when
single machines cannot handle data volumes or processing complexity. The
fundamental constraint is that distributed coordination is limited by
network round-trip times: local operations complete in microseconds
while network coordination requires milliseconds, creating a 1000×
latency difference. This \emph{physics of data locality} shapes every
scalability decision.

\phantomsection\label{callout-notebookux2a-1.17}
\begin{fbx}{callout-notebook}{AI Engineer’s Notebook:}{The Physics of Data Locality}
\phantomsection\label{callout-notebook*-1.17}
\textbf{Why moving data kills performance}: The ``time'' cost of data
movement is intuitive, but the \textbf{energy} cost is the hard physical
constraint. Engineering decisions about storage tiers (RAM vs SSD vs S3)
are ultimately governed by this hierarchy:

\begin{longtable}[]{@{}lrr@{}}
\toprule\noalign{}
\textbf{Operation} & \textbf{Energy (Approx.)} & \textbf{Relative
Cost} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{32-bit Integer Add} & 0.1 pJ & 1x \\
\textbf{32-bit Float Add} & 0.9 pJ & 9x \\
\textbf{32-bit DRAM Read} & 640 pJ & \textbf{6,400x} \\
\textbf{Network Transfer (1KB)} & \textasciitilde1,000,000+ pJ &
\textbf{10,000,000x} \\
\end{longtable}

\textbf{The Engineering Implication}: Fetching a single weight from
off-chip memory (DRAM) costs 3 orders of magnitude more energy than
performing the math on it. Fetching it over the network costs 7 orders
of magnitude more. This physics dictates the design of \textbf{Feature
Stores} (caching data close to compute) and \textbf{Data Lakes}
(batching data to amortize transfer costs).

\end{fbx}

These energy and latency constraints drive ML system design toward
compute-follows-data architectures where processing moves to data rather
than data moving to processing.
Section~\ref{sec-data-engineering-ml-scaling-distributed-processing-cb9b}
examines the specific techniques for implementing distributed
processing---including Amdahl's Law limits, framework selection (Spark,
Beam, tf.data), and single-machine optimization strategies---while
Section~\ref{sec-data-engineering-ml-strategic-storage-architecture-1a6b}
details how storage architectures align with these constraints.

\subsection{Governance Through
Observability}\label{sec-data-engineering-ml-governance-observability-2c05}

Having addressed functional requirements through quality, reliability,
and scalability, we turn to governance. In pipelines, governance
manifests as comprehensive observability: the ability to understand what
data flows through the system, how it transforms, and who accesses it.
Unlike the functional pillars, governance ensures operations occur
within legal, ethical, and business constraints while maintaining
transparency and accountability.

Pipeline-level governance requires three interconnected capabilities.
\textbf{Data lineage tracking} captures the complete provenance of every
dataset: which raw sources contributed, what transformations were
applied, when processing occurred, and what code version executed. When
a model prediction proves incorrect, engineers trace back through the
pipeline to identify which training data contributed and whether they
can recreate the exact scenario for investigation. \textbf{Audit trails}
record who accessed data and when, demonstrating compliance with
regulatory frameworks like GDPR that require organizations to prove
appropriate data handling. \textbf{Access controls} enforce policies
about who can read, write, or transform data at each pipeline stage,
with ML systems often implementing attribute-based policies where data
sensitivity, user roles, and access context determine permissions.

For our KWS system, pipeline governance means tracking which version of
forced alignment generated labels, what audio normalization parameters
were applied, and which crowdsourcing batches contributed to training
data. This metadata enables reproducing training exactly when debugging
model failures and validating that serving uses identical preprocessing
to training.

The integration of these governance mechanisms transforms pipelines from
opaque data transformers into auditable, reproducible systems.
Section~\ref{sec-data-engineering-ml-tracking-data-transformation-lineage-3b09}
examines transformation-specific lineage tracking, while
\textbf{?@sec-responsible-engineering-data-governance-compliance}
provides comprehensive coverage of privacy protection, regulatory
compliance, and the full governance infrastructure required for
production ML systems.

With comprehensive pipeline architecture established---quality through
validation and monitoring, reliability through graceful degradation,
scalability through appropriate patterns, and governance through
observability---we have the infrastructure to handle the heterogeneous
data from our acquisition strategy. The diversity achieved through
multi-source acquisition (crowdsourced audio with varying quality,
synthetic data with perfect consistency, web-scraped content with
unpredictable formats) creates specific challenges at the boundary where
external data enters the controlled pipeline environment.

\section{Data
Ingestion}\label{sec-data-engineering-ml-data-ingestion-8efc}

Ingestion is the boundary where externally acquired data enters the
controlled pipeline environment. The choice of ingestion pattern
determines how quickly new data reaches the model, how much
infrastructure the system requires, and how errors are detected and
recovered.

A critical, often overlooked constraint in ingestion design is the
\textbf{Input/Output (IO) Bottleneck}. Training speed is governed by a
simple inequality: \(T_{step} = \max(T_{compute}, T_{io})\). If your
data pipeline cannot decode images fast enough to keep the GPU busy,
your expensive accelerator sits idle. This phenomenon creates a ``Choke
Point'' where adding more GPUs yields zero speedup.

The following plot (Figure~\ref{fig-dataloader-choke-point}) visualizes
this \textbf{Dataloader Choke Point}, showing the ``Starvation Region''
where the CPU limits performance.

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{contents/vol1/data_engineering/data_engineering_files/figure-pdf/fig-dataloader-choke-point-output-1.pdf}}

}

\caption{\label{fig-dataloader-choke-point}\textbf{The Dataloader Choke
Point.} Training Throughput (img/s) vs.~Number of DataLoader Workers.
The blue curve shows CPU throughput scaling linearly with workers until
hitting disk limits. The red dashed line is the GPU's consumption
capacity (e.g., ResNet-50 consuming 3,000 img/s). The system is
bottlenecked by whichever is lower. In the `Starvation Region' (left),
the GPU is idle waiting for data. In the `Saturated Region' (right), the
GPU is fully utilized, and adding more workers wastes CPU memory.}

\end{figure}%

This section examines the two fundamental ingestion patterns, batch and
streaming, along with the ETL and ELT processing paradigms that govern
how transformations are applied during ingestion. Together, these
choices shape the cost, latency, and reliability profile of every
downstream pipeline stage.

\subsection{Batch vs.~Streaming Ingestion
Patterns}\label{sec-data-engineering-ml-batch-vs-streaming-ingestion-patterns-e1b2}

ML systems follow two primary ingestion patterns that reflect different
approaches to data flow timing and processing. Each pattern has distinct
characteristics and use cases that shape how systems balance latency,
throughput, cost, and complexity. Understanding when to apply batch
versus streaming ingestion, or combinations of both, requires analyzing
workload characteristics against framework requirements.

Batch ingestion involves collecting data in groups or batches over a
specified period before processing. This method proves appropriate when
real-time data processing is not critical and data can be processed at
scheduled intervals. The batch approach enables efficient use of
computational resources by amortizing startup costs across large data
volumes and processing when resources are available or least expensive.
For example, a retail company might use batch ingestion to process daily
sales data overnight, updating their ML models for inventory prediction
each morning (\citeproc{ref-akidau2015dataflow}{Akidau et al. 2015}).
The batch job might process gigabytes of transaction data using dozens
of machines for 30 minutes, then release those resources for other
workloads. This scheduled processing proves far more cost-effective than
maintaining always-on infrastructure, particularly when slight staleness
in predictions does not affect business outcomes.

Batch processing also simplifies error handling and recovery. When a
batch job fails midway, the system can retry the entire batch or resume
from checkpoints without complex state management. Data scientists can
easily inspect failed batches, understand what went wrong, and reprocess
after fixes. The deterministic nature of batch processing (processing
the same input data always produces the same output) simplifies
debugging and validation. These characteristics make batch ingestion
attractive for ML workflows even when real-time processing is
technically feasible but not required.

In contrast to this scheduled approach, stream ingestion processes data
in real-time as it arrives, consuming events continuously rather than
waiting to accumulate batches. This pattern is essential for
applications requiring immediate data processing, scenarios where data
loses value quickly, and systems that need to respond to events as they
occur. A financial institution might use stream ingestion for real-time
fraud detection, processing each transaction as it occurs to flag
suspicious activity immediately before completing the transaction. The
value of fraud detection drops dramatically if detection occurs hours
after the fraudulent transaction completes---by then money has been
transferred and accounts compromised.

However, stream processing introduces complexity that batch processing
avoids. The system must handle backpressure when downstream systems
cannot keep pace with incoming data rates. During traffic spikes, when a
sudden surge produces data faster than processing capacity, the system
must either buffer data (requiring memory and introducing latency),
sample (losing some data), or push back to producers (potentially
causing their failures). Data freshness Service Level Agreements (SLAs)
formalize these requirements, specifying maximum acceptable delays
between data generation and availability for processing. Meeting a
100-millisecond freshness SLA requires different infrastructure than
meeting a 1-hour SLA, affecting everything from networking to storage to
processing architectures.

Recognizing the limitations of either approach alone, many production ML
systems employ hybrid approaches that combine batch and stream ingestion
to handle different data velocities and use cases. A recommendation
system might use streaming ingestion for real-time user
interactions---clicks, views, purchases---to update session-based
recommendations immediately, while using batch ingestion for overnight
processing of user profiles, item features, and collaborative filtering
models that do not require real-time updates.

Production systems must balance cost versus latency trade-offs when
selecting patterns: real-time processing is often materially more
expensive than batch processing, commonly by an order of magnitude or
more in total cost per byte processed. This cost differential arises
from several factors: streaming systems require always-on infrastructure
rather than schedulable resources that can spin up and down based on
workload; maintain redundant processing for fault tolerance to ensure no
events are lost; need low-latency networking and storage to meet
millisecond-scale SLAs; and cannot benefit from economies of scale that
batch processing achieves by amortizing startup costs across large data
volumes. A batch job processing one terabyte might use 100 machines for
10 minutes, while a streaming system processing the same data over 24
hours needs dedicated resources continuously available. This difference
in resource commitment per byte processed drives many architectural
decisions about which data truly requires real-time processing versus
what can tolerate batch delays. Quantifying \emph{the cost of real-time}
makes this trade-off precise.

\phantomsection\label{callout-notebookux2a-1.18}
\begin{fbx}{callout-notebook}{AI Engineer’s Notebook:}{The Cost of Real-Time}
\phantomsection\label{callout-notebook*-1.18}
\textbf{Problem}: Ingest 1 million events/second. Compare Batch (hourly)
vs.~Stream (sub-second) costs.

\textbf{The Physics}:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Throughput}: 1M events/sec \(\times\) 1KB/event = \textbf{1
  GB/s}.
\item
  \textbf{Stream Requirements}: To sustain 1 GB/s with \textless100ms
  latency, you need \textasciitilde50 dedicated cores + redundant
  backups (always on). Cost: 100 cores \(\times\) 24 hrs \(\times\)
  \$0.05/hr = \textbf{\$120/day}.
\item
  \textbf{Batch Requirements}: Process 3.6TB (1 hour data) in 10 mins.
  High throughput (sequential I/O) is efficient. You need 200 cores for
  10 mins/hour = 33 core-hours/day. Cost: 33 \(\times\) \$0.05 =
  \textbf{\$1.65/day}.
\end{enumerate}

\textbf{The Engineering Conclusion}: Real-time is
\textbf{\textasciitilde73× more expensive} for the same data volume.
Only pay this tax if the value of \textless1s latency justifies it.

\end{fbx}

\subsection{ETL and ELT
Comparison}\label{sec-data-engineering-ml-etl-elt-comparison-2e2b}

Beyond choosing ingestion patterns based on timing requirements,
designing effective data ingestion pipelines requires understanding the
differences between Extract, Transform, Load
(ETL)\sidenote{\textbf{Extract, Transform, Load (ETL)}: A data
processing pattern where raw data is extracted from sources, transformed
(cleaned, aggregated, validated) in a separate processing layer, then
loaded into storage. For example, a traditional ETL pipeline might
extract customer purchase logs, transform them by removing duplicates
and aggregating daily totals in Apache Spark, then load only the
aggregated results into a data warehouse. Ensures only high-quality data
reaches storage but requires reprocessing all data when transformation
logic changes. } and Extract, Load, Transform
(ELT)\sidenote{\textbf{Extract, Load, Transform (ELT)}: A data
processing pattern where raw data is extracted and immediately loaded
into scalable storage, then transformed using the storage system's
computational resources. For example, an ELT pipeline might extract raw
clickstream events directly into a data lake like S3, then use SQL
queries in a system like Snowflake or BigQuery to create multiple
transformed views: user sessions for analytics, feature vectors for ML
models, and aggregated metrics for dashboards. Enables faster iteration
and multiple transformation variants but requires more storage capacity
and careful governance of raw data. } approaches.
Figure~\ref{fig-etl-vs-elt} contrasts these two paradigms, showing how
ETL transforms data before loading while ELT loads raw data first and
transforms within the target system. These paradigms determine when data
transformations occur relative to the loading phase, significantly
impacting the flexibility and efficiency of ML pipelines. The choice
between ETL and ELT affects where computational resources are consumed,
how quickly data becomes available for analysis, and how easily
transformation logic can evolve as requirements change.

\begin{figure}[htb]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/f0cdf6da4bf15131110f4f06c6425a7142edbdc4.pdf}}

}

\caption{\label{fig-etl-vs-elt}\textbf{ETL vs.~ELT Comparison}:
Side-by-side view of two pipeline paradigms. ETL transforms data before
loading into a data warehouse, while ELT loads raw data first and
transforms within the warehouse. The choice depends on data volume,
transformation complexity, and target storage capabilities.}

\end{figure}%

The ETL pattern transforms data before loading it into the target
system. For ML pipelines, this means only validated, schema-conformant
data enters the warehouse, enforcing quality and privacy compliance at
ingestion time. For instance, an ML system predicting customer churn
might use ETL to standardize customer interaction data from multiple
sources, converting timestamp formats to UTC, normalizing text
encodings, and computing aggregate features like ``total purchases last
30 days'' before loading (\citeproc{ref-inmon2005building}{Inmon 2005}).
The disadvantage is inflexibility: when feature definitions change, all
source data must be reprocessed through the pipeline, a process that can
take hours or days for large datasets and slows iteration velocity
during development.

The ELT pattern reverses this order, loading raw data first and applying
transformations within the target system. For ML development, this
enables flexible feature experimentation on the same raw data. Multiple
teams can compute different aggregation windows, and when transformation
logic bugs are discovered, teams reprocess by rerunning queries rather
than re-ingesting from sources. This flexibility accelerates ML
experimentation where feature engineering requirements evolve rapidly.
The cost is higher storage requirements (raw data is larger than
transformed data), repeated computation when multiple models transform
the same source data, and greater complexity in enforcing privacy
compliance when raw sensitive data persists in storage.

Production ML systems rarely use one pattern exclusively. Structured
data with stable schemas often flows through ETL for efficiency and
compliance, while unstructured data or rapidly evolving feature
pipelines benefit from ELT's flexibility. Choosing between these
patterns requires understanding the \emph{cost of transformation
placement}.

Deciding where to run these transformations involves calculating
\emph{the cost of transformation placement}, as shown in
Listing~\ref{lst-etl-elt-cost-comparison}.

\begin{codelisting}

\caption{\label{lst-etl-elt-cost-comparison}\textbf{ETL vs ELT Cost
Comparison}: Calculating storage and compute costs for different
transformation placement strategies. ETL processes data before loading
(reducing storage but requiring reprocessing on schema changes), while
ELT loads raw data first (higher storage costs but flexible reprocessing
via SQL).}

\centering{

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# ETL vs ELT cost comparison}
\NormalTok{daily\_raw\_tb }\OperatorTok{=} \DecValTok{10}
\NormalTok{s3\_per\_tb\_mo }\OperatorTok{=} \DecValTok{23}
\NormalTok{spark\_per\_tb }\OperatorTok{=} \DecValTok{5}
\NormalTok{n\_models }\OperatorTok{=} \DecValTok{3}
\NormalTok{retention\_days }\OperatorTok{=} \DecValTok{30}
\NormalTok{query\_cost\_per }\OperatorTok{=} \DecValTok{5}

\CommentTok{\# ETL}
\NormalTok{etl\_spark\_daily }\OperatorTok{=}\NormalTok{ daily\_raw\_tb }\OperatorTok{*}\NormalTok{ spark\_per\_tb}
\NormalTok{etl\_datasets }\OperatorTok{=} \DecValTok{3}
\NormalTok{etl\_tb\_each }\OperatorTok{=} \DecValTok{2}
\NormalTok{etl\_storage\_tb }\OperatorTok{=}\NormalTok{ etl\_datasets }\OperatorTok{*}\NormalTok{ etl\_tb\_each}
\NormalTok{etl\_storage\_mo }\OperatorTok{=}\NormalTok{ etl\_storage\_tb }\OperatorTok{*}\NormalTok{ s3\_per\_tb\_mo}

\CommentTok{\# ELT}
\NormalTok{elt\_storage\_tb }\OperatorTok{=}\NormalTok{ daily\_raw\_tb }\OperatorTok{*}\NormalTok{ retention\_days}
\NormalTok{elt\_storage\_mo }\OperatorTok{=}\NormalTok{ elt\_storage\_tb }\OperatorTok{*}\NormalTok{ s3\_per\_tb\_mo}
\NormalTok{elt\_query\_mo }\OperatorTok{=}\NormalTok{ n\_models }\OperatorTok{*}\NormalTok{ query\_cost\_per }\OperatorTok{*}\NormalTok{ retention\_days}

\CommentTok{\# Savings}
\NormalTok{storage\_savings\_mo }\OperatorTok{=}\NormalTok{ elt\_storage\_mo }\OperatorTok{{-}}\NormalTok{ etl\_storage\_mo}
\end{Highlighting}
\end{Shaded}

}

\end{codelisting}%

\phantomsection\label{callout-notebookux2a-1.19}
\begin{fbx}{callout-notebook}{AI Engineer’s Notebook:}{The Cost of Transformation Placement}
\phantomsection\label{callout-notebook*-1.19}
\textbf{Problem}: Your team processes 10 TB of raw clickstream data
daily. You need to compute user session features for three ML models,
each requiring different aggregation windows (1-hour, 24-hour, 7-day).
Compare ETL vs.~ELT costs.

\textbf{The Math}:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{ETL Approach}: Transform before loading. Compute all three
  aggregation windows in a Spark cluster before loading into the
  warehouse.

  \begin{itemize}
  \tightlist
  \item
    Spark compute: 10 TB at \$5/TB = \$50/day
  \item
    Storage: 3 transformed datasets, \textasciitilde2 TB each = 6 TB at
    \$23/TB per month = \$138/month
  \item
    Schema change cost: Re-run full pipeline (\textasciitilde4 hours)
    per change
  \end{itemize}
\item
  \textbf{ELT Approach}: Load raw data first, transform in warehouse.

  \begin{itemize}
  \tightlist
  \item
    Storage: 10 TB raw/day, 30-day retention = 300 TB at \$23/TB per
    month = \$6,900/month
  \item
    Query compute: 3 models × \$5/query × 30 days = \$450/month
  \item
    Schema change cost: Rewrite SQL query (\textasciitilde30 minutes)
    per change
  \end{itemize}
\end{enumerate}

\textbf{The Engineering Conclusion}: ETL saves \$6,762/month in storage
but costs 8× more engineering time per schema change. If your feature
definitions change weekly, ELT's flexibility pays for itself. If schemas
are stable, ETL's lower storage cost dominates. The break-even point: if
you change schemas fewer than once per month, ETL wins on total cost.

\end{fbx}

When implementing streaming components within ETL/ELT architectures,
distributed systems principles become critical. The CAP
theorem\sidenote{\textbf{CAP Theorem}: Conjectured by Eric Brewer at UC
Berkeley in 2000 and formally proved by Seth Gilbert and Nancy Lynch
(MIT, 2002). The acronym captures the three competing properties:
Consistency (all nodes see the same data), Availability (system remains
operational), and Partition tolerance (system continues despite network
failures). Brewer's insight that distributed systems must sacrifice one
property fundamentally shaped how ML systems choose between databases,
streaming platforms, and storage architectures. } fundamentally
constrains streaming system design choices. Apache
Kafka\sidenote{\textbf{Apache Kafka}: Named after author Franz Kafka by
LinkedIn engineer Jay Kreps (2011), who chose the name because ``Kafka
is a system optimized for writing'' and Kafka the author was known for
his writing. The distributed streaming platform provides ordered,
replicated logs for high-throughput, low-latency event processing. Kafka
handles trillions of messages daily at companies like LinkedIn, making
it essential for ML systems requiring real-time data ingestion and
feature serving. } emphasizes consistency and partition tolerance,
making it well-suited for reliable event ordering but potentially
experiencing availability issues during network partitions. Apache
Pulsar emphasizes availability and partition tolerance, providing better
fault tolerance but with relaxed consistency guarantees. Amazon Kinesis
balances all three properties through careful configuration but requires
understanding these trade-offs for proper deployment.

\subsection{Feature Computation
Placement}\label{sec-data-engineering-ml-feature-computation-placement-a998}

For ML pipelines, an additional decision extends beyond ETL versus ELT:
where to compute features. This choice significantly impacts training
speed, storage costs, and reproducibility.

Pipeline-computed features are precomputed during ETL and stored.
Benefits include fast training iteration (features ready on disk),
reproducibility (same features used consistently), and reduced training
compute. Drawbacks include storage cost (features stored separately from
raw data), staleness risk (precomputed features may diverge from logic
changes), and inflexibility (changes require recomputation).

Loader-computed features are computed on the fly during training.
Benefits include always-fresh computation (logic changes immediately
reflected), flexible experimentation (easy to modify features), and
reduced storage (only raw data stored). Drawbacks include slower
training (computation repeated each epoch), higher compute costs (GPU
often idle waiting for features), and potential non-determinism if not
carefully implemented.

Hybrid patterns predominate in production. Expensive, stable features
(user embeddings requiring matrix factorization, historical aggregations
spanning months of data) are precomputed. Cheap, time-sensitive features
(recency signals, session context, time-based transformations) are
computed in the data loader.

For example, a recommendation system precomputes user embedding features
(expensive, stable over days) while computing
time-since-last-interaction features (cheap, time-sensitive) in the data
loader. This balances storage costs, computation time, and feature
freshness based on each feature's specific characteristics.

\subsection{Integration Strategies and KWS Case
Study}\label{sec-data-engineering-ml-multisource-integration-strategies-0e8a}

Regardless of whether ETL or ELT approaches are used, integrating
diverse data sources remains a core ingestion challenge. Data may
originate from databases, APIs, file systems, and IoT devices, each with
its own format (relational rows, JSON\sidenote{\textbf{JavaScript Object
Notation (JSON)}: A lightweight, text-based data interchange format
using human-readable key-value pairs and arrays. Ubiquitous in web APIs
and modern data systems due to its simplicity and language-agnostic
parsing, though less storage-efficient than binary formats like Parquet
for large-scale ML datasets. } documents, binary streams), access
protocol, and update frequency. The systems principle is to standardize
at the ingestion boundary: normalize formats, validate schemas, and
present a consistent interface to downstream processing regardless of
source. This boundary standardization separates the complexity of source
diversity from the complexity of feature engineering, allowing each to
evolve independently.

\textbf{Selecting Ingestion Patterns for KWS}

KWS production systems use streaming and batch ingestion in concert. The
streaming path handles real-time audio from active devices, using
publish-subscribe mechanisms like Apache Kafka to buffer incoming data
and distribute it across inference servers within the 200 millisecond
latency requirement. The batch path handles training data: new
recordings from crowdsourcing efforts discussed in
Section~\ref{sec-data-engineering-ml-strategic-data-acquisition-418f},
synthetic data addressing coverage gaps, and validated user
interactions. Batch processing typically follows an ETL pattern where
audio undergoes normalization, noise filtering, and segmentation into
consistent durations before storage in training-optimized formats.

Error handling in voice interaction systems requires special attention.
Dead letter queues store failed recognition attempts for subsequent
analysis, revealing edge cases that need coverage in future model
iterations. Each incoming audio sample must pass quality validation
(signal-to-noise ratio, sample rate, duration bounds, speaker proximity)
before entering the processing pipeline. Invalid samples route to
analysis queues rather than being discarded, since these failures often
indicate acoustic conditions underrepresented in training data. Valid
samples flow through to real-time detection while simultaneously being
logged for potential inclusion in future training data.

This ingestion architecture completes the boundary layer where external
data enters our controlled pipeline. With reliable ingestion
established, we now turn to systematic data processing that transforms
ingested raw data into ML-ready features while maintaining the
training-serving consistency essential for production systems.

\section{Systematic Data
Processing}\label{sec-data-engineering-ml-systematic-data-processing-aebc}

With reliable data ingestion established, we enter the most technically
challenging phase of the pipeline: systematic data processing. This
section addresses three interconnected processing challenges: first,
ensuring that transformations remain identical between training and
serving environments; second, building reliable transformations that
produce consistent results regardless of when or how many times they
execute; and third, scaling processing to handle growing data volumes
while maintaining data lineage for reproducibility. Each challenge
requires specific engineering techniques that we examine in turn.

A fundamental requirement, applying identical transformations during
training and serving, represents one of the most common sources of
production ML failures. Industry experience suggests that
training-serving inconsistency contributes to the majority of silent
model degradation issues (\citeproc{ref-sculley2015hidden}{Sculley et
al. 2021}). This finding reveals why training-serving consistency must
be the central organizing principle for all processing decisions.

Data processing implements the quality requirements defined in our
problem definition phase, transforming raw data into ML-ready formats
while maintaining reliability and scalability standards. Processing
decisions must preserve data integrity while improving model readiness,
all while adhering to governance principles throughout the
transformation pipeline. Every transformation, from normalization
parameters to categorical encodings to feature engineering logic, must
be applied identically in both contexts. Consider a simple example:
normalizing transaction amounts during training by removing currency
symbols and converting to floats, but forgetting to apply identical
preprocessing during serving. This seemingly minor inconsistency can
degrade model accuracy by 20-40\%, as the model receives differently
formatted inputs than it was trained on. The severity of this problem
makes training-serving consistency the central organizing principle for
processing system design.

For our KWS system, processing presents a specific tension:
transformations must standardize across diverse recording conditions
(varying microphones, noise levels, sample rates) while preserving the
acoustic characteristics that distinguish wake words from background
speech. This standardization must be identical in both training and
serving paths, a requirement we formalize next.

\subsection{Ensuring Training-Serving
Consistency}\label{sec-data-engineering-ml-ensuring-trainingserving-consistency-c683}

Quality serves as the cornerstone of data processing. Here, the quality
pillar manifests as ensuring that transformations applied during
training match exactly those applied during serving. This consistency
challenge extends beyond just applying the same code---it requires that
parameters computed on training data (normalization constants, encoding
dictionaries, vocabulary mappings) are stored and reused during serving.
Without this discipline, models receive fundamentally different inputs
during serving than they were trained on, causing performance
degradation that is often subtle and difficult to debug. This
requirement is so fundamental that we formalize it as the
\emph{consistency imperative}.

\phantomsection\label{callout-definitionux2a-1.20}
\begin{fbx}{callout-definition}{Definition:}{The Consistency Imperative}
\phantomsection\label{callout-definition*-1.20}
\textbf{\emph{The Consistency Imperative}} is the axiom that
\textbf{Transformation Logic} must be immutable across environments.
Because models learn a function \(f(T(x))\) where \(T\) is the
preprocessing pipeline, any deviation \(T'\) in serving results in the
model receiving out-of-distribution inputs, guaranteeing performance
degradation proportional to the \textbf{KL Divergence} between \(T(x)\)
and \(T'(x)\).

This principle, which we term the \textbf{Consistency Imperative}, is
not a guideline but a \emph{mathematical necessity}. When a model learns
\(f: X \rightarrow Y\) on transformed training data \(T(X_{train})\), it
expects serving inputs in the same transformed space. Serving raw or
differently-transformed data \(T'(X_{serve})\) feeds the model inputs
from a distribution it never encountered:

\[ \text{Accuracy Loss} \propto D_{KL}(T(X) \parallel T'(X)) \]

\textbf{Quantitative Evidence}: Industry studies report that 30--40\% of
initial ML deployments suffer from training-serving skew
(\citeproc{ref-sculley2015hidden}{Sculley et al. 2021}). A
recommendation system computing \texttt{user\_lifetime\_purchases} via
complete transaction history during training but using weekly-cached
aggregates during serving observed:

\begin{itemize}
\tightlist
\item
  \textbf{15\% feature value discrepancy} between training and serving
\item
  \textbf{12\% accuracy drop} in A/B tests
\item
  \textbf{Root cause identification time}: 2 weeks (subtle, no obvious
  errors)
\end{itemize}

\textbf{The Engineering Corollary}: Training-serving consistency is not
achieved by intention but by \emph{architecture}. Feature stores, shared
transformation libraries, and automated consistency validation exist
because good intentions fail at scale.

\end{fbx}

The stakes are high: violating the Consistency Imperative silently
degrades model accuracy in production. Before diving into specific
cleaning and transformation techniques, verify your understanding of
this fundamental requirement.

\phantomsection\label{callout-checkpointux2a-1.21}
\begin{fbx}{callout-checkpoint}{Checkpoint:}{Defensive Processing}
\phantomsection\label{callout-checkpoint*-1.21}

The \#1 cause of ML system failure isn't bad algorithms, it's
\textbf{Training-Serving Skew}.

\begin{itemize}
\tightlist
\item[$\square$]
  \textbf{The Definition}: Skew happens when the code processing data
  during training differs from the code processing live requests.
\item[$\square$]
  \textbf{The Mechanism}: If training normalizes data using
  \texttt{mean=0.5} but serving uses \texttt{mean=0.0}, the model sees
  ``alien'' data and fails silently.
\item[$\square$]
  \textbf{The Fix}: We don't just copy code. We use shared libraries,
  \textbf{Feature Stores}, or graph transformations (like TF Transform)
  to guarantee the logic is identical.
\end{itemize}

\end{fbx}

Data cleaning involves identifying and correcting errors,
inconsistencies, and inaccuracies in datasets. Raw data frequently
contains issues such as missing values, duplicates, or outliers that can
significantly impact model performance if left unaddressed. The key
insight is that cleaning operations must be deterministic and
reproducible: given the same input, cleaning must produce the same
output whether executed during training or serving. This requirement
shapes which cleaning techniques are safe to use in production ML
systems.

Data cleaning might involve removing duplicate records based on
deterministic keys, handling missing values through imputation or
deletion using rules that can be applied consistently, and correcting
formatting inconsistencies systematically. For instance, in a customer
database, names might be inconsistently capitalized or formatted. A data
cleaning process would standardize these entries, ensuring that ``John
Doe,'' ``john doe,'' and ``DOE, John'' are all treated as the same
entity. The cleaning rules---convert to title case, reorder to ``First
Last'' format---must be captured in code that executes identically in
training and serving. As emphasized throughout this chapter, every
cleaning operation must be applied identically in both contexts to
maintain system reliability.

Outlier detection and treatment is another important aspect of data
cleaning, but one that introduces consistency challenges. Outliers can
sometimes represent valuable information about rare events, but they can
also result from measurement errors or data corruption. ML practitioners
must carefully consider the nature of their data and the requirements of
their models when deciding how to handle outliers. Simple
threshold-based outlier removal (removing values more than 3 standard
deviations from the mean) maintains training-serving consistency if the
mean and standard deviation are computed on training data and reused
during serving. However, more sophisticated outlier detection methods
that consider relationships between features or temporal patterns
require careful engineering to ensure consistent application.

Quality assessment goes hand in hand with data cleaning, providing a
systematic approach to evaluating the reliability and usefulness of
data. This process involves examining various aspects of data quality,
including accuracy, completeness, consistency, and timeliness. In
production systems, data quality degrades in subtle ways that basic
metrics miss: fields that never contain nulls suddenly show sparse
patterns, numeric distributions drift from their training ranges, or
categorical values appear that weren't present during model development.

To address these subtle degradation patterns, production quality
monitoring requires specific metrics beyond simple missing value counts
as discussed in
Section~\ref{sec-data-engineering-ml-quality-validation-monitoring-498f}.
Critical indicators include null value patterns by feature (sudden
increases suggest upstream failures), count anomalies (10x increases
often indicate data duplication or pipeline errors), value range
violations (prices becoming negative, ages exceeding realistic bounds),
and join failure rates between data sources. \textbf{Statistical Drift}
detection\sidenote{\textbf{Data Drift}: The term ``drift'' comes from
Old Norse ``drifa'' (to drive or push), originally describing slow
movement caused by external forces like wind or current. In ML, data
drift describes the gradual movement of production data distributions
away from training baselines, pushed by forces like changing user
behavior or evolving systems. Unlike model decay (degradation from
unchanged data), drift specifically denotes distribution shift requiring
monitoring of feature statistics to detect before accuracy degrades. }
becomes essential by monitoring means, variances, and quantiles of
features over time to catch gradual degradation before it impacts model
performance. For example, in an e-commerce recommendation system, the
average user session length might gradually increase from 8 minutes to
12 minutes over six months due to improved site design, but a sudden
drop to 3 minutes suggests a data collection bug.

Supporting these monitoring requirements, quality assessment tools range
from simple statistical measures to complex machine learning-based
approaches. Data profiling tools provide summary statistics and
visualizations that help identify potential quality issues, while
advanced techniques employ unsupervised learning algorithms to detect
anomalies or inconsistencies in large datasets. Establishing clear
quality metrics and thresholds ensures that data entering the ML
pipeline meets necessary standards for reliable model training and
inference. The key is maintaining the same quality standards and
validation logic across training and serving to prevent quality issues
from creating training-serving skew.

Transformation techniques convert data from its raw form into a format
more suitable for analysis and modeling. This process can include a wide
range of operations, from simple conversions to complex mathematical
transformations. Central to effective transformation, common
transformation tasks include normalization and standardization, which
scale numerical features to a common range or distribution. For example,
in a housing price prediction model, features like square footage and
number of rooms might be on vastly different scales. Normalizing these
features ensures that they contribute more equally to the model's
predictions (\citeproc{ref-bishop2006pattern}{Bishop 2006}). Maintaining
training-serving consistency requires that normalization parameters
(mean, standard deviation) computed on training data be stored and
applied identically during serving. This means persisting these
parameters alongside the model itself---often in the model artifact or a
separate parameter file---and loading them during serving
initialization.

Beyond numerical scaling, other transformations might involve encoding
categorical variables, handling date and time data, or creating derived
features. For instance, one-hot encoding is often used to convert
categorical variables into a format that can be readily understood by
many machine learning algorithms. Categorical encodings must handle both
the categories present during training and unknown categories
encountered during serving. A reliable approach computes the category
vocabulary during training (the set of all observed categories),
persists it with the model, and during serving either maps unknown
categories to a special ``unknown'' token or uses default values.
Without this discipline, serving encounters categories the model never
saw during training, potentially causing errors or degraded performance.

Feature engineering is the process of using domain knowledge to create
new features that make machine learning algorithms work more
effectively. This step is often considered more of an art than a
science, requiring creativity and deep understanding of both the data
and the problem at hand. Feature engineering might involve combining
existing features, extracting information from complex data types, or
creating entirely new features based on domain insights. For example, in
a retail recommendation system, engineers might create features that
capture the recency, frequency, and monetary value of customer
purchases, known as RFM analysis (\citeproc{ref-kuhn2013applied}{Kuhn
and Johnson 2013}).

Given these creative possibilities, the importance of feature
engineering cannot be overstated. Well-engineered features can often
lead to significant improvements in model performance, sometimes
outweighing the impact of algorithm selection or hyperparameter tuning.
However, the creativity required for feature engineering must be
balanced against the consistency requirements of production systems.
Every engineered feature must be computed identically during training
and serving. This means that feature engineering logic should be
implemented in libraries or modules that can be shared between training
and serving code, rather than being reimplemented separately. Many
organizations build feature stores, discussed in
Section~\ref{sec-data-engineering-ml-feature-stores-bridging-training-serving-55c8},
specifically to ensure feature computation consistency across
environments.

Applying these processing concepts to our KWS system, the audio
recordings flowing through our ingestion pipeline, whether from
crowdsourcing, synthetic generation, or real-world captures, require
careful cleaning to ensure reliable wake word detection. Raw audio data
often contains imperfections that our problem definition anticipated:
background noise from various environments (quiet bedrooms to noisy
industrial settings), clipped signals from recording level issues,
varying volumes across different microphones and speakers, and
inconsistent sampling rates from diverse capture devices. The cleaning
pipeline must standardize these variations while preserving the acoustic
characteristics that distinguish wake words from background speech---a
quality-preservation requirement that directly impacts our 98\% accuracy
target.

Quality assessment for KWS extends the general principles with
audio-specific metrics. Beyond checking for null values or schema
conformance, our system tracks background noise levels (signal-to-noise
ratio above 20 decibels), audio clarity scores (frequency spectrum
analysis), and speaking rate consistency (wake word duration within
500-800 milliseconds). The quality assessment pipeline automatically
flags recordings where background noise would prevent accurate
detection, where wake words are spoken too quickly or unclearly for the
model to distinguish them, or where clipping or distortion has corrupted
the audio signal. This automated filtering ensures only high-quality
samples reach model development. Recall how Figure~\ref{fig-cascades}
demonstrated the compounding effects of early data quality failures;
this filtering prevents precisely those cascade failures by catching
issues at the source.

Transforming audio data for KWS involves converting raw waveforms into
formats suitable for ML models while maintaining training-serving
consistency. Figure~\ref{fig-spectrogram-example} visualizes this
transformation pipeline, showing how raw audio waveforms convert into
standardized feature representations, typically Mel-frequency cepstral
coefficients (MFCCs)\sidenote{\textbf{Mel-frequency cepstral
coefficients (MFCCs)}: The ``mel'' in MFCC derives from ``melody,''
coined by Stanley Smith Stevens in 1937 to create a perceptual pitch
scale where equal distances sound equally different to humans.
``Cepstral'' is a playful reversal of ``spectral,'' invented by Bogert,
Healy, and Tukey (1963) since cepstral analysis operates on the spectrum
of a spectrum. MFCCs apply mel-scale filtering (emphasizing frequencies
humans hear best) followed by cepstral analysis, typically extracting
13-39 coefficients encoding timbral properties essential for speech
recognition. } or spectrograms,\sidenote{\textbf{Spectrogram}: From
Latin ``spectrum'' (appearance, image) and Greek ``gramma'' (something
written or drawn). The term emerged in the 1940s as sound visualization
technology developed. Spectrograms are computed via Short-Time Fourier
Transform (STFT) that applies FFT to overlapping windows, creating 2D
image-like inputs where x-axis represents time, y-axis represents
frequency, and intensity shows amplitude. This visual representation
enables CNNs to process audio as image patterns, but the transformation
must be identical in training and serving to maintain accuracy. } that
emphasize speech-relevant characteristics while reducing noise and
variability across different recording conditions.

\begin{figure}[t!]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{contents/vol1/data_engineering/images/png/kws_spectrogram.png}}

}

\caption{\label{fig-spectrogram-example}\textbf{Audio Feature
Transformation}: Advanced audio features compress raw audio waveforms
into representations that emphasize perceptually relevant
characteristics for machine learning tasks. This transformation reduces
noise and data dimensionality while preserving essential speech
information, improving model performance in applications like keyword
spotting.}

\end{figure}%

\subsection{Building Idempotent Data
Transformations}\label{sec-data-engineering-ml-building-idempotent-data-transformations-7961}

Building on quality foundations, we turn to reliability. While quality
focuses on what transformations produce, reliability ensures how
consistently they operate. Processing reliability means transformations
produce identical outputs given identical inputs, regardless of when,
where, or how many times they execute. This property, called
idempotency,\sidenote{\textbf{Idempotency}: A mathematical property
where applying an operation multiple times produces the same result as
applying it once. The term derives from Latin ``idem'' (same) and
``potent'' (power). In distributed systems, idempotency enables safe
retries after failures without corrupting state. For ML pipelines,
idempotent feature transformations guarantee reproducible training data
regardless of how many times preprocessing runs, which is essential for
debugging, A/B testing, and regulatory compliance requiring reproducible
model outputs. } proves essential for production ML systems where
processing may be retried due to failures, where data may be reprocessed
to fix bugs, or where the same data flows through multiple processing
paths.

To understand idempotency intuitively, consider a light switch. Flipping
the switch to the ``on'' position turns the light on. Flipping it to
``on'' again leaves the light on; the operation can be repeated without
changing the outcome. This is idempotent behavior. In contrast, a toggle
switch that changes state with each press is not idempotent: pressing it
repeatedly alternates between on and off states. In data processing, we
want light switch behavior where reapplying the same transformation
yields the same result, not toggle switch behavior where repeated
application changes the outcome unpredictably.

Idempotent transformations enable reliable error recovery. When a
processing job fails midway, the system can safely retry processing the
same data without worrying about duplicate transformations or
inconsistent state. A non-idempotent transformation might append data to
existing records, so retrying would create duplicates. An idempotent
transformation would upsert data (insert if not exists, update if
exists), so retrying produces the same final state. This distinction
becomes critical in distributed systems where partial failures are
common and retries are the primary recovery mechanism.

Handling partial processing failures requires careful state management.
Processing pipelines should be designed so that each stage can be
retried independently without affecting other stages. Checkpoint-restart
mechanisms enable recovery from the last successful processing state
rather than restarting from scratch. For long-running data processing
jobs operating on terabyte-scale datasets, checkpointing progress every
few minutes means a failure near the end requires reprocessing only
recent data rather than the entire dataset. The checkpoint logic must
carefully track what data has been processed and what remains, ensuring
no data is lost or processed twice.

Deterministic transformations are those that always produce the same
output for the same input, without dependence on external factors like
time, random numbers, or mutable global state. Transformations that
depend on current time (e.g., computing ``days since event'' based on
current date) break determinism---reprocessing historical data would
produce different results. The solution is to capture temporal reference
points explicitly: instead of ``days since event,'' compute ``days from
event to reference date'' where reference date is fixed and persisted.
Random operations should use seeded random number generators where the
seed is derived deterministically from input data, ensuring
reproducibility.

For our KWS system, reliability requires reproducible feature
extraction. Audio preprocessing must be deterministic: given the same
raw audio file, the same MFCC features are always computed regardless of
when processing occurs or which server executes it. This enables
debugging model behavior (can always recreate exact features for a
problematic example), reprocessing data when bugs are fixed (produces
consistent results), and distributed processing (different workers
produce identical features from the same input). The processing code
captures all parameters---FFT window size, hop length, number of MFCC
coefficients---in configuration that's versioned alongside the code,
ensuring reproducibility across time and execution environments.
However, even with rigorous design, production systems must implement
runtime monitoring to detect skew if it emerges;
\textbf{?@sec-machine-learning-operations-mlops} covers the operational
infrastructure for shadow scoring and distribution monitoring.

\subsection{Scaling Through Distributed
Processing}\label{sec-data-engineering-ml-scaling-distributed-processing-cb9b}

With quality and reliability established, we face the challenge of
scale. As datasets grow larger and ML systems become more complex, the
scalability of data processing becomes the limiting factor. Consider the
data processing stages we've discussed---cleaning, quality assessment,
transformation, and feature engineering. When these operations must
handle terabytes of data, a single machine becomes insufficient. The
cleaning techniques that work on gigabytes of data in memory must be
redesigned to work across distributed systems.

These challenges manifest when quality assessment must process data
faster than it arrives, when feature engineering operations require
computing statistics across entire datasets before transforming
individual records, and when transformation pipelines create bottlenecks
at massive volumes. Processing must scale from development (gigabytes on
laptops) through production (terabytes across clusters) while
maintaining consistent behavior.

To address these scaling bottlenecks, data must be partitioned across
multiple computing resources, which introduces coordination challenges.
Distributed coordination is fundamentally limited by network round-trip
times: local operations complete in microseconds while network
coordination requires milliseconds, creating a 1000\(\times\) latency
difference. This constraint explains why operations requiring global
coordination (like computing normalization statistics across 100
machines) create bottlenecks. Each partition computes local statistics
quickly, but combining them requires information from all partitions.

Data locality becomes critical at this scale. At 10 GB/s peak
throughput, transferring one terabyte of training data across a network
takes on the order of 100 seconds; reading the same amount from a 5 GB/s
SSD takes on the order of 200 seconds. These are the same order of
magnitude, which drives ML system design toward compute-follows-data
architectures. When processing nodes access local data at RAM speeds
(50--200 GB/s) but must coordinate over networks limited to 1--10 GB/s,
the bandwidth mismatch creates fundamental bottlenecks. Geographic
distribution amplifies these challenges: cross-datacenter coordination
must handle network latency (50--200 ms between regions), partial
failures, and regulatory constraints preventing data from crossing
borders. Understanding which operations parallelize easily versus those
requiring expensive coordination determines system architecture and
performance characteristics. This overhead constitutes a
\emph{coordination tax} that fundamentally limits distributed data
processing.

\phantomsection\label{callout-notebookux2a-1.22}
\begin{fbx}{callout-notebook}{AI Engineer’s Notebook:}{The Coordination Tax}
\phantomsection\label{callout-notebook*-1.22}
\textbf{Problem}: You're computing mean normalization across 1TB of
features distributed across 100 nodes. Is it faster to (A) gather all
data to one node and compute, or (B) compute local means and aggregate?

\textbf{Option A (Centralized)}:

\begin{itemize}
\tightlist
\item
  Transfer 1TB at 10 GB/s network: \textbf{100 seconds}
\item
  Compute mean on single node: \textbf{\textasciitilde1 second}
\item
  Total: \textbf{\textasciitilde101 seconds}
\end{itemize}

\textbf{Option B (Distributed)}:

\begin{itemize}
\tightlist
\item
  Each node computes local mean: \textbf{\textasciitilde0.01 seconds}
  (10GB at RAM speed)
\item
  Send 100 partial means (8 bytes each): \textbf{\textless1 ms}
\item
  Aggregate: \textbf{negligible}
\item
  Total: \textbf{\textasciitilde0.01 seconds} (10,000× faster)
\end{itemize}

\textbf{The Engineering Lesson}: Operations that \emph{reduce} data
(sum, mean, count) should always run locally first. Operations that
\emph{expand} data (joins, cross-products) face unavoidable network
costs. Pipeline design should minimize data movement by pushing
computation to where data resides, the compute-follows-data principle
central to systems like MapReduce (\citeproc{ref-dean2004mapreduce}{Dean
and Ghemawat 2008}), Spark (\citeproc{ref-zaharia2010spark}{Zaharia et
al. 2010}), and modern ML frameworks.

\end{fbx}

Single-machine processing suffices for surprisingly large workloads when
engineered carefully. Modern servers with 256 gigabytes RAM can process
datasets of several terabytes using out-of-core processing that streams
data from disk. Libraries like Dask or Vaex enable pandas-like APIs that
automatically stream and parallelize computations across multiple cores.
Before investing in distributed processing infrastructure, teams should
exhaust single-machine optimization: using efficient data formats
(Parquet\sidenote{\textbf{Parquet}: Named after the herringbone wood
flooring pattern, this columnar storage format (developed by Cloudera
and Twitter, 2013) stores data in nested column structures that visually
resemble parquet flooring tiles. The name reflects how data is
interlocked column-by-column rather than row-by-row. For ML systems,
this columnar layout enables reading only required features and achieves
5-10x I/O reduction compared to row-based formats like CSV. } instead of
CSV), minimizing memory allocations, leveraging vectorized operations,
and exploiting multi-core parallelism. The operational simplicity of
single-machine processing---no network coordination, no partial
failures, simple debugging---makes it preferable when performance is
adequate.

Distributed processing frameworks become necessary when data volumes or
computational requirements exceed single-machine capacity, but the
speedup achievable through parallelization faces inherent limits
described by \textbf{Amdahl's Law}:

\[\text{Speedup} \leq \frac{1}{S + \frac{P}{N}}\]

where \(S\) represents the serial fraction of work that cannot
parallelize, \(P\) the parallel fraction, and \(N\) the number of
processors. This explains why distributing our KWS feature extraction
across 64 cores achieves only a 64\(\times\) speedup when the work is
embarrassingly parallel (\(S \approx 0\)), but coordination-heavy
operations like computing global normalization statistics might achieve
only 10\(\times\) speedup even with 64 cores due to the serial
aggregation phase. Understanding this relationship guides architectural
decisions: operations with high serial fractions should run on fewer,
faster cores rather than many slower cores, while highly parallel
workloads benefit from maximum distribution as examined further in
\textbf{?@sec-ai-training}.

Apache Spark provides a distributed computing framework that
parallelizes transformations across clusters of machines, handling data
partitioning, task scheduling, and fault tolerance automatically. Beam
provides a unified API for both batch and streaming processing, enabling
the same transformation logic to run on multiple execution engines
(Spark, Flink, Dataflow). TensorFlow's tf.data API optimizes data
loading pipelines for ML training, supporting distributed reading,
prefetching, and transformation. The choice of framework depends on
whether processing is batch or streaming, how transformations
parallelize, and what execution environment is available.

Another important consideration is the balance between preprocessing and
on-the-fly computation. While extensive preprocessing can speed up model
training and inference, it can also lead to increased storage
requirements and potential data staleness. Production systems often
implement hybrid approaches, preprocessing computationally expensive
features while computing rapidly changing features on-the-fly. This
balance depends on storage costs, computation resources, and freshness
requirements specific to each use case. Features that are expensive to
compute but change slowly (user demographic summaries, item popularity
scores) benefit from preprocessing. Features that change rapidly
(current session state, real-time inventory levels) must be computed
on-the-fly despite computational cost.

For our KWS system, scalability manifests at multiple stages.
Development uses single-machine processing on sample datasets to iterate
rapidly. Training at scale requires distributed processing when dataset
size (23 million examples) exceeds single-machine capacity or when
multiple experiments run concurrently. The processing pipeline
parallelizes naturally: audio files are independent, so transforming
them requires no coordination between workers. Each worker reads its
assigned audio files from distributed storage, computes features, and
writes results back---a trivially parallel pattern achieving near-linear
scalability. Production deployment requires real-time processing on edge
devices with severe resource constraints (our 16 kilobyte memory limit),
necessitating careful optimization and quantization to fit processing
within device capabilities.

\subsection{Tracking Data Transformation
Lineage}\label{sec-data-engineering-ml-tracking-data-transformation-lineage-3b09}

Completing our four-pillar view of data processing, governance ensures
accountability and reproducibility. The governance pillar requires
tracking what transformations were applied, when they executed, which
version of processing code ran, and what parameters were used. This
transformation lineage enables reproducibility essential for debugging,
compliance with regulations requiring explainability, and iterative
improvement when transformation bugs are discovered. Without
comprehensive lineage, teams cannot reproduce training data, cannot
explain why models make specific predictions, and cannot safely fix
processing bugs without risking inconsistency.

Transformation versioning captures which version of processing code
produced each dataset. When transformation logic changes---fixing a bug,
adding features, or improving quality---the version number increments.
Datasets are tagged with the transformation version that created them,
enabling identification of all data requiring reprocessing when bugs are
fixed. This versioning extends beyond just code versions to capture the
entire processing environment: library versions (different NumPy
versions may produce slightly different numerical results), runtime
configurations (environment variables affecting behavior), and execution
infrastructure (CPU architecture affecting floating-point precision).

Parameter tracking maintains the specific values used during
transformation. For normalization, this means storing the mean and
standard deviation computed on training data. For categorical encoding,
this means storing the vocabulary (set of all observed categories). For
feature engineering, this means storing any constants, thresholds, or
parameters used in feature computation. These parameters are typically
serialized alongside model artifacts, ensuring serving uses identical
parameters to training. Modern ML frameworks like TensorFlow and PyTorch
provide mechanisms for bundling preprocessing parameters with models,
simplifying deployment and ensuring consistency.

Processing lineage for reproducibility tracks the complete
transformation history from raw data to final features. This includes
which raw data files were read, what transformations were applied in
what order, what parameters were used, and when processing occurred.
Lineage systems like Apache Atlas, Amundsen, or commercial offerings
instrument pipelines to automatically capture this flow. When model
predictions prove incorrect, engineers can trace back through lineage:
which training data contributed to this behavior, what quality scores
did that data have, what transformations were applied, and can we
recreate this exact scenario to investigate?

Code version ties processing results to the exact code that produced
them. When processing code lives in version control (Git), each dataset
should record the commit hash of the code that created it. This enables
recreating the exact processing environment: checking out the specific
code version, installing dependencies listed at that version, and
running processing with identical parameters. Container technologies
like Docker simplify this by capturing the entire processing environment
(code, dependencies, system libraries) in an immutable image that can be
rerun months or years later with identical results.

For our KWS system, transformation governance tracks audio processing
parameters that critically affect model behavior. When audio is
normalized to standard volume, the reference volume level is persisted.
When FFT transforms audio to frequency domain, the window size, hop
length, and window function (Hamming, Hanning, etc.) are recorded. When
MFCCs are computed, the number of coefficients, frequency range, and mel
filterbank parameters are captured. This comprehensive parameter
tracking enables several critical capabilities: reproducing training
data exactly when debugging model failures, validating that serving uses
identical preprocessing to training, and systematically studying how
preprocessing choices affect model accuracy. Without this governance
infrastructure, teams resort to manual documentation that inevitably
becomes outdated or incorrect, leading to subtle training-serving skew
that degrades production performance.

\subsection{End-to-End Processing Pipeline
Design}\label{sec-data-engineering-ml-endtoend-processing-pipeline-design-f5d2}

Integrating these cleaning, assessment, transformation, and feature
engineering steps, processing pipelines bring together the various data
processing steps into a coherent, reproducible workflow. These pipelines
ensure that data is consistently prepared across training and inference
stages, reducing the risk of data leakage and improving the reliability
of ML systems. Pipeline design determines how easily teams can iterate
on processing logic, how well processing scales as data grows, and how
reliably systems maintain training-serving consistency.

Modern ML frameworks and tools often provide capabilities for building
and managing data processing pipelines. For instance, Apache Beam and
TensorFlow Transform allow developers to define data processing steps
that can be applied consistently during both model training and serving.
The choice of data processing framework must align with the broader ML
framework ecosystem discussed in \textbf{?@sec-ai-frameworks}, where
framework-specific data loaders and preprocessing utilities can
significantly impact development velocity and system performance.

Beyond tool selection, effective pipeline design involves considerations
such as modularity, scalability, and version control. Modular pipelines
allow for easy updates and maintenance of individual processing steps.
Each transformation stage should be implemented as an independent module
with clear inputs and outputs, enabling testing in isolation and
replacement without affecting other stages. Version control for
pipelines is equally important, ensuring that changes in data processing
can be tracked and correlated with changes in model performance. When
model accuracy drops, version control enables identifying whether
processing changes contributed to the degradation.

Figure~\ref{fig-tfx-pipeline-example} illustrates this modular breakdown
using TensorFlow Extended, tracing the complete flow from initial data
ingestion through to final model deployment. Data flows through
validation, transformation, and feature engineering stages before
reaching model training, with each component independently versioned,
tested, and scaled while maintaining overall system consistency.

\begin{figure}[htb]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/d9d4c9c2305a1671758e39468fe80f786e4f3a58.pdf}}

}

\caption{\label{fig-tfx-pipeline-example}\textbf{TFX End-to-End
Pipeline}: A TensorFlow Extended pipeline traces the complete flow from
data ingestion through validation, transformation, training, evaluation,
and deployment. Each component is independently versioned, tested, and
scaled.}

\end{figure}%

Integrating these processing components, our KWS processing pipelines
must handle both batch processing for training and real-time processing
for inference while maintaining consistency between these modes. The
pipeline design ensures that the same normalization parameters computed
on training data---mean volume levels, frequency response curves, and
duration statistics---are stored and applied identically during serving.
This architectural decision reflects our reliability pillar: users
expect consistent wake word detection regardless of when their device
was manufactured or which model version it runs, requiring processing
pipelines that maintain stable behavior across training iterations and
deployment environments.

Effective data processing is the cornerstone of successful ML systems.
By carefully cleaning, transforming, and engineering data through the
lens of our four-pillar framework---quality through training-serving
consistency, reliability through idempotent transformations, scalability
through distributed processing, and governance through comprehensive
lineage---practitioners can significantly improve the performance and
reliability of their models. As the field of machine learning continues
to evolve, so too do the techniques and tools for data processing,
making this an exciting and dynamic area of study and practice.

The processing pipelines we have designed transform raw data into
structured features, but one critical input remains: the labels that
tell our models what patterns to learn. Unlike the automated
transformations examined in this section, labeling introduces human
judgment into our otherwise algorithmic pipelines, creating unique
challenges for maintaining quality, reliability, scalability, and
governance when human attention becomes the limiting resource.

\section{Data
Labeling}\label{sec-data-engineering-ml-data-labeling-6836}

With systematic data processing established, data labeling emerges as a
particularly complex systems challenge. As training datasets grow to
millions or billions of examples, the infrastructure supporting labeling
operations becomes critical to system performance. Labeling represents
human-in-the-loop system engineering where our four pillars guide
infrastructure decisions differently than in automated pipeline stages.
The goal is to establish \textbf{ground truth}\sidenote{\textbf{Ground
Truth}: Originally a term from meteorology and remote sensing. When a
satellite estimates crop yields or weather from orbit, scientists would
send a team to the physical location---the ``ground''---to verify the
``truth.'' In ML, it reminds us that our labels are merely the best
available proxy for reality, verified by direct human observation. },
the foundational reference labels the model will learn from. Quality
manifests as ensuring label accuracy through consensus mechanisms and
gold standard validation. Reliability demands platform architecture that
coordinates thousands of concurrent annotators without data loss or
corruption. Scalability drives AI assistance to amplify human judgment
rather than replace it. Governance requires fair compensation, bias
mitigation, and ethical treatment of human contributors whose labor
creates the training data enabling ML systems.

Modern machine learning systems must efficiently handle the creation,
storage, and management of labels across their data pipeline. The
systems architecture must support various labeling workflows while
maintaining data consistency, ensuring quality, and managing
computational resources effectively. These requirements compound when
dealing with large-scale datasets or real-time labeling needs. The
systematic challenges extend beyond just storing and managing
labels---production ML systems need reliable pipelines that integrate
labeling workflows with data ingestion, preprocessing, and training
components while maintaining high throughput and adapting to changing
requirements.

\subsection{Label Types and Their System
Requirements}\label{sec-data-engineering-ml-label-types-system-requirements-4c33}

To build effective labeling systems, understanding how different types
of labels affect our system architecture and resource requirements is
essential. Consider a practical example: building a smart city system
that needs to detect and track various objects like vehicles,
pedestrians, and traffic signs from video feeds. Labels capture
information about key tasks or concepts, with each label type imposing
distinct storage, computation, and validation requirements.

Classification labels represent the simplest form, categorizing images
with a specific tag or (in multi-label classification) tags such as
labeling an image as ``car'' or ``pedestrian.'' While conceptually
straightforward, a production system processing millions of video frames
must efficiently store and retrieve these labels. Storage requirements
are modest---a single integer or string per image---but retrieval
patterns matter: training often samples random subsets while validation
requires sequential access to all labels, driving different indexing
strategies.

Bounding boxes extend beyond simple classification by identifying object
locations, drawing a box around each object of interest. Our system now
needs to track not just what objects exist, but where they are in each
frame. This spatial information introduces new storage and processing
challenges, especially when tracking moving objects across video frames.
Each bounding box requires storing four coordinates (x, y, width,
height) plus the object class, multiplying storage by 5x compared to
classification. More importantly, bounding box annotation requires
pixel-precise positioning that takes 10-20x longer than classification,
dramatically affecting labeling throughput and cost.

Segmentation maps provide the most comprehensive information by
classifying objects at the pixel level, highlighting each object in a
distinct color. For our traffic monitoring system, this might mean
precisely outlining each vehicle, pedestrian, and road sign. These
detailed annotations significantly increase our storage and processing
requirements. A segmentation mask for a 1920x1080 image requires 2
million labels (one per pixel), compared to perhaps 10 bounding boxes or
a single classification label. This 100,000x storage increase and the
hours required per image for manual segmentation make this approach
suitable only when pixel-level precision is essential.

\begin{figure}

\centering{

\includegraphics[width=0.9\linewidth,height=\textheight,keepaspectratio]{contents/vol1/data_engineering/images/png/cs249r_labels_new.png}

}

\caption{\label{fig-labels}\textbf{Data Annotation Granularity}: Three
versions of the same street scene show increasing annotation detail: a
simple classification label, bounding boxes around vehicles and
pedestrians, and pixel-level semantic segmentation with distinct colors.
Each level increases labeling cost and storage requirements while
providing richer training signal.}

\end{figure}%

Figure~\ref{fig-labels} visualizes these increasing complexity levels,
from simple classification through bounding boxes to pixel-level
segmentation. The choice of label format depends heavily on our system
requirements and resource constraints
(\citeproc{ref-10.1109ux2fICRA.2017.7989092}{Johnson-Roberson et al.
2017}). While classification labels might suffice for simple traffic
counting, autonomous vehicles need detailed segmentation maps to make
precise navigation decisions. Leading autonomous vehicle companies often
maintain hybrid systems that store multiple label types for the same
data, allowing flexible use across different applications. A single
camera frame might have classification labels (scene type: highway,
urban, rural), bounding boxes (vehicles and pedestrians for obstacle
detection), and segmentation masks (road surface for path planning),
with each label type serving distinct downstream models.

Extending beyond these basic label types, production systems must also
handle rich metadata essential for maintaining data quality and
debugging model behavior. The Common Voice dataset
(\citeproc{ref-ardila2020common}{Ardila et al. 2020}) exemplifies
sophisticated metadata management in speech recognition: tracking
speaker demographics for model fairness, recording quality metrics for
data filtering, validation status for label reliability, and language
information for multilingual support. If our traffic monitoring system
performs poorly in rainy conditions, weather condition metadata during
data collection helps identify and address the issue. Modern labeling
platforms have built sophisticated metadata management systems that
efficiently index and query this metadata alongside primary labels,
enabling filtering during training data selection and post-hoc analysis
when model failures are discovered.

These metadata requirements demonstrate how label type choice cascades
through entire system design. A system built for simple classification
labels would need significant modifications to handle segmentation maps
efficiently. The infrastructure must optimize storage systems for the
chosen label format, implement efficient data retrieval patterns for
training, maintain quality control pipelines for validation as
established in
Section~\ref{sec-data-engineering-ml-ensuring-trainingserving-consistency-c683},
and manage version control for label updates. When labels are corrected
or refined, the system must track which model versions used which label
versions, enabling correlation between label quality improvements and
model performance gains.

\subsection{Achieving Label Accuracy and
Consensus}\label{sec-data-engineering-ml-achieving-label-accuracy-consensus-7190}

In the labeling domain, quality takes on unique challenges centered on
ensuring label accuracy despite the inherent subjectivity and ambiguity
in many labeling tasks. Even with clear guidelines and careful system
design, some fraction of labels will inevitably be incorrect Thyagarajan
et al. (\citeproc{ref-thyagarajan2023multilabel}{2022}). The challenge
is not eliminating labeling errors entirely---an impossible goal---but
systematically measuring and managing error rates to keep them within
bounds that do not degrade model performance.

Labeling failures arise from two distinct sources requiring different
engineering responses. Figure~\ref{fig-hard-labels} presents concrete
examples of both failure modes: data quality issues where the underlying
data is genuinely ambiguous or corrupted, like the blurred frog image
where even expert annotators cannot determine the species with
certainty. Other errors require deep domain expertise where the correct
label is determinable but only by experts with specialized knowledge, as
with the black stork identification. These different failure modes drive
architectural decisions about annotator qualification, task routing, and
consensus mechanisms.

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{contents/vol1/data_engineering/images/png/label-errors-examples_new.png}}

}

\caption{\label{fig-hard-labels}\textbf{Labeling Ambiguity}: How
subjective or difficult examples, such as blurry images or rare species,
can introduce errors during data labeling, highlighting the need for
careful quality control and potentially expert annotation. Source:
(\citeproc{ref-northcutt2021pervasive}{Northcutt, Athalye, and Mueller
2021}).}

\end{figure}%

Given these fundamental quality challenges, production ML systems
implement multiple layers of quality control. Systematic quality checks
continuously monitor the labeling pipeline through random sampling of
labeled data for expert review and statistical methods to flag potential
errors. The infrastructure must efficiently process these checks across
millions of examples without creating bottlenecks. Sampling strategies
typically validate 1-10\% of labels, balancing detection sensitivity
against review costs. Higher-risk applications like medical diagnosis or
autonomous vehicles may validate 100\% of labels through multiple
independent reviews, while lower-stakes applications like product
recommendations may validate only 1\% through spot checks.

Beyond random sampling approaches, collecting multiple labels per data
point, often referred to as ``consensus labeling,'' can help identify
controversial or ambiguous cases. Professional labeling companies have
developed sophisticated infrastructure for this process. For example,
\href{https://labelbox.com/}{Labelbox} has consensus tools that track
inter-annotator agreement rates and automatically route controversial
cases for expert review. \href{https://scale.com}{Scale AI} implements
tiered quality control, where experienced annotators verify the work of
newer team members. The consensus infrastructure typically collects 3-5
labels per example, computing inter-annotator agreement using metrics
like Fleiss' kappa\sidenote{\textbf{Fleiss' kappa}: Named after
biostatistician Joseph L. Fleiss who generalized Cohen's kappa in 1971
to handle multiple raters simultaneously. While Cohen's kappa works only
for two raters, Fleiss extended the mathematics to any number of
annotators, making it essential for crowdsourced labeling where
different subsets of workers label each example. Values range from -1 to
1: above 0.8 indicates strong agreement, 0.6-0.8 moderate, and below 0.4
suggests labeling guidelines need clarification. } which measures
agreement beyond what would occur by chance. Examples with low agreement
(kappa below 0.4) route to expert review rather than forcing consensus
from genuinely ambiguous cases.

The consensus approach reflects an economic trade-off essential for
scalable systems. Expert review costs 10-50x more per example than
crowdsourced labeling, but forcing agreement on ambiguous examples
through majority voting of non-experts produces systematically biased
labels. By routing only genuinely ambiguous cases to experts---often
5-15\% of examples identified through low inter-annotator
agreement---systems balance cost against quality. This tiered approach
enables processing millions of examples economically while maintaining
quality standards through targeted expert intervention.

While technical infrastructure provides the foundation for quality
control, successful labeling systems must also consider human factors.
When working with annotators, organizations need reliable systems for
training and guidance. This includes good documentation with clear
examples of correct labeling, visual demonstrations of edge cases and
how to handle them, regular feedback mechanisms showing annotators their
accuracy on gold standard examples, and calibration sessions where
annotators discuss ambiguous cases to develop shared understanding. For
complex or domain-specific tasks, the system might implement tiered
access levels, routing challenging cases to annotators with appropriate
expertise based on their demonstrated accuracy on similar examples.

Quality monitoring generates substantial data that must be efficiently
processed and tracked. Organizations typically monitor inter-annotator
agreement rates (tracking whether multiple annotators agree on the same
example), label confidence scores (how certain annotators are about
their labels), time spent per annotation (both too fast suggesting
careless work and too slow suggesting confusion), error patterns and
types (systematic biases or misunderstandings), annotator performance
metrics (accuracy on gold standard examples), and bias indicators
(whether certain annotator demographics systematically label
differently). These metrics must be computed and updated efficiently
across millions of examples, often requiring dedicated analytics
pipelines that process labeling data in near real-time to catch quality
issues before they affect large volumes of data.

\subsection{Building Reliable Labeling
Platforms}\label{sec-data-engineering-ml-building-reliable-labeling-platforms-686f}

Moving from label quality to system reliability, platform architecture
supports consistent operations. While quality focuses on label accuracy,
reliability ensures the platform architecture itself operates
consistently at scale. Scaling labeling from hundreds to millions of
examples while maintaining quality requires understanding how production
labeling systems separate concerns across multiple architectural
components. The fundamental challenge is that labeling represents a
human-in-the-loop workflow where system performance depends not just on
infrastructure but on managing human attention, expertise, and
consistency.

At the foundation sits a durable task queue that stores labeling tasks
persistently, ensuring no work gets lost when systems restart or
annotators disconnect. Most production systems use message queues like
Apache Kafka or RabbitMQ rather than databases for this purpose, since
message queues provide natural ordering, parallel consumption, and
replay capabilities that databases do not easily support. Each task
carries metadata beyond just the data to be labeled: what type of task
it is (classification, bounding boxes, segmentation), what expertise
level it requires, how urgent it is, and any context needed for accurate
labeling---perhaps related examples or relevant documentation.

The assignment service that routes tasks to annotators implements
matching logic more sophisticated than simple round-robin distribution.
Medical image labeling systems route chest X-rays specifically to
annotators who have demonstrated radiology expertise, measured by their
agreement with expert labels on gold standard examples. But expertise
matching alone is not sufficient; annotators who see only chest images
or only a specific pathology can develop blind spots, performing well on
familiar examples but poorly on less common cases. Production systems
therefore constraint assignment to ensure no annotator receives more
than 30\% of their tasks from a single category, maintaining breadth of
exposure that prevents overspecialization from degrading quality on
less-familiar examples.

When tasks require multiple annotations to ensure quality, the consensus
engine determines both when sufficient labels have been collected and
how to aggregate potentially conflicting opinions. The consensus
mechanisms and economic trade-offs we examined in
Section~\ref{sec-data-engineering-ml-achieving-label-accuracy-consensus-7190}
now become platform requirements: the system must efficiently collect
multiple labels per example, compute agreement metrics, and route
low-agreement cases to expert review. The platform must implement this
routing logic efficiently, tracking which examples need expert review
and ensuring they are delivered to appropriately qualified annotators
without creating bottlenecks.

Maintaining quality at scale requires continuous measurement through
gold standard injection. The system periodically inserts examples with
known correct labels into the task stream without revealing which
examples are gold standard. This enables computing per-annotator
accuracy without the Hawthorne effect where measurement changes
behavior, since annotators cannot ``try harder'' on gold standard
examples if they do not know which ones they are. Annotators
consistently scoring below 85\% on gold standards receive additional
training materials, more detailed guidelines, or removal from the pool
if performance does not improve. Beyond simple accuracy, systems track
quality across multiple dimensions: agreement with peer annotators on
the same tasks (detecting systematic disagreement suggesting
misunderstanding of guidelines), time per task (both too fast suggesting
careless work and too slow suggesting confusion), and consistency where
the same annotator sees similar examples shown days apart to measure
whether they apply labels reliably over time.

The performance requirements of these systems become demanding at scale.
A labeling platform processing 10,000 annotations per hour must balance
latency requirements against database write capacity. Writing each
annotation immediately to a persistent database like PostgreSQL for
durability would require 2-3 writes per second, well within database
capacity. But task serving---delivering new tasks to 100,000 concurrent
annotators requesting work---requires subsecond response times that
databases struggle to provide when serving requests fan out across many
annotators. Production systems therefore maintain a two-tier storage
architecture: Redis caches active tasks enabling sub-100ms task
assignment latency, while annotations batch write to PostgreSQL every
100 annotations (typically every 30-60 seconds), providing durability
without overwhelming the database with small writes.

Horizontal scaling of these systems requires careful data partitioning.
Tasks shard by task\_id enabling independent task queue scaling,
annotator performance metrics shard by annotator\_id for fast lookup
during assignment decisions, and aggregated labels shard by example\_id
for efficient retrieval during model training. This partitioning
strategy enables systems handling millions of tasks daily to support
100,000+ concurrent annotators with median task assignment latency under
50ms, proving that human-in-the-loop systems can scale to match fully
automated pipelines when properly architected.

Beyond these architectural considerations, understanding the economics
of labeling operations reveals why scalability through AI assistance
becomes essential. Data labeling represents one of ML systems' largest
hidden costs, yet it is frequently overlooked in project planning that
focuses primarily on compute infrastructure and model training expenses.
While teams carefully optimize GPU utilization and track training costs
measured in dollars per hour, labeling expenses measured in dollars per
example often receive less scrutiny despite frequently exceeding compute
costs by orders of magnitude. Understanding the full economic model
reveals why scalability through AI assistance becomes not just
beneficial but economically necessary as ML systems mature and data
requirements grow to millions or billions of labeled examples, which
\textbf{?@sec-machine-learning-operations-mlops} examines where
operational costs compound across the ML lifecycle.

The cost structure of labeling operations follows a multiplicative model
capturing both direct annotation costs and quality control overhead:

\[\text{Total Cost} = N \times \text{Cost}_{\text{label}} \times (1 + R_{\text{review}}) \times (1 + R_{\text{rework}})\]

where \(N\) represents the number of examples,
\(\text{Cost}_{\text{label}}\) is the base cost per label,
\(R_{\text{review}}\) is the fraction requiring expert review (typically
0.05-0.15), and \(R_{\text{rework}}\) accounts for labels requiring
correction (typically 0.10-0.30). This equation reveals how quality
requirements compound costs: a dataset requiring 1 million labels at
\$0.10 per label with 10\% expert review (costing 5x more, or \$0.50)
and 20\% rework reaches \$138,000, not the \$100,000 that naive
calculation suggests. For comparison, training a ResNet-50 model on this
data might cost only \$50 for compute---nearly 3,000x less than
labeling, demonstrating why labeling economics dominate total system
costs yet receive insufficient attention during planning phases.
Amdahl's Law formalizes this serial bottleneck principle, which we apply
as \emph{Amdahl's Law for data pipelines}.

\phantomsection\label{notebook-amdahl-pipelines}
\begin{fbx}{callout-notebook}{AI Engineer’s Notebook:}{Amdahl's Law for Data Pipelines}
\phantomsection\label{notebook-amdahl-pipelines}
\textbf{The Serial Bottleneck Principle}: Amdahl's Law reveals why
labeling dominates data engineering costs and why parallelization alone
cannot solve the problem.

The maximum speedup from parallelization is bounded by the serial
fraction:

\[ \text{Speedup}_{\max} = \frac{1}{S + \frac{P}{N}} \]

where \(S\) is the serial (non-parallelizable) fraction, \(P\) is the
parallel fraction, and \(N\) is the number of processors.

\textbf{Applied to Data Pipelines}: Industry surveys show practitioners
spend 60--80\% of their time on data preparation. If human labeling and
review constitute 70\% of total data engineering effort (the serial
component), then:

\[ \text{Speedup}_{\max} = \frac{1}{0.70 + \frac{0.30}{\infty}} = \frac{1}{0.70} \approx 1\.43\times \]

Even infinite parallelization of non-labeling work yields only
\textbf{43\% improvement}.

\textbf{The Engineering Implication}: This explains why: 1.
\textbf{AI-assisted labeling matters}: It attacks the serial bottleneck,
not just adds parallelism 2. \textbf{Data selection matters}: Reducing
label requirements by 50\% is equivalent to a 2x speedup in the serial
component 3. \textbf{Active learning matters}: Labeling the ``right''
10\% of data achieves more than labeling 100\% randomly

Parallelizing storage, ingestion, and processing yields diminishing
returns. The path to 10x improvement runs through reducing or automating
the human-in-the-loop component. \textbf{?@sec-data-selection} explores
these optimization strategies systematically, introducing metrics like
the Information-Compute Ratio to quantify data selection gains.

\end{fbx}

Amdahl's Law explains where to focus optimization effort, but production
budgeting requires a comprehensive view of all data engineering costs,
not just labeling. Organizations often underestimate the total
investment required to build and maintain ML datasets. We capture this
full picture through the \emph{total cost of data ownership (TCDO)}.

\phantomsection\label{callout-perspectiveux2a-1.24}
\begin{fbx}{callout-perspective}{Systems Perspective:}{The Total Cost of Data Ownership (TCDO)}
\phantomsection\label{callout-perspective*-1.24}
\textbf{A Unified Cost Model}: While labeling dominates, a complete data
engineering budget requires accounting for all cost components:

\[\text{TCDO} = C_{\text{acquire}} + C_{\text{label}} + C_{\text{store}} + C_{\text{process}} + C_{\text{govern}} + C_{\text{debt}}\]

where:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3333}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Component}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Formula}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Typical Range}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{\(C_{\text{acquire}}\)} & \(N \times c_{\text{source}}\) &
5--15\% of total \\
\textbf{\(C_{\text{label}}\)} &
\(N \times c_{\text{label}} \times (1 + R_{\text{review}}) \times (1 + R_{\text{rework}})\)
& \textbf{40--70\% of total} \\
\textbf{\(C_{\text{store}}\)} &
\(\text{Size} \times c_{\text{tier}} \times T_{\text{retention}}\) &
5--10\% of total \\
\textbf{\(C_{\text{process}}\)} &
\(\text{FLOPs} \times c_{\text{compute}} \times N_{\text{iterations}}\)
& 10--20\% of total \\
\textbf{\(C_{\text{govern}}\)} &
\(c_{\text{compliance}} + c_{\text{audit}} + c_{\text{access}}\) &
5--15\% of total \\
\textbf{\(C_{\text{debt}}\)} & \(\text{Debt}_0 \times (1 + r)^n\) &
0--30\% (hidden) \\
\end{longtable}

\textbf{Worked Example}: Using the 1M-image project with \$138K labeling
cost computed above:

\begin{longtable}[]{@{}lll@{}}
\toprule\noalign{}
\textbf{Component} & \textbf{Cost} & \textbf{\% of Total} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Acquire} & \$50K & 21\% \\
\textbf{Label} & \$138K & \textbf{59\%} \\
\textbf{Store} & \$15K & 6\% \\
\textbf{Process} & \$10K & 4\% \\
\textbf{Govern} & \$20K & 9\% \\
\textbf{Total} & \$233K & 100\% \\
\end{longtable}

\textbf{The Optimization Priority}: Labeling dominates at 59\%, nearly
15x the compute cost. Teams optimizing GPU utilization while ignoring
labeling efficiency are optimizing the wrong 4\%.

\end{fbx}

Given labeling's dominance in the TCDO model, understanding the factors
that drive labeling costs becomes essential for project planning. The
cost per label varies dramatically by task complexity and required
expertise. Simple image classification ranges from \$0.01-0.05 per label
when crowdsourced but rises to \$0.50-2.00 when requiring expert
verification. Bounding boxes cost \$0.05-0.20 per box for
straightforward cases but \$1.00-5.00 for dense scenes with many
overlapping objects. Semantic segmentation can reach \$5-50 per image
depending on precision requirements and object boundaries. Medical image
annotation by radiologists costs \$50-200 per study. When a computer
vision system requires 10 million labeled images, the difference between
\$0.02 and \$0.05 per label represents \$300,000 in project
costs---often more than the entire infrastructure budget yet frequently
discovered only after labeling begins.

\subsection{Scaling with AI-Assisted
Labeling}\label{sec-data-engineering-ml-scaling-aiassisted-labeling-9360}

As labeling demands grow exponentially with modern ML systems,
scalability becomes critical. The scalability pillar drives AI
assistance as a force multiplier for human labeling rather than a
replacement. Manual annotation alone cannot keep pace with modern ML
systems' data needs, while fully automated labeling lacks the nuanced
judgment that humans provide. AI-assisted labeling finds the sweet spot:
using automation to handle clear cases and accelerate annotation while
preserving human judgment for ambiguous or high-stakes decisions.
Figure~\ref{fig-weak-supervision} illustrates several paths AI
assistance offers to scale labeling operations, each requiring careful
system design to balance speed, quality, and resource usage.

\begin{figure}[htb]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/236d876e8e37cb9689a59172017db7a4e5851c02.pdf}}

}

\caption{\label{fig-weak-supervision}\textbf{AI-Augmented Labeling
Decision Hierarchy}: A top-level question about obtaining labeled data
branches into four paths: traditional supervision, semi-supervised
learning, weak supervision, and transfer learning, with active learning
as a cost-saving alternative. Lower-cost strategies trade labeling
precision for throughput. Source: Stanford AI Lab.}

\end{figure}%

Modern AI-assisted labeling typically employs a combination of
approaches working together in the pipeline. Pre-annotation involves
using AI models to generate preliminary labels for a dataset, which
humans can then review and correct. Major labeling platforms have made
significant investments in this technology.
\href{https://snorkel.ai/}{Snorkel AI} uses programmatic labeling
(\citeproc{ref-ratner2018snorkel}{Ratner et al. 2018}) to automatically
generate initial labels at scale through rule-based heuristics and weak
supervision\sidenote{\textbf{Weak supervision}: A machine learning
paradigm that uses imperfect or approximate labels rather than manual
annotation. Sources include heuristic rules (pattern matching),
knowledge bases (dictionary lookups), existing models (pre-trained
classifiers), and distant supervision (aligning with external data).
Frameworks like Snorkel combine multiple weak sources through
noise-aware learning to produce training labels approaching expert
quality at a fraction of the cost. } signals.

Scale AI deploys pre-trained models to accelerate annotation in specific
domains like autonomous driving, where object detection models pre-label
vehicles and pedestrians that humans then verify and refine. Companies
like \href{https://www.superannotate.com/}{SuperAnnotate} provide
automated pre-labeling tools that can reduce manual effort by 50-80\%
for computer vision tasks. This method, which often employs
semi-supervised learning techniques
(\citeproc{ref-chapelle2009semisupervised}{Chapelle, Scholkopf, and Zien
2009}), can save significant time, especially for extremely large
datasets.

The emergence of Large Language Models (LLMs) like ChatGPT has further
transformed labeling pipelines. Beyond simple classification, LLMs can
generate rich text descriptions, create labeling guidelines from
examples, and even explain their reasoning for label assignments. For
instance, content moderation systems use LLMs to perform initial content
classification and generate explanations for policy violations that
human reviewers can validate. However, integrating LLMs introduces new
system challenges around inference costs (API calls can cost \$0.01-\$1
per example depending on complexity), rate limiting (cloud APIs
typically limit to 100-10,000 requests per minute), and output
validation (LLMs occasionally produce confident but incorrect labels
requiring systematic validation). Many organizations adopt a tiered
approach, using smaller specialized models for routine cases while
reserving larger LLMs for complex scenarios requiring nuanced judgment
or rare domain expertise.

Methods such as active learning\sidenote{\textbf{Active Learning}: The
term contrasts with ``passive'' learning where models receive randomly
selected training data. Formalized by David Cohn, Les Atlas, and Richard
Ladner (1994), active learning inverts the traditional paradigm: instead
of the data determining what the model learns, the model queries for
specific examples it needs most. Selection strategies target uncertain
examples (uncertainty sampling) or those maximizing expected information
gain. Active learning achieves target accuracy with 50-90\% fewer labels
than random sampling, addressing the labeling bottleneck in data
engineering. } complement these approaches by intelligently prioritizing
which examples need human attention
(\citeproc{ref-coleman2022similarity}{Coleman et al. 2022}). These
systems continuously analyze model uncertainty to identify valuable
labeling candidates. Rather than labeling a random sample of unlabeled
data, active learning selects examples where the current model is most
uncertain or where labels would most improve model performance. The
infrastructure must efficiently compute uncertainty metrics (often
prediction entropy or disagreement between ensemble models), maintain
task queues ordered by informativeness, and adapt prioritization
strategies based on incoming labels. Consider a medical imaging system:
active learning might identify unusual pathologies for expert review
while handling routine cases through pre-annotation that experts merely
verify. This approach can reduce required annotations by 50-90\%
compared to random sampling, though it requires careful engineering to
prevent feedback loops where the model's uncertainty biases which data
gets labeled. The following analysis quantifies this \emph{active
learning multiplier} in concrete budget terms.

\phantomsection\label{callout-notebookux2a-1.25}
\begin{fbx}{callout-notebook}{AI Engineer’s Notebook:}{The Active Learning Multiplier}
\phantomsection\label{callout-notebook*-1.25}
\textbf{Problem}: You have a 10M image dataset and a \$50K labeling
budget. Random sampling achieves 85\% accuracy with 100K images. You
need 95\% accuracy.

\textbf{The Physics}:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Sample Efficiency}: Active learning typically achieves target
  accuracy with \textbf{5--10× fewer samples} than random selection.
\item
  \textbf{Cost per Point}: Random sampling = \$0.50/label. Active
  learning adds compute cost (\textasciitilde\$0.01/image for inference)
  to find hard examples.
\item
  \textbf{The Multiplier}:

  \begin{itemize}
  \tightlist
  \item
    \textbf{Random}: To reach 95\%, you might need 1M labels (\$500K).
    \textbf{Budget Exceeded.}
  \item
    \textbf{Active}: You need \textasciitilde100K--200K \emph{hard}
    examples (\$50K--\$100K). \textbf{Feasible.}
  \end{itemize}
\end{enumerate}

\textbf{The Engineering Conclusion}: Algorithm choice is a 10× lever on
data budget. Spending 10\% of your budget on compute to select data
saves 90\% of your budget on human labeling.

\end{fbx}

Quality control becomes increasingly important as these AI components
interact. The system must monitor both AI and human performance through
systematic metrics. Model confidence calibration matters: if the AI says
it's 95\% confident but is actually only 75\% accurate at that
confidence level, pre-annotations mislead human reviewers. Human-AI
agreement rates reveal whether AI assistance helps or hinders: when
humans frequently override AI suggestions, the pre-annotations may be
introducing bias rather than accelerating work. These metrics require
careful instrumentation throughout the labeling pipeline, tracking not
just final labels but the interaction between human and AI at each
stage.

In safety-critical domains like self-driving cars, these systems must
maintain particularly rigorous standards while processing massive
streams of sensor data. Waymo's labeling infrastructure reportedly
processes millions of sensor frames daily, using AI pre-annotation to
label common objects (vehicles, pedestrians, traffic signs) while
routing unusual scenarios (construction zones, emergency vehicles,
unusual road conditions) to human experts. The system must maintain
real-time performance despite this scale, using distributed
architectures where pre-annotation runs on GPU clusters while human
review scales horizontally across thousands of annotators, with careful
load balancing ensuring neither component becomes a bottleneck.

Real-world deployments demonstrate these principles at scale in diverse
domains. Medical imaging systems
(\citeproc{ref-krishnan2022selfsupervised}{Krishnan, Rajpurkar, and
Topol 2022}) combine pre-annotation for common conditions (identifying
normal tissue, standard anatomical structures) with active learning for
unusual cases (rare pathologies, ambiguous findings), all while
maintaining strict patient privacy through secure annotation platforms
with comprehensive audit trails. Self-driving vehicle systems coordinate
multiple AI models to label diverse sensor data: one model pre-labels
camera images, another handles lidar point clouds, a third processes
radar data, with fusion logic combining predictions before human review.
Social media platforms process millions of items hourly using tiered
approaches where simpler models handle clear violations (spam, obvious
hate speech) while complex content routes to more sophisticated models
or human reviewers when initial classification is uncertain.

\subsection{Ensuring Ethical and Fair
Labeling}\label{sec-data-engineering-ml-ensuring-ethical-fair-labeling-73e8}

Unlike previous sections where governance focused on data and processes,
labeling governance centers on human welfare. The governance pillar here
addresses ethical treatment of human contributors, bias mitigation, and
fair compensation---challenges that manifest distinctly from governance
in automated pipeline stages because human welfare is directly at stake.
While governance in processing focuses on data lineage and compliance,
governance in labeling requires ensuring that the humans creating
training data are treated ethically, compensated fairly, and protected
from harm.

The ethical sourcing challenges discussed in
Section~\ref{sec-data-engineering-ml-governance-ethics-sourcing-2d7f}
apply with equal force to labeling operations, where workers face direct
exposure to the content they annotate. Fair compensation, worker
wellbeing, and transparency are particularly acute concerns when
annotators must review sensitive or traumatic material for extended
periods. Organizations such as \href{https://scale.com}{Scale AI} have
responded by implementing structured support: rotating annotators
through different content types, capping hours per day on disturbing
material, providing access to counseling services, and offering
immediate support channels for particularly distressing content. These
measures add operational cost but are essential for ethical operations.
Compensation for sensitive content moderation should reflect the
psychological burden involved, often warranting premium pay well above
base annotation rates.

Beyond working conditions, bias in data labeling represents another
critical governance concern. Annotators bring their own cultural,
personal, and professional biases to the labeling process, which can be
reflected in the resulting dataset. For example, Wang et al.
(\citeproc{ref-wang2019balanced}{2019}) found that image datasets
labeled predominantly by annotators from one geographic region showed
biases in object recognition tasks, performing poorly on images from
other regions. This highlights the need for diverse annotator pools
where demographic diversity among annotators helps counteract individual
biases, though it does not eliminate them. Regular bias audits examining
whether label distributions differ systematically across annotator
demographics, monitoring for patterns suggesting systematic bias (all
images from certain regions receiving lower quality scores), and
addressing identified biases through additional training or guideline
refinement ensure labels support fair model behavior.

Data privacy and ethical considerations also pose challenges in data
labeling. Leading data labeling companies have developed specialized
solutions for these challenges. Scale AI, for instance, maintains
dedicated teams and secure infrastructure for handling sensitive data in
healthcare and finance, with HIPAA-compliant annotation platforms and
strict data access controls. Appen implements strict data access
controls and anonymization protocols, ensuring annotators never see
personally identifiable information when unnecessary. Labelbox offers
private cloud deployments for organizations with strict security
requirements, enabling annotation without data leaving organizational
boundaries. These privacy-preserving techniques connect directly to the
security considerations explored in a companion book, where
comprehensive approaches to protecting sensitive data throughout the ML
lifecycle are examined.\sidenote{\textbf{Security and Privacy in ML
Systems}: A companion book dedicates a full chapter to security and
privacy, building upon the data governance foundations established here.
Topics include differential privacy, secure multi-party computation,
federated learning privacy guarantees, adversarial attacks on data
pipelines, and comprehensive protection strategies for ML systems. }

Beyond privacy and working conditions, the dynamic nature of real-world
data presents another limitation. Labels that are accurate at the time
of annotation may become outdated or irrelevant as the underlying
distribution of data changes over time. This concept, known as concept
drift, necessitates ongoing labeling efforts and periodic re-evaluation
of existing labels. Governance frameworks must account for label
versioning (tracking when labels were created and by whom),
re-annotation policies (systematically re-labeling data when concepts
evolve), and retirement strategies (identifying when old labels should
be deprecated rather than used for training).

Finally, the limitations of current labeling approaches become apparent
when dealing with edge cases or rare events. In many real-world
applications, the unusual or rare instances are often most critical
(e.g., rare diseases in medical diagnosis, or unusual road conditions in
autonomous driving). However, these cases are, by definition,
underrepresented in most datasets and may be overlooked or mislabeled in
large-scale annotation efforts. Governance requires explicit strategies
for handling rare events: targeted collection campaigns for
underrepresented scenarios, expert review requirements for rare cases,
and systematic tracking ensuring rare events receive appropriate
attention despite their low frequency.

These governance considerations underscore that training data is not
just bits and bytes but the product of human labor deserving respect,
fair compensation, and ethical treatment. Organizations must prioritize
the wellbeing of contributors as they build the datasets that drive ML
innovation, recognizing that ethical responsibilities scale alongside
the growing demand for annotated data.

\subsection{Case Study: Automated Labeling in KWS
Systems}\label{sec-data-engineering-ml-case-study-automated-labeling-kws-systems-976d}

Continuing our KWS case study through the labeling stage---having
established systematic problem definition
(Section~\ref{sec-data-engineering-ml-framework-application-keyword-spotting-case-study-c8e9}),
diverse data collection strategies that address quality and coverage
requirements, ingestion patterns handling both batch and streaming
workflows, and processing pipelines ensuring training-serving
consistency---we now confront a challenge unique to speech systems at
scale. Generating millions of labeled wake word samples without
proportional human annotation cost requires moving beyond the manual and
crowdsourced approaches we examined earlier. The Multilingual Spoken
Words Corpus (MSWC) (\citeproc{ref-mazumder2021multilingual}{Mazumder et
al. 2021}) demonstrates how automated labeling addresses this challenge
through its innovative approach to generating labeled wake word data,
containing over 23.4 million one-second spoken examples across 340,000
keywords in 50 different languages.

This scale directly reflects our framework pillars in practice.
Achieving our quality target of 98\% accuracy across diverse
environments requires millions of training examples covering acoustic
variations we identified during problem definition. Reliability demands
representation across varied acoustic conditions---different background
noises, speaking styles, and recording environments. Scalability
necessitates automation rather than manual labeling because 23.4 million
examples would require approximately 2,600 person-years of effort at
even 10 seconds per label, making manual annotation economically
infeasible. Governance requirements mandate transparent sourcing and
language diversity, ensuring voice-activated technology serves speakers
of many languages rather than concentrating on only the most
commercially valuable markets.

Figure~\ref{fig-mswc} depicts this automated system, which begins with
paired sentence audio recordings and corresponding transcriptions from
projects like \href{https://commonvoice.mozilla.org/en}{Common Voice} or
multilingual captioned content platforms, processing these inputs
through forced alignment\sidenote{\textbf{Forced alignment}: The term
``forced'' distinguishes this from ``free'' alignment where the system
must also recognize what was said. In forced alignment, the
transcription is known, so the algorithm is ``forced'' to align specific
words to the audio rather than choosing among alternatives. Developed
alongside early speech recognition research in the 1970s, forced
alignment uses dynamic programming (Viterbi algorithm) to compute
optimal alignment paths matching phonetic sequences to audio frames with
millisecond precision. Tools like Montreal Forced Aligner enable
extracting individual keywords for KWS training. } to identify precise
word boundaries within continuous speech.

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{contents/vol1/data_engineering/images/png/data_engineering_kws2.png}}

}

\caption{\label{fig-mswc}\textbf{Multilingual Data Preparation}: Forced
alignment and segmentation transform paired audio-text data into labeled
one-second segments, creating a large-scale corpus for training keyword
spotting models across 50+ languages. This automated process enables
scalable development of KWS systems by efficiently generating training
examples from readily available speech resources like common voice and
multilingual captioned content.}

\end{figure}%

Building on these precise timing markers, the extraction system
generates clean keyword samples while handling engineering challenges
our problem definition anticipated: background noise interfering with
word boundaries, speakers stretching or compressing words unexpectedly
beyond our target 500-800 millisecond duration, and longer words
exceeding the one-second boundary. MSWC provides automated quality
assessment that analyzes audio characteristics to identify potential
issues with recording quality, speech clarity, or background noise,
which is essential for maintaining consistent standards across 23
million samples without the manual review expenses that would make this
scale prohibitive.

Modern voice assistant developers often build upon this automated
labeling foundation. While automated corpora may not contain the
specific wake words a product requires, they provide starting points for
KWS prototyping, particularly in underserved languages where commercial
datasets do not exist. Production systems typically layer targeted human
recording and verification for challenging cases (unusual accents, rare
words, or difficult acoustic environments that automated systems
struggle with), requiring infrastructure that gracefully coordinates
between automated processing and human expertise. This demonstrates how
the four pillars guide integration: quality through targeted human
verification, reliability through automated consistency, scalability
through forced alignment, and governance through transparent sourcing
and multilingual coverage.

The sophisticated orchestration of forced alignment, extraction, and
quality control demonstrates how thoughtful data engineering directly
impacts production machine learning systems. When a voice assistant
responds to its wake word, it draws upon this labeling infrastructure
combined with the collection strategies, pipeline architectures, and
processing transformations we have examined throughout this chapter.

\section{Strategic Storage
Architecture}\label{sec-data-engineering-ml-strategic-storage-architecture-1a6b}

The carefully labeled datasets emerging from our automated and
human-in-the-loop processes (23 million audio samples spanning 50
languages in our KWS system) now require strategic storage decisions
that determine training efficiency, serving latency, and long-term
maintainability. While pipeline architecture addressed data flow and
transformation, storage architecture addresses the complementary
question: where does this data reside, how is it organized, and how do
we optimize for fundamentally different access patterns? Batch training
requires sequential scans across millions of examples, while real-time
serving demands millisecond lookups of individual feature vectors. These
competing requirements shape every storage decision.

Storage decisions determine how effectively we can maintain data quality
over time, ensure reliable access under varying loads, scale to handle
growing data volumes, and implement governance controls. The seemingly
straightforward question of ``where should we store this data''
encompasses complex trade-offs between access patterns, cost
constraints, consistency requirements, and performance characteristics
that fundamentally shape how ML systems operate.

ML storage requirements differ fundamentally from transactional systems
that power traditional applications. Rather than optimizing for frequent
small writes and point lookups that characterize e-commerce or banking
systems, ML workloads prioritize high-throughput sequential reads over
frequent writes, large-scale scans over row-level updates, and schema
flexibility over rigid structures. A database serving an e-commerce
application performs well with millions of individual product lookups
per second, but an ML training job that needs to scan that entire
product catalog repeatedly across training epochs requires completely
different storage optimization. This section examines matching storage
architectures to ML workload characteristics, comparing databases, data
warehouses, and data lakes before exploring specialized ML
infrastructure like feature stores and examining how storage
requirements evolve across the ML lifecycle.

\subsection{ML Storage Systems Architecture
Options}\label{sec-data-engineering-ml-ml-storage-systems-architecture-options-67fa}

Storage system selection extends beyond capacity planning. The goal is
minimizing the \textbf{Data Term}
(\(\frac{\text{Data}}{\text{Bandwidth}}\)) in the Iron Law of ML
Systems. Every storage medium imposes physical constraints on bandwidth
that determine the maximum speed of your training and serving pipelines.

Optimizing this term requires understanding two fundamental storage
performance metrics:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{IOPS (Input/Output Operations Per Second)}: The number of
  distinct read/write requests a device can handle per second. This
  limits performance for \textbf{random access} workloads (e.g.,
  fetching small batches of images or individual user profiles).
\item
  \textbf{Throughput (Bandwidth)}: The volume of data transferred per
  second, typically \(\text{IOPS} \times \text{Block Size}\). This
  limits performance for \textbf{sequential access} workloads (e.g.,
  scanning a Parquet file for training).
\end{enumerate}

The choice between databases, data warehouses, and data lakes is
fundamentally a choice about which of these metrics to optimize:

\begin{itemize}
\tightlist
\item
  \textbf{Databases (OLTP)}: Optimize for high IOPS with small block
  sizes (random access). Suited for serving individual feature vectors
  in real-time where per-request latency dominates.
\item
  \textbf{Data Warehouses (OLAP)}: Optimize for high Throughput with
  large block sizes (sequential access). Ideal for feature engineering
  and batch analytics.
\item
  \textbf{Data Lakes}: Prioritize capacity and throughput for
  unstructured data. Essential for training jobs where the ``Data''
  numerator is measured in petabytes and aggregate bandwidth must scale
  to thousands of GPUs.
\end{itemize}

Each storage architecture exhibits distinct strengths when applied to
specific ML tasks. For online feature serving, the high-IOPS
characteristics of databases enable millisecond lookups of individual
records. A recommendation system looking up a user's profile during
real-time inference exemplifies this pattern: fetching specific user
features (age, location, preferences) to generate personalized
recommendations requires random access optimized for per-request
latency.

For model training on structured data, the throughput-optimized design
of data warehouses enables high-speed sequential scans over large, clean
tables. Training a fraud detection model that processes millions of
transactions with hundreds of features per transaction benefits from
columnar storage that reads only relevant features efficiently, directly
reducing the Data Term by minimizing bytes transferred.

For exploratory analysis and training on unstructured data (images,
audio, text), data lakes provide the flexibility and low-cost storage
needed for massive volumes. A computer vision system storing terabytes
of raw images alongside metadata, annotations, and intermediate
processing results requires the schema flexibility and cost efficiency
that only data lakes provide, where the sheer scale of the Data
numerator demands the highest aggregate bandwidth.

Databases excel at operational and transactional purposes, maintaining
product catalogs, user profiles, or transaction histories with strong
consistency guarantees and low-latency point lookups. For ML workflows,
databases serve specific roles well: storing feature metadata that
changes frequently, managing experiment tracking where transactional
consistency matters, or maintaining model registries that require atomic
updates. A PostgreSQL database handling structured user attributes
(user\_id, age, country, preferences) provides millisecond lookups for
serving systems that need individual user features in real-time.
However, databases struggle when ML training requires scanning millions
of records repeatedly across multiple epochs. The row-oriented storage
that optimizes transactional lookups becomes inefficient when training
needs only 20 of 100 columns from each record but must read entire rows
to extract those columns.

Data warehouses fill this analytical gap, optimized for complex queries
across integrated datasets transformed into standardized schemas. Modern
warehouses like Google BigQuery, Amazon Redshift, and Snowflake use
columnar storage formats
(\citeproc{ref-stonebraker2005cstore}{Stonebraker et al. 2018}) that
enable reading specific features without loading entire records,
essential when tables contain hundreds of columns but training needs
only a subset. This columnar organization delivers five to ten times I/O
reduction compared to row-based formats for typical ML workloads.
Consider a fraud detection dataset with 100 columns where models
typically use 20 features---columnar storage reads only needed columns,
achieving 80\% I/O reduction before even considering compression. Many
successful ML systems draw training data from warehouses because the
structured environment simplifies exploratory analysis and iterative
development. Data analysts can quickly compute aggregate statistics,
identify correlations between features, and validate data quality using
familiar SQL interfaces.

However, warehouses assume relatively stable schemas and struggle with
truly unstructured data (images, audio, free-form text) or rapidly
evolving formats common in experimental ML pipelines. When a computer
vision team wants to store raw images alongside extracted features,
multiple annotation formats from different labeling vendors,
intermediate model predictions, and embedding vectors, forcing all these
into rigid warehouse schemas creates more friction than value. Schema
evolution becomes painful: adding new feature types requires ALTER TABLE
operations that may take hours on large datasets, blocking other
operations and slowing iteration velocity.

Data lakes address these limitations by storing structured,
semi-structured, and unstructured data in native formats, deferring
schema definitions until the point of reading, a pattern called
schema-on-read.\sidenote{\textbf{Schema-on-read}: The word ``schema''
derives from Greek ``skhema'' meaning shape, form, or plan. In
computing, it describes the structure that defines how data is
organized. Schema-on-read applies this structure at query time rather
than during ingestion, contrasting with schema-on-write (traditional
databases) where data must conform to a predefined structure before
storage. This paradigm enables flexibility for evolving data formats but
requires careful metadata management for discoverability. }

This flexibility proves valuable during early ML development when teams
experiment with diverse data sources and are not yet certain which
features will prove useful. A recommendation system might store in the
same data lake: transaction logs as JSON, product images as JPEGs, user
reviews as text files, clickstream data as Parquet, and model embeddings
as NumPy arrays. Rather than forcing these heterogeneous types into a
common schema upfront, the data lake preserves them in their native
formats. Applications impose schema only when reading, enabling
different consumers to interpret the same data differently: one team
extracts purchase amounts from transaction logs while another analyzes
temporal patterns, each applying schemas suited to their analysis.

This flexibility comes with serious governance challenges. Without
disciplined metadata management and cataloging, data lakes degrade into
``data swamps,'' disorganized repositories where finding relevant data
becomes nearly impossible, undermining the productivity benefits that
motivated their adoption. A data lake might contain thousands of
datasets across hundreds of directories with names like
``userdata\_v2\_final'' and ``userdata\_v2\_final\_ACTUALLY\_FINAL'',
where only the original authors (who have since left the company)
understand what distinguishes them. Successful data lake implementations
maintain searchable metadata about data lineage, quality metrics, update
frequencies, ownership, and access patterns, essentially providing
warehouse-like discoverability over lake-scale data. Tools like AWS Glue
Data Catalog, Apache Atlas, or Databricks Unity Catalog provide this
metadata layer, enabling teams to discover and understand data before
investing effort in processing it.

Table~\ref{tbl-storage} summarizes these essential trade-offs, comparing
databases, warehouses, and data lakes across purpose, data types, scale,
and performance optimization.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2500}}@{}}
\caption{\textbf{Storage System Characteristics}: Different storage
systems suit distinct stages of machine learning workflows based on data
structure and purpose; databases manage transactional data, data
warehouses support analytical reporting, and data lakes accommodate
diverse, raw data for future processing. Understanding these
characteristics enables efficient data management and supports the
scalability of machine learning
applications.}\label{tbl-storage}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Attribute}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Conventional Database}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Data Warehouse}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Data Lake}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Attribute}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Conventional Database}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Data Warehouse}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Data Lake}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Purpose} & Operational and transactional & Analytical and
reporting & Storage for raw and diverse data for future processing \\
\textbf{Data type} & Structured & Structured & Structured,
semi-structured, and unstructured \\
\textbf{Scale} & Small to medium volumes & Medium to large volumes &
Large volumes of diverse data \\
\textbf{Performance Optimization} & Optimized for transactional queries
(OLTP) & Optimized for analytical queries (OLAP) & Optimized for
scalable storage and retrieval \\
\textbf{Examples} & MySQL, PostgreSQL, Oracle DB & Google BigQuery,
Amazon Redshift, Microsoft Azure Synapse & Google Cloud Storage, AWS S3,
Azure Data Lake Storage \\
\end{longtable}

Choosing appropriate storage requires systematic evaluation of workload
requirements rather than following technology trends. Databases are a
strong fit when data volume is modest, query patterns involve frequent
updates and complex joins, latency requirements demand subsecond
response, and strong consistency is required. A user profile store
serving real-time recommendations exemplifies this pattern: small
per-user records measured in kilobytes, frequent reads and writes as
preferences update, strict consistency ensuring users see their own
updates immediately, and tight latency requirements. Databases become
inadequate when analytical queries must span large datasets requiring
table scans, schema evolution occurs frequently as feature requirements
change, or storage cost becomes a dominant driver and cheaper
alternatives become economically compelling.

Data warehouses excel when data volumes span one to 100 terabytes,
analytical query patterns dominate transactional operations, batch
processing latency measured in minutes to hours is acceptable, and
structured data with relatively stable schemas represents the primary
workload. Model training data preparation, batch feature engineering,
and historical analysis fit this profile. The migration path from
databases to warehouses typically occurs when query complexity
increases---requiring aggregations or joins across tables totaling
gigabytes rather than megabytes---or when analytical workloads start
degrading transactional system performance. Warehouses become inadequate
when real-time streaming ingestion is required with latency measured in
seconds, or when unstructured data comprises more than 20\% of
workloads, as warehouse schema rigidity creates excessive friction for
heterogeneous data.

Data lakes become essential when data volumes exceed 100 terabytes,
schema flexibility is critical for evolving data sources or experimental
features, cost optimization is a primary concern (often 10 times cheaper
than warehouses at scale), and diverse data types must coexist.
Large-scale model training, particularly for multimodal systems
combining text, images, audio, and structured features, requires data
lake flexibility. Consider a self-driving car system storing: terabytes
of camera images and lidar point clouds from test vehicles, vehicle
telemetry as time-series data, manually-labeled annotations identifying
objects and behaviors, automatically-generated synthetic data for rare
scenarios, and model predictions for comparison against ground truth.
Forcing these diverse types into warehouse schemas would require
substantial transformation effort and discard nuances that native
formats preserve. However, data lakes demand sophisticated catalog
management and metadata governance to prevent quality degradation---the
critical distinction between a productive data lake and an unusable data
swamp.

Migration patterns between storage types follow predictable trajectories
as ML systems mature and scale. Early-stage projects often start with
databases, drawn by familiar SQL interfaces and existing organizational
infrastructure. As datasets grow beyond database efficiency thresholds
or analytical queries start affecting operational performance, teams
migrate to warehouses. The warehouse serves well during stable
production phases with established feature pipelines and relatively
fixed schemas. When teams need to incorporate new data types (images for
computer vision augmentation, unstructured text for natural language
features, or audio for voice applications) or when cost optimization
becomes critical at terabyte or petabyte scale, migration to data lakes
occurs. Mature ML organizations typically employ all three storage types
orchestrated through unified data catalogs: databases for operational
data and real-time serving, warehouses for curated analytical data and
feature engineering, and data lakes for raw heterogeneous data and
large-scale training datasets.

\subsection{ML Storage Requirements and
Performance}\label{sec-data-engineering-ml-ml-storage-requirements-performance-1b0d}

Beyond the functional differences between storage systems, cost and
performance characteristics directly impact ML system economics and
iteration speed. Understanding these quantitative trade-offs enables
informed architectural decisions based on workload requirements.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2000}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2000}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2000}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2000}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2000}}@{}}
\caption{\textbf{Storage Cost-Performance Trade-offs}: Different storage
tiers provide distinct cost-performance characteristics that determine
their suitability for specific ML workloads. Training data loading
requires high-throughput sequential access, online serving needs
low-latency random reads, while archival storage prioritizes cost over
access speed for compliance and historical
data.}\label{tbl-storage-performance}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Storage Tier}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Cost (\$/TB/month)}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Sequential Read} \textbf{Throughput}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Random Read} \textbf{Latency}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Typical ML Use Case}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Storage Tier}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Cost (\$/TB/month)}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Sequential Read} \textbf{Throughput}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Random Read} \textbf{Latency}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Typical ML Use Case}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{NVME SSD (local)} & \$100--300 & 5--7 GB/s & 10--100 μs &
Training data loading, active feature serving \\
\textbf{Object Storage} & \$20--25 & 100--500 MB/s & 10--50 ms & Data
lake raw storage, \\
\textbf{(S3, GCS)} & & (per connection) & & model artifacts \\
\textbf{Data Warehouse} & \$20--40 & 1--5 GB/s & 100--500 ms & Training
data queries, \\
\textbf{(BigQuery, Redshift)} & & (columnar scan) & (query startup) &
feature engineering \\
\textbf{In-Memory Cache} & \$500--1000 & 20--50 GB/s & 1--10 μs & Online
feature serving, \\
\textbf{(Redis, Memcached)} & & & & real-time inference \\
\textbf{Archival Storage} & \$1--4 & 10--50 MB/s & Hours (retrieval) &
Historical retention, \\
\textbf{(Glacier, Nearline)} & & (after retrieval) & & compliance
archives \\
\end{longtable}

Table~\ref{tbl-storage-performance} reveals why ML systems employ tiered
storage architectures. Consider the economics of storing our KWS
training dataset (736 GB): object storage costs \$15--18/month, enabling
affordable long-term retention of raw audio, while maintaining working
datasets on NVMe\sidenote{\textbf{Non-Volatile Memory Express (NVMe)}: A
storage protocol designed specifically for flash memory, bypassing the
legacy AHCI interface that SATA SSDs use. NVMe connects directly to the
PCIe bus, enabling 64K command queues versus SATA's single queue,
reducing latency from milliseconds to microseconds. For ML training
workloads, NVMe's 5-7 GB/s sequential throughput prevents storage from
bottlenecking GPU utilization, while SATA SSD's 500 MB/s limit would
leave expensive accelerators idle waiting for data. } for active
training costs \(74–220/month but provides 50\)\times\$ faster data
loading.

The performance difference directly impacts iteration velocity. Training
that loads data at 5 GB/s completes dataset loading in 147 seconds,
compared to 7,360 seconds at typical object storage speeds. This
50\(\times\) difference determines whether teams can iterate multiple
times daily or must wait hours between experiments.

To build engineering judgment, practitioners must internalize the orders
of magnitude separating these tiers. Table~\ref{tbl-ml-latencies}
translates these disparities into human-scale analogies: if a CPU cycle
were one second, fetching from local SSD would take two days, while a
cross-country network request would span six years. These are the
``numbers every ML systems engineer should know.''

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2500}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2500}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2500}}@{}}
\caption{\textbf{Latency Numbers Every ML Systems Engineer Should Know}:
Understanding the quantitative disparities in the storage hierarchy is
essential for diagnosing bottlenecks. If a CPU cycle were 1 second,
fetching data from local SSD would be like waiting 2 days, while a
cross-country network request would take 6 years. Source: Adapted from
Jeff Dean's\sidenote{\textbf{Jeff Dean}: Google Senior Fellow and
co-founder of Google Brain. He is the systems architect behind many of
the technologies that define modern distributed computing, including
MapReduce, BigTable, Spanner, and TensorFlow. His focus on ``systems
that scale'' and ``numbers everyone should know'' established the
quantitative engineering culture that allowed ML to transition from
academic research to planetary-scale deployment. } ``Numbers Every
Programmer Should Know''.}\label{tbl-ml-latencies}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Operation}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Latency (ns)}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Human Scale}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{ML System Impact}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Operation}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Latency (ns)}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Human Scale}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{ML System Impact}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{L1 Cache Reference} & 0.5 & 1 second & Immediate \\
\textbf{L2 Cache Reference} & 7 & 14 seconds & Fast computation \\
\textbf{Main Memory (DRAM)} & 100 & 3 minutes & The ``Memory Wall''
threshold \\
\textbf{SSD (local NVMe)} & 100,000 & 2 days & Data loading
bottleneck \\
\textbf{Network (same DC)} & 500,000 & 1 week & Distributed coordination
lag \\
\textbf{SSD (remote network)} & 2,000,000 & 1 month & Training-serving
skew source \\
\textbf{Object Store (S3)} & 20,000,000 & 1 year & Archival access \\
\textbf{Internet (CA to VA)} & 100,000,000 & 6 years & Global user
experience \\
\end{longtable}

Beyond the core storage capabilities we have examined, ML workloads
introduce unique requirements that conventional databases and warehouses
were not designed to handle. Understanding these ML-specific needs and
their performance implications shapes infrastructure decisions that
cascade through the entire development lifecycle, from experimental
notebooks to production serving systems handling millions of requests
per second.

Modern ML models contain millions to billions of parameters requiring
efficient storage and retrieval patterns quite different from
traditional data. GPT-3 (\citeproc{ref-brown2020language}{Brown et al.
2020}) requires approximately 700 gigabytes for model weights when
stored in FP32 format (175 billion parameters times 4 bytes), though
practical deployments typically use FP16 (350 GB) or quantized formats
for reduced storage and faster inference. Even at FP16 precision, this
exceeds many organizations' entire operational databases. The trajectory
reveals accelerating scale: from AlexNet's 60 million parameters in 2012
(\citeproc{ref-krizhevsky2012imagenet}{Krizhevsky, Sutskever, and Hinton
2017}) to GPT-3's 175 billion parameters in 2020, model size grew
approximately 2,900-fold in eight years (60M to 175B parameters).
Storage systems must handle these dense numerical arrays efficiently for
both capacity and access speed. Unlike typical files where sequential
organization matters for readability, model weights benefit from
block-aligned storage enabling parallel reads across parameter groups.
When multiple GPUs need to read model data from shared storage, whether
during training initialization or checkpoint loading, storage systems
must deliver aggregate bandwidth approaching network interface limits,
often 25 Gbps or higher, without introducing bottlenecks that would idle
expensive compute resources.

The iterative nature of ML development introduces versioning
requirements qualitatively different from traditional software. While
Git excels at tracking code changes where files are predominantly text
with small incremental modifications, it fails for large binary files
where even small model changes result in entirely new checkpoints.
Storing 10 versions of a 10 GB model naively would consume 100 GB, but
most ML versioning systems store only deltas between versions, reducing
storage proportionally to how much models actually change. Tools like
DVC (Data Version Control) and MLflow maintain pointers to model
artifacts rather than storing copies, enabling efficient versioning
while preserving the ability to reproduce any historical model. A
typical ML project generates hundreds of model versions during
hyperparameter tuning---one version per training run as engineers
explore learning rates, batch sizes, architectures, and regularization
strategies. Without systematic versioning capturing training
configuration, accuracy metrics, and training data version alongside
model weights, reproducing results becomes impossible when yesterday's
model performed better than today's but teams cannot identify which
configuration produced it. This reproducibility challenge connects
directly to the governance requirements
Section~\ref{sec-data-engineering-ml-governance-observability-2c05}
examines where regulatory compliance often requires demonstrating
exactly which data and process produced specific model predictions.

Large-scale training generates substantial intermediate data requiring
storage systems to handle concurrent read/write operations efficiently.
When training jobs use multiple GPUs, each processing unit works on
different portions of data, requiring storage systems to handle many
simultaneous reads and writes. The specific patterns depend on the
parallelization strategy employed, which \textbf{?@sec-ai-training}
examines in detail. From a storage perspective, systems must handle
concurrent I/O at rates proportional to the number of processing units,
with each potentially writing tens to hundreds of megabytes of
intermediate results during model updates. Memory optimization
strategies that trade computation for storage space reduce memory
requirements but increase storage I/O as intermediate values write to
disk. Storage systems must provide low-latency access to support
efficient coordination. If workers spend more time waiting for storage
than performing computations, parallel processing becomes
counterproductive regardless of the specific training approach used.

The bandwidth hierarchy fundamentally constrains ML system design,
creating bottlenecks that no amount of compute optimization can
overcome. While RAM delivers 50 to 200 gigabytes per second bandwidth on
modern servers, network storage systems typically provide only one to 10
gigabytes per second, and even high-end NVMe SSDs max out at one to
seven gigabytes per second sequential throughput. Modern GPUs can
process data faster than storage can supply it, creating scenarios where
expensive accelerators idle waiting for data. Consider training an image
classification model: loading 1,000 images per second at 150 KB each
requires 150 MB/s sustained throughput from storage. When the GPU can
process images faster than storage delivers them, the data
pipeline---not the model---becomes the bottleneck. A 10-fold mismatch
between GPU processing speed and storage bandwidth means expensive
accelerators sit idle 90\% of the time waiting for data. No amount of
GPU optimization can overcome this fundamental I/O constraint.

Understanding these quantitative relationships enables informed
architectural decisions about storage system selection and data pipeline
optimization, which become even more critical during distributed
training as examined in \textbf{?@sec-ai-training}. The training
throughput equation reveals the critical dependencies:

Designing for high-throughput training requires calculating a
\emph{storage bandwidth budget}.

\phantomsection\label{callout-notebookux2a-1.26}
\begin{fbx}{callout-notebook}{AI Engineer’s Notebook:}{Storage Bandwidth Budget}
\phantomsection\label{callout-notebook*-1.26}
\textbf{The Problem:} Design a storage system that ensures an NVIDIA
A100 is never starved for data while training ResNet-50.

\textbf{1. The Compute Term (Throughput Ceiling)}

\begin{itemize}
\tightlist
\item
  \textbf{Hardware Peak:} NVIDIA A100 = 312 TFLOPS (FP16 Tensor Core,
  dense operations).\sidenote{\textbf{A100 Sparsity Support}: NVIDIA
  also advertises 624 TFLOPS with structured sparsity enabled (2:4
  pattern), doubling effective throughput for models that can leverage
  sparse computation. The 312 TFLOPS dense figure used here represents
  the baseline for general workloads where sparsity cannot be assumed. }
\item
  \textbf{Model Cost:} ResNet-50 = \textasciitilde4 GFLOPs per image
  (forward + backward \(\approx\) 12 GFLOPs).
\item
  \textbf{Maximum Theoretical Throughput}:
  \[ \frac{312 \times 10^{12} \text{ FLOPs/sec}}{12 \times 10^9 \text{ FLOPs/img}} = \mathbf{26\,000 \text{ images/sec}} \]
\end{itemize}

\textbf{2. The Data Term (Bandwidth Requirement)}

\begin{itemize}
\tightlist
\item
  \textbf{Image Size:} 150 KB (JPEG compressed).
\item
  \textbf{Required Bandwidth}:
  \[ 26\,000 \text{ img/sec} \times 150 \text{ KB/img} \approx \mathbf{3\.9 \text{ GB/s}} \]
\end{itemize}

\textbf{The Systems Conclusion:} To saturate a \emph{single} A100, your
storage must deliver \textbf{3.9 GB/s}. * \textbf{S3 Standard}:
\textasciitilde100 MB/s per thread. You need \textbf{39 concurrent
worker threads}. * \textbf{SATA SSD}: \textasciitilde500 MB/s. Totally
insufficient (bottleneck). * \textbf{NVMe SSD}: \textasciitilde3-7 GB/s.
\textbf{Required.}

\textbf{Iron Law Implication:} If you use SATA SSDs, your maximum
throughput is capped at
\(500 \text{ MB/s} / 150 \text{ KB} \approx 3\,333 \text{ img/s}\). Your
\$15,000 GPU will run at \textbf{13\% utilization} (\(3\,333/26\,000\)).
Storage physics dictates training speed.

\end{fbx}

This calculation illustrates the general principle governing data
pipelines:

\[\text{Training Throughput} = \min(\text{Compute Capacity}, \text{Data Supply Rate})\]

\[\text{Data Supply Rate} = \text{Storage Bandwidth} \times (1 - \text{Overhead})\]

When storage bandwidth becomes the limiting factor, teams must either
improve storage performance through faster media, parallelization, or
caching, or reduce data movement requirements through compression,
quantization, or architectural changes. Large language model training
may require processing hundreds of gigabytes of text per hour, while
computer vision models processing high-resolution imagery can demand
sustained data rates exceeding 50 gigabytes per second across
distributed clusters. These requirements explain the rise of specialized
ML storage systems optimizing data loading pipelines: PyTorch DataLoader
with multiple worker processes parallelizing I/O, TensorFlow tf.data API
with prefetching and caching, and frameworks like NVIDIA DALI (Data
Loading Library) that offload data augmentation to GPUs rather than
loading pre-augmented data from storage.

File format selection dramatically impacts the \textbf{Data Term}
(\(\frac{D_{vol}}{BW}\)) of the Iron Law. We can quantify this impact as
\textbf{Format Efficiency} (\(\eta_{format}\)), which acts as a
multiplier on effective bandwidth.

\phantomsection\label{callout-notebookux2a-1.27}
\begin{fbx}{callout-notebook}{AI Engineer’s Notebook:}{Format Efficiency}
\phantomsection\label{callout-notebook*-1.27}
\textbf{The Concept}: Storage formats determine how much ``waste'' data
you move to get the signal you need.
\[ \text{Effective Bandwidth} = \text{Physical Bandwidth} \times \eta_{format} \]

\textbf{Scenario}: Training a fraud model using 20 features from a
100-column table.

\begin{itemize}
\tightlist
\item
  \textbf{Row-Oriented (CSV)}: Must read all 100 columns to get the 20
  needed.

  \begin{itemize}
  \tightlist
  \item
    \(\eta_{format} = \frac{20}{100} = 0\.2\).
  \item
    \textbf{Result}: You waste 80\% of your disk bandwidth.
  \end{itemize}
\item
  \textbf{Column-Oriented (Parquet)}: Reads only the 20 columns needed.

  \begin{itemize}
  \tightlist
  \item
    \(\eta_{format} \approx 1\.0\) (ignoring metadata overhead).
  \item
    \textbf{Result}: You get \textbf{5x higher effective throughput}.
  \end{itemize}
\end{itemize}

\textbf{The Systems Conclusion}: Switching from CSV to Parquet is not
just a file change; it is mathematically equivalent to buying a
\textbf{5x faster hard drive}.

\end{fbx}

Columnar storage formats like Parquet or ORC deliver this five to 10
times I/O reduction compared to row-based formats like CSV or JSON for
typical ML workloads. The reduction comes from two mechanisms: reading
only required columns rather than entire records, and column-level
compression exploiting value patterns within columns. Consider a fraud
detection dataset with 100 columns where models typically use 20
features---columnar formats read only needed columns, achieving 80\% I/O
reduction before compression. Column compression proves particularly
effective for categorical features with limited cardinality: a country
code column with 200 unique values in 100 million records compresses 20
to 50 times through dictionary encoding, while run-length encoding
compresses sorted columns by storing only value changes. The combination
can achieve total I/O reduction of 20 to 100 times compared to
uncompressed row formats, directly translating to faster training
iterations and reduced infrastructure costs.

Compression algorithm selection involves trade-offs between compression
ratio and decompression speed. While gzip achieves higher compression
ratios of six to eight times, Snappy achieves only two to three times
compression but decompresses at 500 megabytes per second---roughly 4
times faster than gzip's 120 megabytes per second. For ML training where
throughput matters more than storage costs, Snappy's speed advantage
often outweighs gzip's space savings. Training on a 100 gigabyte dataset
compressed with gzip requires 14 minutes of decompression time, while
Snappy requires only 3 minutes. When training iterates over data for 50
epochs, this 11-minute difference per epoch compounds to 9 hours
total---potentially the difference between running experiments overnight
versus waiting multiple days for results. The choice cascades through
the system: faster decompression enables higher batch sizes (fitting
more examples in memory after decompression), reduced buffering
requirements (less decompressed data needs staging), and better GPU
utilization (less time idle waiting for data).

Storage performance optimization extends beyond format and compression
to data layout strategies. Data partitioning based on frequently used
query parameters dramatically improves retrieval efficiency. A
recommendation system processing user interactions might partition data
by date and user demographic attributes, enabling training on recent
data subsets or specific user segments without scanning the entire
dataset. Partitioning strategies interact with distributed training
patterns: range partitioning by user ID enables data parallel training
where each worker processes a consistent user subset, while random
partitioning ensures workers see diverse data distributions. The
partitioning granularity matters---too few partitions limit parallelism,
while too many partitions increase metadata overhead and reduce
efficiency of sequential reads within partitions.

\subsection{Storage Across the ML
Lifecycle}\label{sec-data-engineering-ml-storage-across-ml-lifecycle-2b22}

Storage requirements evolve substantially as ML systems progress from
initial development through production deployment and ongoing
maintenance. Understanding these changing requirements enables designing
infrastructure that supports the full lifecycle efficiently rather than
retrofitting storage later when systems scale or requirements change.
The same dataset might be accessed very differently during exploratory
analysis (random sampling for visualization), model training (sequential
scanning for epochs), and production serving (random access for
individual predictions), requiring storage architectures that
accommodate these diverse patterns.

During development, storage systems must support exploratory data
analysis and iterative model development where flexibility and
collaboration matter more than raw performance. Data scientists work
with various datasets simultaneously, experiment with feature
engineering approaches, and rapidly iterate on model designs to refine
approaches. The key challenge involves managing dataset versions without
overwhelming storage capacity. A naive approach copying entire datasets
for each experiment would exhaust storage quickly---10 experiments on a
100 gigabyte dataset would require one terabyte. Tools like DVC address
this by tracking dataset versions through pointers and storing only
deltas, enabling efficient experimentation. The system maintains lineage
from raw data through transformations to final training datasets,
supporting reproducibility when successful experiments need recreation
months later.

Collaboration during development requires balancing data accessibility
with security. Data scientists need efficient access to datasets for
experimentation, but organizations must simultaneously safeguard
sensitive information. Many teams implement tiered access controls where
synthetic or anonymized datasets are broadly available for
experimentation, while access to production data containing sensitive
information requires approval and audit trails. This balances
exploration velocity against governance requirements, enabling rapid
iteration on representative data without exposing sensitive information
unnecessarily.

Training phase requirements shift dramatically toward throughput
optimization. Modern deep learning training processes massive datasets
repeatedly across dozens or hundreds of epochs, making I/O efficiency
critical for acceptable iteration speed. High-performance storage
systems must provide throughput sufficient to feed data to multiple GPU
or TPU accelerators simultaneously without creating bottlenecks. When
training ResNet-50 on ImageNet's 1.2 million images across 8 GPUs, each
GPU processes approximately 4,000 images per epoch at 256 image batch
size. At 30 seconds per epoch, this requires loading 40,000 images per
second across all GPUs---approximately 500 megabytes per second of
decompressed image data. Storage systems unable to sustain this
throughput cause GPUs to idle waiting for data, directly reducing
training efficiency and increasing infrastructure costs.

The balance between preprocessing and on-the-fly computation becomes
critical during training. Extensive preprocessing reduces training-time
computation but increases storage requirements and risks staleness.
Feature extraction for computer vision might precompute ResNet features
from images, converting 150 kilobyte images to five kilobyte feature
vectors---achieving 30-fold storage reduction and eliminating repeated
computation. However, precomputed features become stale when feature
extraction logic changes, requiring recomputation across the entire
dataset. Production systems often implement hybrid approaches:
precomputing expensive, stable transformations like feature extraction
while computing rapidly-changing features on-the-fly during training.
This balances storage costs, computation time, and freshness based on
each feature's specific characteristics.

Deployment and serving requirements prioritize low-latency random access
over high-throughput sequential scanning. Real-time inference demands
storage solutions capable of retrieving model parameters and relevant
features within millisecond timescales. For a recommendation system
serving 10,000 requests per second with 10 millisecond latency budgets,
feature storage must support 100,000 random reads per second. In-memory
databases like Redis or sophisticated caching strategies become
essential for meeting these latency requirements. Edge deployment
scenarios introduce additional constraints: limited storage capacity on
embedded devices, intermittent connectivity to central data stores, and
the need for model updates without disrupting inference. Many edge
systems implement tiered storage where frequently-updated models cache
locally while infrequently-changing reference data pulls from cloud
storage periodically.

Model versioning becomes operationally critical during deployment.
Storage systems must support the deployment patterns examined in
\textbf{?@sec-machine-learning-operations-mlops}: smooth transitions
between model versions, rapid rollback capabilities, and efficient
serving of multiple versions simultaneously for shadow deployments and
A/B testing. These operational strategies impose specific storage
requirements---fast model loading, version-aware caching, and atomic
version switching---that influence architecture decisions.

\subsection{Data Versioning for ML
Reproducibility}\label{sec-data-engineering-ml-data-versioning-ml-reproducibility-16d0}

Data versioning connects model versions to exact training data, enabling
debugging and reproducibility. Without data versioning, teams cannot
answer essential questions like ``what exact data trained model v47?''

Listing~\ref{lst-dvc-workflow} shows how DVC provides Git-like semantics
for data versioning, while Listing~\ref{lst-delta-time-travel}
demonstrates querying historical data states directly in SQL.

\begin{codelisting}

\caption{\label{lst-dvc-workflow}\textbf{DVC Workflow}: Git-like
semantics for data versioning. DVC tracks large data files alongside
code commits, enabling exact reproduction of any historical training
dataset through paired \texttt{git\ checkout} and \texttt{dvc\ checkout}
commands.}

\centering{

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Add data to version control}
\ExtensionTok{dvc}\NormalTok{ add data/training.csv}
\FunctionTok{git}\NormalTok{ add data/training.csv.dvc}
\FunctionTok{git}\NormalTok{ commit }\AttributeTok{{-}m} \StringTok{"Add training data v1"}
\ExtensionTok{dvc}\NormalTok{ push  }\CommentTok{\# Upload to remote storage}

\CommentTok{\# Later: retrieve exact data for any historical commit}
\FunctionTok{git}\NormalTok{ checkout abc123}
\ExtensionTok{dvc}\NormalTok{ checkout  }\CommentTok{\# Restores exact data from that commit}
\end{Highlighting}
\end{Shaded}

}

\end{codelisting}%

\begin{codelisting}

\caption{\label{lst-delta-time-travel}\textbf{Delta Lake Time Travel}:
Querying historical data states directly in SQL. Delta Lake maintains a
transaction log enabling point-in-time queries by date or version
number, eliminating the need for separate snapshot management.}

\centering{

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{{-}{-} Query data as it existed on a specific date}
\KeywordTok{SELECT} \OperatorTok{*} \KeywordTok{FROM}\NormalTok{ training\_data VERSION }\KeywordTok{AS} \KeywordTok{OF} \StringTok{\textquotesingle{}2024{-}01{-}15\textquotesingle{}}

\CommentTok{{-}{-} Or by version number for programmatic access}
\KeywordTok{SELECT} \OperatorTok{*} \KeywordTok{FROM}\NormalTok{ training\_data VERSION }\KeywordTok{AS} \KeywordTok{OF} \DecValTok{47}
\end{Highlighting}
\end{Shaded}

}

\end{codelisting}%

\textbf{Feature Store Point-in-Time Retrieval} maintains historical
feature values, enabling training with features ``as they existed'' at
prediction time. This prevents label leakage where training
inadvertently uses feature values computed after the prediction
timestamp.

\textbf{Model Registry Integration} links each model registry entry to
its complete provenance: Git commit hash (code), data version (DVC
commit or Delta version), feature store snapshot timestamp, and training
configuration file. This complete lineage enables rapid debugging when
production issues arise.

A common failure mode illustrates the importance: Model v47 performed
3\% worse than v46 with identical code. Without data versioning, the
team spent two weeks investigating the accuracy drop. With proper
versioning, they would have immediately seen that the data version was
inadvertently updated mid-experiment and identified the root cause
within hours.

Monitoring and maintenance phases introduce long-term storage
considerations centered on debugging, compliance, and system
improvement. Capturing incoming data alongside prediction results
enables ongoing analysis detecting data drift, identifying model
failures, and maintaining regulatory compliance. For edge and mobile
deployments, storage constraints complicate data collection---systems
must balance gathering sufficient data for drift detection against
limited device storage and network bandwidth for uploading to central
analysis systems. Regulated industries often require immutable storage
supporting auditing: healthcare ML systems must retain not just
predictions but complete data provenance showing which training data and
model version produced each diagnostic recommendation, potentially for
years or decades.

Log and monitoring data volumes grow substantially in high-traffic
production systems. A recommendation system serving 10 million users
might generate terabytes of interaction logs daily. Storage strategies
typically implement tiered retention: hot storage retains recent data
(past week) for rapid analysis, warm storage keeps medium-term data
(past quarter) for periodic analysis, and cold archive storage retains
long-term data (past years) for compliance and rare deep analysis. The
transitions between tiers involve trade-offs between access latency,
storage costs, and retrieval complexity that systems must manage
automatically as data ages.

The storage architectures we have examined address where data resides
and how it is retrieved, but a critical challenge remains: ensuring that
features computed during training match exactly those computed during
serving. This consistency requirement, which we emphasized throughout
the processing section, demands specialized infrastructure that bridges
the gap between batch training environments and real-time serving
systems. Feature stores have emerged as the architectural solution to
this challenge.

\subsection{Feature Stores: Bridging Training and
Serving}\label{sec-data-engineering-ml-feature-stores-bridging-training-serving-55c8}

Feature stores have emerged as critical infrastructure components
addressing the unique challenge of maintaining consistency between
training and serving environments while enabling feature reuse across
models and teams. Traditional ML architectures often compute features
differently offline during training versus online during serving,
creating training-serving skew that silently degrades model performance.

\phantomsection\label{callout-definitionux2a-1.28}
\begin{fbx}{callout-definition}{Definition:}{Feature Store}
\phantomsection\label{callout-definition*-1.28}
\textbf{\emph{Feature Store}} is the architectural component that
enforces \textbf{Point-in-Time Correctness} and \textbf{Consistency}. It
decouples feature computation from consumption, guaranteeing that the
feature vector \(x_t\) served at inference time is computed with
identical logic to the historical vector \(x_{t-\Delta}\) used for
training, eliminating \textbf{Training-Serving Skew} by design.

\end{fbx}

The fundamental problem feature stores address becomes clear when
examining typical ML development workflows. During model development,
data scientists write feature engineering logic in notebooks or scripts,
often using different libraries and languages than production serving
systems. Training might compute a user's ``total purchases last 30
days'' using SQL aggregating historical data, while serving computes the
same feature using a microservice that incrementally updates cached
values. These implementations should produce identical results, but
subtle differences---handling timezone conversions, dealing with missing
data, or rounding numerical values---cause training and serving features
to diverge. A study of production ML systems found that 30\% to 40\% of
initial deployments at Uber suffered from training-serving skew,
motivating development of their Michelangelo platform with integrated
feature stores.

Feature stores provide a single source of truth for feature definitions,
ensuring consistency across all stages of the ML lifecycle. When data
scientists define a feature like ``user\_purchase\_count\_30d'', the
feature store maintains both the definition (SQL query, transformation
logic, or computation graph) and executes it consistently whether
providing historical feature values for training or real-time values for
serving. This architectural pattern eliminates an entire class of subtle
bugs that prove notoriously difficult to debug because models train
successfully but perform poorly in production without obvious errors.
The same centralized approach enables feature reuse across models and
teams: when multiple teams build models requiring similar features, the
feature store prevents each team from reimplementing identical
computations with subtle variations. A recommendation system might
compute user embedding vectors across hundreds of dimensions,
aggregating months of interaction history. Rather than each model team
recomputing these expensive embeddings, the feature store computes them
once and serves them to all consumers.

The architectural pattern typically implements dual storage modes
optimized for different access patterns. The offline store uses columnar
formats like Parquet on object storage, optimized for batch access
during training where sequential scanning of millions of examples is
common. The online store uses key-value systems like Redis, optimized
for random access during serving where individual feature vectors must
be retrieved in milliseconds. Synchronization between stores becomes
critical: as training generates new models using current feature values,
those models deploy to production expecting the online store to serve
consistent features. Feature stores typically implement scheduled batch
updates propagating new feature values from offline to online stores,
with update frequencies depending on feature freshness requirements.

Time-travel capabilities distinguish sophisticated feature stores from
simple caching layers. Training requires accessing feature values as
they existed at specific points in time, not just current values.
Consider training a churn prediction model: for users who churned on
January 15th, the model should use features computed on January 14th,
not current features reflecting their churned status. Point-in-time
correctness ensures training data matches production conditions where
predictions use currently-available features to forecast future
outcomes. Implementing time-travel requires storing feature history, not
just current values, substantially increasing storage requirements but
enabling correct training on historical data.

Feature store performance characteristics directly impact both training
throughput and serving latency. The offline store must support
high-throughput batch reads (millions of feature vectors per minute)
using columnar formats that enable efficient reads of specific features
from wide tables. The online store must support thousands to millions of
reads per second with single-digit millisecond latency. In production,
feature freshness adds further pressure: when users add items to
shopping carts, recommendation systems need updated features within
seconds, not hours. Streaming feature computation pipelines address this
by updating online stores continuously rather than through periodic
batch jobs, though streaming introduces complexity around exactly-once
processing semantics and handling late-arriving events.

\subsection{Case Study: Storage Architecture for KWS
Systems}\label{sec-data-engineering-ml-case-study-storage-architecture-kws-systems-3385}

Completing our comprehensive KWS case study, having traced the system
from initial problem definition through data collection strategies,
pipeline architectures, processing transformations, and labeling
approaches, we now examine how storage architecture supports this entire
data engineering lifecycle. The storage decisions made here directly
reflect and enable choices made in earlier stages. Our crowdsourcing
strategy established in
Section~\ref{sec-data-engineering-ml-framework-application-keyword-spotting-case-study-c8e9}
determines raw audio volume and diversity requirements. Our processing
pipeline designed in
Section~\ref{sec-data-engineering-ml-systematic-data-processing-aebc}
defines what intermediate features must be stored and retrieved
efficiently. Our quality metrics from
Section~\ref{sec-data-engineering-ml-ensuring-trainingserving-consistency-c683}
shape metadata storage needs for tracking data provenance and quality
scores. Storage architecture weaves these threads together, enabling the
system to function cohesively from development through production
deployment.

A typical KWS storage architecture implements the tiered approach
discussed earlier in this section, with each tier serving distinct
purposes that emerged from our earlier engineering decisions. Raw audio
files from various sources (crowd-sourced recordings collected through
the campaigns we designed, synthetic data generated to fill coverage
gaps, and real-world captures from deployed devices) reside in a data
lake using cloud object storage services like S3 or Google Cloud
Storage. This choice reflects our scalability pillar: audio files
accumulate to hundreds of gigabytes or terabytes as we collect the
millions of diverse examples needed for 98\% accuracy across
environments. The flexible schema of data lakes accommodates different
sampling rates, audio formats, and recording conditions without forcing
rigid structure on heterogeneous sources. Low cost per gigabyte that
object storage provides---typically one-tenth the cost of database
storage---enables retaining comprehensive data history for model
improvement and debugging without prohibitive expense.

The data lake stores comprehensive provenance metadata required by our
governance pillar, metadata that proved essential during earlier
pipeline stages. For each audio file, the system maintains source type
(crowdsourced, synthetic, or real-world), collection date, demographic
information when ethically collected and consented to, quality
assessment scores computed by our validation pipeline, and processing
history showing which transformations have been applied. This metadata
enables filtering during training data selection and supports compliance
requirements for privacy regulations and ethical AI practices
Section~\ref{sec-data-engineering-ml-governance-observability-2c05}
examines.

Processed features---spectrograms, MFCCs, and other ML-ready
representations computed by our processing pipeline---move into a
structured data warehouse optimized for training access. This addresses
different performance requirements from raw storage: while raw audio is
accessed infrequently (primarily during processing pipeline execution
when we transform new data), processed features are read repeatedly
during training epochs as models iterate over the dataset dozens of
times. The warehouse uses columnar formats like Parquet, enabling
efficient loading of specific features during training. For a dataset of
23 million examples like MSWC, columnar storage reduces training I/O by
five to 10 times compared to row-based formats, directly impacting
iteration speed during model development---the difference between
training taking hours versus days.

KWS systems benefit significantly from feature stores implementing the
architecture patterns we've examined. Commonly used audio
representations can be computed once and stored for reuse across
different experiments or model versions, avoiding redundant computation.
The feature store implements a dual architecture: an offline store using
Parquet on object storage for training data, providing high throughput
for sequential reads when training loads millions of examples, and an
online store using Redis for low-latency inference, supporting our 200
millisecond latency requirement established during problem definition.
This dual architecture addresses the fundamental tension between
training's batch access patterns---reading millions of examples
sequentially---and serving's random access patterns---retrieving
features for individual audio snippets in real-time as users speak wake
words.

In production, edge storage requirements become critical as our system
deploys to resource-constrained devices. Models must be compact enough
for devices with our 16 kilobyte memory constraint from the problem
definition while maintaining quick parameter access for real-time wake
word detection. Edge devices typically store quantized models using
specialized formats like TensorFlow Lite's FlatBuffers, which enable
memory-mapped access without deserialization overhead that would violate
latency requirements. Caching applies at multiple levels: frequently
accessed model layers reside in SRAM for fastest access, the full model
sits in flash storage for persistence across power cycles, and
cloud-based model updates are fetched periodically to maintain current
wake word detection patterns. This multi-tier caching ensures devices
operate effectively even with intermittent network connectivity---a
reliability requirement for consumer devices deployed in varied network
environments from rural areas with limited connectivity to urban
settings with congested networks.

\section{Data Governance and
Compliance}\label{sec-data-engineering-ml-data-governance-eade}

The storage architectures examined in this chapter---data lakes,
warehouses, and feature stores---are not merely technical
infrastructure; they are governance enforcement mechanisms. Decisions
about how data is acquired, processed, and stored carry significant
legal and ethical implications, from GDPR compliance to
privacy-preserving processing.

Because these concerns extend beyond data engineering into the broader
lifecycle of model development and deployment, we address them in detail
in \textbf{?@sec-responsible-engineering-data-governance-compliance}.
There, we examine how technical controls like role-based access control
(RBAC), data lineage, and audit trails transform abstract policy into
enforceable engineering reality.

Even with robust governance infrastructure in place, ML systems
accumulate hidden liabilities over time---not just governance gaps, but
shortcuts and compromises across every stage we have examined in this
chapter. The undocumented datasets from hasty acquisition, the fragile
schema workarounds in our pipelines, the uncorrected labeling errors we
flagged but deferred, the models trained on data that has drifted from
current distributions: each represents a form of debt that compounds
silently. Governance infrastructure can prevent many issues, but
organizations must also recognize and address the gradual accumulation
of compromises that compound until remediation costs exceed system
value.

\section{Data Debt: The Hidden
Liability}\label{sec-data-engineering-ml-data-debt-hidden-liability-3335}

Every shortcut in data engineering, from deferred label corrections to
undocumented schema changes, creates a form of technical debt that
compounds silently over time. Unlike code debt, which often manifests as
slower development velocity, data debt directly degrades model
performance and can remain invisible until failures become catastrophic.
This section defines the categories of data debt, introduces
quantitative methods for measuring and projecting its growth, and
presents remediation strategies that prioritize debt reduction by
expected impact.

\subsection{Categories of Data
Debt}\label{sec-data-engineering-ml-categories-data-debt-ee90}

Technical debt is well-understood in software engineering: shortcuts
taken today create maintenance burdens tomorrow. Data debt is its
counterpart in ML systems: accumulated compromises in data quality,
documentation, and infrastructure that compound over time, degrading
model performance and increasing operational costs. The categories of
data debt map directly to our Four Pillars: documentation debt
undermines governance, schema debt compromises reliability, quality debt
degrades model accuracy, and freshness debt erodes the training-serving
consistency we established as a mathematical requirement. Unlike
technical debt in code, data debt often remains invisible until
catastrophic failures occur, making it particularly insidious.

\phantomsection\label{callout-definitionux2a-1.29}
\begin{fbx}{callout-definition}{Definition:}{Data Debt}
\phantomsection\label{callout-definition*-1.29}
\textbf{\emph{Data Debt}} is the \textbf{Compound Interest} of implicit
coupling and missing documentation. Unlike code debt which manifests as
slower development, data debt manifests as \textbf{Silent Degradation},
where the cost of maintenance scales superlinearly with system age due
to the entropy of unmanaged dependencies and distribution shifts.

\end{fbx}

Data debt manifests across four distinct categories, each requiring
different detection and remediation strategies.

\textbf{Documentation Debt} accumulates when data provenance, meaning,
and quality characteristics go unrecorded. Datasets without data cards
(\textbf{?@fig-data-card}), unlabeled columns, and undocumented
transformations create debt that compounds when original authors leave
organizations. A survey of production ML systems found that 40\% of data
quality incidents traced to misunderstanding data semantics due to
missing documentation (\citeproc{ref-sambasivan2021everyone}{Sambasivan
et al. 2021}). Documentation debt manifests as: unknown column meanings
requiring reverse-engineering, missing provenance preventing compliance
audits, undocumented assumptions causing silent failures when
assumptions change, and absent quality metrics preventing informed
dataset selection.

\textbf{Schema Debt} emerges from accumulated schema workarounds and
migrations. When upstream systems change data formats, quick fixes
(string parsing instead of proper type handling, NULL coercion instead
of error handling) accumulate into fragile transformation logic. Schema
debt indicators include: multiple date format handlers for the same
logical field, defensive null checks scattered throughout pipeline code,
version-specific parsing branches that grow with each upstream change,
and undocumented enum value mappings that break when new values appear.

\textbf{Quality Debt} consists of known data errors that remain
uncorrected due to time or resource constraints. A dataset with 3\%
known label errors represents quality debt---each training run on this
data produces models carrying those errors. Quality debt compounds:
models trained on erroneous data make predictions that become training
data for downstream systems, amplifying the original errors. Quality
debt metrics include: known label error rates not yet corrected,
documented bias not yet mitigated, identified duplicates not yet
deduplicated, and detected drift not yet addressed through retraining.

\textbf{Freshness Debt} arises when training data diverges from
production distributions over time. A model trained on 2023 user
behavior deployed in 2025 carries freshness debt---the distribution
shift between training and serving degrades performance continuously.
Freshness debt is particularly dangerous because it accumulates
silently: unlike code that breaks obviously, stale models degrade
gradually until performance drops below acceptable thresholds. Freshness
debt indicators include: time since last retraining relative to
distribution shift rate, feature staleness (cached features not updated
at required frequency), and reference data lag (lookup tables not
reflecting current state).

\subsection{Measuring and Projecting Data
Debt}\label{sec-data-engineering-ml-measuring-data-debt-973c}

Unlike technical debt, which can be assessed through code complexity
metrics, data debt requires specialized measurement approaches.

Table~\ref{tbl-data-debt-metrics} provides quantitative indicators for
each debt category:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2500}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2500}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2500}}@{}}
\caption{\textbf{Data Debt Metrics}: Quantitative thresholds for
detecting data debt accumulation. Warning thresholds indicate debt
requiring attention in upcoming planning cycles; critical thresholds
indicate debt requiring immediate remediation to prevent system
degradation.}\label{tbl-data-debt-metrics}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Debt Category}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Metric}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Warning Threshold}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Critical Threshold}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Debt Category}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Metric}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Warning Threshold}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Critical Threshold}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Documentation} & \% datasets with data cards & \textless{} 80\%
& \textless{} 50\% \\
\textbf{Documentation} & \% columns with descriptions & \textless{} 90\%
& \textless{} 70\% \\
\textbf{Schema} & Schema version branches & \textgreater{} 3 &
\textgreater{} 10 \\
\textbf{Schema} & \% transformations with try/catch & \textgreater{}
20\% & \textgreater{} 50\% \\
\textbf{Quality} & Known label error rate & \textgreater{} 1\% &
\textgreater{} 5\% \\
\textbf{Quality} & Documented bias metrics & Not measured & Measured but
not mitigated \\
\textbf{Freshness} & Days since last retraining & \textgreater{} 90 days
& \textgreater{} 365 days \\
\textbf{Freshness} & PSI vs training baseline & \textgreater{} 0.1 &
\textgreater{} 0.25 \\
\end{longtable}

\textbf{The Compound Interest of Data Debt}

Data debt compounds through feedback loops unique to ML systems:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Training Amplification}: Models trained on erroneous data
  learn incorrect patterns. When these models generate predictions used
  as features or pseudo-labels, errors propagate to downstream systems.
\item
  \textbf{Documentation Decay}: As original authors leave and systems
  evolve, undocumented datasets become increasingly opaque. Each year
  without documentation makes future documentation exponentially harder.
\item
  \textbf{Schema Entropy}: Quick fixes to handle schema changes create
  brittle code. Each additional workaround increases the probability of
  the next schema change causing failures.
\item
  \textbf{Distribution Drift}: Without continuous monitoring and
  retraining, the gap between training and serving distributions widens,
  causing accuracy degradation that accelerates as models become less
  calibrated.
\end{enumerate}

The compound nature means that data debt left unaddressed for \emph{n}
periods grows superlinearly:

\[\text{Debt}_n \approx \text{Debt}_0 \times (1 + r)^n\]

where \emph{r} is the debt accumulation rate (typically 10--30\% per
period for undocumented systems).

\subsection{Remediation
Strategies}\label{sec-data-engineering-ml-remediation-strategies-e457}

Addressing data debt requires systematic investment, not heroic one-time
efforts:

\textbf{Documentation Sprints}: Dedicate regular time (e.g., one week
per quarter) exclusively to documentation. Prioritize by dataset usage:
document the 20\% of datasets serving 80\% of models first.

\textbf{Schema Contracts}: Implement explicit schema contracts between
data producers and consumers. Tools like Great Expectations or Pandera
codify expectations, failing fast when schemas drift rather than
accumulating workarounds.

\textbf{Quality Budgets}: Allocate fixed capacity (e.g., 10\% of data
engineering effort) to quality debt remediation. Track known error
backlog and measure burn-down rate.

\textbf{Continuous Retraining}: Implement automated retraining triggers
based on drift detection rather than calendar schedules. Freshness debt
cannot be addressed through periodic heroics; it requires systematic
automation.

The key insight is that data debt, like technical debt, is not
inherently bad. Strategic debt, knowingly accepting documentation
shortcuts to meet a deadline, can be rational. The danger lies in
\emph{unconscious} debt that accumulates untracked until remediation
costs exceed system value.

\section{Debugging Data
Pipelines}\label{sec-data-engineering-ml-debugging-data-pipelines-2f26}

The concepts throughout this chapter (cascading failures, the four
pillars, training-serving consistency, drift detection, labeling
quality, and data debt) converge when systems exhibit problems in
production. Data debt accumulates silently, but its effects eventually
surface as system underperformance, manifesting as model accuracy
degradation, pipeline failures, or subgroup performance disparities that
prove difficult to attribute to specific causes.

Effective debugging requires applying the diagnostic principles
established earlier: the data cascade pattern
(Section~\ref{sec-data-engineering-ml-data-cascades-systematic-foundations-matter-2efe})
reminds us that root causes often lie upstream of symptoms,
training-serving skew
(Section~\ref{sec-data-engineering-ml-ensuring-trainingserving-consistency-c683})
explains many deployment failures, and drift detection
(Section~\ref{sec-data-engineering-ml-detecting-responding-data-drift-509a})
surfaces gradual degradation. When symptoms appear, systematic diagnosis
prevents wasted effort debugging the wrong component.
Figure~\ref{fig-debug-flowchart} synthesizes these concepts into an
actionable diagnostic sequence, working through the most common failure
modes in order of frequency:

\begin{figure}[htb]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/6dd12d22c544dc137062a3a4583d1dba2f2b6583.pdf}}

}

\caption{\label{fig-debug-flowchart}\textbf{Data Pipeline Debugging
Flowchart}: Four sequential decision nodes guide root cause diagnosis:
(1) accuracy degrades over time leads to Data Drift, (2) training
accuracy exceeds validation leads to Overfitting, (3) validation exceeds
production accuracy leads to Training-Serving Skew, and (4) subgroup
inconsistency leads to Bias. If all answers are no, the issue points to
Model Architecture.}

\end{figure}%

\textbf{The Diagnosis Priority}: Most production ML failures trace to
data, not models. Check in this order (approximate proportions based on
industry experience):\sidenote{\textbf{ML Failure Distribution}: These
proportions represent synthesized estimates from industry reports and
practitioner experience, including findings from Sculley et al.
(\citeproc{ref-sculley2015hidden}{2021}) on ML technical debt and
platform experiences at companies like Uber, Google, and Meta. Actual
proportions vary by domain and organizational maturity; the key insight
is that data-related issues collectively dominate model-related issues.
}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Training-serving skew} (30--40\% of failures)
\item
  \textbf{Data drift} (20--30\% of failures)
\item
  \textbf{Label quality} (15--25\% of failures)
\item
  \textbf{Model architecture} (10--15\% of failures)
\end{enumerate}

Debugging the model before verifying data consistency wastes engineering
cycles on the wrong 10--15\%.

\section{Fallacies and
Pitfalls}\label{sec-data-engineering-ml-fallacies-pitfalls-d2f5}

Data engineering involves managing complex distributed systems where
statistical properties dominate correctness more than traditional
software guarantees. The scale and stochastic nature of ML data
pipelines create counterintuitive failure modes that differ
fundamentally from conventional software systems. Engineers transferring
intuition from traditional backend development encounter misconceptions
about data quality, pipeline reliability, and distribution stability.
These fallacies lead to systematic underinvestment in data
infrastructure, resulting in silent model degradation, failed production
deployments, and wasted compute resources on training with corrupted
data.

\paragraph*{\texorpdfstring{Fallacy: \emph{More data always leads to
better model
performance.}}{Fallacy: More data always leads to better model performance.}}\label{fallacy-more-data-always-leads-to-better-model-performance.}
\addcontentsline{toc}{paragraph}{Fallacy: \emph{More data always leads
to better model performance.}}

Conventional wisdom holds that larger datasets improve model performance
by providing more examples to learn from. However, this assumes constant
quality as quantity scales. Research on ImageNet found label errors in
3.4\% of validation set examples despite extensive curation
(\citeproc{ref-northcutt2021pervasive}{Northcutt, Athalye, and Mueller
2021}), meaning a 10-million-example dataset contains 340,000 mislabeled
instances. Empirical studies show 10\% label noise reduces model
accuracy by 5 to 8 percentage points for typical vision tasks, with the
degradation compounding during training. As
Section~\ref{sec-data-engineering-ml-data-quality-code-1cca}
establishes, a curated 100,000-example dataset covering diverse
demographics and edge cases outperforms a haphazard 1-million-example
dataset skewed toward common cases. Training on 10x more data requires
10x more GPU hours while potentially achieving worse accuracy, meaning
teams that prioritize curation over collection achieve better
performance at lower infrastructure costs.

\paragraph*{\texorpdfstring{Pitfall: \emph{Treating data labeling as a
simple mechanical task that can be outsourced without
oversight.}}{Pitfall: Treating data labeling as a simple mechanical task that can be outsourced without oversight.}}\label{pitfall-treating-data-labeling-as-a-simple-mechanical-task-that-can-be-outsourced-without-oversight.}
\addcontentsline{toc}{paragraph}{Pitfall: \emph{Treating data labeling
as a simple mechanical task that can be outsourced without oversight.}}

Teams assume labeling is conceptually simple (show image, get label),
but
Section~\ref{sec-data-engineering-ml-scaling-aiassisted-labeling-9360}
reveals labeling economics dominate total costs. Expert review costs 10
to 50x more than crowdsourcing (\$0.50-2.00 vs \$0.01-0.05 per label),
yet skipping oversight triggers expensive rework. A 1-million-label
dataset at \$0.10 per label with 10\% expert review and 20\% rework
costs \$138,000, not the naive \$100,000 estimate. Training a ResNet-50
on this data costs approximately \$50 in compute, meaning labeling
exceeds training costs by 2,760x. Quality degradation compounds the
economic waste: annotators scoring below 85\% on gold standards
introduce systematic errors that models amplify. Without consensus
mechanisms (3 to 5 labels per example, Fleiss' kappa greater than 0.4),
ambiguous cases receive arbitrary labels causing 8 to 15\% accuracy
drops on production edge cases that require costly relabeling and
retraining cycles.

\paragraph*{\texorpdfstring{Fallacy: \emph{Data engineering is a
one-time setup that can be completed before model development
begins.}}{Fallacy: Data engineering is a one-time setup that can be completed before model development begins.}}\label{fallacy-data-engineering-is-a-one-time-setup-that-can-be-completed-before-model-development-begins.}
\addcontentsline{toc}{paragraph}{Fallacy: \emph{Data engineering is a
one-time setup that can be completed before model development begins.}}

Traditional software infrastructure precedes application development and
remains stable, but ML systems face continuous distribution drift that
static pipelines cannot handle. As
Section~\ref{sec-data-engineering-ml-detecting-responding-data-drift-509a}
explains, drift detection and response consume 30 to 40\% of ongoing ML
operations effort. Data distributions shift through covariate drift
(demographics evolve), label shift (seasonal class frequencies), and
concept drift (feature-label relationships change). The Population
Stability Index (PSI) quantifies retraining triggers: PSI greater than
or equal to 0.2 signals significant changes requiring model updates,
while 0.1 to 0.2 demands investigation. A recommendation system might
observe PSI = 0.19 as users age from 25-35 to 35-45 over six months,
requiring pipeline adaptation. Systems designed as static infrastructure
lack monitoring, alerting, and automated response mechanisms that
production deployments require. Teams must budget 30 to 40\% of ongoing
engineering capacity for continuous evolution, not treat data
engineering as a phase that completes before modeling.

\paragraph*{\texorpdfstring{Fallacy: \emph{Training and test data
splitting is sufficient to ensure model
generalization.}}{Fallacy: Training and test data splitting is sufficient to ensure model generalization.}}\label{fallacy-training-and-test-data-splitting-is-sufficient-to-ensure-model-generalization.}
\addcontentsline{toc}{paragraph}{Fallacy: \emph{Training and test data
splitting is sufficient to ensure model generalization.}}

ML education emphasizes train/test splitting to evaluate how well models
generalize to unseen data, creating the belief that validation accuracy
predicts production performance. This assumes training, testing, and
production data share the same distribution, which rarely holds. As
Section~\ref{sec-data-engineering-ml-ensuring-trainingserving-consistency-c683}
explains, training-serving skew causes silent production failures. A
recommendation system computing ``user\_lifetime\_purchases'' by joining
complete transaction histories during training but using weekly-updated
materialized views during serving creates 15\% feature discrepancy,
causing observed 12\% accuracy drops in A/B tests. Geographic shifts
compound the problem: models achieving 95\% accuracy on North American
test sets drop to 73\% in Southeast Asian markets due to demographic and
linguistic differences. These distribution shifts manifest as accuracy
degradation for specific user segments, creating fairness violations.
Production systems require continuous monitoring using metrics like the
Kolmogorov-Smirnov test (statistically significant drift at
\(p < 0.05\)), detecting issues before they affect users rather than
relying solely on development-time validation.

\paragraph*{\texorpdfstring{Pitfall: \emph{Building data pipelines
without considering failure modes and recovery
mechanisms.}}{Pitfall: Building data pipelines without considering failure modes and recovery mechanisms.}}\label{pitfall-building-data-pipelines-without-considering-failure-modes-and-recovery-mechanisms.}
\addcontentsline{toc}{paragraph}{Pitfall: \emph{Building data pipelines
without considering failure modes and recovery mechanisms.}}

Engineers from traditional request-response services design pipelines
assuming data arrives in expected formats at expected rates, but
production pipelines face fundamentally different failure modes. As
Section~\ref{sec-data-engineering-ml-quality-validation-monitoring-498f}
establishes, severity-based monitoring distinguishes complete failures
(zero throughput for greater than 5 minutes) from gradual degradation
(throughput dropping to 80\% of baseline, error rates exceeding 5\%,
quality metrics drifting more than 2 standard deviations). A
recommendation system processing 50,000 events per second sets
throughput alerts at 40,000 events per second sustained for more than 10
minutes, catching capacity problems with headroom for normal variation.
Feature quality monitoring tracks null rates: when user\_age shows nulls
in more than 5\% of records despite training data containing less than
1\% nulls, upstream sources have failed. Failures cascade across
multiple dimensions simultaneously: database failovers reduce throughput
to 35,000 events per second while increasing null rates to 8\% and
duplicating 3\% of records. Data quality validation catches
approximately 60\% of issues through executable assertions, but the
remaining 40\% require runtime monitoring with automated alerting and
graceful degradation.

\paragraph*{\texorpdfstring{Pitfall: \emph{Choosing storage architecture
based solely on capacity cost without considering access patterns and
performance
requirements.}}{Pitfall: Choosing storage architecture based solely on capacity cost without considering access patterns and performance requirements.}}\label{pitfall-choosing-storage-architecture-based-solely-on-capacity-cost-without-considering-access-patterns-and-performance-requirements.}
\addcontentsline{toc}{paragraph}{Pitfall: \emph{Choosing storage
architecture based solely on capacity cost without considering access
patterns and performance requirements.}}

Teams compare raw storage costs (S3 Standard at \$23 per TB per month vs
Glacier Deep Archive at \$1 per TB per month) and select the cheapest
option without analyzing access patterns. However,
Section~\ref{sec-data-engineering-ml-ml-storage-systems-architecture-options-67fa}
establishes that total cost of ownership includes retrieval costs,
access latency, and throughput limitations. Glacier Deep Archive charges
\$0.02 per GB for retrieval plus 12-hour access latency, meaning a
single full retrieval of 100 TB costs \$2,000, equivalent to 87 months
of S3 Standard storage for that data. Training workloads requiring
random access to 10\% of a 100 TB dataset daily would spend \$200 per
day on Glacier retrievals versus \$2,300 per month for S3 Standard
storage with 3,500 MB/s sustained read throughput. Local NVMe storage at
\$0.10 per GB (\$100 per TB) appears expensive but delivers 7,000 MB/s
sequential reads and sub-millisecond random access latency, completing
training epochs 15 to 30x faster than S3 for I/O-bound workloads.
Organizations must match storage tiers to access patterns: cold archival
data to Glacier, training datasets requiring sequential scans to S3, and
active training data with random access patterns to local SSDs or
high-performance networked storage.

These fallacies and pitfalls underscore a central theme: data
engineering success requires abandoning intuitions from traditional
software development. Statistical systems demand continuous monitoring,
cost models must account for human labor, and storage decisions must
optimize for access patterns rather than capacity alone.

\section{Summary}\label{sec-data-engineering-ml-summary-4ac6}

Data engineering provides the foundational infrastructure that
transforms raw information into the basis of machine learning systems,
determining model performance, system reliability, ethical compliance,
and long-term maintainability. The Four Pillars framework
(Figure~\ref{fig-four-pillars}) and the cascading nature of data quality
failures (Figure~\ref{fig-cascades}) reveal why every stage of the data
pipeline requires careful engineering decisions. The task of ``getting
data ready'' encompasses complex trade-offs quantified throughout this
chapter: the TCDO cost model for budgeting, storage performance
hierarchies (Table~\ref{tbl-storage-performance},
Table~\ref{tbl-ml-latencies}), and drift detection thresholds that guide
production operations.

The technical architecture of data systems demonstrates how engineering
decisions compound across the pipeline to create either reliable,
scalable foundations or brittle, maintenance-heavy technical debt. Data
acquisition strategies must navigate the reality that perfect datasets
rarely exist in nature, requiring sophisticated approaches ranging from
crowdsourcing and synthetic generation to careful curation and active
learning. Storage architectures from traditional databases to modern
data lakes and feature stores represent fundamental choices about how
data flows through the system, affecting everything from training speed
to serving latency.

\phantomsection\label{callout-takeawaysux2a-1.30}
\begin{fbx}{callout-takeaways}{Takeaways:}{Key Takeaways}
\phantomsection\label{callout-takeaways*-1.30}

\begin{itemize}
\item
  \textbf{Data is code: version it, test it, review it.} The
  \textbf{Data as Code Invariant} established in
  \textbf{?@sec-foundations-invariants} defines the engineering mindset:
  the dataset is the source code of an ML system. Apply the same rigor:
  version control, unit tests (validation), and code review (data
  review).
\item
  \textbf{Training-serving consistency is non-negotiable.} Any
  transformation applied during training must be applied identically
  during serving. This is a mathematical requirement, not a best
  practice.
\item
  \textbf{Labeling costs dominate and require substantial resource
  allocation.} Labeling typically costs 1,000--3,000x more than model
  training compute. Labeling is the serial bottleneck that
  parallelization cannot solve.
\item
  \textbf{Storage hierarchy determines iteration speed.} The 70x
  throughput gap between local NVMe (7 GB/s) and cloud object storage
  (100 MB/s) determines whether iterations occur daily or weekly.
\item
  \textbf{Data debt compounds and requires continuous remediation.}
  Documentation, schema, quality, and freshness debt accumulate with
  compound interest. Allocate 10--20\% of effort to continuous
  remediation.
\end{itemize}

\end{fbx}

Our KWS case study demonstrates these principles in action. From problem
definition through production deployment, the Four Pillars guided every
decision: quality requirements drove our multi-source acquisition
strategy combining curated datasets with crowdsourcing and synthetic
generation; reliability shaped our pipeline architecture with graceful
degradation and consistency validation; scalability determined our
tiered storage design handling 23 million audio samples across 736 GB of
raw data; and governance established the privacy protections and lineage
tracking essential for always-listening devices in users' homes. The KWS
system shows that data engineering is not a preprocessing step to be
completed before ``real'' ML work begins---it is the foundation upon
which model performance, user trust, and regulatory compliance rest.

The integration of reliable data governance practices throughout the
pipeline ensures that ML systems remain trustworthy, compliant, and
transparent as they scale in complexity and impact. The full governance
infrastructure, from access control and data lineage to audit trails and
regulatory compliance, extends these pipeline-level practices into the
organizational and legal dimensions examined in
\textbf{?@sec-responsible-engineering-data-governance-compliance}. While
this chapter focuses on \emph{building} reliable data infrastructure,
\textbf{?@sec-data-selection} examines \emph{optimizing} data usage by
reducing the total data required through active learning, data pruning,
and intelligent sampling strategies.

\phantomsection\label{callout-chapter-connectionux2a-1.31}
\begin{fbxSimple}{callout-chapter-connection}{What’s Next:}{From Fuel to Engine}
\phantomsection\label{callout-chapter-connection*-1.31}
Data engineering has provided the fuel: clean, reliable, and scalable
datasets. But fuel alone does not create motion. We turn next to the
mathematical foundations of learning in
\textbf{?@sec-deep-learning-systems-foundations}, which transforms
neural networks from opaque components into engineerable systems whose
behavior we can predict, debug, and optimize.

\end{fbxSimple}

\FloatBarrier\clearpage

\setpartsummary{This part focuses on the development phase of ML systems. It bridges the gap between mathematical theory and working code, covering neural network architectures, framework implementation details, and the systems engineering required to train models effectively.}

\addtocontents{toc}{\par\addvspace{12pt}\noindent\hfil\bfseries\color{crimson}Part~I~Development and Training\color{black}\hfil\par\addvspace{6pt}}

\numberedpart{Development and Training}

\haspartsummaryfalse

\phantomsection\label{refs}
\begin{CSLReferences}{1}{0}
\bibitem[\citeproctext]{ref-akidau2015dataflow}
Akidau, Tyler, Robert Bradshaw, Craig Chambers, Slava Chernyak, Rafael
J. Fernández-Moctezuma, Reuven Lax, Sam McVeety, et al. 2015. {``The
Dataflow Model: A Practical Approach to Balancing Correctness, Latency,
and Cost in Massive-Scale, Unbounded, Out-of-Order Data Processing.''}
\emph{Proceedings of the VLDB Endowment} 8 (12): 1792--1803.
\url{https://doi.org/10.14778/2824032.2824076}.

\bibitem[\citeproctext]{ref-anylogic_synthetic}
AnyLogic. 2024. {``Synthetic Data for Artificial Intelligence.''}
\url{https://www.anylogic.com/features/artificial-intelligence/synthetic-data/}.

\bibitem[\citeproctext]{ref-ardila2020common}
Ardila, Rosana, Megan Branson, Kelly Davis, Michael Kohler, Josh Meyer,
Michael Henretty, Reuben Morais, Lindsay Saunders, Francis Tyers, and
Gregor Weber. 2020. {``Common Voice: A Massively-Multilingual Speech
Corpus.''} In \emph{Proceedings of the Twelfth Language Resources and
Evaluation Conference}, 4218--22. Marseille, France: European Language
Resources Association. \url{https://aclanthology.org/2020.lrec-1.520}.

\bibitem[\citeproctext]{ref-banbury2024wakevisiontailoreddataset}
Banbury, Colby, Emil Njor, Andrea Mattia Garavagno, Mark Mazumder,
Matthew Stewart, Pete Warden, Manjunath Kudlur, Nat Jeffries, Xenofon
Fafoutis, and Vijay Janapa Reddi. 2024. {``Wake Vision: A Tailored
Dataset and Benchmark Suite for TinyML Computer Vision Applications,''}
May. \url{http://arxiv.org/abs/2405.00892v5}.

\bibitem[\citeproctext]{ref-beyer2020we}
Beyer, Lucas, Olivier J. Hénaff, Alexander Kolesnikov, Xiaohua Zhai, and
Aäron van den Oord. 2020. {``Are We Done with ImageNet?''} \emph{arXiv
Preprint arXiv:2006.07159}, June.
\url{http://arxiv.org/abs/2006.07159v1}.

\bibitem[\citeproctext]{ref-bishop2006pattern}
Bishop, Christopher M. 2006. \emph{Pattern Recognition and Machine
Learning}. Springer.

\bibitem[\citeproctext]{ref-imagenet_website}
Blaivas, Laura, and Michael Blaivas. 2020. {``Are Convolutional Neural
Networks Trained on
\textless Scp\textgreater ImageNet\textless/Scp\textgreater{} Images
Wearing
\textless Scp\textgreater rose‐colored\textless/Scp\textgreater{}
Glasses?: A Quantitative Comparison of
\textless Scp\textgreater ImageNet\textless/Scp\textgreater, Computed
Tomographic, Magnetic Resonance, Chest
\textless Scp\textgreater x‐ray\textless/Scp\textgreater, and
\textless Scp\textgreater point‐of‐care\textless/Scp\textgreater{}
Ultrasound Images for Quality.''} \emph{Journal of Ultrasound in
Medicine} 40 (2): 377--83. \url{https://doi.org/10.1002/jum.15413}.

\bibitem[\citeproctext]{ref-polyzotis2019data}
Breck, Eric, Neoklis Polyzotis, Sudip Roy 0002, Steven Whang 0001, and
Martin Zinkevich. 2019. {``Data Validation for Machine Learning.''} In
\emph{Proceedings of the 2nd SysML Conference}.
\url{https://proceedings.mlsys.org/paper/_files/paper/2019/hash/928f1160e52192e3e0017fb63ab65391-Abstract.html}.

\bibitem[\citeproctext]{ref-brewer2000towards}
Brewer, Eric A. 2000. {``Towards Robust Distributed Systems
(Abstract).''} In \emph{Proceedings of the Nineteenth Annual ACM
Symposium on Principles of Distributed Computing}, 7. ACM.
\url{https://doi.org/10.1145/343477.343502}.

\bibitem[\citeproctext]{ref-brown2020language}
Brown, Tom B., Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan,
Prafulla Dhariwal, Arvind Neelakantan, et al. 2020. {``Language Models
Are Few-Shot Learners.''} Edited by Hugo Larochelle, Marc'Aurelio
Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin.
\emph{Advances in Neural Information Processing Systems} 33 (May):
1877--1901. \url{https://doi.org/10.48550/arxiv.2005.14165}.

\bibitem[\citeproctext]{ref-chapelle2009semisupervised}
Chapelle, O., B. Scholkopf, and A. Zien Eds. 2009. {``Semi-Supervised
Learning (Chapelle, o. Et Al., Eds.; 2006) {[}Book Reviews{]}.''}
\emph{IEEE Transactions on Neural Networks} 20 (3): 542--42.
\url{https://doi.org/10.1109/tnn.2009.2015974}.

\bibitem[\citeproctext]{ref-chen2021evaluating}
Chen, Mark, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de
Oliveira Pinto, Jared Kaplan, Harri Edwards, et al. 2021. {``Evaluating
Large Language Models Trained on Code.''} \emph{arXiv Preprint
arXiv:2107.03374}, July. \url{http://arxiv.org/abs/2107.03374v2}.

\bibitem[\citeproctext]{ref-coleman2022similarity}
Coleman, Cody, Edward Chou, Julian Katz-Samuels, Sean Culatana, Peter
Bailis, Alexander C. Berg, Robert Nowak, Roshan Sumbaly, Matei Zaharia,
and I. Zeki Yalniz. 2022. {``Similarity Search for Efficient Active
Learning and Search of Rare Concepts.''} \emph{Proceedings of the AAAI
Conference on Artificial Intelligence} 36 (6): 6402--10.
\url{https://doi.org/10.1609/aaai.v36i6.20591}.

\bibitem[\citeproctext]{ref-nvidia_simulation}
Crewe, Jacob, Aditya Humnabadkar, Yonghuai Liu, Amr Ahmed, and Ardhendu
Behera. 2023. {``SLAV-Sim: A Framework for Self-Learning Autonomous
Vehicle Simulation.''} \emph{Sensors} 23 (20): 8649.
\url{https://doi.org/10.3390/s23208649}.

\bibitem[\citeproctext]{ref-crowdflower2016data}
CrowdFlower. n.d. {``Supplemental Information 1: Source Code for
Analysis in Matlab, Correlation Matrix, XML Code for Crowdflower
Survey.''} CrowdFlower Inc; PeerJ.
\url{https://doi.org/10.7287/peerj.preprints.1069/supp-1}.

\bibitem[\citeproctext]{ref-dean2004mapreduce}
Dean, Jeffrey, and Sanjay Ghemawat. 2008. {``MapReduce: Simplified Data
Processing on Large Clusters.''} \emph{Communications of the ACM} 51
(1): 107--13. \url{https://doi.org/10.1145/1327452.1327492}.

\bibitem[\citeproctext]{ref-dehghani2022data}
Dehghani, Zhamak. 2022. \emph{Data Mesh: Delivering Data-Driven Value at
Scale}. O'Reilly Media.

\bibitem[\citeproctext]{ref-dwork2008differential}
Dwork, Cynthia. n.d. {``Differential Privacy: A Survey of Results.''} In
\emph{Theory and Applications of Models of Computation}, 1--19. Springer
Berlin Heidelberg. \url{https://doi.org/10.1007/978-3-540-79228-4/_1}.

\bibitem[\citeproctext]{ref-gama2014survey}
Gama, João, Indrė Žliobaitė, Albert Bifet, Mykola Pechenizkiy, and
Abdelhamid Bouchachia. 2014. {``A Survey on Concept Drift Adaptation.''}
\emph{ACM Computing Surveys} 46 (4): 1--37.
\url{https://doi.org/10.1145/2523813}.

\bibitem[\citeproctext]{ref-gebru2018datasheets}
Gebru, Timnit, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman
Vaughan, Hanna Wallach, Hal Daumé III, and Kate Crawford. 2021.
{``Datasheets for Datasets.''} \emph{Communications of the ACM} 64 (12):
86--92. \url{https://doi.org/10.1145/3458723}.

\bibitem[\citeproctext]{ref-groeneveld2024olmo}
Groeneveld, Dirk, Iz Beltagy, Pete Walsh, Akshita Bhagia, Rodney Kinney,
Oyvind Tafjord, Ananya Harsh Jha, et al. 2024. {``OLMo: Accelerating the
Science of Language Models.''} \emph{arXiv Preprint arXiv:2402.00838},
February. \url{http://arxiv.org/abs/2402.00838v4}.

\bibitem[\citeproctext]{ref-gudivada2017data}
Gudivada, Venkat N., Dhana Rao Rao, et al. 2017. {``Data Quality
Considerations for Big Data and Machine Learning: Going Beyond Data
Cleaning and Transformations.''} \emph{IEEE Transactions on Knowledge
and Data Engineering}.

\bibitem[\citeproctext]{ref-henderson2018deep}
Henderson, Peter, Riashat Islam, Philip Bachman, Joelle Pineau, Doina
Precup, and David Meger. 2018. {``Deep Reinforcement Learning That
Matters.''} \emph{Proceedings of the AAAI Conference on Artificial
Intelligence} 32 (1): 3207--14.
\url{https://doi.org/10.1609/aaai.v32i1.11694}.

\bibitem[\citeproctext]{ref-openimages_website}
Huang, Zongmo, Xiaorong Pu, Gongshun Tang, Ming Ping, Guo Jiang, Mengjie
Wang, Xiaoyu Wei, and Yazhou Ren. 2022. {``BS-80K: The First Large
Open-Access Dataset of Bone Scan Images.''} \emph{Computers in Biology
and Medicine} 151 (Pt A): 106221.
\url{https://doi.org/10.1016/j.compbiomed.2022.106221}.

\bibitem[\citeproctext]{ref-inmon2005building}
Inmon, W. H. 2005. \emph{Building the Data Warehouse}. John Wiley Sons.

\bibitem[\citeproctext]{ref-10.1109ux2fICRA.2017.7989092}
Johnson-Roberson, Matthew, Charles Barto, Rounak Mehta, Sharath Nittur
Sridhar, Karl Rosaen, and Ram Vasudevan. 2017. {``Driving in the Matrix:
Can Virtual Worlds Replace Human-Generated Annotations for Real World
Tasks?''} In \emph{2017 IEEE International Conference on Robotics and
Automation (ICRA)}, 746--53. Singapore, Singapore: IEEE.
\url{https://doi.org/10.1109/icra.2017.7989092}.

\bibitem[\citeproctext]{ref-kleppmann2017designing}
Kleppmann, Martin. 2016. \emph{Designing Data-Intensive Applications:
The Big Ideas Behind Reliable, Scalable, and Maintainable Systems}.
O'Reilly Media. \url{http://shop.oreilly.com/product/0636920032175.do}.

\bibitem[\citeproctext]{ref-krishnan2022selfsupervised}
Krishnan, Rayan, Pranav Rajpurkar, and Eric J. Topol. 2022.
{``Self-Supervised Learning in Medicine and Healthcare.''} \emph{Nature
Biomedical Engineering} 6 (12): 1346--52.
\url{https://doi.org/10.1038/s41551-022-00914-1}.

\bibitem[\citeproctext]{ref-krizhevsky2012imagenet}
Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E. Hinton. 2017.
{``ImageNet Classification with Deep Convolutional Neural Networks.''}
\emph{Communications of the ACM} 60 (6): 84--90.
\url{https://doi.org/10.1145/3065386}.

\bibitem[\citeproctext]{ref-kuhn2013applied}
Kuhn, Max, and Kjell Johnson. 2013. \emph{Applied Predictive Modeling}.
Springer New York. \url{https://doi.org/10.1007/978-1-4614-6849-3}.

\bibitem[\citeproctext]{ref-mturk_website}
Liebetrau, Judith, Stefanie Nowak, and Sebastian Schneider. 2014.
{``Evaluation of Image Annotation Using Amazon Mechanical Turk in
ImageCLEF.''} In \emph{Towards the Internet of Services: The THESEUS
Research Program}, 245--56. Springer International Publishing.
\url{https://doi.org/10.1007/978-3-319-06755-1/_20}.

\bibitem[\citeproctext]{ref-mazumder2021multilingual}
Mazumder, Mark, Sharad Chitlangia, Colby Banbury, Yiping Kang, Juan
Manuel Ciro, Keith Achorn, Daniel Galvez, et al. 2021. {``Multilingual
Spoken Words Corpus.''} In \emph{Thirty-Fifth Conference on Neural
Information Processing Systems Datasets and Benchmarks Track (Round 2)}.

\bibitem[\citeproctext]{ref-nayak2022improving}
Nayak, Prateeth, Takuya Higuchi, Anmol Gupta, Shivesh Ranjan, Stephen
Shum, Siddharth Sigtia, Erik Marchi, et al. 2022. {``Improving Voice
Trigger Detection with Metric Learning.''} \emph{arXiv Preprint
arXiv:2204.02455}, April. \url{http://arxiv.org/abs/2204.02455v2}.

\bibitem[\citeproctext]{ref-ng2021datacentric}
Ng, Andrew. 2021. {``MLOps: From Model-Centric to Data-Centric AI.''}
DeepLearning.AI.\href{\%0A\%20\%20\%20\%20https://www.deeplearning.ai/wp-content/uploads/2021/06/MLOps-From-Model-centric-to-Data-centric-AI.pdf\%0A\%20\%20}{https://www.deeplearning.ai/wp-content/uploads/2021/06/MLOps-From-Model-centric-to-Data-centric-AI.pdf
}.

\bibitem[\citeproctext]{ref-northcutt2021pervasive}
Northcutt, Curtis G., Anish Athalye, and Jonas Mueller. 2021.
{``Pervasive Label Errors in Test Sets Destabilize Machine Learning
Benchmarks.''} \emph{arXiv Preprint arXiv:2103.14749} 34 (March):
19075--90. \url{https://doi.org/10.48550/arxiv.2103.14749}.

\bibitem[\citeproctext]{ref-oakden2020hidden}
Oakden-Rayner, Luke, Jared Dunnmon, Gustavo Carneiro, and Christopher
Re. 2020. {``Hidden Stratification Causes Clinically Meaningful Failures
in Machine Learning for Medical Imaging.''} In \emph{Proceedings of the
ACM Conference on Health, Inference, and Learning}, 151--59. ACM.
\url{https://doi.org/10.1145/3368555.3384468}.

\bibitem[\citeproctext]{ref-park2019specaugment}
Park, Daniel S., William Chan, Yu Zhang, Chung-Cheng Chiu, Barret Zoph,
Ekin D. Cubuk, and Quoc V. Le. 2019. {``SpecAugment: A Simple Data
Augmentation Method for Automatic Speech Recognition.''} \emph{arXiv
Preprint arXiv:1904.08779}, April.
\url{http://arxiv.org/abs/1904.08779v3}.

\bibitem[\citeproctext]{ref-time_openai_kenya}
Perrigo, Billy. 2023. {``How Can We Make the Media Less Toxic?''}
\emph{Time}. The Conversation.
\url{https://doi.org/10.64628/aa.hxsu9mpgv}.

\bibitem[\citeproctext]{ref-pineau2021improving}
Pineau, Joelle, Philippe Vincent-Lamarre, Koustuv Sinha, Vincent
Larivière, Alina Beygelzimer, Florence d'Alché-Buc, Emily Fox, and Hugo
Larochelle. 2021. {``Improving Reproducibility in Machine Learning
Research (a Report from the Neurips 2019 Reproducibility Program).''}
\emph{Journal of Machine Learning Research} 22 (164): 1--20.

\bibitem[\citeproctext]{ref-ratner2018snorkel}
Ratner, Alex, Braden Hancock, Jared Dunnmon, Roger Goldman, and
Christopher Ré. 2018. {``Snorkel MeTaL: Weak Supervision for Multi-Task
Learning.''} In \emph{Proceedings of the Second Workshop on Data
Management for End-to-End Machine Learning}, 1--4. ACM.
\url{https://doi.org/10.1145/3209889.3209898}.

\bibitem[\citeproctext]{ref-sambasivan2021everyone}
Sambasivan, Nithya, Shivani Kapania, Hannah Highfill, Diana Akrong,
Praveen Paritosh, and Lora M Aroyo. 2021. {``{`Everyone Wants to Do the
Model Work, Not the Data Work'}: Data Cascades in High-Stakes AI.''} In
\emph{Proceedings of the 2021 CHI Conference on Human Factors in
Computing Systems}, 1--15. ACM.
\url{https://doi.org/10.1145/3411764.3445518}.

\bibitem[\citeproctext]{ref-harvard_law_chatgpt}
School, Harvard Law. 2024. {``Does ChatGPT Violate New York Times
Copyrights?''}
\url{https://hls.harvard.edu/today/does-chatgpt-violate-new-york-times-copyrights/}.

\bibitem[\citeproctext]{ref-sculley2015hidden}
Sculley, D., Gary Holt, Daniel Golovin, Eugene Davydov, Todd Phillips,
Dietmar Ebner, Vinay Chaudhary, Michael Young, Jean-François Crespo, and
Dan Dennison. 2021. {``Technical Debt in Machine Learning Systems.''} In
\emph{Technical Debt in Practice}, 28:177--92. The MIT Press.
\url{https://doi.org/10.7551/mitpress/12440.003.0011}.

\bibitem[\citeproctext]{ref-shorten2019survey}
Shorten, Connor, and Taghi M. Khoshgoftaar. 2019. {``A Survey on Image
Data Augmentation for Deep Learning.''} \emph{Journal of Big Data} 6
(1): 1--48. \url{https://doi.org/10.1186/s40537-019-0197-0}.

\bibitem[\citeproctext]{ref-kaggle_website}
Smith, Benjamin. 2025. {``RKaggle: 'Kaggle' Dataset Downloader 'API'.''}
The R Foundation. \url{https://doi.org/10.32614/cran.package.rkaggle}.

\bibitem[\citeproctext]{ref-uci_repo}
Srinivasan, Saravanan, Subathra Gunasekaran, Sandeep Kumar Mathivanan,
Benjula Anbu Malar M. B, Prabhu Jayagopal, and Gemmachis Teshite Dalu.
2023. {``An Active Learning Machine Technique Based Prediction of
Cardiovascular Heart Disease from UCI-Repository Database.''}
\emph{Scientific Reports} 13 (1): 13588.
\url{https://doi.org/10.1038/s41598-023-40717-1}.

\bibitem[\citeproctext]{ref-stonebraker2005cstore}
Stonebraker, Mike, Daniel J. Abadi, Adam Batkin, Xuedong Chen, Mitch
Cherniack, Miguel Ferreira, Edmond Lau, et al. 2018. {``C-Store: A
Column-Oriented DBMS.''} In \emph{Making Databases Work: The Pragmatic
Wisdom of Michael Stonebraker}, 491--518. Association for Computing
Machinery. \url{https://doi.org/10.1145/3226595.3226638}.

\bibitem[\citeproctext]{ref-thyagarajan2023multilabel}
Thyagarajan, Aditya, Elías Snorrason, Curtis G. Northcutt, and Jonas
Mueller 0001. 2022. {``Identifying Incorrect Annotations in Multi-Label
Classification Data.''} \emph{CoRR}.
\url{https://doi.org/10.48550/ARXIV.2211.13895}.

\bibitem[\citeproctext]{ref-venturebeat_datasets}
VentureBeat. 2024. {``3 Big Problems with Datasets in AI and Machine
Learning.''}\href{\%0A\%20\%20\%20\%20https://venturebeat.com/uncategorized/3-big-problems-with-datasets-in-ai-and-machine-learning/\%0A\%20\%20}{https://venturebeat.com/uncategorized/3-big-problems-with-datasets-in-ai-and-machine-learning/
}.

\bibitem[\citeproctext]{ref-wachter2017counterfactual}
Wachter, Sandra, Brent Mittelstadt, and Chris Russell. 2017.
{``Counterfactual Explanations Without Opening the Black Box: Automated
Decisions and the GDPR.''} \emph{SSRN Electronic Journal} 31: 841.
\url{https://doi.org/10.2139/ssrn.3063289}.

\bibitem[\citeproctext]{ref-wang2019balanced}
Wang, Tianlu, Jieyu Zhao, Mark Yatskar, Kai-Wei Chang, and Vicente
Ordonez. 2019. {``Balanced Datasets Are Not Enough: Estimating and
Mitigating Gender Bias in Deep Image Representations.''} In \emph{2019
IEEE/CVF International Conference on Computer Vision (ICCV)}, 5309--18.
IEEE. \url{https://doi.org/10.1109/iccv.2019.00541}.

\bibitem[\citeproctext]{ref-warden2018speech}
Warden, Pete. 2018. {``Speech Commands: A Dataset for Limited-Vocabulary
Speech Recognition.''} \emph{arXiv Preprint arXiv:1804.03209}, April.
\url{https://doi.org/10.48550/arXiv.1804.03209}.

\bibitem[\citeproctext]{ref-werchniak2021exploring}
Werchniak, Andrew, Roberto Barra Chicote, Yuriy Mishchenko, Jasha
Droppo, Jeff Condal, Peng Liu, and Anish Shah. 2021. {``Exploring the
Application of Synthetic Audio in Training Keyword Spotters.''} In
\emph{ICASSP 2021 - 2021 IEEE International Conference on Acoustics,
Speech and Signal Processing (ICASSP)}, 7993--96. IEEE; IEEE.
\url{https://doi.org/10.1109/icassp39728.2021.9413448}.

\bibitem[\citeproctext]{ref-armbrust2021lakehouse}
Zaharia, Matei, Ali Ghodsi 0002, Reynold Xin, and Michael Armbrust.
2021. {``Lakehouse: A New Generation of Open Platforms That Unify Data
Warehousing and Advanced Analytics.''} In \emph{Proceedings of CIDR}.
Vol. 8. \url{http://cidrdb.org/cidr2021/papers/cidr2021/_paper17.pdf}.

\bibitem[\citeproctext]{ref-zaharia2010spark}
Zaharia, Matei, Mosharaf Chowdhury, Michael J. Franklin, Scott Shenker,
and Ion Stoica. 2010. {``Spark: Cluster Computing with Working Sets.''}
In \emph{HotCloud}, 10:10--10.
\url{https://www.usenix.org/conference/hotcloud-10/spark-cluster-computing-working-sets}.

\end{CSLReferences}


\backmatter

\clearpage


\end{document}
