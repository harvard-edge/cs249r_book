% Options for packages loaded elsewhere
% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode,linktoc=all,pdfwindowui,pdfpagemode=FullScreen,pdfpagelayout=TwoPageRight}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  9pt,
  letterpaper,
  abstract,
  titlepage]{scrbook}
\usepackage{xcolor}
\usepackage[left=1in,marginparwidth=2.0666666666667in,textwidth=4.1333333333333in,marginparsep=0.3in]{geometry}
\usepackage{amsmath,amssymb}
\setcounter{secnumdepth}{3}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
% Make \paragraph and \subparagraph free-standing
\makeatletter
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}{
    \@ifstar
      \xxxParagraphStar
      \xxxParagraphNoStar
  }
  \newcommand{\xxxParagraphStar}[1]{\oldparagraph*{#1}\mbox{}}
  \newcommand{\xxxParagraphNoStar}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}{
    \@ifstar
      \xxxSubParagraphStar
      \xxxSubParagraphNoStar
  }
  \newcommand{\xxxSubParagraphStar}[1]{\oldsubparagraph*{#1}\mbox{}}
  \newcommand{\xxxSubParagraphNoStar}[1]{\oldsubparagraph{#1}\mbox{}}
\fi
\makeatother

\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{242,244,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{#1}}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.82,0.10,0.26}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{#1}}
\newcommand{\BuiltInTok}[1]{\textcolor[rgb]{0.88,0.40,0.10}{#1}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.41,0.45,0.49}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.86,0.20,0.18}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.58,0.00,0.30}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{#1}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.75,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.88,0.40,0.10}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.42,0.37,0.78}{#1}}
\newcommand{\ImportTok}[1]{\textcolor[rgb]{0.00,0.45,0.70}{\textbf{#1}}}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.73,0.49,0.84}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.10,0.10,0.10}{#1}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{#1}}
\newcommand{\RegionMarkerTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.12,0.55,0.30}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{#1}}}}

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}\usepackage{longtable,booktabs,array}
\usepackage{multirow}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\newsavebox\pandoc@box
\newcommand*\pandocbounded[1]{% scales image to fit in text height/width
  \sbox\pandoc@box{#1}%
  \Gscale@div\@tempa{\textheight}{\dimexpr\ht\pandoc@box+\dp\pandoc@box\relax}%
  \Gscale@div\@tempb{\linewidth}{\wd\pandoc@box}%
  \ifdim\@tempb\p@<\@tempa\p@\let\@tempa\@tempb\fi% select the smaller of both
  \ifdim\@tempa\p@<\p@\scalebox{\@tempa}{\usebox\pandoc@box}%
  \else\usebox{\pandoc@box}%
  \fi%
}
% Set default figure placement to htbp
\def\fps@figure{htbp}
\makeatother
% definitions for citeproc citations
\NewDocumentCommand\citeproctext{}{}
\NewDocumentCommand\citeproc{mm}{%
  \begingroup\def\citeproctext{#2}\cite{#1}\endgroup}
\makeatletter
 % allow citations to break across lines
 \let\@cite@ofmt\@firstofone
 % avoid brackets around text for \cite:
 \def\@biblabel#1{}
 \def\@cite#1#2{{#1\if@tempswa , #2\fi}}
\makeatother
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newenvironment{CSLReferences}[2] % #1 hanging-indent, #2 entry-spacing
 {\begin{list}{}{%
  \setlength{\itemindent}{0pt}
  \setlength{\leftmargin}{0pt}
  \setlength{\parsep}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1
   \setlength{\leftmargin}{\cslhangindent}
   \setlength{\itemindent}{-1\cslhangindent}
  \fi
  % set entry spacing
  \setlength{\itemsep}{#2\baselineskip}}}
 {\end{list}}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{\hfill\break\parbox[t]{\linewidth}{\strut\ignorespaces#1\strut}}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}

% =============================================================================
% LATEX HEADER CONFIGURATION FOR MLSYSBOOK PDF
% =============================================================================
% This file contains all LaTeX package imports, custom commands, and styling
% definitions for the PDF output of the Machine Learning Systems textbook.
%
% Key Features:
% - Harvard crimson branding throughout
% - Custom part/chapter/section styling
% - Professional table formatting with colored headers
% - Margin notes with custom styling
% - TikZ-based part dividers
% - Page numbering (Roman for frontmatter, Arabic for mainmatter)
%
% Note: This file is included via _quarto-pdf.yml and affects PDF output only.
% HTML/EPUB styling is handled separately via CSS files.
% =============================================================================

% =============================================================================
% PACKAGE IMPORTS
% =============================================================================

% Layout and positioning
% \usepackage[outercaption, ragged]{sidecap}  % Commented out to make figure captions inline instead of in margin
\usepackage{adjustbox}      % Adjusting box dimensions
\usepackage{afterpage}      % Execute commands after page break
\usepackage{morefloats}     % Increase number of floats
\usepackage{array}          % Enhanced table column formatting
\usepackage{atbegshi}       % Insert content at page beginning
%\usepackage{changepage}     % Change page dimensions mid-document
\usepackage{emptypage}      % Clear headers/footers on empty pages

% Language and text
\usepackage[english]{babel} % English language support
\usepackage{microtype}      % Improved typography and hyphenation

% Captions and floats
\usepackage{caption}
% Caption styling configuration
%\captionsetup[table]{belowskip=5pt}
\captionsetup{format=plain}
\DeclareCaptionLabelFormat{mylabel}{#1
#2:\hspace{1.0ex}}
\DeclareCaptionFont{ninept}{\fontsize{7pt}{8}\selectfont #1}

% Figure captions: Small font, bold label, ragged right
\captionsetup[figure]{labelfont={bf,ninept},labelsep=space,
belowskip=2pt,aboveskip=6pt,labelformat=mylabel,
justification=raggedright,singlelinecheck=false,font={ninept}}

% Table captions: Small font, bold label, ragged right
\captionsetup[table]{belowskip=6pt,labelfont={bf,ninept},labelsep=none,
labelformat=mylabel,justification=raggedright,singlelinecheck=false,font={ninept}}

% Typography fine-tuning
\emergencystretch=5pt       % Allow extra stretch to avoid overfull boxes

% Utility packages
\usepackage{etoolbox}       % For patching commands and environments

% Page layout and headers
\usepackage{fancyhdr}       % Custom headers and footers
\usepackage{geometry}       % Page dimensions and margins

% Graphics and figures
\usepackage{graphicx}       % Include graphics
\usepackage{float}          % Improved float placement
\usepackage[skins,breakable]{tcolorbox} % Coloured and framed text boxes
\tcbset{before upper=\setlength{\parskip}{3pt}}

% Tables
\usepackage{longtable}      % Multi-page tables

% Fonts and typography
\usepackage{fontspec}       % Font selection for LuaLaTeX
\usepackage{mathptmx}       % Times-like math fonts
\usepackage{newpxtext}      % Palatino-like font for body text

% Colors and visual elements
\usepackage[dvipsnames]{xcolor}  % Extended color support
\usepackage{tikz}           % Programmatic graphics
\usetikzlibrary{positioning}
\usetikzlibrary{calc}
\usepackage{tikzpagenodes}  % TikZ positioning relative to page

% Code listings
\usepackage{listings}       % Code highlighting

% Hyperlinks
\usepackage{hyperref}       % Clickable links in PDF

% Conditional logic
\usepackage{ifthen}         % If-then-else commands

% Math symbols
\usepackage{amsmath}        % AMS math extensions
\usepackage{amssymb}        % AMS math symbols
\usepackage{latexsym}       % Additional LaTeX symbols
\usepackage{pifont}         % Zapf Dingbats symbols
\providecommand{\blacklozenge}{\ding{117}}  % Black diamond symbol

% Lists
\usepackage{enumitem}       % Customizable lists

% Margin notes and sidenotes
\usepackage{marginfix}      % Fixes margin note overflow
\usepackage{marginnote}     % Margin notes
\usepackage{sidenotes}      % Academic-style sidenotes
\renewcommand\raggedrightmarginnote{\sloppy}
\renewcommand\raggedleftmarginnote{\sloppy}

% Typography improvements
\usepackage{ragged2e}       % Better ragged text
\usepackage[all]{nowidow}   % Prevent widows and orphans
\usepackage{needspace}      % Ensure minimum space on page

% Section formatting
\usepackage[explicit]{titlesec}  % Custom section titles
\usepackage{tocloft}        % Table of contents formatting

% QR codes and icons
\usepackage{fontawesome5}   % Font Awesome icons
\usepackage{qrcode}         % QR code generation
\qrset{link, height=15mm}

% =============================================================================
% FLOAT CONFIGURATION
% =============================================================================
% Allow more floats per page to handle figure-heavy chapters
\extrafloats{200}
\setcounter{topnumber}{12}       % Max floats at top of page
\setcounter{bottomnumber}{12}    % Max floats at bottom of page
\setcounter{totalnumber}{24}     % Max floats per page
\setcounter{dbltopnumber}{8}     % Max floats at top of two-column page
\renewcommand{\topfraction}{.95}  % Max fraction of page for top floats
\renewcommand{\bottomfraction}{.95}
\renewcommand{\textfraction}{.05}  % Min fraction of page for text
\renewcommand{\floatpagefraction}{.7}  % Min fraction of float page
\renewcommand{\dbltopfraction}{.95}

% Prevent "Float(s) lost" errors by flushing floats more aggressively
\usepackage{placeins}  % Provides \FloatBarrier

% =============================================================================
% COLOR DEFINITIONS
% =============================================================================
% Harvard crimson - primary brand color used throughout
\definecolor{crimson}{HTML}{A51C30}

% Quiz element colors
\definecolor{quiz-question-color1}{RGB}{225,243,248}  % Light blue background
\definecolor{quiz-question-color2}{RGB}{17,158,199}   % Blue border
\definecolor{quiz-answer-color1}{RGB}{250,234,241}    % Light pink background
\definecolor{quiz-answer-color2}{RGB}{152,14,90}      % Magenta border

% =============================================================================
% LIST FORMATTING
% =============================================================================
% Tighter list spacing for academic style
\def\tightlist{}
\setlist{itemsep=1pt, parsep=1pt, topsep=0pt,after={\vspace{0.3\baselineskip}}}
\let\tightlist\relax

\makeatletter
\@ifpackageloaded{framed}{}{\usepackage{framed}}
\@ifpackageloaded{fancyvrb}{}{\usepackage{fancyvrb}}
\makeatother

\makeatletter
%New float "codelisting" has been updated
\AtBeginDocument{%
\floatstyle{ruled}
\newfloat{codelisting}{!htb}{lop}
\floatname{codelisting}{Listing}
\floatplacement{codelisting}{!htb}
\captionsetup[codelisting]{labelfont={bf,ninept},labelformat=mylabel,
  singlelinecheck=false,width=\linewidth,labelsep=none,font={ninept}}%
\renewenvironment{snugshade}{%
   \def\OuterFrameSep{3pt}%
   \def\FrameCommand{\fboxsep=5pt\colorbox{shadecolor}}%
   \MakeFramed{\advance\hsize-\width\FrameRestore}%
   \leftskip 0.5em \rightskip 0.5em%
   \small% decrease font size
   }{\endMakeFramed}%
}
\makeatother

%The space before and after the verbatim environment "Highlighting" has been reduced
\fvset{listparameters=\setlength{\topsep}{0pt}\setlength{\partopsep}{0pt}}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{framesep=0mm,commandchars=\\\{\}}

\makeatletter
\renewcommand\fs@ruled{\def\@fs@cfont{\bfseries}\let\@fs@capt\floatc@ruled
\def\@fs@pre{\hrule height.8pt depth0pt \kern2pt}%
\def\@fs@post{\kern2pt\hrule\relax}%
\def\@fs@mid{\kern2pt\hrule\kern1pt}%space between float and caption
\let\@fs@iftopcapt\iftrue}
\makeatother


% =============================================================================
% HYPHENATION RULES
% =============================================================================
% Explicit hyphenation points for technical terms to avoid bad breaks
\hyphenation{
  light-weight
  light-weight-ed
  de-vel-op-ment
  un-der-stand-ing
  mod-els
  prin-ci-ples
  ex-per-tise
  com-pli-cat-ed
  blue-print
  per‧for‧mance
  com-mu-ni-ca-tion
  par-a-digms
  hy-per-ten-sion
  a-chieved
}

% =============================================================================
% CODE LISTING CONFIGURATION
% =============================================================================
% Settings for code blocks using listings package
\lstset{
breaklines=true,              % Automatic line wrapping
breakatwhitespace=true,       % Break at whitespace only
basicstyle=\ttfamily,         % Monospace font
frame=none,                   % No frame around code
keepspaces=true,              % Preserve spaces
showspaces=false,             % Don't show space characters
showtabs=false,               % Don't show tab characters
columns=flexible,             % Flexible column width
belowskip=0pt,               % Minimal spacing
aboveskip=0pt
}

% =============================================================================
% PAGE GEOMETRY
% =============================================================================
% MIT Press trim size: 7" x 10" (per publisher specifications)
% This is a standard academic textbook format providing good readability
% for technical content with figures and code blocks.
% Wide outer margin accommodates sidenotes/margin notes.
\geometry{
  paperwidth=7in,
  paperheight=10in,
  top=0.875in,
  bottom=0.875in,
  inner=0.875in,              % Inner margin (binding side)
  outer=1.75in,               % Outer margin (includes space for sidenotes)
  footskip=30pt,
  marginparwidth=1.25in,      % Width for margin notes
  twoside                     % Different left/right pages
}

% =============================================================================
% SIDENOTE STYLING
% =============================================================================
% Custom sidenote design with crimson vertical bar
\renewcommand{\thefootnote}{\textcolor{crimson}{\arabic{footnote}}}

% Save original sidenote command
\makeatletter
\@ifundefined{oldsidenote}{
  \let\oldsidenote\sidenote%
}{}
\makeatother

% Redefine sidenote with vertical crimson bar
\renewcommand{\sidenote}[1]{%
  \oldsidenote{%
    \noindent
    \color{crimson!100}                        % Crimson vertical line
    \raisebox{0em}{%
      \rule{0.5pt}{1.5em}                      % Thin vertical line
    }
    \hspace{0.3em}                             % Space after line
    \color{black}                              % Reset text color
    \footnotesize #1                           % Sidenote content
  }%
}

% =============================================================================
% FLOAT HANDLING
% =============================================================================
% Patch LaTeX's output routine to handle float overflow gracefully
% The "Float(s) lost" error occurs in \@doclearpage when \@currlist is not empty
% This patch silently clears pending floats that can't be placed
\makeatletter
\let\orig@doclearpage\@doclearpage
\def\@doclearpage{%
  \ifx\@currlist\@empty\else
    \global\let\@currlist\@empty
    \typeout{Warning: Floats cleared to prevent overflow}%
  \fi
  \orig@doclearpage
}
\makeatother

% Additional safety for structural commands
\let\originalbackmatter\backmatter
\renewcommand{\backmatter}{%
  \clearpage%
  \originalbackmatter%
}

\let\originalfrontmatter\frontmatter
\renewcommand{\frontmatter}{%
  \clearpage%
  \originalfrontmatter%
}

\let\originalmainmatter\mainmatter
\renewcommand{\mainmatter}{%
  \clearpage%
  \originalmainmatter%
}

% =============================================================================
% PAGE HEADERS AND FOOTERS
% =============================================================================
% Ensure chapters use fancy page style (not plain)
\patchcmd{\chapter}{\thispagestyle{plain}}{\thispagestyle{fancy}}{}{}

% Main page style with crimson headers
\pagestyle{fancy}
\fancyhf{}                                              % Clear all
\fancyhead[LE]{\small\color{crimson}\nouppercase{\rightmark}}  % Left even: section
\fancyhead[RO]{\color{crimson}\thepage}                 % Right odd: page number
\fancyhead[LO]{\small\color{crimson}\nouppercase{\leftmark}}   % Left odd: chapter
\fancyhead[RE]{\color{crimson}\thepage}                 % Right even: page number
\renewcommand{\headrulewidth}{0.4pt}                    % Thin header line
\renewcommand{\footrulewidth}{0pt}                      % No footer line

% Plain page style (for chapter openings)
\fancypagestyle{plain}{
  \fancyhf{}
  \fancyfoot[C]{\color{crimson}\thepage}                % Centered page number
  \renewcommand{\headrulewidth}{0pt}
  \renewcommand{\footrulewidth}{0pt}
}

% =============================================================================
% KOMA-SCRIPT FONT ADJUSTMENTS
% =============================================================================
% Apply crimson color to all heading levels
\addtokomafont{disposition}{\rmfamily\color{crimson}}
\addtokomafont{chapter}{\color{crimson}}
\addtokomafont{section}{\color{crimson}}
\addtokomafont{subsection}{\color{crimson}}

% =============================================================================
% ABSTRACT ENVIRONMENT
% =============================================================================
\newenvironment{abstract}{
  \chapter*{\abstractname}
  \addcontentsline{toc}{chapter}{\abstractname}
  \small
}{
  \clearpage
}

% =============================================================================
% HYPERLINK CONFIGURATION
% =============================================================================
% Crimson-colored links throughout, two-page PDF layout
\hypersetup{
  linkcolor=crimson,
  citecolor=crimson,
  urlcolor=crimson,
  pdfpagelayout=TwoPageRight,   % Two-page spread view
  pdfstartview=Fit               % Initial zoom fits page
}

% =============================================================================
% PART SUMMARY SYSTEM
% =============================================================================
% Allows adding descriptive text below part titles
\newcommand{\partsummary}{}     % Empty by default
\newif\ifhaspartsummary%
\haspartsummaryfalse%

\newcommand{\setpartsummary}[1]{%
  \renewcommand{\partsummary}{#1}%
  \haspartsummarytrue%
}

% Additional colors for part page backgrounds
\definecolor{BrownLL}{RGB}{233,222,220}
\definecolor{BlueDD}{RGB}{62,100,125}
\colorlet{BlueDD}{magenta}

% ===============================================================================
% PART STYLING SYSTEM
% ===============================================================================
%
% This system provides three distinct visual styles for book organization:
%
% 1. NUMBERED PARTS (\part{title}) - For main book sections
%    - Roman numerals (I, II, III, etc.) in top right corner
%    - Crimson title with horizontal lines above/below
%    - "Part I" label in sidebar
%    - Used for: foundations, principles, optimization, deployment, etc.
%
% 2. UNNUMBERED PARTS (\part*{title}) - For special sections like "Labs"
%    - Division-style geometric background (left side)
%    - No Roman numerals
%    - Used for: labs section
%
% 3. DIVISIONS (\division{title}) - For major book divisions
%    - Clean geometric background with centered title
%    - Used for: frontmatter, main_content, backmatter
%
% The Lua filter (inject-parts.lua) automatically routes parts by {key:xxx} commands
% to the appropriate LaTeX command based on the key name.
% ===============================================================================

% NUMBERED PARTS: Roman numeral styling for main book sections
\titleformat{\part}[display]
{\thispagestyle{empty}}{}{20pt}{
\begin{tikzpicture}[remember picture,overlay]
%%%
%%
\node[crimson,align=flush right,
inner sep=0,outer sep=0mm,draw=none,%
anchor=east,minimum height=31mm, text width=1.2\textwidth,
yshift=-30mm,font={%
\fontsize{98pt}{104}\selectfont\bfseries}]  (BG) at (current page text area.north east){\thepart};
%
\node[black,inner sep=0mm,draw=none,
anchor=mid,text width=1.2\textwidth,
 minimum height=35mm, align=right,
node distance=7mm,below=of BG,
font={\fontsize{30pt}{34}\selectfont}]
(BGG)  {\hyphenchar\font=-1 \color{black}\MakeUppercase {#1}};
\draw [crimson,line width=3pt] ([yshift=0mm]BGG.north west) -- ([yshift=0mm]BGG.north east);
\draw [crimson,line width=2pt] ([yshift=0mm]BGG.south west) -- ([yshift=0mm]BGG.south east);
%
\node[fill=crimson,text=white,rotate=90,%
anchor=south west,minimum height=15mm,
minimum width=40mm,font={%
\fontsize{20pt}{20}\selectfont\bfseries}](BP)  at
(current page text area.south east)
{{\sffamily Part}~\thepart};
%
\path[red](BP.north west)-|coordinate(PS)(BGG.south west);
%
% Part summary box commented out for cleaner design
% \ifhaspartsummary
% \node[inner sep=4pt,text width=0.7\textwidth,draw=none,fill=BrownLL!40,
% align=justify,font={\fontsize{9pt}{12}\selectfont},anchor=south west]
% at (PS) {\partsummary};
% \fi
\end{tikzpicture}
}[]

\renewcommand{\thepart}{\Roman{part}}

% UNNUMBERED PARTS: Division-style background for special sections
\titleformat{name=\part,numberless}[display]
{\thispagestyle{empty}}{}{20pt}{
\begin{tikzpicture}[remember picture,overlay]
%%%
\coordinate(S1)at([yshift=-200mm]current page.north west);
\draw[draw=none,fill=BlueDD!7](S1)--++(45:16)coordinate(S2)-
|(S2|-current page.north west)--(current page.north west)coordinate(S3)--(S1);
%
\coordinate(E1)at([yshift=-98mm]current page.north west);
\draw[draw=none,fill=BlueDD!15](E1)--(current page.north west)coordinate(E2)
--++(0:98mm)coordinate(E3)--(E1);
%
\coordinate(D1)at([yshift=15mm]current page.south west);
\draw[draw=none,fill=BlueDD!40,opacity=0.5](D1)--++(45:5.5)coordinate(D2)
-|(D2|-current page.north west)--(current page.north west)coordinate(D3)--(D1);
%%%%
\path[red](S2)-|(S2-|current page.east)coordinate(SS2);
%PART
\node[crimson,align=flush right,inner sep=0,outer sep=0mm,draw=none,anchor=south,
font={\fontsize{48pt}{48}\selectfont\bfseries}]  (BG) at ($(S2)!0.5!(SS2)$){\hphantom{Part}};
%%%
\path[green]([yshift=15mm]D2)-|coordinate(TPD)(BG.south east);
\node[inner sep=0mm,draw=none,anchor=south east,%text width=0.9\textwidth,
align=right,font={\fontsize{40pt}{40}\selectfont}]
(BGG) at (TPD)  {\color{crimson}\MakeUppercase {#1}};%\MakeUppercase {}
\end{tikzpicture}
}

% Define \numberedpart command for numbered parts
\newcommand{\numberedpart}[1]{%
\FloatBarrier%  % Flush all pending floats before part break
\clearpage
\thispagestyle{empty}
\stepcounter{part}%
\begin{tikzpicture}[remember picture,overlay]
%%%
%%
\node[crimson,align=flush right,
inner sep=0,outer sep=0mm,draw=none,%
anchor=east,minimum height=31mm, text width=1.2\textwidth,
yshift=-30mm,font={%
\fontsize{98pt}{104}\selectfont\bfseries}]  (BG) at (current page text area.north east){\thepart};
%
\node[black,inner sep=0mm,draw=none,
anchor=mid,text width=1.2\textwidth,
 minimum height=35mm, align=right,
node distance=7mm,below=of BG,
font={\fontsize{30pt}{34}\selectfont}]
(BGG)  {\hyphenchar\font=-1 \color{black}\MakeUppercase {#1}};
\draw [crimson,line width=3pt] ([yshift=0mm]BGG.north west) -- ([yshift=0mm]BGG.north east);
\draw [crimson,line width=2pt] ([yshift=0mm]BGG.south west) -- ([yshift=0mm]BGG.south east);
%
\node[fill=crimson,text=white,rotate=90,%
anchor=south west,minimum height=15mm,
minimum width=40mm,font={%
\fontsize{20pt}{20}\selectfont\bfseries}](BP)  at
(current page text area.south east)
{{\sffamily Part}~\thepart};
%
\path[red](BP.north west)-|coordinate(PS)(BGG.south west);
%
% Part summary box commented out for cleaner design
% \ifhaspartsummary
% \node[inner sep=4pt,text width=0.7\textwidth,draw=none,fill=BrownLL!40,
% align=justify,font={\fontsize{9pt}{12}\selectfont},anchor=south west]
% at (PS) {\partsummary};
% \fi
\end{tikzpicture}
\clearpage
}



% DIVISIONS: Clean geometric styling with subtle tech elements
% Used for frontmatter, main_content, and backmatter divisions
\newcommand{\division}[1]{%
\FloatBarrier%  % Flush all pending floats before division break
\clearpage
\thispagestyle{empty}
\begin{tikzpicture}[remember picture,overlay]

% Clean geometric background (original design)
\coordinate(S1)at([yshift=-200mm]current page.north west);
\draw[draw=none,fill=BlueDD!7](S1)--++(45:16)coordinate(S2)-
|(S2|-current page.north west)--(current page.north west)coordinate(S3)--(S1);

\coordinate(E1)at([yshift=-98mm]current page.north west);
\draw[draw=none,fill=BlueDD!15](E1)--(current page.north west)coordinate(E2)
--++(0:98mm)coordinate(E3)--(E1);

\coordinate(D1)at([yshift=15mm]current page.south west);
\draw[draw=none,fill=BlueDD!40,opacity=0.5](D1)--++(45:5.5)coordinate(D2)
-|(D2|-current page.north west)--(current page.north west)coordinate(D3)--(D1);

% Subtle tech elements - positioned in white areas for better visibility
% Upper right white area - more visible
\draw[crimson!40, line width=0.8pt] ([xshift=140mm,yshift=-60mm]current page.north west) -- ++(40mm,0);
\draw[crimson!40, line width=0.8pt] ([xshift=150mm,yshift=-70mm]current page.north west) -- ++(30mm,0);
\draw[crimson!35, line width=0.7pt] ([xshift=160mm,yshift=-60mm]current page.north west) -- ++(0,-15mm);
\draw[crimson!35, line width=0.7pt] ([xshift=170mm,yshift=-70mm]current page.north west) -- ++(0,10mm);

% Circuit nodes - upper right
\fill[crimson!50] ([xshift=160mm,yshift=-60mm]current page.north west) circle (1.5mm);
\fill[white] ([xshift=160mm,yshift=-60mm]current page.north west) circle (0.8mm);
\fill[crimson!50] ([xshift=170mm,yshift=-70mm]current page.north west) circle (1.3mm);
\fill[white] ([xshift=170mm,yshift=-70mm]current page.north west) circle (0.6mm);

% Lower right white area - enhanced visibility
\draw[crimson!45, line width=0.9pt] ([xshift=140mm,yshift=-190mm]current page.north west) -- ++(45mm,0);
\draw[crimson!45, line width=0.9pt] ([xshift=150mm,yshift=-200mm]current page.north west) -- ++(35mm,0);
\draw[crimson!40, line width=0.8pt] ([xshift=160mm,yshift=-190mm]current page.north west) -- ++(0,-20mm);
\draw[crimson!40, line width=0.8pt] ([xshift=170mm,yshift=-200mm]current page.north west) -- ++(0,15mm);

% Additional connecting lines in lower right
\draw[crimson!35, line width=0.7pt] ([xshift=130mm,yshift=-180mm]current page.north west) -- ++(25mm,0);
\draw[crimson!35, line width=0.7pt] ([xshift=145mm,yshift=-180mm]current page.north west) -- ++(0,-25mm);

% Circuit nodes - lower right (more prominent)
\fill[crimson!55] ([xshift=160mm,yshift=-190mm]current page.north west) circle (1.6mm);
\fill[white] ([xshift=160mm,yshift=-190mm]current page.north west) circle (0.9mm);
\fill[crimson!55] ([xshift=170mm,yshift=-200mm]current page.north west) circle (1.4mm);
\fill[white] ([xshift=170mm,yshift=-200mm]current page.north west) circle (0.7mm);
\fill[crimson!50] ([xshift=145mm,yshift=-180mm]current page.north west) circle (1.2mm);
\fill[white] ([xshift=145mm,yshift=-180mm]current page.north west) circle (0.6mm);

% Title positioned in center - clean and readable
\node[inner sep=0mm,draw=none,anchor=center,text width=0.8\textwidth,
align=center,font={\fontsize{40pt}{40}\selectfont}]
(BGG) at (current page.center)  {\color{crimson}\MakeUppercase {#1}};

\end{tikzpicture}
\clearpage
}

% LAB DIVISIONS: Circuit-style neural network design for lab sections
% Used specifically for lab platform sections (arduino, xiao, grove, etc.)
\newcommand{\labdivision}[1]{%
\FloatBarrier%  % Flush all pending floats before lab division break
\clearpage
\thispagestyle{empty}
\begin{tikzpicture}[remember picture,overlay]
% Circuit background with subtle gradient
\coordinate(S1)at([yshift=-200mm]current page.north west);
\draw[draw=none,fill=BlueDD!5](S1)--++(45:16)coordinate(S2)-
|(S2|-current page.north west)--(current page.north west)coordinate(S3)--(S1);

% TOP AREA: Circuit lines in upper white space
\draw[crimson!50, line width=1.5pt] ([xshift=30mm,yshift=-40mm]current page.north west) -- ++(60mm,0);
\draw[crimson!40, line width=1pt] ([xshift=120mm,yshift=-50mm]current page.north west) -- ++(50mm,0);
\draw[crimson!50, line width=1.5pt] ([xshift=40mm,yshift=-70mm]current page.north west) -- ++(40mm,0);

% Connecting lines in top area
\draw[crimson!30, line width=1pt] ([xshift=60mm,yshift=-40mm]current page.north west) -- ++(0,-20mm);
\draw[crimson!30, line width=1pt] ([xshift=145mm,yshift=-50mm]current page.north west) -- ++(0,10mm);

% Neural nodes in top area
\fill[crimson!70] ([xshift=60mm,yshift=-40mm]current page.north west) circle (2.5mm);
\fill[white] ([xshift=60mm,yshift=-40mm]current page.north west) circle (1.5mm);
\fill[crimson!60] ([xshift=145mm,yshift=-50mm]current page.north west) circle (2mm);
\fill[white] ([xshift=145mm,yshift=-50mm]current page.north west) circle (1mm);
\fill[crimson!80] ([xshift=80mm,yshift=-70mm]current page.north west) circle (2mm);
\fill[white] ([xshift=80mm,yshift=-70mm]current page.north west) circle (1mm);

% BOTTOM AREA: Circuit lines in lower white space
\draw[crimson!50, line width=1.5pt] ([xshift=20mm,yshift=-200mm]current page.north west) -- ++(70mm,0);
\draw[crimson!40, line width=1pt] ([xshift=110mm,yshift=-210mm]current page.north west) -- ++(60mm,0);
\draw[crimson!50, line width=1.5pt] ([xshift=35mm,yshift=-230mm]current page.north west) -- ++(45mm,0);

% Connecting lines in bottom area
\draw[crimson!30, line width=1pt] ([xshift=55mm,yshift=-200mm]current page.north west) -- ++(0,-20mm);
\draw[crimson!30, line width=1pt] ([xshift=140mm,yshift=-210mm]current page.north west) -- ++(0,15mm);

% Neural nodes in bottom area
\fill[crimson!70] ([xshift=55mm,yshift=-200mm]current page.north west) circle (2.5mm);
\fill[white] ([xshift=55mm,yshift=-200mm]current page.north west) circle (1.5mm);
\fill[crimson!60] ([xshift=140mm,yshift=-210mm]current page.north west) circle (2mm);
\fill[white] ([xshift=140mm,yshift=-210mm]current page.north west) circle (1mm);
\fill[crimson!80] ([xshift=80mm,yshift=-230mm]current page.north west) circle (2mm);
\fill[white] ([xshift=80mm,yshift=-230mm]current page.north west) circle (1mm);

% SIDE AREAS: Subtle circuit elements on left and right edges
\draw[crimson!30, line width=1pt] ([xshift=15mm,yshift=-120mm]current page.north west) -- ++(20mm,0);
\draw[crimson!30, line width=1pt] ([xshift=175mm,yshift=-130mm]current page.north west) -- ++(15mm,0);
\fill[crimson!50] ([xshift=25mm,yshift=-120mm]current page.north west) circle (1.5mm);
\fill[white] ([xshift=25mm,yshift=-120mm]current page.north west) circle (0.8mm);
\fill[crimson!50] ([xshift=185mm,yshift=-130mm]current page.north west) circle (1.5mm);
\fill[white] ([xshift=185mm,yshift=-130mm]current page.north west) circle (0.8mm);

% Title positioned in center - CLEAN AREA
\node[inner sep=0mm,draw=none,anchor=center,text width=0.8\textwidth,
align=center,font={\fontsize{44pt}{44}\selectfont\bfseries}]
(BGG) at (current page.center)  {\color{crimson}\MakeUppercase {#1}};

\end{tikzpicture}
\clearpage
}

% Define \lab command for lab styling (different visual treatment)
\newcommand{\lab}[1]{%
\begin{tikzpicture}[remember picture,overlay]
%%%
% Different background pattern for labs
\coordinate(S1)at([yshift=-200mm]current page.north west);
\draw[draw=none,fill=BlueDD!15](S1)--++(45:16)coordinate(S2)-
|(S2|-current page.north west)--(current page.north west)coordinate(S3)--(S1);
%
\coordinate(E1)at([yshift=-98mm]current page.north west);
\draw[draw=none,fill=BlueDD!25](E1)--(current page.north west)coordinate(E2)
--++(0:98mm)coordinate(E3)--(E1);
%
\coordinate(D1)at([yshift=15mm]current page.south west);
\draw[draw=none,fill=BlueDD!60,opacity=0.7](D1)--++(45:5.5)coordinate(D2)
-|(D2|-current page.north west)--(current page.north west)coordinate(D3)--(D1);
%%%%
\path[red](S2)-|(S2-|current page.east)coordinate(SS2);
%LAB - Different styling
\node[crimson,align=flush right,inner sep=0,outer sep=0mm,draw=none,anchor=south,
font={\fontsize{48pt}{48}\selectfont\bfseries}]  (BG) at ($(S2)!0.5!(SS2)$){\hphantom{Workshop}};
%%%
\path[green]([yshift=15mm]D2)-|coordinate(TPD)(BG.south east);
\node[inner sep=0mm,draw=none,anchor=south east,%text width=0.9\textwidth,
align=right,font={\fontsize{40pt}{40}\selectfont}]
(BGG) at (TPD)  {\color{crimson}\MakeUppercase {#1}};%\MakeUppercase {}
\end{tikzpicture}
\thispagestyle{empty}
\clearpage
}

% =============================================================================
% SECTION FORMATTING
% =============================================================================
% All section levels use crimson color and are ragged right

% Section (Large, bold, crimson)
\titleformat{\section}
  {\normalfont\Large\bfseries\color{crimson}\raggedright}
  {\thesection}
  {0.5em}
  {#1}
\titlespacing*{\section}{0pc}{14pt plus 4pt minus 4pt}{6pt plus 2pt minus 2pt}[0pc]

% Subsection (large, bold, crimson)
\titleformat{\subsection}
  {\normalfont\large\bfseries\color{crimson}\raggedright}
  {\thesubsection}
  {0.5em}
  {#1}
\titlespacing*{\subsection}{0pc}{12pt plus 4pt minus 4pt}{5pt plus 1pt minus 2pt}[0pc]

% Subsubsection (normal size, bold, crimson)
\titleformat{\subsubsection}
  {\normalfont\normalsize\bfseries\color{crimson}\raggedright}
  {\thesubsubsection}
  {0.5em}
  {#1}
\titlespacing*{\subsubsection}{0pc}{12pt plus 4pt minus 4pt}{5pt plus 1pt minus 2pt}[0pc]

% Paragraph (run-in, bold, crimson, ends with period)
\titleformat{\paragraph}[runin]
  {\normalfont\normalsize\bfseries\color{crimson}}
  {\theparagraph}
  {0.5em}
  {#1}
  [\textbf{.}]
  \titlespacing*{\paragraph}{0pc}{6pt plus 2pt minus 2pt}{0.5em}[0pc]

% Subparagraph (run-in, italic, crimson, ends with period)
\titleformat{\subparagraph}[runin]
  {\normalfont\normalsize\itshape\color{crimson}}
  {\thesubparagraph}
  {0.5em}
  {#1}
  [\textbf{.}]
  \titlespacing*{\subparagraph}{0pc}{6pt plus 2pt minus 2pt}{0.5em}[0pc]

% =============================================================================
% CHAPTER FORMATTING
% =============================================================================
% Numbered chapters: "Chapter X" prefix, huge crimson title
\titleformat{\chapter}[display]
  {\normalfont\huge\bfseries\color{crimson}}
  {\chaptername\ \thechapter}
  {20pt}
  {\Huge #1}
  []

% Unnumbered chapters: no prefix, huge crimson title
\titleformat{name=\chapter,numberless}
  {\normalfont\huge\bfseries\color{crimson}}
  {}
  {0pt}
  {\Huge #1}
  []

\renewcommand{\chaptername}{Chapter}
% =============================================================================
% TABLE OF CONTENTS FORMATTING
% =============================================================================
\setcounter{tocdepth}{2}                      % Show chapters, sections, subsections

% TOC spacing adjustments for number widths and indentation
\setlength{\cftchapnumwidth}{2em}             % Chapter number width
\setlength{\cftsecnumwidth}{2.75em}           % Section number width
\setlength{\cftsubsecnumwidth}{3.25em}        % Subsection number width
\setlength{\cftsubsubsecnumwidth}{4em}        % Subsubsection number width
\setlength{\cftsubsecindent}{4.25em}          % Subsection indent
\setlength{\cftsubsubsecindent}{7.5em}        % Subsubsection indent

% Chapter entries in TOC: bold crimson with "Chapter" prefix
\renewcommand{\cftchapfont}{\bfseries\color{crimson}}
\renewcommand{\cftchappresnum}{\color{crimson}Chapter~}

% Custom formatting for division entries (styled like parts)
\newcommand{\divisionchapter}[1]{%
  \addvspace{12pt}%
  \noindent\hfil\bfseries\color{crimson}#1\hfil\par%
  \addvspace{6pt}%
}

% Adjust TOC spacing for "Chapter" prefix
\newlength{\xtraspace}
\settowidth{\xtraspace}{\cftchappresnum\cftchapaftersnum}
\addtolength{\cftchapnumwidth}{\xtraspace}

% Unnumbered chapters with TOC entry
\newcommand{\likechapter}[1]{%
    \chapter*{#1}
    \addcontentsline{toc}{chapter}{\textcolor{crimson}{#1}}
}

% =============================================================================
% PAGE NUMBERING SYSTEM
% =============================================================================
% Implements traditional book numbering:
% - Roman numerals (i, ii, iii...) for frontmatter
% - Arabic numerals (1, 2, 3...) for mainmatter
% Automatically switches at first numbered chapter
\makeatletter
\newif\if@firstnumbered%
\@firstnumberedtrue%
\newif\if@firstunnumbered%
\@firstunnumberedtrue%

\newcounter{lastRomanPage}
\setcounter{lastRomanPage}{1}

% Start document with Roman numerals (frontmatter)
\AtBeginDocument{
  \pagenumbering{roman}
  \renewcommand{\thepage}{\roman{page}}
}

% Intercept chapter command
\let\old@chapter\chapter%
\renewcommand{\chapter}{%
  \@ifstar{\unnumbered@chapter}{\numbered@chapter}%
}

% Numbered chapters: switch to Arabic on first occurrence
\newcommand{\numbered@chapter}[1]{%
  \if@firstnumbered%
    \cleardoublepage%
    \setcounter{lastRomanPage}{\value{page}}%
    \pagenumbering{arabic}%
    \@firstnumberedfalse%
  \else
    \setcounter{page}{\value{page}}%
  \fi
  \setcounter{sidenote}{1}                    % Reset footnote counter per chapter
  \old@chapter{#1}%
}

% Unnumbered chapters: stay in Roman numerals
\newcommand{\unnumbered@chapter}[1]{%
  \if@firstunnumbered%
    \clearpage
    \setcounter{lastRomanPage}{\value{page}}%
    \pagenumbering{roman}%
    \@firstunnumberedfalse%
  \fi
  \setcounter{sidenote}{1}
  \old@chapter*{#1}%
}
\makeatother

% =============================================================================
% TABLE SIZING AND SPACING
% =============================================================================
% Make tables slightly smaller to fit more content
\AtBeginEnvironment{longtable}{\scriptsize}

% Increase vertical spacing in table cells (default is 1.0)
\renewcommand{\arraystretch}{1.3}

% Prefer placing tables at the top of pages
\makeatletter
\renewcommand{\fps@table}{t}  % Default placement: top of page
\makeatother

% =============================================================================
% LONGTABLE PAGE BREAKING FIXES (Windows compatibility)
% =============================================================================
% Prevent "Infinite glue shrinkage" errors on Windows LaTeX builds
% by giving longtable more flexibility in page breaking

% Allow more flexible page breaking (vs strict \flushbottom)
\raggedbottom

% Process more rows before attempting page break (default is 20)
\setcounter{LTchunksize}{50}

% Add extra stretch for longtable environments specifically
\AtBeginEnvironment{longtable}{%
  \setlength{\emergencystretch}{3em}%
  \setlength{\parskip}{0pt plus 1pt}%
}

% =============================================================================
% TABLE STYLING - Clean tables with crimson borders
% =============================================================================
% Professional table appearance with:
% - Clean white background (no colored rows)
% - Crimson-colored borders
% - Good spacing for readability
%
% Note: Headers are automatically bolded by Quarto when using **text** in source
\usepackage{booktabs}      % Professional table rules (\toprule, \midrule, \bottomrule)
\usepackage{colortbl}      % For colored borders (\arrayrulecolor)

% Global table styling - crimson borders
\setlength{\arrayrulewidth}{0.5pt}          % Thinner borders than default
%\arrayrulecolor{crimson}                    % Crimson borders matching brand

\setcounter{chapter}{0}
\usepackage{needspace}
\let\Needspace\needspace
\makeatletter
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{plain}
\@ifundefined{c@chapter}{\newfloat{vid}{h}{lovid}}{\newfloat{vid}{h}{lovid}[chapter]}
\floatname{vid}{Video}
\newcommand*\listofvids{\listof{vid}{List of Videos}}
\makeatother
\makeatletter
\@ifpackageloaded{tcolorbox}{}{\usepackage[skins,breakable]{tcolorbox}}
\@ifpackageloaded{fontawesome5}{}{\usepackage{fontawesome5}}
\definecolor{quarto-callout-color}{HTML}{909090}
\definecolor{quarto-callout-note-color}{HTML}{0758E5}
\definecolor{quarto-callout-important-color}{HTML}{CC1914}
\definecolor{quarto-callout-warning-color}{HTML}{EB9113}
\definecolor{quarto-callout-tip-color}{HTML}{00A047}
\definecolor{quarto-callout-caution-color}{HTML}{FC5300}
\definecolor{quarto-callout-color-frame}{HTML}{acacac}
\definecolor{quarto-callout-note-color-frame}{HTML}{4582ec}
\definecolor{quarto-callout-important-color-frame}{HTML}{d9534f}
\definecolor{quarto-callout-warning-color-frame}{HTML}{f0ad4e}
\definecolor{quarto-callout-tip-color-frame}{HTML}{02b875}
\definecolor{quarto-callout-caution-color-frame}{HTML}{fd7e14}
\makeatother
\makeatletter
\@ifpackageloaded{bookmark}{}{\usepackage{bookmark}}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\makeatletter
\@ifpackageloaded{sidenotes}{}{\usepackage{sidenotes}}
\@ifpackageloaded{marginnote}{}{\usepackage{marginnote}}
\makeatother
\newcommand{\fbxIconPath}{assets/images/icons/callouts}
\newcommand{\fbxIconFormat}{pdf}
\makeatletter
\@ifpackageloaded{tcolorbox}{}{\usepackage[many]{tcolorbox}}
\makeatother
%%%% ---foldboxy preamble ----- %%%%%

% Load xstring for string manipulation
\RequirePackage{xstring}

% Icon path and format configuration - can be overridden in filter-metadata
\providecommand{\fbxIconPath}{assets/images/icons/callouts}
\providecommand{\fbxIconFormat}{pdf}

% Helper command to include icon with hyphen-to-underscore conversion
% This ensures consistency: callout-quiz-question -> callout_quiz_question
\newcommand{\fbxIncludeIcon}[2]{%
  \StrSubstitute{#1}{-}{_}[\fbxIconName]%
  \includegraphics[width=#2]{\fbxIconPath/icon_\fbxIconName.\fbxIconFormat}%
}

% Legacy fallback colors (keep for compatibility)
\definecolor{fbx-default-color1}{HTML}{c7c7d0}
\definecolor{fbx-default-color2}{HTML}{a3a3aa}
\definecolor{fbox-color1}{HTML}{c7c7d0}
\definecolor{fbox-color2}{HTML}{a3a3aa}

% arguments: #1 typelabelnummer: #2 titel: #3
\newenvironment{fbx}[3]{%
\begin{tcolorbox}[
  enhanced,
  breakable,
  %fontupper=\fontsize{8pt}{10pt}\selectfont,  % 95% of body text (10pt -> 9.5pt)
  before skip=8pt,  % space above box (increased)
  after skip=8pt,   % space below box (increased)
  attach boxed title to top*={xshift=0pt},
  boxed title style={
  %fuzzy shadow={1pt}{-1pt}{0mm}{0.1mm}{gray},
  arc=1.5pt,
  rounded corners=north,
  sharp corners=south,
  top=6pt,          % Adjusted for ~40px equivalent height
  bottom=5pt,       % Adjusted for ~40px equivalent height
  overlay={
      \node [left,outer sep=0em, black,draw=none,anchor=west,
        rectangle,fill=none,inner sep=0pt]
        at ([xshift=4mm]frame.west) {\fbxIncludeIcon{#1}{4.2mm}};
    },
  },
  colframe=#1-color2,             % Border color (auto-generated from YAML)
  colbacktitle=#1-color1,         % Background color (auto-generated from YAML)
  colback=white,
  coltitle=black,
  titlerule=0mm,
  toprule=0.5pt,
  bottomrule=0.5pt,
  leftrule=2.2pt,
  rightrule=0.5pt,
  outer arc=1.5pt,
  arc=1.5pt,
  left=0.5em,       % increased left padding
  bottomtitle=1.5mm, % increased title bottom margin
  toptitle=1.5mm,    % increased title top margin
  title=\hspace{2.5em}\protect#2\hspace{0.2em}\protect#3, % Protect parameters
  extras middle and last={top=4pt} % increased continuation spacing
]}
{\end{tcolorbox}}


% boxed environment with right border
\newenvironment{fbxSimple}[3]{\begin{tcolorbox}[
  enhanced,
  breakable,
  %fontupper=\fontsize{8pt}{10pt}\selectfont,  % 95% of body text (10pt -> 9.5pt)
  before skip=8pt,  % space above box (increased)
  after skip=8pt,   % space below box (increased)
  attach boxed title to top*={xshift=0pt},
  boxed title style={
  %fuzzy shadow={1pt}{-1pt}{0mm}{0.1mm}{gray},
  arc=1.5pt,
  rounded corners=north,
  sharp corners=south,
  top=6pt,          % Adjusted for ~40px equivalent height
  bottom=5pt,       % Adjusted for ~40px equivalent height
  overlay={
      \node [left,outer sep=0em, black,draw=none,anchor=west,
        rectangle,fill=none,inner sep=0pt]
        at ([xshift=3mm]frame.west) {\fbxIncludeIcon{#1}{4.2mm}};
    },
  },
  colframe=#1-color2,             % Border color (auto-generated from YAML)
  colbacktitle=#1-color1,         % Background color (auto-generated from YAML)
  colback=white,
  coltitle=black,
  titlerule=0mm,
  toprule=0.5pt,
  bottomrule=0.5pt,
  leftrule=2.2pt,
  rightrule=0.5pt,
  outer arc=1.5pt,
  arc=1.5pt,
  left=0.5em,       % increased left padding
  bottomtitle=1.5mm, % increased title bottom margin
  toptitle=1.5mm,    % increased title top margin
  title=\hspace{2.5em}\protect#2\hspace{0.2em}\protect#3, % Protect parameters
  boxsep=1pt,
  extras first={bottom=0pt},
  extras last={top=0pt,bottom=-4pt},
  overlay first={
    \draw[line width=1pt,white] ([xshift=2.2pt]frame.south west)-- ([xshift=-0.5pt]frame.south east);
  },
  overlay last={
    \draw[line width=1pt,white] ([xshift=2.2pt]frame.north west)-- ([xshift=-0.5pt]frame.north east);
   }
]}
{\end{tcolorbox}}

%%%% --- end foldboxy preamble ----- %%%%%
%%==== colors from yaml ===%
\definecolor{callout-colab-color1}{HTML}{FFF5E6}
\definecolor{callout-colab-color2}{HTML}{FF6B35}
\definecolor{callout-definition-color1}{HTML}{F0F4F8}
\definecolor{callout-definition-color2}{HTML}{1B4F72}
\definecolor{callout-notebook-color1}{HTML}{F2F7FF}
\definecolor{callout-notebook-color2}{HTML}{2C5282}
\definecolor{callout-resource-exercises-color1}{HTML}{E0F2F1}
\definecolor{callout-resource-exercises-color2}{HTML}{20B2AA}
\definecolor{callout-quiz-answer-color1}{HTML}{E8F2EA}
\definecolor{callout-quiz-answer-color2}{HTML}{4a7c59}
\definecolor{callout-resource-videos-color1}{HTML}{E0F2F1}
\definecolor{callout-resource-videos-color2}{HTML}{20B2AA}
\definecolor{callout-quiz-question-color1}{HTML}{F0F0F8}
\definecolor{callout-quiz-question-color2}{HTML}{5B4B8A}
\definecolor{callout-perspective-color1}{HTML}{F7F8FA}
\definecolor{callout-perspective-color2}{HTML}{4A5568}
\definecolor{callout-example-color1}{HTML}{F0F8F6}
\definecolor{callout-example-color2}{HTML}{148F77}
\definecolor{callout-code-color1}{HTML}{F2F4F8}
\definecolor{callout-code-color2}{HTML}{D1D7E0}
\definecolor{callout-resource-slides-color1}{HTML}{E0F2F1}
\definecolor{callout-resource-slides-color2}{HTML}{20B2AA}
\definecolor{callout-lighthouse-color1}{HTML}{FDF8E6}
\definecolor{callout-lighthouse-color2}{HTML}{B8860B}
\definecolor{callout-checkpoint-color1}{HTML}{E8F5E9}
\definecolor{callout-checkpoint-color2}{HTML}{2E7D32}
\definecolor{callout-chapter-connection-color1}{HTML}{FDF2F7}
\definecolor{callout-chapter-connection-color2}{HTML}{A51C30}
%=============%

\usepackage{hyphenat}
\usepackage{ifthen}
\usepackage{calc}
\usepackage{calculator}



\usepackage{graphicx}
\usepackage{geometry}
\usepackage{afterpage}
\usepackage{tikz}
\usetikzlibrary{calc}
\usetikzlibrary{fadings}
\usepackage[pagecolor=none]{pagecolor}


% Set the titlepage font families







% Set the coverpage font families

\usepackage{bookmark}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  pdftitle={Machine Learning Systems},
  pdfauthor={Vijay Janapa Reddi},
  colorlinks=true,
  linkcolor={Maroon},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}


\title{Machine Learning Systems}
\usepackage{etoolbox}
\makeatletter
\providecommand{\subtitle}[1]{% add subtitle to \maketitle
  \apptocmd{\@title}{\par {\large #1 \par}}{}{}
}
\makeatother
\subtitle{Volume I: Introduction}
\author{Vijay Janapa Reddi}
\date{January 28, 2026}
\begin{document}
%%%%% begin titlepage extension code

  \begin{frontmatter}

\begin{titlepage}
% This is a combination of Pandoc templating and LaTeX
% Pandoc templating https://pandoc.org/MANUAL.html#templates
% See the README for help

\thispagestyle{empty}

\newgeometry{top=-100in}

% Page color

\newcommand{\coverauthorstyle}[1]{{\fontsize{20}{24.0}\selectfont
{#1}}}

\begin{tikzpicture}[remember picture, overlay, inner sep=0pt, outer sep=0pt]

\tikzfading[name=fadeout, inner color=transparent!0,outer color=transparent!100]
\tikzfading[name=fadein, inner color=transparent!100,outer color=transparent!0]
\node[anchor=south west, rotate=0, opacity=1] at ($(current page.south west)+(0.225\paperwidth, 9)$) {
\includegraphics[width=\paperwidth, keepaspectratio]{assets/images/covers/cover-image-transparent-vol1.png}};

% Title
\newcommand{\titlelocationleft}{0.075\paperwidth}
\newcommand{\titlelocationbottom}{0.4\paperwidth}
\newcommand{\titlealign}{left}

\begin{scope}{%
\fontsize{52}{62.4}\selectfont
\node[anchor=north
west, align=left, rotate=0] (Title1) at ($(current page.south west)+(\titlelocationleft,\titlelocationbottom)$)  [text width = 0.9\paperwidth]  {{\nohyphens{Machine
Learning Systems}}};
}
\end{scope}

% Author
\newcommand{\authorlocationleft}{.925\paperwidth}
\newcommand{\authorlocationbottom}{0.150\paperwidth}
\newcommand{\authoralign}{right}

\begin{scope}
{%
\fontsize{20}{24.0}\selectfont
\node[anchor=north
east, align=right, rotate=0] (Author1) at ($(current page.south west)+(\authorlocationleft,\authorlocationbottom)$)  [text width = 6in]  {\coverauthorstyle{Vijay\\Janapa
Reddi\\}};
}
\end{scope}

% Footer
\newcommand{\footerlocationleft}{0.075\paperwidth}
\newcommand{\footerlocationbottom}{0.475\paperwidth}
\newcommand{\footerlocationalign}{left}

\begin{scope}
{%
\fontsize{25}{30.0}\selectfont
 \node[anchor=north west, align=left, rotate=0] (Footer1) at %
($(current page.south west)+(\footerlocationleft,\footerlocationbottom)$)  [text width = 0.9\paperwidth]  {{\nohyphens{Volume
I: Introduction}}};
}
\end{scope}

\end{tikzpicture}
\clearpage
\restoregeometry
%%% TITLE PAGE START

% Set up alignment commands
%Page
\newcommand{\titlepagepagealign}{
\ifthenelse{\equal{left}{right}}{\raggedleft}{}
\ifthenelse{\equal{left}{center}}{\centering}{}
\ifthenelse{\equal{left}{left}}{\raggedright}{}
}


\newcommand{\titleandsubtitle}{
% Title and subtitle
{{\huge{\bfseries{\nohyphens{Machine Learning Systems}}}}\par
}%

\vspace{\betweentitlesubtitle}
{
{\large{\textit{\nohyphens{Volume I: Introduction}}}}\par
}}
\newcommand{\titlepagetitleblock}{
\titleandsubtitle
}

\newcommand{\authorstyle}[1]{{\large{#1}}}

\newcommand{\affiliationstyle}[1]{{\large{#1}}}

\newcommand{\titlepageauthorblock}{
{\authorstyle{\nohyphens{Vijay Janapa
Reddi}{\textsuperscript{1}}\textsuperscript{,}{\textsuperscript{,*}}}}}

\newcommand{\titlepageaffiliationblock}{
\hangindent=1em
\hangafter=1
{\affiliationstyle{
{1}.~Harvard University


\vspace{1\baselineskip}
* \textit{Correspondence:}~Vijay Janapa Reddi~vj@eecs.harvard.edu
}}
}
\newcommand{\headerstyled}{%
{}
}
\newcommand{\footerstyled}{%
{\large{}}
}
\newcommand{\datestyled}{%
{January 28, 2026}
}


\newcommand{\titlepageheaderblock}{\headerstyled}

\newcommand{\titlepagefooterblock}{
\footerstyled
}

\newcommand{\titlepagedateblock}{
\datestyled
}

%set up blocks so user can specify order
\newcommand{\titleblock}{\newlength{\betweentitlesubtitle}
\setlength{\betweentitlesubtitle}{0.05\textheight}
{

{\titlepagetitleblock}
}

\vspace{4\baselineskip}
}

\newcommand{\authorblock}{{\titlepageauthorblock}

\vspace{2\baselineskip}
}

\newcommand{\affiliationblock}{{\titlepageaffiliationblock}

\vspace{0pt}
}

\newcommand{\logoblock}{}

\newcommand{\footerblock}{}

\newcommand{\dateblock}{{\titlepagedateblock}

\vspace{0pt}
}

\newcommand{\headerblock}{}

\thispagestyle{empty} % no page numbers on titlepages


\newcommand{\vrulecode}{\textcolor{black}{\rule{\vrulewidth}{\textheight}}}
\newlength{\vrulewidth}
\setlength{\vrulewidth}{2pt}
\newlength{\B}
\setlength{\B}{\ifdim\vrulewidth > 0pt 0.05\textwidth\else 0pt\fi}
\newlength{\minipagewidth}
\ifthenelse{\equal{left}{left} \OR \equal{left}{right} }
{% True case
\setlength{\minipagewidth}{\textwidth - \vrulewidth - \B - 0.1\textwidth}
}{
\setlength{\minipagewidth}{\textwidth - 2\vrulewidth - 2\B - 0.1\textwidth}
}
\ifthenelse{\equal{left}{left} \OR \equal{left}{leftright}}
{% True case
\raggedleft % needed for the minipage to work
\vrulecode
\hspace{\B}
}{%
\raggedright % else it is right only and width is not 0
}
% [position of box][box height][inner position]{width}
% [s] means stretch out vertically; assuming there is a vfill
\begin{minipage}[b][\textheight][s]{\minipagewidth}
\titlepagepagealign
\titleblock

Prof.~Vijay Janapa Reddi

School of Engineering and Applied Sciences

Harvard University

\vspace{80mm}

With heartfelt gratitude to the community for their invaluable
contributions and steadfast support.

\vfill

January 28, 2026

\vfill
\par

\end{minipage}\ifthenelse{\equal{left}{right} \OR \equal{left}{leftright} }{
\hspace{\B}
\vrulecode}{}
\clearpage
%%% TITLE PAGE END
\end{titlepage}
\setcounter{page}{1}
\end{frontmatter}

%%%%% end titlepage extension code

\renewcommand*\contentsname{Table of contents}
{
\hypersetup{linkcolor=}
\setcounter{tocdepth}{2}
\tableofcontents
}

\mainmatter
\bookmarksetup{startatroot}

\chapter*{Welcome to Volume I}\label{welcome-to-volume-i}
\addcontentsline{toc}{chapter}{Welcome to Volume I}

\markboth{Welcome to Volume I}{Welcome to Volume I}

\section*{What You Will Learn}\label{what-you-will-learn}
\addcontentsline{toc}{section}{What You Will Learn}

\markright{What You Will Learn}

Volume I progresses through four stages:

\begin{itemize}
\tightlist
\item
  \textbf{Part I: Foundations} --- Build your conceptual foundation with
  mental models that underpin all effective systems work.
\item
  \textbf{Part II: Build} --- Engineer complete workflows from data
  pipelines through training infrastructure.
\item
  \textbf{Part III: Optimize} --- Transform theoretical understanding
  into systems that run efficiently in resource-constrained
  environments.
\item
  \textbf{Part IV: Deploy} --- Navigate serving, operations, and
  responsible engineering practices.
\end{itemize}

\section*{Prerequisites}\label{prerequisites}
\addcontentsline{toc}{section}{Prerequisites}

\markright{Prerequisites}

This volume assumes:

\begin{itemize}
\tightlist
\item
  \textbf{Programming proficiency} in Python with familiarity in NumPy
\item
  \textbf{Mathematics foundations} in linear algebra, calculus, and
  probability at the undergraduate level
\item
  Prior ML experience is helpful but not required;
  \textbf{?@sec-deep-learning-systems-foundations} provides essential
  background
\end{itemize}

\section*{Support Our Mission}\label{support-our-mission}
\addcontentsline{toc}{section}{Support Our Mission}

\markright{Support Our Mission}

\section*{Continue Your Journey}\label{continue-your-journey}
\addcontentsline{toc}{section}{Continue Your Journey}

\markright{Continue Your Journey}

\section*{Listen to the AI Podcast}\label{listen-to-the-ai-podcast}
\addcontentsline{toc}{section}{Listen to the AI Podcast}

\markright{Listen to the AI Podcast}

\section*{Want to Help Out?}\label{want-to-help-out}
\addcontentsline{toc}{section}{Want to Help Out?}

\markright{Want to Help Out?}

This is a collaborative project, and your input matters. If you'd like
to contribute, check out our
\href{https://github.com/harvard-edge/cs249r_book/blob/dev/docs/contribute.md}{contribution
guidelines}. Feedback, corrections, and new ideas are welcome. Simply
file a GitHub
\href{https://github.com/harvard-edge/cs249r_book/issues}{issue}.

\bookmarksetup{startatroot}

\chapter{Data Engineering for ML}\label{sec-data-engineering-ml}

\marginnote{\begin{footnotesize}

\emph{DALL·E 3 Prompt: Create a rectangular illustration visualizing the
concept of data engineering. Include elements such as raw data sources,
data processing pipelines, storage systems, and refined datasets. Show
how raw data is transformed through cleaning, processing, and storage to
become valuable information that can be analyzed and used for
decision-making.}

\end{footnotesize}}

\noindent
\pandocbounded{\includegraphics[keepaspectratio]{contents/vol1/data_engineering/images/png/cover_data_engineering.png}}

\section*{Purpose}\label{purpose}
\addcontentsline{toc}{section}{Purpose}

\markright{Purpose}

\emph{Why does data represent the actual source code of machine learning
systems while traditional code merely describes how to compile it?}

In conventional software, programmers write logic that computers
execute. In machine learning, programmers write optimization procedures
that extract operational logic from data. This inversion makes data the
true source code: change the data and you change what the system does,
regardless of whether a single line of traditional code has been
modified. A dataset with subtle labeling inconsistencies produces a
model with subtle behavioral inconsistencies. A dataset missing edge
cases produces a model that fails on edge cases. A dataset reflecting
historical biases produces a model that perpetuates those biases. No
architecture, hyperparameter, or training trick can recover information
that was never present or correct errors that were baked in from the
start. Data engineering is therefore not preprocessing---it is
programming in a different language, one where quality control
determines whether the compiled system works.

\begin{tcolorbox}[enhanced jigsaw, bottomrule=.15mm, colframe=quarto-callout-tip-color-frame, toprule=.15mm, coltitle=black, colback=white, rightrule=.15mm, opacityback=0, bottomtitle=1mm, opacitybacktitle=0.6, left=2mm, breakable, leftrule=.75mm, titlerule=0mm, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Learning Objectives}, arc=.35mm, colbacktitle=quarto-callout-tip-color!10!white, toptitle=1mm]

\begin{itemize}
\item
  Apply the Four Pillars framework to evaluate data engineering
  trade-offs across quality, reliability, scalability, and governance
\item
  Design data acquisition strategies combining datasets, crowdsourcing,
  and synthetic generation based on quantitative cost-quality trade-offs
\item
  Architect data pipelines with validation, monitoring, and graceful
  degradation for operational reliability
\item
  Implement training-serving consistency through idempotent
  transformations and detect drift using PSI and statistical methods
\item
  Evaluate storage architectures by matching ML workload patterns to
  databases, warehouses, lakes, and feature stores
\item
  Build data labeling systems balancing accuracy, throughput, and cost
  with quality control and consensus mechanisms
\item
  Apply governance practices for lineage tracking, privacy protection,
  and regulatory compliance throughout the data lifecycle
\end{itemize}

\end{tcolorbox}

\section{Data Engineering as Dataset
Compilation}\label{sec-data-engineering-ml-data-engineering-dataset-compilation-0496}

The methodologies examined in \textbf{?@sec-ai-development-workflow}
establish the \textbf{when} and \textbf{why} of data preparation.
However, executing those stages at scale requires dedicated
infrastructure. If the workflow is the plan, \textbf{Data Engineering}
is the factory floor.

\phantomsection\label{callout-definitionux2a-1.1}
\begin{fbx}{callout-definition}{Definition: }{Data Engineering}
\phantomsection\label{callout-definition*-1.1}
\textbf{Data Engineering} refers to the systematic discipline of
designing and maintaining \emph{data infrastructure} that transforms
\emph{raw data} into \emph{reliable}, \emph{accessible}, and
\emph{analysis-ready} datasets through principled acquisition,
processing, storage, and governance practices.

\end{fbx}

In traditional software, computational logic is defined by code. In
machine learning, system behavior is defined by data. This shift makes
data a first-class engineering artifact requiring the same rigor we
apply to code. We therefore reframe data engineering not as ``data
cleaning,'' but as \textbf{Dataset Compilation}: a change in the
training dataset (\(\Delta D\)) is functionally equivalent to a change
in the executable logic (\(\Delta P\)).

\[ \text{System Behavior} \approx f(\text{Data}) \]

Just as a compiler transforms human-readable source code into an
optimized binary executable, a data pipeline transforms raw, noisy
observations into a clean, optimized training set that the model
consumes. This analogy extends to specific operations---standard
compiler optimizations map directly to data engineering tasks:

\begin{itemize}
\tightlist
\item
  \textbf{Dead Code Elimination → Filtering:} Removing corrupted
  records, outliers, or irrelevant features that contribute nothing to
  the learned representation.
\item
  \textbf{Loop Unrolling → Augmentation:} Synthetically expanding
  limited examples (e.g., rotating images, pitch-shifting audio) to
  expose the model to more variations of the underlying pattern.
\item
  \textbf{Common Subexpression Elimination → Deduplication:} Identifying
  and merging duplicate records to prevent bias and wasted compute.
\item
  \textbf{Type Checking → Schema Validation:} Enforcing strict types and
  ranges to ensure the ``runtime'' (model training) does not crash.
\end{itemize}

The engineering implication is direct: datasets must be
\textbf{versioned} (like git), \textbf{unit-tested} (data quality
checks), and \textbf{debugged}. Deleting a row of training data is the
engineering equivalent of deleting a line of code, and retraining a
model is simply recompiling the binary.

\phantomsection\label{callout-perspectiveux2a-1.2}
\begin{fbx}{callout-perspective}{Systems Perspective: }{The Statistical Ceiling}
\phantomsection\label{callout-perspective*-1.2}
A fundamental law of data-centric engineering is the \textbf{Statistical
Ceiling}: The maximum achievable accuracy of any model is strictly
bounded by the quality and noise floor of its training data.

If your labels are only 90\% accurate, your model's ceiling is 90\%
accuracy, regardless of whether you use a simple linear regression or a
trillion-parameter transformer. In the context of the \textbf{Iron Law},
data engineering is the dual process of lowering the \textbf{Data
(\(D\))} term (through filtering and deduplication) while simultaneously
raising this \textbf{Performance Ceiling} (through cleaning and labeling
consistency). No amount of compute (\(Ops\)) can overcome a low
statistical ceiling.

\end{fbx}

The statistical ceiling establishes the theoretical bound on model
performance. But how do we quantify the practical returns from investing
in data quality? The following analysis demonstrates the compounding
economics of one common data quality improvement: deduplication.

\phantomsection\label{callout-perspectiveux2a-1.3}
\begin{fbx}{callout-perspective}{Systems Perspective: }{Napkin Math: The Deduplication Dividend}
\phantomsection\label{callout-perspective*-1.3}
\textbf{Problem}: You have a 10TB dataset of web-scraped images.
Training takes 2 weeks on your GPU cluster (\$5,000). You suspect 30\%
of the images are near-duplicates. Is it worth the engineering time to
remove them?

\textbf{The Math}: If duplicates contribute little training signal,
removing 30\% reduces training time to \textasciitilde10 days, saving
\$1,500 per run. Over 10 training iterations (hyperparameter tuning,
architecture search), that's \$15,000 saved. If deduplication takes 2
engineer-days (\$2,000), the ROI is 7.5×.

\textbf{The Systems Lesson}: Data cleaning has compounding returns
because cleaned data benefits every downstream experiment. This is why
production teams prioritize data quality tooling over model complexity.

\end{fbx}

This compilation metaphor reflects a broader paradigm shift in how ML
practitioners approach system development. Traditional ML development
fixes the dataset early, then iterates on model architectures and
hyperparameters. Research benchmarks reinforce this pattern by providing
static datasets where progress is measured purely through algorithmic
innovation. Production systems face a different reality: datasets
continuously evolve, data quality varies across sources and time, and
model improvements often plateau while data improvements continue
yielding gains. This realization has catalyzed what Andrew Ng termed the
shift from model-centric to \textbf{data-centric AI}
(\citeproc{ref-ng2021datacentric}{Ng 2021}).

\phantomsection\label{callout-definitionux2a-1.4}
\begin{fbx}{callout-definition}{Definition: }{Data-Centric AI}
\phantomsection\label{callout-definition*-1.4}
\textbf{Data-Centric AI} is a development paradigm that systematically
engineers the data used to build AI systems, treating data quality,
consistency, and coverage as the primary levers for improving model
performance rather than focusing primarily on model architecture
changes.

\end{fbx}

Table~\ref{tbl-model-vs-data-centric} contrasts these paradigms:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2778}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3095}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4048}}@{}}
\caption{\textbf{Model-Centric vs.~Data-Centric Development}: The shift
in ML development philosophy. Production experience shows that data
improvements often outperform algorithmic innovation---a dataset with
10,000 consistently labeled examples often outperforms 100,000 examples
with noisy labels.}\label{tbl-model-vs-data-centric}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Aspect}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Model-Centric Approach}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Data-Centric Approach}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Aspect}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Model-Centric Approach}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Data-Centric Approach}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Primary iteration target} & Model architecture, hyperparameters
& Data quality, labeling consistency \\
\textbf{Response to poor performance} & Try different models, add layers
& Analyze errors, improve data for failure cases \\
\textbf{Benchmark philosophy} & Fixed dataset, compete on algorithms &
Improve dataset systematically \\
\textbf{Label disagreement handling} & Majority vote, accept noise &
Resolve inconsistencies, create clear guidelines \\
\textbf{Data augmentation role} & Increase training set size & Target
specific failure modes \\
\end{longtable}

The data-centric approach proves particularly valuable when models have
reached architectural maturity. For many production tasks, the
difference between model architectures is small compared to the impact
of data quality improvements. This mindset---treating data as the
primary improvement lever---motivates why data engineering demands the
same rigor we apply to code.

This rigorous treatment of data becomes even more critical when we
consider how data quality issues propagate differently than software
bugs. Traditional software systems generate predictable error responses
when encountering malformed input, enabling immediate corrective
measures. Machine learning systems present different challenges: data
quality deficiencies manifest as subtle performance degradations that
accumulate throughout the processing pipeline and often remain
undetected until system failures occur in production. A single
mislabeled training instance may appear inconsequential, but systematic
labeling inconsistencies compound into model corruption across entire
feature spaces. Gradual data distribution shifts can silently degrade
performance until complete retraining becomes necessary.

These challenges require systematic engineering approaches beyond ad-hoc
solutions. To understand why, we must first examine how data quality
failures propagate through ML systems.

\section{Four Pillars
Framework}\label{sec-data-engineering-ml-four-pillars-framework-4ef1}

\subsection{Data Cascades: Why Systematic Foundations
Matter}\label{sec-data-engineering-ml-data-cascades-systematic-foundations-matter-2efe}

Machine learning systems face a unique failure pattern called ``Data
Cascades,''\sidenote{\textbf{Data Cascades}: A systems failure pattern
unique to ML where poor data quality in early stages amplifies
throughout the entire pipeline, causing downstream model failures,
project termination, and potential user harm. Unlike traditional
software where bad inputs typically produce immediate errors, ML systems
degrade silently until quality issues become severe enough to
necessitate complete system rebuilds. } where poor data quality in early
stages amplifies throughout the entire pipeline, causing downstream
model failures, project termination, and potential user harm
(\citeproc{ref-sambasivan2021everyone}{Sambasivan et al. 2021}).
Traditional software produces immediate errors when encountering bad
inputs. ML systems degrade silently until quality issues become severe
enough to necessitate complete system rebuilds.

Figure~\ref{fig-cascades} reveals how data quality failures cascade
through every pipeline stage, with data collection errors proving
especially problematic. Lapses in this initial stage become apparent
during model evaluation and deployment, potentially requiring abandoning
the entire model and restarting. This cascading nature of failures
motivates a systematic framework for data engineering decisions.

\begin{figure}[htb]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/8aad4084309888075d9dbf44d0dfd669ee0894e3.pdf}}

}

\caption{\label{fig-cascades}\textbf{Data Quality Cascades}: Errors
introduced early in the machine learning workflow amplify across
subsequent stages, increasing costs and potentially leading to flawed
predictions or harmful outcomes. Recognizing these cascades motivates
proactive investment in data engineering and quality control. Source:
(\citeproc{ref-sambasivan2021everyone}{Sambasivan et al. 2021}).}

\end{figure}%

Data cascades occur when teams skip establishing clear quality criteria,
reliability requirements, and governance principles before beginning
data collection and processing. Preventing these cascades requires a
systematic framework---Quality, Reliability, Scalability, and
Governance---that guides technical choices from data acquisition through
production deployment.

Having established why cascades occur, we must now understand why these
specific four pillars provide the right organizing framework.
Understanding why these four pillars emerge from first principles,
rather than accepting them as arbitrary categories, helps practitioners
internalize and apply them consistently even in novel situations.

\subsection{Deriving the Four Pillars from First
Principles}\label{sec-data-engineering-ml-deriving-four-pillars-first-principles-c833}

Why four pillars? Why not three or five? Unlike arbitrary
categorizations, these four pillars derive naturally from fundamental
constraints that govern all data-intensive systems. Understanding this
derivation provides both memorability and defensibility---you can
reconstruct the framework from first principles rather than memorizing
an arbitrary taxonomy.

\textbf{Quality emerges from statistical learning theory.} Machine
learning assumes that training data and production data are drawn from
the same distribution: \(P_{train}(X, Y) \approx P_{serve}(X, Y)\). When
this assumption breaks---through labeling errors, sampling bias, or data
corruption---the learned function \(f(x)\) fails to generalize. Quality
is not a preference but a \emph{mathematical prerequisite} for learning.
The cascading failures documented in
Section~\ref{sec-data-engineering-ml-data-cascades-systematic-foundations-matter-2efe}
are direct consequences of violating this distributional assumption.

\textbf{Reliability emerges from distributed systems reality.} Any
system with more than one component experiences partial failures.
Networks partition. Disks fail. Services crash. The CAP theorem
formalizes this: distributed systems cannot simultaneously guarantee
Consistency, Availability, and Partition tolerance. Data pipelines
spanning multiple services, storage systems, and processing stages must
assume failures will occur. Reliability is not about preventing failures
but \emph{surviving them gracefully}---through redundancy, idempotent
operations, and graceful degradation.

\textbf{Scalability emerges from the exponential growth of data.} Data
volumes grow exponentially while human attention remains constant. From
2010 to 2020, global data creation grew from 2 zettabytes to 64
zettabytes---a 32x increase. No amount of manual effort can keep pace.
Systems must handle 10x growth without 10x engineering effort.
Scalability is not about big data \emph{per se} but about
\emph{sublinear effort scaling}---ensuring that doubling data volume
does not require doubling infrastructure complexity or operational
burden.

\textbf{Governance emerges from the principal-agent problem.} When data
involves humans---whether as subjects (whose data is collected), as
annotators (who label data), or as consumers (who trust model
predictions)---interests diverge. Users want privacy; models want
features. Annotators want fair compensation; platforms want low costs.
Regulators demand transparency; proprietary systems resist disclosure.
Governance is not bureaucratic overhead but the \emph{coordination
mechanism} that aligns these divergent interests through access
controls, audit trails, and accountability structures.

These derivations reveal why the pillars are \emph{necessary and
sufficient}. Necessary because ignoring any pillar leads to predictable
failure modes: quality violations cause accuracy degradation,
reliability gaps cause service outages, scalability limits cause growth
ceilings, and governance failures cause regulatory penalties or ethical
harm. Sufficient because together they address the complete space of
data system concerns: what data contains (quality), how data flows
(reliability), how data grows (scalability), and who data serves
(governance).

\subsection{The Four Foundational
Pillars}\label{sec-data-engineering-ml-four-foundational-pillars-c119}

Every data engineering decision, from choosing storage formats to
designing ingestion pipelines, should be evaluated against four
principles. Figure~\ref{fig-four-pillars} illustrates how these pillars
interact, with each contributing to system success through systematic
decision-making.

Data quality provides the foundation for system success. Quality issues
compound throughout the ML lifecycle through ``Data Cascades''
(Section~\ref{sec-data-engineering-ml-data-cascades-systematic-foundations-matter-2efe}),
where early failures propagate and amplify downstream. Quality includes
accuracy, completeness, consistency, and fitness for the intended ML
task. The mathematical foundations of this relationship appear in
\textbf{?@sec-deep-learning-systems-foundations} and
\textbf{?@sec-dnn-architectures}.

ML systems require consistent, predictable data processing that handles
failures gracefully. Reliability means building systems that continue
operating despite component failures, data anomalies, or unexpected load
patterns. This includes error handling, monitoring, and recovery
mechanisms throughout the data pipeline.

Scalability addresses the challenge of growth. As ML systems grow from
prototypes to production services, data volumes and processing
requirements increase dramatically. Systems must handle growing data
volumes, user bases, and computational demands without complete
redesigns. Scalability must be cost-effective: raw capacity means little
if infrastructure costs grow faster than business value. Cost
effectiveness spans resource efficiency (compute proportional to
workload), storage optimization (balancing access speed against
retention costs), and operational sustainability (avoiding technical
debt that compounds maintenance burden).

\phantomsection\label{callout-lighthouseux2a-1.5}
\begin{fbx}{callout-lighthouse}{Lighthouse Example: }{DLRM (Recommendation Lighthouse)}
\phantomsection\label{callout-lighthouse*-1.5}
\textbf{Why it matters:} Recommendation systems like DLRM exemplify the
\textbf{scalability} challenge of modern data engineering. They rely on
high-cardinality categorical features (like User IDs or Product IDs)
that must be mapped to dense vectors via embedding tables.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.1683}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2277}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.5941}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Property}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Value}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{System Implication}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Data Scale} & Billion+ users/items & Embedding tables grow to
TB/PB scale. \\
\textbf{Constraint} & Memory Capacity & Tables exceed single-GPU memory;
requires sharding. \\
\textbf{Bottleneck} & Sparse Access & Random lookups stress memory
bandwidth more than compute. \\
\end{longtable}

Unlike ResNet (compute-bound) or GPT-2 (bandwidth-bound), DLRM is
limited by \textbf{memory capacity} and the sheer logistics of storing
and accessing massive lookup tables efficiently.

\end{fbx}

Governance provides the framework within which quality, reliability, and
scalability operate. Data governance ensures systems operate within
legal, ethical, and business constraints while maintaining transparency
and accountability. This includes privacy protection, bias mitigation,
regulatory compliance, and clear data ownership and access controls.

\begin{figure}[htb]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/ea84e4082582a8dbeb28a2fd90cad655366e939d.pdf}}

}

\caption{\label{fig-four-pillars}\textbf{The Four Pillars of Data
Engineering}: Quality, Reliability, Scalability, and Governance form the
foundational framework for ML data systems. Each pillar contributes
essential capabilities (solid arrows), while trade-offs between pillars
(dashed lines) require careful balancing: validation overhead affects
throughput, consistency constraints limit distributed scale, privacy
requirements impact performance, and bias mitigation may reduce
available training data. Effective data engineering requires managing
these tensions systematically rather than optimizing any single pillar
in isolation.}

\end{figure}%

When ML systems exhibit failures,
Table~\ref{tbl-four-pillars-diagnostic} helps teams identify which
pillar to investigate first based on observed symptoms:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3725}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2549}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3627}}@{}}
\caption{\textbf{Four Pillars Diagnostic Guide}: When ML systems exhibit
failures, this mapping helps teams identify which pillar to investigate
first based on observed
symptoms.}\label{tbl-four-pillars-diagnostic}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Symptom}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Likely Pillar}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Investigation}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Symptom}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Likely Pillar}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Investigation}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Model accuracy drops gradually} & Quality & Check for data
drift, label degradation \\
\textbf{Pipeline fails intermittently} & Reliability & Review error
handling, retry logic \\
\textbf{Training takes too long} & Scalability & Profile bottlenecks,
check parallelization \\
\textbf{Audit finds compliance gaps} & Governance & Review lineage
tracking, access controls \\
\textbf{Features differ train vs.~serve} & Quality and Reliability &
Check consistency contracts \\
\end{longtable}

\subsection{Integrating the Pillars Through Systems
Thinking}\label{sec-data-engineering-ml-integrating-pillars-systems-thinking-a32e}

Having examined how each pillar addresses specific concerns and how they
manifest across pipeline stages, we now turn to how they function
together as a system. Understanding each pillar individually provides
important insights, but effective data engineering requires recognizing
their interconnections. These four pillars are not independent
components but interconnected aspects of a unified system where
decisions in one area affect all others. Quality improvements must
account for scalability constraints, reliability requirements influence
governance implementations, and governance policies shape quality
metrics. This systems perspective guides our exploration of data
engineering, examining how each technical topic supports and balances
these principles while managing their tensions.

Why does this integrated framework matter? According to industry
surveys, data scientists spend an estimated 60--80\% of their time on
data preparation tasks.\sidenote{\textbf{Data Quality Reality}: The
famous ``garbage in, garbage out'' principle was first coined by IBM
computer programmer George Fuechsel in the 1960s, describing how flawed
input data produces nonsense output. This principle remains critically
relevant in modern ML systems
(\citeproc{ref-crowdflower2016data}{CrowdFlower 2016}). }
Figure~\ref{fig-ds-time} quantifies this reality, revealing that data
cleaning alone consumes up to 60\% of practitioners' effort. This
imbalance reflects a state where data engineering practices are often
ad-hoc rather than systematic. Applying the four-pillar framework
consistently can reduce data preparation time while building more
reliable and maintainable systems.

\begin{figure}[htb]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/333977c0c561e3bc7c5ab598add44246fe50c102.pdf}}

}

\caption{\label{fig-ds-time}\textbf{Data Scientist Time Allocation}:
Data preparation consumes a majority of data science effort, up to 60\%,
underscoring the need for systematic data engineering practices to
prevent downstream model failures and ensure project success.
Prioritizing data quality and pipeline development yields greater
returns than solely focusing on advanced algorithms. Source: Various
industry reports.}

\end{figure}%

The time allocation chart reveals a striking pattern: data scientists
spend the majority of their time on data preparation rather than model
development. This distribution motivates knowing the key constants that
govern data engineering costs and timelines.

\phantomsection\label{callout-perspectiveux2a-1.6}
\begin{fbx}{callout-perspective}{Systems Perspective: }{Data Engineering Numbers Every ML Engineer Should Know}
\phantomsection\label{callout-perspective*-1.6}
Just as systems engineers memorize latency numbers, ML engineers should
internalize these data engineering constants:

\textbf{Costs (2024 estimates)}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3483}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1685}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2809}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1798}}@{}}
\toprule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\multirow{2}{=}{\textbf{Operation} :=============================
\textbf{Crowdsourced image label}} & \multirow{2}{=}{\textbf{Cost}
=============: \$0.01--0.05} & \multirow{2}{=}{\textbf{Notes}
:======================= Simple classification} & \\
& & & \\
\textbf{Bounding box annotation} & \$0.05--0.20 & Per box, simple scenes
& \\
\textbf{Expert medical label} & \$50--200 & Per study, radiologist & \\
\textbf{S3 storage (Standard)} & \$23/TB/month & Hot storage & \\
\textbf{S3 retrieval (Glacier)} & \$0.01/GB &
\multicolumn{2}{>{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.4607} + 2\tabcolsep}@{}}{%
Standard: 3-5 hours\sidenote{\textbf{S3 Glacier Retrieval Tiers}: AWS S3
Glacier offers multiple retrieval options with different cost and
latency trade-offs: Standard (\$0.01/GB, 3-5 hours), Expedited
(\$0.03/GB, 1-5 minutes), and Bulk (free, 5-12 hours). Glacier Deep
Archive has longer retrieval times (up to 12 hours for Standard).
Pricing as of 2024; see aws.amazon.com/s3/pricing for current rates.
}} \\
\textbf{GPU training hour (A100)} & \$2--4 & Cloud spot pricing & \\
\textbf{Human review hour} & \$15--50 & Depending on expertise & \\
\end{longtable}

\textbf{Time Constants}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.4353}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1765}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2824}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.0824}}@{}}
\toprule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\multirow{2}{=}{\textbf{Operation} :===================================
\textbf{Label 1M images (crowdsourced)}} &
\multirow{2}{=}{\textbf{Duration} =============: 2--4 weeks} &
\multirow{2}{=}{\textbf{Bottleneck} :====================== Annotation
throughput} & \\
& & & \\
\textbf{Train ResNet-50 on ImageNet} & 4--6 hours &
\multicolumn{2}{>{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3647} + 2\tabcolsep}@{}}{%
Compute (8× A100, optimized)} \\
\textbf{Feature store lookup} & 1--10 ms & Network + cache & \\
\end{longtable}

The contrast matters: \textbf{weeks} for human labeling, \textbf{hours}
for GPU training, \textbf{milliseconds} for serving. Labeling is the
bottleneck.

\textbf{The 1000× Rule}: Labeling typically costs \textbf{1,000--3,000×}
more than the compute to train on that data. A \$100K labeling budget
buys data that trains on \$30--100 of GPU time.

\textbf{The 80/20 Split}: 80\% of data engineering effort goes to 20\%
of features---the ``long tail'' of edge cases, rare categories, and
quality exceptions.

\end{fbx}

\subsection{Framework Application Across Data
Lifecycle}\label{sec-data-engineering-ml-framework-application-across-data-lifecycle-c512}

This four-pillar framework guides our exploration from problem
definition through production operations. Establishing clear problem
definitions and governance principles shapes all subsequent technical
decisions. The framework guides acquisition strategies, where quality
and reliability requirements determine how we source and validate data.
Processing and storage decisions follow from scalability and governance
constraints, while operational practices maintain all four pillars
throughout the system lifecycle.

Subsequent sections examine how these pillars manifest in specific
technical decisions: sourcing techniques that balance quality with
scalability, storage architectures that support performance within
governance constraints, and processing pipelines that maintain
reliability while handling massive scale.

Table~\ref{tbl-four-pillars-matrix} shows how each pillar manifests
across major stages of the data pipeline, providing both a planning tool
for system design and a reference for troubleshooting when issues arise
at different pipeline stages.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.1268}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.1901}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2113}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2254}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2254}}@{}}
\caption{\textbf{Four Pillars Applied Across Data Pipeline Stages}: This
matrix illustrates how Quality, Reliability, Scalability, and Governance
principles manifest in each major stage of the data engineering
pipeline. Each cell shows specific techniques and practices that
implement the corresponding pillar at that stage, providing a
comprehensive framework for systematic decision-making and
troubleshooting.}\label{tbl-four-pillars-matrix}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Stage}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Quality}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Reliability}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Scalability}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Governance}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Stage}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Quality}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Reliability}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Scalability}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Governance}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Acquisition} & Representative sampling, bias detection & Diverse
sources, redundant collection strategies & Web scraping, synthetic data
generation & Consent, anonymization, ethical sourcing \\
\textbf{Ingestion} & Schema validation, data profiling & Dead letter
queues, graceful degradation & Batch vs stream processing, autoscaling
pipelines & Access controls, audit logs, data lineage \\
\textbf{Processing} & Consistency validation, training-serving parity &
Idempotent transformations, retry mechanisms & Distributed frameworks,
horizontal scaling & Lineage tracking, privacy preservation, bias
monitoring \\
\textbf{Storage} & Data validation checks, freshness monitoring &
Backups, replication, disaster recovery & Tiered storage, partitioning,
compression optimization & Access audits, encryption, retention
policies \\
\end{longtable}

\section{Applying the
Framework}\label{sec-data-engineering-ml-applying-framework-3af9}

The Four Pillars framework provides the conceptual foundation; now we
must translate principles into practice. This section demonstrates how
to apply the framework systematically through structured problem
definition and a detailed case study that will recur throughout the
chapter. Governance principles---privacy protection, bias mitigation,
regulatory compliance, and documentation---must be established before
any data collection begins, not retrofitted later. The full
implementation of governance infrastructure is examined in
Section~\ref{sec-data-engineering-ml-governance-observability-2c05};
here we focus on how governance shapes initial problem definition.

\subsection{Structured Approach to Problem
Definition}\label{sec-data-engineering-ml-structured-approach-problem-definition-f7ad}

Building on these governance foundations, systematic problem definition
becomes essential. ML systems require problem framing that goes beyond
traditional software development
(\citeproc{ref-sculley2015hidden}{Sculley et al. 2021}). Whether
developing recommendation engines processing millions of user
interactions, computer vision systems analyzing medical images, or
natural language models handling diverse text data, each system brings
unique challenges requiring careful consideration within our governance
and technical framework.

Establishing clear objectives provides unified direction that guides the
entire project, from data collection strategies through deployment
operations. These objectives must balance technical performance with
governance requirements, creating measurable outcomes that include both
accuracy metrics and fairness criteria.

This approach ensures governance principles and technical requirements
are integrated from the start rather than retrofitted later. Key steps
must precede any data collection effort:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Identify and clearly state the problem definition
\item
  Set clear objectives to meet
\item
  Establish success benchmarks
\item
  Understand end-user engagement/use
\item
  Understand the constraints and limitations of deployment
\item
  Perform data collection.
\item
  Iterate and refine.
\end{enumerate}

\subsection{Framework Application Through Keyword Spotting Case
Study}\label{sec-data-engineering-ml-framework-application-keyword-spotting-case-study-c8e9}

Keyword Spotting (KWS) systems provide an ideal case study for applying
our four-pillar framework to real-world data engineering challenges.
These systems power voice-activated devices like smartphones and smart
speakers, detecting specific wake words such as ``OK, Google'' or
``Alexa'' within continuous audio streams while operating under strict
resource constraints.

Figure~\ref{fig-keywords} depicts a KWS system operating as a
lightweight, always-on front-end that triggers more complex voice
processing systems. These systems demonstrate interconnected challenges
across all four pillars: Quality (accuracy across diverse environments),
Reliability (consistent battery-powered operation), Scalability (severe
memory constraints), and Governance (privacy protection). These
constraints explain why many KWS systems support only a limited number
of languages: collecting high-quality, representative voice data for
smaller linguistic populations proves prohibitively difficult given
governance and scalability challenges. All four pillars must work
together to achieve successful deployment.

\begin{figure}

\centering{

\includegraphics[width=0.55\linewidth,height=\textheight,keepaspectratio]{contents/vol1/data_engineering/images/png/data_engineering_kws.png}

}

\caption{\label{fig-keywords}\textbf{Keyword Spotting System}: A typical
deployment of keyword spotting (KWS) technology in a voice-activated
device, where a constantly-listening system detects a wake word to
initiate further processing. This example demonstrates how KWS functions
as a lightweight, always-on front-end for more complex voice
interfaces.}

\end{figure}%

With this framework understanding established, we can apply our problem
definition approach to our KWS example, demonstrating how the four
pillars guide practical engineering decisions:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Identifying the Problem}: KWS detects specific keywords amidst
  ambient sounds and other spoken words. The primary problem is to
  design a system that can recognize these keywords with high accuracy,
  low latency, and minimal false positives or negatives, especially when
  deployed on devices with limited computational resources. A
  well-specified problem definition for developing a new KWS model
  should identify the desired keywords along with the envisioned
  application and deployment scenario.
\item
  \textbf{Setting Clear Objectives}: The objectives for a KWS system
  must balance multiple competing requirements. Performance targets
  include achieving high accuracy rates (98\% accuracy in keyword
  detection) while ensuring low latency (keyword detection and response
  within 200 milliseconds). Resource constraints demand minimizing power
  consumption to extend battery life on embedded devices and ensuring
  the model size is optimized for available memory on the device.
\item
  \textbf{Benchmarks for Success}: Establish clear metrics to measure
  the success of the KWS system. Key performance indicators include true
  positive rate (the percentage of correctly identified keywords
  relative to all spoken keywords) and false positive rate (the
  percentage of non-keywords including silence, background noise, and
  out-of-vocabulary words incorrectly identified as keywords).
  Detection/error tradeoff curves evaluate KWS on streaming audio
  representative of real-world deployment scenarios by comparing false
  accepts per hour (false positives over total evaluation audio
  duration) against false rejection rate (missed keywords relative to
  spoken keywords in evaluation audio), as demonstrated by Nayak et al.
  (\citeproc{ref-nayak2022improving}{2022}).
\end{enumerate}

\phantomsection\label{callout-notebook-1.1}
\begin{fbx}{callout-notebook}{AI Engineer’s Notebook 1.1: }{Engineering Calculation: False Positive Targets}
\phantomsection\label{callout-notebook-1.1}
\textbf{Constraint}: User tolerance is max 1 false wake-up per month.

\textbf{Operational Parameters} - \textbf{Duty Cycle}: Always-on (24
hours/day). - \textbf{Window Size}: 1 second classification windows. -
\textbf{Windows per Month}:
\(60 \times 60 \times 24 \times 30 = 2,592,000\) windows.

\textbf{Required Accuracy} - \textbf{False Positive Rate (FPR)}:
\(\frac{1}{2,592,000} \approx 3.8 \times 10^{-7}\) - \textbf{Precision
Requirement}: 99.99996\% rejection of non-keywords.

\textbf{Implication}: Standard accuracy metrics (e.g., ``99\%
accuracy'') are meaningless here. We must evaluate specifically on
\textbf{False Accepts per Hour (FA/Hr)}.

\end{fbx}

Operational metrics track response time (keyword utterance to system
response) and power consumption (average power used during keyword
detection).

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\item
  \textbf{Stakeholder Engagement and Understanding}: Engage with
  stakeholders, which include device manufacturers, hardware and
  software developers, and end-users. Understand their needs,
  capabilities, and constraints. Different stakeholders bring competing
  priorities: device manufacturers might prioritize low power
  consumption, software developers might emphasize ease of integration,
  and end-users would prioritize accuracy and responsiveness. Balancing
  these competing requirements shapes system architecture decisions
  throughout development.
\item
  \textbf{Understanding the Constraints and Limitations of Embedded
  Systems}: Embedded devices come with their own set of challenges that
  shape KWS system design. Memory limitations require extremely
  lightweight models, often in the tens-of-kilobytes range, to fit in
  the always-on island of the SoC\sidenote{\textbf{System on Chip
  (SoC)}: An integrated circuit that combines all essential computer
  components (processor, memory, I/O interfaces) on a single chip.
  Modern SoCs include specialized ``always-on'' low-power domains that
  continuously monitor for triggers like wake words while the main
  processor sleeps, often achieving sub-milliwatt power consumption for
  continuous listening workloads (exact power depends on implementation
  and duty cycle). }; this constraint covers only model weights while
  preprocessing code must also fit within tight memory bounds.
  Processing power constraints from limited computational capabilities
  (often a few hundred MHz of clock speed) demand aggressive model
  optimization for efficiency. Power consumption becomes critical since
  most embedded devices run on batteries, so KWS systems often target
  sub-milliwatt power consumption during continuous listening.
  Environmental challenges add another layer of complexity, as devices
  must function effectively across diverse deployment scenarios ranging
  from quiet bedrooms to noisy industrial settings.
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{5}
\item
  \textbf{Data Collection and Analysis}: For a KWS system, data quality
  and diversity determine success. The dataset must capture demographic
  diversity by including speakers with various accents across age and
  gender to ensure wide-ranging recognition support. Keyword variations
  require attention since people pronounce wake words differently,
  requiring the dataset to capture these pronunciation nuances and
  slight variations. Background noise diversity proves essential,
  necessitating data samples that include or are augmented with
  different ambient noises to train the model for real-world scenarios
  ranging from quiet environments to noisy conditions.
\item
  \textbf{Iterative Feedback and Refinement}: Finally, once a prototype
  KWS system is developed, teams must ensure the system remains aligned
  with the defined problem and objectives as deployment scenarios change
  over time and use-cases evolve. This requires testing in real-world
  scenarios, gathering feedback about whether some users or deployment
  scenarios encounter underperformance relative to others, and
  iteratively refining both the dataset and model based on observed
  failure patterns.
\end{enumerate}

\textbf{The KWS Design Space}: These requirements create a
multi-dimensional design space where data engineering choices cascade
through system performance. Table~\ref{tbl-kws-design-space} quantifies
key trade-offs, enabling principled decisions rather than ad-hoc
selection.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2960}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.1840}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.1680}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.1680}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.1600}}@{}}
\caption{\textbf{KWS Data Engineering Design Space}: Each design choice
creates quantifiable trade-offs across the four pillars. Higher sampling
rates improve quality but double storage and processing (scalability
impact). More training data improves accuracy but multiplies labeling
costs (governance/cost impact). Local inference eliminates latency but
requires aggressive quantization (quality/reliability trade-off). This
design space analysis guides systematic optimization rather than
intuition-based decisions.}\label{tbl-kws-design-space}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Design Choice}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Quality Impact}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Latency Impact}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Cost Impact}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Memory Impact}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Design Choice}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Quality Impact}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Latency Impact}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Cost Impact}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Memory Impact}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{16kHz vs 8kHz sampling} & +2--4\% accuracy & 2× storage & 2×
processing & 2× feature size \\
\textbf{13 vs 40 MFCC coefficients} & +3--5\% accuracy & 3× feature
compute & Minimal & 3× feature memory \\
\textbf{1M vs 10M training examples} & +5--8\% accuracy & 10× training
time & 10× labeling cost & 10× storage \\
\textbf{Clean vs noisy training data} & +10--15\% real-world & Minimal &
3× collection cost & Minimal \\
\textbf{Local vs cloud inference} & −2\% accuracy (quant) & 10ms vs
100ms & \$0 vs \$0.001/query & 16KB vs unlimited \\
\textbf{Synthetic vs real augmentation} & +3--5\% robustness & Minimal &
10× cheaper & Minimal \\
\end{longtable}

\phantomsection\label{callout-notebook-1.2}
\begin{fbx}{callout-notebook}{AI Engineer’s Notebook 1.2: }{Worked Example: Optimizing the KWS Design Space}
\phantomsection\label{callout-notebook-1.2}
\textbf{Scenario}: You're building a KWS system for a smart speaker with
these constraints:

\begin{itemize}
\tightlist
\item
  \textbf{Target}: 98\% accuracy, \textless1 false wake per month
\item
  \textbf{Budget}: \$150K total data engineering budget
\item
  \textbf{Memory}: 64KB model size limit (always-on island)
\item
  \textbf{Timeline}: 6 months to production
\end{itemize}

\textbf{Step 1: Apply Constraints to Eliminate Options}

From Table~\ref{tbl-kws-design-space}, the 64KB memory limit eliminates:

\begin{itemize}
\tightlist
\item
  40 MFCC coefficients (3× memory) → Must use 13 MFCCs
\item
  Cloud inference (requires network stack) → Must use local inference
\end{itemize}

\textbf{Step 2: Calculate Budget Allocation}

Using the TCDO model with \$150K budget:

\begin{itemize}
\tightlist
\item
  \textbf{Labeling} (\textasciitilde60\%): \$90K available
\item
  \textbf{Storage/Processing} (\textasciitilde25\%): \$37.5K
\item
  \textbf{Governance/Other} (\textasciitilde15\%): \$22.5K
\end{itemize}

At \$0.10/label with 20\% review overhead: \$90K ÷ \$0.12 = \textbf{750K
labeled examples}

This falls between 1M and 10M in our design space---closer to 1M,
suggesting +5--6\% accuracy contribution from data volume.

\textbf{Step 3: Maximize Remaining Accuracy}

Current accuracy budget:

\begin{itemize}
\tightlist
\item
  Base model: \textasciitilde90\% (minimal data)
\item
  +5--6\% from 750K examples
\item
  Need: +2--3\% more to reach 98\%
\end{itemize}

Options from design space:

\begin{itemize}
\tightlist
\item
  16kHz sampling: +2--4\% accuracy, 2× storage cost ✓ (fits budget)
\item
  Noisy training data: +10--15\% real-world accuracy, 3× collection cost
\item
  Synthetic augmentation: +3--5\% robustness, 10× cheaper than real data
  ✓
\end{itemize}

\textbf{Step 4: Final Configuration}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2449}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2755}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4694}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Choice}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Selection}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Rationale}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Sampling rate} & 16kHz & +3\% accuracy worth 2× storage within
budget \\
\textbf{MFCC coefficients} & 13 & Memory-constrained, non-negotiable \\
\textbf{Training examples} & 750K real + 2M synthetic & Budget-optimal
mix \\
\textbf{Data diversity} & Noisy + clean mix & Critical for real-world
deployment \\
\textbf{Inference} & Local, 8-bit quantized & Memory-constrained \\
\textbf{Augmentation} & Heavy synthetic & 10× cost efficiency \\
\end{longtable}

\textbf{Projected Outcome}: 97--99\% accuracy (meeting target), \$145K
spend (under budget), 48KB model (under limit).

\textbf{The Engineering Lesson}: Systematic design space analysis
transformed intuition (``we need more data'') into quantified decisions
(``750K real + 2M synthetic maximizes accuracy per dollar given memory
constraints'').

\end{fbx}

With optimal parameters selected from our design space, implementation
requires combining multiple data collection approaches. Our KWS system
demonstrates how these approaches work together across the project
lifecycle. Pre-existing datasets like Google's Speech Commands
(\citeproc{ref-warden2018speech}{Warden 2018}) provide a foundation for
initial development, offering carefully curated voice samples for common
wake words. However, these datasets often lack diversity in accents,
environments, and languages, necessitating additional strategies.

To address coverage gaps, web scraping supplements baseline datasets by
gathering diverse voice samples from video platforms and speech
databases, capturing natural speech patterns and wake word variations.
Crowdsourcing platforms like Amazon Mechanical
Turk\sidenote{\textbf{Mechanical Turk Origins}: Amazon's crowdsourcing
platform (2005) named after the 18th-century chess ``automaton'' that
secretly concealed a human player. MTurk enables distributed human
computation: ImageNet's 14M labels came from 49,000 MTurk workers. The
platform ironically reverses the original Turk's deception---presenting
human intelligence as AI then, ML training leverages human intelligence
now. } enable targeted collection of wake word samples across different
demographics and environments. This approach is particularly valuable
for underrepresented languages or specific acoustic conditions.

Finally, synthetic data generation fills remaining gaps through speech
synthesis (\citeproc{ref-werchniak2021exploring}{Werchniak et al. 2021})
and audio augmentation, creating unlimited wake word variations across
acoustic environments, speaker characteristics, and background
conditions. This comprehensive approach enables KWS systems that perform
well across diverse real-world conditions while demonstrating how
systematic problem definition guides data strategy throughout the
project lifecycle.

The multi-source acquisition strategy we have developed---combining
curated datasets, web scraping, crowdsourcing, and synthetic
generation---creates data flowing from diverse external sources with
heterogeneous formats, quality levels, and update frequencies. This
diversity at the boundary layer, where carefully acquired data enters
our controlled systems, demands robust pipeline architecture. The
data-centric mindset introduced in
Section~\ref{sec-data-engineering-ml-data-engineering-dataset-compilation-0496}
shapes every architectural decision: pipeline architecture, acquisition
strategies, and labeling systems all prioritize data quality as the
primary lever for system improvement.

With our data acquisition strategy defined, we now examine the
infrastructure that receives, validates, and routes this heterogeneous
data---the pipeline architecture that transforms raw inputs into
reliable ML training inputs.

\section{Data Pipeline
Architecture}\label{sec-data-engineering-ml-data-pipeline-architecture-b527}

Data pipelines implement our four-pillar framework, transforming raw
data into ML-ready formats while maintaining quality, reliability,
scalability, and governance standards. The heterogeneity resulting from
multi-source acquisition---audio files from crowdsourcing platforms,
synthetic waveforms from generation systems, and real-world captures
from deployed devices---requires pipelines that can normalize, validate,
and route data while enforcing consistent standards. Pipeline
architecture translates framework principles into operational reality,
where each pillar manifests as concrete engineering decisions about
validation strategies, error handling mechanisms, throughput
optimization, and observability infrastructure.

Our KWS system pipeline architecture must handle continuous audio
streams, maintain low-latency processing for real-time keyword
detection, and ensure privacy-preserving data handling. The pipeline
must scale from development environments processing sample audio files
to production deployments handling millions of concurrent audio streams
while maintaining strict quality and governance standards.

\begin{figure}[htb]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/d841870bf29a27292d16ccde7a22231c4f074c46.pdf}}

}

\caption{\label{fig-pipeline-flow}\textbf{Data Pipeline Architecture}:
Modular pipelines ingest, process, and deliver data for machine learning
tasks, enabling independent scaling of components and improved data
quality control. Distinct stages (ingestion, storage, and preparation)
transform raw data into a format suitable for model training and
validation, forming the foundation of reliable ML systems.}

\end{figure}%

Figure~\ref{fig-pipeline-flow} breaks down ML data pipelines into
several distinct layers: data sources, ingestion, processing, labeling,
storage, and ML training. Each layer plays a specific role in the data
preparation workflow. Selecting appropriate technologies requires
understanding how our four framework pillars manifest at each stage.
Quality requirements at one stage affect scalability constraints at
another, reliability needs shape governance implementations, and the
pillars interact to determine overall system effectiveness.

Data pipeline design is constrained by storage hierarchies and I/O
bandwidth limitations rather than CPU capacity. Understanding these
constraints enables building efficient systems for modern ML workloads.
Storage hierarchy trade-offs, ranging from high-latency object storage
(ideal for archival) to low-latency in-memory stores (essential for
real-time serving), and bandwidth limitations (spinning disks at 100-200
MB/s versus RAM at 50-200 GB/s) shape every pipeline decision.
Section~\ref{sec-data-engineering-ml-strategic-storage-architecture-1a6b}
covers detailed storage architecture considerations.

Design decisions should align with specific requirements. For streaming
data, consider message durability (ability to replay failed processing),
ordering properties (what ordering is provided, under what conditions),
and geographic distribution. For batch processing, key decision factors
include data volume relative to memory, processing complexity, and
whether computation must be distributed. Single-machine tools suffice
for gigabyte-scale data, but terabyte-scale processing often benefits
from distributed frameworks that partition work across clusters. The
interactions between these layers, viewed through our four-pillar lens,
determine system effectiveness and guide the specific engineering
decisions we examine in the following subsections.

\subsection{Quality Through Validation and
Monitoring}\label{sec-data-engineering-ml-quality-validation-monitoring-498f}

Quality represents the foundation of reliable ML systems. Pipelines
implement quality through systematic validation and monitoring at every
stage. Production experience shows data pipeline issues represent a
major source of ML failures. Schema changes breaking downstream
processing, distribution drift degrading model accuracy, and data
corruption silently introducing errors account for a substantial
fraction of production incidents
(\citeproc{ref-sculley2015hidden}{Sculley et al. 2021}). These failures
are insidious because they often don't cause obvious system crashes but
slowly degrade model performance in ways that become apparent only after
affecting users. Achieving quality therefore demands proactive
monitoring and validation that catches issues before they cascade into
model failures.

Production teams implement monitoring at scale through severity-based
alerting systems where different failure types trigger different
response protocols. The most critical alerts indicate complete system
failure: the pipeline has stopped processing entirely, showing zero
throughput for more than 5 minutes, or a primary data source has become
unavailable. These situations demand immediate attention because they
halt all downstream model training or serving. More subtle degradation
patterns require different detection strategies. When throughput drops
to 80\% of baseline levels, error rates climb above 5\%, or quality
metrics drift more than 2 standard deviations from training data
characteristics, the system signals degradation requiring urgent but not
immediate attention. These gradual failures often prove more dangerous
than complete outages because they can persist undetected for hours or
days, silently corrupting model inputs and degrading prediction quality.

Consider how these principles apply to a recommendation system
processing user interaction events. With a baseline throughput of 50,000
records per second, the monitoring system tracks several interdependent
signals. Instantaneous throughput alerts fire if processing drops below
40,000 records per second for more than 10 minutes, accounting for
normal traffic variation while catching genuine capacity or processing
problems. Each feature in the data stream has its own quality profile:
if a feature like user\_age shows null values in more than 5\% of
records when the training data contained less than 1\% nulls, something
has likely broken in the upstream data source. Duplicate detection runs
on sampled data, watching for the same event appearing multiple
times---a pattern that might indicate retry logic gone wrong or a
database query accidentally returning the same records repeatedly.

These monitoring dimensions become particularly important when
considering end-to-end latency. The system must track not just whether
data arrives, but how long it takes to flow through the entire pipeline
from the moment an event occurs to when the resulting features become
available for model inference. When 95th
percentile\sidenote{\textbf{95th percentile}: A statistical measure
indicating that 95\% of values fall below this threshold, commonly used
in performance monitoring to capture typical worst-case behavior while
excluding outliers. For latency monitoring, the 95th percentile provides
more stable insights than maximum values (which may be anomalies) while
revealing performance degradation that averages would hide. } latency
exceeds 30 seconds in a system with a 10-second service level agreement,
the monitoring system needs to pinpoint which pipeline stage introduced
the delay: ingestion, transformation, validation, or storage.

Quality monitoring extends beyond simple schema validation to
statistical properties that capture whether serving data resembles
training data. Rather than just checking that values fall within valid
ranges, production systems track rolling statistics over 24-hour
windows. For numerical features like transaction\_amount or
session\_duration, the system computes means and standard deviations
continuously, then applies statistical tests like the Kolmogorov-Smirnov
test\sidenote{\textbf{Kolmogorov-Smirnov test}: Named after Soviet
mathematicians Andrey Kolmogorov (1933) and Nikolai Smirnov (1939) who
independently developed this non-parametric test quantifying whether two
datasets come from the same distribution. The test measures maximum
distance between cumulative distribution functions, requiring no
assumptions about underlying distributions. In ML systems, K-S tests
detect data drift by comparing serving data against training baselines;
p-values below 0.05 indicate statistically significant distribution
shifts requiring investigation. } to compare serving distributions
against training distributions.

\phantomsection\label{callout-notebook-1.3}
\begin{fbx}{callout-notebook}{AI Engineer’s Notebook 1.3: }{Worked Example: Detecting Drift with K-S Test}
\phantomsection\label{callout-notebook-1.3}

\textbf{Scenario}: Monitoring \texttt{session\_duration} distribution
stability between training (\(P\)) and serving (\(Q\)).

\textbf{Methodology} 1. \textbf{Compute CDFs}: Calculate Cumulative
Distribution Functions for both datasets. 2. \textbf{Calculate Statistic
(\(D_{KS}\))}: Find the maximum absolute difference between the CDFs.
\[ D_{KS} = \max_x | F_P(x) - F_Q(x) | \] 3. \textbf{Determine
Significance}: Compare \(D_{KS}\) to critical value \(D_{crit}\) based
on sample size (\(n\)) and confidence level (\(\alpha=0.05\)).
\[ D_{crit} \approx \frac{1.36}{\sqrt{n}} \]

\textbf{Example Calculation} - Sample size \(n=1000\). - Critical value
\(D_{crit} \approx 1.36 / \sqrt{1000} \approx 0.043\). - If observed max
difference \(D_{KS} = 0.08\):

\begin{itemize}
\tightlist
\item
  \textbf{Result}: \(0.08 > 0.043\) \(\rightarrow\) \textbf{Reject Null
  Hypothesis}. Significant drift detected. Trigger retraining or
  investigation.
\end{itemize}

\end{fbx}

Categorical features require different statistical approaches. Instead
of comparing means and variances, monitoring systems track category
frequency distributions. When new categories appear that never existed
in training data, or when existing categories shift substantially in
relative frequency---say, the proportion of ``mobile'' versus
``desktop'' traffic changes by more than 20\%, the system flags
potential data quality issues or genuine distribution shifts. This
statistical vigilance catches subtle problems that simple schema
validation misses entirely: imagine if age values remain in the valid
range of 18-95, but the distribution shifts from primarily 25-45 year
olds to primarily 65+ year olds, indicating the data source has changed
in ways that will affect model performance.

Validation at the pipeline level encompasses multiple strategies working
together. Schema validation executes synchronously as data enters the
pipeline, rejecting malformed records immediately before they can
propagate downstream. Modern tools like TensorFlow Data Validation
(TFDV)\sidenote{\textbf{TensorFlow Data Validation (TFDV)}: A
production-grade library for analyzing and validating ML data that
automatically infers schemas, detects anomalies, and identifies
training-serving skew. TFDV computes descriptive statistics, identifies
data drift through distribution comparisons, and generates
human-readable validation reports, integrating with TFX pipelines for
automated data quality monitoring. For a feature vector containing user
demographics, the inferred schema might specify that user\_age must be a
64-bit integer between 18 and 95 and cannot be null, user\_country must
be a string from a specific set of country codes, and session\_duration
must be a floating-point number between 0 and 7200 seconds but is
optional. During serving, the validator checks each incoming record
against these specifications, rejecting records with null required
fields, out-of-range values, or type mismatches before they reach
feature computation logic. } automatically infer schemas from training
data, capturing expected data types, value ranges, and presence
requirements.

This synchronous validation necessarily remains simple and fast,
checking properties that can be evaluated on individual records in
microseconds. More sophisticated validation that requires comparing
serving data against training data distributions or aggregating
statistics across many records must run asynchronously to avoid blocking
the ingestion pipeline. Statistical validation systems typically sample
1-10\% of serving traffic---enough to detect meaningful shifts while
avoiding the computational cost of analyzing every record. These samples
accumulate in rolling windows, commonly 1 hour, 24 hours, and 7 days,
with different windows revealing different patterns. Hourly windows
detect sudden shifts like a data source failing over to a backup with
different characteristics, while weekly windows reveal gradual drift in
user populations or behavior.

Perhaps the most insidious validation challenge arises from
training-serving skew\sidenote{\textbf{Training-Serving Skew}: A ML
systems failure where identical features are computed differently during
training versus serving, causing silent model degradation. Occurs when
training uses batch processing with one implementation while serving
uses real-time processing with different libraries, creating subtle
differences that compound to degrade accuracy significantly without
obvious errors. }, where the same features get computed differently in
training versus serving environments. This typically happens when
training pipelines process data in batch using one set of libraries or
logic, while serving systems compute features in real-time using
different implementations. A recommendation system might compute
``user\_lifetime\_purchases'' in training by joining user profiles
against complete transaction histories, while the serving system
inadvertently uses a cached materialized
view\sidenote{\textbf{Materialized view}: A database optimization that
pre-computes and stores query results as physical tables, trading
storage space for query performance. Unlike standard views that compute
results on-demand, materialized views cache expensive join and
aggregation operations but require refresh strategies to maintain data
freshness, creating potential training-serving skew when refresh
schedules differ between environments. The resulting 15\% discrepancy
between training and serving features directly explains seemingly
mysterious 12\% accuracy drops observed in production A/B tests.
Detecting training-serving skew requires infrastructure that can
recompute training features on serving data for comparison. Production
systems implement periodic validation where they sample raw serving
data, process it through both training and serving feature pipelines,
and measure discrepancies. } updated only weekly. The data consistency
patterns we establish here provide the foundation for the comprehensive
treatment of training-serving skew in
\textbf{?@sec-machine-learning-operations-mlops}, which examines this
challenge from the operational monitoring perspective.

\subsection{Data Quality as
Code}\label{sec-data-engineering-ml-data-quality-code-1cca}

Just as unit tests protect software systems, data expectation tests
protect ML pipelines. Using libraries like Great Expectations or
Pandera, teams codify quality expectations as executable assertions
(Listing~\ref{lst-data-expectations}) that run on every pipeline
execution.

\begin{codelisting}

\caption{\label{lst-data-expectations}\textbf{Data Quality Assertions}:
Executable data contracts catch schema violations, missing values, and
invalid entries before training begins. Production systems using this
pattern detect approximately 60\% of data issues at pipeline execution
time, preventing cascading failures that would otherwise propagate to
model training.}

\centering{

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ great\_expectations }\ImportTok{as}\NormalTok{ gx}

\CommentTok{\# Create a data context and load training data}
\NormalTok{context }\OperatorTok{=}\NormalTok{ gx.get\_context()}
\NormalTok{batch }\OperatorTok{=}\NormalTok{ context.get\_batch(}\StringTok{"training\_users"}\NormalTok{)}

\CommentTok{\# Define an expectation suite as executable quality contract}
\NormalTok{suite }\OperatorTok{=}\NormalTok{ context.create\_expectation\_suite(}\StringTok{"user\_data\_quality"}\NormalTok{)}

\CommentTok{\# Range validation: prevents physiologically impossible values}
\NormalTok{batch.expect\_column\_values\_to\_be\_between(}
\NormalTok{    column}\OperatorTok{=}\StringTok{"age"}\NormalTok{, min\_value}\OperatorTok{=}\DecValTok{0}\NormalTok{, max\_value}\OperatorTok{=}\DecValTok{120}
\NormalTok{)}

\CommentTok{\# Null detection: ensures primary key integrity for joins}
\NormalTok{batch.expect\_column\_values\_to\_not\_be\_null(column}\OperatorTok{=}\StringTok{"user\_id"}\NormalTok{)}

\CommentTok{\# Uniqueness: prevents duplicate training examples}
\NormalTok{batch.expect\_column\_values\_to\_be\_unique(column}\OperatorTok{=}\StringTok{"user\_id"}\NormalTok{)}

\CommentTok{\# Categorical validation: detects unexpected values from upstream changes}
\NormalTok{batch.expect\_column\_distinct\_values\_to\_be\_in\_set(}
\NormalTok{    column}\OperatorTok{=}\StringTok{"country\_code"}\NormalTok{, value\_set}\OperatorTok{=}\NormalTok{[}\StringTok{"US"}\NormalTok{, }\StringTok{"CA"}\NormalTok{, }\StringTok{"UK"}\NormalTok{, }\StringTok{"DE"}\NormalTok{, }\StringTok{"FR"}\NormalTok{]}
\NormalTok{)}

\CommentTok{\# Run validation and fail pipeline if expectations not met}
\NormalTok{results }\OperatorTok{=}\NormalTok{ context.run\_validation\_operator(}
    \StringTok{"action\_list\_operator"}\NormalTok{, assets\_to\_validate}\OperatorTok{=}\NormalTok{[batch]}
\NormalTok{)}
\ControlFlowTok{if} \KeywordTok{not}\NormalTok{ results[}\StringTok{"success"}\NormalTok{]:}
    \ControlFlowTok{raise} \PreprocessorTok{ValueError}\NormalTok{(}\SpecialStringTok{f"Data quality check failed: }\SpecialCharTok{\{}\NormalTok{results}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

}

\end{codelisting}%

CI/CD integration runs expectations in the deployment pipeline.
Expectation violations fail deployments before bad data reaches
training. A pipeline structured as data ingestion followed by data
validation followed by training blocks deployment when validation
detects anomalies like age values of 150, triggering alerts for
investigation.

Expectation suites as artifacts version alongside training code. When
training code changes, expectation updates help keep data contracts
evolving together. This coupling reduces the risk of silent divergence
where code assumes data properties that the upstream pipeline no longer
provides.

This pattern catches approximately 60\% of production data issues before
they reach training, based on industry experience with tools like Great
Expectations, Pandera, and Pydantic. The remaining issues require the
runtime monitoring discussed in the previous sections, as some quality
problems only emerge in the full production data stream.

\subsection{Detecting and Responding to Data
Drift}\label{sec-data-engineering-ml-detecting-responding-data-drift-509a}

ML models fundamentally assume that production data resembles training
data. When this assumption breaks, model performance degrades silently
without obvious errors or system failures. Data drift detection provides
early warning before accuracy drops become severe, enabling proactive
response rather than reactive recovery after users experience degraded
service. Unlike the validation and monitoring techniques we have
examined that catch immediate data quality issues, drift detection
identifies gradual statistical changes in data distributions that
compound over time to undermine model effectiveness.

This subsection provides comprehensive coverage of drift detection
because production experience reveals that drift detection and response
consume 30--40\% of ongoing ML operations effort, making this a core
data engineering responsibility rather than an optional advanced topic.
We examine drift types, quantitative detection metrics, monitoring
infrastructure, and response strategies here;
\textbf{?@sec-machine-learning-operations-mlops} builds on this
foundation to address operational response orchestration and automated
retraining pipelines at scale. The insidious nature of drift-induced
failures motivates systematic detection: model accuracy degrades
gradually over weeks or months rather than failing catastrophically,
making problems difficult to attribute to specific causes without
quantitative distribution monitoring.

\textbf{Types of Distribution Shift}

Understanding the three core types of drift enables targeted detection
and response strategies. Each type manifests differently in production
systems and requires distinct monitoring approaches.

Covariate shift\sidenote{\textbf{Covariate Shift}: A type of data drift
where the distribution of input features \(P(X)\) changes between
training and serving while the conditional relationship \(P(Y|X)\)
remains constant. Common in production ML when data collection contexts
change (new sensors, different user populations, seasonal effects) but
core relationships persist. Detectable through feature distribution
monitoring without requiring ground truth labels. } occurs when input
feature distributions change while the relationship between features and
labels remains constant: \(P(X)\) changes but \(P(Y|X)\) stays the same.
A medical imaging system trained on one camera model might see
production data from a different camera manufacturer. The disease-image
relationship remains unchanged (same pathologies produce same visual
indicators), but pixel value distributions shift due to different sensor
characteristics, color calibration, or image processing pipelines.
Detection focuses on monitoring feature distributions using statistical
metrics like PSI or KL divergence applied to input features.

Label shift occurs when the output label distribution changes while the
relationship between labels and features remains constant: \(P(Y)\)
changes but \(P(X|Y)\) stays the same. Disease prevalence might change
seasonally (flu cases spike in winter) while symptoms remain consistent
predictors of each disease. A recommendation system might see label
shift when new product categories launch, changing the distribution of
user preferences without altering what makes products appealing within
each category. Detection monitors prediction distributions for shifts in
relative frequencies of predicted classes, which can be done without
ground truth labels by tracking model output distributions.

Concept drift\sidenote{\textbf{Concept Drift}: A type of data drift
where the relationship between features and labels \(P(Y|X)\) changes
over time, substantially altering what the model must learn. Most
challenging drift type to detect and respond to, requiring ground truth
labels and potentially model retraining with recent data emphasizing new
patterns. May indicate need for architectural changes if relationships
have substantially altered. } represents the most challenging case: the
relationship between features and labels changes, meaning \(P(Y|X)\)
evolves over time. Medical treatment protocols change, altering disease
outcomes for given symptoms. User preferences shift as social trends
evolve, changing what product features drive purchases. Fraud patterns
evolve as attackers adapt to detection systems. Concept drift requires
ground truth labels for detection since we must monitor whether the
feature-to-label relationship has changed, making it inherently more
difficult and delayed than detecting covariate or label shift.

Label quality drift\sidenote{\textbf{Label Quality Drift}: Degradation
in annotation reliability over time, distinct from changes in label
distributions. Sources include annotator fatigue or turnover, evolving
labeling guidelines, changing domain knowledge, and adversarial label
noise. Detection requires monitoring inter-annotator agreement,
comparing automated labels against periodic human audits, and tracking
proxy metrics indicating labeling consistency. } represents a meta-level
shift distinct from the three distribution shifts above: the reliability
of ground truth labels degrades over time even when the underlying data
distributions remain stable. This drift type proves particularly
insidious because standard feature distribution monitoring fails to
detect it. Crowdsourced labels may degrade as annotator pools change,
training materials become outdated, or labeling guidelines evolve
without corresponding model updates. Automated labeling systems
accumulate errors as the models powering them drift from their original
operating conditions. A recommendation system using click feedback as
implicit labels may see label quality degrade as user behavior becomes
more exploratory, as bot traffic patterns change, or as interface
modifications alter how users interact with content.

Detection requires monitoring annotation consistency rather than feature
distributions. Inter-annotator agreement metrics like Cohen's
kappa\sidenote{\textbf{Cohen's kappa}: Named after psychologist Jacob
Cohen who introduced it in 1960, this statistic measures inter-rater
agreement for categorical items while accounting for agreement occurring
by chance. Cohen developed the metric while studying psychological
assessment reliability. Values range from -1 to 1: above 0.8 indicates
almost perfect agreement, 0.6-0.8 substantial, 0.4-0.6 moderate, and
below 0.4 suggests guidelines need clarification or genuine ambiguity
exists. } (\(\kappa\)) provide quantitative assessment:

\[
\kappa = \frac{p_o - p_e}{1 - p_e}
\]

where \(p_o\) represents observed agreement between annotators and
\(p_e\) represents agreement expected by chance. Monitoring \(\kappa\)
over time windows reveals degradation trends. A medical imaging
annotation project might establish a baseline \(\kappa = 0.85\)
(substantial agreement) during initial data collection, then observe
decline to \(\kappa = 0.72\) (moderate agreement) after six months as
new annotators join without receiving equivalent domain training.

For systems with calibrated model probabilities, label confidence
entropy provides an alternative detection signal:

\[
H_{\text{label}} = -\sum_i p_i \log p_i
\]

Rising entropy in model confidence distributions suggests increasing
ambiguity or mislabeling in training data, as the model learns from
inconsistent supervision.

Mitigation strategies depend on root cause analysis. Annotator
retraining addresses systematic errors from unclear guidelines at low
cost with high effectiveness. Multi-annotator voting with majority or
consensus rules provides very high accuracy for high-stakes domains but
significantly increases annotation costs. Model-assisted labeling
reduces annotator fatigue but risks introducing bias if the assisting
model has its own systematic errors. Expert review sampling, where
domain specialists audit a random sample of annotations, enables root
cause analysis when quality decline is detected but provides medium
coverage of the overall annotation stream.

\textbf{Quantitative Drift Detection Metrics}

Production drift detection requires quantitative metrics with
interpretable thresholds that trigger investigation or automated
responses. We examine the primary statistical approaches used in ML
systems.

The Population Stability Index (PSI)\sidenote{\textbf{Population
Stability Index (PSI)}: A statistical metric originally developed for
credit risk modeling in the 1990s to monitor scorecard stability over
time. PSI measures divergence between two distributions using a
symmetric variant of KL divergence, with thresholds empirically
calibrated from decades of financial industry experience. The metric's
adoption in ML drift detection reflects its interpretability and proven
reliability in high-stakes production monitoring. } provides an
industry-standard metric for detecting changes in feature distributions
(\citeproc{ref-siddiqi2006credit}{Siddiqi 2012}). PSI discretizes
continuous features into bins (typically 10--20 bins) and compares
actual serving distribution against expected training distribution:

\[
\text{PSI} = \sum_{i=1}^{n} (\text{actual}_i - \text{expected}_i) \times \ln\left(\frac{\text{actual}_i}{\text{expected}_i}\right)
\]

where \(\text{actual}_i\) and \(\text{expected}_i\) represent the
proportion of data falling in bin \(i\) for serving and training data
respectively. The metric yields interpretable thresholds based on
extensive industry experience:

\begin{itemize}
\tightlist
\item
  \(\text{PSI} < 0.1\): No significant change; distribution remains
  stable
\item
  \(0.1 \leq \text{PSI} < 0.2\): Moderate change; investigate for
  potential issues
\item
  \(\text{PSI} \geq 0.2\): Significant change; retraining likely needed
\end{itemize}

Consider a concrete example monitoring user age distribution in a
recommendation system. Training data from March has age distribution
binned into deciles. By September, serving data shows drift in the age
distribution:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 10\tabcolsep) * \real{0.1228}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 10\tabcolsep) * \real{0.1491}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 10\tabcolsep) * \real{0.1404}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 10\tabcolsep) * \real{0.1491}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 10\tabcolsep) * \real{0.2368}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 10\tabcolsep) * \real{0.1667}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Age Bin}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Training \%}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Serving \%}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Difference}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{ln(Serving/Training)}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Contribution}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{18--25} & 15.0 & 12.0 & -3.0 & -0.223 & 0.670 \\
\textbf{26--35} & 25.0 & 22.0 & -3.0 & -0.128 & 0.385 \\
\textbf{36--45} & 20.0 & 18.0 & -2.0 & -0.105 & 0.211 \\
\textbf{46--55} & 18.0 & 20.0 & +2.0 & +0.105 & 0.211 \\
\textbf{56--65} & 12.0 & 15.0 & +3.0 & +0.223 & 0.670 \\
\textbf{66+} & 10.0 & 13.0 & +3.0 & +0.262 & 0.787 \\
\end{longtable}

Computing
\(\text{PSI} = \sum (S_i - T_i) \times \ln(S_i/T_i) = 0.670 + 0.385 + 0.211 + 0.211 + 0.670 + 0.787 = 2.934\)
across all bins yields \(\text{PSI} = 0.193\). This falls in the
``moderate change'' range (0.1--0.2), triggering investigation but not
immediate automated retraining. Analysis reveals gradual demographic
shift as platform gains older users, representing genuine covariate
shift requiring model updates within 2--4 weeks.

PSI's primary advantage is its invariance to feature scale and its
proven track record in financial risk modeling where it originated. The
binning approach makes it resilient to outliers while remaining
computationally efficient for real-time monitoring. For high-cardinality
categorical features like product categories or user segments, PSI
applies directly without binning, comparing category frequency
proportions between training and serving data.

Kullback-Leibler (KL) divergence quantifies distribution differences
with greater sensitivity than PSI for continuous distributions:

\[
D_{\text{KL}}(P \parallel Q) = \sum_{x} P(x) \log\left(\frac{P(x)}{Q(x)}\right)
\]

where \(P\) represents the training distribution and \(Q\) represents
the serving distribution. KL divergence is non-negative
(\(D_{\text{KL}} \geq 0\)) with \(D_{\text{KL}} = 0\) indicating
identical distributions. Unlike PSI, KL divergence is asymmetric:
\(D_{\text{KL}}(P \parallel Q) \neq D_{\text{KL}}(Q \parallel P)\). This
asymmetry proves useful when we specifically care about how much serving
data diverges from training data rather than general distributional
differences.

For a concrete example, consider monitoring click-through rates (CTR)
for product recommendations. Training data from Q1 shows CTR
distribution across product categories:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1957}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2174}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2065}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3587}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Category}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Training P(x)}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Serving Q(x)}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{P(x) log(P(x)/Q(x))}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Electronics} & 0.35 & 0.30 & 0.35 × log(0.35/0.30) = 0.053 \\
\textbf{Home} & 0.25 & 0.25 & 0.25 × log(0.25/0.25) = 0.000 \\
\textbf{Fashion} & 0.20 & 0.25 & 0.20 × log(0.20/0.25) = -0.045 \\
\textbf{Sports} & 0.15 & 0.15 & 0.15 × log(0.15/0.15) = 0.000 \\
\textbf{Books} & 0.05 & 0.05 & 0.05 × log(0.05/0.05) = 0.000 \\
\end{longtable}

Computing
\(D_{\text{KL}}(P \parallel Q) = 0.053 + 0.000 + (-0.045) + 0.000 + 0.000 = 0.008\).
This negligible divergence (well below 0.1) indicates stable
distributions requiring no action. However, if serving data shifted to
Electronics=0.45, Home=0.20, Fashion=0.15, Sports=0.15, Books=0.05, then
\(D_{\text{KL}} = 0.35 \times \log(0.35/0.45) + 0.25 \times \log(0.25/0.20) + \ldots = 0.067\),
still below alert thresholds but indicating emerging drift worth
monitoring.

KL divergence thresholds depend more on application context than PSI,
but typical production systems alert when \(D_{\text{KL}} > 0.5\) for
critical features and investigate when \(D_{\text{KL}} > 1.0\) for any
feature. The metric's continuous nature makes it more sensitive to
subtle shifts than PSI's binned approach, useful for detecting drift
early but potentially producing more false positives. In practice,
systems combine both metrics: PSI for interpretability and established
thresholds, KL divergence for early detection of gradual shifts.

Statistical hypothesis tests provide rigorous frameworks for detecting
distribution changes with controlled false positive rates. For
categorical features, the \textbf{chi-squared test} evaluates whether
observed category frequencies in serving data differ significantly from
training data expectations. For continuous features, the
\textbf{Kolmogorov-Smirnov (K-S) test} compares cumulative distribution
functions, measuring maximum distance between training and serving
distributions. The \textbf{Mann-Whitney U test} provides a
non-parametric alternative for continuous features, testing whether two
distributions have the same median without assuming normality.

These tests produce p-values where \(p < 0.05\) indicates statistically
significant drift at the 95\% confidence level. The advantage over
distance metrics like PSI or KL divergence lies in statistical rigor and
controlled false positive rates. However, with large production
datasets, even tiny distributional differences become statistically
significant despite having negligible practical impact on model
performance. Production systems typically combine statistical tests with
practical significance thresholds: require both \(p < 0.05\) and
\(\text{PSI} \geq 0.1\) before triggering alerts.

Together, these quantitative metrics provide a comprehensive toolkit for
drift detection: PSI offers interpretable thresholds with proven
reliability from financial risk modeling, KL divergence enables early
detection of subtle distributional shifts, and statistical hypothesis
tests provide rigorous significance assessment. The challenge now shifts
from measuring drift to operationalizing these metrics within production
monitoring infrastructure that balances detection sensitivity against
the practical constraints of alert fatigue and computational resources.

\textbf{Monitoring Infrastructure and Thresholds}

Production drift monitoring implements multi-tiered alerting strategies
that balance sensitivity against alert fatigue. Critical features (those
most predictive in the model) receive tighter monitoring thresholds and
immediate alerts, while less important features use relaxed thresholds
and batched notifications. A typical production configuration for a
recommendation system might specify:

\begin{itemize}
\tightlist
\item
  \textbf{Tier 1 (Critical features)}: User engagement history, purchase
  behavior, session context

  \begin{itemize}
  \tightlist
  \item
    Alert when \(\text{PSI} > 0.15\) for any feature sustained over 2
    hours
  \item
    Alert when \(D_{\text{KL}} > 0.3\) for any feature
  \item
    Page on-call engineer immediately; potential revenue impact
  \end{itemize}
\item
  \textbf{Tier 2 (Important features)}: User demographics, device
  characteristics, time features

  \begin{itemize}
  \tightlist
  \item
    Alert when \(\text{PSI} > 0.2\) for any feature sustained over 24
    hours
  \item
    Alert when \(D_{\text{KL}} > 0.5\) for any feature
  \item
    Notify team via email/Slack; investigate within 24 hours
  \end{itemize}
\item
  \textbf{Tier 3 (Supporting features)}: Auxiliary metadata, computed
  features, aggregates

  \begin{itemize}
  \tightlist
  \item
    Alert when \(\text{PSI} > 0.25\) for any feature sustained over 1
    week
  \item
    Log anomalies for periodic review; no immediate action required
  \end{itemize}
\end{itemize}

The monitoring system computes drift metrics on rolling time windows
(typically 24-hour windows compared against training baseline) with
measurements every 1--6 hours depending on data volume and computational
budget. For high-throughput systems processing millions of examples per
hour, sampling 10--20\% of traffic for drift computation provides
sufficient statistical power while reducing monitoring overhead from
100\% to manageable levels.

Window size selection depends on traffic volume and acceptable detection
latency. High-traffic systems processing more than one million events
per hour can use 1-4 hour windows for rapid detection with sufficient
statistical power. Medium-traffic systems (100,000 to 1 million events
per hour) typically require 12-24 hour windows to balance sensitivity
against statistical stability. Low-traffic systems with fewer than
100,000 events per hour need 24-72 hour windows to accumulate sufficient
samples for meaningful drift computation. Seasonal businesses benefit
from sliding windows that compare against the same period from the
previous year rather than fixed historical baselines, avoiding false
drift alerts from predictable seasonal patterns.

\textbf{Cold Start Monitoring}

Newly deployed features and models present a distinct monitoring
challenge: drift detection requires baseline distributions that do not
yet exist. Statistical tests like PSI require approximately 1,000
samples for reliable results, and new deployments lack the historical
data needed for meaningful comparison. This cold start
problem\sidenote{\textbf{Cold Start Monitoring}: The challenge of
detecting drift immediately after model deployment or data source
changes, before sufficient production data accumulates to establish
meaningful baselines. Requires bootstrapping techniques, borrowed
thresholds, or distribution-free outlier detection during the initial
deployment period. } requires specialized strategies that establish
baselines quickly while providing interim safeguards.

Pre-deployment validation provides the first line of defense. Before
enabling drift alerts, teams compare production sample distributions
against training data using available statistical tests. The
Kolmogorov-Smirnov test can detect significant distributional
differences even with limited samples. Low model confidence on
production samples (mean confidence below 95\% when the training set
achieved higher confidence) signals potential distribution mismatch
requiring investigation before full deployment. When labeled production
samples are available, accuracy comparison against baseline performance
identifies degradation early.

Shadow deployment periods (typically 2--4 weeks) accumulate baseline
statistics before enabling alerts. During this phase, the system logs
feature distributions and model predictions without triggering drift
alerts, establishing the reference distributions that subsequent
monitoring will compare against. This approach trades detection latency
for accuracy, accepting that early drift will go undetected in exchange
for reduced false positive rates once monitoring activates.

Threshold borrowing provides an alternative when shadow periods are
impractical. Teams identify existing features with similar
characteristics (same data type, similar cardinality, comparable value
ranges) and apply their established thresholds to new features while
gathering feature-specific data. A newly deployed user engagement
feature might borrow thresholds from an existing session duration
feature until sufficient data accumulates for calibration.

For immediate protection without historical baselines, distribution-free
outlier detection methods provide initial safeguards. Z-score thresholds
flag individual observations more than 3 standard deviations from the
running mean. Interquartile range (IQR) bounds classify values below
\(Q_1 - 1.5 \times \text{IQR}\) or above \(Q_3 + 1.5 \times \text{IQR}\)
as anomalous. These methods detect extreme outliers without requiring
reference distributions, providing protection against catastrophic drift
while baseline statistics accumulate.

Accelerated feedback collection shortens the cold start period by
prioritizing ground truth acquisition. Human-in-the-loop sampling
manually labels 100--500 examples in the first 24 hours, enabling early
accuracy assessment. Confidence-based prioritization focuses labeling
effort on low-confidence predictions where drift is most likely to
manifest. Stratified sampling ensures coverage across all feature bins,
preventing blind spots in underrepresented segments.

The monitoring timeline progresses through distinct phases. During the
first 24 hours, shadow scoring and human-in-the-loop sampling with
100-500 labeled examples enables detection of severe issues, with any
confidence below 90\% triggering investigation. From 24-72 hours,
accelerated labeling expands the sample to 500-2,000 examples, enabling
PSI computation with an initial threshold of 0.15 (more sensitive than
steady-state to catch early problems). After 72 hours to one week,
standard drift detection activates with 2,000-5,000 samples and standard
PSI threshold of 0.2. Beyond one week, full production monitoring
operates on all data with standard thresholds calibrated to the
feature's observed variability.

\textbf{Drift Response Strategies}

Detecting drift provides value only when coupled with appropriate
response strategies. The optimal response depends on drift type,
magnitude, and root cause.
\textbf{?@sec-machine-learning-operations-mlops} examines operational
response strategies in detail---automated retraining triggers, canary
deployments, and validation gates. From a data engineering perspective,
the critical first step is determining whether drift reflects genuine
distribution changes or data pipeline issues.

\textbf{Data pipeline fixes} address the most common root cause of
detected drift: upstream data collection or processing changes rather
than real-world distribution shifts. When drift detection reveals schema
changes, new null patterns, or sudden distribution shifts coinciding
with system deployments, the appropriate response involves fixing the
data source rather than retraining models.

Production experience suggests 40-60\% of drift alerts result from data
pipeline issues rather than genuine distribution changes. This makes
drift detection a critical diagnostic tool for data quality, surfacing
problems that simple validation might miss. When an e-commerce system
detects sudden drift in product category distributions, investigation
might reveal that a data pipeline bug filtered out certain categories,
that a new data source was integrated with different categorization
schemes, or that schema evolution in upstream systems broke feature
extraction logic.

\textbf{Case Study: E-Commerce Recommendation Drift Detection}

Consider a production e-commerce recommendation system serving 10
million users. During November, drift monitoring detects
\(\text{PSI} = 0.28\) for product category distributions and
\(\text{PSI} = 0.31\) for user demographic features, both exceeding the
0.2 threshold requiring investigation.

Initial analysis reveals the drift coincides with the holiday shopping
season. Product category distribution has shifted from everyday items
(electronics, home goods) toward gift categories (toys, luxury items,
seasonal decorations). User demographics show increased participation
from age groups 55+ who typically shop less frequently but become active
gift purchasers in November-December.

This represents covariate shift: input feature distributions have
changed due to seasonal patterns, but the core relationship between user
characteristics and product preferences remains consistent.
Recommendation quality metrics show a 3.2\% decline in click-through
rates compared to October baseline, confirming that drift impacts user
experience.

The response strategy involves three parallel actions:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Immediate}: Retrain the model using data from previous holiday
  seasons (November-December from past 3 years) combined with recent
  data. This provides examples of holiday shopping patterns the model
  can learn from, deployed within 48 hours to recover recommendation
  quality.
\item
  \textbf{Short-term}: Investigate whether new product lines launched
  without corresponding historical data. Analysis reveals a new
  ``sustainable gifts'' category with no training examples. Emergency
  data collection gathers early user interactions to populate training
  data for this category.
\item
  \textbf{Long-term}: Implement seasonal model versioning where
  holiday-trained models automatically deploy November-December, then
  revert to standard models in January. This prevents annual recurrence
  of the same drift response cycle.
\end{enumerate}

Post-deployment validation confirms the retrained model recovers
click-through rates to within 0.3\% of October baseline. The seasonal
model strategy prevents similar drift the following year, demonstrating
how drift detection informs both immediate responses and architectural
improvements.

The distinction between drift detection in single-machine systems
(Volume I focus) and distributed drift monitoring across multiple
services and models is covered in
\textbf{?@sec-machine-learning-operations-mlops} where we examine
distributed monitoring infrastructure, cross-service alerting, and
coordinated response strategies for multi-model systems experiencing
correlated drift patterns.

Drift detection represents one dimension of quality monitoring, focused
on identifying statistical changes in data distributions over time.
However, detecting issues is only half the challenge; the other half is
ensuring systems continue operating effectively even when problems are
detected. This leads us from quality monitoring to the reliability
pillar, which addresses how pipelines maintain service continuity under
adverse conditions.

\subsection{Reliability Through Graceful
Degradation}\label{sec-data-engineering-ml-reliability-graceful-degradation-f83d}

While quality monitoring detects issues, reliability ensures systems
continue operating effectively when problems occur. Pipelines face
constant challenges: data sources become temporarily unavailable,
network partitions separate components, upstream schema changes break
parsing logic, or unexpected load spikes exhaust resources. Robust
systems handle these failures gracefully through systematic failure
analysis, intelligent error handling, and automated recovery strategies
that maintain service continuity even under adverse conditions.

Systematic failure mode analysis for ML data pipelines reveals
predictable patterns that require specific engineering countermeasures.
Data corruption failures occur when upstream systems introduce subtle
format changes, encoding issues, or field value modifications that pass
basic validation but corrupt model inputs. A date field switching from
``YYYY-MM-DD'' to ``MM/DD/YYYY'' format might not trigger schema
validation but will break any date-based feature computation. Schema
evolution\sidenote{\textbf{Schema Evolution}: The challenge of managing
changes to data structure over time as source systems add fields, rename
columns, or modify data types. Critical for ML systems because model
training expects consistent feature schemas, and schema changes can
silently break feature computation or introduce training-serving skew. }
failures happen when source systems add fields, rename columns, or
change data types without coordination, breaking downstream processing
assumptions that expected specific field names or types. Resource
exhaustion manifests as gradually degrading performance when data volume
growth outpaces capacity planning, eventually causing pipeline failures
during peak load periods.

Building on this failure analysis, effective error handling strategies
ensure problems are contained and recovered from systematically.
Implementing intelligent retry logic for transient errors, such as
network interruptions or temporary service outages, requires exponential
backoff strategies to avoid overwhelming recovering services. A simple
linear retry that attempts reconnection every second would flood a
struggling service with connection attempts, potentially preventing its
recovery. Exponential backoff---retrying after 1 second, then 2 seconds,
then 4 seconds, doubling with each attempt---gives services breathing
room to recover while still maintaining persistence. Many ML systems
employ the concept of dead letter queues\sidenote{\textbf{Dead Letter
Queue}: Borrowed from postal terminology where ``dead letters'' are
undeliverable mail held at a Dead Letter Office for investigation (the
first US Dead Letter Office opened in 1825). In computing, DLQs store
messages that fail processing after exhausting retry attempts, enabling
later analysis without blocking the main pipeline. For ML systems, DLQs
are essential where data loss is unacceptable: malformed training
examples can be fixed and reprocessed. }, using separate storage for
data that fails processing after multiple retry attempts. This allows
for later analysis and potential reprocessing of problematic data
without blocking the main pipeline
(\citeproc{ref-kleppmann2017designing}{Kleppmann 2016}). A pipeline
processing financial transactions that encounters malformed data can
route it to a dead letter queue rather than losing critical records or
halting all processing.

In ML systems, dead letter queues serve dual purposes beyond failure
analysis. Production teams implement systematic review of DLQ contents
to identify: (1) schema violations indicating upstream changes, (2) edge
case patterns the model should handle, and (3) data quality issues
requiring source system fixes. For example, a fraud detection system's
DLQ revealed transactions from a new payment type the model had never
seen, prompting targeted data collection and retraining rather than
simply logging the failures. This transforms DLQs from passive error
storage into active sources for identifying model blind spots and
driving improvement.

Moving beyond ad-hoc error handling, cascade failure prevention requires
circuit breaker\sidenote{\textbf{Circuit Breaker}: Named after the
electrical safety device invented by Thomas Edison (1879) that
interrupts current flow when overload is detected. Michael Nygard
popularized the software pattern in ``Release It!'' (2007), applying the
same principle: automatically stop calling a failing service to prevent
cascade failures. The pattern has three states mirroring electrical
behavior: closed (normal flow), open (circuit broken, service blocked),
and half-open (testing if service recovered). } patterns and bulkhead
isolation to prevent single component failures from propagating
throughout the system. When a feature computation service fails, the
circuit breaker pattern stops calling that service after detecting
repeated failures, preventing the caller from waiting on timeouts that
would cascade into its own failure.

Automated recovery engineering implements sophisticated strategies
beyond simple retry logic. Progressive timeout increases prevent
overwhelming struggling services while maintaining rapid recovery for
transient issues---initial requests timeout after 1 second, but after
detecting service degradation, timeouts extend to 5 seconds, then 30
seconds, giving the service time to stabilize. Multi-tier fallback
systems provide degraded service when primary data sources fail: serving
slightly stale cached features when real-time computation fails, or
using approximate features when exact computation times out. A
recommendation system unable to compute user preferences from the past
30 days might fall back to preferences from the past 90 days, providing
somewhat less accurate but still useful recommendations rather than
failing entirely. Comprehensive alerting and escalation procedures
ensure human intervention occurs when automated recovery fails, with
sufficient diagnostic information captured during the failure to enable
rapid debugging.

These concepts become concrete when considering a financial ML system
ingesting market data. Error handling might involve falling back to
slightly delayed data sources if real-time feeds fail, while
simultaneously alerting the operations team to the issue. Dead letter
queues capture malformed price updates for investigation rather than
dropping them silently. Circuit breakers prevent the system from
overwhelming a struggling market data provider during recovery. This
comprehensive approach to error management ensures that downstream
processes have access to reliable, high-quality data for training and
inference tasks, even in the face of the inevitable failures that occur
in distributed systems at scale.

\subsection{Scalability
Patterns}\label{sec-data-engineering-ml-scalability-patterns-b9b1}

While quality and reliability ensure correct system operation,
scalability addresses a different challenge: how systems evolve as data
volumes grow and ML systems mature from prototypes to production
services. Pipelines that work effectively at gigabyte scale often break
at terabyte scale without architectural changes that enable distributed
processing. Scalability involves designing systems that handle growing
data volumes, user bases, and computational demands without requiring
complete redesigns.

At the ingestion layer, systems choose between batch processing
(collecting data in groups before processing) and stream processing
(processing data in real-time as it arrives). Batch processing enables
efficient resource utilization by amortizing costs across large volumes;
stream processing enables real-time responsiveness but at significantly
higher cost---often 10× or more per byte processed. Most production ML
systems employ hybrid approaches. The detailed trade-offs between these
patterns are examined in
Section~\ref{sec-data-engineering-ml-data-ingestion-8efc}.

Beyond ingestion patterns, distributed processing becomes necessary when
single machines cannot handle data volumes or processing complexity. The
fundamental constraint is that distributed coordination is limited by
network round-trip times: local operations complete in microseconds
while network coordination requires milliseconds, creating a 1000×
latency difference. This physics-level constraint shapes every
scalability decision.

\phantomsection\label{callout-perspectiveux2a-1.7}
\begin{fbx}{callout-perspective}{Systems Perspective: }{The Physics of Data Locality}
\phantomsection\label{callout-perspective*-1.7}
\textbf{Why moving data kills performance}: The ``time'' cost of data
movement is intuitive, but the \textbf{energy} cost is the hard physical
constraint. Engineering decisions about storage tiers (RAM vs SSD vs S3)
are ultimately governed by this hierarchy:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3973}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3151}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2740}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Operation}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Energy (Approx.)}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Relative Cost}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{32-bit Integer Add} & 0.1 pJ & 1x \\
\textbf{32-bit Float Add} & 0.9 pJ & 9x \\
\textbf{32-bit DRAM Read} & 640 pJ & \textbf{6,400x} \\
\textbf{Network Transfer (1KB)} & \textasciitilde1,000,000+ pJ &
\textbf{10,000,000x} \\
\end{longtable}

\textbf{The Engineering Implication}: Fetching a single weight from
off-chip memory (DRAM) costs 3 orders of magnitude more energy than
performing the math on it. Fetching it over the network costs 7 orders
of magnitude more. This physics dictates the design of \textbf{Feature
Stores} (caching data close to compute) and \textbf{Data Lakes}
(batching data to amortize transfer costs).

\end{fbx}

These energy and latency constraints drive ML system design toward
compute-follows-data architectures where processing moves to data rather
than data moving to processing.
Section~\ref{sec-data-engineering-ml-scaling-distributed-processing-cb9b}
examines the specific techniques for implementing distributed
processing---including Amdahl's Law limits, framework selection (Spark,
Beam, tf.data), and single-machine optimization strategies---while
Section~\ref{sec-data-engineering-ml-strategic-storage-architecture-1a6b}
details how storage architectures align with these constraints.

\subsection{Governance Through
Observability}\label{sec-data-engineering-ml-governance-observability-2c05}

Having addressed functional requirements through quality, reliability,
and scalability, we turn to governance. In pipelines, governance
manifests as comprehensive observability---the ability to understand
what data flows through the system, how it transforms, and who accesses
it. Unlike the functional pillars, governance ensures operations occur
within legal, ethical, and business constraints while maintaining
transparency and accountability.

Pipeline-level governance requires three interconnected capabilities.
\textbf{Data lineage tracking} captures the complete provenance of every
dataset: which raw sources contributed, what transformations were
applied, when processing occurred, and what code version executed. When
a model prediction proves incorrect, engineers trace back through the
pipeline to identify which training data contributed and whether they
can recreate the exact scenario for investigation. \textbf{Audit trails}
record who accessed data and when, demonstrating compliance with
regulatory frameworks like GDPR that require organizations to prove
appropriate data handling. \textbf{Access controls} enforce policies
about who can read, write, or transform data at each pipeline stage,
with ML systems often implementing attribute-based policies where data
sensitivity, user roles, and access context determine permissions.

For our KWS system, pipeline governance means tracking which version of
forced alignment generated labels, what audio normalization parameters
were applied, and which crowdsourcing batches contributed to training
data. This metadata enables reproducing training exactly when debugging
model failures and validating that serving uses identical preprocessing
to training.

The integration of these governance mechanisms transforms pipelines from
opaque data transformers into auditable, reproducible systems.
Section~\ref{sec-data-engineering-ml-tracking-data-transformation-lineage-3b09}
examines transformation-specific lineage tracking, while
Section~\ref{sec-data-engineering-ml-governance-observability-2c05}
provides comprehensive coverage of privacy protection, regulatory
compliance, and the full governance infrastructure required for
production ML systems.

With comprehensive pipeline architecture established---quality through
validation and monitoring, reliability through graceful degradation,
scalability through appropriate patterns, and governance through
observability---we must now determine what actually flows through these
carefully designed systems. The data sources we choose shape every
downstream characteristic of our ML systems.

\section{Strategic Data
Acquisition}\label{sec-data-engineering-ml-strategic-data-acquisition-418f}

Data acquisition is a strategic decision that determines a system's
capabilities and limitations. Each sourcing strategy---existing
datasets, web scraping, crowdsourcing, synthetic generation---offers
different trade-offs across quality, cost, scale, and ethical
considerations. No single approach satisfies all requirements;
successful ML systems typically combine multiple strategies, balancing
complementary strengths against competing constraints.

Our KWS system illustrates these interconnected requirements. Achieving
98\% accuracy across diverse acoustic environments requires
representative data spanning accents, ages, and recording conditions
(quality). Maintaining consistent detection despite device variations
demands data from varied hardware (reliability). Supporting millions of
concurrent users requires data volumes that manual collection cannot
economically provide (scalability). Protecting user privacy in
always-listening systems constrains collection methods and requires
careful anonymization (governance). These interconnected requirements
demonstrate why acquisition strategy must be evaluated systematically
rather than through ad-hoc source selection.

\subsection{Data Source Evaluation and
Selection}\label{sec-data-engineering-ml-data-source-evaluation-selection-cd87}

Having established the strategic importance of data acquisition, quality
serves as the primary driver. When quality requirements dominate
acquisition decisions, the choice between curated datasets, expert
crowdsourcing, and controlled web scraping depends on the accuracy
targets, domain expertise needed, and benchmark requirements that guide
model development. Achieving quality requires understanding not just
that data appears correct but that it accurately represents the
deployment environment and provides sufficient coverage of edge cases
that might cause failures.

Platforms like Kaggle (\citeproc{ref-kaggle_website}{Kaggle 2024}) and
UCI Machine Learning Repository (\citeproc{ref-uci_repo}{Information and
Sciences 2024}) provide ML practitioners with ready-to-use datasets that
can jumpstart system development. These pre-existing datasets are
particularly valuable when building ML systems as they offer immediate
access to cleaned, formatted data with established benchmarks. One of
their primary advantages is cost efficiency, as creating datasets from
scratch requires significant time and resources, especially when
building production ML systems that need large amounts of high-quality
training data. Many of these datasets, such as ImageNet
(\citeproc{ref-imagenet_website}{Lab 2024}), have become standard
benchmarks in the machine learning community, enabling consistent
performance comparisons across different models and architectures. For
ML system developers, this standardization provides clear metrics for
evaluating model improvements and system performance. The immediate
availability of these datasets allows teams to begin experimentation and
prototyping without delays in data collection and preprocessing.

Despite these advantages, ML practitioners must carefully consider the
quality assurance aspects of pre-existing datasets. For instance, the
ImageNet dataset was found to have label errors on 3.4\% of the
validation set (\citeproc{ref-northcutt2021pervasive}{Northcutt,
Athalye, and Mueller 2021}). While popular datasets benefit from
community scrutiny that helps identify and correct errors and biases,
most datasets remain ``untended gardens'' where quality issues can
significantly impact downstream system performance if not properly
addressed. As (\citeproc{ref-gebru2018datasheets}{Gebru et al. 2021})
highlighted in her paper, simply providing a dataset without
documentation can lead to misuse and misinterpretation, potentially
amplifying biases present in the data.

Beyond quality concerns, supporting documentation accompanying existing
datasets is invaluable, yet is often only present in widely-used
datasets. Good documentation provides insights into the data collection
process and variable definitions and sometimes even offers baseline
model performances. This information not only aids understanding but
also promotes reproducibility in research, a cornerstone of scientific
integrity; currently, there is a crisis around improving reproducibility
in machine learning systems (\citeproc{ref-pineau2021improving}{Pineau
et al. 2021}; \citeproc{ref-henderson2018deep}{Henderson et al. 2018}).
When other researchers have access to the same data, they can validate
findings, test new hypotheses, or apply different methodologies, thus
allowing us to build on each other's work more rapidly. The challenges
of data quality extend particularly to big data scenarios where volume
and variety compound quality concerns
(\citeproc{ref-gudivada2017data}{Gudivada, Rao, et al. 2017}), requiring
systematic approaches to quality validation at scale.

Even with proper documentation, understanding the context in which the
data was collected becomes necessary. Researchers must avoid potential
overfitting when using popular datasets such as ImageNet
(\citeproc{ref-beyer2020we}{Beyer et al. 2020}), which can lead to
inflated performance metrics. Sometimes, these datasets do not reflect
the real-world data (\citeproc{ref-venturebeat_datasets}{VentureBeat
2024}).

Central to these contextual concerns, a key consideration for ML systems
is how well pre-existing datasets reflect real-world deployment
conditions. Relying on standard datasets can create a concerning
disconnect between training and production environments.
Figure~\ref{fig-misalignment} visualizes this risk: when multiple ML
systems train on identical datasets, they propagate shared biases and
limitations throughout an entire ecosystem of deployed models.

\begin{figure}[htb]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/97a4e36279b090e2b80f69982cc6030221c502b4.pdf}}

}

\caption{\label{fig-misalignment}\textbf{Dataset Convergence}: Shared
datasets can mask limitations and propagate biases across multiple
machine learning systems, potentially leading to overoptimistic
performance evaluations and reduced generalization to unseen data.
Reliance on common datasets creates a false sense of progress within an
ecosystem of models, hindering the development of dependable AI
applications.}

\end{figure}%

For our KWS system, pre-existing datasets like Google's Speech Commands
(\citeproc{ref-warden2018speech}{Warden 2018}) provide essential
starting points, offering carefully curated voice samples for common
wake words. These datasets enable rapid prototyping and establish
baseline performance metrics. However, evaluating them against our
quality requirements immediately reveals coverage gaps: limited accent
diversity, predominantly quiet recording environments, and support for
only major languages. Quality-driven acquisition strategy recognizes
these limitations and plans complementary approaches to address them,
demonstrating how framework-based thinking guides source selection
beyond simply choosing available datasets.

\subsection{Scalability and Cost
Optimization}\label{sec-data-engineering-ml-scalability-cost-optimization-b9b3}

While quality-focused approaches excel at creating accurate,
well-curated datasets, they face inherent scaling limitations. When
scale requirements dominate---needing millions or billions of examples
that manual curation cannot economically provide---web scraping and
synthetic generation offer paths to massive datasets. Scalability
requires understanding the economic models underlying different
acquisition strategies: cost per labeled example, throughput
limitations, and how these scale with data volume. What proves
cost-effective at thousand-example scale often becomes prohibitive at
million-example scale, while approaches that require high setup costs
amortize favorably across large volumes.

Web scraping offers a powerful approach to gathering training data at
scale, particularly in domains where pre-existing datasets are
insufficient. This automated technique for extracting data from websites
has become essential for modern ML system development, enabling teams to
build custom datasets tailored to their specific needs. When
human-labeled data is scarce, web scraping demonstrates its value.
Consider computer vision systems: major datasets like ImageNet
(\citeproc{ref-imagenet_website}{Lab 2024}) and OpenImages
(\citeproc{ref-openimages_website}{Google 2024b}) were built through
systematic web scraping, significantly advancing the field of computer
vision.

Expanding beyond these computer vision applications, the impact of web
scraping extends well beyond image recognition systems. In natural
language processing, web-scraped data has enabled the development of
increasingly sophisticated ML systems. Large language models, such as
ChatGPT and Claude, rely on vast amounts of text scraped from the public
internet and media to learn language patterns and generate responses
(\citeproc{ref-groeneveld2024olmo}{Groeneveld et al. 2024}). Similarly,
specialized ML systems like GitHub's Copilot demonstrate how targeted
web scraping, in this case of code repositories, can create powerful
domain-specific assistants (\citeproc{ref-chen2021evaluating}{Chen et
al. 2021}).

Building on these foundational developments, production ML systems often
require continuous data collection to maintain relevance and
performance. Web scraping facilitates this by gathering structured data
like stock prices, weather patterns, or product information for
analytical applications. This continuous collection introduces unique
challenges for ML systems. Data consistency becomes crucial, as
variations in website structure or content formatting can disrupt the
data pipeline and affect model performance. Proper data management
through databases or warehouses becomes essential not just for storage,
but for maintaining data quality and enabling model updates.

However, alongside these powerful capabilities, web scraping presents
several challenges that ML system developers must carefully consider.
Legal and ethical constraints can limit data collection, as not all
websites permit scraping, and violating these restrictions can have
serious consequences (\citeproc{ref-harvard_law_chatgpt}{School 2024}).
When building ML systems with scraped data, teams must carefully
document data sources and ensure compliance with terms of service and
copyright laws. Privacy considerations become important when dealing
with user-generated content, often requiring systematic anonymization
procedures.

Complementing these legal and ethical constraints, technical limitations
also affect the reliability of web-scraped training data. Rate limiting
by websites can slow data collection, while the dynamic nature of web
content can introduce inconsistencies that impact model training. Web
scraping can yield unexpected or irrelevant data---for example,
historical images appearing in contemporary image searches
(Figure~\ref{fig-traffic-light})---that can pollute training datasets
and degrade model performance. These issues highlight the importance of
thorough data validation and cleaning processes in ML pipelines built on
web-scraped data.

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{contents/vol1/data_engineering/images/jpg/1914_traffic.jpeg}}

}

\caption{\label{fig-traffic-light}\textbf{Data Source Noise}: Web
scraping introduces irrelevant or outdated data into training sets,
requiring systematic data validation and cleaning to maintain model
performance and prevent spurious correlations. Historical images
appearing in contemporary searches exemplify this noise, underscoring
the need for careful filtering and quality control in web-sourced
datasets. Source: Vox.}

\end{figure}%

Crowdsourcing offers another scalable approach, using distributed human
computation to accelerate dataset creation. Platforms like Amazon
Mechanical Turk (\citeproc{ref-mturk_website}{Services 2024}) exemplify
how crowdsourcing facilitates this process by distributing annotation
tasks to a global workforce. This enables rapid collection of labels for
complex tasks such as sentiment analysis, image recognition, and speech
transcription, significantly expediting the data preparation phase. One
of the most impactful examples of crowdsourcing in machine learning is
the creation of the ImageNet dataset. ImageNet, which revolutionized
computer vision, was built by distributing image labeling tasks to
contributors via Amazon Mechanical Turk. The contributors categorized
millions of images into thousands of classes, enabling researchers to
train and benchmark models for a wide variety of visual recognition
tasks.

The dataset's availability spurred advancements in deep learning,
including the breakthrough AlexNet model in 2012
(\citeproc{ref-krizhevsky2012imagenet}{Krizhevsky, Sutskever, and Hinton
2017}) that demonstrated the power of large-scale neural networks.
ImageNet's success shows how diverse contributors for annotation can
enable machine learning systems to achieve unprecedented performance.
Beyond academic research, Google Crowdsource
(\citeproc{ref-google_crowdsource}{Google 2024a}) is a platform where
volunteers contribute labeled data to improve AI systems in applications
like language translation, handwriting recognition, and image
understanding.

Beyond these static dataset creation efforts, crowdsourcing has also
been instrumental in applications beyond traditional dataset annotation.
For instance, the navigation app Waze
(\citeproc{ref-waze_website}{Mobile 2024}) uses crowdsourced data from
its users to provide real-time traffic updates, route suggestions, and
incident reporting. These diverse applications highlight one of the
primary advantages of crowdsourcing: its scalability. By distributing
microtasks to a large audience, projects can process enormous volumes of
data quickly and cost-effectively. This scalability is particularly
beneficial for machine learning systems that require extensive datasets
to achieve high performance. The diversity of contributors introduces a
wide range of perspectives, cultural insights, and linguistic
variations, enriching datasets and improving models' ability to
generalize across populations.

Flexibility is another key benefit of crowdsourcing. Tasks can be
adjusted dynamically based on initial results, allowing for iterative
improvements in data collection. For example, Google's reCAPTCHA
(\citeproc{ref-google_recaptcha}{Google 2024c}) system uses
crowdsourcing to verify human users while simultaneously labeling
datasets for training machine learning models.

Moving beyond human-generated data entirely, synthetic data generation
represents the ultimate scalability solution, creating unlimited
training examples through algorithmic generation rather than manual
collection. This approach changes the economics of data acquisition by
removing human labor from the equation. Figure~\ref{fig-synthetic-data}
shows how synthetic data combines with historical datasets to create
larger, more diverse training sets that would be impractical to collect
manually.

\begin{figure}[htb]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/0ad0a651bda0da9003970f068660b4b451e4f52e.pdf}}

}

\caption{\label{fig-synthetic-data}\textbf{Synthetic Data Augmentation}:
Combining algorithmically generated data with historical datasets
expands training set size and diversity, mitigating limitations caused
by scarce or biased real-world data and improving model generalization.
This approach enables reliable machine learning system development when
acquiring sufficient real-world data is impractical or unethical.
Source: AnyLogic (\citeproc{ref-anylogic_synthetic}{AnyLogic 2024}).}

\end{figure}%

Building on this foundation, advancements in generative modeling
techniques have greatly enhanced the quality of synthetic data. Modern
AI systems can produce data that closely resembles real-world
distributions, making it suitable for applications ranging from computer
vision to natural language processing. For example, generative models
have been used to create synthetic images for object recognition tasks,
producing diverse datasets that closely match real-world images.
Similarly, synthetic data has been leveraged to simulate speech
patterns, enhancing the reliability of voice recognition systems.

Beyond these quality improvements, synthetic data has become
particularly valuable in domains where obtaining real-world data is
either impractical or costly. The automotive industry has embraced
synthetic data to train autonomous vehicle systems; there are only so
many cars you can physically crash to get crash-test data that might
help an ML system know how to avoid crashes in the first place.
Capturing real-world scenarios, especially rare edge cases such as
near-accidents or unusual road conditions, is inherently difficult.
Synthetic data allows researchers to simulate these scenarios in a
controlled virtual environment (\citeproc{ref-nvidia_simulation}{NVIDIA
2024}), ensuring that models are trained to handle a wide range of
conditions. This approach has proven invaluable for advancing the
capabilities of self-driving cars.

Complementing these safety-critical applications, another important
application of synthetic data lies in augmenting existing datasets.
Introducing variations into datasets enhances model reliability by
exposing the model to diverse conditions
(\citeproc{ref-shorten2019survey}{Shorten and Khoshgoftaar 2019}). For
instance, in speech recognition, data augmentation techniques like
SpecAugment (\citeproc{ref-park2019specaugment}{Park et al. 2019})
introduce noise, shifts, or pitch variations, enabling models to
generalize better across different environments and speaker styles. This
principle extends to other domains as well, where synthetic data can
fill gaps in underrepresented scenarios or edge cases.

For our KWS system, the scalability pillar drove the need for 23 million
training examples across 50 languages---a volume that manual collection
cannot economically provide. Web scraping supplements baseline datasets
with diverse voice samples from video platforms. Crowdsourcing enables
targeted collection for underrepresented languages. Synthetic data
generation fills remaining gaps through speech synthesis
(\citeproc{ref-werchniak2021exploring}{Werchniak et al. 2021}) and audio
augmentation, creating unlimited wake word variations across acoustic
environments, speaker characteristics, and background conditions. This
comprehensive multi-source strategy demonstrates how scalability
requirements shape acquisition decisions, with each approach
contributing specific capabilities to the overall data ecosystem.

\subsection{Reliability Across Diverse
Conditions}\label{sec-data-engineering-ml-reliability-across-diverse-conditions-d63c}

Beyond quality and scale considerations, the reliability pillar
addresses a critical question: will our collected data enable models
that perform consistently across the deployment environment's full range
of conditions? A dataset might achieve high quality by established
metrics yet fail to support reliable production systems if it doesn't
capture the diversity encountered during deployment. Coverage
requirements for reliable models extend beyond simple volume to
encompass geographic diversity, demographic representation, temporal
variation, and edge case inclusion that stress-test model behavior.

Understanding coverage requirements requires examining potential failure
modes. Geographic bias occurs when training data comes predominantly
from specific regions, causing models to underperform in other areas. A
study of image datasets found significant geographic skew, with image
recognition systems trained on predominantly Western imagery performing
poorly on images from other regions
(\citeproc{ref-wang2019balanced}{Wang et al. 2019}). Demographic bias
emerges when training data doesn't represent the full user population,
potentially causing discriminatory outcomes. Hidden
stratification---where subpopulations are underrepresented or exhibit
different patterns---can cause systematic failures even in models that
perform well on aggregate metrics
(\citeproc{ref-oakden2020hidden}{Oakden-Rayner et al. 2020}). Temporal
variation matters when phenomena change over time---a fraud detection
model trained only on historical data may fail against new fraud
patterns. Edge case collection proves particularly challenging yet
critical, as rare scenarios often represent high-stakes situations where
failures cause the most harm.

The challenge of edge case collection becomes apparent in autonomous
vehicle development. While normal driving conditions are easy to capture
through test fleet operation, near-accidents, unusual pedestrian
behavior, or rare weather conditions occur infrequently. Synthetic data
generation helps address this by simulating rare scenarios, but
validating that synthetic examples accurately represent real edge cases
requires careful engineering. Some organizations employ targeted data
collection where test drivers deliberately create edge cases or where
engineers identify scenarios from incident reports that need better
coverage.

Dataset convergence represents another reliability challenge.
Figure~\ref{fig-misalignment} illustrates how multiple systems training
on identical datasets inherit identical blind spots and biases. An
entire ecosystem of models may fail on the same edge cases because all
trained on data with the same coverage gaps. This systemic risk
motivates diverse data sourcing strategies where each organization
collects supplementary data beyond common benchmarks, ensuring their
models develop different strengths and weaknesses rather than shared
failure modes.

For our KWS system, reliability manifests as consistent wake word
detection across acoustic environments from quiet bedrooms to noisy
streets, across accents from various geographic regions, and across age
ranges from children to elderly speakers. The data sourcing strategy
explicitly addresses these diversity requirements: web scraping captures
natural speech variation from diverse video sources, crowdsourcing
targets underrepresented demographics and environments, and synthetic
data systematically explores the parameter space of acoustic conditions.
Without this deliberate diversity in sourcing, the system might achieve
high accuracy on test sets while failing unreliably in production
deployment.

\subsection{Governance and Ethics in
Sourcing}\label{sec-data-engineering-ml-governance-ethics-sourcing-2d7f}

The governance pillar in data acquisition encompasses legal compliance,
ethical treatment of data contributors, privacy protection, and
transparency about data origins and limitations. Unlike the other
pillars that focus on system capabilities, governance ensures data
sourcing occurs within appropriate legal and ethical boundaries. The
consequences of governance failures extend beyond system performance to
reputational damage, legal liability, and potential harm to individuals
whose data was improperly collected or used.

Legal constraints significantly limit data collection methods across
different jurisdictions and domains. Not all websites permit scraping,
and violating these restrictions can have serious consequences, as
ongoing litigation around training data for large language models
demonstrates. Copyright law governs what publicly available content can
be used for training, with different standards emerging across
jurisdictions. Terms of service agreements may prohibit using data for
ML training even when technically accessible. Privacy regulations like
GDPR in Europe and CCPA in California impose strict requirements on
personal data collection, requiring consent, enabling deletion requests,
and sometimes demanding explanations of algorithmic decisions
(\citeproc{ref-wachter2017counterfactual}{Wachter, Mittelstadt, and
Russell 2017}). Healthcare data falls under additional regulations like
HIPAA in the United States, requiring specific safeguards for patient
information. Organizations must carefully navigate these legal
frameworks, documenting data sources and ensuring compliance throughout
the acquisition process.

Beyond legal compliance, ethical sourcing requires fair treatment of
human contributors. The crowdsourcing example we examined
earlier---where OpenAI outsourced data annotation to workers in Kenya
(\citeproc{ref-time_openai_kenya}{Perrigo 2023}) paying as little as
\$1.32 per hour for reviewing traumatic content---highlights governance
failures that can occur when economic pressures override ethical
considerations. Many workers reportedly suffered psychological harm from
exposure to disturbing material without adequate mental health support.
This case underscores power imbalances that can emerge when outsourcing
data work to economically disadvantaged regions. The lack of fair
compensation, inadequate support for workers dealing with traumatic
content, and insufficient transparency about working conditions
represent governance failures that affect human welfare beyond just
system performance.

Industry-wide standards for ethical crowdsourcing have begun emerging in
response to such concerns. Fair compensation means paying at least local
minimum wages, ideally benchmarked against comparable work in workers'
regions. Worker wellbeing requires providing mental health resources for
those dealing with sensitive content, limiting exposure to traumatic
material, and ensuring reasonable working conditions. Transparency
demands clear communication about task purposes, how contributions will
be used, and worker rights. Organizations like the Partnership on AI
have published guidelines for ethical crowdwork, establishing baselines
for acceptable practices.

While quality, scalability, and reliability focus on system
capabilities, the governance pillar ensures our data acquisition occurs
within appropriate ethical and legal boundaries. Privacy protection
forms another critical governance concern, particularly when sourcing
data involving individuals who didn't explicitly consent to ML training
use. Anonymization emerges as a critical capability when handling
sensitive data. From a systems engineering perspective, anonymization
represents more than regulatory compliance; it constitutes a core design
constraint affecting data pipeline architecture, storage strategies, and
processing efficiency. ML systems must handle sensitive data throughout
their lifecycle: during collection, storage, transformation, model
training, and even in error logs and debugging outputs. A single privacy
breach can compromise not just individual records but entire datasets,
making the system unusable for future development.

Practitioners have developed a range of anonymization techniques to
mitigate privacy risks. The most straightforward approach, masking,
involves altering or obfuscating sensitive values so that they cannot be
directly traced back to the original data subject. For instance, digits
in financial account numbers or credit card numbers can be replaced with
asterisks, fixed dummy characters, or hashed values to protect sensitive
information during display or logging.

Building on this direct protection approach, generalization reduces the
precision or granularity of data to decrease the likelihood of
re-identification. Instead of revealing an exact date of birth or
address, the data is aggregated into broader categories such as age
ranges or zip code prefixes. For example, a user's exact age of 37 might
be generalized to an age range of 30-39, while their exact address might
be bucketed to city-level granularity. This technique reduces
re-identification risk by sharing data in aggregated form, though
careful granularity selection is crucial---too coarse loses analytical
value while too fine may still enable re-identification under certain
conditions.

While generalization reduces data precision, pseudonymization takes a
different approach by replacing direct identifiers---names, Social
Security numbers, email addresses---with artificial identifiers or
``pseudonyms.'' These pseudonyms must not reveal or be easily traceable
to the original data subject, enabling analysis that links records for
the same individual without exposing their identity.

Moving beyond simple identifier replacement, k-anonymity provides a more
formal approach, ensuring that each record in a dataset is
indistinguishable from at least (k-1) other records under a chosen set
of quasi-identifiers. This is achieved by suppressing or generalizing
quasi-identifiers, attributes that in combination could be used to
re-identify individuals, such as zip code, age, and gender. For example,
if (k=5), every record must share the same combination of
quasi-identifiers with at least four other records, preventing attackers
from pinpointing individuals simply by looking at these attributes. This
approach provides a formal indistinguishability guarantee under the
k-anonymity definition, but it may require significant data distortion
and does not protect against homogeneity or background knowledge
attacks.

At the most sophisticated end of this spectrum, differential privacy
(\citeproc{ref-dwork2008differential}{Dwork, n.d.}) adds carefully
calibrated noise or randomized data perturbations to query results or
datasets. The goal is to make outputs stable to the inclusion or
exclusion of any single individual's data, thereby limiting what can be
inferred about that individual. Introduced noise is controlled by the
(\varepsilon) parameter in (\varepsilon)-differential privacy, balancing
data utility and privacy. Under its formal definition, this approach
provides strong mathematical privacy guarantees, though realized
protection depends on how it is applied and how parameters are chosen.
Added noise can affect data accuracy and model performance, requiring
careful tuning to balance privacy and usefulness.

Table~\ref{tbl-anonymization-comparison} compares these approaches
across four dimensions: data utility preservation, privacy protection
level, implementation complexity, and optimal use cases, guiding
practitioners in selecting appropriate techniques based on their
specific requirements.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2015}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.1418}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.1493}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.1567}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.3284}}@{}}
\caption{\textbf{Anonymization Techniques Comparison}: Privacy-utility
trade-offs across anonymization methods. Masking preserves utility for
display but offers minimal protection; differential privacy provides
mathematical guarantees but reduces data accuracy. Practitioners must
select techniques based on their specific regulatory requirements, data
sensitivity, and analytical
needs.}\label{tbl-anonymization-comparison}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Technique}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Data Utility}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Privacy Level}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Implementation}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Best Use Case}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Technique}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Data Utility}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Privacy Level}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Implementation}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Best Use Case}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Masking} & High & Low-Medium & Simple & Displaying sensitive
data \\
\textbf{Generalization} & Medium & Medium & Moderate & Age ranges,
location bucketing \\
\textbf{Pseudonymization} & High & Medium & Moderate & Individual
tracking needed \\
\textbf{K-anonymity} & Low-Medium & High & Complex & Formal
indistinguishability (k-anonymity) \\
\textbf{Differential Privacy} & Medium & Very High & Complex &
Statistical guarantees \\
\end{longtable}

As the comparison table illustrates, effective data anonymization
balances privacy and utility. Techniques such as masking,
generalization, pseudonymization, k-anonymity, and differential privacy
each target different aspects of re-identification risk. By carefully
selecting and combining these methods, organizations can responsibly
derive value from sensitive datasets while respecting the privacy rights
and expectations of the individuals represented within them.

For our KWS system, governance constraints shape acquisition throughout.
Voice data inherently contains biometric information requiring privacy
protection, driving decisions about anonymization, consent requirements,
and data retention policies. Multilingual support raises equity
concerns---will the system work only for commercially valuable languages
or also serve smaller linguistic communities? Fair crowdsourcing
practices ensure that annotators providing voice samples or labeling
receive appropriate compensation and understand how their contributions
will be used. Transparency about data sources and limitations enables
users to understand system capabilities and potential biases. These
governance considerations don't just constrain acquisition but shape
which approaches are ethically acceptable and legally permissible.

\subsection{Integrated Acquisition
Strategy}\label{sec-data-engineering-ml-integrated-acquisition-strategy-9821}

Having examined how each pillar shapes acquisition choices, we now see
why real-world ML systems rarely use a single acquisition method in
isolation. Instead, they combine approaches strategically to balance
competing pillar requirements, recognizing that each method contributes
complementary strengths. The art of data acquisition lies in
understanding how these sources work together to create datasets that
satisfy quality, scalability, reliability, and governance constraints
simultaneously.

Our KWS system exemplifies this integrated approach. Google's Speech
Commands dataset provides a quality-assured baseline enabling rapid
prototyping and establishing performance benchmarks. However, evaluating
this against our requirements reveals gaps: limited accent diversity,
coverage of only major languages, predominantly clean recording
environments. Web scraping addresses some gaps by gathering diverse
voice samples from video platforms and speech databases, capturing
natural speech patterns across varied acoustic conditions. This scales
beyond what manual collection could provide while maintaining reasonable
quality through automated filtering.

Crowdsourcing fills targeted gaps that neither existing datasets nor web
scraping adequately address: underrepresented accents, specific
demographic groups, or particular acoustic environments identified as
weak points. By carefully designing crowdsourcing tasks with clear
guidelines and quality control, the system balances scale with quality
while ensuring ethical treatment of contributors. Synthetic data
generation completes the picture by systematically exploring the
parameter space: varying background noise levels, speaker ages,
microphone characteristics, and wake word pronunciations. This addresses
the long tail of rare conditions that are impractical to collect
naturally while enabling controlled experiments about which acoustic
variations most affect model performance.

The synthesis of these approaches demonstrates how our framework guides
strategy. Quality requirements drive use of curated datasets and expert
review. Scalability needs motivate synthetic generation and web
scraping. Reliability demands mandate diverse sourcing across
demographics and environments. Governance constraints shape consent
requirements, anonymization practices, and fair compensation policies.
Rather than selecting sources based on convenience, the integrated
strategy systematically addresses each pillar's requirements through
complementary methods.

The diversity achieved through multi-source acquisition---crowdsourced
audio with varying quality, synthetic data with perfect consistency,
web-scraped content with unpredictable formats---creates specific
challenges at the boundary where external data enters our controlled
pipeline environment.

\section{Data
Ingestion}\label{sec-data-engineering-ml-data-ingestion-8efc}

Data ingestion represents the critical junction where carefully acquired
data enters our ML systems, transforming from diverse external formats
into standardized pipeline inputs. This boundary layer must handle the
heterogeneity resulting from our multi-source acquisition strategy while
maintaining the quality, reliability, scalability, and governance
standards we've established. This transformation presents several
challenges that manifest distinctly across our framework pillars. The
quality pillar demands validation that catches issues at the entry point
before they propagate downstream. The reliability pillar requires error
handling that maintains operation despite source failures and data
anomalies. The scalability pillar necessitates throughput optimization
that handles growing data volumes and velocity. The governance pillar
enforces access controls and audit trails at the system boundary where
external data enters trusted environments.

\subsection{Batch vs.~Streaming Ingestion
Patterns}\label{sec-data-engineering-ml-batch-vs-streaming-ingestion-patterns-e1b2}

To address these ingestion challenges systematically, ML systems
typically follow two primary patterns that reflect different approaches
to data flow timing and processing. Each pattern has distinct
characteristics and use cases that shape how systems balance latency,
throughput, cost, and complexity. Understanding when to apply batch
versus streaming ingestion---or combinations thereof---requires
analyzing workload characteristics against our framework requirements.

Batch ingestion involves collecting data in groups or batches over a
specified period before processing. This method proves appropriate when
real-time data processing is not critical and data can be processed at
scheduled intervals. The batch approach enables efficient use of
computational resources by amortizing startup costs across large data
volumes and processing when resources are available or least expensive.
For example, a retail company might use batch ingestion to process daily
sales data overnight, updating their ML models for inventory prediction
each morning (\citeproc{ref-akidau2015dataflow}{Akidau et al. 2015}).
The batch job might process gigabytes of transaction data using dozens
of machines for 30 minutes, then release those resources for other
workloads. This scheduled processing proves far more cost-effective than
maintaining always-on infrastructure, particularly when slight staleness
in predictions doesn't affect business outcomes.

Batch processing also simplifies error handling and recovery. When a
batch job fails midway, the system can retry the entire batch or resume
from checkpoints without complex state management. Data scientists can
easily inspect failed batches, understand what went wrong, and reprocess
after fixes. The deterministic nature of batch processing---processing
the same input data always produces the same output---simplifies
debugging and validation. These characteristics make batch ingestion
attractive for ML workflows even when real-time processing is
technically feasible but not required.

In contrast to this scheduled approach, stream ingestion processes data
in real-time as it arrives, consuming events continuously rather than
waiting to accumulate batches. This pattern proves crucial for
applications requiring immediate data processing, scenarios where data
loses value quickly, and systems that need to respond to events as they
occur. A financial institution might use stream ingestion for real-time
fraud detection, processing each transaction as it occurs to flag
suspicious activity immediately before completing the transaction. The
value of fraud detection drops dramatically if detection occurs hours
after the fraudulent transaction completes---by then money has been
transferred and accounts compromised.

However, stream processing introduces complexity that batch processing
avoids. The system must handle backpressure when downstream systems
cannot keep pace with incoming data rates. During traffic spikes, when a
sudden surge produces data faster than processing capacity, the system
must either buffer data (requiring memory and introducing latency),
sample (losing some data), or push back to producers (potentially
causing their failures). Data freshness Service Level Agreements (SLAs)
formalize these requirements, specifying maximum acceptable delays
between data generation and availability for processing. Meeting a
100-millisecond freshness SLA requires different infrastructure than
meeting a 1-hour SLA, affecting everything from networking to storage to
processing architectures.

Recognizing the limitations of either approach alone, many modern ML
systems employ hybrid approaches, combining both batch and stream
ingestion to handle different data velocities and use cases. This
flexibility allows systems to process both historical data in batches
and real-time data streams, providing a comprehensive view of the data
landscape. A recommendation system might use streaming ingestion for
real-time user interactions---clicks, views, purchases---to update
session-based recommendations immediately, while using batch ingestion
for overnight processing of user profiles, item features, and
collaborative filtering models that don't require real-time updates.

Production systems must balance cost versus latency trade-offs when
selecting patterns: real-time processing is often materially more
expensive than batch processing, commonly by an order of magnitude or
more in total cost per byte processed. This cost differential arises
from several factors: streaming systems require always-on infrastructure
rather than schedulable resources that can spin up and down based on
workload; maintain redundant processing for fault tolerance to ensure no
events are lost; need low-latency networking and storage to meet
millisecond-scale SLAs; and cannot benefit from economies of scale that
batch processing achieves by amortizing startup costs across large data
volumes. A batch job processing one terabyte might use 100 machines for
10 minutes, while a streaming system processing the same data over 24
hours needs dedicated resources continuously available. This difference
in resource commitment per byte processed drives many architectural
decisions about which data truly requires real-time processing versus
what can tolerate batch delays.

\subsection{ETL and ELT
Comparison}\label{sec-data-engineering-ml-etl-elt-comparison-2e2b}

Beyond choosing ingestion patterns based on timing requirements,
designing effective data ingestion pipelines requires understanding the
differences between Extract, Transform, Load
(ETL)\sidenote{\textbf{Extract, Transform, Load (ETL)}: A data
processing pattern where raw data is extracted from sources, transformed
(cleaned, aggregated, validated) in a separate processing layer, then
loaded into storage. For example, a traditional ETL pipeline might
extract customer purchase logs, transform them by removing duplicates
and aggregating daily totals in Apache Spark, then load only the
aggregated results into a data warehouse. Ensures only high-quality data
reaches storage but requires reprocessing all data when transformation
logic changes. } and Extract, Load, Transform
(ELT)\sidenote{\textbf{Extract, Load, Transform (ELT)}: A data
processing pattern where raw data is extracted and immediately loaded
into scalable storage, then transformed using the storage system's
computational resources. For example, an ELT pipeline might extract raw
clickstream events directly into a data lake like S3, then use SQL
queries in a system like Snowflake or BigQuery to create multiple
transformed views: user sessions for analytics, feature vectors for ML
models, and aggregated metrics for dashboards. Enables faster iteration
and multiple transformation variants but requires more storage capacity
and careful governance of raw data. } approaches.
Figure~\ref{fig-etl-vs-elt} contrasts these two paradigms, showing how
ETL transforms data before loading while ELT loads raw data first and
transforms within the target system. These paradigms determine when data
transformations occur relative to the loading phase, significantly
impacting the flexibility and efficiency of ML pipelines. The choice
between ETL and ELT affects where computational resources are consumed,
how quickly data becomes available for analysis, and how easily
transformation logic can evolve as requirements change.

\begin{figure}[htb]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/0cf360a8bff1f6c1efa851380b98aa0743a5c794.pdf}}

}

\caption{\label{fig-etl-vs-elt}\textbf{Data Pipeline Architectures}: ETL
pipelines transform data \emph{before} loading it into a data warehouse,
while ELT pipelines load raw data first and transform it within the
warehouse, impacting system flexibility and resource allocation for
machine learning workflows. Choosing between ETL and ELT depends on data
volume, transformation complexity, and the capabilities of the target
data storage system.}

\end{figure}%

ETL is a well-established paradigm in which data is first gathered from
a source, then transformed to match the target schema or model, and
finally loaded into a data warehouse or other repository. This approach
typically results in data being stored in a ready-to-query format, which
can be advantageous for ML systems that require consistent,
pre-processed data. The transformation step occurs in a separate
processing layer before data reaches the warehouse, enabling validation
and standardization before persistence. For instance, an ML system
predicting customer churn might use ETL to standardize and aggregate
customer interaction data from multiple sources---converting different
timestamp formats to UTC, normalizing text encodings to UTF-8, and
computing aggregate features like ``total purchases last 30
days''---before loading into a format suitable for model training
(\citeproc{ref-inmon2005building}{Inmon 2005}).

The advantages of ETL become apparent in scenarios with well-defined
schemas and transformation requirements. Only cleaned, validated,
transformed data reaches the warehouse, reducing storage requirements
and simplifying downstream queries. Security and privacy compliance can
be enforced during transformation, ensuring sensitive data is masked or
encrypted before reaching storage. Quality validation occurs before
loading, preventing corrupted or invalid data from entering the
warehouse. For ML systems with stable feature pipelines and clear data
quality requirements, ETL provides a clean separation between messy
source data and curated training data.

However, ETL can be less flexible when schemas or requirements change
frequently, a common occurrence in evolving ML projects. When
transformation logic changes---adding new features, modifying
aggregations, or correcting bugs---all source data must be reprocessed
through the ETL pipeline to update the warehouse. This reprocessing can
take hours or days for large datasets, slowing iteration velocity during
ML development. The transformation layer requires dedicated
infrastructure and expertise, adding operational complexity and cost to
the data pipeline.

This is where the ELT approach offers advantages. ELT reverses the order
by first loading raw data and then applying transformations as needed
within the target system. This method is often seen in modern data lake
or schema-on-read environments, allowing for a more agile approach when
addressing evolving analytical needs in ML systems. Raw source data is
loaded quickly into scalable storage, with transformations applied using
the warehouse's computational resources. Modern cloud data warehouses
like BigQuery, Snowflake, and Redshift provide massive computational
capacity that can execute complex transformations on terabyte-scale data
in minutes.

By deferring transformations, ELT can accommodate varying uses of the
same dataset, which is particularly useful in exploratory data analysis
phases of ML projects or when multiple models with different data
requirements are being developed simultaneously. One team might compute
daily aggregates while another computes hourly aggregates, each
transforming the same raw data differently. When transformation logic
bugs are discovered, teams can reprocess data by simply rerunning
transformation queries rather than re-ingesting from sources. This
flexibility accelerates ML experimentation where feature engineering
requirements evolve rapidly.

However, ELT places greater demands on storage systems and query
engines, which must handle large amounts of unprocessed information. Raw
data storage grows larger than transformed data, increasing costs. Query
performance may suffer when transformations execute repeatedly on the
same raw data rather than reading pre-computed results. Privacy and
compliance become more complex when raw sensitive data persists in
storage rather than being masked during ingestion.

In practice, many ML systems employ hybrid approaches, selecting ETL or
ELT on a case-by-case basis depending on the specific requirements of
each data source or ML model. For example, a system might use ETL for
structured data from relational databases where schemas are well-defined
and stable, while employing ELT for unstructured data like text or
images where transformation requirements may evolve as the ML models are
refined. High-volume clickstream data might use ELT to enable rapid
loading and flexible transformation, while sensitive financial data
might use ETL to enforce encryption and masking before persistence.

When implementing streaming components within ETL/ELT architectures,
distributed systems principles become critical. The CAP
theorem\sidenote{\textbf{CAP Theorem}: Conjectured by Eric Brewer at UC
Berkeley in 2000 and formally proved by Seth Gilbert and Nancy Lynch
(MIT, 2002). The acronym captures the three competing properties:
Consistency (all nodes see the same data), Availability (system remains
operational), and Partition tolerance (system continues despite network
failures). Brewer's insight that distributed systems must sacrifice one
property fundamentally shaped how ML systems choose between databases,
streaming platforms, and storage architectures. } fundamentally
constrains streaming system design choices. Apache
Kafka\sidenote{\textbf{Apache Kafka}: Named after author Franz Kafka by
LinkedIn engineer Jay Kreps (2011), who chose the name because ``Kafka
is a system optimized for writing'' and Kafka the author was known for
his writing. The distributed streaming platform provides ordered,
replicated logs for high-throughput, low-latency event processing. Kafka
handles trillions of messages daily at companies like LinkedIn, making
it essential for ML systems requiring real-time data ingestion and
feature serving. } emphasizes consistency and partition tolerance,
making it well-suited for reliable event ordering but potentially
experiencing availability issues during network partitions. Apache
Pulsar emphasizes availability and partition tolerance, providing better
fault tolerance but with relaxed consistency guarantees. Amazon Kinesis
balances all three properties through careful configuration but requires
understanding these trade-offs for proper deployment.

\subsection{Feature Computation
Placement}\label{sec-data-engineering-ml-feature-computation-placement-a998}

For ML pipelines, an additional decision extends beyond ETL versus ELT:
where to compute features. This choice significantly impacts training
speed, storage costs, and reproducibility.

Pipeline-computed features are precomputed during ETL and stored.
Benefits include fast training iteration (features ready on disk),
reproducibility (same features used consistently), and reduced training
compute. Drawbacks include storage cost (features stored separately from
raw data), staleness risk (precomputed features may diverge from logic
changes), and inflexibility (changes require recomputation).

Loader-computed features are computed on the fly during training.
Benefits include always-fresh computation (logic changes immediately
reflected), flexible experimentation (easy to modify features), and
reduced storage (only raw data stored). Drawbacks include slower
training (computation repeated each epoch), higher compute costs (GPU
often idle waiting for features), and potential non-determinism if not
carefully implemented.

Hybrid patterns predominate in production. Expensive, stable features
(user embeddings requiring matrix factorization, historical aggregations
spanning months of data) are precomputed. Cheap, time-sensitive features
(recency signals, session context, time-based transformations) are
computed in the data loader.

For example, a recommendation system precomputes user embedding features
(expensive, stable over days) while computing
time-since-last-interaction features (cheap, time-sensitive) in the data
loader. This balances storage costs, computation time, and feature
freshness based on each feature's specific characteristics.

\subsection{Multi-Source Integration
Strategies}\label{sec-data-engineering-ml-multisource-integration-strategies-0e8a}

Regardless of whether ETL or ELT approaches are used, integrating
diverse data sources represents a key challenge in data ingestion for ML
systems. Data may originate from various sources including databases,
APIs, file systems, and IoT devices. Each source may have its own data
format, access protocol, and update frequency. The integration challenge
lies not just in connecting to these sources but in normalizing their
disparate characteristics into a unified pipeline that subsequent
processing stages can consume reliably.

Given this source diversity, ML engineers must develop reliable
connectors or adapters for each data source to effectively integrate
these sources. These connectors handle the specifics of data extraction,
including authentication, rate limiting, and error handling. For
example, when integrating with a REST API, the connector would manage
API keys, respect rate limits specified in API documentation or HTTP
headers, and handle HTTP status codes appropriately---retrying on
transient errors (500, 503), aborting on authentication failures (401,
403), and backing off when rate limited (429). A well-designed connector
abstracts these details from downstream processing, presenting a
consistent interface regardless of whether data originates from APIs,
databases, or file systems.

Beyond basic connectivity, source integration often involves data
transformation at the ingestion point. This might include parsing
JSON\sidenote{\textbf{JavaScript Object Notation (JSON)}: A lightweight,
text-based data interchange format using human-readable key-value pairs
and arrays. Ubiquitous in web APIs and modern data systems due to its
simplicity and language-agnostic parsing, though less storage-efficient
than binary formats like Parquet for large-scale ML datasets. } or XML
responses into structured formats, converting timestamps to a standard
timezone and format (typically UTC and ISO 8601), or performing basic
data cleaning operations like trimming whitespace or normalizing text
encodings. The goal is to standardize the data format as it enters the
ML pipeline, simplifying downstream processing. These transformations
differ from the business logic transformations in ETL or ELT---they
address technical format variations rather than semantic transformation
of content.

In addition to data format standardization, it's essential to consider
the reliability and availability of data sources. Some sources may
experience downtime or have inconsistent data quality. Implementing
retry mechanisms with exponential backoff handles transient failures
gracefully. Data quality checks at ingestion catch systematic problems
early---if a source suddenly starts producing null values for previously
required fields, immediate detection prevents corrupted data from
flowing downstream. Fallback procedures enable continued operation when
primary sources fail: switching to backup data sources, serving cached
data, or degrading gracefully rather than failing completely. A stock
price ingestion system might fall back to delayed prices if real-time
feeds fail, maintaining service with slightly stale data rather than
complete outage.

\subsection{Case Study: Selecting Ingestion Patterns for
KWS}\label{sec-data-engineering-ml-case-study-selecting-ingestion-patterns-kws-1ecf}

Applying these ingestion concepts to our KWS system, production
implementations demonstrate both streaming and batch patterns working in
concert, reflecting the dual operational modes we established during
problem definition. The ingestion architecture directly implements
requirements from our four-pillar framework: quality through validation
of audio characteristics, reliability through consistent operation
despite source diversity, scalability through handling millions of
concurrent streams, and governance through source authentication and
tracking.

The streaming ingestion pattern handles real-time audio data from active
devices where wake words must be detected within our 200 millisecond
latency requirement. This requires careful implementation of
publish-subscribe mechanisms using systems like Apache Kafka that buffer
incoming audio data and enable parallel processing across multiple
inference servers. The streaming path prioritizes our reliability and
scalability pillars: maintaining consistent low-latency operation
despite varying device loads and network conditions while handling
millions of concurrent audio streams from deployed devices.

Parallel to this real-time processing, batch ingestion handles data for
model training and updates. This includes the diverse data sources we
established during acquisition: new wake word recordings from
crowdsourcing efforts discussed in
Section~\ref{sec-data-engineering-ml-strategic-data-acquisition-418f},
synthetic data from voice generation systems that address coverage gaps
we identified, and validated user interactions that provide real-world
examples of both successful detections and false rejections. The batch
processing typically follows an ETL pattern where audio data undergoes
preprocessing---normalization to standard volume levels, filtering to
remove extreme noise, and segmentation into consistent
durations---before being stored in formats optimized for model training.
This processing addresses our quality pillar by ensuring training data
undergoes consistent transformations that preserve the acoustic
characteristics distinguishing wake words from background speech.

Integrating these diverse data sources presents unique challenges for
KWS systems. Real-time audio streams require rate limiting to prevent
system overload during usage spikes---imagine millions of users
simultaneously asking their voice assistants about breaking news.
Crowdsourced data needs systematic validation to ensure recording
quality meets the specifications we established during problem
definition: adequate signal-to-noise ratios, appropriate speaker
distances, and correct labeling. Synthetic data must be verified for
realistic representation of wake word variations rather than generating
acoustically implausible samples that would mislead model training.

The sophisticated error handling mechanisms required by voice
interaction systems become apparent when processing real-time audio.
Dead letter queues store failed recognition attempts for subsequent
analysis, helping identify patterns in false negatives or system
failures that might indicate acoustic conditions we didn't adequately
cover during data collection. For example, a smart home device
processing the wake word ``Alexa'' must validate several audio quality
metrics: signal-to-noise ratio above our minimum threshold established
during requirements definition, appropriate sample rate matching
training data specifications, recording duration within expected bounds
of one to two seconds, and speaker proximity indicators suggesting the
utterance was directed at the device rather than incidental speech.
Invalid samples route to dead letter queues for analysis rather than
discarding them entirely---these failures often reveal edge cases
requiring attention in the next model iteration. Valid samples flow
through to real-time processing for wake word detection while
simultaneously being logged for potential inclusion in future training
data, demonstrating how production systems continuously improve through
careful data engineering.

This ingestion architecture completes the boundary layer where external
data enters our controlled pipeline. With reliable ingestion
established---validating data quality, handling errors gracefully,
scaling to required throughput, and maintaining governance controls---we
now turn to systematic data processing that transforms ingested raw data
into ML-ready features while maintaining the training-serving
consistency essential for production systems.

\section{Systematic Data
Processing}\label{sec-data-engineering-ml-systematic-data-processing-aebc}

With reliable data ingestion established, we enter the most technically
challenging phase of the pipeline: systematic data processing. This
section addresses three interconnected processing challenges: first,
ensuring that transformations remain identical between training and
serving environments; second, building reliable transformations that
produce consistent results regardless of when or how many times they
execute; and third, scaling processing to handle growing data volumes
while maintaining data lineage for reproducibility. Each challenge
requires specific engineering techniques that we examine in turn.

A fundamental requirement, applying identical transformations during
training and serving, represents one of the most common sources of
production ML failures. Industry experience suggests that
training-serving inconsistency contributes to the majority of silent
model degradation issues (\citeproc{ref-sculley2015hidden}{Sculley et
al. 2021}). This finding reveals why training-serving consistency must
be the central organizing principle for all processing decisions.

Data processing implements the quality requirements defined in our
problem definition phase, transforming raw data into ML-ready formats
while maintaining reliability and scalability standards. Processing
decisions must preserve data integrity while improving model readiness,
all while adhering to governance principles throughout the
transformation pipeline. Every transformation---from normalization
parameters to categorical encodings to feature engineering logic---must
be applied identically in both contexts. Consider a simple example:
normalizing transaction amounts during training by removing currency
symbols and converting to floats, but forgetting to apply identical
preprocessing during serving. This seemingly minor inconsistency can
degrade model accuracy by 20-40\%, as the model receives differently
formatted inputs than it was trained on. The severity of this problem
makes training-serving consistency the central organizing principle for
processing system design.

For our KWS system, processing decisions directly impact all four
pillars as established in our problem definition
(Section~\ref{sec-data-engineering-ml-framework-application-keyword-spotting-case-study-c8e9}).
Quality transformations must preserve acoustic characteristics essential
for wake word detection while standardizing across diverse recording
conditions. Reliability requires consistent processing despite varying
audio formats collected through our multi-source acquisition strategy.
Scalability demands efficient algorithms that handle millions of audio
streams from deployed devices. Governance ensures privacy-preserving
transformations that protect user voice data throughout processing.

\subsection{Ensuring Training-Serving
Consistency}\label{sec-data-engineering-ml-ensuring-trainingserving-consistency-c683}

Quality serves as the cornerstone of data processing. Here, the quality
pillar manifests as ensuring that transformations applied during
training match exactly those applied during serving. This consistency
challenge extends beyond just applying the same code---it requires that
parameters computed on training data (normalization constants, encoding
dictionaries, vocabulary mappings) are stored and reused during serving.
Without this discipline, models receive fundamentally different inputs
during serving than they were trained on, causing performance
degradation that's often subtle and difficult to debug.

\phantomsection\label{callout-definitionux2a-1.8}
\begin{fbx}{callout-definition}{Definition: }{The Consistency Imperative}
\phantomsection\label{callout-definition*-1.8}
\textbf{Any transformation applied to data must be applied identically
in training and serving, or model performance will degrade
proportionally to the inconsistency magnitude.}

This principle---which we term the \textbf{Consistency Imperative}---is
not a guideline but a \emph{mathematical necessity}. When a model learns
\(f: X \rightarrow Y\) on transformed training data \(T(X_{train})\), it
expects serving inputs in the same transformed space. Serving raw or
differently-transformed data \(T'(X_{serve})\) feeds the model inputs
from a distribution it never encountered:

\[ \text{Accuracy Loss} \propto D_{KL}(T(X) \parallel T'(X)) \]

\textbf{Quantitative Evidence}: Industry studies report that 30--40\% of
initial ML deployments suffer from training-serving skew
(\citeproc{ref-sculley2015hidden}{Sculley et al. 2021}). A
recommendation system computing \texttt{user\_lifetime\_purchases} via
complete transaction history during training but using weekly-cached
aggregates during serving observed:

\begin{itemize}
\tightlist
\item
  \textbf{15\% feature value discrepancy} between training and serving
\item
  \textbf{12\% accuracy drop} in A/B tests
\item
  \textbf{Root cause identification time}: 2 weeks (subtle, no obvious
  errors)
\end{itemize}

\textbf{The Engineering Corollary}: Training-serving consistency is not
achieved by intention but by \emph{architecture}. Feature stores, shared
transformation libraries, and automated consistency validation exist
because good intentions fail at scale.

\end{fbx}

Data cleaning involves identifying and correcting errors,
inconsistencies, and inaccuracies in datasets. Raw data frequently
contains issues such as missing values, duplicates, or outliers that can
significantly impact model performance if left unaddressed. The key
insight is that cleaning operations must be deterministic and
reproducible: given the same input, cleaning must produce the same
output whether executed during training or serving. This requirement
shapes which cleaning techniques are safe to use in production ML
systems.

Data cleaning might involve removing duplicate records based on
deterministic keys, handling missing values through imputation or
deletion using rules that can be applied consistently, and correcting
formatting inconsistencies systematically. For instance, in a customer
database, names might be inconsistently capitalized or formatted. A data
cleaning process would standardize these entries, ensuring that ``John
Doe,'' ``john doe,'' and ``DOE, John'' are all treated as the same
entity. The cleaning rules---convert to title case, reorder to ``First
Last'' format---must be captured in code that executes identically in
training and serving. As emphasized throughout this chapter, every
cleaning operation must be applied identically in both contexts to
maintain system reliability.

Outlier detection and treatment is another important aspect of data
cleaning, but one that introduces consistency challenges. Outliers can
sometimes represent valuable information about rare events, but they can
also result from measurement errors or data corruption. ML practitioners
must carefully consider the nature of their data and the requirements of
their models when deciding how to handle outliers. Simple
threshold-based outlier removal (removing values more than 3 standard
deviations from the mean) maintains training-serving consistency if the
mean and standard deviation are computed on training data and reused
during serving. However, more sophisticated outlier detection methods
that consider relationships between features or temporal patterns
require careful engineering to ensure consistent application.

Quality assessment goes hand in hand with data cleaning, providing a
systematic approach to evaluating the reliability and usefulness of
data. This process involves examining various aspects of data quality,
including accuracy, completeness, consistency, and timeliness. In
production systems, data quality degrades in subtle ways that basic
metrics miss: fields that never contain nulls suddenly show sparse
patterns, numeric distributions drift from their training ranges, or
categorical values appear that weren't present during model development.

To address these subtle degradation patterns, production quality
monitoring requires specific metrics beyond simple missing value counts
as discussed in
Section~\ref{sec-data-engineering-ml-quality-validation-monitoring-498f}.
Critical indicators include null value patterns by feature (sudden
increases suggest upstream failures), count anomalies (10x increases
often indicate data duplication or pipeline errors), value range
violations (prices becoming negative, ages exceeding realistic bounds),
and join failure rates between data sources. Statistical drift
detection\sidenote{\textbf{Data Drift}: The term ``drift'' comes from
Old Norse ``drifa'' (to drive or push), originally describing slow
movement caused by external forces like wind or current. In ML, data
drift describes the gradual movement of production data distributions
away from training baselines, pushed by forces like changing user
behavior or evolving systems. Unlike model decay (degradation from
unchanged data), drift specifically denotes distribution shift requiring
monitoring of feature statistics to detect before accuracy degrades. }
becomes essential by monitoring means, variances, and quantiles of
features over time to catch gradual degradation before it impacts model
performance. For example, in an e-commerce recommendation system, the
average user session length might gradually increase from 8 minutes to
12 minutes over six months due to improved site design, but a sudden
drop to 3 minutes suggests a data collection bug.

Supporting these monitoring requirements, quality assessment tools range
from simple statistical measures to complex machine learning-based
approaches. Data profiling tools provide summary statistics and
visualizations that help identify potential quality issues, while
advanced techniques employ unsupervised learning algorithms to detect
anomalies or inconsistencies in large datasets. Establishing clear
quality metrics and thresholds ensures that data entering the ML
pipeline meets necessary standards for reliable model training and
inference. The key is maintaining the same quality standards and
validation logic across training and serving to prevent quality issues
from creating training-serving skew.

Transformation techniques convert data from its raw form into a format
more suitable for analysis and modeling. This process can include a wide
range of operations, from simple conversions to complex mathematical
transformations. Central to effective transformation, common
transformation tasks include normalization and standardization, which
scale numerical features to a common range or distribution. For example,
in a housing price prediction model, features like square footage and
number of rooms might be on vastly different scales. Normalizing these
features ensures that they contribute more equally to the model's
predictions (\citeproc{ref-bishop2006pattern}{Bishop 2006}). Maintaining
training-serving consistency requires that normalization parameters
(mean, standard deviation) computed on training data be stored and
applied identically during serving. This means persisting these
parameters alongside the model itself---often in the model artifact or a
separate parameter file---and loading them during serving
initialization.

Beyond numerical scaling, other transformations might involve encoding
categorical variables, handling date and time data, or creating derived
features. For instance, one-hot encoding is often used to convert
categorical variables into a format that can be readily understood by
many machine learning algorithms. Categorical encodings must handle both
the categories present during training and unknown categories
encountered during serving. A reliable approach computes the category
vocabulary during training (the set of all observed categories),
persists it with the model, and during serving either maps unknown
categories to a special ``unknown'' token or uses default values.
Without this discipline, serving encounters categories the model never
saw during training, potentially causing errors or degraded performance.

Feature engineering is the process of using domain knowledge to create
new features that make machine learning algorithms work more
effectively. This step is often considered more of an art than a
science, requiring creativity and deep understanding of both the data
and the problem at hand. Feature engineering might involve combining
existing features, extracting information from complex data types, or
creating entirely new features based on domain insights. For example, in
a retail recommendation system, engineers might create features that
capture the recency, frequency, and monetary value of customer
purchases, known as RFM analysis (\citeproc{ref-kuhn2013applied}{Kuhn
and Johnson 2013}).

Given these creative possibilities, the importance of feature
engineering cannot be overstated. Well-engineered features can often
lead to significant improvements in model performance, sometimes
outweighing the impact of algorithm selection or hyperparameter tuning.
However, the creativity required for feature engineering must be
balanced against the consistency requirements of production systems.
Every engineered feature must be computed identically during training
and serving. This means that feature engineering logic should be
implemented in libraries or modules that can be shared between training
and serving code, rather than being reimplemented separately. Many
organizations build feature stores, discussed in
Section~\ref{sec-data-engineering-ml-feature-stores-bridging-training-serving-55c8},
specifically to ensure feature computation consistency across
environments.

Applying these processing concepts to our KWS system, the audio
recordings flowing through our ingestion pipeline---whether from
crowdsourcing, synthetic generation, or real-world captures---require
careful cleaning to ensure reliable wake word detection. Raw audio data
often contains imperfections that our problem definition anticipated:
background noise from various environments (quiet bedrooms to noisy
industrial settings), clipped signals from recording level issues,
varying volumes across different microphones and speakers, and
inconsistent sampling rates from diverse capture devices. The cleaning
pipeline must standardize these variations while preserving the acoustic
characteristics that distinguish wake words from background speech---a
quality-preservation requirement that directly impacts our 98\% accuracy
target.

Quality assessment for KWS extends the general principles with
audio-specific metrics. Beyond checking for null values or schema
conformance, our system tracks background noise levels (signal-to-noise
ratio above 20 decibels), audio clarity scores (frequency spectrum
analysis), and speaking rate consistency (wake word duration within
500-800 milliseconds). The quality assessment pipeline automatically
flags recordings where background noise would prevent accurate
detection, where wake words are spoken too quickly or unclearly for the
model to distinguish them, or where clipping or distortion has corrupted
the audio signal. This automated filtering ensures only high-quality
samples reach model development. Recall how Figure~\ref{fig-cascades}
demonstrated the compounding effects of early data quality failures;
this filtering prevents precisely those cascade failures by catching
issues at the source.

Transforming audio data for KWS involves converting raw waveforms into
formats suitable for ML models while maintaining training-serving
consistency. Figure~\ref{fig-spectrogram-example} visualizes this
transformation pipeline, showing how raw audio waveforms convert into
standardized feature representations, typically Mel-frequency cepstral
coefficients (MFCCs)\sidenote{\textbf{Mel-frequency cepstral
coefficients (MFCCs)}: The ``mel'' in MFCC derives from ``melody,''
coined by Stanley Smith Stevens in 1937 to create a perceptual pitch
scale where equal distances sound equally different to humans.
``Cepstral'' is a playful reversal of ``spectral,'' invented by Bogert,
Healy, and Tukey (1963) since cepstral analysis operates on the spectrum
of a spectrum. MFCCs apply mel-scale filtering (emphasizing frequencies
humans hear best) followed by cepstral analysis, typically extracting
13-39 coefficients encoding timbral properties essential for speech
recognition. } or spectrograms,\sidenote{\textbf{Spectrogram}: From
Latin ``spectrum'' (appearance, image) and Greek ``gramma'' (something
written or drawn). The term emerged in the 1940s as sound visualization
technology developed. Spectrograms are computed via Short-Time Fourier
Transform (STFT) that applies FFT to overlapping windows, creating 2D
image-like inputs where x-axis represents time, y-axis represents
frequency, and intensity shows amplitude. This visual representation
enables CNNs to process audio as image patterns, but the transformation
must be identical in training and serving to maintain accuracy. } that
emphasize speech-relevant characteristics while reducing noise and
variability across different recording conditions.

\begin{figure}[t!]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{contents/vol1/data_engineering/images/png/kws_spectrogram.png}}

}

\caption{\label{fig-spectrogram-example}\textbf{Audio Feature
Transformation}: Advanced audio features compress raw audio waveforms
into representations that emphasize perceptually relevant
characteristics for machine learning tasks. This transformation reduces
noise and data dimensionality while preserving essential speech
information, improving model performance in applications like keyword
spotting.}

\end{figure}%

\subsection{Building Idempotent Data
Transformations}\label{sec-data-engineering-ml-building-idempotent-data-transformations-7961}

Building on quality foundations, we turn to reliability. While quality
focuses on what transformations produce, reliability ensures how
consistently they operate. Processing reliability means transformations
produce identical outputs given identical inputs, regardless of when,
where, or how many times they execute. This property, called
idempotency,\sidenote{\textbf{Idempotency}: A mathematical property
where applying an operation multiple times produces the same result as
applying it once. The term derives from Latin ``idem'' (same) and
``potent'' (power). In distributed systems, idempotency enables safe
retries after failures without corrupting state. For ML pipelines,
idempotent feature transformations guarantee reproducible training data
regardless of how many times preprocessing runs, which is essential for
debugging, A/B testing, and regulatory compliance requiring reproducible
model outputs. } proves essential for production ML systems where
processing may be retried due to failures, where data may be reprocessed
to fix bugs, or where the same data flows through multiple processing
paths.

To understand idempotency intuitively, consider a light switch. Flipping
the switch to the ``on'' position turns the light on. Flipping it to
``on'' again leaves the light on; the operation can be repeated without
changing the outcome. This is idempotent behavior. In contrast, a toggle
switch that changes state with each press is not idempotent: pressing it
repeatedly alternates between on and off states. In data processing, we
want light switch behavior where reapplying the same transformation
yields the same result, not toggle switch behavior where repeated
application changes the outcome unpredictably.

Idempotent transformations enable reliable error recovery. When a
processing job fails midway, the system can safely retry processing the
same data without worrying about duplicate transformations or
inconsistent state. A non-idempotent transformation might append data to
existing records, so retrying would create duplicates. An idempotent
transformation would upsert data (insert if not exists, update if
exists), so retrying produces the same final state. This distinction
becomes critical in distributed systems where partial failures are
common and retries are the primary recovery mechanism.

Handling partial processing failures requires careful state management.
Processing pipelines should be designed so that each stage can be
retried independently without affecting other stages. Checkpoint-restart
mechanisms enable recovery from the last successful processing state
rather than restarting from scratch. For long-running data processing
jobs operating on terabyte-scale datasets, checkpointing progress every
few minutes means a failure near the end requires reprocessing only
recent data rather than the entire dataset. The checkpoint logic must
carefully track what data has been processed and what remains, ensuring
no data is lost or processed twice.

Deterministic transformations are those that always produce the same
output for the same input, without dependence on external factors like
time, random numbers, or mutable global state. Transformations that
depend on current time (e.g., computing ``days since event'' based on
current date) break determinism---reprocessing historical data would
produce different results. The solution is to capture temporal reference
points explicitly: instead of ``days since event,'' compute ``days from
event to reference date'' where reference date is fixed and persisted.
Random operations should use seeded random number generators where the
seed is derived deterministically from input data, ensuring
reproducibility.

For our KWS system, reliability requires reproducible feature
extraction. Audio preprocessing must be deterministic: given the same
raw audio file, the same MFCC features are always computed regardless of
when processing occurs or which server executes it. This enables
debugging model behavior (can always recreate exact features for a
problematic example), reprocessing data when bugs are fixed (produces
consistent results), and distributed processing (different workers
produce identical features from the same input). The processing code
captures all parameters---FFT window size, hop length, number of MFCC
coefficients---in configuration that's versioned alongside the code,
ensuring reproducibility across time and execution environments.

\subsection{Detecting Training-Serving Skew in
Production}\label{sec-data-engineering-ml-detecting-trainingserving-skew-production-2998}

The idempotent, deterministic transformations we have examined provide
essential prevention mechanisms for training-serving consistency.
However, prevention through careful engineering is necessary but
insufficient. Even with rigorous transformation design, subtle
inconsistencies can emerge from library version differences, hardware
floating-point variations, or unintended code path divergences between
training and serving environments. Production systems therefore require
automated detection of skew as it emerges, providing the monitoring
layer that catches issues prevention cannot eliminate.

\textbf{Feature Distribution Monitoring} compares serving feature
distributions against training baselines in real time. Key metrics
include: Kolmogorov-Smirnov statistic for continuous features (alert if
KS \textgreater{} 0.1), Population Stability Index (PSI) for categorical
features (alert if PSI \textgreater{} 0.2), and per-feature
mean/variance drift with Welch's t-test.

\textbf{Shadow Scoring Pipelines} recompute training-time features on
serving data using the training code path. Comparing against serving
features catches implementation differences:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Shadow pipeline: compare training and serving feature computation}
\NormalTok{serving\_features }\OperatorTok{=}\NormalTok{ compute\_serving\_features(request)}
\NormalTok{training\_features }\OperatorTok{=}\NormalTok{ compute\_training\_features(}
\NormalTok{    request}
\NormalTok{)  }\CommentTok{\# Same logic as training}
\NormalTok{discrepancy }\OperatorTok{=}\NormalTok{ compare\_features(serving\_features, training\_features)}
\ControlFlowTok{if}\NormalTok{ discrepancy }\OperatorTok{\textgreater{}}\NormalTok{ threshold:}
\NormalTok{    alert(}\StringTok{"Training{-}serving skew detected"}\NormalTok{, details}\OperatorTok{=}\NormalTok{discrepancy)}
\end{Highlighting}
\end{Shaded}

A recommendation system detected 15\% feature drift in user lifetime
value within 48 hours of a database migration that changed aggregation
logic. Without automated monitoring, this would have caused weeks of
degraded recommendations before detection through model accuracy drops.

These monitoring signals integrate with the alerting infrastructure
covered in \textbf{?@sec-machine-learning-operations-mlops} to enable
rapid response to skew incidents. For KWS systems, monitoring would
detect if edge devices compute spectrograms differently than the
training pipeline, catching subtle differences in FFT implementations
across hardware platforms.

\subsection{Scaling Through Distributed
Processing}\label{sec-data-engineering-ml-scaling-distributed-processing-cb9b}

With quality and reliability established, we face the challenge of
scale. As datasets grow larger and ML systems become more complex, the
scalability of data processing becomes the limiting factor. Consider the
data processing stages we've discussed---cleaning, quality assessment,
transformation, and feature engineering. When these operations must
handle terabytes of data, a single machine becomes insufficient. The
cleaning techniques that work on gigabytes of data in memory must be
redesigned to work across distributed systems.

These challenges manifest when quality assessment must process data
faster than it arrives, when feature engineering operations require
computing statistics across entire datasets before transforming
individual records, and when transformation pipelines create bottlenecks
at massive volumes. Processing must scale from development (gigabytes on
laptops) through production (terabytes across clusters) while
maintaining consistent behavior.

To address these scaling bottlenecks, data must be partitioned across
multiple computing resources, which introduces coordination challenges.
Distributed coordination is fundamentally limited by network round-trip
times: local operations complete in microseconds while network
coordination requires milliseconds, creating a 1000\(\times\) latency
difference. This constraint explains why operations requiring global
coordination (like computing normalization statistics across 100
machines) create bottlenecks. Each partition computes local statistics
quickly, but combining them requires information from all partitions.

Data locality becomes critical at this scale. At 10 GB/s peak
throughput, transferring one terabyte of training data across a network
takes on the order of 100 seconds; reading the same amount from a 5 GB/s
SSD takes on the order of 200 seconds. These are the same order of
magnitude, which drives ML system design toward compute-follows-data
architectures. When processing nodes access local data at RAM speeds
(50--200 GB/s) but must coordinate over networks limited to 1--10 GB/s,
the bandwidth mismatch creates fundamental bottlenecks. Geographic
distribution amplifies these challenges: cross-datacenter coordination
must handle network latency (50--200 ms between regions), partial
failures, and regulatory constraints preventing data from crossing
borders. Understanding which operations parallelize easily versus those
requiring expensive coordination determines system architecture and
performance characteristics.

Single-machine processing suffices for surprisingly large workloads when
engineered carefully. Modern servers with 256 gigabytes RAM can process
datasets of several terabytes using out-of-core processing that streams
data from disk. Libraries like Dask or Vaex enable pandas-like APIs that
automatically stream and parallelize computations across multiple cores.
Before investing in distributed processing infrastructure, teams should
exhaust single-machine optimization: using efficient data formats
(Parquet\sidenote{\textbf{Parquet}: Named after the herringbone wood
flooring pattern, this columnar storage format (developed by Cloudera
and Twitter, 2013) stores data in nested column structures that visually
resemble parquet flooring tiles. The name reflects how data is
interlocked column-by-column rather than row-by-row. For ML systems,
this columnar layout enables reading only required features and achieves
5-10x I/O reduction compared to row-based formats like CSV. } instead of
CSV), minimizing memory allocations, leveraging vectorized operations,
and exploiting multi-core parallelism. The operational simplicity of
single-machine processing---no network coordination, no partial
failures, simple debugging---makes it preferable when performance is
adequate.

Distributed processing frameworks become necessary when data volumes or
computational requirements exceed single-machine capacity, but the
speedup achievable through parallelization faces inherent limits
described by Amdahl's Law:

\[\text{Speedup} \leq \frac{1}{S + \frac{P}{N}}\]

where \(S\) represents the serial fraction of work that cannot
parallelize, \(P\) the parallel fraction, and \(N\) the number of
processors. This explains why distributing our KWS feature extraction
across 64 cores achieves only a 64\(\times\) speedup when the work is
embarrassingly parallel (\(S \approx 0\)), but coordination-heavy
operations like computing global normalization statistics might achieve
only 10\(\times\) speedup even with 64 cores due to the serial
aggregation phase. Understanding this relationship guides architectural
decisions: operations with high serial fractions should run on fewer,
faster cores rather than many slower cores, while highly parallel
workloads benefit from maximum distribution as examined further in
\textbf{?@sec-ai-training}.

Apache Spark provides a distributed computing framework that
parallelizes transformations across clusters of machines, handling data
partitioning, task scheduling, and fault tolerance automatically. Beam
provides a unified API for both batch and streaming processing, enabling
the same transformation logic to run on multiple execution engines
(Spark, Flink, Dataflow). TensorFlow's tf.data API optimizes data
loading pipelines for ML training, supporting distributed reading,
prefetching, and transformation. The choice of framework depends on
whether processing is batch or streaming, how transformations
parallelize, and what execution environment is available.

Another important consideration is the balance between preprocessing and
on-the-fly computation. While extensive preprocessing can speed up model
training and inference, it can also lead to increased storage
requirements and potential data staleness. Production systems often
implement hybrid approaches, preprocessing computationally expensive
features while computing rapidly changing features on-the-fly. This
balance depends on storage costs, computation resources, and freshness
requirements specific to each use case. Features that are expensive to
compute but change slowly (user demographic summaries, item popularity
scores) benefit from preprocessing. Features that change rapidly
(current session state, real-time inventory levels) must be computed
on-the-fly despite computational cost.

For our KWS system, scalability manifests at multiple stages.
Development uses single-machine processing on sample datasets to iterate
rapidly. Training at scale requires distributed processing when dataset
size (23 million examples) exceeds single-machine capacity or when
multiple experiments run concurrently. The processing pipeline
parallelizes naturally: audio files are independent, so transforming
them requires no coordination between workers. Each worker reads its
assigned audio files from distributed storage, computes features, and
writes results back---a trivially parallel pattern achieving near-linear
scalability. Production deployment requires real-time processing on edge
devices with severe resource constraints (our 16 kilobyte memory limit),
necessitating careful optimization and quantization to fit processing
within device capabilities.

\subsection{Tracking Data Transformation
Lineage}\label{sec-data-engineering-ml-tracking-data-transformation-lineage-3b09}

Completing our four-pillar view of data processing, governance ensures
accountability and reproducibility. The governance pillar requires
tracking what transformations were applied, when they executed, which
version of processing code ran, and what parameters were used. This
transformation lineage enables reproducibility essential for debugging,
compliance with regulations requiring explainability, and iterative
improvement when transformation bugs are discovered. Without
comprehensive lineage, teams cannot reproduce training data, cannot
explain why models make specific predictions, and cannot safely fix
processing bugs without risking inconsistency.

Transformation versioning captures which version of processing code
produced each dataset. When transformation logic changes---fixing a bug,
adding features, or improving quality---the version number increments.
Datasets are tagged with the transformation version that created them,
enabling identification of all data requiring reprocessing when bugs are
fixed. This versioning extends beyond just code versions to capture the
entire processing environment: library versions (different NumPy
versions may produce slightly different numerical results), runtime
configurations (environment variables affecting behavior), and execution
infrastructure (CPU architecture affecting floating-point precision).

Parameter tracking maintains the specific values used during
transformation. For normalization, this means storing the mean and
standard deviation computed on training data. For categorical encoding,
this means storing the vocabulary (set of all observed categories). For
feature engineering, this means storing any constants, thresholds, or
parameters used in feature computation. These parameters are typically
serialized alongside model artifacts, ensuring serving uses identical
parameters to training. Modern ML frameworks like TensorFlow and PyTorch
provide mechanisms for bundling preprocessing parameters with models,
simplifying deployment and ensuring consistency.

Processing lineage for reproducibility tracks the complete
transformation history from raw data to final features. This includes
which raw data files were read, what transformations were applied in
what order, what parameters were used, and when processing occurred.
Lineage systems like Apache Atlas, Amundsen, or commercial offerings
instrument pipelines to automatically capture this flow. When model
predictions prove incorrect, engineers can trace back through lineage:
which training data contributed to this behavior, what quality scores
did that data have, what transformations were applied, and can we
recreate this exact scenario to investigate?

Code version ties processing results to the exact code that produced
them. When processing code lives in version control (Git), each dataset
should record the commit hash of the code that created it. This enables
recreating the exact processing environment: checking out the specific
code version, installing dependencies listed at that version, and
running processing with identical parameters. Container technologies
like Docker simplify this by capturing the entire processing environment
(code, dependencies, system libraries) in an immutable image that can be
rerun months or years later with identical results.

For our KWS system, transformation governance tracks audio processing
parameters that critically affect model behavior. When audio is
normalized to standard volume, the reference volume level is persisted.
When FFT transforms audio to frequency domain, the window size, hop
length, and window function (Hamming, Hanning, etc.) are recorded. When
MFCCs are computed, the number of coefficients, frequency range, and mel
filterbank parameters are captured. This comprehensive parameter
tracking enables several critical capabilities: reproducing training
data exactly when debugging model failures, validating that serving uses
identical preprocessing to training, and systematically studying how
preprocessing choices affect model accuracy. Without this governance
infrastructure, teams resort to manual documentation that inevitably
becomes outdated or incorrect, leading to subtle training-serving skew
that degrades production performance.

\subsection{End-to-End Processing Pipeline
Design}\label{sec-data-engineering-ml-endtoend-processing-pipeline-design-f5d2}

Integrating these cleaning, assessment, transformation, and feature
engineering steps, processing pipelines bring together the various data
processing steps into a coherent, reproducible workflow. These pipelines
ensure that data is consistently prepared across training and inference
stages, reducing the risk of data leakage and improving the reliability
of ML systems. Pipeline design determines how easily teams can iterate
on processing logic, how well processing scales as data grows, and how
reliably systems maintain training-serving consistency.

Modern ML frameworks and tools often provide capabilities for building
and managing data processing pipelines. For instance, Apache Beam and
TensorFlow Transform allow developers to define data processing steps
that can be applied consistently during both model training and serving.
The choice of data processing framework must align with the broader ML
framework ecosystem discussed in \textbf{?@sec-ai-frameworks}, where
framework-specific data loaders and preprocessing utilities can
significantly impact development velocity and system performance.

Beyond tool selection, effective pipeline design involves considerations
such as modularity, scalability, and version control. Modular pipelines
allow for easy updates and maintenance of individual processing steps.
Each transformation stage should be implemented as an independent module
with clear inputs and outputs, enabling testing in isolation and
replacement without affecting other stages. Version control for
pipelines is crucial, ensuring that changes in data processing can be
tracked and correlated with changes in model performance. When model
accuracy drops, version control enables identifying whether processing
changes contributed to the degradation.

Figure~\ref{fig-tfx-pipeline-example} illustrates this modular breakdown
using TensorFlow Extended, tracing the complete flow from initial data
ingestion through to final model deployment. Data flows through
validation, transformation, and feature engineering stages before
reaching model training, with each component independently versioned,
tested, and scaled while maintaining overall system consistency.

\begin{figure}[htb]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/0246b26f74bfa4e1488aa1c122ee619bec15f941.pdf}}

}

\caption{\label{fig-tfx-pipeline-example}\textbf{Data Processing
Pipeline}: A modular end-to-end ML pipeline, as implemented in
TensorFlow extended, highlighting key stages from raw data ingestion to
trained model deployment and serving. This decomposition enables
independent development, versioning, and scaling of each component,
improving maintainability and reproducibility of ML systems.}

\end{figure}%

Integrating these processing components, our KWS processing pipelines
must handle both batch processing for training and real-time processing
for inference while maintaining consistency between these modes. The
pipeline design ensures that the same normalization parameters computed
on training data---mean volume levels, frequency response curves, and
duration statistics---are stored and applied identically during serving.
This architectural decision reflects our reliability pillar: users
expect consistent wake word detection regardless of when their device
was manufactured or which model version it runs, requiring processing
pipelines that maintain stable behavior across training iterations and
deployment environments.

Effective data processing is the cornerstone of successful ML systems.
By carefully cleaning, transforming, and engineering data through the
lens of our four-pillar framework---quality through training-serving
consistency, reliability through idempotent transformations, scalability
through distributed processing, and governance through comprehensive
lineage---practitioners can significantly improve the performance and
reliability of their models. As the field of machine learning continues
to evolve, so too do the techniques and tools for data processing,
making this an exciting and dynamic area of study and practice.

The processing pipelines we have designed transform raw data into
structured features, but one critical input remains: the labels that
tell our models what patterns to learn. Unlike the automated
transformations examined in this section, labeling introduces human
judgment into our otherwise algorithmic pipelines---creating unique
challenges for maintaining quality, reliability, scalability, and
governance when human attention becomes the limiting resource.

\section{Data
Labeling}\label{sec-data-engineering-ml-data-labeling-6836}

With systematic data processing established, data labeling emerges as a
particularly complex systems challenge. As training datasets grow to
millions or billions of examples, the infrastructure supporting labeling
operations becomes critical to system performance. Labeling represents
human-in-the-loop system engineering where our four pillars guide
infrastructure decisions differently than in automated pipeline stages.
Quality manifests as ensuring label accuracy through consensus
mechanisms and gold standard validation. Reliability demands platform
architecture that coordinates thousands of concurrent annotators without
data loss or corruption. Scalability drives AI assistance to amplify
human judgment rather than replace it. Governance requires fair
compensation, bias mitigation, and ethical treatment of human
contributors whose labor creates the training data enabling ML systems.

Modern machine learning systems must efficiently handle the creation,
storage, and management of labels across their data pipeline. The
systems architecture must support various labeling workflows while
maintaining data consistency, ensuring quality, and managing
computational resources effectively. These requirements compound when
dealing with large-scale datasets or real-time labeling needs. The
systematic challenges extend beyond just storing and managing
labels---production ML systems need reliable pipelines that integrate
labeling workflows with data ingestion, preprocessing, and training
components while maintaining high throughput and adapting to changing
requirements.

\subsection{Label Types and Their System
Requirements}\label{sec-data-engineering-ml-label-types-system-requirements-4c33}

To build effective labeling systems, understanding how different types
of labels affect our system architecture and resource requirements is
essential. Consider a practical example: building a smart city system
that needs to detect and track various objects like vehicles,
pedestrians, and traffic signs from video feeds. Labels capture
information about key tasks or concepts, with each label type imposing
distinct storage, computation, and validation requirements.

Classification labels represent the simplest form, categorizing images
with a specific tag or (in multi-label classification) tags such as
labeling an image as ``car'' or ``pedestrian.'' While conceptually
straightforward, a production system processing millions of video frames
must efficiently store and retrieve these labels. Storage requirements
are modest---a single integer or string per image---but retrieval
patterns matter: training often samples random subsets while validation
requires sequential access to all labels, driving different indexing
strategies.

Bounding boxes extend beyond simple classification by identifying object
locations, drawing a box around each object of interest. Our system now
needs to track not just what objects exist, but where they are in each
frame. This spatial information introduces new storage and processing
challenges, especially when tracking moving objects across video frames.
Each bounding box requires storing four coordinates (x, y, width,
height) plus the object class, multiplying storage by 5x compared to
classification. More importantly, bounding box annotation requires
pixel-precise positioning that takes 10-20x longer than classification,
dramatically affecting labeling throughput and cost.

Segmentation maps provide the most comprehensive information by
classifying objects at the pixel level, highlighting each object in a
distinct color. For our traffic monitoring system, this might mean
precisely outlining each vehicle, pedestrian, and road sign. These
detailed annotations significantly increase our storage and processing
requirements. A segmentation mask for a 1920x1080 image requires 2
million labels (one per pixel), compared to perhaps 10 bounding boxes or
a single classification label. This 100,000x storage increase and the
hours required per image for manual segmentation make this approach
suitable only when pixel-level precision is essential.

\begin{figure}

\centering{

\includegraphics[width=0.9\linewidth,height=\textheight,keepaspectratio]{contents/vol1/data_engineering/images/png/cs249r_labels_new.png}

}

\caption{\label{fig-labels}\textbf{Data Annotation Granularity}:
Increasing levels of detail in data labeling---from bounding boxes to
pixel-level segmentation---impact both annotation cost and potential
model accuracy. Fine-grained segmentation provides richer information
for training but demands significantly more labeling effort and storage
capacity than coarser annotations.}

\end{figure}%

Figure~\ref{fig-labels} visualizes these increasing complexity levels,
from simple classification through bounding boxes to pixel-level
segmentation. The choice of label format depends heavily on our system
requirements and resource constraints
(\citeproc{ref-10.1109ux2fICRA.2017.7989092}{Johnson-Roberson et al.
2017}). While classification labels might suffice for simple traffic
counting, autonomous vehicles need detailed segmentation maps to make
precise navigation decisions. Leading autonomous vehicle companies often
maintain hybrid systems that store multiple label types for the same
data, allowing flexible use across different applications. A single
camera frame might have classification labels (scene type: highway,
urban, rural), bounding boxes (vehicles and pedestrians for obstacle
detection), and segmentation masks (road surface for path planning),
with each label type serving distinct downstream models.

Extending beyond these basic label types, production systems must also
handle rich metadata essential for maintaining data quality and
debugging model behavior. The Common Voice dataset
(\citeproc{ref-ardila2020common}{Ardila et al. 2020}) exemplifies
sophisticated metadata management in speech recognition: tracking
speaker demographics for model fairness, recording quality metrics for
data filtering, validation status for label reliability, and language
information for multilingual support. If our traffic monitoring system
performs poorly in rainy conditions, weather condition metadata during
data collection helps identify and address the issue. Modern labeling
platforms have built sophisticated metadata management systems that
efficiently index and query this metadata alongside primary labels,
enabling filtering during training data selection and post-hoc analysis
when model failures are discovered.

These metadata requirements demonstrate how label type choice cascades
through entire system design. A system built for simple classification
labels would need significant modifications to handle segmentation maps
efficiently. The infrastructure must optimize storage systems for the
chosen label format, implement efficient data retrieval patterns for
training, maintain quality control pipelines for validation as
established in
Section~\ref{sec-data-engineering-ml-ensuring-trainingserving-consistency-c683},
and manage version control for label updates. When labels are corrected
or refined, the system must track which model versions used which label
versions, enabling correlation between label quality improvements and
model performance gains.

\subsection{Achieving Label Accuracy and
Consensus}\label{sec-data-engineering-ml-achieving-label-accuracy-consensus-7190}

In the labeling domain, quality takes on unique challenges centered on
ensuring label accuracy despite the inherent subjectivity and ambiguity
in many labeling tasks. Even with clear guidelines and careful system
design, some fraction of labels will inevitably be incorrect Thyagarajan
et al. (\citeproc{ref-thyagarajan2023multilabel}{2022}). The challenge
is not eliminating labeling errors entirely---an impossible goal---but
systematically measuring and managing error rates to keep them within
bounds that don't degrade model performance.

Labeling failures arise from two distinct sources requiring different
engineering responses. Figure~\ref{fig-hard-labels} presents concrete
examples of both failure modes: data quality issues where the underlying
data is genuinely ambiguous or corrupted, like the blurred frog image
where even expert annotators cannot determine the species with
certainty. Other errors require deep domain expertise where the correct
label is determinable but only by experts with specialized knowledge, as
with the black stork identification. These different failure modes drive
architectural decisions about annotator qualification, task routing, and
consensus mechanisms.

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{contents/vol1/data_engineering/images/png/label-errors-examples_new.png}}

}

\caption{\label{fig-hard-labels}\textbf{Labeling Ambiguity}: How
subjective or difficult examples, such as blurry images or rare species,
can introduce errors during data labeling, highlighting the need for
careful quality control and potentially expert annotation. Source:
(\citeproc{ref-northcutt2021pervasive}{Northcutt, Athalye, and Mueller
2021}).}

\end{figure}%

Given these fundamental quality challenges, production ML systems
implement multiple layers of quality control. Systematic quality checks
continuously monitor the labeling pipeline through random sampling of
labeled data for expert review and statistical methods to flag potential
errors. The infrastructure must efficiently process these checks across
millions of examples without creating bottlenecks. Sampling strategies
typically validate 1-10\% of labels, balancing detection sensitivity
against review costs. Higher-risk applications like medical diagnosis or
autonomous vehicles may validate 100\% of labels through multiple
independent reviews, while lower-stakes applications like product
recommendations may validate only 1\% through spot checks.

Beyond random sampling approaches, collecting multiple labels per data
point, often referred to as ``consensus labeling,'' can help identify
controversial or ambiguous cases. Professional labeling companies have
developed sophisticated infrastructure for this process. For example,
\href{https://labelbox.com/}{Labelbox} has consensus tools that track
inter-annotator agreement rates and automatically route controversial
cases for expert review. \href{https://scale.com}{Scale AI} implements
tiered quality control, where experienced annotators verify the work of
newer team members. The consensus infrastructure typically collects 3-5
labels per example, computing inter-annotator agreement using metrics
like Fleiss' kappa\sidenote{\textbf{Fleiss' kappa}: Named after
biostatistician Joseph L. Fleiss who generalized Cohen's kappa in 1971
to handle multiple raters simultaneously. While Cohen's kappa works only
for two raters, Fleiss extended the mathematics to any number of
annotators, making it essential for crowdsourced labeling where
different subsets of workers label each example. Values range from -1 to
1: above 0.8 indicates strong agreement, 0.6-0.8 moderate, and below 0.4
suggests labeling guidelines need clarification. } which measures
agreement beyond what would occur by chance. Examples with low agreement
(kappa below 0.4) route to expert review rather than forcing consensus
from genuinely ambiguous cases.

The consensus approach reflects an economic trade-off essential for
scalable systems. Expert review costs 10-50x more per example than
crowdsourced labeling, but forcing agreement on ambiguous examples
through majority voting of non-experts produces systematically biased
labels. By routing only genuinely ambiguous cases to experts---often
5-15\% of examples identified through low inter-annotator
agreement---systems balance cost against quality. This tiered approach
enables processing millions of examples economically while maintaining
quality standards through targeted expert intervention.

While technical infrastructure provides the foundation for quality
control, successful labeling systems must also consider human factors.
When working with annotators, organizations need reliable systems for
training and guidance. This includes good documentation with clear
examples of correct labeling, visual demonstrations of edge cases and
how to handle them, regular feedback mechanisms showing annotators their
accuracy on gold standard examples, and calibration sessions where
annotators discuss ambiguous cases to develop shared understanding. For
complex or domain-specific tasks, the system might implement tiered
access levels, routing challenging cases to annotators with appropriate
expertise based on their demonstrated accuracy on similar examples.

Quality monitoring generates substantial data that must be efficiently
processed and tracked. Organizations typically monitor inter-annotator
agreement rates (tracking whether multiple annotators agree on the same
example), label confidence scores (how certain annotators are about
their labels), time spent per annotation (both too fast suggesting
careless work and too slow suggesting confusion), error patterns and
types (systematic biases or misunderstandings), annotator performance
metrics (accuracy on gold standard examples), and bias indicators
(whether certain annotator demographics systematically label
differently). These metrics must be computed and updated efficiently
across millions of examples, often requiring dedicated analytics
pipelines that process labeling data in near real-time to catch quality
issues before they affect large volumes of data.

\subsection{Building Reliable Labeling
Platforms}\label{sec-data-engineering-ml-building-reliable-labeling-platforms-686f}

Moving from label quality to system reliability, platform architecture
supports consistent operations. While quality focuses on label accuracy,
reliability ensures the platform architecture itself operates
consistently at scale. Scaling labeling from hundreds to millions of
examples while maintaining quality requires understanding how production
labeling systems separate concerns across multiple architectural
components. The fundamental challenge is that labeling represents a
human-in-the-loop workflow where system performance depends not just on
infrastructure but on managing human attention, expertise, and
consistency.

At the foundation sits a durable task queue that stores labeling tasks
persistently, ensuring no work gets lost when systems restart or
annotators disconnect. Most production systems use message queues like
Apache Kafka or RabbitMQ rather than databases for this purpose, since
message queues provide natural ordering, parallel consumption, and
replay capabilities that databases don't easily support. Each task
carries metadata beyond just the data to be labeled: what type of task
it is (classification, bounding boxes, segmentation), what expertise
level it requires, how urgent it is, and any context needed for accurate
labeling---perhaps related examples or relevant documentation.

The assignment service that routes tasks to annotators implements
matching logic that's more sophisticated than simple round-robin
distribution. Medical image labeling systems route chest X-rays
specifically to annotators who have demonstrated radiology expertise,
measured by their agreement with expert labels on gold standard
examples. But expertise matching alone isn't sufficient---annotators who
see only chest images or only a specific pathology can develop blind
spots, performing well on familiar examples but poorly on less common
cases. Production systems therefore constraint assignment to ensure no
annotator receives more than 30\% of their tasks from a single category,
maintaining breadth of exposure that prevents overspecialization from
degrading quality on less-familiar examples.

When tasks require multiple annotations to ensure quality, the consensus
engine determines both when sufficient labels have been collected and
how to aggregate potentially conflicting opinions. The consensus
mechanisms and economic trade-offs we examined in
Section~\ref{sec-data-engineering-ml-achieving-label-accuracy-consensus-7190}
now become platform requirements: the system must efficiently collect
multiple labels per example, compute agreement metrics, and route
low-agreement cases to expert review. The platform must implement this
routing logic efficiently, tracking which examples need expert review
and ensuring they're delivered to appropriately qualified annotators
without creating bottlenecks.

Maintaining quality at scale requires continuous measurement through
gold standard injection. The system periodically inserts examples with
known correct labels into the task stream without revealing which
examples are gold standard. This enables computing per-annotator
accuracy without the Hawthorne effect where measurement changes
behavior---annotators can't ``try harder'' on gold standard examples if
they don't know which ones they are. Annotators consistently scoring
below 85\% on gold standards receive additional training materials, more
detailed guidelines, or removal from the pool if performance doesn't
improve. Beyond simple accuracy, systems track quality across multiple
dimensions: agreement with peer annotators on the same tasks (detecting
systematic disagreement suggesting misunderstanding of guidelines), time
per task (both too fast suggesting careless work and too slow suggesting
confusion), and consistency where the same annotator sees similar
examples shown days apart to measure whether they apply labels reliably
over time.

The performance requirements of these systems become demanding at scale.
A labeling platform processing 10,000 annotations per hour must balance
latency requirements against database write capacity. Writing each
annotation immediately to a persistent database like PostgreSQL for
durability would require 2-3 writes per second, well within database
capacity. But task serving---delivering new tasks to 100,000 concurrent
annotators requesting work---requires subsecond response times that
databases struggle to provide when serving requests fan out across many
annotators. Production systems therefore maintain a two-tier storage
architecture: Redis caches active tasks enabling sub-100ms task
assignment latency, while annotations batch write to PostgreSQL every
100 annotations (typically every 30-60 seconds), providing durability
without overwhelming the database with small writes.

Horizontal scaling of these systems requires careful data partitioning.
Tasks shard by task\_id enabling independent task queue scaling,
annotator performance metrics shard by annotator\_id for fast lookup
during assignment decisions, and aggregated labels shard by example\_id
for efficient retrieval during model training. This partitioning
strategy enables systems handling millions of tasks daily to support
100,000+ concurrent annotators with median task assignment latency under
50ms, proving that human-in-the-loop systems can scale to match fully
automated pipelines when properly architected.

Beyond these architectural considerations, understanding the economics
of labeling operations reveals why scalability through AI assistance
becomes essential. Data labeling represents one of ML systems' largest
hidden costs, yet it is frequently overlooked in project planning that
focuses primarily on compute infrastructure and model training expenses.
While teams carefully optimize GPU utilization and track training costs
measured in dollars per hour, labeling expenses measured in dollars per
example often receive less scrutiny despite frequently exceeding compute
costs by orders of magnitude. Understanding the full economic model
reveals why scalability through AI assistance becomes not just
beneficial but economically necessary as ML systems mature and data
requirements grow to millions or billions of labeled examples, which
\textbf{?@sec-machine-learning-operations-mlops} examines where
operational costs compound across the ML lifecycle.

The cost structure of labeling operations follows a multiplicative model
capturing both direct annotation costs and quality control overhead:

\[\text{Total Cost} = N \times \text{Cost}_{\text{label}} \times (1 + R_{\text{review}}) \times (1 + R_{\text{rework}})\]

where \(N\) represents the number of examples,
\(\text{Cost}_{\text{label}}\) is the base cost per label,
\(R_{\text{review}}\) is the fraction requiring expert review (typically
0.05-0.15), and \(R_{\text{rework}}\) accounts for labels requiring
correction (typically 0.10-0.30). This equation reveals how quality
requirements compound costs: a dataset requiring 1 million labels at
\$0.10 per label with 10\% expert review (costing 5x more, or \$0.50)
and 20\% rework reaches \$138,000, not the \$100,000 that naive
calculation suggests. For comparison, training a ResNet-50 model on this
data might cost only \$50 for compute---nearly 3,000x less than
labeling, demonstrating why labeling economics dominate total system
costs yet receive insufficient attention during planning phases.

\phantomsection\label{callout-perspectiveux2a-1.9}
\begin{fbx}{callout-perspective}{Systems Perspective: }{Amdahl's Law for Data Pipelines}
\phantomsection\label{callout-perspective*-1.9}
\textbf{The Serial Bottleneck Principle}: Amdahl's Law reveals why
labeling dominates data engineering costs and why parallelization alone
cannot solve the problem.

The maximum speedup from parallelization is bounded by the serial
fraction:

\[ \text{Speedup}_{\max} = \frac{1}{S + \frac{P}{N}} \]

where \(S\) is the serial (non-parallelizable) fraction, \(P\) is the
parallel fraction, and \(N\) is the number of processors.

\textbf{Applied to Data Pipelines}: Industry surveys show practitioners
spend 60--80\% of their time on data preparation
(Figure~\ref{fig-ds-time}). If human labeling and review constitute 70\%
of total data engineering effort (the serial component), then:

\[ \text{Speedup}_{\max} = \frac{1}{0.70 + \frac{0.30}{\infty}} = \frac{1}{0.70} \approx 1.43\times \]

Even infinite parallelization of non-labeling work yields only
\textbf{43\% improvement}.

\textbf{The Engineering Implication}: This explains why: 1.
\textbf{AI-assisted labeling matters}: It attacks the serial bottleneck,
not just adds parallelism 2. \textbf{Data efficiency matters}: Reducing
label requirements by 50\% is equivalent to a 2x speedup in the serial
component 3. \textbf{Active learning matters}: Labeling the ``right''
10\% of data achieves more than labeling 100\% randomly

Parallelizing storage, ingestion, and processing yields diminishing
returns. The path to 10x improvement runs through reducing or automating
the human-in-the-loop component. \textbf{?@sec-data-efficiency} explores
these optimization strategies systematically, introducing metrics like
the Information-Compute Ratio to quantify data efficiency gains.

\end{fbx}

Amdahl's Law explains where to focus optimization effort, but production
budgeting requires a comprehensive view of all data engineering
costs---not just labeling. Organizations often underestimate the total
investment required to build and maintain ML datasets.

\phantomsection\label{callout-perspectiveux2a-1.10}
\begin{fbx}{callout-perspective}{Systems Perspective: }{The Total Cost of Data Ownership (TCDO)}
\phantomsection\label{callout-perspective*-1.10}
\textbf{A Unified Cost Model}: While labeling dominates, a complete data
engineering budget requires accounting for all cost components:

\[\text{TCDO} = C_{\text{acquire}} + C_{\text{label}} + C_{\text{store}} + C_{\text{process}} + C_{\text{govern}} + C_{\text{debt}}\]

where:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.1901}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.6479}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.1549}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Component}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Formula}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Typical Range}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{\(C_{\text{acquire}}\)} & \(N \times c_{\text{source}}\) &
5--15\% of total \\
\textbf{\(C_{\text{label}}\)} &
\(N \times c_{\text{label}} \times (1 + R_{\text{review}}) \times (1 + R_{\text{rework}})\)
& \textbf{40--70\% of total} \\
\textbf{\(C_{\text{store}}\)} &
\(\text{Size} \times c_{\text{tier}} \times T_{\text{retention}}\) &
5--10\% of total \\
\textbf{\(C_{\text{process}}\)} &
\(\text{FLOPs} \times c_{\text{compute}} \times N_{\text{iterations}}\)
& 10--20\% of total \\
\textbf{\(C_{\text{govern}}\)} &
\(c_{\text{compliance}} + c_{\text{audit}} + c_{\text{access}}\) &
5--15\% of total \\
\textbf{\(C_{\text{debt}}\)} & \(\text{Debt}_0 \times (1 + r)^n\) &
0--30\% (hidden) \\
\end{longtable}

\textbf{Worked Example}: Using the 1M-image project with \$138K labeling
cost computed above:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2222}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.1667}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2361}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Component}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Cost}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{\% of Total}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Acquire} & \$50K & 21\% \\
\textbf{Label} & \textbf{\$138K} & \textbf{59\%} \\
\textbf{Store} & \$15K & 6\% \\
\textbf{Process} & \$10K & 4\% \\
\textbf{Govern} & \$20K & 9\% \\
\textbf{Total} & \textbf{\$233K} & 100\% \\
\end{longtable}

\textbf{The Optimization Priority}: Labeling dominates at 59\%---nearly
14× the compute cost. Teams optimizing GPU utilization while ignoring
labeling efficiency are optimizing the wrong 4\%.

\end{fbx}

Given labeling's dominance in the TCDO model, understanding the factors
that drive labeling costs becomes essential for project planning. The
cost per label varies dramatically by task complexity and required
expertise. Simple image classification ranges from \$0.01-0.05 per label
when crowdsourced but rises to \$0.50-2.00 when requiring expert
verification. Bounding boxes cost \$0.05-0.20 per box for
straightforward cases but \$1.00-5.00 for dense scenes with many
overlapping objects. Semantic segmentation can reach \$5-50 per image
depending on precision requirements and object boundaries. Medical image
annotation by radiologists costs \$50-200 per study. When a computer
vision system requires 10 million labeled images, the difference between
\$0.02 and \$0.05 per label represents \$300,000 in project
costs---often more than the entire infrastructure budget yet frequently
discovered only after labeling begins.

\subsection{Scaling with AI-Assisted
Labeling}\label{sec-data-engineering-ml-scaling-aiassisted-labeling-9360}

As labeling demands grow exponentially with modern ML systems,
scalability becomes critical. The scalability pillar drives AI
assistance as a force multiplier for human labeling rather than a
replacement. Manual annotation alone cannot keep pace with modern ML
systems' data needs, while fully automated labeling lacks the nuanced
judgment that humans provide. AI-assisted labeling finds the sweet spot:
using automation to handle clear cases and accelerate annotation while
preserving human judgment for ambiguous or high-stakes decisions.
Figure~\ref{fig-weak-supervision} illustrates several paths AI
assistance offers to scale labeling operations, each requiring careful
system design to balance speed, quality, and resource usage.

\begin{figure}[htb]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/5e60da8e21c38a223c92fc49c8e3f5aaf3a8c697.pdf}}

}

\caption{\label{fig-weak-supervision}\textbf{AI-Augmented Labeling}:
Programmatic labeling, distant supervision, and active learning scale
data annotation by trading potential labeling errors for increased
throughput, necessitating careful system design to balance labeling
speed, cost, and model quality. These strategies enable machine learning
systems to overcome limitations imposed by manual annotation alone,
facilitating deployment in data-scarce environments. Source: Stanford AI
Lab.}

\end{figure}%

Modern AI-assisted labeling typically employs a combination of
approaches working together in the pipeline. Pre-annotation involves
using AI models to generate preliminary labels for a dataset, which
humans can then review and correct. Major labeling platforms have made
significant investments in this technology.
\href{https://snorkel.ai/}{Snorkel AI} uses programmatic labeling
(\citeproc{ref-ratner2018snorkel}{Ratner et al. 2018}) to automatically
generate initial labels at scale through rule-based heuristics and weak
supervision\sidenote{\textbf{Weak supervision}: A machine learning
paradigm that uses imperfect or approximate labels rather than manual
annotation. Sources include heuristic rules (pattern matching),
knowledge bases (dictionary lookups), existing models (pre-trained
classifiers), and distant supervision (aligning with external data).
Frameworks like Snorkel combine multiple weak sources through
noise-aware learning to produce training labels approaching expert
quality at a fraction of the cost. } signals.

Scale AI deploys pre-trained models to accelerate annotation in specific
domains like autonomous driving, where object detection models pre-label
vehicles and pedestrians that humans then verify and refine. Companies
like \href{https://www.superannotate.com/}{SuperAnnotate} provide
automated pre-labeling tools that can reduce manual effort by 50-80\%
for computer vision tasks. This method, which often employs
semi-supervised learning techniques
(\citeproc{ref-chapelle2009semisupervised}{Chapelle, Scholkopf, and Zien
2009}), can save significant time, especially for extremely large
datasets.

The emergence of Large Language Models (LLMs) like ChatGPT has further
transformed labeling pipelines. Beyond simple classification, LLMs can
generate rich text descriptions, create labeling guidelines from
examples, and even explain their reasoning for label assignments. For
instance, content moderation systems use LLMs to perform initial content
classification and generate explanations for policy violations that
human reviewers can validate. However, integrating LLMs introduces new
system challenges around inference costs (API calls can cost \$0.01-\$1
per example depending on complexity), rate limiting (cloud APIs
typically limit to 100-10,000 requests per minute), and output
validation (LLMs occasionally produce confident but incorrect labels
requiring systematic validation). Many organizations adopt a tiered
approach, using smaller specialized models for routine cases while
reserving larger LLMs for complex scenarios requiring nuanced judgment
or rare domain expertise.

Methods such as active learning\sidenote{\textbf{Active Learning}: The
term contrasts with ``passive'' learning where models receive randomly
selected training data. Formalized by David Cohn, Les Atlas, and Richard
Ladner (1994), active learning inverts the traditional paradigm: instead
of the data determining what the model learns, the model queries for
specific examples it needs most. Selection strategies target uncertain
examples (uncertainty sampling) or those maximizing expected information
gain. Active learning achieves target accuracy with 50-90\% fewer labels
than random sampling, addressing the labeling bottleneck in data
engineering. } complement these approaches by intelligently prioritizing
which examples need human attention
(\citeproc{ref-coleman2022similarity}{Coleman et al. 2022}).

These systems continuously analyze model uncertainty to identify
valuable labeling candidates. Rather than labeling a random sample of
unlabeled data, active learning selects examples where the current model
is most uncertain or where labels would most improve model performance.
The infrastructure must efficiently compute uncertainty metrics (often
prediction entropy or disagreement between ensemble models), maintain
task queues ordered by informativeness, and adapt prioritization
strategies based on incoming labels. Consider a medical imaging system:
active learning might identify unusual pathologies for expert review
while handling routine cases through pre-annotation that experts merely
verify. This approach can reduce required annotations by 50-90\%
compared to random sampling, though it requires careful engineering to
prevent feedback loops where the model's uncertainty biases which data
gets labeled.

Quality control becomes increasingly crucial as these AI components
interact. The system must monitor both AI and human performance through
systematic metrics. Model confidence calibration matters: if the AI says
it's 95\% confident but is actually only 75\% accurate at that
confidence level, pre-annotations mislead human reviewers. Human-AI
agreement rates reveal whether AI assistance helps or hinders: when
humans frequently override AI suggestions, the pre-annotations may be
introducing bias rather than accelerating work. These metrics require
careful instrumentation throughout the labeling pipeline, tracking not
just final labels but the interaction between human and AI at each
stage.

In safety-critical domains like self-driving cars, these systems must
maintain particularly rigorous standards while processing massive
streams of sensor data. Waymo's labeling infrastructure reportedly
processes millions of sensor frames daily, using AI pre-annotation to
label common objects (vehicles, pedestrians, traffic signs) while
routing unusual scenarios (construction zones, emergency vehicles,
unusual road conditions) to human experts. The system must maintain
real-time performance despite this scale, using distributed
architectures where pre-annotation runs on GPU clusters while human
review scales horizontally across thousands of annotators, with careful
load balancing ensuring neither component becomes a bottleneck.

Real-world deployments demonstrate these principles at scale in diverse
domains. Medical imaging systems
(\citeproc{ref-krishnan2022selfsupervised}{Krishnan, Rajpurkar, and
Topol 2022}) combine pre-annotation for common conditions (identifying
normal tissue, standard anatomical structures) with active learning for
unusual cases (rare pathologies, ambiguous findings), all while
maintaining strict patient privacy through secure annotation platforms
with comprehensive audit trails. Self-driving vehicle systems coordinate
multiple AI models to label diverse sensor data: one model pre-labels
camera images, another handles lidar point clouds, a third processes
radar data, with fusion logic combining predictions before human review.
Social media platforms process millions of items hourly using tiered
approaches where simpler models handle clear violations (spam, obvious
hate speech) while complex content routes to more sophisticated models
or human reviewers when initial classification is uncertain.

\subsection{Ensuring Ethical and Fair
Labeling}\label{sec-data-engineering-ml-ensuring-ethical-fair-labeling-73e8}

Unlike previous sections where governance focused on data and processes,
labeling governance centers on human welfare. The governance pillar here
addresses ethical treatment of human contributors, bias mitigation, and
fair compensation---challenges that manifest distinctly from governance
in automated pipeline stages because human welfare is directly at stake.
While governance in processing focuses on data lineage and compliance,
governance in labeling requires ensuring that the humans creating
training data are treated ethically, compensated fairly, and protected
from harm.

However, alongside these compelling advantages of crowdsourcing, the
challenges highlighted by real-world examples demonstrate why governance
cannot be an afterthought. The issue of fair compensation and ethical
data sourcing was brought into sharp focus during the development of
large-scale AI systems like OpenAI's ChatGPT. Reports revealed that
\href{https://time.com/6247678/openai-chatgpt-kenya-workers/}{OpenAI
outsourced data annotation tasks to workers in Kenya}, employing them to
moderate content and identify harmful or inappropriate material that the
model might generate. This involved reviewing and labeling distressing
content, such as graphic violence and explicit material, to train the AI
in recognizing and avoiding such outputs. While this approach enabled
OpenAI to improve the safety and utility of ChatGPT, significant ethical
concerns arose around the working conditions, the nature of the tasks,
and the compensation provided to Kenyan workers.

Many of the contributors were reportedly paid as little as \$1.32 per
hour for reviewing and labeling highly traumatic material. The emotional
toll of such work, coupled with low wages, raised serious questions
about the fairness and transparency of the crowdsourcing process. This
controversy highlights a critical gap in ethical crowdsourcing
practices. The workers, often from economically disadvantaged regions,
were not adequately supported to cope with the psychological impact of
their tasks. The lack of mental health resources and insufficient
compensation underscored the power imbalances that can emerge when
outsourcing data annotation tasks to lower-income regions.

Unfortunately, the challenges highlighted by the ChatGPT Kenya
controversy are not unique to OpenAI. Many organizations that rely on
crowdsourcing for data annotation face similar issues. As machine
learning systems grow more complex and require larger datasets, the
demand for annotated data will continue to increase. This shows the need
for industry-wide standards and best practices to ensure ethical data
sourcing. Fair compensation means paying at least local minimum wages,
ideally benchmarked against comparable work in workers' regions---not
just the legally minimum but what would be considered fair for skilled
work requiring sustained attention. For sensitive content moderation,
this often means premium pay reflecting psychological burden, sometimes
2-3x base rates.

Worker wellbeing requires providing mental health resources for those
dealing with sensitive content. Organizations like
\href{https://scale.com}{Scale AI} have implemented structured support
including: limiting exposure to traumatic content (rotating annotators
through different content types, capping hours per day on disturbing
material), providing access to counseling services at no cost to
workers, and offering immediate support channels when annotators
encounter particularly disturbing content. These measures add
operational cost but are essential for ethical operations. Transparency
demands clear communication about task purposes, how contributions will
be used, what kind of content workers might encounter, and worker rights
including ability to skip tasks that cause distress.

Beyond working conditions, bias in data labeling represents another
critical governance concern. Annotators bring their own cultural,
personal, and professional biases to the labeling process, which can be
reflected in the resulting dataset. For example, Wang et al.
(\citeproc{ref-wang2019balanced}{2019}) found that image datasets
labeled predominantly by annotators from one geographic region showed
biases in object recognition tasks, performing poorly on images from
other regions. This highlights the need for diverse annotator pools
where demographic diversity among annotators helps counteract individual
biases, though it doesn't eliminate them. Regular bias audits examining
whether label distributions differ systematically across annotator
demographics, monitoring for patterns suggesting systematic bias (all
images from certain regions receiving lower quality scores), and
addressing identified biases through additional training or guideline
refinement ensure labels support fair model behavior.

Data privacy and ethical considerations also pose challenges in data
labeling. Leading data labeling companies have developed specialized
solutions for these challenges. Scale AI, for instance, maintains
dedicated teams and secure infrastructure for handling sensitive data in
healthcare and finance, with HIPAA-compliant annotation platforms and
strict data access controls. Appen implements strict data access
controls and anonymization protocols, ensuring annotators never see
personally identifiable information when unnecessary. Labelbox offers
private cloud deployments for organizations with strict security
requirements, enabling annotation without data leaving organizational
boundaries. These privacy-preserving techniques connect directly to the
security considerations explored in a companion book, where
comprehensive approaches to protecting sensitive data throughout the ML
lifecycle are examined.\sidenote{\textbf{Security and Privacy in ML
Systems}: A companion book dedicates a full chapter to security and
privacy, building upon the data governance foundations established here.
Topics include differential privacy, secure multi-party computation,
federated learning privacy guarantees, adversarial attacks on data
pipelines, and comprehensive protection strategies for ML systems. }

Beyond privacy and working conditions, the dynamic nature of real-world
data presents another limitation. Labels that are accurate at the time
of annotation may become outdated or irrelevant as the underlying
distribution of data changes over time. This concept, known as concept
drift, necessitates ongoing labeling efforts and periodic re-evaluation
of existing labels. Governance frameworks must account for label
versioning (tracking when labels were created and by whom),
re-annotation policies (systematically re-labeling data when concepts
evolve), and retirement strategies (identifying when old labels should
be deprecated rather than used for training).

Finally, the limitations of current labeling approaches become apparent
when dealing with edge cases or rare events. In many real-world
applications, it's the unusual or rare instances that are often most
critical (e.g., rare diseases in medical diagnosis, or unusual road
conditions in autonomous driving). However, these cases are, by
definition, underrepresented in most datasets and may be overlooked or
mislabeled in large-scale annotation efforts. Governance requires
explicit strategies for handling rare events: targeted collection
campaigns for underrepresented scenarios, expert review requirements for
rare cases, and systematic tracking ensuring rare events receive
appropriate attention despite their low frequency.

This case emphasizes the importance of considering the human labor
behind AI systems. While crowdsourcing offers scalability and diversity,
it also brings ethical responsibilities that cannot be overlooked.
Organizations must prioritize the well-being and fair treatment of
contributors as they build the datasets that drive AI innovation.
Governance in labeling ultimately means recognizing that training data
isn't just bits and bytes but the product of human labor deserving
respect, fair compensation, and ethical treatment.

\subsection{Case Study: Automated Labeling in KWS
Systems}\label{sec-data-engineering-ml-case-study-automated-labeling-kws-systems-976d}

Continuing our KWS case study through the labeling stage---having
established systematic problem definition
(Section~\ref{sec-data-engineering-ml-framework-application-keyword-spotting-case-study-c8e9}),
diverse data collection strategies that address quality and coverage
requirements, ingestion patterns handling both batch and streaming
workflows, and processing pipelines ensuring training-serving
consistency---we now confront a challenge unique to speech systems at
scale. Generating millions of labeled wake word samples without
proportional human annotation cost requires moving beyond the manual and
crowdsourced approaches we examined earlier. The Multilingual Spoken
Words Corpus (MSWC) (\citeproc{ref-mazumder2021multilingual}{Mazumder et
al. 2021}) demonstrates how automated labeling addresses this challenge
through its innovative approach to generating labeled wake word data,
containing over 23.4 million one-second spoken examples across 340,000
keywords in 50 different languages.

This scale directly reflects our framework pillars in practice.
Achieving our quality target of 98\% accuracy across diverse
environments requires millions of training examples covering acoustic
variations we identified during problem definition. Reliability demands
representation across varied acoustic conditions---different background
noises, speaking styles, and recording environments. Scalability
necessitates automation rather than manual labeling because 23.4 million
examples would require approximately 2,600 person-years of effort at
even 10 seconds per label, making manual annotation economically
infeasible. Governance requirements mandate transparent sourcing and
language diversity, ensuring voice-activated technology serves speakers
of many languages rather than concentrating on only the most
commercially valuable markets.

Figure~\ref{fig-mswc} depicts this automated system, which begins with
paired sentence audio recordings and corresponding transcriptions from
projects like \href{https://commonvoice.mozilla.org/en}{Common Voice} or
multilingual captioned content platforms, processing these inputs
through forced alignment\sidenote{\textbf{Forced alignment}: The term
``forced'' distinguishes this from ``free'' alignment where the system
must also recognize what was said. In forced alignment, the
transcription is known, so the algorithm is ``forced'' to align specific
words to the audio rather than choosing among alternatives. Developed
alongside early speech recognition research in the 1970s, forced
alignment uses dynamic programming (Viterbi algorithm) to compute
optimal alignment paths matching phonetic sequences to audio frames with
millisecond precision. Tools like Montreal Forced Aligner enable
extracting individual keywords for KWS training. } to identify precise
word boundaries within continuous speech.

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{contents/vol1/data_engineering/images/png/data_engineering_kws2.png}}

}

\caption{\label{fig-mswc}\textbf{Multilingual Data Preparation}: Forced
alignment and segmentation transform paired audio-text data into labeled
one-second segments, creating a large-scale corpus for training keyword
spotting models across 50+ languages. This automated process enables
scalable development of KWS systems by efficiently generating training
examples from readily available speech resources like common voice and
multilingual captioned content.}

\end{figure}%

Building on these precise timing markers, the extraction system
generates clean keyword samples while handling engineering challenges
our problem definition anticipated: background noise interfering with
word boundaries, speakers stretching or compressing words unexpectedly
beyond our target 500-800 millisecond duration, and longer words
exceeding the one-second boundary. MSWC provides automated quality
assessment that analyzes audio characteristics to identify potential
issues with recording quality, speech clarity, or background
noise---crucial for maintaining consistent standards across 23 million
samples without the manual review expenses that would make this scale
prohibitive.

Modern voice assistant developers often build upon this automated
labeling foundation. While automated corpora may not contain the
specific wake words a product requires, they provide starting points for
KWS prototyping, particularly in underserved languages where commercial
datasets don't exist. Production systems typically layer targeted human
recording and verification for challenging cases---unusual accents, rare
words, or difficult acoustic environments that automated systems
struggle with---requiring infrastructure that gracefully coordinates
between automated processing and human expertise. This demonstrates how
the four pillars guide integration: quality through targeted human
verification, reliability through automated consistency, scalability
through forced alignment, and governance through transparent sourcing
and multilingual coverage.

The sophisticated orchestration of forced alignment, extraction, and
quality control demonstrates how thoughtful data engineering directly
impacts production machine learning systems. When a voice assistant
responds to its wake word, it draws upon this labeling infrastructure
combined with the collection strategies, pipeline architectures, and
processing transformations we have examined throughout this chapter.

\section{Strategic Storage
Architecture}\label{sec-data-engineering-ml-strategic-storage-architecture-1a6b}

The carefully labeled datasets emerging from our automated and
human-in-the-loop processes---23 million audio samples spanning 50
languages in our KWS system---now require strategic storage decisions
that determine training efficiency, serving latency, and long-term
maintainability. While pipeline architecture addressed data flow and
transformation, storage architecture addresses the complementary
question: where does this data reside, how is it organized, and how do
we optimize for fundamentally different access patterns? Batch training
requires sequential scans across millions of examples, while real-time
serving demands millisecond lookups of individual feature vectors. These
competing requirements shape every storage decision.

Storage decisions determine how effectively we can maintain data quality
over time, ensure reliable access under varying loads, scale to handle
growing data volumes, and implement governance controls. The seemingly
straightforward question of ``where should we store this data''
encompasses complex trade-offs between access patterns, cost
constraints, consistency requirements, and performance characteristics
that fundamentally shape how ML systems operate.

ML storage requirements differ fundamentally from transactional systems
that power traditional applications. Rather than optimizing for frequent
small writes and point lookups that characterize e-commerce or banking
systems, ML workloads prioritize high-throughput sequential reads over
frequent writes, large-scale scans over row-level updates, and schema
flexibility over rigid structures. A database serving an e-commerce
application performs well with millions of individual product lookups
per second, but an ML training job that needs to scan that entire
product catalog repeatedly across training epochs requires completely
different storage optimization. This section examines matching storage
architectures to ML workload characteristics, comparing databases, data
warehouses, and data lakes before exploring specialized ML
infrastructure like feature stores and examining how storage
requirements evolve across the ML lifecycle.

\subsection{ML Storage Systems Architecture
Options}\label{sec-data-engineering-ml-ml-storage-systems-architecture-options-67fa}

Storage system selection extends beyond capacity planning. The goal is
minimizing the \textbf{Data Term}
(\(\frac{\text{Data}}{\text{Bandwidth}}\)) in the Iron Law of ML
Systems. Every storage medium imposes physical constraints on bandwidth
that determine the maximum speed of your training and serving pipelines.

Optimizing this term requires understanding two fundamental storage
performance metrics:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{IOPS (Input/Output Operations Per Second)}: The number of
  distinct read/write requests a device can handle per second. This
  limits performance for \textbf{random access} workloads (e.g.,
  fetching small batches of images or individual user profiles).
\item
  \textbf{Throughput (Bandwidth)}: The volume of data transferred per
  second, typically \(\text{IOPS} \times \text{Block Size}\). This
  limits performance for \textbf{sequential access} workloads (e.g.,
  scanning a Parquet file for training).
\end{enumerate}

The choice between databases, data warehouses, and data lakes is
fundamentally a choice about which of these metrics to optimize:

\begin{itemize}
\tightlist
\item
  \textbf{Databases (OLTP)}: Optimize for high IOPS with small block
  sizes (random access). Suited for serving individual feature vectors
  in real-time where per-request latency dominates.
\item
  \textbf{Data Warehouses (OLAP)}: Optimize for high Throughput with
  large block sizes (sequential access). Ideal for feature engineering
  and batch analytics.
\item
  \textbf{Data Lakes}: Prioritize capacity and throughput for
  unstructured data. Essential for training jobs where the ``Data''
  numerator is measured in petabytes and aggregate bandwidth must scale
  to thousands of GPUs.
\end{itemize}

Each storage architecture exhibits distinct strengths when applied to
specific ML tasks. For online feature serving, the high-IOPS
characteristics of databases enable millisecond lookups of individual
records. A recommendation system looking up a user's profile during
real-time inference exemplifies this pattern: fetching specific user
features (age, location, preferences) to generate personalized
recommendations requires random access optimized for per-request
latency.

For model training on structured data, the throughput-optimized design
of data warehouses enables high-speed sequential scans over large, clean
tables. Training a fraud detection model that processes millions of
transactions with hundreds of features per transaction benefits from
columnar storage that reads only relevant features efficiently, directly
reducing the Data Term by minimizing bytes transferred.

For exploratory analysis and training on unstructured data (images,
audio, text), data lakes provide the flexibility and low-cost storage
needed for massive volumes. A computer vision system storing terabytes
of raw images alongside metadata, annotations, and intermediate
processing results requires the schema flexibility and cost efficiency
that only data lakes provide, where the sheer scale of the Data
numerator demands the highest aggregate bandwidth.

Databases excel at operational and transactional purposes, maintaining
product catalogs, user profiles, or transaction histories with strong
consistency guarantees and low-latency point lookups. For ML workflows,
databases serve specific roles well: storing feature metadata that
changes frequently, managing experiment tracking where transactional
consistency matters, or maintaining model registries that require atomic
updates. A PostgreSQL database handling structured user
attributes---user\_id, age, country, preferences---provides millisecond
lookups for serving systems that need individual user features in
real-time. However, databases struggle when ML training requires
scanning millions of records repeatedly across multiple epochs. The
row-oriented storage that optimizes transactional lookups becomes
inefficient when training needs only 20 of 100 columns from each record
but must read entire rows to extract those columns.

Data warehouses fill this analytical gap, optimized for complex queries
across integrated datasets transformed into standardized schemas. Modern
warehouses like Google BigQuery, Amazon Redshift, and Snowflake use
columnar storage formats
(\citeproc{ref-stonebraker2005cstore}{Stonebraker et al. 2018}) that
enable reading specific features without loading entire
records---essential when tables contain hundreds of columns but training
needs only a subset. This columnar organization delivers five to ten
times I/O reduction compared to row-based formats for typical ML
workloads. Consider a fraud detection dataset with 100 columns where
models typically use 20 features---columnar storage reads only needed
columns, achieving 80\% I/O reduction before even considering
compression. Many successful ML systems draw training data from
warehouses because the structured environment simplifies exploratory
analysis and iterative development. Data analysts can quickly compute
aggregate statistics, identify correlations between features, and
validate data quality using familiar SQL interfaces.

However, warehouses assume relatively stable schemas and struggle with
truly unstructured data---images, audio, free-form text---or rapidly
evolving formats common in experimental ML pipelines. When a computer
vision team wants to store raw images alongside extracted features,
multiple annotation formats from different labeling vendors,
intermediate model predictions, and embedding vectors, forcing all these
into rigid warehouse schemas creates more friction than value. Schema
evolution becomes painful: adding new feature types requires ALTER TABLE
operations that may take hours on large datasets, blocking other
operations and slowing iteration velocity.

Data lakes address these limitations by storing structured,
semi-structured, and unstructured data in native formats, deferring
schema definitions until the point of reading---a pattern called
schema-on-read.\sidenote{\textbf{Schema-on-read}: The word ``schema''
derives from Greek ``skhema'' meaning shape, form, or plan. In
computing, it describes the structure that defines how data is
organized. Schema-on-read applies this structure at query time rather
than during ingestion, contrasting with schema-on-write (traditional
databases) where data must conform to a predefined structure before
storage. This paradigm enables flexibility for evolving data formats but
requires careful metadata management for discoverability. }

This flexibility proves valuable during early ML development when teams
experiment with diverse data sources and aren't certain which features
will prove useful. A recommendation system might store in the same data
lake: transaction logs as JSON, product images as JPEGs, user reviews as
text files, clickstream data as Parquet, and model embeddings as NumPy
arrays. Rather than forcing these heterogeneous types into a common
schema upfront, the data lake preserves them in their native formats.
Applications impose schema only when reading, enabling different
consumers to interpret the same data differently---one team extracts
purchase amounts from transaction logs while another analyzes temporal
patterns, each applying schemas suited to their analysis.

This flexibility comes with serious governance challenges. Without
disciplined metadata management and cataloging, data lakes degrade into
``data swamps''---disorganized repositories where finding relevant data
becomes nearly impossible, undermining the productivity benefits that
motivated their adoption. A data lake might contain thousands of
datasets across hundreds of directories with names like
``userdata\_v2\_final'' and ``userdata\_v2\_final\_ACTUALLY\_FINAL'',
where only the original authors (who have since left the company)
understand what distinguishes them. Successful data lake implementations
maintain searchable metadata about data lineage, quality metrics, update
frequencies, ownership, and access patterns---essentially providing
warehouse-like discoverability over lake-scale data. Tools like AWS Glue
Data Catalog, Apache Atlas, or Databricks Unity Catalog provide this
metadata layer, enabling teams to discover and understand data before
investing effort in processing it.

Table~\ref{tbl-storage} summarizes these essential trade-offs, comparing
databases, warehouses, and data lakes across purpose, data types, scale,
and performance optimization.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2581}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2258}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2500}}@{}}
\caption{\textbf{Storage System Characteristics}: Different storage
systems suit distinct stages of machine learning workflows based on data
structure and purpose; databases manage transactional data, data
warehouses support analytical reporting, and data lakes accommodate
diverse, raw data for future processing. Understanding these
characteristics enables efficient data management and supports the
scalability of machine learning
applications.}\label{tbl-storage}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Attribute}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Conventional Database}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Data Warehouse}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Data Lake}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Attribute}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Conventional Database}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Data Warehouse}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Data Lake}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Purpose} & Operational and transactional & Analytical and
reporting & Storage for raw and diverse data for future processing \\
\textbf{Data type} & Structured & Structured & Structured,
semi-structured, and unstructured \\
\textbf{Scale} & Small to medium volumes & Medium to large volumes &
Large volumes of diverse data \\
\textbf{Performance Optimization} & Optimized for transactional queries
(OLTP) & Optimized for analytical queries (OLAP) & Optimized for
scalable storage and retrieval \\
\textbf{Examples} & MySQL, PostgreSQL, Oracle DB & Google BigQuery,
Amazon Redshift, Microsoft Azure Synapse & Google Cloud Storage, AWS S3,
Azure Data Lake Storage \\
\end{longtable}

Choosing appropriate storage requires systematic evaluation of workload
requirements rather than following technology trends. Databases are a
strong fit when data volume is modest, query patterns involve frequent
updates and complex joins, latency requirements demand subsecond
response, and strong consistency is required. A user profile store
serving real-time recommendations exemplifies this pattern: small
per-user records measured in kilobytes, frequent reads and writes as
preferences update, strict consistency ensuring users see their own
updates immediately, and tight latency requirements. Databases become
inadequate when analytical queries must span large datasets requiring
table scans, schema evolution occurs frequently as feature requirements
change, or storage cost becomes a dominant driver and cheaper
alternatives become economically compelling.

Data warehouses excel when data volumes span one to 100 terabytes,
analytical query patterns dominate transactional operations, batch
processing latency measured in minutes to hours is acceptable, and
structured data with relatively stable schemas represents the primary
workload. Model training data preparation, batch feature engineering,
and historical analysis fit this profile. The migration path from
databases to warehouses typically occurs when query complexity
increases---requiring aggregations or joins across tables totaling
gigabytes rather than megabytes---or when analytical workloads start
degrading transactional system performance. Warehouses become inadequate
when real-time streaming ingestion is required with latency measured in
seconds, or when unstructured data comprises more than 20\% of
workloads, as warehouse schema rigidity creates excessive friction for
heterogeneous data.

Data lakes become essential when data volumes exceed 100 terabytes,
schema flexibility is critical for evolving data sources or experimental
features, cost optimization is a primary concern (often 10 times cheaper
than warehouses at scale), and diverse data types must coexist.
Large-scale model training, particularly for multimodal systems
combining text, images, audio, and structured features, requires data
lake flexibility. Consider a self-driving car system storing: terabytes
of camera images and lidar point clouds from test vehicles, vehicle
telemetry as time-series data, manually-labeled annotations identifying
objects and behaviors, automatically-generated synthetic data for rare
scenarios, and model predictions for comparison against ground truth.
Forcing these diverse types into warehouse schemas would require
substantial transformation effort and discard nuances that native
formats preserve. However, data lakes demand sophisticated catalog
management and metadata governance to prevent quality degradation---the
critical distinction between a productive data lake and an unusable data
swamp.

Migration patterns between storage types follow predictable trajectories
as ML systems mature and scale. Early-stage projects often start with
databases, drawn by familiar SQL interfaces and existing organizational
infrastructure. As datasets grow beyond database efficiency thresholds
or analytical queries start affecting operational performance, teams
migrate to warehouses. The warehouse serves well during stable
production phases with established feature pipelines and relatively
fixed schemas. When teams need to incorporate new data types---images
for computer vision augmentation, unstructured text for natural language
features, or audio for voice applications---or when cost optimization
becomes critical at terabyte or petabyte scale, migration to data lakes
occurs. Mature ML organizations typically employ all three storage types
orchestrated through unified data catalogs: databases for operational
data and real-time serving, warehouses for curated analytical data and
feature engineering, and data lakes for raw heterogeneous data and
large-scale training datasets.

\subsection{ML Storage Requirements and
Performance}\label{sec-data-engineering-ml-ml-storage-requirements-performance-1b0d}

Beyond the functional differences between storage systems, cost and
performance characteristics directly impact ML system economics and
iteration speed. Understanding these quantitative trade-offs enables
informed architectural decisions based on workload requirements.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2213}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.1967}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.1803}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.1639}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2131}}@{}}
\caption{\textbf{Storage Cost-Performance Trade-offs}: Different storage
tiers provide distinct cost-performance characteristics that determine
their suitability for specific ML workloads. Training data loading
requires high-throughput sequential access, online serving needs
low-latency random reads, while archival storage prioritizes cost over
access speed for compliance and historical
data.}\label{tbl-storage-performance}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Storage Tier}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Cost (\$/TB/month)}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Sequential Read} \textbf{Throughput}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Random Read} \textbf{Latency}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Typical ML Use Case}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Storage Tier}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Cost (\$/TB/month)}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Sequential Read} \textbf{Throughput}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Random Read} \textbf{Latency}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Typical ML Use Case}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{NVME SSD (local)} & \$100--300 & 5--7 GB/s & 10--100 μs &
Training data loading, active feature serving \\
\textbf{Object Storage} \textbf{(S3, GCS)} & \$20--25 & 100--500 MB/s
(per connection) & 10--50 ms & Data lake raw storage, model artifacts \\
\textbf{Data Warehouse} \textbf{(BigQuery, Redshift)} & \$20--40 & 1--5
GB/s (columnar scan) & 100--500 ms (query startup) & Training data
queries, feature engineering \\
\textbf{In-Memory Cache} \textbf{(Redis, Memcached)} & \$500--1000 &
20--50 GB/s & 1--10 μs & Online feature serving, real-time inference \\
\textbf{Archival Storage} \textbf{(Glacier, Nearline)} & \$1--4 & 10--50
MB/s (after retrieval) & Hours (retrieval) & Historical retention,
compliance archives \\
\end{longtable}

Table~\ref{tbl-storage-performance} reveals why ML systems employ tiered
storage architectures. Consider the economics of storing our KWS
training dataset (736 GB): object storage costs \$15--18/month, enabling
affordable long-term retention of raw audio, while maintaining working
datasets on NVMe\sidenote{\textbf{Non-Volatile Memory Express (NVMe)}: A
storage protocol designed specifically for flash memory, bypassing the
legacy AHCI interface that SATA SSDs use. NVMe connects directly to the
PCIe bus, enabling 64K command queues versus SATA's single queue,
reducing latency from milliseconds to microseconds. For ML training
workloads, NVMe's 5-7 GB/s sequential throughput prevents storage from
bottlenecking GPU utilization, while SATA SSD's 500 MB/s limit would
leave expensive accelerators idle waiting for data. } for active
training costs \(74–220/month but provides 50\)\times\$ faster data
loading.

The performance difference directly impacts iteration velocity. Training
that loads data at 5 GB/s completes dataset loading in 150 seconds,
compared to 7,360 seconds at typical object storage speeds. This
50\(\times\) difference determines whether teams can iterate multiple
times daily or must wait hours between experiments.

To build engineering judgment, practitioners must internalize the orders
of magnitude separating these tiers. Table~\ref{tbl-ml-latencies}
translates these disparities into human-scale analogies: if a CPU cycle
were one second, fetching from local SSD would take two days, while a
cross-country network request would span six years. These are the
``numbers every ML systems engineer should know.''

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2784}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1959}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1856}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3196}}@{}}
\caption{\textbf{Latency Numbers Every ML Systems Engineer Should Know}:
Understanding the quantitative disparities in the storage hierarchy is
essential for diagnosing bottlenecks. If a CPU cycle were 1 second,
fetching data from local SSD would be like waiting 2 days, while a
cross-country network request would take 6 years. Source: Adapted from
Jeff Dean's ``Numbers Every Programmer Should
Know''.}\label{tbl-ml-latencies}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Operation}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Latency (ns)}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Human Scale}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{ML System Impact}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Operation}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Latency (ns)}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Human Scale}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{ML System Impact}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{L1 Cache Reference} & 0.5 & 1 second & Immediate \\
\textbf{L2 Cache Reference} & 7 & 14 seconds & Fast computation \\
\textbf{Main Memory (DRAM)} & 100 & 3 minutes & The ``Memory Wall''
threshold \\
\textbf{SSD (local NVMe)} & 100,000 & 2 days & Data loading
bottleneck \\
\textbf{Network (same DC)} & 500,000 & 1 week & Distributed coordination
lag \\
\textbf{SSD (remote network)} & 2,000,000 & 1 month & Training-serving
skew source \\
\textbf{Object Store (S3)} & 20,000,000 & 1 year & Archival access \\
\textbf{Internet (CA to VA)} & 100,000,000 & 6 years & Global user
experience \\
\end{longtable}

Beyond the core storage capabilities we've examined, ML workloads
introduce unique requirements that conventional databases and warehouses
weren't designed to handle. Understanding these ML-specific needs and
their performance implications shapes infrastructure decisions that
cascade through the entire development lifecycle, from experimental
notebooks to production serving systems handling millions of requests
per second.

Modern ML models contain millions to billions of parameters requiring
efficient storage and retrieval patterns quite different from
traditional data. GPT-3 (\citeproc{ref-brown2020language}{Brown et al.
2020}) requires approximately 700 gigabytes for model weights when
stored in FP32 format (175 billion parameters times 4 bytes), though
practical deployments typically use FP16 (350 GB) or quantized formats
for reduced storage and faster inference. Even at FP16 precision, this
exceeds many organizations' entire operational databases. The trajectory
reveals accelerating scale: from AlexNet's 60 million parameters in 2012
(\citeproc{ref-krizhevsky2012imagenet}{Krizhevsky, Sutskever, and Hinton
2017}) to GPT-3's 175 billion parameters in 2020, model size grew
approximately 2,900-fold in eight years (60M to 175B parameters).
Storage systems must handle these dense numerical arrays efficiently for
both capacity and access speed. Unlike typical files where sequential
organization matters for readability, model weights benefit from
block-aligned storage enabling parallel reads across parameter groups.
When multiple GPUs need to read model data from shared storage, whether
during training initialization or checkpoint loading, storage systems
must deliver aggregate bandwidth approaching network interface limits,
often 25 Gbps or higher, without introducing bottlenecks that would idle
expensive compute resources.

The iterative nature of ML development introduces versioning
requirements qualitatively different from traditional software. While
Git excels at tracking code changes where files are predominantly text
with small incremental modifications, it fails for large binary files
where even small model changes result in entirely new checkpoints.
Storing 10 versions of a 10 GB model naively would consume 100 GB, but
most ML versioning systems store only deltas between versions, reducing
storage proportionally to how much models actually change. Tools like
DVC (Data Version Control) and MLflow maintain pointers to model
artifacts rather than storing copies, enabling efficient versioning
while preserving the ability to reproduce any historical model. A
typical ML project generates hundreds of model versions during
hyperparameter tuning---one version per training run as engineers
explore learning rates, batch sizes, architectures, and regularization
strategies. Without systematic versioning capturing training
configuration, accuracy metrics, and training data version alongside
model weights, reproducing results becomes impossible when yesterday's
model performed better than today's but teams cannot identify which
configuration produced it. This reproducibility challenge connects
directly to the governance requirements
Section~\ref{sec-data-engineering-ml-governance-observability-2c05}
examines where regulatory compliance often requires demonstrating
exactly which data and process produced specific model predictions.

Large-scale training generates substantial intermediate data requiring
storage systems to handle concurrent read/write operations efficiently.
When training jobs use multiple GPUs, each processing unit works on
different portions of data, requiring storage systems to handle many
simultaneous reads and writes. The specific patterns depend on the
parallelization strategy employed, which \textbf{?@sec-ai-training}
examines in detail. From a storage perspective, systems must handle
concurrent I/O at rates proportional to the number of processing units,
with each potentially writing tens to hundreds of megabytes of
intermediate results during model updates. Memory optimization
strategies that trade computation for storage space reduce memory
requirements but increase storage I/O as intermediate values write to
disk. Storage systems must provide low-latency access to support
efficient coordination. If workers spend more time waiting for storage
than performing computations, parallel processing becomes
counterproductive regardless of the specific training approach used.

The bandwidth hierarchy fundamentally constrains ML system design,
creating bottlenecks that no amount of compute optimization can
overcome. While RAM delivers 50 to 200 gigabytes per second bandwidth on
modern servers, network storage systems typically provide only one to 10
gigabytes per second, and even high-end NVMe SSDs max out at one to
seven gigabytes per second sequential throughput. Modern GPUs can
process data faster than storage can supply it, creating scenarios where
expensive accelerators idle waiting for data. Consider training an image
classification model: loading 1,000 images per second at 150 KB each
requires 150 MB/s sustained throughput from storage. When the GPU can
process images faster than storage delivers them, the data
pipeline---not the model---becomes the bottleneck. A 10-fold mismatch
between GPU processing speed and storage bandwidth means expensive
accelerators sit idle 90\% of the time waiting for data. No amount of
GPU optimization can overcome this fundamental I/O constraint.

Understanding these quantitative relationships enables informed
architectural decisions about storage system selection and data pipeline
optimization, which become even more critical during distributed
training as examined in \textbf{?@sec-ai-training}. The training
throughput equation reveals the critical dependencies:

\phantomsection\label{callout-notebook-1.4}
\begin{fbx}{callout-notebook}{AI Engineer’s Notebook 1.4: }{Engineering Calculation: The Storage Bandwidth Budget}
\phantomsection\label{callout-notebook-1.4}
\textbf{The Problem:} Design a storage system that ensures an NVIDIA
A100 is never starved for data while training ResNet-50.

\textbf{1. The Compute Term (Throughput Ceiling)} * \textbf{Hardware
Peak:} NVIDIA A100 = 312 TFLOPS (FP16 Tensor Core, dense
operations).\sidenote{\textbf{A100 Sparsity Support}: NVIDIA also
advertises 624 TFLOPS with structured sparsity enabled (2:4 pattern),
doubling effective throughput for models that can leverage sparse
computation. The 312 TFLOPS dense figure used here represents the
baseline for general workloads where sparsity cannot be assumed. } *
\textbf{Model Cost:} ResNet-50 = \textasciitilde4 GFLOPs per image
(forward + backward \(\approx\) 12 GFLOPs). * \textbf{Maximum
Theoretical Throughput}:
\[ \frac{312 \times 10^{12} \text{ FLOPs/sec}}{12 \times 10^9 \text{ FLOPs/img}} = \mathbf{26,000 \text{ images/sec}} \]

\textbf{2. The Data Term (Bandwidth Requirement)} * \textbf{Image Size:}
150 KB (JPEG compressed). * \textbf{Required Bandwidth}:
\[ 26,000 \text{ img/sec} \times 150 \text{ KB/img} \approx \mathbf{3.9 \text{ GB/s}} \]

\textbf{The Systems Conclusion:} To saturate a \emph{single} A100, your
storage must deliver \textbf{3.9 GB/s}. * \textbf{S3 Standard}:
\textasciitilde100 MB/s per thread. You need \textbf{40 concurrent
worker threads}. * \textbf{SATA SSD}: \textasciitilde500 MB/s. Totally
insufficient (bottleneck). * \textbf{NVMe SSD}: \textasciitilde3-7 GB/s.
\textbf{Required.}

\textbf{Iron Law Implication:} If you use SATA SSDs, your maximum
throughput is capped at
\(500 \text{ MB/s} / 150 \text{ KB} \approx 3,300 \text{ img/s}\). Your
\$15,000 GPU will run at \textbf{12\% utilization} (\(3,300/26,000\)).
Storage physics dictates training speed.

\end{fbx}

This calculation illustrates the general principle governing data
pipelines:

\[\text{Training Throughput} = \min(\text{Compute Capacity}, \text{Data Supply Rate})\]

\[\text{Data Supply Rate} = \text{Storage Bandwidth} \times (1 - \text{Overhead})\]

When storage bandwidth becomes the limiting factor, teams must either
improve storage performance through faster media, parallelization, or
caching, or reduce data movement requirements through compression,
quantization, or architectural changes. Large language model training
may require processing hundreds of gigabytes of text per hour, while
computer vision models processing high-resolution imagery can demand
sustained data rates exceeding 50 gigabytes per second across
distributed clusters. These requirements explain the rise of specialized
ML storage systems optimizing data loading pipelines: PyTorch DataLoader
with multiple worker processes parallelizing I/O, TensorFlow tf.data API
with prefetching and caching, and frameworks like NVIDIA DALI (Data
Loading Library) that offload data augmentation to GPUs rather than
loading pre-augmented data from storage.

File format selection dramatically impacts both throughput and latency
through effects on I/O volume and decompression overhead. Columnar
storage formats like Parquet or ORC deliver five to 10 times I/O
reduction compared to row-based formats like CSV or JSON for typical ML
workloads. The reduction comes from two mechanisms: reading only
required columns rather than entire records, and column-level
compression exploiting value patterns within columns. Consider a fraud
detection dataset with 100 columns where models typically use 20
features---columnar formats read only needed columns, achieving 80\% I/O
reduction before compression. Column compression proves particularly
effective for categorical features with limited cardinality: a country
code column with 200 unique values in 100 million records compresses 20
to 50 times through dictionary encoding, while run-length encoding
compresses sorted columns by storing only value changes. The combination
can achieve total I/O reduction of 20 to 100 times compared to
uncompressed row formats, directly translating to faster training
iterations and reduced infrastructure costs.

Compression algorithm selection involves trade-offs between compression
ratio and decompression speed. While gzip achieves higher compression
ratios of six to eight times, Snappy achieves only two to three times
compression but decompresses at 500 megabytes per second---roughly three
to four times faster than gzip's 120 megabytes per second. For ML
training where throughput matters more than storage costs, Snappy's
speed advantage often outweighs gzip's space savings. Training on a 100
gigabyte dataset compressed with gzip requires 17 minutes of
decompression time, while Snappy requires only five minutes. When
training iterates over data for 50 epochs, this 12-minute difference per
epoch compounds to 10 hours total---potentially the difference between
running experiments overnight versus waiting multiple days for results.
The choice cascades through the system: faster decompression enables
higher batch sizes (fitting more examples in memory after
decompression), reduced buffering requirements (less decompressed data
needs staging), and better GPU utilization (less time idle waiting for
data).

Storage performance optimization extends beyond format and compression
to data layout strategies. Data partitioning based on frequently used
query parameters dramatically improves retrieval efficiency. A
recommendation system processing user interactions might partition data
by date and user demographic attributes, enabling training on recent
data subsets or specific user segments without scanning the entire
dataset. Partitioning strategies interact with distributed training
patterns: range partitioning by user ID enables data parallel training
where each worker processes a consistent user subset, while random
partitioning ensures workers see diverse data distributions. The
partitioning granularity matters---too few partitions limit parallelism,
while too many partitions increase metadata overhead and reduce
efficiency of sequential reads within partitions.

\subsection{Storage Across the ML
Lifecycle}\label{sec-data-engineering-ml-storage-across-ml-lifecycle-2b22}

Storage requirements evolve substantially as ML systems progress from
initial development through production deployment and ongoing
maintenance. Understanding these changing requirements enables designing
infrastructure that supports the full lifecycle efficiently rather than
retrofitting storage later when systems scale or requirements change.
The same dataset might be accessed very differently during exploratory
analysis (random sampling for visualization), model training (sequential
scanning for epochs), and production serving (random access for
individual predictions), requiring storage architectures that
accommodate these diverse patterns.

During development, storage systems must support exploratory data
analysis and iterative model development where flexibility and
collaboration matter more than raw performance. Data scientists work
with various datasets simultaneously, experiment with feature
engineering approaches, and rapidly iterate on model designs to refine
approaches. The key challenge involves managing dataset versions without
overwhelming storage capacity. A naive approach copying entire datasets
for each experiment would exhaust storage quickly---10 experiments on a
100 gigabyte dataset would require one terabyte. Tools like DVC address
this by tracking dataset versions through pointers and storing only
deltas, enabling efficient experimentation. The system maintains lineage
from raw data through transformations to final training datasets,
supporting reproducibility when successful experiments need recreation
months later.

Collaboration during development requires balancing data accessibility
with security. Data scientists need efficient access to datasets for
experimentation, but organizations must simultaneously safeguard
sensitive information. Many teams implement tiered access controls where
synthetic or anonymized datasets are broadly available for
experimentation, while access to production data containing sensitive
information requires approval and audit trails. This balances
exploration velocity against governance requirements, enabling rapid
iteration on representative data without exposing sensitive information
unnecessarily.

Training phase requirements shift dramatically toward throughput
optimization. Modern deep learning training processes massive datasets
repeatedly across dozens or hundreds of epochs, making I/O efficiency
critical for acceptable iteration speed. High-performance storage
systems must provide throughput sufficient to feed data to multiple GPU
or TPU accelerators simultaneously without creating bottlenecks. When
training ResNet-50 on ImageNet's 1.2 million images across 8 GPUs, each
GPU processes approximately 4,000 images per epoch at 256 image batch
size. At 30 seconds per epoch, this requires loading 40,000 images per
second across all GPUs---approximately 500 megabytes per second of
decompressed image data. Storage systems unable to sustain this
throughput cause GPUs to idle waiting for data, directly reducing
training efficiency and increasing infrastructure costs.

The balance between preprocessing and on-the-fly computation becomes
critical during training. Extensive preprocessing reduces training-time
computation but increases storage requirements and risks staleness.
Feature extraction for computer vision might precompute ResNet features
from images, converting 150 kilobyte images to five kilobyte feature
vectors---achieving 30-fold storage reduction and eliminating repeated
computation. However, precomputed features become stale when feature
extraction logic changes, requiring recomputation across the entire
dataset. Production systems often implement hybrid approaches:
precomputing expensive, stable transformations like feature extraction
while computing rapidly-changing features on-the-fly during training.
This balances storage costs, computation time, and freshness based on
each feature's specific characteristics.

Deployment and serving requirements prioritize low-latency random access
over high-throughput sequential scanning. Real-time inference demands
storage solutions capable of retrieving model parameters and relevant
features within millisecond timescales. For a recommendation system
serving 10,000 requests per second with 10 millisecond latency budgets,
feature storage must support 100,000 random reads per second. In-memory
databases like Redis or sophisticated caching strategies become
essential for meeting these latency requirements. Edge deployment
scenarios introduce additional constraints: limited storage capacity on
embedded devices, intermittent connectivity to central data stores, and
the need for model updates without disrupting inference. Many edge
systems implement tiered storage where frequently-updated models cache
locally while infrequently-changing reference data pulls from cloud
storage periodically.

Model versioning becomes operationally critical during deployment.
Storage systems must support the deployment patterns examined in
\textbf{?@sec-machine-learning-operations-mlops}: smooth transitions
between model versions, rapid rollback capabilities, and efficient
serving of multiple versions simultaneously for shadow deployments and
A/B testing. These operational strategies impose specific storage
requirements---fast model loading, version-aware caching, and atomic
version switching---that influence architecture decisions.

\subsection{Data Versioning for ML
Reproducibility}\label{sec-data-engineering-ml-data-versioning-ml-reproducibility-16d0}

Data versioning connects model versions to exact training data, enabling
debugging and reproducibility. Without data versioning, teams cannot
answer essential questions like ``what exact data trained model v47?''

\textbf{DVC Workflow} provides Git-like semantics for data versioning:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Add data to version control}
\ExtensionTok{dvc}\NormalTok{ add data/training.csv}
\FunctionTok{git}\NormalTok{ add data/training.csv.dvc}
\FunctionTok{git}\NormalTok{ commit }\AttributeTok{{-}m} \StringTok{"Add training data v1"}
\ExtensionTok{dvc}\NormalTok{ push  }\CommentTok{\# Upload to remote storage}

\CommentTok{\# Later: retrieve exact data for any historical commit}
\FunctionTok{git}\NormalTok{ checkout abc123}
\ExtensionTok{dvc}\NormalTok{ checkout  }\CommentTok{\# Restores exact data from that commit}
\end{Highlighting}
\end{Shaded}

\textbf{Delta Lake Time Travel} enables querying historical data states
directly in SQL:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{{-}{-} Query data as it existed on a specific date}
\KeywordTok{SELECT} \OperatorTok{*} \KeywordTok{FROM}\NormalTok{ training\_data VERSION }\KeywordTok{AS} \KeywordTok{OF} \StringTok{\textquotesingle{}2024{-}01{-}15\textquotesingle{}}

\CommentTok{{-}{-} Or by version number for programmatic access}
\KeywordTok{SELECT} \OperatorTok{*} \KeywordTok{FROM}\NormalTok{ training\_data VERSION }\KeywordTok{AS} \KeywordTok{OF} \DecValTok{47}
\end{Highlighting}
\end{Shaded}

\textbf{Feature Store Point-in-Time Retrieval} maintains historical
feature values, enabling training with features ``as they existed'' at
prediction time. This prevents label leakage where training
inadvertently uses feature values computed after the prediction
timestamp.

\textbf{Model Registry Integration} links each model registry entry to
its complete provenance: Git commit hash (code), data version (DVC
commit or Delta version), feature store snapshot timestamp, and training
configuration file. This complete lineage enables rapid debugging when
production issues arise.

A common failure mode illustrates the importance: Model v47 performed
3\% worse than v46 with identical code. Without data versioning, the
team spent two weeks investigating the accuracy drop. With proper
versioning, they would have immediately seen that the data version was
inadvertently updated mid-experiment and identified the root cause
within hours.

Monitoring and maintenance phases introduce long-term storage
considerations centered on debugging, compliance, and system
improvement. Capturing incoming data alongside prediction results
enables ongoing analysis detecting data drift, identifying model
failures, and maintaining regulatory compliance. For edge and mobile
deployments, storage constraints complicate data collection---systems
must balance gathering sufficient data for drift detection against
limited device storage and network bandwidth for uploading to central
analysis systems. Regulated industries often require immutable storage
supporting auditing: healthcare ML systems must retain not just
predictions but complete data provenance showing which training data and
model version produced each diagnostic recommendation, potentially for
years or decades.

Log and monitoring data volumes grow substantially in high-traffic
production systems. A recommendation system serving 10 million users
might generate terabytes of interaction logs daily. Storage strategies
typically implement tiered retention: hot storage retains recent data
(past week) for rapid analysis, warm storage keeps medium-term data
(past quarter) for periodic analysis, and cold archive storage retains
long-term data (past years) for compliance and rare deep analysis. The
transitions between tiers involve trade-offs between access latency,
storage costs, and retrieval complexity that systems must manage
automatically as data ages.

The storage architectures we have examined address where data resides
and how it is retrieved, but a critical challenge remains: ensuring that
features computed during training match exactly those computed during
serving. This consistency requirement, which we emphasized throughout
the processing section, demands specialized infrastructure that bridges
the gap between batch training environments and real-time serving
systems. Feature stores have emerged as the architectural solution to
this challenge.

\subsection{Feature Stores: Bridging Training and
Serving}\label{sec-data-engineering-ml-feature-stores-bridging-training-serving-55c8}

Feature stores have emerged as critical infrastructure components
addressing the unique challenge of maintaining consistency between
training and serving environments while enabling feature reuse across
models and teams. Traditional ML architectures often compute features
differently offline during training versus online during serving,
creating training-serving skew that silently degrades model performance.

\phantomsection\label{callout-definitionux2a-1.11}
\begin{fbx}{callout-definition}{Definition: }{Feature Store}
\phantomsection\label{callout-definition*-1.11}
\textbf{Feature Store} refers to a centralized data management system
that standardizes the \emph{definition}, \emph{storage}, and
\emph{access} of machine learning features. It ensures
\textbf{training-serving consistency} by serving historical feature
values for training (batch) and current values for inference (online)
from a unified source of truth.

\end{fbx}

The fundamental problem feature stores address becomes clear when
examining typical ML development workflows. During model development,
data scientists write feature engineering logic in notebooks or scripts,
often using different libraries and languages than production serving
systems. Training might compute a user's ``total purchases last 30
days'' using SQL aggregating historical data, while serving computes the
same feature using a microservice that incrementally updates cached
values. These implementations should produce identical results, but
subtle differences---handling timezone conversions, dealing with missing
data, or rounding numerical values---cause training and serving features
to diverge. A study of production ML systems found that 30\% to 40\% of
initial deployments at Uber suffered from training-serving skew,
motivating development of their Michelangelo platform with integrated
feature stores.

Feature stores provide a single source of truth for feature definitions,
ensuring consistency across all stages of the ML lifecycle. When data
scientists define a feature like ``user\_purchase\_count\_30d'', the
feature store maintains both the definition (SQL query, transformation
logic, or computation graph) and executes it consistently whether
providing historical feature values for training or real-time values for
serving. This architectural pattern eliminates an entire class of subtle
bugs that prove notoriously difficult to debug because models train
successfully but perform poorly in production without obvious errors.

Beyond consistency, feature stores enable feature reuse across models
and teams, significantly reducing redundant work. When multiple teams
build models requiring similar features---customer lifetime value for
churn prediction and upsell models, user demographic features for
recommendations and personalization, product attributes for search
ranking and related item suggestions---the feature store prevents each
team from reimplementing identical features with subtle variations.
Centralized feature computation reduces both development time and
infrastructure costs while improving consistency across models. A
recommendation system might compute user embedding vectors representing
preferences across hundreds of dimensions---expensive computation
requiring aggregating months of interaction history. Rather than each
model team recomputing embeddings, the feature store computes them once
and serves them to all consumers.

The architectural pattern typically implements dual storage modes
optimized for different access patterns. The offline store uses columnar
formats like Parquet on object storage, optimized for batch access
during training where sequential scanning of millions of examples is
common. The online store uses key-value systems like Redis, optimized
for random access during serving where individual feature vectors must
be retrieved in milliseconds. Synchronization between stores becomes
critical: as training generates new models using current feature values,
those models deploy to production expecting the online store to serve
consistent features. Feature stores typically implement scheduled batch
updates propagating new feature values from offline to online stores,
with update frequencies depending on feature freshness requirements.

Time-travel capabilities distinguish sophisticated feature stores from
simple caching layers. Training requires accessing feature values as
they existed at specific points in time, not just current values.
Consider training a churn prediction model: for users who churned on
January 15th, the model should use features computed on January 14th,
not current features reflecting their churned status. Point-in-time
correctness ensures training data matches production conditions where
predictions use currently-available features to forecast future
outcomes. Implementing time-travel requires storing feature history, not
just current values, substantially increasing storage requirements but
enabling correct training on historical data.

Feature store performance characteristics directly impact both training
throughput and serving latency. For training, the offline store must
support high-throughput batch reads, typically loading millions of
feature vectors per minute when training begins epochs. Columnar storage
formats enable efficient reads of specific features from wide feature
tables containing hundreds of potential columns. For serving, the online
store must support thousands to millions of reads per second with
single-digit millisecond latency. This dual-mode optimization reflects
fundamentally different access patterns: training performs large
sequential scans while serving performs small random lookups, requiring
different storage technologies optimized for each pattern.

Production deployments face additional challenges around feature
freshness and cost management. Real-time features requiring immediate
updates create pressure on online store capacity and synchronization
logic. When users add items to shopping carts, recommendation systems
want updated features reflecting current cart contents within seconds,
not hours. Streaming feature computation pipelines process events in
real-time, updating online stores continuously rather than through
periodic batch jobs. However, streaming introduces complexity around
exactly-once processing semantics, handling late-arriving events, and
managing computation costs for features updated millions of times per
second.

Cost management for feature stores becomes significant at scale. Storing
comprehensive feature history for time-travel capabilities multiplies
storage requirements: retaining daily feature snapshots for one year
requires 365 times the storage of keeping only current values.
Production systems implement retention policies balancing point-in-time
correctness against storage costs, perhaps retaining daily snapshots for
one year, weekly snapshots for five years, and purging older history
unless required for compliance. Online store costs grow with both
feature dimensions and entity counts: storing 512-dimensional embedding
vectors for 100 million users requires approximately 200 gigabytes at
single-precision (32-bit floats), often replicated across regions for
availability and low-latency access, multiplying costs substantially.

Feature store migration represents a significant undertaking for
organizations with existing ML infrastructure. Legacy systems compute
features ad-hoc across numerous repositories and pipelines, making
centralization challenging. Successful migrations typically proceed
incrementally: starting with new features in the feature store while
gradually migrating high-value legacy features, prioritizing those used
across multiple models or causing known training-serving skew issues.
Maintaining abstraction layers that enable application-agnostic feature
access prevents tight coupling to specific feature store
implementations, facilitating future migrations when requirements evolve
or better technologies emerge.

Modern feature store implementations include open-source projects like
Feast and Tecton, commercial offerings from Databricks Feature Store and
AWS SageMaker Feature Store, and custom-built solutions at major
technology companies. Each makes different trade-offs between feature
types supported (structured vs.~unstructured), supported infrastructure
(cloud-native vs.~on-premise), and integration with ML frameworks. The
convergence toward feature stores as essential ML infrastructure
reflects recognition that feature engineering represents a substantial
portion of ML development effort, and systematic infrastructure
supporting features provides compounding benefits across an
organization's entire ML portfolio.

\subsection{Case Study: Storage Architecture for KWS
Systems}\label{sec-data-engineering-ml-case-study-storage-architecture-kws-systems-3385}

Completing our comprehensive KWS case study---having traced the system
from initial problem definition through data collection strategies,
pipeline architectures, processing transformations, and labeling
approaches---we now examine how storage architecture supports this
entire data engineering lifecycle. The storage decisions made here
directly reflect and enable choices made in earlier stages. Our
crowdsourcing strategy established in
Section~\ref{sec-data-engineering-ml-framework-application-keyword-spotting-case-study-c8e9}
determines raw audio volume and diversity requirements. Our processing
pipeline designed in
Section~\ref{sec-data-engineering-ml-systematic-data-processing-aebc}
defines what intermediate features must be stored and retrieved
efficiently. Our quality metrics from
Section~\ref{sec-data-engineering-ml-ensuring-trainingserving-consistency-c683}
shape metadata storage needs for tracking data provenance and quality
scores. Storage architecture weaves these threads together, enabling the
system to function cohesively from development through production
deployment.

A typical KWS storage architecture implements the tiered approach
discussed earlier in this section, with each tier serving distinct
purposes that emerged from our earlier engineering decisions. Raw audio
files from various sources---crowd-sourced recordings collected through
the campaigns we designed, synthetic data generated to fill coverage
gaps, and real-world captures from deployed devices---reside in a data
lake using cloud object storage services like S3 or Google Cloud
Storage. This choice reflects our scalability pillar: audio files
accumulate to hundreds of gigabytes or terabytes as we collect the
millions of diverse examples needed for 98\% accuracy across
environments. The flexible schema of data lakes accommodates different
sampling rates, audio formats, and recording conditions without forcing
rigid structure on heterogeneous sources. Low cost per gigabyte that
object storage provides---typically one-tenth the cost of database
storage---enables retaining comprehensive data history for model
improvement and debugging without prohibitive expense.

The data lake stores comprehensive provenance metadata required by our
governance pillar, metadata that proved essential during earlier
pipeline stages. For each audio file, the system maintains source type
(crowdsourced, synthetic, or real-world), collection date, demographic
information when ethically collected and consented to, quality
assessment scores computed by our validation pipeline, and processing
history showing which transformations have been applied. This metadata
enables filtering during training data selection and supports compliance
requirements for privacy regulations and ethical AI practices
Section~\ref{sec-data-engineering-ml-governance-observability-2c05}
examines.

Processed features---spectrograms, MFCCs, and other ML-ready
representations computed by our processing pipeline---move into a
structured data warehouse optimized for training access. This addresses
different performance requirements from raw storage: while raw audio is
accessed infrequently (primarily during processing pipeline execution
when we transform new data), processed features are read repeatedly
during training epochs as models iterate over the dataset dozens of
times. The warehouse uses columnar formats like Parquet, enabling
efficient loading of specific features during training. For a dataset of
23 million examples like MSWC, columnar storage reduces training I/O by
five to 10 times compared to row-based formats, directly impacting
iteration speed during model development---the difference between
training taking hours versus days.

KWS systems benefit significantly from feature stores implementing the
architecture patterns we've examined. Commonly used audio
representations can be computed once and stored for reuse across
different experiments or model versions, avoiding redundant computation.
The feature store implements a dual architecture: an offline store using
Parquet on object storage for training data, providing high throughput
for sequential reads when training loads millions of examples, and an
online store using Redis for low-latency inference, supporting our 200
millisecond latency requirement established during problem definition.
This dual architecture addresses the fundamental tension between
training's batch access patterns---reading millions of examples
sequentially---and serving's random access patterns---retrieving
features for individual audio snippets in real-time as users speak wake
words.

In production, edge storage requirements become critical as our system
deploys to resource-constrained devices. Models must be compact enough
for devices with our 16 kilobyte memory constraint from the problem
definition while maintaining quick parameter access for real-time wake
word detection. Edge devices typically store quantized models using
specialized formats like TensorFlow Lite's FlatBuffers, which enable
memory-mapped access without deserialization overhead that would violate
latency requirements. Caching applies at multiple levels: frequently
accessed model layers reside in SRAM for fastest access, the full model
sits in flash storage for persistence across power cycles, and
cloud-based model updates are fetched periodically to maintain current
wake word detection patterns. This multi-tier caching ensures devices
operate effectively even with intermittent network connectivity---a
reliability requirement for consumer devices deployed in varied network
environments from rural areas with limited connectivity to urban
settings with congested networks.

\section{Data
Governance}\label{sec-data-engineering-ml-data-governance-eade}

The storage architectures we've examined---data lakes, warehouses,
feature stores---are not merely technical infrastructure but governance
enforcement mechanisms that determine who accesses data, how usage is
tracked, and whether systems comply with regulatory requirements. Every
architectural decision we've made throughout this chapter, from
acquisition strategies through processing pipelines to storage design,
carries governance implications that manifest most clearly when systems
face regulatory audits, privacy violations, or ethical challenges. Data
governance transforms from abstract policy into concrete engineering:
access control systems that enforce who can read training data, audit
infrastructure that tracks every data access for compliance,
privacy-preserving techniques that protect individuals while enabling
model training, and lineage systems that document how raw audio
recordings become production models.

Our KWS system exemplifies governance challenges that arise when
sophisticated storage meets sensitive data. The always-listening
architecture that enables convenient voice activation creates profound
privacy concerns: devices continuously process audio in users' homes,
feature stores maintain voice pattern histories across millions of
users, and edge storage caches acoustic models derived from
population-wide training data. These technical capabilities that enable
our quality, reliability, and scalability requirements simultaneously
create governance obligations around consent management, data
minimization, access auditing, and deletion rights that require equally
sophisticated engineering solutions.
Figure~\ref{fig-data-governance-pillars} maps these interconnected
challenges, revealing how effective governance requires systematic
implementation of privacy protection, security controls, compliance
mechanisms, and accountability infrastructure throughout the ML
lifecycle.

\begin{figure}[htb]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/27d20c23f91146db5c29cdf3428f274c5c4e375a.pdf}}

}

\caption{\label{fig-data-governance-pillars}\textbf{Data Governance
Pillars}: Robust data governance establishes ethical and reliable
machine learning systems by prioritizing privacy, fairness,
transparency, and accountability throughout the data lifecycle. These
interconnected pillars address unique challenges in ML workflows,
ensuring responsible data usage and auditable decision-making
processes.}

\end{figure}%

\subsection{Security and Access Control
Architecture}\label{sec-data-engineering-ml-security-access-control-architecture-c098}

Production ML systems implement layered security architectures where
governance requirements translate into enforceable technical controls at
each pipeline stage. Modern feature stores exemplify this integration by
implementing role-based access control (RBAC) that maps organizational
policies---data scientists can read training features, serving systems
can read online features, but neither can modify raw source data---into
database permissions that prevent unauthorized access. These access
control systems operate across the storage tiers we examined: object
storage like S3 enforces bucket policies that determine which services
can read training data, data warehouses implement column-level security
that hides sensitive fields like user identifiers from most queries, and
feature stores maintain separate read/write paths with different
permission requirements.

Our KWS system requires particularly sophisticated access controls
because voice data flows across organizational and device boundaries.
Edge devices store quantized models and cached audio features locally,
requiring encryption to prevent extraction if devices are
compromised---a voice assistant's model parameters, though individually
non-sensitive, could enable competitive reverse-engineering or reveal
training data characteristics. The feature store maintains separate
security zones: a production zone where serving systems retrieve
real-time features using service credentials with read-only access, a
training zone where data scientists access historical features using
individual credentials tracked for audit purposes, and an operations
zone where SRE teams can access pipeline health metrics without viewing
actual voice data. This architectural separation, implemented through
Kubernetes namespaces with separate IAM roles in cloud deployments,
ensures that compromising one component---say, a serving system
vulnerability---doesn't expose training data or grant write access to
production features.

Access control systems integrate with encryption throughout the data
lifecycle. Training data stored in data lakes uses server-side
encryption with keys managed through dedicated key management services
(AWS KMS, Google Cloud KMS) that enforce separation: training job
credentials can decrypt current training data but not historical
versions already used, implementing data minimization by limiting access
scope. Feature stores implement encryption both at rest---storage
encrypted using platform-managed keys---and in transit---TLS 1.3 for all
communication between pipeline components and feature stores. For KWS
edge devices, model updates transmitted from cloud training systems to
millions of distributed devices require end-to-end encryption and code
signing that verifies model integrity, preventing adversarial model
injection that could compromise device security or user privacy.

\subsection{Technical Privacy Protection
Methods}\label{sec-data-engineering-ml-technical-privacy-protection-methods-7789}

While access controls determine who can use data, privacy-preserving
techniques determine what information systems expose even to authorized
users. Differential privacy provides formal mathematical guarantees that
individual training examples do not leak through model behavior.
Implementing differential privacy in production requires careful
engineering: adding calibrated noise during model development, tracking
privacy budgets across all data uses (each query or training run
consumes budget, enforcing system-wide limits on total privacy loss),
and validating that deployed models satisfy privacy guarantees through
testing infrastructure that attempts to extract training data through
membership inference attacks.

KWS systems face particularly acute privacy challenges because the
always-listening architecture requires processing audio continuously
while minimizing data retention and exposure. Production systems
implement privacy through architectural choices: on-device processing
where wake word detection runs entirely locally using models stored in
edge flash memory, with audio never transmitted unless the wake word is
detected; federated learning approaches where devices train on local
audio to improve wake word detection but only share aggregated model
updates, never raw audio, back to central servers; and automatic
deletion policies where detected wake word audio is retained only
briefly for quality monitoring before being permanently removed from
storage. These aren't just policy statements but engineering
requirements that manifest in storage system design---data lakes
implement lifecycle policies that automatically delete voice samples
after 30 days unless explicitly tagged for long-term research use with
additional consent, and feature stores implement time-to-live (TTL)
fields that cause user voice patterns to expire and be purged from
online serving stores.

The implementation complexity extends to handling deletion requests
required by GDPR and similar regulations. When users invoke their
``right to be forgotten,'' systems must locate and remove not just
source audio recordings but also derived features stored in feature
stores, model embeddings that might encode voice characteristics, and
audit logs that reference the user, all while preserving audit integrity
for compliance. This requires sophisticated data lineage tracking that
we examine next, enabling systems to identify all data artifacts derived
from a user's voice samples across distributed storage tiers and
pipeline stages. The data governance foundations we establish here
connect to the broader responsible engineering framework in
\textbf{?@sec-responsible-engineering}, which examines fairness,
accountability, and ethical considerations that extend beyond data
infrastructure into model development and deployment practices.

\subsection{Architecting for Regulatory
Compliance}\label{sec-data-engineering-ml-architecting-regulatory-compliance-95c7}

Compliance requirements transform from legal obligations into system
architecture constraints that shape pipeline design, storage choices,
and operational procedures. GDPR's data minimization principle requires
limiting collection and retention to what's necessary for stated
purposes---for KWS systems, this means justifying why voice samples need
retention beyond training, documenting retention periods in system
design documents, and implementing automated deletion once periods
expire. The ``right to access'' requires systems to retrieve all data
associated with a user---in practice, querying distributed storage
systems (data lakes, warehouses, feature stores) and consolidating
results, a capability that necessitates consistent user identifiers
across all storage tiers and indexes that enable efficient user-level
queries rather than full table scans.

Voice assistants operating globally face particularly complex compliance
landscapes because regulatory requirements vary by jurisdiction and
apply differently based on user age, data sensitivity, and processing
location. California's CCPA grants deletion rights similar to GDPR but
with different timelines and exceptions. Children's voice data triggers
COPPA requirements in the United States, requiring verifiable parental
consent before collecting data from users under 13---a technical
challenge when voice characteristics don't reliably reveal age,
requiring supplementary authentication mechanisms. European requirements
for cross-border data transfer restrict storing EU users' voice data on
servers outside designated countries unless specific safeguards exist,
driving architectural decisions about regional data lakes, feature store
replication strategies, and processing localization.

Standardized documentation frameworks like data cards
(\citeproc{ref-pushkarna2022data}{Pushkarna, Zaldivar, and Kjartansson
2022}) translate these compliance requirements into operational
artifacts. Figure~\ref{fig-data-card} demonstrates the structured format
that makes compliance executable rather than merely aspirational. Rather
than legal documents maintained separately from systems, data cards
become executable specifications: training pipelines check that input
datasets have valid data cards before processing, model registries
require data card references for all training data, and serving systems
enforce that only models trained on compliant data can deploy to
production. For our KWS training pipeline, data cards document not just
the MSWC dataset characteristics but also consent basis (research use,
commercial deployment), geographic restrictions (can train global
models, cannot train region-specific models without additional consent),
and retention commitments (audio deleted after feature extraction,
features retained for model iteration).

\begin{figure}[t!]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/e95cc89e0036255dac046ac71bec290399d01667.pdf}}

}

\caption{\label{fig-data-card}\textbf{Data Governance Documentation}:
Data cards standardize critical dataset information, enabling
transparency and accountability required for regulatory compliance with
laws like GDPR and HIPAA. By providing a structured overview of dataset
characteristics, intended uses, and potential risks, data cards
facilitate responsible AI practices and support data subject rights.}

\end{figure}%

\subsection{Building Data Lineage
Infrastructure}\label{sec-data-engineering-ml-building-data-lineage-infrastructure-e2b9}

Data lineage transforms from compliance documentation into operational
infrastructure that powers governance capabilities across the ML
lifecycle. Modern lineage systems like Apache Atlas and
DataHub\sidenote{\textbf{Data Lineage Systems}: Apache Atlas
(Hortonworks, now Apache, 2015) and DataHub (LinkedIn, 2020) enable
lineage tracking at enterprise scale. These systems capture metadata
about data flows automatically from pipeline execution logs, creating
graphs where nodes represent datasets (tables, files, feature
collections) and edges represent transformations (SQL queries, Python
scripts, model training jobs). GDPR Article 30 requires detailed records
of data processing activities, making automated lineage tracking
essential for demonstrating compliance during regulatory audits. }
integrate with pipeline orchestrators (Airflow, Kubeflow) to
automatically capture relationships: when an Airflow DAG reads audio
files from S3, transforms them into spectrograms, and writes features to
a warehouse, the lineage system records each step, creating a graph that
traces any feature back to its source audio file and forward to all
models trained using it. This automated tracking proves essential for
deletion requests---when a user invokes GDPR rights, the lineage graph
identifies all derived artifacts (extracted features, computed
embeddings, trained model versions) that must be removed or retrained.

Production KWS systems implement lineage tracking across all stages
we've examined in this chapter. Source audio ingestion creates lineage
records linking each audio file to its acquisition method (crowdsourced
platform, web scraping source, synthetic generation parameters),
enabling verification of consent requirements. Processing pipeline
execution extends lineage graphs as audio becomes MFCC features,
spectrograms, and embeddings---each transformation adds nodes that
record not just output artifacts but also code versions,
hyperparameters, and execution timestamps. Training jobs create lineage
edges from feature collections to model artifacts, recording which data
versions trained which model versions. When a voice assistant device
downloads a model update, lineage tracking records the deployment,
enabling recall if training data is later discovered to have quality or
compliance issues.

The operational value extends beyond compliance to debugging and
reproducibility. When KWS accuracy degrades for a specific accent,
lineage systems enable tracing affected predictions back through
deployed models to training features, identifying that the training data
lacked sufficient representation of that accent. When research teams
want to reproduce an experiment from six months ago, lineage graphs
capture exact data versions, code commits, and hyperparameters that
produced those results. Feature stores integrate lineage natively: each
feature includes metadata about the source data, transformation logic,
and computation time, enabling queries like ``which models depend on
user location data'' to guide impact analysis when data sources change.

\subsection{Audit Infrastructure and
Accountability}\label{sec-data-engineering-ml-audit-infrastructure-accountability-9b12}

While lineage tracks what data exists and how it transforms, audit
systems record who accessed data and when, creating accountability
trails required by regulations like HIPAA and SOX\sidenote{\textbf{ML
Audit Requirements}: SOX compliance requires immutable audit logs for
financial ML models, while HIPAA mandates detailed access logs for
healthcare AI systems. Modern ML platforms generate massive audit
volumes---Uber's Michelangelo platform logs over 50 billion events daily
for compliance, debugging, and performance monitoring. Audit log
retention periods vary by regulation: HIPAA requires six years, GDPR's
Article 30 doesn't specify duration but implies logs must cover data
subject access requests, and SOX requires seven years for financial
data. }. Production ML systems generate enormous audit volumes---every
training data access, feature store query, and model prediction can
generate audit events, quickly accumulating to billions of events daily
for large-scale systems. This scale necessitates specialized
infrastructure: immutable append-only storage (often using cloud-native
services like AWS CloudTrail or Google Cloud Audit Logs) that prevents
tampering with historical records, efficient indexing (typically
Elasticsearch or similar systems) that enables querying specific user or
dataset accesses without full scans, and automated analysis that detects
anomalous patterns indicating potential security breaches or policy
violations.

KWS systems implement multi-tier audit architectures that balance
granularity against performance and cost. Edge devices log critical
events locally---wake word detections, model updates, privacy setting
changes---with logs periodically uploaded to centralized storage for
compliance retention. Feature stores log every query with request
metadata: which service requested features, which user IDs were
accessed, and what features were retrieved, enabling analysis like ``who
accessed this specific user's voice patterns'' for security
investigations. Training infrastructure logs dataset access, recording
which jobs read which data partitions and when, implementing the
accountability needed to demonstrate that deleted user data no longer
appears in new model versions.

The integration of lineage and audit systems creates comprehensive
governance observability. When regulators audit a voice assistant
provider, the combination of lineage graphs showing how user audio
becomes models and audit logs proving who accessed that audio provides
the transparency needed to demonstrate compliance. When security teams
investigate suspected data exfiltration, audit logs identify suspicious
access patterns while lineage graphs reveal what data the compromised
credentials could reach. When ML teams debug model quality issues,
lineage traces problems to specific training data while audit logs
confirm no unauthorized modifications occurred. This operational
governance infrastructure, built systematically throughout the data
engineering practices we've examined in this chapter, transforms
abstract compliance requirements into enforceable technical controls
that maintain trust as ML systems scale in complexity and impact.

As ML systems become increasingly embedded in high-stakes applications
(healthcare diagnosis, financial decisions, autonomous vehicles), the
engineering rigor applied to governance infrastructure will determine
not just regulatory compliance but public trust and system
accountability. Emerging approaches like blockchain-inspired
tamper-evident logs\sidenote{\textbf{Blockchain for ML Governance}:
Immutable distributed ledgers provide tamper-proof audit trails for ML
model decisions and data provenance. Ocean Protocol (2017) and similar
projects use blockchain to track data usage rights and provide
transparent data marketplaces. While promising for high-stakes
applications like healthcare AI where audit integrity is paramount,
blockchain's energy costs (proof-of-work consensus), throughput
limitations (thousands versus millions of transactions per second), and
complexity limit widespread ML adoption. Most production systems use
centralized append-only logging with cryptographic integrity checks as a
pragmatic middle ground. } and automated policy enforcement through
infrastructure-as-code promise to make governance controls more reliable
and auditable, though they introduce their own complexity and cost
trade-offs that organizations must carefully evaluate against their
specific requirements.

Even with robust governance infrastructure in place, ML systems
accumulate hidden liabilities over time---not just governance gaps, but
shortcuts and compromises across every stage we have examined in this
chapter. The undocumented datasets from hasty acquisition, the fragile
schema workarounds in our pipelines, the uncorrected labeling errors we
flagged but deferred, the models trained on data that has drifted from
current distributions: each represents a form of debt that compounds
silently. The lineage tracking, audit systems, and access controls we
have examined can prevent many issues, but organizations must also
recognize and address the gradual accumulation of compromises that
compound until remediation costs exceed system value.

\section{Data Debt: The Hidden
Liability}\label{sec-data-engineering-ml-data-debt-hidden-liability-3335}

Technical debt is well-understood in software engineering: shortcuts
taken today create maintenance burdens tomorrow. Data debt is its
counterpart in ML systems---accumulated compromises in data quality,
documentation, and infrastructure that compound over time, degrading
model performance and increasing operational costs. The categories of
data debt map directly to our Four Pillars: documentation debt
undermines governance, schema debt compromises reliability, quality debt
degrades model accuracy, and freshness debt erodes the training-serving
consistency we established as a mathematical requirement. Unlike
technical debt in code, data debt often remains invisible until
catastrophic failures occur, making it particularly insidious.

\phantomsection\label{callout-definitionux2a-1.12}
\begin{fbx}{callout-definition}{Definition: }{Data Debt}
\phantomsection\label{callout-definition*-1.12}
\textbf{Data Debt} refers to the accumulated cost of deferred data
quality improvements, missing documentation, schema inconsistencies, and
infrastructure shortcuts that compound over time to degrade ML system
performance, increase maintenance burden, and raise the cost of future
development.

\end{fbx}

\subsection{Categories of Data
Debt}\label{sec-data-engineering-ml-categories-data-debt-ee90}

Data debt manifests across four distinct categories, each requiring
different detection and remediation strategies.

\textbf{Documentation Debt} accumulates when data provenance, meaning,
and quality characteristics go unrecorded. Datasets without data cards
(Figure~\ref{fig-data-card}), unlabeled columns, and undocumented
transformations create debt that compounds when original authors leave
organizations. A survey of production ML systems found that 40\% of data
quality incidents traced to misunderstanding data semantics due to
missing documentation (\citeproc{ref-sambasivan2021everyone}{Sambasivan
et al. 2021}). Documentation debt manifests as: unknown column meanings
requiring reverse-engineering, missing provenance preventing compliance
audits, undocumented assumptions causing silent failures when
assumptions change, and absent quality metrics preventing informed
dataset selection.

\textbf{Schema Debt} emerges from accumulated schema workarounds and
migrations. When upstream systems change data formats, quick fixes
(string parsing instead of proper type handling, NULL coercion instead
of error handling) accumulate into fragile transformation logic. Schema
debt indicators include: multiple date format handlers for the same
logical field, defensive null checks scattered throughout pipeline code,
version-specific parsing branches that grow with each upstream change,
and undocumented enum value mappings that break when new values appear.

\textbf{Quality Debt} consists of known data errors that remain
uncorrected due to time or resource constraints. A dataset with 3\%
known label errors represents quality debt---each training run on this
data produces models carrying those errors. Quality debt compounds:
models trained on erroneous data make predictions that become training
data for downstream systems, amplifying the original errors. Quality
debt metrics include: known label error rates not yet corrected,
documented bias not yet mitigated, identified duplicates not yet
deduplicated, and detected drift not yet addressed through retraining.

\textbf{Freshness Debt} arises when training data diverges from
production distributions over time. A model trained on 2023 user
behavior deployed in 2025 carries freshness debt---the distribution
shift between training and serving degrades performance continuously.
Freshness debt is particularly dangerous because it accumulates
silently: unlike code that breaks obviously, stale models degrade
gradually until performance drops below acceptable thresholds. Freshness
debt indicators include: time since last retraining relative to
distribution shift rate, feature staleness (cached features not updated
at required frequency), and reference data lag (lookup tables not
reflecting current state).

\subsection{Measuring Data
Debt}\label{sec-data-engineering-ml-measuring-data-debt-973c}

Unlike technical debt, which can be assessed through code complexity
metrics, data debt requires specialized measurement approaches.

Table~\ref{tbl-data-debt-metrics} provides quantitative indicators for
each debt category:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1818}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3182}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2182}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2636}}@{}}
\caption{\textbf{Data Debt Metrics}: Quantitative thresholds for
detecting data debt accumulation. Warning thresholds indicate debt
requiring attention in upcoming planning cycles; critical thresholds
indicate debt requiring immediate remediation to prevent system
degradation.}\label{tbl-data-debt-metrics}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Debt Category}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Metric}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Warning Threshold}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Critical Threshold}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Debt Category}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Metric}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Warning Threshold}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Critical Threshold}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Documentation} & \% datasets with data cards & \textless{} 80\%
& \textless{} 50\% \\
\textbf{Documentation} & \% columns with descriptions & \textless{} 90\%
& \textless{} 70\% \\
\textbf{Schema} & Schema version branches & \textgreater{} 3 &
\textgreater{} 10 \\
\textbf{Schema} & \% transformations with try/catch & \textgreater{}
20\% & \textgreater{} 50\% \\
\textbf{Quality} & Known label error rate & \textgreater{} 1\% &
\textgreater{} 5\% \\
\textbf{Quality} & Documented bias metrics & Not measured & Measured but
not mitigated \\
\textbf{Freshness} & Days since last retraining & \textgreater{} 90 days
& \textgreater{} 365 days \\
\textbf{Freshness} & PSI vs training baseline & \textgreater{} 0.1 &
\textgreater{} 0.25 \\
\end{longtable}

\subsection{The Compound Interest of Data
Debt}\label{sec-data-engineering-ml-compound-interest-data-debt-e5d0}

Data debt compounds through feedback loops unique to ML systems:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Training Amplification}: Models trained on erroneous data
  learn incorrect patterns. When these models generate predictions used
  as features or pseudo-labels, errors propagate to downstream systems.
\item
  \textbf{Documentation Decay}: As original authors leave and systems
  evolve, undocumented datasets become increasingly opaque. Each year
  without documentation makes future documentation exponentially harder.
\item
  \textbf{Schema Entropy}: Quick fixes to handle schema changes create
  brittle code. Each additional workaround increases the probability of
  the next schema change causing failures.
\item
  \textbf{Distribution Drift}: Without continuous monitoring and
  retraining, the gap between training and serving distributions widens,
  causing accuracy degradation that accelerates as models become less
  calibrated.
\end{enumerate}

The compound nature means that data debt left unaddressed for \emph{n}
periods grows superlinearly:

\[\text{Debt}_n \approx \text{Debt}_0 \times (1 + r)^n\]

where \emph{r} is the debt accumulation rate (typically 10--30\% per
period for undocumented systems).

\subsection{Remediation
Strategies}\label{sec-data-engineering-ml-remediation-strategies-e457}

Addressing data debt requires systematic investment, not heroic one-time
efforts:

\textbf{Documentation Sprints}: Dedicate regular time (e.g., one week
per quarter) exclusively to documentation. Prioritize by dataset usage:
document the 20\% of datasets serving 80\% of models first.

\textbf{Schema Contracts}: Implement explicit schema contracts between
data producers and consumers. Tools like Great Expectations or Pandera
codify expectations, failing fast when schemas drift rather than
accumulating workarounds.

\textbf{Quality Budgets}: Allocate fixed capacity (e.g., 10\% of data
engineering effort) to quality debt remediation. Track known error
backlog and measure burn-down rate.

\textbf{Continuous Retraining}: Implement automated retraining triggers
based on drift detection rather than calendar schedules. Freshness debt
cannot be addressed through periodic heroics---it requires systematic
automation.

The key insight is that data debt, like technical debt, is not
inherently bad. Strategic debt---knowingly accepting documentation
shortcuts to meet a deadline---can be rational. The danger lies in
\emph{unconscious} debt that accumulates untracked until remediation
costs exceed system value.

\section{Debugging Data
Pipelines}\label{sec-data-engineering-ml-debugging-data-pipelines-2f26}

The concepts throughout this chapter---cascading failures, the four
pillars, training-serving consistency, drift detection, labeling
quality, and data debt---converge when systems exhibit problems in
production. Data debt accumulates silently, but its effects eventually
surface as system underperformance, manifesting as model accuracy
degradation, pipeline failures, or subgroup performance disparities that
prove difficult to attribute to specific causes.

Effective debugging requires applying the diagnostic principles
established earlier: the data cascade pattern
(Section~\ref{sec-data-engineering-ml-data-cascades-systematic-foundations-matter-2efe})
reminds us that root causes often lie upstream of symptoms,
training-serving skew
(Section~\ref{sec-data-engineering-ml-detecting-trainingserving-skew-production-2998})
explains many deployment failures, and drift detection
(Section~\ref{sec-data-engineering-ml-detecting-responding-data-drift-509a})
surfaces gradual degradation. When symptoms appear, systematic diagnosis
prevents wasted effort debugging the wrong component. The following
flowchart synthesizes these concepts into an actionable diagnostic
sequence, working through the most common failure modes in order of
frequency:

\begin{tcolorbox}[enhanced jigsaw, bottomrule=.15mm, colframe=quarto-callout-tip-color-frame, toprule=.15mm, coltitle=black, colback=white, rightrule=.15mm, opacityback=0, bottomtitle=1mm, opacitybacktitle=0.6, left=2mm, breakable, leftrule=.75mm, titlerule=0mm, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Data Pipeline Debugging Flowchart}, arc=.35mm, colbacktitle=quarto-callout-tip-color!10!white, toptitle=1mm]

\textbf{Step 1: Is accuracy degrading over time or constant?}

\begin{itemize}
\tightlist
\item
  \textbf{Degrading over time} → Likely \textbf{Data Drift} (freshness
  debt)

  \begin{itemize}
  \tightlist
  \item
    Check: PSI scores, feature distribution plots vs.~training baseline
  \item
    Fix: Implement drift monitoring, trigger retraining
  \end{itemize}
\item
  \textbf{Constant underperformance} → Proceed to Step 2
\end{itemize}

\textbf{Step 2: Does training accuracy match validation accuracy?}

\begin{itemize}
\tightlist
\item
  \textbf{Training \textgreater\textgreater{} Validation} (large gap) →
  Likely \textbf{Overfitting} to data artifacts

  \begin{itemize}
  \tightlist
  \item
    Check: Label noise rate, duplicate detection, class imbalance
  \item
    Fix: Data deduplication, label audit, stratified sampling
  \end{itemize}
\item
  \textbf{Training ≈ Validation} (both low) → Likely \textbf{Data
  Quality} or \textbf{Coverage} issue

  \begin{itemize}
  \tightlist
  \item
    Check: Label consistency (inter-annotator agreement), feature
    completeness
  \item
    Fix: Labeling guidelines, consensus mechanisms, targeted collection
  \end{itemize}
\end{itemize}

\textbf{Step 3: Does validation accuracy match production accuracy?}

\begin{itemize}
\tightlist
\item
  \textbf{Validation \textgreater\textgreater{} Production} (deployment
  drop) → Likely \textbf{Training-Serving Skew}

  \begin{itemize}
  \tightlist
  \item
    Check: Feature computation parity, transformation parameters, schema
    alignment
  \item
    Fix: Feature store adoption, shared transformation code, consistency
    tests
  \end{itemize}
\item
  \textbf{Validation ≈ Production} → Likely \textbf{Representation Gap}
  (training data doesn't match deployment)

  \begin{itemize}
  \tightlist
  \item
    Check: Deployment population vs.~training demographics, edge case
    coverage
  \item
    Fix: Stratified sampling from production, active learning on failure
    cases
  \end{itemize}
\end{itemize}

\textbf{Step 4: Is performance inconsistent across subgroups?}

\begin{itemize}
\tightlist
\item
  \textbf{Yes} → Likely \textbf{Bias} or \textbf{Coverage Gap} in
  training data

  \begin{itemize}
  \tightlist
  \item
    Check: Slice-level metrics (by demographic, by category, by
    difficulty)
  \item
    Fix: Targeted data collection, reweighting, fairness-aware sampling
  \end{itemize}
\end{itemize}

\textbf{The Diagnosis Priority}: Most production ML failures trace to
data, not models. Check in this order (approximate proportions based on
industry experience):\sidenote{\textbf{ML Failure Distribution}: These
proportions represent synthesized estimates from industry reports and
practitioner experience, including findings from Sculley et al.
(\citeproc{ref-sculley2015hidden}{2021}) on ML technical debt and
platform experiences at companies like Uber, Google, and Meta. Actual
proportions vary by domain and organizational maturity; the key insight
is that data-related issues collectively dominate model-related issues.
}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Training-serving skew} (30--40\% of failures)
\item
  \textbf{Data drift} (20--30\% of failures)
\item
  \textbf{Label quality} (15--25\% of failures)
\item
  \textbf{Model architecture} (10--15\% of failures)
\end{enumerate}

Debugging the model before verifying data consistency wastes engineering
cycles on the wrong 10--15\%.

\end{tcolorbox}

\section{Fallacies and
Pitfalls}\label{sec-data-engineering-ml-fallacies-pitfalls-d2f5}

Data engineering involves managing complex distributed systems where
statistical properties dominate correctness more than traditional
software guarantees. The scale and stochastic nature of ML data
pipelines create counterintuitive failure modes that differ
fundamentally from conventional software systems. Engineers transferring
intuition from traditional backend development encounter misconceptions
about data quality, pipeline reliability, and distribution stability.
These fallacies lead to systematic underinvestment in data
infrastructure, resulting in silent model degradation, failed production
deployments, and wasted compute resources on training with corrupted
data.

\textbf{Anti-Patterns: Don't Do This}

\textbf{Anti-Pattern 1: The ``Throw Data at It'' Strategy}

\begin{verbatim}
# DON'T: Assume more data fixes accuracy problems
dataset_v2 = dataset_v1 + scrape_more_web_data(10x_size)
# Result: 10x compute cost, same accuracy, new bias sources
\end{verbatim}

\textbf{Do instead}: Analyze failure modes first. Often 10\% more
\emph{targeted} data beats 10× more \emph{random} data.

\textbf{Anti-Pattern 2: The ``It Works on My Machine'' Pipeline}

\begin{verbatim}
# DON'T: Implement transformations separately for training and serving
# training_pipeline.py
features = normalize(data, compute_mean(data), compute_std(data))
# serving_pipeline.py
features = normalize(data, 0.0, 1.0)  # Hardcoded "close enough" values
# Result: 12% accuracy drop in production, 2 weeks to diagnose
\end{verbatim}

\textbf{Do instead}: Single transformation codebase, persisted
parameters, automated consistency tests.

\textbf{Anti-Pattern 3: The ``Labels are Cheap'' Assumption}

\begin{verbatim}
# DON'T: Budget $50K for compute, $5K for labeling on a 1M-image project
actual_labeling_cost = 1_000_000 * $0.12 = $120,000  # 24x over budget
actual_compute_cost = $50  # ResNet-50 training
# Result: Project cancelled at 80% completion
\end{verbatim}

\textbf{Do instead}: Use TCDO model to budget. Labeling is typically
60\% of total cost.

\textbf{Anti-Pattern 4: The ``Deploy and Forget'' Model}

\begin{verbatim}
# DON'T: Deploy model without drift monitoring
model.deploy(production)
# 6 months later: accuracy degraded from 95% to 78%, undetected
# Result: Customer complaints reveal problem, expensive remediation
\end{verbatim}

\textbf{Do instead}: PSI monitoring, automated alerts at threshold
\textgreater{} 0.1, scheduled retraining triggers.

\textbf{Anti-Pattern 5: The ``Schema Will Never Change'' Assumption}

\begin{verbatim}
# DON'T: Parse upstream data without validation
user_age = int(record["age"])  # Works until upstream sends "25 years"
# Result: Pipeline crash at 3am, manual recovery, lost training day
\end{verbatim}

\textbf{Do instead}: Schema contracts (Great Expectations/Pandera), dead
letter queues, graceful degradation.

\textbf{Fallacy:} \emph{More data always leads to better model
performance.}

Learning theory emphasizes sample complexity, creating the intuition
that larger datasets reduce overfitting. However, this assumes constant
quality as quantity scales. Research on ImageNet found label errors in
3.4\% of validation set examples despite extensive curation
(\citeproc{ref-northcutt2021pervasive}{Northcutt, Athalye, and Mueller
2021}), meaning a 10-million-example dataset contains 340,000 mislabeled
instances. Empirical studies show 10\% label noise reduces model
accuracy by 5 to 8 percentage points for typical vision tasks, with the
degradation compounding during training. As
Section~\ref{sec-data-engineering-ml-data-quality-code-1cca}
establishes, a curated 100,000-example dataset covering diverse
demographics and edge cases outperforms a haphazard 1-million-example
dataset skewed toward common cases. Training on 10x more data requires
10x more GPU hours while potentially achieving worse accuracy, meaning
teams that prioritize curation over collection achieve better
performance at lower infrastructure costs.

\textbf{Pitfall:} \emph{Treating data labeling as a simple mechanical
task that can be outsourced without oversight.}

Teams assume labeling is conceptually simple (show image, get label),
but
Section~\ref{sec-data-engineering-ml-scaling-aiassisted-labeling-9360}
reveals labeling economics dominate total costs. Expert review costs 10
to 50x more than crowdsourcing (\$0.50-2.00 vs \$0.01-0.05 per label),
yet skipping oversight triggers expensive rework. A 1-million-label
dataset at \$0.10 per label with 10\% expert review and 20\% rework
costs \$138,000, not the naive \$100,000 estimate. Training a ResNet-50
on this data costs approximately \$50 in compute, meaning labeling
exceeds training costs by 2,760x. Quality degradation compounds the
economic waste: annotators scoring below 85\% on gold standards
introduce systematic errors that models amplify. Without consensus
mechanisms (3 to 5 labels per example, Fleiss' kappa greater than 0.4),
ambiguous cases receive arbitrary labels causing 8 to 15\% accuracy
drops on production edge cases that require costly relabeling and
retraining cycles.

\textbf{Fallacy:} \emph{Data engineering is a one-time setup that can be
completed before model development begins.}

Traditional software infrastructure precedes application development and
remains stable, but ML systems face continuous distribution drift that
static pipelines cannot handle. As
Section~\ref{sec-data-engineering-ml-detecting-responding-data-drift-509a}
explains, drift detection and response consume 30 to 40\% of ongoing ML
operations effort. Data distributions shift through covariate drift
(demographics evolve), label shift (seasonal class frequencies), and
concept drift (feature-label relationships change). The Population
Stability Index (PSI) quantifies retraining triggers: PSI greater than
or equal to 0.2 signals significant changes requiring model updates,
while 0.1 to 0.2 demands investigation. A recommendation system might
observe PSI = 0.19 as users age from 25-35 to 35-45 over six months,
requiring pipeline adaptation. Systems designed as static infrastructure
lack monitoring, alerting, and automated response mechanisms that
production deployments require. Teams must budget 30 to 40\% of ongoing
engineering capacity for continuous evolution, not treat data
engineering as a phase that completes before modeling.

\textbf{Fallacy:} \emph{Training and test data splitting is sufficient
to ensure model generalization.}

ML education emphasizes train/test splitting to prevent overfitting,
creating the belief that validation accuracy predicts production
performance. This assumes training, testing, and production data share
the same distribution, which rarely holds. As
Section~\ref{sec-data-engineering-ml-ensuring-trainingserving-consistency-c683}
explains, training-serving skew causes silent production failures. A
recommendation system computing ``user\_lifetime\_purchases'' by joining
complete transaction histories during training but using weekly-updated
materialized views during serving creates 15\% feature discrepancy,
causing observed 12\% accuracy drops in A/B tests. Geographic shifts
compound the problem: models achieving 95\% accuracy on North American
test sets drop to 73\% in Southeast Asian markets due to demographic and
linguistic differences. These distribution shifts manifest as accuracy
degradation for specific user segments, creating fairness violations.
Production systems require continuous monitoring using metrics like the
Kolmogorov-Smirnov test (statistically significant drift at
\(p < 0.05\)), detecting issues before they affect users rather than
relying solely on development-time validation.

\textbf{Pitfall:} \emph{Building data pipelines without considering
failure modes and recovery mechanisms.}

Engineers from traditional request-response services design pipelines
assuming data arrives in expected formats at expected rates, but
production pipelines face fundamentally different failure modes. As
Section~\ref{sec-data-engineering-ml-quality-validation-monitoring-498f}
establishes, severity-based monitoring distinguishes complete failures
(zero throughput for greater than 5 minutes) from gradual degradation
(throughput dropping to 80\% of baseline, error rates exceeding 5\%,
quality metrics drifting more than 2 standard deviations). A
recommendation system processing 50,000 events per second sets
throughput alerts at 40,000 events per second sustained for more than 10
minutes, catching capacity problems with headroom for normal variation.
Feature quality monitoring tracks null rates: when user\_age shows nulls
in more than 5\% of records despite training data containing less than
1\% nulls, upstream sources have failed. Failures cascade across
multiple dimensions simultaneously: database failovers reduce throughput
to 35,000 events per second while increasing null rates to 8\% and
duplicating 3\% of records. Data quality validation catches
approximately 60\% of issues through executable assertions, but the
remaining 40\% require runtime monitoring with automated alerting and
graceful degradation.

\textbf{Pitfall:} \emph{Choosing storage architecture based solely on
capacity cost without considering access patterns and performance
requirements.}

Teams compare raw storage costs (S3 Standard at \$23 per TB per month vs
Glacier Deep Archive at \$1 per TB per month) and select the cheapest
option without analyzing access patterns. However,
Section~\ref{sec-data-engineering-ml-ml-storage-systems-architecture-options-67fa}
establishes that total cost of ownership includes retrieval costs,
access latency, and throughput limitations. Glacier Deep Archive charges
\$0.02 per GB for retrieval plus 12-hour access latency, meaning a
single full retrieval of 100 TB costs \$2,000, equivalent to 87 months
of S3 Standard storage for that data. Training workloads requiring
random access to 10\% of a 100 TB dataset daily would spend \$600 per
day on Glacier retrievals versus \$230 per month for S3 Standard storage
with 3,500 MB/s sustained read throughput. Local NVMe storage at \$0.10
per GB (\$100 per TB) appears expensive but delivers 7,000 MB/s
sequential reads and sub-millisecond random access latency, completing
training epochs 15 to 30x faster than S3 for I/O-bound workloads.
Organizations must match storage tiers to access patterns: cold archival
data to Glacier, training datasets requiring sequential scans to S3, and
active training data with random access patterns to local SSDs or
high-performance networked storage.

These fallacies and pitfalls underscore a central theme: data
engineering success requires abandoning intuitions from traditional
software development. Statistical systems demand continuous monitoring,
cost models must account for human labor, and storage decisions must
optimize for access patterns rather than capacity alone.

\section{Summary}\label{sec-data-engineering-ml-summary-4ac6}

Data engineering provides the foundational infrastructure that
transforms raw information into the basis of machine learning systems,
determining model performance, system reliability, ethical compliance,
and long-term maintainability. The Four Pillars framework
(Figure~\ref{fig-four-pillars}) and the cascading nature of data quality
failures (Figure~\ref{fig-cascades}) reveal why every stage of the data
pipeline requires careful engineering decisions. The task of ``getting
data ready'' encompasses complex trade-offs quantified throughout this
chapter: the TCDO cost model for budgeting, storage performance
hierarchies (Table~\ref{tbl-storage-performance},
Table~\ref{tbl-ml-latencies}), and drift detection thresholds that guide
production operations.

The technical architecture of data systems demonstrates how engineering
decisions compound across the pipeline to create either reliable,
scalable foundations or brittle, maintenance-heavy technical debt. Data
acquisition strategies must navigate the reality that perfect datasets
rarely exist in nature, requiring sophisticated approaches ranging from
crowdsourcing and synthetic generation to careful curation and active
learning. Storage architectures from traditional databases to modern
data lakes and feature stores represent fundamental choices about how
data flows through the system, affecting everything from training speed
to serving latency.

\begin{tcolorbox}[enhanced jigsaw, bottomrule=.15mm, colframe=quarto-callout-important-color-frame, toprule=.15mm, coltitle=black, colback=white, rightrule=.15mm, opacityback=0, bottomtitle=1mm, opacitybacktitle=0.6, left=2mm, breakable, leftrule=.75mm, titlerule=0mm, title=\textcolor{quarto-callout-important-color}{\faExclamation}\hspace{0.5em}{Key Takeaways}, arc=.35mm, colbacktitle=quarto-callout-important-color!10!white, toptitle=1mm]

\begin{itemize}
\item
  \textbf{Data is code---version it, test it, review it}: The dataset is
  the source code of an ML system. Apply the same rigor: version
  control, unit tests (validation), and code review (data review).
\item
  \textbf{Training-serving consistency is non-negotiable}: Any
  transformation applied during training must be applied identically
  during serving. This is a mathematical requirement, not a best
  practice.
\item
  \textbf{Labeling costs dominate---require substantial resource
  allocation}: Labeling typically costs 1,000--3,000× more than model
  training compute. Labeling is the serial bottleneck that
  parallelization cannot solve.
\item
  \textbf{Storage hierarchy determines iteration speed}: The 70×
  throughput gap between local NVMe (7 GB/s) and cloud object storage
  (100 MB/s) determines whether iterations occur daily or weekly.
\item
  \textbf{Data debt compounds---requires continuous remediation}:
  Documentation, schema, quality, and freshness debt accumulate with
  compound interest. Allocate 10--20\% of effort to continuous
  remediation.
\end{itemize}

\end{tcolorbox}

Our KWS case study demonstrates these principles in action. From problem
definition through production deployment, the Four Pillars guided every
decision: quality requirements drove our multi-source acquisition
strategy combining curated datasets with crowdsourcing and synthetic
generation; reliability shaped our pipeline architecture with graceful
degradation and consistency validation; scalability determined our
tiered storage design handling 23 million audio samples across 736 GB of
raw data; and governance established the privacy protections and lineage
tracking essential for always-listening devices in users' homes. The KWS
system shows that data engineering is not a preprocessing step to be
completed before ``real'' ML work begins---it is the foundation upon
which model performance, user trust, and regulatory compliance rest.

The integration of reliable data governance practices throughout the
pipeline ensures that ML systems remain trustworthy, compliant, and
transparent as they scale in complexity and impact. While this chapter
focuses on \emph{building} reliable data infrastructure,
\textbf{?@sec-data-efficiency} examines \emph{optimizing} data
usage---reducing the total data required through active learning, data
pruning, and intelligent sampling strategies.

Part I has established the landscape of ML systems: where they deploy,
how practitioners work with them, and why data infrastructure determines
success or failure. Data Engineering has provided us with the
fuel---clean, reliable, and scalable datasets. But fuel alone does not
create motion. To transform these bits into intelligence, we need to
understand the mechanics of the engine itself. \textbf{We have the fuel;
now we need the engine theory.} We turn next to the mathematical
foundations of learning:
\textbf{?@sec-deep-learning-systems-foundations}, which transforms
neural networks from opaque components into engineerable systems whose
behavior we can predict, debug, and optimize.

\FloatBarrier\clearpage

\setpartsummary{This part focuses on the development phase of ML systems. It bridges the gap between mathematical theory and working code, covering neural network architectures, framework implementation details, and the systems engineering required to train models effectively.}

\addtocontents{toc}{\par\addvspace{12pt}\noindent\hfil\bfseries\color{crimson}Part~I~Development and Training\color{black}\hfil\par\addvspace{6pt}}

\numberedpart{Development and Training}

\haspartsummaryfalse

\phantomsection\label{refs}
\begin{CSLReferences}{1}{0}
\bibitem[\citeproctext]{ref-akidau2015dataflow}
Akidau, Tyler, Robert Bradshaw, Craig Chambers, Slava Chernyak, Rafael
J. Fernández-Moctezuma, Reuven Lax, Sam McVeety, et al. 2015. {``The
Dataflow Model: A Practical Approach to Balancing Correctness, Latency,
and Cost in Massive-Scale, Unbounded, Out-of-Order Data Processing.''}
\emph{Proceedings of the VLDB Endowment} 8 (12): 1792--1803.
\url{https://doi.org/10.14778/2824032.2824076}.

\bibitem[\citeproctext]{ref-anylogic_synthetic}
AnyLogic. 2024. {``Synthetic Data for Artificial Intelligence.''}
\url{https://www.anylogic.com/features/artificial-intelligence/synthetic-data/}.

\bibitem[\citeproctext]{ref-ardila2020common}
Ardila, Rosana, Megan Branson, Kelly Davis, Michael Kohler, Josh Meyer,
Michael Henretty, Reuben Morais, Lindsay Saunders, Francis Tyers, and
Gregor Weber. 2020. {``Common Voice: A Massively-Multilingual Speech
Corpus.''} In \emph{Proceedings of the Twelfth Language Resources and
Evaluation Conference}, 4218--22. Marseille, France: European Language
Resources Association. \url{https://aclanthology.org/2020.lrec-1.520}.

\bibitem[\citeproctext]{ref-beyer2020we}
Beyer, Lucas, Olivier J. Hénaff, Alexander Kolesnikov, Xiaohua Zhai, and
A"aron van den Oord. 2020. {``Are We Done with ImageNet?''} \emph{arXiv
Preprint arXiv:2006.07159}, June.
\url{http://arxiv.org/abs/2006.07159v1}.

\bibitem[\citeproctext]{ref-bishop2006pattern}
Bishop, Christopher M. 2006. \emph{Pattern Recognition and Machine
Learning}. Springer.

\bibitem[\citeproctext]{ref-brown2020language}
Brown, Tom B., Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan,
Prafulla Dhariwal, Arvind Neelakantan, et al. 2020. {``Language Models
Are Few-Shot Learners.''} Edited by Hugo Larochelle, Marc'Aurelio
Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin.
\emph{Advances in Neural Information Processing Systems} 33 (May):
1877--1901. \url{https://doi.org/10.48550/arxiv.2005.14165}.

\bibitem[\citeproctext]{ref-chapelle2009semisupervised}
Chapelle, O., B. Scholkopf, and A. Zien Eds. 2009. {``Semi-Supervised
Learning (Chapelle, o. Et Al., Eds.; 2006) {[}Book Reviews{]}.''}
\emph{IEEE Transactions on Neural Networks} 20 (3): 542--42.
\url{https://doi.org/10.1109/tnn.2009.2015974}.

\bibitem[\citeproctext]{ref-chen2021evaluating}
Chen, Mark, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de
Oliveira Pinto, Jared Kaplan, Harri Edwards, et al. 2021. {``Evaluating
Large Language Models Trained on Code.''} \emph{arXiv Preprint
arXiv:2107.03374}, July. \url{http://arxiv.org/abs/2107.03374v2}.

\bibitem[\citeproctext]{ref-coleman2022similarity}
Coleman, Cody, Edward Chou, Julian Katz-Samuels, Sean Culatana, Peter
Bailis, Alexander C. Berg, Robert Nowak, Roshan Sumbaly, Matei Zaharia,
and I. Zeki Yalniz. 2022. {``Similarity Search for Efficient Active
Learning and Search of Rare Concepts.''} \emph{Proceedings of the AAAI
Conference on Artificial Intelligence} 36 (6): 6402--10.
\url{https://doi.org/10.1609/aaai.v36i6.20591}.

\bibitem[\citeproctext]{ref-crowdflower2016data}
CrowdFlower. 2016. {``2016 Data Science Report.''} CrowdFlower Inc.
\url{https://visit.figure-eight.com/rs/416-ZBE-142/images/CrowdFlower_DataScienceReport_2016.pdf}.

\bibitem[\citeproctext]{ref-dwork2008differential}
Dwork, Cynthia. n.d. {``Differential Privacy: A Survey of Results.''} In
\emph{Theory and Applications of Models of Computation}, 1--19. Springer
Berlin Heidelberg. \url{https://doi.org/10.1007/978-3-540-79228-4_1}.

\bibitem[\citeproctext]{ref-gebru2018datasheets}
Gebru, Timnit, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman
Vaughan, Hanna Wallach, Hal Daumé III, and Kate Crawford. 2021.
{``Datasheets for Datasets.''} \emph{Communications of the ACM} 64 (12):
86--92. \url{https://doi.org/10.1145/3458723}.

\bibitem[\citeproctext]{ref-google_crowdsource}
Google. 2024a. {``Google Crowdsource.''}
\url{https://crowdsource.google.com/}.

\bibitem[\citeproctext]{ref-openimages_website}
---------. 2024b. {``Open Images Dataset.''}
\url{https://storage.googleapis.com/openimages/web/index.html}.

\bibitem[\citeproctext]{ref-google_recaptcha}
---------. 2024c. {``reCAPTCHA.''}
\url{https://www.google.com/recaptcha/about/}.

\bibitem[\citeproctext]{ref-groeneveld2024olmo}
Groeneveld, Dirk, Iz Beltagy, Pete Walsh, Akshita Bhagia, Rodney Kinney,
Oyvind Tafjord, Ananya Harsh Jha, et al. 2024. {``OLMo: Accelerating the
Science of Language Models.''} \emph{arXiv Preprint arXiv:2402.00838},
February. \url{http://arxiv.org/abs/2402.00838v4}.

\bibitem[\citeproctext]{ref-gudivada2017data}
Gudivada, Venkat N., Dhana Rao Rao, et al. 2017. {``Data Quality
Considerations for Big Data and Machine Learning: Going Beyond Data
Cleaning and Transformations.''} \emph{IEEE Transactions on Knowledge
and Data Engineering}.

\bibitem[\citeproctext]{ref-henderson2018deep}
Henderson, Peter, Riashat Islam, Philip Bachman, Joelle Pineau, Doina
Precup, and David Meger. 2018. {``Deep Reinforcement Learning That
Matters.''} \emph{Proceedings of the AAAI Conference on Artificial
Intelligence} 32 (1): 3207--14.
\url{https://doi.org/10.1609/aaai.v32i1.11694}.

\bibitem[\citeproctext]{ref-uci_repo}
Information, UCI School of, and Computer Sciences. 2024. {``UCI Machine
Learning Repository.''} \url{https://archive.ics.uci.edu/}.

\bibitem[\citeproctext]{ref-inmon2005building}
Inmon, W. H. 2005. \emph{Building the Data Warehouse}. John Wiley Sons.

\bibitem[\citeproctext]{ref-10.1109ux2fICRA.2017.7989092}
Johnson-Roberson, Matthew, Charles Barto, Rounak Mehta, Sharath Nittur
Sridhar, Karl Rosaen, and Ram Vasudevan. 2017. {``Driving in the Matrix:
Can Virtual Worlds Replace Human-Generated Annotations for Real World
Tasks?''} In \emph{2017 IEEE International Conference on Robotics and
Automation (ICRA)}, 746--53. Singapore, Singapore: IEEE.
\url{https://doi.org/10.1109/icra.2017.7989092}.

\bibitem[\citeproctext]{ref-kaggle_website}
Kaggle. 2024. {``Kaggle: Your Machine Learning and Data Science
Community.''} \url{https://www.kaggle.com/}.

\bibitem[\citeproctext]{ref-kleppmann2017designing}
Kleppmann, Martin. 2016. \emph{Designing Data-Intensive Applications:
The Big Ideas Behind Reliable, Scalable, and Maintainable Systems}.
O'Reilly Media. \url{http://shop.oreilly.com/product/0636920032175.do}.

\bibitem[\citeproctext]{ref-krishnan2022selfsupervised}
Krishnan, Rayan, Pranav Rajpurkar, and Eric J. Topol. 2022.
{``Self-Supervised Learning in Medicine and Healthcare.''} \emph{Nature
Biomedical Engineering} 6 (12): 1346--52.
\url{https://doi.org/10.1038/s41551-022-00914-1}.

\bibitem[\citeproctext]{ref-krizhevsky2012imagenet}
Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E. Hinton. 2017.
{``ImageNet Classification with Deep Convolutional Neural Networks.''}
\emph{Communications of the ACM} 60 (6): 84--90.
\url{https://doi.org/10.1145/3065386}.

\bibitem[\citeproctext]{ref-kuhn2013applied}
Kuhn, Max, and Kjell Johnson. 2013. \emph{Applied Predictive Modeling}.
Springer New York. \url{https://doi.org/10.1007/978-1-4614-6849-3}.

\bibitem[\citeproctext]{ref-imagenet_website}
Lab, Stanford Vision. 2024. {``ImageNet.''}
\url{https://www.image-net.org/}.

\bibitem[\citeproctext]{ref-mazumder2021multilingual}
Mazumder, Mark, Sharad Chitlangia, Colby Banbury, Yiping Kang, Juan
Manuel Ciro, Keith Achorn, Daniel Galvez, et al. 2021. {``Multilingual
Spoken Words Corpus.''} In \emph{Thirty-Fifth Conference on Neural
Information Processing Systems Datasets and Benchmarks Track (Round 2)}.

\bibitem[\citeproctext]{ref-waze_website}
Mobile, Waze. 2024. {``Waze.''} \url{https://www.waze.com/}.

\bibitem[\citeproctext]{ref-nayak2022improving}
Nayak, Prateeth, Takuya Higuchi, Anmol Gupta, Shivesh Ranjan, Stephen
Shum, Siddharth Sigtia, Erik Marchi, et al. 2022. {``Improving Voice
Trigger Detection with Metric Learning.''} \emph{arXiv Preprint
arXiv:2204.02455}, April. \url{http://arxiv.org/abs/2204.02455v2}.

\bibitem[\citeproctext]{ref-ng2021datacentric}
Ng, Andrew. 2021. {``MLOps: From Model-Centric to Data-Centric AI.''}
DeepLearning.AI.\href{\%0A\%20\%20\%20\%20https://www.deeplearning.ai/wp-content/uploads/2021/06/MLOps-From-Model-centric-to-Data-centric-AI.pdf\%0A\%20\%20}{https://www.deeplearning.ai/wp-content/uploads/2021/06/MLOps-From-Model-centric-to-Data-centric-AI.pdf
}.

\bibitem[\citeproctext]{ref-northcutt2021pervasive}
Northcutt, Curtis G., Anish Athalye, and Jonas Mueller. 2021.
{``Pervasive Label Errors in Test Sets Destabilize Machine Learning
Benchmarks.''} \emph{arXiv Preprint arXiv:2103.14749} 34 (March):
19075--90. \url{https://doi.org/10.48550/arxiv.2103.14749}.

\bibitem[\citeproctext]{ref-nvidia_simulation}
NVIDIA. 2024. {``Autonomous Vehicle Simulation.''}
\url{https://www.nvidia.com/en-us/use-cases/autonomous-vehicle-simulation/}.

\bibitem[\citeproctext]{ref-oakden2020hidden}
Oakden-Rayner, Luke, Jared Dunnmon, Gustavo Carneiro, and Christopher
Re. 2020. {``Hidden Stratification Causes Clinically Meaningful Failures
in Machine Learning for Medical Imaging.''} In \emph{Proceedings of the
ACM Conference on Health, Inference, and Learning}, 151--59. ACM.
\url{https://doi.org/10.1145/3368555.3384468}.

\bibitem[\citeproctext]{ref-park2019specaugment}
Park, Daniel S., William Chan, Yu Zhang, Chung-Cheng Chiu, Barret Zoph,
Ekin D. Cubuk, and Quoc V. Le. 2019. {``SpecAugment: A Simple Data
Augmentation Method for Automatic Speech Recognition.''} \emph{arXiv
Preprint arXiv:1904.08779}, April.
\url{http://arxiv.org/abs/1904.08779v3}.

\bibitem[\citeproctext]{ref-time_openai_kenya}
Perrigo, Billy. 2023. {``OpenAI Used Kenyan Workers on Less Than \$2 Per
Hour to Make ChatGPT Less Toxic.''} \emph{Time}, January.
\url{https://time.com/6247678/openai-chatgpt-kenya-workers/}.

\bibitem[\citeproctext]{ref-pineau2021improving}
Pineau, Joelle, Philippe Vincent-Lamarre, Koustuv Sinha, Vincent
Larivière, Alina Beygelzimer, Florence d'Alché-Buc, Emily Fox, and Hugo
Larochelle. 2021. {``Improving Reproducibility in Machine Learning
Research (a Report from the Neurips 2019 Reproducibility Program).''}
\emph{Journal of Machine Learning Research} 22 (164): 1--20.

\bibitem[\citeproctext]{ref-pushkarna2022data}
Pushkarna, Mahima, Andrew Zaldivar, and Oddur Kjartansson. 2022. {``Data
Cards: Purposeful and Transparent Dataset Documentation for Responsible
AI.''} In \emph{2022 ACM Conference on Fairness Accountability and
Transparency}, 1776--826. ACM.
\url{https://doi.org/10.1145/3531146.3533231}.

\bibitem[\citeproctext]{ref-ratner2018snorkel}
Ratner, Alex, Braden Hancock, Jared Dunnmon, Roger Goldman, and
Christopher Ré. 2018. {``Snorkel MeTaL: Weak Supervision for Multi-Task
Learning.''} In \emph{Proceedings of the Second Workshop on Data
Management for End-to-End Machine Learning}, 1--4. ACM.
\url{https://doi.org/10.1145/3209889.3209898}.

\bibitem[\citeproctext]{ref-sambasivan2021everyone}
Sambasivan, Nithya, Shivani Kapania, Hannah Highfill, Diana Akrong,
Praveen Paritosh, and Lora M Aroyo. 2021. {``{`Everyone Wants to Do the
Model Work, Not the Data Work'}: Data Cascades in High-Stakes AI.''} In
\emph{Proceedings of the 2021 CHI Conference on Human Factors in
Computing Systems}, 1--15. ACM.
\url{https://doi.org/10.1145/3411764.3445518}.

\bibitem[\citeproctext]{ref-harvard_law_chatgpt}
School, Harvard Law. 2024. {``Does ChatGPT Violate New York Times
Copyrights?''}
\url{https://hls.harvard.edu/today/does-chatgpt-violate-new-york-times-copyrights/}.

\bibitem[\citeproctext]{ref-sculley2015hidden}
Sculley, D., Gary Holt, Daniel Golovin, Eugene Davydov, Todd Phillips,
Dietmar Ebner, Vinay Chaudhary, Michael Young, Jean-François Crespo, and
Dan Dennison. 2021. {``Technical Debt in Machine Learning Systems.''} In
\emph{Technical Debt in Practice}, 28:177--92. The MIT Press.
\url{https://doi.org/10.7551/mitpress/12440.003.0011}.

\bibitem[\citeproctext]{ref-mturk_website}
Services, Amazon Web. 2024. {``Amazon Mechanical Turk.''}
\url{https://www.mturk.com/}.

\bibitem[\citeproctext]{ref-shorten2019survey}
Shorten, Connor, and Taghi M Khoshgoftaar. 2019. {``A Survey on Image
Data Augmentation for Deep Learning.''} \emph{Journal of Big Data} 6
(1): 1--48.

\bibitem[\citeproctext]{ref-siddiqi2006credit}
Siddiqi, Naeem. 2012. \emph{Credit Risk Scorecards: Developing and
Implementing Intelligent Credit Scoring}. Hoboken, NJ: Wiley.
\url{https://doi.org/10.1002/9781119201731}.

\bibitem[\citeproctext]{ref-stonebraker2005cstore}
Stonebraker, Mike, Daniel J. Abadi, Adam Batkin, Xuedong Chen, Mitch
Cherniack, Miguel Ferreira, Edmond Lau, et al. 2018. {``C-Store: A
Column-Oriented DBMS.''} In \emph{Making Databases Work: The Pragmatic
Wisdom of Michael Stonebraker}, 491--518. Association for Computing
Machinery. \url{https://doi.org/10.1145/3226595.3226638}.

\bibitem[\citeproctext]{ref-thyagarajan2023multilabel}
Thyagarajan, Aditya, Elías Snorrason, Curtis G. Northcutt, and Jonas
Mueller 0001. 2022. {``Identifying Incorrect Annotations in Multi-Label
Classification Data.''} \emph{CoRR}.
\url{https://doi.org/10.48550/ARXIV.2211.13895}.

\bibitem[\citeproctext]{ref-venturebeat_datasets}
VentureBeat. 2024. {``3 Big Problems with Datasets in AI and Machine
Learning.''}\href{\%0A\%20\%20\%20\%20https://venturebeat.com/uncategorized/3-big-problems-with-datasets-in-ai-and-machine-learning/\%0A\%20\%20}{https://venturebeat.com/uncategorized/3-big-problems-with-datasets-in-ai-and-machine-learning/
}.

\bibitem[\citeproctext]{ref-wachter2017counterfactual}
Wachter, Sandra, Brent Mittelstadt, and Chris Russell. 2017.
{``Counterfactual Explanations Without Opening the Black Box: Automated
Decisions and the GDPR.''} \emph{SSRN Electronic Journal} 31: 841.
\url{https://doi.org/10.2139/ssrn.3063289}.

\bibitem[\citeproctext]{ref-wang2019balanced}
Wang, Tianlu, Jieyu Zhao, Mark Yatskar, Kai-Wei Chang, and Vicente
Ordonez. 2019. {``Balanced Datasets Are Not Enough: Estimating and
Mitigating Gender Bias in Deep Image Representations.''} In \emph{2019
IEEE/CVF International Conference on Computer Vision (ICCV)}, 5309--18.
IEEE. \url{https://doi.org/10.1109/iccv.2019.00541}.

\bibitem[\citeproctext]{ref-warden2018speech}
Warden, Pete. 2018. {``Speech Commands: A Dataset for Limited-Vocabulary
Speech Recognition.''} \emph{arXiv Preprint arXiv:1804.03209}, April.
\url{https://doi.org/10.48550/arXiv.1804.03209}.

\bibitem[\citeproctext]{ref-werchniak2021exploring}
Werchniak, Andrew, Roberto Barra Chicote, Yuriy Mishchenko, Jasha
Droppo, Jeff Condal, Peng Liu, and Anish Shah. 2021. {``Exploring the
Application of Synthetic Audio in Training Keyword Spotters.''} In
\emph{ICASSP 2021 - 2021 IEEE International Conference on Acoustics,
Speech and Signal Processing (ICASSP)}, 7993--96. IEEE; IEEE.
\url{https://doi.org/10.1109/icassp39728.2021.9413448}.

\end{CSLReferences}


\backmatter


\end{document}
