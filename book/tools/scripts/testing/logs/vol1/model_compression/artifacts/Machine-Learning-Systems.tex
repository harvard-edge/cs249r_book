% Options for packages loaded elsewhere
% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode,linktoc=all,pdfwindowui,pdfpagemode=FullScreen,pdfpagelayout=TwoPageRight}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  9pt,
  letterpaper,
  abstract,
  titlepage]{scrbook}
\usepackage{xcolor}
\usepackage[left=1in,marginparwidth=2.0666666666667in,textwidth=4.1333333333333in,marginparsep=0.3in]{geometry}
\usepackage{amsmath,amssymb}
\setcounter{secnumdepth}{3}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
% Make \paragraph and \subparagraph free-standing
\makeatletter
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}{
    \@ifstar
      \xxxParagraphStar
      \xxxParagraphNoStar
  }
  \newcommand{\xxxParagraphStar}[1]{\oldparagraph*{#1}\mbox{}}
  \newcommand{\xxxParagraphNoStar}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}{
    \@ifstar
      \xxxSubParagraphStar
      \xxxSubParagraphNoStar
  }
  \newcommand{\xxxSubParagraphStar}[1]{\oldsubparagraph*{#1}\mbox{}}
  \newcommand{\xxxSubParagraphNoStar}[1]{\oldsubparagraph{#1}\mbox{}}
\fi
\makeatother

\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{242,244,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{#1}}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.82,0.10,0.26}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{#1}}
\newcommand{\BuiltInTok}[1]{\textcolor[rgb]{0.88,0.40,0.10}{#1}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.41,0.45,0.49}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.86,0.20,0.18}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.58,0.00,0.30}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{#1}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.75,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.88,0.40,0.10}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.42,0.37,0.78}{#1}}
\newcommand{\ImportTok}[1]{\textcolor[rgb]{0.00,0.45,0.70}{\textbf{#1}}}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.73,0.49,0.84}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.10,0.10,0.10}{#1}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{#1}}
\newcommand{\RegionMarkerTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.12,0.55,0.30}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{#1}}}}

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\newsavebox\pandoc@box
\newcommand*\pandocbounded[1]{% scales image to fit in text height/width
  \sbox\pandoc@box{#1}%
  \Gscale@div\@tempa{\textheight}{\dimexpr\ht\pandoc@box+\dp\pandoc@box\relax}%
  \Gscale@div\@tempb{\linewidth}{\wd\pandoc@box}%
  \ifdim\@tempb\p@<\@tempa\p@\let\@tempa\@tempb\fi% select the smaller of both
  \ifdim\@tempa\p@<\p@\scalebox{\@tempa}{\usebox\pandoc@box}%
  \else\usebox{\pandoc@box}%
  \fi%
}
% Set default figure placement to htbp
\def\fps@figure{htbp}
\makeatother
% definitions for citeproc citations
\NewDocumentCommand\citeproctext{}{}
\NewDocumentCommand\citeproc{mm}{%
  \begingroup\def\citeproctext{#2}\cite{#1}\endgroup}
\makeatletter
 % allow citations to break across lines
 \let\@cite@ofmt\@firstofone
 % avoid brackets around text for \cite:
 \def\@biblabel#1{}
 \def\@cite#1#2{{#1\if@tempswa , #2\fi}}
\makeatother
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newenvironment{CSLReferences}[2] % #1 hanging-indent, #2 entry-spacing
 {\begin{list}{}{%
  \setlength{\itemindent}{0pt}
  \setlength{\leftmargin}{0pt}
  \setlength{\parsep}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1
   \setlength{\leftmargin}{\cslhangindent}
   \setlength{\itemindent}{-1\cslhangindent}
  \fi
  % set entry spacing
  \setlength{\itemsep}{#2\baselineskip}}}
 {\end{list}}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{\hfill\break\parbox[t]{\linewidth}{\strut\ignorespaces#1\strut}}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}

% =============================================================================
% LATEX HEADER CONFIGURATION FOR MLSYSBOOK PDF
% =============================================================================
% This file contains all LaTeX package imports, custom commands, and styling
% definitions for the PDF output of the Machine Learning Systems textbook.
%
% Key Features:
% - Harvard crimson branding throughout
% - Custom part/chapter/section styling
% - Professional table formatting with colored headers
% - Margin notes with custom styling
% - TikZ-based part dividers
% - Page numbering (Roman for frontmatter, Arabic for mainmatter)
%
% Note: This file is included via _quarto-pdf.yml and affects PDF output only.
% HTML/EPUB styling is handled separately via CSS files.
% =============================================================================

% =============================================================================
% PACKAGE IMPORTS
% =============================================================================

% Layout and positioning
% \usepackage[outercaption, ragged]{sidecap}  % Commented out to make figure captions inline instead of in margin
\usepackage{adjustbox}      % Adjusting box dimensions
\usepackage{afterpage}      % Execute commands after page break
\usepackage{morefloats}     % Increase number of floats
\usepackage{array}          % Enhanced table column formatting
\usepackage{atbegshi}       % Insert content at page beginning
%\usepackage{changepage}     % Change page dimensions mid-document
\usepackage{emptypage}      % Clear headers/footers on empty pages

% Language and text
\usepackage[english]{babel} % English language support
\usepackage{microtype}      % Improved typography and hyphenation

% Captions and floats
\usepackage{caption}
% Caption styling configuration
%\captionsetup[table]{belowskip=5pt}
\captionsetup{format=plain}
\DeclareCaptionLabelFormat{mylabel}{#1
#2:\hspace{1.0ex}}
\DeclareCaptionFont{ninept}{\fontsize{7pt}{8}\selectfont #1}

% Figure captions: Small font, bold label, ragged right
\captionsetup[figure]{labelfont={bf,ninept},labelsep=space,
belowskip=2pt,aboveskip=6pt,labelformat=mylabel,
justification=raggedright,singlelinecheck=false,font={ninept}}

% Table captions: Small font, bold label, ragged right
\captionsetup[table]{belowskip=6pt,labelfont={bf,ninept},labelsep=none,
labelformat=mylabel,justification=raggedright,singlelinecheck=false,font={ninept}}

% Typography fine-tuning
\emergencystretch=5pt       % Allow extra stretch to avoid overfull boxes

% Utility packages
\usepackage{etoolbox}       % For patching commands and environments

% Page layout and headers
\usepackage{fancyhdr}       % Custom headers and footers
\usepackage{geometry}       % Page dimensions and margins

% Graphics and figures
\usepackage{graphicx}       % Include graphics
\usepackage{float}          % Improved float placement
\usepackage[skins,breakable]{tcolorbox} % Coloured and framed text boxes
\tcbset{before upper=\setlength{\parskip}{3pt}}

% Tables
\usepackage{longtable}      % Multi-page tables

% Fonts and typography
\usepackage{fontspec}       % Font selection for LuaLaTeX
\usepackage{mathptmx}       % Times-like math fonts
\usepackage{newpxtext}      % Palatino-like font for body text

% Colors and visual elements
\usepackage[dvipsnames]{xcolor}  % Extended color support
\usepackage{tikz}           % Programmatic graphics
\usetikzlibrary{positioning}
\usetikzlibrary{calc}
\usepackage{tikzpagenodes}  % TikZ positioning relative to page

% Code listings
\usepackage{listings}       % Code highlighting

% Hyperlinks
\usepackage{hyperref}       % Clickable links in PDF

% Conditional logic
\usepackage{ifthen}         % If-then-else commands

% Math symbols
\usepackage{amsmath}        % AMS math extensions
\usepackage{amssymb}        % AMS math symbols
\usepackage{latexsym}       % Additional LaTeX symbols
\usepackage{pifont}         % Zapf Dingbats symbols
\providecommand{\blacklozenge}{\ding{117}}  % Black diamond symbol

% Lists
\usepackage{enumitem}       % Customizable lists

% Margin notes and sidenotes
\usepackage{marginfix}      % Fixes margin note overflow
\usepackage{marginnote}     % Margin notes
\usepackage{sidenotes}      % Academic-style sidenotes
\renewcommand\raggedrightmarginnote{\sloppy}
\renewcommand\raggedleftmarginnote{\sloppy}

% Typography improvements
\usepackage{ragged2e}       % Better ragged text
\usepackage[all]{nowidow}   % Prevent widows and orphans
\usepackage{needspace}      % Ensure minimum space on page

% Section formatting
\usepackage[explicit]{titlesec}  % Custom section titles
\usepackage{tocloft}        % Table of contents formatting

% QR codes and icons
\usepackage{fontawesome5}   % Font Awesome icons
\usepackage{qrcode}         % QR code generation
\qrset{link, height=15mm}

% =============================================================================
% FLOAT CONFIGURATION
% =============================================================================
% Allow more floats per page to handle figure-heavy chapters
\extrafloats{200}
\setcounter{topnumber}{12}       % Max floats at top of page
\setcounter{bottomnumber}{12}    % Max floats at bottom of page
\setcounter{totalnumber}{24}     % Max floats per page
\setcounter{dbltopnumber}{8}     % Max floats at top of two-column page
\renewcommand{\topfraction}{.95}  % Max fraction of page for top floats
\renewcommand{\bottomfraction}{.95}
\renewcommand{\textfraction}{.05}  % Min fraction of page for text
\renewcommand{\floatpagefraction}{.7}  % Min fraction of float page
\renewcommand{\dbltopfraction}{.95}

% Prevent "Float(s) lost" errors by flushing floats more aggressively
\usepackage{placeins}  % Provides \FloatBarrier

% =============================================================================
% COLOR DEFINITIONS
% =============================================================================
% Harvard crimson - primary brand color used throughout
\definecolor{crimson}{HTML}{A51C30}

% Quiz element colors
\definecolor{quiz-question-color1}{RGB}{225,243,248}  % Light blue background
\definecolor{quiz-question-color2}{RGB}{17,158,199}   % Blue border
\definecolor{quiz-answer-color1}{RGB}{250,234,241}    % Light pink background
\definecolor{quiz-answer-color2}{RGB}{152,14,90}      % Magenta border

% =============================================================================
% LIST FORMATTING
% =============================================================================
% Tighter list spacing for academic style
\def\tightlist{}
\setlist{itemsep=1pt, parsep=1pt, topsep=0pt,after={\vspace{0.3\baselineskip}}}
\let\tightlist\relax

\makeatletter
\@ifpackageloaded{framed}{}{\usepackage{framed}}
\@ifpackageloaded{fancyvrb}{}{\usepackage{fancyvrb}}
\makeatother

\makeatletter
%New float "codelisting" has been updated
\AtBeginDocument{%
\floatstyle{ruled}
\newfloat{codelisting}{!htb}{lop}
\floatname{codelisting}{Listing}
\floatplacement{codelisting}{!htb}
\captionsetup[codelisting]{labelfont={bf,ninept},labelformat=mylabel,
  singlelinecheck=false,width=\linewidth,labelsep=none,font={ninept}}%
\renewenvironment{snugshade}{%
   \def\OuterFrameSep{3pt}%
   \def\FrameCommand{\fboxsep=5pt\colorbox{shadecolor}}%
   \MakeFramed{\advance\hsize-\width\FrameRestore}%
   \leftskip 0.5em \rightskip 0.5em%
   \small% decrease font size
   }{\endMakeFramed}%
}
\makeatother

%The space before and after the verbatim environment "Highlighting" has been reduced
\fvset{listparameters=\setlength{\topsep}{0pt}\setlength{\partopsep}{0pt}}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{framesep=0mm,commandchars=\\\{\}}

\makeatletter
\renewcommand\fs@ruled{\def\@fs@cfont{\bfseries}\let\@fs@capt\floatc@ruled
\def\@fs@pre{\hrule height.8pt depth0pt \kern2pt}%
\def\@fs@post{\kern2pt\hrule\relax}%
\def\@fs@mid{\kern2pt\hrule\kern1pt}%space between float and caption
\let\@fs@iftopcapt\iftrue}
\makeatother


% =============================================================================
% HYPHENATION RULES
% =============================================================================
% Explicit hyphenation points for technical terms to avoid bad breaks
\hyphenation{
  light-weight
  light-weight-ed
  de-vel-op-ment
  un-der-stand-ing
  mod-els
  prin-ci-ples
  ex-per-tise
  com-pli-cat-ed
  blue-print
  per‧for‧mance
  com-mu-ni-ca-tion
  par-a-digms
  hy-per-ten-sion
  a-chieved
}

% =============================================================================
% CODE LISTING CONFIGURATION
% =============================================================================
% Settings for code blocks using listings package
\lstset{
breaklines=true,              % Automatic line wrapping
breakatwhitespace=true,       % Break at whitespace only
basicstyle=\ttfamily,         % Monospace font
frame=none,                   % No frame around code
keepspaces=true,              % Preserve spaces
showspaces=false,             % Don't show space characters
showtabs=false,               % Don't show tab characters
columns=flexible,             % Flexible column width
belowskip=0pt,               % Minimal spacing
aboveskip=0pt
}

% =============================================================================
% PAGE GEOMETRY
% =============================================================================
% MIT Press trim size: 7" x 10" (per publisher specifications)
% This is a standard academic textbook format providing good readability
% for technical content with figures and code blocks.
% Wide outer margin accommodates sidenotes/margin notes.
\geometry{
  paperwidth=7in,
  paperheight=10in,
  top=0.875in,
  bottom=0.875in,
  inner=0.875in,              % Inner margin (binding side)
  outer=1.75in,               % Outer margin (includes space for sidenotes)
  footskip=30pt,
  marginparwidth=1.25in,      % Width for margin notes
  twoside                     % Different left/right pages
}

% =============================================================================
% SIDENOTE STYLING
% =============================================================================
% Custom sidenote design with crimson vertical bar
\renewcommand{\thefootnote}{\textcolor{crimson}{\arabic{footnote}}}

% Save original sidenote command
\makeatletter
\@ifundefined{oldsidenote}{
  \let\oldsidenote\sidenote%
}{}
\makeatother

% Redefine sidenote with vertical crimson bar
\renewcommand{\sidenote}[1]{%
  \oldsidenote{%
    \noindent
    \color{crimson!100}                        % Crimson vertical line
    \raisebox{0em}{%
      \rule{0.5pt}{1.5em}                      % Thin vertical line
    }
    \hspace{0.3em}                             % Space after line
    \color{black}                              % Reset text color
    \footnotesize #1                           % Sidenote content
  }%
}

% =============================================================================
% FLOAT HANDLING
% =============================================================================
% Patch LaTeX's output routine to handle float overflow gracefully
% The "Float(s) lost" error occurs in \@doclearpage when \@currlist is not empty
% This patch silently clears pending floats that can't be placed
\makeatletter
\let\orig@doclearpage\@doclearpage
\def\@doclearpage{%
  \ifx\@currlist\@empty\else
    \global\let\@currlist\@empty
    \typeout{Warning: Floats cleared to prevent overflow}%
  \fi
  \orig@doclearpage
}
\makeatother

% Additional safety for structural commands
\let\originalbackmatter\backmatter
\renewcommand{\backmatter}{%
  \clearpage%
  \originalbackmatter%
}

\let\originalfrontmatter\frontmatter
\renewcommand{\frontmatter}{%
  \clearpage%
  \originalfrontmatter%
}

\let\originalmainmatter\mainmatter
\renewcommand{\mainmatter}{%
  \clearpage%
  \originalmainmatter%
}

% =============================================================================
% PAGE HEADERS AND FOOTERS
% =============================================================================
% Ensure chapters use fancy page style (not plain)
\patchcmd{\chapter}{\thispagestyle{plain}}{\thispagestyle{fancy}}{}{}

% Main page style with crimson headers
\pagestyle{fancy}
\fancyhf{}                                              % Clear all
\fancyhead[LE]{\small\color{crimson}\nouppercase{\rightmark}}  % Left even: section
\fancyhead[RO]{\color{crimson}\thepage}                 % Right odd: page number
\fancyhead[LO]{\small\color{crimson}\nouppercase{\leftmark}}   % Left odd: chapter
\fancyhead[RE]{\color{crimson}\thepage}                 % Right even: page number
\renewcommand{\headrulewidth}{0.4pt}                    % Thin header line
\renewcommand{\footrulewidth}{0pt}                      % No footer line

% Plain page style (for chapter openings)
\fancypagestyle{plain}{
  \fancyhf{}
  \fancyfoot[C]{\color{crimson}\thepage}                % Centered page number
  \renewcommand{\headrulewidth}{0pt}
  \renewcommand{\footrulewidth}{0pt}
}

% =============================================================================
% KOMA-SCRIPT FONT ADJUSTMENTS
% =============================================================================
% Apply crimson color to all heading levels
\addtokomafont{disposition}{\rmfamily\color{crimson}}
\addtokomafont{chapter}{\color{crimson}}
\addtokomafont{section}{\color{crimson}}
\addtokomafont{subsection}{\color{crimson}}

% =============================================================================
% ABSTRACT ENVIRONMENT
% =============================================================================
\newenvironment{abstract}{
  \chapter*{\abstractname}
  \addcontentsline{toc}{chapter}{\abstractname}
  \small
}{
  \clearpage
}

% =============================================================================
% HYPERLINK CONFIGURATION
% =============================================================================
% Crimson-colored links throughout, two-page PDF layout
\hypersetup{
  linkcolor=crimson,
  citecolor=crimson,
  urlcolor=crimson,
  pdfpagelayout=TwoPageRight,   % Two-page spread view
  pdfstartview=Fit               % Initial zoom fits page
}

% =============================================================================
% PART SUMMARY SYSTEM
% =============================================================================
% Allows adding descriptive text below part titles
\newcommand{\partsummary}{}     % Empty by default
\newif\ifhaspartsummary%
\haspartsummaryfalse%

\newcommand{\setpartsummary}[1]{%
  \renewcommand{\partsummary}{#1}%
  \haspartsummarytrue%
}

% Additional colors for part page backgrounds
\definecolor{BrownLL}{RGB}{233,222,220}
\definecolor{BlueDD}{RGB}{62,100,125}
\colorlet{BlueDD}{magenta}

% ===============================================================================
% PART STYLING SYSTEM
% ===============================================================================
%
% This system provides three distinct visual styles for book organization:
%
% 1. NUMBERED PARTS (\part{title}) - For main book sections
%    - Roman numerals (I, II, III, etc.) in top right corner
%    - Crimson title with horizontal lines above/below
%    - "Part I" label in sidebar
%    - Used for: foundations, principles, optimization, deployment, etc.
%
% 2. UNNUMBERED PARTS (\part*{title}) - For special sections like "Labs"
%    - Division-style geometric background (left side)
%    - No Roman numerals
%    - Used for: labs section
%
% 3. DIVISIONS (\division{title}) - For major book divisions
%    - Clean geometric background with centered title
%    - Used for: frontmatter, main_content, backmatter
%
% The Lua filter (inject-parts.lua) automatically routes parts by {key:xxx} commands
% to the appropriate LaTeX command based on the key name.
% ===============================================================================

% NUMBERED PARTS: Roman numeral styling for main book sections
\titleformat{\part}[display]
{\thispagestyle{empty}}{}{20pt}{
\begin{tikzpicture}[remember picture,overlay]
%%%
%%
\node[crimson,align=flush right,
inner sep=0,outer sep=0mm,draw=none,%
anchor=east,minimum height=31mm, text width=1.2\textwidth,
yshift=-30mm,font={%
\fontsize{98pt}{104}\selectfont\bfseries}]  (BG) at (current page text area.north east){\thepart};
%
\node[black,inner sep=0mm,draw=none,
anchor=mid,text width=1.2\textwidth,
 minimum height=35mm, align=right,
node distance=7mm,below=of BG,
font={\fontsize{30pt}{34}\selectfont}]
(BGG)  {\hyphenchar\font=-1 \color{black}\MakeUppercase {#1}};
\draw [crimson,line width=3pt] ([yshift=0mm]BGG.north west) -- ([yshift=0mm]BGG.north east);
\draw [crimson,line width=2pt] ([yshift=0mm]BGG.south west) -- ([yshift=0mm]BGG.south east);
%
\node[fill=crimson,text=white,rotate=90,%
anchor=south west,minimum height=15mm,
minimum width=40mm,font={%
\fontsize{20pt}{20}\selectfont\bfseries}](BP)  at
(current page text area.south east)
{{\sffamily Part}~\thepart};
%
\path[red](BP.north west)-|coordinate(PS)(BGG.south west);
%
% Part summary box commented out for cleaner design
% \ifhaspartsummary
% \node[inner sep=4pt,text width=0.7\textwidth,draw=none,fill=BrownLL!40,
% align=justify,font={\fontsize{9pt}{12}\selectfont},anchor=south west]
% at (PS) {\partsummary};
% \fi
\end{tikzpicture}
}[]

\renewcommand{\thepart}{\Roman{part}}

% UNNUMBERED PARTS: Division-style background for special sections
\titleformat{name=\part,numberless}[display]
{\thispagestyle{empty}}{}{20pt}{
\begin{tikzpicture}[remember picture,overlay]
%%%
\coordinate(S1)at([yshift=-200mm]current page.north west);
\draw[draw=none,fill=BlueDD!7](S1)--++(45:16)coordinate(S2)-
|(S2|-current page.north west)--(current page.north west)coordinate(S3)--(S1);
%
\coordinate(E1)at([yshift=-98mm]current page.north west);
\draw[draw=none,fill=BlueDD!15](E1)--(current page.north west)coordinate(E2)
--++(0:98mm)coordinate(E3)--(E1);
%
\coordinate(D1)at([yshift=15mm]current page.south west);
\draw[draw=none,fill=BlueDD!40,opacity=0.5](D1)--++(45:5.5)coordinate(D2)
-|(D2|-current page.north west)--(current page.north west)coordinate(D3)--(D1);
%%%%
\path[red](S2)-|(S2-|current page.east)coordinate(SS2);
%PART
\node[crimson,align=flush right,inner sep=0,outer sep=0mm,draw=none,anchor=south,
font={\fontsize{48pt}{48}\selectfont\bfseries}]  (BG) at ($(S2)!0.5!(SS2)$){\hphantom{Part}};
%%%
\path[green]([yshift=15mm]D2)-|coordinate(TPD)(BG.south east);
\node[inner sep=0mm,draw=none,anchor=south east,%text width=0.9\textwidth,
align=right,font={\fontsize{40pt}{40}\selectfont}]
(BGG) at (TPD)  {\color{crimson}\MakeUppercase {#1}};%\MakeUppercase {}
\end{tikzpicture}
}

% Define \numberedpart command for numbered parts
\newcommand{\numberedpart}[1]{%
\FloatBarrier%  % Flush all pending floats before part break
\clearpage
\thispagestyle{empty}
\stepcounter{part}%
\begin{tikzpicture}[remember picture,overlay]
%%%
%%
\node[crimson,align=flush right,
inner sep=0,outer sep=0mm,draw=none,%
anchor=east,minimum height=31mm, text width=1.2\textwidth,
yshift=-30mm,font={%
\fontsize{98pt}{104}\selectfont\bfseries}]  (BG) at (current page text area.north east){\thepart};
%
\node[black,inner sep=0mm,draw=none,
anchor=mid,text width=1.2\textwidth,
 minimum height=35mm, align=right,
node distance=7mm,below=of BG,
font={\fontsize{30pt}{34}\selectfont}]
(BGG)  {\hyphenchar\font=-1 \color{black}\MakeUppercase {#1}};
\draw [crimson,line width=3pt] ([yshift=0mm]BGG.north west) -- ([yshift=0mm]BGG.north east);
\draw [crimson,line width=2pt] ([yshift=0mm]BGG.south west) -- ([yshift=0mm]BGG.south east);
%
\node[fill=crimson,text=white,rotate=90,%
anchor=south west,minimum height=15mm,
minimum width=40mm,font={%
\fontsize{20pt}{20}\selectfont\bfseries}](BP)  at
(current page text area.south east)
{{\sffamily Part}~\thepart};
%
\path[red](BP.north west)-|coordinate(PS)(BGG.south west);
%
% Part summary box commented out for cleaner design
% \ifhaspartsummary
% \node[inner sep=4pt,text width=0.7\textwidth,draw=none,fill=BrownLL!40,
% align=justify,font={\fontsize{9pt}{12}\selectfont},anchor=south west]
% at (PS) {\partsummary};
% \fi
\end{tikzpicture}
\clearpage
}



% DIVISIONS: Clean geometric styling with subtle tech elements
% Used for frontmatter, main_content, and backmatter divisions
\newcommand{\division}[1]{%
\FloatBarrier%  % Flush all pending floats before division break
\clearpage
\thispagestyle{empty}
\begin{tikzpicture}[remember picture,overlay]

% Clean geometric background (original design)
\coordinate(S1)at([yshift=-200mm]current page.north west);
\draw[draw=none,fill=BlueDD!7](S1)--++(45:16)coordinate(S2)-
|(S2|-current page.north west)--(current page.north west)coordinate(S3)--(S1);

\coordinate(E1)at([yshift=-98mm]current page.north west);
\draw[draw=none,fill=BlueDD!15](E1)--(current page.north west)coordinate(E2)
--++(0:98mm)coordinate(E3)--(E1);

\coordinate(D1)at([yshift=15mm]current page.south west);
\draw[draw=none,fill=BlueDD!40,opacity=0.5](D1)--++(45:5.5)coordinate(D2)
-|(D2|-current page.north west)--(current page.north west)coordinate(D3)--(D1);

% Subtle tech elements - positioned in white areas for better visibility
% Upper right white area - more visible
\draw[crimson!40, line width=0.8pt] ([xshift=140mm,yshift=-60mm]current page.north west) -- ++(40mm,0);
\draw[crimson!40, line width=0.8pt] ([xshift=150mm,yshift=-70mm]current page.north west) -- ++(30mm,0);
\draw[crimson!35, line width=0.7pt] ([xshift=160mm,yshift=-60mm]current page.north west) -- ++(0,-15mm);
\draw[crimson!35, line width=0.7pt] ([xshift=170mm,yshift=-70mm]current page.north west) -- ++(0,10mm);

% Circuit nodes - upper right
\fill[crimson!50] ([xshift=160mm,yshift=-60mm]current page.north west) circle (1.5mm);
\fill[white] ([xshift=160mm,yshift=-60mm]current page.north west) circle (0.8mm);
\fill[crimson!50] ([xshift=170mm,yshift=-70mm]current page.north west) circle (1.3mm);
\fill[white] ([xshift=170mm,yshift=-70mm]current page.north west) circle (0.6mm);

% Lower right white area - enhanced visibility
\draw[crimson!45, line width=0.9pt] ([xshift=140mm,yshift=-190mm]current page.north west) -- ++(45mm,0);
\draw[crimson!45, line width=0.9pt] ([xshift=150mm,yshift=-200mm]current page.north west) -- ++(35mm,0);
\draw[crimson!40, line width=0.8pt] ([xshift=160mm,yshift=-190mm]current page.north west) -- ++(0,-20mm);
\draw[crimson!40, line width=0.8pt] ([xshift=170mm,yshift=-200mm]current page.north west) -- ++(0,15mm);

% Additional connecting lines in lower right
\draw[crimson!35, line width=0.7pt] ([xshift=130mm,yshift=-180mm]current page.north west) -- ++(25mm,0);
\draw[crimson!35, line width=0.7pt] ([xshift=145mm,yshift=-180mm]current page.north west) -- ++(0,-25mm);

% Circuit nodes - lower right (more prominent)
\fill[crimson!55] ([xshift=160mm,yshift=-190mm]current page.north west) circle (1.6mm);
\fill[white] ([xshift=160mm,yshift=-190mm]current page.north west) circle (0.9mm);
\fill[crimson!55] ([xshift=170mm,yshift=-200mm]current page.north west) circle (1.4mm);
\fill[white] ([xshift=170mm,yshift=-200mm]current page.north west) circle (0.7mm);
\fill[crimson!50] ([xshift=145mm,yshift=-180mm]current page.north west) circle (1.2mm);
\fill[white] ([xshift=145mm,yshift=-180mm]current page.north west) circle (0.6mm);

% Title positioned in center - clean and readable
\node[inner sep=0mm,draw=none,anchor=center,text width=0.8\textwidth,
align=center,font={\fontsize{40pt}{40}\selectfont}]
(BGG) at (current page.center)  {\color{crimson}\MakeUppercase {#1}};

\end{tikzpicture}
\clearpage
}

% LAB DIVISIONS: Circuit-style neural network design for lab sections
% Used specifically for lab platform sections (arduino, xiao, grove, etc.)
\newcommand{\labdivision}[1]{%
\FloatBarrier%  % Flush all pending floats before lab division break
\clearpage
\thispagestyle{empty}
\begin{tikzpicture}[remember picture,overlay]
% Circuit background with subtle gradient
\coordinate(S1)at([yshift=-200mm]current page.north west);
\draw[draw=none,fill=BlueDD!5](S1)--++(45:16)coordinate(S2)-
|(S2|-current page.north west)--(current page.north west)coordinate(S3)--(S1);

% TOP AREA: Circuit lines in upper white space
\draw[crimson!50, line width=1.5pt] ([xshift=30mm,yshift=-40mm]current page.north west) -- ++(60mm,0);
\draw[crimson!40, line width=1pt] ([xshift=120mm,yshift=-50mm]current page.north west) -- ++(50mm,0);
\draw[crimson!50, line width=1.5pt] ([xshift=40mm,yshift=-70mm]current page.north west) -- ++(40mm,0);

% Connecting lines in top area
\draw[crimson!30, line width=1pt] ([xshift=60mm,yshift=-40mm]current page.north west) -- ++(0,-20mm);
\draw[crimson!30, line width=1pt] ([xshift=145mm,yshift=-50mm]current page.north west) -- ++(0,10mm);

% Neural nodes in top area
\fill[crimson!70] ([xshift=60mm,yshift=-40mm]current page.north west) circle (2.5mm);
\fill[white] ([xshift=60mm,yshift=-40mm]current page.north west) circle (1.5mm);
\fill[crimson!60] ([xshift=145mm,yshift=-50mm]current page.north west) circle (2mm);
\fill[white] ([xshift=145mm,yshift=-50mm]current page.north west) circle (1mm);
\fill[crimson!80] ([xshift=80mm,yshift=-70mm]current page.north west) circle (2mm);
\fill[white] ([xshift=80mm,yshift=-70mm]current page.north west) circle (1mm);

% BOTTOM AREA: Circuit lines in lower white space
\draw[crimson!50, line width=1.5pt] ([xshift=20mm,yshift=-200mm]current page.north west) -- ++(70mm,0);
\draw[crimson!40, line width=1pt] ([xshift=110mm,yshift=-210mm]current page.north west) -- ++(60mm,0);
\draw[crimson!50, line width=1.5pt] ([xshift=35mm,yshift=-230mm]current page.north west) -- ++(45mm,0);

% Connecting lines in bottom area
\draw[crimson!30, line width=1pt] ([xshift=55mm,yshift=-200mm]current page.north west) -- ++(0,-20mm);
\draw[crimson!30, line width=1pt] ([xshift=140mm,yshift=-210mm]current page.north west) -- ++(0,15mm);

% Neural nodes in bottom area
\fill[crimson!70] ([xshift=55mm,yshift=-200mm]current page.north west) circle (2.5mm);
\fill[white] ([xshift=55mm,yshift=-200mm]current page.north west) circle (1.5mm);
\fill[crimson!60] ([xshift=140mm,yshift=-210mm]current page.north west) circle (2mm);
\fill[white] ([xshift=140mm,yshift=-210mm]current page.north west) circle (1mm);
\fill[crimson!80] ([xshift=80mm,yshift=-230mm]current page.north west) circle (2mm);
\fill[white] ([xshift=80mm,yshift=-230mm]current page.north west) circle (1mm);

% SIDE AREAS: Subtle circuit elements on left and right edges
\draw[crimson!30, line width=1pt] ([xshift=15mm,yshift=-120mm]current page.north west) -- ++(20mm,0);
\draw[crimson!30, line width=1pt] ([xshift=175mm,yshift=-130mm]current page.north west) -- ++(15mm,0);
\fill[crimson!50] ([xshift=25mm,yshift=-120mm]current page.north west) circle (1.5mm);
\fill[white] ([xshift=25mm,yshift=-120mm]current page.north west) circle (0.8mm);
\fill[crimson!50] ([xshift=185mm,yshift=-130mm]current page.north west) circle (1.5mm);
\fill[white] ([xshift=185mm,yshift=-130mm]current page.north west) circle (0.8mm);

% Title positioned in center - CLEAN AREA
\node[inner sep=0mm,draw=none,anchor=center,text width=0.8\textwidth,
align=center,font={\fontsize{44pt}{44}\selectfont\bfseries}]
(BGG) at (current page.center)  {\color{crimson}\MakeUppercase {#1}};

\end{tikzpicture}
\clearpage
}

% Define \lab command for lab styling (different visual treatment)
\newcommand{\lab}[1]{%
\begin{tikzpicture}[remember picture,overlay]
%%%
% Different background pattern for labs
\coordinate(S1)at([yshift=-200mm]current page.north west);
\draw[draw=none,fill=BlueDD!15](S1)--++(45:16)coordinate(S2)-
|(S2|-current page.north west)--(current page.north west)coordinate(S3)--(S1);
%
\coordinate(E1)at([yshift=-98mm]current page.north west);
\draw[draw=none,fill=BlueDD!25](E1)--(current page.north west)coordinate(E2)
--++(0:98mm)coordinate(E3)--(E1);
%
\coordinate(D1)at([yshift=15mm]current page.south west);
\draw[draw=none,fill=BlueDD!60,opacity=0.7](D1)--++(45:5.5)coordinate(D2)
-|(D2|-current page.north west)--(current page.north west)coordinate(D3)--(D1);
%%%%
\path[red](S2)-|(S2-|current page.east)coordinate(SS2);
%LAB - Different styling
\node[crimson,align=flush right,inner sep=0,outer sep=0mm,draw=none,anchor=south,
font={\fontsize{48pt}{48}\selectfont\bfseries}]  (BG) at ($(S2)!0.5!(SS2)$){\hphantom{Workshop}};
%%%
\path[green]([yshift=15mm]D2)-|coordinate(TPD)(BG.south east);
\node[inner sep=0mm,draw=none,anchor=south east,%text width=0.9\textwidth,
align=right,font={\fontsize{40pt}{40}\selectfont}]
(BGG) at (TPD)  {\color{crimson}\MakeUppercase {#1}};%\MakeUppercase {}
\end{tikzpicture}
\thispagestyle{empty}
\clearpage
}

% =============================================================================
% SECTION FORMATTING
% =============================================================================
% All section levels use crimson color and are ragged right

% Section (Large, bold, crimson)
\titleformat{\section}
  {\normalfont\Large\bfseries\color{crimson}\raggedright}
  {\thesection}
  {0.5em}
  {#1}
\titlespacing*{\section}{0pc}{14pt plus 4pt minus 4pt}{6pt plus 2pt minus 2pt}[0pc]

% Subsection (large, bold, crimson)
\titleformat{\subsection}
  {\normalfont\large\bfseries\color{crimson}\raggedright}
  {\thesubsection}
  {0.5em}
  {#1}
\titlespacing*{\subsection}{0pc}{12pt plus 4pt minus 4pt}{5pt plus 1pt minus 2pt}[0pc]

% Subsubsection (normal size, bold, crimson)
\titleformat{\subsubsection}
  {\normalfont\normalsize\bfseries\color{crimson}\raggedright}
  {\thesubsubsection}
  {0.5em}
  {#1}
\titlespacing*{\subsubsection}{0pc}{12pt plus 4pt minus 4pt}{5pt plus 1pt minus 2pt}[0pc]

% Paragraph (run-in, bold, crimson, ends with period)
\titleformat{\paragraph}[runin]
  {\normalfont\normalsize\bfseries\color{crimson}}
  {\theparagraph}
  {0.5em}
  {#1}
  [\textbf{.}]
  \titlespacing*{\paragraph}{0pc}{6pt plus 2pt minus 2pt}{0.5em}[0pc]

% Subparagraph (run-in, italic, crimson, ends with period)
\titleformat{\subparagraph}[runin]
  {\normalfont\normalsize\itshape\color{crimson}}
  {\thesubparagraph}
  {0.5em}
  {#1}
  [\textbf{.}]
  \titlespacing*{\subparagraph}{0pc}{6pt plus 2pt minus 2pt}{0.5em}[0pc]

% =============================================================================
% CHAPTER FORMATTING
% =============================================================================
% Numbered chapters: "Chapter X" prefix, huge crimson title
\titleformat{\chapter}[display]
  {\normalfont\huge\bfseries\color{crimson}}
  {\chaptername\ \thechapter}
  {20pt}
  {\Huge #1}
  []

% Unnumbered chapters: no prefix, huge crimson title
\titleformat{name=\chapter,numberless}
  {\normalfont\huge\bfseries\color{crimson}}
  {}
  {0pt}
  {\Huge #1}
  []

\renewcommand{\chaptername}{Chapter}
% =============================================================================
% TABLE OF CONTENTS FORMATTING
% =============================================================================
\setcounter{tocdepth}{2}                      % Show chapters, sections, subsections

% TOC spacing adjustments for number widths and indentation
\setlength{\cftchapnumwidth}{2em}             % Chapter number width
\setlength{\cftsecnumwidth}{2.75em}           % Section number width
\setlength{\cftsubsecnumwidth}{3.25em}        % Subsection number width
\setlength{\cftsubsubsecnumwidth}{4em}        % Subsubsection number width
\setlength{\cftsubsecindent}{4.25em}          % Subsection indent
\setlength{\cftsubsubsecindent}{7.5em}        % Subsubsection indent

% Chapter entries in TOC: bold crimson with "Chapter" prefix
\renewcommand{\cftchapfont}{\bfseries\color{crimson}}
\renewcommand{\cftchappresnum}{\color{crimson}Chapter~}

% Custom formatting for division entries (styled like parts)
\newcommand{\divisionchapter}[1]{%
  \addvspace{12pt}%
  \noindent\hfil\bfseries\color{crimson}#1\hfil\par%
  \addvspace{6pt}%
}

% Adjust TOC spacing for "Chapter" prefix
\newlength{\xtraspace}
\settowidth{\xtraspace}{\cftchappresnum\cftchapaftersnum}
\addtolength{\cftchapnumwidth}{\xtraspace}

% Unnumbered chapters with TOC entry
\newcommand{\likechapter}[1]{%
    \chapter*{#1}
    \addcontentsline{toc}{chapter}{\textcolor{crimson}{#1}}
}

% =============================================================================
% PAGE NUMBERING SYSTEM
% =============================================================================
% Implements traditional book numbering:
% - Roman numerals (i, ii, iii...) for frontmatter
% - Arabic numerals (1, 2, 3...) for mainmatter
% Automatically switches at first numbered chapter
\makeatletter
\newif\if@firstnumbered%
\@firstnumberedtrue%
\newif\if@firstunnumbered%
\@firstunnumberedtrue%

\newcounter{lastRomanPage}
\setcounter{lastRomanPage}{1}

% Start document with Roman numerals (frontmatter)
\AtBeginDocument{
  \pagenumbering{roman}
  \renewcommand{\thepage}{\roman{page}}
}

% Intercept chapter command
\let\old@chapter\chapter%
\renewcommand{\chapter}{%
  \@ifstar{\unnumbered@chapter}{\numbered@chapter}%
}

% Numbered chapters: switch to Arabic on first occurrence
\newcommand{\numbered@chapter}[1]{%
  \if@firstnumbered%
    \cleardoublepage%
    \setcounter{lastRomanPage}{\value{page}}%
    \pagenumbering{arabic}%
    \@firstnumberedfalse%
  \else
    \setcounter{page}{\value{page}}%
  \fi
  \setcounter{sidenote}{1}                    % Reset footnote counter per chapter
  \old@chapter{#1}%
}

% Unnumbered chapters: stay in Roman numerals
\newcommand{\unnumbered@chapter}[1]{%
  \if@firstunnumbered%
    \clearpage
    \setcounter{lastRomanPage}{\value{page}}%
    \pagenumbering{roman}%
    \@firstunnumberedfalse%
  \fi
  \setcounter{sidenote}{1}
  \old@chapter*{#1}%
}
\makeatother

% =============================================================================
% TABLE SIZING AND SPACING
% =============================================================================
% Make tables slightly smaller to fit more content
\AtBeginEnvironment{longtable}{\scriptsize}

% Increase vertical spacing in table cells (default is 1.0)
\renewcommand{\arraystretch}{1.3}

% Prefer placing tables at the top of pages
\makeatletter
\renewcommand{\fps@table}{t}  % Default placement: top of page
\makeatother

% =============================================================================
% LONGTABLE PAGE BREAKING FIXES (Windows compatibility)
% =============================================================================
% Prevent "Infinite glue shrinkage" errors on Windows LaTeX builds
% by giving longtable more flexibility in page breaking

% Allow more flexible page breaking (vs strict \flushbottom)
\raggedbottom

% Process more rows before attempting page break (default is 20)
\setcounter{LTchunksize}{50}

% Add extra stretch for longtable environments specifically
\AtBeginEnvironment{longtable}{%
  \setlength{\emergencystretch}{3em}%
  \setlength{\parskip}{0pt plus 1pt}%
}

% =============================================================================
% TABLE STYLING - Clean tables with crimson borders
% =============================================================================
% Professional table appearance with:
% - Clean white background (no colored rows)
% - Crimson-colored borders
% - Good spacing for readability
%
% Note: Headers are automatically bolded by Quarto when using **text** in source
\usepackage{booktabs}      % Professional table rules (\toprule, \midrule, \bottomrule)
\usepackage{colortbl}      % For colored borders (\arrayrulecolor)

% Global table styling - crimson borders
\setlength{\arrayrulewidth}{0.5pt}          % Thinner borders than default
%\arrayrulecolor{crimson}                    % Crimson borders matching brand

\setcounter{chapter}{0}
\usepackage{needspace}
\let\Needspace\needspace
\makeatletter
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{plain}
\@ifundefined{c@chapter}{\newfloat{vid}{h}{lovid}}{\newfloat{vid}{h}{lovid}[chapter]}
\floatname{vid}{Video}
\newcommand*\listofvids{\listof{vid}{List of Videos}}
\makeatother
\makeatletter
\@ifpackageloaded{tcolorbox}{}{\usepackage[skins,breakable]{tcolorbox}}
\@ifpackageloaded{fontawesome5}{}{\usepackage{fontawesome5}}
\definecolor{quarto-callout-color}{HTML}{909090}
\definecolor{quarto-callout-note-color}{HTML}{0758E5}
\definecolor{quarto-callout-important-color}{HTML}{CC1914}
\definecolor{quarto-callout-warning-color}{HTML}{EB9113}
\definecolor{quarto-callout-tip-color}{HTML}{00A047}
\definecolor{quarto-callout-caution-color}{HTML}{FC5300}
\definecolor{quarto-callout-color-frame}{HTML}{acacac}
\definecolor{quarto-callout-note-color-frame}{HTML}{4582ec}
\definecolor{quarto-callout-important-color-frame}{HTML}{d9534f}
\definecolor{quarto-callout-warning-color-frame}{HTML}{f0ad4e}
\definecolor{quarto-callout-tip-color-frame}{HTML}{02b875}
\definecolor{quarto-callout-caution-color-frame}{HTML}{fd7e14}
\makeatother
\makeatletter
\@ifpackageloaded{bookmark}{}{\usepackage{bookmark}}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\makeatletter
\@ifpackageloaded{sidenotes}{}{\usepackage{sidenotes}}
\@ifpackageloaded{marginnote}{}{\usepackage{marginnote}}
\makeatother
\newcommand{\fbxIconPath}{assets/images/icons/callouts}
\newcommand{\fbxIconFormat}{pdf}
\makeatletter
\@ifpackageloaded{tcolorbox}{}{\usepackage[many]{tcolorbox}}
\makeatother
%%%% ---foldboxy preamble ----- %%%%%

% Load xstring for string manipulation
\RequirePackage{xstring}

% Icon path and format configuration - can be overridden in filter-metadata
\providecommand{\fbxIconPath}{assets/images/icons/callouts}
\providecommand{\fbxIconFormat}{pdf}

% Helper command to include icon with hyphen-to-underscore conversion
% This ensures consistency: callout-quiz-question -> callout_quiz_question
\newcommand{\fbxIncludeIcon}[2]{%
  \StrSubstitute{#1}{-}{_}[\fbxIconName]%
  \includegraphics[width=#2]{\fbxIconPath/icon_\fbxIconName.\fbxIconFormat}%
}

% Legacy fallback colors (keep for compatibility)
\definecolor{fbx-default-color1}{HTML}{c7c7d0}
\definecolor{fbx-default-color2}{HTML}{a3a3aa}
\definecolor{fbox-color1}{HTML}{c7c7d0}
\definecolor{fbox-color2}{HTML}{a3a3aa}

% arguments: #1 typelabelnummer: #2 titel: #3
\newenvironment{fbx}[3]{%
\begin{tcolorbox}[
  enhanced,
  breakable,
  %fontupper=\fontsize{8pt}{10pt}\selectfont,  % 95% of body text (10pt -> 9.5pt)
  before skip=8pt,  % space above box (increased)
  after skip=8pt,   % space below box (increased)
  attach boxed title to top*={xshift=0pt},
  boxed title style={
  %fuzzy shadow={1pt}{-1pt}{0mm}{0.1mm}{gray},
  arc=1.5pt,
  rounded corners=north,
  sharp corners=south,
  top=6pt,          % Adjusted for ~40px equivalent height
  bottom=5pt,       % Adjusted for ~40px equivalent height
  overlay={
      \node [left,outer sep=0em, black,draw=none,anchor=west,
        rectangle,fill=none,inner sep=0pt]
        at ([xshift=4mm]frame.west) {\fbxIncludeIcon{#1}{4.2mm}};
    },
  },
  colframe=#1-color2,             % Border color (auto-generated from YAML)
  colbacktitle=#1-color1,         % Background color (auto-generated from YAML)
  colback=white,
  coltitle=black,
  titlerule=0mm,
  toprule=0.5pt,
  bottomrule=0.5pt,
  leftrule=2.2pt,
  rightrule=0.5pt,
  outer arc=1.5pt,
  arc=1.5pt,
  left=0.5em,       % increased left padding
  bottomtitle=1.5mm, % increased title bottom margin
  toptitle=1.5mm,    % increased title top margin
  title=\hspace{2.5em}\protect#2\hspace{0.2em}\protect#3, % Protect parameters
  extras middle and last={top=4pt} % increased continuation spacing
]}
{\end{tcolorbox}}


% boxed environment with right border
\newenvironment{fbxSimple}[3]{\begin{tcolorbox}[
  enhanced,
  breakable,
  %fontupper=\fontsize{8pt}{10pt}\selectfont,  % 95% of body text (10pt -> 9.5pt)
  before skip=8pt,  % space above box (increased)
  after skip=8pt,   % space below box (increased)
  attach boxed title to top*={xshift=0pt},
  boxed title style={
  %fuzzy shadow={1pt}{-1pt}{0mm}{0.1mm}{gray},
  arc=1.5pt,
  rounded corners=north,
  sharp corners=south,
  top=6pt,          % Adjusted for ~40px equivalent height
  bottom=5pt,       % Adjusted for ~40px equivalent height
  overlay={
      \node [left,outer sep=0em, black,draw=none,anchor=west,
        rectangle,fill=none,inner sep=0pt]
        at ([xshift=3mm]frame.west) {\fbxIncludeIcon{#1}{4.2mm}};
    },
  },
  colframe=#1-color2,             % Border color (auto-generated from YAML)
  colbacktitle=#1-color1,         % Background color (auto-generated from YAML)
  colback=white,
  coltitle=black,
  titlerule=0mm,
  toprule=0.5pt,
  bottomrule=0.5pt,
  leftrule=2.2pt,
  rightrule=0.5pt,
  outer arc=1.5pt,
  arc=1.5pt,
  left=0.5em,       % increased left padding
  bottomtitle=1.5mm, % increased title bottom margin
  toptitle=1.5mm,    % increased title top margin
  title=\hspace{2.5em}\protect#2\hspace{0.2em}\protect#3, % Protect parameters
  boxsep=1pt,
  extras first={bottom=0pt},
  extras last={top=0pt,bottom=-4pt},
  overlay first={
    \draw[line width=1pt,white] ([xshift=2.2pt]frame.south west)-- ([xshift=-0.5pt]frame.south east);
  },
  overlay last={
    \draw[line width=1pt,white] ([xshift=2.2pt]frame.north west)-- ([xshift=-0.5pt]frame.north east);
   }
]}
{\end{tcolorbox}}

%%%% --- end foldboxy preamble ----- %%%%%
%%==== colors from yaml ===%
\definecolor{callout-colab-color1}{HTML}{FFF5E6}
\definecolor{callout-colab-color2}{HTML}{FF6B35}
\definecolor{callout-resource-videos-color1}{HTML}{E0F2F1}
\definecolor{callout-resource-videos-color2}{HTML}{20B2AA}
\definecolor{callout-example-color1}{HTML}{F0F8F6}
\definecolor{callout-example-color2}{HTML}{148F77}
\definecolor{callout-code-color1}{HTML}{F2F4F8}
\definecolor{callout-code-color2}{HTML}{D1D7E0}
\definecolor{callout-notebook-color1}{HTML}{F2F7FF}
\definecolor{callout-notebook-color2}{HTML}{2C5282}
\definecolor{callout-resource-exercises-color1}{HTML}{E0F2F1}
\definecolor{callout-resource-exercises-color2}{HTML}{20B2AA}
\definecolor{callout-perspective-color1}{HTML}{F7F8FA}
\definecolor{callout-perspective-color2}{HTML}{4A5568}
\definecolor{callout-lighthouse-color1}{HTML}{FDF8E6}
\definecolor{callout-lighthouse-color2}{HTML}{B8860B}
\definecolor{callout-definition-color1}{HTML}{F0F4F8}
\definecolor{callout-definition-color2}{HTML}{1B4F72}
\definecolor{callout-checkpoint-color1}{HTML}{E8F5E9}
\definecolor{callout-checkpoint-color2}{HTML}{2E7D32}
\definecolor{callout-quiz-answer-color1}{HTML}{E8F2EA}
\definecolor{callout-quiz-answer-color2}{HTML}{4a7c59}
\definecolor{callout-resource-slides-color1}{HTML}{E0F2F1}
\definecolor{callout-resource-slides-color2}{HTML}{20B2AA}
\definecolor{callout-quiz-question-color1}{HTML}{F0F0F8}
\definecolor{callout-quiz-question-color2}{HTML}{5B4B8A}
\definecolor{callout-chapter-connection-color1}{HTML}{FDF2F7}
\definecolor{callout-chapter-connection-color2}{HTML}{A51C30}
%=============%

\usepackage{hyphenat}
\usepackage{ifthen}
\usepackage{calc}
\usepackage{calculator}



\usepackage{graphicx}
\usepackage{geometry}
\usepackage{afterpage}
\usepackage{tikz}
\usetikzlibrary{calc}
\usetikzlibrary{fadings}
\usepackage[pagecolor=none]{pagecolor}


% Set the titlepage font families







% Set the coverpage font families

\usepackage{bookmark}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  pdftitle={Machine Learning Systems},
  pdfauthor={Vijay Janapa Reddi},
  colorlinks=true,
  linkcolor={Maroon},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}


\title{Machine Learning Systems}
\usepackage{etoolbox}
\makeatletter
\providecommand{\subtitle}[1]{% add subtitle to \maketitle
  \apptocmd{\@title}{\par {\large #1 \par}}{}{}
}
\makeatother
\subtitle{Volume I: Introduction}
\author{Vijay Janapa Reddi}
\date{January 28, 2026}
\begin{document}
%%%%% begin titlepage extension code

  \begin{frontmatter}

\begin{titlepage}
% This is a combination of Pandoc templating and LaTeX
% Pandoc templating https://pandoc.org/MANUAL.html#templates
% See the README for help

\thispagestyle{empty}

\newgeometry{top=-100in}

% Page color

\newcommand{\coverauthorstyle}[1]{{\fontsize{20}{24.0}\selectfont
{#1}}}

\begin{tikzpicture}[remember picture, overlay, inner sep=0pt, outer sep=0pt]

\tikzfading[name=fadeout, inner color=transparent!0,outer color=transparent!100]
\tikzfading[name=fadein, inner color=transparent!100,outer color=transparent!0]
\node[anchor=south west, rotate=0, opacity=1] at ($(current page.south west)+(0.225\paperwidth, 9)$) {
\includegraphics[width=\paperwidth, keepaspectratio]{assets/images/covers/cover-image-transparent-vol1.png}};

% Title
\newcommand{\titlelocationleft}{0.075\paperwidth}
\newcommand{\titlelocationbottom}{0.4\paperwidth}
\newcommand{\titlealign}{left}

\begin{scope}{%
\fontsize{52}{62.4}\selectfont
\node[anchor=north
west, align=left, rotate=0] (Title1) at ($(current page.south west)+(\titlelocationleft,\titlelocationbottom)$)  [text width = 0.9\paperwidth]  {{\nohyphens{Machine
Learning Systems}}};
}
\end{scope}

% Author
\newcommand{\authorlocationleft}{.925\paperwidth}
\newcommand{\authorlocationbottom}{0.150\paperwidth}
\newcommand{\authoralign}{right}

\begin{scope}
{%
\fontsize{20}{24.0}\selectfont
\node[anchor=north
east, align=right, rotate=0] (Author1) at ($(current page.south west)+(\authorlocationleft,\authorlocationbottom)$)  [text width = 6in]  {\coverauthorstyle{Vijay\\Janapa
Reddi\\}};
}
\end{scope}

% Footer
\newcommand{\footerlocationleft}{0.075\paperwidth}
\newcommand{\footerlocationbottom}{0.475\paperwidth}
\newcommand{\footerlocationalign}{left}

\begin{scope}
{%
\fontsize{25}{30.0}\selectfont
 \node[anchor=north west, align=left, rotate=0] (Footer1) at %
($(current page.south west)+(\footerlocationleft,\footerlocationbottom)$)  [text width = 0.9\paperwidth]  {{\nohyphens{Volume
I: Introduction}}};
}
\end{scope}

\end{tikzpicture}
\clearpage
\restoregeometry
%%% TITLE PAGE START

% Set up alignment commands
%Page
\newcommand{\titlepagepagealign}{
\ifthenelse{\equal{left}{right}}{\raggedleft}{}
\ifthenelse{\equal{left}{center}}{\centering}{}
\ifthenelse{\equal{left}{left}}{\raggedright}{}
}


\newcommand{\titleandsubtitle}{
% Title and subtitle
{{\huge{\bfseries{\nohyphens{Machine Learning Systems}}}}\par
}%

\vspace{\betweentitlesubtitle}
{
{\large{\textit{\nohyphens{Volume I: Introduction}}}}\par
}}
\newcommand{\titlepagetitleblock}{
\titleandsubtitle
}

\newcommand{\authorstyle}[1]{{\large{#1}}}

\newcommand{\affiliationstyle}[1]{{\large{#1}}}

\newcommand{\titlepageauthorblock}{
{\authorstyle{\nohyphens{Vijay Janapa
Reddi}{\textsuperscript{1}}\textsuperscript{,}{\textsuperscript{,*}}}}}

\newcommand{\titlepageaffiliationblock}{
\hangindent=1em
\hangafter=1
{\affiliationstyle{
{1}.~Harvard University


\vspace{1\baselineskip}
* \textit{Correspondence:}~Vijay Janapa Reddi~vj@eecs.harvard.edu
}}
}
\newcommand{\headerstyled}{%
{}
}
\newcommand{\footerstyled}{%
{\large{}}
}
\newcommand{\datestyled}{%
{January 28, 2026}
}


\newcommand{\titlepageheaderblock}{\headerstyled}

\newcommand{\titlepagefooterblock}{
\footerstyled
}

\newcommand{\titlepagedateblock}{
\datestyled
}

%set up blocks so user can specify order
\newcommand{\titleblock}{\newlength{\betweentitlesubtitle}
\setlength{\betweentitlesubtitle}{0.05\textheight}
{

{\titlepagetitleblock}
}

\vspace{4\baselineskip}
}

\newcommand{\authorblock}{{\titlepageauthorblock}

\vspace{2\baselineskip}
}

\newcommand{\affiliationblock}{{\titlepageaffiliationblock}

\vspace{0pt}
}

\newcommand{\logoblock}{}

\newcommand{\footerblock}{}

\newcommand{\dateblock}{{\titlepagedateblock}

\vspace{0pt}
}

\newcommand{\headerblock}{}

\thispagestyle{empty} % no page numbers on titlepages


\newcommand{\vrulecode}{\textcolor{black}{\rule{\vrulewidth}{\textheight}}}
\newlength{\vrulewidth}
\setlength{\vrulewidth}{2pt}
\newlength{\B}
\setlength{\B}{\ifdim\vrulewidth > 0pt 0.05\textwidth\else 0pt\fi}
\newlength{\minipagewidth}
\ifthenelse{\equal{left}{left} \OR \equal{left}{right} }
{% True case
\setlength{\minipagewidth}{\textwidth - \vrulewidth - \B - 0.1\textwidth}
}{
\setlength{\minipagewidth}{\textwidth - 2\vrulewidth - 2\B - 0.1\textwidth}
}
\ifthenelse{\equal{left}{left} \OR \equal{left}{leftright}}
{% True case
\raggedleft % needed for the minipage to work
\vrulecode
\hspace{\B}
}{%
\raggedright % else it is right only and width is not 0
}
% [position of box][box height][inner position]{width}
% [s] means stretch out vertically; assuming there is a vfill
\begin{minipage}[b][\textheight][s]{\minipagewidth}
\titlepagepagealign
\titleblock

Prof.~Vijay Janapa Reddi

School of Engineering and Applied Sciences

Harvard University

\vspace{80mm}

With heartfelt gratitude to the community for their invaluable
contributions and steadfast support.

\vfill

January 28, 2026

\vfill
\par

\end{minipage}\ifthenelse{\equal{left}{right} \OR \equal{left}{leftright} }{
\hspace{\B}
\vrulecode}{}
\clearpage
%%% TITLE PAGE END
\end{titlepage}
\setcounter{page}{1}
\end{frontmatter}

%%%%% end titlepage extension code

\renewcommand*\contentsname{Table of contents}
{
\hypersetup{linkcolor=}
\setcounter{tocdepth}{2}
\tableofcontents
}

\mainmatter
\bookmarksetup{startatroot}

\chapter*{Welcome to Volume I}\label{welcome-to-volume-i}
\addcontentsline{toc}{chapter}{Welcome to Volume I}

\markboth{Welcome to Volume I}{Welcome to Volume I}

\section*{What You Will Learn}\label{what-you-will-learn}
\addcontentsline{toc}{section}{What You Will Learn}

\markright{What You Will Learn}

Volume I progresses through four stages:

\begin{itemize}
\tightlist
\item
  \textbf{Part I: Foundations} --- Build your conceptual foundation with
  mental models that underpin all effective systems work.
\item
  \textbf{Part II: Build} --- Engineer complete workflows from data
  pipelines through training infrastructure.
\item
  \textbf{Part III: Optimize} --- Transform theoretical understanding
  into systems that run efficiently in resource-constrained
  environments.
\item
  \textbf{Part IV: Deploy} --- Navigate serving, operations, and
  responsible engineering practices.
\end{itemize}

\section*{Prerequisites}\label{prerequisites}
\addcontentsline{toc}{section}{Prerequisites}

\markright{Prerequisites}

This volume assumes:

\begin{itemize}
\tightlist
\item
  \textbf{Programming proficiency} in Python with familiarity in NumPy
\item
  \textbf{Mathematics foundations} in linear algebra, calculus, and
  probability at the undergraduate level
\item
  Prior ML experience is helpful but not required;
  \textbf{?@sec-deep-learning-systems-foundations} provides essential
  background
\end{itemize}

\section*{Support Our Mission}\label{support-our-mission}
\addcontentsline{toc}{section}{Support Our Mission}

\markright{Support Our Mission}

\section*{Continue Your Journey}\label{continue-your-journey}
\addcontentsline{toc}{section}{Continue Your Journey}

\markright{Continue Your Journey}

\section*{Listen to the AI Podcast}\label{listen-to-the-ai-podcast}
\addcontentsline{toc}{section}{Listen to the AI Podcast}

\markright{Listen to the AI Podcast}

\section*{Want to Help Out?}\label{want-to-help-out}
\addcontentsline{toc}{section}{Want to Help Out?}

\markright{Want to Help Out?}

This is a collaborative project, and your input matters. If you'd like
to contribute, check out our
\href{https://github.com/harvard-edge/cs249r_book/blob/dev/docs/contribute.md}{contribution
guidelines}. Feedback, corrections, and new ideas are welcome. Simply
file a GitHub
\href{https://github.com/harvard-edge/cs249r_book/issues}{issue}.

\bookmarksetup{startatroot}

\chapter{Model Compression}\label{sec-model-compression}

\marginnote{\begin{footnotesize}

\emph{DALL·E 3 Prompt: Illustration of a neural network model
represented as a busy construction site, with a diverse group of
construction workers, both male and female, of various ethnicities,
labeled as `pruning', `quantization', and `sparsity'. They are working
together to make the neural network more efficient and smaller, while
maintaining high accuracy. The `pruning' worker, a Hispanic female, is
cutting unnecessary connections from the middle of the network. The
`quantization' worker, a Caucasian male, is adjusting or tweaking the
weights all over the place. The `sparsity' worker, an African female, is
removing unnecessary nodes to shrink the model. Construction trucks and
cranes are in the background, assisting the workers in their tasks. The
neural network is visually transforming from a complex and large
structure to a more streamlined and smaller one.}

\end{footnotesize}}

\noindent
\pandocbounded{\includegraphics[keepaspectratio]{contents/vol1/optimizations/images/png/cover_model_optimizations.png}}

\section*{Purpose}\label{purpose}
\addcontentsline{toc}{section}{Purpose}

\markright{Purpose}

\emph{Why do the models that win benchmarks rarely become the models
that run in production?}

Research optimizes for accuracy on held-out test sets; production
optimizes for accuracy per dollar, accuracy per watt, accuracy per
millisecond. These objectives diverge dramatically. The model that
achieves state-of-the-art performance typically does so by being larger,
slower, and more resource-intensive than any production constraint
permits. A research breakthrough that improves accuracy by 2\% while
tripling compute requirements is a step backward for deployment, where
the original model already exceeded accuracy requirements and the
bottleneck was latency or cost. This gap between research achievement
and deployment viability is not a failure of either community but a
reflection of different optimization targets. Model compression exists
to bridge this gap---to transform research artifacts into production
assets by systematically trading capabilities the deployment does not
need for constraints the deployment cannot violate. The discipline is
not about making models smaller but about making the right models
possible.

\begin{tcolorbox}[enhanced jigsaw, coltitle=black, breakable, colback=white, bottomtitle=1mm, colframe=quarto-callout-tip-color-frame, opacitybacktitle=0.6, bottomrule=.15mm, left=2mm, opacityback=0, arc=.35mm, toptitle=1mm, colbacktitle=quarto-callout-tip-color!10!white, titlerule=0mm, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Learning Objectives}, toprule=.15mm, rightrule=.15mm, leftrule=.75mm]

\begin{itemize}
\item
  Explain the tripartite optimization framework and how model
  representation, numerical precision, and architectural efficiency
  address different deployment constraints
\item
  Apply pruning techniques to reduce model parameters while quantifying
  the accuracy-sparsity trade-off through L0-norm optimization
\item
  Evaluate quantization strategies by comparing precision formats in
  terms of memory reduction, energy consumption, and inference accuracy
\item
  Implement knowledge distillation to transfer capabilities from large
  teacher models to efficient student architectures
\item
  Analyze hardware-aware design principles to align model operations
  with target platform capabilities and memory hierarchies
\item
  Design integrated optimization pipelines combining multiple techniques
  to achieve deployment objectives under resource constraints
\item
  Assess AutoML approaches for automating optimization decisions and
  discovering technique combinations beyond manual exploration
\end{itemize}

\end{tcolorbox}

\section{Model Optimization
Fundamentals}\label{sec-model-compression-model-optimization-fundamentals-c0d7}

Part II established how to build ML systems---architectures, frameworks,
and training pipelines that transform data into capable models. This
chapter opens Part III by addressing what happens next: optimizing the
\emph{outputs} of training to transform capable models into deployable
ones that meet real-world constraints.

The models that win benchmarks rarely become the models that run in
production. Model compression bridges this gap by systematically trading
capabilities the deployment does not need for constraints the deployment
cannot violate.

As established in the \textbf{Silicon Contract}
(\textbf{?@sec-silicon-contract}), every model architecture makes
assumptions about available hardware resources. Model compression is the
systematic process of renegotiating that contract---reducing memory
footprint, computational cost, and energy consumption while preserving
the model's ability to perform its task.

\phantomsection\label{callout-definitionux2a-1.1}
\begin{fbx}{callout-definition}{Definition: }{Model Compression}
\phantomsection\label{callout-definition*-1.1}
\textbf{Model Compression} refers to the systematic reduction of a
trained model's \emph{size}, \emph{computational requirements}, and
\emph{memory footprint} while preserving acceptable \emph{task
performance}, enabling deployment on resource-constrained hardware.
Compression techniques---including pruning, quantization, and knowledge
distillation---transform research-scale models into production-ready
artifacts by removing redundancy, reducing numerical precision, and
transferring knowledge to more efficient architectures.

\end{fbx}

\phantomsection\label{quiz-question-sec-model-compression-model-optimization-fundamentals-c0d7}
\begin{fbx}{callout-quiz-question}{Self-Check: Question 1.1}{}
\phantomsection\label{quiz-question-sec-model-compression-model-optimization-fundamentals-c0d7}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Which of the following best describes the primary goal of model
  optimization in machine learning systems?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii})}
  \tightlist
  \item
    Maximize model accuracy regardless of resource constraints.
  \item
    Reduce the size of the model to the smallest possible footprint.
  \item
    Achieve efficient execution in target environments while maintaining
    accuracy and functionality.
  \item
    Increase the complexity of the model to improve performance.
  \end{enumerate}
\item
  Explain how model optimization techniques like quantization and
  pruning contribute to efficient deployment of machine learning models.
\item
  What is a common challenge when deploying sophisticated machine
  learning models on mobile devices?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii})}
  \tightlist
  \item
    Excessive computational throughput
  \item
    Unlimited thermal constraints
  \item
    High latency requirements
  \item
    Limited memory and energy resources
  \end{enumerate}
\item
  True or False: Model optimization only focuses on reducing the
  computational complexity of machine learning models.
\item
  In a production system, what trade-offs might you consider when
  implementing model optimization techniques?
\end{enumerate}

\noindent\hspace*{1.25em}\hyperref[quiz-answer-sec-model-compression-model-optimization-fundamentals-c0d7]{\textbf{See Answer~$\rightarrow$}}

\end{fbx}

\section{Optimization
Framework}\label{sec-model-compression-optimization-framework-9e21}

Model optimization is not a single technique but a \emph{framework} with
three complementary dimensions, each addressing different bottlenecks.
Figure~\ref{fig-3-sections} illustrates how these dimensions span from
pure software concerns to hardware-level execution.

\begin{figure}[htb]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/c249d5f12cf52ecb88d27345bbfef7634b804986.pdf}}

}

\caption{\label{fig-3-sections}\textbf{Optimization Stack}: Model
optimization progresses through three layers: efficient model
representation, efficient numerics representation, and efficient
hardware implementation.}

\end{figure}%

\textbf{1. Efficient Model Representation:} Focuses on eliminating
redundancy in the model structure. Techniques like \textbf{pruning},
\textbf{knowledge distillation}, and \textbf{Neural Architecture Search
(NAS)} reduce the number of parameters or operations required,
addressing memory footprint and computational complexity at the
algorithmic level.

\textbf{2. Efficient Numerics Representation:} Optimizes how numerical
values are stored and processed. \textbf{Quantization} and
\textbf{mixed-precision} training reduce the bit-width of weights and
activations (e.g., from 32-bit floating point to 8-bit integers),
enabling faster execution and lower memory usage on specialized
hardware.

\phantomsection\label{callout-perspectiveux2a-1.2}
\begin{fbx}{callout-perspective}{Systems Perspective: }{The Physics of Quantization}
\phantomsection\label{callout-perspective*-1.2}
Why do we obsess over quantization? Because in the physics of silicon,
\textbf{bits represent energy}.

According to the \textbf{Iron Law}
(\(L = \frac{D}{B} + \frac{Ops}{P \cdot \eta} + L_{fixed}\)), reducing
the bit-width of a weight has a quadratic effect on efficiency:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Memory Energy (\(D\))}: Fetching a 32-bit float from DRAM
  costs \(\approx\) \textbf{640 pJ}. Fetching an 8-bit integer costs
  \(\approx\) \textbf{160 pJ}.
\item
  \textbf{Compute Energy (\(Ops\))}: A 32-bit FLOP costs \(\approx\)
  \textbf{4 pJ}. An 8-bit integer OP costs \(\approx\) \textbf{0.2 pJ}.
\end{enumerate}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2778}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2222}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3056}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Operation}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Bit-Width}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Relative Energy}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Integer Add} & 8-bit & 1x \\
\textbf{Float Add} & 32-bit & 30x \\
\textbf{Memory Access} & 64-bit & \textbf{2000x} \\
\end{longtable}

\textbf{For Inference}: Moving from FP32 to INT8 doesn't just save 4×
memory; it can reduce the \textbf{energy per inference} by nearly
\textbf{20×} if the hardware supports it. This is the difference between
a battery lasting 1 hour or 20 hours.

These same physics apply at datacenter scale: distributed training
systems use reduced precision to cut gradient communication overhead, a
topic covered in \textbf{?@sec-ai-training}.

\end{fbx}

These physics-level savings translate directly into deployment
capabilities. A model that cannot fit on a device at full precision may
run comfortably, and faster, when quantized. The following calculation
demonstrates this for a concrete LLM deployment scenario.

\phantomsection\label{callout-perspectiveux2a-1.3}
\begin{fbx}{callout-perspective}{Systems Perspective: }{Napkin Math: The Quantization Speedup}
\phantomsection\label{callout-perspective*-1.3}
\textbf{Problem}: You want to deploy a 7B parameter LLM on a device with
16GB RAM. The weights are FP16 (2 bytes).

\textbf{The Math}: 1. \textbf{Model Size}:
\(7 \times 10^9 \text{ params} \times 2 \text{ bytes} = 14 \text{ GB}\).
2. \textbf{KV Cache}: Context window (4096 tokens) requires
\(\approx 1 \text{ GB}\). 3. \textbf{Total Memory}:
\(14 + 1 = 15 \text{ GB}\). This barely fits, leaving no room for OS or
buffers. 4. \textbf{Bandwidth Cost}: Loading 14GB at 50 GB/s takes
\textbf{280 ms} per token. That is 3.5 tokens/sec---too slow for chat.

\textbf{The Fix (INT4)}: 1. \textbf{Quantization}: Convert weights to
4-bit integers (0.5 bytes). 2. \textbf{New Size}:
\(7 \times 10^9 \times 0.5 = 3.5 \text{ GB}\). 3. \textbf{New Speed}:
Loading 3.5GB takes \textbf{70 ms}. Speed jumps to \textbf{14
tokens/sec}.

\textbf{The Conclusion}: Quantization is not just about ``fitting'' the
model; it is a \textbf{4x Linear Speedup} because LLM generation is
bandwidth-bound.

\end{fbx}

\textbf{3. Efficient Hardware Implementation:} Ensures operations run
efficiently on target processors. Techniques like \textbf{operator
fusion}, \textbf{sparsity exploitation}, and \textbf{hardware-aware
scheduling} align computational patterns with hardware capabilities
(memory hierarchy, vector units) to maximize utilization and throughput.

These dimensions are interdependent. Pruning reduces complexity but may
require architectural changes for hardware efficiency. Quantization
reduces precision but impacts execution logic. The most effective
strategies combine techniques across all three layers. For practitioners
seeking immediate guidance on which techniques to apply,
Section~\ref{sec-model-compression-decision-framework-1896} provides a
decision framework that maps deployment constraints to specific
technique recommendations. The intervening sections provide the
technical foundation needed to apply that framework effectively.

\phantomsection\label{quiz-question-sec-model-compression-optimization-framework-9e21}
\begin{fbx}{callout-quiz-question}{Self-Check: Question 1.2}{}
\phantomsection\label{quiz-question-sec-model-compression-optimization-framework-9e21}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Which of the following layers in the optimization stack primarily
  focuses on aligning computation patterns with processor designs?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii})}
  \tightlist
  \item
    Efficient Model Representation
  \item
    Efficient Numerics Representation
  \item
    Efficient Data Handling
  \item
    Efficient Hardware Implementation
  \end{enumerate}
\item
  Explain how model representation techniques such as pruning and
  distillation can create opportunities for numerical precision
  optimization.
\item
  In the context of the optimization framework, what is the primary
  benefit of using quantization techniques?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii})}
  \tightlist
  \item
    Increasing model accuracy
  \item
    Reducing computational cost
  \item
    Enhancing data privacy
  \item
    Improving data collection
  \end{enumerate}
\item
  True or False: The optimization framework's effectiveness is
  independent of the target hardware characteristics.
\end{enumerate}

\noindent\hspace*{1.25em}\hyperref[quiz-answer-sec-model-compression-optimization-framework-9e21]{\textbf{See Answer~$\rightarrow$}}

\end{fbx}

\section{Deployment
Context}\label{sec-model-compression-deployment-context-0d88}

Optimization requirements vary dramatically across deployment contexts.
What matters for cloud inference differs fundamentally from mobile or
embedded systems. Table~\ref{tbl-deployment-scenarios} summarizes the
key constraints across deployment environments.

\subsection{Deployment
Scenarios}\label{sec-model-compression-deployment-scenarios-70c9}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2278}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.1646}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.1772}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.1519}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2405}}@{}}
\caption{\textbf{Deployment Constraints}: Each deployment context
imposes different optimization
priorities.}\label{tbl-deployment-scenarios}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Context}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Memory}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Latency}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Power}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Primary Goal}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Context}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Memory}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Latency}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Power}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Primary Goal}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Cloud} & 10s GB & 10-100ms & Flexible & Throughput, cost \\
\textbf{Mobile/Edge} & 100s MB--GB & 10-50ms & Watts & Size, latency \\
\textbf{TinyML} & KB--MB & 1-10ms & mW & Size, energy \\
\end{longtable}

\textbf{Cloud inference} optimizes for throughput
(requests/second/dollar). Quantization enables serving more concurrent
requests; operator fusion reduces per-request latency
(\citeproc{ref-choudhary2020comprehensive}{Choudhary et al. 2020};
\citeproc{ref-dean2018new}{Dean, Patterson, and Young 2018}).
\textbf{Mobile and edge} deployments must fit device memory while
meeting real-time targets. A camera app processing 30 fps has 33ms per
frame---any optimization reducing inference below this threshold
directly improves user experience.

\textbf{TinyML}\sidenote{\textbf{Microcontroller Constraints}:
Microcontrollers operate under severe constraints relative to servers
and modern accelerators, often with \emph{kilobytes to low megabytes} of
RAM and limited persistent storage. A practical mental model is that you
may have (10\^{}3) to (10\^{}6) bytes of memory available for the entire
pipeline, which is why ``model optimization'' is often a prerequisite
rather than an optional improvement in embedded deployments. } makes
optimization existential, not optional. A microcontroller with 256KB RAM
cannot run a 100MB model regardless of accuracy. The model must compress
below hardware limits or deployment is impossible
(\citeproc{ref-banbury2020benchmarking}{Banbury et al. 2020}).

Optimization also contributes to sustainable and accessible AI
deployment. Reducing a model's energy footprint is important as AI
workloads scale, helping mitigate the environmental impact of
large-scale ML training and inference
(\citeproc{ref-patterson2021carbon}{Patterson et al. 2021}). At the same
time, optimized models can expand the reach of machine learning,
supporting applications in low-resource environments, from rural
healthcare to autonomous systems operating in the field.

\subsection{Balancing
Trade-offs}\label{sec-model-compression-balancing-tradeoffs-6ae3}

The tension between accuracy and efficiency drives optimization
decisions across all dimensions. Increasing model capacity generally
enhances predictive performance while increasing computational cost,
resulting in slower, more resource-intensive inference. These
improvements introduce challenges related to memory
footprint\sidenote{Memory bandwidth (introduced in
\textbf{?@sec-introduction}) constrains how fast data moves between
memory and processors. For compression, bandwidth differences across
deployment targets matter: datacenter accelerators reach TB/s while
mobile devices achieve only tens of GB/s. Compression techniques that
reduce memory traffic often yield larger speedups than those that only
reduce computation. }, inference latency, power consumption, and
training efficiency. As machine learning systems are deployed across a
wide range of hardware platforms, balancing accuracy and efficiency
becomes a key challenge in model optimization.

This tension manifests differently across deployment contexts. Training
requires computational resources that scale with model size, while
inference demands strict latency and power constraints in real-time
applications.

The following sections examine each optimization dimension in depth:
structural methods that modify what computations occur, precision
techniques that reduce numerical fidelity, and architectural approaches
that improve execution efficiency. After covering these techniques, we
provide a practical guide for selecting and combining them based on
deployment constraints.

\phantomsection\label{quiz-question-sec-model-compression-deployment-context-0d88}
\begin{fbx}{callout-quiz-question}{Self-Check: Question 1.3}{}
\phantomsection\label{quiz-question-sec-model-compression-deployment-context-0d88}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Which of the following is a primary constraint when deploying machine
  learning models on microcontrollers?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii})}
  \tightlist
  \item
    High memory bandwidth
  \item
    Limited computational resources
  \item
    Large storage capacity
  \item
    Unlimited power supply
  \end{enumerate}
\item
  True or False: In cloud environments, optimizing machine learning
  models primarily focuses on reducing the model's memory footprint.
\item
  Explain why balancing accuracy and efficiency is crucial when
  deploying machine learning models on edge devices.
\item
  In the context of deployment, the term `\_\_\_\_' refers to the
  computational paradigm where ML inference occurs on local devices
  rather than cloud servers.
\item
  In a production system, how might you address the trade-off between
  model complexity and energy efficiency?
\end{enumerate}

\noindent\hspace*{1.25em}\hyperref[quiz-answer-sec-model-compression-deployment-context-0d88]{\textbf{See Answer~$\rightarrow$}}

\end{fbx}

\section{Structural Model Optimization
Methods}\label{sec-model-compression-structural-model-optimization-methods-8ed4}

We begin with the first dimension of our optimization framework:
\textbf{Efficient Model Representation}. This dimension addresses
\emph{what} to compute by modifying neural network structure and
parameters. Modern models often prioritize accuracy over efficiency,
containing excessive parameters that increase costs and slow inference.
This optimization addresses inefficiencies through two objectives:
eliminating redundancy (exploiting overparameterization where models
achieve similar performance with fewer parameters) and structuring
computations for efficient hardware execution through techniques like
gradient checkpointing\sidenote{\textbf{Gradient Checkpointing}: Memory
optimization technique that trades computation for memory by recomputing
intermediate activations during backpropagation instead of storing them.
Reduces memory usage by 20-50\% in transformer models, enabling larger
batch sizes or model sizes within same GPU memory. } and parallel
processing patterns\sidenote{\textbf{Parallel Processing in ML}:
Datacenter GPUs and other accelerators provide massive parallelism and
specialized matrix primitives compared to general-purpose CPUs. For
highly parallelizable ML workloads, accelerators can deliver
orders-of-magnitude higher throughput, although realized speedups depend
on kernel mix, memory behavior, and software stack. }.

The optimization challenge lies in balancing competing
constraints\sidenote{\textbf{Pareto Frontier}: Named after Italian
economist Vilfredo Pareto (1848-1923), who observed that 80\% of Italian
land was owned by 20\% of the population. In optimization, the Pareto
frontier represents solutions where improving one objective (speed)
requires sacrificing another (accuracy). EfficientNet traces this
frontier: B0 (77.1\%, 390M FLOPs) to B7 (84.4\%, 37B FLOPs).
Multi-objective NAS explicitly optimizes for Pareto-optimal
architectures. }. Aggressive compression risks accuracy degradation that
renders models unreliable for production use, while insufficient
optimization leaves models too large or slow for target deployment
environments. Selecting appropriate techniques requires understanding
trade-offs between model size, computational complexity, and
generalization performance.

Three key techniques address this challenge: pruning eliminates
low-impact parameters, knowledge distillation transfers capabilities to
smaller models, and NAS automates architecture design for specific
constraints. Each technique offers distinct optimization pathways while
maintaining model performance.

These three techniques represent distinct but complementary approaches
within our optimization framework. Pruning and knowledge distillation
reduce redundancy in existing models, while NAS addresses building
optimized architectures from the ground up. In many cases, they can be
combined to achieve even greater optimization.

\subsection{Pruning}\label{sec-model-compression-pruning-2b06}

Pruning\sidenote{\textbf{Pruning}: Borrowed from horticulture, where
gardeners prune branches to improve plant health and growth. The ML
metaphor fits precisely: just as removing unproductive branches
redirects resources to productive growth, removing low-magnitude weights
redirects computational resources to parameters that matter. The
technique dates to optimal brain damage
(\citeproc{ref-lecun1990optimal}{LeCun, Denker, and Solla 1989}), which
formalized the gardener's intuition with second-derivative analysis. }
directly addresses memory efficiency constraints by eliminating
redundant parameters. Modern neural networks are heavily
overparameterized, often containing far more weights than are strictly
necessary to solve a task. This redundancy allows us to remove a
significant fraction of weights without substantial performance
degradation. As we will explore in \textbf{?@sec-ai-acceleration},
specialized hardware can further exploit these sparse structures.

\phantomsection\label{callout-definitionux2a-1.4}
\begin{fbx}{callout-definition}{Definition: }{Pruning}
\phantomsection\label{callout-definition*-1.4}
\textbf{Pruning} refers to a model optimization technique that removes
\emph{redundant parameters} from neural networks while preserving
\emph{performance}, reducing \emph{model size} and \emph{computational
cost} for efficient deployment.

\end{fbx}

The goal of pruning is to find a sparse version of the model parameters
\(\hat{W}\) that minimizes the increase in prediction error (loss) while
satisfying a fixed parameter budget \(k\). This is formalized as an
optimization problem:

\[
\min_{\hat{W}} \mathcal{L}(\hat{W}) \quad \text{subject to} \quad \|\hat{W}\|_0 \leq k
\]

where \(\|\hat{W}\|_0\) is the \textbf{L0-norm} (the count of non-zero
parameters). Since minimizing the L0-norm is
NP-hard\sidenote{\textbf{NP-hard}: From computational complexity theory,
``NP'' stands for ``nondeterministic polynomial time.'' A problem is
NP-hard if solving it in polynomial time would imply P=NP, widely
believed false. Finding the optimal sparse subnetwork requires examining
exponentially many subsets, making exact solutions infeasible for
networks with millions of parameters. }, we use
heuristics\sidenote{\textbf{Heuristic}: From Greek ``heuriskein'' (to
discover or find), the same root as ``eureka.'' Heuristics are practical
methods that find good solutions without guarantees of optimality.
Magnitude-based pruning assumes larger weights are more important, a
reasonable heuristic that works well empirically even though
counterexamples exist. } like \textbf{magnitude-based pruning}.
Listing~\ref{lst-pruning_example} demonstrates this approach, removing
weights with small absolute values to transform a dense weight matrix
into the sparse representation shown in Figure~\ref{fig-sparse-matrix}.

\begin{codelisting}

\caption{\label{lst-pruning_example}\textbf{Magnitude-Based Pruning}:
Removes weights below a threshold to create sparse matrices, reducing
the number of nonzero parameters from 9 to 4 (\(k=4\)).}

\centering{

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ torch}

\CommentTok{\# Original dense weight matrix}
\NormalTok{weights }\OperatorTok{=}\NormalTok{ torch.tensor(}
\NormalTok{    [[}\FloatTok{0.8}\NormalTok{, }\FloatTok{0.1}\NormalTok{, }\OperatorTok{{-}}\FloatTok{0.7}\NormalTok{], [}\FloatTok{0.05}\NormalTok{, }\OperatorTok{{-}}\FloatTok{0.9}\NormalTok{, }\FloatTok{0.03}\NormalTok{], [}\OperatorTok{{-}}\FloatTok{0.6}\NormalTok{, }\FloatTok{0.02}\NormalTok{, }\FloatTok{0.4}\NormalTok{]]}
\NormalTok{)}

\CommentTok{\# Simple magnitude{-}based pruning: keep only the 4 largest weights}
\NormalTok{threshold }\OperatorTok{=} \FloatTok{0.5}
\NormalTok{mask }\OperatorTok{=}\NormalTok{ torch.}\BuiltInTok{abs}\NormalTok{(weights) }\OperatorTok{\textgreater{}=}\NormalTok{ threshold}
\NormalTok{pruned\_weights }\OperatorTok{=}\NormalTok{ weights }\OperatorTok{*}\NormalTok{ mask}

\BuiltInTok{print}\NormalTok{(}\StringTok{"Original:"}\NormalTok{, weights)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Pruned (4 non{-}zeros):"}\NormalTok{, pruned\_weights)}
\end{Highlighting}
\end{Shaded}

}

\end{codelisting}%

\begin{figure}[htb]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/b9c6ca7093829e7949c3c7e35695873897e77975.pdf}}

}

\caption{\label{fig-sparse-matrix}\textbf{Sparse Matrix Transformation}:
Pruning removes small-magnitude weights (shown as white/zero in the
right matrix) while preserving large-magnitude weights (shown in color),
creating a sparse representation that reduces memory usage while
maintaining model accuracy.}

\end{figure}%

To make pruning computationally feasible, practical methods often
replace the hard L0 constraint with soft regularization like L1-norm
(\(\lambda \| W \|_1\)), which encourages small values that can later be
thresholded to zero. Practitioners typically use \textbf{iterative
pruning}, where parameters are removed in successive steps interleaved
with fine-tuning to recover lost accuracy
(\citeproc{ref-gale2020sparse}{Gale, Elsen, and Hooker 2019a};
\citeproc{ref-blalock2020state}{Labarge, n.d.}).

\subsubsection{Target
Structures}\label{sec-model-compression-target-structures-c474}

Pruning methods vary based on which structures within a neural network
are removed. The primary targets include neurons, channels, and layers,
each with distinct implications for the model's architecture and
performance.

\begin{itemize}
\item
  \textbf{Neuron pruning} removes entire neurons along with their
  associated weights and biases, reducing the width of a layer. This
  technique is often applied to fully connected layers.
\item
  \textbf{Channel pruning} (or filter pruning), commonly used in
  convolutional neural networks, eliminates entire channels or filters.
  This reduces the depth of feature maps, which impacts the network's
  ability to extract certain features. Channel pruning proves valuable
  in image-processing tasks where computational efficiency is a
  priority.
\item
  \textbf{Layer pruning} removes entire layers from the network,
  significantly reducing depth. While this approach can yield
  significant efficiency gains, it requires careful balance to ensure
  the model retains sufficient capacity to capture complex patterns.
\end{itemize}

Figure~\ref{fig-channel-layer-pruning} illustrates the differences
between channel pruning and layer pruning. When a channel is pruned, the
model's architecture must be adjusted to accommodate the structural
change. Specifically, the number of input channels in subsequent layers
must be modified, requiring alterations to the depths of the filters
applied to the layer with the removed channel. In contrast, layer
pruning removes all channels within a layer, necessitating more
significant architectural modifications. In this case, connections
between remaining layers must be reconfigured to bypass the removed
layer. Regardless of the pruning approach, fine-tuning is important to
adapt the remaining network and restore performance.

\begin{figure}[htb]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/0ab52dc06bff0c5e9872d54fb976ffc1bb7d9b1f.pdf}}

}

\caption{\label{fig-channel-layer-pruning}\textbf{Pruning Strategies}:
Channel pruning adjusts filter sizes within layers, while layer pruning
removes entire layers and necessitates reconnection of remaining network
components. These approaches reduce model size and computational cost,
but require fine-tuning to mitigate performance loss due to reduced
model capacity.}

\end{figure}%

\subsubsection{Unstructured
Pruning}\label{sec-model-compression-unstructured-pruning-280e}

Unstructured pruning removes individual weights while preserving the
overall network architecture. During training, some connections become
redundant, contributing little to the final computation. Pruning these
weak connections reduces memory requirements while preserving most of
the model's accuracy.

The mathematical foundation for unstructured pruning helps understand
how sparsity is systematically introduced. Mathematically, unstructured
pruning introduces sparsity into the weight matrices of a neural
network. Let \(W \in \mathbb{R}^{m \times n}\) represent a weight matrix
in a given layer of a network. Pruning removes a subset of weights by
applying a binary mask \(M \in \{0,1\}^{m \times n}\), yielding a pruned
weight matrix: \[
\hat{W} = M \odot W
\] where \(\odot\) represents the element-wise Hadamard product. The
mask \(M\) is constructed based on a pruning criterion, typically weight
magnitude. A common approach is magnitude-based pruning, which removes a
fraction \(s\) of the lowest-magnitude weights. This is achieved by
defining a threshold \(\tau\) such that: \[
M_{i,j} =
\begin{cases}
1, & \text{if } |W_{i,j}| > \tau \\
0, & \text{otherwise}
\end{cases}
\] where \(\tau\) is chosen to ensure that only the largest \((1 - s)\)
fraction of weights remain. This method assumes that larger-magnitude
weights contribute more to the network's function, making them
preferable for retention.

The primary advantage of unstructured pruning is memory efficiency. By
reducing the number of nonzero parameters, pruned models require less
storage, which benefits deployment to embedded or mobile devices with
limited memory.

However, unstructured pruning does not necessarily improve computational
efficiency on modern machine learning hardware. Standard GPUs and TPUs
are optimized for dense matrix multiplications, and a sparse weight
matrix often cannot fully utilize hardware acceleration unless
specialized sparse computation kernels are available. Therefore,
unstructured pruning primarily benefits model storage rather than
inference acceleration. While unstructured pruning improves model
efficiency at the parameter level, it does not alter the structural
organization of the network.

\subsubsection{Structured
Pruning}\label{sec-model-compression-structured-pruning-b651}

While unstructured pruning removes individual weights from a neural
network, structured pruning\sidenote{\textbf{Structured Pruning}:
Removing entire filters/channels rather than individual weights,
enabling immediate hardware speedups without sparse computation support.
ResNet-34 filter pruning achieves 50\% FLOP reduction with 1.0\%
accuracy loss; MobileNetV2 channel pruning yields 3.2× faster ARM
inference at 96.5\% accuracy. Importance metrics include magnitude,
gradient, and Taylor expansion. } eliminates entire computational units,
such as neurons, filters, channels, or layers. This approach is
particularly beneficial for hardware efficiency, as it produces smaller
dense models that can be directly mapped to modern machine learning
accelerators. Unlike unstructured pruning, which results in sparse
weight matrices that require specialized execution kernels to exploit
computational benefits, structured pruning leads to more efficient
inference on general-purpose hardware by reducing the overall size of
the network architecture.

Structured pruning is motivated by the observation that not all neurons,
filters, or layers contribute equally to a model's predictions. Some
units primarily carry redundant or low-impact information, and removing
them does not significantly degrade model performance. Identifying which
structures can be pruned while preserving accuracy remains the core
challenge.

Figure~\ref{fig-structured-unstructured} illustrates the key differences
between unstructured and structured pruning. On the left, unstructured
pruning removes individual weights (depicted as dashed connections),
creating a sparse weight matrix. This can disrupt the original network
structure, as shown in the fully connected network where certain
connections have been randomly pruned. While this reduces the number of
active parameters, the resulting sparsity requires specialized execution
kernels to fully utilize computational benefits.

In contrast, structured pruning (depicted in the middle and right
sections of the figure) removes entire neurons or filters while
preserving the network's overall structure. In the middle section, a
pruned fully connected network retains its fully connected nature but
with fewer neurons. On the right, structured pruning is applied to a CNN
by removing convolutional kernels or entire channels (dashed squares).
This method maintains the CNN's core convolutional operations while
reducing the computational load, making it more compatible with hardware
accelerators.

\begin{figure}[htb]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/86c74f8168d6d048d2e0a4e702cdfc19fbb9a2bf.pdf}}

}

\caption{\label{fig-structured-unstructured}\textbf{Pruning Strategies}:
Unstructured pruning achieves sparsity by removing individual weights,
requiring specialized hardware for efficient computation, while
structured pruning removes entire neurons or filters, preserving network
structure and enabling acceleration on standard hardware. This figure
contrasts the resulting weight matrices and network architectures from
both approaches, highlighting the trade-offs between sparsity level and
computational efficiency. Source: (\citeproc{ref-qi2021efficient}{Qi et
al. 2021}).}

\end{figure}%

A common approach to structured pruning is magnitude-based pruning,
where entire neurons or filters are removed based on the magnitude of
their associated weights. The intuition behind this method is that
parameters with smaller magnitudes contribute less to the model's
output, making them prime candidates for elimination. The importance of
a neuron or filter is often measured using a norm function, such as the
\(\ell_1\)-norm or \(\ell_2\)-norm, applied to the weights associated
with that unit. If the norm falls below a predefined threshold, the
corresponding neuron or filter is pruned. This method is straightforward
to implement and does not require additional computational overhead
beyond computing norms across layers.

Another strategy is activation-based pruning, which evaluates the
average activation values of neurons or filters over a dataset. Neurons
that consistently produce low activations contribute less information to
the network's decision process and can be safely removed. This method
captures the dynamic behavior of the network rather than relying solely
on static weight values. Activation-based pruning requires profiling the
model over a representative dataset to estimate the average activation
magnitudes before making pruning decisions.

Gradient-based pruning uses information from the model's training
process to identify less significant neurons or filters. The key idea is
that units with smaller gradient magnitudes contribute less to reducing
the loss function, making them less important for learning. By ranking
neurons based on their gradient values, structured pruning can remove
those with the least impact on model optimization. Unlike
magnitude-based or activation-based pruning, which rely on static
properties of the trained model, gradient-based pruning requires access
to gradient computations and is typically applied during training rather
than as a post-processing step.

Each of these methods presents trade-offs in terms of computational
complexity and effectiveness. Magnitude-based pruning is computationally
inexpensive and easy to implement but does not account for how neurons
behave across different data distributions. Activation-based pruning
provides a more data-driven pruning approach but requires additional
computations to estimate neuron importance. Gradient-based pruning
leverages training dynamics but may introduce additional complexity if
applied to large-scale models. The choice of method depends on the
specific constraints of the target deployment environment and the
performance requirements of the pruned model.

\subsubsection{Dynamic
Pruning}\label{sec-model-compression-dynamic-pruning-adee}

Traditional pruning methods, whether unstructured or structured,
typically involve static pruning, where parameters are permanently
removed after training or at fixed intervals during training. However,
this approach assumes that the importance of parameters is fixed, which
is not always the case. In contrast, dynamic pruning adapts pruning
decisions based on the input data or training dynamics, allowing the
model to adjust its structure in real time.

Dynamic pruning can be implemented using runtime sparsity techniques,
where the model actively determines which parameters to utilize based on
input characteristics. Activation-conditioned pruning exemplifies this
approach by selectively deactivating neurons or channels that exhibit
low activation values for specific inputs
(\citeproc{ref-dynamicpruning2023}{J. Hu et al. 2023}). This method
introduces input-dependent sparsity patterns, effectively reducing the
computational workload during inference without permanently modifying
the model architecture.

For instance, consider a convolutional neural network processing images
with varying complexity. During inference of a simple image containing
mostly uniform regions, many convolutional filters may produce
negligible activations. Dynamic pruning identifies these low-impact
filters and temporarily excludes them from computation, improving
efficiency while maintaining accuracy for the current input. This
adaptive behavior is particularly advantageous in latency-sensitive
applications, where computational resources must be allocated
judiciously based on input complexity. \textbf{?@sec-benchmarking-ai}
presents measurement strategies for evaluating such efficiency gains.

Another class of dynamic pruning operates during training, where
sparsity is gradually introduced and adjusted throughout the
optimization process. Methods such as gradual magnitude pruning start
with a dense network and progressively increase the fraction of pruned
parameters as training progresses. Instead of permanently removing
parameters, these approaches allow the network to recover from
pruning-induced capacity loss by regrowing connections that prove to be
important in later stages of training.

Dynamic pruning presents several advantages over static pruning. It
allows models to adapt to different workloads, potentially improving
efficiency while maintaining accuracy. Unlike static pruning, which
risks over-pruning and degrading performance, dynamic pruning provides a
mechanism for selectively reactivating parameters when necessary.
However, implementing dynamic pruning requires additional computational
overhead, as pruning decisions must be made in real-time, either during
training or inference. This makes it more complex to integrate into
standard machine learning pipelines compared to static pruning,
requiring sophisticated production deployment strategies and monitoring
frameworks covered in \textbf{?@sec-machine-learning-operations-mlops}.

Despite its challenges, dynamic pruning proves useful in edge computing
and efficient AI contexts discussed in \textbf{?@sec-introduction},
where resource constraints and real-time efficiency requirements vary
across different inputs.

\subsubsection{Pruning
Trade-offs}\label{sec-model-compression-pruning-tradeoffs-c139}

Pruning techniques offer different trade-offs in terms of memory
efficiency, computational efficiency, accuracy retention, hardware
compatibility, and implementation complexity. The choice of pruning
strategy depends on the specific constraints of the machine learning
system and the deployment environment, integrating with operational
considerations from \textbf{?@sec-machine-learning-operations-mlops}.

Unstructured pruning effectively reduces model size and memory footprint
by removing individual weights while keeping the overall model
architecture intact. However, since machine learning accelerators are
optimized for dense matrix operations, unstructured pruning does not
always translate to significant computational speed-ups unless
specialized sparse execution kernels are used.

Structured pruning, in contrast, eliminates entire neurons, channels, or
layers, leading to a more hardware-friendly model. This technique
provides direct computational savings, as it reduces the number of
floating-point operations (FLOPs)\sidenote{\textbf{FLOPs (Floating-Point
Operations)}: Computational complexity metric counting multiply-add
operations. ResNet-50 requires approximately 3.8 billion FLOPs per
inference (\citeproc{ref-he2016deep}{K. He et al. 2016}), GPT-3 training
required an estimated 3.14E23 FLOPs
(\citeproc{ref-patterson2021carbon}{Patterson et al. 2021}). Modern GPUs
achieve 100-300 TFLOPS (trillion FLOPs/second), making FLOP reduction
important for efficiency. } required during inference.

The downside is that modifying the network structure can lead to a
greater accuracy drop, requiring careful fine-tuning to recover lost
performance.

Dynamic pruning introduces adaptability into the pruning process by
adjusting which parameters are pruned at runtime based on input data or
training dynamics. This allows for a better balance between accuracy and
efficiency, as the model retains the flexibility to reintroduce
previously pruned parameters if needed. However, dynamic pruning
increases implementation complexity, as it requires additional
computations to determine which parameters to prune on-the-fly.

Table~\ref{tbl-pruning} summarizes the structural differences between
pruning approaches, revealing distinct trade-offs in memory efficiency,
computational speed, and hardware compatibility.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1120}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3707}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2934}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2162}}@{}}
\caption{\textbf{Pruning Strategies}: Unstructured, structured, and
dynamic pruning each modify model weights differently, impacting both
model size and computational efficiency. Unstructured pruning offers the
greatest compression but requires specialized hardware, while dynamic
pruning adapts to input data for a balance between accuracy and resource
usage.}\label{tbl-pruning}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Aspect}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Unstructured Pruning}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Structured Pruning}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Dynamic Pruning}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Aspect}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Unstructured Pruning}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Structured Pruning}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Dynamic Pruning}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{What is removed?} & Individual weights in the model & Entire
neurons, channels, filters, or layers & Adjusts pruning based on runtime
conditions \\
\textbf{Model structure} & Sparse weight matrices; original architecture
remains unchanged & Model architecture is modified; pruned layers are
fully removed & Structure adapts dynamically \\
\textbf{Impact on memory} & Reduces model storage by eliminating nonzero
weights & Reduces model storage by removing entire components & Varies
based on real-time pruning \\
\textbf{Impact on computation} & Limited; dense matrix operations still
required unless specialized sparse computation is used & Directly
reduces FLOPs and speeds up inference & Balances accuracy and efficiency
dynamically \\
\textbf{Hardware compatibility} & Sparse weight matrices require
specialized execution support for efficiency & Works efficiently with
standard deep learning hardware & Requires adaptive inference engines \\
\textbf{Fine-tuning required?} & Often necessary to recover accuracy
after pruning & More likely to require fine-tuning due to larger
structural modifications & Adjusts dynamically, reducing the need for
retraining \\
\textbf{Use cases} & Memory-efficient model compression for cloud
deployment & Real-time inference optimization, mobile/edge AI, and
efficient training & Adaptive AI applications, real-time systems \\
\end{longtable}

\subsubsection{Pruning
Strategies}\label{sec-model-compression-pruning-strategies-d1ce}

Beyond the broad categories of unstructured, structured, and dynamic
pruning, different pruning workflows can impact model efficiency and
accuracy retention. Two widely used pruning strategies are iterative
pruning and one-shot pruning, each with its own benefits and trade-offs.

\paragraph{Iterative
Pruning}\label{sec-model-compression-iterative-pruning-6c58}

Iterative pruning implements a gradual approach to structure removal
through multiple cycles of pruning followed by fine-tuning. During each
cycle, the algorithm removes a small subset of structures based on
predefined importance metrics. The model then undergoes fine-tuning to
adapt to these structural modifications before proceeding to the next
pruning iteration. This methodical approach helps prevent sudden drops
in accuracy while allowing the network to progressively adjust to
reduced complexity.

Figure~\ref{fig-iterative-pruning} illustrates this process using a
convolutional neural network where six channels are pruned. Rather than
removing all channels simultaneously, iterative pruning eliminates two
channels per iteration over three cycles. Following each pruning step,
the model undergoes fine-tuning to recover performance. The first
iteration, which removes two channels, results in an accuracy decrease
from 0.995 to 0.971, but subsequent fine-tuning restores accuracy to
0.992. After completing two additional pruning-tuning cycles, the final
model achieves 0.991 accuracy, which represents only a 0.4\% reduction
from the original, while operating with 27\% fewer channels. By
distributing structural modifications across multiple iterations, the
network maintains its performance capabilities while achieving improved
computational efficiency.

\begin{figure}[htb]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/29338469580e85fb84e888f1673084aa160165b2.pdf}}

}

\caption{\label{fig-iterative-pruning}\textbf{Iterative Pruning
Performance}: Gradual channel removal with interleaved fine-tuning
maintains high accuracy while reducing model size. This figure shows a
0.4\% accuracy drop with a 27\% reduction in channels, demonstrating the
benefits of distributing structural modifications across multiple
iterations. This approach contrasts with one-shot pruning, which often
leads to significant performance degradation.}

\end{figure}%

\paragraph{One-shot
Pruning}\label{sec-model-compression-oneshot-pruning-bad9}

One-shot pruning removes multiple architectural components in a single
step, followed by an extensive fine-tuning phase to recover model
accuracy. This aggressive approach compresses the model quickly but
risks greater accuracy degradation, as the network must adapt to
significant structural changes simultaneously.

Consider applying one-shot pruning to the same network from the
iterative pruning example. Instead of removing two channels at a time
over multiple iterations, one-shot pruning eliminates all six channels
simultaneously, as illustrated in Figure~\ref{fig-oneshot-pruning}.
Removing 27\% of the network's channels simultaneously causes the
accuracy to drop significantly, from 0.995 to 0.914. Even after
fine-tuning, the network only recovers to an accuracy of 0.943, which is
a 5\% degradation from the original unpruned network. While both
iterative and one-shot pruning ultimately produce identical network
structures, the gradual approach of iterative pruning better preserves
model performance.

\begin{figure}[htb]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/5b615bce15c4556e457ae88e52c3f51088d47da2.pdf}}

}

\caption{\label{fig-oneshot-pruning}\textbf{One-Shot Pruning Impact}:
Aggressive removal of architectural components, like the 27\% of
channels shown, causes significant initial accuracy loss because the
network struggles to adapt to significant structural changes
simultaneously. Fine-tuning partially recovers performance, but
establishes that iterative pruning preserves accuracy more effectively
than single-step approaches.}

\end{figure}%

The choice of pruning strategy requires careful consideration of several
key factors that influence both model efficiency and performance. The
desired level of parameter reduction, or sparsity target, directly
impacts strategy selection. Higher reduction targets often necessitate
iterative approaches to maintain accuracy, while moderate sparsity goals
may be achievable through simpler one-shot methods.

Available computational resources significantly influence strategy
choice. Iterative pruning demands significant resources for multiple
fine-tuning cycles, whereas one-shot approaches require fewer resources
but may sacrifice accuracy. This resource consideration connects to
performance requirements, where applications with strict accuracy
requirements typically benefit from gradual, iterative pruning to
carefully preserve model capabilities. Use cases with more flexible
performance constraints may accommodate more aggressive one-shot
approaches.

Development timeline also impacts pruning decisions. One-shot methods
enable faster deployment when time is limited, though iterative
approaches generally achieve superior results given sufficient
optimization periods. Finally, target platform capabilities
significantly influence strategy selection, as certain hardware
architectures may better support specific sparsity patterns, making
particular pruning approaches more advantageous for deployment. By
evaluating these factors systematically, practitioners can select a
pruning approach that optimally balances efficiency gains with model
performance for their specific use case.

\subsubsection{Lottery Ticket
Hypothesis}\label{sec-model-compression-lottery-ticket-hypothesis-a5bd}

Pruning is widely used to reduce the size and computational cost of
neural networks, but the process of determining which parameters to
remove is not always straightforward. While traditional pruning methods
eliminate weights based on magnitude, structure, or dynamic conditions,
recent research suggests that pruning is not just about reducing
redundancy. It may also reveal inherently efficient subnetworks that
exist within the original model.

This perspective leads to the Lottery Ticket
Hypothesis\sidenote{\textbf{Lottery Ticket Hypothesis}: Named for the
intuition that training a large network is like buying many lottery
tickets: most lose, but a few ``winning tickets'' (sparse subnetworks
with lucky initializations) can win (train to full accuracy) on their
own. Frankle and Carbin (\citeproc{ref-frankle2019lottery}{Frankle and
Carbin 2019}) showed ResNet-18 subnetworks at 10-20\% original size
achieve 93.2\% vs.~94.1\% accuracy. BERT-base winning tickets retain
97\% performance with 90\% fewer parameters. } (LTH), which challenges
conventional pruning workflows by proposing that within large neural
networks, there exist small, well-initialized subnetworks (referred to
as ``winning tickets'') that can achieve comparable accuracy to the full
model when trained in isolation. Rather than viewing pruning as just a
post-training compression step, LTH suggests it can serve as a discovery
mechanism to identify these efficient subnetworks early in training.

LTH is validated through an iterative pruning process, illustrated in
Figure~\ref{fig-winning-ticket}. A large network is first trained to
convergence. The lowest-magnitude weights are then pruned, and the
remaining weights are reset to their original initialization rather than
being re-randomized. This process is repeated iteratively, gradually
reducing the network's size while preserving performance. After multiple
iterations, the remaining subnetwork (the ``winning ticket'') proves
capable of training to the same or higher accuracy as the original full
model.

\begin{figure}[htb]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/f45fc048643fe0c1a42e0b162ea1bb602baff252.pdf}}

}

\caption{\label{fig-winning-ticket}\textbf{Winning Ticket Discovery}:
Iterative pruning and weight resetting identify subnetworks within
larger models that, when trained in isolation, achieve comparable or
superior accuracy, challenging the conventional view of pruning as
solely a compression technique. This process establishes that
well-initialized subnetworks exist and can be efficiently trained,
suggesting that much of a large network's capacity may be redundant.}

\end{figure}%

The implications of the Lottery Ticket Hypothesis extend beyond
conventional pruning techniques. Instead of training large models and
pruning them later, LTH suggests that compact, high-performing
subnetworks could be trained directly from the start, eliminating the
need for overparameterization. This insight challenges the traditional
assumption that model size is necessary for effective learning. It also
emphasizes the importance of initialization, as winning tickets only
retain their performance when reset to their original weight values.
This finding raises deeper questions about the role of initialization in
shaping a network's learning trajectory.

The hypothesis further reinforces the effectiveness of iterative pruning
over one-shot pruning. Gradually refining the model structure allows the
network to adapt at each stage, preserving accuracy more effectively
than removing large portions of the model in a single step. This process
aligns well with practical pruning strategies used in deployment, where
preserving accuracy while reducing computation is important.

Despite its promise, applying LTH in practice remains computationally
expensive because identifying winning tickets requires multiple cycles
of pruning and retraining. Ongoing research explores whether winning
subnetworks can be detected early without full training, potentially
leading to more efficient sparse training techniques. If such methods
become practical, LTH could fundamentally reshape how machine learning
models are trained, shifting the focus from pruning large networks after
training to discovering and training only the important components from
the beginning.

While LTH presents a compelling theoretical perspective on pruning,
practical implementations rely on established framework-level tools to
integrate structured and unstructured pruning techniques.

\subsubsection{Pruning in
Practice}\label{sec-model-compression-pruning-practice-bb32}

Modern machine learning frameworks provide dedicated APIs to automate
the pruning and fine-tuning workflow.

In \textbf{PyTorch}, the \texttt{torch.nn.utils.prune} module provides a
flexible interface for pruning individual layers or entire models. Users
can apply unstructured pruning (e.g., \texttt{l1\_unstructured}) or
structured pruning (e.g., \texttt{ln\_structured}) with just a few lines
of code. PyTorch uses ``masks'' to handle pruning---the original
parameters are preserved, but a binary mask is multiplied element-wise
during the forward pass. To realize actual memory savings for
deployment, these masks must be ``permanently'' applied using
\texttt{prune.remove(module,\ \textquotesingle{}weight\textquotesingle{})}.

\textbf{TensorFlow} implements pruning through the \textbf{TensorFlow
Model Optimization Toolkit (TF-MOT)}. Unlike PyTorch's post-training
approach, TF-MOT often integrates pruning into the training process
itself. By using \texttt{prune\_low\_magnitude}, the framework gradually
increases sparsity during training, allowing the model to adapt its
remaining weights to the sparse structure in real-time.

Understanding these trade-offs is important when deploying pruned models
in real-world settings. Several high-profile models have successfully
integrated pruning to optimize performance. MobileNet, designed for
mobile and embedded applications, has been pruned to reduce inference
latency while preserving accuracy
(\citeproc{ref-howard2017mobilenets}{Howard et al. 2017}).
BERT\sidenote{\textbf{BERT Compression}: BERT-Base (110M params) can be
compressed to 67M params (39\% reduction) with only 1.2\% GLUE score
drop. Attention head pruning removes 144 of 192 heads with minimal
impact, while layer pruning reduces 12 layers to 6 layers maintaining
97.8\% performance. }, a widely used transformer model for natural
language processing, has undergone structured pruning of attention heads
and intermediate layers to create efficient versions such as DistilBERT
and TinyBERT, which retain much of the original performance while
reducing computational overhead (\citeproc{ref-sanh2019distilbert}{Sanh
et al. 2019}). In computer vision,
EfficientNet\sidenote{\textbf{EfficientNet Pruning}: Compound scaling
makes EfficientNet amenable to structured pruning. EfficientNet-B0 with
70\% pruning maintains 75.8\% accuracy (vs.~77.1\% baseline), achieving
2.8× speedup. Channel pruning reduces FLOPs from 390M to 140M, enabling
sub-20ms inference on Pixel 4. Iterative magnitude pruning with
fine-tuning preserves accuracy better than one-shot approaches. } has
been pruned to remove unnecessary filters, optimizing it for deployment
in resource-constrained environments
(\citeproc{ref-tan2019efficientnet}{Tan and Le 2019a}).

Pruning is powerful but has a fundamental limitation: it starts with an
existing architecture and carves away pieces. What if the original
architecture itself is inefficient for deployment? What if we want a
model with a completely different structure---say, a 6-layer transformer
instead of a 12-layer one---that still captures the original model's
capabilities?

This leads us to \textbf{knowledge distillation}, a fundamentally
different approach. Rather than modifying an existing model's weights,
distillation trains a new, compact ``student'' model to mimic the
behavior of a larger ``teacher'' model. The student inherits the
teacher's learned knowledge without inheriting its computational
overhead.

\subsection{Knowledge
Distillation}\label{sec-model-compression-knowledge-distillation-e02e}

Knowledge distillation\sidenote{\textbf{Distillation}: Borrowed from
chemistry, where distillation separates mixtures by selective
evaporation and condensation, extracting the essence while leaving
impurities behind. Hinton et al.
(\citeproc{ref-hinton2015distilling}{Hinton, Vinyals, and Dean 2015})
chose this metaphor deliberately: the teacher's ``dark knowledge'' about
class relationships is the essence being extracted, while the massive
parameter count is the impurity left behind. The temperature parameter
\(T\) in the softmax even mirrors the literal temperature control in
chemical distillation. } trains a smaller \textbf{student} model using
guidance from a larger, pre-trained \textbf{teacher} model. The core
insight is that a well-trained teacher provides a richer learning signal
than simple ground-truth labels. While a hard label is binary (e.g.,
\([1, 0, 0]\) for cat), a teacher's probability distribution (e.g.,
\([0.85, 0.10, 0.05]\)) reveals \textbf{inter-class similarity}---that a
cat shares more features with a dog than a fox.
Figure~\ref{fig-kd-targets} visualizes how this ``dark knowledge''
embedded in the teacher's learned representations guides the student to
generalize better.

\begin{figure}[htb]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/3fd50a80f53f58f19b2c9f29c05618618806c4cd.pdf}}

}

\caption{\label{fig-kd-targets}\textbf{Soft Target Distribution}: The
teacher's relative confidence levels indicate which classes are
semantically similar (e.g., cat vs.~dog), providing a much richer
supervision signal than a binary ``correct'' label.}

\end{figure}%

\subsubsection{The Distillation
Process}\label{sec-model-compression-distillation-process-2f08}

Figure~\ref{fig-kd-overview} illustrates the distillation workflow,
which trains the student model to minimize a combination of two loss
functions:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Distillation Loss}: Typically the Kullback-Leibler (KL)
  divergence\sidenote{\textbf{Kullback-Leibler Divergence}: Named after
  Solomon Kullback and Richard Leibler, who introduced it at the
  National Security Agency in 1951 for cryptanalysis. Measures how one
  probability distribution differs from a reference distribution. In
  information theory, KL(P\textbar\textbar Q) quantifies the extra bits
  needed to encode samples from P using a code optimized for Q. Zero
  when distributions match; always non-negative. } between the teacher's
  softened output distribution and the student's distribution.
\item
  \textbf{Student Loss}: Standard cross-entropy loss against the
  ground-truth hard labels.
\end{enumerate}

\begin{figure}[htb]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/3121ac100abe40141547e96911da28a78c4791e0.pdf}}

}

\caption{\label{fig-kd-overview}\textbf{Knowledge Distillation
Workflow}: The student model inherits the teacher's generalization
capabilities by matching its softened probability distributions. This
dual-loss training enables the student to absorb complex class
relationships that are absent from binary labels.}

\end{figure}%

\subsubsection{Distillation
Mathematics}\label{sec-model-compression-distillation-mathematics-fc81}

To reveal the inter-class similarity information, we use a
\textbf{temperature parameter}\sidenote{\textbf{Temperature in Softmax}:
Borrowed from statistical mechanics, where the Boltzmann distribution
\(p_i \propto \exp(-E_i/kT)\) describes particle states at temperature
\(T\). Higher temperature means more uniform distribution across states.
Hinton adopted this analogy for neural networks: temperature \(T\) in
softmax controls how ``soft'' the probability distribution becomes. At
\(T=1\) (standard softmax), peaks are sharp; at \(T \to \infty\), the
distribution becomes uniform. } \(T\) to soften the probability
distribution. The softmax output for class \(i\) becomes:

\[
p_i(T) = \frac{\exp(z_i / T)}{\sum_j \exp(z_j / T)}
\]

A higher \(T\) (typically 3 to 5) produces a smoother distribution,
allowing the student to learn from the ``uncertainty'' the teacher
assigns to incorrect classes. The total loss
\(\mathcal{L}_{\text{distill}}\) balances standard cross-entropy with
the KL divergence:

\[
\mathcal{L}_{\text{distill}} = (1 - \alpha) \mathcal{L}_{\text{CE}}(y_s, y) + \alpha T^2 \text{KL}(p_{\text{teacher}}^T, p_{\text{student}}^T)
\]

The factor \(T^2\) ensures that gradient scales remain consistent when
\(T\) is changed. This hybrid approach enables compact models (like
DistilBERT) to achieve up to 97\% of their teacher's performance with a
fraction of the memory and compute.

\subsubsection{Efficiency
Gains}\label{sec-model-compression-efficiency-gains-f589}

Knowledge distillation's efficiency benefits span three key areas:
memory efficiency, computational efficiency, and deployment flexibility.
Unlike pruning which modifies trained models, distillation trains
compact models from the start using teacher guidance, enabling accuracy
levels difficult to achieve through standard training alone
(\citeproc{ref-sanh2019distilbert}{Sanh et al. 2019}).
\textbf{?@sec-benchmarking-ai} provides structured evaluation approaches
for measuring these efficiency gains.

\paragraph{Memory and Model
Compression}\label{sec-model-compression-memory-model-compression-310d}

A key advantage of knowledge distillation is that it enables smaller
models to retain much of the predictive power of larger models,
significantly reducing memory footprint. This is valuable in
resource-constrained environments such as mobile and embedded AI
systems, where model size directly impacts storage requirements and load
times.

For instance, models such as
\textbf{DistilBERT}\sidenote{\textbf{DistilBERT Performance}: Achieves
97\% of BERT-Base performance with 40\% fewer parameters (66M vs.~110M)
and 60\% faster inference. On SQuAD v1.1, DistilBERT scores 86.9 F1
vs.~BERT's 88.5 F1, while reducing memory from 1.35GB to 0.54GB and
latency from 85ms to 34ms. } in NLP and MobileNet distillation variants
(\citeproc{ref-howard2017mobilenets}{Howard et al. 2017}) in computer
vision have been shown to retain up to 97\% of the accuracy of their
larger teacher models while using only half the number of parameters.
This level of compression is often superior to pruning, where aggressive
parameter reduction can lead to deterioration in representational power.

Another key benefit of knowledge distillation is its ability to transfer
robustness and generalization from the teacher to the student. Large
models are often trained with extensive datasets and develop strong
generalization capabilities, meaning they are less sensitive to noise
and data shifts. A well-trained student model inherits these properties,
making it less prone to overfitting and more stable across diverse
deployment conditions. This is particularly useful in low-data regimes,
where training a small model from scratch may result in poor
generalization due to insufficient training examples.

\paragraph{Computation and Inference
Speed}\label{sec-model-compression-computation-inference-speed-71a6}

By training the student model to approximate the teacher's knowledge in
a more compact representation, distillation results in models that
require fewer FLOPs per inference, leading to faster execution times.
Unlike unstructured pruning, which may require specialized hardware
support for sparse computation, a distilled model remains densely
structured, making it more compatible with existing machine learning
accelerators such as GPUs, TPUs, and edge AI chips
(\citeproc{ref-jiao2020tinybert}{Jiao et al. 2020}).

In real-world deployments, this translates to:

\begin{itemize}
\tightlist
\item
  Reduced inference latency, which is important for real-time AI
  applications such as speech recognition, recommendation systems, and
  self-driving perception models.
\item
  Lower energy consumption, making distillation relevant for low-power
  AI on mobile devices and IoT systems.
\item
  Higher throughput in cloud inference, where serving a distilled model
  allows large-scale AI applications to reduce computational cost while
  maintaining model quality.
\end{itemize}

For example, when deploying transformer models for NLP, organizations
often use teacher-student distillation to create models that achieve
similar accuracy at 2-4\(\times\) lower latency, making it feasible to
serve billions of requests per day with significantly lower
computational overhead.

\paragraph{Deployment and System
Considerations}\label{sec-model-compression-deployment-system-considerations-be9a}

Knowledge distillation is also effective in multi-task learning
scenarios, where a single teacher model can guide multiple student
models for different tasks. For example, in multi-lingual NLP models, a
large teacher trained on multiple languages can transfer
language-specific knowledge to smaller, task-specific student models,
enabling efficient deployment across different languages without
retraining from scratch. Similarly, in computer vision, a teacher
trained on diverse object categories can distill knowledge into
specialized students optimized for tasks such as face recognition,
medical imaging, or autonomous driving.

Once a student model is distilled, it can be further optimized for
hardware-specific acceleration using techniques such as pruning,
quantization, and graph optimization. This ensures that compressed
models remain inference-efficient across multiple hardware environments,
especially in edge AI and mobile deployments
(\citeproc{ref-gordon2020compressing}{Gordon, Duh, and Andrews 2020}).

Despite these advantages, knowledge distillation has important
limitations. The effectiveness of distillation depends on the quality of
the teacher model: a poorly trained teacher may transfer incorrect
biases to the student. Distillation also introduces an additional
training phase where both the teacher and student must be used together,
increasing computational costs during training. In some cases, designing
an appropriate student model architecture that can fully benefit from
the teacher's knowledge remains a challenge, as overly small student
models may not have enough capacity to absorb all the relevant
information.

\subsubsection{Trade-offs}\label{sec-model-compression-tradeoffs-f796}

Compared to pruning, knowledge distillation preserves accuracy better
but requires higher training complexity through training a new model
rather than modifying an existing one. However, pruning provides a more
direct computational efficiency gain, especially when structured pruning
is used. In practice, combining pruning and distillation often yields
the best trade-off, as seen in models like DistilBERT and MobileBERT,
where pruning first reduces unnecessary parameters before distillation
optimizes a final student model. Table~\ref{tbl-kd-pruning} contrasts
the key trade-offs between knowledge distillation and pruning across
accuracy retention, training cost, inference speed, hardware
compatibility, and implementation complexity.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.1706}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3529}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4706}}@{}}
\caption{\textbf{Model Compression Trade-Offs}: Knowledge distillation
and pruning represent distinct approaches to reducing model size and
improving efficiency, each with unique strengths and weaknesses
regarding accuracy, computational cost, and implementation complexity.
Distillation prioritizes preserving accuracy through knowledge transfer,
while pruning directly reduces computational demands by eliminating
redundant parameters, making their combined use a common strategy for
optimal performance.}\label{tbl-kd-pruning}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Criterion}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Knowledge Distillation}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Pruning}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Criterion}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Knowledge Distillation}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Pruning}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Accuracy retention} & High -- Student learns from teacher,
better generalization & Varies -- Can degrade accuracy if over-pruned \\
\textbf{Training cost} & Higher -- Requires training both teacher and
student & Lower -- Only fine-tuning needed \\
\textbf{Inference speed} & High -- Produces dense, optimized models &
Depends -- Structured pruning is efficient, unstructured needs special
support \\
\textbf{Hardware compatibility} & High -- Works on standard accelerators
& Limited -- Sparse models may need specialized execution \\
\textbf{Ease of implementation} & Complex -- Requires designing a
teacher-student pipeline & Simple -- Applied post-training \\
\end{longtable}

Knowledge distillation is frequently used alongside pruning and
quantization for deployment-ready models. How distillation interacts
with these complementary techniques determines the effectiveness of
multi-stage optimization pipelines.

\subsection{Structured
Approximations}\label{sec-model-compression-structured-approximations-1139}

Rather than eliminating parameters (pruning) or transferring knowledge
(distillation), approximation methods decompose large weight matrices
and tensors into lower-dimensional components. These techniques leverage
the observation that many high-dimensional representations can be
well-approximated by lower-rank structures, reducing parameters without
significant performance loss.

\subsubsection{Low-Rank
Factorization}\label{sec-model-compression-lowrank-factorization-2ef5}

Low-Rank Matrix Factorization (LRMF) approximates weight matrices with
lower-rank representations. Given a matrix
\(A \in \mathbb{R}^{m \times n}\), LRMF finds matrices
\(U \in \mathbb{R}^{m \times k}\) and \(V \in \mathbb{R}^{k \times n}\)
such that: \[
A \approx UV
\] where \(k \ll m, n\) is the approximation rank. This is typically
computed via singular value decomposition
(SVD)\sidenote{\textbf{Singular Value Decomposition (SVD)}: Factorizes
matrix \(A = U \Sigma V^T\) where singular values in \(\Sigma\) indicate
importance. Truncating to top-k values minimizes Frobenius norm error
(Eckart-Young theorem). For a 4096×4096 weight matrix, rank-128 SVD
reduces storage from 64MB to 4MB while preserving 95\% of spectral
energy. GPU implementations achieve O(mn min(m,n)) complexity. },
retaining only the top \(k\) singular values.

\phantomsection\label{callout-perspectiveux2a-1.5}
\begin{fbx}{callout-perspective}{Systems Perspective: }{The Bandwidth-Compute Trade-off}
\phantomsection\label{callout-perspective*-1.5}
\textbf{Reducing the Memory Pressure}: Low-rank factorization
illustrates a classic systems trade-off: \textbf{trading computation for
bandwidth reduction}. Storing a \(4096 \times 4096\) matrix requires
64MB (at FP32). Fetching this matrix for a single inference is a massive
memory bandwidth hit, especially when limited by physical memory
bandwidth constraints.

If we factorize it with rank \(k=128\), we store two matrices
(\(4096 \times 128\) and \(128 \times 4096\)), totaling only 4MB---a
\textbf{16× reduction in data movement}. While the number of
floating-point operations (FLOPs) actually \emph{increases} slightly
because we perform two smaller matrix multiplies instead of one large
one, the system speedup is often dramatic. By reducing data movement by
16×, we allow the processor to spend more time computing and less time
waiting for memory.

\end{fbx}

This bandwidth-compute trade-off reflects the broader memory
wall\sidenote{\textbf{Memory Wall}: The growing disparity between
processor speed and memory access speed, where memory bandwidth becomes
the dominant bottleneck. We explore this constraint in depth in
\textbf{?@sec-ai-acceleration}. } phenomenon where memory access becomes
the dominant bottleneck.

Figure~\ref{fig-matrix-factorization} illustrates the decrease in
parameterization enabled by low-rank matrix factorization. Observe how
the matrix \(M\) can be approximated by the product of matrices \(L_k\)
and \(R_k^T\). For intuition, most fully connected layers in networks
are stored as a projection matrix \(M\), which requires \(m \times n\)
parameters to be loaded during computation. However, by decomposing and
approximating it as the product of two lower-rank matrices, we only need
to store \(m \times k + k \times n\) parameters in terms of storage
while incurring an additional compute cost of the matrix multiplication.
So long as \(k < n/2\), this factorization has fewer total parameters to
store while adding a computation of runtime \(O(mkn)\)
(\citeproc{ref-gu2023deep}{Gu 2023}).

\begin{figure}[htb]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/1b789495cd3f0dc96b9acb1f7533ab24610be6f8.pdf}}

}

\caption{\label{fig-matrix-factorization}\textbf{Low-Rank
Factorization}: Decomposing a matrix into lower-rank approximations
reduces the number of parameters needed for storage and computation,
enabling efficient model representation. By expressing a matrix \(a\) as
the product of two smaller matrices, \(u\) and \(v\), we transition from
storing \(m \times n\) parameters to \(m \times k + k \times n\)
parameters, with \(k\) representing the reduced rank. Source: The Clever
Machine.}

\end{figure}%

LRMF applies to fully connected layers (large weight matrices) and
convolutional layers (via depthwise-separable convolutions). The key
trade-off: storage reduces from \(O(mn)\) to \(O(mk + kn)\), but
inference requires an additional matrix multiplication. Choosing rank
\(k\) balances compression against information loss.

\subsubsection{Tensor
Decomposition}\label{sec-model-compression-tensor-decomposition-89ef}

Tensor decomposition\sidenote{\textbf{Tensor}: From Latin ``tensus''
(stretched), originally describing stress in materials. Mathematicians
generalized it to multi-dimensional arrays that transform according to
specific rules under coordinate changes. In ML, ``tensor'' simply means
n-dimensional array: scalars (0D), vectors (1D), matrices (2D), and
higher. TensorFlow and PyTorch use the term to emphasize that neural
network computations generalize beyond matrices to arbitrary dimensions.
} extends factorization to multi-dimensional tensors common in
convolutional layers and attention mechanisms.

\begin{figure}[htb]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/81d910559ec1e38ca982f26ef79708102abebd10.pdf}}

}

\caption{\label{fig-tensor-decomposition}\textbf{Tensor Decomposition}:
Multi-dimensional tensors enable compact representations of
high-dimensional data by factorizing them into lower-rank components,
reducing computational costs and memory requirements compared to direct
manipulation of the original tensor. This technique extends matrix
factorization to handle the multi-way interactions common in modern
machine learning models like convolutional neural networks. Source:
(\citeproc{ref-xinyu}{Richter and Zhao 2021}).}

\end{figure}%

Figure~\ref{fig-tensor-decomposition} illustrates how a 3D tensor can be
decomposed into factor matrices. Common decomposition methods include:

\begin{itemize}
\tightlist
\item
  \textbf{CP decomposition}: Expresses a tensor as a sum of rank-one
  components:
  \(\mathcal{A} \approx \sum_{r=1}^{k} u_r \otimes v_r \otimes w_r\)
\item
  \textbf{Tucker decomposition}: Uses a core tensor with factor
  matrices:
  \(\mathcal{A} \approx \mathcal{G} \times_1 U \times_2 V \times_3 W\)
\item
  \textbf{Tensor-Train (TT)}: Factorizes into a sequence of lower-rank
  matrices, particularly effective for very high-dimensional tensors
\end{itemize}

Tensor decomposition applies to convolutional filters (approximating 4D
weight tensors), attention mechanisms in transformers, and embedding
layers in NLP models. The trade-offs mirror LRMF: compression versus
information loss, and the additional computational overhead of tensor
contractions during inference.

\subsubsection{Comparing Factorization
Approaches}\label{sec-model-compression-comparing-factorization-approaches-831c}

Table~\ref{tbl-lrmf-tensor} compares LRMF and tensor decomposition:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.1711}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3743}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4492}}@{}}
\caption{\textbf{Dimensionality \& Factorization}: Low-rank matrix
factorization (LRMF) and tensor decomposition reduce model storage
requirements by representing data with fewer parameters, but introduce
computational trade-offs during inference; LRMF applies to
two-dimensional matrices, while tensor decomposition extends this
approach to multi-dimensional tensors for greater compression
potential.}\label{tbl-lrmf-tensor}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Feature}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Low-Rank Matrix Factorization (LRMF)}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Tensor Decomposition}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Feature}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Low-Rank Matrix Factorization (LRMF)}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Tensor Decomposition}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Applicable Data Structure} & Two-dimensional matrices &
Multi-dimensional tensors \\
\textbf{Compression Mechanism} & Factorizes a matrix into two or more
lower-rank matrices & Decomposes a tensor into multiple lower-rank
components \\
\textbf{Common Methods} & Singular Value Decomposition (SVD),
Alternating Least Squares (ALS) & CP Decomposition, Tucker
Decomposition, Tensor-Train (TT) \\
\textbf{Computational Complexity} & Generally lower, often \$ O(mnk) \$
for a rank-\$ k \$ approximation & Higher, due to iterative optimization
and tensor contractions \\
\textbf{Storage Reduction} & Reduces storage from \$ O(mn) \$ to \$ O(mk
+ kn) \$ & Achieves higher compression but requires more complex storage
representations \\
\textbf{Inference Overhead} & Requires additional matrix multiplication
& Introduces additional tensor operations, potentially increasing
inference latency \\
\textbf{Primary Use Cases} & Fully connected layers, embeddings,
recommendation systems & Convolutional filters, attention mechanisms,
multi-modal learning \\
\textbf{Implementation Complexity} & Easier to implement, often involves
direct factorization methods & More complex, requiring iterative
optimization and rank selection \\
\end{longtable}

In practice, LRMF and tensor decomposition can be combined: fully
connected layers compressed via LRMF while convolutional kernels use
tensor decomposition. The choice depends on the model's structure and
whether memory or latency is the primary constraint.

The techniques explored so far---pruning, distillation, and
factorization---all optimize \emph{existing} architectures. Neural
Architecture Search takes a different approach: discovering
architectures that are efficient \emph{by construction}.

\subsection{Neural Architecture
Search}\label{sec-model-compression-neural-architecture-search-f1e9}

Pruning, knowledge distillation, and other techniques explored in
previous sections rely on human expertise to determine optimal model
configurations. While these manual approaches have led to significant
advancements, selecting optimal architectures requires extensive
experimentation, and even experienced practitioners may overlook more
efficient designs (\citeproc{ref-elsken2019neural}{Elsken, Metzen, and
Hutter 2019a}). Neural Architecture Search (NAS) automates this process
by systematically exploring large spaces of possible architectures to
identify those that best balance accuracy, computational cost, memory
efficiency, and inference latency.

Figure~\ref{fig-nas-flow} illustrates the NAS process.
NAS\sidenote{\textbf{Hardware-Aware NAS}: Architecture search directly
optimizing for target hardware latency rather than proxy metrics like
FLOPs. MnasNet (2019) uses actual measured latency in the search
objective, finding architectures with 1.8× speedup over MobileNetV2 at
higher accuracy. Platform-specific search discovers that optimal
architectures differ significantly between mobile CPUs, GPUs, and TPUs.
} operates through three interconnected stages: defining the search
space (architectural components and constraints), applying search
strategies (reinforcement learning(\citeproc{ref-zoph2017neural}{Zoph
and Le 2017a}), evolutionary algorithms, or gradient-based methods) to
explore candidate architectures, and evaluating performance to ensure
discovered designs satisfy accuracy and efficiency objectives. This
automation enables the discovery of novel architectures that often match
or surpass human-designed models while requiring substantially less
expert effort.

\begin{figure}[htb]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/db6a23d888d7b5885861859484080caa280a7581.pdf}}

}

\caption{\label{fig-nas-flow}\textbf{Neural Architecture Search Flow}:
Automated NAS techniques iteratively refine model architectures and
their weights, jointly optimizing for performance and efficiency, a
departure from manual design approaches that rely on human expertise and
extensive trial-and-error. This process enables the discovery of novel,
high-performing architectures tailored to specific computational
constraints.}

\end{figure}%

NAS search strategies employ diverse optimization techniques.
Reinforcement learning\sidenote{\textbf{Reinforcement Learning NAS}:
Uses RL controller networks to generate architectures, with accuracy as
reward signal. Google's NASNet controller was trained for 22,400
GPU-hours on 800 GPUs, but discovered architectures achieving 82.7\%
ImageNet accuracy---28\% better than human-designed ResNet at similar
FLOP budgets. } treats architecture selection as a sequential decision
problem, using accuracy as reward signal. Evolutionary
algorithms\sidenote{\textbf{Evolutionary NAS}: Treating architectures as
genomes evolved through mutation (adding/removing layers) and crossover
(combining parent architectures). AmoebaNet required 3,150 GPU-days
achieving 83.9\% ImageNet accuracy. Regularized evolution outperformed
RL-based NAS in head-to-head comparisons. Modern approaches combine
evolution with weight-sharing for 1000× speedup. } evolve populations of
architectures through mutation and crossover. Gradient-based methods
enable differentiable architecture search, reducing computational cost.

\subsubsection{Search Space
Definition}\label{sec-model-compression-search-space-definition-654a}

The first step in NAS is determining the set of architectures it is
allowed to explore, known as the search space. The size and structure of
this space directly affect how efficiently NAS can discover optimal
models. A well-defined search space must be broad enough to allow
innovation while remaining narrow enough to prevent unnecessary
computation on impractical designs.

A typical NAS search space consists of modular building blocks that
define the structure of the model. These include the types of layers
available for selection, such as standard convolutions, depthwise
separable convolutions, attention mechanisms, and residual blocks. The
search space also defines constraints on network depth and width,
specifying how many layers the model can have and how many channels each
layer should include. NAS considers activation functions, such as ReLU,
Swish, or GELU, which influence both model expressiveness and
computational efficiency.

Other architectural decisions within the search space include kernel
sizes, receptive fields, and skip connections, which impact both feature
extraction and model complexity. Some NAS implementations also
incorporate hardware-aware optimizations, ensuring that the discovered
architectures align with specific hardware, such as GPUs, TPUs, or
mobile CPUs.

The choice of search space determines the extent to which NAS can
optimize a model. If the space is too constrained, the search algorithm
may fail to discover novel and efficient architectures. If it is too
large, the search becomes computationally expensive, requiring extensive
resources to explore a vast number of possibilities. Striking the right
balance ensures that NAS can efficiently identify architectures that
improve upon human-designed models.

\subsubsection{Search Space
Exploration}\label{sec-model-compression-search-space-exploration-9a78}

Once the search space is defined, NAS must determine how to explore
different architectures effectively. The search strategy guides this
process by selecting which architectures to evaluate based on past
observations. An effective search strategy must balance exploration
(testing new architectures) with exploitation (refining promising
designs).

Several methods have been developed to explore the search space
efficiently. Reinforcement learning-based NAS formulates the search
process as a decision-making problem, where an agent sequentially
selects architectural components and receives a reward signal based on
the performance of the generated model. Over time, the agent learns to
generate better architectures by maximizing this reward. While
effective, reinforcement learning-based NAS can be computationally
expensive because it requires training many candidate models before
converging on an optimal design.

An alternative approach uses evolutionary algorithms, which maintain a
population of candidate architectures and iteratively improve them
through mutation and selection. Stronger architectures, which possess
higher accuracy and efficiency, are retained, while modifications such
as changing layer types or filter sizes introduce new variations. This
approach has been shown to balance exploration and computational
feasibility more effectively than reinforcement learning-based NAS.

More recent methods, such as gradient-based NAS, introduce
differentiable parameters that represent architectural choices. Instead
of treating architectures as discrete entities, gradient-based methods
optimize both model weights and architectural parameters simultaneously
using standard gradient descent. This significantly reduces the
computational cost of the search, making NAS more practical for
real-world applications.

The choice of search strategy has a direct impact on the feasibility of
NAS. Early NAS methods that relied on reinforcement learning required
weeks of GPU computation to discover a single architecture. More recent
methods, especially those based on gradient-based search, have
significantly reduced this cost, making NAS more efficient and
accessible.

\subsubsection{Candidate Architecture
Evaluation}\label{sec-model-compression-candidate-architecture-evaluation-7add}

Every architecture explored by NAS must be evaluated based on a set of
predefined criteria. While accuracy is a core metric, NAS also optimizes
for efficiency constraints to ensure that models are practical for
deployment. The evaluation process determines whether an architecture
should be retained for further refinement or discarded in favor of more
promising designs.

The primary evaluation metrics include computational complexity, memory
consumption, inference latency, and energy
efficiency\sidenote{\textbf{NAS Evaluation Metrics}: Multi-objective
optimization balancing accuracy, latency, memory, and energy creates
Pareto frontiers of non-dominated architectures. Practitioners select
architectures based on deployment constraints: edge devices prioritize
latency/energy; servers prioritize throughput. Scalarization weights or
evolutionary multi-objective methods explore these tradeoffs
systematically. }. Computational complexity, often measured in FLOPs,
determines the overall resource demands of a model. NAS favors
architectures that achieve high accuracy while reducing unnecessary
computations. Memory consumption, which includes both parameter count
and activation storage, ensures that models fit within hardware
constraints. For real-time applications, inference latency is a key
factor, with NAS selecting architectures that minimize execution time on
specific hardware platforms. Finally, some NAS implementations
explicitly optimize for power consumption, ensuring that models are
suitable for mobile and edge devices.

For example, FBNet\sidenote{\textbf{FBNet (Facebook Net)}:
Differentiable NAS achieving 74.9\% ImageNet accuracy at 23ms on Samsung
S8, 15\% faster than MobileNetV2 (\citeproc{ref-wu2019fbnet}{B. Wu et
al. 2019}). Uses device-specific latency lookup tables instead of proxy
metrics, enabling architecture customization per deployment target.
Demonstrates that hardware-software co-design through NAS significantly
outperforms one-size-fits-all architectures. }, a NAS-generated
architecture optimized for mobile inference, incorporated latency
constraints into the search process.

By integrating these constraints into the search process, NAS
systematically discovers architectures that balance accuracy,
efficiency, and hardware adaptability. Instead of manually fine-tuning
these trade-offs, NAS automates the selection of optimal architectures,
ensuring that models are well-suited for real-world deployment
scenarios.

\subsubsection{The NAS Optimization
Problem}\label{sec-model-compression-nas-optimization-problem-376d}

NAS is a \textbf{bi-level optimization problem}: the outer loop searches
the architecture space \(\mathcal{A}\), while the inner loop trains
candidate architectures to evaluate performance. Formally, we seek the
optimal architecture \(\alpha^*\) that minimizes validation loss
\(\mathcal{L}_{\text{val}}\) under constraints \(C\) (latency, memory):

\[
\alpha^* = \arg\min_{\alpha \in \mathcal{A}} \mathcal{L}_{\text{val}}(w^*(\alpha), \alpha) \quad \text{subject to} \quad C(\alpha) \leq C_{\text{max}}
\]

where \(w^*(\alpha)\) represents the optimal weights for architecture
\(\alpha\), obtained by minimizing training loss:

\[
w^*(\alpha) = \arg\min_{w} \mathcal{L}_{\text{train}}(w, \alpha)
\]

The core challenge is the cost of the inner loop: evaluating each
candidate requires expensive training. A search space with just 10
choices across 20 layers yields \(10^{20}\) architectures, making
exhaustive search impossible. Efficient NAS methods address this by
restricting the search space, using faster search strategies, or
accelerating evaluation.

\subsubsection{Search Space
Design}\label{sec-model-compression-search-space-design-451c}

The search space defines what architectures NAS can discover.
Well-designed search spaces incorporate domain knowledge to focus search
on promising regions while remaining flexible enough to discover novel
patterns.

\textbf{Cell-Based Search Spaces}

Rather than searching entire network architectures, cell-based NAS
searches for reusable computational blocks (cells) that can be stacked
to form complete networks. For example, a convolutional cell might
choose from operations like 3×3 convolution, 5×5 convolution, depthwise
separable convolution, max pooling, or identity connections. A
simplified cell with 4 nodes and 2 operations per edge yields roughly
10,000 possible cell designs, far more tractable than searching full
architectures. EfficientNet uses this approach to discover scalable cell
designs that generalize across different model sizes.

\textbf{Hardware-Aware Search Spaces}

Hardware-aware NAS extends search spaces to include deployment
constraints as first-class objectives. Rather than optimizing solely for
accuracy and FLOPs, the search explicitly minimizes actual latency on
target hardware (mobile CPUs, GPUs, edge accelerators). MobileNetV3's
search space includes a latency prediction model that estimates
inference time for each candidate architecture on Pixel phones without
actually deploying them. This hardware-in-the-loop approach ensures
discovered architectures run efficiently on real devices rather than
just achieving low theoretical FLOP counts.

\subsubsection{Search
Strategies}\label{sec-model-compression-search-strategies-34c5}

Search strategies determine how to explore the architecture space
efficiently without exhaustive enumeration.
Table~\ref{tbl-nas-strategies} compares the trade-offs between search
cost, architectural diversity, and optimality guarantees for each
approach.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2206}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1765}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2794}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3088}}@{}}
\caption{\textbf{NAS Search Strategy Comparison}: Trade-offs between
search efficiency, use cases, and limitations for different NAS
approaches. Reinforcement learning offers unconstrained exploration at
high cost, evolutionary methods leverage parallelism, and gradient-based
approaches achieve dramatic speedups with potential optimality
trade-offs.}\label{tbl-nas-strategies}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Strategy}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Search Efficiency}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{When to Use}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Key Challenge}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Strategy}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Search Efficiency}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{When to Use}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Key Challenge}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Reinforcement Learning} \textbf{Evolutionary Algorithms}
\textbf{Gradient-Based (DARTS)} & 400-1000 GPU-days 200-500 GPU-days 1-4
GPU-days & Novel domains, unconstrained search Parallel infrastructure
available Limited compute budget & High computational cost Requires
large populations May converge to suboptimal local minima \\
\end{longtable}

Reinforcement learning based NAS treats architecture search as a
sequential decision problem where a controller generates architectures
and receives accuracy as reward. The controller (typically an LSTM)
learns to propose better architectures over time through policy gradient
optimization. While this approach discovered groundbreaking
architectures like NASNet, the sequential nature limits parallelism and
requires hundreds of GPU-days.

Evolutionary algorithms maintain a population of candidate architectures
and iteratively apply mutations (changing operations, adding
connections) and crossover (combining parent architectures) to generate
offspring. Fitness-based selection retains high-performing architectures
for the next generation. AmoebaNet used evolution to achieve
state-of-the-art results, with massive parallelism amortizing the cost
across thousands of workers.

Gradient-based methods like DARTS (Differentiable Architecture Search)
represent the search space as a continuous relaxation where all possible
operations are weighted combinations. Rather than discrete sampling,
DARTS optimizes architecture weights and model weights jointly using
gradient descent. By making the search differentiable, DARTS reduces
search cost from hundreds to just 1-4 GPU-days, though the continuous
relaxation may miss discrete architectural patterns that discrete search
methods discover.

\subsubsection{NAS in
Practice}\label{sec-model-compression-nas-practice-2d81}

Hardware-aware NAS moves beyond FLOPs as a proxy for efficiency,
directly optimizing for actual deployment metrics. MnasNet's search
incorporates a latency prediction model trained on thousands of
architecture-latency pairs measured on actual mobile phones. The search
objective combines accuracy and latency through a weighted product:

\[
\text{Reward}(\alpha) = \text{Accuracy}(\alpha) \times \left(\frac{L(\alpha)}{L_{\text{target}}}\right)^\beta
\]

where \(L(\alpha)\) is measured latency, \(L_{\text{target}}\) is the
latency constraint, and \(\beta\) controls the accuracy-latency
trade-off. This formulation penalizes architectures that exceed latency
targets while rewarding those that achieve high accuracy within the
budget. MnasNet discovered that inverted residuals with varying
expansion ratios achieve better accuracy-latency trade-offs than uniform
expansion, a design insight that manual exploration likely would have
missed.

\subsubsection{When to Use
NAS}\label{sec-model-compression-use-nas-2d7e}

Neural Architecture Search is a powerful tool, but its significant
computational cost demands careful consideration of when the investment
is justified.

NAS becomes worthwhile when dealing with novel hardware platforms with
unique constraints (new accelerator architectures, extreme edge devices)
where existing architectures are poorly optimized. It also makes sense
for deployment at massive scale (billions of inferences) where even
1-2\% efficiency improvements justify the upfront search cost, or when
multiple deployment configurations require architecture families (cloud,
edge, mobile) that can amortize one search across many variants.

Conversely, avoid NAS when working with standard deployment constraints
(e.g., ResNet-50 accuracy on NVIDIA GPUs) where well-optimized
architectures already exist. Similarly, if the compute budget is limited
(less than 100 GPU-days available), even efficient NAS methods like
DARTS become infeasible. Rapidly changing requirements also make NAS
impractical, as architecture selection may become obsolete before the
search completes.

For most practitioners, starting with existing NAS-discovered
architectures (EfficientNet, MobileNetV3, MnasNet) provides better ROI
than running NAS from scratch. These architectures are highly tuned and
generalize well across tasks. Reserve custom NAS for scenarios with
truly novel constraints or deployment scales that justify the
investment.

\subsubsection{Architecture
Examples}\label{sec-model-compression-architecture-examples-b5b5}

NAS has been successfully used to design several state-of-the-art
architectures that outperform manually designed models in terms of
efficiency and accuracy. These architectures illustrate how NAS
integrates scaling optimization, computation reduction, memory
efficiency, and hardware-aware design into an automated process.

One of the most well-known NAS-generated models is EfficientNet, which
was discovered using a NAS framework that searched for the most
effective combination of depth, width, and resolution scaling. Unlike
traditional scaling strategies that independently adjust these factors,
NAS optimized the model using compound scaling, which applies a fixed
set of scaling coefficients to ensure that the network grows in a
balanced way. EfficientNet achieves higher accuracy with fewer
parameters and lower FLOPs than previous architectures, making it ideal
for both cloud and mobile deployment.

Another key example is MobileNetV3, which used NAS to optimize its
network structure for mobile hardware. The search process led to the
discovery of inverted residual blocks with squeeze-and-excitation
layers, which improve accuracy while reducing computational cost. NAS
also selected optimized activation functions and efficient depthwise
separable convolutions, leading to a \(5\times\) reduction in FLOPs
compared to earlier MobileNet versions.

FBNet, another NAS-generated model, was specifically optimized for
real-time inference on mobile CPUs. Unlike architectures designed for
general-purpose acceleration, FBNet's search process explicitly
considered latency constraints during training, ensuring that the final
model runs efficiently on low-power hardware. Similar approaches have
been used in TPU-optimized NAS models, where the search process is
guided by hardware-aware cost functions to maximize parallel execution
efficiency.

NAS has also been applied beyond convolutional networks. NAS-BERT
explores transformer-based architectures, searching for efficient model
structures that retain strong natural language understanding
capabilities while reducing compute and memory overhead. NAS proves
useful in designing efficient vision transformers (ViTs) by
automatically discovering lightweight attention mechanisms tailored for
edge AI applications.

Each of these NAS-generated models demonstrates how automated
architecture search can uncover novel efficiency trade-offs that may not
be immediately intuitive to human designers. Explicit encoding of
efficiency constraints into the search process enables NAS to
systematically produce architectures that are more computationally
efficient, memory-friendly, and hardware-adapted than those designed
manually (\citeproc{ref-radosavovic2020designing}{Radosavovic et al.
2020}).

The structural techniques covered so far---pruning, distillation,
factorization, and NAS---all optimize \emph{what} computations the model
performs: which parameters exist, which connections remain, and how the
architecture is structured. But even a perfectly pruned model with an
optimal architecture faces a fundamental constraint: every weight and
activation must be stored and processed at some numerical precision.

This brings us to the second dimension of our optimization framework:
\emph{how precisely} should those computations be performed? A 32-bit
floating-point number uses 4 bytes of memory and requires expensive
floating-point arithmetic. An 8-bit integer uses 1 byte and enables fast
integer math. For many models, this 4× reduction in precision translates
directly to 4× reduction in memory bandwidth---and since LLM inference
is bandwidth-bound, this means 4× faster token generation. The accuracy
cost? Often less than 1\%.

Quantization is arguably the single most impactful optimization
technique for deployment, especially for large language models. It
requires no architectural changes, applies post-training in many cases,
and delivers immediate, hardware-agnostic benefits.

\phantomsection\label{quiz-question-sec-model-compression-structural-model-optimization-methods-8ed4}
\begin{fbx}{callout-quiz-question}{Self-Check: Question 1.4}{}
\phantomsection\label{quiz-question-sec-model-compression-structural-model-optimization-methods-8ed4}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Which of the following best describes the purpose of gradient
  checkpointing in neural network optimization?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii})}
  \tightlist
  \item
    To reduce memory usage by recomputing intermediate activations
    during backpropagation.
  \item
    To improve model accuracy by increasing the number of parameters.
  \item
    To enhance computational speed by parallelizing model training.
  \item
    To eliminate redundant parameters through pruning.
  \end{enumerate}
\item
  Explain the trade-offs involved in model pruning and how it affects
  deployment in different environments.
\item
  The process of systematically removing redundant parameters from a
  neural network while preserving accuracy is known as \_\_\_\_. This
  technique reduces model size and computational cost.
\item
  What is a primary advantage of using parallel processing patterns in
  machine learning model optimization?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii})}
  \tightlist
  \item
    It increases the number of parameters in the model.
  \item
    It reduces the need for gradient checkpointing.
  \item
    It allows for faster training by utilizing multiple cores
    simultaneously.
  \item
    It eliminates the need for model pruning.
  \end{enumerate}
\end{enumerate}

\noindent\hspace*{1.25em}\hyperref[quiz-answer-sec-model-compression-structural-model-optimization-methods-8ed4]{\textbf{See Answer~$\rightarrow$}}

\end{fbx}

\section{Quantization and Precision
Optimization}\label{sec-model-compression-quantization-precision-optimization-31d3}

\phantomsection\label{callout-definitionux2a-1.6}
\begin{fbx}{callout-definition}{Definition: }{Quantization}
\phantomsection\label{callout-definition*-1.6}
\textbf{Quantization} refers to a model compression technique that
reduces \emph{numerical precision} of weights and activations from
floating-point to lower-bit representations, decreasing \emph{model
size} and \emph{computational cost} with minimal accuracy loss.

\end{fbx}

Quantization\sidenote{\textbf{Quantization}: From Latin ``quantus'' (how
much), via quantum physics where it describes discrete energy levels. In
signal processing (1940s-1960s), quantization meant mapping continuous
values to discrete levels, introducing ``quantization error.'' ML
borrowed this term directly: converting FP32 weights to INT8 maps
continuous values to 256 discrete levels, trading precision for
efficiency. The error analysis parallels signal processing exactly. }
affects every neural network weight and activation stored at some
numerical precision: FP32 (32 bits), FP16 (16 bits), INT8 (8 bits), or
lower.

This choice directly impacts three system properties:

\begin{itemize}
\tightlist
\item
  \textbf{Memory}: An INT8 model is 4× smaller than FP32, enabling
  deployment on smaller devices
\item
  \textbf{Bandwidth}: Loading INT8 weights requires 4× less memory
  bandwidth, directly accelerating bandwidth-bound inference
\item
  \textbf{Compute}: INT8 arithmetic is faster and cheaper than FP32 on
  most hardware (\citeproc{ref-gupta2015deep}{Gupta et al. 2015};
  \citeproc{ref-wang2019benchmarking}{Y. E. Wang, Wei, and Brooks 2019})
\end{itemize}

The accuracy cost of reduced precision varies by model and technique.
CNNs typically tolerate INT8 quantization with \textless1\% accuracy
loss; transformers may require more care. This section covers three
approaches in increasing complexity: \textbf{post-training quantization}
(PTQ) for rapid deployment, \textbf{quantization-aware training} (QAT)
for production systems requiring minimal accuracy loss, and
\textbf{extreme quantization} (INT4, binary) for the most constrained
environments.

\subsection{Precision and
Energy}\label{sec-model-compression-precision-energy-7eea}

Efficient numerical representations enable significant reductions in
storage requirements, computation latency, and power usage, making them
beneficial for mobile AI, embedded systems, and cloud inference.
Precision levels can be tuned to specific hardware capabilities,
maximizing throughput on AI accelerators such as GPUs, TPUs, NPUs, and
edge AI chips.

\subsubsection{Energy
Costs}\label{sec-model-compression-energy-costs-fb60}

Beyond computational and memory benefits, the energy costs associated
with different numerical precisions further highlight the benefits of
reducing precision. Figure~\ref{fig-quantized-energy} quantifies these
energy differences: a 32-bit floating-point addition (FAdd) consumes
approximately 0.9 pJ, whereas a 16-bit floating-point addition requires
only 0.4 pJ. Similarly, a 32-bit integer addition costs 0.1 pJ, while an
8-bit integer addition is significantly lower at just 0.03 pJ. These
savings compound when considering large-scale models operating across
billions of operations, supporting sustainability goals. The energy
efficiency gained through quantization also enhances security posture by
reducing the computational resources available to potential attackers.

\begin{figure}[htb]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/458dbec578c9c7a32d2f991fdc936d5d3221a3e6.pdf}}

}

\caption{\label{fig-quantized-energy}\textbf{Energy Costs}: Lower
precision reduces computational energy, illustrating trade-offs in model
accuracy. Machine learning systems can optimize efficiency by reducing
floating-point operations from 32-bit to 16-bit or even lower for
significant savings. Source: IEEE spectrum.}

\end{figure}%

Beyond direct compute savings, reducing numerical precision has a
significant impact on memory energy consumption, which often dominates
total system power. Lower-precision representations reduce data storage
requirements and memory bandwidth usage, leading to fewer and more
efficient memory accesses. This is important because accessing memory,
particularly off-chip DRAM, is far more energy-intensive than performing
arithmetic operations. For instance, DRAM accesses require orders of
magnitude more energy (1.3--2.6 nJ) compared to cache accesses (e.g., 10
pJ for an 8 KB L1 cache access). The breakdown of instruction energy
underscores the cost of moving data within the memory hierarchy, where
an instruction's total energy can be significantly impacted by memory
access patterns\sidenote{\textbf{Energy Efficiency Metrics}: INT8
quantization reduces energy consumption by 4-8x over FP32 on supported
hardware. MobileNetV2 INT8 consumes 47mJ vs.~312mJ FP32 per inference on
Cortex-A75. ResNet-50 on TPU v4 achieves 0.9 TOPS/Watt vs.~0.3 TOPS/Watt
on V100 GPU. }.

By reducing numerical precision, models can not only execute
computations more efficiently but also reduce data movement, leading to
lower overall energy consumption. This is important for hardware
accelerators and edge devices, where memory bandwidth and power
efficiency are key constraints.

\subsubsection{Performance
Gains}\label{sec-model-compression-performance-gains-53a1}

Figure~\ref{fig-quantization_impact} illustrates the impact of
quantization on both inference time and model size using a stacked bar
chart with a dual-axis representation. The left bars in each category
show inference time improvements when moving from FP32 to INT8, while
the right bars depict the corresponding reduction in model size. The
results indicate that quantized models achieve up to \(4\times\) faster
inference while reducing storage requirements by a factor of
\(4\times\), making them highly suitable for deployment in
resource-constrained environments.

\begin{figure}[htb]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/aa5391e78a3f9af748f673b8ca472563fdd97091.pdf}}

}

\caption{\label{fig-quantization_impact}\textbf{Quantization Impact}:
Moving from FP32 to INT8 reduces inference time by up to 4 times while
decreasing model size by a factor of 4, making models more efficient for
resource-constrained environments.}

\end{figure}%

\phantomsection\label{callout-notebook-1.1}
\begin{fbx}{callout-notebook}{AI Engineer’s Notebook 1.1: }{Engineering Calculation: Quantization Savings}
\phantomsection\label{callout-notebook-1.1}
\textbf{Scenario}: Deploying Llama 3 8B (8 billion parameters).

\textbf{FP16 (Half Precision)} - \textbf{Size}:
\(8 \times 10^9 \times 2 \text{ bytes (16-bit)} = 16 \text{ GB}\) -
\textbf{Hardware Req}: Requires 24GB GPU (e.g., A10G, 3090, 4090).

\textbf{INT4 (4-bit Quantization)} - \textbf{Size}:
\(8 \times 10^9 \times 0.5 \text{ bytes (4-bit)} = 4 \text{ GB}\) -
\textbf{Hardware Req}: Fits comfortably on 8GB GPU (e.g., T4, consumer
laptops).

\textbf{Impact}: 4x compression allows deployment on commodity hardware,
reducing cost by 5-10x.

\end{fbx}

However, reducing numerical precision introduces trade-offs.
Lower-precision formats can lead to numerical instability and
quantization noise, potentially affecting model accuracy. Some
architectures, such as large transformer-based NLP models, tolerate
quantization well, whereas others may experience significant
degradation. Thus, selecting the appropriate numerical precision
requires balancing accuracy constraints, hardware support, and
efficiency gains.

\begin{figure}

\centering{

\includegraphics[width=0.8\linewidth,height=\textheight,keepaspectratio]{contents/vol1/optimizations/images/png/modeloptimization_quant_hist.png}

}

\caption{\label{fig-quantization}Quantization error weighted by p(x).}

\end{figure}%

Figure~\ref{fig-quantization} reveals how the quantization error
distribution differs across numerical formats, with each format
introducing varying levels of quantization noise that directly influence
model accuracy and stability.

\phantomsection\label{callout-perspectiveux2a-1.7}
\begin{fbx}{callout-perspective}{Systems Perspective: }{Napkin Math: The Quantization Speedup (Compute-Bound)}
\phantomsection\label{callout-perspective*-1.7}
\textbf{Problem}: You have a compute-bound matrix multiplication (e.g.,
in a Transformer MLP block). You switch from FP16 to INT8. What is the
expected speedup?

\textbf{The Math}: On modern hardware with dedicated INT8 units:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Tensor Core Throughput}: NVIDIA A100 delivers 312 TFLOPS for
  FP16 vs 624 TOPS for INT8---a 2× peak throughput increase.
\item
  \textbf{Memory Bandwidth}: INT8 weights are half the size, so loading
  them from memory takes half the time.
\item
  \textbf{Combined Effect}: For compute-bound operations, the speedup is
  primarily from compute throughput: \textbf{\textasciitilde2× speedup}.
\end{enumerate}

\textbf{The Systems Insight}: The speedup from quantization depends on
the bottleneck. Compute-bound operations (large batch sizes, high
arithmetic intensity) see \textasciitilde2× from faster INT8 units.
Memory-bound operations (small batches, attention) see up to
\textasciitilde4× because both bandwidth and compute improve. The
earlier Napkin Math on LLM inference shows the memory-bound case.

\end{fbx}

\subsection{Numerical Format
Comparison}\label{sec-model-compression-numerical-format-comparison-3866}

Table~\ref{tbl-numerics} compares commonly used numerical precision
formats in machine learning, each exhibiting distinct trade-offs in
storage efficiency, computational speed, and energy consumption.
Emerging formats like FP8 and TF32 have been introduced to further
optimize performance, especially on AI accelerators.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 10\tabcolsep) * \real{0.1948}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 10\tabcolsep) * \real{0.0693}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 10\tabcolsep) * \real{0.1472}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 10\tabcolsep) * \real{0.1991}}
  >{\raggedright\arraybackslash}p{(\linewidth - 10\tabcolsep) * \real{0.1039}}
  >{\raggedright\arraybackslash}p{(\linewidth - 10\tabcolsep) * \real{0.2684}}@{}}
\caption{Comparison of numerical precision
formats.}\label{tbl-numerics}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Precision Format}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Bit-Width}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Storage Reduction (vs FP32)}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Compute Speed (vs FP32)}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Power Consumption}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Use Cases}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Precision Format}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Bit-Width}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Storage Reduction (vs FP32)}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Compute Speed (vs FP32)}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Power Consumption}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Use Cases}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{FP32 (Single-Precision Floating Point)} & 32-bit & Baseline (1×)
& Baseline (1×) & High & Training \& inference (general-purpose) \\
\textbf{FP16 (Half-Precision Floating Point)} & 16-bit & 2× smaller & 2×
faster on FP16-optimized hardware & Lower & Accelerated training,
inference (NVIDIA Tensor Cores, TPUs) \\
\textbf{bfloat16 (Brain Floating Point)} & 16-bit & 2× smaller & Similar
speed to FP16, better dynamic range & Lower & Training on TPUs,
transformer-based models \\
\textbf{TF32 (TensorFloat-32)} & 19-bit & Similar to FP16 & Up to 8×
faster on NVIDIA Ampere GPUs & Lower & Training on NVIDIA GPUs \\
\textbf{FP8 (Floating-Point 8-bit)} & 8-bit & 4× smaller & Faster than
INT8 in some cases & Significantly lower & Efficient training/inference
(H100, AI accelerators) \\
\textbf{INT8 (8-bit Integer)} & 8-bit & 4× smaller & 4--8× faster than
FP32 & Significantly lower & Quantized inference (Edge AI, mobile AI,
NPUs) \\
\textbf{INT4 (4-bit Integer)} & 4-bit & 8× smaller & Hardware-dependent
& Extremely low & Ultra-low-power AI, experimental quantization \\
\textbf{Binary/Ternary (1-bit / 2-bit)} & 1--2-bit & 16--32× smaller &
Highly hardware-dependent & Lowest & Extreme efficiency (binary/ternary
neural networks) \\
\end{longtable}

FP16 and bfloat16 formats provide moderate efficiency gains while
preserving model accuracy. Many AI accelerators, such as NVIDIA Tensor
Cores and TPUs, include dedicated support for FP16 computations,
enabling \(2\times\) faster matrix operations compared to FP32.
BFloat16, in particular, retains the same 8-bit exponent as FP32 but
with a reduced 7-bit mantissa, allowing it to maintain a similar dynamic
range (\textasciitilde{}\(10^{-38}\) to \(10^{38}\)) while sacrificing
precision. In contrast, FP16, with its 5-bit exponent and 10-bit
mantissa, has a significantly reduced dynamic range
(\textasciitilde{}\(10^{-5}\) to \(10^5\)), making it more suitable for
inference rather than training. Since BFloat16 preserves the exponent
size of FP32, it better handles extreme values encountered during
training, whereas FP16 may struggle with underflow or overflow. This
makes BFloat16 a more robust alternative for deep learning workloads
that require a wide dynamic range.

Figure~\ref{fig-3float} illustrates how bit-width allocations impact the
trade-offs between precision and numerical
range.\footnote{The dynamic range of a floating-point format is determined by its exponent bit-width and bias. FP32 and BFloat16 both use an 8-bit exponent with a bias of 127, resulting in an exponent range of $[-126, 127]$ and an approximate numerical range of $10^{-38}$ to $10^{38}$. FP16, with a 5-bit exponent and a bias of 15, has an exponent range of $[-14, 15]$, leading to a more constrained numerical range of roughly $10^{-5}$ to $10^5$. This reduced range in FP16 can lead to numerical instability in training, whereas BFloat16 retains FP32's broader range, making it more suitable for training deep neural networks.}

\begin{figure}[htb]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/8e66e3a5f006292c5f67a3106f095e540ad591bf.pdf}}

}

\caption{\label{fig-3float}\textbf{Floating-Point Precision}:
Reduced-precision formats like FP16 and bfloat16 trade off numerical
range for computational efficiency and memory savings. Bfloat16
maintains the exponent size of FP32, preserving its dynamic range and
suitability for training, while FP16's smaller exponent limits its use
to inference or carefully scaled training scenarios.}

\end{figure}%

INT8 precision offers more aggressive efficiency improvements for
inference workloads. Many quantized models use INT8 for inference,
reducing storage by \(4\times\) while accelerating computation by
4--8\(\times\) on optimized hardware. INT8 is widely used in mobile and
embedded AI, where energy constraints are significant.

\phantomsection\label{callout-notebook-1.2}
\begin{fbx}{callout-notebook}{AI Engineer’s Notebook 1.2: }{Calculating Quantization Memory Savings}
\phantomsection\label{callout-notebook-1.2}
Quantizing model weights from FP32 (4 bytes) to INT8 (1 byte) offers a
theoretical \(4\times\) reduction in model size. For a model with \(P\)
parameters:

\[ \text{Size}_{\text{FP32}} = P \times 4 \text{ bytes} \quad \text{vs} \quad \text{Size}_{\text{INT8}} = P \times 1 \text{ byte} \]

For a 7 billion parameter model (e.g., Llama 2 7B):
\[ \text{Size}_{\text{FP32}} = 7 \times 10^9 \times 4 \text{ B} \approx 28 \text{ GB} \]
\[ \text{Size}_{\text{INT8}} = 7 \times 10^9 \times 1 \text{ B} \approx 7 \text{ GB} \]

This optimization enables the model to fit on a single consumer GPU
(e.g., 24GB VRAM), whereas the FP32 version typically requires
data-center class hardware or multi-GPU setups.

\end{fbx}

Binary and ternary networks represent the extreme end of quantization,
where weights and activations are constrained to 1-bit (binary) or 2-bit
(ternary) values. This results in massive storage and energy savings,
but model accuracy often degrades significantly unless specialized
architectures are used.

\subsection{Precision Reduction
Trade-offs}\label{sec-model-compression-precision-reduction-tradeoffs-ae15}

Reducing numerical precision in machine learning systems offers
significant gains in efficiency, including lower memory requirements,
reduced power consumption, and increased computational throughput.
However, these benefits come with trade-offs, as lower-precision
representations introduce numerical error and quantization noise, which
can affect model accuracy. The extent of this impact depends on multiple
factors, including the model architecture, the dataset, and the specific
precision format used.

Models exhibit varying levels of tolerance to quantization. Large-scale
architectures, such as convolutional neural networks and
transformer-based models, often retain high accuracy even when using
reduced-precision formats such as bfloat16 or INT8. In contrast, smaller
models or those trained on tasks requiring high numerical precision may
experience greater degradation in performance. Not all layers within a
neural network respond equally to precision reduction. Certain layers,
such as batch normalization and attention mechanisms, may be more
sensitive to numerical precision than standard feedforward layers. As a
result, techniques such as mixed-precision training, where different
layers operate at different levels of precision, can help maintain
accuracy while optimizing computational efficiency.

Hardware support is another important factor in determining the
effectiveness of precision reduction. AI accelerators, including GPUs,
TPUs, and NPUs, are designed with dedicated low-precision arithmetic
units that enable efficient computation using FP16, bfloat16, INT8, and,
more recently, FP8. These architectures exploit reduced precision to
perform high-throughput matrix operations, improving both speed and
energy efficiency. In contrast, general-purpose CPUs often lack
specialized hardware for low-precision computations, limiting the
potential benefits of numerical quantization. The introduction of newer
floating-point formats, such as TF32 for NVIDIA GPUs and FP8 for AI
accelerators, seeks to optimize the trade-off between precision and
efficiency, offering an alternative for hardware that is not explicitly
designed for extreme quantization.

In addition to hardware constraints, reducing numerical precision
impacts power consumption. Lower-precision arithmetic reduces the number
of required memory accesses and simplifies computational operations,
leading to lower overall energy use. This approach is particularly
advantageous for energy-constrained environments such as mobile devices
and edge AI systems. At the extreme end, ultra-low precision formats,
including INT4 and binary/ternary representations, provide significant
reductions in power and memory usage. However, these formats often
require specialized architectures to compensate for the accuracy loss
associated with such aggressive quantization.

To mitigate accuracy loss associated with reduced precision, various
quantization strategies can be employed. Ultimately, selecting the
appropriate numerical precision for a given machine learning model
requires balancing efficiency gains against accuracy constraints. This
selection depends on the model's architecture, the computational
requirements of the target application, and the underlying hardware's
support for low-precision operations. By using advancements in both
hardware and software optimization techniques, practitioners can
effectively integrate lower-precision numerics into machine learning
pipelines, maximizing efficiency while maintaining performance.

\subsection{Precision Reduction
Strategies}\label{sec-model-compression-precision-reduction-strategies-9cbc}

Reducing numerical precision is an important optimization technique for
improving the efficiency of machine learning models. By lowering the
bit-width of weights and activations, models can reduce memory
footprint, improve computational throughput, and decrease power
consumption. However, naive quantization can introduce quantization
errors, leading to accuracy degradation. To address this, different
precision reduction strategies have been developed, allowing models to
balance efficiency gains while preserving predictive performance.

Quantization techniques can be applied at different stages of a model's
lifecycle. Post-training quantization reduces precision after training,
making it a simple and low-cost approach for optimizing inference.
Quantization-aware training incorporates quantization effects into the
training process, enabling models to adapt to lower precision and retain
higher accuracy. Mixed-precision training leverages hardware support to
dynamically assign precision levels to different computations,
optimizing execution efficiency without sacrificing accuracy.

Figure~\ref{fig-quantization-roadmap} organizes quantization techniques
into three progressive tiers based on implementation complexity,
resource requirements, and target use cases.

\begin{figure}[htb]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/eeee0468842cb0232fca773b1fafe8327ed8b4db.pdf}}

}

\caption{\label{fig-quantization-roadmap}\textbf{Quantization Complexity
Roadmap}: Three progressive tiers of quantization techniques, from
foundational approaches suitable for quick deployment to research
frontier methods for extreme resource constraints, reflecting increasing
implementation effort, resource requirements, and potential accuracy
trade-offs.}

\end{figure}%

\subsubsection{Post-Training
Quantization}\label{sec-model-compression-posttraining-quantization-924a}

Quantization is the specific algorithmic technique that enables
significant memory bandwidth reduction when addressing the memory wall.
These quantization methods provide standardized APIs across different
platforms, showing exactly how to implement the efficiency principles
established earlier.

Post-training quantization (PTQ) reduces numerical precision after
training, converting weights and activations from high-precision formats
(FP32) to lower-precision representations (INT8 or FP16) without
retraining (\citeproc{ref-jacob2018quantization}{Jacob et al. 2018a}).
This achieves smaller model sizes, faster computation, and reduced
energy consumption, making it practical for resource-constrained
environments such as mobile devices, edge AI systems, and cloud
inference platforms (\citeproc{ref-wu2020integer}{H. Wu et al. 2020}).

PTQ's key advantage is low computational cost---it requires no
retraining or access to training data. However, reducing precision
introduces quantization error that can degrade accuracy, especially for
tasks requiring fine-grained numerical precision. Machine learning
frameworks (TensorFlow Lite, ONNX Runtime, PyTorch) provide built-in PTQ
support.

\paragraph{PTQ
Functionality}\label{sec-model-compression-ptq-functionality-3aaf}

PTQ converts a trained model's weights and activations from
high-precision floating-point representations (e.g., FP32) to
lower-precision formats (e.g., INT8 or FP16). This process reduces the
memory footprint of the model, accelerates inference, and lowers power
consumption. However, since lower-precision formats have a smaller
numerical range, quantization introduces rounding errors, which can
impact model accuracy.

The core mechanism behind PTQ is scaling and mapping high-precision
values into a reduced numerical range. A widely used approach is uniform
quantization, which maps floating-point values to discrete integer
levels using a consistent scaling factor. In uniform quantization, the
interval between each quantized value is constant, simplifying
implementation and ensuring efficient execution on hardware. The
quantized value \(q\) is computed as: \[
q = \text{round} \left(\frac{x}{s} \right)
\] where:

\begin{itemize}
\tightlist
\item
  \(q\) is the quantized integer representation,
\item
  \(x\) is the original floating-point value,
\item
  \(s\) is a scaling factor that maps the floating-point range to the
  available integer range.
\end{itemize}

Listing~\ref{lst-quantization_example} demonstrates uniform quantization
from FP32 to INT8, achieving 4x memory reduction while measuring the
resulting quantization error.

\begin{codelisting}

\caption{\label{lst-quantization_example}\textbf{Uniform Quantization}:
Converts FP32 weights to INT8 format, achieving 4x memory reduction
while measuring quantization error.}

\centering{

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ torch}

\CommentTok{\# Original FP32 weights}
\NormalTok{weights\_fp32 }\OperatorTok{=}\NormalTok{ torch.tensor(}
\NormalTok{    [}\FloatTok{0.127}\NormalTok{, }\OperatorTok{{-}}\FloatTok{0.084}\NormalTok{, }\FloatTok{0.392}\NormalTok{, }\OperatorTok{{-}}\FloatTok{0.203}\NormalTok{], dtype}\OperatorTok{=}\NormalTok{torch.float32}
\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Original FP32: }\SpecialCharTok{\{}\NormalTok{weights\_fp32}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Memory per weight: 32 bits"}\NormalTok{)}

\CommentTok{\# Simple uniform quantization to INT8 ({-}128 to 127)}
\CommentTok{\# Step 1: Find scale factor}
\NormalTok{max\_val }\OperatorTok{=}\NormalTok{ weights\_fp32.}\BuiltInTok{abs}\NormalTok{().}\BuiltInTok{max}\NormalTok{()}
\NormalTok{scale }\OperatorTok{=}\NormalTok{ max\_val }\OperatorTok{/} \DecValTok{127}  \CommentTok{\# 127 is max positive INT8 value}

\CommentTok{\# Step 2: Quantize using our formula q = round(x/s)}
\NormalTok{weights\_int8 }\OperatorTok{=}\NormalTok{ torch.}\BuiltInTok{round}\NormalTok{(weights\_fp32 }\OperatorTok{/}\NormalTok{ scale).to(torch.int8)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Quantized INT8: }\SpecialCharTok{\{}\NormalTok{weights\_int8}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Memory per weight: 8 bits (reduced from 32)"}\NormalTok{)}

\CommentTok{\# Step 3: Dequantize to verify}
\NormalTok{weights\_dequantized }\OperatorTok{=}\NormalTok{ weights\_int8.}\BuiltInTok{float}\NormalTok{() }\OperatorTok{*}\NormalTok{ scale}
\BuiltInTok{print}\NormalTok{(}\SpecialStringTok{f"Dequantized: }\SpecialCharTok{\{}\NormalTok{weights\_dequantized}\SpecialCharTok{\}}\SpecialStringTok{"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(}
    \SpecialStringTok{f"Quantization error: "}
    \SpecialStringTok{f"}\SpecialCharTok{\{}\NormalTok{(weights\_fp32 }\OperatorTok{{-}}\NormalTok{ weights\_dequantized)}\SpecialCharTok{.}\BuiltInTok{abs}\NormalTok{()}\SpecialCharTok{.}\NormalTok{mean()}\SpecialCharTok{:.6f\}}\SpecialStringTok{"}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

}

\end{codelisting}%

This example demonstrates the compression from 32 bits to 8 bits per
weight, with minimal quantization error.

For example, in INT8 quantization, the model's floating-point values
(typically ranging from \([-r, r]\)) are mapped to an integer range of
\([-128, 127]\). The scaling factor ensures that the most significant
information is retained while reducing precision loss. Once the model
has been quantized, inference is performed using integer arithmetic,
which is significantly more efficient than floating-point operations on
many hardware platforms (\citeproc{ref-gholami2021survey}{Gholami et al.
2021}). However, due to rounding errors and numerical approximation,
quantized models may experience slight accuracy degradation compared to
their full-precision counterparts.

Once the model has been quantized, inference is performed using integer
arithmetic, which is significantly more efficient than floating-point
operations on many hardware platforms. However, due to rounding errors
and numerical approximation, quantized models may experience slight
accuracy degradation compared to their full-precision counterparts.

In addition to uniform quantization, non-uniform quantization can be
employed to preserve accuracy in certain scenarios. Unlike uniform
quantization, which uses a consistent scaling factor, non-uniform
quantization assigns finer-grained precision to numerical ranges that
are more densely populated. This approach can be beneficial for models
with weight distributions that concentrate around certain values, as it
allows more details to be retained where it matters most. However,
non-uniform quantization typically requires more complex calibration and
may involve additional computational overhead. While it is not as
commonly used as uniform quantization in production environments,
non-uniform techniques can be effective for preserving accuracy in
models that are particularly sensitive to precision changes.

PTQ proves effective for computer vision models, where CNNs often
tolerate quantization well. However, models that rely on small numerical
differences, such as NLP transformers or speech recognition models, may
require additional tuning or alternative quantization techniques,
including non-uniform strategies, to retain performance.

\paragraph{Calibration}\label{sec-model-compression-calibration-fe14}

An important aspect of PTQ is the calibration step, which involves
selecting the most effective clipping range {[}\(\alpha\), \(\beta\){]}
for quantizing model weights and activations. During PTQ, the model's
weights and activations are converted to lower-precision formats (e.g.,
INT8), but the effectiveness of this reduction depends heavily on the
chosen quantization range. Without proper calibration, the quantization
process may cause significant accuracy degradation, even if the overall
precision is reduced. Calibration ensures that the chosen range
minimizes loss of information and helps preserve the model's performance
after precision reduction.

Figure~\ref{fig-ptq-calibration} illustrates the post-training
quantization workflow, which begins with a pre-trained model and passes
a calibration dataset through to estimate numerical distributions. To
determine an effective quantization range, a calibration dataset, which
is a representative subset of training or validation data, is passed
through the model. This step allows the calibration process to estimate
the numerical distribution of activations and weights, which is then
used to define the clipping range for quantization. Following
calibration, the quantization step converts the model parameters to a
lower-precision format, producing the final quantized model, which is
more efficient in terms of memory and computation.

\begin{figure}[htb]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/b41df883ffa84cd71a2c16ea891ba4e7f5de899f.pdf}}

}

\caption{\label{fig-ptq-calibration}\textbf{Post-Training Quantization}:
Calibration with a representative dataset determines optimal
quantization ranges for model weights and activations, minimizing
information loss during quantization to create efficient,
lower-precision models. This process converts a pre-trained model into a
quantized version suitable for deployment on resource-constrained
devices.}

\end{figure}%

For example, consider quantizing activations that originally have a
floating-point range between --6 and 6 to 8-bit integers. Simply using
the full integer range of --128 to 127 for quantization might not be the
most effective approach. Instead, calibration involves passing a
representative dataset through the model and observing the actual range
of the activations. The observed range can then be used to set a more
effective quantization range, reducing information loss.

Common calibration methods include \textbf{Max} (uses maximum absolute
value---simple but susceptible to outliers), \textbf{Entropy} (minimizes
KL divergence between original and quantized distributions---TensorRT's
default), and \textbf{Percentile} (clips to a percentile, e.g., 99\%,
avoiding outlier impact). Figure~\ref{fig-resnet-activations-histogram}
shows why outlier handling matters: ResNet50 activations exhibit long
tails where outliers can skew the quantization range.

\begin{figure}

\centering{

\includegraphics[width=0.85\linewidth,height=\textheight,keepaspectratio]{contents/vol1/optimizations/images/png/efficientnumerics_calibrationcopy.png}

}

\caption{\label{fig-resnet-activations-histogram}\textbf{Activation
Distribution}: Resnet50 layer activations exhibit a long tail, with
outlier values that can lead to inefficient precision use if not handled
carefully. Source: (\citeproc{ref-wu2020integer}{H. Wu et al. 2020}).}

\end{figure}%

Calibration ranges can be \textbf{symmetric} (equal positive and
negative scaling) or \textbf{asymmetric} (different scaling factors for
each side, useful when distributions are skewed). The choice of method
and range significantly affects quantized model accuracy.

\subparagraph{Calibration
Ranges}\label{sec-model-compression-calibration-ranges-cd54}

A key challenge in post-training quantization is selecting the
appropriate calibration range \([\alpha, \beta]\) to map floating-point
values into a lower-precision representation. The choice of this range
directly affects the quantization error and, consequently, the accuracy
of the quantized model. Figure~\ref{fig-calibration-ranges} contrasts
the two primary calibration strategies: symmetric calibration and
asymmetric calibration.

\begin{figure}[htb]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/7ea1f4006b04239f4526534d28678fac6774fc6a.pdf}}

}

\caption{\label{fig-calibration-ranges}\textbf{Calibration Range
Selection}: Symmetric calibration uses a fixed range around zero, while
asymmetric calibration adapts the range to the data distribution,
potentially minimizing quantization error and preserving model accuracy.
Choosing an appropriate calibration strategy balances precision with the
risk of saturation for outlier values.}

\end{figure}%

Figure~\ref{fig-calibration-ranges} illustrates both approaches.
Symmetric calibration (left) maps \([-1, 1]\) to \([-127, 127]\) with
zero preserved---simpler to implement and common for zero-centered
weight distributions. Asymmetric calibration (right) uses different
ranges (\(\alpha = -0.5\), \(\beta = 1.5\)), better utilizing the
quantized range for skewed distributions at the cost of additional
complexity. Most frameworks (TensorRT, PyTorch) support both modes.

\paragraph{Granularity}\label{sec-model-compression-granularity-42da}

After determining the clipping range, the next step in optimizing
quantization involves adjusting the granularity of the clipping range to
ensure that the model retains as much accuracy as possible. In CNNs, for
instance, the input activations of a layer undergo convolution with
multiple convolutional filters, each of which may have a unique range of
values. The quantization process, therefore, must account for these
differences in range across filters to preserve the model's performance.

Figure~\ref{fig-quantization-granularity} demonstrates this variation:
the range for Filter 1 is significantly smaller than that for Filter 3,
illustrating the variation in the magnitude of values across different
filters. The precision with which the clipping range {[}\(\alpha\),
\(\beta\){]} is determined for the weights becomes an important factor
in effective quantization. This variability in ranges is a key reason
why different quantization strategies, based on granularity, are
employed.

\begin{figure}[htb]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/8636513b6210a71c9779ab05ba56d70f01c757c1.pdf}}

}

\caption{\label{fig-quantization-granularity}\textbf{Quantization Range
Variation}: Different convolutional filters exhibit unique activation
ranges, necessitating per-filter quantization to minimize accuracy loss
during quantization. Adjusting the granularity of clipping ranges---as
shown by the differing scales for each filter---optimizes the trade-off
between model size and performance. Source:
(\citeproc{ref-gholami2021survey}{Gholami et al. 2021}).}

\end{figure}%

Quantization granularity determines how many parameters share the same
clipping range:

\begin{itemize}
\tightlist
\item
  \textbf{Layerwise}: One range per layer---simple but suboptimal when
  filter ranges vary widely
\item
  \textbf{Groupwise}: Filters grouped with shared ranges---used in
  Q-BERT (\citeproc{ref-sheng2019qbert}{Shen et al. 2019}) for
  transformer attention layers
\item
  \textbf{Channelwise}: One range per filter---the current standard,
  balancing accuracy and efficiency
\item
  \textbf{Sub-channelwise}: Ranges within each filter---maximum
  precision but significant overhead
\end{itemize}

Channelwise quantization has become the dominant approach, providing
significant accuracy improvements over layerwise quantization with
minimal computational overhead.

With granularity determined, the next consideration is what to quantize.
Neural networks contain two primary numerical components: the static
weights learned during training and the dynamic activations computed
during inference. Each presents distinct quantization challenges.

\paragraph{Weights
vs.~Activations}\label{sec-model-compression-weights-vs-activations-101f}

Weight Quantization involves converting the continuous, high-precision
weights of a model into lower-precision values, such as converting
32-bit floating-point (Float32) weights to 8-bit integer (INT8) weights.
Figure~\ref{fig-weight-activations-quantization} illustrates how weight
quantization occurs in the second step (red squares) during the
multiplication of inputs. This process significantly reduces the model
size, decreasing both the memory required to store the model and the
computational resources needed for inference. For example, a weight
matrix in a neural network layer with Float32 weights like
\([0.215, -1.432, 0.902,\ldots]\) might be mapped to INT8 values such as
\([27, -183, 115, \ldots]\), leading to a significant reduction in
memory usage.

\begin{figure}[htb]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/445f1be5adf4647219fc58607fd48fb30b302ad1.pdf}}

}

\caption{\label{fig-weight-activations-quantization}\textbf{Quantization
and Weight Precision}: Reducing weight and activation precision from
float32 to INT8 significantly lowers model size and computational cost
during inference by representing values with fewer bits, though it may
introduce a trade-off with model accuracy. This process alters the
numerical representation of model parameters and intermediate results,
impacting both memory usage and processing speed. Source: HarvardX.}

\end{figure}%

Activation Quantization refers to the process of quantizing the
activation values, or outputs of the layers, during model inference.
This quantization can reduce the computational resources required during
inference, particularly when targeting hardware optimized for integer
arithmetic. It introduces challenges related to maintaining model
accuracy, as the precision of intermediate computations is reduced. For
instance, in a CNN, the activation maps (or feature maps) produced by
convolutional layers, originally represented in Float32, may be
quantized to INT8 during inference. This can significantly accelerate
computation on hardware capable of efficiently processing
lower-precision integers.

Recent advancements have explored \textbf{Activation-aware Weight
Quantization (AWQ)}\sidenote{\textbf{Activation-aware Weight
Quantization (AWQ)}: Observes that only approximately 1\% of weights
disproportionately affect accuracy based on activation patterns. By
protecting these salient weights while aggressively quantizing others,
AWQ achieves INT4 quantization of LLaMA-7B with \textless1\% perplexity
degradation. Delivers 3.2× speedup on A100 GPUs by reducing memory
bandwidth requirements from 14GB to 3.5GB per inference. } for the
compression and acceleration of large language models (LLMs). This
approach is particularly relevant for our \textbf{GPT-2 / Llama
Lighthouse}, which is memory-bandwidth bound. By protecting only a small
fraction of the most salient weights (approximately 1\%) based on
activation magnitude, AWQ enables effective 4-bit weight quantization.
This reduces the memory traffic required to load the massive parameter
set for every token generation, directly attacking the primary
bottleneck of generative inference (\citeproc{ref-lin2023awq}{Lin et al.
2023}).

\paragraph{Static vs.~Dynamic
Quantization}\label{sec-model-compression-static-vs-dynamic-quantization-ca12}

After determining the type and granularity of the clipping range,
practitioners must decide when the clipping ranges are calculated in
their quantization algorithms. Two primary approaches exist for
quantizing activations: static quantization and dynamic quantization.

Static Quantization is the more commonly used approach. In static
quantization, the clipping range is pre-calculated and remains fixed
during inference. This method does not introduce any additional
computational overhead during runtime, which makes it efficient in terms
of computational resources. However, the fixed range can lead to lower
accuracy compared to dynamic quantization. A typical implementation of
static quantization involves running a series of calibration inputs to
compute the typical range of activations
(\citeproc{ref-jacob2018quantization}{Jacob et al. 2018a};
\citeproc{ref-yao2021hawq}{Yao et al. 2021}).

In contrast, Dynamic Quantization dynamically calculates the range for
each activation map during runtime. This approach allows the
quantization process to adjust in real time based on the input,
potentially yielding higher accuracy since the range is specifically
calculated for each input activation. However, dynamic quantization
incurs higher computational overhead because the range must be
recalculated at each step. Although this often results in higher
accuracy, the real-time computations can be expensive, particularly when
deployed at scale.

These timing and granularity decisions interact with the broader choice
of quantization methodology. Table~\ref{tbl-quantization_methods}
compares post-training quantization, quantization-aware training, and
dynamic quantization, each offering distinct strengths, limitations, and
trade-offs. These methods are widely deployed across machine learning
systems of varying scales, and understanding their pros and cons is
important for selecting the appropriate approach for a given
application.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2578}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2656}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2109}}@{}}
\caption{\textbf{Quantization Trade-Offs}: Post-training quantization,
quantization-aware training, and dynamic quantization represent distinct
approaches to model compression, each balancing accuracy, computational
cost, and implementation complexity for machine learning systems.
Understanding these trade-offs is important for selecting the optimal
quantization strategy based on application requirements and resource
constraints.}\label{tbl-quantization_methods}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Aspect}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Post Training Quantization}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Quantization-Aware Training}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Dynamic Quantization}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Aspect}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Post Training Quantization}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Quantization-Aware Training}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Dynamic Quantization}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Pros} & & & \\
\textbf{Simplicity} & ✓ & ✗ & ✗ \\
\textbf{Accuracy Preservation} & ✗ & ✓ & ✓ \\
\textbf{Adaptability} & ✗ & ✗ & ✓ \\
\textbf{Optimized Performance} & ✗ & ✓ & Potentially \\
\textbf{Cons} & & & \\
\textbf{Accuracy Degradation} & ✓ & ✗ & Potentially \\
\textbf{Computational Overhead} & ✗ & ✓ & ✓ \\
\textbf{Implementation Complexity} & ✗ & ✓ & ✓ \\
\textbf{Tradeoffs} & & & \\
\textbf{Speed vs.~Accuracy} & ✓ & ✗ & ✗ \\
\textbf{Accuracy vs.~Cost} & ✗ & ✓ & ✗ \\
\textbf{Adaptability vs.~Overhead} & ✗ & ✗ & ✓ \\
\end{longtable}

\paragraph{PTQ
Advantages}\label{sec-model-compression-ptq-advantages-c3d6}

One of the key advantages of PTQ is its low computational cost, as it
does not require retraining the model. This makes it an attractive
option for the rapid deployment of trained models, especially when
retraining is computationally expensive or infeasible. Since PTQ only
modifies the numerical representation of weights and activations, the
underlying model architecture remains unchanged, allowing it to be
applied to a wide range of pre-trained models without modification.

PTQ also provides significant memory and storage savings by reducing the
bit-width of model parameters. For instance, converting a model from
FP32 to INT8 results in a \(4\times\) reduction in storage size, making
it feasible to deploy larger models on resource-constrained devices such
as mobile phones, edge AI hardware, and embedded systems. These
reductions in memory footprint also lead to lower bandwidth requirements
when transferring models across networked systems.

In terms of computational efficiency, PTQ allows inference to be
performed using integer arithmetic, which is inherently faster than
floating-point operations on many hardware platforms. AI accelerators
such as TPUs and Neural Processing Units (NPUs) are optimized for
lower-precision computations, enabling higher throughput and reduced
power consumption when executing quantized models. This makes PTQ
particularly useful for applications requiring real-time inference, such
as object detection in autonomous systems or speech recognition on
mobile devices.

\paragraph{PTQ Challenges and
Limitations}\label{sec-model-compression-ptq-challenges-limitations-e51d}

Despite its advantages, PTQ introduces quantization errors due to
rounding effects when mapping floating-point values to discrete
lower-precision representations. While some models remain robust to
these changes, others may experience notable accuracy degradation,
especially in tasks that rely on small numerical differences.

The extent of accuracy loss depends on both the model architecture and
the task domain. CNNs for image classification are generally tolerant to
PTQ, often maintaining near-original accuracy even with aggressive INT8
quantization. Transformer-based models used in NLP and speech
recognition tend to be more sensitive, as these architectures rely on
the precision of numerical relationships in attention mechanisms.

To mitigate accuracy loss, calibration techniques such as KL
divergence-based scaling or per-channel quantization are commonly
applied to fine-tune the scaling factor and minimize information loss.
Some frameworks, including TensorFlow Lite and PyTorch, provide
automated quantization tools with built-in calibration methods to
improve accuracy retention.

Another limitation of PTQ is that not all hardware supports efficient
integer arithmetic. While GPUs, TPUs, and specialized edge AI chips
often include dedicated support for INT8 inference, general-purpose CPUs
may lack the optimized instructions for low-precision execution,
resulting in suboptimal performance improvements.

PTQ is not always suitable for training purposes. Since PTQ applies
quantization after training, models that require further fine-tuning or
adaptation may benefit more from alternative approaches to ensure that
precision constraints are adequately considered during the learning
process.

Post-training quantization provides the foundation for more advanced
quantization methods. The core concepts of quantization workflows,
numerical format trade-offs, and calibration methods remain essential
throughout all precision optimization techniques. For rapid deployment
scenarios with production deadlines under two weeks and acceptable
accuracy loss of 1-2\%, PTQ with min-max calibration often provides a
complete solution. Extreme constraints like sub-1MB models or sub-10mW
power budgets may require INT4 or binary quantization, accepting 5-20\%
accuracy degradation that necessitates architectural changes.

When PTQ's accuracy preservation is insufficient, particularly for
transformer-based models where attention mechanisms amplify small
numerical differences, practitioners need a more powerful approach.
Rather than applying quantization as a post-hoc transformation, we can
integrate precision constraints directly into the training process
itself.

\subsubsection{Quantization-Aware
Training}\label{sec-model-compression-quantizationaware-training-108e}

QAT integrates quantization constraints directly into the training
process, simulating low-precision arithmetic during forward passes to
allow the model to adapt to quantization effects
(\citeproc{ref-jacob2018quantization}{Jacob et al. 2018a}). Production
systems requiring less than 1\% accuracy loss benefit most from this
approach, which recovers accuracy through fine-tuning with quantization
simulation at the cost of 20-50\% additional training time. This
approach is particularly important for models requiring fine-grained
numerical precision, such as transformers used in NLP and speech
recognition systems (\citeproc{ref-nagel2021whitepaper}{Nagel et al.
2021a}). Figure~\ref{fig-qat} illustrates the QAT process: quantization
is applied to a pre-trained model, followed by fine-tuning to adapt
weights to low-precision constraints.

\begin{figure}[htb]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/ba28b0fc57c9e01f6b14a1773392d1e067a928d0.pdf}}

}

\caption{\label{fig-qat}\textbf{Quantization-Aware Training}: Retraining
a pre-trained model with simulated low-precision arithmetic adapts
weights to mitigate accuracy loss during deployment with reduced
numerical precision, enabling efficient inference on
resource-constrained devices. This process refines the model to become
robust to the effects of quantization, maintaining performance despite
lower precision representations.}

\end{figure}%

In many cases, QAT can also build off PTQ (discussed in detail in the
previous section), as shown in Figure~\ref{fig-ptq-qat}. Instead of
starting from a full-precision model, PTQ is first applied to produce an
initial quantized model using calibration data. This quantized model
then serves as the starting point for QAT, where additional fine-tuning
with training data helps the model better adapt to low-precision
constraints. This hybrid approach combines PTQ's efficiency with QAT's
accuracy preservation, reducing the degradation typically associated
with post-training approaches alone.

\begin{figure}[htb]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/b239f11126a0c832ef596ca66410c4d2b076933e.pdf}}

}

\caption{\label{fig-ptq-qat}\textbf{Hybrid Quantization Approach}:
Post-training quantization (PTQ) generates an initial quantized model
that serves as a warm start for quantization-aware training (QAT),
accelerating convergence and mitigating accuracy loss compared to
quantizing a randomly initialized network. This two-stage process
leverages the efficiency of PTQ while refining the model with training
data to optimize performance under low-precision constraints.}

\end{figure}%

\paragraph{Training
Mathematics}\label{sec-model-compression-training-mathematics-4a6c}

During forward propagation, weights and activations are quantized and
dequantized to mimic reduced precision. This process is typically
represented as: \[
q = \text{round} \left(\frac{x}{s} \right) \times s
\] where \(q\) represents the simulated quantized value, \(x\) denotes
the full-precision weight or activation, and \(s\) is the scaling factor
mapping floating-point values to lower-precision integers.

Although the forward pass utilizes quantized values, gradient
calculations during backpropagation remain in full precision. This is
accomplished using the Straight-Through Estimator
(STE)\sidenote{\textbf{Straight-Through Estimator (STE)}: Gradient
approximation technique for non-differentiable functions, introduced by
Bengio et al. (\citeproc{ref-bengio2013estimating}{Y. Bengio, Léonard,
and Courville 2013a}). Sets gradient of step function to 1 everywhere,
enabling backpropagation through quantization layers. Crucial for
training binarized neural networks and quantization-aware training,
despite theoretical limitations around zero. }, which approximates the
gradient of the quantized function by treating the rounding operation as
if it had a derivative of one. This approach prevents the gradient from
being obstructed due to the non-differentiable nature of the
quantization operation, thereby allowing effective model training
(\citeproc{ref-bengio2013estimating}{Y. Bengio, Léonard, and Courville
2013a}).

Integrating quantization effects during training enables the model to
learn an optimal distribution of weights and activations that minimizes
the impact of numerical precision loss. The resulting model, when
deployed using true low-precision arithmetic (e.g., INT8 inference),
maintains significantly higher accuracy than one that is quantized post
hoc (\citeproc{ref-krishnamoorthi2018quantizing}{Krishnamoorthi 2018}).

\paragraph{Fake Quantization Nodes and
Implementation}\label{sec-model-compression-fake-quantization-nodes-implementation-b63e}

QAT implementation relies on fake quantization operations that simulate
quantization during forward propagation while maintaining full precision
for gradient computation. These operations insert quantize-dequantize
pairs into the computational graph, creating a training-time simulation
of inference-time behavior.

A fake quantization node performs three operations sequentially:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Quantization}: Map floating-point value to discrete
  quantization level
\item
  \textbf{Clipping}: Enforce range constraints based on bit width
\item
  \textbf{Dequantization}: Convert back to floating-point for subsequent
  operations
\end{enumerate}

Mathematically, for symmetric quantization with bit width \(b\): \[
\begin{aligned}
q_{level} &= \text{clip}\left(\text{round}\left(\frac{x}{s}\right), -2^{b-1}, 2^{b-1} - 1\right) \\
x_{fake} &= q_{level} \times s
\end{aligned}
\] where \(s = \frac{\max(|x|)}{2^{b-1} - 1}\) is the scale factor
computed from the input distribution, and \(x_{fake}\) represents the
fake-quantized output that mimics INT8 values but remains in
floating-point format.

For asymmetric quantization supporting unsigned integers: \[
\begin{aligned}
s &= \frac{\max(x) - \min(x)}{2^b - 1} \\
z &= \text{round}\left(-\frac{\min(x)}{s}\right) \\
q_{level} &= \text{clip}\left(\text{round}\left(\frac{x}{s} + z\right), 0, 2^b - 1\right) \\
x_{fake} &= (q_{level} - z) \times s
\end{aligned}
\] where \(z\) is the zero-point offset enabling asymmetric range
representation.

The following implementation demonstrates how frameworks simulate
quantization during training while maintaining gradient flow. As you
read the code, focus on two key aspects: first, the forward pass applies
quantization to both inputs and weights before convolution, mimicking
INT8 inference behavior; second, the implementation maintains
floating-point precision throughout, allowing gradients to flow during
backpropagation through the Straight-Through Estimator discussed
earlier. Listing~\ref{lst-qat-conv-forward} demonstrates the
computational graph for a quantized convolution layer, which contains
fake quantization nodes for both weights and activations:

\begin{codelisting}

\caption{\label{lst-qat-conv-forward}\textbf{QAT Convolution Forward
Pass}: Fake quantization nodes simulate integer quantization during
training while maintaining gradient flow through the straight-through
estimator.}

\centering{

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Forward pass with fake quantization}
\KeywordTok{def}\NormalTok{ qat\_conv\_forward(x, weight):}
    \CommentTok{\# Fake quantize input activations}
\NormalTok{    x\_scale }\OperatorTok{=}\NormalTok{ compute\_scale(x, bits}\OperatorTok{=}\DecValTok{8}\NormalTok{, symmetric}\OperatorTok{=}\VariableTok{False}\NormalTok{)}
\NormalTok{    x\_zero }\OperatorTok{=}\NormalTok{ compute\_zero\_point(x, x\_scale, bits}\OperatorTok{=}\DecValTok{8}\NormalTok{)}
\NormalTok{    x\_quant }\OperatorTok{=}\NormalTok{ fake\_quantize(x, x\_scale, x\_zero, bits}\OperatorTok{=}\DecValTok{8}\NormalTok{)}

    \CommentTok{\# Fake quantize weights (typically symmetric)}
\NormalTok{    w\_scale }\OperatorTok{=}\NormalTok{ compute\_scale(weight, bits}\OperatorTok{=}\DecValTok{8}\NormalTok{, symmetric}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\NormalTok{    w\_quant }\OperatorTok{=}\NormalTok{ fake\_quantize(weight, w\_scale, zero}\OperatorTok{=}\DecValTok{0}\NormalTok{, bits}\OperatorTok{=}\DecValTok{8}\NormalTok{)}

    \CommentTok{\# Convolution with fake{-}quantized values}
\NormalTok{    output }\OperatorTok{=}\NormalTok{ conv2d(x\_quant, w\_quant)}
    \ControlFlowTok{return}\NormalTok{ output}
\end{Highlighting}
\end{Shaded}

}

\end{codelisting}%

The critical aspect of fake quantization is gradient handling during
backpropagation. The rounding and clipping operations are
non-differentiable, requiring gradient approximation through the
Straight-Through Estimator: \[
\frac{\partial x_{fake}}{\partial x} = \begin{cases}
1 & \text{if } x \in [x_{min}, x_{max}] \\
0 & \text{otherwise}
\end{cases}
\] This approximation treats the quantization function as identity
within the valid range, allowing gradients to flow unchanged through the
fake quantization nodes except for values that exceed clipping bounds.

During backpropagation, the full-precision gradient
\(\frac{\partial L}{\partial x_{fake}}\) propagates directly to \(x\)
for values within the quantization range. For weights and activations
exceeding the range, gradients become zero, preventing further updates
that would push values beyond representable limits. This gradient
behavior encourages the model to learn weight distributions that
naturally fit within quantization constraints.

Listing~\ref{lst-fake-quantize-autograd} shows how modern deep learning
frameworks implement fake quantization as custom operators with forward
and backward functions:

\begin{codelisting}

\caption{\label{lst-fake-quantize-autograd}\textbf{Fake Quantization
Autograd Function}: Custom operator implementing the straight-through
estimator, passing gradients unchanged within the quantization range.}

\centering{

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{class}\NormalTok{ FakeQuantize(torch.autograd.Function):}
    \AttributeTok{@staticmethod}
    \KeywordTok{def}\NormalTok{ forward(ctx, x, scale, zero\_point, bits}\OperatorTok{=}\DecValTok{8}\NormalTok{):}
        \CommentTok{\# Quantize to integer levels}
\NormalTok{        q\_min, q\_max }\OperatorTok{=} \DecValTok{0}\NormalTok{, }\DecValTok{2}\OperatorTok{**}\NormalTok{bits }\OperatorTok{{-}} \DecValTok{1}
\NormalTok{        q }\OperatorTok{=}\NormalTok{ torch.}\BuiltInTok{round}\NormalTok{(x }\OperatorTok{/}\NormalTok{ scale }\OperatorTok{+}\NormalTok{ zero\_point)}
\NormalTok{        q }\OperatorTok{=}\NormalTok{ torch.clamp(q, q\_min, q\_max)}

        \CommentTok{\# Dequantize back to float}
\NormalTok{        x\_fake }\OperatorTok{=}\NormalTok{ (q }\OperatorTok{{-}}\NormalTok{ zero\_point) }\OperatorTok{*}\NormalTok{ scale}

        \CommentTok{\# Save for backward pass}
\NormalTok{        ctx.save\_for\_backward(x, scale, zero\_point)}
\NormalTok{        ctx.q\_min }\OperatorTok{=}\NormalTok{ q\_min}
\NormalTok{        ctx.q\_max }\OperatorTok{=}\NormalTok{ q\_max}
        \ControlFlowTok{return}\NormalTok{ x\_fake}

    \AttributeTok{@staticmethod}
    \KeywordTok{def}\NormalTok{ backward(ctx, grad\_output):}
\NormalTok{        x, scale, zero\_point }\OperatorTok{=}\NormalTok{ ctx.saved\_tensors}

        \CommentTok{\# STE: gradient passes through for values in range}
\NormalTok{        q }\OperatorTok{=}\NormalTok{ x }\OperatorTok{/}\NormalTok{ scale }\OperatorTok{+}\NormalTok{ zero\_point}
\NormalTok{        mask }\OperatorTok{=}\NormalTok{ (q }\OperatorTok{\textgreater{}=}\NormalTok{ ctx.q\_min) }\OperatorTok{\&}\NormalTok{ (q }\OperatorTok{\textless{}=}\NormalTok{ ctx.q\_max)}
\NormalTok{        grad\_input }\OperatorTok{=}\NormalTok{ grad\_output }\OperatorTok{*}\NormalTok{ mask.}\BuiltInTok{float}\NormalTok{()}

        \ControlFlowTok{return}\NormalTok{ grad\_input, }\VariableTok{None}\NormalTok{, }\VariableTok{None}\NormalTok{, }\VariableTok{None}
\end{Highlighting}
\end{Shaded}

}

\end{codelisting}%

This implementation shows how fake quantization nodes maintain the
illusion of quantized arithmetic during forward propagation while
preserving gradient flow during backward propagation. The forward pass
produces outputs matching INT8 inference behavior, while the backward
pass uses the STE approximation to update full-precision parameters.

Scale factor adaptation during QAT training affects convergence. Static
scales computed once at initialization may become suboptimal as weights
evolve during training. Listing~\ref{lst-qat-scale-adaptation}
demonstrates dynamic scale updates that track distribution changes:

\begin{codelisting}

\caption{\label{lst-qat-scale-adaptation}\textbf{Dynamic Scale
Adaptation}: Exponential moving average updates track evolving
activation distributions during QAT training.}

\centering{

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Moving average scale computation}
\KeywordTok{def}\NormalTok{ update\_scales(current\_x, running\_scale, momentum}\OperatorTok{=}\FloatTok{0.9}\NormalTok{):}
    \CommentTok{\# Compute current scale from batch statistics}
\NormalTok{    current\_scale }\OperatorTok{=}\NormalTok{ compute\_scale(current\_x, bits}\OperatorTok{=}\DecValTok{8}\NormalTok{)}

    \CommentTok{\# Exponential moving average}
\NormalTok{    running\_scale }\OperatorTok{=}\NormalTok{ (}
\NormalTok{        momentum }\OperatorTok{*}\NormalTok{ running\_scale }\OperatorTok{+}\NormalTok{ (}\DecValTok{1} \OperatorTok{{-}}\NormalTok{ momentum) }\OperatorTok{*}\NormalTok{ current\_scale}
\NormalTok{    )}
    \ControlFlowTok{return}\NormalTok{ running\_scale}
\end{Highlighting}
\end{Shaded}

}

\end{codelisting}%

During training, scales update based on observed activation
distributions using exponential moving averages. This adaptation enables
the quantization scheme to track evolving activation patterns as the
model learns, preventing scale mismatch between training and deployment.

Batch normalization interactions with QAT require special consideration.
Batch normalization layers compute running statistics during training
for use at inference. For quantized models, these statistics must
reflect the distribution of fake-quantized activations, not
full-precision values. Listing~\ref{lst-qat-batch-norm} illustrates this
requirement:

\begin{codelisting}

\caption{\label{lst-qat-batch-norm}\textbf{QAT-Aware Batch
Normalization}: Running statistics must be computed on fake-quantized
activations to match inference behavior with true INT8 operations.}

\centering{

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# QAT{-}aware batch normalization}
\KeywordTok{def}\NormalTok{ qat\_batch\_norm(x, weight, bias, running\_mean, running\_var):}
    \CommentTok{\# Fake quantize input}
\NormalTok{    x\_quant }\OperatorTok{=}\NormalTok{ fake\_quantize(x, scale, zero\_point, bits}\OperatorTok{=}\DecValTok{8}\NormalTok{)}

    \CommentTok{\# Batch norm on quantized values}
\NormalTok{    x\_norm }\OperatorTok{=}\NormalTok{ (x\_quant }\OperatorTok{{-}}\NormalTok{ running\_mean) }\OperatorTok{/}\NormalTok{ torch.sqrt(running\_var }\OperatorTok{+}\NormalTok{ eps)}
\NormalTok{    output }\OperatorTok{=}\NormalTok{ weight }\OperatorTok{*}\NormalTok{ x\_norm }\OperatorTok{+}\NormalTok{ bias}

    \CommentTok{\# Update running statistics with quantized distribution}
    \ControlFlowTok{if}\NormalTok{ training:}
\NormalTok{        batch\_mean }\OperatorTok{=}\NormalTok{ torch.mean(x\_quant)}
\NormalTok{        batch\_var }\OperatorTok{=}\NormalTok{ torch.var(x\_quant)}
\NormalTok{        running\_mean }\OperatorTok{=}\NormalTok{ (}
\NormalTok{            momentum }\OperatorTok{*}\NormalTok{ running\_mean }\OperatorTok{+}\NormalTok{ (}\DecValTok{1} \OperatorTok{{-}}\NormalTok{ momentum) }\OperatorTok{*}\NormalTok{ batch\_mean}
\NormalTok{        )}
\NormalTok{        running\_var }\OperatorTok{=}\NormalTok{ (}
\NormalTok{            momentum }\OperatorTok{*}\NormalTok{ running\_var }\OperatorTok{+}\NormalTok{ (}\DecValTok{1} \OperatorTok{{-}}\NormalTok{ momentum) }\OperatorTok{*}\NormalTok{ batch\_var}
\NormalTok{        )}

    \ControlFlowTok{return}\NormalTok{ output}
\end{Highlighting}
\end{Shaded}

}

\end{codelisting}%

This ensures that inference with true INT8 operations uses batch
normalization parameters calibrated for quantized value distributions,
preventing accuracy degradation from distribution mismatch.

\paragraph{QAT
Advantages}\label{sec-model-compression-qat-advantages-31dc}

A primary advantage of QAT\sidenote{\textbf{Quantization-Aware
Training}: QAT enables INT8 inference with minimal accuracy loss -
ResNet-50 maintains 76.1\% vs.~76.2\% FP32 ImageNet accuracy, while
MobileNetV2 achieves 71.8\% vs.~72.0\%. BERT-Base INT8 retains 99.1\% of
FP32 performance on GLUE, compared to 96.8\% with post-training
quantization alone. } is its ability to maintain model accuracy, even
under low-precision inference conditions. Incorporating quantization
during training helps the model to compensate for precision loss,
reducing the impact of rounding errors and numerical instability. This
is important for quantization-sensitive models commonly used in NLP,
speech recognition, and high-resolution computer vision
(\citeproc{ref-gholami2021survey}{Gholami et al. 2021}).

Another major benefit is that QAT permits low-precision inference on
hardware accelerators without significant accuracy degradation. AI
processors such as TPUs, NPUs, and specialized edge devices include
dedicated hardware for integer operations, permitting INT8 models to run
much faster and with lower energy consumption compared to FP32 models.
Training with quantization effects in mind ensures that the final model
can fully leverage these hardware optimizations
(\citeproc{ref-wu2020integer}{H. Wu et al. 2020}).

\paragraph{QAT Challenges and
Trade-offs}\label{sec-model-compression-qat-challenges-tradeoffs-f52c}

Despite its benefits, QAT introduces additional computational overhead
during training. Simulated quantization at every forward pass slows down
training relative to full-precision methods. The process adds complexity
to the training schedule, making QAT less practical for very large-scale
models where the additional training time might be prohibitive.

QAT introduces extra hyperparameters and design considerations, such as
choosing appropriate quantization schemes and scaling factors. Unlike
PTQ, which applies quantization after training, QAT requires careful
tuning of the training dynamics to ensure that the model suitably adapts
to low-precision constraints (\citeproc{ref-choukroun2019low}{Gong et
al. 2019}).

Table~\ref{tbl-qat} contrasts QAT and PTQ across accuracy retention,
training complexity, and deployment readiness:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2000}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4444}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3481}}@{}}
\caption{\textbf{Quantization Trade-Offs}: Quantization-aware training
(QAT) minimizes accuracy loss from reduced numerical precision by
incorporating quantization into the training process, while
post-training quantization (PTQ) offers faster deployment but may
require calibration to mitigate accuracy degradation. QAT's retraining
requirement increases training complexity compared to the simplicity of
applying PTQ to a pre-trained model.}\label{tbl-qat}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Aspect}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{QAT (Quantization-Aware Training)}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{PTQ (Post-Training Quantization)}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Aspect}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{QAT (Quantization-Aware Training)}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{PTQ (Post-Training Quantization)}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Accuracy Retention} & Minimizes accuracy loss from quantization
& May suffer from accuracy degradation \\
\textbf{Inference Efficiency} & Optimized for low-precision hardware
(e.g., INT8 on TPUs) & Optimized but may require calibration \\
\textbf{Training Complexity} & Requires retraining with quantization
constraints & No retraining required \\
\textbf{Training Time} & Slower due to simulated quantization in forward
pass & Faster, as quantization is applied post hoc \\
\textbf{Deployment Readiness} & Best for models sensitive to
quantization errors & Fastest way to optimize models for inference \\
\end{longtable}

Integrating quantization into the training process preserves model
accuracy more effectively than post-training quantization, although it
requires additional training resources and time.

\paragraph{PTQ vs.~QAT}\label{sec-model-compression-ptq-vs-qat-c9f6}

Having examined both PTQ and QAT in detail, practitioners face a
fundamental decision: when should each approach be used? The choice
depends on trade-offs between accuracy, computational cost, and
deployment constraints. PTQ provides computationally inexpensive
optimization requiring only post-training conversion, making it ideal
for rapid deployment. However, effectiveness varies by
architecture---CNNs tolerate PTQ well while NLP and speech models may
experience degradation due to reliance on precise numerical
representations.

QAT proves necessary when high accuracy retention is critical.
Integrating quantization effects during training allows models to adapt
to lower-precision arithmetic, reducing quantization errors
(\citeproc{ref-Jacob2018}{Jacob et al. 2018b}). While achieving higher
low-precision accuracy, QAT requires additional training time and
computational resources. In practice, a hybrid approach starting with
PTQ and selectively applying QAT for accuracy-critical models provides
optimal balance between efficiency and performance.

The techniques discussed so far, PTQ and QAT, typically target 8-bit or
4-bit precision while maintaining near-original accuracy. However, some
deployment scenarios demand even more aggressive compression, pushing
precision to the absolute limits of what neural networks can tolerate.

\subsection{Extreme
Quantization}\label{sec-model-compression-extreme-quantization-030c}

Beyond INT8 and INT4 quantization, extreme quantization techniques use
1-bit (binarization) or 2-bit (ternarization) representations to achieve
dramatic reductions in memory usage and computational requirements
(\citeproc{ref-Courbariaux2016}{Courbariaux, Bengio, and David 2016}).
Binarization constrains weights and activations to two values (typically
-1 and +1, or 0 and 1), drastically reducing model size and accelerating
inference on specialized hardware like binary neural networks
(\citeproc{ref-Rastegari2016}{Rastegari et al. 2016}). However, this
constraint severely limits model expressiveness, often degrading
accuracy on tasks requiring high precision such as image recognition or
natural language processing (\citeproc{ref-Hubara2018}{Hubara et al.
2018}).

Ternarization extends binarization by allowing three values (-1, 0, +1),
providing additional flexibility that slightly improves accuracy over
pure binarization (\citeproc{ref-Zhu2017}{Zhu et al. 2017}). The zero
value enables greater sparsity while maintaining more representational
power. Both techniques require gradient approximation methods like
Straight-Through Estimator (STE) to handle non-differentiable
quantization operations during training (\citeproc{ref-Bengio2013}{Y.
Bengio, Léonard, and Courville 2013b}), with QAT integration helping
mitigate accuracy loss (\citeproc{ref-Choi2019}{Choi et al. 2018}).

\paragraph{Challenges and
Limitations}\label{sec-model-compression-challenges-limitations-b212}

Despite enabling ultra-low-power machine learning for embedded systems
and mobile devices, binarization and ternarization face significant
challenges. Performance maintenance proves difficult with such drastic
quantization, requiring specialized hardware capable of efficiently
handling binary or ternary operations
(\citeproc{ref-Umuroglu2017}{Umuroglu et al. 2017}). Traditional
processors lack optimization for these computations, necessitating
custom hardware accelerators.

Accuracy loss remains a critical concern. These methods suit tasks where
high precision is not critical or where QAT can compensate for precision
constraints. Despite challenges, the ability to drastically reduce model
size while maintaining acceptable accuracy makes them attractive for
edge AI and resource-constrained environments
(\citeproc{ref-Jacob2018}{Jacob et al. 2018b}). Future advances in
specialized hardware and training techniques will likely enhance their
role in efficient, scalable AI.

We have now covered two optimization dimensions: structural optimization
(pruning, distillation, NAS) determines \emph{what} to compute, and
precision optimization (quantization) determines \emph{how precisely} to
compute it. Together, these techniques can reduce a model's theoretical
complexity by 80\% or more, from removing half the parameters through
pruning to compressing weights from 32-bit floats to 4-bit integers
through quantization.

Yet practitioners often discover a frustrating gap between theory and
practice: a model pruned to 50\% parameters and quantized to INT8 may
achieve only 20\% latency improvement on actual hardware. This
theory-practice gap reveals that optimization must extend beyond the
model itself to how computations execute on physical hardware.

Why does this gap exist? Sparse matrices stored in dense format waste
memory bandwidth loading zeros. Operations that could run in parallel
execute sequentially due to data dependencies. Simple inputs receive the
same computational budget as complex ones. The gap between ``optimized
on paper'' and ``optimized in practice'' is the domain of our third and
final optimization dimension: \textbf{architectural efficiency}. This
dimension ensures that structural and precision optimizations actually
translate into real-world speedups by aligning computation patterns with
hardware capabilities.

\phantomsection\label{quiz-question-sec-model-compression-quantization-precision-optimization-31d3}
\begin{fbx}{callout-quiz-question}{Self-Check: Question 1.5}{}
\phantomsection\label{quiz-question-sec-model-compression-quantization-precision-optimization-31d3}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Which of the following precision formats offers the best balance
  between computational speed and accuracy for training on AI
  accelerators?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii})}
  \tightlist
  \item
    BFloat16
  \item
    FP16
  \item
    INT8
  \item
    FP32
  \end{enumerate}
\item
  Explain the trade-offs involved in using INT8 precision for inference
  in machine learning models.
\item
  The process of reducing numerical precision to improve computational
  efficiency is known as \_\_\_\_. This technique is essential for
  optimizing machine learning models for deployment in
  resource-constrained environments.
\item
  True or False: Reducing precision from FP32 to FP16 always leads to a
  proportional decrease in power consumption.
\item
  Order the following precision formats by their typical storage
  reduction compared to FP32: (1) FP16, (2) INT8, (3) BFloat16.
\end{enumerate}

\noindent\hspace*{1.25em}\hyperref[quiz-answer-sec-model-compression-quantization-precision-optimization-31d3]{\textbf{See Answer~$\rightarrow$}}

\end{fbx}

\section{Architectural Efficiency
Techniques}\label{sec-model-compression-architectural-efficiency-techniques-daa6}

Architectural efficiency optimization ensures that computations execute
efficiently on target hardware by aligning model operations with
processor capabilities and memory hierarchies. Unlike representation
optimization (which determines what computations to perform) and
precision optimization (which determines numerical fidelity),
architectural efficiency addresses how operations are scheduled, how
memory is accessed, and how workloads adapt to input characteristics and
hardware constraints. This optimization dimension proves particularly
important for resource-constrained scenarios examined in
\textbf{?@sec-introduction}.

Four complementary approaches to architectural efficiency are examined:
hardware-aware design principles that proactively integrate deployment
constraints during model development, sparsity exploitation techniques
that accelerate computation on pruned models, dynamic computation
strategies that adapt workload to input complexity, and operator fusion
methods that reduce memory traffic by combining operations. These
techniques transform algorithmic optimizations into realized performance
gains.

\subsection{Hardware-Aware
Design}\label{sec-model-compression-hardwareaware-design-7b7c}

Hardware-aware design incorporates target platform constraints---memory
bandwidth, processing power, parallelism capabilities, and energy
budgets---directly into model architecture decisions. Rather than
optimizing models after training, this approach ensures computational
patterns, memory access, and operation types match hardware capabilities
from the outset, maximizing efficiency across diverse deployment
platforms.

\subsubsection{Efficient Design
Principles}\label{sec-model-compression-efficient-design-principles-0b50}

Designing machine learning models for hardware efficiency requires
structuring architectures to account for computational cost, memory
usage, inference latency, and power consumption, all while maintaining
strong predictive performance. A key aspect involves leveraging the
strengths of specific hardware platforms (GPUs, TPUs, mobile or edge
devices) to maximize parallelism, optimize memory hierarchies, and
minimize latency through hardware-optimized operations.
Table~\ref{tbl-hardware-efficient-design} categorizes these design
principles, each addressing a core aspect of computational and system
constraints.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.1176}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.7353}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.1429}}@{}}
\caption{\textbf{Hardware-Aware Design Principles}: Categorizing model
design choices by their impact on computational cost, memory usage, and
inference latency enables structured optimization for diverse hardware
platforms and deployment scenarios. MobileNet exemplifies computation
reduction through depthwise separable convolutions, while DenseNet and
SqueezeNet demonstrate memory optimization
strategies.}\label{tbl-hardware-efficient-design}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Principle}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Goal}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Example Networks}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Principle}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Goal}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Example Networks}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Scaling Optimization} & Adjust model depth, width, and
resolution to balance efficiency and hardware constraints. &
EfficientNet, RegNet \\
\textbf{Computation Reduction} & Minimize redundant operations to reduce
computational cost, using hardware-specific optimizations (e.g.,
depthwise separable convolutions on mobile chips). & MobileNet,
ResNeXt \\
\textbf{Memory Optimization} & Ensure efficient memory usage by reducing
activation and parameter storage requirements, using hardware-specific
memory hierarchies (e.g., local and global memory in GPUs). & DenseNet,
SqueezeNet \\
\textbf{Hardware-Aware Design} & Optimize architectures for specific
hardware constraints (e.g., low power, parallelism, high throughput). &
TPU-optimized models, MobileNet \\
\end{longtable}

The principles in Table~\ref{tbl-hardware-efficient-design} work
synergistically: scaling optimization sizes models appropriately for
available resources, computation reduction eliminates redundant
operations through techniques like depthwise separable
convolutions\sidenote{\textbf{Depthwise Separable Convolutions}:
Factorizes standard convolution into depthwise (per-channel) and
pointwise (1×1) operations, reducing computation by 8-9x. MobileNetV2
achieves 72\% ImageNet accuracy with 300M FLOPs vs.~ResNet-50's 76\%
with 4.1B FLOPs (13.7x fewer operations). Enables real-time inference on
mobile devices. }, memory optimization aligns access patterns with
hardware hierarchies, and hardware-aware design ensures architectural
decisions match platform capabilities. Together, these principles enable
models that balance accuracy with efficiency while maintaining
consistent behavior across deployment environments.

The following subsections examine each principle in detail, beginning
with how to scale model dimensions effectively.

\subsubsection{Scaling
Optimization}\label{sec-model-compression-scaling-optimization-2e3a}

Scaling a model's architecture involves balancing accuracy with
computational cost and aligning it with the capabilities of the target
hardware. Each component of a model, whether its depth, width, or input
resolution, impacts resource consumption. In hardware-aware design,
these dimensions should not only be optimized for accuracy but also for
efficiency in memory usage, processing power, and energy consumption,
especially when the model is deployed on specific hardware like GPUs,
TPUs, or edge devices.

From a hardware-aware perspective, it is important to consider how
different hardware platforms, such as GPUs, TPUs, or edge devices,
interact with scaling dimensions. For instance, deeper models can
capture more complex representations, but excessive depth can lead to
increased inference latency, longer training times, and higher memory
consumption, issues that are particularly problematic on
resource-constrained platforms. Similarly, increasing the width of the
model to process more parallel information may be beneficial for GPUs
and TPUs with high parallelism, but it requires careful management of
memory usage. In contrast, increasing the input resolution can provide
finer details for tasks like image classification, but it exponentially
increases computational costs, potentially overloading hardware memory
or causing power inefficiencies on edge devices.

Mathematically, the total FLOPs for a convolutional model can be
approximated as: \[
\text{FLOPs} \propto d \cdot w^2 \cdot r^2,
\] where \(d\) is depth, \(w\) is width, and \(r\) is the input
resolution. Increasing all three dimensions without considering the
hardware limitations can result in suboptimal performance, especially on
devices with limited computational power or memory bandwidth.

For efficient model scaling, managing these parameters in a balanced way
becomes essential, ensuring that the model remains within the limits of
the hardware while maximizing performance. This is where compound
scaling comes into play. Instead of adjusting depth, width, and
resolution independently, compound scaling balances all three dimensions
together by applying fixed ratios \((\alpha, \beta, \gamma)\) relative
to a base model: \[
d = \alpha^\phi d_0, \quad w = \beta^\phi w_0, \quad r = \gamma^\phi r_0
\] Here, \(\phi\) is a scaling coefficient, and \(\alpha\), \(\beta\),
and \(\gamma\) are scaling factors determined based on hardware
constraints and empirical data. This approach ensures that models grow
in a way that optimizes hardware resource usage, keeping them efficient
while improving accuracy.

For example, EfficientNet, which employs compound scaling, demonstrates
how carefully balancing depth, width, and resolution results in models
that are both computationally efficient and high-performing. Compound
scaling reduces computational cost while preserving accuracy, making it
a key consideration for hardware-aware model design. This approach is
particularly beneficial when deploying models on GPUs or TPUs, where
parallelism can be fully leveraged, but memory and power usage need to
be carefully managed. \textbf{?@sec-benchmarking-ai} examines
performance evaluation methods for measuring these efficiency gains.

This principle extends beyond convolutional models to other
architectures like transformers. Adjusting the number of layers,
attention heads, or embedding dimensions impacts computational
efficiency similarly. Hardware-aware scaling has become central to
optimizing model performance across various computational constraints,
especially when working with large models or resource-constrained
devices.

\subsubsection{Computation
Reduction}\label{sec-model-compression-computation-reduction-3222}

Modern architectures leverage factorized computations to decompose
complex operations into simpler components, reducing computational
overhead while maintaining representational power. Standard convolutions
apply filters uniformly across all spatial locations and channels,
creating computational bottlenecks on resource-constrained hardware.
Factorization techniques address this inefficiency by restructuring
operations to minimize redundant computation.

Depthwise separable convolutions, introduced in MobileNet, exemplify
this approach by decomposing standard convolutions into two stages:
depthwise convolution (applying separate filters to each input channel
independently) and pointwise convolution (1×1 convolution mixing outputs
across channels). The computational complexity of standard convolution
with input size \(h \times w\), \(C_{\text{in}}\) input channels, and
\(C_{\text{out}}\) output channels is: \[
\mathcal{O}(h w C_{\text{in}} C_{\text{out}} k^2)
\] where \(k\) is kernel size. Depthwise separable convolutions reduce
this to: \[
\mathcal{O}(h w C_{\text{in}} k^2) + \mathcal{O}(h w C_{\text{in}} C_{\text{out}})
\] eliminating the \(k^2\) factor from channel-mixing operations,
achieving 5×-10× FLOP reduction. This directly translates to reduced
memory bandwidth requirements and improved inference latency on mobile
and edge devices.

Complementary factorization techniques extend these benefits. Grouped
convolutions (ResNeXt) partition feature maps into independent groups
processed separately before merging, maintaining accuracy while reducing
redundant operations. Bottleneck layers (ResNet) apply 1×1 convolutions
to reduce feature dimensionality before expensive operations,
concentrating computation where it provides maximum value. Combined with
sparsity and hardware-aware scheduling, these techniques maximize
accelerator utilization across GPUs, TPUs, and specialized edge
processors.

While reducing computation is essential, memory constraints often prove
more limiting than compute capacity on resource-constrained devices. The
next section addresses these memory bottlenecks directly.

\subsubsection{Memory
Optimization}\label{sec-model-compression-memory-optimization-e6ac}

Memory optimization\sidenote{\textbf{Memory Optimization}: Techniques
reducing peak memory during training and inference. DenseNet-121's
feature reuse significantly reduces activation memory compared to
ResNet-50; gradient checkpointing typically trades 15-25\% additional
compute for substantial memory reduction (up to 10× for some
architectures). In-place operations, memory pooling, and operator fusion
further reduce footprint, enabling LLM inference on consumer GPUs. }
addresses performance bottlenecks arising when memory demands for
activations, feature maps, and parameters exceed hardware capacity on
resource-constrained devices. Modern architectures employ
memory-efficient strategies to reduce storage requirements while
maintaining performance, ensuring computational tractability and energy
efficiency on GPUs, TPUs, and edge AI platforms.

One effective technique for memory optimization is feature reuse, a
strategy employed in DenseNet. In traditional convolutional networks,
each layer typically computes a new set of feature maps, increasing the
model's memory footprint. However, DenseNet reduces the need for
redundant activations by reusing feature maps from previous layers and
selectively applying transformations. This method reduces the total
number of feature maps that need to be stored, which in turn lowers the
memory requirements without sacrificing accuracy. In a standard
convolutional network with \(L\) layers, if each layer generates \(k\)
new feature maps, the total number of feature maps grows linearly: \[
\mathcal{O}(L k)
\]

In contrast, DenseNet reuses feature maps from earlier layers, reducing
the number of feature maps stored. This leads to improved parameter
efficiency and a reduced memory footprint, which is important for
hardware with limited memory resources.

Another useful technique is activation
checkpointing\sidenote{\textbf{Activation Checkpointing}: Memory-time
trade-off technique that stores only selected activations during forward
pass, recomputing others during backpropagation. Reduces memory usage by
20-50\% in large transformers with only 15-20\% training time overhead.
Essential for training models like GPT-3 on limited GPU memory. }, which
is especially beneficial during training. In a typical neural network,
backpropagation requires storing all forward activations for the
backward pass. This can lead to a significant memory overhead,
especially for large models. Activation checkpointing reduces memory
consumption by only storing a subset of activations and recomputing the
remaining ones when needed.

If an architecture requires storing \(A_{\text{total}}\) activations,
the standard backpropagation method requires the full storage: \[
\mathcal{O}(A_{\text{total}})
\]

With activation checkpointing, however, only a fraction of activations
is stored, and the remaining ones are recomputed on-the-fly, reducing
storage requirements to: \[
\mathcal{O}\Big(\sqrt{A_{\text{total}}}\Big)
\]

Feature reuse can significantly reduce peak memory consumption, making
it useful for training large models on hardware with limited memory.

Parameter reduction is another important technique for models that use
large filters. For instance, SqueezeNet uses a novel architecture where
it applies \(1\times 1\) convolutions to reduce the number of input
channels before applying standard convolutions. By first reducing the
number of channels with \(1\times 1\) convolutions, SqueezeNet reduces
the model size significantly without compromising the model's expressive
power. The number of parameters in a standard convolutional layer is: \[
\mathcal{O}(C_{\text{in}} C_{\text{out}} k^2)
\]

By reducing \(C_{\text{in}}\) using \(1\times 1\) convolutions,
SqueezeNet\sidenote{\textbf{SqueezeNet}: DeepScale/Berkeley architecture
using fire modules (squeeze + expand layers) achieves AlexNet-level
accuracy (57.5\% top-1 ImageNet) with 50x fewer parameters (1.25M vs
60M). Model size drops from 240MB to 0.5MB uncompressed, enabling
deployment on smartphones and embedded systems with limited storage. }
reduces the number of parameters, achieving a 50x reduction in model
size compared to AlexNet while maintaining similar performance. This
method proves valuable for edge devices that have strict memory and
storage constraints.

Feature reuse, activation checkpointing, and parameter reduction form
key components of hardware-aware model design, allowing models to fit
within memory limits of modern accelerators while reducing power
consumption through fewer memory accesses. Specialized accelerators like
TPUs and GPUs leverage memory hierarchies, caching, and high bandwidth
memory to efficiently handle sparse or reduced-memory representations,
enabling faster inference with minimal overhead.

Beyond reducing what data must be stored, substantial efficiency gains
emerge from optimizing how operations access memory. The next technique
addresses this by combining multiple operations to reduce memory
traffic.

\subsubsection{Operator
Fusion}\label{sec-model-compression-operator-fusion-c1b4}

Operator fusion combines multiple computational operations into single
fused kernels, reducing intermediate memory traffic and kernel launch
overhead. This graph-level optimization technique addresses a
fundamental inefficiency in deep learning execution: sequential
operations that write intermediate results to memory only to immediately
read them back for the next operation. By fusing operations, inference
engines eliminate these redundant memory transactions, improving both
throughput and latency on memory-bound workloads.

Modern neural networks consist of sequences of operations such as
convolution, batch normalization, activation functions, and element-wise
operations. When executed independently, each operation requires:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Loading input tensors from global memory
\item
  Performing computation
\item
  Writing output tensors back to global memory
\item
  Launching the next kernel
\end{enumerate}

This pattern creates memory bandwidth bottlenecks for operations with
low arithmetic intensity (FLOPs per byte accessed). The memory traffic
for \(N\) unfused operations operating on tensors of size \(M\) bytes
is: \[
\text{Memory}_{\text{unfused}} = 2NM
\] where each operation reads (\(M\) bytes) and writes (\(M\) bytes)
intermediate results. Operator fusion reduces this to: \[
\text{Memory}_{\text{fused}} = 2M
\] by reading inputs once, computing all operations in sequence, and
writing final outputs once.

Common fusion patterns in neural network inference optimize specific
operation sequences that appear repeatedly in modern architectures:

\textbf{Convolution-BatchNorm-ReLU Fusion}: This ubiquitous pattern
appears in nearly every modern CNN architecture.
Listing~\ref{lst-conv-bn-relu-fusion} shows how fusion reduces three
memory round-trips to a single kernel launch:

\begin{codelisting}

\caption{\label{lst-conv-bn-relu-fusion}\textbf{Conv-BN-ReLU Fusion}:
Combining three operations into a single kernel reduces memory traffic
from 6 transfers to 2, eliminating intermediate memory writes.}

\centering{

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Unfused execution (3 kernel launches, 6 memory transfers)}
\NormalTok{conv\_out }\OperatorTok{=}\NormalTok{ conv2d(}\BuiltInTok{input}\NormalTok{, weight)}
\NormalTok{bn\_out }\OperatorTok{=}\NormalTok{ batch\_norm(conv\_out, ...)}
\NormalTok{relu\_out }\OperatorTok{=}\NormalTok{ relu(bn\_out)}


\CommentTok{\# Fused execution (1 kernel launch, 2 memory transfers)}
\KeywordTok{def}\NormalTok{ conv\_bn\_relu\_fused(}\BuiltInTok{input}\NormalTok{, weight, gamma, beta, mean, var):}
    \CommentTok{\# Read input and weight once}
\NormalTok{    conv }\OperatorTok{=}\NormalTok{ conv2d(}\BuiltInTok{input}\NormalTok{, weight)}

    \CommentTok{\# Apply batch norm in registers (no memory write)}
\NormalTok{    bn }\OperatorTok{=}\NormalTok{ gamma }\OperatorTok{*}\NormalTok{ (conv }\OperatorTok{{-}}\NormalTok{ mean) }\OperatorTok{/}\NormalTok{ sqrt(var }\OperatorTok{+}\NormalTok{ eps) }\OperatorTok{+}\NormalTok{ beta}

    \CommentTok{\# Apply ReLU in registers (no memory write)}
\NormalTok{    output }\OperatorTok{=} \BuiltInTok{max}\NormalTok{(bn, }\DecValTok{0}\NormalTok{)}

    \CommentTok{\# Write final result once}
    \ControlFlowTok{return}\NormalTok{ output}
\end{Highlighting}
\end{Shaded}

}

\end{codelisting}%

The arithmetic operations remain identical, but memory traffic drops
from 6 transfers to 2 transfers (3× reduction). For a ResNet-50 layer
with 256 channels and spatial size \(28 \times 28\), this eliminates
\(2 \times 256 \times 28 \times 28 \times 4 \text{ bytes} = 1.5\text{ MB}\)
of intermediate memory traffic per layer.

\textbf{Element-wise Operation Fusion}: Element-wise operations
(addition, multiplication, activation functions) have extremely low
arithmetic intensity, making them severely memory-bound.
Listing~\ref{lst-elementwise-fusion} demonstrates how combining these
operations reduces memory traffic:

\begin{codelisting}

\caption{\label{lst-elementwise-fusion}\textbf{Element-wise Fusion}:
Combining multiple memory-bound operations into a single kernel reduces
memory traffic from 8 transfers to 3.}

\centering{

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Unfused: 4 kernel launches, 8 memory transfers}
\NormalTok{t1 }\OperatorTok{=}\NormalTok{ x }\OperatorTok{+}\NormalTok{ y}
\NormalTok{t2 }\OperatorTok{=}\NormalTok{ t1 }\OperatorTok{*}\NormalTok{ scale}
\NormalTok{t3 }\OperatorTok{=}\NormalTok{ relu(t2)}
\NormalTok{t4 }\OperatorTok{=}\NormalTok{ t3 }\OperatorTok{+}\NormalTok{ bias}

\CommentTok{\# Fused: 1 kernel launch, 3 memory transfers}
\NormalTok{output }\OperatorTok{=}\NormalTok{ relu((x }\OperatorTok{+}\NormalTok{ y) }\OperatorTok{*}\NormalTok{ scale) }\OperatorTok{+}\NormalTok{ bias}
\end{Highlighting}
\end{Shaded}

}

\end{codelisting}%

For tensors of size 1 MB, unfused execution performs 8 MB of memory
traffic. Fused execution reduces this to 3 MB (2.67× reduction). On a
GPU with 900 GB/s memory bandwidth, this saves approximately 6
microseconds per operation, accumulating to milliseconds of latency
reduction over thousands of operations in a full model.

\textbf{Matrix Multiply-Add Fusion (GEMM Fusion)}: Linear layers
followed by bias addition and activation appear throughout transformers
and MLPs. Listing~\ref{lst-gemm-fusion} illustrates the fusion pattern:

\begin{codelisting}

\caption{\label{lst-gemm-fusion}\textbf{GEMM Fusion}: Fusing bias
addition and activation into matrix multiplication eliminates
intermediate memory writes by computing element-wise operations in
registers.}

\centering{

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Unfused: 3 operations}
\NormalTok{linear }\OperatorTok{=}\NormalTok{ matmul(}\BuiltInTok{input}\NormalTok{, weight)}
\NormalTok{biased }\OperatorTok{=}\NormalTok{ linear }\OperatorTok{+}\NormalTok{ bias}
\NormalTok{output }\OperatorTok{=}\NormalTok{ gelu(biased)}

\CommentTok{\# Fused: 1 operation}
\NormalTok{output }\OperatorTok{=}\NormalTok{ gelu(matmul(}\BuiltInTok{input}\NormalTok{, weight) }\OperatorTok{+}\NormalTok{ bias)}
\end{Highlighting}
\end{Shaded}

}

\end{codelisting}%

This fusion is particularly effective because matrix multiplication is
compute-bound (high arithmetic intensity), while bias addition and
activation are memory-bound (low arithmetic intensity). Fusing them
allows the GPU to perform element-wise operations in registers
immediately after computing each output element, before writing to
memory. For BERT-Base with hidden size 768, this eliminates
\(768 \times 512 \times 4 \text{ bytes} = 1.5\text{ MB}\) of
intermediate activation memory per linear layer.

\textbf{Attention Pattern Fusion}: Transformer attention mechanisms
involve multiple operations that benefit from fusion.
Listing~\ref{lst-attention-fusion} compares unfused attention with
FlashAttention's tile-based approach:

\begin{codelisting}

\caption{\label{lst-attention-fusion}\textbf{Attention Fusion}:
FlashAttention reorganizes attention computation to process in tiles
that fit in SRAM, reducing HBM traffic from O(n²) to O(n).}

\centering{

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Unfused attention (6+ kernel launches)}
\NormalTok{scores }\OperatorTok{=}\NormalTok{ matmul(Q, K.T) }\OperatorTok{/}\NormalTok{ sqrt(d\_k)}
\NormalTok{attn\_weights }\OperatorTok{=}\NormalTok{ softmax(scores)}
\NormalTok{attn\_output }\OperatorTok{=}\NormalTok{ matmul(attn\_weights, V)}
\NormalTok{output }\OperatorTok{=}\NormalTok{ linear(attn\_output)}

\CommentTok{\# FlashAttention: fused attention in blocks}
\CommentTok{\# Processes attention in tiles that fit in SRAM}
\CommentTok{\# Reduces HBM traffic from O(n\^{}2) to O(n)}
\end{Highlighting}
\end{Shaded}

}

\end{codelisting}%

This fusion pattern\sidenote{FlashAttention (introduced in
\textbf{?@sec-dnn-architectures}) demonstrates fusion's power for
memory-bound attention: tiling to SRAM yields 2-4x speedup and enables
64K context windows by reducing memory traffic from O(n\^{}2) to O(n).
This exemplifies how operator fusion can transform memory-bound
bottlenecks into compute-bound operations. } achieves 2-4× speedup on
long sequences by dramatically reducing memory bandwidth requirements,
as detailed in
\textbf{?@sec-ai-training-flash-attention-ioaware-attention-optimization-3da0}.

Memory bandwidth analysis quantifies fusion benefits. Consider a
Conv-BN-ReLU sequence operating on a \(28 \times 28 \times 256\) feature
map (784 KB):

\textbf{Unfused execution}:

\begin{itemize}
\tightlist
\item
  Conv reads input (784 KB) + weights (2.4 MB), writes output (784 KB) =
  4.0 MB
\item
  BN reads input (784 KB) + params (2 KB), writes output (784 KB) = 1.6
  MB
\item
  ReLU reads input (784 KB), writes output (784 KB) = 1.6 MB
\item
  \textbf{Total memory traffic: 7.2 MB}
\end{itemize}

\textbf{Fused execution}:

\begin{itemize}
\tightlist
\item
  Conv-BN-ReLU reads input (784 KB) + weights (2.4 MB), writes output
  (784 KB) = 3.9 MB
\item
  \textbf{Total memory traffic: 3.9 MB}
\item
  \textbf{Bandwidth reduction: 46\%}
\end{itemize}

On a V100 GPU with 900 GB/s HBM bandwidth and 14 TFLOPS FP32 compute,
the unfused sequence takes approximately 8 microseconds (memory-bound),
while the fused version takes 4.3 microseconds (1.86× speedup). The
speedup comes entirely from reducing memory traffic, as the compute
remains identical.

Operator fusion implementation occurs at multiple levels in the software
stack:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Framework-level fusion}: TensorFlow's Grappler and PyTorch's
  TorchScript identify fusible patterns in the computational graph and
  apply transformations automatically.
\item
  \textbf{Compiler-level fusion}: TVM, XLA, and TensorRT perform more
  aggressive fusion using polyhedral optimization and code generation.
\item
  \textbf{Runtime-level fusion}: ONNX Runtime and TensorRT apply runtime
  fusion based on input shapes and hardware characteristics.
\end{enumerate}

Listing~\ref{lst-graph-pattern-matching} demonstrates how graph pattern
matching identifies fusible operation sequences:

\begin{codelisting}

\caption{\label{lst-graph-pattern-matching}\textbf{Graph Pattern
Matching}: Compilers identify fusible operation sequences and generate
specialized fused kernels for common patterns.}

\centering{

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Graph pattern matching}
\KeywordTok{def}\NormalTok{ identify\_fusible\_patterns(graph):}
\NormalTok{    patterns }\OperatorTok{=}\NormalTok{ [}
\NormalTok{        (}\StringTok{"Conv2D"}\NormalTok{, }\StringTok{"BatchNorm"}\NormalTok{, }\StringTok{"ReLU"}\NormalTok{),}
\NormalTok{        (}\StringTok{"MatMul"}\NormalTok{, }\StringTok{"BiasAdd"}\NormalTok{, }\StringTok{"GELU"}\NormalTok{),}
\NormalTok{        (}\StringTok{"Add"}\NormalTok{, }\StringTok{"Multiply"}\NormalTok{, }\StringTok{"ReLU"}\NormalTok{),}
\NormalTok{    ]}

\NormalTok{    fused\_ops }\OperatorTok{=}\NormalTok{ []}
    \ControlFlowTok{for}\NormalTok{ pattern }\KeywordTok{in}\NormalTok{ patterns:}
\NormalTok{        matches }\OperatorTok{=}\NormalTok{ find\_pattern(graph, pattern)}
\NormalTok{        fused\_ops.extend(create\_fused\_kernel(m) }\ControlFlowTok{for}\NormalTok{ m }\KeywordTok{in}\NormalTok{ matches)}

    \ControlFlowTok{return}\NormalTok{ fused\_ops}
\end{Highlighting}
\end{Shaded}

}

\end{codelisting}%

Fusion effectiveness varies by workload characteristics. Memory-bound
operations benefit most from fusion, while compute-bound operations see
minimal improvement. Quantitative analysis shows:

\begin{itemize}
\tightlist
\item
  \textbf{Element-wise operations}: 2-4× speedup (highly memory-bound,
  low arithmetic intensity)
\item
  \textbf{Conv-BN-Act patterns}: 1.5-2× speedup (mixed memory/compute
  characteristics)
\item
  \textbf{GEMM-based operations}: 1.2-1.5× speedup (compute-bound,
  fusion reduces memory-bound tail)
\item
  \textbf{Attention mechanisms}: 2-4× speedup on long sequences
  (quadratic memory scaling)
\end{itemize}

Fusion also reduces kernel launch overhead. Each CUDA kernel launch
incurs approximately 5-10 microseconds of latency. For a ResNet-50 with
53 convolutional layers, unfused execution launches 159 kernels (Conv +
BN + ReLU), while fused execution launches 53 kernels, saving
approximately 1 millisecond from launch overhead alone.

Hardware considerations affect fusion strategies. GPUs with large
register files and shared memory benefit most from fusion, as
intermediate results remain on-chip. TPUs with software-managed memory
hierarchies perform fusion at the compiler level. Edge accelerators with
limited memory make fusion critical for fitting models within
constrained memory budgets.

Fusion trade-offs include increased compilation time, reduced
flexibility for dynamic shapes, and potential code bloat from
specialized kernels. Advanced inference frameworks balance these
trade-offs by selectively fusing hot paths while maintaining flexibility
for dynamic workloads.

Operator fusion optimizes how operations execute by reducing memory
traffic between fixed computational steps. A complementary approach asks
a different question: must we execute all computational steps at all?
This leads to adaptive computation methods that vary the amount of work
performed based on input characteristics.

\subsection{Adaptive Computation
Methods}\label{sec-model-compression-adaptive-computation-methods-3494}

While operator fusion optimizes memory access patterns for fixed
computation graphs, adaptive computation methods address a different
inefficiency: conventional models apply uniform processing to all inputs
regardless of complexity, wasting resources on simple cases and
increasing power consumption. Dynamic computation allows models to skip
layers or operations for simple inputs while engaging deeper networks
for complex cases, optimizing computational efficiency, energy
consumption, and latency while preserving predictive performance. This
capability proves essential for resource-constrained hardware in mobile
devices, embedded systems, and autonomous vehicles where real-time
processing is critical.

\subsubsection{Dynamic
Schemes}\label{sec-model-compression-dynamic-schemes-6b78}

Dynamic schemes enable models to selectively reduce computation when
inputs are simple, preserving resources while maintaining predictive
performance. The approaches discussed below, beginning with early exit
architectures, illustrate how to implement this adaptive strategy
effectively.

\paragraph{Early Exit
Architectures}\label{sec-model-compression-early-exit-architectures-9b61}

Early exit architectures allow a model to make predictions at
intermediate points in the network rather than completing the full
forward pass for every input. This approach proves effective for
real-time applications and energy-efficient inference, as it enables
selective computation based on the complexity of individual inputs
(\citeproc{ref-teerapittayanon2016branchynet}{Teerapittayanon, McDanel,
and Kung 2017}).

The core mechanism in early exit architectures involves multiple exit
points embedded within the network. Simpler inputs, which can be
classified with high confidence early in the model, exit at an
intermediate layer, reducing unnecessary computations. Conversely, more
complex inputs continue processing through deeper layers to ensure
accuracy.

A well-known example is BranchyNet\sidenote{\textbf{BranchyNet}:
Pioneered adaptive inference with early exit branches at multiple
network depths, achieving significant speedups on image classification
tasks. Reduces average inference time substantially for simple inputs
while maintaining full computation for complex cases, enabling real-time
processing on mobile devices. The exact speedup depends on input
distribution and confidence thresholds. }, which introduces multiple
exit points throughout the network. For each input, the model evaluates
intermediate predictions using confidence thresholds. If the prediction
confidence exceeds a predefined threshold at an exit point, the model
terminates further computations and outputs the result. Otherwise, it
continues processing until the final layer
(\citeproc{ref-teerapittayanon2016branchynet}{Teerapittayanon, McDanel,
and Kung 2017}). This approach minimizes inference time without
compromising performance on challenging inputs.

Another example is multi-exit vision transformers, which extend early
exits to transformer-based architectures. These models use lightweight
classifiers at various transformer layers, allowing predictions to be
generated early when possible
(\citeproc{ref-scardapane2020should}{Scardapane, Wang, and Panella
2020}). This technique significantly reduces inference time while
maintaining robust performance for complex samples.

Early exit models prove advantageous for resource-constrained devices,
such as mobile processors and edge accelerators. By dynamically
adjusting computational effort, these architectures reduce power
consumption and processing latency, making them ideal for real-time
decision-making (\citeproc{ref-hu2021triple}{B. Hu, Zhang, and Fu
2021}).

When deployed on hardware accelerators such as GPUs and TPUs, early exit
architectures can be further optimized by exploiting parallelism. For
instance, different exit paths can be evaluated concurrently, thereby
improving throughput while preserving the benefits of adaptive
computation (\citeproc{ref-yu2023efficient}{Yu, Li, and Wang 2023}).
This approach is illustrated in
Figure~\ref{fig-early-exit-transformers}, where each transformer layer
is followed by a classifier and an optional early exit mechanism based
on confidence estimation or latency-to-accuracy trade-offs (LTE). At
each stage, the system may choose to exit early if sufficient confidence
is achieved, or continue processing through deeper layers, enabling
dynamic allocation of computational resources.

\begin{figure}[htb]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/39bea5f8852957025881759aef88a548da2e0a5e.pdf}}

}

\caption{\label{fig-early-exit-transformers}\textbf{Early Exit
Architecture}: Transformer layers dynamically adjust computation by
classifying each layer's output and enabling early termination if
sufficient confidence is reached, reducing latency and power consumption
for resource-constrained devices. This approach allows for parallel
evaluation of different exit paths, improving throughput on hardware
accelerators like GPUs and TPUs. Source:
(\citeproc{ref-xin-etal-2021-berxit}{Xin et al. 2021}).}

\end{figure}%

\paragraph{Conditional
Computation}\label{sec-model-compression-conditional-computation-a653}

Conditional computation refers to the ability of a neural network to
decide which parts of the model to activate based on the input, thereby
reducing unnecessary computation. This approach can be highly beneficial
in resource-constrained environments, such as mobile devices or
real-time systems, where reducing the number of operations directly
translates to lower computational cost, power consumption, and inference
latency (\citeproc{ref-bengio2015conditional}{E. Bengio et al. 2015}).

In contrast to Early Exit Architectures, where the decision to exit
early is typically made once a threshold confidence level is met,
conditional computation works by dynamically selecting which layers,
units, or paths in the network should be computed based on the
characteristics of the input. This can be achieved through mechanisms
such as gating functions or dynamic routing, which ``turn off'' parts of
the network that are not needed for a particular input, allowing the
model to focus computational resources where they are most required.

One example of conditional computation is SkipNet, which uses a gating
mechanism to skip layers in a CNN when the input is deemed simple
enough. The gating mechanism uses a lightweight classifier to predict if
the layer should be skipped. This prediction is made based on the input,
and the model adjusts the number of layers used during inference
accordingly (\citeproc{ref-wang2018skipnet}{X. Wang et al. 2018}). If
the gating function determines that the input is simple, certain layers
are bypassed, resulting in faster inference. However, for more complex
inputs, the model uses the full depth of the network to achieve the
necessary accuracy.

Another example is Dynamic Routing Networks, such as in the Capsule
Networks (CapsNets), where routing mechanisms dynamically choose the
path that activations take through the network. In these networks, the
decision-making process involves selecting specific pathways for
information flow based on the input's complexity, which can
significantly reduce the number of operations and computations required
(\citeproc{ref-sabour2017dynamic}{Sabour, Frosst, and Hinton 2017}).
This mechanism introduces adaptability by using different routing
strategies, providing computational efficiency while preserving the
quality of predictions.

These conditional computation strategies have significant advantages in
real-world applications where computational resources are limited. For
example, in autonomous driving, the system must process a variety of
inputs (e.g., pedestrians, traffic signs, road lanes) with varying
complexity. In cases where the input is straightforward, a simpler, less
computationally demanding path can be taken, whereas more complex
scenarios (such as detecting obstacles or performing detailed scene
understanding) will require full use of the model's capacity.
Conditional computation ensures that the system adapts its computation
based on the real-time complexity of the input, leading to improved
speed and efficiency (\citeproc{ref-huang2023adaptive}{Huang, Chen, and
Zhang 2023}).

\paragraph{Gate-Based
Computation}\label{sec-model-compression-gatebased-computation-778c}

Gate-based conditional computation introduces learned gating mechanisms
that dynamically control which parts of a neural network are activated
based on input complexity. Unlike static architectures that process all
inputs with the same computational effort, this approach enables dynamic
activation of sub-networks or layers by learning decision boundaries
during training (\citeproc{ref-shazeer2017outrageously}{Shazeer et al.
2017}).

Gating mechanisms are typically implemented using binary or continuous
gating functions, wherein a lightweight control module (often called a
router or gating network) predicts whether a particular layer or path
should be executed. This decision-making occurs dynamically at inference
time, allowing the model to allocate computational resources adaptively.

A well-known example of this paradigm is the Dynamic Filter Network
(DFN), which applies input-dependent filtering by selecting different
convolutional kernels at runtime. DFN reduces unnecessary computation by
avoiding uniform filter application across inputs, tailoring its
computations based on input complexity
(\citeproc{ref-jia2016dynamic}{Jia et al. 2016}).

Another widely adopted strategy is the Mixture of Experts (MoE)
framework. In this architecture, a gating network selects a subset of
specialized expert subnetworks to process each input
(\citeproc{ref-shazeer2017outrageously}{Shazeer et al. 2017}). This
allows only a small portion of the total model to be active for any
given input, significantly improving computational efficiency without
sacrificing model capacity. A notable instantiation of this idea is
Google's Switch Transformer\sidenote{\textbf{Switch Transformer}: Scales
to 1.6 trillion parameters while activating only 2 billion per token,
achieving 7× faster pretraining than dense T5 at equivalent FLOPs.
Routes each token to a single expert (vs.~top-k), reducing communication
overhead. Training instability from load imbalance requires auxiliary
loss terms; production deployments use capacity factors of 1.25-2× to
handle routing variance. }, which extends the transformer architecture
with expert-based conditional computation
(\citeproc{ref-fedus2021switch}{Fedus, Zoph, and Shazeer 2021}). While
we introduce MoE principles here for single-system context, large-scale
MoE deployments involving distributed expert placement are explored in
advanced coverage of large-scale systems.

\begin{figure}[htb]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/4edec9088e9eddeae0f2f202dffad4b2bb7b3065.pdf}}

}

\caption{\label{fig-switch-transformer}\textbf{Conditional Computation}:
Switch transformers enhance efficiency by dynamically routing tokens to
specialized expert subnetworks, enabling parallel processing and
reducing the computational load per input. This architecture implements
a form of mixture of experts where a gating network selects which
experts process each token, allowing for increased model capacity
without a proportional increase in computation. \emph{Source:
(\citeproc{ref-fedus2021switch}{Fedus, Zoph, and Shazeer 2021})}.}

\end{figure}%

As Figure~\ref{fig-switch-transformer} illustrates, the Switch
Transformer replaces the traditional feedforward layer with a Switching
FFN Layer. For each token, a lightweight router selects a single expert
from a pool of feedforward networks. The router outputs a probability
distribution over available experts, and the highest-probability expert
is activated per token. This design enables large models to scale
parameter count without proportionally increasing inference cost.

Gate-based conditional computation proves effective for multi-task and
transfer learning settings, where inputs may benefit from specialized
processing pathways. By enabling fine-grained control over model
execution, such mechanisms allow for adaptive specialization across
tasks while maintaining efficiency.

However, these benefits come at the cost of increased architectural
complexity. The routing and gating operations themselves introduce
additional overhead, both in terms of latency and memory access.
Efficient deployment on hardware accelerators such as GPUs, TPUs, or
edge devices requires careful attention to the scheduling and batching
of expert activations (\citeproc{ref-lepikhin2020gshard}{Lepikhin et al.
2020}).

\paragraph{Adaptive
Inference}\label{sec-model-compression-adaptive-inference-7392}

Adaptive inference refers to a model's ability to dynamically adjust its
computational effort during inference based on input complexity. Unlike
earlier approaches that rely on predefined exit points or discrete layer
skipping, adaptive inference continuously modulates computational depth
and resource allocation based on real-time confidence and task
complexity (\citeproc{ref-yang2020resolution}{Yang et al. 2020}).

This flexibility allows models to make on-the-fly decisions about how
much computation is required, balancing efficiency and accuracy without
rigid thresholds. Instead of committing to a fixed computational path,
adaptive inference enables models to dynamically allocate layers,
operations, or specialized computations based on intermediate
assessments of the input (\citeproc{ref-yang2020resolution}{Yang et al.
2020}).

One example of adaptive inference is Fast Neural Networks (FNNs), which
adjust the number of active layers based on real-time complexity
estimation. If an input is deemed straightforward, only a subset of
layers is activated, reducing inference time. However, if early layers
produce low-confidence outputs, additional layers are engaged to refine
the prediction (\citeproc{ref-wu2019fast}{Jian Wu, Cheng, and Zhang
2019}).

A related approach is dynamic layer scaling, where models progressively
increase computational depth based on uncertainty estimates. This
technique proves useful for fine-grained classification tasks, where
some inputs require only coarse-grained processing while others need
deeper feature extraction (\citeproc{ref-wang2021glam}{Contro et al.
2021}).

Adaptive inference proves effective in latency-sensitive applications
where resource constraints fluctuate dynamically. For instance, in
autonomous systems, tasks such as lane detection may require minimal
computation, while multi-object tracking in dense environments demands
additional processing power. By adjusting computational effort in
real-time, adaptive inference ensures that models operate within strict
timing constraints without unnecessary resource consumption.

On hardware accelerators such as GPUs and TPUs, adaptive inference
leverages parallel processing capabilities by distributing workloads
dynamically. This adaptability maximizes throughput while minimizing
energy expenditure, making it ideal for real-time, power-sensitive
applications.

\subsubsection{Implementation
Challenges}\label{sec-model-compression-implementation-challenges-8254}

Dynamic computation introduces several practical challenges:

\textbf{Training complexity}: Discrete gating decisions cannot be
optimized with standard backpropagation, requiring reinforcement
learning or continuous approximations. Different inputs following
different paths leads to inconsistent gradient updates, requiring
careful regularization.

\textbf{Overhead and latency variability}: Gating decisions add
computational overhead that can offset savings from skipped
computations. Variable inference times are problematic for real-time
applications with strict latency requirements.

\textbf{Hardware inefficiency}: Dynamic computation patterns reduce
hardware utilization because modern accelerators are optimized for
regular, predictable operations. When inputs follow different paths,
some hardware resources remain idle. See \textbf{?@sec-ai-acceleration}
for hardware-aware strategies.

\textbf{Generalization risks}: Models may learn to allocate insufficient
computation to rare but important inputs, creating biased predictions.
Dynamic models also introduce new adversarial attack vectors where
attackers manipulate gating mechanisms.

\textbf{Evaluation difficulty}: Standard benchmarks assume fixed
computational budgets. FLOPs and latency metrics do not capture adaptive
computation, and variable execution paths complicate reproducibility.

Despite these challenges, dynamic computation remains promising for
efficiency optimization. Addressing these limitations requires robust
training techniques, hardware-aware execution strategies, and evaluation
frameworks that account for adaptive scaling.

\subsection{Sparsity
Exploitation}\label{sec-model-compression-sparsity-exploitation-5abb}

Dynamic computation decides \emph{whether} to perform certain
operations, but many computations still involve multiplying by zero, a
waste that sparsity exploitation directly addresses.
Sparsity\sidenote{\textbf{Sparsity}: From Latin ``sparsus'' (scattered,
spread out), past participle of ``spargere'' (to scatter). The
mathematical sense dates to the 1950s-1960s when researchers studying
linear systems noticed that many real-world matrices had mostly zero
entries. In ML, sparsity became central with the development of L1
regularization (LASSO, 1996), which induces exact zeros in weights
rather than just small values. } in machine learning refers to the
condition where a significant portion of the elements within a tensor,
such as weight matrices or activation tensors, are zero or nearly zero.

More formally, for a tensor \(T \in \mathbb{R}^{m \times n}\) (or higher
dimensions), the sparsity \(S\) can be expressed as: \[
S = \frac{\Vert \mathbf{1}_{\{T_{ij} = 0\}} \Vert_0}{m \times n}
\] where \(\mathbf{1}_{\{T_{ij} = 0\}}\) is an indicator function that
yields 1 if \(T_{ij} = 0\) and 0 otherwise, and \(\Vert \cdot \Vert_0\)
represents the L0 norm, which counts the number of non-zero elements.

Due to the nature of floating-point representations, we often extend
this definition to include elements that are close to zero. This leads
to: \[
S_{\epsilon} = \frac{\Vert \mathbf{1}_{\{|T_{ij}| < \epsilon\}} \Vert_0}{m \times n}
\] where \(\epsilon\) is a small threshold value.

Sparsity can emerge naturally during training, often as a result of
regularization techniques, or be deliberately introduced through methods
like pruning, where elements below a specific threshold are forced to
zero. Effectively exploiting sparsity leads to significant computational
efficiency, memory savings, and reduced power consumption, which prove
valuable when deploying models on devices with limited resources, such
as mobile phones, embedded systems, and edge devices.

\subsubsection{Sparsity
Types}\label{sec-model-compression-sparsity-types-6196}

Sparsity in neural networks can be broadly classified into two types:
unstructured sparsity and structured sparsity.

Unstructured sparsity occurs when individual weights are set to zero
without any specific pattern. This type of sparsity can be achieved
through techniques like pruning, where weights that are considered less
important (often based on magnitude or other criteria) are removed.
While unstructured sparsity is highly flexible and can be applied to any
part of the network, it can be less efficient on hardware since it lacks
a predictable structure. In practice, exploiting unstructured sparsity
requires specialized hardware or software optimizations to make the most
of it.

In contrast, structured sparsity involves removing entire components of
the network, such as filters, neurons, or channels, in a more structured
manner. By eliminating entire parts of the network, structured sparsity
is more efficient on hardware accelerators like GPUs or TPUs, which can
leverage this structure for faster computations. Structured sparsity is
often used when there is a need for predictability and efficiency in
computational resources, as it enables the hardware to fully exploit
regular patterns in the network.

\subsubsection{Sparsity Utilization
Methods}\label{sec-model-compression-sparsity-utilization-methods-9958}

With the distinction between unstructured and structured sparsity
patterns established, translating these theoretical advantages into
practical performance gains becomes the focus. The challenge lies in the
gap between theoretical parameter reduction and actual speedup: sparse
models with 90\% of weights zeroed may still run at nearly full
computational cost on hardware not designed for irregular memory access.
Bridging this gap requires specialized utilization methods and hardware
support that can efficiently skip zero-valued computations
(\citeproc{ref-Hoefler2021}{Hoefler, Alistarh, Ben-Nun, Dryden, and
Peste 2021}). Structured sparsity proves more hardware-efficient,
enabling accelerators like GPUs and TPUs to fully exploit regular
patterns (\citeproc{ref-Han2015}{Han et al. 2015}).

Sparse matrix operations skip zero elements during computation,
significantly reducing arithmetic operations. For example, multiplying a
dense \(4\times 4\) matrix with a vector typically requires 16
multiplications, while a sparse-aware implementation computes only the 6
nonzero operations: \[
\begin{bmatrix}
2 & 0 & 0 & 1 \\
0 & 3 & 0 & 0 \\
4 & 0 & 5 & 0 \\
0 & 0 & 0 & 6
\end{bmatrix}
\begin{bmatrix} x_1 \\ x_2 \\ x_3 \\ x_4 \end{bmatrix}
=
\begin{bmatrix} 2x_1 + x_4 \\ 3x_2 \\ 4x_1 + 5x_3 \\ 6x_4 \end{bmatrix}
\]

A third important technique for exploiting sparsity is low-rank
approximation. In this approach, large, dense weight matrices are
approximated by smaller, lower-rank matrices that capture the most
important information while discarding redundant components. This
reduces both the storage requirements and computational cost. For
instance, a weight matrix of size \(1000 \times 1000\) with one million
parameters can be factorized into two smaller matrices, say \(U\) (size
\(1000 \times 50\)) and \(V\) (size \(50 \times 1000\)), which results
in only 100,000 parameters, much fewer than the original one million.
This smaller representation retains the key features of the original
matrix while significantly reducing the computational burden
(\citeproc{ref-Denton2014}{Denton, Chintala, and Fergus 2014}).

Low-rank approximations, such as Singular Value Decomposition, are
commonly used to compress weight matrices in neural networks. These
approximations are widely applied in recommendation systems and natural
language processing models to reduce computational complexity and memory
usage without a significant loss in performance
(\citeproc{ref-Joulin2017}{Joulin et al. 2017}).

In addition to these core methods, other techniques like sparsity-aware
training can also help models to learn sparse representations during
training. For instance, using sparse gradient descent, where the
training algorithm updates only non-zero elements, can help the model
operate with fewer active parameters. While pruning and low-rank
approximations directly reduce parameters or factorize weight matrices,
sparsity-aware training helps maintain efficient models throughout the
training process (\citeproc{ref-Bellec2018}{Liu et al. 2018}).

\subsubsection{Sparsity Hardware
Support}\label{sec-model-compression-sparsity-hardware-support-ea16}

Achieving actual speedups from sparsity requires hardware that can
efficiently skip zero-valued computations. The hardware acceleration
principles in \textbf{?@sec-ai-acceleration} examine how different
processor architectures handle sparse patterns with varying
effectiveness. Software libraries can help bridge this gap by
reformulating sparse computations into patterns that current hardware
handles efficiently. For example, MegaBlocks
(\citeproc{ref-gale2022megablocksefficientsparsetraining}{Gale et al.
2022}) reformulates sparse Mixture of Experts training into block-sparse
operations, developing specialized kernels that maintain high
accelerator utilization despite irregular sparsity patterns.

\subsubsection{Structured
Patterns}\label{sec-model-compression-structured-patterns-b9bb}

Various sparsity formats have been developed, each with unique
structural characteristics and implications. Two of the most prominent
are block sparse matrices and N:M sparsity patterns. Block sparse
matrices generally have isolated blocks of zero and non-zero dense
submatrices such that a matrix operation on the large sparse matrix can
be easily re-expressed as a smaller (overall arithmetic-wise) number of
dense operations on submatrices. This sparsity allows more efficient
storage of the dense submatricies while maintaining shape compatibility
for operations like matrix or vector products. For example,
Figure~\ref{fig-block-sparse-gemm} shows how NVIDIA's cuSPARSE
(\citeproc{ref-nvidia_cusparse_block}{NVIDIA 2020}) library supports
sparse block matrix operations and storage. Several other works, such as
Monarch matrices
(\citeproc{ref-dao2022monarchexpressivestructuredmatrices}{Dao et al.
2022}), have extended on this block-sparsity to strike an improved
balance between matrix expressivity and compute/memory efficiency.

\begin{figure}[htb]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/dd068d2d1497db0a30e05fdf8cf86bdae9281b58.pdf}}

}

\caption{\label{fig-block-sparse-gemm}\textbf{Block Sparse
Representation}: NVIDIA's cusparse library efficiently stores block
sparse matrices by exploiting dense submatrix structures, enabling
accelerated matrix operations while maintaining compatibility with dense
matrix computations through block indexing. This approach reduces memory
footprint and arithmetic complexity for sparse linear algebra, important
for scaling machine learning models. \emph{Source: NVIDIA.}}

\end{figure}%

Similarly, the \(N\):\(M\) sparsity pattern is a structured sparsity
format where, in every set of \(M\) consecutive elements (e.g., weights
or activations), exactly \(N\) are non-zero, and the other two are zero
(\citeproc{ref-zhou2021learningnmfinegrainedstructured}{Zhou et al.
2021}). This deterministic pattern facilitates efficient hardware
acceleration, as it allows for predictable memory access patterns and
optimized computations. By enforcing this structure, models can achieve
a balance between sparsity-induced efficiency gains and maintaining
sufficient capacity for learning complex representations.
Figure~\ref{fig-2-4-gemm} below shows a comparison between accelerating
dense versus 2:4 sparsity matrix multiplication, a common sparsity
pattern used in model training. Later works like STEP
(\citeproc{ref-lu2023steplearningnmstructured}{Lu et al. 2023}) have
examined learning more general \(N\):\(M\) sparsity masks for
accelerating deep learning inference under the same principles.

\begin{figure}[htb]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/aaeeb94a533b80df75a97081a8e2501c24a80339.pdf}}

}

\caption{\label{fig-2-4-gemm}\textbf{Sparse Matrix Multiplication}:
Block sparsity optimizes matrix operations by storing only non-zero
elements and using structured indexing, enabling efficient GPU
acceleration for neural network computations. This technique maintains
compatibility with dense matrix operations while reducing memory access
and computational cost, particularly beneficial for large-scale models.
Source: PyTorch blog (\citeproc{ref-pytorch_sparsity_blog}{PyTorch
2022}).}

\end{figure}%

\paragraph{Hardware Acceleration of Sparse
Operations}\label{sec-model-compression-hardware-acceleration-sparse-operations-d609}

Modern hardware accelerators provide specialized support for sparse
operations, though the degree of acceleration depends on sparsity
structure and hardware capabilities. \textbf{?@sec-ai-acceleration}
examines these accelerators in depth; here we summarize their
sparsity-specific features.

\textbf{GPUs} with Sparse Tensor Cores (NVIDIA Ampere and later)
accelerate structured sparsity patterns like 2:4, achieving up to 2×
speedup by skipping zero multiplications
(\citeproc{ref-NVIDIA2020}{Abdelkhalik et al. 2022}). However, this
acceleration requires the sparsity pattern to match hardware
expectations---unstructured sparsity typically sees limited benefit
(\citeproc{ref-Hoefler2021}{Hoefler, Alistarh, Ben-Nun, Dryden, and
Peste 2021}).

\textbf{TPUs} provide support for sparse weight matrices, though this
capability has evolved across generations. The original TPU was designed
primarily for dense operations; later versions and research adaptations
(such as Sparse-TPU) have added sparse matrix support. The systolic
array architecture can process non-zero elements efficiently when
sparsity patterns are predictable, making this particularly beneficial
for transformer models where large matrix multiplications dominate
(\citeproc{ref-Jouppi2021}{Jouppi et al. 2021}).

\textbf{FPGAs} offer flexibility for custom sparsity patterns. Unlike
GPUs and TPUs, FPGAs can be programmed to handle arbitrary sparse
formats, making them suitable for unstructured pruning or
application-specific patterns where general-purpose accelerators
underperform.

\textbf{Memory and Energy Benefits}: Across all platforms, sparse
operations reduce memory bandwidth requirements and energy consumption
by accessing fewer elements\sidenote{\textbf{Sparse Energy Savings}:
90\% sparsity in BERT reduces training energy by 2.3× and inference
energy by 4.1× on V100. Structured 2:4 sparsity delivers 1.6× energy
savings on A100 while maintaining 99\% of dense accuracy
(\citeproc{ref-Cheng2022}{Cheng et al. 2022}). }. This benefit compounds
with quantization---a sparse INT8 model requires less memory traffic
than either technique alone (\citeproc{ref-Gale2020}{Baraglia and Konno
2019}).

\subsubsection{Challenges and
Limitations}\label{sec-model-compression-challenges-limitations-17c7}

While sparsity offers significant efficiency advantages, several
challenges limit its practical effectiveness.
Table~\ref{tbl-sparsity-optimization} summarizes these challenges.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.1640}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4480}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3840}}@{}}
\caption{\textbf{Sparsity Optimization Challenges}: Unstructured
sparsity, while reducing model size, hinders hardware acceleration due
to irregular memory access patterns, limiting potential computational
savings and requiring specialized hardware or software to realize
efficiency gains.}\label{tbl-sparsity-optimization}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Challenge}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Description}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Impact}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Challenge}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Description}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Impact}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Unstructured Sparsity Optimization} & Irregular sparse patterns
make it difficult to exploit sparsity on hardware. & Limited hardware
acceleration and reduced computational savings. \\
\textbf{Algorithmic Complexity} & Sophisticated pruning and sparse
matrix operations require complex algorithms. & High computational
overhead and algorithmic complexity for large models. \\
\textbf{Hardware Support} & Hardware accelerators are optimized for
structured sparsity, making unstructured sparsity harder to optimize. &
Suboptimal hardware utilization and lower performance for unstructured
sparsity. \\
\textbf{Accuracy Trade-off} & Aggressive sparsity may degrade model
accuracy if not carefully balanced. & Potential loss in performance,
requiring careful tuning and validation. \\
\textbf{Energy Efficiency} & Overhead from sparse matrix storage and
management can offset the energy savings from reduced computation. &
Power consumption may not improve if the overhead surpasses savings from
sparse computations. \\
\textbf{Limited Applicability} & Sparsity may not benefit all models or
tasks, especially in domains requiring dense representations. & Not all
models or hardware benefit equally from sparsity. \\
\end{longtable}

The central challenge is the gap between theoretical and practical
speedups. Unstructured pruning removes individual weights based on
importance, creating irregular patterns that hardware accelerators
struggle to exploit. Most GPUs and TPUs optimize for structured data;
without regular patterns, they cannot skip zero elements efficiently.
Pruning algorithms themselves introduce overhead, as determining which
weights to prune requires sophisticated importance estimation that can
be computationally expensive for large models. Even when sparsity is
achieved, sparse matrix storage formats add indexing overhead that can
offset computational savings.

The accuracy-efficiency trade-off requires careful calibration.
Aggressive sparsity can degrade accuracy beyond acceptable thresholds,
and the relationship is often non-linear---models may tolerate 70\%
sparsity with minimal impact but collapse at 80\%. Finding the optimal
operating point requires extensive experimentation.

Energy efficiency is not guaranteed. While sparse operations reduce
arithmetic operations, the overhead of sparse indexing and irregular
memory access can increase power consumption on hardware not optimized
for sparse patterns. On edge devices with tight power budgets, these
overheads may outweigh the benefits.

Finally, sparsity benefits vary by model type. Tasks requiring dense
representations (image segmentation, some reinforcement learning) may
not benefit from sparsity, and older hardware lacking sparse
acceleration may see no improvement or even regression.

\subsubsection{Combined
Optimizations}\label{sec-model-compression-combined-optimizations-6038}

While sparsity offers significant efficiency advantages on its own, it
achieves its full potential when combined with other optimization
techniques. These combinations introduce coordination challenges that
require careful management (\citeproc{ref-hoefler2021sparsity}{Hoefler,
Alistarh, Ben-Nun, Dryden, and Ziogas 2021}).

\textbf{Sparsity-Pruning Interaction}: Pruning creates sparsity, but the
\emph{pattern} determines hardware efficiency. Structured pruning
(entire filters or layers) produces regular sparsity that GPUs and TPUs
accelerate efficiently. Unstructured pruning creates irregular patterns
that may require specialized sparse matrix formats to realize speedups
(\citeproc{ref-elsen2020fast}{Elsen et al. 2020};
\citeproc{ref-gale2019state}{Gale, Elsen, and Hooker 2019b}).

\textbf{Sparsity-Quantization Interaction}: Combining sparsity with
low-precision arithmetic yields multiplicative compression but
introduces complexity. GPUs and TPUs with dedicated sparse tensor cores
accelerate this combination effectively, while general-purpose CPUs
often struggle with the combined overhead of sparse indexing and
dequantization (\citeproc{ref-nagel2021white}{Nagel et al. 2021b};
\citeproc{ref-zhang2021learning}{Zhang et al. 2021}).

\textbf{Hardware Alignment Imperative}: The recurring theme across all
combinations is hardware alignment. Efficient model designs (depthwise
separable convolutions, dynamic computation) amplify sparsity benefits
only when the target hardware supports the resulting operation patterns
(\citeproc{ref-dettmers2019sparse}{Dettmers and Zettlemoyer 2019}).
Selecting technique combinations requires understanding target platform
capabilities, as explored in \textbf{?@sec-ai-acceleration}.

The coordination challenges inherent in combining sparsity with other
techniques point to a broader principle: optimization techniques rarely
succeed in isolation, and their effectiveness depends heavily on
sequencing decisions and hardware alignment.

\phantomsection\label{quiz-question-sec-model-compression-architectural-efficiency-techniques-daa6}
\begin{fbx}{callout-quiz-question}{Self-Check: Question 1.6}{}
\phantomsection\label{quiz-question-sec-model-compression-architectural-efficiency-techniques-daa6}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Which of the following best describes the goal of architectural
  efficiency optimization in machine learning systems?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii})}
  \tightlist
  \item
    Aligning model operations with hardware capabilities
  \item
    Improving numerical precision of computations
  \item
    Increasing the number of model parameters
  \item
    Enhancing the theoretical accuracy of models
  \end{enumerate}
\item
  Explain how hardware-aware design principles can improve the
  deployment efficiency of machine learning models.
\item
  Which architectural efficiency technique involves reducing memory
  traffic by combining operations?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii})}
  \tightlist
  \item
    Dynamic computation strategies
  \item
    Sparsity exploitation techniques
  \item
    Operator fusion methods
  \item
    Hardware-aware design
  \end{enumerate}
\item
  In a production system, what trade-offs would you consider when
  implementing dynamic computation strategies?
\end{enumerate}

\noindent\hspace*{1.25em}\hyperref[quiz-answer-sec-model-compression-architectural-efficiency-techniques-daa6]{\textbf{See Answer~$\rightarrow$}}

\end{fbx}

\section{Technique Selection
Guide}\label{sec-model-compression-technique-selection-guide-ac37}

The preceding sections have systematically explored all three dimensions
of our optimization framework: structural methods (pruning,
distillation, factorization, NAS) determine \emph{what} computations
occur; precision techniques (quantization, mixed-precision) determine
\emph{how precisely} to compute; and architectural approaches (operator
fusion, dynamic computation, sparsity exploitation) determine \emph{how
efficiently} to execute on hardware. With this complete toolkit
established, practitioners need systematic guidance for selecting and
combining these techniques based on deployment constraints.

\subsection{Mapping Constraints to
Techniques}\label{sec-model-compression-mapping-constraints-techniques-fdbd}

Understanding how system constraints map to optimization dimensions
guides practitioners toward the most relevant approaches.
Table~\ref{tbl-constraint-opt-mapping} maps system constraints to
specific optimization dimensions, guiding technique selection based on
deployment requirements.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2522}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2348}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2261}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2696}}@{}}
\caption{\textbf{Optimization Dimensions}: System constraints drive
optimization along three core dimensions---model representation,
numerical precision, and architectural efficiency---each addressing
different resource limitations and performance
goals.}\label{tbl-constraint-opt-mapping}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{System Constraint}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Model Representation}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Numerical Precision}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Architectural Efficiency}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{System Constraint}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Model Representation}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Numerical Precision}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Architectural Efficiency}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Computational Cost} & ✗ & ✓ & ✓ \\
\textbf{Memory and Storage} & ✓ & ✓ & ✗ \\
\textbf{Latency and Throughput} & ✓ & ✗ & ✓ \\
\textbf{Energy Efficiency} & ✗ & ✓ & ✓ \\
\textbf{Scalability} & ✓ & ✗ & ✓ \\
\end{longtable}

Although each system constraint primarily aligns with one or more
optimization dimensions, the relationships are not strictly one-to-one.
Many optimization techniques affect multiple constraints simultaneously.
Structuring model optimization along these three dimensions allows
practitioners to analyze trade-offs more effectively and select
optimizations that best align with deployment requirements.

\subsection{Decision
Framework}\label{sec-model-compression-decision-framework-1896}

The following decision framework guides technique selection based on
primary deployment constraints:

\textbf{If your primary constraint is model size (storage/download):}

\begin{itemize}
\tightlist
\item
  \textbf{First choice}: INT8 quantization via PTQ (4× reduction,
  minimal accuracy loss)
\item
  \textbf{If more reduction needed}: INT4 quantization (8× reduction,
  1-3\% accuracy loss typical)
\item
  \textbf{If accuracy-critical}: Knowledge distillation to smaller
  architecture + quantization
\end{itemize}

\textbf{If your primary constraint is inference latency:}

\begin{itemize}
\tightlist
\item
  \textbf{First choice}: Structured pruning (removes operations, not
  just parameters)
\item
  \textbf{If hardware supports INT8}: Add quantization for faster
  arithmetic
\item
  \textbf{If latency-critical with accuracy flexibility}: Early-exit
  architectures
\end{itemize}

\textbf{If your primary constraint is memory bandwidth (LLM
generation):}

\begin{itemize}
\tightlist
\item
  \textbf{First choice}: Weight-only quantization (INT4/INT8 weights,
  FP16 activations)
\item
  \textbf{Key insight}: Memory bandwidth dominates; quantization
  provides linear speedup
\end{itemize}

\textbf{If your primary constraint is energy/power:}

\begin{itemize}
\tightlist
\item
  \textbf{First choice}: Quantization (reduces both compute and memory
  energy)
\item
  \textbf{Second choice}: Structured pruning (reduces operation count)
\item
  \textbf{Combined approach}: Pruning + quantization for multiplicative
  benefits
\end{itemize}

\textbf{If you have time for retraining:}

\begin{itemize}
\tightlist
\item
  Use QAT instead of PTQ for better accuracy at same precision
\item
  Consider knowledge distillation for maximum accuracy preservation
\item
  Explore NAS for hardware-specific architecture optimization
\end{itemize}

\textbf{If you need rapid deployment (no retraining):}

\begin{itemize}
\tightlist
\item
  PTQ with calibration dataset (hours, not days)
\item
  Magnitude-based pruning with fine-tuning (days)
\item
  Avoid: NAS, full QAT, architecture redesign
\end{itemize}

This decision framework provides starting points for individual
technique selection. However, production deployments rarely rely on a
single technique. Combining pruning with quantization, or distillation
with hardware-aware design, introduces interaction effects that can
either amplify benefits or create unexpected accuracy degradation. The
following section addresses how to sequence and combine techniques
effectively.

\section{Optimization
Strategies}\label{sec-model-compression-optimization-strategies-f2f6}

The preceding decision framework guides individual technique selection,
but the most significant optimization gains emerge from combining
multiple techniques across our three-dimensional framework. Model
representation techniques (pruning) reduce parameter count, numerical
precision techniques (quantization) reduce computational cost per
operation, and architectural efficiency techniques (operator fusion,
dynamic computation) reduce execution overhead. These techniques operate
at different optimization dimensions, providing multiplicative benefits
when sequenced appropriately.

Understanding why certain combinations work requires examining their
complementary characteristics. Pruning and quantization create
synergistic effects because pruning reduces parameter count while
quantization reduces precision, creating multiplicative compression
effects. Applying pruning first reduces the parameter set, making
subsequent quantization more effective and reducing the search space for
optimal quantization strategies. This sequential approach can achieve
compression ratios exceeding either technique alone.

Knowledge distillation integrates effectively with quantization by
mitigating accuracy loss from aggressive quantization. This approach
trains student models to match teacher behavior rather than just
minimizing task loss, proving effective for extreme quantization
scenarios where direct quantization would cause unacceptable accuracy
degradation. Neural architecture search enables co-design approaches
that optimize model structures specifically for quantization
constraints, identifying architectures that maintain accuracy under
low-precision operations. This co-design approach produces models
inherently suited for subsequent optimization, improving the
effectiveness of both quantization and pruning techniques.

Figure~\ref{fig-compression-methods} compares how different compression
strategies exhibit varying trade-offs between model size and accuracy
loss. Pruning combined with quantization (red circles) achieves high
compression ratios with minimal accuracy loss, while quantization alone
(yellow squares) provides a reasonable balance. In contrast, SVD (green
diamonds) requires a larger model size to maintain accuracy,
illustrating how different techniques impact compression effectiveness.

\begin{figure}[htb]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/829b0aef659418f6338134a551ad09f658e725ea.pdf}}

}

\caption{\label{fig-compression-methods}\textbf{Compression Trade-Offs}:
Combining pruning and quantization achieves superior compression ratios
with minimal accuracy loss compared to quantization or singular value
decomposition (SVD) alone, demonstrating the impact of different
numerical precision optimization techniques on model size and
performance. Architectural and numerical optimizations can complement
each other to efficiently deploy machine learning models via this
figure. Source: (\citeproc{ref-han2015deep}{Han, Mao, and Dally 2015}).}

\end{figure}%

Sequencing critically impacts results. Consider deploying BERT-Base on
mobile devices through three stages. Stage one applies structured
pruning, removing 30\% of attention heads and 40\% of intermediate FFN
dimensions, resulting in 75\% parameter reduction with accuracy dropping
from 76.2\% to 75.1\%. Stage two uses knowledge distillation to recover
accuracy to 75.9\%. Stage three applies quantization-aware training with
INT8 quantization, achieving 4x additional memory reduction with final
accuracy of 75.6\%. The combined impact shows 16x memory reduction
(440MB to 28MB), 12x inference speedup on mobile CPU, and 0.6\% final
accuracy loss versus 2.1\% if quantization had been applied before
pruning.

This example illustrates why sequencing matters: pruning first
concentrates important weights into smaller ranges, making subsequent
quantization more effective. Applying quantization before pruning
reduces numerical precision available for importance-based pruning
decisions, degrading final accuracy. Effective combination requires
understanding these dependencies and developing application sequences
that maximize cumulative benefits.

The challenge is clear: with dozens of techniques across three
optimization dimensions, each with multiple hyperparameters and
hardware-specific considerations, the configuration space explodes
combinatorially. Modern automated approaches address this complexity by
treating optimization pipeline design as a search problem.

\section{AutoML and Automated Optimization
Strategies}\label{sec-model-compression-automl-automated-optimization-strategies-cf08}

The preceding sections have equipped us with a toolkit of optimization
techniques: pruning to eliminate redundant parameters, quantization to
compress numerical precision, knowledge distillation to transfer
capabilities efficiently, and sparsity exploitation to skip unnecessary
computation. Each technique addresses specific efficiency challenges
across our three-dimensional framework, and combining them strategically
yields multiplicative benefits. However, as the BERT compression example
demonstrated, determining which techniques to apply, in what order, and
with what hyperparameters requires navigating an enormous configuration
space. The number of possible combinations grows exponentially with
technique count, while optimal configurations depend on target hardware,
deployment constraints, and accuracy requirements that vary across
applications.

Automated Machine Learning (AutoML) aims to streamline this process by
automating the search for optimal model configurations, building on the
training methodologies from \textbf{?@sec-ai-training}. AutoML
frameworks leverage machine learning algorithms to optimize
architectures, hyperparameters, model compression techniques, and other
important parameters, reducing the need for human intervention
(\citeproc{ref-Hutter2019}{Hutter, Kotthoff, and Vanschoren 2019}). By
systematically exploring the vast design space of possible models,
AutoML can improve efficiency while maintaining competitive accuracy,
often discovering novel solutions that may be overlooked through manual
tuning (\citeproc{ref-Zoph2017}{Zoph and Le 2017b}).

AutoML does not replace the need for human expertise but rather enhances
it by providing a structured and scalable approach to model
optimization. Instead of manually adjusting pruning thresholds,
quantization strategies, or architecture designs, practitioners can
define high-level objectives (latency constraints, memory limits,
accuracy targets) and allow AutoML systems to explore configurations
that best satisfy these constraints (\citeproc{ref-Feurer2015}{Feurer et
al. 2019}). As illustrated in Figure~\ref{fig-automl-comparison}, the
key difference between traditional workflows and AutoML is that
preprocessing, training, and evaluation are automated in the latter.

\begin{figure}[htb]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/1d30b4505094ba31670ba29069497371d25b3e44.pdf}}

}

\caption{\label{fig-automl-comparison}\textbf{AutoML Workflow}:
Automated machine learning (automl) streamlines model development by
structurally automating data preprocessing, model selection, and
hyperparameter tuning, contrasting with traditional workflows requiring
extensive manual effort for each stage. This automation enables
practitioners to define high-level objectives and constraints, allowing
automl systems to efficiently explore a vast design space and identify
optimal model configurations.}

\end{figure}%

AutoML unifies many optimization strategies by jointly optimizing
architectures, hyperparameters, and compression strategies rather than
treating them independently (\citeproc{ref-He2018}{Y. He et al. 2018}).

\subsection{AutoML
Capabilities}\label{sec-model-compression-automl-capabilities-3a7e}

AutoML systems address multiple optimization dimensions simultaneously:

\begin{itemize}
\tightlist
\item
  \textbf{Architecture search}: NAS automatically discovers efficient
  architectures like MobileNetV3 and EfficientNet that outperform manual
  designs (\citeproc{ref-Elsken2019}{Elsken, Metzen, and Hutter 2019b};
  \citeproc{ref-Tan2019}{Tan and Le 2019b})
\item
  \textbf{Hyperparameter optimization}: Bayesian
  optimization\sidenote{\textbf{Bayesian Optimization}: Achieves 10-50×
  sample efficiency vs.~random search using Gaussian processes to model
  objective uncertainty. Optuna and Ray Tune provide production-ready
  implementations. } efficiently searches learning rate, batch size, and
  weight decay settings (\citeproc{ref-Bergstra2011}{Bardenet et al.
  2015})
\item
  \textbf{Compression selection}: Automated selection of pruning
  thresholds, sparsity patterns, and quantization levels based on
  deployment constraints (\citeproc{ref-Wu2016}{Jiaxiang Wu et al.
  2016})
\item
  \textbf{Hardware-aware optimization}: Models tailored to specific
  devices by adjusting computational workloads and memory access
  patterns (\citeproc{ref-Cai2020}{Cai, Gan, and Han 2020})
\end{itemize}

Modern platforms (Google AutoML, Amazon SageMaker Autopilot, Microsoft
Azure AutoML) integrate these capabilities into end-to-end pipelines
(\citeproc{ref-Li2021}{Li et al. 2017}).

\subsection{AutoML
Challenges}\label{sec-model-compression-automl-challenges-3334}

AutoML is not a universal solution. Key challenges include:

\begin{itemize}
\tightlist
\item
  \textbf{Computational cost}: NAS can require thousands of GPU hours.
  Weight sharing and surrogate models reduce but don't eliminate this
  overhead.
\item
  \textbf{Search bias}: Predefined objectives and search spaces can miss
  configurations that domain experts would consider.
\item
  \textbf{Generalization}: Models optimized for specific datasets may
  degrade on new tasks or environments.
\item
  \textbf{Interpretability}: Understanding \emph{why} an
  AutoML-discovered architecture works is often difficult.
\item
  \textbf{Control trade-off}: Automation abstracts decisions that
  experts might fine-tune for domain-specific constraints.
\end{itemize}

Despite these limitations, AutoML continues to improve and plays an
increasingly important role in model optimization. Whether applying
optimization techniques manually or through AutoML, rigorous measurement
is essential for validating that optimizations achieve their intended
goals. The following section provides the methodological foundation for
profiling systems and evaluating optimization effectiveness.

\phantomsection\label{quiz-question-sec-model-compression-automl-automated-optimization-strategies-cf08}
\begin{fbx}{callout-quiz-question}{Self-Check: Question 1.7}{}
\phantomsection\label{quiz-question-sec-model-compression-automl-automated-optimization-strategies-cf08}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Which of the following best describes the role of AutoML in machine
  learning model optimization?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii})}
  \tightlist
  \item
    It completely replaces the need for human expertise in model design.
  \item
    It automates the search for optimal model configurations, reducing
    manual effort.
  \item
    It focuses solely on improving model accuracy without considering
    efficiency.
  \item
    It is primarily used for data collection and preprocessing.
  \end{enumerate}
\item
  Explain how AutoML frameworks balance accuracy and efficiency in model
  optimization.
\item
  True or False: AutoML can fully eliminate the need for domain
  expertise in model optimization.
\item
  The process of automatically selecting pruning thresholds and
  quantization levels in AutoML is known as \_\_\_\_.
\item
  In a production system, what trade-offs might you consider when
  implementing AutoML for model optimization?
\end{enumerate}

\noindent\hspace*{1.25em}\hyperref[quiz-answer-sec-model-compression-automl-automated-optimization-strategies-cf08]{\textbf{See Answer~$\rightarrow$}}

\end{fbx}

\section{Measuring Model
Efficiency}\label{sec-model-compression-measuring-model-efficiency-00c7}

Translating optimization strategies into measurable improvements
requires systematic evaluation approaches. Production systems employ
coordinated optimization strategies that balance multiple constraints
simultaneously, and effective deployment requires structured approaches
for profiling systems, measuring optimization impact, and validating
that technique combinations achieve deployment goals.

This section addresses three critical questions: Where should
optimization efforts focus? How do we measure whether optimizations
achieve their intended goals? How do we validate that combined
techniques deliver expected benefits?

\subsection{Profiling and Opportunity
Analysis}\label{sec-model-compression-profiling-opportunity-analysis-ad25}

Optimization begins with profiling to identify where computational
resources are being consumed and which components offer the greatest
optimization potential. A critical first step is determining whether
model optimization will actually improve system performance, as model
computation often represents only a fraction of total system overhead in
production environments.

Modern machine learning models exhibit heterogeneous resource
consumption patterns, where specific layers, operations, or data paths
contribute disproportionately to memory usage, computational cost, or
latency. Understanding these patterns is important for prioritizing
optimization efforts and achieving maximum impact with minimal accuracy
degradation.

Effective profiling begins with establishing baseline measurements
across all relevant performance dimensions. Memory profiling reveals
both static memory consumption (model parameters and buffers) and
dynamic memory allocation patterns during training and inference.
Computational profiling identifies bottleneck operations, typically
measured in FLOPS and actual wall-clock execution time. Energy profiling
becomes important for battery-powered and edge deployment scenarios,
where power consumption directly impacts operational feasibility.
Latency profiling measures end-to-end response times and identifies
which operations contribute most to inference delay.

Consider profiling a Vision Transformer (ViT) for edge deployment. Using
PyTorch Profiler reveals attention layers consuming 65\% of total FLOPs
(highly amenable to structured pruning), layer normalization consuming
8\% of latency despite only 2\% of FLOPs (memory-bound operation), and
the final classification head consuming 1\% of computation but 15\% of
parameter memory. This profile suggests applying magnitude-based pruning
to attention layers as priority one (high FLOP reduction potential),
quantizing the classification head to INT8 as priority two (large memory
savings, minimal accuracy impact), and fusing layer normalization
operations as priority three (reduces memory bandwidth bottleneck).

Extending beyond these baseline measurements, modern optimization
requires understanding model sensitivity to different types of
modifications. Not all parameters contribute equally to model accuracy,
and structured sensitivity analysis helps identify which components can
be optimized aggressively versus those that require careful
preservation. Layer-wise sensitivity analysis reveals which network
components are most important for maintaining accuracy, guiding
decisions about where to apply aggressive pruning or quantization versus
where to use conservative approaches.

\subsection{Measuring Optimization
Effectiveness}\label{sec-model-compression-measuring-optimization-effectiveness-71a9}

Optimization requires rigorous measurement frameworks that go beyond
simple accuracy metrics to capture the full impact of optimization
decisions. Effective measurement considers multiple objectives
simultaneously, including accuracy preservation, computational
efficiency gains, memory reduction, latency improvement, and energy
savings. Balancing these often-competing objectives while maintaining
structured decision-making processes requires careful tradeoff analysis.

The measurement framework should establish clear baselines before
applying any optimizations, capturing thorough performance profiles
across all relevant metrics. Accuracy baselines include not only
top-line metrics like classification accuracy but also more nuanced
measures such as calibration, fairness across demographic groups, and
robustness to input variations. Efficiency baselines capture
computational cost (FLOPS, memory bandwidth), actual execution time
across different hardware platforms, peak memory consumption during
training and inference, and energy consumption profiles.

When quantizing ResNet-50 from FP32 to INT8, baseline metrics show Top-1
accuracy of 76.1\%, inference latency on V100 of 4.2ms, model size of
98MB, and energy per inference of 0.31J. Post-quantization metrics
reveal Top-1 accuracy of 75.8\% (0.3\% degradation), inference latency
of 1.3ms (3.2x speedup), model size of 25MB (3.9x reduction), and energy
per inference of 0.08J (3.9x improvement). Additional analysis shows
per-class accuracy degradation ranging from 0.1\% to 1.2\% with highest
impact on fine-grained categories, calibration error increasing from
2.1\% to 3.4\%, and INT8 quantization providing 3.2x speedup on GPU but
only 1.8x on CPU, demonstrating hardware-dependent gains.

With these comprehensive baselines in place, the measurement framework
must track optimization impact systematically. Rather than evaluating
techniques in isolation, applying our three-dimensional framework
requires understanding how different approaches interact when combined.
Sequential application can lead to compounding benefits or unexpected
interactions that diminish overall effectiveness.
\textbf{?@sec-benchmarking-ai} provides additional structured evaluation
methods for comprehensive performance assessment.

\phantomsection\label{quiz-question-sec-model-compression-measuring-model-efficiency-00c7}
\begin{fbx}{callout-quiz-question}{Self-Check: Question 1.8}{}
\phantomsection\label{quiz-question-sec-model-compression-measuring-model-efficiency-00c7}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Which of the following is a critical first step in the optimization
  process for machine learning systems?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii})}
  \tightlist
  \item
    Applying quantization to reduce model size
  \item
    Conducting sensitivity analysis on model parameters
  \item
    Implementing structured pruning techniques
  \item
    Establishing baseline measurements through profiling
  \end{enumerate}
\item
  True or False: In a production environment, model computation often
  represents the majority of system overhead.
\item
  Explain how a systematic measurement framework can help balance
  competing optimization objectives in ML systems.
\item
  Order the following stages of deploying BERT-Base on mobile devices:
  (1) Quantization-aware training, (2) Structured pruning, (3) Knowledge
  distillation.
\item
  In a production system, what trade-offs would you consider when
  integrating multiple optimization techniques?
\end{enumerate}

\noindent\hspace*{1.25em}\hyperref[quiz-answer-sec-model-compression-measuring-model-efficiency-00c7]{\textbf{See Answer~$\rightarrow$}}

\end{fbx}

\section{Implementation Tools and Software
Frameworks}\label{sec-model-compression-implementation-tools-software-frameworks-e939}

The theoretical understanding of model optimization techniques like
pruning, quantization, and efficient numerics is important, but their
practical implementation relies heavily on robust software support.
Without extensive framework development and tooling, these optimization
methods would remain largely inaccessible to practitioners. Implementing
quantization would require manual modification of model definitions and
careful insertion of quantization operations throughout the network.
Pruning would involve direct manipulation of weight tensors, tasks that
become prohibitively complex as models scale.

Modern machine learning frameworks provide high-level APIs and automated
workflows that abstract away implementation complexity, making
sophisticated optimization techniques accessible to practitioners.
Frameworks address key challenges: providing pre-built modules for
common optimization techniques, assisting with hyperparameter tuning
(pruning schedules, quantization bit-widths), managing
accuracy-compression trade-offs through automated evaluation, and
ensuring hardware compatibility through device-specific code generation.

This software infrastructure transforms theoretical optimization
techniques into practical tools readily applied in production
environments. \textbf{?@sec-machine-learning-operations-mlops} details
the operational considerations for these workflows, including model
versioning strategies, monitoring optimization impact on data pipelines,
managing optimization artifacts across development and deployment
environments, and establishing rollback procedures when optimizations
fail. This accessibility bridges the gap between academic research and
industrial applications, enabling widespread deployment of efficient
machine learning models.

\subsection{Model Optimization APIs and
Tools}\label{sec-model-compression-model-optimization-apis-tools-9693}

Leading frameworks such as TensorFlow, PyTorch, and MXNet provide
comprehensive APIs enabling practitioners to apply optimization
techniques without implementing complex algorithms from scratch.
\textbf{?@sec-ai-frameworks} examines these frameworks in depth.
Built-in optimizations enhance model efficiency while ensuring adherence
to established best practices.

TensorFlow's Model Optimization Toolkit facilitates quantization,
pruning, and clustering. QAT converts floating-point models to
lower-precision formats (INT8) while preserving accuracy, systematically
managing both weight and activation quantization across diverse
architectures. Pruning algorithms introduce sparsity by removing
redundant connections at varying granularity levels---individual weights
to entire layers---allowing practitioners to tailor strategies to
specific requirements. Weight clustering groups similar weights for
compression while preserving functionality, providing multiple pathways
for improving model efficiency.

Similarly, PyTorch offers optimization support through built-in modules
for quantization and pruning. The \texttt{torch.quantization} package
provides tools for converting models to lower-precision representations,
supporting both post-training quantization and quantization-aware
training. Listing~\ref{lst-qat_example} demonstrates PyTorch's
quantization-aware training API:

\begin{codelisting}

\caption{\label{lst-qat_example}\textbf{Quantization-Aware Training}:
Prepares a model to be trained in lower-precision formats, ensuring that
quantization errors are accounted for during training.}

\centering{

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ torch}
\ImportTok{from}\NormalTok{ torch.quantization }\ImportTok{import}\NormalTok{ QuantStub, DeQuantStub,}
\NormalTok{     prepare\_qat}

\CommentTok{\# Define a model with quantization support}
\KeywordTok{class}\NormalTok{ QuantizedModel(torch.nn.Module):}
    \KeywordTok{def} \FunctionTok{\_\_init\_\_}\NormalTok{(}\VariableTok{self}\NormalTok{):}
        \BuiltInTok{super}\NormalTok{().}\FunctionTok{\_\_init\_\_}\NormalTok{()}
        \VariableTok{self}\NormalTok{.quant }\OperatorTok{=}\NormalTok{ QuantStub()}
        \VariableTok{self}\NormalTok{.conv }\OperatorTok{=}\NormalTok{ torch.nn.Conv2d(}\DecValTok{3}\NormalTok{, }\DecValTok{64}\NormalTok{, }\DecValTok{3}\NormalTok{)}
        \VariableTok{self}\NormalTok{.dequant }\OperatorTok{=}\NormalTok{ DeQuantStub()}

    \KeywordTok{def}\NormalTok{ forward(}\VariableTok{self}\NormalTok{, x):}
\NormalTok{        x }\OperatorTok{=} \VariableTok{self}\NormalTok{.quant(x)}
\NormalTok{        x }\OperatorTok{=} \VariableTok{self}\NormalTok{.conv(x)}
        \ControlFlowTok{return} \VariableTok{self}\NormalTok{.dequant(x)}

\CommentTok{\# Prepare model for quantization{-}aware training}
\NormalTok{model }\OperatorTok{=}\NormalTok{ QuantizedModel()}
\NormalTok{model.qconfig }\OperatorTok{=}\NormalTok{ torch.quantization.get\_default\_qat\_qconfig()}
\NormalTok{model\_prepared }\OperatorTok{=}\NormalTok{ prepare\_qat(model)}
\end{Highlighting}
\end{Shaded}

}

\end{codelisting}%

For pruning, PyTorch provides the \texttt{torch.nn.utils.prune} module,
which supports both unstructured and structured pruning.
Listing~\ref{lst-pytorch_pruning} illustrates both pruning approaches:

\begin{codelisting}

\caption{\label{lst-pytorch_pruning}\textbf{PyTorch Pruning APIs}:
Applies unstructured and structured pruning techniques to reduce model
complexity while maintaining performance. \emph{Source: PyTorch
Documentation}}

\centering{

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ torch.nn.utils.prune }\ImportTok{as}\NormalTok{ prune}

\CommentTok{\# Apply unstructured pruning}
\NormalTok{module }\OperatorTok{=}\NormalTok{ torch.nn.Linear(}\DecValTok{10}\NormalTok{, }\DecValTok{10}\NormalTok{)}
\NormalTok{prune.l1\_unstructured(module, name}\OperatorTok{=}\StringTok{"weight"}\NormalTok{, amount}\OperatorTok{=}\FloatTok{0.3}\NormalTok{)}
\CommentTok{\# Prune 30\% of weights}

\CommentTok{\# Apply structured pruning}
\NormalTok{prune.ln\_structured(module, name}\OperatorTok{=}\StringTok{"weight"}\NormalTok{, amount}\OperatorTok{=}\FloatTok{0.5}\NormalTok{, n}\OperatorTok{=}\DecValTok{2}\NormalTok{, dim}\OperatorTok{=}\DecValTok{0}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

}

\end{codelisting}%

These tools integrate seamlessly into PyTorch's training pipelines,
enabling efficient experimentation with different optimization
strategies.

Built-in optimization APIs offer significant benefits that make model
optimization more accessible and reliable. By providing pre-tested,
production-ready tools with standardized interfaces, these APIs
dramatically reduce implementation complexity while ensuring consistent,
reproducible results across different model architectures and teams.

These frameworks also bridge the gap between cutting-edge research and
practical applications. As new optimization techniques emerge from the
research community, framework maintainers incorporate these advances
into their APIs, making state-of-the-art methods readily available to
practitioners. The comprehensive nature of these APIs enables rapid
experimentation: developers can easily test various strategies, compare
their effectiveness, and iterate quickly to find the optimal
configuration for their specific use case.

\subsection{Hardware-Specific Optimization
Libraries}\label{sec-model-compression-hardwarespecific-optimization-libraries-2653}

The optimization techniques in this chapter produce models ready for
hardware-specific acceleration. Libraries like TensorRT,
XLA\sidenote{\textbf{XLA (Accelerated Linear Algebra)}: Google's ML
compiler that optimizes computational graphs for efficient execution on
TPUs and GPUs, examined in \textbf{?@sec-ai-acceleration}. }, OpenVINO,
and TVM\sidenote{\textbf{TVM (Tensor Virtual Machine)}: Apache's
cross-platform ML compiler enabling deployment across diverse hardware
from a single model definition, as discussed in
\textbf{?@sec-ai-acceleration}. } translate these optimized models into
efficient execution on target platforms. \textbf{?@sec-ai-acceleration}
examines the hardware acceleration principles underlying how these tools
exploit accelerator capabilities for pruned, quantized, and
architecturally optimized models.

Framework integration enables practitioners to apply optimizations
without implementing hardware-specific code directly. For model
representation optimizations like pruning, these libraries provide
sparsity-aware acceleration through optimized kernels. For numerical
precision optimization, they offer extensive support for both PTQ and
QAT, implementing INT8 and INT4 quantization during model conversion.
Architectural efficiency techniques integrate through operator-level
tuning, including aggressive fusion and kernel reordering.

This integration of hardware optimization libraries with machine
learning frameworks enables developers to effectively implement the
optimization techniques covered in this chapter while ensuring optimal
adaptation to target hardware. \textbf{?@sec-benchmarking-ai} and
\textbf{?@sec-machine-learning-operations-mlops} detail deployment
strategies that build on these optimization foundations.

\subsection{Optimization Visualization
Tools}\label{sec-model-compression-optimization-visualization-tools-835d}

Beyond APIs and hardware-specific libraries, visualization tools help
practitioners understand how pruning, quantization, and other
optimizations affect model behavior.

\textbf{Quantization visualization}: Error histograms reveal whether
quantization errors are Gaussian or contain problematic outliers.
Activation visualizations help detect overflow and saturation issues.
Figure~\ref{fig-color-mapping} shows color-mapped AlexNet kernels.
TensorFlow's Quantization Debugger, PyTorch's FX Graph Mode, and
TensorRT Inspector provide these capabilities.

\begin{figure}

\centering{

\includegraphics[width=0.7\linewidth,height=\textheight,keepaspectratio]{contents/vol1/optimizations/images/jpeg/color-mapping_9f68f817.jpeg}

}

\caption{\label{fig-color-mapping}\textbf{Convolutional Kernel Weights}:
Color mapping reveals learned feature patterns in convolutional filters.
Analyzing weight distributions helps diagnose issues like dead or
saturated filters. Source: (\citeproc{ref-alexnet2012}{Krizhevsky,
Sutskever, and Hinton 2017}).}

\end{figure}%

\textbf{Sparsity visualization}: Heat maps show sparsity distribution
across layers (Figure~\ref{fig-sprase-heat-map}). Darker regions
indicate higher sparsity. Trend plots track sparsity progression across
pruning iterations. TensorBoard, Netron, and SparseML provide these
tools.

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{contents/vol1/optimizations/images/png/sprase-heat-map_a50fe5a9.png}}

}

\caption{\label{fig-sprase-heat-map}\textbf{Sparsity Distribution}:
Darker shades indicate higher sparsity where more weights were removed.
Source: Numenta (\citeproc{ref-numenta_sparsity}{Numenta 2024})}

\end{figure}%

\phantomsection\label{quiz-question-sec-model-compression-implementation-tools-software-frameworks-e939}
\begin{fbx}{callout-quiz-question}{Self-Check: Question 1.9}{}
\phantomsection\label{quiz-question-sec-model-compression-implementation-tools-software-frameworks-e939}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Which of the following best describes the role of modern machine
  learning frameworks in model optimization?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii})}
  \tightlist
  \item
    They provide theoretical insights into optimization techniques.
  \item
    They replace the need for model optimization entirely.
  \item
    They automate the implementation of complex optimization algorithms.
  \item
    They focus solely on hardware-specific optimizations.
  \end{enumerate}
\item
  Explain how built-in optimization APIs in frameworks like TensorFlow
  and PyTorch enhance model efficiency.
\item
  True or False: Visualization tools are unnecessary in understanding
  the impact of model optimization techniques like pruning and
  quantization.
\item
  In machine learning frameworks, the process of reducing numerical
  precision to improve computational efficiency is known as \_\_\_\_.
  This process involves converting models to lower-precision formats
  while maintaining accuracy.
\item
  In a production system, what trade-offs might you consider when
  choosing between different optimization techniques provided by
  software frameworks?
\end{enumerate}

\noindent\hspace*{1.25em}\hyperref[quiz-answer-sec-model-compression-implementation-tools-software-frameworks-e939]{\textbf{See Answer~$\rightarrow$}}

\end{fbx}

\section{Technique
Comparison}\label{sec-model-compression-technique-comparison-3142}

Having explored the three major optimization approaches in depth, along
with frameworks and tools for implementation, a comparative analysis
reveals how different techniques address distinct aspects of the
efficiency-accuracy trade-off. This comparison guides technique
selection based on deployment constraints and available resources.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 10\tabcolsep) * \real{0.1377}}
  >{\raggedright\arraybackslash}p{(\linewidth - 10\tabcolsep) * \real{0.1594}}
  >{\raggedright\arraybackslash}p{(\linewidth - 10\tabcolsep) * \real{0.1594}}
  >{\raggedright\arraybackslash}p{(\linewidth - 10\tabcolsep) * \real{0.1449}}
  >{\raggedright\arraybackslash}p{(\linewidth - 10\tabcolsep) * \real{0.1884}}
  >{\raggedright\arraybackslash}p{(\linewidth - 10\tabcolsep) * \real{0.1812}}@{}}
\caption{\textbf{Optimization Technique Trade-offs}: Comparison of the
three major optimization approaches across key performance dimensions,
highlighting how each technique addresses different constraints and
deployment scenarios. Pruning excels for computational reduction but
requires sparse hardware support, quantization provides balanced size
and speed improvements with wide hardware compatibility, while
distillation produces high-quality compressed models at higher training
cost.}\label{tbl-optimization-comparison}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Technique}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Primary Goal}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Accuracy Impact}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Training Cost}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Hardware Dependency}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Best For}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Technique}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Primary Goal}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Accuracy Impact}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Training Cost}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Hardware Dependency}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Best For}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Pruning} & Reduce FLOPs/Size & Moderate & Low (fine-tuning) &
High (for sparse ops) & Latency-critical apps \\
\textbf{Quantization} & Reduce Size/Latency & Low & Low (PTQ) / High
(QAT) & High (INT8 support) & Edge/Mobile deployment \\
\textbf{Distillation} & Reduce Size & Low-Moderate & High (retraining) &
Low & Creating smaller, high-quality models \\
\end{longtable}

Table~\ref{tbl-optimization-comparison} enables systematic technique
selection based on these trade-offs. Pruning works best when sparse
computation hardware is available and when reducing floating-point
operations is critical. Quantization provides the most versatile
approach with broad hardware support, making it ideal for diverse
deployment scenarios. Knowledge distillation requires significant
computational investment but produces consistently high-quality
compressed models, making it valuable when accuracy preservation is
paramount.

These techniques combine synergistically, with quantization often
applied after pruning or distillation to achieve compound compression
benefits. Production systems frequently employ sequential application:
initial pruning reduces parameter count, quantization optimizes
numerical representation, and fine-tuning through distillation
principles recovers any accuracy loss. Sequential application enables
compression ratios of 10-50x while maintaining competitive accuracy
across diverse deployment scenarios.

\phantomsection\label{quiz-question-sec-model-compression-technique-comparison-3142}
\begin{fbx}{callout-quiz-question}{Self-Check: Question 1.10}{}
\phantomsection\label{quiz-question-sec-model-compression-technique-comparison-3142}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Which optimization technique is best suited for latency-critical
  applications where sparse computation hardware is available?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii})}
  \tightlist
  \item
    Pruning
  \item
    Quantization
  \item
    Distillation
  \item
    All of the above
  \end{enumerate}
\item
  True or False: Quantization is the most versatile optimization
  technique due to its broad hardware support.
\item
  Explain why knowledge distillation might be preferred when accuracy
  preservation is paramount in a production system.
\item
  Order the following optimization techniques based on their typical
  application sequence in a production system: (1) Pruning, (2)
  Quantization, (3) Distillation.
\end{enumerate}

\noindent\hspace*{1.25em}\hyperref[quiz-answer-sec-model-compression-technique-comparison-3142]{\textbf{See Answer~$\rightarrow$}}

\end{fbx}

\section{Fallacies and
Pitfalls}\label{sec-model-compression-fallacies-pitfalls-1b5e}

Model optimization involves counterintuitive interactions between
techniques that appear independent. Engineers frequently assume
optimization strategies compose linearly, that theoretical metrics
predict deployment performance, or that aggressive compression maintains
accuracy. These fallacies and pitfalls capture errors that waste
optimization effort, degrade production accuracy, or fail to achieve
deployment requirements despite substantial engineering investment.

\textbf{Fallacy:} \emph{Optimization techniques can be applied
independently without considering their interactions.}

Engineers assume optimization strategies compose additively: 50\%
pruning plus 4× quantization yields combined benefits. In reality,
techniques interact non-linearly and compound losses. A BERT model
pruned to 70\% parameters maintains 97.8\% performance, but applying
INT8 quantization afterward drops accuracy to 94.2\%, while
quantization-aware training on the pruned model achieves 96.5\%.
Knowledge distillation from heavily pruned teachers transfers degenerate
attention patterns that reduce student accuracy by an additional 3-5\%
compared to distilling from dense models. As demonstrated in
Section~\ref{sec-model-compression-optimization-strategies-f2f6},
successful optimization requires coordinated application where
techniques are sequenced and calibrated together rather than stacked
independently. Organizations that apply aggressive combinations without
measuring interactions waste weeks iterating to recover lost accuracy.

\textbf{Pitfall:} \emph{Optimizing for theoretical metrics rather than
actual deployment performance.}

Teams reduce FLOPs by 60\% and celebrate efficiency gains without
profiling deployment hardware. A pruned model with 40\% fewer parameters
shows irregular sparsity patterns that prevent vectorization, achieving
only 12\% latency reduction instead of the expected 40\% on ARM
processors. INT8 quantization reduces a transformer model from 440MB to
110MB, but dequantization overhead on GPUs without INT8 tensor cores
increases inference latency by 15\% despite the 4× size reduction. As
shown in
Section~\ref{sec-model-compression-profiling-opportunity-analysis-ad25},
memory bandwidth, cache behavior, and instruction-level parallelism
determine actual performance, not just operation counts. Production
deployments require measuring wall-clock latency, throughput, and energy
consumption on target hardware rather than trusting theoretical
complexity metrics.

\textbf{Fallacy:} \emph{Aggressive quantization maintains model
performance with minimal accuracy loss.}

Engineers assume quantization scales uniformly: if INT8 loses 1\%, then
INT4 loses 2\%. In practice, precision reduction exhibits threshold
effects where models collapse catastrophically. ResNet-50 quantized to
INT8 maintains 76.1\% vs 76.15\% FP32 accuracy, but naive INT4
quantization can drop accuracy significantly (the exact degradation
depends on the quantization method used, with modern techniques like
GPTQ and AWQ recovering much of this loss). Binary weights achieve only
approximately 51\% on ImageNet. BERT with INT8 weights retains 99.1\% of
FP32 GLUE performance, but INT4 attention mechanisms cause numerical
instability that reduces scores by 8-12\%. LayerNorm and Softmax
operations require FP16 minimum precision; quantizing them to INT8
causes training divergence. The quantization strategies in
Section~\ref{sec-model-compression-precision-reduction-strategies-9cbc}
demonstrate that mixed-precision approaches maintain accuracy where
uniform quantization fails, requiring per-layer analysis rather than
blanket bit-width reduction.

\textbf{Pitfall:} \emph{Using post-training optimization without
considering training-aware alternatives.}

Teams apply post-training quantization to avoid retraining and achieve
96.8\% of baseline BERT performance. However, quantization-aware
training (QAT) on the same model retains 99.1\% of performance,
recovering the 3.2-point gap through learned quantization parameters.
Post-training pruning of ResNet-50 to 70\% parameters drops ImageNet
accuracy to 73.8\%, while gradual magnitude pruning during training
maintains 75.6\% accuracy at the same sparsity. Knowledge distillation
integrated during student training achieves 97\% of teacher performance,
compared to 92-94\% when distilling from a frozen teacher post-hoc. As
detailed in
Section~\ref{sec-model-compression-precision-reduction-strategies-9cbc},
the 2-4\% accuracy improvements from training-aware methods often
determine whether optimized models meet production thresholds,
justifying the additional training cost.

\textbf{Pitfall:} \emph{Focusing on individual model optimization
without considering system-level performance bottlenecks.}

Teams reduce model inference from 45ms to 15ms through aggressive
quantization and celebrate 67\% latency reduction. However, profiling
reveals data preprocessing consumes 120ms, I/O operations add 35ms, and
network overhead contributes 28ms, making total latency 198ms vs 228ms
originally, only 13\% improvement. A recommendation system optimizes its
neural network to 8ms inference but remains memory-bandwidth bound,
achieving only 15\% throughput gain despite 60\% computational
reduction. As demonstrated in
Section~\ref{sec-model-compression-profiling-opportunity-analysis-ad25}
and \textbf{?@sec-benchmarking-ai}, Amdahl's Law bounds speedup: if
model inference represents only 20\% of end-to-end latency, perfect
model optimization yields at most 1.25× overall improvement. Production
systems require whole-system profiling to identify whether model
complexity, data pipelines, or infrastructure overhead dominates,
ensuring optimization effort targets the actual bottleneck.

\phantomsection\label{quiz-question-sec-model-compression-fallacies-pitfalls-1b5e}
\begin{fbx}{callout-quiz-question}{Self-Check: Question 1.11}{}
\phantomsection\label{quiz-question-sec-model-compression-fallacies-pitfalls-1b5e}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  True or False: Optimization techniques like pruning and quantization
  can be applied independently without considering their interactions.
\item
  Which of the following is a potential pitfall when optimizing machine
  learning models?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii})}
  \tightlist
  \item
    Profiling the entire system for bottlenecks
  \item
    Measuring actual deployment performance
  \item
    Applying optimization-aware training
  \item
    Focusing solely on reducing parameter counts
  \end{enumerate}
\item
  Explain why aggressive quantization might not always maintain model
  performance.
\item
  What is a disadvantage of using post-training optimization techniques?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii})}
  \tightlist
  \item
    They require modification of existing training pipelines
  \item
    They are more complex to implement than training-aware techniques
  \item
    They typically achieve inferior results compared to training-aware
    approaches
  \item
    They always result in higher inference latency
  \end{enumerate}
\item
  In a production system, why is it important to consider system-level
  performance bottlenecks when optimizing models?
\end{enumerate}

\noindent\hspace*{1.25em}\hyperref[quiz-answer-sec-model-compression-fallacies-pitfalls-1b5e]{\textbf{See Answer~$\rightarrow$}}

\end{fbx}

\section{Summary}\label{sec-model-compression-summary-8229}

Model optimization bridges the gap between research advances and
deployment realities. The core insight is that optimization operates
across three complementary dimensions: \emph{structural} (what to
compute), \emph{precision} (how precisely), and \emph{architectural}
(how efficiently). The best results combine techniques across all three
dimensions---BERT's 16× compression (440MB → 28MB) exemplifies this
through pruning + distillation + quantization.

Combined with the data efficiency techniques from
\textbf{?@sec-data-efficiency}, these model-centric optimizations
complete the efficiency toolkit for machine learning practitioners: data
efficiency maximizes learning from available examples, while model
compression minimizes resources required for deployment. The journey
from research prototype to deployment-ready model requires systematic
application of these techniques, matched to specific deployment
constraints and hardware capabilities.

The emergence of AutoML frameworks represents a paradigm shift toward
automated discovery of optimization strategies. These systems enable
practitioners to explore vast optimization spaces more systematically
than manual approaches, often uncovering novel technique combinations
that achieve superior efficiency-accuracy trade-offs. However, AutoML is
not a universal solution---computational cost, search bias, and
interpretability challenges limit its applicability. Whether optimizing
manually or through automation, rigorous profiling and measurement
remain essential for validating that optimizations achieve their
intended goals on target hardware.

\begin{tcolorbox}[enhanced jigsaw, coltitle=black, breakable, colback=white, bottomtitle=1mm, colframe=quarto-callout-important-color-frame, opacitybacktitle=0.6, bottomrule=.15mm, left=2mm, opacityback=0, arc=.35mm, toptitle=1mm, colbacktitle=quarto-callout-important-color!10!white, titlerule=0mm, title=\textcolor{quarto-callout-important-color}{\faExclamation}\hspace{0.5em}{Key Takeaways}, toprule=.15mm, rightrule=.15mm, leftrule=.75mm]

\begin{itemize}
\item
  \textbf{Three dimensions of optimization}: Structural (what to
  compute), precision (how precisely), architectural (how efficiently).
  Combine all three for maximum compression.
\item
  \textbf{Start with PTQ quantization}: INT8 post-training quantization
  requires no retraining and delivers 4× compression. Consider QAT or
  distillation only if PTQ accuracy is insufficient for the application.
\item
  \textbf{For LLMs, weight-only quantization wins}: INT4 weights with
  FP16 activations dominates because generation is memory-bandwidth
  bound, not compute bound.
\item
  \textbf{Structured pruning for commodity hardware}: Unstructured
  sparsity requires specialized accelerators. Structured pruning
  (channels, heads) delivers real latency gains on GPUs.
\item
  \textbf{Order matters when combining techniques}: Pruning before
  quantization is more effective; architecture changes should align with
  quantization constraints. Distillation can mitigate quantization
  accuracy loss.
\item
  \textbf{Profile on target hardware---paper metrics mislead}: FLOPs and
  parameter count often mispredict real-world performance. A 2× FLOP
  reduction may yield only 1.2× speedup.
\end{itemize}

\end{tcolorbox}

Model optimization transforms computational requirements, but realizing
these gains requires hardware that can exploit the optimizations. We
have compressed the model's logic, shaving off every unnecessary bit.
But logic must eventually run on physics. \textbf{We have optimized the
math; now we must optimize the silicon.} We turn next to the engine
itself: \textbf{Hardware Acceleration} (\textbf{?@sec-ai-acceleration}),
where we explore how GPUs, TPUs, and NPUs execute these compressed
models at the speed of light.

\phantomsection\label{quiz-question-sec-model-compression-summary-8229}
\begin{fbx}{callout-quiz-question}{Self-Check: Question 1.12}{}
\phantomsection\label{quiz-question-sec-model-compression-summary-8229}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Which of the following best illustrates the trade-off between model
  accuracy and resource efficiency in optimization?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii})}
  \tightlist
  \item
    Increasing model size to improve accuracy
  \item
    Applying structural pruning to reduce model size
  \item
    Using more complex algorithms without considering deployment
  \item
    Focusing solely on reducing computational time
  \end{enumerate}
\item
  Explain how combining structural pruning, knowledge distillation, and
  quantization can achieve significant model size reduction while
  maintaining performance.
\item
  True or False: AutoML frameworks can discover optimization strategies
  that outperform manual methods by exploring vast optimization spaces.
\item
  The process of reducing numerical precision to improve computational
  efficiency is known as \_\_\_\_. This technique helps in minimizing
  model size and computational load.
\item
  In a production system, what trade-offs might you consider when
  implementing hardware-aware optimization strategies?
\end{enumerate}

\noindent\hspace*{1.25em}\hyperref[quiz-answer-sec-model-compression-summary-8229]{\textbf{See Answer~$\rightarrow$}}

\end{fbx}

\section{Self-Check Answers}\label{self-check-answers}

\phantomsection\label{quiz-answer-sec-model-compression-model-optimization-fundamentals-c0d7}
\begin{fbx}{callout-quiz-answer}{Self-Check: Answer 1.1}{}
\phantomsection\label{quiz-answer-sec-model-compression-model-optimization-fundamentals-c0d7}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Which of the following best describes the primary goal of
  model optimization in machine learning systems?}

  \begin{enumerate}
  \def\labelenumii{\alph{enumii})}
  \tightlist
  \item
    Maximize model accuracy regardless of resource constraints.
  \item
    Reduce the size of the model to the smallest possible footprint.
  \item
    Achieve efficient execution in target environments while maintaining
    accuracy and functionality.
  \item
    Increase the complexity of the model to improve performance.
  \end{enumerate}

  \emph{Answer}: The correct answer is C. Achieve efficient execution in
  target environments while maintaining accuracy and functionality. This
  is correct because model optimization focuses on balancing efficiency
  with performance across various constraints.

  \emph{Learning Objective}: Understand the primary goal of model
  optimization in ML systems.
\item
  \textbf{Explain how model optimization techniques like quantization
  and pruning contribute to efficient deployment of machine learning
  models.}

  \emph{Answer}: Quantization reduces memory usage and speeds up
  inference by using lower precision data types. Pruning removes
  redundant parameters, maintaining model accuracy while reducing
  computational load. These techniques enable deployment in
  resource-constrained environments by optimizing resource utilization.

  \emph{Learning Objective}: Describe how specific optimization
  techniques improve model deployment efficiency.
\item
  \textbf{What is a common challenge when deploying sophisticated
  machine learning models on mobile devices?}

  \begin{enumerate}
  \def\labelenumii{\alph{enumii})}
  \tightlist
  \item
    Excessive computational throughput
  \item
    Unlimited thermal constraints
  \item
    High latency requirements
  \item
    Limited memory and energy resources
  \end{enumerate}

  \emph{Answer}: The correct answer is D. Limited memory and energy
  resources. Mobile devices often have limited memory and battery life,
  making it challenging to deploy large, sophisticated models without
  optimization.

  \emph{Learning Objective}: Identify challenges associated with
  deploying ML models on resource-constrained devices.
\item
  \textbf{True or False: Model optimization only focuses on reducing the
  computational complexity of machine learning models.}

  \emph{Answer}: False. Model optimization also addresses memory
  utilization, inference latency, and energy efficiency, balancing
  multiple performance objectives.

  \emph{Learning Objective}: Recognize the multi-faceted nature of model
  optimization beyond computational complexity.
\item
  \textbf{In a production system, what trade-offs might you consider
  when implementing model optimization techniques?}

  \emph{Answer}: Consider trade-offs between model accuracy and resource
  usage, such as memory and energy consumption. Balancing these can
  affect performance metrics like latency and throughput, impacting user
  experience and operational costs.

  \emph{Learning Objective}: Analyze trade-offs involved in applying
  model optimization techniques in production systems.
\end{enumerate}

\noindent\hspace*{1.25em}\hyperref[quiz-question-sec-model-compression-model-optimization-fundamentals-c0d7]{\textbf{$\leftarrow$~Back to Question}}

\end{fbx}

\phantomsection\label{quiz-answer-sec-model-compression-optimization-framework-9e21}
\begin{fbx}{callout-quiz-answer}{Self-Check: Answer 1.2}{}
\phantomsection\label{quiz-answer-sec-model-compression-optimization-framework-9e21}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Which of the following layers in the optimization stack
  primarily focuses on aligning computation patterns with processor
  designs?}

  \begin{enumerate}
  \def\labelenumii{\alph{enumii})}
  \tightlist
  \item
    Efficient Model Representation
  \item
    Efficient Numerics Representation
  \item
    Efficient Data Handling
  \item
    Efficient Hardware Implementation
  \end{enumerate}

  \emph{Answer}: The correct answer is D. Efficient Hardware
  Implementation. This layer focuses on aligning computation patterns
  with processor designs to enhance execution efficiency. Other options
  address different aspects of optimization.

  \emph{Learning Objective}: Understand the role of efficient hardware
  implementation in the optimization stack.
\item
  \textbf{Explain how model representation techniques such as pruning
  and distillation can create opportunities for numerical precision
  optimization.}

  \emph{Answer}: Model representation techniques like pruning and
  distillation reduce computational complexity, which allows for
  numerical precision optimization by freeing up resources. For example,
  pruning reduces model size, enabling the use of quantization
  techniques that exploit hardware capabilities for faster execution.
  This is important because it enhances model efficiency while
  maintaining performance.

  \emph{Learning Objective}: Analyze the interaction between model
  representation techniques and numerical precision optimization.
\item
  \textbf{In the context of the optimization framework, what is the
  primary benefit of using quantization techniques?}

  \begin{enumerate}
  \def\labelenumii{\alph{enumii})}
  \tightlist
  \item
    Increasing model accuracy
  \item
    Reducing computational cost
  \item
    Enhancing data privacy
  \item
    Improving data collection
  \end{enumerate}

  \emph{Answer}: The correct answer is B. Reducing computational cost.
  Quantization techniques primarily reduce computational cost by using
  reduced-precision arithmetic, which exploits hardware capabilities for
  faster execution. Other options do not directly relate to the primary
  benefit of quantization.

  \emph{Learning Objective}: Understand the primary benefit of
  quantization techniques in the optimization framework.
\item
  \textbf{True or False: The optimization framework's effectiveness is
  independent of the target hardware characteristics.}

  \emph{Answer}: False. The effectiveness of the optimization framework
  depends on the target hardware characteristics, as the techniques must
  align with the hardware's capabilities to maximize performance and
  efficiency.

  \emph{Learning Objective}: Recognize the dependency of optimization
  effectiveness on hardware characteristics.
\end{enumerate}

\noindent\hspace*{1.25em}\hyperref[quiz-question-sec-model-compression-optimization-framework-9e21]{\textbf{$\leftarrow$~Back to Question}}

\end{fbx}

\phantomsection\label{quiz-answer-sec-model-compression-deployment-context-0d88}
\begin{fbx}{callout-quiz-answer}{Self-Check: Answer 1.3}{}
\phantomsection\label{quiz-answer-sec-model-compression-deployment-context-0d88}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Which of the following is a primary constraint when deploying
  machine learning models on microcontrollers?}

  \begin{enumerate}
  \def\labelenumii{\alph{enumii})}
  \tightlist
  \item
    High memory bandwidth
  \item
    Limited computational resources
  \item
    Large storage capacity
  \item
    Unlimited power supply
  \end{enumerate}

  \emph{Answer}: The correct answer is B. Limited computational
  resources. Microcontrollers have limited computational power and
  memory, which constrain the complexity of models that can be deployed.

  \emph{Learning Objective}: Understand the constraints specific to
  deploying ML models on microcontrollers.
\item
  \textbf{True or False: In cloud environments, optimizing machine
  learning models primarily focuses on reducing the model's memory
  footprint.}

  \emph{Answer}: False. In cloud environments, optimization focuses on
  minimizing training time, computational cost, and power consumption,
  not just memory footprint.

  \emph{Learning Objective}: Recognize the primary optimization goals in
  different deployment contexts.
\item
  \textbf{Explain why balancing accuracy and efficiency is crucial when
  deploying machine learning models on edge devices.}

  \emph{Answer}: Balancing accuracy and efficiency is crucial on edge
  devices because these devices have limited computational resources and
  must process data locally to reduce latency. For example, a highly
  accurate model that is too resource-intensive cannot run effectively
  on a smartphone. This balance ensures real-time responsiveness while
  maintaining acceptable performance.

  \emph{Learning Objective}: Analyze the trade-offs between accuracy and
  efficiency in edge ML deployments.
\item
  \textbf{In the context of deployment, the term `\_\_\_\_' refers to
  the computational paradigm where ML inference occurs on local devices
  rather than cloud servers.}

  \emph{Answer}: Edge ML. Edge ML refers to performing machine learning
  inference on local devices, reducing latency and dependency on cloud
  resources.

  \emph{Learning Objective}: Recall specific terminology related to
  deployment contexts.
\item
  \textbf{In a production system, how might you address the trade-off
  between model complexity and energy efficiency?}

  \emph{Answer}: In a production system, addressing the trade-off
  between model complexity and energy efficiency might involve
  techniques such as model pruning, quantization, or using more
  efficient algorithms. For example, pruning can reduce the number of
  parameters, lowering energy consumption while maintaining acceptable
  accuracy. This is important to ensure the model operates effectively
  within the energy constraints of the deployment environment.

  \emph{Learning Objective}: Evaluate strategies for balancing model
  complexity with energy efficiency in practical deployments.
\end{enumerate}

\noindent\hspace*{1.25em}\hyperref[quiz-question-sec-model-compression-deployment-context-0d88]{\textbf{$\leftarrow$~Back to Question}}

\end{fbx}

\phantomsection\label{quiz-answer-sec-model-compression-structural-model-optimization-methods-8ed4}
\begin{fbx}{callout-quiz-answer}{Self-Check: Answer 1.4}{}
\phantomsection\label{quiz-answer-sec-model-compression-structural-model-optimization-methods-8ed4}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Which of the following best describes the purpose of gradient
  checkpointing in neural network optimization?}

  \begin{enumerate}
  \def\labelenumii{\alph{enumii})}
  \tightlist
  \item
    To reduce memory usage by recomputing intermediate activations
    during backpropagation.
  \item
    To improve model accuracy by increasing the number of parameters.
  \item
    To enhance computational speed by parallelizing model training.
  \item
    To eliminate redundant parameters through pruning.
  \end{enumerate}

  \emph{Answer}: The correct answer is A. To reduce memory usage by
  recomputing intermediate activations during backpropagation. Gradient
  checkpointing trades computation for memory, allowing larger models or
  batch sizes to fit into the same GPU memory.

  \emph{Learning Objective}: Understand the role of gradient
  checkpointing in optimizing memory usage during model training.
\item
  \textbf{Explain the trade-offs involved in model pruning and how it
  affects deployment in different environments.}

  \emph{Answer}: Pruning reduces model size by removing redundant
  parameters, improving memory and computational efficiency. However,
  aggressive pruning can degrade accuracy, making models unreliable for
  production. In cloud environments, pruning helps scale models
  efficiently, while on edge devices, it ensures models fit within
  resource constraints. Balancing pruning extent is crucial to maintain
  performance across diverse deployment scenarios.

  \emph{Learning Objective}: Analyze the trade-offs of model pruning in
  various deployment environments.
\item
  \textbf{The process of systematically removing redundant parameters
  from a neural network while preserving accuracy is known as \_\_\_\_.
  This technique reduces model size and computational cost.}

  \emph{Answer}: pruning. Pruning eliminates unnecessary parameters,
  making models more efficient for storage, inference, and deployment.

  \emph{Learning Objective}: Recall the definition and purpose of
  pruning in neural network optimization.
\item
  \textbf{What is a primary advantage of using parallel processing
  patterns in machine learning model optimization?}

  \begin{enumerate}
  \def\labelenumii{\alph{enumii})}
  \tightlist
  \item
    It increases the number of parameters in the model.
  \item
    It reduces the need for gradient checkpointing.
  \item
    It allows for faster training by utilizing multiple cores
    simultaneously.
  \item
    It eliminates the need for model pruning.
  \end{enumerate}

  \emph{Answer}: The correct answer is C. It allows for faster training
  by utilizing multiple cores simultaneously. Parallel processing
  leverages high core counts in GPUs to speed up model training and
  inference.

  \emph{Learning Objective}: Understand the benefits of parallel
  processing in accelerating model training and deployment.
\end{enumerate}

\noindent\hspace*{1.25em}\hyperref[quiz-question-sec-model-compression-structural-model-optimization-methods-8ed4]{\textbf{$\leftarrow$~Back to Question}}

\end{fbx}

\phantomsection\label{quiz-answer-sec-model-compression-quantization-precision-optimization-31d3}
\begin{fbx}{callout-quiz-answer}{Self-Check: Answer 1.5}{}
\phantomsection\label{quiz-answer-sec-model-compression-quantization-precision-optimization-31d3}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Which of the following precision formats offers the best
  balance between computational speed and accuracy for training on AI
  accelerators?}

  \begin{enumerate}
  \def\labelenumii{\alph{enumii})}
  \tightlist
  \item
    BFloat16
  \item
    FP16
  \item
    INT8
  \item
    FP32
  \end{enumerate}

  \emph{Answer}: The correct answer is A. BFloat16. BFloat16 retains the
  same exponent size as FP32, allowing it to maintain a wider dynamic
  range while reducing precision in the fraction, making it effective
  for training on AI accelerators.

  \emph{Learning Objective}: Understand the balance between
  computational speed and accuracy provided by different precision
  formats.
\item
  \textbf{Explain the trade-offs involved in using INT8 precision for
  inference in machine learning models.}

  \emph{Answer}: Using INT8 precision significantly reduces storage and
  computational requirements, leading to faster inference and lower
  power consumption. However, it may introduce quantization noise and
  accuracy degradation, especially in models sensitive to precision
  loss. Balancing these trade-offs involves ensuring hardware support
  and employing techniques like quantization-aware training to mitigate
  accuracy loss.

  \emph{Learning Objective}: Analyze the trade-offs of using INT8
  precision in terms of efficiency and accuracy.
\item
  \textbf{The process of reducing numerical precision to improve
  computational efficiency is known as \_\_\_\_. This technique is
  essential for optimizing machine learning models for deployment in
  resource-constrained environments.}

  \emph{Answer}: quantization. Quantization reduces the bit-width of
  numerical representations, enhancing computational efficiency and
  reducing power consumption, particularly in resource-constrained
  environments.

  \emph{Learning Objective}: Recall the term used for reducing numerical
  precision to enhance computational efficiency.
\item
  \textbf{True or False: Reducing precision from FP32 to FP16 always
  leads to a proportional decrease in power consumption.}

  \emph{Answer}: False. While reducing precision from FP32 to FP16 can
  lead to significant power savings, the relationship is not always
  proportional due to factors such as hardware support and the need for
  additional techniques to manage quantization errors.

  \emph{Learning Objective}: Understand the non-linear relationship
  between precision reduction and power consumption.
\item
  \textbf{Order the following precision formats by their typical storage
  reduction compared to FP32: (1) FP16, (2) INT8, (3) BFloat16.}

  \emph{Answer}: The correct order is: (2) INT8, (1) FP16, (3) BFloat16.
  INT8 offers the most significant storage reduction, followed by FP16,
  and then BFloat16, which maintains a larger exponent for dynamic
  range.

  \emph{Learning Objective}: Sequence precision formats based on their
  storage efficiency compared to FP32.
\end{enumerate}

\noindent\hspace*{1.25em}\hyperref[quiz-question-sec-model-compression-quantization-precision-optimization-31d3]{\textbf{$\leftarrow$~Back to Question}}

\end{fbx}

\phantomsection\label{quiz-answer-sec-model-compression-architectural-efficiency-techniques-daa6}
\begin{fbx}{callout-quiz-answer}{Self-Check: Answer 1.6}{}
\phantomsection\label{quiz-answer-sec-model-compression-architectural-efficiency-techniques-daa6}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Which of the following best describes the goal of
  architectural efficiency optimization in machine learning systems?}

  \begin{enumerate}
  \def\labelenumii{\alph{enumii})}
  \tightlist
  \item
    Aligning model operations with hardware capabilities
  \item
    Improving numerical precision of computations
  \item
    Increasing the number of model parameters
  \item
    Enhancing the theoretical accuracy of models
  \end{enumerate}

  \emph{Answer}: The correct answer is A. Aligning model operations with
  hardware capabilities. This is correct because architectural
  efficiency focuses on optimizing how operations are scheduled and
  executed on specific hardware, rather than altering the computations
  themselves. Other options do not address the system-level optimization
  focus of architectural efficiency.

  \emph{Learning Objective}: Understand the primary goal of
  architectural efficiency optimization in ML systems.
\item
  \textbf{Explain how hardware-aware design principles can improve the
  deployment efficiency of machine learning models.}

  \emph{Answer}: Hardware-aware design principles improve deployment
  efficiency by ensuring that model architectures are tailored to the
  specific capabilities and constraints of the target hardware. This
  includes optimizing for memory bandwidth, processing power, and
  parallelism, which reduces latency and increases throughput. For
  example, using depthwise separable convolutions on mobile chips can
  significantly reduce computational cost while maintaining performance.
  This is important because it allows models to be efficiently deployed
  across diverse hardware environments.

  \emph{Learning Objective}: Analyze the impact of hardware-aware design
  principles on model deployment efficiency.
\item
  \textbf{Which architectural efficiency technique involves reducing
  memory traffic by combining operations?}

  \begin{enumerate}
  \def\labelenumii{\alph{enumii})}
  \tightlist
  \item
    Dynamic computation strategies
  \item
    Sparsity exploitation techniques
  \item
    Operator fusion methods
  \item
    Hardware-aware design
  \end{enumerate}

  \emph{Answer}: The correct answer is C. Operator fusion methods. This
  is correct because operator fusion reduces memory traffic by combining
  multiple operations into a single operation, thereby minimizing the
  need for intermediate data storage and transfer. Other options focus
  on different aspects of architectural efficiency.

  \emph{Learning Objective}: Identify techniques that reduce memory
  traffic in ML systems.
\item
  \textbf{In a production system, what trade-offs would you consider
  when implementing dynamic computation strategies?}

  \emph{Answer}: When implementing dynamic computation strategies,
  trade-offs include balancing computational efficiency with predictive
  performance. These strategies allow models to adapt computational load
  based on input complexity, which can reduce energy consumption and
  latency. However, this may introduce variability in inference time and
  require additional control mechanisms, potentially complicating
  deployment. For example, in real-time applications, ensuring
  consistent latency while maintaining accuracy is crucial. This is
  important because it affects the system's ability to meet performance
  requirements under varying conditions.

  \emph{Learning Objective}: Evaluate trade-offs involved in
  implementing dynamic computation strategies in production systems.
\end{enumerate}

\noindent\hspace*{1.25em}\hyperref[quiz-question-sec-model-compression-architectural-efficiency-techniques-daa6]{\textbf{$\leftarrow$~Back to Question}}

\end{fbx}

\phantomsection\label{quiz-answer-sec-model-compression-automl-automated-optimization-strategies-cf08}
\begin{fbx}{callout-quiz-answer}{Self-Check: Answer 1.7}{}
\phantomsection\label{quiz-answer-sec-model-compression-automl-automated-optimization-strategies-cf08}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Which of the following best describes the role of AutoML in
  machine learning model optimization?}

  \begin{enumerate}
  \def\labelenumii{\alph{enumii})}
  \tightlist
  \item
    It completely replaces the need for human expertise in model design.
  \item
    It automates the search for optimal model configurations, reducing
    manual effort.
  \item
    It focuses solely on improving model accuracy without considering
    efficiency.
  \item
    It is primarily used for data collection and preprocessing.
  \end{enumerate}

  \emph{Answer}: The correct answer is B. It automates the search for
  optimal model configurations, reducing manual effort. AutoML enhances
  human expertise by providing a structured approach to optimization,
  balancing accuracy and efficiency.

  \emph{Learning Objective}: Understand the primary function and
  benefits of AutoML in the context of model optimization.
\item
  \textbf{Explain how AutoML frameworks balance accuracy and efficiency
  in model optimization.}

  \emph{Answer}: AutoML frameworks employ techniques like neural
  architecture search and hyperparameter optimization to explore a vast
  design space, selecting configurations that meet predefined accuracy
  and efficiency objectives. This structured approach allows for the
  discovery of novel solutions that balance these factors effectively.

  \emph{Learning Objective}: Analyze the methods AutoML uses to optimize
  machine learning models while balancing multiple objectives.
\item
  \textbf{True or False: AutoML can fully eliminate the need for domain
  expertise in model optimization.}

  \emph{Answer}: False. AutoML enhances but does not replace domain
  expertise. It provides a structured approach to optimization, allowing
  experts to focus on high-level objectives while automating routine
  tasks.

  \emph{Learning Objective}: Challenge misconceptions about the role of
  AutoML in replacing human expertise.
\item
  \textbf{The process of automatically selecting pruning thresholds and
  quantization levels in AutoML is known as \_\_\_\_.}

  \emph{Answer}: model compression. This process reduces the memory
  footprint and computational requirements of a model, making it
  suitable for deployment on resource-constrained hardware.

  \emph{Learning Objective}: Recall specific optimization techniques
  used in AutoML for efficient model deployment.
\item
  \textbf{In a production system, what trade-offs might you consider
  when implementing AutoML for model optimization?}

  \emph{Answer}: Consider trade-offs between computational cost and
  optimization quality. AutoML requires significant computational
  resources, but can yield highly optimized models. Balancing these
  factors involves assessing the available infrastructure and the
  importance of achieving optimal performance versus resource
  expenditure.

  \emph{Learning Objective}: Evaluate the practical considerations and
  trade-offs involved in deploying AutoML in real-world systems.
\end{enumerate}

\noindent\hspace*{1.25em}\hyperref[quiz-question-sec-model-compression-automl-automated-optimization-strategies-cf08]{\textbf{$\leftarrow$~Back to Question}}

\end{fbx}

\phantomsection\label{quiz-answer-sec-model-compression-measuring-model-efficiency-00c7}
\begin{fbx}{callout-quiz-answer}{Self-Check: Answer 1.8}{}
\phantomsection\label{quiz-answer-sec-model-compression-measuring-model-efficiency-00c7}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Which of the following is a critical first step in the
  optimization process for machine learning systems?}

  \begin{enumerate}
  \def\labelenumii{\alph{enumii})}
  \tightlist
  \item
    Applying quantization to reduce model size
  \item
    Conducting sensitivity analysis on model parameters
  \item
    Implementing structured pruning techniques
  \item
    Establishing baseline measurements through profiling
  \end{enumerate}

  \emph{Answer}: The correct answer is D. Establishing baseline
  measurements through profiling. This is correct because profiling
  identifies where resources are consumed and which components offer
  optimization potential. Other options are specific techniques applied
  after profiling.

  \emph{Learning Objective}: Understand the importance of profiling as
  the foundational step in optimization.
\item
  \textbf{True or False: In a production environment, model computation
  often represents the majority of system overhead.}

  \emph{Answer}: False. This is false because model computation
  typically represents only a fraction of total system overhead,
  highlighting the importance of profiling to identify optimization
  opportunities.

  \emph{Learning Objective}: Challenge the misconception about the role
  of model computation in system overhead.
\item
  \textbf{Explain how a systematic measurement framework can help
  balance competing optimization objectives in ML systems.}

  \emph{Answer}: A systematic measurement framework helps balance
  competing objectives by establishing clear baselines across multiple
  metrics, such as accuracy, computational efficiency, and energy
  consumption. For example, quantizing a model may reduce latency but
  affect accuracy, so a framework ensures trade-offs are evaluated
  comprehensively. This is important because it guides informed
  decision-making in optimization.

  \emph{Learning Objective}: Understand the role of measurement
  frameworks in balancing optimization objectives.
\item
  \textbf{Order the following stages of deploying BERT-Base on mobile
  devices: (1) Quantization-aware training, (2) Structured pruning, (3)
  Knowledge distillation.}

  \emph{Answer}: The correct order is: (2) Structured pruning, (3)
  Knowledge distillation, (1) Quantization-aware training. Pruning first
  concentrates important weights, making subsequent quantization more
  effective. Knowledge distillation helps recover accuracy before final
  quantization.

  \emph{Learning Objective}: Understand the importance of sequencing in
  multi-technique optimization strategies.
\item
  \textbf{In a production system, what trade-offs would you consider
  when integrating multiple optimization techniques?}

  \emph{Answer}: When integrating multiple optimization techniques,
  consider trade-offs between accuracy, computational efficiency, and
  resource consumption. For instance, pruning may reduce model size but
  affect accuracy, while quantization reduces computational cost but may
  impact precision. Balancing these requires understanding dependencies
  and sequencing techniques to maximize cumulative benefits. This is
  important because it ensures that optimizations do not introduce
  conflicts or diminish returns.

  \emph{Learning Objective}: Evaluate trade-offs in integrating multiple
  optimization techniques in production systems.
\end{enumerate}

\noindent\hspace*{1.25em}\hyperref[quiz-question-sec-model-compression-measuring-model-efficiency-00c7]{\textbf{$\leftarrow$~Back to Question}}

\end{fbx}

\phantomsection\label{quiz-answer-sec-model-compression-implementation-tools-software-frameworks-e939}
\begin{fbx}{callout-quiz-answer}{Self-Check: Answer 1.9}{}
\phantomsection\label{quiz-answer-sec-model-compression-implementation-tools-software-frameworks-e939}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Which of the following best describes the role of modern
  machine learning frameworks in model optimization?}

  \begin{enumerate}
  \def\labelenumii{\alph{enumii})}
  \tightlist
  \item
    They provide theoretical insights into optimization techniques.
  \item
    They replace the need for model optimization entirely.
  \item
    They automate the implementation of complex optimization algorithms.
  \item
    They focus solely on hardware-specific optimizations.
  \end{enumerate}

  \emph{Answer}: The correct answer is C. They automate the
  implementation of complex optimization algorithms. This is correct
  because modern frameworks offer high-level APIs and automated
  workflows that simplify the application of optimization techniques.
  Option A is incorrect as frameworks focus on practical implementation,
  not theoretical insights. Option A is incorrect as frameworks assist
  but do not replace optimization. Option D is incorrect as frameworks
  address a broader range of optimization challenges.

  \emph{Learning Objective}: Understand the role of machine learning
  frameworks in simplifying the implementation of optimization
  techniques.
\item
  \textbf{Explain how built-in optimization APIs in frameworks like
  TensorFlow and PyTorch enhance model efficiency.}

  \emph{Answer}: Built-in optimization APIs provide pre-tested modules
  for techniques like quantization and pruning, reducing the need for
  manual implementation. For example, TensorFlow's Model Optimization
  Toolkit facilitates quantization by converting models to
  lower-precision formats while preserving accuracy. This is important
  because it enables practitioners to apply complex optimizations
  reliably and consistently across different architectures.

  \emph{Learning Objective}: Explain the benefits of using built-in
  optimization APIs for enhancing model efficiency.
\item
  \textbf{True or False: Visualization tools are unnecessary in
  understanding the impact of model optimization techniques like pruning
  and quantization.}

  \emph{Answer}: False. Visualization tools are essential for
  understanding the impact of optimization techniques. They provide
  insights into sparsity patterns and quantization errors, helping
  practitioners diagnose and mitigate issues. For example, quantization
  error histograms reveal error distributions, guiding adjustments to
  maintain model accuracy.

  \emph{Learning Objective}: Recognize the importance of visualization
  tools in interpreting the effects of optimization techniques.
\item
  \textbf{In machine learning frameworks, the process of reducing
  numerical precision to improve computational efficiency is known as
  \_\_\_\_. This process involves converting models to lower-precision
  formats while maintaining accuracy.}

  \emph{Answer}: quantization. This process involves converting models
  to lower-precision formats while maintaining accuracy.

  \emph{Learning Objective}: Recall the term for reducing numerical
  precision in model optimization.
\item
  \textbf{In a production system, what trade-offs might you consider
  when choosing between different optimization techniques provided by
  software frameworks?}

  \emph{Answer}: When choosing optimization techniques, consider
  trade-offs between model accuracy and computational efficiency. For
  example, quantization can reduce precision and computational load but
  may introduce rounding errors affecting accuracy. Pruning reduces
  model size but may impact performance if not carefully managed.
  Balancing these factors is crucial for optimal deployment.

  \emph{Learning Objective}: Evaluate trade-offs in selecting
  optimization techniques for production systems.
\end{enumerate}

\noindent\hspace*{1.25em}\hyperref[quiz-question-sec-model-compression-implementation-tools-software-frameworks-e939]{\textbf{$\leftarrow$~Back to Question}}

\end{fbx}

\phantomsection\label{quiz-answer-sec-model-compression-technique-comparison-3142}
\begin{fbx}{callout-quiz-answer}{Self-Check: Answer 1.10}{}
\phantomsection\label{quiz-answer-sec-model-compression-technique-comparison-3142}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Which optimization technique is best suited for
  latency-critical applications where sparse computation hardware is
  available?}

  \begin{enumerate}
  \def\labelenumii{\alph{enumii})}
  \tightlist
  \item
    Pruning
  \item
    Quantization
  \item
    Distillation
  \item
    All of the above
  \end{enumerate}

  \emph{Answer}: The correct answer is A. Pruning. This is correct
  because pruning reduces floating-point operations and is most
  effective when sparse computation hardware is available. Quantization
  and distillation do not specifically target latency-critical
  applications in the same way.

  \emph{Learning Objective}: Understand the specific use cases and
  hardware dependencies of different optimization techniques.
\item
  \textbf{True or False: Quantization is the most versatile optimization
  technique due to its broad hardware support.}

  \emph{Answer}: True. This is true because quantization can be applied
  to a wide range of hardware platforms, making it ideal for diverse
  deployment scenarios.

  \emph{Learning Objective}: Recognize the versatility and hardware
  compatibility of quantization as an optimization technique.
\item
  \textbf{Explain why knowledge distillation might be preferred when
  accuracy preservation is paramount in a production system.}

  \emph{Answer}: Knowledge distillation is preferred when accuracy
  preservation is paramount because it produces high-quality compressed
  models. For example, in scenarios where model size needs to be reduced
  without significant loss of accuracy, distillation can provide a
  balance between compression and performance. This is important because
  maintaining model accuracy is crucial in applications where precision
  is critical.

  \emph{Learning Objective}: Analyze the trade-offs involved in using
  knowledge distillation for accuracy-sensitive applications.
\item
  \textbf{Order the following optimization techniques based on their
  typical application sequence in a production system: (1) Pruning, (2)
  Quantization, (3) Distillation.}

  \emph{Answer}: The correct order is: (1) Pruning, (3) Distillation,
  (2) Quantization. Pruning is typically applied first to reduce the
  parameter count, distillation is used to recover accuracy, and
  quantization is applied last to optimize numerical representation.
  This sequence maximizes compression while maintaining accuracy.

  \emph{Learning Objective}: Understand the sequential application of
  optimization techniques in production systems.
\end{enumerate}

\noindent\hspace*{1.25em}\hyperref[quiz-question-sec-model-compression-technique-comparison-3142]{\textbf{$\leftarrow$~Back to Question}}

\end{fbx}

\phantomsection\label{quiz-answer-sec-model-compression-fallacies-pitfalls-1b5e}
\begin{fbx}{callout-quiz-answer}{Self-Check: Answer 1.11}{}
\phantomsection\label{quiz-answer-sec-model-compression-fallacies-pitfalls-1b5e}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{True or False: Optimization techniques like pruning and
  quantization can be applied independently without considering their
  interactions.}

  \emph{Answer}: False. Applying optimization techniques independently
  without considering their interactions can lead to compounded accuracy
  losses and suboptimal results.

  \emph{Learning Objective}: Understand the importance of coordinating
  optimization techniques to avoid negative interactions.
\item
  \textbf{Which of the following is a potential pitfall when optimizing
  machine learning models?}

  \begin{enumerate}
  \def\labelenumii{\alph{enumii})}
  \tightlist
  \item
    Profiling the entire system for bottlenecks
  \item
    Measuring actual deployment performance
  \item
    Applying optimization-aware training
  \item
    Focusing solely on reducing parameter counts
  \end{enumerate}

  \emph{Answer}: The correct answer is D. Focusing solely on reducing
  parameter counts. This is a pitfall because it may not translate to
  actual deployment performance improvements.

  \emph{Learning Objective}: Identify common pitfalls in model
  optimization and their impact on deployment.
\item
  \textbf{Explain why aggressive quantization might not always maintain
  model performance.}

  \emph{Answer}: Aggressive quantization can lead to catastrophic
  accuracy degradation and numerical instability, as different model
  architectures and tasks have varying sensitivity to quantization. For
  example, operations like attention mechanisms may require higher
  precision to function correctly. This is important because it
  highlights the need for careful analysis rather than assuming
  universal applicability.

  \emph{Learning Objective}: Analyze the limitations and risks
  associated with aggressive quantization in model optimization.
\item
  \textbf{What is a disadvantage of using post-training optimization
  techniques?}

  \begin{enumerate}
  \def\labelenumii{\alph{enumii})}
  \tightlist
  \item
    They require modification of existing training pipelines
  \item
    They are more complex to implement than training-aware techniques
  \item
    They typically achieve inferior results compared to training-aware
    approaches
  \item
    They always result in higher inference latency
  \end{enumerate}

  \emph{Answer}: The correct answer is C. They typically achieve
  inferior results compared to training-aware approaches. This is
  because post-training optimization does not integrate optimization
  techniques during the training process, which can lead to suboptimal
  results.

  \emph{Learning Objective}: Understand the trade-offs between
  post-training and training-aware optimization techniques.
\item
  \textbf{In a production system, why is it important to consider
  system-level performance bottlenecks when optimizing models?}

  \emph{Answer}: Considering system-level performance bottlenecks is
  crucial because model-level optimizations may not translate to overall
  system improvements. For example, a highly optimized model might offer
  minimal benefit if data preprocessing or network communication
  dominates system latency. This is important because it ensures that
  optimizations lead to measurable system-level performance gains.

  \emph{Learning Objective}: Emphasize the importance of a holistic
  approach to optimization that considers the entire system.
\end{enumerate}

\noindent\hspace*{1.25em}\hyperref[quiz-question-sec-model-compression-fallacies-pitfalls-1b5e]{\textbf{$\leftarrow$~Back to Question}}

\end{fbx}

\phantomsection\label{quiz-answer-sec-model-compression-summary-8229}
\begin{fbx}{callout-quiz-answer}{Self-Check: Answer 1.12}{}
\phantomsection\label{quiz-answer-sec-model-compression-summary-8229}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Which of the following best illustrates the trade-off between
  model accuracy and resource efficiency in optimization?}

  \begin{enumerate}
  \def\labelenumii{\alph{enumii})}
  \tightlist
  \item
    Increasing model size to improve accuracy
  \item
    Applying structural pruning to reduce model size
  \item
    Using more complex algorithms without considering deployment
  \item
    Focusing solely on reducing computational time
  \end{enumerate}

  \emph{Answer}: The correct answer is B. Applying structural pruning to
  reduce model size. This illustrates the trade-off by maintaining
  essential model capabilities while reducing resource demands. Options
  A and C increase resource use, and D overlooks accuracy.

  \emph{Learning Objective}: Understand the trade-offs involved in model
  optimization techniques.
\item
  \textbf{Explain how combining structural pruning, knowledge
  distillation, and quantization can achieve significant model size
  reduction while maintaining performance.}

  \emph{Answer}: Combining these techniques allows for a layered
  approach: pruning reduces unnecessary parameters, distillation
  transfers knowledge to a smaller model, and quantization reduces
  numerical precision. Together, they maintain performance while
  significantly reducing size, as seen in BERT's compression.

  \emph{Learning Objective}: Analyze the integration of multiple
  optimization techniques for effective model size reduction.
\item
  \textbf{True or False: AutoML frameworks can discover optimization
  strategies that outperform manual methods by exploring vast
  optimization spaces.}

  \emph{Answer}: True. This is true because AutoML frameworks
  systematically explore optimization spaces, often uncovering novel
  combinations of techniques that achieve superior efficiency-accuracy
  trade-offs compared to manual methods.

  \emph{Learning Objective}: Recognize the advantages of using AutoML
  for discovering optimization strategies.
\item
  \textbf{The process of reducing numerical precision to improve
  computational efficiency is known as \_\_\_\_. This technique helps in
  minimizing model size and computational load.}

  \emph{Answer}: quantization. This technique helps in minimizing model
  size and computational load by reducing the number of bits used for
  representing numbers.

  \emph{Learning Objective}: Recall specific optimization techniques and
  their impact on model efficiency.
\item
  \textbf{In a production system, what trade-offs might you consider
  when implementing hardware-aware optimization strategies?}

  \emph{Answer}: When implementing hardware-aware optimization, consider
  the balance between maximizing performance benefits and the
  compatibility with existing hardware. For example, aligning model
  characteristics with specific computational architectures can enhance
  efficiency but may limit flexibility across different platforms.

  \emph{Learning Objective}: Evaluate trade-offs in applying
  hardware-aware optimization strategies in real-world scenarios.
\end{enumerate}

\noindent\hspace*{1.25em}\hyperref[quiz-question-sec-model-compression-summary-8229]{\textbf{$\leftarrow$~Back to Question}}

\end{fbx}

\phantomsection\label{refs}
\begin{CSLReferences}{1}{0}
\bibitem[\citeproctext]{ref-NVIDIA2020}
Abdelkhalik, Hamdy, Yehia Arafa, Nandakishore Santhi, and Abdel-Hameed
A. Badawy. 2022. {``Demystifying the Nvidia Ampere Architecture Through
Microbenchmarking and Instruction-Level Analysis.''} In \emph{2022 IEEE
High Performance Extreme Computing Conference (HPEC)}. IEEE.
\url{https://doi.org/10.1109/hpec55821.2022.9926299}.

\bibitem[\citeproctext]{ref-banbury2020benchmarking}
Banbury, Colby R., Vijay Janapa Reddi, Max Lam, William Fu, Amin Fazel,
Jeremy Holleman, Xinyuan Huang, et al. 2020. {``Benchmarking TinyML
Systems: Challenges and Direction.''} \emph{arXiv Preprint
arXiv:2003.04821}, March. \url{http://arxiv.org/abs/2003.04821v4}.

\bibitem[\citeproctext]{ref-Gale2020}
Baraglia, David, and Hokuto Konno. 2019. {``On the Bauer-Furuta and
Seiberg-Witten Invariants of Families of \(4\)-Manifolds.''} \emph{arXiv
Preprint arXiv:1903.01649}, March, 8955--67.
\url{http://arxiv.org/abs/1903.01649v3}.

\bibitem[\citeproctext]{ref-Bergstra2011}
Bardenet, Rémi, Olivier Cappé, Gersende Fort, and Balázs Kégl. 2015.
{``Adaptive MCMC with Online Relabeling.''} \emph{Bernoulli} 21 (3).
\url{https://doi.org/10.3150/13-bej578}.

\bibitem[\citeproctext]{ref-bengio2015conditional}
Bengio, Emmanuel, Pierre-Luc Bacon, Joelle Pineau, and Doina Precup.
2015. {``Conditional Computation in Neural Networks for Faster
Models.''} \emph{arXiv Preprint arXiv:1511.06297}, November.
\url{http://arxiv.org/abs/1511.06297v2}.

\bibitem[\citeproctext]{ref-Bengio2013}
Bengio, Yoshua, Nicholas Léonard, and Aaron Courville. 2013b.
{``Estimating or Propagating Gradients Through Stochastic Neurons for
Conditional Computation.''} \emph{arXiv Preprint}, August.
\url{http://arxiv.org/abs/1308.3432v1}.

\bibitem[\citeproctext]{ref-bengio2013estimating}
---------. 2013a. {``Estimating or Propagating Gradients Through
Stochastic Neurons for Conditional Computation.''} \emph{arXiv Preprint
arXiv:1308.3432}, August. \url{http://arxiv.org/abs/1308.3432v1}.

\bibitem[\citeproctext]{ref-Cai2020}
Cai, Han, Chuang Gan, and Song Han. 2020. {``Once-for-All: Train One
Network and Specialize It for Efficient Deployment.''} In
\emph{International Conference on Learning Representations}.

\bibitem[\citeproctext]{ref-Cheng2022}
Cheng, Yu et al. 2022. {``Memory-Efficient Deep Learning: Advances in
Model Compression and Sparsification.''} \emph{ACM Computing Surveys}.

\bibitem[\citeproctext]{ref-Choi2019}
Choi, Jungwook, Zhuo Wang, Swagath Venkataramani, Pierce I-Jen Chuang,
Vijayalakshmi Srinivasan, and Kailash Gopalakrishnan. 2018. {``PACT:
Parameterized Clipping Activation for Quantized Neural Networks.''}
\emph{arXiv Preprint}, May. \url{http://arxiv.org/abs/1805.06085v2}.

\bibitem[\citeproctext]{ref-choudhary2020comprehensive}
Choudhary, Tejalal, Vipul Mishra, Anurag Goswami, and Jagannathan
Sarangapani. 2020. {``A Comprehensive Survey on Model Compression and
Acceleration.''} \emph{Artificial Intelligence Review} 53 (7): 5113--55.
\url{https://doi.org/10.1007/s10462-020-09816-7}.

\bibitem[\citeproctext]{ref-wang2021glam}
Contro, Filippo, Marco Crosara, Mariano Ceccato, and Mila Dalla Preda.
2021. {``EtherSolve: Computing an Accurate Control-Flow Graph from
Ethereum Bytecode.''} \emph{arXiv Preprint arXiv:2103.09113}, March.
\url{http://arxiv.org/abs/2103.09113v1}.

\bibitem[\citeproctext]{ref-Courbariaux2016}
Courbariaux, Matthieu, Yoshua Bengio, and Jean-Pierre David. 2016.
{``BinaryConnect: Training Deep Neural Networks with Binary Weights
During Propagations.''} \emph{Advances in Neural Information Processing
Systems (NeurIPS)} 28: 3123--31.

\bibitem[\citeproctext]{ref-dao2022monarchexpressivestructuredmatrices}
Dao, Tri, Beidi Chen, Nimit Sohoni, Arjun Desai, Michael Poli, Jessica
Grogan, Alexander Liu, Aniruddh Rao, Atri Rudra, and Christopher Ré.
2022. {``Monarch: Expressive Structured Matrices for Efficient and
Accurate Training,''} April. \url{http://arxiv.org/abs/2204.00595v1}.

\bibitem[\citeproctext]{ref-dean2018new}
Dean, Jeff, David Patterson, and Cliff Young. 2018. {``A New Golden Age
in Computer Architecture: Empowering the Machine-Learning Revolution.''}
\emph{IEEE Micro} 38 (2): 21--29.
\url{https://doi.org/10.1109/mm.2018.112130030}.

\bibitem[\citeproctext]{ref-Denton2014}
Denton, Emily L, Soumith Chintala, and Rob Fergus. 2014. {``Exploiting
Linear Structure Within Convolutional Networks for Efficient
Evaluation.''} In \emph{Advances in Neural Information Processing
Systems (NeurIPS)}, 1269--77.

\bibitem[\citeproctext]{ref-dettmers2019sparse}
Dettmers, Tim, and Luke Zettlemoyer. 2019. {``Sparse Networks from
Scratch: Faster Training Without Losing Performance.''} \emph{arXiv
Preprint arXiv:1907.04840}, July.
\url{http://arxiv.org/abs/1907.04840v2}.

\bibitem[\citeproctext]{ref-elsen2020fast}
Elsen, Erich, Marat Dukhan, Trevor Gale, and Karen Simonyan. 2020.
{``Fast Sparse ConvNets.''} In \emph{2020 IEEE/CVF Conference on
Computer Vision and Pattern Recognition (CVPR)}, 14617--26. IEEE.
\url{https://doi.org/10.1109/cvpr42600.2020.01464}.

\bibitem[\citeproctext]{ref-Elsken2019}
Elsken, Thomas, Jan Hendrik Metzen, and Frank Hutter. 2019b. {``Neural
Architecture Search.''} In \emph{Automated Machine Learning}, 63--77.
Springer International Publishing.
\url{https://doi.org/10.1007/978-3-030-05318-5/_3}.

\bibitem[\citeproctext]{ref-elsken2019neural}
---------. 2019a. {``Neural Architecture Search.''} In \emph{Automated
Machine Learning}, 20:63--77. 55. Springer International Publishing.
\url{https://doi.org/10.1007/978-3-030-05318-5/_3}.

\bibitem[\citeproctext]{ref-fedus2021switch}
Fedus, William, Barret Zoph, and Noam Shazeer. 2021. {``Switch
Transformers: Scaling to Trillion Parameter Models with Simple and
Efficient Sparsity.''} \emph{Journal of Machine Learning Research}.

\bibitem[\citeproctext]{ref-Feurer2015}
Feurer, Matthias, Aaron Klein, Katharina Eggensperger, Jost Tobias
Springenberg, Manuel Blum, and Frank Hutter. 2019. {``Auto-Sklearn:
Efficient and Robust Automated Machine Learning.''} In \emph{Automated
Machine Learning}, 113--34. Springer International Publishing.
\url{https://doi.org/10.1007/978-3-030-05318-5/_6}.

\bibitem[\citeproctext]{ref-frankle2019lottery}
Frankle, Jonathan, and Michael Carbin. 2019. {``The Lottery Ticket
Hypothesis: Finding Sparse, Trainable Neural Networks.''} In
\emph{International Conference on Learning Representations}.
\url{https://openreview.net/forum?id=rJl-b3RcF7}.

\bibitem[\citeproctext]{ref-gale2019state}
Gale, Trevor, Erich Elsen, and Sara Hooker. 2019b. {``The State of
Sparsity in Deep Neural Networks.''} \emph{arXiv Preprint
arXiv:1902.09574}, February. \url{http://arxiv.org/abs/1902.09574v1}.

\bibitem[\citeproctext]{ref-gale2020sparse}
---------. 2019a. {``The State of Sparsity in Deep Neural Networks.''}
\emph{arXiv Preprint arXiv:1902.09574}, February.
\url{http://arxiv.org/abs/1902.09574v1}.

\bibitem[\citeproctext]{ref-gale2022megablocksefficientsparsetraining}
Gale, Trevor, Deepak Narayanan, Cliff Young, and Matei Zaharia. 2022.
{``MegaBlocks: Efficient Sparse Training with Mixture-of-Experts,''}
November. \url{http://arxiv.org/abs/2211.15841v1}.

\bibitem[\citeproctext]{ref-gholami2021survey}
Gholami, Amir, Sehoon Kim, Zhen Dong, Zhewei Yao, Michael W. Mahoney,
and Kurt Keutzer. 2021. {``A Survey of Quantization Methods for
Efficient Neural Network Inference.''} \emph{arXiv Preprint
arXiv:2103.13630} abs/2103.13630 (March).
\url{http://arxiv.org/abs/2103.13630v3}.

\bibitem[\citeproctext]{ref-choukroun2019low}
Gong, Ruihao, Xianglong Liu, Shenghu Jiang, Tianxiang Li, Peng Hu,
Jiazhen Lin, Fengwei Yu, and Junjie Yan. 2019. {``Differentiable Soft
Quantization: Bridging Full-Precision and Low-Bit Neural Networks.''}
\emph{arXiv Preprint arXiv:1908.05033}, August.
\url{http://arxiv.org/abs/1908.05033v1}.

\bibitem[\citeproctext]{ref-gordon2020compressing}
Gordon, Mitchell, Kevin Duh, and Nicholas Andrews. 2020. {``Compressing
BERT: Studying the Effects of Weight Pruning on Transfer Learning.''} In
\emph{Proceedings of the 5th Workshop on Representation Learning for
NLP}. Association for Computational Linguistics.
\url{https://doi.org/10.18653/v1/2020.repl4nlp-1.18}.

\bibitem[\citeproctext]{ref-gu2023deep}
Gu, Ivy. 2023. {``Deep Learning Model Compression (Ii) by Ivy Gu
Medium.''}
\url{https://ivygdy.medium.com/deep-learning-model-compression-ii-546352ea9453}.

\bibitem[\citeproctext]{ref-gupta2015deep}
Gupta, Suyog, Ankur Agrawal, Kailash Gopalakrishnan, and Pritish
Narayanan. 2015. {``Deep Learning with Limited Numerical Precision.''}
In \emph{International Conference on Machine Learning}, 1737--46. PMLR.

\bibitem[\citeproctext]{ref-han2015deep}
Han, Song, Huizi Mao, and William J. Dally. 2015. {``Deep Compression:
Compressing Deep Neural Networks with Pruning, Trained Quantization and
Huffman Coding.''} \emph{arXiv Preprint arXiv:1510.00149}, October.
\url{http://arxiv.org/abs/1510.00149v5}.

\bibitem[\citeproctext]{ref-Han2015}
Han, Song, Jeff Pool, John Tran, and William J. Dally. 2015. {``Learning
Both Weights and Connections for Efficient Neural Networks.''}
\emph{CoRR} abs/1506.02626 (June): 1135--43.
\url{http://arxiv.org/abs/1506.02626v3}.

\bibitem[\citeproctext]{ref-he2016deep}
He, Kaiming, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. {``Deep
Residual Learning for Image Recognition.''} In \emph{2016 IEEE
Conference on Computer Vision and Pattern Recognition (CVPR)}, 770--78.
IEEE. \url{https://doi.org/10.1109/cvpr.2016.90}.

\bibitem[\citeproctext]{ref-He2018}
He, Yihui, Ji Lin, Zhijian Liu, Hanrui Wang, Li-Jia Li, and Song Han.
2018. {``AMC: AutoML for Model Compression and Acceleration on Mobile
Devices.''} In \emph{Computer Vision -- ECCV 2018}, 815--32. Springer
International Publishing.
\url{https://doi.org/10.1007/978-3-030-01234-2/_48}.

\bibitem[\citeproctext]{ref-hinton2015distilling}
Hinton, Geoffrey, Oriol Vinyals, and Jeff Dean. 2015. {``Distilling the
Knowledge in a Neural Network,''} March.
\url{https://doi.org/10.1002/0471743984.vse0673}.

\bibitem[\citeproctext]{ref-Hoefler2021}
Hoefler, Torsten, Dan Alistarh, Tal Ben-Nun, Nikoli Dryden, and
Alexandra Peste. 2021. {``Sparsity in Deep Learning: Pruning and Growth
for Efficient Inference and Training in Neural Networks.''} \emph{arXiv
Preprint arXiv:2102.00554} 22 (January): 1--124.
\url{http://arxiv.org/abs/2102.00554v1}.

\bibitem[\citeproctext]{ref-hoefler2021sparsity}
Hoefler, Torsten, Dan Alistarh, Tal Ben-Nun, Nikoli Dryden, and
Alexandros Nikolaos Ziogas. 2021. {``Sparsity in Deep Learning: Pruning
and Growth for Efficient Inference and Training in Neural Networks.''}
\emph{Journal of Machine Learning Research} 22 (241): 1--124.

\bibitem[\citeproctext]{ref-howard2017mobilenets}
Howard, Andrew G., Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun
Wang, Tobias Weyand, Marco Andreetto, and Hartwig Adam. 2017.
{``MobileNets: Efficient Convolutional Neural Networks for Mobile Vision
Applications.''} \emph{ArXiv Preprint} abs/1704.04861 (April).
\url{http://arxiv.org/abs/1704.04861v1}.

\bibitem[\citeproctext]{ref-hu2021triple}
Hu, Bowen, Zhiqiang Zhang, and Yun Fu. 2021. {``Triple Wins: Boosting
Accuracy, Robustness and Efficiency Together by Enabling Input-Adaptive
Inference.''} \emph{Advances in Neural Information Processing Systems}
34: 18537--50.

\bibitem[\citeproctext]{ref-dynamicpruning2023}
Hu, Jie, Peng Lin, Huajun Zhang, Zining Lan, Wenxin Chen, Kailiang Xie,
Siyun Chen, Hao Wang, and Sheng Chang. 2023. {``A Dynamic Pruning Method
on Multiple Sparse Structures in Deep Neural Networks.''} \emph{IEEE
Access} 11: 38448--57.
\url{https://doi.org/10.1109/access.2023.3267469}.

\bibitem[\citeproctext]{ref-huang2023adaptive}
Huang, Wei, Jie Chen, and Lei Zhang. 2023. {``Adaptive Neural Networks
for Real-Time Processing in Autonomous Systems.''} \emph{IEEE
Transactions on Intelligent Transportation Systems}.

\bibitem[\citeproctext]{ref-Hubara2018}
Hubara, Itay, Matthieu Courbariaux, Daniel Soudry, Ran El-Yaniv, and
Yoshua Bengio. 2018. {``Quantized Neural Networks: Training Neural
Networks with Low Precision Weights and Activations.''} \emph{Journal of
Machine Learning Research (JMLR)} 18: 1--30.

\bibitem[\citeproctext]{ref-Hutter2019}
Hutter, Frank, Lars Kotthoff, and Joaquin Vanschoren. 2019.
\emph{Automated Machine Learning: Methods, Systems, Challenges}.
Springer International Publishing.
\url{https://doi.org/10.1007/978-3-030-05318-5}.

\bibitem[\citeproctext]{ref-Jacob2018}
Jacob, Benoit, Skirmantas Kligys, Bo Chen, Menglong Zhu, Matthew Tang,
Andrew Howard, Hartwig Adam, and Dmitry Kalenichenko. 2018b.
{``Quantization and Training of Neural Networks for Efficient
Integer-Arithmetic-Only Inference.''} In \emph{2018 IEEE/CVF Conference
on Computer Vision and Pattern Recognition}, 2704--13. IEEE.
\url{https://doi.org/10.1109/cvpr.2018.00286}.

\bibitem[\citeproctext]{ref-jacob2018quantization}
---------. 2018a. {``Quantization and Training of Neural Networks for
Efficient Integer-Arithmetic-Only Inference.''} In \emph{2018 IEEE/CVF
Conference on Computer Vision and Pattern Recognition}, 2704--13. IEEE.
\url{https://doi.org/10.1109/cvpr.2018.00286}.

\bibitem[\citeproctext]{ref-jia2016dynamic}
Jia, Xu, Bert De Brabandere, Tinne Tuytelaars, and Luc Van Gool. 2016.
{``Dynamic Filter Networks.''} \emph{Advances in Neural Information
Processing Systems} 29.

\bibitem[\citeproctext]{ref-jiao2020tinybert}
Jiao, Xiaoqi, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li,
Fang Wang, and Qun Liu. 2020. {``TinyBERT: Distilling BERT for Natural
Language Understanding.''} In \emph{Findings of the Association for
Computational Linguistics: EMNLP 2020}. Association for Computational
Linguistics. \url{https://doi.org/10.18653/v1/2020.findings-emnlp.372}.

\bibitem[\citeproctext]{ref-Joulin2017}
Joulin, Armand, Edouard Grave, Piotr Bojanowski, and Tomas Mikolov.
2017. {``Bag of Tricks for Efficient Text Classification.''} In
\emph{Proceedings of the 15th Conference of the European Chapter of the
Association for Computational Linguistics: Volume 2, Short Papers},
18:1--42. Association for Computational Linguistics.
\url{https://doi.org/10.18653/v1/e17-2068}.

\bibitem[\citeproctext]{ref-Jouppi2021}
Jouppi, Norman P., Doe Hyun Yoon, Matthew Ashcraft, Mark Gottscho,
Thomas B. Jablin, George Kurian, James Laudon, et al. 2021. {``Ten
Lessons from Three Generations Shaped Google's TPUv4i : Industrial
Product.''} In \emph{2021 ACM/IEEE 48th Annual International Symposium
on Computer Architecture (ISCA)}, 1--14. IEEE.
\url{https://doi.org/10.1109/isca52012.2021.00010}.

\bibitem[\citeproctext]{ref-krishnamoorthi2018quantizing}
Krishnamoorthi, Raghuraman. 2018. {``Quantizing Deep Convolutional
Networks for Efficient Inference: A Whitepaper.''} \emph{arXiv Preprint
arXiv:1806.08342} abs/1806.08342 (June).
\url{http://arxiv.org/abs/1806.08342v1}.

\bibitem[\citeproctext]{ref-alexnet2012}
Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E. Hinton. 2017.
{``ImageNet Classification with Deep Convolutional Neural Networks.''}
Edited by F. Pereira, C. J. Burges, L. Bottou, and K. Q. Weinberger.
\emph{Communications of the ACM} 60 (6): 84--90.
\url{https://doi.org/10.1145/3065386}.

\bibitem[\citeproctext]{ref-blalock2020state}
Labarge, Isaac E. n.d. {``Neural Network Pruning for ECG Arrhythmia
Classification.''} \emph{Proceedings of Machine Learning and Systems
(MLSys)}. PhD thesis, California Polytechnic State University.
\url{https://doi.org/10.15368/theses.2020.76}.

\bibitem[\citeproctext]{ref-lecun1990optimal}
LeCun, Yann, John S. Denker, and Sara A. Solla. 1989. {``Optimal Brain
Damage.''} In \emph{Advances in Neural Information Processing Systems 2
(NIPS 1989)}, 598--605.
\url{http://papers.nips.cc/paper/250-optimal-brain-damage}.

\bibitem[\citeproctext]{ref-lepikhin2020gshard}
Lepikhin, Dmitry et al. 2020. {``GShard: Scaling Giant Models with
Conditional Computation.''} In \emph{Proceedings of the International
Conference on Learning Representations}.

\bibitem[\citeproctext]{ref-Li2021}
Li, Lisha, Kevin G. Jamieson, Giulia DeSalvo, Afshin Rostamizadeh, and
Ameet Talwalkar. 2017. {``Hyperband: A Novel Bandit-Based Approach to
Hyperparameter Optimization.''} \emph{J. Mach. Learn. Res.} 18:
185:1--52. \url{https://jmlr.org/papers/v18/16-558.html}.

\bibitem[\citeproctext]{ref-lin2023awq}
Lin, Ji, Jiaming Tang, Haotian Tang, Shang Yang, Wei-Ming Chen, Wei-Chen
Wang, Guangxuan Xiao, Xingyu Dang, Chuang Gan, and Song Han. 2023.
{``AWQ: Activation-Aware Weight Quantization for LLM Compression and
Acceleration.''} \emph{arXiv Preprint arXiv:2306.00978} abs/2306.00978
(June). \url{http://arxiv.org/abs/2306.00978v5}.

\bibitem[\citeproctext]{ref-Bellec2018}
Liu, Chen, Guillaume Bellec, Bernhard Vogginger, David Kappel, Johannes
Partzsch, Felix Neumärker, Sebastian Höppner, et al. 2018.
{``Memory-Efficient Deep Learning on a SpiNNaker 2 Prototype.''}
\emph{Frontiers in Neuroscience} 12 (November): 840.
\url{https://doi.org/10.3389/fnins.2018.00840}.

\bibitem[\citeproctext]{ref-lu2023steplearningnmstructured}
Lu, Yucheng, Shivani Agrawal, Suvinay Subramanian, Oleg Rybakov,
Christopher De Sa, and Amir Yazdanbakhsh. 2023. {``STEP: Learning n:m
Structured Sparsity Masks from Scratch with Precondition,''} February.
\url{http://arxiv.org/abs/2302.01172v1}.

\bibitem[\citeproctext]{ref-nagel2021white}
Nagel, Markus, Marios Fournarakis, Rana Ali Amjad, Yelysei Bondarenko,
Mart van Baalen, and Tijmen Blankevoort. 2021b. {``A White Paper on
Neural Network Quantization.''} \emph{arXiv Preprint arXiv:2106.08295},
June. \url{http://arxiv.org/abs/2106.08295v1}.

\bibitem[\citeproctext]{ref-nagel2021whitepaper}
---------. 2021a. {``A White Paper on Neural Network Quantization.''}
\emph{arXiv Preprint arXiv:2106.08295}, June.
\url{http://arxiv.org/abs/2106.08295v1}.

\bibitem[\citeproctext]{ref-numenta_sparsity}
Numenta. 2024. {``The Numenta Blog.''}
\url{https://www.numenta.com/blog/}.

\bibitem[\citeproctext]{ref-nvidia_cusparse_block}
NVIDIA. 2020. {``Accelerating Matrix Multiplication with Block Sparse
Format and NVIDIA Tensor
Cores.''}\href{\%0A\%20\%20\%20\%20https://developer.nvidia.com/blog/accelerating-matrix-multiplication-with-block-sparse-format-and-nvidia-tensor-cores/\%0A\%20\%20}{https://developer.nvidia.com/blog/accelerating-matrix-multiplication-with-block-sparse-format-and-nvidia-tensor-cores/
}.

\bibitem[\citeproctext]{ref-patterson2021carbon}
Patterson, David, Joseph Gonzalez, Quoc Le, Chen Liang, Lluis-Miquel
Munguia, Daniel Rothchild, David So, Maud Texier, and Jeff Dean. 2021.
{``Carbon Emissions and Large Neural Network Training.''} \emph{arXiv
Preprint arXiv:2104.10350}, April.
\url{http://arxiv.org/abs/2104.10350v3}.

\bibitem[\citeproctext]{ref-pytorch_sparsity_blog}
PyTorch. 2022. {``Accelerating Neural Network Training with Sparse
Tensors.''}
\url{https://pytorch.org/blog/accelerating-neural-network-training/}.

\bibitem[\citeproctext]{ref-qi2021efficient}
Qi, Chen, Shibo Shen, Rongpeng Li, Zhifeng Zhao, Qing Liu, Jing Liang,
and Honggang Zhang. 2021. {``An Efficient Pruning Scheme of Deep Neural
Networks for Internet of Things Applications.''} \emph{EURASIP Journal
on Advances in Signal Processing} 2021 (1): 31.
\url{https://doi.org/10.1186/s13634-021-00744-4}.

\bibitem[\citeproctext]{ref-radosavovic2020designing}
Radosavovic, Ilija, Raj Prateek Kosaraju, Ross Girshick, Kaiming He, and
Piotr Dollar. 2020. {``Designing Network Design Spaces.''} In \emph{2020
IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
10428--36. IEEE. \url{https://doi.org/10.1109/cvpr42600.2020.01044}.

\bibitem[\citeproctext]{ref-Rastegari2016}
Rastegari, Mohammad, Vicente Ordonez, Joseph Redmon, and Ali Farhadi.
2016. {``XNOR-Net: ImageNet Classification Using Binary Convolutional
Neural Networks.''} In \emph{Computer Vision -- ECCV 2016}, 525--42.
Springer International Publishing.
\url{https://doi.org/10.1007/978-3-319-46493-0/_32}.

\bibitem[\citeproctext]{ref-xinyu}
Richter, Joel D., and Xinyu Zhao. 2021. {``The Molecular Biology of
FMRP: New Insights into Fragile x Syndrome.''} \emph{Nature Reviews
Neuroscience} 22 (4): 209--22.
\url{https://doi.org/10.1038/s41583-021-00432-0}.

\bibitem[\citeproctext]{ref-sabour2017dynamic}
Sabour, Sara, Nicholas Frosst, and Geoffrey E Hinton. 2017. {``Dynamic
Routing Between Capsules.''} In \emph{Advances in Neural Information
Processing Systems}. Vol. 30.

\bibitem[\citeproctext]{ref-sanh2019distilbert}
Sanh, Victor, Lysandre Debut, Julien Chaumond, and Thomas Wolf. 2019.
{``DistilBERT, a Distilled Version of BERT: Smaller, Faster, Cheaper and
Lighter.''} \emph{arXiv Preprint arXiv:1910.01108}, October.
\url{http://arxiv.org/abs/1910.01108v4}.

\bibitem[\citeproctext]{ref-scardapane2020should}
Scardapane, Simone, Ye Wang, and Massimo Panella. 2020. {``Why Should i
Trust You? A Survey of Explainability of Machine Learning for
Healthcare.''} \emph{Pattern Recognition Letters} 140: 47--57.

\bibitem[\citeproctext]{ref-shazeer2017outrageously}
Shazeer, Noam, Azalia Mirhoseini, Piotr Maziarz, et al. 2017.
{``Outrageously Large Neural Networks: The Sparsely-Gated
Mixture-of-Experts Layer.''} In \emph{International Conference on
Learning Representations}.

\bibitem[\citeproctext]{ref-sheng2019qbert}
Shen, Sheng, Zhen Dong, Jiayu Ye, Linjian Ma, Zhewei Yao, Amir Gholami,
Michael W. Mahoney, and Kurt Keutzer. 2019. {``Q-BERT: Hessian Based
Ultra Low Precision Quantization of BERT.''} \emph{Proceedings of the
AAAI Conference on Artificial Intelligence} 34 (05): 8815--21.
\url{https://doi.org/10.1609/aaai.v34i05.6409}.

\bibitem[\citeproctext]{ref-tan2019efficientnet}
Tan, Mingxing, and Quoc V Le. 2019a. {``EfficientNet: Rethinking Model
Scaling for Convolutional Neural Networks.''} In \emph{International
Conference on Machine Learning (ICML)}, 6105--14.

\bibitem[\citeproctext]{ref-Tan2019}
Tan, Mingxing, and Quoc V. Le. 2019b. {``EfficientNet: Rethinking Model
Scaling for Convolutional Neural Networks.''} In \emph{International
Conference on Machine Learning}.

\bibitem[\citeproctext]{ref-teerapittayanon2016branchynet}
Teerapittayanon, Surat, Bradley McDanel, and H. T. Kung. 2017.
{``BranchyNet: Fast Inference via Early Exiting from Deep Neural
Networks.''} \emph{arXiv Preprint arXiv:1709.01686}, September,
2464--69. \url{https://doi.org/10.1109/icpr.2016.7900006}.

\bibitem[\citeproctext]{ref-Umuroglu2017}
Umuroglu, Yaman, Nicholas J. Fraser, Giulio Gambardella, Michaela Blott,
Philip Leong, Magnus Jahre, and Kees Vissers. 2017. {``FINN: A Framework
for Fast, Scalable Binarized Neural Network Inference.''} In
\emph{Proceedings of the 2017 ACM/SIGDA International Symposium on
Field-Programmable Gate Arrays}, 65--74. ACM.
\url{https://doi.org/10.1145/3020078.3021744}.

\bibitem[\citeproctext]{ref-wang2018skipnet}
Wang, Xin, Fisher Yu, Zi-Yi Dou, Trevor Darrell, and Joseph E. Gonzalez.
2018. {``SkipNet: Learning Dynamic Routing in Convolutional Networks.''}
In \emph{Computer Vision -- ECCV 2018}, 420--36. Springer; Springer
International Publishing.
\url{https://doi.org/10.1007/978-3-030-01261-8/_25}.

\bibitem[\citeproctext]{ref-wang2019benchmarking}
Wang, Yu Emma, Gu-Yeon Wei, and David Brooks. 2019. {``Benchmarking TPU,
GPU, and CPU Platforms for Deep Learning.''} \emph{arXiv Preprint
arXiv:1907.10701}, July. \url{http://arxiv.org/abs/1907.10701v4}.

\bibitem[\citeproctext]{ref-wu2019fbnet}
Wu, Bichen, Kurt Keutzer, Xiaoliang Dai, Peizhao Zhang, Yanghan Wang,
Fei Sun, Yiming Wu, Yuandong Tian, Peter Vajda, and Yangqing Jia. 2019.
{``FBNet: Hardware-Aware Efficient ConvNet Design via Differentiable
Neural Architecture Search.''} In \emph{2019 IEEE/CVF Conference on
Computer Vision and Pattern Recognition (CVPR)}, 10726--34. IEEE.
\url{https://doi.org/10.1109/cvpr.2019.01099}.

\bibitem[\citeproctext]{ref-wu2020integer}
Wu, Hao, Patrick Judd, Xiaojie Zhang, Mikhail Isaev, and Paulius
Micikevicius. 2020. {``Integer Quantization for Deep Learning Inference:
Principles and Empirical Evaluation.''} \emph{arXiv Preprint
arXiv:2004.09602} abs/2004.09602 (April).
\url{http://arxiv.org/abs/2004.09602v1}.

\bibitem[\citeproctext]{ref-wu2019fast}
Wu, Jian, Hao Cheng, and Yifan Zhang. 2019. {``Fast Neural Networks:
Efficient and Adaptive Computation for Inference.''} In \emph{Advances
in Neural Information Processing Systems}.

\bibitem[\citeproctext]{ref-Wu2016}
Wu, Jiaxiang, Cong Leng, Yuhang Wang, Qinghao Hu, and Jian Cheng. 2016.
{``Quantized Convolutional Neural Networks for Mobile Devices.''} In
\emph{2016 IEEE Conference on Computer Vision and Pattern Recognition
(CVPR)}, 4820--28. IEEE. \url{https://doi.org/10.1109/cvpr.2016.521}.

\bibitem[\citeproctext]{ref-xin-etal-2021-berxit}
Xin, Ji, Raphael Tang, Yaoliang Yu, and Jimmy Lin. 2021. {``BERxiT:
Early Exiting for BERT with Better Fine-Tuning and Extension to
Regression.''} In \emph{Proceedings of the 16th Conference of the
European Chapter of the Association for Computational Linguistics: Main
Volume}, edited by Paola Merlo, Jorg Tiedemann, and Reut Tsarfaty,
91--104. Online: Association for Computational Linguistics.
\url{https://doi.org/10.18653/v1/2021.eacl-main.8}.

\bibitem[\citeproctext]{ref-yang2020resolution}
Yang, Le, Yizeng Han, Xi Chen, Shiji Song, Jifeng Dai, and Gao Huang.
2020. {``Resolution Adaptive Networks for Efficient Inference.''} In
\emph{2020 IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR)}, 2366--75. IEEE.
\url{https://doi.org/10.1109/cvpr42600.2020.00244}.

\bibitem[\citeproctext]{ref-yao2021hawq}
Yao, Zhewei, Amir Gholami, Sheng Shen, Kurt Keutzer, and Michael W.
Mahoney. 2021. {``HAWQ-V3: Dyadic Neural Network Quantization.''} In
\emph{Proceedings of the 38th International Conference on Machine
Learning (ICML)}, 11875--86. PMLR.

\bibitem[\citeproctext]{ref-yu2023efficient}
Yu, Jun, Peng Li, and Zhenhua Wang. 2023. {``Efficient Early Exiting
Strategies for Neural Network Acceleration.''} \emph{IEEE Transactions
on Neural Networks and Learning Systems}.

\bibitem[\citeproctext]{ref-zhang2021learning}
Zhang, Yi, Jianlei Yang, Linghao Song, Yiyu Shi, Yu Wang, and Yuan Xie.
2021. {``Learning-Based Efficient Sparsity and Quantization for Neural
Network Compression.''} \emph{IEEE Transactions on Neural Networks and
Learning Systems} 32 (9): 3980--94.

\bibitem[\citeproctext]{ref-zhou2021learningnmfinegrainedstructured}
Zhou, Aojun, Yukun Ma, Junnan Zhu, Jianbo Liu, Zhijie Zhang, Kun Yuan,
Wenxiu Sun, and Hongsheng Li. 2021. {``Learning n:m Fine-Grained
Structured Sparse Neural Networks from Scratch,''} February.
\url{http://arxiv.org/abs/2102.04010v2}.

\bibitem[\citeproctext]{ref-Zhu2017}
Zhu, Chenzhuo, Song Han, Huizi Mao, and William J. Dally. 2017.
{``Trained Ternary Quantization.''} \emph{International Conference on
Learning Representations (ICLR)}.

\bibitem[\citeproctext]{ref-zoph2017neural}
Zoph, Barret, and Quoc V Le. 2017a. {``Neural Architecture Search with
Reinforcement Learning.''} In \emph{International Conference on Learning
Representations (ICLR)}.

\bibitem[\citeproctext]{ref-Zoph2017}
Zoph, Barret, and Quoc V. Le. 2017b. {``Neural Architecture Search with
Reinforcement Learning.''} In \emph{International Conference on Learning
Representations}.

\end{CSLReferences}


\backmatter


\end{document}
