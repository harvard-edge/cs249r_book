% Options for packages loaded elsewhere
% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode,linktoc=all,pdfwindowui,pdfpagemode=FullScreen,pdfpagelayout=TwoPageRight}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  9pt,
  letterpaper,
  abstract,
  titlepage]{scrbook}
\usepackage{xcolor}
\usepackage{amsmath,amssymb}
\setcounter{secnumdepth}{3}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
% Make \paragraph and \subparagraph free-standing
\makeatletter
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}{
    \@ifstar
      \xxxParagraphStar
      \xxxParagraphNoStar
  }
  \newcommand{\xxxParagraphStar}[1]{\oldparagraph*{#1}\mbox{}}
  \newcommand{\xxxParagraphNoStar}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}{
    \@ifstar
      \xxxSubParagraphStar
      \xxxSubParagraphNoStar
  }
  \newcommand{\xxxSubParagraphStar}[1]{\oldsubparagraph*{#1}\mbox{}}
  \newcommand{\xxxSubParagraphNoStar}[1]{\oldsubparagraph{#1}\mbox{}}
\fi
\makeatother


\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\newsavebox\pandoc@box
\newcommand*\pandocbounded[1]{% scales image to fit in text height/width
  \sbox\pandoc@box{#1}%
  \Gscale@div\@tempa{\textheight}{\dimexpr\ht\pandoc@box+\dp\pandoc@box\relax}%
  \Gscale@div\@tempb{\linewidth}{\wd\pandoc@box}%
  \ifdim\@tempb\p@<\@tempa\p@\let\@tempa\@tempb\fi% select the smaller of both
  \ifdim\@tempa\p@<\p@\scalebox{\@tempa}{\usebox\pandoc@box}%
  \else\usebox{\pandoc@box}%
  \fi%
}
% Set default figure placement to htbp
\def\fps@figure{htbp}
\makeatother

% =============================================================================
% LATEX HEADER CONFIGURATION FOR MLSYSBOOK PDF
% =============================================================================
% This file contains all LaTeX package imports, custom commands, and styling
% definitions for the PDF output of the Machine Learning Systems textbook.
%
% Key Features:
% - Harvard crimson branding throughout
% - Custom part/chapter/section styling
% - Professional table formatting with colored headers
% - Margin notes with custom styling
% - TikZ-based part dividers
% - Page numbering (Roman for frontmatter, Arabic for mainmatter)
%
% Note: This file is included via _quarto-pdf.yml and affects PDF output only.
% HTML/EPUB styling is handled separately via CSS files.
% =============================================================================

% =============================================================================
% PACKAGE IMPORTS
% =============================================================================

% Layout and positioning
% \usepackage[outercaption, ragged]{sidecap}  % Commented out to make figure captions inline instead of in margin
\usepackage{adjustbox}      % Adjusting box dimensions
\usepackage{afterpage}      % Execute commands after page break
\usepackage{morefloats}     % Increase number of floats
\usepackage{array}          % Enhanced table column formatting
\usepackage{atbegshi}       % Insert content at page beginning
%\usepackage{changepage}     % Change page dimensions mid-document
\usepackage{emptypage}      % Clear headers/footers on empty pages

% Language and text
\usepackage[english]{babel} % English language support
\usepackage{microtype}      % Improved typography and hyphenation

% Captions and floats
\usepackage{caption}
% Caption styling configuration
%\captionsetup[table]{belowskip=5pt}
\captionsetup{format=plain}
\DeclareCaptionLabelFormat{mylabel}{#1
#2:\hspace{1.0ex}}
\DeclareCaptionFont{ninept}{\fontsize{7pt}{8}\selectfont #1}

% Figure captions: Small font, bold label, ragged right
\captionsetup[figure]{labelfont={bf,ninept},labelsep=space,
belowskip=2pt,aboveskip=6pt,labelformat=mylabel,
justification=raggedright,singlelinecheck=false,font={ninept}}

% Table captions: Small font, bold label, ragged right
\captionsetup[table]{belowskip=6pt,labelfont={bf,ninept},labelsep=none,
labelformat=mylabel,justification=raggedright,singlelinecheck=false,font={ninept}}

% Typography fine-tuning
\emergencystretch=5pt       % Allow extra stretch to avoid overfull boxes

% Utility packages
\usepackage{etoolbox}       % For patching commands and environments

% Page layout and headers
\usepackage{fancyhdr}       % Custom headers and footers
\usepackage{geometry}       % Page dimensions and margins

% Graphics and figures
\usepackage{graphicx}       % Include graphics
\usepackage{float}          % Improved float placement
\usepackage[skins,breakable]{tcolorbox} % Coloured and framed text boxes
\tcbset{before upper=\setlength{\parskip}{3pt}}

% Tables
\usepackage{longtable}      % Multi-page tables

% Fonts and typography
\usepackage{fontspec}       % Font selection for LuaLaTeX
\usepackage{mathptmx}       % Times-like math fonts
\usepackage{newpxtext}      % Palatino-like font for body text

% Colors and visual elements
\usepackage[dvipsnames]{xcolor}  % Extended color support
\usepackage{tikz}           % Programmatic graphics
\usetikzlibrary{positioning}
\usetikzlibrary{calc}
\usepackage{tikzpagenodes}  % TikZ positioning relative to page

% Code listings
\usepackage{listings}       % Code highlighting

% Hyperlinks
\usepackage{hyperref}       % Clickable links in PDF

% Conditional logic
\usepackage{ifthen}         % If-then-else commands

% Math symbols
\usepackage{amsmath}        % AMS math extensions
\usepackage{amssymb}        % AMS math symbols
\usepackage{latexsym}       % Additional LaTeX symbols
\usepackage{pifont}         % Zapf Dingbats symbols
\providecommand{\blacklozenge}{\ding{117}}  % Black diamond symbol

% Lists
\usepackage{enumitem}       % Customizable lists

% Margin notes and sidenotes
\usepackage{marginfix}      % Fixes margin note overflow
\usepackage{marginnote}     % Margin notes
\usepackage{sidenotes}      % Academic-style sidenotes
\renewcommand\raggedrightmarginnote{\sloppy}
\renewcommand\raggedleftmarginnote{\sloppy}

% Typography improvements
\usepackage{ragged2e}       % Better ragged text
\usepackage[all]{nowidow}   % Prevent widows and orphans
\usepackage{needspace}      % Ensure minimum space on page

% Section formatting
\usepackage[explicit]{titlesec}  % Custom section titles
\usepackage{tocloft}        % Table of contents formatting

% QR codes and icons
\usepackage{fontawesome5}   % Font Awesome icons
\usepackage{qrcode}         % QR code generation
\qrset{link, height=15mm}

% =============================================================================
% FLOAT CONFIGURATION
% =============================================================================
% Allow more floats per page to handle figure-heavy chapters
\extrafloats{200}
\setcounter{topnumber}{12}       % Max floats at top of page
\setcounter{bottomnumber}{12}    % Max floats at bottom of page
\setcounter{totalnumber}{24}     % Max floats per page
\setcounter{dbltopnumber}{8}     % Max floats at top of two-column page
\renewcommand{\topfraction}{.95}  % Max fraction of page for top floats
\renewcommand{\bottomfraction}{.95}
\renewcommand{\textfraction}{.05}  % Min fraction of page for text
\renewcommand{\floatpagefraction}{.7}  % Min fraction of float page
\renewcommand{\dbltopfraction}{.95}

% Prevent "Float(s) lost" errors by flushing floats more aggressively
\usepackage{placeins}  % Provides \FloatBarrier

% =============================================================================
% COLOR DEFINITIONS
% =============================================================================
% Harvard crimson - primary brand color used throughout
\definecolor{crimson}{HTML}{A51C30}

% Quiz element colors
\definecolor{quiz-question-color1}{RGB}{225,243,248}  % Light blue background
\definecolor{quiz-question-color2}{RGB}{17,158,199}   % Blue border
\definecolor{quiz-answer-color1}{RGB}{250,234,241}    % Light pink background
\definecolor{quiz-answer-color2}{RGB}{152,14,90}      % Magenta border

% =============================================================================
% LIST FORMATTING
% =============================================================================
% Tighter list spacing for academic style
\def\tightlist{}
\setlist{itemsep=1pt, parsep=1pt, topsep=0pt,after={\vspace{0.3\baselineskip}}}
\let\tightlist\relax

\makeatletter
\@ifpackageloaded{framed}{}{\usepackage{framed}}
\@ifpackageloaded{fancyvrb}{}{\usepackage{fancyvrb}}
\makeatother

\makeatletter
%New float "codelisting" has been updated
\AtBeginDocument{%
\floatstyle{ruled}
\newfloat{codelisting}{!htb}{lop}
\floatname{codelisting}{Listing}
\floatplacement{codelisting}{!htb}
\captionsetup[codelisting]{labelfont={bf,ninept},labelformat=mylabel,
  singlelinecheck=false,width=\linewidth,labelsep=none,font={ninept}}%
\renewenvironment{snugshade}{%
   \def\OuterFrameSep{3pt}%
   \def\FrameCommand{\fboxsep=5pt\colorbox{shadecolor}}%
   \MakeFramed{\advance\hsize-\width\FrameRestore}%
   \leftskip 0.5em \rightskip 0.5em%
   \small% decrease font size
   }{\endMakeFramed}%
}
\makeatother

%The space before and after the verbatim environment "Highlighting" has been reduced
\fvset{listparameters=\setlength{\topsep}{0pt}\setlength{\partopsep}{0pt}}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{framesep=0mm,commandchars=\\\{\}}

\makeatletter
\renewcommand\fs@ruled{\def\@fs@cfont{\bfseries}\let\@fs@capt\floatc@ruled
\def\@fs@pre{\hrule height.8pt depth0pt \kern2pt}%
\def\@fs@post{\kern2pt\hrule\relax}%
\def\@fs@mid{\kern2pt\hrule\kern1pt}%space between float and caption
\let\@fs@iftopcapt\iftrue}
\makeatother


% =============================================================================
% HYPHENATION RULES
% =============================================================================
% Explicit hyphenation points for technical terms to avoid bad breaks
\hyphenation{
  light-weight
  light-weight-ed
  de-vel-op-ment
  un-der-stand-ing
  mod-els
  prin-ci-ples
  ex-per-tise
  com-pli-cat-ed
  blue-print
  per‧for‧mance
  com-mu-ni-ca-tion
  par-a-digms
  hy-per-ten-sion
  a-chieved
}

% =============================================================================
% CODE LISTING CONFIGURATION
% =============================================================================
% Settings for code blocks using listings package
\lstset{
breaklines=true,              % Automatic line wrapping
breakatwhitespace=true,       % Break at whitespace only
basicstyle=\ttfamily,         % Monospace font
frame=none,                   % No frame around code
keepspaces=true,              % Preserve spaces
showspaces=false,             % Don't show space characters
showtabs=false,               % Don't show tab characters
columns=flexible,             % Flexible column width
belowskip=0pt,               % Minimal spacing
aboveskip=0pt
}

% =============================================================================
% PAGE GEOMETRY
% =============================================================================
% MIT Press trim size: 7" x 10" (per publisher specifications)
% This is a standard academic textbook format providing good readability
% for technical content with figures and code blocks.
% Wide outer margin accommodates sidenotes/margin notes.
\geometry{
  paperwidth=7in,
  paperheight=10in,
  top=0.875in,
  bottom=0.875in,
  inner=0.875in,              % Inner margin (binding side)
  outer=1.75in,               % Outer margin (includes space for sidenotes)
  footskip=30pt,
  marginparwidth=1.25in,      % Width for margin notes
  twoside                     % Different left/right pages
}

% =============================================================================
% SIDENOTE STYLING
% =============================================================================
% Custom sidenote design with crimson vertical bar
\renewcommand{\thefootnote}{\textcolor{crimson}{\arabic{footnote}}}

% Save original sidenote command
\makeatletter
\@ifundefined{oldsidenote}{
  \let\oldsidenote\sidenote%
}{}
\makeatother

% Redefine sidenote with vertical crimson bar
\renewcommand{\sidenote}[1]{%
  \oldsidenote{%
    \noindent
    \color{crimson!100}                        % Crimson vertical line
    \raisebox{0em}{%
      \rule{0.5pt}{1.5em}                      % Thin vertical line
    }
    \hspace{0.3em}                             % Space after line
    \color{black}                              % Reset text color
    \footnotesize #1                           % Sidenote content
  }%
}

% =============================================================================
% FLOAT HANDLING
% =============================================================================
% Patch LaTeX's output routine to handle float overflow gracefully
% The "Float(s) lost" error occurs in \@doclearpage when \@currlist is not empty
% This patch silently clears pending floats that can't be placed
\makeatletter
\let\orig@doclearpage\@doclearpage
\def\@doclearpage{%
  \ifx\@currlist\@empty\else
    \global\let\@currlist\@empty
    \typeout{Warning: Floats cleared to prevent overflow}%
  \fi
  \orig@doclearpage
}
\makeatother

% Additional safety for structural commands
\let\originalbackmatter\backmatter
\renewcommand{\backmatter}{%
  \clearpage%
  \originalbackmatter%
}

\let\originalfrontmatter\frontmatter
\renewcommand{\frontmatter}{%
  \clearpage%
  \originalfrontmatter%
}

\let\originalmainmatter\mainmatter
\renewcommand{\mainmatter}{%
  \clearpage%
  \originalmainmatter%
}

% =============================================================================
% PAGE HEADERS AND FOOTERS
% =============================================================================
% Ensure chapters use fancy page style (not plain)
\patchcmd{\chapter}{\thispagestyle{plain}}{\thispagestyle{fancy}}{}{}

% Main page style with crimson headers
\pagestyle{fancy}
\fancyhf{}                                              % Clear all
\fancyhead[LE]{\small\color{crimson}\nouppercase{\rightmark}}  % Left even: section
\fancyhead[RO]{\color{crimson}\thepage}                 % Right odd: page number
\fancyhead[LO]{\small\color{crimson}\nouppercase{\leftmark}}   % Left odd: chapter
\fancyhead[RE]{\color{crimson}\thepage}                 % Right even: page number
\renewcommand{\headrulewidth}{0.4pt}                    % Thin header line
\renewcommand{\footrulewidth}{0pt}                      % No footer line

% Plain page style (for chapter openings)
\fancypagestyle{plain}{
  \fancyhf{}
  \fancyfoot[C]{\color{crimson}\thepage}                % Centered page number
  \renewcommand{\headrulewidth}{0pt}
  \renewcommand{\footrulewidth}{0pt}
}

% =============================================================================
% KOMA-SCRIPT FONT ADJUSTMENTS
% =============================================================================
% Apply crimson color to all heading levels
\addtokomafont{disposition}{\rmfamily\color{crimson}}
\addtokomafont{chapter}{\color{crimson}}
\addtokomafont{section}{\color{crimson}}
\addtokomafont{subsection}{\color{crimson}}

% =============================================================================
% ABSTRACT ENVIRONMENT
% =============================================================================
\newenvironment{abstract}{
  \chapter*{\abstractname}
  \addcontentsline{toc}{chapter}{\abstractname}
  \small
}{
  \clearpage
}

% =============================================================================
% HYPERLINK CONFIGURATION
% =============================================================================
% Crimson-colored links throughout, two-page PDF layout
\hypersetup{
  linkcolor=crimson,
  citecolor=crimson,
  urlcolor=crimson,
  pdfpagelayout=TwoPageRight,   % Two-page spread view
  pdfstartview=Fit               % Initial zoom fits page
}

% =============================================================================
% PART SUMMARY SYSTEM
% =============================================================================
% Allows adding descriptive text below part titles
\newcommand{\partsummary}{}     % Empty by default
\newif\ifhaspartsummary%
\haspartsummaryfalse%

\newcommand{\setpartsummary}[1]{%
  \renewcommand{\partsummary}{#1}%
  \haspartsummarytrue%
}

% Additional colors for part page backgrounds
\definecolor{BrownLL}{RGB}{233,222,220}
\definecolor{BlueDD}{RGB}{62,100,125}
\colorlet{BlueDD}{magenta}

% ===============================================================================
% PART STYLING SYSTEM
% ===============================================================================
%
% This system provides three distinct visual styles for book organization:
%
% 1. NUMBERED PARTS (\part{title}) - For main book sections
%    - Roman numerals (I, II, III, etc.) in top right corner
%    - Crimson title with horizontal lines above/below
%    - "Part I" label in sidebar
%    - Used for: foundations, principles, optimization, deployment, etc.
%
% 2. UNNUMBERED PARTS (\part*{title}) - For special sections like "Labs"
%    - Division-style geometric background (left side)
%    - No Roman numerals
%    - Used for: labs section
%
% 3. DIVISIONS (\division{title}) - For major book divisions
%    - Clean geometric background with centered title
%    - Used for: frontmatter, main_content, backmatter
%
% The Lua filter (inject-parts.lua) automatically routes parts by {key:xxx} commands
% to the appropriate LaTeX command based on the key name.
% ===============================================================================

% NUMBERED PARTS: Roman numeral styling for main book sections
\titleformat{\part}[display]
{\thispagestyle{empty}}{}{20pt}{
\begin{tikzpicture}[remember picture,overlay]
%%%
%%
\node[crimson,align=flush right,
inner sep=0,outer sep=0mm,draw=none,%
anchor=east,minimum height=31mm, text width=1.2\textwidth,
yshift=-30mm,font={%
\fontsize{98pt}{104}\selectfont\bfseries}]  (BG) at (current page text area.north east){\thepart};
%
\node[black,inner sep=0mm,draw=none,
anchor=mid,text width=1.2\textwidth,
 minimum height=35mm, align=right,
node distance=7mm,below=of BG,
font={\fontsize{30pt}{34}\selectfont}]
(BGG)  {\hyphenchar\font=-1 \color{black}\MakeUppercase {#1}};
\draw [crimson,line width=3pt] ([yshift=0mm]BGG.north west) -- ([yshift=0mm]BGG.north east);
\draw [crimson,line width=2pt] ([yshift=0mm]BGG.south west) -- ([yshift=0mm]BGG.south east);
%
\node[fill=crimson,text=white,rotate=90,%
anchor=south west,minimum height=15mm,
minimum width=40mm,font={%
\fontsize{20pt}{20}\selectfont\bfseries}](BP)  at
(current page text area.south east)
{{\sffamily Part}~\thepart};
%
\path[red](BP.north west)-|coordinate(PS)(BGG.south west);
%
% Part summary box commented out for cleaner design
% \ifhaspartsummary
% \node[inner sep=4pt,text width=0.7\textwidth,draw=none,fill=BrownLL!40,
% align=justify,font={\fontsize{9pt}{12}\selectfont},anchor=south west]
% at (PS) {\partsummary};
% \fi
\end{tikzpicture}
}[]

\renewcommand{\thepart}{\Roman{part}}

% UNNUMBERED PARTS: Division-style background for special sections
\titleformat{name=\part,numberless}[display]
{\thispagestyle{empty}}{}{20pt}{
\begin{tikzpicture}[remember picture,overlay]
%%%
\coordinate(S1)at([yshift=-200mm]current page.north west);
\draw[draw=none,fill=BlueDD!7](S1)--++(45:16)coordinate(S2)-
|(S2|-current page.north west)--(current page.north west)coordinate(S3)--(S1);
%
\coordinate(E1)at([yshift=-98mm]current page.north west);
\draw[draw=none,fill=BlueDD!15](E1)--(current page.north west)coordinate(E2)
--++(0:98mm)coordinate(E3)--(E1);
%
\coordinate(D1)at([yshift=15mm]current page.south west);
\draw[draw=none,fill=BlueDD!40,opacity=0.5](D1)--++(45:5.5)coordinate(D2)
-|(D2|-current page.north west)--(current page.north west)coordinate(D3)--(D1);
%%%%
\path[red](S2)-|(S2-|current page.east)coordinate(SS2);
%PART
\node[crimson,align=flush right,inner sep=0,outer sep=0mm,draw=none,anchor=south,
font={\fontsize{48pt}{48}\selectfont\bfseries}]  (BG) at ($(S2)!0.5!(SS2)$){\hphantom{Part}};
%%%
\path[green]([yshift=15mm]D2)-|coordinate(TPD)(BG.south east);
\node[inner sep=0mm,draw=none,anchor=south east,%text width=0.9\textwidth,
align=right,font={\fontsize{40pt}{40}\selectfont}]
(BGG) at (TPD)  {\color{crimson}\MakeUppercase {#1}};%\MakeUppercase {}
\end{tikzpicture}
}

% Define \numberedpart command for numbered parts
\newcommand{\numberedpart}[1]{%
\FloatBarrier%  % Flush all pending floats before part break
\clearpage
\thispagestyle{empty}
\stepcounter{part}%
\begin{tikzpicture}[remember picture,overlay]
%%%
%%
\node[crimson,align=flush right,
inner sep=0,outer sep=0mm,draw=none,%
anchor=east,minimum height=31mm, text width=1.2\textwidth,
yshift=-30mm,font={%
\fontsize{98pt}{104}\selectfont\bfseries}]  (BG) at (current page text area.north east){\thepart};
%
\node[black,inner sep=0mm,draw=none,
anchor=mid,text width=1.2\textwidth,
 minimum height=35mm, align=right,
node distance=7mm,below=of BG,
font={\fontsize{30pt}{34}\selectfont}]
(BGG)  {\hyphenchar\font=-1 \color{black}\MakeUppercase {#1}};
\draw [crimson,line width=3pt] ([yshift=0mm]BGG.north west) -- ([yshift=0mm]BGG.north east);
\draw [crimson,line width=2pt] ([yshift=0mm]BGG.south west) -- ([yshift=0mm]BGG.south east);
%
\node[fill=crimson,text=white,rotate=90,%
anchor=south west,minimum height=15mm,
minimum width=40mm,font={%
\fontsize{20pt}{20}\selectfont\bfseries}](BP)  at
(current page text area.south east)
{{\sffamily Part}~\thepart};
%
\path[red](BP.north west)-|coordinate(PS)(BGG.south west);
%
% Part summary box commented out for cleaner design
% \ifhaspartsummary
% \node[inner sep=4pt,text width=0.7\textwidth,draw=none,fill=BrownLL!40,
% align=justify,font={\fontsize{9pt}{12}\selectfont},anchor=south west]
% at (PS) {\partsummary};
% \fi
\end{tikzpicture}
\clearpage
}



% DIVISIONS: Clean geometric styling with subtle tech elements
% Used for frontmatter, main_content, and backmatter divisions
\newcommand{\division}[1]{%
\FloatBarrier%  % Flush all pending floats before division break
\clearpage
\thispagestyle{empty}
\begin{tikzpicture}[remember picture,overlay]

% Clean geometric background (original design)
\coordinate(S1)at([yshift=-200mm]current page.north west);
\draw[draw=none,fill=BlueDD!7](S1)--++(45:16)coordinate(S2)-
|(S2|-current page.north west)--(current page.north west)coordinate(S3)--(S1);

\coordinate(E1)at([yshift=-98mm]current page.north west);
\draw[draw=none,fill=BlueDD!15](E1)--(current page.north west)coordinate(E2)
--++(0:98mm)coordinate(E3)--(E1);

\coordinate(D1)at([yshift=15mm]current page.south west);
\draw[draw=none,fill=BlueDD!40,opacity=0.5](D1)--++(45:5.5)coordinate(D2)
-|(D2|-current page.north west)--(current page.north west)coordinate(D3)--(D1);

% Subtle tech elements - positioned in white areas for better visibility
% Upper right white area - more visible
\draw[crimson!40, line width=0.8pt] ([xshift=140mm,yshift=-60mm]current page.north west) -- ++(40mm,0);
\draw[crimson!40, line width=0.8pt] ([xshift=150mm,yshift=-70mm]current page.north west) -- ++(30mm,0);
\draw[crimson!35, line width=0.7pt] ([xshift=160mm,yshift=-60mm]current page.north west) -- ++(0,-15mm);
\draw[crimson!35, line width=0.7pt] ([xshift=170mm,yshift=-70mm]current page.north west) -- ++(0,10mm);

% Circuit nodes - upper right
\fill[crimson!50] ([xshift=160mm,yshift=-60mm]current page.north west) circle (1.5mm);
\fill[white] ([xshift=160mm,yshift=-60mm]current page.north west) circle (0.8mm);
\fill[crimson!50] ([xshift=170mm,yshift=-70mm]current page.north west) circle (1.3mm);
\fill[white] ([xshift=170mm,yshift=-70mm]current page.north west) circle (0.6mm);

% Lower right white area - enhanced visibility
\draw[crimson!45, line width=0.9pt] ([xshift=140mm,yshift=-190mm]current page.north west) -- ++(45mm,0);
\draw[crimson!45, line width=0.9pt] ([xshift=150mm,yshift=-200mm]current page.north west) -- ++(35mm,0);
\draw[crimson!40, line width=0.8pt] ([xshift=160mm,yshift=-190mm]current page.north west) -- ++(0,-20mm);
\draw[crimson!40, line width=0.8pt] ([xshift=170mm,yshift=-200mm]current page.north west) -- ++(0,15mm);

% Additional connecting lines in lower right
\draw[crimson!35, line width=0.7pt] ([xshift=130mm,yshift=-180mm]current page.north west) -- ++(25mm,0);
\draw[crimson!35, line width=0.7pt] ([xshift=145mm,yshift=-180mm]current page.north west) -- ++(0,-25mm);

% Circuit nodes - lower right (more prominent)
\fill[crimson!55] ([xshift=160mm,yshift=-190mm]current page.north west) circle (1.6mm);
\fill[white] ([xshift=160mm,yshift=-190mm]current page.north west) circle (0.9mm);
\fill[crimson!55] ([xshift=170mm,yshift=-200mm]current page.north west) circle (1.4mm);
\fill[white] ([xshift=170mm,yshift=-200mm]current page.north west) circle (0.7mm);
\fill[crimson!50] ([xshift=145mm,yshift=-180mm]current page.north west) circle (1.2mm);
\fill[white] ([xshift=145mm,yshift=-180mm]current page.north west) circle (0.6mm);

% Title positioned in center - clean and readable
\node[inner sep=0mm,draw=none,anchor=center,text width=0.8\textwidth,
align=center,font={\fontsize{40pt}{40}\selectfont}]
(BGG) at (current page.center)  {\color{crimson}\MakeUppercase {#1}};

\end{tikzpicture}
\clearpage
}

% LAB DIVISIONS: Circuit-style neural network design for lab sections
% Used specifically for lab platform sections (arduino, xiao, grove, etc.)
\newcommand{\labdivision}[1]{%
\FloatBarrier%  % Flush all pending floats before lab division break
\clearpage
\thispagestyle{empty}
\begin{tikzpicture}[remember picture,overlay]
% Circuit background with subtle gradient
\coordinate(S1)at([yshift=-200mm]current page.north west);
\draw[draw=none,fill=BlueDD!5](S1)--++(45:16)coordinate(S2)-
|(S2|-current page.north west)--(current page.north west)coordinate(S3)--(S1);

% TOP AREA: Circuit lines in upper white space
\draw[crimson!50, line width=1.5pt] ([xshift=30mm,yshift=-40mm]current page.north west) -- ++(60mm,0);
\draw[crimson!40, line width=1pt] ([xshift=120mm,yshift=-50mm]current page.north west) -- ++(50mm,0);
\draw[crimson!50, line width=1.5pt] ([xshift=40mm,yshift=-70mm]current page.north west) -- ++(40mm,0);

% Connecting lines in top area
\draw[crimson!30, line width=1pt] ([xshift=60mm,yshift=-40mm]current page.north west) -- ++(0,-20mm);
\draw[crimson!30, line width=1pt] ([xshift=145mm,yshift=-50mm]current page.north west) -- ++(0,10mm);

% Neural nodes in top area
\fill[crimson!70] ([xshift=60mm,yshift=-40mm]current page.north west) circle (2.5mm);
\fill[white] ([xshift=60mm,yshift=-40mm]current page.north west) circle (1.5mm);
\fill[crimson!60] ([xshift=145mm,yshift=-50mm]current page.north west) circle (2mm);
\fill[white] ([xshift=145mm,yshift=-50mm]current page.north west) circle (1mm);
\fill[crimson!80] ([xshift=80mm,yshift=-70mm]current page.north west) circle (2mm);
\fill[white] ([xshift=80mm,yshift=-70mm]current page.north west) circle (1mm);

% BOTTOM AREA: Circuit lines in lower white space
\draw[crimson!50, line width=1.5pt] ([xshift=20mm,yshift=-200mm]current page.north west) -- ++(70mm,0);
\draw[crimson!40, line width=1pt] ([xshift=110mm,yshift=-210mm]current page.north west) -- ++(60mm,0);
\draw[crimson!50, line width=1.5pt] ([xshift=35mm,yshift=-230mm]current page.north west) -- ++(45mm,0);

% Connecting lines in bottom area
\draw[crimson!30, line width=1pt] ([xshift=55mm,yshift=-200mm]current page.north west) -- ++(0,-20mm);
\draw[crimson!30, line width=1pt] ([xshift=140mm,yshift=-210mm]current page.north west) -- ++(0,15mm);

% Neural nodes in bottom area
\fill[crimson!70] ([xshift=55mm,yshift=-200mm]current page.north west) circle (2.5mm);
\fill[white] ([xshift=55mm,yshift=-200mm]current page.north west) circle (1.5mm);
\fill[crimson!60] ([xshift=140mm,yshift=-210mm]current page.north west) circle (2mm);
\fill[white] ([xshift=140mm,yshift=-210mm]current page.north west) circle (1mm);
\fill[crimson!80] ([xshift=80mm,yshift=-230mm]current page.north west) circle (2mm);
\fill[white] ([xshift=80mm,yshift=-230mm]current page.north west) circle (1mm);

% SIDE AREAS: Subtle circuit elements on left and right edges
\draw[crimson!30, line width=1pt] ([xshift=15mm,yshift=-120mm]current page.north west) -- ++(20mm,0);
\draw[crimson!30, line width=1pt] ([xshift=175mm,yshift=-130mm]current page.north west) -- ++(15mm,0);
\fill[crimson!50] ([xshift=25mm,yshift=-120mm]current page.north west) circle (1.5mm);
\fill[white] ([xshift=25mm,yshift=-120mm]current page.north west) circle (0.8mm);
\fill[crimson!50] ([xshift=185mm,yshift=-130mm]current page.north west) circle (1.5mm);
\fill[white] ([xshift=185mm,yshift=-130mm]current page.north west) circle (0.8mm);

% Title positioned in center - CLEAN AREA
\node[inner sep=0mm,draw=none,anchor=center,text width=0.8\textwidth,
align=center,font={\fontsize{44pt}{44}\selectfont\bfseries}]
(BGG) at (current page.center)  {\color{crimson}\MakeUppercase {#1}};

\end{tikzpicture}
\clearpage
}

% Define \lab command for lab styling (different visual treatment)
\newcommand{\lab}[1]{%
\begin{tikzpicture}[remember picture,overlay]
%%%
% Different background pattern for labs
\coordinate(S1)at([yshift=-200mm]current page.north west);
\draw[draw=none,fill=BlueDD!15](S1)--++(45:16)coordinate(S2)-
|(S2|-current page.north west)--(current page.north west)coordinate(S3)--(S1);
%
\coordinate(E1)at([yshift=-98mm]current page.north west);
\draw[draw=none,fill=BlueDD!25](E1)--(current page.north west)coordinate(E2)
--++(0:98mm)coordinate(E3)--(E1);
%
\coordinate(D1)at([yshift=15mm]current page.south west);
\draw[draw=none,fill=BlueDD!60,opacity=0.7](D1)--++(45:5.5)coordinate(D2)
-|(D2|-current page.north west)--(current page.north west)coordinate(D3)--(D1);
%%%%
\path[red](S2)-|(S2-|current page.east)coordinate(SS2);
%LAB - Different styling
\node[crimson,align=flush right,inner sep=0,outer sep=0mm,draw=none,anchor=south,
font={\fontsize{48pt}{48}\selectfont\bfseries}]  (BG) at ($(S2)!0.5!(SS2)$){\hphantom{Workshop}};
%%%
\path[green]([yshift=15mm]D2)-|coordinate(TPD)(BG.south east);
\node[inner sep=0mm,draw=none,anchor=south east,%text width=0.9\textwidth,
align=right,font={\fontsize{40pt}{40}\selectfont}]
(BGG) at (TPD)  {\color{crimson}\MakeUppercase {#1}};%\MakeUppercase {}
\end{tikzpicture}
\thispagestyle{empty}
\clearpage
}

% =============================================================================
% SECTION FORMATTING
% =============================================================================
% All section levels use crimson color and are ragged right

% Section (Large, bold, crimson)
\titleformat{\section}
  {\normalfont\Large\bfseries\color{crimson}\raggedright}
  {\thesection}
  {0.5em}
  {#1}
\titlespacing*{\section}{0pc}{14pt plus 4pt minus 4pt}{6pt plus 2pt minus 2pt}[0pc]

% Subsection (large, bold, crimson)
\titleformat{\subsection}
  {\normalfont\large\bfseries\color{crimson}\raggedright}
  {\thesubsection}
  {0.5em}
  {#1}
\titlespacing*{\subsection}{0pc}{12pt plus 4pt minus 4pt}{5pt plus 1pt minus 2pt}[0pc]

% Subsubsection (normal size, bold, crimson)
\titleformat{\subsubsection}
  {\normalfont\normalsize\bfseries\color{crimson}\raggedright}
  {\thesubsubsection}
  {0.5em}
  {#1}
\titlespacing*{\subsubsection}{0pc}{12pt plus 4pt minus 4pt}{5pt plus 1pt minus 2pt}[0pc]

% Paragraph (run-in, bold, crimson, ends with period)
\titleformat{\paragraph}[runin]
  {\normalfont\normalsize\bfseries\color{crimson}}
  {\theparagraph}
  {0.5em}
  {#1}
  [\textbf{.}]
  \titlespacing*{\paragraph}{0pc}{6pt plus 2pt minus 2pt}{0.5em}[0pc]

% Subparagraph (run-in, italic, crimson, ends with period)
\titleformat{\subparagraph}[runin]
  {\normalfont\normalsize\itshape\color{crimson}}
  {\thesubparagraph}
  {0.5em}
  {#1}
  [\textbf{.}]
  \titlespacing*{\subparagraph}{0pc}{6pt plus 2pt minus 2pt}{0.5em}[0pc]

% =============================================================================
% CHAPTER FORMATTING
% =============================================================================
% Numbered chapters: "Chapter X" prefix, huge crimson title
\titleformat{\chapter}[display]
  {\normalfont\huge\bfseries\color{crimson}}
  {\chaptername\ \thechapter}
  {20pt}
  {\Huge #1}
  []

% Unnumbered chapters: no prefix, huge crimson title
\titleformat{name=\chapter,numberless}
  {\normalfont\huge\bfseries\color{crimson}}
  {}
  {0pt}
  {\Huge #1}
  []

\renewcommand{\chaptername}{Chapter}
% =============================================================================
% TABLE OF CONTENTS FORMATTING
% =============================================================================
\setcounter{tocdepth}{2}                      % Show chapters, sections, subsections

% TOC spacing adjustments for number widths and indentation
\setlength{\cftchapnumwidth}{2em}             % Chapter number width
\setlength{\cftsecnumwidth}{2.75em}           % Section number width
\setlength{\cftsubsecnumwidth}{3.25em}        % Subsection number width
\setlength{\cftsubsubsecnumwidth}{4em}        % Subsubsection number width
\setlength{\cftsubsecindent}{4.25em}          % Subsection indent
\setlength{\cftsubsubsecindent}{7.5em}        % Subsubsection indent

% Chapter entries in TOC: bold crimson with "Chapter" prefix
\renewcommand{\cftchapfont}{\bfseries\color{crimson}}
\renewcommand{\cftchappresnum}{\color{crimson}Chapter~}

% Custom formatting for division entries (styled like parts)
\newcommand{\divisionchapter}[1]{%
  \addvspace{12pt}%
  \noindent\hfil\bfseries\color{crimson}#1\hfil\par%
  \addvspace{6pt}%
}

% Adjust TOC spacing for "Chapter" prefix
\newlength{\xtraspace}
\settowidth{\xtraspace}{\cftchappresnum\cftchapaftersnum}
\addtolength{\cftchapnumwidth}{\xtraspace}

% Unnumbered chapters with TOC entry
\newcommand{\likechapter}[1]{%
    \chapter*{#1}
    \addcontentsline{toc}{chapter}{\textcolor{crimson}{#1}}
}

% =============================================================================
% PAGE NUMBERING SYSTEM
% =============================================================================
% Implements traditional book numbering:
% - Roman numerals (i, ii, iii...) for frontmatter
% - Arabic numerals (1, 2, 3...) for mainmatter
% Automatically switches at first numbered chapter
\makeatletter
\newif\if@firstnumbered%
\@firstnumberedtrue%
\newif\if@firstunnumbered%
\@firstunnumberedtrue%

\newcounter{lastRomanPage}
\setcounter{lastRomanPage}{1}

% Start document with Roman numerals (frontmatter)
\AtBeginDocument{
  \pagenumbering{roman}
  \renewcommand{\thepage}{\roman{page}}
}

% Intercept chapter command
\let\old@chapter\chapter%
\renewcommand{\chapter}{%
  \@ifstar{\unnumbered@chapter}{\numbered@chapter}%
}

% Numbered chapters: switch to Arabic on first occurrence
\newcommand{\numbered@chapter}[1]{%
  \if@firstnumbered%
    \cleardoublepage%
    \setcounter{lastRomanPage}{\value{page}}%
    \pagenumbering{arabic}%
    \@firstnumberedfalse%
  \else
    \setcounter{page}{\value{page}}%
  \fi
  \setcounter{sidenote}{1}                    % Reset footnote counter per chapter
  \old@chapter{#1}%
}

% Unnumbered chapters: stay in Roman numerals
\newcommand{\unnumbered@chapter}[1]{%
  \if@firstunnumbered%
    \clearpage
    \setcounter{lastRomanPage}{\value{page}}%
    \pagenumbering{roman}%
    \@firstunnumberedfalse%
  \fi
  \setcounter{sidenote}{1}
  \old@chapter*{#1}%
}
\makeatother

% =============================================================================
% TABLE SIZING AND SPACING
% =============================================================================
% Make tables slightly smaller to fit more content
\AtBeginEnvironment{longtable}{\scriptsize}

% Increase vertical spacing in table cells (default is 1.0)
\renewcommand{\arraystretch}{1.3}

% Prefer placing figures and tables at the top of pages
\makeatletter
\renewcommand{\fps@figure}{t}  % Default placement: top of page
\renewcommand{\fps@table}{t}   % Default placement: top of page
\makeatother

% =============================================================================
% LONGTABLE PAGE BREAKING FIXES (Windows compatibility)
% =============================================================================
% Prevent "Infinite glue shrinkage" errors on Windows LaTeX builds
% by giving longtable more flexibility in page breaking

% Allow more flexible page breaking (vs strict \flushbottom)
\raggedbottom

% Process more rows before attempting page break (default is 20)
\setcounter{LTchunksize}{50}

% Add extra stretch for longtable environments specifically
\AtBeginEnvironment{longtable}{%
  \setlength{\emergencystretch}{3em}%
  \setlength{\parskip}{0pt plus 1pt}%
}

% =============================================================================
% TABLE STYLING - Clean tables with crimson borders
% =============================================================================
% Professional table appearance with:
% - Clean white background (no colored rows)
% - Crimson-colored borders
% - Good spacing for readability
%
% Note: Headers are automatically bolded by Quarto when using **text** in source
\usepackage{booktabs}      % Professional table rules (\toprule, \midrule, \bottomrule)
\usepackage{colortbl}      % For colored borders (\arrayrulecolor)

% Global table styling - crimson borders
\setlength{\arrayrulewidth}{0.5pt}          % Thinner borders than default
%\arrayrulecolor{crimson}                    % Crimson borders matching brand

\setcounter{chapter}{0}

% =============================================================================
% DROP CAPS (Lettrine)
% =============================================================================
% Decorative large first letter at chapter openings, following the tradition
% of Hennessy & Patterson and other MIT Press textbooks.
% Usage in QMD: \lettrine{T}{he first sentence...}
\usepackage{lettrine}
\renewcommand{\LettrineFontHook}{\color{crimson}\bfseries}
\setcounter{DefaultLines}{3}          % Drop cap spans 3 lines
\renewcommand{\DefaultLoversize}{0.1} % Slight oversize for visual weight
\renewcommand{\DefaultLraise}{0}      % No vertical shift
\setlength{\DefaultNindent}{0.5em}    % Indent of continuation text
\setlength{\DefaultSlope}{0pt}        % No slope on continuation

% =============================================================================
% RUNNING HEADERS — Truncation Safety
% =============================================================================
% Long chapter/section titles can overflow the header. These marks truncate
% gracefully so headers stay within the text block.
\renewcommand{\chaptermark}[1]{%
  \markboth{\thechapter.\ #1}{}}
\renewcommand{\sectionmark}[1]{%
  \markright{\thesection\ #1}}

% =============================================================================
% EPIGRAPH ENVIRONMENT
% =============================================================================
% For chapter-opening quotations. Renders as right-aligned italic block
% with attribution in small caps below.
% Usage: \epigraph{Quote text}{Author Name, \textit{Source}}
\newcommand{\bookepigraph}[2]{%
  \vspace{1em}%
  \begin{flushright}%
    \begin{minipage}{0.75\textwidth}%
      \raggedleft\itshape\small #1\\[0.5em]%
      \normalfont\small --- #2%
    \end{minipage}%
  \end{flushright}%
  \vspace{1.5em}%
}

% =============================================================================
% THUMB INDEX TABS
% =============================================================================
% Colored tabs on the outer page edge for quick chapter navigation.
% Each Part gets a different vertical position; chapters within a Part
% share the same tab position. Visible when flipping through the book.
\newcounter{thumbindex}
\setcounter{thumbindex}{0}
\newlength{\thumbtabheight}
\setlength{\thumbtabheight}{16mm}     % Height of each tab
\newlength{\thumbtabwidth}
\setlength{\thumbtabwidth}{8mm}       % Width protruding from edge
\newlength{\thumbtabgap}
\setlength{\thumbtabgap}{1mm}         % Gap between tabs

% Advance to next thumb tab position (call at each \part)
\newcommand{\nextthumb}{%
  \stepcounter{thumbindex}%
}

% Draw the thumb tab on every page (placed in header via fancyhdr)
\newcommand{\drawthumb}{%
  \ifnum\value{thumbindex}>0%
    \begin{tikzpicture}[remember picture,overlay]
      \pgfmathsetmacro{\thumboffset}{%
        20 + (\value{thumbindex}-1) * (16 + 1)}  % mm from top
      \ifodd\value{page}%
        % Odd pages: tab on right edge
        \fill[crimson!80]
          ([yshift=-\thumboffset mm]current page.north east)
          rectangle +(-\thumbtabwidth, -\thumbtabheight);
        \node[white,font=\tiny\bfseries,rotate=90]
          at ([yshift=-\thumboffset mm - 0.5\thumbtabheight,
               xshift=-0.5\thumbtabwidth]current page.north east)
          {\Roman{thumbindex}};
      \else
        % Even pages: tab on left edge
        \fill[crimson!80]
          ([yshift=-\thumboffset mm]current page.north west)
          rectangle +(\thumbtabwidth, -\thumbtabheight);
        \node[white,font=\tiny\bfseries,rotate=-90]
          at ([yshift=-\thumboffset mm - 0.5\thumbtabheight,
               xshift=0.5\thumbtabwidth]current page.north west)
          {\Roman{thumbindex}};
      \fi
    \end{tikzpicture}%
  \fi
}

% Hook into fancyhdr to draw thumb on every content page
\AddToHook{shipout/foreground}{%
  \drawthumb%
}

% =============================================================================
% CROP / BLEED MARKS
% =============================================================================
% For final print submission, uncomment the line below to add crop marks.
% MIT Press production will advise on exact requirements.
% \usepackage[cam,center,width=7.5in,height=10.5in]{crop}

% =============================================================================
% PDF/A ARCHIVAL COMPLIANCE
% =============================================================================
% MIT Press increasingly requires PDF/A for long-term preservation.
% This embeds all fonts and removes transparency.
% Note: pdfx must be loaded early; if it conflicts with hyperref,
% MIT Press production can handle the conversion post-build.
% Uncomment when ready for final submission:
% \usepackage[a-3u]{pdfx}

% =============================================================================
% ENHANCED WIDOW / ORPHAN CONTROL
% =============================================================================
% Prevent single lines at top/bottom of pages and breaks before equations
\clubpenalty=10000          % No orphans (single first line at bottom)
\widowpenalty=10000         % No widows (single last line at top)
\displaywidowpenalty=10000  % No widow before display math
\predisplaypenalty=10000    % No page break just before display math
\postdisplaypenalty=0       % Allow break after display math (natural)
\usepackage{needspace}
\let\Needspace\needspace
\makeatletter
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{plain}
\@ifundefined{c@chapter}{\newfloat{vid}{h}{lovid}}{\newfloat{vid}{h}{lovid}[chapter]}
\floatname{vid}{Video}
\newcommand*\listofvids{\listof{vid}{List of Videos}}
\makeatother
\makeatletter
\@ifpackageloaded{tcolorbox}{}{\usepackage[skins,breakable]{tcolorbox}}
\@ifpackageloaded{fontawesome5}{}{\usepackage{fontawesome5}}
\definecolor{quarto-callout-color}{HTML}{909090}
\definecolor{quarto-callout-note-color}{HTML}{0758E5}
\definecolor{quarto-callout-important-color}{HTML}{CC1914}
\definecolor{quarto-callout-warning-color}{HTML}{EB9113}
\definecolor{quarto-callout-tip-color}{HTML}{00A047}
\definecolor{quarto-callout-caution-color}{HTML}{FC5300}
\definecolor{quarto-callout-color-frame}{HTML}{acacac}
\definecolor{quarto-callout-note-color-frame}{HTML}{4582ec}
\definecolor{quarto-callout-important-color-frame}{HTML}{d9534f}
\definecolor{quarto-callout-warning-color-frame}{HTML}{f0ad4e}
\definecolor{quarto-callout-tip-color-frame}{HTML}{02b875}
\definecolor{quarto-callout-caution-color-frame}{HTML}{fd7e14}
\makeatother
\makeatletter
\@ifpackageloaded{bookmark}{}{\usepackage{bookmark}}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\newcommand{\fbxIconPath}{assets/images/icons/callouts}
\newcommand{\fbxIconFormat}{pdf}
\makeatletter
\@ifpackageloaded{tcolorbox}{}{\usepackage[many]{tcolorbox}}
\makeatother
%%%% ---foldboxy preamble ----- %%%%%

% Load xstring for string manipulation
\RequirePackage{xstring}

% Icon path and format configuration - can be overridden in filter-metadata
\providecommand{\fbxIconPath}{assets/images/icons/callouts}
\providecommand{\fbxIconFormat}{pdf}

% Helper command to include icon with hyphen-to-underscore conversion
% This ensures consistency: callout-quiz-question -> callout_quiz_question
\newcommand{\fbxIncludeIcon}[2]{%
  \StrSubstitute{#1}{-}{_}[\fbxIconName]%
  \includegraphics[width=#2]{\fbxIconPath/icon_\fbxIconName.\fbxIconFormat}%
}

% Legacy fallback colors (keep for compatibility)
\definecolor{fbx-default-color1}{HTML}{c7c7d0}
\definecolor{fbx-default-color2}{HTML}{a3a3aa}
\definecolor{fbox-color1}{HTML}{c7c7d0}
\definecolor{fbox-color2}{HTML}{a3a3aa}

% arguments: #1 typelabelnummer: #2 titel: #3
\newenvironment{fbx}[3]{%
\begin{tcolorbox}[
  enhanced,
  breakable,
  %fontupper=\fontsize{8pt}{10pt}\selectfont,  % 95% of body text (10pt -> 9.5pt)
  before skip=8pt,  % space above box (increased)
  after skip=8pt,   % space below box (increased)
  attach boxed title to top*={xshift=0pt},
  boxed title style={
  %fuzzy shadow={1pt}{-1pt}{0mm}{0.1mm}{gray},
  arc=1.5pt,
  rounded corners=north,
  sharp corners=south,
  top=6pt,          % Adjusted for ~40px equivalent height
  bottom=5pt,       % Adjusted for ~40px equivalent height
  overlay={
      \node [left,outer sep=0em, black,draw=none,anchor=west,
        rectangle,fill=none,inner sep=0pt]
        at ([xshift=4mm]frame.west) {\fbxIncludeIcon{#1}{4.2mm}};
    },
  },
  colframe=#1-color2,             % Border color (auto-generated from YAML)
  colbacktitle=#1-color1,         % Background color (auto-generated from YAML)
  colback=white,
  coltitle=black,
  titlerule=0mm,
  toprule=0.5pt,
  bottomrule=0.5pt,
  leftrule=2.2pt,
  rightrule=0.5pt,
  outer arc=1.5pt,
  arc=1.5pt,
  left=0.5em,       % increased left padding
  bottomtitle=1.5mm, % increased title bottom margin
  toptitle=1.5mm,    % increased title top margin
  title=\hspace{2.5em}\protect#2\hspace{0.5em}\protect#3, % Protect parameters
  extras middle and last={top=4pt} % increased continuation spacing
]}
{\end{tcolorbox}}


% boxed environment with right border
\newenvironment{fbxSimple}[3]{\begin{tcolorbox}[
  enhanced,
  breakable,
  %fontupper=\fontsize{8pt}{10pt}\selectfont,  % 95% of body text (10pt -> 9.5pt)
  before skip=8pt,  % space above box (increased)
  after skip=8pt,   % space below box (increased)
  attach boxed title to top*={xshift=0pt},
  boxed title style={
  %fuzzy shadow={1pt}{-1pt}{0mm}{0.1mm}{gray},
  arc=1.5pt,
  rounded corners=north,
  sharp corners=south,
  top=6pt,          % Adjusted for ~40px equivalent height
  bottom=5pt,       % Adjusted for ~40px equivalent height
  overlay={
      \node [left,outer sep=0em, black,draw=none,anchor=west,
        rectangle,fill=none,inner sep=0pt]
        at ([xshift=3mm]frame.west) {\fbxIncludeIcon{#1}{4.2mm}};
    },
  },
  colframe=#1-color2,             % Border color (auto-generated from YAML)
  colbacktitle=#1-color1,         % Background color (auto-generated from YAML)
  colback=white,
  coltitle=black,
  titlerule=0mm,
  toprule=0.5pt,
  bottomrule=0.5pt,
  leftrule=2.2pt,
  rightrule=0.5pt,
  outer arc=1.5pt,
  arc=1.5pt,
  left=0.5em,       % increased left padding
  bottomtitle=1.5mm, % increased title bottom margin
  toptitle=1.5mm,    % increased title top margin
  title=\hspace{2.5em}\protect#2\hspace{0.5em}\protect#3, % Protect parameters
  boxsep=1pt,
  extras first={bottom=0pt},
  extras last={top=0pt,bottom=-4pt},
  overlay first={
    \draw[line width=1pt,white] ([xshift=2.2pt]frame.south west)-- ([xshift=-0.5pt]frame.south east);
  },
  overlay last={
    \draw[line width=1pt,white] ([xshift=2.2pt]frame.north west)-- ([xshift=-0.5pt]frame.north east);
   }
]}
{\end{tcolorbox}}

%%%% --- end foldboxy preamble ----- %%%%%
%%==== colors from yaml ===%
\definecolor{callout-example-color1}{HTML}{F0F8F6}
\definecolor{callout-example-color2}{HTML}{148F77}
\definecolor{callout-lighthouse-color1}{HTML}{FDF8E6}
\definecolor{callout-lighthouse-color2}{HTML}{B8860B}
\definecolor{callout-resource-videos-color1}{HTML}{E0F2F1}
\definecolor{callout-resource-videos-color2}{HTML}{20B2AA}
\definecolor{callout-quiz-question-color1}{HTML}{F0F0F8}
\definecolor{callout-quiz-question-color2}{HTML}{5B4B8A}
\definecolor{callout-theorem-color1}{HTML}{F5F0FF}
\definecolor{callout-theorem-color2}{HTML}{6B46C1}
\definecolor{callout-definition-color1}{HTML}{F0F4F8}
\definecolor{callout-definition-color2}{HTML}{1B4F72}
\definecolor{callout-colab-color1}{HTML}{FFF5E6}
\definecolor{callout-colab-color2}{HTML}{FF6B35}
\definecolor{callout-chapter-connection-color1}{HTML}{FDF2F7}
\definecolor{callout-chapter-connection-color2}{HTML}{A51C30}
\definecolor{callout-resource-slides-color1}{HTML}{E0F2F1}
\definecolor{callout-resource-slides-color2}{HTML}{20B2AA}
\definecolor{callout-perspective-color1}{HTML}{F7F8FA}
\definecolor{callout-perspective-color2}{HTML}{4A5568}
\definecolor{callout-checkpoint-color1}{HTML}{E8F5E9}
\definecolor{callout-checkpoint-color2}{HTML}{2E7D32}
\definecolor{callout-principle-color1}{HTML}{F3F2FA}
\definecolor{callout-principle-color2}{HTML}{3D3B8E}
\definecolor{callout-resource-exercises-color1}{HTML}{E0F2F1}
\definecolor{callout-resource-exercises-color2}{HTML}{20B2AA}
\definecolor{callout-takeaways-color1}{HTML}{FDF2F7}
\definecolor{callout-takeaways-color2}{HTML}{BE185D}
\definecolor{callout-notebook-color1}{HTML}{F2F7FF}
\definecolor{callout-notebook-color2}{HTML}{2C5282}
\definecolor{callout-code-color1}{HTML}{F2F4F8}
\definecolor{callout-code-color2}{HTML}{D1D7E0}
\definecolor{callout-quiz-answer-color1}{HTML}{E8F2EA}
\definecolor{callout-quiz-answer-color2}{HTML}{4a7c59}
%=============%

\usepackage{hyphenat}
\usepackage{ifthen}
\usepackage{calc}
\usepackage{calculator}



\usepackage{graphicx}
\usepackage{geometry}
\usepackage{afterpage}
\usepackage{tikz}
\usetikzlibrary{calc}
\usetikzlibrary{fadings}
\usepackage[pagecolor=none]{pagecolor}


% Set the titlepage font families







% Set the coverpage font families

\usepackage{bookmark}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  pdftitle={Introduction to Machine Learning Systems},
  pdfauthor={Vijay Janapa Reddi},
  colorlinks=true,
  linkcolor={Maroon},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}


\title{Introduction to Machine Learning Systems}
\author{Vijay Janapa Reddi}
\date{February 1, 2026}
\begin{document}
%%%%% begin titlepage extension code

  \begin{frontmatter}

\begin{titlepage}
% This is a combination of Pandoc templating and LaTeX
% Pandoc templating https://pandoc.org/MANUAL.html#templates
% See the README for help

\thispagestyle{empty}

\newgeometry{top=-100in}

% Page color

\newcommand{\coverauthorstyle}[1]{{\fontsize{20}{24.0}\selectfont
{#1}}}

\begin{tikzpicture}[remember picture, overlay, inner sep=0pt, outer sep=0pt]

\tikzfading[name=fadeout, inner color=transparent!0,outer color=transparent!100]
\tikzfading[name=fadein, inner color=transparent!100,outer color=transparent!0]
\node[anchor=south west, rotate=0, opacity=1] at ($(current page.south west)+(0.225\paperwidth, 9)$) {
\includegraphics[width=\paperwidth, keepaspectratio]{assets/images/covers/cover-image-transparent-vol1.png}};

% Title
\newcommand{\titlelocationleft}{0.075\paperwidth}
\newcommand{\titlelocationbottom}{0.4\paperwidth}
\newcommand{\titlealign}{left}

\begin{scope}{%
\fontsize{52}{62.4}\selectfont
\node[anchor=north
west, align=left, rotate=0] (Title1) at ($(current page.south west)+(\titlelocationleft,\titlelocationbottom)$)  [text width = 0.9\paperwidth]  {{\nohyphens{Machine
Learning Systems}}};
}
\end{scope}

% Author
\newcommand{\authorlocationleft}{.925\paperwidth}
\newcommand{\authorlocationbottom}{0.175\paperwidth}
\newcommand{\authoralign}{right}

\begin{scope}
{%
\fontsize{20}{24.0}\selectfont
\node[anchor=north
east, align=right, rotate=0] (Author1) at ($(current page.south west)+(\authorlocationleft,\authorlocationbottom)$)  [text width = 6in]  {\coverauthorstyle{Vijay
Janapa Reddi\\}};
}
\end{scope}

% Footer
\newcommand{\footerlocationleft}{0.075\paperwidth}
\newcommand{\footerlocationbottom}{0.475\paperwidth}
\newcommand{\footerlocationalign}{left}

\begin{scope}
{%
\fontsize{25}{30.0}\selectfont
 \node[anchor=north west, align=left, rotate=0] (Footer1) at %
($(current page.south west)+(\footerlocationleft,\footerlocationbottom)$)  [text width = 0.9\paperwidth]  {{\nohyphens{Introduction
to}}};
}
\end{scope}

\end{tikzpicture}
\clearpage
\restoregeometry
%%% TITLE PAGE START

% Set up alignment commands
%Page
\newcommand{\titlepagepagealign}{
\ifthenelse{\equal{left}{right}}{\raggedleft}{}
\ifthenelse{\equal{left}{center}}{\centering}{}
\ifthenelse{\equal{left}{left}}{\raggedright}{}
}


\newcommand{\titleandsubtitle}{
% Title and subtitle
{{\huge{\bfseries{\nohyphens{Introduction to Machine Learning
Systems}}}}\par
}%
}
\newcommand{\titlepagetitleblock}{
\titleandsubtitle
}

\newcommand{\authorstyle}[1]{{\large{#1}}}

\newcommand{\affiliationstyle}[1]{{\large{#1}}}

\newcommand{\titlepageauthorblock}{
{\authorstyle{\nohyphens{Vijay Janapa
Reddi}{\textsuperscript{1}}\textsuperscript{,}{\textsuperscript{,*}}}}}

\newcommand{\titlepageaffiliationblock}{
\hangindent=1em
\hangafter=1
{\affiliationstyle{
{1}.~Harvard University


\vspace{1\baselineskip}
* \textit{Correspondence:}~Vijay Janapa Reddi~vj@eecs.harvard.edu
}}
}
\newcommand{\headerstyled}{%
{}
}
\newcommand{\footerstyled}{%
{\large{}}
}
\newcommand{\datestyled}{%
{February 1, 2026}
}


\newcommand{\titlepageheaderblock}{\headerstyled}

\newcommand{\titlepagefooterblock}{
\footerstyled
}

\newcommand{\titlepagedateblock}{
\datestyled
}

%set up blocks so user can specify order
\newcommand{\titleblock}{{

{\titlepagetitleblock}
}

\vspace{4\baselineskip}
}

\newcommand{\authorblock}{{\titlepageauthorblock}

\vspace{2\baselineskip}
}

\newcommand{\affiliationblock}{{\titlepageaffiliationblock}

\vspace{0pt}
}

\newcommand{\logoblock}{}

\newcommand{\footerblock}{}

\newcommand{\dateblock}{{\titlepagedateblock}

\vspace{0pt}
}

\newcommand{\headerblock}{}

\thispagestyle{empty} % no page numbers on titlepages


\newcommand{\vrulecode}{\textcolor{black}{\rule{\vrulewidth}{\textheight}}}
\newlength{\vrulewidth}
\setlength{\vrulewidth}{2pt}
\newlength{\B}
\setlength{\B}{\ifdim\vrulewidth > 0pt 0.05\textwidth\else 0pt\fi}
\newlength{\minipagewidth}
\ifthenelse{\equal{left}{left} \OR \equal{left}{right} }
{% True case
\setlength{\minipagewidth}{\textwidth - \vrulewidth - \B - 0.1\textwidth}
}{
\setlength{\minipagewidth}{\textwidth - 2\vrulewidth - 2\B - 0.1\textwidth}
}
\ifthenelse{\equal{left}{left} \OR \equal{left}{leftright}}
{% True case
\raggedleft % needed for the minipage to work
\vrulecode
\hspace{\B}
}{%
\raggedright % else it is right only and width is not 0
}
% [position of box][box height][inner position]{width}
% [s] means stretch out vertically; assuming there is a vfill
\begin{minipage}[b][\textheight][s]{\minipagewidth}
\titlepagepagealign
\titleblock

Prof.~Vijay Janapa Reddi

School of Engineering and Applied Sciences

Harvard University

\vspace{80mm}

With heartfelt gratitude to the community for their invaluable
contributions and steadfast support.

\vfill

February 1, 2026

\vfill
\par

\end{minipage}\ifthenelse{\equal{left}{right} \OR \equal{left}{leftright} }{
\hspace{\B}
\vrulecode}{}
\clearpage
%%% TITLE PAGE END
\end{titlepage}
\setcounter{page}{1}
\end{frontmatter}

%%%%% end titlepage extension code

% =============================================================================
% HALF-TITLE PAGE (Volume I)
% =============================================================================
% Standard academic book sequence: half-title -> blank -> title page -> copyright
% The half-title shows only the book title -- no author, no publisher, no date.
\thispagestyle{empty}
\begin{center}
\vspace*{0.3\textheight}
{\fontsize{24pt}{28pt}\selectfont\bfseries\color{crimson} Introduction to\\[0.4em] Machine Learning Systems}\\[2em]
{\large\itshape Volume~I}
\vfill
\end{center}
\clearpage
\thispagestyle{empty}\null\clearpage  % Blank verso (back of half-title)

\renewcommand*\contentsname{Table of contents}
{
\hypersetup{linkcolor=}
\setcounter{tocdepth}{2}
\tableofcontents
}
\listoffigures
\listoftables

\mainmatter
\bookmarksetup{startatroot}

\chapter*{Welcome to Volume I}\label{welcome-to-volume-i}
\addcontentsline{toc}{chapter}{Welcome to Volume I}

\markboth{Welcome to Volume I}{Welcome to Volume I}

\bookmarksetup{startatroot}

\chapter*{Glossary (Volume I:
Foundations)}\label{glossary-volume-i-foundations}
\addcontentsline{toc}{chapter}{Glossary (Volume I: Foundations)}

\markboth{Glossary (Volume I: Foundations)}{Glossary (Volume I:
Foundations)}

This glossary contains definitions of key terms used in Volume I:
Foundations. Terms are organized alphabetically and include references
to the chapters where they appear.

\begin{tcolorbox}[enhanced jigsaw, coltitle=black, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Using the Glossary}, toprule=.15mm, rightrule=.15mm, opacitybacktitle=0.6, left=2mm, opacityback=0, breakable, colbacktitle=quarto-callout-note-color!10!white, bottomrule=.15mm, colframe=quarto-callout-note-color-frame, colback=white, toptitle=1mm, bottomtitle=1mm, titlerule=0mm, arc=.35mm, leftrule=.75mm]

\begin{itemize}
\tightlist
\item
  \textbf{Terms are alphabetically ordered} for easy reference
\item
  \textbf{Chapter references} show where terms are introduced or
  discussed
\item
  \textbf{Cross-references} help you explore related concepts
\item
  \textbf{Interactive tooltips} appear when you hover over glossary
  terms throughout the book
\end{itemize}

\end{tcolorbox}

\section*{3}\label{section}
\addcontentsline{toc}{section}{3}

\markright{3}

\begin{description}
\tightlist
\item[\textbf{3dmark}]
Graphics performance benchmark suite that evaluates real-time 3D
rendering capabilities, measuring triangle throughput, texture fill
rates, and modern features like ray tracing and DLSS performance.
\emph{Appears in: \textbf{?@sec-benchmarking-ai}}
\end{description}

\section*{A}\label{a}
\addcontentsline{toc}{section}{A}

\markright{A}

\begin{description}
\tightlist
\item[\textbf{a/b testing}]
A controlled experimental method for comparing two versions of a system
or model by randomly dividing users into groups and measuring
performance differences between the variants \emph{Appears in:
\textbf{?@sec-machine-learning-operations-mlops},
\textbf{?@sec-ai-development-workflow}}
\item[\textbf{activation checkpointing}]
A memory optimization technique that reduces memory usage during
backpropagation by selectively discarding and recomputing activations
instead of storing all intermediate results. \emph{Appears in:
\textbf{?@sec-ai-training}}
\item[\textbf{activation function}]
A mathematical function applied to the weighted sum of inputs in a
neural network neuron to introduce nonlinearity, enabling the network to
learn complex patterns beyond simple linear combinations. \emph{Appears
in: \textbf{?@sec-deep-learning-systems-foundations},
\textbf{?@sec-dnn-architectures}, \textbf{?@sec-ai-frameworks},
\textbf{?@sec-ai-training}}
\item[\textbf{activation-based pruning}]
A pruning method that evaluates the average activation values of neurons
or filters over a dataset to identify and remove neurons that
consistently produce low activations and contribute little information
to the network's decision process. \emph{Appears in:
\textbf{?@sec-model-compression}}
\item[\textbf{active learning}]
An approach that intelligently selects the most informative examples for
human annotation based on model uncertainty, reducing the amount of
labeled data needed for effective training. \emph{Appears in:
\textbf{?@sec-data-engineering-ml}}
\item[\textbf{adam optimization}]
An adaptive learning rate optimization algorithm that combines momentum
and RMSprop by maintaining exponentially decaying averages of both
gradients and squared gradients for each parameter. \emph{Appears in:
\textbf{?@sec-ai-training}}
\item[\textbf{ai triad}]
A framework modeling ML systems as three interdependent components: data
that guides behavior, algorithms that learn patterns, and computational
infrastructure that enables training and inference. Limitations in any
component constrain the capabilities of the others. \emph{Appears in:
\textbf{?@sec-introduction}}
\item[\textbf{alerting}]
Automated notification systems that inform teams when metrics exceed
predefined thresholds or anomalies are detected in production ML
systems. \emph{Appears in:
\textbf{?@sec-machine-learning-operations-mlops}}
\item[\textbf{alexnet}]
A groundbreaking convolutional neural network architecture that won the
2012 ImageNet challenge, reducing error rates from 26\% to 16\% and
sparking the deep learning renaissance. \emph{Appears in:
\textbf{?@sec-benchmarking-ai}, \textbf{?@sec-dnn-architectures},
\textbf{?@sec-introduction}}
\item[\textbf{all-reduce}]
A collective communication operation in distributed computing where each
process contributes data and all processes receive the combined result,
commonly used for gradient aggregation in distributed training.
\emph{Appears in: \textbf{?@sec-ai-training}}
\item[\textbf{alphafold}]
A landmark AI system developed by DeepMind that predicts the
three-dimensional structure of proteins from their amino acid sequences,
solving the decades-old protein folding problem and demonstrating how
large-scale ML systems can accelerate scientific discovery.
\emph{Appears in: \textbf{?@sec-introduction}}
\item[\textbf{apache kafka}]
A distributed streaming platform that handles real-time data feeds using
a publish-subscribe messaging system, commonly used for building ML data
pipelines with high throughput and fault tolerance. \emph{Appears in:
\textbf{?@sec-data-engineering-ml}}
\item[\textbf{apache spark}]
An open-source distributed computing framework that enables large-scale
data processing across clusters of computers, revolutionizing ETL
operations with in-memory computing capabilities. \emph{Appears in:
\textbf{?@sec-data-engineering-ml}}
\item[\textbf{application-specific integrated circuit}]
A specialized chip designed for specific tasks that offers maximum
efficiency by abandoning general-purpose flexibility, exemplified by
Cerebras Wafer-Scale Engine for machine learning training. \emph{Appears
in: \textbf{?@sec-ai-training}}
\item[\textbf{application-specific integrated circuit (asic)}]
Custom chips designed for specific computational tasks that offer
superior performance and energy efficiency compared to general-purpose
processors, exemplified by Google's TPUs and Bitcoin mining ASICs
\emph{Appears in: \textbf{?@sec-benchmarking-ai},
\textbf{?@sec-ai-acceleration}}
\item[\textbf{architectural efficiency}]
The dimension of model optimization that focuses on how computations are
performed efficiently during training and inference by exploiting
sparsity, factorizing large components, and dynamically adjusting
computation based on input complexity. \emph{Appears in:
\textbf{?@sec-model-compression}}
\item[\textbf{arithmetic intensity}]
The ratio of floating-point operations to bytes of memory accessed
(FLOPs/byte), used in roofline analysis to determine whether workloads
are memory-bound or compute-bound and to guide optimization priorities.
\emph{Appears in: \textbf{?@sec-conclusion}}
\item[\textbf{artificial general intelligence}]
AI systems capable of matching human-level performance across all
cognitive tasks, requiring novel distributed architectures,
energy-efficient hardware, and unprecedented infrastructure scale.
\emph{Appears in: \textbf{?@sec-conclusion}}
\item[\textbf{artificial intelligence}]
The field of computer science focused on creating systems that can
perform tasks typically requiring human intelligence, such as
perception, reasoning, learning, and decision-making. \emph{Appears in:
\textbf{?@sec-benchmarking-ai}, \textbf{?@sec-conclusion},
\textbf{?@sec-deep-learning-systems-foundations},
\textbf{?@sec-introduction}, \textbf{?@sec-ml-system-architecture}}
\item[\textbf{artificial neural network}]
A computational model inspired by biological neural networks, consisting
of interconnected nodes (neurons) organized in layers that can learn
patterns from data through adjustable weights and biases. \emph{Appears
in: \textbf{?@sec-deep-learning-systems-foundations}}
\item[\textbf{artificial neurons}]
Basic computational units in neural networks that mimic biological
neurons, taking multiple inputs, applying weights and biases, and
producing an output signal through an activation function. \emph{Appears
in: \textbf{?@sec-introduction}}
\item[\textbf{attention mechanism}]
A neural network component that computes weighted connections between
elements based on their content, allowing dynamic focus on relevant
parts of the input rather than fixed architectural connections.
\emph{Appears in: \textbf{?@sec-dnn-architectures}}
\item[\textbf{automatic differentiation}]
A computational technique that automatically calculates exact
derivatives of functions implemented as computer programs by
systematically applying the chain rule at the elementary operation
level, essential for training neural networks through gradient-based
optimization. \emph{Appears in: \textbf{?@sec-ai-frameworks}}
\item[\textbf{automatic mixed precision}]
A training technique that automatically manages the use of different
numerical precisions (FP16, FP32) to optimize memory usage and
computational speed while maintaining model accuracy. \emph{Appears in:
\textbf{?@sec-ai-training}}
\item[\textbf{automl}]
Automated Machine Learning that uses machine learning itself to automate
model design decisions, including architecture search, hyperparameter
optimization, and feature selection to create efficient models without
manual intervention. \emph{Appears in: \textbf{?@sec-model-compression}}
\item[\textbf{autoscaling}]
Dynamic adjustment of compute resources based on workload demand,
automatically scaling up during peak usage and scaling down during low
usage to optimize costs and performance. \emph{Appears in:
\textbf{?@sec-machine-learning-operations-mlops}}
\end{description}

\section*{B}\label{b}
\addcontentsline{toc}{section}{B}

\markright{B}

\begin{description}
\tightlist
\item[\textbf{backpropagation}]
An algorithm that computes gradients of the loss function with respect
to network weights by propagating error signals backward through the
network layers, enabling systematic weight updates during training.
\emph{Appears in: \textbf{?@sec-deep-learning-systems-foundations},
\textbf{?@sec-dnn-architectures}, \textbf{?@sec-ai-frameworks},
\textbf{?@sec-ai-training}}
\item[\textbf{bandwidth}]
The maximum rate of data transfer across a communication channel or
memory interface, typically measured in bytes per second and critical
for optimizing data movement in AI accelerators. \emph{Appears in:
\textbf{?@sec-ai-acceleration}}
\item[\textbf{batch inference}]
The process of using a trained machine learning model to make
predictions or decisions on new, previously unseen data. \emph{Appears
in: \textbf{?@sec-benchmarking-ai},
\textbf{?@sec-ml-system-architecture},
\textbf{?@sec-machine-learning-operations-mlops}}
\item[\textbf{batch ingestion}]
A data processing pattern that collects and processes data in groups or
batches at scheduled intervals, suitable for scenarios where real-time
processing is not critical. \emph{Appears in:
\textbf{?@sec-data-engineering-ml}}
\item[\textbf{batch normalization}]
A technique that normalizes inputs to each layer to have zero mean and
unit variance, which stabilizes training and often allows for higher
learning rates and faster convergence. \emph{Appears in:
\textbf{?@sec-dnn-architectures}, \textbf{?@sec-ai-frameworks},
\textbf{?@sec-ai-training}}
\item[\textbf{batch processing}]
The technique of processing multiple data samples simultaneously to
amortize computation and memory access costs, improving overall
throughput in neural network training and inference \emph{Appears in:
\textbf{?@sec-benchmarking-ai}, \textbf{?@sec-data-engineering-ml},
\textbf{?@sec-ai-acceleration}}
\item[\textbf{batch size}]
The number of training examples processed simultaneously during one
iteration of neural network training, affecting both computational
efficiency and gradient estimation quality. \emph{Appears in:
\textbf{?@sec-deep-learning-systems-foundations}}
\item[\textbf{batch throughput optimization}]
Techniques for maximizing the number of samples processed per unit time
when handling multiple inputs simultaneously, leveraging parallelism and
batching efficiencies. \emph{Appears in: \textbf{?@sec-benchmarking-ai}}
\item[\textbf{batched operations}]
Matrix computations that process multiple inputs simultaneously,
converting matrix-vector operations into more efficient matrix-matrix
operations to improve hardware utilization. \emph{Appears in:
\textbf{?@sec-ai-training}}
\item[\textbf{benchmark engineering}]
The systematic design and development of performance evaluation
frameworks, involving test harness creation, metric selection, and
result interpretation methodologies. \emph{Appears in:
\textbf{?@sec-benchmarking-ai}}
\item[\textbf{benchmark harness}]
Systematic infrastructure component that controls test execution,
manages input delivery, and collects performance measurements under
controlled conditions to ensure reproducible evaluations. \emph{Appears
in: \textbf{?@sec-benchmarking-ai}}
\item[\textbf{benchmarking}]
Systematic evaluation of compute performance, algorithmic effectiveness,
and data quality in machine learning systems to optimize performance
across diverse workloads and ensure reproducibility. \emph{Appears in:
\textbf{?@sec-benchmarking-ai}}
\item[\textbf{bert}]
Bidirectional Encoder Representations from Transformers, a
transformer-based language model introduced by Google in 2018 that
revolutionized natural language processing through masked language
modeling pre-training. \emph{Appears in: \textbf{?@sec-benchmarking-ai}}
\item[\textbf{bfloat16}]
A 16-bit floating-point format developed by Google Brain that maintains
the same dynamic range as FP32 but with reduced precision, making it
particularly suitable for deep learning training. \emph{Appears in:
\textbf{?@sec-ai-training}}
\item[\textbf{bias}]
A learnable parameter added to the weighted sum in each neuron that
allows the activation function to shift, providing additional
flexibility for the network to fit complex patterns. \emph{Appears in:
\textbf{?@sec-deep-learning-systems-foundations}}
\item[\textbf{bias terms}]
Learnable parameters in neural networks that shift the activation
function, allowing neurons to activate even when all inputs are zero,
providing additional flexibility for fitting complex patterns.
\emph{Appears in: \textbf{?@sec-deep-learning-systems-foundations}}
\item[\textbf{binarization}]
An extreme quantization technique that reduces neural network weights
and activations to binary values (typically -1 and +1), achieving
maximum compression but often requiring specialized training procedures
and hardware support. \emph{Appears in:
\textbf{?@sec-model-compression}}
\item[\textbf{biological neuron}]
A cell in the nervous system that receives, processes, and transmits
information through electrical and chemical signals, serving as
inspiration for artificial neural networks. \emph{Appears in:
\textbf{?@sec-deep-learning-systems-foundations}}
\item[\textbf{bitter lesson}]
Richard Sutton's 2019 observation that general methods leveraging
computation consistently outperform approaches encoding human expertise,
suggesting that systems engineering enabling computational scale is
central to AI advancement. \emph{Appears in:
\textbf{?@sec-introduction}}
\item[\textbf{black box}]
A system where you can observe the inputs and outputs but cannot see or
understand the internal workings, particularly problematic in AI when
systems make important decisions affecting people's lives without
providing explanations for their reasoning. \emph{Appears in:
\textbf{?@sec-introduction}}
\item[\textbf{blas}]
Basic Linear Algebra Subprograms, a specification for low-level routines
that perform common linear algebra operations such as vector addition,
scalar multiplication, dot products, and matrix operations, forming the
computational foundation of modern ML frameworks. \emph{Appears in:
\textbf{?@sec-ai-frameworks}}
\item[\textbf{bounding box}]
A rectangular annotation that identifies object locations in images by
drawing a box around each object of interest, commonly used in computer
vision training datasets. \emph{Appears in:
\textbf{?@sec-data-engineering-ml}}
\item[\textbf{brittleness}]
The tendency of rule-based AI systems to fail completely when
encountering inputs that fall outside their programmed scenarios, no
matter how similar those inputs might be to what they were designed to
handle. \emph{Appears in: \textbf{?@sec-introduction}}
\end{description}

\section*{C}\label{c}
\addcontentsline{toc}{section}{C}

\markright{C}

\begin{description}
\tightlist
\item[\textbf{caching}]
A technique for storing frequently accessed data in high-speed storage
systems to reduce retrieval latency and improve system performance in ML
pipelines. \emph{Appears in: \textbf{?@sec-data-engineering-ml}}
\item[\textbf{calibration}]
The process in post-training quantization of analyzing a representative
dataset to determine optimal quantization parameters, including scale
factors and zero points, that minimize accuracy loss when converting
from high to low precision \emph{Appears in:
\textbf{?@sec-model-compression},
\textbf{?@sec-ai-development-workflow}}
\item[\textbf{canary deployment}]
A deployment strategy where new model versions receive a small
percentage of production traffic to validate behavior before full
rollout, enabling early detection of issues with minimal user impact
\emph{Appears in: \textbf{?@sec-machine-learning-operations-mlops},
\textbf{?@sec-ai-development-workflow}}
\item[\textbf{cap theorem}]
A distributed systems principle stating that a data store can only
provide two of the three guarantees: Consistency, Availability, and
Partition Tolerance. \emph{Appears in:
\textbf{?@sec-data-engineering-ml}}
\item[\textbf{carbon footprint}]
The total greenhouse gas emissions, typically measured in CO2
equivalent, produced directly and indirectly by training and operating
an ML system. \emph{Appears in: \textbf{?@sec-responsible-engineering}}
\item[\textbf{cerebras wafer-scale engine}]
A revolutionary single-wafer processor containing 2.6 trillion
transistors and 850,000 cores, designed to eliminate inter-device
communication bottlenecks in large-scale machine learning training.
\emph{Appears in: \textbf{?@sec-ai-training}}
\item[\textbf{chain rule}]
The calculus rule enabling computation of derivatives for composite
functions, fundamental to backpropagation. \emph{Appears in:
\textbf{?@sec-deep-learning-systems-foundations}}
\item[\textbf{channelwise quantization}]
A quantization granularity approach where each channel in a layer uses
its own set of quantization parameters, providing more precise
representation than layerwise quantization while maintaining hardware
efficiency. \emph{Appears in: \textbf{?@sec-model-compression}}
\item[\textbf{ci/cd pipelines}]
Continuous Integration and Continuous Delivery automated workflows that
streamline model development by integrating testing, validation, and
deployment processes. \emph{Appears in:
\textbf{?@sec-machine-learning-operations-mlops}}
\item[\textbf{classification labels}]
Simple categorical annotations that assign specific tags or categories
to data examples, representing the most basic form of supervised
learning annotation. \emph{Appears in:
\textbf{?@sec-data-engineering-ml}}
\item[\textbf{cloudsuite}]
Benchmark suite developed at EPFL that addresses modern datacenter
workloads including web search, data analytics, and media streaming,
measuring end-to-end performance across network, storage, and compute
dimensions. \emph{Appears in: \textbf{?@sec-benchmarking-ai}}
\item[\textbf{cold-start performance}]
Time required for a system to transition from idle state to active
execution, particularly important in serverless environments where
models are loaded on demand. \emph{Appears in:
\textbf{?@sec-benchmarking-ai}}
\item[\textbf{collaborative filtering}]
A technique used in recommendation systems that predicts user
preferences by identifying patterns in interactions from many users,
leveraging the collective behavior of the crowd rather than just item
properties. \emph{Appears in: \textbf{?@sec-ml-system-architecture}}
\item[\textbf{communication tax}]
The performance penalty incurred in distributed systems due to the
latency and bandwidth costs of synchronizing state between nodes.
\emph{Appears in: \textbf{?@sec-ai-training}}
\item[\textbf{compound ai systems}]
AI architectures that combine multiple specialized models and components
working together, rather than relying on a single monolithic model,
enabling modularity, specialization, and improved interpretability.
\emph{Appears in: \textbf{?@sec-conclusion}}
\item[\textbf{computational graph}]
A directed graph representing the sequence of operations in neural
network computation, enabling automatic differentiation. \emph{Appears
in: \textbf{?@sec-deep-learning-systems-foundations},
\textbf{?@sec-ai-frameworks}}
\item[\textbf{computer engineering}]
An engineering discipline that emerged in the late 1960s to address the
growing complexity of integrating hardware and software systems,
combining expertise from electrical engineering and computer science to
design and build complex computing systems. \emph{Appears in:
\textbf{?@sec-introduction}}
\item[\textbf{concept drift}]
The phenomenon where the statistical relationship between input features
and target outputs changes over time, distinct from data drift where
only input distributions change, causing model performance to degrade
\emph{Appears in: \textbf{?@sec-machine-learning-operations-mlops},
\textbf{?@sec-ai-development-workflow}}
\item[\textbf{conditional computation}]
A dynamic optimization technique where different parts of a neural
network are selectively activated based on input characteristics,
reducing computational load by skipping unnecessary computations for
specific inputs. \emph{Appears in: \textbf{?@sec-model-compression}}
\item[\textbf{connectionism}]
An approach to AI modeling that emphasizes learning and intelligence
emerging from simple interconnected units, serving as the theoretical
foundation for neural networks and contrasting with symbolic AI
approaches. \emph{Appears in: \textbf{?@sec-introduction}}
\item[\textbf{consensus labeling}]
A quality control approach that collects multiple annotations for the
same data point to identify controversial cases and improve label
reliability through inter-annotator agreement. \emph{Appears in:
\textbf{?@sec-data-engineering-ml}}
\item[\textbf{containerization}]
Packaging applications and their dependencies into portable, isolated
containers using tools like Docker to ensure consistent execution across
different environments. \emph{Appears in:
\textbf{?@sec-machine-learning-operations-mlops}}
\item[\textbf{containerized microservices}]
Architectural pattern using lightweight containers to package individual
services, enabling scalable, maintainable deployment of ML systems
across distributed environments. \emph{Appears in:
\textbf{?@sec-machine-learning-operations-mlops}}
\item[\textbf{continuous integration}]
A software development practice where code changes are automatically
integrated, tested, and validated multiple times per day to detect
issues early in the development cycle. \emph{Appears in:
\textbf{?@sec-ai-development-workflow}}
\item[\textbf{convolution}]
A mathematical operation fundamental to convolutional neural networks
that applies filters (kernels) to input data to extract features such as
edges, textures, or patterns, particularly effective for processing
images and spatial data. \emph{Appears in: \textbf{?@sec-ai-frameworks}}
\item[\textbf{convolution operation}]
A mathematical operation that slides a filter (kernel) across input data
to detect local features, forming the foundation of convolutional neural
networks for spatial pattern recognition. \emph{Appears in:
\textbf{?@sec-dnn-architectures}}
\item[\textbf{convolutional neural network}]
A specialized neural network architecture designed for processing
grid-like data such as images, using convolutional layers that apply
filters to detect local features. \emph{Appears in:
\textbf{?@sec-benchmarking-ai},
\textbf{?@sec-deep-learning-systems-foundations},
\textbf{?@sec-dnn-architectures}}
\item[\textbf{cp decomposition}]
CANDECOMP/PARAFAC decomposition that expresses a tensor as a sum of
rank-one components, used to compress neural network layers by reducing
the number of parameters while preserving computational functionality.
\emph{Appears in: \textbf{?@sec-model-compression}}
\item[\textbf{credit assignment problem}]
The challenge of determining which weights in a multi-layer network
contributed to prediction errors. \emph{Appears in:
\textbf{?@sec-deep-learning-systems-foundations}}
\item[\textbf{crisp-dm}]
Cross-Industry Standard Process for Data Mining, a structured
methodology developed in 1996 that defines six phases for data projects:
business understanding, data understanding, data preparation, modeling,
evaluation, and deployment. \emph{Appears in:
\textbf{?@sec-ai-development-workflow}}
\item[\textbf{cross-entropy loss}]
A loss function commonly used in classification tasks that measures the
difference between predicted probability distributions and true class
labels, providing strong gradients for effective learning. \emph{Appears
in: \textbf{?@sec-deep-learning-systems-foundations}}
\item[\textbf{crowdsourcing}]
A collaborative data collection approach that leverages distributed
individuals via the internet to perform annotation tasks, enabling
scalable dataset creation through platforms like Amazon Mechanical Turk.
\emph{Appears in: \textbf{?@sec-data-engineering-ml}}
\item[\textbf{cublas}]
NVIDIA's CUDA Basic Linear Algebra Subprograms library that provides
GPU-accelerated implementations of standard linear algebra operations,
enabling high-performance matrix computations on NVIDIA graphics
processing units. \emph{Appears in: \textbf{?@sec-ai-frameworks}}
\item[\textbf{cuda}]
NVIDIA's parallel computing platform and programming model that enables
general-purpose computing on graphics processing units (GPUs), allowing
machine learning frameworks to leverage massive parallelism for
accelerated tensor operations. \emph{Appears in:
\textbf{?@sec-ai-frameworks}}
\item[\textbf{cuda (compute unified device architecture)}]
NVIDIA's parallel computing platform and programming model that enables
developers to use GPUs for general-purpose computing beyond graphics
rendering. \emph{Appears in: \textbf{?@sec-ai-acceleration}}
\end{description}

\section*{D}\label{d}
\addcontentsline{toc}{section}{D}

\markright{D}

\begin{description}
\tightlist
\item[\textbf{dartmouth conference}]
The legendary 8-week workshop at Dartmouth College in 1956 where AI was
officially born, organized by John McCarthy, Marvin Minsky, Nathaniel
Rochester, and Claude Shannon, where the term artificial intelligence
was first coined. \emph{Appears in: \textbf{?@sec-introduction}}
\item[\textbf{data augmentation}]
The process of artificially expanding training datasets by creating
modified versions of existing data through transformations like
rotation, scaling, or noise injection. \emph{Appears in:
\textbf{?@sec-data-engineering-ml}}
\item[\textbf{data cascades}]
Systemic failures where data quality issues compound over time, creating
downstream negative consequences such as model failures, costly
rebuilding, or project termination. \emph{Appears in:
\textbf{?@sec-data-engineering-ml}}
\item[\textbf{data center}]
A facility that houses computer systems and associated components such
as telecommunications and storage systems, typically containing
thousands of servers for cloud computing operations. \emph{Appears in:
\textbf{?@sec-ml-system-architecture}}
\item[\textbf{data contracts}]
Explicit agreements between data producers and consumers defining
schema, quality expectations, and service levels to prevent downstream
breakage. \emph{Appears in: \textbf{?@sec-data-engineering-ml}}
\item[\textbf{data curation}]
The process of selecting, organizing, and maintaining high-quality
datasets by removing irrelevant information, correcting errors, and
ensuring data meets specific standards for machine learning
applications. \emph{Appears in: \textbf{?@sec-ai-development-workflow}}
\item[\textbf{data drift}]
The phenomenon where the statistical properties of input data change
over time, causing machine learning model performance to degrade even
when the underlying code remains unchanged \emph{Appears in:
\textbf{?@sec-machine-learning-operations-mlops},
\textbf{?@sec-ai-development-workflow}}
\item[\textbf{data governance}]
The framework of policies, procedures, and technologies that ensure data
security, privacy, compliance, and ethical use throughout the machine
learning pipeline. \emph{Appears in: \textbf{?@sec-data-engineering-ml}}
\item[\textbf{data ingestion}]
The process of collecting and importing raw data from various sources
into a system where it can be stored, processed, and prepared for
machine learning applications \emph{Appears in:
\textbf{?@sec-data-engineering-ml},
\textbf{?@sec-ai-development-workflow}}
\item[\textbf{data lake}]
A storage repository that holds structured, semi-structured, and
unstructured data in its native format, using schema-on-read approaches
for flexible data analysis. \emph{Appears in:
\textbf{?@sec-data-engineering-ml}}
\item[\textbf{data lineage}]
The documentation and tracking of data flow through various
transformations and processes, providing visibility into data origins
and modifications for compliance and debugging \emph{Appears in:
\textbf{?@sec-data-engineering-ml},
\textbf{?@sec-machine-learning-operations-mlops}}
\item[\textbf{data parallelism}]
A distributed training strategy that splits the dataset across multiple
devices while each device maintains a complete copy of the model,
enabling parallel computation of gradients. \emph{Appears in:
\textbf{?@sec-benchmarking-ai}, \textbf{?@sec-ai-frameworks},
\textbf{?@sec-ai-training}}
\item[\textbf{data pipeline}]
The infrastructure and workflows that automate the movement and
transformation of data from sources through processing stages to final
storage or consumption. \emph{Appears in:
\textbf{?@sec-data-engineering-ml}}
\item[\textbf{data quality}]
The degree to which data meets requirements for accuracy, completeness,
consistency, and timeliness, directly impacting machine learning model
performance. \emph{Appears in: \textbf{?@sec-data-engineering-ml}}
\item[\textbf{data quality multiplier}]
The concept that improvements in data quality (like cleaner labels)
yield multiplicative rather than additive gains in model performance
compared to model tweaks. \emph{Appears in:
\textbf{?@sec-data-engineering-ml}}
\item[\textbf{data validation}]
The systematic verification that collected data meets quality standards,
is properly formatted, and contains accurate information suitable for
machine learning model training and evaluation \emph{Appears in:
\textbf{?@sec-data-engineering-ml},
\textbf{?@sec-ai-development-workflow}}
\item[\textbf{data versioning}]
The practice of tracking and managing different versions of datasets
over time, similar to code versioning, to ensure reproducibility and
enable rollback to previous data states when needed \emph{Appears in:
\textbf{?@sec-machine-learning-operations-mlops},
\textbf{?@sec-ai-development-workflow}}
\item[\textbf{data warehouse}]
A centralized repository optimized for analytical queries (OLAP) that
stores integrated, structured data from multiple sources in a
standardized schema. \emph{Appears in:
\textbf{?@sec-data-engineering-ml}}
\item[\textbf{data-centric approach}]
A machine learning paradigm that prioritizes improving data quality,
diversity, and curation rather than solely focusing on model
architecture improvements to achieve better performance. \emph{Appears
in: \textbf{?@sec-conclusion}}
\item[\textbf{data-centric computing}]
Systems optimized for the efficient ingestion of data and iterative
refinement of model parameters, where the programmer's job is to curate
data. \emph{Appears in: \textbf{?@sec-introduction}}
\item[\textbf{dataflow architecture}]
Specialized computing architecture where instruction execution is
determined by data availability rather than a program counter, enabling
highly parallel processing of neural network operations. \emph{Appears
in: \textbf{?@sec-ai-acceleration}}
\item[\textbf{dataflow challenges}]
Technical difficulties in managing data movement and dependencies in
hardware accelerators, including memory bandwidth limitations and
synchronization requirements. \emph{Appears in:
\textbf{?@sec-ai-acceleration}}
\item[\textbf{datasheets for datasets}]
Documentation for training data that captures provenance, collection
methodology, demographic composition, and known limitations affecting
model behavior. \emph{Appears in:
\textbf{?@sec-responsible-engineering}}
\item[\textbf{dead letter queue}]
A separate storage mechanism for data that fails processing, allowing
for later analysis and potential reprocessing of problematic data
without blocking the main pipeline. \emph{Appears in:
\textbf{?@sec-data-engineering-ml}}
\item[\textbf{deep learning}]
A subfield of machine learning that uses artificial neural networks with
multiple layers to automatically learn hierarchical representations from
data without explicit feature engineering. \emph{Appears in:
\textbf{?@sec-benchmarking-ai},
\textbf{?@sec-deep-learning-systems-foundations},
\textbf{?@sec-dnn-architectures}}
\item[\textbf{demographic parity}]
A fairness criterion requiring that the probability of receiving a
positive prediction is independent of group membership across protected
attributes. \emph{Appears in: \textbf{?@sec-responsible-engineering}}
\item[\textbf{dennard scaling}]
The observation that as transistors became smaller, their power density
remained constant, enabling higher performance at the same power
envelope. Its breakdown around 2005 ended the era of free frequency
scaling. \emph{Appears in: \textbf{?@sec-ml-system-architecture}}
\item[\textbf{dense layer}]
A fully-connected neural network layer where each neuron receives input
from all neurons in the previous layer, enabling comprehensive
information integration across features. \emph{Appears in:
\textbf{?@sec-deep-learning-systems-foundations},
\textbf{?@sec-dnn-architectures}}
\item[\textbf{dense matrix-matrix multiplication}]
The fundamental computational operation in neural networks that
dominates training time, accounting for 60-90\% of computation in
typical models. \emph{Appears in: \textbf{?@sec-ai-training}}
\item[\textbf{deployment constraints}]
Operational limitations such as hardware resources, network
connectivity, regulatory requirements, and integration requirements that
influence how machine learning models are implemented in production
environments. \emph{Appears in: \textbf{?@sec-ai-development-workflow}}
\item[\textbf{deployment paradigm}]
A distinct approach to hosting and executing ML models characterized by
specific resource constraints and operational properties, such as Cloud
ML, Edge ML, Mobile ML, or TinyML. \emph{Appears in:
\textbf{?@sec-ml-system-architecture}}
\item[\textbf{devops}]
Software development practice that combines development and operations
teams to shorten development cycles and deliver high-quality software
through automation and collaboration. \emph{Appears in:
\textbf{?@sec-machine-learning-operations-mlops}}
\item[\textbf{dhrystone}]
Integer-based benchmark introduced in 1984 that measures integer and
string operations in DMIPS (Dhrystone MIPS), designed to complement
floating-point benchmarks with typical programming constructs.
\emph{Appears in: \textbf{?@sec-benchmarking-ai}}
\item[\textbf{diabetic retinopathy}]
A diabetes complication that damages blood vessels in the retina,
serving as a leading cause of preventable blindness and a key
application area for medical AI screening systems. \emph{Appears in:
\textbf{?@sec-ai-development-workflow}}
\item[\textbf{differential privacy}]
A mathematical framework for quantifying and limiting the privacy loss
when releasing statistical information about datasets, ensuring
individual privacy while enabling useful data analysis \emph{Appears in:
\textbf{?@sec-conclusion}, \textbf{?@sec-data-engineering-ml}}
\item[\textbf{disaggregated evaluation}]
The practice of breaking down model performance metrics by demographic
groups or other factors to reveal disparities that are hidden by
aggregate measures. \emph{Appears in:
\textbf{?@sec-responsible-engineering}}
\item[\textbf{distributed computing}]
An approach that processes data across multiple machines or processors
simultaneously, enabling scalable handling of large datasets through
frameworks like Apache Spark. \emph{Appears in:
\textbf{?@sec-data-engineering-ml}}
\item[\textbf{distributed intelligence}]
The placement of computational capabilities across multiple devices and
locations rather than relying on a single centralized system, enabling
local processing and decision-making. \emph{Appears in:
\textbf{?@sec-ml-system-architecture}}
\item[\textbf{distributed training}]
A method of training machine learning models across multiple machines or
devices to handle larger datasets and models that exceed single-device
computational or memory capacity. \emph{Appears in:
\textbf{?@sec-benchmarking-ai}, \textbf{?@sec-conclusion},
\textbf{?@sec-ai-training}}
\item[\textbf{distribution shift}]
A change in the statistical properties of data between training and
deployment, or over time during deployment. Types include covariate
shift (input distribution changes), label shift (output distribution
changes), and concept drift (relationship between inputs and outputs
changes). \emph{Appears in: \textbf{?@sec-introduction}}
\item[\textbf{dlrm}]
Deep Learning Recommendation Model, an architecture developed by Meta
that combines categorical embeddings with a bottom multi-layer
perceptron (MLP) and top MLP to handle the massive scale and sparsity of
recommendation system workloads. \emph{Appears in:
\textbf{?@sec-dnn-architectures}}
\item[\textbf{domain-specific architecture}]
Hardware designs tailored to optimize specific computational workloads,
trading flexibility for improved performance and energy efficiency
compared to general-purpose processors. \emph{Appears in:
\textbf{?@sec-ai-acceleration}}
\item[\textbf{dropout}]
A regularization technique that randomly sets a fraction of input units
to zero during training to prevent overfitting and improve
generalization. \emph{Appears in: \textbf{?@sec-dnn-architectures}}
\item[\textbf{dying relu}]
A failure mode where ReLU neurons become permanently inactive,
outputting zero for all inputs due to negative pre-activations.
\emph{Appears in: \textbf{?@sec-deep-learning-systems-foundations}}
\item[\textbf{dying relu problem}]
A phenomenon where ReLU neurons become permanently inactive and output
zero for all inputs, preventing them from contributing to learning when
weighted inputs consistently produce negative values. \emph{Appears in:
\textbf{?@sec-ai-training}}
\item[\textbf{dynamic graph}]
A computational graph that is built and modified during program
execution, allowing for flexible model architectures and easier
debugging but potentially limiting optimization opportunities compared
to static graphs. \emph{Appears in: \textbf{?@sec-ai-frameworks}}
\item[\textbf{dynamic pruning}]
A model optimization technique that removes unnecessary parameters from
neural networks while maintaining predictive performance, reducing model
size and computational cost by eliminating redundant weights, neurons,
or layers. \emph{Appears in: \textbf{?@sec-model-compression}}
\item[\textbf{dynamic quantization}]
The process of reducing numerical precision in neural networks by
mapping high-precision weights and activations to lower-bit
representations, significantly reducing memory usage and computational
requirements \emph{Appears in: \textbf{?@sec-benchmarking-ai},
\textbf{?@sec-model-compression}}
\item[\textbf{dynamic random access memory (dram)}]
A type of volatile memory that stores data in capacitors and requires
periodic refresh cycles, commonly used as main memory in computer
systems. \emph{Appears in: \textbf{?@sec-ai-acceleration}}
\item[\textbf{dynamic voltage and frequency scaling (dvfs)}]
Power management technique that adjusts processor voltage and clock
frequency based on workload demands to optimize energy consumption while
maintaining performance. \emph{Appears in:
\textbf{?@sec-benchmarking-ai}}
\end{description}

\section*{E}\label{e}
\addcontentsline{toc}{section}{E}

\markright{E}

\begin{description}
\tightlist
\item[\textbf{eager execution}]
An execution mode where operations are evaluated immediately as they are
called in the code, providing intuitive debugging and development
experience but potentially sacrificing some optimization opportunities
available in graph-based execution. \emph{Appears in:
\textbf{?@sec-ai-frameworks}}
\item[\textbf{early exit architectures}]
Neural network designs that include multiple prediction heads at
different depths, allowing samples to exit early when confident
predictions can be made, reducing average computational cost per
inference. \emph{Appears in: \textbf{?@sec-model-compression}}
\item[\textbf{edge computing}]
A distributed computing paradigm that brings computation and data
storage closer to the sources of data, reducing latency and bandwidth
usage. \emph{Appears in: \textbf{?@sec-conclusion},
\textbf{?@sec-ai-acceleration}, \textbf{?@sec-ml-system-architecture}}
\item[\textbf{edge deployment}]
A deployment strategy where machine learning models run locally on
devices at the network edge rather than in centralized cloud servers,
reducing latency and enabling operation without constant internet
connectivity. \emph{Appears in: \textbf{?@sec-ai-development-workflow}}
\item[\textbf{efficiency frontier}]
The optimal trade-off curve between accuracy and computational cost
(latency, energy, or memory), where no model exists that is both more
accurate and more efficient. \emph{Appears in:
\textbf{?@sec-dnn-architectures}}
\item[\textbf{efficientnet}]
A family of neural network architectures discovered through Neural
Architecture Search that achieves better accuracy-efficiency trade-offs
by using compound scaling to balance network depth, width, and input
resolution. \emph{Appears in: \textbf{?@sec-model-compression}}
\item[\textbf{eliza}]
One of the first chatbots created by MIT's Joseph Weizenbaum in 1966
that could simulate human conversation through pattern matching and
substitution, notable because people began forming emotional attachments
to this simple program. \emph{Appears in: \textbf{?@sec-introduction}}
\item[\textbf{elt (extract, load, transform)}]
A data processing paradigm that first loads raw data into the target
system before applying transformations, providing flexibility for
evolving analytical needs. \emph{Appears in:
\textbf{?@sec-data-engineering-ml}}
\item[\textbf{embedded systems}]
Computer systems with dedicated functions within larger mechanical or
electrical systems, typically designed for specific tasks with real-time
computing constraints. \emph{Appears in:
\textbf{?@sec-ml-system-architecture}}
\item[\textbf{emergent behaviors}]
Unexpected system-wide patterns or characteristics that arise from the
interaction of individual components, often becoming apparent only when
systems operate at scale or in real-world conditions. \emph{Appears in:
\textbf{?@sec-ai-development-workflow}}
\item[\textbf{encoder-decoder}]
An architectural pattern where an encoder processes input into a
compressed representation and a decoder generates output from this
representation, commonly used in sequence-to-sequence tasks.
\emph{Appears in: \textbf{?@sec-dnn-architectures}}
\item[\textbf{end-to-end benchmarks}]
Comprehensive evaluation methodology that assesses entire AI system
pipelines including data processing, model execution, post-processing,
and infrastructure components. \emph{Appears in:
\textbf{?@sec-benchmarking-ai}}
\item[\textbf{energy bandwidth}]
The constraint in battery-powered systems where the energy cost of
transmitting data exceeds the energy cost of processing it locally,
dictating edge processing. \emph{Appears in:
\textbf{?@sec-ml-system-architecture}}
\item[\textbf{energy efficiency}]
The measure of computational work performed per unit of energy consumed,
typically expressed as operations per joule and crucial for
battery-powered and data center deployments \emph{Appears in:
\textbf{?@sec-benchmarking-ai}, \textbf{?@sec-ai-acceleration}}
\item[\textbf{energy star}]
EPA certification program that establishes energy efficiency standards
for computing equipment, requiring systems to meet strict efficiency
requirements during operation and sleep modes. \emph{Appears in:
\textbf{?@sec-benchmarking-ai}}
\item[\textbf{epoch}]
One complete pass through the entire training dataset during neural
network training, consisting of multiple batch iterations depending on
dataset size and batch size. \emph{Appears in:
\textbf{?@sec-deep-learning-systems-foundations},
\textbf{?@sec-ai-frameworks}}
\item[\textbf{equal opportunity}]
A fairness criterion requiring equal true positive rates among qualified
applicants across different demographic groups. \emph{Appears in:
\textbf{?@sec-responsible-engineering}}
\item[\textbf{equalized odds}]
A fairness criterion requiring that both true positive and false
positive rates are equal across different demographic groups.
\emph{Appears in: \textbf{?@sec-responsible-engineering}}
\item[\textbf{etl (extract, transform, load)}]
A traditional data processing paradigm that transforms data before
loading it into a data warehouse, resulting in ready-to-query formatted
data. \emph{Appears in: \textbf{?@sec-data-engineering-ml}}
\item[\textbf{experiment tracking}]
The systematic recording and management of machine learning experiments,
including hyperparameters, model versions, training data, and
performance metrics, to enable comparison and reproducibility
\emph{Appears in: \textbf{?@sec-machine-learning-operations-mlops},
\textbf{?@sec-ai-development-workflow}}
\item[\textbf{expert systems}]
AI systems from the mid-1970s that captured human expert knowledge in
specific domains, exemplified by MYCIN for diagnosing blood infections,
representing a shift from general AI to domain-specific applications.
\emph{Appears in: \textbf{?@sec-introduction}}
\end{description}

\section*{F}\label{f}
\addcontentsline{toc}{section}{F}

\markright{F}

\begin{description}
\tightlist
\item[\textbf{farmbeats}]
A Microsoft Research project that applies machine learning and IoT
technologies to agriculture, using edge computing to collect real-time
data on soil conditions and crop health while demonstrating distributed
AI systems in challenging real-world environments. \emph{Appears in:
\textbf{?@sec-introduction}}
\item[\textbf{feature engineering}]
The process of manually designing and extracting relevant features from
raw data to improve machine learning model performance, largely
automated in deep learning systems. \emph{Appears in:
\textbf{?@sec-data-engineering-ml},
\textbf{?@sec-deep-learning-systems-foundations}}
\item[\textbf{feature map}]
The output of a convolutional layer representing the response of learned
filters to different spatial locations in the input, capturing detected
features at various positions. \emph{Appears in:
\textbf{?@sec-dnn-architectures}}
\item[\textbf{feature store}]
A specialized data storage system that provides standardized, reusable
features for machine learning, enabling feature sharing across multiple
models and teams \emph{Appears in: \textbf{?@sec-data-engineering-ml},
\textbf{?@sec-machine-learning-operations-mlops}}
\item[\textbf{federated learning}]
A machine learning approach that trains algorithms across decentralized
edge devices or servers holding local data samples, without exchanging
the raw data. \emph{Appears in: \textbf{?@sec-conclusion},
\textbf{?@sec-ai-frameworks}, \textbf{?@sec-ml-system-architecture},
\textbf{?@sec-ai-development-workflow}}
\item[\textbf{feedback loop}]
A phenomenon where a model's outputs influence its own future training
data, potentially reinforcing and amplifying initial biases over time.
\emph{Appears in: \textbf{?@sec-responsible-engineering}}
\item[\textbf{feedback loops}]
Cyclical processes where outputs from later stages of the machine
learning lifecycle inform and influence decisions in earlier stages,
enabling continuous system improvement and adaptation. \emph{Appears in:
\textbf{?@sec-ai-development-workflow}}
\item[\textbf{feedforward network}]
A neural network architecture where information flows in one direction
from input to output layers without cycles, forming the foundation for
many deep learning models. \emph{Appears in:
\textbf{?@sec-deep-learning-systems-foundations},
\textbf{?@sec-dnn-architectures}}
\item[\textbf{field-programmable gate array}]
Reconfigurable hardware that can be programmed for specific tasks,
offering flexibility between general-purpose processors and
application-specific integrated circuits, useful for custom ML
accelerations. \emph{Appears in: \textbf{?@sec-ai-training}}
\item[\textbf{field-programmable gate array (fpga)}]
A reconfigurable integrated circuit that can be programmed after
manufacturing to implement custom digital circuits and specialized
computations. \emph{Appears in: \textbf{?@sec-ai-acceleration}}
\item[\textbf{five-pillar framework}]
An organizational structure for ML systems engineering comprising five
interconnected disciplines: Data Engineering, Training Systems,
Deployment Infrastructure, Operations and Monitoring, and Ethics and
Governance. \emph{Appears in: \textbf{?@sec-introduction}}
\item[\textbf{floating-point unit (fpu)}]
A specialized processor component designed to perform arithmetic
operations on floating-point numbers with high precision and efficiency.
\emph{Appears in: \textbf{?@sec-ai-acceleration}}
\item[\textbf{flops}]
Floating Point Operations Per Second, a measure of computational
throughput that quantifies the number of mathematical operations
involving decimal numbers a system can perform. \emph{Appears in:
\textbf{?@sec-benchmarking-ai},
\textbf{?@sec-deep-learning-systems-foundations},
\textbf{?@sec-model-compression}}
\item[\textbf{forward pass}]
The computation phase where input data flows through a neural network's
layers to produce outputs, involving matrix multiplications and
activation function applications. \emph{Appears in:
\textbf{?@sec-ai-training}}
\item[\textbf{forward propagation}]
The process of computing neural network predictions by passing input
data through successive layers, applying weights, biases, and activation
functions at each stage. \emph{Appears in:
\textbf{?@sec-deep-learning-systems-foundations}}
\item[\textbf{foundation model}]
Large-scale machine learning models trained on broad data that can be
adapted to a wide range of downstream tasks, serving as a base for
specialized applications. \emph{Appears in:
\textbf{?@sec-ml-system-architecture}}
\item[\textbf{four pillars framework}]
A data engineering framework organizing concerns into Quality,
Reliability, Scalability, and Governance to manage the data lifecycle.
\emph{Appears in: \textbf{?@sec-data-engineering-ml}}
\item[\textbf{fp16}]
16-bit floating-point numerical representation that reduces memory usage
and accelerates computation while maintaining acceptable precision for
many machine learning applications. \emph{Appears in:
\textbf{?@sec-benchmarking-ai}}
\item[\textbf{fp16 computation}]
The use of 16-bit floating-point arithmetic for neural network
operations to reduce memory usage and increase computational speed on
modern hardware accelerators. \emph{Appears in:
\textbf{?@sec-ai-training}}
\item[\textbf{fp32}]
32-bit floating-point numerical representation that provides standard
precision for mathematical computations but requires more memory and
computational resources than lower-precision formats. \emph{Appears in:
\textbf{?@sec-benchmarking-ai}}
\item[\textbf{fp32 to int8}]
A common quantization transformation that converts 32-bit floating point
weights and activations to 8-bit integers, achieving roughly 4x memory
reduction while maintaining acceptable accuracy for many models.
\emph{Appears in: \textbf{?@sec-model-compression}}
\item[\textbf{framework decomposition}]
The systematic breakdown of neural network frameworks into
hardware-mappable components, enabling efficient distribution of
operations across processing elements. \emph{Appears in:
\textbf{?@sec-ai-acceleration}}
\end{description}

\section*{G}\label{g}
\addcontentsline{toc}{section}{G}

\markright{G}

\begin{description}
\tightlist
\item[\textbf{gemm}]
General Matrix Multiply operations that follow the pattern C = αAB + βC,
representing the fundamental computational kernel underlying most neural
network operations including fully connected layers and convolutional
layers. \emph{Appears in: \textbf{?@sec-ai-frameworks}}
\item[\textbf{gemv}]
General Matrix-Vector multiplication operations that compute the product
of a matrix and a vector, commonly used in neural network computations
and requiring careful optimization for memory access patterns.
\emph{Appears in: \textbf{?@sec-ai-frameworks}}
\item[\textbf{generalization}]
The ability of a machine learning model to perform well on unseen data
that differs from the training set, often improved through diverse and
high-quality training data. \emph{Appears in:
\textbf{?@sec-data-engineering-ml}}
\item[\textbf{generalization gap}]
The difference between a model's performance on training data and its
performance on unseen real-world data. A large generalization gap
indicates the model has memorized training examples rather than learning
transferable patterns. \emph{Appears in: \textbf{?@sec-introduction}}
\item[\textbf{generative ai}]
A category of artificial intelligence systems capable of creating new
content such as text, images, audio, or video based on learned patterns
from training data. \emph{Appears in: \textbf{?@sec-conclusion}}
\item[\textbf{gpt-2}]
A 1.5-billion parameter autoregressive language model released by OpenAI
in 2019, serving as a primary Lighthouse Archetype for analyzing memory
bandwidth constraints in Transformer inference. \emph{Appears in:
\textbf{?@sec-dnn-architectures}}
\item[\textbf{gpu}]
Graphics Processing Unit, a specialized electronic circuit designed to
rapidly manipulate and alter memory to accelerate the creation of images
and parallel processing tasks. \emph{Appears in:
\textbf{?@sec-benchmarking-ai}, \textbf{?@sec-ml-system-architecture}}
\item[\textbf{graceful degradation}]
A system design principle where services continue functioning with
reduced capabilities when faced with partial failures or data
unavailability. \emph{Appears in: \textbf{?@sec-data-engineering-ml}}
\item[\textbf{gradient accumulation}]
A technique that simulates larger batch sizes by accumulating gradients
from multiple smaller batches before updating model parameters, enabling
training with limited memory. \emph{Appears in:
\textbf{?@sec-ai-training}}
\item[\textbf{gradient clipping}]
A regularization technique that prevents gradient explosion by limiting
the magnitude of gradients during backpropagation, typically by scaling
gradients when their norm exceeds a threshold. \emph{Appears in:
\textbf{?@sec-ai-frameworks}, \textbf{?@sec-ai-training}}
\item[\textbf{gradient compression}]
A technique used in distributed training to reduce the communication
overhead by compressing gradient information exchanged between computing
nodes. \emph{Appears in: \textbf{?@sec-conclusion}}
\item[\textbf{gradient descent}]
An optimization algorithm that iteratively adjusts neural network
parameters in the direction that minimizes the loss function, using
gradients to determine update directions and magnitudes. \emph{Appears
in: \textbf{?@sec-deep-learning-systems-foundations},
\textbf{?@sec-dnn-architectures}, \textbf{?@sec-ai-frameworks},
\textbf{?@sec-ai-training}}
\item[\textbf{gradient synchronization}]
The process in distributed training where locally computed gradients are
aggregated across devices to ensure all devices update their parameters
consistently. \emph{Appears in: \textbf{?@sec-ai-training}}
\item[\textbf{gradient-based pruning}]
A pruning method that uses gradient information during training to
identify neurons or filters with smaller gradient magnitudes, which
contribute less to reducing the loss function and can be safely removed.
\emph{Appears in: \textbf{?@sec-model-compression}}
\item[\textbf{graphics processing unit}]
A specialized processor originally designed for rendering graphics that
provides parallel processing capabilities essential for efficient neural
network computation and training. \emph{Appears in:
\textbf{?@sec-deep-learning-systems-foundations},
\textbf{?@sec-ai-training}}
\item[\textbf{graphics processing unit (gpu)}]
A specialized processor originally designed for graphics rendering that
provides massive parallel computing capabilities well-suited for neural
network computations. \emph{Appears in: \textbf{?@sec-ai-acceleration}}
\item[\textbf{green ai}]
A movement in AI research and practice that prioritizes computational
efficiency and energy consumption as primary metrics alongside
traditional performance metrics like accuracy. \emph{Appears in:
\textbf{?@sec-responsible-engineering}}
\item[\textbf{green500}]
Ranking system that evaluates the world's most powerful supercomputers
based on energy efficiency measured in FLOPS per watt rather than raw
computational performance. \emph{Appears in:
\textbf{?@sec-benchmarking-ai}}
\item[\textbf{ground truth}]
The objective reality or actual state of the phenomenon being observed,
used as the reference standard (labels) for training and evaluating
machine learning models. \emph{Appears in:
\textbf{?@sec-data-engineering-ml}}
\item[\textbf{groupwise quantization}]
A quantization approach where parameters are divided into groups, with
each group sharing quantization parameters, offering a balance between
compression and accuracy by providing more granular control than
layerwise methods. \emph{Appears in: \textbf{?@sec-model-compression}}
\item[\textbf{gru}]
Gated Recurrent Unit, a simplified variant of LSTM that uses fewer gates
while maintaining the ability to capture long-term dependencies in
sequential data. \emph{Appears in: \textbf{?@sec-dnn-architectures}}
\end{description}

\section*{H}\label{h}
\addcontentsline{toc}{section}{H}

\markright{H}

\begin{description}
\tightlist
\item[\textbf{hardware abstraction}]
The layer in ML frameworks that provides a unified interface to diverse
computing hardware (CPUs, GPUs, TPUs, accelerators) while handling
device-specific optimizations and memory management behind the scenes.
\emph{Appears in: \textbf{?@sec-ai-frameworks}}
\item[\textbf{hardware acceleration}]
The use of specialized computing hardware to perform certain operations
faster and more efficiently than software running on general-purpose
processors. \emph{Appears in: \textbf{?@sec-ai-acceleration}}
\item[\textbf{hardware accelerator}]
Specialized computing hardware designed to efficiently execute specific
types of computations, such as GPUs for parallel processing or TPUs for
machine learning workloads. \emph{Appears in:
\textbf{?@sec-benchmarking-ai}}
\item[\textbf{hardware-aware design}]
The practice of designing neural network architectures specifically
optimized for target hardware platforms, considering factors like memory
hierarchy, compute units, and data movement patterns to maximize
efficiency. \emph{Appears in: \textbf{?@sec-model-compression}}
\item[\textbf{hdfs (hadoop distributed file system)}]
A distributed file system designed to store large datasets across
clusters of commodity hardware, providing scalability and fault
tolerance for big data applications. \emph{Appears in:
\textbf{?@sec-data-engineering-ml}}
\item[\textbf{hidden layer}]
An intermediate layer in a neural network between input and output
layers that learns abstract representations by transforming data through
learned weights and activation functions. \emph{Appears in:
\textbf{?@sec-deep-learning-systems-foundations}}
\item[\textbf{hidden state}]
The internal memory of recurrent neural networks that carries
information from previous time steps, enabling the network to maintain
context across sequential inputs. \emph{Appears in:
\textbf{?@sec-dnn-architectures}}
\item[\textbf{hierarchical processing}]
A multi-tier system architecture where data and intelligence flow
between different levels of the computing stack, from sensors to edge
devices to cloud systems. \emph{Appears in:
\textbf{?@sec-ml-system-architecture}}
\item[\textbf{high bandwidth memory (hbm)}]
An advanced memory technology that provides much higher bandwidth than
traditional DRAM by using 3D stacking and wide interfaces, critical for
data-intensive AI workloads. \emph{Appears in:
\textbf{?@sec-ai-acceleration}}
\item[\textbf{horizontal scaling}]
Increasing system capacity by adding more machines or instances rather
than upgrading existing hardware, providing better fault tolerance and
load distribution. \emph{Appears in:
\textbf{?@sec-machine-learning-operations-mlops}}
\item[\textbf{hybrid machine learning}]
The integration of multiple ML paradigms such as cloud, edge, mobile,
and tiny ML to form unified distributed systems that leverage
complementary strengths. \emph{Appears in:
\textbf{?@sec-ml-system-architecture}}
\item[\textbf{hybrid parallelism}]
A distributed training approach that combines data parallelism and model
parallelism to leverage the benefits of both strategies for training
very large models. \emph{Appears in: \textbf{?@sec-ai-training}}
\item[\textbf{hyperparameter}]
A configuration setting that controls the learning process but is not
learned from data, such as learning rate, batch size, or network
architecture choices. \emph{Appears in: \textbf{?@sec-benchmarking-ai},
\textbf{?@sec-deep-learning-systems-foundations},
\textbf{?@sec-ai-frameworks}}
\item[\textbf{hyperparameters}]
Configuration settings that control the learning process of machine
learning algorithms but are not learned from data, such as learning
rate, batch size, and network architecture parameters. \emph{Appears in:
\textbf{?@sec-ai-development-workflow}}
\item[\textbf{hyperscale data center}]
Large-scale data center facilities containing thousands of servers and
covering extensive floor space, designed to efficiently support massive
computing workloads. \emph{Appears in:
\textbf{?@sec-ml-system-architecture}}
\end{description}

\section*{I}\label{i}
\addcontentsline{toc}{section}{I}

\markright{I}

\begin{description}
\tightlist
\item[\textbf{imagenet}]
A massive visual database containing over 14 million labeled images
across 20,000+ categories, created by Stanford's Fei-Fei Li starting in
2009, whose annual challenge became instrumental in driving breakthrough
advances in computer vision \emph{Appears in:
\textbf{?@sec-benchmarking-ai}, \textbf{?@sec-introduction}}
\item[\textbf{imperative programming}]
A programming paradigm where operations are executed immediately as they
are encountered in the code, allowing for natural control flow and
easier debugging but potentially limiting optimization opportunities.
\emph{Appears in: \textbf{?@sec-ai-frameworks}}
\item[\textbf{inference}]
The operational phase where trained neural networks make predictions on
new data using fixed parameters, without weight updates. \emph{Appears
in: \textbf{?@sec-deep-learning-systems-foundations},
\textbf{?@sec-ai-acceleration}}
\item[\textbf{information-compute ratio (icr)}]
A metric quantifying the efficiency of data selection, defined as the
ratio of model performance gain to the computational cost (FLOPs)
required to achieve it. Maximizing ICR is the primary goal of data
selection strategies. \emph{Appears in: \textbf{?@sec-data-selection}}
\item[\textbf{infrastructure as code}]
Practice of managing and provisioning computing infrastructure through
machine-readable configuration files rather than manual processes,
enabling version control and automation. \emph{Appears in:
\textbf{?@sec-machine-learning-operations-mlops}}
\item[\textbf{instruction set architecture (isa)}]
The interface between software and hardware that defines the set of
instructions a processor can execute, including data types and
addressing modes. \emph{Appears in: \textbf{?@sec-ai-acceleration}}
\item[\textbf{int8}]
8-bit integer numerical representation used in quantized neural networks
to reduce memory usage and accelerate inference while attempting to
maintain model accuracy. \emph{Appears in:
\textbf{?@sec-benchmarking-ai}}
\item[\textbf{int8 quantization}]
A numerical precision reduction technique that represents model weights
and activations using 8-bit integers instead of 32-bit floating point
numbers, reducing memory usage and enabling faster inference on
specialized hardware \emph{Appears in: \textbf{?@sec-ai-acceleration},
\textbf{?@sec-model-compression}}
\item[\textbf{internet of things}]
A network of physical objects embedded with sensors, software, and other
technologies that connect and exchange data with other devices and
systems over the internet. \emph{Appears in:
\textbf{?@sec-ml-system-architecture}}
\item[\textbf{intersectional analysis}]
Evaluation that considers combinations of demographic attributes (e.g.,
race and gender simultaneously) to detect concentrated harms not visible
in single-factor analysis. \emph{Appears in:
\textbf{?@sec-responsible-engineering}}
\item[\textbf{iops}]
Input/Output Operations Per Second; a storage performance metric
measuring the number of read/write operations a device can handle per
second, critical for random access workloads like training data loading.
\emph{Appears in: \textbf{?@sec-data-engineering-ml}}
\item[\textbf{iron law of ml systems}]
A quantitative framework decomposing ML system performance into three
terms: Data (limited by bandwidth), Compute (limited by FLOPS), and
Latency (limited by overhead). Formulated as T = D\_vol/BW + O/(R\_peak
* eta) + L\_lat. \emph{Appears in: \textbf{?@sec-introduction}}
\item[\textbf{iterative pruning}]
A gradual pruning strategy that removes parameters in multiple stages
with fine-tuning between each stage, allowing the model to adapt to
reduced capacity and typically achieving better accuracy than one-shot
pruning. \emph{Appears in: \textbf{?@sec-model-compression}}
\end{description}

\section*{J}\label{j}
\addcontentsline{toc}{section}{J}

\markright{J}

\begin{description}
\tightlist
\item[\textbf{jax}]
A numerical computing library developed by Google Research that combines
NumPy's API with functional programming transformations including
automatic differentiation, just-in-time compilation, and automatic
vectorization for high-performance machine learning research.
\emph{Appears in: \textbf{?@sec-ai-frameworks}}
\item[\textbf{jit compilation}]
Just-In-Time compilation that analyzes and optimizes code at runtime,
enabling frameworks to balance the flexibility of eager execution with
the performance benefits of graph optimization by compiling frequently
used functions. \emph{Appears in: \textbf{?@sec-ai-frameworks}}
\end{description}

\section*{K}\label{k}
\addcontentsline{toc}{section}{K}

\markright{K}

\begin{description}
\tightlist
\item[\textbf{k-anonymity}]
A privacy technique that ensures each record in a dataset is
indistinguishable from at least k-1 other records by generalizing
quasi-identifiers. \emph{Appears in: \textbf{?@sec-data-engineering-ml}}
\item[\textbf{kernel}]
A small matrix of learnable weights used in convolutional layers to
detect specific features through the convolution operation, also called
a filter. \emph{Appears in: \textbf{?@sec-dnn-architectures}}
\item[\textbf{kernel fusion}]
An optimization technique that combines multiple computational
operations into a single kernel to reduce memory transfers and improve
performance on parallel processors. \emph{Appears in:
\textbf{?@sec-ai-acceleration}}
\item[\textbf{key performance indicators}]
Specific, measurable metrics used to evaluate the success and
effectiveness of machine learning systems, such as accuracy, precision,
recall, latency, and throughput. \emph{Appears in:
\textbf{?@sec-ai-development-workflow}}
\item[\textbf{keyword spotting (kws)}]
A technology that detects specific wake words or phrases in audio
streams, typically used in voice-activated devices with constraints on
power consumption and latency. \emph{Appears in:
\textbf{?@sec-data-engineering-ml}}
\item[\textbf{knowledge distillation}]
A model compression technique where a smaller ``student'' network learns
to mimic the behavior of a larger ``teacher'' network by training on the
teacher's soft output probabilities rather than just hard labels
\emph{Appears in: \textbf{?@sec-conclusion},
\textbf{?@sec-model-compression}}
\end{description}

\section*{L}\label{l}
\addcontentsline{toc}{section}{L}

\markright{L}

\begin{description}
\tightlist
\item[\textbf{l0-norm constraint}]
A regularization technique that counts the number of non-zero parameters
in a model, used in structured pruning to directly control model
sparsity by penalizing the number of active weights. \emph{Appears in:
\textbf{?@sec-model-compression}}
\item[\textbf{lapack}]
Linear Algebra Package that extends BLAS with higher-level linear
algebra operations including matrix decompositions, eigenvalue problems,
and linear system solutions, providing essential mathematical
foundations for machine learning computations. \emph{Appears in:
\textbf{?@sec-ai-frameworks}}
\item[\textbf{latency}]
The time delay between a request for data and the delivery of that data,
critical in real-time applications where immediate responses are
required. \emph{Appears in: \textbf{?@sec-benchmarking-ai},
\textbf{?@sec-ai-acceleration}, \textbf{?@sec-ml-system-architecture}}
\item[\textbf{latency constraints}]
Real-time requirements that limit the maximum acceptable delay for model
inference, driving optimization decisions in deployment scenarios where
response time is critical. \emph{Appears in:
\textbf{?@sec-model-compression}}
\item[\textbf{layer normalization}]
A normalization technique that normalizes inputs across the features
dimension for each sample, commonly used in transformer architectures to
stabilize training. \emph{Appears in: \textbf{?@sec-dnn-architectures}}
\item[\textbf{layerwise quantization}]
A quantization granularity where all parameters within a single layer
share the same quantization parameters, providing computational
efficiency but potentially limiting representational precision compared
to finer-grained approaches. \emph{Appears in:
\textbf{?@sec-model-compression}}
\item[\textbf{learning rate}]
A hyperparameter that determines the step size for weight updates during
gradient descent optimization, critically affecting training stability
and convergence speed. \emph{Appears in:
\textbf{?@sec-deep-learning-systems-foundations},
\textbf{?@sec-ai-frameworks}}
\item[\textbf{learning rate scheduling}]
The systematic adjustment of learning rates during training, using
strategies like step decay, exponential decay, or cosine annealing to
improve convergence and final model performance. \emph{Appears in:
\textbf{?@sec-ai-training}}
\item[\textbf{lifecycle coherence}]
The principle that all stages of ML development should align with
overall system objectives, maintaining consistency in data handling,
model architecture, and evaluation criteria. \emph{Appears in:
\textbf{?@sec-ai-development-workflow}}
\item[\textbf{linear scaling failure}]
The phenomenon where increasing computing resources (e.g., doubling
GPUs) results in sub-linear performance gains due to communication
overhead and synchronization costs. \emph{Appears in:
\textbf{?@sec-ai-training}}
\item[\textbf{linpack}]
Benchmark developed at Argonne National Laboratory that measures system
performance by solving dense systems of linear equations, famous for its
use in Top500 supercomputer rankings. \emph{Appears in:
\textbf{?@sec-benchmarking-ai}}
\item[\textbf{llama}]
Large Language Model Meta AI, a family of open foundation models that
popularized efficient architectural choices like RMSNorm and SwiGLU,
serving as a modern reference point for large-scale Transformer
efficiency. \emph{Appears in: \textbf{?@sec-dnn-architectures}}
\item[\textbf{load balancing}]
Distribution of incoming requests across multiple server instances to
prevent bottlenecks, improve response times, and ensure high
availability. \emph{Appears in:
\textbf{?@sec-machine-learning-operations-mlops}}
\item[\textbf{logits}]
The raw, unnormalized scores output by the last layer of a neural
network before the activation function (like Softmax) converts them into
probabilities. \emph{Appears in:
\textbf{?@sec-deep-learning-systems-foundations}}
\item[\textbf{loss function}]
A mathematical function that quantifies the difference between neural
network predictions and true labels, providing the optimization
objective for training algorithms. \emph{Appears in:
\textbf{?@sec-deep-learning-systems-foundations}}
\item[\textbf{loss scaling}]
A technique used in mixed-precision training that multiplies the loss by
a large factor before backpropagation to prevent gradient underflow in
reduced precision formats. \emph{Appears in: \textbf{?@sec-ai-training}}
\item[\textbf{lottery ticket hypothesis}]
The theory that large neural networks contain sparse subnetworks that,
when trained in isolation from proper initialization, can achieve
comparable accuracy to the full network while being significantly
smaller. \emph{Appears in: \textbf{?@sec-model-compression}}
\item[\textbf{low-rank factorization}]
A matrix decomposition technique that approximates large weight matrices
as products of smaller matrices, reducing the number of parameters and
computational operations required for neural network layers.
\emph{Appears in: \textbf{?@sec-model-compression}}
\item[\textbf{lstm}]
Long Short-Term Memory, a type of recurrent neural network architecture
designed to handle long-term dependencies through gating mechanisms that
control information flow. \emph{Appears in:
\textbf{?@sec-dnn-architectures}}
\end{description}

\section*{M}\label{m}
\addcontentsline{toc}{section}{M}

\markright{M}

\begin{description}
\tightlist
\item[\textbf{machine learning}]
A subset of artificial intelligence that enables systems to
automatically improve performance on tasks through experience and data
rather than explicit programming. \emph{Appears in:
\textbf{?@sec-benchmarking-ai},
\textbf{?@sec-deep-learning-systems-foundations},
\textbf{?@sec-introduction}, \textbf{?@sec-ml-system-architecture}}
\item[\textbf{machine learning accelerator (ml accelerator)}]
Specialized computing hardware designed to efficiently execute machine
learning workloads through optimized matrix operations, memory
hierarchies, and parallel processing units. \emph{Appears in:
\textbf{?@sec-ai-acceleration}}
\item[\textbf{machine learning framework}]
A software platform that provides tools and abstractions for designing,
training, and deploying machine learning models, bridging user
applications with infrastructure through computational graphs, hardware
optimization, and workflow orchestration. \emph{Appears in:
\textbf{?@sec-ai-frameworks}}
\item[\textbf{machine learning frameworks}]
Software libraries and platforms that provide tools, APIs, and
abstractions for developing, training, and deploying machine learning
models, such as TensorFlow and PyTorch. \emph{Appears in:
\textbf{?@sec-conclusion}}
\item[\textbf{machine learning lifecycle}]
A structured, iterative process that encompasses all stages involved in
developing, deploying, and maintaining machine learning systems, from
problem definition through ongoing monitoring and improvement.
\emph{Appears in: \textbf{?@sec-ai-development-workflow}}
\item[\textbf{machine learning operations}]
The practice and set of tools focused on operationalizing machine
learning models through automation, monitoring, and management of the
entire ML pipeline from development to production. \emph{Appears in:
\textbf{?@sec-ai-development-workflow}}
\item[\textbf{machine learning systems engineering}]
The engineering discipline focused on building reliable, efficient, and
scalable AI systems across computational platforms, spanning the entire
AI lifecycle from data acquisition through deployment and operations
with emphasis on resource-awareness and system-level optimization.
\emph{Appears in: \textbf{?@sec-introduction}}
\item[\textbf{macro benchmarks}]
Evaluation methodology that assesses complete machine learning models to
understand how architectural choices and component interactions affect
overall system behavior and performance. \emph{Appears in:
\textbf{?@sec-benchmarking-ai}}
\item[\textbf{magnitude-based pruning}]
The most common pruning method that removes parameters with the smallest
absolute values, based on the assumption that weights with smaller
magnitudes contribute less to the model's output \emph{Appears in:
\textbf{?@sec-conclusion}, \textbf{?@sec-model-compression}}
\item[\textbf{mapping optimization}]
The process of assigning neural network operations to hardware resources
in a way that minimizes communication overhead and maximizes utilization
of available compute units. \emph{Appears in:
\textbf{?@sec-ai-acceleration}}
\item[\textbf{masking}]
An anonymization technique that alters or obfuscates sensitive values so
they cannot be directly traced back to the original data subject.
\emph{Appears in: \textbf{?@sec-data-engineering-ml}}
\item[\textbf{memory bandwidth}]
Rate at which data can be read from or written to memory, measured in
bytes per second, which often becomes a bottleneck in memory-intensive
machine learning workloads. \emph{Appears in:
\textbf{?@sec-benchmarking-ai}}
\item[\textbf{memory hierarchy}]
The organization of memory systems with different access speeds and
capacities, from fast on-chip caches to slower off-chip main memory.
\emph{Appears in: \textbf{?@sec-ai-acceleration}}
\item[\textbf{memory wall}]
The widening performance gap between processor speed and memory
bandwidth, where computational capacity outpaces the rate at which data
can be delivered to the processor, becoming a primary bottleneck for
large ML models. \emph{Appears in:
\textbf{?@sec-ml-system-architecture}}
\item[\textbf{metadata}]
Descriptive information about datasets that includes details about data
collection, quality metrics, validation status, and other contextual
information essential for data management. \emph{Appears in:
\textbf{?@sec-data-engineering-ml}}
\item[\textbf{micro benchmarks}]
Specialized evaluation tools that assess individual components or
specific operations within machine learning systems, such as tensor
operations or neural network layers. \emph{Appears in:
\textbf{?@sec-benchmarking-ai}}
\item[\textbf{microcontroller}]
A small computer on a single integrated circuit containing a processor
core, memory, and programmable input/output peripherals, commonly used
in embedded systems. \emph{Appears in:
\textbf{?@sec-ml-system-architecture}}
\item[\textbf{mini-batch gradient descent}]
A training approach that computes gradients and updates weights using a
small subset of training examples simultaneously, balancing
computational efficiency with gradient estimation quality. \emph{Appears
in: \textbf{?@sec-deep-learning-systems-foundations}}
\item[\textbf{mini-batch processing}]
An optimization approach that computes gradients over small batches of
examples, balancing the computational efficiency of batch processing
with the memory constraints of stochastic methods. \emph{Appears in:
\textbf{?@sec-ai-training}}
\item[\textbf{mixed-precision computing}]
A technique that uses different numerical precisions at various stages
of computation, such as FP16 for matrix multiplications and FP32 for
accumulations. \emph{Appears in: \textbf{?@sec-ai-acceleration}}
\item[\textbf{mixed-precision training}]
A training methodology that combines different numerical precisions
(typically FP16 and FP32) to optimize memory usage and computational
speed while maintaining training stability. \emph{Appears in:
\textbf{?@sec-benchmarking-ai}, \textbf{?@sec-conclusion},
\textbf{?@sec-model-compression}, \textbf{?@sec-ai-training}}
\item[\textbf{ml lifecycle}]
The comprehensive process of developing, deploying, and maintaining
machine learning systems, encompassing data collection, model
development, training, evaluation, deployment, monitoring, and iterative
improvement with continuous feedback loops. \emph{Appears in:
\textbf{?@sec-introduction}}
\item[\textbf{ml systems}]
Integrated computing systems comprising three core components: data that
guides algorithmic behavior, learning algorithms that extract patterns
from data, and computing infrastructure that enables both training and
inference processes. \emph{Appears in: \textbf{?@sec-introduction}}
\item[\textbf{ml systems spectrum}]
The range of machine learning system deployments from cloud-based
systems with abundant resources to tiny embedded devices with severe
constraints, each requiring different optimization strategies and
trade-offs. \emph{Appears in:
\textbf{?@sec-deep-learning-systems-foundations}}
\item[\textbf{mlcommons}]
Organization that develops and maintains industry-standard benchmarks
for machine learning systems, including the MLPerf suite for training
and inference evaluation. \emph{Appears in:
\textbf{?@sec-benchmarking-ai}}
\item[\textbf{mlops}]
Engineering discipline that manages the end-to-end lifecycle of machine
learning systems, combining ML development with operational practices
for reliable production deployment \emph{Appears in:
\textbf{?@sec-conclusion},
\textbf{?@sec-machine-learning-operations-mlops}}
\item[\textbf{mlperf}]
Industry-standard benchmark suite that provides standardized tests for
training and inference across various deep learning workloads, enabling
fair comparisons of machine learning systems. \emph{Appears in:
\textbf{?@sec-benchmarking-ai}}
\item[\textbf{mlperf inference}]
Benchmark framework that evaluates machine learning inference
performance across different deployment environments, from cloud data
centers to mobile devices and embedded systems. \emph{Appears in:
\textbf{?@sec-benchmarking-ai}}
\item[\textbf{mlperf mobile}]
Specialized benchmark that extends MLPerf evaluation to smartphones and
mobile devices, measuring latency and responsiveness under strict power
and memory constraints. \emph{Appears in:
\textbf{?@sec-benchmarking-ai}}
\item[\textbf{mlperf tiny}]
Benchmark designed for embedded and ultra-low-power AI systems such as
IoT devices, wearables, and microcontrollers operating with minimal
processing capabilities. \emph{Appears in:
\textbf{?@sec-benchmarking-ai}}
\item[\textbf{mlperf training}]
Standardized benchmark that evaluates machine learning training
performance by measuring time-to-accuracy, throughput, and resource
utilization across different hardware platforms. \emph{Appears in:
\textbf{?@sec-benchmarking-ai}}
\item[\textbf{mobile machine learning}]
The execution of machine learning models directly on portable,
battery-powered devices like smartphones and tablets, enabling
personalized and responsive applications. \emph{Appears in:
\textbf{?@sec-ml-system-architecture}}
\item[\textbf{model cards}]
A standardized format for documenting machine learning models, capturing
information essential for responsible deployment, including intended
use, performance factors, and ethical considerations. \emph{Appears in:
\textbf{?@sec-responsible-engineering}}
\item[\textbf{model compression}]
Techniques used to reduce the size and computational requirements of
machine learning models while preserving accuracy, enabling deployment
on resource-constrained devices. \emph{Appears in:
\textbf{?@sec-conclusion}, \textbf{?@sec-ml-system-architecture},
\textbf{?@sec-machine-learning-operations-mlops},
\textbf{?@sec-model-compression}}
\item[\textbf{model deployment}]
The process of integrating trained machine learning models into
production systems where they can make predictions on new data and
provide value to end users \emph{Appears in:
\textbf{?@sec-machine-learning-operations-mlops},
\textbf{?@sec-ai-development-workflow}}
\item[\textbf{model drift}]
The degradation of machine learning model performance over time due to
changes in data patterns, user behavior, or environmental conditions
that differ from the original training conditions \emph{Appears in:
\textbf{?@sec-machine-learning-operations-mlops},
\textbf{?@sec-ai-development-workflow}}
\item[\textbf{model evaluation}]
The systematic assessment of machine learning model performance using
various metrics and validation techniques to determine whether the model
meets requirements and is ready for deployment. \emph{Appears in:
\textbf{?@sec-ai-development-workflow}}
\item[\textbf{model optimization}]
The systematic refinement of machine learning models to enhance their
efficiency while maintaining effectiveness, balancing trade-offs between
accuracy, computational cost, memory usage, latency, and energy
efficiency \emph{Appears in: \textbf{?@sec-model-compression},
\textbf{?@sec-ai-development-workflow}}
\item[\textbf{model parallelism}]
A distributed training strategy that splits a neural network model
across multiple devices, with each device responsible for computing a
portion of the network. \emph{Appears in:
\textbf{?@sec-benchmarking-ai}, \textbf{?@sec-conclusion},
\textbf{?@sec-ai-frameworks}, \textbf{?@sec-ai-training}}
\item[\textbf{model quantization}]
The process of reducing the precision of numerical representations in
machine learning models, typically from 32-bit to 8-bit integers, to
decrease model size and increase inference speed. \emph{Appears in:
\textbf{?@sec-ml-system-architecture}}
\item[\textbf{model registry}]
Centralized repository for storing, versioning, and managing trained
machine learning models with associated metadata, facilitating model
governance and deployment. \emph{Appears in:
\textbf{?@sec-machine-learning-operations-mlops}}
\item[\textbf{model serving}]
Infrastructure and systems that expose deployed machine learning models
through APIs to handle prediction requests at scale with appropriate
latency and throughput. \emph{Appears in:
\textbf{?@sec-machine-learning-operations-mlops}}
\item[\textbf{model training}]
The process of using machine learning algorithms to learn patterns from
training data, adjusting model parameters to minimize prediction errors
and create a functional predictive system. \emph{Appears in:
\textbf{?@sec-ai-development-workflow}}
\item[\textbf{model validation}]
The process of testing machine learning models on independent datasets
to assess their generalization ability and ensure they perform reliably
on unseen data \emph{Appears in:
\textbf{?@sec-machine-learning-operations-mlops},
\textbf{?@sec-ai-development-workflow}}
\item[\textbf{model versioning}]
The systematic tracking and management of different versions of machine
learning models, including their parameters, training data, and
performance metrics, to enable comparison and rollback capabilities
\emph{Appears in: \textbf{?@sec-machine-learning-operations-mlops},
\textbf{?@sec-ai-development-workflow}}
\item[\textbf{model-centric ai}]
A research paradigm where the dataset is treated as fixed and
engineering effort focuses on optimizing model architecture.
\emph{Appears in: \textbf{?@sec-data-engineering-ml}}
\item[\textbf{momentum}]
An optimization technique that accumulates a velocity vector across
iterations to help gradient descent navigate through local minima and
accelerate convergence in consistent gradient directions. \emph{Appears
in: \textbf{?@sec-ai-training}}
\item[\textbf{monitoring}]
The continuous observation and measurement of machine learning system
performance, data quality, and operational metrics in production to
detect issues and trigger maintenance actions. \emph{Appears in:
\textbf{?@sec-ai-development-workflow}}
\item[\textbf{multi-head attention}]
An attention mechanism that uses multiple parallel attention heads, each
focusing on different aspects of the input to capture diverse types of
relationships simultaneously. \emph{Appears in:
\textbf{?@sec-dnn-architectures}}
\item[\textbf{multi-layer perceptron}]
A feedforward neural network with one or more hidden layers between
input and output, capable of learning non-linear mappings through dense
connections and activation functions. \emph{Appears in:
\textbf{?@sec-dnn-architectures}}
\item[\textbf{multilayer perceptron}]
A feedforward neural network with one or more hidden layers between
input and output layers, capable of learning nonlinear relationships in
data. \emph{Appears in:
\textbf{?@sec-deep-learning-systems-foundations}}
\item[\textbf{mycin}]
One of the first large-scale expert systems developed at Stanford in
1976 to diagnose blood infections, representing the shift toward
capturing human expert knowledge in specific domains rather than
pursuing general artificial intelligence. \emph{Appears in:
\textbf{?@sec-introduction}}
\end{description}

\section*{N}\label{n}
\addcontentsline{toc}{section}{N}

\markright{N}

\begin{description}
\tightlist
\item[\textbf{nas-generated architecture}]
Neural network architectures discovered through automated Neural
Architecture Search rather than manual design, often achieving better
efficiency-accuracy trade-offs through exhaustive exploration of design
spaces. \emph{Appears in: \textbf{?@sec-model-compression}}
\item[\textbf{network structure modification}]
Architectural changes to neural networks that improve efficiency,
including techniques like depthwise separable convolutions, bottleneck
layers, and efficient attention mechanisms that reduce computational
complexity. \emph{Appears in: \textbf{?@sec-model-compression}}
\item[\textbf{neural architecture search}]
An automated approach that uses machine learning algorithms to discover
optimal neural network architectures by searching through possible
combinations of layers, connections, and hyperparameters for specific
constraints. \emph{Appears in: \textbf{?@sec-model-compression}}
\item[\textbf{neural network}]
A computational model consisting of interconnected nodes organized in
layers that can learn to map inputs to outputs through adjustable
connection weights. \emph{Appears in: \textbf{?@sec-benchmarking-ai},
\textbf{?@sec-deep-learning-systems-foundations},
\textbf{?@sec-dnn-architectures}}
\item[\textbf{neural processing unit (npu)}]
Specialized processors designed specifically for accelerating neural
network operations and machine learning computations, optimized for
parallel processing of AI workloads. \emph{Appears in:
\textbf{?@sec-benchmarking-ai}, \textbf{?@sec-ai-acceleration},
\textbf{?@sec-ml-system-architecture}}
\item[\textbf{neuromorphic computing}]
A computing approach that mimics the structure and function of
biological neural networks, potentially offering more energy-efficient
processing for AI applications. \emph{Appears in:
\textbf{?@sec-conclusion}}
\item[\textbf{nosql}]
A category of database systems designed to handle large volumes of
unstructured or semi-structured data with flexible schemas, often used
in big data applications. \emph{Appears in:
\textbf{?@sec-data-engineering-ml}}
\item[\textbf{numerical precision optimization}]
The dimension of model optimization that addresses how numerical values
are represented and processed, including quantization techniques that
map high-precision values to lower-bit representations. \emph{Appears
in: \textbf{?@sec-model-compression}}
\end{description}

\section*{O}\label{o}
\addcontentsline{toc}{section}{O}

\markright{O}

\begin{description}
\tightlist
\item[\textbf{observability}]
Comprehensive monitoring approach that provides insight into system
behavior through metrics, logs, and traces, enabling understanding of
internal states from external outputs. \emph{Appears in:
\textbf{?@sec-machine-learning-operations-mlops}}
\item[\textbf{olap (online analytical processing)}]
A database approach optimized for complex analytical queries across
large datasets, typically used in data warehouses for business
intelligence. \emph{Appears in: \textbf{?@sec-data-engineering-ml}}
\item[\textbf{oltp (online transaction processing)}]
A database approach optimized for frequent, short transactions and
real-time processing, commonly used in operational applications.
\emph{Appears in: \textbf{?@sec-data-engineering-ml}}
\item[\textbf{on-chip memory}]
Fast memory integrated directly onto the processor chip, including
caches and scratchpad memory, providing high bandwidth and low latency
data access. \emph{Appears in: \textbf{?@sec-ai-acceleration}}
\item[\textbf{on-device learning}]
The capability for machine learning models to adapt and learn directly
on edge devices without requiring data transmission to external servers.
\emph{Appears in: \textbf{?@sec-conclusion}}
\item[\textbf{one-hot encoding}]
A representation where categorical labels become vectors with a single 1
and remaining 0s. \emph{Appears in:
\textbf{?@sec-deep-learning-systems-foundations}}
\item[\textbf{one-shot pruning}]
A pruning strategy where a large fraction of parameters is removed in a
single step, typically followed by fine-tuning to recover accuracy,
offering simplicity but potentially requiring more aggressive
fine-tuning. \emph{Appears in: \textbf{?@sec-model-compression}}
\item[\textbf{online inference}]
Real-time prediction serving that processes individual requests with low
latency, suitable for interactive applications requiring immediate
responses. \emph{Appears in:
\textbf{?@sec-machine-learning-operations-mlops}}
\item[\textbf{onnx}]
Open Neural Network Exchange, a standardized format for representing
machine learning models that enables interoperability between different
frameworks, allowing models trained in one framework to be deployed
using another. \emph{Appears in: \textbf{?@sec-ai-frameworks}}
\item[\textbf{onnx runtime}]
Cross-platform inference engine that optimizes machine learning models
through techniques like operator fusion and kernel tuning to improve
inference speed and reduce computational overhead. \emph{Appears in:
\textbf{?@sec-benchmarking-ai}}
\item[\textbf{optimizer}]
An algorithm that adjusts model parameters during training to minimize
the loss function, with common examples including SGD (Stochastic
Gradient Descent), Adam, and RMSprop, each with different strategies for
parameter updates. \emph{Appears in: \textbf{?@sec-ai-frameworks}}
\item[\textbf{orchestration}]
Coordination and management of complex workflows and distributed
computing tasks, often using platforms like Kubernetes for container
management. \emph{Appears in:
\textbf{?@sec-machine-learning-operations-mlops}}
\item[\textbf{outlier detection}]
The process of identifying data points that significantly deviate from
normal patterns, which may represent errors, anomalies, or valuable rare
events. \emph{Appears in: \textbf{?@sec-data-engineering-ml}}
\item[\textbf{overfitting}]
A phenomenon where a model learns specific details of training data so
well that it fails to generalize to new, unseen examples, typically
indicated by high training accuracy but poor validation performance.
\emph{Appears in: \textbf{?@sec-deep-learning-systems-foundations}}
\end{description}

\section*{P}\label{p}
\addcontentsline{toc}{section}{P}

\markright{P}

\begin{description}
\tightlist
\item[\textbf{padding}]
A technique in convolutional networks that adds zeros or other values
around the input borders to control the spatial dimensions of the output
feature maps. \emph{Appears in: \textbf{?@sec-dnn-architectures}}
\item[\textbf{paradigm shift}]
A fundamental change in scientific approach, like the shift from
symbolic reasoning to statistical learning in AI during the 1990s, and
from shallow to deep learning in the 2010s, requiring researchers to
abandon established methods for radically different approaches.
\emph{Appears in: \textbf{?@sec-introduction}}
\item[\textbf{parallelism}]
The simultaneous execution of multiple computational tasks or
operations, fundamental to achieving high performance in neural network
processing. \emph{Appears in: \textbf{?@sec-ai-acceleration}}
\item[\textbf{parameter}]
A learnable component of a neural network, including weights and biases,
that gets adjusted during training to minimize the loss function.
\emph{Appears in: \textbf{?@sec-deep-learning-systems-foundations}}
\item[\textbf{partitioning}]
A database technique that divides large datasets into smaller,
manageable segments based on specific criteria to improve query
performance and system scalability. \emph{Appears in:
\textbf{?@sec-data-engineering-ml}}
\item[\textbf{perceptron}]
The fundamental building block of neural networks, consisting of
weighted inputs, a bias term, and an activation function that produces a
single output. \emph{Appears in:
\textbf{?@sec-deep-learning-systems-foundations},
\textbf{?@sec-introduction}}
\item[\textbf{performance insights}]
Analytical observations derived from monitoring production machine
learning systems that reveal opportunities for improvement in model
accuracy, system efficiency, or user experience. \emph{Appears in:
\textbf{?@sec-ai-development-workflow}}
\item[\textbf{performance-per-data (ppd)}]
A metric measuring the accuracy gain per training sample, used to
evaluate the quality of a dataset or selection strategy. High PPD
indicates a dataset rich in information and low in redundancy.
\emph{Appears in: \textbf{?@sec-data-selection}}
\item[\textbf{pipeline jungle}]
Anti-pattern where complex, interdependent data processing pipelines
become difficult to maintain, debug, and modify, leading to technical
debt and operational complexity \emph{Appears in:
\textbf{?@sec-data-engineering-ml},
\textbf{?@sec-machine-learning-operations-mlops}}
\item[\textbf{pipeline parallelism}]
A form of model parallelism where different layers of a model are placed
on different devices and data flows through them in a pipeline fashion,
allowing multiple batches to be processed simultaneously. \emph{Appears
in: \textbf{?@sec-ai-frameworks}, \textbf{?@sec-ai-training}}
\item[\textbf{pooling}]
A downsampling operation in convolutional networks that reduces spatial
dimensions while retaining important features, commonly using max or
average operations over local regions. \emph{Appears in:
\textbf{?@sec-dnn-architectures}}
\item[\textbf{positional encoding}]
A method used in transformer architectures to inject information about
the position of tokens in a sequence, since transformers lack inherent
sequential processing. \emph{Appears in:
\textbf{?@sec-dnn-architectures}}
\item[\textbf{post-training quantization}]
A quantization approach applied to already-trained models without
modifying the training process, typically involving calibration on
representative data to determine optimal quantization parameters.
\emph{Appears in: \textbf{?@sec-model-compression}}
\item[\textbf{power usage effectiveness (pue)}]
Metric used in data centers to measure energy efficiency, calculated as
the ratio of total facility power consumption to IT equipment power
consumption. \emph{Appears in: \textbf{?@sec-benchmarking-ai}}
\item[\textbf{power wall}]
The technological barrier reached around 2005 where increasing processor
frequency no longer yielded performance gains without unsustainable
increases in power density and heat generation, forcing a shift to
parallel and specialized architectures. \emph{Appears in:
\textbf{?@sec-ml-system-architecture}}
\item[\textbf{precision}]
In numerical computing, the number of bits used to represent numbers,
affecting both computational accuracy and resource requirements in
machine learning systems. \emph{Appears in:
\textbf{?@sec-benchmarking-ai}}
\item[\textbf{prefetching}]
A system optimization technique that loads data into memory before it is
needed, overlapping data loading with computation to reduce idle time
and improve training throughput. \emph{Appears in:
\textbf{?@sec-ai-training}}
\item[\textbf{problem definition}]
The initial stage of machine learning development that involves clearly
specifying objectives, constraints, success metrics, and operational
requirements to guide all subsequent development decisions.
\emph{Appears in: \textbf{?@sec-ai-development-workflow}}
\item[\textbf{programmatic logic controllers}]
Industrial control systems used in manufacturing and IoT environments
that can be integrated with ML models for automated decision-making in
operational technology contexts. \emph{Appears in:
\textbf{?@sec-machine-learning-operations-mlops}}
\item[\textbf{protein folding problem}]
The scientific challenge of predicting the three-dimensional structure
of proteins from their amino acid sequences, a problem that puzzled
scientists for decades until systems like AlphaFold achieved
breakthrough accuracy using deep learning approaches. \emph{Appears in:
\textbf{?@sec-introduction}}
\item[\textbf{pseudonymization}]
A privacy technique that replaces direct identifiers with artificial
identifiers while maintaining the ability to trace records for analysis
purposes. \emph{Appears in: \textbf{?@sec-data-engineering-ml}}
\item[\textbf{pytorch}]
A deep learning framework developed by Facebook's AI Research lab that
emphasizes dynamic computational graphs, eager execution, and intuitive
Python integration, particularly popular for research and
experimentation. \emph{Appears in: \textbf{?@sec-ai-frameworks}}
\end{description}

\section*{Q}\label{q}
\addcontentsline{toc}{section}{Q}

\markright{Q}

\begin{description}
\tightlist
\item[\textbf{quantization}]
A model compression technique that reduces the precision of model
parameters and activations from higher precision formats (like 32-bit
floats) to lower precision (like 8-bit integers), significantly reducing
memory usage and computational requirements \emph{Appears in:
\textbf{?@sec-conclusion}, \textbf{?@sec-ai-frameworks},
\textbf{?@sec-ai-acceleration}}
\item[\textbf{quantization granularity}]
The level at which quantization parameters are applied, ranging from
per-tensor (coarsest) to per-channel or per-group (finer), with finer
granularity typically preserving more accuracy but requiring more
storage. \emph{Appears in: \textbf{?@sec-model-compression}}
\item[\textbf{quantization-aware training}]
A training approach where quantization effects are simulated during the
training process, allowing the model to adapt to reduced precision and
typically achieving better accuracy than post-training quantization.
\emph{Appears in: \textbf{?@sec-model-compression}}
\item[\textbf{queries per second (qps)}]
Performance metric that measures how many inference requests a system
can process in one second, commonly used to evaluate throughput in
production deployments. \emph{Appears in:
\textbf{?@sec-benchmarking-ai}}
\item[\textbf{query key value}]
The three components of attention mechanisms where queries determine
what to look for, keys represent what is available, and values contain
the actual information to be weighted and combined. \emph{Appears in:
\textbf{?@sec-dnn-architectures}}
\end{description}

\section*{R}\label{r}
\addcontentsline{toc}{section}{R}

\markright{R}

\begin{description}
\tightlist
\item[\textbf{real-time processing}]
The processing of data as it becomes available, with guaranteed response
times that meet strict timing constraints for immediate decision-making.
\emph{Appears in: \textbf{?@sec-ml-system-architecture}}
\item[\textbf{receptive field}]
The region of the input that influences a particular neuron's output,
determining the spatial extent of patterns that can be detected by that
neuron. \emph{Appears in: \textbf{?@sec-dnn-architectures}}
\item[\textbf{rectified linear unit}]
An activation function that outputs the input if positive and zero
otherwise, widely used in modern neural networks for its computational
simplicity and ability to avoid vanishing gradients. \emph{Appears in:
\textbf{?@sec-ai-training}}
\item[\textbf{recurrent neural network}]
A type of neural network designed for sequential data processing,
featuring connections that create loops allowing information to persist
across time steps. \emph{Appears in: \textbf{?@sec-dnn-architectures}}
\item[\textbf{red ai}]
AI research and development that prioritizes maximizing accuracy or
performance without regard for the increasing computational and
environmental costs required. \emph{Appears in:
\textbf{?@sec-responsible-engineering}}
\item[\textbf{regularization}]
Techniques used to prevent overfitting in neural networks by adding
constraints or penalties, including methods like dropout, weight decay,
and data augmentation. \emph{Appears in:
\textbf{?@sec-dnn-architectures}}
\item[\textbf{relu}]
Rectified Linear Unit activation function defined as f(x) = max(0,x)
that introduces nonlinearity while maintaining computational efficiency
and avoiding vanishing gradient problems. \emph{Appears in:
\textbf{?@sec-deep-learning-systems-foundations}}
\item[\textbf{residual connection}]
A skip connection that adds the input of a layer to its output, enabling
the training of very deep networks by mitigating the vanishing gradient
problem. \emph{Appears in: \textbf{?@sec-dnn-architectures}}
\item[\textbf{resnet}]
Residual Network, a deep convolutional architecture that introduced skip
connections, enabling the training of networks with hundreds of layers
and achieving breakthrough performance. \emph{Appears in:
\textbf{?@sec-benchmarking-ai}, \textbf{?@sec-dnn-architectures}}
\item[\textbf{resnet-50}]
A specific 50-layer variant of the Residual Network architecture that
serves as the canonical Lighthouse Archetype for compute-bound
workloads, balancing depth and computational cost for vision benchmarks.
\emph{Appears in: \textbf{?@sec-dnn-architectures}}
\item[\textbf{responsible engineering gap}]
The disparity between technical optimization success (e.g., high
benchmark accuracy) and responsible deployment outcomes (e.g., fairness
and safety in production). \emph{Appears in:
\textbf{?@sec-responsible-engineering}}
\item[\textbf{retinal fundus photographs}]
Medical images of the interior surface of the eye, including the retina,
optic disc, and blood vessels, commonly used for diagnosing eye diseases
and training medical AI systems. \emph{Appears in:
\textbf{?@sec-ai-development-workflow}}
\item[\textbf{reverse-mode differentiation}]
An automatic differentiation technique that computes gradients by
traversing the computational graph in reverse order, highly efficient
for functions with many inputs and few outputs, making it ideal for
neural network training. \emph{Appears in: \textbf{?@sec-ai-frameworks}}
\item[\textbf{rmsprop}]
An adaptive learning rate optimization algorithm that maintains a moving
average of squared gradients to automatically adjust learning rates for
each parameter during training. \emph{Appears in:
\textbf{?@sec-ai-training}}
\item[\textbf{rollback}]
Process of reverting to a previous stable version of a model or system
when issues are detected in production, ensuring service continuity.
\emph{Appears in: \textbf{?@sec-machine-learning-operations-mlops}}
\item[\textbf{roofline analysis}]
A performance modeling technique that plots operational intensity
against peak performance to identify whether a system is memory-bound or
compute-bound, guiding optimization efforts. \emph{Appears in:
\textbf{?@sec-conclusion}}
\end{description}

\section*{S}\label{s}
\addcontentsline{toc}{section}{S}

\markright{S}

\begin{description}
\tightlist
\item[\textbf{scalability}]
The ability of machine learning systems to handle increasing amounts of
data, users, or computational demands without significant degradation in
performance or user experience \emph{Appears in:
\textbf{?@sec-benchmarking-ai}, \textbf{?@sec-ai-development-workflow}}
\item[\textbf{schema}]
The structure and format definition of data that specifies data types,
field names, and relationships, essential for data validation and
processing consistency. \emph{Appears in:
\textbf{?@sec-data-engineering-ml}}
\item[\textbf{schema evolution}]
The process of modifying data schemas over time while maintaining
backward compatibility and ensuring continued functionality of dependent
systems and applications. \emph{Appears in:
\textbf{?@sec-data-engineering-ml}}
\item[\textbf{schema-on-read}]
An approach used in data lakes where data structure is defined and
enforced at the time of reading rather than when storing, providing
flexibility for diverse data types. \emph{Appears in:
\textbf{?@sec-data-engineering-ml}}
\item[\textbf{segmentation maps}]
Detailed annotations that classify objects at the pixel level, providing
the most granular labeling information but requiring significantly more
storage and processing resources. \emph{Appears in:
\textbf{?@sec-data-engineering-ml}}
\item[\textbf{self-attention}]
An attention mechanism where queries, keys, and values all come from the
same sequence, allowing each position to attend to all positions
including itself. \emph{Appears in: \textbf{?@sec-dnn-architectures}}
\item[\textbf{semi-supervised learning}]
A machine learning approach that uses both labeled and unlabeled data
for training, leveraging structural assumptions to improve model
performance with limited labels. \emph{Appears in:
\textbf{?@sec-data-engineering-ml}}
\item[\textbf{sensitivity}]
The proportion of actual positive cases correctly identified by a
classification model, also known as true positive rate or recall;
critical in medical applications where missing positive cases has severe
consequences. \emph{Appears in: \textbf{?@sec-ai-development-workflow}}
\item[\textbf{sequential neural networks}]
Neural network architectures designed to process data that occurs in
sequences over time, maintaining a form of memory of previous inputs to
inform current decisions, essential for tasks like predicting pedestrian
movement patterns. \emph{Appears in: \textbf{?@sec-introduction}}
\item[\textbf{serverless}]
Cloud computing model where infrastructure is automatically managed by
the provider, allowing code execution without server management
concerns. \emph{Appears in:
\textbf{?@sec-machine-learning-operations-mlops}}
\item[\textbf{service level agreement (sla)}]
Formal contract specifying minimum performance standards and uptime
guarantees for production services, with penalties for non-compliance.
\emph{Appears in: \textbf{?@sec-machine-learning-operations-mlops}}
\item[\textbf{service level objective (slo)}]
Internal targets for service reliability and performance metrics such as
latency, error rates, and availability that guide operational decisions.
\emph{Appears in: \textbf{?@sec-machine-learning-operations-mlops}}
\item[\textbf{shadow deployment}]
A deployment strategy where a new model runs in parallel with the
production model, making predictions that are logged but not served to
users, enabling validation without user impact \emph{Appears in:
\textbf{?@sec-machine-learning-operations-mlops},
\textbf{?@sec-ai-development-workflow}}
\item[\textbf{shallow learning}]
Machine learning approaches that use algorithms with limited complexity,
such as support vector machines and decision trees, which require
carefully engineered features but cannot automatically discover
hierarchical representations like deep learning methods. \emph{Appears
in: \textbf{?@sec-introduction}}
\item[\textbf{sigmoid}]
An activation function that maps input values to a range between 0 and
1, historically popular but prone to vanishing gradient problems in deep
networks. \emph{Appears in:
\textbf{?@sec-deep-learning-systems-foundations},
\textbf{?@sec-ai-training}}
\item[\textbf{silent bias}]
Model unfairness that produces valid-looking but discriminatory outputs,
evading traditional error monitoring and requiring disaggregated
evaluation to detect. \emph{Appears in:
\textbf{?@sec-responsible-engineering}}
\item[\textbf{silent degradation}]
The gradual decline in ML system performance that occurs without
triggering errors, exceptions, or alerts. Unlike traditional software
that crashes observably, ML systems can continue operating while
producing increasingly inaccurate predictions. \emph{Appears in:
\textbf{?@sec-introduction}}
\item[\textbf{silent failure}]
A system failure mode where an ML model continues to produce
plausible-looking outputs that are gradually less accurate or
contextually relevant without triggering conventional error alerts.
\emph{Appears in: \textbf{?@sec-responsible-engineering}}
\item[\textbf{simd (single instruction, multiple data)}]
A parallel computing architecture that applies the same operation to
multiple data elements simultaneously, effective for regular
data-parallel computations. \emph{Appears in:
\textbf{?@sec-ai-acceleration}}
\item[\textbf{simt (single instruction, multiple thread)}]
An extension of SIMD that enables parallel execution across multiple
independent threads, each maintaining its own state and program counter.
\emph{Appears in: \textbf{?@sec-ai-acceleration}}
\item[\textbf{single-instance throughput}]
Performance measurement focusing on the rate at which a single model
instance can process requests, contrasting with batch throughput
metrics. \emph{Appears in: \textbf{?@sec-benchmarking-ai}}
\item[\textbf{singular value decomposition}]
A matrix factorization technique that decomposes a matrix into the
product of three matrices, commonly used in low-rank approximations to
compress neural network layers by retaining only the most significant
singular values. \emph{Appears in: \textbf{?@sec-model-compression}}
\item[\textbf{skip connection}]
A direct connection that bypasses one or more layers, allowing gradients
to flow more easily through deep networks and enabling better training
of very deep architectures. \emph{Appears in:
\textbf{?@sec-dnn-architectures}}
\item[\textbf{softmax}]
An activation function that converts logits into a probability
distribution where outputs sum to 1, used in multi-class classification.
\emph{Appears in: \textbf{?@sec-deep-learning-systems-foundations},
\textbf{?@sec-dnn-architectures}, \textbf{?@sec-ai-training}}
\item[\textbf{sparsity}]
The property of neural networks where many weights are zero or
near-zero, which can be exploited for computational efficiency through
specialized hardware support and algorithms designed for sparse
operations. \emph{Appears in: \textbf{?@sec-model-compression}}
\item[\textbf{spec cpu}]
Standardized benchmark suite developed by the System Performance
Evaluation Cooperative that measures processor performance using
real-world applications rather than synthetic tests. \emph{Appears in:
\textbf{?@sec-benchmarking-ai}}
\item[\textbf{spec power}]
Benchmark methodology that measures server energy efficiency across
varying workload levels, enabling direct comparisons of
power-performance trade-offs in computing systems. \emph{Appears in:
\textbf{?@sec-benchmarking-ai}}
\item[\textbf{specificity}]
The proportion of actual negative cases correctly identified by a
classification model, also known as true negative rate; important for
avoiding overwhelming referral systems with false positives.
\emph{Appears in: \textbf{?@sec-ai-development-workflow}}
\item[\textbf{speculative decoding}]
An optimization technique for autoregressive language models where a
smaller model generates draft tokens that are then verified by a larger
model, accelerating inference while maintaining quality. \emph{Appears
in: \textbf{?@sec-conclusion}}
\item[\textbf{speed of light (latency)}]
The physical speed limit of information transmission (approx. 200,000
km/s in fiber), creating an irreducible lower bound on network latency
that necessitates edge computing for applications requiring sub-10ms
response times over long distances. \emph{Appears in:
\textbf{?@sec-ml-system-architecture}}
\item[\textbf{stage-specific metrics}]
Performance indicators tailored to individual lifecycle phases, such as
data quality metrics during preparation, training convergence during
modeling, and latency metrics during deployment. \emph{Appears in:
\textbf{?@sec-ai-development-workflow}}
\item[\textbf{static graph}]
A computational graph that is defined completely before execution
begins, enabling comprehensive optimization and efficient deployment but
requiring all operations to be specified upfront, limiting runtime
flexibility. \emph{Appears in: \textbf{?@sec-ai-frameworks}}
\item[\textbf{static graphs vs dynamic graphs}]
Two fundamental approaches to representing computations in ML
frameworks: static graphs are defined before execution and enable
optimization but limit flexibility, while dynamic graphs are built
during execution allowing for flexible control flow but with potential
optimization limitations. \emph{Appears in:
\textbf{?@sec-ai-frameworks}}
\item[\textbf{static quantization}]
A quantization approach where quantization parameters are determined
once during calibration and remain fixed during inference, providing
computational efficiency but less adaptability than dynamic approaches.
\emph{Appears in: \textbf{?@sec-model-compression}}
\item[\textbf{statistical learning}]
The era of machine learning that emerged in the 1990s, shifting focus
from rule-based symbolic AI to algorithms that could learn patterns from
data, laying the groundwork for modern data-driven approaches to
artificial intelligence. \emph{Appears in: \textbf{?@sec-introduction}}
\item[\textbf{stochastic gradient descent}]
A variant of gradient descent that estimates gradients using individual
training examples or small batches rather than the entire dataset,
reducing memory requirements and enabling online learning. \emph{Appears
in: \textbf{?@sec-ai-training}}
\item[\textbf{stream ingestion}]
A data processing pattern that handles data in real-time as it arrives,
essential for applications requiring immediate processing and
low-latency responses. \emph{Appears in:
\textbf{?@sec-data-engineering-ml}}
\item[\textbf{stream processing}]
Real-time data processing approach that handles continuous flows of data
as it arrives, enabling immediate responses to events and pattern
detection. \emph{Appears in: \textbf{?@sec-data-engineering-ml}}
\item[\textbf{stride}]
The step size by which a convolutional filter moves across the input,
controlling the spatial dimensions of the output and the degree of
overlap between filter applications. \emph{Appears in:
\textbf{?@sec-dnn-architectures}}
\item[\textbf{structured pruning}]
A pruning approach that removes entire computational units such as
neurons, channels, or layers, producing smaller dense models that are
more hardware-friendly than the sparse matrices created by unstructured
pruning. \emph{Appears in: \textbf{?@sec-model-compression}}
\item[\textbf{student system}]
One of the first AI programs from 1964 by Daniel Bobrow that
demonstrated natural language understanding by converting English
algebra word problems into mathematical equations, marking an important
milestone in symbolic AI. \emph{Appears in: \textbf{?@sec-introduction}}
\item[\textbf{student-teacher learning}]
The core mechanism of knowledge distillation where a smaller student
network learns from a larger teacher network, typically using soft
targets that provide more information than hard classification labels.
\emph{Appears in: \textbf{?@sec-model-compression}}
\item[\textbf{supervised learning}]
A machine learning approach where models learn from labeled training
examples to make predictions on new, unlabeled data. \emph{Appears in:
\textbf{?@sec-deep-learning-systems-foundations}}
\item[\textbf{symbolic ai}]
An approach to artificial intelligence that uses high-level symbolic
representations of problems, logic, and search algorithms, dominant
before the deep learning revolution and characterized by expert systems
and rule-based reasoning. \emph{Appears in: \textbf{?@sec-introduction}}
\item[\textbf{symbolic programming}]
A programming paradigm where computations are represented as abstract
symbols and expressions that are constructed first and executed later,
allowing for comprehensive optimization but requiring explicit execution
phases. \emph{Appears in: \textbf{?@sec-ai-frameworks}}
\item[\textbf{synthetic benchmark}]
Artificial test program designed to measure specific aspects of system
performance, as opposed to benchmarks based on real-world applications
and workloads. \emph{Appears in: \textbf{?@sec-benchmarking-ai}}
\item[\textbf{synthetic data}]
Artificially generated data created using algorithms, simulations, or
generative models to supplement real-world datasets, addressing
limitations in data availability or privacy concerns. \emph{Appears in:
\textbf{?@sec-data-engineering-ml}}
\item[\textbf{system entropy}]
The tendency of ML systems to degrade over time as the world changes
(drift) or as hidden dependencies accumulate (technical debt), requiring
active energy (ops) to maintain order. \emph{Appears in:
\textbf{?@sec-ml-system-architecture}}
\item[\textbf{system on chip}]
An integrated circuit that incorporates most or all components of a
computer or electronic system, including CPU, GPU, memory, and
specialized processors on a single chip. \emph{Appears in:
\textbf{?@sec-ml-system-architecture}}
\item[\textbf{system-on-chip (soc)}]
Integrated circuit that contains most or all components of a computer
system, commonly used in mobile devices and embedded systems for space
and power efficiency. \emph{Appears in: \textbf{?@sec-benchmarking-ai}}
\item[\textbf{systems integration}]
The process of combining various components and subsystems into a
unified, functional system that operates efficiently and reliably as a
whole. \emph{Appears in: \textbf{?@sec-conclusion}}
\item[\textbf{systems thinking}]
An approach to understanding complex systems by considering how
individual components interact and affect the whole system, particularly
important in ML where data, algorithms, hardware, and deployment
environments must work together effectively \emph{Appears in:
\textbf{?@sec-introduction}, \textbf{?@sec-ai-development-workflow}}
\item[\textbf{systolic array}]
A specialized hardware architecture that efficiently performs matrix
operations by streaming data through a grid of processing elements,
minimized data movement and energy consumption. \emph{Appears in:
\textbf{?@sec-ai-acceleration}, \textbf{?@sec-ai-training}}
\end{description}

\section*{T}\label{t}
\addcontentsline{toc}{section}{T}

\markright{T}

\begin{description}
\tightlist
\item[\textbf{tail latency}]
Worst-case response times in a system, typically measured as 95th or
99th percentile latency, important for understanding system reliability
under peak load conditions. \emph{Appears in:
\textbf{?@sec-benchmarking-ai}}
\item[\textbf{tailored inference benchmarks}]
Specialized performance tests designed for specific deployment
environments or use cases, accounting for unique constraints and
optimization requirements. \emph{Appears in:
\textbf{?@sec-benchmarking-ai}}
\item[\textbf{tanh}]
Hyperbolic tangent activation function that maps inputs to (-1, 1),
providing zero-centered outputs. \emph{Appears in:
\textbf{?@sec-deep-learning-systems-foundations},
\textbf{?@sec-ai-training}}
\item[\textbf{technical debt}]
Long-term maintenance cost accumulated from expedient design decisions
during development, particularly problematic in ML systems due to data
dependencies and model complexity. \emph{Appears in:
\textbf{?@sec-machine-learning-operations-mlops}}
\item[\textbf{telemetry}]
Automated collection and transmission of performance data and metrics
from distributed systems, enabling remote monitoring and analysis.
\emph{Appears in: \textbf{?@sec-machine-learning-operations-mlops}}
\item[\textbf{tensor}]
A multi-dimensional array used to represent data in neural networks,
generalizing scalars (0D), vectors (1D), and matrices (2D) to higher
dimensions. \emph{Appears in: \textbf{?@sec-benchmarking-ai},
\textbf{?@sec-deep-learning-systems-foundations},
\textbf{?@sec-ai-frameworks}, \textbf{?@sec-ai-acceleration}}
\item[\textbf{tensor decomposition}]
The extension of matrix factorization to higher-order tensors, used to
compress neural network layers by representing weight tensors as
combinations of smaller tensors with fewer parameters. \emph{Appears in:
\textbf{?@sec-model-compression}}
\item[\textbf{tensor parallelism}]
A model parallelism strategy where individual tensor operations (like
matrix multiplication) are split across multiple devices, reducing
memory per device and latency for large layers. \emph{Appears in:
\textbf{?@sec-ai-frameworks}, \textbf{?@sec-ai-training}}
\item[\textbf{tensor processing unit}]
Google's custom application-specific integrated circuit designed
specifically for machine learning workloads, optimized for matrix
operations and featuring systolic array architecture. \emph{Appears in:
\textbf{?@sec-ai-training}}
\item[\textbf{tensor processing unit (tpu)}]
Google's custom application-specific integrated circuit designed
specifically for neural network machine learning, optimized for
TensorFlow operations. \emph{Appears in: \textbf{?@sec-benchmarking-ai},
\textbf{?@sec-ai-acceleration}, \textbf{?@sec-ml-system-architecture}}
\item[\textbf{tensorflow}]
A comprehensive machine learning framework developed by Google that
provides tools for the entire ML pipeline from research to production,
featuring both eager execution and graph-based computation with
extensive ecosystem support. \emph{Appears in:
\textbf{?@sec-ai-frameworks}}
\item[\textbf{tensorrt}]
NVIDIA's inference optimization library that applies techniques like
operator fusion and precision reduction to accelerate deep learning
inference on GPU hardware. \emph{Appears in:
\textbf{?@sec-benchmarking-ai}}
\item[\textbf{ternarization}]
An extreme quantization technique that constrains weights to three
values (typically -1, 0, +1), providing significant compression while
maintaining more representational capacity than binary quantization.
\emph{Appears in: \textbf{?@sec-model-compression}}
\item[\textbf{thermal throttling}]
A protective mechanism in mobile and embedded devices that reduces
processor clock speed and performance to prevent overheating, often
limiting the sustained performance of on-device ML inference.
\emph{Appears in: \textbf{?@sec-ml-system-architecture}}
\item[\textbf{threshold for activation}]
The input level at which a neuron begins to produce significant output,
determined by the combination of weights, biases, and the chosen
activation function, controlling when the neuron contributes to the
network's computation. \emph{Appears in:
\textbf{?@sec-deep-learning-systems-foundations}}
\item[\textbf{throughput}]
The rate at which a system can process data or complete operations,
typically measured in operations per second and crucial for training
large models \emph{Appears in: \textbf{?@sec-benchmarking-ai},
\textbf{?@sec-ai-acceleration}}
\item[\textbf{time-to-accuracy}]
The wall-clock time required to train a model to a specified validation
accuracy, the ultimate metric for training system performance.
\emph{Appears in: \textbf{?@sec-benchmarking-ai},
\textbf{?@sec-ai-training}}
\item[\textbf{tiny machine learning}]
The execution of machine learning models on ultra-constrained devices
such as microcontrollers and sensors, operating in the milliwatt to
sub-watt power range. \emph{Appears in:
\textbf{?@sec-ml-system-architecture}}
\item[\textbf{tinyml}]
A field focused on deploying machine learning models on
resource-constrained embedded devices and microcontrollers with severe
limitations on memory, power, and computational capacity. \emph{Appears
in: \textbf{?@sec-conclusion}}
\item[\textbf{tops}]
Tera Operations Per Second, a measure of computational performance
indicating how many trillion operations a system can execute in one
second. \emph{Appears in: \textbf{?@sec-benchmarking-ai}}
\item[\textbf{total cost of ownership (tco)}]
A comprehensive financial metric for ML systems encompassing training,
inference, and operational costs over the system's entire lifecycle.
\emph{Appears in: \textbf{?@sec-responsible-engineering}}
\item[\textbf{tpu}]
Tensor Processing Unit, Google's custom Application-Specific Integrated
Circuits (ASICs) designed specifically for accelerating tensor
operations in machine learning workloads, offering significant
performance and energy efficiency improvements over general-purpose
processors. \emph{Appears in: \textbf{?@sec-ai-frameworks}}
\item[\textbf{train-serve split}]
A hybrid architecture pattern where computationally intensive model
training occurs on powerful cloud infrastructure, while the trained
model is optimized and deployed for inference on resource-constrained
edge or mobile devices. \emph{Appears in:
\textbf{?@sec-ml-system-architecture}}
\item[\textbf{training}]
The process of adjusting neural network parameters using labeled data
and optimization algorithms to minimize prediction errors and improve
performance. \emph{Appears in: \textbf{?@sec-benchmarking-ai},
\textbf{?@sec-deep-learning-systems-foundations},
\textbf{?@sec-ai-acceleration}, \textbf{?@sec-ml-system-architecture}}
\item[\textbf{training-serving skew}]
A mismatch between how features or data are computed during model
training versus serving in production, causing model performance to
degrade despite unchanged code. Common causes include different
preprocessing pipelines, feature computation timing, or data sources
between training and inference \emph{Appears in:
\textbf{?@sec-introduction},
\textbf{?@sec-machine-learning-operations-mlops}}
\item[\textbf{transfer learning}]
A machine learning technique that leverages knowledge gained from
pre-trained models on related tasks, allowing faster training and better
performance on new tasks with limited data by reusing learned features
and representations \emph{Appears in: \textbf{?@sec-conclusion},
\textbf{?@sec-data-engineering-ml}, \textbf{?@sec-ai-frameworks},
\textbf{?@sec-introduction}, \textbf{?@sec-ai-development-workflow}}
\item[\textbf{transformer}]
A neural network architecture based entirely on attention mechanisms,
eliminating recurrence and convolution while achieving state-of-the-art
performance across many domains. \emph{Appears in:
\textbf{?@sec-benchmarking-ai}, \textbf{?@sec-dnn-architectures}}
\item[\textbf{translation invariance}]
The property of convolutional networks to recognize patterns regardless
of their position in the input, achieved through weight sharing and
pooling operations. \emph{Appears in: \textbf{?@sec-dnn-architectures}}
\item[\textbf{tucker decomposition}]
A tensor decomposition method that generalizes singular value
decomposition to higher-order tensors using a core tensor and factor
matrices, commonly used for compressing convolutional neural network
layers. \emph{Appears in: \textbf{?@sec-model-compression}}
\item[\textbf{tv white spaces}]
Unused broadcasting frequencies that can be repurposed for internet
connectivity, as employed by systems like FarmBeats to extend network
access to remote agricultural sensors and IoT devices. \emph{Appears in:
\textbf{?@sec-introduction}}
\end{description}

\section*{U}\label{u}
\addcontentsline{toc}{section}{U}

\markright{U}

\begin{description}
\tightlist
\item[\textbf{uniform quantization}]
A quantization approach where the range of values is divided into evenly
spaced intervals, providing simple implementation but potentially
suboptimal for non-uniform value distributions. \emph{Appears in:
\textbf{?@sec-model-compression}}
\item[\textbf{universal approximation theorem}]
A theoretical result proving that neural networks with sufficient width
and non-linear activation functions can approximate any continuous
function on a compact domain. \emph{Appears in:
\textbf{?@sec-dnn-architectures}}
\item[\textbf{unreasonable effectiveness of data}]
The empirical observation that for many problems, adding more data is
more effective than improving algorithms, driving the data-centric AI
paradigm. \emph{Appears in: \textbf{?@sec-introduction}}
\item[\textbf{unstructured pruning}]
A pruning approach that removes individual weights while preserving the
overall network architecture, creating sparse weight matrices that
require specialized hardware support to realize computational benefits.
\emph{Appears in: \textbf{?@sec-model-compression}}
\item[\textbf{unstructured sparsity}]
A form of model sparsity where individual weights are set to zero
without following any particular pattern, creating irregular sparsity
patterns that require specialized hardware support to realize
computational benefits. \emph{Appears in:
\textbf{?@sec-model-compression}}
\end{description}

\section*{V}\label{v}
\addcontentsline{toc}{section}{V}

\markright{V}

\begin{description}
\tightlist
\item[\textbf{validation issues}]
Problems identified during model testing that indicate poor performance,
overfitting, data quality problems, or other issues that must be
resolved before deployment. \emph{Appears in:
\textbf{?@sec-ai-development-workflow}}
\item[\textbf{vanishing gradient}]
A problem in deep neural networks where gradients become exponentially
smaller as they propagate backward through layers, making it difficult
for early layers to learn effectively. \emph{Appears in:
\textbf{?@sec-deep-learning-systems-foundations},
\textbf{?@sec-dnn-architectures}}
\item[\textbf{vanishing gradient problem}]
A challenge in training deep neural networks where gradients become
exponentially smaller as they propagate backward through layers, making
it difficult to train early layers effectively. \emph{Appears in:
\textbf{?@sec-ai-training}}
\item[\textbf{vector operations}]
Computational operations that process multiple data elements
simultaneously, enabling efficient parallel execution of element-wise
transformations in neural networks. \emph{Appears in:
\textbf{?@sec-ai-acceleration}}
\item[\textbf{versioning}]
The practice of tracking changes to datasets, models, and pipelines over
time, enabling reproducibility, rollback capabilities, and audit trails
in ML systems. \emph{Appears in: \textbf{?@sec-data-engineering-ml}}
\item[\textbf{virtuous cycle}]
The self-reinforcing process in deep learning where improvements in data
availability, algorithms, and computing power each enable further
advances in the other areas, accelerating overall progress.
\emph{Appears in: \textbf{?@sec-deep-learning-systems-foundations}}
\item[\textbf{von neumann bottleneck}]
The performance limitation caused by the shared bus between processor
and memory in traditional computer architectures, where data movement
becomes more expensive than computation. \emph{Appears in:
\textbf{?@sec-ai-acceleration}}
\end{description}

\section*{W}\label{w}
\addcontentsline{toc}{section}{W}

\markright{W}

\begin{description}
\tightlist
\item[\textbf{warp}]
A group of threads (typically 32 on NVIDIA GPUs) that execute the same
instruction in lock-step; the fundamental unit of scheduling and
execution on GPUs. \emph{Appears in: \textbf{?@sec-ai-training}}
\item[\textbf{waymo}]
A subsidiary of Alphabet Inc.~that represents a leading deployment of
machine learning systems in autonomous vehicle technology, demonstrating
how ML systems can span from embedded systems to cloud infrastructure in
safety-critical environments. \emph{Appears in:
\textbf{?@sec-introduction}}
\item[\textbf{weak supervision}]
An approach that uses lower-quality labels obtained more efficiently
through heuristics, distant supervision, or programmatic methods rather
than manual expert annotation. \emph{Appears in:
\textbf{?@sec-data-engineering-ml}}
\item[\textbf{web scraping}]
An automated technique for extracting data from websites to build custom
datasets, requiring careful consideration of legal, ethical, and
technical constraints. \emph{Appears in:
\textbf{?@sec-data-engineering-ml}}
\item[\textbf{weight}]
A learnable parameter that determines the strength of connection between
neurons in different layers, adjusted during training to minimize the
loss function. \emph{Appears in:
\textbf{?@sec-deep-learning-systems-foundations}}
\item[\textbf{weight matrix}]
An organized collection of weights connecting one layer to another in a
neural network, enabling efficient computation through matrix
operations. \emph{Appears in:
\textbf{?@sec-deep-learning-systems-foundations}}
\item[\textbf{weight sharing}]
The practice of using the same parameters across different spatial
locations, as in convolutional networks, reducing the number of
parameters while maintaining pattern detection capabilities.
\emph{Appears in: \textbf{?@sec-dnn-architectures}}
\item[\textbf{whetstone}]
Early benchmark introduced in 1964 that measured floating-point
arithmetic performance in KIPS (thousands of instructions per second),
becoming the first widely-adopted standardized performance test.
\emph{Appears in: \textbf{?@sec-benchmarking-ai}}
\item[\textbf{workflow orchestration}]
Automated coordination and management of complex ML pipeline sequences,
ensuring proper execution order, dependency management, and error
handling across distributed systems. \emph{Appears in:
\textbf{?@sec-ai-development-workflow}}
\end{description}

\section*{X}\label{x}
\addcontentsline{toc}{section}{X}

\markright{X}

\begin{description}
\tightlist
\item[\textbf{xla}]
Accelerated Linear Algebra, a domain-specific compiler for linear
algebra operations that optimizes TensorFlow and JAX computations by
generating efficient code for various hardware platforms including CPUs,
GPUs, and TPUs. \emph{Appears in: \textbf{?@sec-ai-frameworks}}
\end{description}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\section*{About This Glossary}\label{about-this-glossary}
\addcontentsline{toc}{section}{About This Glossary}

\markright{About This Glossary}

This glossary was automatically generated from chapter glossaries in
Volume I: Foundations. Each term is defined in the context of machine
learning systems and includes references to help you explore related
concepts.

\textbf{Updates}: The glossary is maintained alongside the textbook
content to ensure definitions remain current and accurate.

\emph{Generated on 2026-02-01 at 18:26}


\backmatter

\clearpage


\end{document}
