% Options for packages loaded elsewhere
% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode,linktoc=all,pdfwindowui,pdfpagemode=FullScreen,pdfpagelayout=TwoPageRight}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  9pt,
  letterpaper,
  abstract,
  titlepage]{scrbook}
\usepackage{xcolor}
\usepackage[left=1in,marginparwidth=2.0666666666667in,textwidth=4.1333333333333in,marginparsep=0.3in]{geometry}
\usepackage{amsmath,amssymb}
\setcounter{secnumdepth}{3}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
% Make \paragraph and \subparagraph free-standing
\makeatletter
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}{
    \@ifstar
      \xxxParagraphStar
      \xxxParagraphNoStar
  }
  \newcommand{\xxxParagraphStar}[1]{\oldparagraph*{#1}\mbox{}}
  \newcommand{\xxxParagraphNoStar}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}{
    \@ifstar
      \xxxSubParagraphStar
      \xxxSubParagraphNoStar
  }
  \newcommand{\xxxSubParagraphStar}[1]{\oldsubparagraph*{#1}\mbox{}}
  \newcommand{\xxxSubParagraphNoStar}[1]{\oldsubparagraph{#1}\mbox{}}
\fi
\makeatother


\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\newsavebox\pandoc@box
\newcommand*\pandocbounded[1]{% scales image to fit in text height/width
  \sbox\pandoc@box{#1}%
  \Gscale@div\@tempa{\textheight}{\dimexpr\ht\pandoc@box+\dp\pandoc@box\relax}%
  \Gscale@div\@tempb{\linewidth}{\wd\pandoc@box}%
  \ifdim\@tempb\p@<\@tempa\p@\let\@tempa\@tempb\fi% select the smaller of both
  \ifdim\@tempa\p@<\p@\scalebox{\@tempa}{\usebox\pandoc@box}%
  \else\usebox{\pandoc@box}%
  \fi%
}
% Set default figure placement to htbp
\def\fps@figure{htbp}
\makeatother
% definitions for citeproc citations
\NewDocumentCommand\citeproctext{}{}
\NewDocumentCommand\citeproc{mm}{%
  \begingroup\def\citeproctext{#2}\cite{#1}\endgroup}
\makeatletter
 % allow citations to break across lines
 \let\@cite@ofmt\@firstofone
 % avoid brackets around text for \cite:
 \def\@biblabel#1{}
 \def\@cite#1#2{{#1\if@tempswa , #2\fi}}
\makeatother
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newenvironment{CSLReferences}[2] % #1 hanging-indent, #2 entry-spacing
 {\begin{list}{}{%
  \setlength{\itemindent}{0pt}
  \setlength{\leftmargin}{0pt}
  \setlength{\parsep}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1
   \setlength{\leftmargin}{\cslhangindent}
   \setlength{\itemindent}{-1\cslhangindent}
  \fi
  % set entry spacing
  \setlength{\itemsep}{#2\baselineskip}}}
 {\end{list}}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{\hfill\break\parbox[t]{\linewidth}{\strut\ignorespaces#1\strut}}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}

% =============================================================================
% LATEX HEADER CONFIGURATION FOR MLSYSBOOK PDF
% =============================================================================
% This file contains all LaTeX package imports, custom commands, and styling
% definitions for the PDF output of the Machine Learning Systems textbook.
%
% Key Features:
% - Harvard crimson branding throughout
% - Custom part/chapter/section styling
% - Professional table formatting with colored headers
% - Margin notes with custom styling
% - TikZ-based part dividers
% - Page numbering (Roman for frontmatter, Arabic for mainmatter)
%
% Note: This file is included via _quarto-pdf.yml and affects PDF output only.
% HTML/EPUB styling is handled separately via CSS files.
% =============================================================================

% =============================================================================
% PACKAGE IMPORTS
% =============================================================================

% Layout and positioning
% \usepackage[outercaption, ragged]{sidecap}  % Commented out to make figure captions inline instead of in margin
\usepackage{adjustbox}      % Adjusting box dimensions
\usepackage{afterpage}      % Execute commands after page break
\usepackage{morefloats}     % Increase number of floats
\usepackage{array}          % Enhanced table column formatting
\usepackage{atbegshi}       % Insert content at page beginning
%\usepackage{changepage}     % Change page dimensions mid-document
\usepackage{emptypage}      % Clear headers/footers on empty pages

% Language and text
\usepackage[english]{babel} % English language support
\usepackage{microtype}      % Improved typography and hyphenation

% Captions and floats
\usepackage{caption}
% Caption styling configuration
%\captionsetup[table]{belowskip=5pt}
\captionsetup{format=plain}
\DeclareCaptionLabelFormat{mylabel}{#1
#2:\hspace{1.0ex}}
\DeclareCaptionFont{ninept}{\fontsize{7pt}{8}\selectfont #1}

% Figure captions: Small font, bold label, ragged right
\captionsetup[figure]{labelfont={bf,ninept},labelsep=space,
belowskip=2pt,aboveskip=6pt,labelformat=mylabel,
justification=raggedright,singlelinecheck=false,font={ninept}}

% Table captions: Small font, bold label, ragged right
\captionsetup[table]{belowskip=6pt,labelfont={bf,ninept},labelsep=none,
labelformat=mylabel,justification=raggedright,singlelinecheck=false,font={ninept}}

% Typography fine-tuning
\emergencystretch=5pt       % Allow extra stretch to avoid overfull boxes

% Utility packages
\usepackage{etoolbox}       % For patching commands and environments

% Page layout and headers
\usepackage{fancyhdr}       % Custom headers and footers
\usepackage{geometry}       % Page dimensions and margins

% Graphics and figures
\usepackage{graphicx}       % Include graphics
\usepackage{float}          % Improved float placement
\usepackage[skins,breakable]{tcolorbox} % Coloured and framed text boxes
\tcbset{before upper=\setlength{\parskip}{3pt}}

% Tables
\usepackage{longtable}      % Multi-page tables

% Fonts and typography
\usepackage{fontspec}       % Font selection for LuaLaTeX
\usepackage{mathptmx}       % Times-like math fonts
\usepackage{newpxtext}      % Palatino-like font for body text

% Colors and visual elements
\usepackage[dvipsnames]{xcolor}  % Extended color support
\usepackage{tikz}           % Programmatic graphics
\usetikzlibrary{positioning}
\usetikzlibrary{calc}
\usepackage{tikzpagenodes}  % TikZ positioning relative to page

% Code listings
\usepackage{listings}       % Code highlighting

% Hyperlinks
\usepackage{hyperref}       % Clickable links in PDF

% Conditional logic
\usepackage{ifthen}         % If-then-else commands

% Math symbols
\usepackage{amsmath}        % AMS math extensions
\usepackage{amssymb}        % AMS math symbols
\usepackage{latexsym}       % Additional LaTeX symbols
\usepackage{pifont}         % Zapf Dingbats symbols
\providecommand{\blacklozenge}{\ding{117}}  % Black diamond symbol

% Lists
\usepackage{enumitem}       % Customizable lists

% Margin notes and sidenotes
\usepackage{marginfix}      % Fixes margin note overflow
\usepackage{marginnote}     % Margin notes
\usepackage{sidenotes}      % Academic-style sidenotes
\renewcommand\raggedrightmarginnote{\sloppy}
\renewcommand\raggedleftmarginnote{\sloppy}

% Typography improvements
\usepackage{ragged2e}       % Better ragged text
\usepackage[all]{nowidow}   % Prevent widows and orphans
\usepackage{needspace}      % Ensure minimum space on page

% Section formatting
\usepackage[explicit]{titlesec}  % Custom section titles
\usepackage{tocloft}        % Table of contents formatting

% QR codes and icons
\usepackage{fontawesome5}   % Font Awesome icons
\usepackage{qrcode}         % QR code generation
\qrset{link, height=15mm}

% =============================================================================
% FLOAT CONFIGURATION
% =============================================================================
% Allow more floats per page to handle figure-heavy chapters
\extrafloats{200}
\setcounter{topnumber}{12}       % Max floats at top of page
\setcounter{bottomnumber}{12}    % Max floats at bottom of page
\setcounter{totalnumber}{24}     % Max floats per page
\setcounter{dbltopnumber}{8}     % Max floats at top of two-column page
\renewcommand{\topfraction}{.95}  % Max fraction of page for top floats
\renewcommand{\bottomfraction}{.95}
\renewcommand{\textfraction}{.05}  % Min fraction of page for text
\renewcommand{\floatpagefraction}{.7}  % Min fraction of float page
\renewcommand{\dbltopfraction}{.95}

% Prevent "Float(s) lost" errors by flushing floats more aggressively
\usepackage{placeins}  % Provides \FloatBarrier

% =============================================================================
% COLOR DEFINITIONS
% =============================================================================
% Harvard crimson - primary brand color used throughout
\definecolor{crimson}{HTML}{A51C30}

% Quiz element colors
\definecolor{quiz-question-color1}{RGB}{225,243,248}  % Light blue background
\definecolor{quiz-question-color2}{RGB}{17,158,199}   % Blue border
\definecolor{quiz-answer-color1}{RGB}{250,234,241}    % Light pink background
\definecolor{quiz-answer-color2}{RGB}{152,14,90}      % Magenta border

% =============================================================================
% LIST FORMATTING
% =============================================================================
% Tighter list spacing for academic style
\def\tightlist{}
\setlist{itemsep=1pt, parsep=1pt, topsep=0pt,after={\vspace{0.3\baselineskip}}}
\let\tightlist\relax

\makeatletter
\@ifpackageloaded{framed}{}{\usepackage{framed}}
\@ifpackageloaded{fancyvrb}{}{\usepackage{fancyvrb}}
\makeatother

\makeatletter
%New float "codelisting" has been updated
\AtBeginDocument{%
\floatstyle{ruled}
\newfloat{codelisting}{!htb}{lop}
\floatname{codelisting}{Listing}
\floatplacement{codelisting}{!htb}
\captionsetup[codelisting]{labelfont={bf,ninept},labelformat=mylabel,
  singlelinecheck=false,width=\linewidth,labelsep=none,font={ninept}}%
\renewenvironment{snugshade}{%
   \def\OuterFrameSep{3pt}%
   \def\FrameCommand{\fboxsep=5pt\colorbox{shadecolor}}%
   \MakeFramed{\advance\hsize-\width\FrameRestore}%
   \leftskip 0.5em \rightskip 0.5em%
   \small% decrease font size
   }{\endMakeFramed}%
}
\makeatother

%The space before and after the verbatim environment "Highlighting" has been reduced
\fvset{listparameters=\setlength{\topsep}{0pt}\setlength{\partopsep}{0pt}}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{framesep=0mm,commandchars=\\\{\}}

\makeatletter
\renewcommand\fs@ruled{\def\@fs@cfont{\bfseries}\let\@fs@capt\floatc@ruled
\def\@fs@pre{\hrule height.8pt depth0pt \kern2pt}%
\def\@fs@post{\kern2pt\hrule\relax}%
\def\@fs@mid{\kern2pt\hrule\kern1pt}%space between float and caption
\let\@fs@iftopcapt\iftrue}
\makeatother


% =============================================================================
% HYPHENATION RULES
% =============================================================================
% Explicit hyphenation points for technical terms to avoid bad breaks
\hyphenation{
  light-weight
  light-weight-ed
  de-vel-op-ment
  un-der-stand-ing
  mod-els
  prin-ci-ples
  ex-per-tise
  com-pli-cat-ed
  blue-print
  per‧for‧mance
  com-mu-ni-ca-tion
  par-a-digms
  hy-per-ten-sion
  a-chieved
}

% =============================================================================
% CODE LISTING CONFIGURATION
% =============================================================================
% Settings for code blocks using listings package
\lstset{
breaklines=true,              % Automatic line wrapping
breakatwhitespace=true,       % Break at whitespace only
basicstyle=\ttfamily,         % Monospace font
frame=none,                   % No frame around code
keepspaces=true,              % Preserve spaces
showspaces=false,             % Don't show space characters
showtabs=false,               % Don't show tab characters
columns=flexible,             % Flexible column width
belowskip=0pt,               % Minimal spacing
aboveskip=0pt
}

% =============================================================================
% PAGE GEOMETRY
% =============================================================================
% MIT Press trim size: 7" x 10" (per publisher specifications)
% This is a standard academic textbook format providing good readability
% for technical content with figures and code blocks.
% Wide outer margin accommodates sidenotes/margin notes.
\geometry{
  paperwidth=7in,
  paperheight=10in,
  top=0.875in,
  bottom=0.875in,
  inner=0.875in,              % Inner margin (binding side)
  outer=1.75in,               % Outer margin (includes space for sidenotes)
  footskip=30pt,
  marginparwidth=1.25in,      % Width for margin notes
  twoside                     % Different left/right pages
}

% =============================================================================
% SIDENOTE STYLING
% =============================================================================
% Custom sidenote design with crimson vertical bar
\renewcommand{\thefootnote}{\textcolor{crimson}{\arabic{footnote}}}

% Save original sidenote command
\makeatletter
\@ifundefined{oldsidenote}{
  \let\oldsidenote\sidenote%
}{}
\makeatother

% Redefine sidenote with vertical crimson bar
\renewcommand{\sidenote}[1]{%
  \oldsidenote{%
    \noindent
    \color{crimson!100}                        % Crimson vertical line
    \raisebox{0em}{%
      \rule{0.5pt}{1.5em}                      % Thin vertical line
    }
    \hspace{0.3em}                             % Space after line
    \color{black}                              % Reset text color
    \footnotesize #1                           % Sidenote content
  }%
}

% =============================================================================
% FLOAT HANDLING
% =============================================================================
% Patch LaTeX's output routine to handle float overflow gracefully
% The "Float(s) lost" error occurs in \@doclearpage when \@currlist is not empty
% This patch silently clears pending floats that can't be placed
\makeatletter
\let\orig@doclearpage\@doclearpage
\def\@doclearpage{%
  \ifx\@currlist\@empty\else
    \global\let\@currlist\@empty
    \typeout{Warning: Floats cleared to prevent overflow}%
  \fi
  \orig@doclearpage
}
\makeatother

% Additional safety for structural commands
\let\originalbackmatter\backmatter
\renewcommand{\backmatter}{%
  \clearpage%
  \originalbackmatter%
}

\let\originalfrontmatter\frontmatter
\renewcommand{\frontmatter}{%
  \clearpage%
  \originalfrontmatter%
}

\let\originalmainmatter\mainmatter
\renewcommand{\mainmatter}{%
  \clearpage%
  \originalmainmatter%
}

% =============================================================================
% PAGE HEADERS AND FOOTERS
% =============================================================================
% Ensure chapters use fancy page style (not plain)
\patchcmd{\chapter}{\thispagestyle{plain}}{\thispagestyle{fancy}}{}{}

% Main page style with crimson headers
\pagestyle{fancy}
\fancyhf{}                                              % Clear all
\fancyhead[LE]{\small\color{crimson}\nouppercase{\rightmark}}  % Left even: section
\fancyhead[RO]{\color{crimson}\thepage}                 % Right odd: page number
\fancyhead[LO]{\small\color{crimson}\nouppercase{\leftmark}}   % Left odd: chapter
\fancyhead[RE]{\color{crimson}\thepage}                 % Right even: page number
\renewcommand{\headrulewidth}{0.4pt}                    % Thin header line
\renewcommand{\footrulewidth}{0pt}                      % No footer line

% Plain page style (for chapter openings)
\fancypagestyle{plain}{
  \fancyhf{}
  \fancyfoot[C]{\color{crimson}\thepage}                % Centered page number
  \renewcommand{\headrulewidth}{0pt}
  \renewcommand{\footrulewidth}{0pt}
}

% =============================================================================
% KOMA-SCRIPT FONT ADJUSTMENTS
% =============================================================================
% Apply crimson color to all heading levels
\addtokomafont{disposition}{\rmfamily\color{crimson}}
\addtokomafont{chapter}{\color{crimson}}
\addtokomafont{section}{\color{crimson}}
\addtokomafont{subsection}{\color{crimson}}

% =============================================================================
% ABSTRACT ENVIRONMENT
% =============================================================================
\newenvironment{abstract}{
  \chapter*{\abstractname}
  \addcontentsline{toc}{chapter}{\abstractname}
  \small
}{
  \clearpage
}

% =============================================================================
% HYPERLINK CONFIGURATION
% =============================================================================
% Crimson-colored links throughout, two-page PDF layout
\hypersetup{
  linkcolor=crimson,
  citecolor=crimson,
  urlcolor=crimson,
  pdfpagelayout=TwoPageRight,   % Two-page spread view
  pdfstartview=Fit               % Initial zoom fits page
}

% =============================================================================
% PART SUMMARY SYSTEM
% =============================================================================
% Allows adding descriptive text below part titles
\newcommand{\partsummary}{}     % Empty by default
\newif\ifhaspartsummary%
\haspartsummaryfalse%

\newcommand{\setpartsummary}[1]{%
  \renewcommand{\partsummary}{#1}%
  \haspartsummarytrue%
}

% Additional colors for part page backgrounds
\definecolor{BrownLL}{RGB}{233,222,220}
\definecolor{BlueDD}{RGB}{62,100,125}
\colorlet{BlueDD}{magenta}

% ===============================================================================
% PART STYLING SYSTEM
% ===============================================================================
%
% This system provides three distinct visual styles for book organization:
%
% 1. NUMBERED PARTS (\part{title}) - For main book sections
%    - Roman numerals (I, II, III, etc.) in top right corner
%    - Crimson title with horizontal lines above/below
%    - "Part I" label in sidebar
%    - Used for: foundations, principles, optimization, deployment, etc.
%
% 2. UNNUMBERED PARTS (\part*{title}) - For special sections like "Labs"
%    - Division-style geometric background (left side)
%    - No Roman numerals
%    - Used for: labs section
%
% 3. DIVISIONS (\division{title}) - For major book divisions
%    - Clean geometric background with centered title
%    - Used for: frontmatter, main_content, backmatter
%
% The Lua filter (inject-parts.lua) automatically routes parts by {key:xxx} commands
% to the appropriate LaTeX command based on the key name.
% ===============================================================================

% NUMBERED PARTS: Roman numeral styling for main book sections
\titleformat{\part}[display]
{\thispagestyle{empty}}{}{20pt}{
\begin{tikzpicture}[remember picture,overlay]
%%%
%%
\node[crimson,align=flush right,
inner sep=0,outer sep=0mm,draw=none,%
anchor=east,minimum height=31mm, text width=1.2\textwidth,
yshift=-30mm,font={%
\fontsize{98pt}{104}\selectfont\bfseries}]  (BG) at (current page text area.north east){\thepart};
%
\node[black,inner sep=0mm,draw=none,
anchor=mid,text width=1.2\textwidth,
 minimum height=35mm, align=right,
node distance=7mm,below=of BG,
font={\fontsize{30pt}{34}\selectfont}]
(BGG)  {\hyphenchar\font=-1 \color{black}\MakeUppercase {#1}};
\draw [crimson,line width=3pt] ([yshift=0mm]BGG.north west) -- ([yshift=0mm]BGG.north east);
\draw [crimson,line width=2pt] ([yshift=0mm]BGG.south west) -- ([yshift=0mm]BGG.south east);
%
\node[fill=crimson,text=white,rotate=90,%
anchor=south west,minimum height=15mm,
minimum width=40mm,font={%
\fontsize{20pt}{20}\selectfont\bfseries}](BP)  at
(current page text area.south east)
{{\sffamily Part}~\thepart};
%
\path[red](BP.north west)-|coordinate(PS)(BGG.south west);
%
% Part summary box commented out for cleaner design
% \ifhaspartsummary
% \node[inner sep=4pt,text width=0.7\textwidth,draw=none,fill=BrownLL!40,
% align=justify,font={\fontsize{9pt}{12}\selectfont},anchor=south west]
% at (PS) {\partsummary};
% \fi
\end{tikzpicture}
}[]

\renewcommand{\thepart}{\Roman{part}}

% UNNUMBERED PARTS: Division-style background for special sections
\titleformat{name=\part,numberless}[display]
{\thispagestyle{empty}}{}{20pt}{
\begin{tikzpicture}[remember picture,overlay]
%%%
\coordinate(S1)at([yshift=-200mm]current page.north west);
\draw[draw=none,fill=BlueDD!7](S1)--++(45:16)coordinate(S2)-
|(S2|-current page.north west)--(current page.north west)coordinate(S3)--(S1);
%
\coordinate(E1)at([yshift=-98mm]current page.north west);
\draw[draw=none,fill=BlueDD!15](E1)--(current page.north west)coordinate(E2)
--++(0:98mm)coordinate(E3)--(E1);
%
\coordinate(D1)at([yshift=15mm]current page.south west);
\draw[draw=none,fill=BlueDD!40,opacity=0.5](D1)--++(45:5.5)coordinate(D2)
-|(D2|-current page.north west)--(current page.north west)coordinate(D3)--(D1);
%%%%
\path[red](S2)-|(S2-|current page.east)coordinate(SS2);
%PART
\node[crimson,align=flush right,inner sep=0,outer sep=0mm,draw=none,anchor=south,
font={\fontsize{48pt}{48}\selectfont\bfseries}]  (BG) at ($(S2)!0.5!(SS2)$){\hphantom{Part}};
%%%
\path[green]([yshift=15mm]D2)-|coordinate(TPD)(BG.south east);
\node[inner sep=0mm,draw=none,anchor=south east,%text width=0.9\textwidth,
align=right,font={\fontsize{40pt}{40}\selectfont}]
(BGG) at (TPD)  {\color{crimson}\MakeUppercase {#1}};%\MakeUppercase {}
\end{tikzpicture}
}

% Define \numberedpart command for numbered parts
\newcommand{\numberedpart}[1]{%
\FloatBarrier%  % Flush all pending floats before part break
\clearpage
\thispagestyle{empty}
\stepcounter{part}%
\begin{tikzpicture}[remember picture,overlay]
%%%
%%
\node[crimson,align=flush right,
inner sep=0,outer sep=0mm,draw=none,%
anchor=east,minimum height=31mm, text width=1.2\textwidth,
yshift=-30mm,font={%
\fontsize{98pt}{104}\selectfont\bfseries}]  (BG) at (current page text area.north east){\thepart};
%
\node[black,inner sep=0mm,draw=none,
anchor=mid,text width=1.2\textwidth,
 minimum height=35mm, align=right,
node distance=7mm,below=of BG,
font={\fontsize{30pt}{34}\selectfont}]
(BGG)  {\hyphenchar\font=-1 \color{black}\MakeUppercase {#1}};
\draw [crimson,line width=3pt] ([yshift=0mm]BGG.north west) -- ([yshift=0mm]BGG.north east);
\draw [crimson,line width=2pt] ([yshift=0mm]BGG.south west) -- ([yshift=0mm]BGG.south east);
%
\node[fill=crimson,text=white,rotate=90,%
anchor=south west,minimum height=15mm,
minimum width=40mm,font={%
\fontsize{20pt}{20}\selectfont\bfseries}](BP)  at
(current page text area.south east)
{{\sffamily Part}~\thepart};
%
\path[red](BP.north west)-|coordinate(PS)(BGG.south west);
%
% Part summary box commented out for cleaner design
% \ifhaspartsummary
% \node[inner sep=4pt,text width=0.7\textwidth,draw=none,fill=BrownLL!40,
% align=justify,font={\fontsize{9pt}{12}\selectfont},anchor=south west]
% at (PS) {\partsummary};
% \fi
\end{tikzpicture}
\clearpage
}



% DIVISIONS: Clean geometric styling with subtle tech elements
% Used for frontmatter, main_content, and backmatter divisions
\newcommand{\division}[1]{%
\FloatBarrier%  % Flush all pending floats before division break
\clearpage
\thispagestyle{empty}
\begin{tikzpicture}[remember picture,overlay]

% Clean geometric background (original design)
\coordinate(S1)at([yshift=-200mm]current page.north west);
\draw[draw=none,fill=BlueDD!7](S1)--++(45:16)coordinate(S2)-
|(S2|-current page.north west)--(current page.north west)coordinate(S3)--(S1);

\coordinate(E1)at([yshift=-98mm]current page.north west);
\draw[draw=none,fill=BlueDD!15](E1)--(current page.north west)coordinate(E2)
--++(0:98mm)coordinate(E3)--(E1);

\coordinate(D1)at([yshift=15mm]current page.south west);
\draw[draw=none,fill=BlueDD!40,opacity=0.5](D1)--++(45:5.5)coordinate(D2)
-|(D2|-current page.north west)--(current page.north west)coordinate(D3)--(D1);

% Subtle tech elements - positioned in white areas for better visibility
% Upper right white area - more visible
\draw[crimson!40, line width=0.8pt] ([xshift=140mm,yshift=-60mm]current page.north west) -- ++(40mm,0);
\draw[crimson!40, line width=0.8pt] ([xshift=150mm,yshift=-70mm]current page.north west) -- ++(30mm,0);
\draw[crimson!35, line width=0.7pt] ([xshift=160mm,yshift=-60mm]current page.north west) -- ++(0,-15mm);
\draw[crimson!35, line width=0.7pt] ([xshift=170mm,yshift=-70mm]current page.north west) -- ++(0,10mm);

% Circuit nodes - upper right
\fill[crimson!50] ([xshift=160mm,yshift=-60mm]current page.north west) circle (1.5mm);
\fill[white] ([xshift=160mm,yshift=-60mm]current page.north west) circle (0.8mm);
\fill[crimson!50] ([xshift=170mm,yshift=-70mm]current page.north west) circle (1.3mm);
\fill[white] ([xshift=170mm,yshift=-70mm]current page.north west) circle (0.6mm);

% Lower right white area - enhanced visibility
\draw[crimson!45, line width=0.9pt] ([xshift=140mm,yshift=-190mm]current page.north west) -- ++(45mm,0);
\draw[crimson!45, line width=0.9pt] ([xshift=150mm,yshift=-200mm]current page.north west) -- ++(35mm,0);
\draw[crimson!40, line width=0.8pt] ([xshift=160mm,yshift=-190mm]current page.north west) -- ++(0,-20mm);
\draw[crimson!40, line width=0.8pt] ([xshift=170mm,yshift=-200mm]current page.north west) -- ++(0,15mm);

% Additional connecting lines in lower right
\draw[crimson!35, line width=0.7pt] ([xshift=130mm,yshift=-180mm]current page.north west) -- ++(25mm,0);
\draw[crimson!35, line width=0.7pt] ([xshift=145mm,yshift=-180mm]current page.north west) -- ++(0,-25mm);

% Circuit nodes - lower right (more prominent)
\fill[crimson!55] ([xshift=160mm,yshift=-190mm]current page.north west) circle (1.6mm);
\fill[white] ([xshift=160mm,yshift=-190mm]current page.north west) circle (0.9mm);
\fill[crimson!55] ([xshift=170mm,yshift=-200mm]current page.north west) circle (1.4mm);
\fill[white] ([xshift=170mm,yshift=-200mm]current page.north west) circle (0.7mm);
\fill[crimson!50] ([xshift=145mm,yshift=-180mm]current page.north west) circle (1.2mm);
\fill[white] ([xshift=145mm,yshift=-180mm]current page.north west) circle (0.6mm);

% Title positioned in center - clean and readable
\node[inner sep=0mm,draw=none,anchor=center,text width=0.8\textwidth,
align=center,font={\fontsize{40pt}{40}\selectfont}]
(BGG) at (current page.center)  {\color{crimson}\MakeUppercase {#1}};

\end{tikzpicture}
\clearpage
}

% LAB DIVISIONS: Circuit-style neural network design for lab sections
% Used specifically for lab platform sections (arduino, xiao, grove, etc.)
\newcommand{\labdivision}[1]{%
\FloatBarrier%  % Flush all pending floats before lab division break
\clearpage
\thispagestyle{empty}
\begin{tikzpicture}[remember picture,overlay]
% Circuit background with subtle gradient
\coordinate(S1)at([yshift=-200mm]current page.north west);
\draw[draw=none,fill=BlueDD!5](S1)--++(45:16)coordinate(S2)-
|(S2|-current page.north west)--(current page.north west)coordinate(S3)--(S1);

% TOP AREA: Circuit lines in upper white space
\draw[crimson!50, line width=1.5pt] ([xshift=30mm,yshift=-40mm]current page.north west) -- ++(60mm,0);
\draw[crimson!40, line width=1pt] ([xshift=120mm,yshift=-50mm]current page.north west) -- ++(50mm,0);
\draw[crimson!50, line width=1.5pt] ([xshift=40mm,yshift=-70mm]current page.north west) -- ++(40mm,0);

% Connecting lines in top area
\draw[crimson!30, line width=1pt] ([xshift=60mm,yshift=-40mm]current page.north west) -- ++(0,-20mm);
\draw[crimson!30, line width=1pt] ([xshift=145mm,yshift=-50mm]current page.north west) -- ++(0,10mm);

% Neural nodes in top area
\fill[crimson!70] ([xshift=60mm,yshift=-40mm]current page.north west) circle (2.5mm);
\fill[white] ([xshift=60mm,yshift=-40mm]current page.north west) circle (1.5mm);
\fill[crimson!60] ([xshift=145mm,yshift=-50mm]current page.north west) circle (2mm);
\fill[white] ([xshift=145mm,yshift=-50mm]current page.north west) circle (1mm);
\fill[crimson!80] ([xshift=80mm,yshift=-70mm]current page.north west) circle (2mm);
\fill[white] ([xshift=80mm,yshift=-70mm]current page.north west) circle (1mm);

% BOTTOM AREA: Circuit lines in lower white space
\draw[crimson!50, line width=1.5pt] ([xshift=20mm,yshift=-200mm]current page.north west) -- ++(70mm,0);
\draw[crimson!40, line width=1pt] ([xshift=110mm,yshift=-210mm]current page.north west) -- ++(60mm,0);
\draw[crimson!50, line width=1.5pt] ([xshift=35mm,yshift=-230mm]current page.north west) -- ++(45mm,0);

% Connecting lines in bottom area
\draw[crimson!30, line width=1pt] ([xshift=55mm,yshift=-200mm]current page.north west) -- ++(0,-20mm);
\draw[crimson!30, line width=1pt] ([xshift=140mm,yshift=-210mm]current page.north west) -- ++(0,15mm);

% Neural nodes in bottom area
\fill[crimson!70] ([xshift=55mm,yshift=-200mm]current page.north west) circle (2.5mm);
\fill[white] ([xshift=55mm,yshift=-200mm]current page.north west) circle (1.5mm);
\fill[crimson!60] ([xshift=140mm,yshift=-210mm]current page.north west) circle (2mm);
\fill[white] ([xshift=140mm,yshift=-210mm]current page.north west) circle (1mm);
\fill[crimson!80] ([xshift=80mm,yshift=-230mm]current page.north west) circle (2mm);
\fill[white] ([xshift=80mm,yshift=-230mm]current page.north west) circle (1mm);

% SIDE AREAS: Subtle circuit elements on left and right edges
\draw[crimson!30, line width=1pt] ([xshift=15mm,yshift=-120mm]current page.north west) -- ++(20mm,0);
\draw[crimson!30, line width=1pt] ([xshift=175mm,yshift=-130mm]current page.north west) -- ++(15mm,0);
\fill[crimson!50] ([xshift=25mm,yshift=-120mm]current page.north west) circle (1.5mm);
\fill[white] ([xshift=25mm,yshift=-120mm]current page.north west) circle (0.8mm);
\fill[crimson!50] ([xshift=185mm,yshift=-130mm]current page.north west) circle (1.5mm);
\fill[white] ([xshift=185mm,yshift=-130mm]current page.north west) circle (0.8mm);

% Title positioned in center - CLEAN AREA
\node[inner sep=0mm,draw=none,anchor=center,text width=0.8\textwidth,
align=center,font={\fontsize{44pt}{44}\selectfont\bfseries}]
(BGG) at (current page.center)  {\color{crimson}\MakeUppercase {#1}};

\end{tikzpicture}
\clearpage
}

% Define \lab command for lab styling (different visual treatment)
\newcommand{\lab}[1]{%
\begin{tikzpicture}[remember picture,overlay]
%%%
% Different background pattern for labs
\coordinate(S1)at([yshift=-200mm]current page.north west);
\draw[draw=none,fill=BlueDD!15](S1)--++(45:16)coordinate(S2)-
|(S2|-current page.north west)--(current page.north west)coordinate(S3)--(S1);
%
\coordinate(E1)at([yshift=-98mm]current page.north west);
\draw[draw=none,fill=BlueDD!25](E1)--(current page.north west)coordinate(E2)
--++(0:98mm)coordinate(E3)--(E1);
%
\coordinate(D1)at([yshift=15mm]current page.south west);
\draw[draw=none,fill=BlueDD!60,opacity=0.7](D1)--++(45:5.5)coordinate(D2)
-|(D2|-current page.north west)--(current page.north west)coordinate(D3)--(D1);
%%%%
\path[red](S2)-|(S2-|current page.east)coordinate(SS2);
%LAB - Different styling
\node[crimson,align=flush right,inner sep=0,outer sep=0mm,draw=none,anchor=south,
font={\fontsize{48pt}{48}\selectfont\bfseries}]  (BG) at ($(S2)!0.5!(SS2)$){\hphantom{Workshop}};
%%%
\path[green]([yshift=15mm]D2)-|coordinate(TPD)(BG.south east);
\node[inner sep=0mm,draw=none,anchor=south east,%text width=0.9\textwidth,
align=right,font={\fontsize{40pt}{40}\selectfont}]
(BGG) at (TPD)  {\color{crimson}\MakeUppercase {#1}};%\MakeUppercase {}
\end{tikzpicture}
\thispagestyle{empty}
\clearpage
}

% =============================================================================
% SECTION FORMATTING
% =============================================================================
% All section levels use crimson color and are ragged right

% Section (Large, bold, crimson)
\titleformat{\section}
  {\normalfont\Large\bfseries\color{crimson}\raggedright}
  {\thesection}
  {0.5em}
  {#1}
\titlespacing*{\section}{0pc}{14pt plus 4pt minus 4pt}{6pt plus 2pt minus 2pt}[0pc]

% Subsection (large, bold, crimson)
\titleformat{\subsection}
  {\normalfont\large\bfseries\color{crimson}\raggedright}
  {\thesubsection}
  {0.5em}
  {#1}
\titlespacing*{\subsection}{0pc}{12pt plus 4pt minus 4pt}{5pt plus 1pt minus 2pt}[0pc]

% Subsubsection (normal size, bold, crimson)
\titleformat{\subsubsection}
  {\normalfont\normalsize\bfseries\color{crimson}\raggedright}
  {\thesubsubsection}
  {0.5em}
  {#1}
\titlespacing*{\subsubsection}{0pc}{12pt plus 4pt minus 4pt}{5pt plus 1pt minus 2pt}[0pc]

% Paragraph (run-in, bold, crimson, ends with period)
\titleformat{\paragraph}[runin]
  {\normalfont\normalsize\bfseries\color{crimson}}
  {\theparagraph}
  {0.5em}
  {#1}
  [\textbf{.}]
  \titlespacing*{\paragraph}{0pc}{6pt plus 2pt minus 2pt}{0.5em}[0pc]

% Subparagraph (run-in, italic, crimson, ends with period)
\titleformat{\subparagraph}[runin]
  {\normalfont\normalsize\itshape\color{crimson}}
  {\thesubparagraph}
  {0.5em}
  {#1}
  [\textbf{.}]
  \titlespacing*{\subparagraph}{0pc}{6pt plus 2pt minus 2pt}{0.5em}[0pc]

% =============================================================================
% CHAPTER FORMATTING
% =============================================================================
% Numbered chapters: "Chapter X" prefix, huge crimson title
\titleformat{\chapter}[display]
  {\normalfont\huge\bfseries\color{crimson}}
  {\chaptername\ \thechapter}
  {20pt}
  {\Huge #1}
  []

% Unnumbered chapters: no prefix, huge crimson title
\titleformat{name=\chapter,numberless}
  {\normalfont\huge\bfseries\color{crimson}}
  {}
  {0pt}
  {\Huge #1}
  []

\renewcommand{\chaptername}{Chapter}
% =============================================================================
% TABLE OF CONTENTS FORMATTING
% =============================================================================
\setcounter{tocdepth}{2}                      % Show chapters, sections, subsections

% TOC spacing adjustments for number widths and indentation
\setlength{\cftchapnumwidth}{2em}             % Chapter number width
\setlength{\cftsecnumwidth}{2.75em}           % Section number width
\setlength{\cftsubsecnumwidth}{3.25em}        % Subsection number width
\setlength{\cftsubsubsecnumwidth}{4em}        % Subsubsection number width
\setlength{\cftsubsecindent}{4.25em}          % Subsection indent
\setlength{\cftsubsubsecindent}{7.5em}        % Subsubsection indent

% Chapter entries in TOC: bold crimson with "Chapter" prefix
\renewcommand{\cftchapfont}{\bfseries\color{crimson}}
\renewcommand{\cftchappresnum}{\color{crimson}Chapter~}

% Custom formatting for division entries (styled like parts)
\newcommand{\divisionchapter}[1]{%
  \addvspace{12pt}%
  \noindent\hfil\bfseries\color{crimson}#1\hfil\par%
  \addvspace{6pt}%
}

% Adjust TOC spacing for "Chapter" prefix
\newlength{\xtraspace}
\settowidth{\xtraspace}{\cftchappresnum\cftchapaftersnum}
\addtolength{\cftchapnumwidth}{\xtraspace}

% Unnumbered chapters with TOC entry
\newcommand{\likechapter}[1]{%
    \chapter*{#1}
    \addcontentsline{toc}{chapter}{\textcolor{crimson}{#1}}
}

% =============================================================================
% PAGE NUMBERING SYSTEM
% =============================================================================
% Implements traditional book numbering:
% - Roman numerals (i, ii, iii...) for frontmatter
% - Arabic numerals (1, 2, 3...) for mainmatter
% Automatically switches at first numbered chapter
\makeatletter
\newif\if@firstnumbered%
\@firstnumberedtrue%
\newif\if@firstunnumbered%
\@firstunnumberedtrue%

\newcounter{lastRomanPage}
\setcounter{lastRomanPage}{1}

% Start document with Roman numerals (frontmatter)
\AtBeginDocument{
  \pagenumbering{roman}
  \renewcommand{\thepage}{\roman{page}}
}

% Intercept chapter command
\let\old@chapter\chapter%
\renewcommand{\chapter}{%
  \@ifstar{\unnumbered@chapter}{\numbered@chapter}%
}

% Numbered chapters: switch to Arabic on first occurrence
\newcommand{\numbered@chapter}[1]{%
  \if@firstnumbered%
    \cleardoublepage%
    \setcounter{lastRomanPage}{\value{page}}%
    \pagenumbering{arabic}%
    \@firstnumberedfalse%
  \else
    \setcounter{page}{\value{page}}%
  \fi
  \setcounter{sidenote}{1}                    % Reset footnote counter per chapter
  \old@chapter{#1}%
}

% Unnumbered chapters: stay in Roman numerals
\newcommand{\unnumbered@chapter}[1]{%
  \if@firstunnumbered%
    \clearpage
    \setcounter{lastRomanPage}{\value{page}}%
    \pagenumbering{roman}%
    \@firstunnumberedfalse%
  \fi
  \setcounter{sidenote}{1}
  \old@chapter*{#1}%
}
\makeatother

% =============================================================================
% TABLE SIZING AND SPACING
% =============================================================================
% Make tables slightly smaller to fit more content
\AtBeginEnvironment{longtable}{\scriptsize}

% Increase vertical spacing in table cells (default is 1.0)
\renewcommand{\arraystretch}{1.3}

% Prefer placing tables at the top of pages
\makeatletter
\renewcommand{\fps@table}{t}  % Default placement: top of page
\makeatother

% =============================================================================
% LONGTABLE PAGE BREAKING FIXES (Windows compatibility)
% =============================================================================
% Prevent "Infinite glue shrinkage" errors on Windows LaTeX builds
% by giving longtable more flexibility in page breaking

% Allow more flexible page breaking (vs strict \flushbottom)
\raggedbottom

% Process more rows before attempting page break (default is 20)
\setcounter{LTchunksize}{50}

% Add extra stretch for longtable environments specifically
\AtBeginEnvironment{longtable}{%
  \setlength{\emergencystretch}{3em}%
  \setlength{\parskip}{0pt plus 1pt}%
}

% =============================================================================
% TABLE STYLING - Clean tables with crimson borders
% =============================================================================
% Professional table appearance with:
% - Clean white background (no colored rows)
% - Crimson-colored borders
% - Good spacing for readability
%
% Note: Headers are automatically bolded by Quarto when using **text** in source
\usepackage{booktabs}      % Professional table rules (\toprule, \midrule, \bottomrule)
\usepackage{colortbl}      % For colored borders (\arrayrulecolor)

% Global table styling - crimson borders
\setlength{\arrayrulewidth}{0.5pt}          % Thinner borders than default
%\arrayrulecolor{crimson}                    % Crimson borders matching brand

\setcounter{chapter}{0}
\usepackage{needspace}
\let\Needspace\needspace
\makeatletter
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{plain}
\@ifundefined{c@chapter}{\newfloat{vid}{h}{lovid}}{\newfloat{vid}{h}{lovid}[chapter]}
\floatname{vid}{Video}
\newcommand*\listofvids{\listof{vid}{List of Videos}}
\makeatother
\makeatletter
\@ifpackageloaded{tcolorbox}{}{\usepackage[skins,breakable]{tcolorbox}}
\@ifpackageloaded{fontawesome5}{}{\usepackage{fontawesome5}}
\definecolor{quarto-callout-color}{HTML}{909090}
\definecolor{quarto-callout-note-color}{HTML}{0758E5}
\definecolor{quarto-callout-important-color}{HTML}{CC1914}
\definecolor{quarto-callout-warning-color}{HTML}{EB9113}
\definecolor{quarto-callout-tip-color}{HTML}{00A047}
\definecolor{quarto-callout-caution-color}{HTML}{FC5300}
\definecolor{quarto-callout-color-frame}{HTML}{acacac}
\definecolor{quarto-callout-note-color-frame}{HTML}{4582ec}
\definecolor{quarto-callout-important-color-frame}{HTML}{d9534f}
\definecolor{quarto-callout-warning-color-frame}{HTML}{f0ad4e}
\definecolor{quarto-callout-tip-color-frame}{HTML}{02b875}
\definecolor{quarto-callout-caution-color-frame}{HTML}{fd7e14}
\makeatother
\makeatletter
\@ifpackageloaded{bookmark}{}{\usepackage{bookmark}}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\makeatletter
\@ifpackageloaded{sidenotes}{}{\usepackage{sidenotes}}
\@ifpackageloaded{marginnote}{}{\usepackage{marginnote}}
\makeatother
\newcommand{\fbxIconPath}{assets/images/icons/callouts}
\newcommand{\fbxIconFormat}{pdf}
\makeatletter
\@ifpackageloaded{tcolorbox}{}{\usepackage[many]{tcolorbox}}
\makeatother
%%%% ---foldboxy preamble ----- %%%%%

% Load xstring for string manipulation
\RequirePackage{xstring}

% Icon path and format configuration - can be overridden in filter-metadata
\providecommand{\fbxIconPath}{assets/images/icons/callouts}
\providecommand{\fbxIconFormat}{pdf}

% Helper command to include icon with hyphen-to-underscore conversion
% This ensures consistency: callout-quiz-question -> callout_quiz_question
\newcommand{\fbxIncludeIcon}[2]{%
  \StrSubstitute{#1}{-}{_}[\fbxIconName]%
  \includegraphics[width=#2]{\fbxIconPath/icon_\fbxIconName.\fbxIconFormat}%
}

% Legacy fallback colors (keep for compatibility)
\definecolor{fbx-default-color1}{HTML}{c7c7d0}
\definecolor{fbx-default-color2}{HTML}{a3a3aa}
\definecolor{fbox-color1}{HTML}{c7c7d0}
\definecolor{fbox-color2}{HTML}{a3a3aa}

% arguments: #1 typelabelnummer: #2 titel: #3
\newenvironment{fbx}[3]{%
\begin{tcolorbox}[
  enhanced,
  breakable,
  %fontupper=\fontsize{8pt}{10pt}\selectfont,  % 95% of body text (10pt -> 9.5pt)
  before skip=8pt,  % space above box (increased)
  after skip=8pt,   % space below box (increased)
  attach boxed title to top*={xshift=0pt},
  boxed title style={
  %fuzzy shadow={1pt}{-1pt}{0mm}{0.1mm}{gray},
  arc=1.5pt,
  rounded corners=north,
  sharp corners=south,
  top=6pt,          % Adjusted for ~40px equivalent height
  bottom=5pt,       % Adjusted for ~40px equivalent height
  overlay={
      \node [left,outer sep=0em, black,draw=none,anchor=west,
        rectangle,fill=none,inner sep=0pt]
        at ([xshift=4mm]frame.west) {\fbxIncludeIcon{#1}{4.2mm}};
    },
  },
  colframe=#1-color2,             % Border color (auto-generated from YAML)
  colbacktitle=#1-color1,         % Background color (auto-generated from YAML)
  colback=white,
  coltitle=black,
  titlerule=0mm,
  toprule=0.5pt,
  bottomrule=0.5pt,
  leftrule=2.2pt,
  rightrule=0.5pt,
  outer arc=1.5pt,
  arc=1.5pt,
  left=0.5em,       % increased left padding
  bottomtitle=1.5mm, % increased title bottom margin
  toptitle=1.5mm,    % increased title top margin
  title=\hspace{2.5em}\protect#2\hspace{0.2em}\protect#3, % Protect parameters
  extras middle and last={top=4pt} % increased continuation spacing
]}
{\end{tcolorbox}}


% boxed environment with right border
\newenvironment{fbxSimple}[3]{\begin{tcolorbox}[
  enhanced,
  breakable,
  %fontupper=\fontsize{8pt}{10pt}\selectfont,  % 95% of body text (10pt -> 9.5pt)
  before skip=8pt,  % space above box (increased)
  after skip=8pt,   % space below box (increased)
  attach boxed title to top*={xshift=0pt},
  boxed title style={
  %fuzzy shadow={1pt}{-1pt}{0mm}{0.1mm}{gray},
  arc=1.5pt,
  rounded corners=north,
  sharp corners=south,
  top=6pt,          % Adjusted for ~40px equivalent height
  bottom=5pt,       % Adjusted for ~40px equivalent height
  overlay={
      \node [left,outer sep=0em, black,draw=none,anchor=west,
        rectangle,fill=none,inner sep=0pt]
        at ([xshift=3mm]frame.west) {\fbxIncludeIcon{#1}{4.2mm}};
    },
  },
  colframe=#1-color2,             % Border color (auto-generated from YAML)
  colbacktitle=#1-color1,         % Background color (auto-generated from YAML)
  colback=white,
  coltitle=black,
  titlerule=0mm,
  toprule=0.5pt,
  bottomrule=0.5pt,
  leftrule=2.2pt,
  rightrule=0.5pt,
  outer arc=1.5pt,
  arc=1.5pt,
  left=0.5em,       % increased left padding
  bottomtitle=1.5mm, % increased title bottom margin
  toptitle=1.5mm,    % increased title top margin
  title=\hspace{2.5em}\protect#2\hspace{0.2em}\protect#3, % Protect parameters
  boxsep=1pt,
  extras first={bottom=0pt},
  extras last={top=0pt,bottom=-4pt},
  overlay first={
    \draw[line width=1pt,white] ([xshift=2.2pt]frame.south west)-- ([xshift=-0.5pt]frame.south east);
  },
  overlay last={
    \draw[line width=1pt,white] ([xshift=2.2pt]frame.north west)-- ([xshift=-0.5pt]frame.north east);
   }
]}
{\end{tcolorbox}}

%%%% --- end foldboxy preamble ----- %%%%%
%%==== colors from yaml ===%
\definecolor{callout-quiz-question-color1}{HTML}{F0F0F8}
\definecolor{callout-quiz-question-color2}{HTML}{5B4B8A}
\definecolor{callout-resource-exercises-color1}{HTML}{E0F2F1}
\definecolor{callout-resource-exercises-color2}{HTML}{20B2AA}
\definecolor{callout-chapter-connection-color1}{HTML}{FDF2F7}
\definecolor{callout-chapter-connection-color2}{HTML}{A51C30}
\definecolor{callout-notebook-color1}{HTML}{F2F7FF}
\definecolor{callout-notebook-color2}{HTML}{2C5282}
\definecolor{callout-colab-color1}{HTML}{FFF5E6}
\definecolor{callout-colab-color2}{HTML}{FF6B35}
\definecolor{callout-definition-color1}{HTML}{F0F4F8}
\definecolor{callout-definition-color2}{HTML}{1B4F72}
\definecolor{callout-lighthouse-color1}{HTML}{FDF8E6}
\definecolor{callout-lighthouse-color2}{HTML}{B8860B}
\definecolor{callout-checkpoint-color1}{HTML}{E8F5E9}
\definecolor{callout-checkpoint-color2}{HTML}{2E7D32}
\definecolor{callout-perspective-color1}{HTML}{F7F8FA}
\definecolor{callout-perspective-color2}{HTML}{4A5568}
\definecolor{callout-resource-videos-color1}{HTML}{E0F2F1}
\definecolor{callout-resource-videos-color2}{HTML}{20B2AA}
\definecolor{callout-example-color1}{HTML}{F0F8F6}
\definecolor{callout-example-color2}{HTML}{148F77}
\definecolor{callout-quiz-answer-color1}{HTML}{E8F2EA}
\definecolor{callout-quiz-answer-color2}{HTML}{4a7c59}
\definecolor{callout-resource-slides-color1}{HTML}{E0F2F1}
\definecolor{callout-resource-slides-color2}{HTML}{20B2AA}
\definecolor{callout-code-color1}{HTML}{F2F4F8}
\definecolor{callout-code-color2}{HTML}{D1D7E0}
%=============%

\usepackage{hyphenat}
\usepackage{ifthen}
\usepackage{calc}
\usepackage{calculator}



\usepackage{graphicx}
\usepackage{geometry}
\usepackage{afterpage}
\usepackage{tikz}
\usetikzlibrary{calc}
\usetikzlibrary{fadings}
\usepackage[pagecolor=none]{pagecolor}


% Set the titlepage font families







% Set the coverpage font families

\usepackage{bookmark}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  pdftitle={Machine Learning Systems},
  pdfauthor={Vijay Janapa Reddi},
  colorlinks=true,
  linkcolor={Maroon},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}


\title{Machine Learning Systems}
\usepackage{etoolbox}
\makeatletter
\providecommand{\subtitle}[1]{% add subtitle to \maketitle
  \apptocmd{\@title}{\par {\large #1 \par}}{}{}
}
\makeatother
\subtitle{Volume I: Introduction}
\author{Vijay Janapa Reddi}
\date{January 28, 2026}
\begin{document}
%%%%% begin titlepage extension code

  \begin{frontmatter}

\begin{titlepage}
% This is a combination of Pandoc templating and LaTeX
% Pandoc templating https://pandoc.org/MANUAL.html#templates
% See the README for help

\thispagestyle{empty}

\newgeometry{top=-100in}

% Page color

\newcommand{\coverauthorstyle}[1]{{\fontsize{20}{24.0}\selectfont
{#1}}}

\begin{tikzpicture}[remember picture, overlay, inner sep=0pt, outer sep=0pt]

\tikzfading[name=fadeout, inner color=transparent!0,outer color=transparent!100]
\tikzfading[name=fadein, inner color=transparent!100,outer color=transparent!0]
\node[anchor=south west, rotate=0, opacity=1] at ($(current page.south west)+(0.225\paperwidth, 9)$) {
\includegraphics[width=\paperwidth, keepaspectratio]{assets/images/covers/cover-image-transparent-vol1.png}};

% Title
\newcommand{\titlelocationleft}{0.075\paperwidth}
\newcommand{\titlelocationbottom}{0.4\paperwidth}
\newcommand{\titlealign}{left}

\begin{scope}{%
\fontsize{52}{62.4}\selectfont
\node[anchor=north
west, align=left, rotate=0] (Title1) at ($(current page.south west)+(\titlelocationleft,\titlelocationbottom)$)  [text width = 0.9\paperwidth]  {{\nohyphens{Machine
Learning Systems}}};
}
\end{scope}

% Author
\newcommand{\authorlocationleft}{.925\paperwidth}
\newcommand{\authorlocationbottom}{0.150\paperwidth}
\newcommand{\authoralign}{right}

\begin{scope}
{%
\fontsize{20}{24.0}\selectfont
\node[anchor=north
east, align=right, rotate=0] (Author1) at ($(current page.south west)+(\authorlocationleft,\authorlocationbottom)$)  [text width = 6in]  {\coverauthorstyle{Vijay\\Janapa
Reddi\\}};
}
\end{scope}

% Footer
\newcommand{\footerlocationleft}{0.075\paperwidth}
\newcommand{\footerlocationbottom}{0.475\paperwidth}
\newcommand{\footerlocationalign}{left}

\begin{scope}
{%
\fontsize{25}{30.0}\selectfont
 \node[anchor=north west, align=left, rotate=0] (Footer1) at %
($(current page.south west)+(\footerlocationleft,\footerlocationbottom)$)  [text width = 0.9\paperwidth]  {{\nohyphens{Volume
I: Introduction}}};
}
\end{scope}

\end{tikzpicture}
\clearpage
\restoregeometry
%%% TITLE PAGE START

% Set up alignment commands
%Page
\newcommand{\titlepagepagealign}{
\ifthenelse{\equal{left}{right}}{\raggedleft}{}
\ifthenelse{\equal{left}{center}}{\centering}{}
\ifthenelse{\equal{left}{left}}{\raggedright}{}
}


\newcommand{\titleandsubtitle}{
% Title and subtitle
{{\huge{\bfseries{\nohyphens{Machine Learning Systems}}}}\par
}%

\vspace{\betweentitlesubtitle}
{
{\large{\textit{\nohyphens{Volume I: Introduction}}}}\par
}}
\newcommand{\titlepagetitleblock}{
\titleandsubtitle
}

\newcommand{\authorstyle}[1]{{\large{#1}}}

\newcommand{\affiliationstyle}[1]{{\large{#1}}}

\newcommand{\titlepageauthorblock}{
{\authorstyle{\nohyphens{Vijay Janapa
Reddi}{\textsuperscript{1}}\textsuperscript{,}{\textsuperscript{,*}}}}}

\newcommand{\titlepageaffiliationblock}{
\hangindent=1em
\hangafter=1
{\affiliationstyle{
{1}.~Harvard University


\vspace{1\baselineskip}
* \textit{Correspondence:}~Vijay Janapa Reddi~vj@eecs.harvard.edu
}}
}
\newcommand{\headerstyled}{%
{}
}
\newcommand{\footerstyled}{%
{\large{}}
}
\newcommand{\datestyled}{%
{January 28, 2026}
}


\newcommand{\titlepageheaderblock}{\headerstyled}

\newcommand{\titlepagefooterblock}{
\footerstyled
}

\newcommand{\titlepagedateblock}{
\datestyled
}

%set up blocks so user can specify order
\newcommand{\titleblock}{\newlength{\betweentitlesubtitle}
\setlength{\betweentitlesubtitle}{0.05\textheight}
{

{\titlepagetitleblock}
}

\vspace{4\baselineskip}
}

\newcommand{\authorblock}{{\titlepageauthorblock}

\vspace{2\baselineskip}
}

\newcommand{\affiliationblock}{{\titlepageaffiliationblock}

\vspace{0pt}
}

\newcommand{\logoblock}{}

\newcommand{\footerblock}{}

\newcommand{\dateblock}{{\titlepagedateblock}

\vspace{0pt}
}

\newcommand{\headerblock}{}

\thispagestyle{empty} % no page numbers on titlepages


\newcommand{\vrulecode}{\textcolor{black}{\rule{\vrulewidth}{\textheight}}}
\newlength{\vrulewidth}
\setlength{\vrulewidth}{2pt}
\newlength{\B}
\setlength{\B}{\ifdim\vrulewidth > 0pt 0.05\textwidth\else 0pt\fi}
\newlength{\minipagewidth}
\ifthenelse{\equal{left}{left} \OR \equal{left}{right} }
{% True case
\setlength{\minipagewidth}{\textwidth - \vrulewidth - \B - 0.1\textwidth}
}{
\setlength{\minipagewidth}{\textwidth - 2\vrulewidth - 2\B - 0.1\textwidth}
}
\ifthenelse{\equal{left}{left} \OR \equal{left}{leftright}}
{% True case
\raggedleft % needed for the minipage to work
\vrulecode
\hspace{\B}
}{%
\raggedright % else it is right only and width is not 0
}
% [position of box][box height][inner position]{width}
% [s] means stretch out vertically; assuming there is a vfill
\begin{minipage}[b][\textheight][s]{\minipagewidth}
\titlepagepagealign
\titleblock

Prof.~Vijay Janapa Reddi

School of Engineering and Applied Sciences

Harvard University

\vspace{80mm}

With heartfelt gratitude to the community for their invaluable
contributions and steadfast support.

\vfill

January 28, 2026

\vfill
\par

\end{minipage}\ifthenelse{\equal{left}{right} \OR \equal{left}{leftright} }{
\hspace{\B}
\vrulecode}{}
\clearpage
%%% TITLE PAGE END
\end{titlepage}
\setcounter{page}{1}
\end{frontmatter}

%%%%% end titlepage extension code

\renewcommand*\contentsname{Table of contents}
{
\hypersetup{linkcolor=}
\setcounter{tocdepth}{2}
\tableofcontents
}

\mainmatter
\bookmarksetup{startatroot}

\chapter*{Welcome to Volume I}\label{welcome-to-volume-i}
\addcontentsline{toc}{chapter}{Welcome to Volume I}

\markboth{Welcome to Volume I}{Welcome to Volume I}

\section*{What You Will Learn}\label{what-you-will-learn}
\addcontentsline{toc}{section}{What You Will Learn}

\markright{What You Will Learn}

Volume I progresses through four stages:

\begin{itemize}
\tightlist
\item
  \textbf{Part I: Foundations} --- Build your conceptual foundation with
  mental models that underpin all effective systems work.
\item
  \textbf{Part II: Build} --- Engineer complete workflows from data
  pipelines through training infrastructure.
\item
  \textbf{Part III: Optimize} --- Transform theoretical understanding
  into systems that run efficiently in resource-constrained
  environments.
\item
  \textbf{Part IV: Deploy} --- Navigate serving, operations, and
  responsible engineering practices.
\end{itemize}

\section*{Prerequisites}\label{prerequisites}
\addcontentsline{toc}{section}{Prerequisites}

\markright{Prerequisites}

This volume assumes:

\begin{itemize}
\tightlist
\item
  \textbf{Programming proficiency} in Python with familiarity in NumPy
\item
  \textbf{Mathematics foundations} in linear algebra, calculus, and
  probability at the undergraduate level
\item
  Prior ML experience is helpful but not required;
  \textbf{?@sec-deep-learning-systems-foundations} provides essential
  background
\end{itemize}

\section*{Support Our Mission}\label{support-our-mission}
\addcontentsline{toc}{section}{Support Our Mission}

\markright{Support Our Mission}

\section*{Continue Your Journey}\label{continue-your-journey}
\addcontentsline{toc}{section}{Continue Your Journey}

\markright{Continue Your Journey}

\section*{Listen to the AI Podcast}\label{listen-to-the-ai-podcast}
\addcontentsline{toc}{section}{Listen to the AI Podcast}

\markright{Listen to the AI Podcast}

\section*{Want to Help Out?}\label{want-to-help-out}
\addcontentsline{toc}{section}{Want to Help Out?}

\markright{Want to Help Out?}

This is a collaborative project, and your input matters. If you'd like
to contribute, check out our
\href{https://github.com/harvard-edge/cs249r_book/blob/dev/docs/contribute.md}{contribution
guidelines}. Feedback, corrections, and new ideas are welcome. Simply
file a GitHub
\href{https://github.com/harvard-edge/cs249r_book/issues}{issue}.

\bookmarksetup{startatroot}

\chapter{The AI Development Workflow}\label{sec-ai-development-workflow}

\marginnote{\begin{footnotesize}

\emph{DALL·E 3 Prompt: Create a rectangular illustration of a stylized
flowchart representing the AI workflow/pipeline. From left to right,
depict the stages as follows: `Data Collection' with a database icon,
`Data Preprocessing' with a filter icon, `Model Design' with a brain
icon, `Training' with a weight icon, `Evaluation' with a checkmark, and
`Deployment' with a rocket. Connect each stage with arrows to guide the
viewer horizontally through the AI processes, emphasizing these steps'
sequential and interconnected nature.}

\end{footnotesize}}

\noindent
\pandocbounded{\includegraphics[keepaspectratio]{contents/vol1/workflow/images/png/cover_ai_workflow.png}}

\section*{Purpose}\label{purpose}
\addcontentsline{toc}{section}{Purpose}

\markright{Purpose}

\emph{Why do most machine learning projects fail to reach production
despite achieving strong experimental results?}

The gap between a working prototype and a deployed system is where most
ML projects die. A model achieving 95\% accuracy in a notebook means
nothing if the deployment target has half the required memory, or if the
training data distribution diverges from production reality.

Traditional software fails from bugs in code; ML projects fail from
\textbf{misalignment between stages}. Data assumptions are violated by
model choices. Model requirements prove incompatible with infrastructure
constraints. Production behaviors diverge from training conditions.

This structural fragility explains why the majority of ML initiatives
never escape the prototype phase: not because the algorithms failed, but
because no systematic process propagated constraints forward from
deployment requirements or backward from experimental discoveries. The
\textbf{Workflow} is the connective tissue that binds problem definition
to production operation. Its absence leaves ML projects as isolated
experiments that cannot survive contact with the real world.

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-tip-color-frame, colback=white, titlerule=0mm, bottomrule=.15mm, toprule=.15mm, opacitybacktitle=0.6, arc=.35mm, breakable, colbacktitle=quarto-callout-tip-color!10!white, bottomtitle=1mm, rightrule=.15mm, toptitle=1mm, opacityback=0, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Learning Objectives}, coltitle=black, leftrule=.75mm, left=2mm]

\begin{itemize}
\item
  Compare ML lifecycle workflows to traditional software development and
  explain fundamental differences
\item
  Analyze the six core ML lifecycle stages and their feedback-driven
  relationships
\item
  Apply systems thinking principles to trace constraint propagation
  across lifecycle stages
\item
  Evaluate trade-offs between model performance, deployment constraints,
  and resource limitations
\item
  Design data collection and validation strategies that anticipate
  deployment environment requirements
\item
  Assess how problem definition and architectural decisions propagate
  through the complete ML lifecycle
\end{itemize}

\end{tcolorbox}

\section{Systematic Framework for ML
Development}\label{sec-ai-development-workflow-systematic-framework-ml-development-7a1b}

\textbf{?@sec-introduction} established the \textbf{AI Triad} --- Data,
Algorithm, and Machine (the \textbf{DAM}) --- as the three pillars of
machine learning systems. \textbf{?@sec-ml-system-architecture} revealed
the \textbf{Physical Constraints} that govern where and how ML systems
can operate, introducing four deployment paradigms (Cloud, Edge, Mobile,
TinyML) and four Workload Archetypes that characterize different
computational patterns. This chapter introduces the \textbf{Workflow}
--- the engineering control system that orchestrates these components
across the complete development lifecycle.

The transition from understanding \emph{components} to building
\emph{systems} is the transition from \textbf{Model Researcher} to
\textbf{Systems Engineer}. A researcher optimizes individual elements: a
better architecture, a cleaner dataset, a faster accelerator. A systems
engineer orchestrates these elements into production systems that
reliably deliver value. The workflow framework provides this
orchestration logic.

Consider what happens without systematic workflow. A team builds a model
achieving 95\% accuracy on research data. They optimize it for a week,
pushing accuracy to 96\%. They hand it to deployment engineers who
discover it requires 4 GB of memory --- impossible on the target mobile
device. Three months of work is discarded. This failure was not
algorithmic; it was workflow failure. The deployment constraint should
have propagated backward to the first design meeting, not forward to the
final deployment attempt. The workflow framework prevents such failures
by making constraints explicit at each stage and propagating them
throughout development.

The deployment paradigm selected during problem definition --- Cloud,
Edge, Mobile, or TinyML (\textbf{?@sec-ml-system-architecture}) ---
fundamentally shapes every subsequent workflow stage. This is not a
late-stage implementation detail but a first-order constraint that
determines what you can build.

Recall that \textbf{?@sec-ml-system-architecture} introduced four
Workload Archetypes characterizing distinct computational patterns:
Compute Beasts are compute-bound workloads like training ResNet,
Bandwidth Hogs are memory-bound workloads like LLM inference, Sparse
Scatter workloads involve irregular memory access patterns like
recommendation systems, and Tiny Constraints operate within extreme
resource limits like microcontrollers. Each archetype imposes distinct
workflow constraints:

\begin{itemize}
\tightlist
\item
  \textbf{Archetype I (Compute Beast)}: Cloud deployment permits large
  model architectures (ResNet-50, BERT) but demands latency-aware
  serving infrastructure and distributed training pipelines. Workflow
  emphasizes training efficiency and serving scalability.
\item
  \textbf{Archetype II (Bandwidth Hog)}: LLM deployments face memory
  bandwidth constraints that shape data preparation (KV-cache
  optimization), model development (quantization strategies), and
  monitoring (token throughput metrics). Workflow emphasizes memory
  optimization throughout.
\item
  \textbf{Archetype III (Sparse Scatter)}: Recommendation systems
  require workflow stages optimized for TB-scale embedding tables, with
  data collection emphasizing user interaction patterns and deployment
  focusing on caching strategies. Workflow emphasizes data
  infrastructure and latency.
\item
  \textbf{Archetype IV (Tiny Constraint)}: TinyML deployment imposes the
  \textless256 KB memory and \textless1 mW power envelope from problem
  definition through monitoring. Workflow emphasizes aggressive
  optimization at every stage.
\end{itemize}

These paradigm-specific constraints propagate backward through the
workflow, determining data collection strategies (what preprocessing can
occur on-device?), model development choices (what architectures fit the
memory budget?), and monitoring approaches (can the device report
metrics without connectivity?). The workflow framework presented here
applies across all paradigms, but the specific constraints at each stage
vary dramatically based on where the system will ultimately run.

ML systems evolve through \textbf{iterative experimentation} rather than
linear implementation. Teams hypothesize model architecture choices,
experiment through training and validation, analyze performance metrics,
and iterate based on findings. This scientific methodology emerged from
academic research labs in the 1990s-2000s but became essential for
production ML when Google, Facebook, and others discovered that
empirical validation outperformed theoretical predictions in complex,
real-world systems. In Software 2.0\sidenote{\textbf{Software 2.0}: A
paradigm articulated by Andrej Karpathy in 2017
(\citeproc{ref-karpathy2017software}{Karpathy 2017}) describing how
neural networks represent a shift from explicit programming to learned
behavior. In Software 1.0, engineers write explicit rules; in Software
2.0, engineers curate datasets and neural networks learn the rules. This
reframes the engineering challenge: debugging means examining training
data, optimization means data augmentation, and version control must
track datasets alongside code. The paradigm explains why data
engineering consumes 60-80\% of ML project effort and why workflow
systems must treat data as a first-class artifact. }, where \textbf{Data
is Source Code}, the workflow is the ``Compiler'' that translates raw
observations into operational logic. This empirical, data-centric
approach requires specialized workflow methodologies that accommodate
uncertainty, coordinate parallel development streams, and establish
continuous improvement mechanisms.

The systematic framework presented here provides the theoretical
foundation for understanding Part II's design principles. This workflow
perspective clarifies the rationale for specialized data engineering
pipelines (\textbf{?@sec-data-engineering-ml}), the role of software
frameworks in enabling iterative methodologies
(\textbf{?@sec-ai-frameworks}), and the integration of model training
within comprehensive system lifecycles (\textbf{?@sec-ai-training}).
Throughout this chapter, we use \textbf{diabetic retinopathy screening
system development} as a pedagogical case study, demonstrating how
workflow principles bridge laboratory research and clinical deployment.
This example illustrates the interdependencies among data acquisition
strategies, architectural design decisions, deployment constraint
management, and operational requirements that characterize
production-scale ML systems.

\phantomsection\label{quiz-question-sec-ai-development-workflow-systematic-framework-ml-development-7a1b}
\begin{fbx}{callout-quiz-question}{Self-Check: Question 1.1}{}
\phantomsection\label{quiz-question-sec-ai-development-workflow-systematic-framework-ml-development-7a1b}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  How does the machine learning workflow differ from traditional
  software engineering processes?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii})}
  \tightlist
  \item
    ML workflow is iterative and data-centric, involving experimentation
    and empirical validation.
  \item
    ML workflow is deterministic and follows a strict
    requirement-to-implementation path.
  \item
    ML workflow does not involve any feedback mechanisms.
  \item
    ML workflow is identical to traditional software engineering.
  \end{enumerate}
\item
  Why is iterative experimentation crucial in the development of machine
  learning systems?
\item
  What role do feedback mechanisms play in the ML system development
  workflow?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii})}
  \tightlist
  \item
    They are unnecessary as ML systems are static once deployed.
  \item
    They are used to finalize the initial model without further changes.
  \item
    They only apply to traditional software engineering.
  \item
    They inform earlier development phases and help refine models.
  \end{enumerate}
\item
  How does the diabetic retinopathy screening system case study
  illustrate the iterative workflow principles and data-driven decision
  making discussed in this section?
\end{enumerate}

\noindent\hspace*{1.25em}\hyperref[quiz-answer-sec-ai-development-workflow-systematic-framework-ml-development-7a1b]{\textbf{See Answer~$\rightarrow$}}

\end{fbx}

\section{Understanding the ML
Lifecycle}\label{sec-ai-development-workflow-understanding-ml-lifecycle-ca87}

The machine learning lifecycle is a structured, iterative process that
guides the development, evaluation, and improvement of machine learning
systems. This approach integrates systematic experimentation,
evaluation, and adaptation over time
(\citeproc{ref-amershi2019software}{Amershi et al. 2019}), building upon
decades of structured development approaches
(\citeproc{ref-chapman2000crisp}{Chapman et al.
2000})\sidenote{\textbf{CRISP-DM (Cross-Industry Standard Process for
Data Mining)}: A methodology developed in 1996-1997 by a consortium
including Integral Solutions Ltd (later acquired by SPSS), Daimler-Benz,
NCR, and OHRA as an EU ESPRIT project to provide a standard framework
for data mining projects. CRISP-DM defined six phases: Business
Understanding, Data Understanding, Data Preparation, Modeling,
Evaluation, and Deployment. While predating modern ML, CRISP-DM
established the iterative, data-centric workflow principles that evolved
into today's MLOps practices, becoming the dominant methodology for data
mining projects and serving as the foundation for ML lifecycle
frameworks like Team Data Science Process (TDSP) and KDD. } while
addressing the unique challenges of data-driven systems.

Understanding this lifecycle requires a systems
thinking\sidenote{\textbf{Systems Thinking}: A holistic approach to
analysis that focuses on the ways that a system's constituent parts
interrelate and how systems work over time and within larger systems.
Developed by MIT's Jay Forrester in the 1950s for industrial dynamics,
systems thinking became crucial for ML engineering because models, data,
infrastructure, and operations interact in complex ways that produce
emergent behaviors. Unlike traditional software where components can be
optimized independently, ML systems require understanding
interdependencies such as how data quality affects model performance,
how model complexity influences deployment constraints, and how
monitoring insights drive system evolution. } approach that recognizes
four fundamental patterns. Constraint propagation describes how
decisions in one stage influence all others. Multi-scale feedback loops
capture how systems adapt across different timescales. Emergent
complexity explains how system-wide behaviors differ from component
behaviors. Resource optimization reveals how trade-offs create
interdependencies. These patterns, which we explore throughout this
chapter using diabetic retinopathy screening as a case study, provide
the analytical framework for understanding why ML systems require
integrated engineering approaches rather than sequential component
optimization.

\phantomsection\label{callout-definitionux2a-1.1}
\begin{fbx}{callout-definition}{Definition: }{Machine Learning Lifecycle}
\phantomsection\label{callout-definition*-1.1}
\textbf{Machine Learning Lifecycle} refers to the iterative process of
\emph{developing}, \emph{deploying}, and \emph{refining} ML systems
through feedback-driven stages, emphasizing \emph{continuous
improvement} in response to evolving data and requirements.

\end{fbx}

Figure~\ref{fig-ml-lifecycle} visualizes two parallel
pipelines\sidenote{\textbf{Pipeline}: Borrowed from the oil industry,
where pipelines transported crude oil from wells to refineries starting
in the 1860s. The computing metaphor emerged in the 1960s at IBM to
describe data flowing through connected processing stages, just as oil
flows through physical pipes. In ML, the metaphor extends naturally: raw
data enters one end, flows through transformation stages, and emerges as
trained models or predictions. The term captures the key insight that ML
development requires continuous flow rather than discrete steps. } that
characterize the complete lifecycle. The data pipeline (green, top row)
transforms raw inputs through collection, ingestion, analysis, labeling,
validation, and preparation into ML-ready datasets. The model
development pipeline (blue, bottom row) takes these datasets through
training, evaluation, validation, and deployment to create production
systems. Their interconnections reveal the distinctive character of ML
development. The curved feedback arrows show how deployment insights
trigger data refinements, creating continuous improvement cycles that
distinguish ML from traditional linear development.

\begin{figure}[htb]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/3b8b6955147e668ebf7804bbf69175635b49ab23.pdf}}

}

\caption{\label{fig-ml-lifecycle}\textbf{ML Lifecycle Stages}: Two
parallel pipelines characterize production ML development. The data
pipeline (green, top) progresses from raw collection through ingestion,
analysis, labeling, validation, and preparation. The model pipeline
(blue, bottom) takes prepared datasets through training, evaluation,
validation, and deployment. The prominent feedback arrows emphasize what
distinguishes ML from traditional software: monitoring insights
continuously inform data refinements, evaluation results trigger model
improvements, and deployment experiences reshape collection strategies.
These bidirectional flows explain why ML projects average 4-8 iteration
cycles before production readiness.}

\end{figure}%

This workflow framework serves as scaffolding for the technical chapters
ahead. \textbf{?@sec-data-engineering-ml} provides comprehensive
treatment of the data pipeline, addressing how to ensure data quality
and manage data throughout the ML lifecycle. \textbf{?@sec-ai-training}
expands on model training, covering how to efficiently train models at
scale. \textbf{?@sec-ai-frameworks} details the software frameworks that
enable this iterative development process.
\textbf{?@sec-machine-learning-operations-mlops} extends into deployment
and ongoing operations, addressing how systems maintain performance in
production. This chapter establishes how these pieces interconnect
before we explore each in depth.

The conceptual stages of the ML lifecycle establish the ``what'' and
``why'' of the development process. The operational implementation of
this lifecycle through automation, tooling, and infrastructure
constitutes the ``how'' --- the domain of MLOps.
\textbf{?@sec-machine-learning-operations-mlops} explores these
operational practices in detail. This distinction matters: the lifecycle
provides the systematic framework for understanding ML development
stages, while MLOps provides the operational practices for implementing
these stages at scale.

\subsection{Quantifying the ML
Lifecycle}\label{sec-ai-development-workflow-quantifying-ml-lifecycle-bd69}

Understanding the lifecycle conceptually is necessary but insufficient
for engineering decisions. Quantitative characterization reveals where
effort and compute actually go in ML projects, exposing which stages
bottleneck development and where optimization investments yield the
highest returns.

\textbf{Time allocation across stages} follows a consistent pattern
across industries. Data-related activities---collection, cleaning,
labeling, validation, and preparation---consume 60-80\% of total project
time (\citeproc{ref-crowdflower2016data}{CrowdFlower 2016}). Model
development and training, despite receiving the most research attention,
typically represents only 10-20\% of effort. The remaining 10-20\% goes
to deployment, integration, and initial monitoring setup. This
distribution surprises teams accustomed to traditional software where
implementation dominates. In ML projects, the ``source code'' is the
data, and preparing that source code is the primary engineering
activity.

\textbf{Iteration cycles} characterize successful ML projects.
Figure~\ref{fig-ml-lifecycle} shows the feedback loops that drive these
iterations. Production-ready ML systems typically require 4-8 complete
iteration cycles, where each cycle may revisit multiple stages. The
distribution of iteration causes reveals where to invest in quality:

\begin{itemize}
\tightlist
\item
  \textbf{Data quality issues} drive approximately 60\% of iterations
  (missing labels, distribution mismatch, preprocessing errors)
\item
  \textbf{Architecture and training choices} drive approximately 25\% of
  iterations (model capacity, hyperparameters, training instability)
\item
  \textbf{Infrastructure and deployment issues} drive approximately 15\%
  of iterations (latency violations, resource constraints, integration
  failures)
\end{itemize}

These proportions explain why data engineering capabilities often
determine project success more than modeling sophistication.

\textbf{Cost of late discovery} follows an exponential pattern that we
formalize as the \textbf{Constraint Propagation Principle} in
Section~\ref{sec-ai-development-workflow-integrating-systems-thinking-principles-24c0}.
A constraint violation discovered at stage \(N\) costs roughly
\(2^{N-1}\) times more to fix than if discovered at stage 1. A
deployment paradigm mismatch discovered during deployment (stage 5) that
should have been identified during problem definition (stage 1) requires
revisiting data collection (incompatible preprocessing), model
development (architecture doesn't fit constraints), and evaluation (need
device-specific testing)---a 4-stage cascade costing approximately 16×
the original problem definition effort. This exponential cost structure
motivates the stage interface contracts in
Table~\ref{tbl-stage-interface}: validating outputs at each stage
transition catches violations early when correction costs remain
manageable.

\phantomsection\label{callout-perspectiveux2a-1.2}
\begin{fbx}{callout-perspective}{Systems Perspective: }{Napkin Math: The Iteration Tax}
\phantomsection\label{callout-perspective*-1.2}
\textbf{Problem}: You are choosing between a large model (training time:
1 week, accuracy: 95\%) and a small model (training time: 1 hour,
accuracy: 90\%). Which one yields a better system in 6 months?

\textbf{The Math}: In 6 months (\textasciitilde26 weeks), you can run:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Large Model}: 26 experiments at 1 week each. Each experiment
  improves accuracy by \textasciitilde0.5\% (diminishing returns).
\item
  \textbf{Small Model}: 26 × 168 = 4,368 experiments at 1 hour each.
  Even with smaller gains per iteration, the compound effect is
  substantial.
\end{enumerate}

\textbf{The Systems Insight}: If each iteration improves accuracy by
0.1\% on average, the small model reaches:
\(90\% + (100 \times 0.1\%) = 100\%\) theoretical ceiling. The large
model reaches: \(95\% + (26 \times 0.5\%) = 108\%\) (capped at ceiling).
In practice, the small model's rapid iteration enables discovering
better architectures, data augmentations, and hyperparameters. This
``iteration tax'' explains why startups with fast iteration often
outperform larger teams with slower cycles.

\end{fbx}

\phantomsection\label{quiz-question-sec-ai-development-workflow-understanding-ml-lifecycle-ca87}
\begin{fbx}{callout-quiz-question}{Self-Check: Question 1.2}{}
\phantomsection\label{quiz-question-sec-ai-development-workflow-understanding-ml-lifecycle-ca87}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Which of the following best describes the role of feedback loops in
  the ML lifecycle?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii})}
  \tightlist
  \item
    They ensure that each stage of the lifecycle is completed before
    moving to the next.
  \item
    They are used to validate the final model before deployment.
  \item
    They allow for continuous improvement by informing earlier stages
    with insights from later stages.
  \item
    They help in maintaining a linear development process.
  \end{enumerate}
\item
  Explain how systems thinking applies to the machine learning lifecycle
  and why it is important.
\item
  Order the following stages of the ML lifecycle from data collection to
  deployment: (1) Model Training, (2) Data Preparation, (3) Model
  Evaluation, (4) Data Collection, (5) ML System Deployment.
\item
  True or False: The ML lifecycle is a linear process where each stage
  is independent of the others.
\end{enumerate}

\noindent\hspace*{1.25em}\hyperref[quiz-answer-sec-ai-development-workflow-understanding-ml-lifecycle-ca87]{\textbf{See Answer~$\rightarrow$}}

\end{fbx}

\section{ML vs Traditional Software
Development}\label{sec-ai-development-workflow-ml-vs-traditional-software-development-c5f5}

These quantitative realities---60-80\% data allocation, 4-8 iteration
cycles, and \(2^{N-1}\) cost escalation---represent fundamental
departures from the assumptions underlying traditional software
engineering. These numbers are foreign to waterfall or even standard
agile processes, which is why ML requires specialized lifecycle
approaches.

Traditional lifecycles consist of sequential phases: requirements
gathering, system design, implementation, testing, and deployment
(\citeproc{ref-royce1970managing}{Royce
1970})\sidenote{\textbf{Waterfall Model}: A sequential software
development methodology described by Winston Royce in his 1970 paper,
where development flows through distinct phases (requirements → design →
implementation → testing → deployment) like water flowing down stairs.
Notably, Royce presented this model as flawed and advocated for
iterative approaches; the term ``waterfall'' was later coined by Bell
and Thayer in 1976. Despite Royce's criticisms, this linear
interpretation dominated enterprise software development for decades and
still suits projects with stable, well-understood requirements. The
model's rigid approach contrasts starkly with ML development's inherent
uncertainty and need for experimentation. }. Each phase produces
specific artifacts that serve as inputs to subsequent phases. In
financial software development, the requirements phase produces detailed
specifications for transaction processing, security protocols, and
regulatory compliance. These specifications translate directly into
system behavior through explicit programming. This deterministic
approach contrasts sharply with the probabilistic nature of ML systems
that \textbf{?@sec-introduction} introduced.

Machine learning systems require a fundamentally different approach. The
deterministic nature of conventional software, where behavior is
explicitly programmed, contrasts with the probabilistic nature of ML
systems. Consider financial transaction processing: traditional systems
follow predetermined rules (if account balance \textgreater{}
transaction amount, then allow transaction), while ML-based fraud
detection systems\sidenote{\textbf{ML-Based Fraud Detection Evolution}:
Compared to rule-based systems, ML-based fraud detection can reduce
false positives and improve detection by leveraging richer behavioral
features (\citeproc{ref-stripe2019machine}{Stripe Engineering 2019}).
However, deployed systems face evolving adversaries, requiring ongoing
monitoring and periodic model updates rather than one-time rule
definition. } learn to recognize suspicious patterns from historical
transaction data. This shift from explicit programming to learned
behavior reshapes the development lifecycle, altering how we approach
system reliability and robustness.

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-note-color-frame, colback=white, titlerule=0mm, bottomrule=.15mm, toprule=.15mm, opacitybacktitle=0.6, arc=.35mm, breakable, colbacktitle=quarto-callout-note-color!10!white, bottomtitle=1mm, rightrule=.15mm, toptitle=1mm, opacityback=0, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Representative snapshot (fraud detection metrics, as of 2019)}, coltitle=black, leftrule=.75mm, left=2mm]

Reported case studies commonly cite higher accuracy and lower false
positive rates for ML-based approaches than for rule-based systems, but
exact values vary substantially by domain, population shift, fraud
strategy, and decision thresholds
(\citeproc{ref-stripe2019machine}{Stripe Engineering 2019}).

\end{tcolorbox}

These fundamental differences in system behavior introduce new dynamics
that alter how lifecycle stages interact. These systems require ongoing
refinement through continuous feedback loops that enable insights from
deployment to inform earlier development phases. Machine learning
systems are inherently dynamic and must adapt to changing data
distributions and objectives through continuous
deployment\sidenote{\textbf{Continuous Deployment}: Software engineering
practice where code changes are automatically deployed to production
after passing automated tests, enabling multiple deployments per day
instead of monthly releases. Popularized by companies like Netflix
(2008) and Etsy (2009), continuous deployment reduces deployment risk
through small, frequent changes rather than large, infrequent releases.
However, ML systems require specialized continuous deployment because
models need statistical validation, gradual rollouts with A/B testing,
and rollback mechanisms based on performance metrics rather than just
functional correctness. } practices.

Table~\ref{tbl-sw-ml-cycles} contrasts these differences across six
development dimensions, from problem definition through maintenance.
These differences reflect the core challenge of working with data as a
first-class citizen in system design, something traditional software
engineering methodologies were not designed to
handle\sidenote{\textbf{Data Versioning Challenges}: Unlike code, which
changes through discrete edits, data can change gradually through drift,
suddenly through schema changes, or subtly through quality degradation.
Traditional version control systems like Git struggle with large
datasets, leading to specialized tools like Git LFS and DVC. }.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2261}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3478}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4174}}@{}}
\caption{\textbf{Traditional Software vs ML Development Lifecycles}: Six
dimensions where ML development diverges fundamentally from traditional
software engineering. The most critical difference appears in the final
row: while traditional software rarely sees later stages influence
earlier phases, ML systems require continuous feedback loops where
deployment insights reshape data collection, monitoring drives model
updates, and production experiences inform architectural decisions.
These differences explain why traditional project management approaches
fail when applied to ML projects without
modification.}\label{tbl-sw-ml-cycles}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Aspect}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Traditional Software Lifecycles}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Machine Learning Lifecycles}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Aspect}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Traditional Software Lifecycles}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Machine Learning Lifecycles}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Problem Definition} & Precise functional specifications are
defined upfront. & Performance-driven objectives evolve as the problem
space is explored. \\
\textbf{Development Process} & Linear progression of feature
implementation. & Iterative experimentation with data, features and
models. \\
\textbf{Testing and} & Deterministic, binary pass/fail & Statistical
validation and metrics that \\
\textbf{Validation} & testing criteria. & involve uncertainty. \\
\textbf{Deployment} & Behavior remains static until explicitly updated.
& Performance may change over time due to shifts in data
distributions. \\
\textbf{Maintenance} & Maintenance involves modifying code to address
bugs or add features. & Continuous monitoring, updating data pipelines,
retraining models, and adapting to new data distributions. \\
\textbf{Feedback Loops} & Minimal; later stages rarely impact earlier
phases. & Frequent; insights from deployment and monitoring often refine
earlier stages like data preparation and model design. \\
\end{longtable}

This shift is most visible in experimentation. In traditional software,
testing verifies code behavior against predetermined specifications---a
quality assurance step. In ML, experimentation \emph{is} the core
development process: systematically testing hypotheses about data
sources, feature engineering approaches, model architectures, and
hyperparameters. This scientific process of discovery explains why ML
projects require 4-8 iteration cycles where traditional software might
require 1-2.

These distinctions are not merely academic observations. They translate
directly into the structured six-stage framework that organizes how ML
projects unfold, each stage presenting unique challenges that
traditional software methodologies cannot adequately address.

\phantomsection\label{quiz-question-sec-ai-development-workflow-ml-vs-traditional-software-development-c5f5}
\begin{fbx}{callout-quiz-question}{Self-Check: Question 1.3}{}
\phantomsection\label{quiz-question-sec-ai-development-workflow-ml-vs-traditional-software-development-c5f5}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Which of the following best describes a key difference between
  traditional software development and machine learning development?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii})}
  \tightlist
  \item
    Traditional software development follows a linear progression with
    predefined specifications, whereas ML development involves iterative
    experimentation and evolving objectives.
  \item
    ML development relies on deterministic specifications, while
    traditional development is probabilistic.
  \item
    Traditional software development is iterative, while ML development
    is linear.
  \item
    ML development does not require feedback loops, unlike traditional
    software development.
  \end{enumerate}
\item
  Explain why continuous feedback loops are crucial in the machine
  learning development lifecycle.
\item
  Order the following dimensions of development lifecycle differences
  between traditional software and ML systems: (1) Deployment, (2)
  Testing and Validation, (3) Feedback Loops.
\end{enumerate}

\noindent\hspace*{1.25em}\hyperref[quiz-answer-sec-ai-development-workflow-ml-vs-traditional-software-development-c5f5]{\textbf{See Answer~$\rightarrow$}}

\end{fbx}

\section{Six Core Lifecycle
Stages}\label{sec-ai-development-workflow-six-core-lifecycle-stages-00b0}

Where traditional software follows requirements through implementation
to testing, ML systems require a fundamentally different organization
that accommodates iterative experimentation, data-driven evolution, and
continuous feedback. This section presents the six-stage framework that
captures these differences.

Figure~\ref{fig-lifecycle-overview} presents a simplified view of the
six stages that structure this approach. Problem Definition establishes
objectives and constraints. Data Collection and Preparation encompasses
the data pipeline. Model Development and Training creates models.
Evaluation and Validation ensures quality. Deployment and Integration
brings systems to production. Monitoring and Maintenance ensures
continued effectiveness. The prominent feedback loop emphasizes that
insights from later stages inform earlier phases, capturing the cyclical
nature that distinguishes ML from linear software development.

To make these stages concrete, consider how they apply to MobileNetV2
(\textbf{?@sec-dnn-architectures}), one of our Lighthouse Examples
targeting mobile deployment. Problem Definition establishes the
constraint: \textless14 MB model size, \textless300 MFLOPs, real-time
inference on mobile GPUs. Data Collection must account for on-device
preprocessing limitations. Model Development uses depthwise separable
convolutions specifically designed to meet the FLOP budget. Evaluation
validates not just accuracy but latency on target devices. Deployment
targets mobile NPUs with quantization. Monitoring tracks performance
across diverse device populations. Each stage's decisions propagate
through subsequent stages---the workflow framework makes these
dependencies explicit.

\begin{figure}[htb]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/7e961cf6b744f6d88a6f4f27d417a435a02f1e11.pdf}}

}

\caption{\label{fig-lifecycle-overview}\textbf{Six-Stage ML Lifecycle
Framework}: A simplified view of ML system development emphasizing
sequential progression from problem definition through monitoring. The
prominent feedback loop from monitoring back to data collection captures
the essential insight: production insights drive continuous refinement
across all earlier stages. Unlike traditional software where later
phases rarely influence earlier ones, ML systems require this
bidirectional flow because data distributions shift, model performance
drifts, and operational requirements evolve. Teams that design for this
feedback from the start report 2-3x faster iteration cycles than those
retrofitting feedback mechanisms.}

\end{figure}%

The lifecycle diagram above presents a linear narrative, but experienced
practitioners recognize that these stages interconnect closely. Each
stage corresponds to specific terms in the performance
equation---decisions made during data collection constrain what is
achievable during model development, which in turn determines deployment
requirements.

\phantomsection\label{callout-perspectiveux2a-1.3}
\begin{fbx}{callout-perspective}{Systems Perspective: }{The Iron Law of Workflow}
\phantomsection\label{callout-perspective*-1.3}
The six lifecycle stages are not just procedural steps; they are the
engineering levers used to optimize the variables in the \textbf{Iron
Law of ML Systems}
(\(L = \frac{D}{B} + \frac{Ops}{P \cdot \eta} + L_{fixed}\)):

\begin{itemize}
\item
  \textbf{Data Collection \& Preparation}: Primarily determines the
  \textbf{Data (\(D\))} term. High-quality curation reduces the volume
  of data needed to reach a target accuracy.
\item
  \textbf{Model Development \& Training}: Defines the \textbf{Operations
  (\(Ops\))} term. Architectural choices (e.g., Transformers vs.~CNNs)
  set the computational floor.
\item
  \textbf{Evaluation \& Validation}: Measures the \textbf{Efficiency
  (\(\eta\))} achieved on the target hardware.
\item
  \textbf{Deployment \& Integration}: Focuses on minimizing the
  \textbf{Overhead (\(L_{fixed}\))} tax through efficient serving
  infrastructure.
\end{itemize}

Viewed this way, managing the workflow is mathematically equivalent to
minimizing the total system latency and cost.

\end{fbx}

The binding constraint differs dramatically across workload archetypes,
causing each lifecycle stage to optimize different Iron Law terms.
Table~\ref{tbl-lighthouse-workflow-comparison} shows how the same
workflow stages manifest for three Lighthouse Archetypes introduced in
\textbf{?@sec-introduction}:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1181}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2917}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2847}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2917}}@{}}
\caption{Workflow variations by Lighthouse Archetype. The same lifecycle
stages target different Iron Law terms depending on the workload's
binding constraint. ResNet-50 optimizes for Throughput (\(Ops/s\)); DLRM
is bound by Memory Bandwidth (\(D/B\)); TinyML is strictly bound by
Energy (\(J\)) and Memory
Capacity.}\label{tbl-lighthouse-workflow-comparison}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Stage}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{ResNet-50 (Compute Beast)}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{DLRM (Sparse Scatter)}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{TinyML (Constraint-Driven)}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Stage}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{ResNet-50 (Compute Beast)}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{DLRM (Sparse Scatter)}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{TinyML (Constraint-Driven)}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Data Eng} & \emph{Throughput}: Target \textbf{\textgreater{}
80\% GPU} utilization via prefetching and compiled augmentation &
\emph{Latency}: Feature store lookups \textbf{\textless{} 2ms};
embedding tables dominate storage costs & \emph{Capacity}: Curate data
to fit \textbf{256KB} RAM; aggressive filtering over accumulation \\
\textbf{Training} & \emph{Compute Bound}: Maximize MFU (\(\eta\)); mixed
precision to saturate Tensor Cores & \emph{I/O Bound}: Optimize sparse
embedding lookups; memory bandwidth (\(B\)) limits throughput &
\emph{Model Search}: NAS for smallest architecture; quantization-aware
training (QAT) required \\
\textbf{Deploy} & \emph{Batching}: Batch size \textbf{\textgreater{}
128} to maximize throughput; latency secondary to cost & \emph{SLA}:
Strict \textbf{\textless{} 10ms p99} latency; feature freshness
requirements & \emph{Energy}: \textbf{\textless{} 1mW} budget; always-on
inference without battery drain \\
\end{longtable}

The lifecycle begins with problem definition and requirements gathering,
where teams clearly define the problem to be solved, establish
measurable performance objectives, and identify key constraints. Precise
problem definition ensures alignment between the system's goals and the
desired outcomes, setting the foundation for all subsequent work.

Building on this foundation, the next stage assembles the data resources
needed to realize these objectives. Data collection and preparation
includes gathering relevant data, cleaning it, and preparing it for
model training. This process involves curating diverse datasets,
ensuring high-quality labeling, and developing preprocessing pipelines
to address variations in the data. \textbf{?@sec-data-engineering-ml}
explores these complexities in depth.

With data resources in place, the development process creates models
that can learn from these resources. Model development and training
involves selecting appropriate algorithms, designing model
architectures, and training models using the prepared data. Success
depends on choosing techniques suited to the problem and iterating on
the model design for optimal performance. \textbf{?@sec-ai-training}
details advanced training approaches and distributed training
strategies, while \textbf{?@sec-dnn-architectures} covers the underlying
architectures.

Once models are trained, rigorous evaluation ensures they meet
performance requirements before deployment. This evaluation and
validation stage involves rigorously testing the model's performance
against predefined metrics and validating its behavior in different
scenarios, ensuring the model is accurate, reliable, and robust in
real-world conditions.

With validation complete, models transition from development
environments to operational systems through careful deployment
processes. Deployment and integration requires addressing practical
challenges such as system compatibility, scalability, and operational
constraints across different deployment contexts.
\textbf{?@sec-ml-system-architecture} explores these deployment
contexts, ranging from cloud to edge environments.

The final stage recognizes that deployed systems require ongoing
oversight to maintain performance and adapt to changing conditions. This
monitoring and maintenance stage focuses on continuously tracking the
system's performance in real-world environments and updating it as
necessary. Effective monitoring ensures the system remains relevant and
accurate over time, adapting to changes in data, requirements, or
external conditions.

\subsection{Case Study: Diabetic Retinopathy
Screening}\label{sec-ai-development-workflow-case-study-diabetic-retinopathy-screening-7d71}

Before examining the stage interfaces and detailed workflows, we
introduce a case study that will ground our discussion throughout this
chapter. Diabetic retinopathy (DR) screening systems
(\citeproc{ref-gulshan2016deep}{Gulshan et al. 2016}) provide an ideal
lens because the problem appears straightforward (image classification)
but reveals deep complexity in deployment; the development journey from
research to clinical use is well documented; and the challenges span
every lifecycle stage from data collection through monitoring.

Diabetic retinopathy affects over 100 million people worldwide and is a
leading cause of preventable blindness\sidenote{\textbf{Diabetic
Retinopathy Global Impact}: Affects 93-103 million people worldwide,
with 22-35\% of diabetic patients developing retinopathy
(\citeproc{ref-who2019classification}{Steinmetz et al. 2024}). In
developing countries, up to 90\% of vision loss from diabetes is
preventable with early detection, but access to specialists remains
severely limited (\citeproc{ref-rajkomar2019machine}{Rajkomar, Dean, and
Kohane 2019}). }. Figure~\ref{fig-eye-dr} illustrates the clinical
challenge: detecting characteristic hemorrhages (dark red spots) that
indicate disease progression. Rural areas in developing countries have
approximately one ophthalmologist per 100,000+ people, making
AI-assisted screening not just convenient but medically essential.

\begin{figure}

\centering{

\includegraphics[width=0.9\linewidth,height=\textheight,keepaspectratio]{contents/vol1/workflow/images/png/eye-dr.png}

}

\caption{\label{fig-eye-dr}\textbf{Retinal Hemorrhages}: Diabetic
retinopathy causes visible hemorrhages in retinal images. While this
appears to be straightforward image classification, the path from
laboratory success to clinical deployment illustrates every aspect of AI
lifecycle complexity. Source: Google.}

\end{figure}%

Initial research achieved expert-level performance in controlled
settings. However, the journey to clinical deployment revealed how
technical excellence must integrate with data quality challenges,
infrastructure constraints in rural clinics, regulatory requirements,
and workflow integration\sidenote{\textbf{Healthcare AI Deployment
Reality}: Studies suggest that a significant majority of healthcare AI
projects never reach clinical deployment, with many failing due to
integration challenges, regulatory hurdles, and workflow disruption
rather than algorithmic issues (\citeproc{ref-chen2017machine}{Chen and
Asch 2017}; \citeproc{ref-kelly2019key}{Kelly et al. 2019}). }. As we
examine each lifecycle stage in the sections that follow, we will trace
how the same constraint propagation dynamics apply whether you're
building medical imaging systems or mobile applications like
MobileNetV2.

\subsection{Stage Interface
Specification}\label{sec-ai-development-workflow-stage-interface-specification-ae3c}

Each lifecycle stage operates as a distinct engineering phase with
defined inputs, outputs, and quality invariants.
Table~\ref{tbl-stage-interface} formalizes these contracts, making
explicit what each stage must receive and produce. This specification
transforms the abstract lifecycle diagram into actionable engineering
requirements. When a stage's output fails to meet its contract, the
deficiency propagates forward, compounding costs at each subsequent
stage.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1812}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2464}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2609}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2971}}@{}}
\caption{\textbf{Stage Interface Specification}: Each lifecycle stage
has explicit input requirements, output deliverables, and quality
invariants that must hold for the stage to be considered complete.
Violations of these contracts create technical debt that compounds
through subsequent stages. The deployment paradigm selection in Problem
Definition (Cloud, Edge, Mobile, or TinyML from
\textbf{?@sec-ml-system-architecture}) constrains all downstream
stages---a TinyML target imposes different data, model, and monitoring
requirements than a Cloud
target.}\label{tbl-stage-interface}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Stage}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Input Contract}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Output Contract}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Quality Invariant}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Stage}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Input Contract}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Output Contract}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Quality Invariant}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Problem Definition} & Business requirements; operational context
& Measurable objectives; deployment paradigm selection; resource
constraints & All success criteria are quantifiable; target deployment
paradigm is explicit \\
\textbf{Data Collection} \textbf{\& Preparation} & Objectives;
deployment target; quality requirements & Versioned dataset with schema;
preprocessing pipeline; data validation rules & Distribution
approximates anticipated production environment; labeling meets accuracy
requirements \\
\textbf{Model Development} \textbf{\& Training} & Dataset; accuracy
targets; resource constraints & Trained model weights; training
configuration; experiment logs & Meets accuracy thresholds within
computational budget; architecture compatible with deployment target \\
\textbf{Evaluation} \textbf{\& Validation} & Trained model; held-out
test data; evaluation criteria & Performance metrics across subgroups;
failure mode analysis; validation certificate & No critical subgroup
falls below minimum thresholds; calibration meets domain requirements \\
\textbf{Deployment} \textbf{\& Integration} & Validated model;
infrastructure requirements; SLA targets & Serving endpoint; monitoring
instrumentation; rollback procedures & Latency and throughput meet
paradigm requirements; integration tests pass \\
\textbf{Monitoring} \textbf{\& Maintenance} & Live system; performance
baselines; alert thresholds & Drift detection alerts; retraining
triggers; incident reports & Performance stays within acceptable bounds;
degradation detected before user impact \\
\end{longtable}

This specification reveals why ML projects experience the iteration
cycles shown in Figure~\ref{fig-lifecycle-overview}. When a downstream
stage discovers that an upstream contract was violated---for example,
evaluation reveals the training data distribution does not match
production---the project must iterate back to fix the root cause. Teams
that validate contracts at each stage transition catch violations early,
when correction costs are lowest.

\phantomsection\label{callout-exampleux2a-1.4}
\begin{fbx}{callout-example}{Example: }{Auditing a Stage Transition: Problem Definition → Data Collection}
\phantomsection\label{callout-example*-1.4}
\textbf{Scenario}: Your team claims to have completed Problem Definition
for a medical imaging classifier. Before proceeding to Data Collection,
audit the stage transition against Table~\ref{tbl-stage-interface}.

\textbf{Audit Checklist} (from Output Contract):

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Measurable objectives}: ✓ ``Achieve \textgreater90\%
  sensitivity and \textgreater80\% specificity for referable cases''
\item
  \textbf{Deployment paradigm selection}: ✗ \emph{Missing} --- Team says
  ``we'll figure out deployment later''
\item
  \textbf{Resource constraints}: ✗ \emph{Incomplete} --- Budget
  specified, but no latency or memory targets
\end{enumerate}

\textbf{Quality Invariant Check}:

\begin{itemize}
\tightlist
\item
  ``All success criteria are quantifiable'' --- ✓
  Sensitivity/specificity targets are quantifiable
\item
  ``Target deployment paradigm is explicit'' --- ✗ \textbf{VIOLATION}
  --- No paradigm selected
\end{itemize}

\textbf{Audit Result}: Stage transition \textbf{blocked}. Two contract
violations detected.

\textbf{Cost Analysis}: Proceeding without deployment paradigm selection
risks discovering at stage 5 (Deployment) that the target is Edge
deployment with \textless100ms latency and \textless500MB memory. By the
Constraint Propagation Principle, this would cost \(2^{5-1} = 16×\) the
effort of resolving it now at stage 1.

\textbf{Resolution}: Return to Problem Definition. Establish deployment
target (e.g., ``Edge deployment on NVIDIA Jetson with \textless50ms
inference latency and \textless200MB model size''). This constraint will
shape Data Collection (preprocessing must be device-compatible), Model
Development (architecture must fit memory budget), and Evaluation (must
include device-specific performance testing).

\textbf{Time saved}: 2-4 iteration cycles avoided, approximately 8-16
weeks of rework prevented.

\textbf{The same pattern applies to MobileNetV2}: If Problem Definition
specifies ``mobile deployment'' but omits ``\textless14 MB model size,
\textless300 MFLOPs, real-time on mobile GPU,'' the team might develop a
200 MB ResNet-50 variant that achieves state-of-the-art accuracy---then
discover at Deployment that it violates every mobile constraint.

\end{fbx}

With the DR case study providing concrete context and the Stage
Interface Specification establishing formal contracts, we now examine
each lifecycle stage in detail. The sections that follow trace how
constraint propagation, feedback loops, and systems thinking principles
manifest at each stage.

\phantomsection\label{quiz-question-sec-ai-development-workflow-six-core-lifecycle-stages-00b0}
\begin{fbx}{callout-quiz-question}{Self-Check: Question 1.4}{}
\phantomsection\label{quiz-question-sec-ai-development-workflow-six-core-lifecycle-stages-00b0}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Which of the following best describes the purpose of the `Problem
  Definition' stage in the ML lifecycle?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii})}
  \tightlist
  \item
    To define objectives and constraints for the ML system.
  \item
    To gather and clean data for model training.
  \item
    To deploy the model into production environments.
  \item
    To monitor the system's performance post-deployment.
  \end{enumerate}
\item
  Order the following ML lifecycle stages from start to finish: (1)
  Deployment \& Integration, (2) Model Development \& Training, (3) Data
  Collection \& Preparation, (4) Monitoring \& Maintenance.
\item
  How does the feedback loop in the ML lifecycle contribute to the
  system's adaptability and improvement?
\item
  In the context of the DR screening system, which lifecycle stage
  likely involves ensuring model performance in real-world conditions?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii})}
  \tightlist
  \item
    Problem Definition
  \item
    Data Collection \& Preparation
  \item
    Evaluation \& Validation
  \item
    Monitoring \& Maintenance
  \end{enumerate}
\end{enumerate}

\noindent\hspace*{1.25em}\hyperref[quiz-answer-sec-ai-development-workflow-six-core-lifecycle-stages-00b0]{\textbf{See Answer~$\rightarrow$}}

\end{fbx}

\section{Problem Definition
Stage}\label{sec-ai-development-workflow-problem-definition-stage-5974}

Machine learning system development begins with a challenge distinct
from traditional software development: define not just what the system
should do, but how it should learn to do it. Conventional software
requirements translate directly into implementation rules, while ML
systems require teams to consider how the system will learn from data
while operating within real-world constraints\sidenote{\textbf{ML
vs.~Traditional Problem Definition}: Traditional software problems are
defined by deterministic specifications (``if input X, then output Y''),
but ML problems are defined by examples and desired behaviors. This
shift means that ML projects face higher failure rates, with industry
surveys suggesting 70-90\% of ML projects fail to reach production
deployment, many during problem formulation and requirements phases,
compared to lower failure rates in traditional software projects
(\citeproc{ref-standish2020chaos}{The Standish Group 2020}). The
challenge lies in translating business objectives into learning
objectives, something that did not exist in software engineering until
the rise of data-driven systems in the 2000s
(\citeproc{ref-amershi2019software}{Amershi et al. 2019}). }. This first
stage, positioned at the left of Figure~\ref{fig-lifecycle-overview},
lays the foundation for all subsequent phases in the ML lifecycle.

The DR screening example illustrates how this complexity manifests in
practice. What appears to be a straightforward image classification
task---detect disease in retinal photographs---actually required
defining multiple interconnected objectives that shaped every subsequent
lifecycle stage.

Development teams in such systems must balance competing constraints
that span technical, operational, and regulatory domains. Diagnostic
accuracy ensures patient safety, while computational efficiency enables
deployment on rural clinic hardware with limited resources. Workflow
integration drives clinical adoption by fitting seamlessly into existing
practices, and regulatory compliance meets medical device approval
requirements that govern what can be deployed. Cost-effectiveness
supports sustainable deployment across resource-limited settings. Each
constraint influences the others, creating a complex optimization
problem that traditional software development approaches cannot address.
This multi-dimensional problem definition drives data collection
strategies, model architecture choices, and deployment infrastructure
decisions throughout the project lifecycle.

\subsection{Balancing Competing
Constraints}\label{sec-ai-development-workflow-balancing-competing-constraints-b92a}

Problem definition decisions cascade through system design. Requirements
analysis in a DR screening system evolves from initial focus on
diagnostic accuracy metrics to encompass deployment environment
constraints and opportunities.

Achieving 90\%+ sensitivity for detecting referable diabetic retinopathy
prevents vision loss, while maintaining 80\%+ specificity avoids
overwhelming referral systems with false positives. These metrics must
be achieved across diverse patient populations, camera equipment, and
image quality conditions typical in resource-limited settings.

Beyond accuracy requirements, rural clinic deployments impose strict
infrastructure constraints that reflect the edge deployment challenges
\textbf{?@sec-ml-system-architecture} examines. Models must run on
devices with limited computational power, operate reliably with
intermittent internet connectivity, and produce results within clinical
workflow timeframes. These systems also require operation by healthcare
workers with minimal technical training.

Layered on top of these technical and operational constraints, medical
device regulations require extensive validation, audit trails, and
performance monitoring capabilities that influence data collection,
model development, and deployment strategies.

These interconnected requirements demonstrate how problem definition in
ML systems requires understanding the complete ecosystem in which the
system will operate. Early recognition of these constraints enables
teams to make architecture decisions crucial for successful deployment,
rather than discovering limitations after significant development
investment.

\subsection{Collaborative Problem Definition
Process}\label{sec-ai-development-workflow-collaborative-problem-definition-process-1538}

The specific constraints we just examined (90\%+ sensitivity, 80\%+
specificity, edge deployment, regulatory compliance) did not emerge from
technical analysis alone. They required systematic collaboration between
engineers, domain experts, and stakeholders to translate clinical needs
into measurable engineering requirements.

In our DR example, this collaboration involved close work with
ophthalmologists to determine the diagnostic needs of rural clinics. Key
decisions, such as balancing model complexity with hardware limitations
and ensuring interpretability for healthcare providers, emerge during
this phase. The approach must account for regulatory considerations,
such as patient privacy and compliance with healthcare standards. This
collaborative process keeps the problem definition grounded in both
technical feasibility and clinical relevance.

\subsection{Adapting Definitions for
Scale}\label{sec-ai-development-workflow-adapting-definitions-scale-40ed}

As ML systems scale, problem definitions must adapt to new operational
challenges\sidenote{\textbf{ML System Scaling Complexity}: Scaling ML
systems is often more complex than scaling traditional software due to
data heterogeneity, model drift, and the need for continuous measurement
and retraining. In practice, ML deployments can require substantially
more monitoring and validation infrastructure than non-ML services
(\citeproc{ref-paleyes2022challenges}{Paleyes, Urma, and Lawrence
2022}), and operational burden can grow quickly as the number of models,
data sources, and teams increases
(\citeproc{ref-uber2017michelangelo}{Hermann and Del Balso 2017};
\citeproc{ref-kreuzberger2023machine}{Kreuzberger, Kühl, and Hirschl
2023}). }. A DR-type system might initially focus on a limited number of
clinics with consistent imaging setups. However, as such a system
expands to include clinics with varying equipment, staff expertise, and
patient demographics\sidenote{\textbf{Algorithmic Fairness in
Healthcare}: Medical AI systems show significant performance disparities
across demographic groups. Dermatology AI systems trained predominantly
on lighter skin tones show reduced accuracy for patients with darker
skin (\citeproc{ref-adamson2018dermatology}{Adamson and Smith 2018}),
while diabetic retinopathy models trained primarily on European
populations show accuracy drops for Asian and African populations
(\citeproc{ref-gulshan2016deep}{Gulshan et al. 2016}). The FDA's 2021
Action Plan for AI/ML-based medical devices now requires demographic
performance reporting (\citeproc{ref-fda2021artificial}{Food and
Administration 2021}), and companies like Google Health invest
substantial development resources in fairness testing and bias
mitigation across racial, gender, and socioeconomic groups
(\citeproc{ref-rajkomar2019machine}{Rajkomar, Dean, and Kohane 2019}).
}, the original problem definition requires adjustments to accommodate
these variations.

Scaling also introduces data challenges. Larger datasets may include
more diverse edge cases, which can expose weaknesses in the initial
model design. Expanding deployment to new regions introduces variations
in imaging equipment and patient populations that require further system
tuning. Defining a problem that accommodates such diversity from the
outset ensures the system can handle future expansion without requiring
a complete redesign.

In our DR example, the problem definition process shapes data collection
strategy. Requirements for multi-population validation drive the need
for diverse training data, while edge deployment constraints influence
data preprocessing approaches. Regulatory compliance needs determine
annotation protocols and quality assurance standards. These
interconnected requirements demonstrate how effective problem definition
anticipates constraints that will emerge in subsequent lifecycle stages,
establishing a foundation for integrated system development rather than
sequential, isolated optimization.

\phantomsection\label{quiz-question-sec-ai-development-workflow-problem-definition-stage-5974}
\begin{fbx}{callout-quiz-question}{Self-Check: Question 1.5}{}
\phantomsection\label{quiz-question-sec-ai-development-workflow-problem-definition-stage-5974}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  How does problem definition in machine learning differ from
  traditional software development?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii})}
  \tightlist
  \item
    It involves defining how the system should learn from data.
  \item
    It focuses solely on deterministic specifications.
  \item
    It requires no consideration of real-world constraints.
  \item
    It is based on fixed input-output rules.
  \end{enumerate}
\item
  Why is it crucial to align problem definition with real-world
  constraints in ML system development?
\item
  In ML systems, the process of translating business objectives into
  learning objectives is known as \_\_\_\_.
\item
  Which of the following best describes a key challenge in scaling ML
  systems?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii})}
  \tightlist
  \item
    Data homogeneity across all environments.
  \item
    Consistent model performance without additional tuning.
  \item
    Data heterogeneity and infrastructure requirements.
  \item
    Simplified monitoring infrastructure compared to traditional
    applications.
  \end{enumerate}
\item
  In a production system, how might problem definition influence the
  choice of deployment infrastructure?
\end{enumerate}

\noindent\hspace*{1.25em}\hyperref[quiz-answer-sec-ai-development-workflow-problem-definition-stage-5974]{\textbf{See Answer~$\rightarrow$}}

\end{fbx}

\section{Data Collection \& Preparation
Stage}\label{sec-ai-development-workflow-data-collection-preparation-stage-ae99}

Problem definition establishes what we want to build: measurable
objectives, a deployment paradigm, and resource constraints. The
immediate next question becomes: where will the data come from that
teaches the model to achieve these objectives?

This transition from defining goals to acquiring training data marks a
critical juncture where many projects fail. Teams often underestimate
data collection, treating it as a preliminary step before the ``real
work'' of model development. In reality, data activities consume 60-80\%
of total project time, and the decisions made during this stage
propagate through every subsequent phase. The deployment constraints
established during problem definition now become data requirements: if
the model must run on edge devices, the data pipeline must produce
inputs compatible with edge preprocessing. If the model must achieve
90\% sensitivity across diverse populations, the data must include
sufficient examples from each population.

Raw data gathering, processing, and preparation for model development
characterize this second lifecycle stage. These challenges extend far
beyond gathering sufficient training examples\sidenote{\textbf{The 80/20
Rule in ML}: Data scientists typically spend 60-80\% of their time on
data collection, cleaning, and preparation, with the remainder on
modeling and analysis (\citeproc{ref-crowdflower2016data}{CrowdFlower
2016}). This ratio remains consistent across industries despite advances
in automated tools. The ``data preparation tax'' includes handling
missing values (common in real-world datasets), resolving
inconsistencies across data sources, and ensuring legal compliance with
varying consent requirements. This explains why successful ML teams
invest heavily in data engineering capabilities from day one. }.
\textbf{?@sec-data-engineering-ml} addresses these complexities as its
core focus. For medical AI systems like DR screening, data collection
must balance statistical rigor with operational feasibility while
meeting the highest standards for diagnostic accuracy.

Problem definition decisions shape data requirements in the DR example.
The multi-dimensional success criteria established (accuracy across
diverse populations, hardware efficiency, and regulatory compliance)
demand a data collection strategy that goes beyond typical computer
vision datasets.

Building this foundation in such a system might require assembling a
development dataset on the order of (10\^{}5) retinal fundus
photographs, with each image reviewed by multiple expert
ophthalmologists\sidenote{\textbf{Medical Data Annotation Costs}: Expert
medical annotation can be extraordinarily expensive, often requiring
specialist time billed at hundreds of dollars per hour. For large
medical imaging datasets, total annotation cost can reach millions of
dollars, motivating techniques such as active learning and synthetic
data generation. }. This expert consensus approach addresses the
inherent subjectivity in medical diagnosis while establishing ground
truth labels that can withstand regulatory scrutiny. The annotation
process captures clinically relevant features like microaneurysms,
hemorrhages, and hard exudates across the spectrum of disease severity.

High-resolution retinal scans can generate files on the order of tens of
megabytes per image (depending on resolution and compression), creating
substantial infrastructure challenges. A clinic processing dozens of
patients per day can produce gigabytes to tens of gigabytes of imaging
data per week, exceeding the capacity of rural internet connections with
only a few megabits per second of upload. This data volume constraint
forces architectural decisions toward edge-computing solutions rather
than cloud-based processing.

\phantomsection\label{callout-perspectiveux2a-1.5}
\begin{fbx}{callout-perspective}{Systems Perspective: }{Napkin Math: Bandwidth vs. Compute}
\phantomsection\label{callout-perspective*-1.5}
\textbf{Problem}: You have a cluster of 8 GPUs. Training is slow. Should
you buy faster GPUs or a faster network switch?

\textbf{The Math}: 1. \textbf{Daily Data}:
\(100 \text{ patients} \times 10 \text{ photos} \times 5 \text{ MB/photo} = \mathbf{5 \text{ GB/day}}\).
2. \textbf{Upload Time}:
\(5,000 \text{ MB} / (2/8 \text{ MB/s}) = 20,000 \text{ seconds} \approx \mathbf{5.5 \text{ hours}}\).
3. \textbf{The Constraint}: If the clinic operates for 8 hours, you are
consuming \textbf{70\%} of their total bandwidth just for one AI
feature.

\textbf{The Engineering Conclusion}: A Cloud-only architecture is too
``expensive'' in terms of bandwidth. By moving to the \textbf{Edge}, you
only need to upload \textbf{Detection Summaries} (\textasciitilde10
KB/patient), reducing bandwidth usage by \textbf{500,000\(\times\)}.

\end{fbx}

\subsection{Bridging Laboratory and Real-World
Data}\label{sec-ai-development-workflow-bridging-laboratory-realworld-data-e5b6}

Transitioning from laboratory-quality training data to real-world
deployment reveals fundamental gaps when such a system moves to rural
clinic settings.

When deployment begins in rural clinics across regions like Thailand and
India, real-world data differs dramatically from carefully curated
training sets. Images come from diverse camera equipment operated by
staff with varying expertise levels, often under suboptimal lighting
conditions and with inconsistent patient positioning. These variations
threaten model performance and reveal the need for robust preprocessing
and quality assurance systems.

This data volume constraint drives a fundamental architectural decision
between the deployment paradigms \textbf{?@sec-ml-system-architecture}
introduces: edge computing deployment rather than cloud-based inference.
Local preprocessing can reduce bandwidth requirements by roughly one to
two orders of magnitude (for example, from tens of gigabytes per week
down to under a gigabyte) but requires materially more local
computation. This trade-off shapes both model optimization strategies
and deployment hardware requirements using specialized edge devices such
as NVIDIA Jetson\sidenote{\textbf{NVIDIA Jetson}: A family of embedded
edge compute modules and developer kits that integrate GPU acceleration
for on-prem inference. In representative deployments, these systems
operate within a few watts to a few tens of watts and offer sufficient
on-device compute and memory to run optimized vision and perception
models in low-latency settings where cloud connectivity is unreliable,
costly, or too slow. }.

A typical solution architecture emerges from data collection
constraints: edge devices for local inference and preprocessing, clinic
aggregation servers for data management and buffering, and cloud
training infrastructure for periodic model updates. In practice,
end-to-end latency targets are often in the tens of milliseconds, and
availability targets depend on clinical workflow and local connectivity
constraints.

Patient privacy regulations often motivate federated learning
architectures, enabling model training without centralizing sensitive
patient data. This approach adds complexity to both data collection
workflows and model training infrastructure but can be important for
regulatory approval and clinical adoption.

These experiences demonstrate the constraint propagation principles we
established earlier. Lifecycle decisions in data collection create
constraints and opportunities that propagate through the entire system
development process, shaping everything from infrastructure design to
model architecture.

\subsection{Data Infrastructure for Distributed
Deployment}\label{sec-ai-development-workflow-data-infrastructure-distributed-deployment-23ec}

Understanding how data characteristics and deployment constraints drive
architectural decisions becomes critical at scale. Each retinal image
travels through multiple stages: clinic cameras capture the image, local
systems provide initial storage and processing, quality validation
checks ensure usability, secure transmission moves data to central
systems, and finally, integration with training datasets completes the
pipeline.

Different data access patterns demand different storage solutions. Teams
typically implement \textbf{tiered storage
architectures}\sidenote{\textbf{Tiered Storage}: A storage architecture
that places data on different storage media based on access frequency
and performance requirements. The cost-performance gap between storage
tiers is substantial: high-performance NVMe SSDs deliver 500,000+ IOPS
(cloud block storage like AWS EBS gp2 costs approximately
\$0.10/GB/month), while object storage like S3 costs approximately
\$0.023/GB/month but with 100-200ms latency. For ML workloads, the hot
tier feeds training loops requiring sustained sequential reads at 1-10
GB/s, while cold archival storage holds audit trails and historical
datasets accessed only during investigations or regulatory reviews. }:

\begin{itemize}
\tightlist
\item
  \textbf{Hot Storage}: High-throughput NVMe SSDs for data currently
  used in training loops.
\item
  \textbf{Warm Storage}: S3-compatible object storage for recent
  datasets and active validation sets.
\item
  \textbf{Cold Storage}: Low-cost archival storage (e.g., AWS Glacier)
  for historical data required for regulatory audit trails but rarely
  accessed.
\end{itemize}

Intelligent caching systems optimize data access based on usage
patterns, ensuring that relevant data remains readily available without
incurring excessive costs.

Rural clinic deployments face severe connectivity constraints, requiring
flexible data transmission strategies. Real-time transmission works well
for clinics with reliable internet, while store-and-forward systems
enable operation in areas with intermittent connectivity. This adaptive
approach ensures consistent system operation regardless of local
infrastructure limitations.

Infrastructure design must anticipate growth from pilot deployments to
hundreds of clinics. The architecture accommodates varying data volumes,
different hardware configurations, and diverse operational requirements
while maintaining data consistency and system reliability. This
scalability foundation proves essential as systems expand to new
regions.

\subsection{Managing Data at
Scale}\label{sec-ai-development-workflow-managing-data-scale-4942}

As ML systems expand, data collection challenges grow exponentially. In
our DR example, scaling from initial clinics to a broader network
introduces emergent complexity: significant variability in equipment,
workflows, and operating conditions. Each clinic effectively becomes an
independent data node\sidenote{\textbf{Federated Learning Architecture}:
Federated learning (\citeproc{ref-mcmahan2017communication}{McMahan et
al. 2017}), introduced by Google in 2016 for mobile keyboards, enables
training across distributed data sources without centralizing data.
Healthcare applications are particularly suited for federated learning
due to privacy regulations, with federated medical models approaching
centralized model accuracy while keeping data local. However, federated
learning introduces new challenges: communication costs increase
substantially per training iteration, and statistical heterogeneity
across sites can cause model convergence issues that centralized
training does not face. }, yet the system must ensure consistent
performance across all locations. Following the collaborative
coordination patterns established earlier, teams implement specialized
orchestration with shared artifact repositories, versioned APIs, and
automated testing pipelines that enable efficient management of large
clinic networks.

Scaling such systems to additional clinics also brings increasing data
volumes, as higher-resolution imaging devices become standard,
generating larger and more detailed images. These advances amplify the
demands on storage and processing infrastructure, requiring
optimizations to maintain efficiency without compromising quality.
Differences in patient demographics, clinic workflows, and connectivity
patterns further underscore the need for robust design to handle these
variations gracefully.

Scaling challenges highlight how decisions made during the data
collection phase ripple through the lifecycle, impacting subsequent
stages like model development, deployment, and monitoring. For instance,
accommodating higher-resolution data during collection directly
influences computational requirements for training and inference,
emphasizing the need for lifecycle thinking even at this early stage.

\subsection{Quality Assurance and
Validation}\label{sec-ai-development-workflow-quality-assurance-validation-f923}

Quality assurance is an integral part of the data collection process,
ensuring that data meets the requirements for downstream stages. In our
DR example, automated checks at the point of collection flag issues like
poor focus or incorrect framing, allowing clinic staff to address
problems immediately. These proactive measures ensure that low-quality
data is not propagated through the pipeline.

Validation systems extend these efforts by verifying not just image
quality but also proper labeling, patient association, and compliance
with privacy regulations. Operating at both local and centralized
levels, these systems ensure data reliability and robustness,
safeguarding the integrity of the entire ML pipeline.

The data collection experiences in such systems directly inform model
development approaches. The infrastructure constraints discovered during
data collection establish requirements for model efficiency that drive
architectural decisions. Limited bandwidth, diverse hardware, and
intermittent connectivity all shape what architectures become feasible.
The distributed federated learning approach required by privacy
constraints influences training pipeline design. The quality variations
observed across different clinic environments shape validation
strategies and robustness requirements. This coupling between data
collection insights and model development strategies exemplifies how
integrated lifecycle planning trumps sequential stage optimization.

Figure~\ref{fig-ml-lifecycle-feedback} maps these critical feedback
loops that enable continuous system improvement. Data gaps identified
during evaluation flow back to data collection. Validation issues inform
model training adjustments. Performance insights from production
monitoring trigger refinements across the pipeline. The foundation
established during data collection both enables and constrains the
technical approaches available for creating effective models. This
dynamic becomes apparent as we now transition to model development.

\begin{figure}[htb]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/d878e360b04b07b20cfdbc38a8e95f74e08cb217.pdf}}

}

\caption{\label{fig-ml-lifecycle-feedback}\textbf{ML Lifecycle
Dependencies}: The interconnected feedback paths between stages reveal
why ML systems require integrated rather than sequential development.
Data gaps identified during evaluation flow back to collection.
Validation issues inform training adjustments. Performance insights from
monitoring trigger refinements across the pipeline. Deployment
constraints propagate backward to influence model design. These
dependencies explain why ML technical debt compounds faster than
traditional software debt: changes in any stage can cascade through
multiple connected stages, requiring coordinated updates rather than
isolated fixes.}

\end{figure}%

\phantomsection\label{quiz-question-sec-ai-development-workflow-data-collection-preparation-stage-ae99}
\begin{fbx}{callout-quiz-question}{Self-Check: Question 1.6}{}
\phantomsection\label{quiz-question-sec-ai-development-workflow-data-collection-preparation-stage-ae99}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Which of the following best describes a major challenge in data
  collection for medical AI systems like diabetic retinopathy screening?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii})}
  \tightlist
  \item
    Ensuring high-resolution images are captured consistently.
  \item
    Reducing the cost of expert annotation.
  \item
    Balancing statistical rigor with operational feasibility.
  \item
    Maximizing the number of images collected daily.
  \end{enumerate}
\item
  How does the data volume constraint in rural clinics influence
  architectural decisions in ML systems?
\item
  Order the following steps involved in the data collection process for
  a medical AI system: (1) Initial processing and storage, (2) Data
  capture, (3) Quality validation, (4) Secure transmission.
\item
  What is a key reason for using federated learning in the data
  collection strategy for medical AI systems?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii})}
  \tightlist
  \item
    To improve model accuracy by centralizing data.
  \item
    To comply with patient privacy regulations.
  \item
    To reduce the cost of data annotation.
  \item
    To increase the speed of data processing.
  \end{enumerate}
\end{enumerate}

\noindent\hspace*{1.25em}\hyperref[quiz-answer-sec-ai-development-workflow-data-collection-preparation-stage-ae99]{\textbf{See Answer~$\rightarrow$}}

\end{fbx}

\section{Model Development \& Training
Stage}\label{sec-ai-development-workflow-model-development-training-stage-d901}

Model development and training form the core of machine learning
systems, yet this stage presents unique challenges extending beyond
selecting algorithms and tuning
hyperparameters\sidenote{\textbf{Hyperparameter}: From Greek
\emph{hyper-} (``over, above'') + \emph{parametros}
(``beside-measure''). The prefix indicates these are parameters at a
``higher level'' that govern how regular parameters are learned. Unlike
model parameters (weights learned from data), hyperparameters are set
before training begins: learning rate, batch size, network depth. Modern
deep learning exposes dozens of hyperparameters, creating search spaces
too large to explore exhaustively. \textbf{?@sec-ai-training} presents
systematic optimization approaches that significantly reduce
computational costs. }. \textbf{?@sec-ai-training} covers the training
methodologies, infrastructure requirements, and distributed training
strategies in detail. In high-stakes domains like healthcare, every
design decision impacts clinical outcomes, making the integration of
technical performance with operational constraints critical.

Early lifecycle decisions cascade through model development in our DR
example. The problem definition requirements established (expert-level
accuracy combined with edge device compatibility) create an optimization
challenge that demands innovative approaches to both model architecture
and training strategies.

\textbf{Transfer learning}\sidenote{\textbf{Transfer Learning}: The term
derives from educational psychology, where Edward Thorndike and Robert
Woodworth's 1901 research on ``transfer of practice'' studied how skills
learned in one context apply to new situations. The ML community adopted
and formalized this concept starting with Bozinovski and Fulgosi (1976)
and Lorien Pratt's discriminability-based transfer algorithm (1992). The
1998 book ``Learning to Learn'' established the theoretical foundations.
Andrew Ng's 2016 NIPS tutorial declared transfer learning the next
driver of commercial ML success, and he was right: pre-trained models
now underpin most production computer vision and NLP systems. } provides
the foundation for achieving expert-level performance without requiring
millions of training examples. Rather than training a model from
scratch, transfer learning adapts models pre-trained on large datasets
(like ImageNet's 14 million images) to specific tasks
(\citeproc{ref-krizhevsky2012imagenet}{Krizhevsky, Sutskever, and Hinton
2017}; \citeproc{ref-deng2009imagenet}{Deng et al. 2009}). With roots in
earlier neural network research, transfer learning became widespread in
the 2013-2014 era through influential papers by Yosinski et al.~and
Oquab et al., establishing it as the foundation for practical computer
vision applications because it sharply reduces both training time and
data requirements. Practitioners can achieve expert-level performance
with thousands rather than millions of domain-specific training
examples.

Using transfer learning combined with a meticulously labeled dataset of
128,000 images, developers in DR projects achieve
AUC\sidenote{\textbf{AUC (Area Under the ROC Curve)}: A performance
metric that measures the entire two-dimensional area underneath the
Receiver Operating Characteristic (ROC) curve, which plots true positive
rate against false positive rate at various classification thresholds.
AUC values range from 0.5 (random classifier) to 1.0 (perfect
classifier). Unlike accuracy, AUC is threshold-independent and robust to
class imbalance, making it the preferred metric for medical diagnostics
where the cost of false negatives (missed diagnoses) and false positives
(unnecessary referrals) differ substantially. The companion metrics of
sensitivity (true positive rate) and specificity (true negative rate)
quantify performance at a specific operating threshold. } of 0.99 with
sensitivity of 97.5\% and specificity of 93.4\%
(\citeproc{ref-gulshan2016deep}{Gulshan et al. 2016}), comparable to or
exceeding ophthalmologist performance in controlled settings. This
result validates approaches that combine large-scale pre-training with
domain-specific fine-tuning. The training strategy leverages the
gradient-based optimization principles
\textbf{?@sec-deep-learning-systems-foundations} establishes to adapt
the pre-trained convolutional architectures
\textbf{?@sec-dnn-architectures} presents for medical imaging.

Achieving high accuracy is only the first challenge. Data collection
insights about edge deployment constraints impose strict efficiency
requirements: models may need to fit within tens to hundreds of
megabytes, achieve low-latency inference (often on the order of tens of
milliseconds), and operate within memory budgets on the order of
hundreds of megabytes.

\textbf{Ensemble learning}\sidenote{\textbf{Ensemble}: From French
\emph{ensemblée} meaning ``together, at the same time,'' derived from
Late Latin \emph{insimul} (``at the same time''). The musical sense of
performers working in harmony emerged in 1844. In ML, the metaphor fits
precisely: just as a musical ensemble achieves richer sound than any
solo performer, ensemble learning combines diverse models to achieve
better predictions than any single model. The ``voting'' mechanism in
classification ensembles extends this metaphor, with models collectively
deciding outcomes like a democratic assembly. } --- combining
predictions from multiple models to achieve better performance than any
individual model --- often drives research accuracy but creates
deployment challenges. Common ensemble methods include bagging (training
multiple models on different data subsets), boosting (sequentially
training models to correct previous errors), and stacking (using a
meta-model to combine base model predictions). Netflix's recommendation
system uses ensembles of 100+ algorithms, while winning entries in ML
competitions typically ensemble 10-50 models. However, ensembles trade
inference speed and memory usage for accuracy --- a critical constraint
in edge deployment scenarios.

Initial research models are often much larger (sometimes multiple
gigabytes when using ensembles) and therefore violate deployment
constraints, requiring systematic optimization to reach a deployable
form factor while preserving clinical utility.

These constraints drive architectural innovations including model
optimization techniques for size reduction, inference acceleration, and
efficient deployment scenarios. Teams must balance the computational
demands of deep convolutional networks
(\textbf{?@sec-dnn-architectures}) with the resource limitations of edge
devices (\textbf{?@sec-ml-system-architecture}).

The model development process requires continuous iteration between
accuracy optimization and efficiency optimization. Each architectural
decision must be validated against test set metrics and the
infrastructure constraints identified during data collection. From the
number of convolutional layers to the choice of activation functions
(\textbf{?@sec-deep-learning-systems-foundations}) to the overall
network depth (\textbf{?@sec-dnn-architectures}), every choice affects
both accuracy and efficiency. This multi-objective optimization approach
exemplifies the interdependence principle where deployment constraints
shape development decisions.

\subsection{Systems Artifacts: Beyond Model
Weights}\label{sec-ai-development-workflow-systems-artifacts-beyond-model-weights-b895}

This multi-objective optimization produces more than just trained
weights. A common failure mode is treating the trained model weights as
the sole output of this stage. In a mature ML systems workflow, the
deliverable is a \textbf{reproducible system artifact}. This includes:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Model Weights}: The learned parameters.
\item
  \textbf{Inference Code}: The exact code used to run the model,
  including preprocessing logic.
\item
  \textbf{Environment Specification}: The complete dependency graph
  (e.g., Docker container, \texttt{requirements.txt}, CUDA drivers)
  required to execute the code.
\item
  \textbf{Configuration}: Hyperparameters and runtime settings.
\end{enumerate}

Without bundling the environment with the model, the ``it works on my
machine'' problem creates catastrophic failures during deployment. A
system that achieves 99\% accuracy but relies on a specific library
version not present in production is a broken system.

\subsection{Balancing Performance and Deployment
Constraints}\label{sec-ai-development-workflow-balancing-performance-deployment-constraints-1e8e}

The model development experiences in our DR example illustrate
fundamental trade-offs between clinical effectiveness and deployment
feasibility that characterize real-world AI systems.

Medical applications demand specific performance
metrics\sidenote{\textbf{Medical AI Performance Metrics}: Medical AI
requires different metrics than general ML: sensitivity (true positive
rate) and specificity (true negative rate) are often more important than
overall accuracy. For diabetic retinopathy screening, \textgreater90\%
sensitivity is crucial (missing cases causes blindness), while
\textgreater80\% specificity prevents unnecessary referrals. Medical AI
also requires metrics like positive predictive value (PPV) and negative
predictive value (NPV) that vary with disease prevalence in different
populations---a model with 95\% accuracy in a lab setting might have
only 50\% PPV in a low-prevalence population, making it clinically
useless despite high technical performance. } that differ significantly
from the standard classification metrics
\textbf{?@sec-deep-learning-systems-foundations} introduces. A DR system
requires high sensitivity (to prevent vision loss from missed cases) and
high specificity (to avoid overwhelming referral systems). These metrics
must be maintained across diverse patient populations and image quality
conditions.

Optimizing for clinical performance alone proves insufficient. Edge
deployment constraints from the data collection phase impose additional
requirements: the model must run efficiently on resource-limited
hardware while maintaining inference speeds compatible with clinical
workflows. This creates a multi-objective optimization problem where
improvements in one dimension often come at the cost of others.
\textbf{?@sec-dnn-architectures} explores model capacity, while
\textbf{?@sec-ml-system-architecture} discusses deployment feasibility,
and the fundamental tension between them drives architectural decisions.
In practice, teams often reduce model size by one to two orders of
magnitude through systematic application of quantization, pruning, and
knowledge distillation\sidenote{Model compression (introduced in
\textbf{?@sec-introduction}) encompasses quantization, pruning, and
knowledge distillation to reduce model size while preserving accuracy.
For edge deployment workflows, these techniques are often applied
iteratively, with each compression step validated against deployment
constraints. \textbf{?@sec-model-compression} details systematic
approaches. } techniques, meeting deployment requirements while aiming
to preserve clinical utility.

The choice to use an ensemble of lightweight models rather than a single
large model exemplifies how model development decisions propagate
through the system lifecycle. This architectural decision reduces
individual model complexity (enabling edge deployment) but increases
inference pipeline complexity (affecting deployment and monitoring
strategies). Teams must develop orchestration logic for model ensembles
and create monitoring systems that can track performance across multiple
model components.

Architecture decisions influence data preprocessing pipelines, training
infrastructure requirements, and deployment strategies. Whether choosing
CNN architectures for spatial feature extraction
(\textbf{?@sec-dnn-architectures}) or configuring training
hyperparameters (\textbf{?@sec-deep-learning-systems-foundations}), each
choice cascades through the system. This demonstrates how successful
model development requires anticipating constraints from subsequent
lifecycle stages rather than optimizing models in isolation, reflecting
our systems thinking approach.

\subsection{Constraint-Driven Development
Process}\label{sec-ai-development-workflow-constraintdriven-development-process-cc90}

Real-world constraints shape the entire model development process from
initial exploration through final optimization, demanding systematic
approaches to experimentation.

Development begins with collaboration between data scientists and domain
experts (like ophthalmologists in medical imaging) to identify
characteristics indicative of the target conditions. This
interdisciplinary approach ensures that model architectures capture
clinically relevant features while meeting the computational constraints
identified during data collection.

Computational constraints profoundly shape experimental approaches.
Production ML workflows can create multiplicative costs: multiple model
variants, multiple hyperparameter sweeps, and multiple preprocessing
approaches can quickly translate into on the order of (10\^{}2) training
runs. When each run costs hundreds to thousands of dollars in compute,
iteration costs can reach six figures per experiment cycle. This
economic reality drives investments in efficient experimentation: better
job scheduling, caching of intermediate results, early stopping, and
automated resource optimization.

Systematic hyperparameter optimization dramatically reduces
computational costs compared to exhaustive search.
\textbf{?@sec-ai-training} presents techniques that can substantially
reduce experiment counts while achieving comparable or better results.
Teams that invest in optimization infrastructure early recover the
investment within the first few experiment cycles.

ML model development exhibits emergent behaviors that make outcomes
inherently uncertain, demanding scientific methodology principles:
controlled variables through fixed random seeds and environment
versions, systematic ablation studies\sidenote{\textbf{Ablation
Studies}: Systematic experiments that remove or modify individual
components to understand their contribution to overall performance. In
ML, ablation studies might remove specific layers, change activation
functions, or exclude data augmentation techniques to isolate their
effects. Named after medical ablation (surgical removal of tissue), this
method became standard in ML research after the 2012 AlexNet paper used
ablation to validate each architectural choice. Ablation studies are
essential for complex models where component interactions make it
difficult to determine which design decisions actually improve
performance. } to isolate component contributions, confounding factor
analysis to separate architecture effects from optimization effects, and
statistical significance testing across multiple runs using A/B
testing\sidenote{\textbf{A/B Testing in ML}: Statistical method for
comparing two model versions by randomly assigning users to different
groups and measuring performance differences. Originally developed for
web optimization (2000s), A/B testing became crucial for ML deployment
because models can perform differently in production than in
development. Companies like Netflix run hundreds of concurrent
experiments with users participating in multiple tests simultaneously,
while Uber's Michelangelo platform supports hundreds of ML models in
production across approximately 100 use cases
(\citeproc{ref-uber2017michelangelo}{Hermann and Del Balso 2017}). A/B
testing requires careful statistical design to avoid confounding
variables and ensure sufficient sample sizes for reliable conclusions. }
frameworks. This approach proves essential for distinguishing genuine
performance improvements from statistical noise.

Throughout development, teams validate models against deployment
constraints identified in earlier lifecycle stages. Each architectural
innovation must be evaluated for accuracy improvements and compatibility
with edge device limitations and clinical workflow requirements. This
dual validation approach ensures that development efforts align with
deployment goals rather than optimizing for laboratory conditions that
don't translate to real-world performance.

\subsection{From Prototype to Production-Scale
Development}\label{sec-ai-development-workflow-prototype-productionscale-development-168a}

As projects like our DR example evolve from prototype to production
systems, teams encounter emergent complexity across multiple dimensions:
larger datasets, more sophisticated models, concurrent experiments, and
distributed training infrastructure. These scaling challenges illustrate
systems thinking principles that apply broadly to large-scale AI system
development.

Moving from single-machine training to distributed systems introduces
coordination requirements that demand balancing training speed
improvements against increased system complexity. This leads to
implementing fault tolerance mechanisms and automated failure recovery
systems. Orchestration frameworks enable component-based pipeline
construction with reusable stages, automatic resource scaling, and
monitoring across distributed components.

Systematic tracking becomes critical as experiments generate
artifacts\sidenote{\textbf{Artifact}: From Latin \emph{arte factum}
(``something made by skill''), first used in archaeology (1885) for
objects revealing past human activity. Software engineering borrowed the
term for development outputs like binaries and documentation. ML
artifacts are all digital outputs: trained models, datasets,
preprocessing code, hyperparameter configurations, training logs, and
evaluation metrics. Unlike traditional software artifacts, ML artifacts
are deeply interdependent, since model performance depends on specific
data versions, preprocessing steps, and hyperparameter settings. Tools
like MLflow and Weights \& Biases track lineage between artifacts,
enabling the reproducibility that scientific methodology demands. }
including model checkpoints, training logs, and performance metrics.
Without structured organization, teams risk losing institutional
knowledge from their experimentation efforts. Addressing this requires
implementing systematic experiment identification, automated artifact
versioning, and search capabilities to query experiments by performance
characteristics and configuration parameters.

Large-scale model development demands resource allocation between
training computation and supporting infrastructure. While effective
experiment management requires computational overhead, this investment
pays dividends in accelerated development cycles and improved model
quality through systematic performance analysis and optimization.

The model development process establishes both capabilities and
constraints that directly influence subsequent lifecycle stages. An
edge-optimized ensemble architecture that meets device constraints still
requires sophisticated serving infrastructure for clinic deployment. The
distributed training approach that enables rapid iteration demands model
versioning and synchronization across deployments. Regulatory validation
requirements that guide development decisions shape deployment
validation protocols and monitoring strategies. These interconnections
demonstrate how successful model development must anticipate deployment
challenges, ensuring that technical innovations translate into
operational systems that deliver value.

\phantomsection\label{quiz-question-sec-ai-development-workflow-model-development-training-stage-d901}
\begin{fbx}{callout-quiz-question}{Self-Check: Question 1.7}{}
\phantomsection\label{quiz-question-sec-ai-development-workflow-model-development-training-stage-d901}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Which of the following best describes the trade-off between model
  accuracy and deployment feasibility in the context of edge devices?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii})}
  \tightlist
  \item
    Increasing model accuracy always improves deployment feasibility.
  \item
    Model accuracy and deployment feasibility are unrelated aspects of
    model development.
  \item
    Deployment feasibility is independent of model accuracy.
  \item
    Higher model accuracy often requires more computational resources,
    which can hinder deployment on edge devices.
  \end{enumerate}
\item
  Explain how model compression techniques like quantization and pruning
  help in meeting deployment constraints for edge devices.
\item
  The process of training a smaller model to mimic the behavior of a
  larger model is known as \_\_\_\_. This technique helps in reducing
  model size while maintaining accuracy.
\item
  Order the following steps in optimizing a model for edge deployment:
  (1) Initial model training, (2) Model compression, (3) Performance
  evaluation, (4) Deployment testing.
\item
  In a production system, how might the choice of model architecture
  impact the system's operational constraints?
\end{enumerate}

\noindent\hspace*{1.25em}\hyperref[quiz-answer-sec-ai-development-workflow-model-development-training-stage-d901]{\textbf{See Answer~$\rightarrow$}}

\end{fbx}

\section{Evaluation \& Validation
Stage}\label{sec-ai-development-workflow-evaluation-validation-stage-b47d}

Model development produces trained artifacts that achieve strong metrics
on training data, but laboratory success does not guarantee production
value. The gap between development performance and deployment
reliability is where many ML projects fail. Before deployment, trained
models must undergo rigorous evaluation and validation to ensure they
meet performance requirements and behave reliably across the diverse
conditions encountered in production. This stage bridges model
development and deployment, transforming experimental artifacts into
production-ready systems through systematic testing against predefined
metrics, edge cases, and real-world scenarios.

Evaluation and validation serve distinct but complementary purposes.
Evaluation measures model performance against held-out test data using
metrics established during problem definition. Validation confirms that
the model generalizes appropriately to conditions it will encounter in
production, including edge cases, distribution shifts, and adversarial
inputs. Together these processes establish the evidence base required
for deployment decisions.

\phantomsection\label{callout-definitionux2a-1.6}
\begin{fbx}{callout-definition}{Definition: }{Model Validation}
\phantomsection\label{callout-definition*-1.6}
\textbf{Model Validation} refers to the systematic process of confirming
that a trained model \emph{generalizes} beyond training data, performs
\emph{reliably} under production conditions, and meets
\emph{domain-specific requirements} before deployment.

\end{fbx}

In our DR example, evaluation and validation take on particular
significance due to the clinical stakes involved. A model that performs
well on curated research datasets may fail when confronted with the
image quality variations, equipment differences, and patient population
diversity encountered in rural clinic deployments. The validation
process must anticipate these challenges.

\subsection{Evaluation Metrics and
Thresholds}\label{sec-ai-development-workflow-evaluation-metrics-thresholds-ca81}

Effective evaluation begins with metrics that align with problem
definition objectives. For our DR screening system, standard
classification metrics like accuracy prove insufficient. Clinical
requirements demand specific sensitivity and specificity
thresholds\sidenote{\textbf{Sensitivity vs.~Specificity Trade-offs}:
These metrics exist in tension. Increasing sensitivity (catching more
true positives) typically decreases specificity (more false positives).
For screening applications like DR detection, high sensitivity is
prioritized because missing a case has severe consequences (potential
blindness), while false positives result in unnecessary but non-harmful
referrals. The receiver operating characteristic (ROC) curve visualizes
this trade-off, and the area under the ROC curve (AUC) provides a single
metric summarizing performance across all threshold choices. }:
sensitivity above 90\% ensures few cases of disease-causing retinopathy
are missed, while specificity above 80\% prevents overwhelming referral
systems with false positives.

Beyond aggregate metrics, stratified evaluation reveals performance
variations across patient subgroups. A model achieving 94\% overall
accuracy might exhibit significantly lower performance for patients with
specific comorbidities, particular age groups, or images captured under
certain lighting conditions. These disparities, invisible in aggregate
metrics, become critical in production where every patient deserves
reliable predictions. \textbf{?@sec-benchmarking-ai} provides systematic
treatment of these evaluation methodologies.

Evaluation must also address calibration\sidenote{\textbf{Calibration}:
From ``caliber'' (1580s, via French from possibly Arabic \emph{qalib},
``a mold''), originally meaning to measure gun barrel diameter
precisely. By 1869, it extended to ``determine relative value'' on any
scale. In ML, a calibrated model's predicted probabilities match
observed frequencies: if a model says ``80\% confident,'' approximately
80\% of such predictions should be correct. Calibration is distinct from
accuracy since a model can be highly accurate yet poorly calibrated.
Platt scaling and temperature scaling adjust outputs post-training. In
medical AI, calibration often matters more than accuracy because
clinicians use confidence scores for risk-stratified decisions. }: when
the model predicts 80\% confidence, does the prediction prove correct
80\% of the time? Poorly calibrated models undermine clinical trust even
when accuracy metrics appear strong. Clinicians relying on confidence
scores for triage decisions need those scores to reflect true
uncertainty.

\subsection{Offline and Online
Evaluation}\label{sec-ai-development-workflow-offline-online-evaluation-3a6b}

Offline evaluation on held-out test sets establishes baseline
performance but cannot predict production behavior. Online evaluation
deploys models in controlled production conditions through progressive
stages\sidenote{\textbf{Progressive Deployment Stages}: Shadow mode (or
dark launching) runs the new model in parallel with production, logging
predictions without serving them, to detect integration failures and
latency regressions without user impact. The term ``canary deployment''
derives from coal mining: British physiologist John Scott Haldane
proposed in 1895 using canaries to detect toxic gases, since the birds'
sensitivity meant they would fall ill before miners, providing early
warning. Similarly, canary deployment exposes 1-5\% of traffic to detect
problems before full rollout. If metrics hold, traffic gradually
increases (5\%, 25\%, 50\%, 100\%). Companies like Google and Netflix
pioneered these patterns, catching 70-80\% of issues before full
production. \textbf{?@sec-machine-learning-operations-mlops} details
implementation strategies. }: shadow mode runs the model to make
predictions but does not serve them to users, canary deployment routes a
small percentage of traffic to test production behavior, and A/B testing
provides statistical comparison against the baseline with larger traffic
volumes.

Each stage catches different failure modes. Offline evaluation catches
algorithmic issues, shadow mode catches integration issues, canary
deployment catches scaling issues, and A/B testing catches user-facing
issues. Teams should plan for this staged validation workflow from the
beginning, as retrofitting progressive deployment to an already-deployed
system proves more difficult than building it into the original
deployment architecture.

\subsection{Validation Under Production
Conditions}\label{sec-ai-development-workflow-validation-production-conditions-a351}

Validation extends beyond test set performance to assess model behavior
under conditions that approximate production deployment. This process
reveals failure modes that standard evaluation cannot detect.

Cross-validation across data sources tests whether the model has learned
generalizable patterns or overfit to characteristics specific to
training data sources. A DR model trained primarily on images from
high-quality research cameras must demonstrate robust performance on
images from the diverse equipment deployed across clinic networks.
Validation datasets should include images from equipment manufacturers,
lighting conditions, and operator skill levels representative of actual
deployment contexts.

Robustness testing subjects models to realistic perturbations and edge
cases. For image-based systems, this includes testing with varying
brightness, contrast, focus quality, and partial occlusions. In our DR
example, teams discover that models optimized for research-quality
images may fail on images captured by technicians with minimal training,
requiring preprocessing pipelines that normalize image quality before
inference.

Temporal validation assesses whether models maintain performance over
time. Data distributions shift as patient populations change, equipment
ages, and clinical practices evolve. Models validated only on historical
data may degrade unexpectedly when deployed, a phenomenon called concept
drift\sidenote{\textbf{Concept Drift}: The phenomenon where the
statistical properties of the target variable change over time, causing
model performance to degrade. Concept drift differs from data drift
(changes in input features) because the underlying relationship between
inputs and outputs changes. In medical imaging, concept drift might
occur as disease presentation patterns evolve, imaging technology
advances, or treatment protocols change patient populations. Detecting
and adapting to concept drift requires continuous monitoring and
periodic model retraining. } that motivates the continuous monitoring
discussed in subsequent sections.

\subsection{Regulatory and Domain-Specific
Validation}\label{sec-ai-development-workflow-regulatory-domainspecific-validation-f8ab}

Healthcare AI systems face additional validation requirements mandated
by regulatory frameworks. FDA clearance for medical devices requires
demonstration of safety and effectiveness through clinical validation
studies with appropriate sample sizes and statistical
rigor\sidenote{\textbf{FDA AI/ML Regulation}: The FDA regulates
AI/ML-based medical devices under its Software as a Medical Device
(SaMD) framework. As of 2023, over 500 AI/ML-enabled medical devices
have received FDA authorization, with the majority in radiology and
cardiology. The FDA's 2021 Action Plan for AI/ML addresses the unique
challenge of continuously learning systems, proposing a ``predetermined
change control plan'' that allows manufacturers to document intended
modifications in advance rather than seeking new clearance for each
model update. This regulatory evolution reflects growing recognition
that AI systems differ fundamentally from static medical software. }.
These requirements influence the entire development process, from study
design through documentation practices.

Domain-specific validation goes beyond regulatory compliance to address
stakeholder requirements. Clinical validation studies in our DR example
involve deploying the system alongside expert graders and comparing
predictions against ground truth established by consensus panels of
ophthalmologists. These studies must demonstrate not just comparable
accuracy but also acceptable failure modes: systems that fail safely
(referring uncertain cases to specialists) receive more clinical trust
than those that fail silently.

Human factors validation assesses how clinicians interact with system
predictions and whether the overall workflow achieves intended outcomes.
A technically accurate model that clinicians distrust or misuse fails to
deliver clinical value. Validation studies should measure not just model
performance but end-to-end workflow outcomes including clinician
confidence, referral appropriateness, and patient satisfaction.

\subsection{From Validation to Deployment
Readiness}\label{sec-ai-development-workflow-validation-deployment-readiness-207c}

Successful validation produces artifacts that enable informed deployment
decisions: documentation covering performance across relevant metrics
and subgroups, characterization of failure modes and their frequencies,
validated preprocessing and inference pipelines, and evidence of
regulatory compliance where required.

The transition from validation to deployment represents a decision point
where teams assess whether accumulated evidence supports production
release. This decision balances multiple factors: technical performance
metrics, operational readiness, regulatory status, and organizational
capacity for monitoring and maintenance. Incomplete validation creates
deployment risks that compound throughout the system lifecycle.

Validation failures drive model architecture revisions, training data
augmentation, and preprocessing pipeline improvements. Validation
successes establish the performance baselines and monitoring thresholds
that guide production operations. This bidirectional influence
exemplifies the systems thinking approach that characterizes effective
ML lifecycle management.

\section{Deployment \& Integration
Stage}\label{sec-ai-development-workflow-deployment-integration-stage-d549}

At the deployment and integration stage, trained models integrate into
production systems and workflows. Deployment requires addressing
practical challenges such as system compatibility, scalability, and
operational constraints. Successful integration makes the model's
predictions accurate and actionable in real-world settings where
resource limitations and workflow disruptions pose barriers.
\textbf{?@sec-machine-learning-operations-mlops} covers the operational
aspects of deployment and maintenance.

In our DR example, deployment strategies are shaped by the diverse
environments we identified earlier. Edge deployment enables local
processing of retinal images in rural clinics with intermittent
connectivity, while automated quality checks flag poor-quality images
for recapture, ensuring reliable predictions. These measures demonstrate
how deployment must bridge technological sophistication with usability
and scalability across clinical settings.

\subsection{Technical and Operational
Requirements}\label{sec-ai-development-workflow-technical-operational-requirements-36ab}

The requirements for deployment stem from both the technical
specifications of the model and the operational constraints of its
intended environment. In our DR-type system, the model must operate in
rural clinics with limited computational resources and intermittent
internet connectivity. It must fit into the existing clinical workflow,
requiring rapid, interpretable results that assist healthcare providers
without causing disruption.

These requirements influence deployment strategies. A cloud-based
deployment, while technically simpler, may not be feasible due to
unreliable connectivity in many clinics. Instead, teams often opt for
edge deployment, where models run locally on clinic hardware. This
approach requires model optimization to meet hardware constraints, such
as tight model size, latency, and memory budgets. Achieving these
targets requires systematic application of optimization techniques that
reduce model size and computational requirements while balancing
accuracy trade-offs.

\phantomsection\label{callout-perspectiveux2a-1.7}
\begin{fbx}{callout-perspective}{Systems Perspective: }{Napkin Math: Cloud vs. Edge Deployment Economics}
\phantomsection\label{callout-perspective*-1.7}
\textbf{Problem}: You have a model that processes 1 million images per
month. Should you deploy on Cloud (AWS Lambda) or Edge (on-premise
server)?

\textbf{Option A: Cloud Inference}

\begin{itemize}
\tightlist
\item
  Model runs on centralized GPU servers
\item
  Inference cost: \textasciitilde\$0.01 per image (cloud GPU time + API
  overhead)
\item
  Annual cost: 500 clinics × 50 patients × 365 days × \$0.01 =
  \textbf{\$91,250/year}
\item
  Plus: Network costs for uploading 5 MB images =
  \textasciitilde\$45,000/year
\item
  \textbf{Total: \textasciitilde\$136,000/year} operational cost
\item
  Risk: 200ms+ latency breaks clinical workflow; connectivity outages
  halt screening
\end{itemize}

\textbf{Option B: Edge Deployment (NVIDIA Jetson)}

\begin{itemize}
\tightlist
\item
  One-time hardware: 500 × \$500 = \textbf{\$250,000} capital expense
\item
  Inference cost: \textasciitilde\$0.001 per image (electricity only)
\item
  Annual cost: negligible operational, \textasciitilde\$25,000
  maintenance
\item
  \textbf{Total: \$250,000 upfront + \textasciitilde\$25,000/year}
\item
  Benefit: \textless50ms latency; works offline; no per-inference cost
\end{itemize}

\textbf{The Engineering Conclusion}: Edge deployment pays back in
\textless2 years and provides better reliability. But it requires
tighter model optimization (must fit in edge memory) and more complex
update pipelines. The deployment paradigm selected during Problem
Definition determines whether you can even consider the edge option.

\end{fbx}

Integration with existing systems poses additional challenges. The ML
system must interface with hospital information systems (HIS) for
accessing patient records and storing results. Privacy regulations
mandate secure data handling at every step, shaping deployment
decisions. These considerations ensure that the system adheres to
clinical and legal standards while remaining practical for daily use.
\textbf{?@sec-machine-learning-operations-mlops} details operational
considerations that apply to these deployments.

\subsection{Scaling Deployment: From Pilot to
Production}\label{sec-ai-development-workflow-scaling-deployment-pilot-production-b3b2}

Deployment proceeds through phases that progressively expose the system
to real-world complexity. Teams begin with simulated environments
replicating target constraints, then deploy to pilot sites for
controlled real-world testing, gathering feedback from clinical staff
before scaling to full deployment.

Scaling across multiple sites reveals challenges invisible in controlled
settings. Each clinic presents unique constraints: different imaging
equipment, varying network reliability, diverse operator expertise
levels, and distinct workflow patterns. Variations in equipment and
operator skill create data quality inconsistencies that force model
preprocessing adjustments. Edge deployment minimizes latency but imposes
strict model complexity constraints; cloud deployment enables
flexibility but introduces latency that may violate workflow
requirements.

Successful deployment requires more than technical optimization.
Clinician feedback often reveals that initial interfaces need
significant redesign for adoption. User trust and proficiency matter as
much as algorithmic performance. Reliability mechanisms --- automated
image quality checks, fallback workflows for errors, stress testing for
peak volumes --- ensure systems operate robustly across conditions.

Managing improvements across distributed deployments requires
centralized version control and automated update pipelines. Deployment
feedback (usability concerns, performance issues, integration
challenges) shapes monitoring strategies, demonstrating that deployment
is not an endpoint but a transition into continuous operations.

\phantomsection\label{quiz-question-sec-ai-development-workflow-deployment-integration-stage-d549}
\begin{fbx}{callout-quiz-question}{Self-Check: Question 1.8}{}
\phantomsection\label{quiz-question-sec-ai-development-workflow-deployment-integration-stage-d549}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Which of the following is a primary reason for choosing edge
  deployment over cloud deployment in rural clinics?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii})}
  \tightlist
  \item
    To increase model complexity
  \item
    To leverage cloud computing resources
  \item
    To reduce latency and ensure reliability despite intermittent
    connectivity
  \item
    To simplify the deployment process
  \end{enumerate}
\item
  Explain how deployment requirements in rural clinics influence the
  choice of model optimization techniques.
\item
  Order the following steps in the deployment workflow: (1) Pilot site
  rollout, (2) Simulated environment testing, (3) Full-scale rollout.
\item
  What is a key challenge when integrating an ML system with existing
  hospital information systems (HIS)?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii})}
  \tightlist
  \item
    Maintaining secure data handling and compatibility
  \item
    Ensuring the model is interpretable
  \item
    Increasing the model's accuracy
  \item
    Reducing the model's training time
  \end{enumerate}
\end{enumerate}

\noindent\hspace*{1.25em}\hyperref[quiz-answer-sec-ai-development-workflow-deployment-integration-stage-d549]{\textbf{See Answer~$\rightarrow$}}

\end{fbx}

\section{Monitoring \& Maintenance
Stage}\label{sec-ai-development-workflow-monitoring-maintenance-stage-e79a}

Once AI systems transition from deployment to production operation, they
enter a fundamentally different operational phase than traditional
software systems. The feedback loop returning from the final stage back
to data collection demonstrates how monitoring and maintenance create
the continuous cycle that keeps systems performing reliably.
Conventional applications maintain static behavior until explicitly
updated, while ML systems must account for evolving data distributions,
changing usage patterns, and model performance drift.

Monitoring and maintenance are ongoing processes that keep deployed
machine learning systems effective and reliable. Unlike traditional
software, ML systems exhibit ``silent failures'' where performance
degrades without triggering obvious errors. Monitoring provides the
statistical telemetry necessary to detect these shifts, while
maintenance ensures the system evolves to meet new needs.
\textbf{?@sec-machine-learning-operations-mlops} builds upon these
operational practices as its foundation.

Monitoring serves as a central hub for system improvement, generating
three critical feedback loops: performance insights flow back to data
collection to address gaps, data quality issues trigger refinements in
data preparation, and model updates initiate retraining when performance
drifts. In our DR example, these feedback loops enable continuous system
improvement by identifying underrepresented patient demographics,
detecting image quality issues, and addressing model drift.

For DR screening systems, continuous monitoring tracks system
performance across diverse clinics, detecting issues such as changing
patient demographics or new imaging technologies that could impact
accuracy. Proactive maintenance includes plans to incorporate 3D imaging
modalities like OCT, expanding the system's capabilities to diagnose a
wider range of conditions. Such proactive planning keeps systems
adaptable to future challenges while maintaining compliance with
healthcare regulations and responsible AI principles.

\subsection{Production Monitoring for Dynamic
Systems}\label{sec-ai-development-workflow-production-monitoring-dynamic-systems-49ea}

The requirements for monitoring and maintenance emerge from both
technical needs and operational realities. In our DR example, monitoring
from a technical perspective requires continuous tracking of model
performance, data quality, and system resource usage. However,
operational constraints add layers of complexity: monitoring systems
must align with clinical workflows, detect shifts in patient
demographics, and provide actionable insights to both technical teams
and healthcare providers.

Initial deployment often highlights several areas where systems fail to
meet real-world needs. Clinics with older equipment or lower-resolution
imaging can show significant accuracy decreases. Monitoring systems
detect performance drops in specific subgroups, such as patients with
proliferative diabetic retinopathy or images complicated by cataracts in
elderly patients. These blind spots, invisible during laboratory
validation but critical in clinical practice\sidenote{\textbf{The
Lab-to-Clinic Performance Gap}: Medical AI systems typically see 10-30\%
performance drops when deployed in real-world settings, a phenomenon
known as the ``deployment reality gap.'' This occurs because training
data, despite best efforts, cannot capture the full diversity of
real-world conditions---different camera models, varying image quality,
diverse patient populations, and operator skill levels all contribute to
this gap. The gap is so consistent that regulatory bodies like the FDA
now require ``real-world performance studies'' for medical AI approval,
acknowledging that laboratory performance is insufficient to predict
clinical utility. }, inform maintenance strategies including targeted
data collection and architectural improvements.

These requirements influence system design significantly. The critical
nature of such systems demands real-time monitoring capabilities rather
than periodic offline evaluations. Teams establish quantitative
performance thresholds for latency, accuracy, and data distribution
stability. As detailed in
\textbf{?@sec-machine-learning-operations-mlops}, these metrics
(including Population Stability Index\sidenote{\textbf{Population
Stability Index (PSI)} and \textbf{Kolmogorov-Smirnov (KS) Test}:
Statistical methods for detecting distribution drift between training
and production data. PSI bins the feature distribution and computes
divergence: PSI \textless{} 0.1 indicates stable distributions, 0.1-0.2
suggests moderate drift, and \textgreater0.2 signals significant drift
requiring investigation. The KS test measures maximum distance between
cumulative distributions, providing a p-value for hypothesis testing.
Both are computationally lightweight (O(n) for PSI, O(n log n) for KS),
making them suitable for real-time monitoring.
\textbf{?@sec-machine-learning-operations-mlops} covers drift detection
pipelines in depth. } and Kolmogorov-Smirnov tests) trigger automated
responses ranging from on-call alerts to retraining workflows.

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-note-color-frame, colback=white, titlerule=0mm, bottomrule=.15mm, toprule=.15mm, opacitybacktitle=0.6, arc=.35mm, breakable, colbacktitle=quarto-callout-note-color!10!white, bottomtitle=1mm, rightrule=.15mm, toptitle=1mm, opacityback=0, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Monitoring Metrics in Practice}, coltitle=black, leftrule=.75mm, left=2mm]

A production DR system tracks several categories of metrics:

\textbf{Model Performance} (requires ground truth, available with delay)

\begin{itemize}
\tightlist
\item
  Sensitivity: Target \textgreater90\%, alert if 7-day rolling average
  drops below 88\%
\item
  Specificity: Target \textgreater80\%, alert if drops below 78\%
\item
  Subgroup performance: Alert if any demographic drops \textgreater5\%
  below baseline
\end{itemize}

\textbf{Proxy Metrics} (available immediately, no ground truth required)

\begin{itemize}
\tightlist
\item
  Prediction confidence distribution: Alert if mean confidence drops
  \textgreater10\%
\item
  Referral rate: Alert if rate changes \textgreater15\% from baseline
  (may indicate drift)
\item
  Image quality rejection rate: Alert if \textgreater20\% of images fail
  quality checks
\end{itemize}

\textbf{Operational Metrics}

\begin{itemize}
\tightlist
\item
  Inference latency: P95 \textless50ms, alert if \textgreater100ms
\item
  Throughput: Alert if queue depth \textgreater50 images
\item
  Error rate: Alert if \textgreater0.1\% of requests fail
\end{itemize}

\textbf{Data Drift Detection}

\begin{itemize}
\tightlist
\item
  Population Stability Index (PSI): Alert if \textgreater0.2
  (significant drift)
\item
  Feature distribution changes: Kolmogorov-Smirnov test, alert if
  p\textless0.01
\end{itemize}

The hierarchy matters: operational metrics catch immediate problems,
proxy metrics catch model issues within hours, performance metrics catch
accuracy degradation within weeks.

\end{tcolorbox}

Monitoring requirements also affect model design, as teams incorporate
mechanisms for granular performance tracking and anomaly detection. The
system's user interface must also present monitoring data clearly and
actionably for both clinical and technical staff.

\subsection{Maintenance at
Scale}\label{sec-ai-development-workflow-maintenance-scale-554c}

Model updates require careful validation and controlled rollouts. Teams
employ A/B testing frameworks to evaluate updates and implement rollback
mechanisms\sidenote{\textbf{Rollback Mechanisms}: ML rollbacks are more
complex than traditional software because model behavior depends on
current data distributions. Companies like Uber maintain shadow
deployments enabling instant rollbacks within 60 seconds
(\citeproc{ref-uber2017michelangelo}{Hermann and Del Balso 2017}). }
that address issues quickly. Unlike traditional software where CI/CD
handles changes deterministically, ML systems must account for data
evolution that affects behavior in ways traditional pipelines weren't
designed to handle.

Scaling from pilot sites to hundreds of clinics causes monitoring
complexity to grow rapidly. Each additional clinic generates operational
logs (inference times, quality metrics, error rates), creating data
volumes reaching hundreds of gigabytes per week. The monitoring
infrastructure must track both global metrics and site-specific
behaviors, maintain data lineage\sidenote{\textbf{Data Lineage}:
Complete record of data flow from source through transformations to
outputs, enabling traceability and regulatory compliance. Regulations
like GDPR ``right to explanation'' require organizations to trace how
data points influence ML decisions. } for regulatory compliance, and
correlate production issues with training experiments for root cause
analysis.

Proactive maintenance becomes essential: predictive models identify
potential problems from operational patterns, while continuous learning
pipelines retrain on new data. Production insights inform refined
problem definitions, data quality improvements, and architectural
enhancements --- the feedback arrows in Figure~\ref{fig-ml-lifecycle}
capture this dynamic, closing the loop that distinguishes ML systems
from traditional linear development.

\phantomsection\label{quiz-question-sec-ai-development-workflow-monitoring-maintenance-stage-e79a}
\begin{fbx}{callout-quiz-question}{Self-Check: Question 1.9}{}
\phantomsection\label{quiz-question-sec-ai-development-workflow-monitoring-maintenance-stage-e79a}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Which of the following best describes the primary purpose of
  monitoring in ML systems?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii})}
  \tightlist
  \item
    To maintain static behavior of the system.
  \item
    To eliminate the need for human oversight.
  \item
    To ensure deterministic outputs from the system.
  \item
    To detect and adapt to data and model drift.
  \end{enumerate}
\item
  Explain how data drift can impact the performance of a machine
  learning model in production.
\item
  Order the following steps in a typical ML maintenance workflow: (1)
  Model Retraining, (2) Data Drift Detection, (3) Performance
  Monitoring, (4) Feedback Loop Initiation.
\item
  What are the benefits of implementing proactive maintenance strategies
  in ML systems?
\end{enumerate}

\noindent\hspace*{1.25em}\hyperref[quiz-answer-sec-ai-development-workflow-monitoring-maintenance-stage-e79a]{\textbf{See Answer~$\rightarrow$}}

\end{fbx}

\section{Integrating Systems Thinking
Principles}\label{sec-ai-development-workflow-integrating-systems-thinking-principles-24c0}

Having traced the complete lifecycle from problem definition through
monitoring, we can now step back and identify the deeper patterns that
unified every stage. The DR case study was not just a sequence of
technical decisions; it revealed four fundamental systems thinking
principles that appeared repeatedly: when we discovered bandwidth
constraints drove edge deployment (constraint propagation), when we
designed monitoring across minute-to-quarterly timescales (multi-scale
feedback), when system-wide demographic biases emerged invisible to
single-site monitoring (emergent complexity), and when accuracy
improvements created cascading deployment costs (resource optimization).

These patterns distinguish successful AI projects from those that
struggle with integration challenges. The DR example demonstrates that
building effective machine learning systems requires more than technical
excellence; it demands understanding how technical decisions create
interdependencies that cascade throughout the entire development and
deployment process.

Four systems thinking patterns emerge from our analysis. Constraint
propagation shows how early decisions shape later stages, while
multi-scale feedback reveals how systems adapt across different
timescales. Emergent complexity explains how system-wide behaviors
differ from component behaviors, and resource optimization demonstrates
how trade-offs create interdependencies across the system. Together,
these patterns provide the analytical framework for understanding how
the technical chapters ahead interconnect, revealing why specialized
approaches to data engineering, frameworks, training, and operations
collectively enable integrated systems that individual optimizations
cannot achieve.

\subsection{How Decisions Cascade Through the
System}\label{sec-ai-development-workflow-decisions-cascade-system-98f4}

Constraint propagation represents the most fundamental systems thinking
pattern in ML development, as early decisions create cascading effects
that shape every subsequent stage. Our DR example illustrates this
pattern clearly: regulatory requirements for high sensitivity drive data
collection strategies that require expert consensus labeling, which in
turn influences model architecture choices. These architectural
decisions determine deployment constraints, and edge optimization shapes
monitoring approaches that require distributed performance tracking
across the entire clinic network.

\phantomsection\label{callout-definitionux2a-1.8}
\begin{fbx}{callout-definition}{Definition: }{The Constraint Propagation Principle}
\phantomsection\label{callout-definition*-1.8}
The \textbf{cost of a constraint discovered at lifecycle stage \(N\)}
grows exponentially relative to the stage where it should have been
defined:

\[\text{Correction Cost} \approx 2^{(N-1)} \times \text{Base Effort}\]

where \(N\) is the stage number (Problem Definition = 1, Data Collection
= 2, \ldots, Monitoring = 6).

\begin{itemize}
\tightlist
\item
  \textbf{Stage 1 Discovery}: Incorporating a ``Low Power'' constraint
  into the Problem Definition is a text change. (Cost: \(1\times\))
\item
  \textbf{Stage 5 Discovery}: Discovering the same constraint during
  Deployment means the model you built is too big, the data you
  collected is too high-res, and your evaluation metrics were wrong. You
  must redo Stages 2, 3, and 4. (Cost: \(16\times\))
\end{itemize}

This exponential relationship explains why deployment paradigm selection
(Cloud vs.~Edge) must happen at \textbf{Day 1}, not Day 100.

\end{fbx}

This propagation operates bidirectionally, creating dynamic constraint
networks rather than linear dependencies. When rural clinic deployment
reveals tight bandwidth limitations, teams must redesign data
preprocessing pipelines to reduce transmitted data by large factors.
This requires model architectures optimized for compressed inputs, which
influences training strategies that account for data degradation.
Understanding these cascading relationships enables teams to make
architectural decisions that accommodate rather than fight against
systemic constraints.

The Constraint Propagation Principle quantifies what experienced ML
engineers know intuitively: decisions made in ignorance of downstream
constraints create compounding technical debt\sidenote{\textbf{ML
Technical Debt}: A concept from Sculley et al.'s influential 2015 paper
``Hidden Technical Debt in Machine Learning Systems''
(\citeproc{ref-sculley2015hidden}{Sculley et al. 2021}), which
identified that ML systems accumulate debt faster than traditional
software due to entanglement (changing one feature affects all others),
hidden feedback loops (model predictions influence future training
data), and undeclared consumers (downstream systems depending on model
outputs without explicit contracts). The paper found that ML code often
represents less than 5\% of a production ML system, with configuration,
data pipelines, and serving infrastructure dominating complexity.
\textbf{?@sec-machine-learning-operations-mlops} addresses debt
management strategies. }. The stage interface specification
(Table~\ref{tbl-stage-interface}) operationalizes this principle by
making constraints explicit at each stage boundary, enabling early
detection before propagation costs escalate.

\subsection{Orchestrating Feedback Across Multiple
Timescales}\label{sec-ai-development-workflow-orchestrating-feedback-across-multiple-timescales-4e0b}

ML systems succeed through orchestrating feedback loops across multiple
timescales, each serving different system optimization purposes. Our DR
deployment exemplifies this pattern. Minute-level loops handle real-time
quality checks and automated image validation. Daily loops monitor model
performance across distributed deployments. Weekly loops perform
aggregated accuracy analysis and drift detection. Monthly loops assess
demographic bias and review hardware performance. Quarterly loops
evaluate architecture and plan capacity for new regions.

The temporal structure of these feedback loops reflects the inherent
dynamics of ML systems. Rapid loops enable quick correction of
operational issues---a clinic's misconfigured camera can be detected and
corrected within minutes. Slower loops enable strategic
adaptation---recognizing that population demographic shifts require
expanded training data takes months of monitoring to detect reliably.
This multi-scale approach prevents both reactionary changes
(over-responding to daily fluctuations) and sluggish adaptation
(under-responding to meaningful trends).

\subsection{Emergent Complexity and Resource
Trade-offs}\label{sec-ai-development-workflow-emergent-complexity-resource-tradeoffs-14c9}

Complex systems exhibit emergent behaviors invisible when analyzing
individual components. In our DR deployment, individual clinics show
stable performance, yet system-wide analysis detects subtle degradation
affecting specific demographic groups --- patterns invisible in
single-site monitoring but critical for equitable delivery. ML systems
exhibit \emph{probabilistic} degradation through data drift and bias
amplification, unlike traditional distributed systems that fail through
\emph{deterministic} cascades like server crashes.

Resource optimization involves multi-dimensional trade-offs absent in
traditional software. Small accuracy improvements can require materially
larger models, forcing deployment onto more powerful hardware; when
multiplied across fleets, incremental costs become significant relative
to marginal gains. These trade-offs manifest the \textbf{power wall} and
\textbf{memory wall} from \textbf{?@sec-ml-system-architecture}: edge
deployment reduces latency but constrains model complexity; cloud
deployment enables flexibility but introduces network latency that may
violate workflow requirements. Understanding these non-linear
relationships enables strategic architectural decisions rather than
isolated component optimization.

\subsection{Engineering Discipline for ML
Systems}\label{sec-ai-development-workflow-engineering-discipline-ml-systems-8a55}

These four systems thinking patterns---constraint propagation,
multi-scale feedback, emergent complexity, and resource
optimization---converge to define a fundamentally different approach to
engineering machine learning systems. Unlike traditional software where
components can be optimized independently, ML systems demand integrated
optimization that accounts for cross-component dependencies, temporal
dynamics, and resource constraints simultaneously.

The DR case study demonstrates that this integrated approach yields
systems that are more robust, adaptive, and effective than those
developed through sequential optimization of individual stages. When
teams design data collection strategies that anticipate deployment
constraints, create model architectures that accommodate operational
realities, and implement monitoring systems that drive continuous
improvement, they achieve performance levels that isolated optimization
approaches cannot reach. This systematic integration represents the core
engineering discipline that transforms machine learning from
experimental technique into reliable system engineering practice.

\phantomsection\label{quiz-question-sec-ai-development-workflow-integrating-systems-thinking-principles-24c0}
\begin{fbx}{callout-quiz-question}{Self-Check: Question 1.10}{}
\phantomsection\label{quiz-question-sec-ai-development-workflow-integrating-systems-thinking-principles-24c0}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Which of the following best illustrates the concept of constraint
  propagation in AI development?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii})}
  \tightlist
  \item
    Using high-capacity networks to improve model accuracy.
  \item
    Deploying models on edge devices to reduce latency.
  \item
    Adjusting data preprocessing pipelines due to bandwidth limitations.
  \item
    Increasing model size to enhance performance.
  \end{enumerate}
\item
  Explain how multi-scale feedback loops contribute to the robustness of
  an AI system.
\item
  In managing emergent complexity, what is a key difference between ML
  systems and traditional software systems?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii})}
  \tightlist
  \item
    ML systems require monitoring for data drift and model bias.
  \item
    Traditional systems exhibit probabilistic degradation.
  \item
    ML systems rely on deterministic processes.
  \item
    Traditional systems focus on hardware performance.
  \end{enumerate}
\item
  Discuss the trade-offs involved in resource optimization for ML
  systems, using the DR case study as an example.
\end{enumerate}

\noindent\hspace*{1.25em}\hyperref[quiz-answer-sec-ai-development-workflow-integrating-systems-thinking-principles-24c0]{\textbf{See Answer~$\rightarrow$}}

\end{fbx}

\section{Fallacies and
Pitfalls}\label{sec-ai-development-workflow-fallacies-pitfalls-4d91}

Machine learning workflows introduce counterintuitive complexities that
lead teams to apply familiar software development patterns to
fundamentally different problems. These fallacies and pitfalls capture
errors that waste development cycles, cause production failures, and
create technical debt that compounds as systems scale.

\textbf{Fallacy:} \emph{ML development can follow traditional software
workflows without modification.}

Engineers assume waterfall or standard agile processes will work for ML
projects. In production, ML replaces deterministic specifications with
probabilistic optimization, static behavior with dynamic adaptation, and
isolated development with continuous feedback loops
(Table~\ref{tbl-sw-ml-cycles}). Traditional approaches treat
requirements as fixed and testing as binary pass/fail, but ML systems
require iterative experimentation where problem definitions evolve
through exploration. ML projects fail at 2-3× the rate of traditional
software, with 60-80\% never reaching deployment. Projects forced into
rigid phase gates miss the 4-8 iteration cycles that production-ready
systems require. Organizations that adapt workflows to accommodate ML's
experimental nature report 40-60\% shorter time-to-deployment.

\textbf{Pitfall:} \emph{Treating data preparation as a one-time
preprocessing step.}

Teams assume they can ``finish'' data preparation and move on to
modeling. In production, data distributions shift continuously. The
two-pipeline architecture (Figure~\ref{fig-ml-lifecycle}) shows data and
model pipelines run in parallel with continuous feedback, not
sequentially. Data quality decisions cascade through model training,
validation, and deployment---data issues compound rather than isolate.
Data quality issues account for 60-80\% of production ML failures.
Recommendation systems see 10-15\% of features requiring updates
monthly. Models degrade 5-10\% within months as distributions shift,
requiring emergency retraining that costs 3-5× more than proactive
monitoring. Organizations that build continuous data validation
pipelines from the start detect drift within days rather than months,
maintaining accuracy within 2-3\% of development baselines.

\textbf{Fallacy:} \emph{Development performance accurately predicts
production performance.}

Engineers assume 95\% test set accuracy guarantees production success.
In deployment, curated development data differs from production quality
variations. Development assumes fixed resources while production faces
latency constraints and contention. Development optimizes for aggregate
metrics while production must handle long-tail edge cases
(Section~\ref{sec-ai-development-workflow-case-study-diabetic-retinopathy-screening-7d71}).
Medical AI sees 10-30\% performance drops in deployment. Recommendation
systems see 15-25\% metric degradation. Computer vision models suffer
5-15\% accuracy loss. A model achieving F-score 0.95 on research-quality
images drops to 0.75-0.85 in real clinics. Effective teams validate
against production-representative data, including equipment variations,
operator skill levels, and demographic diversity, planning for 3-5×
input quality range.

\textbf{Pitfall:} \emph{Skipping validation stages to accelerate
timelines.}

Teams assume cutting validation time ships faster. In production, the
multi-stage validation process
(Section~\ref{sec-ai-development-workflow-evaluation-validation-stage-b47d})
exists because each stage catches different failure modes. Skipping
shadow mode testing causes integration issues (10-50× latency spikes).
Bypassing canary deployment leads to incidents affecting millions of
users. Post-deployment fixes cost 10-100× more than validation.
Inadequate validation extends time-to-production by 2-5 months through
unplanned remediation. A team that ``saves'' 2 weeks by skipping
validation spends 6-8 weeks on emergency remediation. Organizations that
invest in systematic validation infrastructure reduce production
incidents by 60-80\% and achieve 70-85\% first-deployment success
(vs.~30-40\% without).

\textbf{Pitfall:} \emph{Deferring deployment paradigm selection until
after model development.}

Teams assume they can ``figure out deployment later'' and focus first on
model accuracy. In production, deployment paradigm (Cloud, Edge, Mobile,
TinyML) is not a late-stage detail---it's a fundamental constraint
shaping every preceding stage (Table~\ref{tbl-stage-interface}). A team
that develops a 2 GB ensemble model discovers their target is TinyML
with 256 KB memory. The resulting cascade requires revisiting Data
Collection, Model Development, and Evaluation. By the Constraint
Propagation Principle, a stage-5 discovery costs \(2^{5-1} = 16×\) the
effort of incorporating the constraint at stage 1. Teams that defer
paradigm selection report 2-4 additional iteration cycles and 3-6 month
delays. The paradigm is not where you deploy---it is what you can build.

\phantomsection\label{quiz-question-sec-ai-development-workflow-fallacies-pitfalls-4d91}
\begin{fbx}{callout-quiz-question}{Self-Check: Question 1.11}{}
\phantomsection\label{quiz-question-sec-ai-development-workflow-fallacies-pitfalls-4d91}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  True or False: Machine learning development can effectively follow
  traditional software engineering workflows without any modifications.
\item
  Which of the following is a common pitfall in ML development?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii})}
  \tightlist
  \item
    Using agile methodologies for iterative development.
  \item
    Treating data preparation as a one-time preprocessing step.
  \item
    Incorporating feedback loops in the ML lifecycle.
  \item
    Ensuring continuous data validation and monitoring.
  \end{enumerate}
\item
  Explain why model performance in development environments may not
  accurately predict production performance.
\item
  The belief that achieving good metrics during development ensures
  successful deployment is a common \_\_\_\_. This assumption overlooks
  the differences between development and production environments.
\item
  In a production system, how might you address the pitfall of skipping
  systematic validation stages to accelerate development timelines?
\end{enumerate}

\noindent\hspace*{1.25em}\hyperref[quiz-answer-sec-ai-development-workflow-fallacies-pitfalls-4d91]{\textbf{See Answer~$\rightarrow$}}

\end{fbx}

\section{Summary}\label{sec-ai-development-workflow-summary-fb13}

This chapter established the ML lifecycle as the systematic framework
for engineering machine learning systems, the mental roadmap organizing
how data, models, and deployment infrastructure interconnect throughout
development. Figure~\ref{fig-ml-lifecycle} captures this framework
through two parallel pipelines. The data pipeline transforms raw inputs
through collection, ingestion, analysis, labeling, validation, and
preparation into ML-ready datasets. The model development pipeline takes
these datasets through training, evaluation, validation, and deployment
to create production systems. Their interconnections reveal the
distinctive nature of ML development. The feedback arrows show how
deployment insights trigger data refinements, creating the continuous
improvement cycles that distinguish ML from traditional linear
development.

Understanding this framework explains why machine learning systems
demand specialized approaches distinct from traditional software. ML
workflows replace deterministic specifications with probabilistic
optimization, static behavior with dynamic adaptation, and isolated
development with continuous feedback loops. This systematic perspective
recognizes that success emerges not from perfecting individual stages in
isolation, but from understanding how data quality affects model
performance, how deployment constraints shape training strategies, and
how production insights inform each subsequent development iteration.

Three quantitative insights from this chapter should guide your
engineering decisions:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{60-80\%}: The proportion of project time consumed by
  data-related activities. Model development, despite receiving the most
  attention, represents only 10-20\% of effort. Plan accordingly.
\item
  \textbf{4-8 iteration cycles}: The number of complete cycles
  production-ready ML systems typically require. Of these iterations,
  60\% are driven by data quality issues, 25\% by architecture choices,
  and 15\% by infrastructure problems. Investment in data engineering
  yields the highest returns.
\item
  \textbf{\(2^{N-1}\) cost escalation}: The Constraint Propagation
  Principle---a constraint discovered at stage \(N\) costs roughly
  \(2^{N-1}\) times more to fix than if caught at stage 1. A deployment
  paradigm mismatch discovered at stage 5 triggers a 16× cost
  multiplier. Early validation pays exponential dividends.
\end{enumerate}

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-important-color-frame, colback=white, titlerule=0mm, bottomrule=.15mm, toprule=.15mm, opacitybacktitle=0.6, arc=.35mm, breakable, colbacktitle=quarto-callout-important-color!10!white, bottomtitle=1mm, rightrule=.15mm, toptitle=1mm, opacityback=0, title=\textcolor{quarto-callout-important-color}{\faExclamation}\hspace{0.5em}{Key Takeaways}, coltitle=black, leftrule=.75mm, left=2mm]

\begin{itemize}
\item
  \textbf{Two pipelines, one system}: Data processing (collection →
  preparation) and model development (training → deployment) run in
  parallel, unified by continuous feedback loops.
\item
  \textbf{Iteration speed significantly influences success}: Teams that
  reduce experiment cycles from days to hours compound learning
  advantages. Successful teams prioritize iteration velocity alongside
  model accuracy.
\item
  \textbf{Stage interfaces are contracts}: Explicit inputs, outputs, and
  quality invariants at each stage help prevent the 60--70\% of ML
  project failures caused by integration problems.
\item
  \textbf{Feedback loops span multiple timescales}: Real-time inference
  monitoring (seconds), batch retraining triggers (days), and strategic
  model updates (months) all require distinct automation.
\item
  \textbf{Constraint propagation is bidirectional}: Deployment
  constraints (latency, memory) flow backward to model selection; data
  constraints (volume, quality) flow forward to architecture choices.
\end{itemize}

\end{tcolorbox}

The workflow framework established here provides the organizing
structure for Part II's technical chapters. We now have the blueprint
for the engine. But an engine without fuel is just a heavy block of
metal. \textbf{How do we fuel this engine?} We begin with the foundation
of the AI Triad: \textbf{?@sec-data-engineering-ml}. Each subsequent
chapter assumes you understand where its specific techniques fit within
this complete workflow, building upon the systematic perspective
developed here.

\phantomsection\label{quiz-question-sec-ai-development-workflow-summary-fb13}
\begin{fbx}{callout-quiz-question}{Self-Check: Question 1.12}{}
\phantomsection\label{quiz-question-sec-ai-development-workflow-summary-fb13}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Which of the following best describes the role of feedback loops in
  the ML lifecycle?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii})}
  \tightlist
  \item
    They enable continuous improvement by refining data and model
    performance.
  \item
    They provide a mechanism for error correction in static systems.
  \item
    They are used to validate models before deployment.
  \item
    They ensure that the ML lifecycle is a linear process.
  \end{enumerate}
\item
  Explain how the interconnection between data and model pipelines
  contributes to the success of machine learning systems.
\item
  Order the following stages of the ML lifecycle from data collection to
  deployment: (1) Model Training, (2) Data Preparation, (3) Model
  Evaluation, (4) Data Collection, (5) Deployment.
\item
  True or False: The ML lifecycle is characterized by deterministic
  specifications and static behavior.
\end{enumerate}

\noindent\hspace*{1.25em}\hyperref[quiz-answer-sec-ai-development-workflow-summary-fb13]{\textbf{See Answer~$\rightarrow$}}

\end{fbx}

\section{Self-Check Answers}\label{self-check-answers}

\phantomsection\label{quiz-answer-sec-ai-development-workflow-systematic-framework-ml-development-7a1b}
\begin{fbx}{callout-quiz-answer}{Self-Check: Answer 1.1}{}
\phantomsection\label{quiz-answer-sec-ai-development-workflow-systematic-framework-ml-development-7a1b}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{How does the machine learning workflow differ from traditional
  software engineering processes?}

  \begin{enumerate}
  \def\labelenumii{\alph{enumii})}
  \tightlist
  \item
    ML workflow is iterative and data-centric, involving experimentation
    and empirical validation.
  \item
    ML workflow is deterministic and follows a strict
    requirement-to-implementation path.
  \item
    ML workflow does not involve any feedback mechanisms.
  \item
    ML workflow is identical to traditional software engineering.
  \end{enumerate}

  \emph{Answer}: The correct answer is A. ML workflow is iterative and
  data-centric, involving experimentation and empirical validation.
  Traditional software engineering is more deterministic, whereas ML
  systems evolve through iterative experimentation and data-driven
  insights.

  \emph{Learning Objective}: Understand the fundamental differences
  between ML workflows and traditional software engineering processes.
\item
  \textbf{Why is iterative experimentation crucial in the development of
  machine learning systems?}

  \emph{Answer}: Iterative experimentation is crucial because it allows
  ML systems to evolve by continuously testing and validating models
  against data, refining them based on performance metrics. This process
  accommodates uncertainty and enables the system to adapt to new data
  and deployment constraints, ensuring robust performance in real-world
  scenarios.

  \emph{Learning Objective}: Explain the importance of iterative
  experimentation in ML system development.
\item
  \textbf{What role do feedback mechanisms play in the ML system
  development workflow?}

  \begin{enumerate}
  \def\labelenumii{\alph{enumii})}
  \tightlist
  \item
    They are unnecessary as ML systems are static once deployed.
  \item
    They are used to finalize the initial model without further changes.
  \item
    They only apply to traditional software engineering.
  \item
    They inform earlier development phases and help refine models.
  \end{enumerate}

  \emph{Answer}: The correct answer is D. They inform earlier
  development phases and help refine models. Feedback mechanisms are
  essential in ML workflows as they provide insights that guide
  iterative improvements and model adjustments.

  \emph{Learning Objective}: Understand the function of feedback
  mechanisms in ML system workflows.
\item
  \textbf{How does the diabetic retinopathy screening system case study
  illustrate the iterative workflow principles and data-driven decision
  making discussed in this section?}

  \emph{Answer}: The diabetic retinopathy screening system illustrates
  iterative workflow principles by demonstrating how initial model
  development leads to discoveries about operational constraints (like
  hardware limitations in rural clinics), which then inform data
  collection strategies and model optimization decisions. This cycle of
  experimentation, validation, and refinement exemplifies the
  data-driven, empirical nature of ML workflows.

  \emph{Learning Objective}: Apply workflow principles to a real-world
  ML system case study.
\end{enumerate}

\noindent\hspace*{1.25em}\hyperref[quiz-question-sec-ai-development-workflow-systematic-framework-ml-development-7a1b]{\textbf{$\leftarrow$~Back to Question}}

\end{fbx}

\phantomsection\label{quiz-answer-sec-ai-development-workflow-understanding-ml-lifecycle-ca87}
\begin{fbx}{callout-quiz-answer}{Self-Check: Answer 1.2}{}
\phantomsection\label{quiz-answer-sec-ai-development-workflow-understanding-ml-lifecycle-ca87}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Which of the following best describes the role of feedback
  loops in the ML lifecycle?}

  \begin{enumerate}
  \def\labelenumii{\alph{enumii})}
  \tightlist
  \item
    They ensure that each stage of the lifecycle is completed before
    moving to the next.
  \item
    They are used to validate the final model before deployment.
  \item
    They allow for continuous improvement by informing earlier stages
    with insights from later stages.
  \item
    They help in maintaining a linear development process.
  \end{enumerate}

  \emph{Answer}: The correct answer is C. They allow for continuous
  improvement by informing earlier stages with insights from later
  stages. This iterative process is crucial for adapting to real-world
  conditions and improving system performance.

  \emph{Learning Objective}: Understand the role of feedback loops in
  the ML lifecycle and their impact on continuous improvement.
\item
  \textbf{Explain how systems thinking applies to the machine learning
  lifecycle and why it is important.}

  \emph{Answer}: Systems thinking in the ML lifecycle involves
  understanding how different stages interrelate and influence each
  other. This approach is important because it ensures that changes in
  one part of the system, such as data quality, are considered in the
  context of the entire lifecycle, leading to more robust and adaptable
  ML systems. For example, improving data quality can enhance model
  performance, which in turn affects deployment strategies.

  \emph{Learning Objective}: Apply systems thinking to the ML lifecycle
  to understand interdependencies and their implications.
\item
  \textbf{Order the following stages of the ML lifecycle from data
  collection to deployment: (1) Model Training, (2) Data Preparation,
  (3) Model Evaluation, (4) Data Collection, (5) ML System Deployment.}

  \emph{Answer}: The correct order is: (4) Data Collection, (2) Data
  Preparation, (1) Model Training, (3) Model Evaluation, (5) ML System
  Deployment. This sequence represents the flow from gathering raw data
  to deploying a validated ML system.

  \emph{Learning Objective}: Understand the sequential flow of stages in
  the ML lifecycle from data collection to deployment.
\item
  \textbf{True or False: The ML lifecycle is a linear process where each
  stage is independent of the others.}

  \emph{Answer}: False. The ML lifecycle is not linear; it is an
  iterative process where each stage is interconnected, and feedback
  from later stages can influence earlier ones.

  \emph{Learning Objective}: Recognize the iterative and interconnected
  nature of the ML lifecycle.
\end{enumerate}

\noindent\hspace*{1.25em}\hyperref[quiz-question-sec-ai-development-workflow-understanding-ml-lifecycle-ca87]{\textbf{$\leftarrow$~Back to Question}}

\end{fbx}

\phantomsection\label{quiz-answer-sec-ai-development-workflow-ml-vs-traditional-software-development-c5f5}
\begin{fbx}{callout-quiz-answer}{Self-Check: Answer 1.3}{}
\phantomsection\label{quiz-answer-sec-ai-development-workflow-ml-vs-traditional-software-development-c5f5}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Which of the following best describes a key difference between
  traditional software development and machine learning development?}

  \begin{enumerate}
  \def\labelenumii{\alph{enumii})}
  \tightlist
  \item
    Traditional software development follows a linear progression with
    predefined specifications, whereas ML development involves iterative
    experimentation and evolving objectives.
  \item
    ML development relies on deterministic specifications, while
    traditional development is probabilistic.
  \item
    Traditional software development is iterative, while ML development
    is linear.
  \item
    ML development does not require feedback loops, unlike traditional
    software development.
  \end{enumerate}

  \emph{Answer}: The correct answer is A. Traditional software
  development follows a linear progression with predefined
  specifications, whereas ML development involves iterative
  experimentation and evolving objectives. This is correct because
  traditional methods rely on fixed requirements, while ML adapts to
  data-driven insights.

  \emph{Learning Objective}: Understand the fundamental differences in
  development approaches between traditional software and ML systems.
\item
  \textbf{Explain why continuous feedback loops are crucial in the
  machine learning development lifecycle.}

  \emph{Answer}: Continuous feedback loops are crucial in ML development
  because they allow insights from deployment to refine earlier stages
  such as data preparation and model design. For example, performance
  metrics from a deployed model can highlight areas for improvement in
  feature engineering. This is important because ML systems must adapt
  to changing data distributions and objectives, unlike traditional
  software.

  \emph{Learning Objective}: Analyze the role of feedback loops in
  adapting ML systems to dynamic environments.
\item
  \textbf{Order the following dimensions of development lifecycle
  differences between traditional software and ML systems: (1)
  Deployment, (2) Testing and Validation, (3) Feedback Loops.}

  \emph{Answer}: The correct order is: (2) Testing and Validation, (1)
  Deployment, (3) Feedback Loops. Testing in traditional software is
  deterministic, while ML requires statistical validation. Deployment in
  traditional systems is static, whereas ML systems adapt over time.
  Feedback loops are minimal in traditional development but frequent in
  ML to refine earlier stages.

  \emph{Learning Objective}: Understand the sequence and interaction of
  lifecycle dimensions in ML versus traditional software development.
\end{enumerate}

\noindent\hspace*{1.25em}\hyperref[quiz-question-sec-ai-development-workflow-ml-vs-traditional-software-development-c5f5]{\textbf{$\leftarrow$~Back to Question}}

\end{fbx}

\phantomsection\label{quiz-answer-sec-ai-development-workflow-six-core-lifecycle-stages-00b0}
\begin{fbx}{callout-quiz-answer}{Self-Check: Answer 1.4}{}
\phantomsection\label{quiz-answer-sec-ai-development-workflow-six-core-lifecycle-stages-00b0}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Which of the following best describes the purpose of the
  `Problem Definition' stage in the ML lifecycle?}

  \begin{enumerate}
  \def\labelenumii{\alph{enumii})}
  \tightlist
  \item
    To define objectives and constraints for the ML system.
  \item
    To gather and clean data for model training.
  \item
    To deploy the model into production environments.
  \item
    To monitor the system's performance post-deployment.
  \end{enumerate}

  \emph{Answer}: The correct answer is A. To define objectives and
  constraints for the ML system. This stage sets the foundation for all
  subsequent work by ensuring alignment between the system's goals and
  desired outcomes. Other options describe different stages of the
  lifecycle.

  \emph{Learning Objective}: Understand the role and importance of the
  `Problem Definition' stage in the ML lifecycle.
\item
  \textbf{Order the following ML lifecycle stages from start to finish:
  (1) Deployment \& Integration, (2) Model Development \& Training, (3)
  Data Collection \& Preparation, (4) Monitoring \& Maintenance.}

  \emph{Answer}: The correct order is: (3) Data Collection \&
  Preparation, (2) Model Development \& Training, (1) Deployment \&
  Integration, (4) Monitoring \& Maintenance. This sequence reflects the
  progression from data preparation to model training, deployment, and
  ongoing maintenance.

  \emph{Learning Objective}: Understand the sequential order of ML
  lifecycle stages and their interdependencies.
\item
  \textbf{How does the feedback loop in the ML lifecycle contribute to
  the system's adaptability and improvement?}

  \emph{Answer}: The feedback loop allows insights from later stages,
  such as Monitoring \& Maintenance, to inform refinements in earlier
  stages like Data Collection \& Preparation. For example, if monitoring
  reveals performance issues, data preprocessing can be adjusted to
  improve model accuracy. This is important because it enables the
  system to adapt to changing requirements and data distributions.

  \emph{Learning Objective}: Analyze the role of feedback loops in
  enhancing the adaptability and continuous improvement of ML systems.
\item
  \textbf{In the context of the DR screening system, which lifecycle
  stage likely involves ensuring model performance in real-world
  conditions?}

  \begin{enumerate}
  \def\labelenumii{\alph{enumii})}
  \tightlist
  \item
    Problem Definition
  \item
    Data Collection \& Preparation
  \item
    Evaluation \& Validation
  \item
    Monitoring \& Maintenance
  \end{enumerate}

  \emph{Answer}: The correct answer is C. Evaluation \& Validation. This
  stage involves testing the model's performance against predefined
  metrics and validating its behavior in different scenarios to ensure
  it is accurate and robust in real-world conditions.

  \emph{Learning Objective}: Connect lifecycle stages to practical
  applications in real-world ML systems, such as the DR screening
  system.
\end{enumerate}

\noindent\hspace*{1.25em}\hyperref[quiz-question-sec-ai-development-workflow-six-core-lifecycle-stages-00b0]{\textbf{$\leftarrow$~Back to Question}}

\end{fbx}

\phantomsection\label{quiz-answer-sec-ai-development-workflow-problem-definition-stage-5974}
\begin{fbx}{callout-quiz-answer}{Self-Check: Answer 1.5}{}
\phantomsection\label{quiz-answer-sec-ai-development-workflow-problem-definition-stage-5974}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{How does problem definition in machine learning differ from
  traditional software development?}

  \begin{enumerate}
  \def\labelenumii{\alph{enumii})}
  \tightlist
  \item
    It involves defining how the system should learn from data.
  \item
    It focuses solely on deterministic specifications.
  \item
    It requires no consideration of real-world constraints.
  \item
    It is based on fixed input-output rules.
  \end{enumerate}

  \emph{Answer}: The correct answer is A. It involves defining how the
  system should learn from data. ML problem definition requires
  understanding how a system learns from data, unlike traditional
  software which relies on deterministic rules.

  \emph{Learning Objective}: Understand the fundamental differences
  between ML and traditional software problem definitions.
\item
  \textbf{Why is it crucial to align problem definition with real-world
  constraints in ML system development?}

  \emph{Answer}: Aligning problem definition with real-world constraints
  ensures the system is practical and effective in its deployment
  environment. For example, a diabetic retinopathy screening system must
  consider diagnostic accuracy, hardware limitations, and regulatory
  compliance. This alignment is important because it influences data
  collection, model design, and deployment strategies.

  \emph{Learning Objective}: Explain the importance of considering
  real-world constraints in the problem definition of ML systems.
\item
  \textbf{In ML systems, the process of translating business objectives
  into learning objectives is known as \_\_\_\_.}

  \emph{Answer}: problem formulation. This process is crucial in
  defining how a system will learn and achieve business goals.

  \emph{Learning Objective}: Recall the term for translating business
  objectives into learning objectives in ML systems.
\item
  \textbf{Which of the following best describes a key challenge in
  scaling ML systems?}

  \begin{enumerate}
  \def\labelenumii{\alph{enumii})}
  \tightlist
  \item
    Data homogeneity across all environments.
  \item
    Consistent model performance without additional tuning.
  \item
    Data heterogeneity and infrastructure requirements.
  \item
    Simplified monitoring infrastructure compared to traditional
    applications.
  \end{enumerate}

  \emph{Answer}: The correct answer is C. Data heterogeneity and
  infrastructure requirements. Scaling ML systems involves managing
  diverse data and complex infrastructure, unlike traditional software.

  \emph{Learning Objective}: Identify challenges specific to scaling ML
  systems compared to traditional software.
\item
  \textbf{In a production system, how might problem definition influence
  the choice of deployment infrastructure?}

  \emph{Answer}: Problem definition influences deployment infrastructure
  by dictating requirements such as computational efficiency and
  reliability. For instance, a DR screening system in rural clinics must
  operate on limited hardware and intermittent internet. This is
  important because it ensures the system is feasible and effective in
  its intended environment.

  \emph{Learning Objective}: Analyze how problem definition impacts
  deployment infrastructure decisions in ML systems.
\end{enumerate}

\noindent\hspace*{1.25em}\hyperref[quiz-question-sec-ai-development-workflow-problem-definition-stage-5974]{\textbf{$\leftarrow$~Back to Question}}

\end{fbx}

\phantomsection\label{quiz-answer-sec-ai-development-workflow-data-collection-preparation-stage-ae99}
\begin{fbx}{callout-quiz-answer}{Self-Check: Answer 1.6}{}
\phantomsection\label{quiz-answer-sec-ai-development-workflow-data-collection-preparation-stage-ae99}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Which of the following best describes a major challenge in
  data collection for medical AI systems like diabetic retinopathy
  screening?}

  \begin{enumerate}
  \def\labelenumii{\alph{enumii})}
  \tightlist
  \item
    Ensuring high-resolution images are captured consistently.
  \item
    Reducing the cost of expert annotation.
  \item
    Balancing statistical rigor with operational feasibility.
  \item
    Maximizing the number of images collected daily.
  \end{enumerate}

  \emph{Answer}: The correct answer is C. Balancing statistical rigor
  with operational feasibility. This is correct because medical AI
  systems require data that meets high standards for diagnostic accuracy
  while being practical to collect in real-world settings. Other options
  do not fully capture the dual challenge of rigor and feasibility.

  \emph{Learning Objective}: Understand the specific challenges of data
  collection in medical AI systems.
\item
  \textbf{How does the data volume constraint in rural clinics influence
  architectural decisions in ML systems?}

  \emph{Answer}: Data volume constraints in rural clinics necessitate
  edge-computing solutions to reduce bandwidth requirements. For
  example, local preprocessing can decrease weekly data transmission
  from 15 GB to 750 MB, but requires more local computational resources.
  This is important because it shapes the deployment strategy and
  hardware requirements.

  \emph{Learning Objective}: Analyze how infrastructure constraints
  drive architectural decisions in ML deployments.
\item
  \textbf{Order the following steps involved in the data collection
  process for a medical AI system: (1) Initial processing and storage,
  (2) Data capture, (3) Quality validation, (4) Secure transmission.}

  \emph{Answer}: The correct order is: (2) Data capture, (1) Initial
  processing and storage, (3) Quality validation, (4) Secure
  transmission. This sequence reflects the logical flow from capturing
  data to ensuring its quality and securely transmitting it for further
  use.

  \emph{Learning Objective}: Understand the sequential steps in the data
  collection workflow for medical AI systems.
\item
  \textbf{What is a key reason for using federated learning in the data
  collection strategy for medical AI systems?}

  \begin{enumerate}
  \def\labelenumii{\alph{enumii})}
  \tightlist
  \item
    To improve model accuracy by centralizing data.
  \item
    To comply with patient privacy regulations.
  \item
    To reduce the cost of data annotation.
  \item
    To increase the speed of data processing.
  \end{enumerate}

  \emph{Answer}: The correct answer is B. To comply with patient privacy
  regulations. This is correct because federated learning allows model
  training without centralizing sensitive patient data, which is crucial
  for meeting privacy requirements.

  \emph{Learning Objective}: Understand the role of federated learning
  in addressing privacy concerns in data collection.
\end{enumerate}

\noindent\hspace*{1.25em}\hyperref[quiz-question-sec-ai-development-workflow-data-collection-preparation-stage-ae99]{\textbf{$\leftarrow$~Back to Question}}

\end{fbx}

\phantomsection\label{quiz-answer-sec-ai-development-workflow-model-development-training-stage-d901}
\begin{fbx}{callout-quiz-answer}{Self-Check: Answer 1.7}{}
\phantomsection\label{quiz-answer-sec-ai-development-workflow-model-development-training-stage-d901}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Which of the following best describes the trade-off between
  model accuracy and deployment feasibility in the context of edge
  devices?}

  \begin{enumerate}
  \def\labelenumii{\alph{enumii})}
  \tightlist
  \item
    Increasing model accuracy always improves deployment feasibility.
  \item
    Model accuracy and deployment feasibility are unrelated aspects of
    model development.
  \item
    Deployment feasibility is independent of model accuracy.
  \item
    Higher model accuracy often requires more computational resources,
    which can hinder deployment on edge devices.
  \end{enumerate}

  \emph{Answer}: The correct answer is D. Higher model accuracy often
  requires more computational resources, which can hinder deployment on
  edge devices. This is because edge devices have limited computational
  capacity, and optimizing for accuracy alone can lead to models that
  are too large or slow for practical deployment.

  \emph{Learning Objective}: Understand the trade-offs between model
  accuracy and deployment feasibility in edge device scenarios.
\item
  \textbf{Explain how model compression techniques like quantization and
  pruning help in meeting deployment constraints for edge devices.}

  \emph{Answer}: Model compression techniques such as quantization and
  pruning reduce the size and computational requirements of models,
  making them suitable for deployment on resource-constrained edge
  devices. Quantization reduces numerical precision, while pruning
  removes unnecessary parameters, both of which help maintain
  performance while fitting within hardware limits. In practice, these
  techniques enable models to run efficiently without sacrificing
  significant accuracy, crucial for real-time applications.

  \emph{Learning Objective}: Explain the role of model compression
  techniques in optimizing models for edge deployment.
\item
  \textbf{The process of training a smaller model to mimic the behavior
  of a larger model is known as \_\_\_\_. This technique helps in
  reducing model size while maintaining accuracy.}

  \emph{Answer}: knowledge distillation. This technique helps in
  reducing model size while maintaining accuracy by transferring learned
  knowledge from a large `teacher' model to a smaller `student' model.

  \emph{Learning Objective}: Recall the concept and purpose of knowledge
  distillation in model development.
\item
  \textbf{Order the following steps in optimizing a model for edge
  deployment: (1) Initial model training, (2) Model compression, (3)
  Performance evaluation, (4) Deployment testing.}

  \emph{Answer}: The correct order is: (1) Initial model training, (3)
  Performance evaluation, (2) Model compression, (4) Deployment testing.
  Initially, the model is trained, then its performance is evaluated.
  Compression techniques are applied to meet deployment constraints,
  followed by testing to ensure the model functions correctly in the
  deployment environment.

  \emph{Learning Objective}: Understand the sequence of steps involved
  in optimizing a model for deployment on edge devices.
\item
  \textbf{In a production system, how might the choice of model
  architecture impact the system's operational constraints?}

  \emph{Answer}: The choice of model architecture directly affects the
  system's operational constraints such as computational load, memory
  usage, and latency. For example, a complex architecture might offer
  high accuracy but require more resources, making it unsuitable for
  edge devices. Conversely, a simpler architecture might meet
  operational constraints but at the cost of reduced accuracy. Balancing
  these aspects is crucial for effective deployment. This is important
  because operational constraints dictate the feasibility and efficiency
  of deploying models in real-world environments.

  \emph{Learning Objective}: Analyze the impact of model architecture
  choices on operational constraints in production systems.
\end{enumerate}

\noindent\hspace*{1.25em}\hyperref[quiz-question-sec-ai-development-workflow-model-development-training-stage-d901]{\textbf{$\leftarrow$~Back to Question}}

\end{fbx}

\phantomsection\label{quiz-answer-sec-ai-development-workflow-deployment-integration-stage-d549}
\begin{fbx}{callout-quiz-answer}{Self-Check: Answer 1.8}{}
\phantomsection\label{quiz-answer-sec-ai-development-workflow-deployment-integration-stage-d549}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Which of the following is a primary reason for choosing edge
  deployment over cloud deployment in rural clinics?}

  \begin{enumerate}
  \def\labelenumii{\alph{enumii})}
  \tightlist
  \item
    To increase model complexity
  \item
    To leverage cloud computing resources
  \item
    To reduce latency and ensure reliability despite intermittent
    connectivity
  \item
    To simplify the deployment process
  \end{enumerate}

  \emph{Answer}: The correct answer is C. To reduce latency and ensure
  reliability despite intermittent connectivity. Edge deployment allows
  models to run locally, which is crucial in environments with
  unreliable internet connectivity, ensuring timely and reliable model
  predictions.

  \emph{Learning Objective}: Understand the trade-offs between edge and
  cloud deployment in specific environments.
\item
  \textbf{Explain how deployment requirements in rural clinics influence
  the choice of model optimization techniques.}

  \emph{Answer}: Deployment in rural clinics requires models to be
  optimized for limited computational resources and intermittent
  connectivity. Techniques like model quantization and pruning reduce
  model size and computational load, ensuring that the model fits within
  hardware constraints while maintaining performance. This is important
  because it allows the model to operate effectively in
  resource-constrained environments.

  \emph{Learning Objective}: Analyze how environmental constraints
  dictate model optimization strategies.
\item
  \textbf{Order the following steps in the deployment workflow: (1)
  Pilot site rollout, (2) Simulated environment testing, (3) Full-scale
  rollout.}

  \emph{Answer}: The correct order is: (2) Simulated environment
  testing, (1) Pilot site rollout, (3) Full-scale rollout. Simulated
  testing helps identify potential issues, pilot rollouts provide
  real-world feedback, and full-scale rollout ensures widespread
  implementation.

  \emph{Learning Objective}: Understand the sequential steps involved in
  deploying a model to production.
\item
  \textbf{What is a key challenge when integrating an ML system with
  existing hospital information systems (HIS)?}

  \begin{enumerate}
  \def\labelenumii{\alph{enumii})}
  \tightlist
  \item
    Maintaining secure data handling and compatibility
  \item
    Ensuring the model is interpretable
  \item
    Increasing the model's accuracy
  \item
    Reducing the model's training time
  \end{enumerate}

  \emph{Answer}: The correct answer is A. Maintaining secure data
  handling and compatibility. Integration with HIS requires secure data
  management to comply with privacy regulations and ensure seamless data
  exchange.

  \emph{Learning Objective}: Identify integration challenges between ML
  systems and existing infrastructure.
\end{enumerate}

\noindent\hspace*{1.25em}\hyperref[quiz-question-sec-ai-development-workflow-deployment-integration-stage-d549]{\textbf{$\leftarrow$~Back to Question}}

\end{fbx}

\phantomsection\label{quiz-answer-sec-ai-development-workflow-monitoring-maintenance-stage-e79a}
\begin{fbx}{callout-quiz-answer}{Self-Check: Answer 1.9}{}
\phantomsection\label{quiz-answer-sec-ai-development-workflow-monitoring-maintenance-stage-e79a}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Which of the following best describes the primary purpose of
  monitoring in ML systems?}

  \begin{enumerate}
  \def\labelenumii{\alph{enumii})}
  \tightlist
  \item
    To maintain static behavior of the system.
  \item
    To eliminate the need for human oversight.
  \item
    To ensure deterministic outputs from the system.
  \item
    To detect and adapt to data and model drift.
  \end{enumerate}

  \emph{Answer}: The correct answer is D. To detect and adapt to data
  and model drift. Monitoring is essential for identifying changes in
  data distributions and model performance, allowing the system to adapt
  and maintain reliability.

  \emph{Learning Objective}: Understand the role of monitoring in
  identifying and adapting to changes in ML systems.
\item
  \textbf{Explain how data drift can impact the performance of a machine
  learning model in production.}

  \emph{Answer}: Data drift impacts ML models by altering the input data
  distribution from what the model was trained on, leading to potential
  performance degradation. For example, if user behavior changes, the
  model may make less accurate predictions. This is important because it
  necessitates ongoing monitoring and potential model retraining to
  maintain accuracy.

  \emph{Learning Objective}: Analyze the effects of data drift on ML
  model performance and the need for continuous monitoring.
\item
  \textbf{Order the following steps in a typical ML maintenance
  workflow: (1) Model Retraining, (2) Data Drift Detection, (3)
  Performance Monitoring, (4) Feedback Loop Initiation.}

  \emph{Answer}: The correct order is: (3) Performance Monitoring, (2)
  Data Drift Detection, (4) Feedback Loop Initiation, (1) Model
  Retraining. Monitoring identifies performance issues, drift detection
  confirms the cause, feedback loops trigger necessary actions, and
  retraining updates the model.

  \emph{Learning Objective}: Understand the sequence of steps involved
  in maintaining ML systems in production.
\item
  \textbf{What are the benefits of implementing proactive maintenance
  strategies in ML systems?}

  \emph{Answer}: Proactive maintenance prevents issues before they
  impact operations by using predictive models to identify potential
  problems early. For example, continuous learning pipelines can adapt
  models to new data trends. This ensures system reliability and
  performance, reducing downtime and maintaining service quality.

  \emph{Learning Objective}: Evaluate the advantages of proactive
  maintenance in ensuring ML system reliability and performance.
\end{enumerate}

\noindent\hspace*{1.25em}\hyperref[quiz-question-sec-ai-development-workflow-monitoring-maintenance-stage-e79a]{\textbf{$\leftarrow$~Back to Question}}

\end{fbx}

\phantomsection\label{quiz-answer-sec-ai-development-workflow-integrating-systems-thinking-principles-24c0}
\begin{fbx}{callout-quiz-answer}{Self-Check: Answer 1.10}{}
\phantomsection\label{quiz-answer-sec-ai-development-workflow-integrating-systems-thinking-principles-24c0}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Which of the following best illustrates the concept of
  constraint propagation in AI development?}

  \begin{enumerate}
  \def\labelenumii{\alph{enumii})}
  \tightlist
  \item
    Using high-capacity networks to improve model accuracy.
  \item
    Deploying models on edge devices to reduce latency.
  \item
    Adjusting data preprocessing pipelines due to bandwidth limitations.
  \item
    Increasing model size to enhance performance.
  \end{enumerate}

  \emph{Answer}: The correct answer is C. Adjusting data preprocessing
  pipelines due to bandwidth limitations. This illustrates constraint
  propagation because an initial constraint (bandwidth limitation)
  influences subsequent stages like data preprocessing.

  \emph{Learning Objective}: Understand how constraint propagation
  affects various stages of AI system development.
\item
  \textbf{Explain how multi-scale feedback loops contribute to the
  robustness of an AI system.}

  \emph{Answer}: Multi-scale feedback loops contribute to robustness by
  enabling quick correction of operational issues through rapid loops
  and strategic adaptation through slower loops. For example,
  minute-level loops can detect and correct misconfigured cameras, while
  monthly loops can identify demographic shifts requiring data
  expansion. This prevents both overreaction to daily fluctuations and
  underreaction to meaningful trends.

  \emph{Learning Objective}: Analyze the role of feedback loops in
  maintaining AI system robustness.
\item
  \textbf{In managing emergent complexity, what is a key difference
  between ML systems and traditional software systems?}

  \begin{enumerate}
  \def\labelenumii{\alph{enumii})}
  \tightlist
  \item
    ML systems require monitoring for data drift and model bias.
  \item
    Traditional systems exhibit probabilistic degradation.
  \item
    ML systems rely on deterministic processes.
  \item
    Traditional systems focus on hardware performance.
  \end{enumerate}

  \emph{Answer}: The correct answer is A. ML systems require monitoring
  for data drift and model bias. Unlike traditional systems, ML systems
  exhibit probabilistic degradation through data drift and bias,
  necessitating different monitoring approaches.

  \emph{Learning Objective}: Differentiate between emergent complexity
  in ML systems and traditional software systems.
\item
  \textbf{Discuss the trade-offs involved in resource optimization for
  ML systems, using the DR case study as an example.}

  \emph{Answer}: Resource optimization involves trade-offs like model
  accuracy versus deployment cost. In the DR case, increasing accuracy
  from 94.8\% to 95.2\% requires larger models, leading to higher
  hardware costs. This illustrates non-linear relationships where small
  accuracy gains can result in significant cost increases, highlighting
  the need for strategic decision-making.

  \emph{Learning Objective}: Evaluate resource optimization trade-offs
  in ML system development.
\end{enumerate}

\noindent\hspace*{1.25em}\hyperref[quiz-question-sec-ai-development-workflow-integrating-systems-thinking-principles-24c0]{\textbf{$\leftarrow$~Back to Question}}

\end{fbx}

\phantomsection\label{quiz-answer-sec-ai-development-workflow-fallacies-pitfalls-4d91}
\begin{fbx}{callout-quiz-answer}{Self-Check: Answer 1.11}{}
\phantomsection\label{quiz-answer-sec-ai-development-workflow-fallacies-pitfalls-4d91}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{True or False: Machine learning development can effectively
  follow traditional software engineering workflows without any
  modifications.}

  \emph{Answer}: False. ML development introduces uncertainties and
  requires specialized workflows for data validation and iterative model
  refinement.

  \emph{Learning Objective}: Understand the fallacy of applying
  traditional software engineering workflows to ML development.
\item
  \textbf{Which of the following is a common pitfall in ML development?}

  \begin{enumerate}
  \def\labelenumii{\alph{enumii})}
  \tightlist
  \item
    Using agile methodologies for iterative development.
  \item
    Treating data preparation as a one-time preprocessing step.
  \item
    Incorporating feedback loops in the ML lifecycle.
  \item
    Ensuring continuous data validation and monitoring.
  \end{enumerate}

  \emph{Answer}: The correct answer is B. Treating data preparation as a
  one-time preprocessing step. This is a pitfall because it ignores the
  dynamic nature of real-world data, leading to model degradation.

  \emph{Learning Objective}: Identify common pitfalls in ML development
  workflows.
\item
  \textbf{Explain why model performance in development environments may
  not accurately predict production performance.}

  \emph{Answer}: Development environments often use clean datasets and
  controlled resources, creating artificial conditions. In contrast,
  production systems face data quality issues, latency constraints, and
  adversarial inputs. For example, a model might perform well in a
  controlled setting but fail in production due to unexpected data
  variations. This is important because it highlights the need for
  robust deployment practices.

  \emph{Learning Objective}: Analyze the discrepancy between development
  and production performance in ML systems.
\item
  \textbf{The belief that achieving good metrics during development
  ensures successful deployment is a common \_\_\_\_. This assumption
  overlooks the differences between development and production
  environments.}

  \emph{Answer}: fallacy. This assumption overlooks the differences
  between development and production environments.

  \emph{Learning Objective}: Recall specific fallacies related to ML
  system development.
\item
  \textbf{In a production system, how might you address the pitfall of
  skipping systematic validation stages to accelerate development
  timelines?}

  \emph{Answer}: To address this pitfall, integrate validation
  throughout the development process rather than treating it as a final
  step. For example, incorporate benchmarking and evaluation at each
  stage. This is important because it prevents hidden biases and poor
  generalization, which are costly to fix post-deployment.

  \emph{Learning Objective}: Apply strategies to mitigate common
  pitfalls in ML development workflows.
\end{enumerate}

\noindent\hspace*{1.25em}\hyperref[quiz-question-sec-ai-development-workflow-fallacies-pitfalls-4d91]{\textbf{$\leftarrow$~Back to Question}}

\end{fbx}

\phantomsection\label{quiz-answer-sec-ai-development-workflow-summary-fb13}
\begin{fbx}{callout-quiz-answer}{Self-Check: Answer 1.12}{}
\phantomsection\label{quiz-answer-sec-ai-development-workflow-summary-fb13}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Which of the following best describes the role of feedback
  loops in the ML lifecycle?}

  \begin{enumerate}
  \def\labelenumii{\alph{enumii})}
  \tightlist
  \item
    They enable continuous improvement by refining data and model
    performance.
  \item
    They provide a mechanism for error correction in static systems.
  \item
    They are used to validate models before deployment.
  \item
    They ensure that the ML lifecycle is a linear process.
  \end{enumerate}

  \emph{Answer}: The correct answer is A. They enable continuous
  improvement by refining data and model performance. Feedback loops are
  crucial for adapting and improving ML systems based on deployment
  insights, distinguishing ML from traditional software development.

  \emph{Learning Objective}: Understand the function and importance of
  feedback loops in the ML lifecycle.
\item
  \textbf{Explain how the interconnection between data and model
  pipelines contributes to the success of machine learning systems.}

  \emph{Answer}: The interconnection between data and model pipelines
  allows for continuous feedback and refinement, ensuring that data
  quality directly influences model performance. For example, insights
  from model deployment can trigger data collection adjustments, leading
  to improved model accuracy. This is important because it enables
  adaptive learning and system optimization.

  \emph{Learning Objective}: Analyze the relationship between data and
  model pipelines and its impact on system success.
\item
  \textbf{Order the following stages of the ML lifecycle from data
  collection to deployment: (1) Model Training, (2) Data Preparation,
  (3) Model Evaluation, (4) Data Collection, (5) Deployment.}

  \emph{Answer}: The correct order is: (4) Data Collection, (2) Data
  Preparation, (1) Model Training, (3) Model Evaluation, (5) Deployment.
  This sequence reflects the progression from gathering raw data to
  preparing it for use, training and evaluating models, and finally
  deploying them.

  \emph{Learning Objective}: Understand the sequential stages of the ML
  lifecycle and their logical progression.
\item
  \textbf{True or False: The ML lifecycle is characterized by
  deterministic specifications and static behavior.}

  \emph{Answer}: False. The ML lifecycle is characterized by
  probabilistic optimization and dynamic adaptation, which are essential
  for handling the complexities of machine learning systems.

  \emph{Learning Objective}: Differentiate between the characteristics
  of ML systems and traditional software systems.
\end{enumerate}

\noindent\hspace*{1.25em}\hyperref[quiz-question-sec-ai-development-workflow-summary-fb13]{\textbf{$\leftarrow$~Back to Question}}

\end{fbx}

\phantomsection\label{refs}
\begin{CSLReferences}{1}{0}
\bibitem[\citeproctext]{ref-adamson2018dermatology}
Adamson, Adewole S., and Avery Smith. 2018. {``Machine Learning and the
Cancer-Diagnosis Problem --- No Gold Standard.''} \emph{New England
Journal of Medicine} 379 (13): 1294--95.
\url{https://doi.org/10.1056/NEJMp1803881}.

\bibitem[\citeproctext]{ref-amershi2019software}
Amershi, Saleema, Andrew Begel, Christian Bird, Robert DeLine, Harald
Gall, Ece Kamar, Nachiappan Nagappan, Besmira Nushi, and Thomas
Zimmermann. 2019. {``Software Engineering for Machine Learning: A Case
Study.''} In \emph{2019 IEEE/ACM 41st International Conference on
Software Engineering: Software Engineering in Practice (ICSE-SEIP)},
291--300. IEEE. \url{https://doi.org/10.1109/icse-seip.2019.00042}.

\bibitem[\citeproctext]{ref-chapman2000crisp}
Chapman, Pete, Julian Clinton, Randy Kerber, Thomas Khabaza, Thomas
Reinartz, Colin Shearer, and Rudiger Wirth. 2000. {``CRISP-DM 1.0:
Step-by-Step Data Mining Guide.''} \emph{SPSS Inc}, 78.
\url{https://www.the-modeling-agency.com/crisp-dm.pdf}.

\bibitem[\citeproctext]{ref-chen2017machine}
Chen, Jonathan H., and Steven M. Asch. 2017. {``Machine Learning and
Prediction in Medicine --- Beyond the Peak of Inflated Expectations.''}
\emph{New England Journal of Medicine} 376 (26): 2507--9.
\url{https://doi.org/10.1056/nejmp1702071}.

\bibitem[\citeproctext]{ref-crowdflower2016data}
CrowdFlower. 2016. {``2016 Data Science Report.''} CrowdFlower Inc.
\url{https://visit.figure-eight.com/rs/416-ZBE-142/images/CrowdFlower_DataScienceReport_2016.pdf}.

\bibitem[\citeproctext]{ref-deng2009imagenet}
Deng, Jia, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei.
2009. {``ImageNet: A Large-Scale Hierarchical Image Database.''} In
\emph{2009 IEEE Conference on Computer Vision and Pattern Recognition},
248--55. Ieee; IEEE. \url{https://doi.org/10.1109/cvpr.2009.5206848}.

\bibitem[\citeproctext]{ref-fda2021artificial}
Food, U. S., and Drug Administration. 2021. {``Artificial
Intelligence/Machine Learning (AI/ML)-Based Software as a Medical Device
(SaMD) Action Plan.''} U.S. Department of Health; Human Services.
\url{https://www.fda.gov/media/145022/download}.

\bibitem[\citeproctext]{ref-gulshan2016deep}
Gulshan, Varun, Lily Peng, Marc Coram, Martin C. Stumpe, Derek Wu,
Arunachalam Narayanaswamy, Subhashini Venugopalan, et al. 2016.
{``Development and Validation of a Deep Learning Algorithm for Detection
of Diabetic Retinopathy in Retinal Fundus Photographs.''} \emph{JAMA}
316 (22): 2402. \url{https://doi.org/10.1001/jama.2016.17216}.

\bibitem[\citeproctext]{ref-uber2017michelangelo}
Hermann, Jeremy, and Mike Del Balso. 2017. {``Michelangelo: Uber's
Machine Learning Platform.''} In \emph{Data Engineering Bulletin},
40:8--21. 4.

\bibitem[\citeproctext]{ref-karpathy2017software}
Karpathy, Andrej. 2017. {``Software 2.0.''} \emph{Medium}.
\url{https://karpathy.medium.com/software-2-0-a64152b37c35}.

\bibitem[\citeproctext]{ref-kelly2019key}
Kelly, Christopher J., Alan Karthikesalingam, Mustafa Suleyman, Greg
Corrado, and Dominic King. 2019. {``Key Challenges for Delivering
Clinical Impact with Artificial Intelligence.''} \emph{BMC Medicine} 17
(1): 1--9. \url{https://doi.org/10.1186/s12916-019-1426-2}.

\bibitem[\citeproctext]{ref-kreuzberger2023machine}
Kreuzberger, Dominik, Niklas Kühl, and Sebastian Hirschl. 2023.
{``Machine Learning Operations (MLOps): Overview, Definition, and
Architecture.''} \emph{IEEE Access} 11: 31866--79.
\url{https://doi.org/10.1109/access.2023.3262138}.

\bibitem[\citeproctext]{ref-krizhevsky2012imagenet}
Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E. Hinton. 2017.
{``ImageNet Classification with Deep Convolutional Neural Networks.''}
\emph{Communications of the ACM} 60 (6): 84--90.
\url{https://doi.org/10.1145/3065386}.

\bibitem[\citeproctext]{ref-mcmahan2017communication}
McMahan, Brendan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise
Agüera y Arcas. 2017. {``Communication-Efficient Learning of Deep
Networks from Decentralized Data.''} In \emph{Artificial Intelligence
and Statistics}, 1273--82. PMLR.
\url{http://proceedings.mlr.press/v54/mcmahan17a.html}.

\bibitem[\citeproctext]{ref-paleyes2022challenges}
Paleyes, Andrei, Raoul-Gabriel Urma, and Neil D. Lawrence. 2022.
{``Challenges in Deploying Machine Learning: A Survey of Case
Studies.''} \emph{ACM Computing Surveys} 55 (6): 1--29.
\url{https://doi.org/10.1145/3533378}.

\bibitem[\citeproctext]{ref-rajkomar2019machine}
Rajkomar, Alvin, Jeffrey Dean, and Isaac Kohane. 2019. {``Machine
Learning in Medicine.''} \emph{New England Journal of Medicine} 380
(14): 1347--58. \url{https://doi.org/10.1056/nejmra1814259}.

\bibitem[\citeproctext]{ref-royce1970managing}
Royce, Winston W. 1970. {``Managing the Development of Large Software
Systems.''} In \emph{Proceedings of IEEE WESCON}, 1--9. IEEE.
\url{https://www.praxisframework.org/files/royce1970.pdf}.

\bibitem[\citeproctext]{ref-sculley2015hidden}
Sculley, D., Gary Holt, Daniel Golovin, Eugene Davydov, Todd Phillips,
Dietmar Ebner, Vinay Chaudhary, Michael Young, Jean-François Crespo, and
Dan Dennison. 2021. {``Technical Debt in Machine Learning Systems.''} In
\emph{Technical Debt in Practice}, 28:177--92. The MIT Press.
\url{https://doi.org/10.7551/mitpress/12440.003.0011}.

\bibitem[\citeproctext]{ref-who2019classification}
Steinmetz, Jaimie D, Katrin Maria Seeher, Nicoline Schiess, Emma
Nichols, Bochen Cao, Chiara Servili, Vanessa Cavallera, et al. 2024.
{``Global, Regional, and National Burden of Disorders Affecting the
Nervous System, 1990--2021: A Systematic Analysis for the Global Burden
of Disease Study 2021.''} \emph{The Lancet Neurology} 23 (4): 344--81.
\url{https://doi.org/10.1016/s1474-4422(24)00038-3}.

\bibitem[\citeproctext]{ref-stripe2019machine}
Stripe Engineering. 2019. {``Machine Learning at Stripe.''} Stripe
Engineering Blog.
\url{https://stripe.com/blog/machine-learning-at-stripe}.

\bibitem[\citeproctext]{ref-standish2020chaos}
The Standish Group. 2020. {``CHAOS 2020: Beyond Infinity.''} The
Standish Group International. \url{https://www.standishgroup.com/}.

\end{CSLReferences}


\backmatter


\end{document}
