% Options for packages loaded elsewhere
% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode,linktoc=all,pdfwindowui,pdfpagemode=FullScreen,pdfpagelayout=TwoPageRight}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  9pt,
  letterpaper,
  abstract,
  titlepage]{scrbook}
\usepackage{xcolor}
\usepackage[left=1in,marginparwidth=2.0666666666667in,textwidth=4.1333333333333in,marginparsep=0.3in]{geometry}
\usepackage{amsmath,amssymb}
\setcounter{secnumdepth}{3}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
% Make \paragraph and \subparagraph free-standing
\makeatletter
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}{
    \@ifstar
      \xxxParagraphStar
      \xxxParagraphNoStar
  }
  \newcommand{\xxxParagraphStar}[1]{\oldparagraph*{#1}\mbox{}}
  \newcommand{\xxxParagraphNoStar}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}{
    \@ifstar
      \xxxSubParagraphStar
      \xxxSubParagraphNoStar
  }
  \newcommand{\xxxSubParagraphStar}[1]{\oldsubparagraph*{#1}\mbox{}}
  \newcommand{\xxxSubParagraphNoStar}[1]{\oldsubparagraph{#1}\mbox{}}
\fi
\makeatother


\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\newsavebox\pandoc@box
\newcommand*\pandocbounded[1]{% scales image to fit in text height/width
  \sbox\pandoc@box{#1}%
  \Gscale@div\@tempa{\textheight}{\dimexpr\ht\pandoc@box+\dp\pandoc@box\relax}%
  \Gscale@div\@tempb{\linewidth}{\wd\pandoc@box}%
  \ifdim\@tempb\p@<\@tempa\p@\let\@tempa\@tempb\fi% select the smaller of both
  \ifdim\@tempa\p@<\p@\scalebox{\@tempa}{\usebox\pandoc@box}%
  \else\usebox{\pandoc@box}%
  \fi%
}
% Set default figure placement to htbp
\def\fps@figure{htbp}
\makeatother
% definitions for citeproc citations
\NewDocumentCommand\citeproctext{}{}
\NewDocumentCommand\citeproc{mm}{%
  \begingroup\def\citeproctext{#2}\cite{#1}\endgroup}
\makeatletter
 % allow citations to break across lines
 \let\@cite@ofmt\@firstofone
 % avoid brackets around text for \cite:
 \def\@biblabel#1{}
 \def\@cite#1#2{{#1\if@tempswa , #2\fi}}
\makeatother
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newenvironment{CSLReferences}[2] % #1 hanging-indent, #2 entry-spacing
 {\begin{list}{}{%
  \setlength{\itemindent}{0pt}
  \setlength{\leftmargin}{0pt}
  \setlength{\parsep}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1
   \setlength{\leftmargin}{\cslhangindent}
   \setlength{\itemindent}{-1\cslhangindent}
  \fi
  % set entry spacing
  \setlength{\itemsep}{#2\baselineskip}}}
 {\end{list}}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{\hfill\break\parbox[t]{\linewidth}{\strut\ignorespaces#1\strut}}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}

% =============================================================================
% LATEX HEADER CONFIGURATION FOR MLSYSBOOK PDF
% =============================================================================
% This file contains all LaTeX package imports, custom commands, and styling
% definitions for the PDF output of the Machine Learning Systems textbook.
%
% Key Features:
% - Harvard crimson branding throughout
% - Custom part/chapter/section styling
% - Professional table formatting with colored headers
% - Margin notes with custom styling
% - TikZ-based part dividers
% - Page numbering (Roman for frontmatter, Arabic for mainmatter)
%
% Note: This file is included via _quarto-pdf.yml and affects PDF output only.
% HTML/EPUB styling is handled separately via CSS files.
% =============================================================================

% =============================================================================
% PACKAGE IMPORTS
% =============================================================================

% Layout and positioning
% \usepackage[outercaption, ragged]{sidecap}  % Commented out to make figure captions inline instead of in margin
\usepackage{adjustbox}      % Adjusting box dimensions
\usepackage{afterpage}      % Execute commands after page break
\usepackage{morefloats}     % Increase number of floats
\usepackage{array}          % Enhanced table column formatting
\usepackage{atbegshi}       % Insert content at page beginning
%\usepackage{changepage}     % Change page dimensions mid-document
\usepackage{emptypage}      % Clear headers/footers on empty pages

% Language and text
\usepackage[english]{babel} % English language support
\usepackage{microtype}      % Improved typography and hyphenation

% Captions and floats
\usepackage{caption}
% Caption styling configuration
%\captionsetup[table]{belowskip=5pt}
\captionsetup{format=plain}
\DeclareCaptionLabelFormat{mylabel}{#1
#2:\hspace{1.0ex}}
\DeclareCaptionFont{ninept}{\fontsize{7pt}{8}\selectfont #1}

% Figure captions: Small font, bold label, ragged right
\captionsetup[figure]{labelfont={bf,ninept},labelsep=space,
belowskip=2pt,aboveskip=6pt,labelformat=mylabel,
justification=raggedright,singlelinecheck=false,font={ninept}}

% Table captions: Small font, bold label, ragged right
\captionsetup[table]{belowskip=6pt,labelfont={bf,ninept},labelsep=none,
labelformat=mylabel,justification=raggedright,singlelinecheck=false,font={ninept}}

% Typography fine-tuning
\emergencystretch=5pt       % Allow extra stretch to avoid overfull boxes

% Utility packages
\usepackage{etoolbox}       % For patching commands and environments

% Page layout and headers
\usepackage{fancyhdr}       % Custom headers and footers
\usepackage{geometry}       % Page dimensions and margins

% Graphics and figures
\usepackage{graphicx}       % Include graphics
\usepackage{float}          % Improved float placement
\usepackage[skins,breakable]{tcolorbox} % Coloured and framed text boxes
\tcbset{before upper=\setlength{\parskip}{3pt}}

% Tables
\usepackage{longtable}      % Multi-page tables

% Fonts and typography
\usepackage{fontspec}       % Font selection for LuaLaTeX
\usepackage{mathptmx}       % Times-like math fonts
\usepackage{newpxtext}      % Palatino-like font for body text

% Colors and visual elements
\usepackage[dvipsnames]{xcolor}  % Extended color support
\usepackage{tikz}           % Programmatic graphics
\usetikzlibrary{positioning}
\usetikzlibrary{calc}
\usepackage{tikzpagenodes}  % TikZ positioning relative to page

% Code listings
\usepackage{listings}       % Code highlighting

% Hyperlinks
\usepackage{hyperref}       % Clickable links in PDF

% Conditional logic
\usepackage{ifthen}         % If-then-else commands

% Math symbols
\usepackage{amsmath}        % AMS math extensions
\usepackage{amssymb}        % AMS math symbols
\usepackage{latexsym}       % Additional LaTeX symbols
\usepackage{pifont}         % Zapf Dingbats symbols
\providecommand{\blacklozenge}{\ding{117}}  % Black diamond symbol

% Lists
\usepackage{enumitem}       % Customizable lists

% Margin notes and sidenotes
\usepackage{marginfix}      % Fixes margin note overflow
\usepackage{marginnote}     % Margin notes
\usepackage{sidenotes}      % Academic-style sidenotes
\renewcommand\raggedrightmarginnote{\sloppy}
\renewcommand\raggedleftmarginnote{\sloppy}

% Typography improvements
\usepackage{ragged2e}       % Better ragged text
\usepackage[all]{nowidow}   % Prevent widows and orphans
\usepackage{needspace}      % Ensure minimum space on page

% Section formatting
\usepackage[explicit]{titlesec}  % Custom section titles
\usepackage{tocloft}        % Table of contents formatting

% QR codes and icons
\usepackage{fontawesome5}   % Font Awesome icons
\usepackage{qrcode}         % QR code generation
\qrset{link, height=15mm}

% =============================================================================
% FLOAT CONFIGURATION
% =============================================================================
% Allow more floats per page to handle figure-heavy chapters
\extrafloats{200}
\setcounter{topnumber}{12}       % Max floats at top of page
\setcounter{bottomnumber}{12}    % Max floats at bottom of page
\setcounter{totalnumber}{24}     % Max floats per page
\setcounter{dbltopnumber}{8}     % Max floats at top of two-column page
\renewcommand{\topfraction}{.95}  % Max fraction of page for top floats
\renewcommand{\bottomfraction}{.95}
\renewcommand{\textfraction}{.05}  % Min fraction of page for text
\renewcommand{\floatpagefraction}{.7}  % Min fraction of float page
\renewcommand{\dbltopfraction}{.95}

% Prevent "Float(s) lost" errors by flushing floats more aggressively
\usepackage{placeins}  % Provides \FloatBarrier

% =============================================================================
% COLOR DEFINITIONS
% =============================================================================
% Harvard crimson - primary brand color used throughout
\definecolor{crimson}{HTML}{A51C30}

% Quiz element colors
\definecolor{quiz-question-color1}{RGB}{225,243,248}  % Light blue background
\definecolor{quiz-question-color2}{RGB}{17,158,199}   % Blue border
\definecolor{quiz-answer-color1}{RGB}{250,234,241}    % Light pink background
\definecolor{quiz-answer-color2}{RGB}{152,14,90}      % Magenta border

% =============================================================================
% LIST FORMATTING
% =============================================================================
% Tighter list spacing for academic style
\def\tightlist{}
\setlist{itemsep=1pt, parsep=1pt, topsep=0pt,after={\vspace{0.3\baselineskip}}}
\let\tightlist\relax

\makeatletter
\@ifpackageloaded{framed}{}{\usepackage{framed}}
\@ifpackageloaded{fancyvrb}{}{\usepackage{fancyvrb}}
\makeatother

\makeatletter
%New float "codelisting" has been updated
\AtBeginDocument{%
\floatstyle{ruled}
\newfloat{codelisting}{!htb}{lop}
\floatname{codelisting}{Listing}
\floatplacement{codelisting}{!htb}
\captionsetup[codelisting]{labelfont={bf,ninept},labelformat=mylabel,
  singlelinecheck=false,width=\linewidth,labelsep=none,font={ninept}}%
\renewenvironment{snugshade}{%
   \def\OuterFrameSep{3pt}%
   \def\FrameCommand{\fboxsep=5pt\colorbox{shadecolor}}%
   \MakeFramed{\advance\hsize-\width\FrameRestore}%
   \leftskip 0.5em \rightskip 0.5em%
   \small% decrease font size
   }{\endMakeFramed}%
}
\makeatother

%The space before and after the verbatim environment "Highlighting" has been reduced
\fvset{listparameters=\setlength{\topsep}{0pt}\setlength{\partopsep}{0pt}}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{framesep=0mm,commandchars=\\\{\}}

\makeatletter
\renewcommand\fs@ruled{\def\@fs@cfont{\bfseries}\let\@fs@capt\floatc@ruled
\def\@fs@pre{\hrule height.8pt depth0pt \kern2pt}%
\def\@fs@post{\kern2pt\hrule\relax}%
\def\@fs@mid{\kern2pt\hrule\kern1pt}%space between float and caption
\let\@fs@iftopcapt\iftrue}
\makeatother


% =============================================================================
% HYPHENATION RULES
% =============================================================================
% Explicit hyphenation points for technical terms to avoid bad breaks
\hyphenation{
  light-weight
  light-weight-ed
  de-vel-op-ment
  un-der-stand-ing
  mod-els
  prin-ci-ples
  ex-per-tise
  com-pli-cat-ed
  blue-print
  per‧for‧mance
  com-mu-ni-ca-tion
  par-a-digms
  hy-per-ten-sion
  a-chieved
}

% =============================================================================
% CODE LISTING CONFIGURATION
% =============================================================================
% Settings for code blocks using listings package
\lstset{
breaklines=true,              % Automatic line wrapping
breakatwhitespace=true,       % Break at whitespace only
basicstyle=\ttfamily,         % Monospace font
frame=none,                   % No frame around code
keepspaces=true,              % Preserve spaces
showspaces=false,             % Don't show space characters
showtabs=false,               % Don't show tab characters
columns=flexible,             % Flexible column width
belowskip=0pt,               % Minimal spacing
aboveskip=0pt
}

% =============================================================================
% PAGE GEOMETRY
% =============================================================================
% MIT Press trim size: 7" x 10" (per publisher specifications)
% This is a standard academic textbook format providing good readability
% for technical content with figures and code blocks.
% Wide outer margin accommodates sidenotes/margin notes.
\geometry{
  paperwidth=7in,
  paperheight=10in,
  top=0.875in,
  bottom=0.875in,
  inner=0.875in,              % Inner margin (binding side)
  outer=1.75in,               % Outer margin (includes space for sidenotes)
  footskip=30pt,
  marginparwidth=1.25in,      % Width for margin notes
  twoside                     % Different left/right pages
}

% =============================================================================
% SIDENOTE STYLING
% =============================================================================
% Custom sidenote design with crimson vertical bar
\renewcommand{\thefootnote}{\textcolor{crimson}{\arabic{footnote}}}

% Save original sidenote command
\makeatletter
\@ifundefined{oldsidenote}{
  \let\oldsidenote\sidenote%
}{}
\makeatother

% Redefine sidenote with vertical crimson bar
\renewcommand{\sidenote}[1]{%
  \oldsidenote{%
    \noindent
    \color{crimson!100}                        % Crimson vertical line
    \raisebox{0em}{%
      \rule{0.5pt}{1.5em}                      % Thin vertical line
    }
    \hspace{0.3em}                             % Space after line
    \color{black}                              % Reset text color
    \footnotesize #1                           % Sidenote content
  }%
}

% =============================================================================
% FLOAT HANDLING
% =============================================================================
% Patch LaTeX's output routine to handle float overflow gracefully
% The "Float(s) lost" error occurs in \@doclearpage when \@currlist is not empty
% This patch silently clears pending floats that can't be placed
\makeatletter
\let\orig@doclearpage\@doclearpage
\def\@doclearpage{%
  \ifx\@currlist\@empty\else
    \global\let\@currlist\@empty
    \typeout{Warning: Floats cleared to prevent overflow}%
  \fi
  \orig@doclearpage
}
\makeatother

% Additional safety for structural commands
\let\originalbackmatter\backmatter
\renewcommand{\backmatter}{%
  \clearpage%
  \originalbackmatter%
}

\let\originalfrontmatter\frontmatter
\renewcommand{\frontmatter}{%
  \clearpage%
  \originalfrontmatter%
}

\let\originalmainmatter\mainmatter
\renewcommand{\mainmatter}{%
  \clearpage%
  \originalmainmatter%
}

% =============================================================================
% PAGE HEADERS AND FOOTERS
% =============================================================================
% Ensure chapters use fancy page style (not plain)
\patchcmd{\chapter}{\thispagestyle{plain}}{\thispagestyle{fancy}}{}{}

% Main page style with crimson headers
\pagestyle{fancy}
\fancyhf{}                                              % Clear all
\fancyhead[LE]{\small\color{crimson}\nouppercase{\rightmark}}  % Left even: section
\fancyhead[RO]{\color{crimson}\thepage}                 % Right odd: page number
\fancyhead[LO]{\small\color{crimson}\nouppercase{\leftmark}}   % Left odd: chapter
\fancyhead[RE]{\color{crimson}\thepage}                 % Right even: page number
\renewcommand{\headrulewidth}{0.4pt}                    % Thin header line
\renewcommand{\footrulewidth}{0pt}                      % No footer line

% Plain page style (for chapter openings)
\fancypagestyle{plain}{
  \fancyhf{}
  \fancyfoot[C]{\color{crimson}\thepage}                % Centered page number
  \renewcommand{\headrulewidth}{0pt}
  \renewcommand{\footrulewidth}{0pt}
}

% =============================================================================
% KOMA-SCRIPT FONT ADJUSTMENTS
% =============================================================================
% Apply crimson color to all heading levels
\addtokomafont{disposition}{\rmfamily\color{crimson}}
\addtokomafont{chapter}{\color{crimson}}
\addtokomafont{section}{\color{crimson}}
\addtokomafont{subsection}{\color{crimson}}

% =============================================================================
% ABSTRACT ENVIRONMENT
% =============================================================================
\newenvironment{abstract}{
  \chapter*{\abstractname}
  \addcontentsline{toc}{chapter}{\abstractname}
  \small
}{
  \clearpage
}

% =============================================================================
% HYPERLINK CONFIGURATION
% =============================================================================
% Crimson-colored links throughout, two-page PDF layout
\hypersetup{
  linkcolor=crimson,
  citecolor=crimson,
  urlcolor=crimson,
  pdfpagelayout=TwoPageRight,   % Two-page spread view
  pdfstartview=Fit               % Initial zoom fits page
}

% =============================================================================
% PART SUMMARY SYSTEM
% =============================================================================
% Allows adding descriptive text below part titles
\newcommand{\partsummary}{}     % Empty by default
\newif\ifhaspartsummary%
\haspartsummaryfalse%

\newcommand{\setpartsummary}[1]{%
  \renewcommand{\partsummary}{#1}%
  \haspartsummarytrue%
}

% Additional colors for part page backgrounds
\definecolor{BrownLL}{RGB}{233,222,220}
\definecolor{BlueDD}{RGB}{62,100,125}
\colorlet{BlueDD}{magenta}

% ===============================================================================
% PART STYLING SYSTEM
% ===============================================================================
%
% This system provides three distinct visual styles for book organization:
%
% 1. NUMBERED PARTS (\part{title}) - For main book sections
%    - Roman numerals (I, II, III, etc.) in top right corner
%    - Crimson title with horizontal lines above/below
%    - "Part I" label in sidebar
%    - Used for: foundations, principles, optimization, deployment, etc.
%
% 2. UNNUMBERED PARTS (\part*{title}) - For special sections like "Labs"
%    - Division-style geometric background (left side)
%    - No Roman numerals
%    - Used for: labs section
%
% 3. DIVISIONS (\division{title}) - For major book divisions
%    - Clean geometric background with centered title
%    - Used for: frontmatter, main_content, backmatter
%
% The Lua filter (inject-parts.lua) automatically routes parts by {key:xxx} commands
% to the appropriate LaTeX command based on the key name.
% ===============================================================================

% NUMBERED PARTS: Roman numeral styling for main book sections
\titleformat{\part}[display]
{\thispagestyle{empty}}{}{20pt}{
\begin{tikzpicture}[remember picture,overlay]
%%%
%%
\node[crimson,align=flush right,
inner sep=0,outer sep=0mm,draw=none,%
anchor=east,minimum height=31mm, text width=1.2\textwidth,
yshift=-30mm,font={%
\fontsize{98pt}{104}\selectfont\bfseries}]  (BG) at (current page text area.north east){\thepart};
%
\node[black,inner sep=0mm,draw=none,
anchor=mid,text width=1.2\textwidth,
 minimum height=35mm, align=right,
node distance=7mm,below=of BG,
font={\fontsize{30pt}{34}\selectfont}]
(BGG)  {\hyphenchar\font=-1 \color{black}\MakeUppercase {#1}};
\draw [crimson,line width=3pt] ([yshift=0mm]BGG.north west) -- ([yshift=0mm]BGG.north east);
\draw [crimson,line width=2pt] ([yshift=0mm]BGG.south west) -- ([yshift=0mm]BGG.south east);
%
\node[fill=crimson,text=white,rotate=90,%
anchor=south west,minimum height=15mm,
minimum width=40mm,font={%
\fontsize{20pt}{20}\selectfont\bfseries}](BP)  at
(current page text area.south east)
{{\sffamily Part}~\thepart};
%
\path[red](BP.north west)-|coordinate(PS)(BGG.south west);
%
% Part summary box commented out for cleaner design
% \ifhaspartsummary
% \node[inner sep=4pt,text width=0.7\textwidth,draw=none,fill=BrownLL!40,
% align=justify,font={\fontsize{9pt}{12}\selectfont},anchor=south west]
% at (PS) {\partsummary};
% \fi
\end{tikzpicture}
}[]

\renewcommand{\thepart}{\Roman{part}}

% UNNUMBERED PARTS: Division-style background for special sections
\titleformat{name=\part,numberless}[display]
{\thispagestyle{empty}}{}{20pt}{
\begin{tikzpicture}[remember picture,overlay]
%%%
\coordinate(S1)at([yshift=-200mm]current page.north west);
\draw[draw=none,fill=BlueDD!7](S1)--++(45:16)coordinate(S2)-
|(S2|-current page.north west)--(current page.north west)coordinate(S3)--(S1);
%
\coordinate(E1)at([yshift=-98mm]current page.north west);
\draw[draw=none,fill=BlueDD!15](E1)--(current page.north west)coordinate(E2)
--++(0:98mm)coordinate(E3)--(E1);
%
\coordinate(D1)at([yshift=15mm]current page.south west);
\draw[draw=none,fill=BlueDD!40,opacity=0.5](D1)--++(45:5.5)coordinate(D2)
-|(D2|-current page.north west)--(current page.north west)coordinate(D3)--(D1);
%%%%
\path[red](S2)-|(S2-|current page.east)coordinate(SS2);
%PART
\node[crimson,align=flush right,inner sep=0,outer sep=0mm,draw=none,anchor=south,
font={\fontsize{48pt}{48}\selectfont\bfseries}]  (BG) at ($(S2)!0.5!(SS2)$){\hphantom{Part}};
%%%
\path[green]([yshift=15mm]D2)-|coordinate(TPD)(BG.south east);
\node[inner sep=0mm,draw=none,anchor=south east,%text width=0.9\textwidth,
align=right,font={\fontsize{40pt}{40}\selectfont}]
(BGG) at (TPD)  {\color{crimson}\MakeUppercase {#1}};%\MakeUppercase {}
\end{tikzpicture}
}

% Define \numberedpart command for numbered parts
\newcommand{\numberedpart}[1]{%
\FloatBarrier%  % Flush all pending floats before part break
\clearpage
\thispagestyle{empty}
\stepcounter{part}%
\begin{tikzpicture}[remember picture,overlay]
%%%
%%
\node[crimson,align=flush right,
inner sep=0,outer sep=0mm,draw=none,%
anchor=east,minimum height=31mm, text width=1.2\textwidth,
yshift=-30mm,font={%
\fontsize{98pt}{104}\selectfont\bfseries}]  (BG) at (current page text area.north east){\thepart};
%
\node[black,inner sep=0mm,draw=none,
anchor=mid,text width=1.2\textwidth,
 minimum height=35mm, align=right,
node distance=7mm,below=of BG,
font={\fontsize{30pt}{34}\selectfont}]
(BGG)  {\hyphenchar\font=-1 \color{black}\MakeUppercase {#1}};
\draw [crimson,line width=3pt] ([yshift=0mm]BGG.north west) -- ([yshift=0mm]BGG.north east);
\draw [crimson,line width=2pt] ([yshift=0mm]BGG.south west) -- ([yshift=0mm]BGG.south east);
%
\node[fill=crimson,text=white,rotate=90,%
anchor=south west,minimum height=15mm,
minimum width=40mm,font={%
\fontsize{20pt}{20}\selectfont\bfseries}](BP)  at
(current page text area.south east)
{{\sffamily Part}~\thepart};
%
\path[red](BP.north west)-|coordinate(PS)(BGG.south west);
%
% Part summary box commented out for cleaner design
% \ifhaspartsummary
% \node[inner sep=4pt,text width=0.7\textwidth,draw=none,fill=BrownLL!40,
% align=justify,font={\fontsize{9pt}{12}\selectfont},anchor=south west]
% at (PS) {\partsummary};
% \fi
\end{tikzpicture}
\clearpage
}



% DIVISIONS: Clean geometric styling with subtle tech elements
% Used for frontmatter, main_content, and backmatter divisions
\newcommand{\division}[1]{%
\FloatBarrier%  % Flush all pending floats before division break
\clearpage
\thispagestyle{empty}
\begin{tikzpicture}[remember picture,overlay]

% Clean geometric background (original design)
\coordinate(S1)at([yshift=-200mm]current page.north west);
\draw[draw=none,fill=BlueDD!7](S1)--++(45:16)coordinate(S2)-
|(S2|-current page.north west)--(current page.north west)coordinate(S3)--(S1);

\coordinate(E1)at([yshift=-98mm]current page.north west);
\draw[draw=none,fill=BlueDD!15](E1)--(current page.north west)coordinate(E2)
--++(0:98mm)coordinate(E3)--(E1);

\coordinate(D1)at([yshift=15mm]current page.south west);
\draw[draw=none,fill=BlueDD!40,opacity=0.5](D1)--++(45:5.5)coordinate(D2)
-|(D2|-current page.north west)--(current page.north west)coordinate(D3)--(D1);

% Subtle tech elements - positioned in white areas for better visibility
% Upper right white area - more visible
\draw[crimson!40, line width=0.8pt] ([xshift=140mm,yshift=-60mm]current page.north west) -- ++(40mm,0);
\draw[crimson!40, line width=0.8pt] ([xshift=150mm,yshift=-70mm]current page.north west) -- ++(30mm,0);
\draw[crimson!35, line width=0.7pt] ([xshift=160mm,yshift=-60mm]current page.north west) -- ++(0,-15mm);
\draw[crimson!35, line width=0.7pt] ([xshift=170mm,yshift=-70mm]current page.north west) -- ++(0,10mm);

% Circuit nodes - upper right
\fill[crimson!50] ([xshift=160mm,yshift=-60mm]current page.north west) circle (1.5mm);
\fill[white] ([xshift=160mm,yshift=-60mm]current page.north west) circle (0.8mm);
\fill[crimson!50] ([xshift=170mm,yshift=-70mm]current page.north west) circle (1.3mm);
\fill[white] ([xshift=170mm,yshift=-70mm]current page.north west) circle (0.6mm);

% Lower right white area - enhanced visibility
\draw[crimson!45, line width=0.9pt] ([xshift=140mm,yshift=-190mm]current page.north west) -- ++(45mm,0);
\draw[crimson!45, line width=0.9pt] ([xshift=150mm,yshift=-200mm]current page.north west) -- ++(35mm,0);
\draw[crimson!40, line width=0.8pt] ([xshift=160mm,yshift=-190mm]current page.north west) -- ++(0,-20mm);
\draw[crimson!40, line width=0.8pt] ([xshift=170mm,yshift=-200mm]current page.north west) -- ++(0,15mm);

% Additional connecting lines in lower right
\draw[crimson!35, line width=0.7pt] ([xshift=130mm,yshift=-180mm]current page.north west) -- ++(25mm,0);
\draw[crimson!35, line width=0.7pt] ([xshift=145mm,yshift=-180mm]current page.north west) -- ++(0,-25mm);

% Circuit nodes - lower right (more prominent)
\fill[crimson!55] ([xshift=160mm,yshift=-190mm]current page.north west) circle (1.6mm);
\fill[white] ([xshift=160mm,yshift=-190mm]current page.north west) circle (0.9mm);
\fill[crimson!55] ([xshift=170mm,yshift=-200mm]current page.north west) circle (1.4mm);
\fill[white] ([xshift=170mm,yshift=-200mm]current page.north west) circle (0.7mm);
\fill[crimson!50] ([xshift=145mm,yshift=-180mm]current page.north west) circle (1.2mm);
\fill[white] ([xshift=145mm,yshift=-180mm]current page.north west) circle (0.6mm);

% Title positioned in center - clean and readable
\node[inner sep=0mm,draw=none,anchor=center,text width=0.8\textwidth,
align=center,font={\fontsize{40pt}{40}\selectfont}]
(BGG) at (current page.center)  {\color{crimson}\MakeUppercase {#1}};

\end{tikzpicture}
\clearpage
}

% LAB DIVISIONS: Circuit-style neural network design for lab sections
% Used specifically for lab platform sections (arduino, xiao, grove, etc.)
\newcommand{\labdivision}[1]{%
\FloatBarrier%  % Flush all pending floats before lab division break
\clearpage
\thispagestyle{empty}
\begin{tikzpicture}[remember picture,overlay]
% Circuit background with subtle gradient
\coordinate(S1)at([yshift=-200mm]current page.north west);
\draw[draw=none,fill=BlueDD!5](S1)--++(45:16)coordinate(S2)-
|(S2|-current page.north west)--(current page.north west)coordinate(S3)--(S1);

% TOP AREA: Circuit lines in upper white space
\draw[crimson!50, line width=1.5pt] ([xshift=30mm,yshift=-40mm]current page.north west) -- ++(60mm,0);
\draw[crimson!40, line width=1pt] ([xshift=120mm,yshift=-50mm]current page.north west) -- ++(50mm,0);
\draw[crimson!50, line width=1.5pt] ([xshift=40mm,yshift=-70mm]current page.north west) -- ++(40mm,0);

% Connecting lines in top area
\draw[crimson!30, line width=1pt] ([xshift=60mm,yshift=-40mm]current page.north west) -- ++(0,-20mm);
\draw[crimson!30, line width=1pt] ([xshift=145mm,yshift=-50mm]current page.north west) -- ++(0,10mm);

% Neural nodes in top area
\fill[crimson!70] ([xshift=60mm,yshift=-40mm]current page.north west) circle (2.5mm);
\fill[white] ([xshift=60mm,yshift=-40mm]current page.north west) circle (1.5mm);
\fill[crimson!60] ([xshift=145mm,yshift=-50mm]current page.north west) circle (2mm);
\fill[white] ([xshift=145mm,yshift=-50mm]current page.north west) circle (1mm);
\fill[crimson!80] ([xshift=80mm,yshift=-70mm]current page.north west) circle (2mm);
\fill[white] ([xshift=80mm,yshift=-70mm]current page.north west) circle (1mm);

% BOTTOM AREA: Circuit lines in lower white space
\draw[crimson!50, line width=1.5pt] ([xshift=20mm,yshift=-200mm]current page.north west) -- ++(70mm,0);
\draw[crimson!40, line width=1pt] ([xshift=110mm,yshift=-210mm]current page.north west) -- ++(60mm,0);
\draw[crimson!50, line width=1.5pt] ([xshift=35mm,yshift=-230mm]current page.north west) -- ++(45mm,0);

% Connecting lines in bottom area
\draw[crimson!30, line width=1pt] ([xshift=55mm,yshift=-200mm]current page.north west) -- ++(0,-20mm);
\draw[crimson!30, line width=1pt] ([xshift=140mm,yshift=-210mm]current page.north west) -- ++(0,15mm);

% Neural nodes in bottom area
\fill[crimson!70] ([xshift=55mm,yshift=-200mm]current page.north west) circle (2.5mm);
\fill[white] ([xshift=55mm,yshift=-200mm]current page.north west) circle (1.5mm);
\fill[crimson!60] ([xshift=140mm,yshift=-210mm]current page.north west) circle (2mm);
\fill[white] ([xshift=140mm,yshift=-210mm]current page.north west) circle (1mm);
\fill[crimson!80] ([xshift=80mm,yshift=-230mm]current page.north west) circle (2mm);
\fill[white] ([xshift=80mm,yshift=-230mm]current page.north west) circle (1mm);

% SIDE AREAS: Subtle circuit elements on left and right edges
\draw[crimson!30, line width=1pt] ([xshift=15mm,yshift=-120mm]current page.north west) -- ++(20mm,0);
\draw[crimson!30, line width=1pt] ([xshift=175mm,yshift=-130mm]current page.north west) -- ++(15mm,0);
\fill[crimson!50] ([xshift=25mm,yshift=-120mm]current page.north west) circle (1.5mm);
\fill[white] ([xshift=25mm,yshift=-120mm]current page.north west) circle (0.8mm);
\fill[crimson!50] ([xshift=185mm,yshift=-130mm]current page.north west) circle (1.5mm);
\fill[white] ([xshift=185mm,yshift=-130mm]current page.north west) circle (0.8mm);

% Title positioned in center - CLEAN AREA
\node[inner sep=0mm,draw=none,anchor=center,text width=0.8\textwidth,
align=center,font={\fontsize{44pt}{44}\selectfont\bfseries}]
(BGG) at (current page.center)  {\color{crimson}\MakeUppercase {#1}};

\end{tikzpicture}
\clearpage
}

% Define \lab command for lab styling (different visual treatment)
\newcommand{\lab}[1]{%
\begin{tikzpicture}[remember picture,overlay]
%%%
% Different background pattern for labs
\coordinate(S1)at([yshift=-200mm]current page.north west);
\draw[draw=none,fill=BlueDD!15](S1)--++(45:16)coordinate(S2)-
|(S2|-current page.north west)--(current page.north west)coordinate(S3)--(S1);
%
\coordinate(E1)at([yshift=-98mm]current page.north west);
\draw[draw=none,fill=BlueDD!25](E1)--(current page.north west)coordinate(E2)
--++(0:98mm)coordinate(E3)--(E1);
%
\coordinate(D1)at([yshift=15mm]current page.south west);
\draw[draw=none,fill=BlueDD!60,opacity=0.7](D1)--++(45:5.5)coordinate(D2)
-|(D2|-current page.north west)--(current page.north west)coordinate(D3)--(D1);
%%%%
\path[red](S2)-|(S2-|current page.east)coordinate(SS2);
%LAB - Different styling
\node[crimson,align=flush right,inner sep=0,outer sep=0mm,draw=none,anchor=south,
font={\fontsize{48pt}{48}\selectfont\bfseries}]  (BG) at ($(S2)!0.5!(SS2)$){\hphantom{Workshop}};
%%%
\path[green]([yshift=15mm]D2)-|coordinate(TPD)(BG.south east);
\node[inner sep=0mm,draw=none,anchor=south east,%text width=0.9\textwidth,
align=right,font={\fontsize{40pt}{40}\selectfont}]
(BGG) at (TPD)  {\color{crimson}\MakeUppercase {#1}};%\MakeUppercase {}
\end{tikzpicture}
\thispagestyle{empty}
\clearpage
}

% =============================================================================
% SECTION FORMATTING
% =============================================================================
% All section levels use crimson color and are ragged right

% Section (Large, bold, crimson)
\titleformat{\section}
  {\normalfont\Large\bfseries\color{crimson}\raggedright}
  {\thesection}
  {0.5em}
  {#1}
\titlespacing*{\section}{0pc}{14pt plus 4pt minus 4pt}{6pt plus 2pt minus 2pt}[0pc]

% Subsection (large, bold, crimson)
\titleformat{\subsection}
  {\normalfont\large\bfseries\color{crimson}\raggedright}
  {\thesubsection}
  {0.5em}
  {#1}
\titlespacing*{\subsection}{0pc}{12pt plus 4pt minus 4pt}{5pt plus 1pt minus 2pt}[0pc]

% Subsubsection (normal size, bold, crimson)
\titleformat{\subsubsection}
  {\normalfont\normalsize\bfseries\color{crimson}\raggedright}
  {\thesubsubsection}
  {0.5em}
  {#1}
\titlespacing*{\subsubsection}{0pc}{12pt plus 4pt minus 4pt}{5pt plus 1pt minus 2pt}[0pc]

% Paragraph (run-in, bold, crimson, ends with period)
\titleformat{\paragraph}[runin]
  {\normalfont\normalsize\bfseries\color{crimson}}
  {\theparagraph}
  {0.5em}
  {#1}
  [\textbf{.}]
  \titlespacing*{\paragraph}{0pc}{6pt plus 2pt minus 2pt}{0.5em}[0pc]

% Subparagraph (run-in, italic, crimson, ends with period)
\titleformat{\subparagraph}[runin]
  {\normalfont\normalsize\itshape\color{crimson}}
  {\thesubparagraph}
  {0.5em}
  {#1}
  [\textbf{.}]
  \titlespacing*{\subparagraph}{0pc}{6pt plus 2pt minus 2pt}{0.5em}[0pc]

% =============================================================================
% CHAPTER FORMATTING
% =============================================================================
% Numbered chapters: "Chapter X" prefix, huge crimson title
\titleformat{\chapter}[display]
  {\normalfont\huge\bfseries\color{crimson}}
  {\chaptername\ \thechapter}
  {20pt}
  {\Huge #1}
  []

% Unnumbered chapters: no prefix, huge crimson title
\titleformat{name=\chapter,numberless}
  {\normalfont\huge\bfseries\color{crimson}}
  {}
  {0pt}
  {\Huge #1}
  []

\renewcommand{\chaptername}{Chapter}
% =============================================================================
% TABLE OF CONTENTS FORMATTING
% =============================================================================
\setcounter{tocdepth}{2}                      % Show chapters, sections, subsections

% TOC spacing adjustments for number widths and indentation
\setlength{\cftchapnumwidth}{2em}             % Chapter number width
\setlength{\cftsecnumwidth}{2.75em}           % Section number width
\setlength{\cftsubsecnumwidth}{3.25em}        % Subsection number width
\setlength{\cftsubsubsecnumwidth}{4em}        % Subsubsection number width
\setlength{\cftsubsecindent}{4.25em}          % Subsection indent
\setlength{\cftsubsubsecindent}{7.5em}        % Subsubsection indent

% Chapter entries in TOC: bold crimson with "Chapter" prefix
\renewcommand{\cftchapfont}{\bfseries\color{crimson}}
\renewcommand{\cftchappresnum}{\color{crimson}Chapter~}

% Custom formatting for division entries (styled like parts)
\newcommand{\divisionchapter}[1]{%
  \addvspace{12pt}%
  \noindent\hfil\bfseries\color{crimson}#1\hfil\par%
  \addvspace{6pt}%
}

% Adjust TOC spacing for "Chapter" prefix
\newlength{\xtraspace}
\settowidth{\xtraspace}{\cftchappresnum\cftchapaftersnum}
\addtolength{\cftchapnumwidth}{\xtraspace}

% Unnumbered chapters with TOC entry
\newcommand{\likechapter}[1]{%
    \chapter*{#1}
    \addcontentsline{toc}{chapter}{\textcolor{crimson}{#1}}
}

% =============================================================================
% PAGE NUMBERING SYSTEM
% =============================================================================
% Implements traditional book numbering:
% - Roman numerals (i, ii, iii...) for frontmatter
% - Arabic numerals (1, 2, 3...) for mainmatter
% Automatically switches at first numbered chapter
\makeatletter
\newif\if@firstnumbered%
\@firstnumberedtrue%
\newif\if@firstunnumbered%
\@firstunnumberedtrue%

\newcounter{lastRomanPage}
\setcounter{lastRomanPage}{1}

% Start document with Roman numerals (frontmatter)
\AtBeginDocument{
  \pagenumbering{roman}
  \renewcommand{\thepage}{\roman{page}}
}

% Intercept chapter command
\let\old@chapter\chapter%
\renewcommand{\chapter}{%
  \@ifstar{\unnumbered@chapter}{\numbered@chapter}%
}

% Numbered chapters: switch to Arabic on first occurrence
\newcommand{\numbered@chapter}[1]{%
  \if@firstnumbered%
    \cleardoublepage%
    \setcounter{lastRomanPage}{\value{page}}%
    \pagenumbering{arabic}%
    \@firstnumberedfalse%
  \else
    \setcounter{page}{\value{page}}%
  \fi
  \setcounter{sidenote}{1}                    % Reset footnote counter per chapter
  \old@chapter{#1}%
}

% Unnumbered chapters: stay in Roman numerals
\newcommand{\unnumbered@chapter}[1]{%
  \if@firstunnumbered%
    \clearpage
    \setcounter{lastRomanPage}{\value{page}}%
    \pagenumbering{roman}%
    \@firstunnumberedfalse%
  \fi
  \setcounter{sidenote}{1}
  \old@chapter*{#1}%
}
\makeatother

% =============================================================================
% TABLE SIZING AND SPACING
% =============================================================================
% Make tables slightly smaller to fit more content
\AtBeginEnvironment{longtable}{\scriptsize}

% Increase vertical spacing in table cells (default is 1.0)
\renewcommand{\arraystretch}{1.3}

% Prefer placing figures and tables at the top of pages
\makeatletter
\renewcommand{\fps@figure}{t}  % Default placement: top of page
\renewcommand{\fps@table}{t}   % Default placement: top of page
\makeatother

% =============================================================================
% LONGTABLE PAGE BREAKING FIXES (Windows compatibility)
% =============================================================================
% Prevent "Infinite glue shrinkage" errors on Windows LaTeX builds
% by giving longtable more flexibility in page breaking

% Allow more flexible page breaking (vs strict \flushbottom)
\raggedbottom

% Process more rows before attempting page break (default is 20)
\setcounter{LTchunksize}{50}

% Add extra stretch for longtable environments specifically
\AtBeginEnvironment{longtable}{%
  \setlength{\emergencystretch}{3em}%
  \setlength{\parskip}{0pt plus 1pt}%
}

% =============================================================================
% TABLE STYLING - Clean tables with crimson borders
% =============================================================================
% Professional table appearance with:
% - Clean white background (no colored rows)
% - Crimson-colored borders
% - Good spacing for readability
%
% Note: Headers are automatically bolded by Quarto when using **text** in source
\usepackage{booktabs}      % Professional table rules (\toprule, \midrule, \bottomrule)
\usepackage{colortbl}      % For colored borders (\arrayrulecolor)

% Global table styling - crimson borders
\setlength{\arrayrulewidth}{0.5pt}          % Thinner borders than default
%\arrayrulecolor{crimson}                    % Crimson borders matching brand

\setcounter{chapter}{0}
\usepackage{needspace}
\let\Needspace\needspace
\makeatletter
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{plain}
\@ifundefined{c@chapter}{\newfloat{vid}{h}{lovid}}{\newfloat{vid}{h}{lovid}[chapter]}
\floatname{vid}{Video}
\newcommand*\listofvids{\listof{vid}{List of Videos}}
\makeatother
\makeatletter
\@ifpackageloaded{tcolorbox}{}{\usepackage[skins,breakable]{tcolorbox}}
\@ifpackageloaded{fontawesome5}{}{\usepackage{fontawesome5}}
\definecolor{quarto-callout-color}{HTML}{909090}
\definecolor{quarto-callout-note-color}{HTML}{0758E5}
\definecolor{quarto-callout-important-color}{HTML}{CC1914}
\definecolor{quarto-callout-warning-color}{HTML}{EB9113}
\definecolor{quarto-callout-tip-color}{HTML}{00A047}
\definecolor{quarto-callout-caution-color}{HTML}{FC5300}
\definecolor{quarto-callout-color-frame}{HTML}{acacac}
\definecolor{quarto-callout-note-color-frame}{HTML}{4582ec}
\definecolor{quarto-callout-important-color-frame}{HTML}{d9534f}
\definecolor{quarto-callout-warning-color-frame}{HTML}{f0ad4e}
\definecolor{quarto-callout-tip-color-frame}{HTML}{02b875}
\definecolor{quarto-callout-caution-color-frame}{HTML}{fd7e14}
\makeatother
\makeatletter
\@ifpackageloaded{bookmark}{}{\usepackage{bookmark}}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\makeatletter
\@ifpackageloaded{sidenotes}{}{\usepackage{sidenotes}}
\@ifpackageloaded{marginnote}{}{\usepackage{marginnote}}
\makeatother
\newcommand{\fbxIconPath}{assets/images/icons/callouts}
\newcommand{\fbxIconFormat}{pdf}
\makeatletter
\@ifpackageloaded{tcolorbox}{}{\usepackage[many]{tcolorbox}}
\makeatother
%%%% ---foldboxy preamble ----- %%%%%

% Load xstring for string manipulation
\RequirePackage{xstring}

% Icon path and format configuration - can be overridden in filter-metadata
\providecommand{\fbxIconPath}{assets/images/icons/callouts}
\providecommand{\fbxIconFormat}{pdf}

% Helper command to include icon with hyphen-to-underscore conversion
% This ensures consistency: callout-quiz-question -> callout_quiz_question
\newcommand{\fbxIncludeIcon}[2]{%
  \StrSubstitute{#1}{-}{_}[\fbxIconName]%
  \includegraphics[width=#2]{\fbxIconPath/icon_\fbxIconName.\fbxIconFormat}%
}

% Legacy fallback colors (keep for compatibility)
\definecolor{fbx-default-color1}{HTML}{c7c7d0}
\definecolor{fbx-default-color2}{HTML}{a3a3aa}
\definecolor{fbox-color1}{HTML}{c7c7d0}
\definecolor{fbox-color2}{HTML}{a3a3aa}

% arguments: #1 typelabelnummer: #2 titel: #3
\newenvironment{fbx}[3]{%
\begin{tcolorbox}[
  enhanced,
  breakable,
  %fontupper=\fontsize{8pt}{10pt}\selectfont,  % 95% of body text (10pt -> 9.5pt)
  before skip=8pt,  % space above box (increased)
  after skip=8pt,   % space below box (increased)
  attach boxed title to top*={xshift=0pt},
  boxed title style={
  %fuzzy shadow={1pt}{-1pt}{0mm}{0.1mm}{gray},
  arc=1.5pt,
  rounded corners=north,
  sharp corners=south,
  top=6pt,          % Adjusted for ~40px equivalent height
  bottom=5pt,       % Adjusted for ~40px equivalent height
  overlay={
      \node [left,outer sep=0em, black,draw=none,anchor=west,
        rectangle,fill=none,inner sep=0pt]
        at ([xshift=4mm]frame.west) {\fbxIncludeIcon{#1}{4.2mm}};
    },
  },
  colframe=#1-color2,             % Border color (auto-generated from YAML)
  colbacktitle=#1-color1,         % Background color (auto-generated from YAML)
  colback=white,
  coltitle=black,
  titlerule=0mm,
  toprule=0.5pt,
  bottomrule=0.5pt,
  leftrule=2.2pt,
  rightrule=0.5pt,
  outer arc=1.5pt,
  arc=1.5pt,
  left=0.5em,       % increased left padding
  bottomtitle=1.5mm, % increased title bottom margin
  toptitle=1.5mm,    % increased title top margin
  title=\hspace{2.5em}\protect#2\hspace{0.5em}\protect#3, % Protect parameters
  extras middle and last={top=4pt} % increased continuation spacing
]}
{\end{tcolorbox}}


% boxed environment with right border
\newenvironment{fbxSimple}[3]{\begin{tcolorbox}[
  enhanced,
  breakable,
  %fontupper=\fontsize{8pt}{10pt}\selectfont,  % 95% of body text (10pt -> 9.5pt)
  before skip=8pt,  % space above box (increased)
  after skip=8pt,   % space below box (increased)
  attach boxed title to top*={xshift=0pt},
  boxed title style={
  %fuzzy shadow={1pt}{-1pt}{0mm}{0.1mm}{gray},
  arc=1.5pt,
  rounded corners=north,
  sharp corners=south,
  top=6pt,          % Adjusted for ~40px equivalent height
  bottom=5pt,       % Adjusted for ~40px equivalent height
  overlay={
      \node [left,outer sep=0em, black,draw=none,anchor=west,
        rectangle,fill=none,inner sep=0pt]
        at ([xshift=3mm]frame.west) {\fbxIncludeIcon{#1}{4.2mm}};
    },
  },
  colframe=#1-color2,             % Border color (auto-generated from YAML)
  colbacktitle=#1-color1,         % Background color (auto-generated from YAML)
  colback=white,
  coltitle=black,
  titlerule=0mm,
  toprule=0.5pt,
  bottomrule=0.5pt,
  leftrule=2.2pt,
  rightrule=0.5pt,
  outer arc=1.5pt,
  arc=1.5pt,
  left=0.5em,       % increased left padding
  bottomtitle=1.5mm, % increased title bottom margin
  toptitle=1.5mm,    % increased title top margin
  title=\hspace{2.5em}\protect#2\hspace{0.5em}\protect#3, % Protect parameters
  boxsep=1pt,
  extras first={bottom=0pt},
  extras last={top=0pt,bottom=-4pt},
  overlay first={
    \draw[line width=1pt,white] ([xshift=2.2pt]frame.south west)-- ([xshift=-0.5pt]frame.south east);
  },
  overlay last={
    \draw[line width=1pt,white] ([xshift=2.2pt]frame.north west)-- ([xshift=-0.5pt]frame.north east);
   }
]}
{\end{tcolorbox}}

%%%% --- end foldboxy preamble ----- %%%%%
%%==== colors from yaml ===%
\definecolor{callout-theorem-color1}{HTML}{F5F0FA}
\definecolor{callout-theorem-color2}{HTML}{6B46C1}
\definecolor{callout-checkpoint-color1}{HTML}{FFF8E7}
\definecolor{callout-checkpoint-color2}{HTML}{D97706}
\definecolor{callout-resource-slides-color1}{HTML}{E0F2F1}
\definecolor{callout-resource-slides-color2}{HTML}{20B2AA}
\definecolor{callout-definition-color1}{HTML}{F0F4F8}
\definecolor{callout-definition-color2}{HTML}{1B4F72}
\definecolor{callout-colab-color1}{HTML}{FFF5E6}
\definecolor{callout-colab-color2}{HTML}{FF6B35}
\definecolor{callout-lighthouse-color1}{HTML}{FDF8E6}
\definecolor{callout-lighthouse-color2}{HTML}{B8860B}
\definecolor{callout-quiz-question-color1}{HTML}{F0F0F8}
\definecolor{callout-quiz-question-color2}{HTML}{5B4B8A}
\definecolor{callout-resource-exercises-color1}{HTML}{E0F2F1}
\definecolor{callout-resource-exercises-color2}{HTML}{20B2AA}
\definecolor{callout-code-color1}{HTML}{F2F4F8}
\definecolor{callout-code-color2}{HTML}{D1D7E0}
\definecolor{callout-principle-color1}{HTML}{F3F2FA}
\definecolor{callout-principle-color2}{HTML}{3D3B8E}
\definecolor{callout-quiz-answer-color1}{HTML}{E8F2EA}
\definecolor{callout-quiz-answer-color2}{HTML}{4a7c59}
\definecolor{callout-example-color1}{HTML}{F0F8F6}
\definecolor{callout-example-color2}{HTML}{148F77}
\definecolor{callout-takeaways-color1}{HTML}{FDF2F7}
\definecolor{callout-takeaways-color2}{HTML}{BE185D}
\definecolor{callout-chapter-connection-color1}{HTML}{FDF2F7}
\definecolor{callout-chapter-connection-color2}{HTML}{A51C30}
\definecolor{callout-notebook-color1}{HTML}{F2F7FF}
\definecolor{callout-notebook-color2}{HTML}{2C5282}
\definecolor{callout-resource-videos-color1}{HTML}{E0F2F1}
\definecolor{callout-resource-videos-color2}{HTML}{20B2AA}
\definecolor{callout-perspective-color1}{HTML}{F7F8FA}
\definecolor{callout-perspective-color2}{HTML}{4A5568}
%=============%

\usepackage{hyphenat}
\usepackage{ifthen}
\usepackage{calc}
\usepackage{calculator}



\usepackage{graphicx}
\usepackage{geometry}
\usepackage{afterpage}
\usepackage{tikz}
\usetikzlibrary{calc}
\usetikzlibrary{fadings}
\usepackage[pagecolor=none]{pagecolor}


% Set the titlepage font families







% Set the coverpage font families

\usepackage{bookmark}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  pdftitle={Advanced Machine Learning Systems},
  pdfauthor={Vijay Janapa Reddi},
  colorlinks=true,
  linkcolor={Maroon},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}


\title{Advanced Machine Learning Systems}
\author{Vijay Janapa Reddi}
\date{January 31, 2026}
\begin{document}
%%%%% begin titlepage extension code

  \begin{frontmatter}

\begin{titlepage}
% This is a combination of Pandoc templating and LaTeX
% Pandoc templating https://pandoc.org/MANUAL.html#templates
% See the README for help

\thispagestyle{empty}

\newgeometry{top=-100in}

% Page color

\newcommand{\coverauthorstyle}[1]{{\fontsize{20}{24.0}\selectfont
{#1}}}

\begin{tikzpicture}[remember picture, overlay, inner sep=0pt, outer sep=0pt]

\tikzfading[name=fadeout, inner color=transparent!0,outer color=transparent!100]
\tikzfading[name=fadein, inner color=transparent!100,outer color=transparent!0]
\node[anchor=south west, rotate=0, opacity=1] at ($(current page.south west)+(0.225\paperwidth, 9)$) {
\includegraphics[width=\paperwidth, keepaspectratio]{assets/images/covers/cover-image-transparent-vol2.png}};

% Title
\newcommand{\titlelocationleft}{0.075\paperwidth}
\newcommand{\titlelocationbottom}{0.4\paperwidth}
\newcommand{\titlealign}{left}

\begin{scope}{%
\fontsize{52}{62.4}\selectfont
\node[anchor=north
west, align=left, rotate=0] (Title1) at ($(current page.south west)+(\titlelocationleft,\titlelocationbottom)$)  [text width = 0.9\paperwidth]  {{\nohyphens{Machine
Learning Systems}}};
}
\end{scope}

% Author
\newcommand{\authorlocationleft}{.925\paperwidth}
\newcommand{\authorlocationbottom}{0.225\paperwidth}
\newcommand{\authoralign}{right}

\begin{scope}
{%
\fontsize{20}{24.0}\selectfont
\node[anchor=north
east, align=right, rotate=0] (Author1) at ($(current page.south west)+(\authorlocationleft,\authorlocationbottom)$)  [text width = 6in]  {\coverauthorstyle{Vijay
Janapa Reddi\\}};
}
\end{scope}

% Footer
\newcommand{\footerlocationleft}{0.075\paperwidth}
\newcommand{\footerlocationbottom}{0.475\paperwidth}
\newcommand{\footerlocationalign}{left}

\begin{scope}
{%
\fontsize{25}{30.0}\selectfont
 \node[anchor=north west, align=left, rotate=0] (Footer1) at %
($(current page.south west)+(\footerlocationleft,\footerlocationbottom)$)  [text width = 0.9\paperwidth]  {{\nohyphens{Advanced}}};
}
\end{scope}

\end{tikzpicture}
\clearpage
\restoregeometry
%%% TITLE PAGE START

% Set up alignment commands
%Page
\newcommand{\titlepagepagealign}{
\ifthenelse{\equal{left}{right}}{\raggedleft}{}
\ifthenelse{\equal{left}{center}}{\centering}{}
\ifthenelse{\equal{left}{left}}{\raggedright}{}
}


\newcommand{\titleandsubtitle}{
% Title and subtitle
{{\huge{\bfseries{\nohyphens{Advanced Machine Learning Systems}}}}\par
}%
}
\newcommand{\titlepagetitleblock}{
\titleandsubtitle
}

\newcommand{\authorstyle}[1]{{\large{#1}}}

\newcommand{\affiliationstyle}[1]{{\large{#1}}}

\newcommand{\titlepageauthorblock}{
{\authorstyle{\nohyphens{Vijay Janapa
Reddi}{\textsuperscript{1}}\textsuperscript{,}{\textsuperscript{,*}}}}}

\newcommand{\titlepageaffiliationblock}{
\hangindent=1em
\hangafter=1
{\affiliationstyle{
{1}.~Harvard University


\vspace{1\baselineskip}
* \textit{Correspondence:}~Vijay Janapa Reddi~vj@eecs.harvard.edu
}}
}
\newcommand{\headerstyled}{%
{}
}
\newcommand{\footerstyled}{%
{\large{}}
}
\newcommand{\datestyled}{%
{January 31, 2026}
}


\newcommand{\titlepageheaderblock}{\headerstyled}

\newcommand{\titlepagefooterblock}{
\footerstyled
}

\newcommand{\titlepagedateblock}{
\datestyled
}

%set up blocks so user can specify order
\newcommand{\titleblock}{{

{\titlepagetitleblock}
}

\vspace{4\baselineskip}
}

\newcommand{\authorblock}{{\titlepageauthorblock}

\vspace{2\baselineskip}
}

\newcommand{\affiliationblock}{{\titlepageaffiliationblock}

\vspace{0pt}
}

\newcommand{\logoblock}{}

\newcommand{\footerblock}{}

\newcommand{\dateblock}{{\titlepagedateblock}

\vspace{0pt}
}

\newcommand{\headerblock}{}

\thispagestyle{empty} % no page numbers on titlepages


\newcommand{\vrulecode}{\textcolor{black}{\rule{\vrulewidth}{\textheight}}}
\newlength{\vrulewidth}
\setlength{\vrulewidth}{2pt}
\newlength{\B}
\setlength{\B}{\ifdim\vrulewidth > 0pt 0.05\textwidth\else 0pt\fi}
\newlength{\minipagewidth}
\ifthenelse{\equal{left}{left} \OR \equal{left}{right} }
{% True case
\setlength{\minipagewidth}{\textwidth - \vrulewidth - \B - 0.1\textwidth}
}{
\setlength{\minipagewidth}{\textwidth - 2\vrulewidth - 2\B - 0.1\textwidth}
}
\ifthenelse{\equal{left}{left} \OR \equal{left}{leftright}}
{% True case
\raggedleft % needed for the minipage to work
\vrulecode
\hspace{\B}
}{%
\raggedright % else it is right only and width is not 0
}
% [position of box][box height][inner position]{width}
% [s] means stretch out vertically; assuming there is a vfill
\begin{minipage}[b][\textheight][s]{\minipagewidth}
\titlepagepagealign
\titleblock

Prof.~Vijay Janapa Reddi

School of Engineering and Applied Sciences

Harvard University

\vspace{80mm}

With heartfelt gratitude to the community for their invaluable
contributions and steadfast support.

\vfill

January 31, 2026

\vfill
\par

\end{minipage}\ifthenelse{\equal{left}{right} \OR \equal{left}{leftright} }{
\hspace{\B}
\vrulecode}{}
\clearpage
%%% TITLE PAGE END
\end{titlepage}
\setcounter{page}{1}
\end{frontmatter}

%%%%% end titlepage extension code

\renewcommand*\contentsname{Table of contents}
{
\hypersetup{linkcolor=}
\setcounter{tocdepth}{2}
\tableofcontents
}
\listoffigures
\listoftables

\mainmatter
\bookmarksetup{startatroot}

\chapter*{Welcome to Volume II}\label{welcome-to-volume-ii}
\addcontentsline{toc}{chapter}{Welcome to Volume II}

\markboth{Welcome to Volume II}{Welcome to Volume II}

\section*{What You Will Learn}\label{what-you-will-learn}
\addcontentsline{toc}{section}{What You Will Learn}

\markright{What You Will Learn}

Volume II extends the foundations into production-scale systems through
five parts:

\begin{itemize}
\tightlist
\item
  \textbf{Part I: Foundations of Scale} --- Master the algorithms of
  scale. Learn how to coordinate computation across thousands of devices
  using Data, Tensor, and Pipeline parallelism, and understand the
  collective communication primitives and fault tolerance mechanisms
  that synchronize them.
\item
  \textbf{Part II: Building the Machine Learning Fleet} --- Build the
  physical computer. Architect the datacenter infrastructure,
  accelerators, and high-performance storage systems required to support
  distributed workloads at scale.
\item
  \textbf{Part III: Deployment at Scale} --- Serve the world. Navigate
  the shift from training to inference, push intelligence to the edge,
  and manage the operational lifecycle of production fleets.
\item
  \textbf{Part IV: Production Concerns} --- Harden the system. Address
  the non-functional requirements of privacy, security, robustness, and
  environmental sustainability in large-scale operations.
\item
  \textbf{Part V: Responsible AI at Scale} --- Shape the future. Explore
  responsible AI governance, AI for social good, and the emerging
  frontiers of AGI systems.
\end{itemize}

\section*{Prerequisites}\label{prerequisites}
\addcontentsline{toc}{section}{Prerequisites}

\markright{Prerequisites}

This volume assumes:

\begin{itemize}
\tightlist
\item
  \textbf{Foundational or equivalent} background in single-machine ML
  systems
\item
  \textbf{Programming proficiency} in Python with familiarity in NumPy
\item
  \textbf{Mathematics foundations} in linear algebra, calculus, and
  probability
\item
  Familiarity with distributed systems concepts (networking,
  parallelism) is helpful for advanced topics
\end{itemize}

\section*{Support Our Mission}\label{support-our-mission}
\addcontentsline{toc}{section}{Support Our Mission}

\markright{Support Our Mission}

\section*{Listen to the AI Podcast}\label{listen-to-the-ai-podcast}
\addcontentsline{toc}{section}{Listen to the AI Podcast}

\markright{Listen to the AI Podcast}

\section*{Want to Help Out?}\label{want-to-help-out}
\addcontentsline{toc}{section}{Want to Help Out?}

\markright{Want to Help Out?}

This is a collaborative project, and your input matters. If you'd like
to contribute, check out our
\href{https://github.com/harvard-edge/cs249r_book/blob/dev/docs/contribute.md}{contribution
guidelines}. Feedback, corrections, and new ideas are welcome. Simply
file a GitHub
\href{https://github.com/harvard-edge/cs249r_book/issues}{issue}.

\bookmarksetup{startatroot}

\chapter{---}\label{section}

\bookmarksetup{startatroot}

\chapter{Communication and Collective
Operations}\label{sec-communication-collective-operations-collective-operations}

\marginnote{\begin{footnotesize}

\emph{Gemini Pro 3 Prompt: A technical visualization of collective
communication patterns in distributed computing. The scene shows
multiple compute nodes arranged in various topologies: a ring formation
demonstrating ring allreduce with data flowing clockwise, a star pattern
showing parameter server architecture with a central aggregator, and a
mesh topology with all-to-all connections. Each node is depicted as a
stylized GPU with data packets traveling along luminous pathways between
them. Visual elements include bandwidth indicators showing throughput on
each link, latency clocks measuring communication time, and gradient
tensors being reduced and broadcast. The composition uses a dark
background with nodes in metallic silver and communication paths in
vibrant colors: green for scatter operations, blue for gather, orange
for reduce, and purple for broadcast. Technical diagram style with clear
labeling, suitable for a networking and systems textbook. Rendered in
the style of Nanobanana.}

\end{footnotesize}}

\noindent
\pandocbounded{\includegraphics[keepaspectratio]{contents/vol2/communication/images/png/cover_communication.png}}

\section*{Purpose}\label{purpose}
\addcontentsline{toc}{section}{Purpose}

\markright{Purpose}

\emph{Why does communication between machines become the fundamental
constraint that governs large-scale machine learning systems?}

Computation scales by adding processors; communication scales by moving
data between them. These scale differently: adding a processor increases
aggregate compute linearly, but coordinating that processor with all
others increases communication quadratically or worse depending on the
synchronization pattern. At sufficient scale, the time spent exchanging
gradients, activations, and parameters exceeds the time spent computing
them. This crossover point is not a bug to be fixed but a fundamental
property of distributed systems that determines which parallelization
strategies work, which model sizes are trainable, and which
organizations can operate at frontier scale. The physics of light-speed
delays, bandwidth limits, and energy costs of data movement constrain
communication as firmly as transistor physics constrains
computation---yet communication is far less intuitive to reason about,
making it the hidden bottleneck that undermines systems designed by
those who understand only the compute side.

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-tip-color-frame, toprule=.15mm, left=2mm, breakable, titlerule=0mm, opacityback=0, rightrule=.15mm, opacitybacktitle=0.6, coltitle=black, arc=.35mm, colbacktitle=quarto-callout-tip-color!10!white, toptitle=1mm, leftrule=.75mm, bottomtitle=1mm, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Learning Objectives}, bottomrule=.15mm, colback=white]

\begin{itemize}
\tightlist
\item
  Apply the \(\alpha\)-\(\beta\) model to quantify when communication
  becomes the dominant bottleneck by deriving the bandwidth and latency
  bounds for distributed training at different cluster scales.
\item
  Compare AllReduce algorithms (ring, tree, hierarchical) by analyzing
  their time complexity and identifying the crossover points where each
  becomes optimal based on message size and cluster scale.
\item
  Select appropriate collective primitives (AllReduce, AlltoAll,
  AllGather, ReduceScatter) for different model architectures by
  matching communication patterns to workload requirements.
\item
  Evaluate gradient compression techniques by analyzing bandwidth
  reduction versus convergence impact trade-offs for quantization,
  sparsification, and error feedback mechanisms.
\item
  Design topology-aware communication strategies by mapping collective
  operations to network architectures (fat-tree, rail-optimized, torus)
  to maximize bisection bandwidth utilization.
\item
  Implement communication-computation overlap using pipelining and
  asynchronous operations to hide network latency behind gradient
  computation.
\end{itemize}

\end{tcolorbox}

\section{Communication
Fundamentals}\label{sec-communication-collective-operations-collective-operations-communication-fundamentals-44eb}

In the \textbf{Systems Sandwich} (\textbf{?@sec-vol2-introduction}),
Communication algorithms sit squarely in the \textbf{Operational Layer}.
While the physical network (Part II) provides the raw bandwidth (the
road), these algorithms determine the traffic patterns (the traffic
lights). If the \textbf{Iron Law} defines the step time, then
communication is the \textbf{Friction}---the term that grows with scale
and resists our attempts to train larger models.

The parallelism strategies introduced in
\textbf{?@sec-distributed-training-systems}---data parallelism, model
parallelism, and pipeline parallelism---all share a common assumption:
workers can exchange data efficiently. That assumption hides what
becomes the dominant engineering challenge at scale.

This transition reveals a fundamental asymmetry in how computation and
communication scale. Computation is inherently local: each GPU operates
on its own data independently, so adding GPUs increases aggregate
compute capacity proportionally. Communication, however, is inherently
global: ensuring all GPUs converge on the same synchronized state
requires information to traverse physical distances between them. Adding
more independent workers scales local work linearly, but coordinating
those workers requires moving data across physical space. The most
critical instance of this coordination is \emph{gradient
synchronization}.

\phantomsection\label{callout-definitionux2a-2.1}
\begin{fbx}{callout-definition}{Definition:}{Gradient Synchronization}
\phantomsection\label{callout-definition*-2.1}
\textbf{\emph{Gradient Synchronization}} is the collective communication
phase in distributed training where workers exchange and aggregate their
locally computed gradients. This step ensures that all workers update
their model parameters with the same global gradient information,
maintaining mathematical equivalence to single-device training.

\end{fbx}

\subsection{The Physics of Data
Movement}\label{sec-communication-collective-operations-physics-data-movement}

Before designing algorithms, we must understand the physical constraints
governing data movement. Unlike software, which can be optimized almost
indefinitely, communication is bound by the speed of light and the
Shannon limit of channel capacity.

\textbf{1. The Speed of Light Latency Floor} In a vacuum, light travels
at \(c \approx 300,000\) km/s. In optical fiber, the refractive index
(\(n \approx 1.5\)) slows this to \(\approx 200,000\) km/s, or 5
microseconds per kilometer. A datacenter spanning 500 meters introduces
a minimum 2.5 \(\mu s\) round-trip propagation delay. While negligible
for human perception, this is thousands of clock cycles for a GPU
operating at 1.5 GHz.

\textbf{2. The Bandwidth Energy Tax} Moving data costs energy. Accessing
data from local SRAM costs picojoules. Moving it across a PCB (NVLink)
costs roughly 5-10 pJ/bit. Moving it across a datacenter (InfiniBand
optical) costs 20-50 pJ/bit. At the exascale, the power budget for
communication rivals the power budget for computation.

\textbf{3. Protocol Overhead} The physical wire is fast, but the
software stack is slow. Traditional TCP/IP stacks incur microseconds of
OS kernel overhead per packet. High-performance ML networks bypass the
kernel using \textbf{RDMA (Remote Direct Memory Access)}, allowing the
network card (NIC) to write directly into GPU memory.

\section{The Gradient's Travel Manifest: From Parallelism to
Patterns}\label{sec-communication-parallelism-patterns}

The journey begins at the moment the backward pass completes. In Volume
1, this was the end of the story---the weights were updated, and the
neuron learned. But at production scale, the gradient is born into
isolation. It exists on one GPU, while the ``truth'' of the model is
distributed across thousands. To achieve global convergence, our
gradient must find its peers.

The specific ``travel manifest'' for this journey is dictated by the
parallelism strategy chosen in
\textbf{?@sec-distributed-training-systems}. The choice of \emph{how we
split the math} determines the \emph{how we move the data}. For our
\textbf{Lighthouse Archetypes}, these manifests look very different:

\begin{itemize}
\tightlist
\item
  \textbf{Archetype A (The LLM)}: Our gradient is part of a massive,
  dense tensor. It needs to meet every other gradient in the fleet to
  compute a global average. Its primary vehicle is the
  \textbf{AllReduce}.
\item
  \textbf{Archetype B (The MoE/RecSys)}: Our gradient (or activation) is
  sparse and targeted. It doesn't need to meet everyone; it needs to
  find one specific ``Expert'' GPU across the datacenter. Its vehicle is
  the \textbf{AlltoAll}.
\end{itemize}

Understanding this mapping is essential: the \emph{what} of parallelism
directly determines the \emph{how} of communication.
Table~\ref{tbl-parallelism-communication-mapping} summarizes these
relationships.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2821}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.1795}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2628}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.0064}}@{}}
\caption{\textbf{The Travel Manifest}: How the high-level math of
\textbf{?@sec-distributed-training-systems} manifests as low-level
traffic
patterns.}\label{tbl-parallelism-communication-mapping}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Parallelism Strategy}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{The Gradient's Goal}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Primary Primitive}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Primary Constraint}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Parallelism Strategy}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{The Gradient's Goal}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Primary Primitive}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Primary Constraint}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Data Parallelism} & Meet everyone, compute global average &
AllReduce & Bandwidth (Large payloads) & \\
\textbf{FSDP / ZeRO} & Find shards, reconstruct the whole & AllGather +
ReduceScatter & Bandwidth (High frequency) & \\
\textbf{Tensor Parallelism} & Quick handshake within the node &
AllReduce & Latency (Speed is life) & \\
\textbf{Pipeline Parallelism} & Handoff to the next neighbor &
Point-to-Point (Send/Recv) & Latency (Sequential dependencies) & \\
\multicolumn{5}{@{}>{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.9808} + 8\tabcolsep}@{}}{%
\textbf{Expert Parallelism (MoE)}\sidenote{\textbf{Mixture of Experts
(MoE)}: An architecture where input tokens are routed to specialized
subnetworks (experts) rather than processed by the entire model. Each
token activates only a subset of experts, reducing computation while
maintaining model capacity. GPT-4 and Mixtral use MoE architectures. }
\textbar{} Targeted routing to a specialist \textbar{} AlltoAll
\textbar{} Latency + Contention} \\
\end{longtable}

\section{Mapping the Terrain: Network Performance
Modeling}\label{sec-communication-collective-operations-collective-operations-network-performance-modeling-0d8e}

As our gradient begins its journey, it immediately encounters the
physical reality of the datacenter. In a single-GPU world, memory access
is nearly instantaneous. In the Machine Learning Fleet, the network is a
``dark forest'' of delays and bottlenecks. To navigate this terrain, we
need a map.

\subsection{The α-β Reality: When Physics Fights
Back}\label{sec-communication-collective-operations-collective-operations-alphabeta-model-f9b4}

Every step our gradient takes is governed by the linear cost model
\(T(n) = \alpha + n/\beta\). This isn't just a formula; it's the
``Physics of Failure'' for distributed systems.

\phantomsection\label{callout-definitionux2a-2.2}
\begin{fbx}{callout-definition}{Definition:}{The α-β Model (Hockney Model)}
\phantomsection\label{callout-definition*-2.2}
\textbf{\emph{The α-β Model}} is a linear cost model for network
communication where the time to transfer a message of \(n\) bytes is
\(T(n) = \alpha + n/\beta\). The parameter \textbf{α} (alpha) represents
the fixed startup latency---the cost to send any message regardless of
size. The parameter \textbf{β} (beta) represents the link bandwidth in
bytes per second. This model separates the \textbf{latency-bound regime}
(small messages dominated by α) from the \textbf{bandwidth-bound regime}
(large messages dominated by \(n/\beta\)), enabling architects to
identify which bottleneck to optimize for a given workload.

\end{fbx}

\textbf{Parameter Definitions:}

\begin{itemize}
\tightlist
\item
  \textbf{Latency (\(\alpha\))}: The fixed start-up cost to send a
  message, regardless of size. This includes software overhead (kernel
  launch, NCCL initialization), PCIe traversal, and network switching
  time.
\item
  \textbf{Bandwidth (\(\beta\))}: The sustained data transfer rate
  (bytes per second).
\end{itemize}

The \textbf{critical message size} \(n^* = \alpha \cdot \beta\) marks
the crossover point: messages smaller than \(n^*\) are latency-bound;
messages larger are bandwidth-bound.

Table~\ref{tbl-interconnect-parameters} shows typical values for modern
interconnects:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2989}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1954}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2184}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2874}}@{}}
\caption{\textbf{Interconnect Performance Parameters}: Typical latency
and bandwidth values for modern datacenter interconnects. The critical
size shows the crossover point where communication transitions from
latency-bound to
bandwidth-bound.}\label{tbl-interconnect-parameters}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Interconnect}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Latency (α)}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Bandwidth (β)}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Critical Size (n*)}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Interconnect}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Latency (α)}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Bandwidth (β)}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Critical Size (n*)}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
NVLink 4.0 (intra-node) & 1--2 μs & 900 GB/s & \textasciitilde1 MB \\
InfiniBand NDR 400G & 1--3 μs & 50 GB/s (per port) & \textasciitilde100
KB \\
InfiniBand HDR 200G & 2--5 μs & 25 GB/s & \textasciitilde75 KB \\
PCIe Gen5 (GPU↔CPU) & 2--5 μs & 64 GB/s & \textasciitilde200 KB \\
Ethernet 100G (RoCE) & 5--10 μs & 12 GB/s & \textasciitilde100 KB \\
\end{longtable}

The following example illustrates how to apply the critical message size
formula to determine which optimization strategy matters most for a
given workload:

\phantomsection\label{callout-notebookux2a-2.3}
\begin{fbx}{callout-notebook}{AI Engineer’s Notebook:}{The Critical Message Size}
\phantomsection\label{callout-notebook*-2.3}
\textbf{Problem}: Your cluster uses InfiniBand NDR 400G with
\(\alpha = 2\ \mu s\) and \(\beta = 50\ \text{GB/s}\). At what message
size does optimizing for bandwidth start to matter more than optimizing
for latency?

\textbf{The Math}:
\[n^* = \alpha \cdot \beta = 2 \times 10^{-6}\ \text{s} \times 50 \times 10^9\ \text{B/s} = 100\ \text{KB}\]

\textbf{The Systems Insight}: Messages under 100 KB (like MoE tokens,
pipeline activations) are \textbf{latency-bound}---buy lower-latency
switches, reduce software overhead. Messages over 100 KB (like LLM
gradients) are \textbf{bandwidth-bound}---buy more bandwidth, compress
the data. Applying the wrong optimization wastes money without improving
performance.

\end{fbx}

This reveals two distinct operating regimes:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Latency-Bound (\(n < n^*\))}: For small messages (e.g., MoE
  routing tokens, scalar reductions), time is dominated by \(\alpha\).
  Optimization focuses on \textbf{fusion} (batching small messages),
  \textbf{topology} (reducing hop count), and \textbf{software stack
  tuning} (kernel bypass via RDMA).
\item
  \textbf{Bandwidth-Bound (\(n \gg n^*\))}: For large messages (e.g.,
  LLM gradients, optimizer states), time is dominated by \(n/\beta\).
  Optimization focuses on \textbf{compression} (FP8, Top-K sparsity),
  \textbf{algorithm choice} (Ring vs Tree), and \textbf{link
  aggregation} (multi-rail NICs).
\end{enumerate}

To see how dramatically the bottleneck shifts between these regimes,
consider this comparison:

\phantomsection\label{callout-notebookux2a-2.4}
\begin{fbx}{callout-notebook}{AI Engineer’s Notebook:}{Latency vs. Bandwidth Dominance}
\phantomsection\label{callout-notebook*-2.4}
\textbf{Problem}: You are synchronizing a 1 MB buffer versus a 1 GB
buffer on InfiniBand NDR with \(\alpha = 2\ \mu s\) and
\(\beta = 50\ \text{GB/s}\). How does the bottleneck shift?

\textbf{Case A: 1 MB Message}

\begin{itemize}
\tightlist
\item
  Bandwidth Time: \(10^6 / (50 \times 10^{9}) = 20\ \mu s\).
\item
  Latency Time: \(2\ \mu s\).
\item
  \textbf{Total: 22 μs}. Latency is 9\% of total---still meaningful!
\end{itemize}

\textbf{Case B: 1 GB Message}

\begin{itemize}
\tightlist
\item
  Bandwidth Time:
  \(10^9 / (50 \times 10^{9}) = 20{,}000\ \mu s = 20\ \text{ms}\).
\item
  Latency Time: \(2\ \mu s\).
\item
  \textbf{Total: 20,002 μs}. Latency is 0.01\%---completely negligible.
\end{itemize}

\textbf{The Systems Conclusion}: For Data Parallelism (large gradients),
we optimize for \(\beta\)---compress gradients, add NICs. For
Pipeline/Expert Parallelism (small activations), we fight for every
microsecond of \(\alpha\)---kernel bypass, topology optimization. The
α-β model tells you \emph{which fight to pick}.

\end{fbx}

\subsection{The LogP
Model}\label{sec-communication-collective-operations-collective-operations-logp-model-e45d}

The α-β model assumes the processor is idle during communication. For
\textbf{pipelined systems} where we overlap communication with
computation, this assumption fails. The \textbf{LogP} model
(\citeproc{ref-culler1993logp}{Culler et al. 1993}) extends α-β with two
additional parameters:

\begin{itemize}
\tightlist
\item
  \textbf{L (Latency)}: The time for a message to traverse the network
  (similar to α).
\item
  \textbf{o (Overhead)}: The CPU/GPU time spent initiating or receiving
  a transfer. During this time, the processor \textbf{cannot
  compute}---this is the non-overlappable cost.
\item
  \textbf{g (Gap)}: The minimum time interval between consecutive
  message injections (inverse of message rate). This models \textbf{link
  contention}.
\item
  \textbf{P (Processors)}: The number of processors in the system.
\end{itemize}

The key insight of LogP is distinguishing \textbf{network latency} (L,
which can be hidden) from \textbf{processor overhead} (o, which cannot).
A system can overlap communication with computation only if the compute
kernel runs longer than the overhead. The following example demonstrates
this distinction in practice:

\phantomsection\label{callout-notebookux2a-2.5}
\begin{fbx}{callout-notebook}{AI Engineer’s Notebook:}{Can You Hide the Communication?}
\phantomsection\label{callout-notebook*-2.5}
\textbf{Problem}: You want to overlap gradient AllReduce with the next
layer's backward pass. The backward pass takes 500 μs. The AllReduce has
network latency \(L = 100\ \mu s\) but processor overhead
\(o = 50\ \mu s\) to initiate and \(o = 50\ \mu s\) to receive. Can you
hide the communication?

\textbf{The Math}:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Overlappable portion}: Network latency \(L = 100\ \mu s\)
  (data in flight while GPU computes).
\item
  \textbf{Non-overlappable portion}: \(2o = 100\ \mu s\) (GPU busy
  initiating/receiving).
\item
  \textbf{Compute available}: \(500\ \mu s\).
\item
  \textbf{Hidden}: All 100 μs of \(L\) can overlap with compute.
\item
  \textbf{Exposed}: The 100 μs of \(o\) cannot overlap.
\end{enumerate}

\textbf{Effective time}: \(\max(500, 100 + 100) = 500\ \mu s\)
(communication hidden!).

\textbf{The Systems Insight}: α-β tells you the total communication
time. LogP tells you \textbf{how much of it you can hide}. When
designing pipelined training, optimize for low \(o\) (kernel bypass,
GPUDirect) rather than just high \(\beta\).

\end{fbx}

\textbf{When to use which model:}

\begin{itemize}
\tightlist
\item
  \textbf{α-β Model}: Use for back-of-envelope calculations, algorithm
  selection (Ring vs Tree), and when communication is \textbf{blocking}
  (synchronous barriers).
\item
  \textbf{LogP Model}: Use when analyzing \textbf{pipelined execution},
  compute-communication overlap, or when debugging why a
  theoretically-fast algorithm underperforms (often high \(o\)).
\end{itemize}

\section{Choosing the Vehicle: Collective Operation
Primitives}\label{sec-communication-collective-operations-collective-operations-collective-operation-vocabulary-fdc7}

With the terrain mapped, our gradient must now choose its vehicle. In
the ML Fleet, we don't send raw messages; we use \textbf{Collective
Operations}---coordinated group exchanges that act as the mass-transit
systems of the supercomputer.

\phantomsection\label{callout-definitionux2a-2.6}
\begin{fbx}{callout-definition}{Definition:}{Collective Operation}
\phantomsection\label{callout-definition*-2.6}
\textbf{\emph{Collective Operation}} is a communication primitive where
a group of processes (workers) engage in a coordinated data exchange.
Unlike point-to-point communication (one sender, one receiver),
collective operations (like AllReduce or AllGather) involve all
participating processes simultaneously to aggregate, broadcast, or
redistribute data.

\end{fbx}

Understanding these primitives is essential because different model
architectures stress different operations.

\subsection{The Six Core Primitives}\label{the-six-core-primitives}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Broadcast}: One sender transmits data to all receivers.

  \begin{itemize}
  \tightlist
  \item
    \emph{Use Case}: Distributing initial model weights from rank 0 to
    all workers at startup. Used across \textbf{all parallelism
    strategies} during initialization.
  \item
    \emph{Complexity}: \(O(\log N)\) latency with tree algorithms;
    bandwidth cost \(O(M)\) as data traverses tree levels.
  \end{itemize}
\item
  \textbf{Reduce}: Data from all workers is aggregated (sum, min, max)
  to a single root worker.

  \begin{itemize}
  \tightlist
  \item
    \emph{Use Case}: Aggregating validation metrics (loss, accuracy) to
    a logging process. Common in \textbf{monitoring and checkpointing}
    across all training strategies.
  \item
    \emph{Complexity}: \(O(\log N)\) latency with tree algorithms;
    bandwidth cost \(O(M)\) total.
  \end{itemize}
\item
  \textbf{AllReduce}: Data from all workers is aggregated, and the
  \emph{result} is distributed to all workers.

  \begin{itemize}
  \tightlist
  \item
    \emph{Use Case}: \textbf{Data Parallelism}
    (\textbf{?@sec-distributed-training-systems}). Synchronizing
    gradients so every GPU computes the same weight update.
  \item
    \emph{Semantics}: \(y_i = \sum_{j=0}^{N-1} x_j\) for all \(i\).
  \item
    \emph{Complexity}: Ring achieves bandwidth-optimal
    \(2\frac{N-1}{N}\frac{M}{\beta}\) but \(O(N)\) latency; Tree
    achieves \(O(\log N)\) latency but suboptimal bandwidth.
  \end{itemize}
\item
  \textbf{AllGather}: Every worker sends its data to every other worker.
  The result is a concatenation of all inputs.

  \begin{itemize}
  \tightlist
  \item
    \emph{Use Case}: \textbf{Sharded Data Parallelism
    (FSDP\sidenote{\textbf{Fully Sharded Data Parallel (FSDP)}:
    PyTorch's implementation of ZeRO-3 that shards model parameters,
    gradients, and optimizer states across workers, using AllGather to
    reconstruct parameters on-demand during forward/backward passes.
    }/ZeRO)}. Collecting sharded parameters before a forward/backward
    pass.
  \item
    \emph{Semantics}: Worker \(i\) starts with \(x_i\), ends with
    \([x_0, x_1, \dots, x_{N-1}]\).
  \item
    \emph{Complexity}: Ring achieves \(\frac{N-1}{N}\frac{M}{\beta}\)
    bandwidth cost; total data grows to \(N \times M\) per worker.
  \end{itemize}
\item
  \textbf{ReduceScatter}: Data is reduced (summed), but the result is
  scattered such that each worker receives only a distinct chunk of the
  result.

  \begin{itemize}
  \tightlist
  \item
    \emph{Use Case}: \textbf{Sharded Data Parallelism}. Reducing
    gradients but keeping them sharded to save memory. Also used in
    \textbf{Sequence Parallelism} to distribute activations.
  \item
    \emph{Semantics}: Worker \(i\) receives the \(i\)-th block of
    \(\sum x_j\).
  \item
    \emph{Complexity}: Ring achieves \(\frac{N-1}{N}\frac{M}{\beta}\);
    functionally the inverse of AllGather.
  \end{itemize}
\item
  \textbf{AllToAll}: The most general pattern. Worker \(i\) sends a
  distinct chunk of data to worker \(j\). This is effectively a matrix
  transpose of distributed data.

  \begin{itemize}
  \tightlist
  \item
    \emph{Use Case}: \textbf{Mixture of Experts
    (MoE)}\sidenote{\textbf{MoE Communication Pattern}: In Mixture of
    Experts, each token must reach its assigned expert(s), which may
    reside on different workers. This requires AllToAll to route tokens
    dynamically based on gating decisions. } routing tokens to experts;
    \textbf{DLRM}\sidenote{\textbf{DLRM (Deep Learning Recommendation
    Model)}: Meta's architecture for click-through rate prediction.
    Embedding tables are sharded across workers, requiring AllToAll to
    exchange sparse embedding lookups during each forward pass. }
    exchanging embedding lookups across workers.
  \item
    \emph{Semantics}: Worker \(i\) sends \(x_{i \to j}\) to worker \(j\)
    and receives \(x_{j \to i}\) from worker \(j\), for all \(j\).
  \item
    \emph{Complexity}: \(O(N^2)\) logical connections create potential
    for network congestion; bandwidth cost is \(\frac{N-1}{N}M\) per
    worker, but contention makes this the hardest primitive to scale.
  \end{itemize}
\end{enumerate}

The contrast between AllToAll and AllReduce highlights a fundamental
difference in how collective operations scale with cluster size.

\phantomsection\label{callout-perspectiveux2a-2.7}
\begin{fbx}{callout-perspective}{Systems Perspective:}{AlltoAll vs AllReduce: Why Scale Differs}
\phantomsection\label{callout-perspective*-2.7}
While \textbf{AllReduce} scales efficiently because it can be pipelined
in a ring (where each node only talks to its neighbor),
\textbf{AlltoAll} is fundamentally harder to scale.

In an AlltoAll, every process has a unique piece of data for every other
process. This creates \(O(N^2)\) logical connections. At the hardware
level, this leads to \textbf{network contention}: if 1024 GPUs all try
to send data to different targets simultaneously, the ``Fat-Tree'' or
``Spine'' switches in the datacenter become the bottleneck.

This is why \textbf{Expert Parallelism (MoE)} and large-scale
\textbf{Recommendation Systems} often hit a ``communication wall'' much
earlier than standard data-parallel models. The algorithm choice
(AllReduce vs AlltoAll) determines the scaling ceiling.

\end{fbx}

Table~\ref{tbl-collective-selection} maps these primitives to the
parallelism strategies introduced in
\textbf{?@sec-distributed-training-systems} and the \textbf{Lighthouse}
architectures defined in the Introduction.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2812}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2812}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4271}}@{}}
\caption{\textbf{Collective Operation Selection Guide}: Matching
training strategies to their primary collective operations enables
efficient distributed communication design. MoE and DLRM serve as
canonical ``lighthouse'' workloads throughout Volume II, representing
sparse expert architectures and recommendation systems respectively.
Pipeline Parallelism uniquely relies on point-to-point rather than
collective
communication.}\label{tbl-collective-selection}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Training Strategy}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Primary Collective}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Bottleneck Characteristic}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Training Strategy}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Primary Collective}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Bottleneck Characteristic}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Data Parallelism} & AllReduce & Bandwidth-bound (large
gradients) \\
\textbf{FSDP / ZeRO-3} & AllGather, ReduceScatter & Bandwidth-bound,
high frequency \\
\textbf{Tensor Parallelism} & AllReduce, AllGather & Latency-bound
(requires NVLink) \\
\textbf{Pipeline Parallelism} &
\multicolumn{2}{>{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.7083} + 2\tabcolsep}@{}}{%
Point-to-Point (Send/Recv)\textbar{} Latency-bound (microbatch
handoffs)} \\
\textbf{Sequence Parallelism} & AllGather, ReduceScatter &
Bandwidth-bound (activation exchange) \\
\textbf{MoE (Experts)} & AlltoAll & Latency-bound (dynamic token
routing) \\
\textbf{DLRM (RecSys)} & AlltoAll & Latency \& Bandwidth (sparse
lookups) \\
\end{longtable}

\section{Engineering the Flow: AllReduce
Algorithms}\label{sec-communication-collective-operations-collective-operations-allreduce-algorithms-7545}

The AllReduce is the heartbeat of the fleet. Because it runs on every
training step, its efficiency determines whether our supercomputer is a
cohesive engine or a collection of idling chips. To keep the gradients
moving, we use two primary strategies: the \textbf{Ring} and the
\textbf{Tree}.

\subsection{Naive Approaches vs.~The Bandwidth
Bottleneck}\label{sec-communication-collective-operations-naive-vs-optimal}

Consider a naive implementation using a \textbf{Parameter Server} (Star
topology). All \(N\) workers send their gradients to rank 0; rank 0 sums
them and sends the result back. * \textbf{Bottleneck}: Rank 0's
bandwidth. It must receive \(N \times M\) bytes and send \(N \times M\)
bytes. * \textbf{Time}: \(T \propto N \times M / \beta\). *
\textbf{Result}: Linear slowdown. With 1000 GPUs, rank 0 melts.

To scale, we need algorithms where the communication volume per node is
\textbf{constant}, regardless of \(N\).

\subsection{Ring
AllReduce}\label{sec-communication-collective-operations-collective-operations-ring-allreduce-ffce}

Ring AllReduce\sidenote{\textbf{Ring AllReduce History}: Andrew
Gibiansky's 2017 blog post ``Bringing HPC Techniques to Deep Learning''
at Baidu popularized ring AllReduce for ML, demonstrating near-linear
scaling to hundreds of GPUs. The algorithm itself dates to the 1990s HPC
community, but its application to gradient synchronization catalyzed
modern distributed training. Uber's Horovod (2017) and NVIDIA's NCCL
subsequently optimized ring AllReduce for GPU clusters, making it the
default algorithm in PyTorch's DistributedDataParallel. } arranges nodes
in a logical ring (\(0 \to 1 \to \dots \to N-1 \to 0\)). It achieves
bandwidth optimality by pipelining: every node sends and receives
simultaneously on every link.

\textbf{The Algorithm}: The vector of size \(M\) is split into \(N\)
chunks. 1. \textbf{Scatter-Reduce Phase}: In \(N-1\) steps, chunks flow
around the ring, accumulating sums. At the end, each node holds the
complete sum for \emph{one} chunk (\(1/N\) of the total result). 2.
\textbf{AllGather Phase}: In \(N-1\) steps, the partial sums circulate
the ring until every node has all chunks.

The data flow during Step 1 proceeds as follows:

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-note-color-frame, toprule=.15mm, left=2mm, breakable, titlerule=0mm, opacityback=0, rightrule=.15mm, opacitybacktitle=0.6, coltitle=black, arc=.35mm, colbacktitle=quarto-callout-note-color!10!white, toptitle=1mm, leftrule=.75mm, bottomtitle=1mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Figure: Ring AllReduce Data Flow}, bottomrule=.15mm, colback=white]

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/99d118a2afa68101d0e8bad82ae209616709e1de.pdf}}

\end{tcolorbox}

\textbf{Performance Analysis}: Each node sends and receives
\(\frac{M}{N}\) bytes in each of the \(2(N-1)\) steps. \[
T_{ring} = \underbrace{2(N-1)\alpha}_{\text{Latency Term}} + \underbrace{2\frac{N-1}{N} \frac{M}{\beta}}_{\text{Bandwidth Term}}
\]

\begin{itemize}
\tightlist
\item
  \textbf{Bandwidth}: As \(N \to \infty\), the term approaches
  \(2M/\beta\). This is theoretically optimal (each byte must be sent
  once and received once).
\item
  \textbf{Latency}: The latency scales linearly with \(N\). For 10,000
  nodes, 20,000 sequential hops creates massive latency. This is why
  Ring is bad for small messages.
\end{itemize}

\subsection{Tree
AllReduce}\label{sec-communication-collective-operations-tree-allreduce}

To address the linear latency of Ring, Tree AllReduce uses a binary tree
structure.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Reduce Phase}: Leaves send to parents, parents sum and send
  up. Reaches root in \(\log_2 N\) steps.
\item
  \textbf{Broadcast Phase}: Root sends result down to leaves in another
  \(\log_2 N\) steps.
\end{enumerate}

\textbf{Why Tree Has Worse Bandwidth Efficiency}:

In Ring AllReduce, every node sends and receives simultaneously---all
\(N\) links are active at every step. In Tree AllReduce, only a fraction
of links are active at any time:

\begin{itemize}
\tightlist
\item
  At level 0 (leaves): \(N/2\) nodes send to \(N/2\) parents. Half the
  nodes are idle receivers.
\item
  At level 1: \(N/4\) nodes send to \(N/4\) parents. Three-quarters are
  idle.
\item
  At the root: Only 2 nodes communicate. \((N-2)\) nodes sit idle.
\end{itemize}

The result: while Ring achieves near-100\% link utilization, Tree
achieves only \(\approx 50\%\) on average because at each level, half
the participating nodes are receiving (not sending).

\textbf{Performance Analysis}:
\[ T_{tree} = \underbrace{2\log_2 N \cdot \alpha}_{\text{Latency}} + \underbrace{2 \log_2 N \frac{M}{\beta}}_{\text{Bandwidth}} \]

\begin{itemize}
\tightlist
\item
  \textbf{Latency}: Logarithmic (\(O(\log N)\)). For 1024 nodes, Ring
  needs 2046 steps while Tree needs only 20. This is Tree's advantage.
\item
  \textbf{Bandwidth}: The \(\log_2 N\) factor in the bandwidth term
  (vs.~Ring's constant factor of 2) means Tree sends \(\log_2 N / 2\)
  times more data per node. For 64 GPUs, that's \(6/2 = 3\times\) more
  bandwidth consumed.
\end{itemize}

\subsection{The Algorithm Crossover
Point}\label{sec-communication-collective-operations-algorithm-crossover}

When should we use Ring vs.~Tree? We can derive the crossover point by
setting \(T_{ring} = T_{tree}\) and solving for \(M\).

\subsubsection*{Step 1: Write the Full Time
Equations}\label{step-1-write-the-full-time-equations}
\addcontentsline{toc}{subsubsection}{Step 1: Write the Full Time
Equations}

For Ring (from above):
\[ T_{ring} = 2(N-1)\alpha + 2\frac{N-1}{N}\frac{M}{\beta} \]

For Tree:
\[ T_{tree} = 2\log_2 N \cdot \alpha + 2\log_2 N \cdot \frac{M}{\beta} \]

\subsubsection*{Step 2: Simplify for Large
N}\label{step-2-simplify-for-large-n}
\addcontentsline{toc}{subsubsection}{Step 2: Simplify for Large N}

When \(N\) is large, \((N-1) \approx N\) and
\(\frac{N-1}{N} \approx 1\), so:
\[ T_{ring} \approx 2N\alpha + \frac{2M}{\beta}, \quad T_{tree} \approx 2\log_2 N \cdot \alpha + \frac{2\log_2 N \cdot M}{\beta} \]

\subsubsection*{Step 3: Solve for the Crossover
Point}\label{step-3-solve-for-the-crossover-point}
\addcontentsline{toc}{subsubsection}{Step 3: Solve for the Crossover
Point}

\[ 2N\alpha + \frac{2M}{\beta} = 2\log_2 N \cdot \alpha + \frac{2\log_2 N \cdot M}{\beta} \]

Rearranging the bandwidth terms:
\[ \frac{2M}{\beta} - \frac{2\log_2 N \cdot M}{\beta} = 2\log_2 N \cdot \alpha - 2N\alpha \]

\[ \frac{2M}{\beta}(1 - \log_2 N) = 2\alpha(\log_2 N - N) \]

Since \(N \gg \log_2 N\) for large clusters,
\((\log_2 N - N) \approx -N\) and \((1 - \log_2 N) \approx -\log_2 N\):
\[ \frac{2M}{\beta}(-\log_2 N) \approx 2\alpha(-N) \]

\[ M_{crossover} \approx \frac{N \cdot \alpha \cdot \beta}{\log_2 N} \]

For practical purposes (and because the constants wash out in real
systems), this is often approximated as:
\[ \boxed{M_{crossover} \approx N \cdot \alpha \cdot \beta} \]

\textbf{The Selection Rule:}

\begin{itemize}
\tightlist
\item
  If Message Size \(M > M_{crossover}\): Use \textbf{Ring} (Bandwidth
  limited).
\item
  If Message Size \(M < M_{crossover}\): Use \textbf{Tree} (Latency
  limited).
\end{itemize}

Communication libraries like NCCL dynamically select the algorithm based
on this threshold. For a cluster with \(\alpha=5 \mu s\), \(\beta=50\)
GB/s, and \(N=100\):
\[ M_{crossover} \approx 100 \times 5\cdot 10^{-6} \times 50\cdot 10^9 \approx 25 \text{ MB} \]

Gradients smaller than 25 MB use Tree; larger use Ring. Let's work
through a concrete example to see this crossover in action:

\phantomsection\label{callout-notebookux2a-2.8}
\begin{fbx}{callout-notebook}{AI Engineer’s Notebook:}{The Ring vs. Tree Crossover}
\phantomsection\label{callout-notebook*-2.8}
\textbf{Problem}: You are synchronizing a \textbf{1 MB} buffer across
\textbf{64 GPUs}. The network has latency \(\alpha = 10 \mu s\) and
bandwidth \(\beta = 10 \text{ GB/s}\). Should you use Ring or Tree?

\textbf{The Math}:

\emph{Latency Terms:}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Ring Latency}:
  \(2(N-1)\alpha = 2 \times 63 \times 10\ \mu s = \mathbf{1{,}260\ \mu s}\).
\item
  \textbf{Tree Latency}:
  \(2(\log_2 N)\alpha = 2 \times 6 \times 10\ \mu s = \mathbf{120\ \mu s}\).
\end{enumerate}

\emph{Bandwidth Terms (note the difference!):}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  \textbf{Ring Bandwidth}:
  \(2\frac{N-1}{N}\frac{M}{\beta} \approx 2 \times \frac{1\text{ MB}}{10\text{ GB/s}} = \mathbf{200\ \mu s}\)
  (optimal---each byte sent once).
\item
  \textbf{Tree Bandwidth}:
  \(2\log_2 N \cdot \frac{M}{\beta} = 12 \times \frac{1\text{ MB}}{10\text{ GB/s}} = \mathbf{1{,}200\ \mu s}\)
  (each level sends full message).
\end{enumerate}

\textbf{Total Time}:

\begin{longtable}[]{@{}llll@{}}
\toprule\noalign{}
Algorithm & Latency & Bandwidth & \textbf{Total} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Ring & 1,260 μs & 200 μs & \textbf{1,460 μs} \\
Tree & 120 μs & 1,200 μs & \textbf{1,320 μs} \\
\end{longtable}

\textbf{Tree wins}, but only by 10\%! For this 1 MB message, we're near
the crossover point.

\textbf{The Systems Insight}: Ring's latency penalty (\(10\times\) worse
than Tree) nearly balances Tree's bandwidth penalty (\(6\times\) worse
than Ring). The crossover formula predicts:
\(M_{crossover} = N \cdot \alpha \cdot \beta = 64 \times 10\ \mu s \times 10\ \text{GB/s} = 6.4\ \text{MB}\).
At 1 MB, we're below crossover---Tree wins. At 10 MB, Ring would
dominate.

\end{fbx}

\section{Environmental Mastery: Navigating the
Hierarchy}\label{sec-communication-collective-operations-collective-operations-mapping-collectives-topology-3214}

The final challenge for our gradient is the physical layout of the
datacenter. Not all wires are created equal. A GPU node is a high-speed
sanctuary (NVLink at 900 GB/s), while the space between nodes is a slow,
expensive bridge (InfiniBand at 50 GB/s).

\subsection{Hierarchical
AllReduce}\label{sec-communication-collective-operations-collective-operations-hierarchical-allreduce-1338}

Real clusters are \textbf{hierarchical}, with fundamentally different
bandwidths at each tier, as Table~\ref{tbl-bandwidth-hierarchy}
quantifies:

\begin{longtable}[]{@{}llll@{}}
\caption{\textbf{The Bandwidth Hierarchy}: Modern GPU clusters exhibit
an order-of-magnitude bandwidth gap between intra-node and inter-node
communication. Hierarchical algorithms exploit this
gap.}\label{tbl-bandwidth-hierarchy}\tabularnewline
\toprule\noalign{}
\textbf{Tier} & \textbf{Interconnect} & \textbf{Bandwidth} &
\textbf{Relative Speed} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\textbf{Tier} & \textbf{Interconnect} & \textbf{Bandwidth} &
\textbf{Relative Speed} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Intra-Node & NVLink 4.0 & \textasciitilde900 GB/s & 18× faster \\
Inter-Node & InfiniBand NDR 400G & \textasciitilde50 GB/s & 1×
(baseline) \\
\end{longtable}

A naive flat Ring AllReduce ignores this structure---it might route data
across InfiniBand when NVLink would suffice, wasting the scarce
inter-node bandwidth. \textbf{Hierarchical AllReduce} decomposes the
global operation into three phases that respect the bandwidth hierarchy:

\subsubsection*{Step 1: Intra-Node ReduceScatter
(NVLink)}\label{step-1-intra-node-reducescatter-nvlink}
\addcontentsline{toc}{subsubsection}{Step 1: Intra-Node ReduceScatter
(NVLink)}

Each node performs a local ReduceScatter among its \(G\) GPUs using the
fast NVLink mesh. After this step, each GPU holds \(1/G\) of the
partially reduced data. The key insight: this step uses only the
abundant intra-node bandwidth.

\subsubsection*{Step 2: Inter-Node AllReduce
(InfiniBand)}\label{step-2-inter-node-allreduce-infiniband}
\addcontentsline{toc}{subsubsection}{Step 2: Inter-Node AllReduce
(InfiniBand)}

GPUs at the same position across nodes (e.g., all GPU-0s) perform an
AllReduce using InfiniBand. Because Step 1 already reduced the data by
\(G\times\), each GPU only sends \(M/G\) bytes instead of \(M\)
bytes---reducing inter-node traffic by a factor of \(G\).

\subsubsection*{Step 3: Intra-Node AllGather
(NVLink)}\label{step-3-intra-node-allgather-nvlink}
\addcontentsline{toc}{subsubsection}{Step 3: Intra-Node AllGather
(NVLink)}

Each node performs a local AllGather to distribute the final result to
all \(G\) GPUs. Again, this uses only NVLink, leaving inter-node
bandwidth untouched.

The net effect: inter-node traffic is reduced by a factor of \(G\) (GPUs
per node), effectively \textbf{multiplying the apparent inter-node
bandwidth by \(G\)}. The following example quantifies this bandwidth
multiplication effect:

\phantomsection\label{callout-notebookux2a-2.9}
\begin{fbx}{callout-notebook}{AI Engineer’s Notebook:}{The Hierarchical Bandwidth Multiplier}
\phantomsection\label{callout-notebook*-2.9}
\textbf{Problem}: You have a cluster of 8 nodes, each with 8 GPUs (64
GPUs total). You need to AllReduce a 1 GB gradient buffer. Compare flat
Ring AllReduce vs.~Hierarchical AllReduce.

\textbf{Flat Ring AllReduce (ignoring hierarchy)}:

\begin{itemize}
\tightlist
\item
  Each GPU sends \textasciitilde2 GB total (the bandwidth-optimal Ring
  AllReduce formula).
\item
  The ring crosses node boundaries multiple times.
\item
  \textbf{Effective bandwidth}: Limited by the slowest link = 50 GB/s
  (InfiniBand).
\item
  \textbf{Time}:
  \(\approx 2 \times 1\ \text{GB} / 50\ \text{GB/s} = 40\ \text{ms}\)
  (bandwidth term dominates).
\end{itemize}

\textbf{Hierarchical AllReduce (3-step decomposition)}:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Intra-Node ReduceScatter}: Each GPU sends 875 MB at 900 GB/s →
  \textasciitilde1 ms
\item
  \textbf{Inter-Node AllReduce}: Each GPU sends
  \(1\ \text{GB}/8 = 125\ \text{MB}\) at 50 GB/s → \textasciitilde5 ms

  \emph{(Only 1/8 of the data crosses InfiniBand!)}
\item
  \textbf{Intra-Node AllGather}: Each GPU receives 875 MB at 900 GB/s →
  \textasciitilde1 ms
\end{enumerate}

\begin{itemize}
\tightlist
\item
  \textbf{Total Time}: \(\approx 1 + 5 + 1 = 7\ \text{ms}\)
\end{itemize}

\textbf{The Systems Insight}: Hierarchical AllReduce achieves a
\textbf{5.7× speedup} by reducing inter-node traffic from 1 GB to 125 MB
per GPU. With 8 GPUs per node, we effectively get \(8\times\) the
apparent inter-node bandwidth. This is why NVIDIA's NCCL and similar
libraries default to hierarchical algorithms on multi-node clusters.

\end{fbx}

These three phases confine most traffic within each node before crossing
the slower inter-node fabric.

\begin{tcolorbox}[enhanced jigsaw, colframe=quarto-callout-note-color-frame, toprule=.15mm, left=2mm, breakable, titlerule=0mm, opacityback=0, rightrule=.15mm, opacitybacktitle=0.6, coltitle=black, arc=.35mm, colbacktitle=quarto-callout-note-color!10!white, toptitle=1mm, leftrule=.75mm, bottomtitle=1mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Hierarchical AllReduce Phases}, bottomrule=.15mm, colback=white]

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/ce6bd8a00227b3b7ecf32e5233c04914bb5a9912.pdf}}

\end{tcolorbox}

\subsection{Topology-Aware
Routing}\label{sec-communication-collective-operations-collective-operations-topology-detection-selection-2dc7}

Communication libraries like NCCL perform \textbf{topology detection} at
initialization, running graph search algorithms to discover the physical
network structure and find optimal communication paths. This is critical
because the same collective algorithm can have wildly different
performance depending on how logical ranks map to physical hardware.

\textbf{Torus Topology (TPU Pods)}

Google's TPU pods use a 3D torus topology where each TPU connects
directly to 6 neighbors (±X, ±Y, ±Z). The optimal AllReduce strategy is
\textbf{dimension-ordered reduction}:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Reduce along the X dimension (all TPUs in the same YZ plane)
\item
  Reduce along the Y dimension (all TPUs in the same Z column)
\item
  Reduce along the Z dimension (final global reduction)
\end{enumerate}

This approach minimizes network diameter and ensures each link carries
traffic in only one direction at a time, avoiding congestion.

\textbf{Rail-Optimized Routing (NVIDIA DGX)}

In NVIDIA DGX systems, each GPU has its own dedicated NIC (network
interface card). \textbf{Rail-optimized} routing exploits this by
ensuring that GPUs at the same position within their respective nodes
communicate only with each other:

\begin{itemize}
\tightlist
\item
  GPU 0 on Node A talks only to GPU 0 on Node B, Node C, etc.
\item
  GPU 1 on Node A talks only to GPU 1 on other nodes.
\item
  And so on for GPUs 2--7.
\end{itemize}

This creates 8 independent ``rails'' of communication that operate in
parallel without contention, as Table~\ref{tbl-rail-optimized}
illustrates:

\begin{longtable}[]{@{}lll@{}}
\caption{\textbf{Rail-Optimized Traffic Distribution}: Each rail carries
1/8 of the total traffic
independently.}\label{tbl-rail-optimized}\tabularnewline
\toprule\noalign{}
\textbf{Rail} & \textbf{Participants} & \textbf{Traffic} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\textbf{Rail} & \textbf{Participants} & \textbf{Traffic} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Rail 0 & Node0-GPU0 ↔ Node1-GPU0 ↔ Node2-GPU0 ↔ \ldots{} & \(M/8\)
each \\
Rail 1 & Node0-GPU1 ↔ Node1-GPU1 ↔ Node2-GPU1 ↔ \ldots{} & \(M/8\)
each \\
\ldots{} & \ldots{} & \ldots{} \\
Rail 7 & Node0-GPU7 ↔ Node1-GPU7 ↔ Node2-GPU7 ↔ \ldots{} & \(M/8\)
each \\
\end{longtable}

\textbf{Why Rails Matter}: Without rail alignment, all 8 GPUs on a node
might try to send to the same remote GPU simultaneously, creating 8×
contention on a single NIC. Rail-aligned routing ensures each NIC
handles exactly 1/8 of the traffic, achieving full bisection bandwidth
utilization.

\section{The Last Resort: Gradient
Compression}\label{sec-communication-collective-operations-collective-operations-gradient-compression-7a5c}

Sometimes, even the best algorithms and the fastest rails are not
enough. The payload is simply too heavy. In these moments of
desperation, we turn to \textbf{Compression}.

\subsection{Quantization: Reducing
Precision}\label{sec-communication-collective-operations-collective-operations-quantization-reducing-precision-815b}

Most gradients are computed in FP32 (32-bit floating point) or BF16
(16-bit brain float). Quantization reduces the bit-width of each
gradient value, directly reducing communication volume.

\textbf{The Quantization Progression:}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{FP16 (16-bit)}: The baseline for modern training. Half the
  bits of FP32, with minimal impact on convergence for most models.
  Provides \textbf{2× compression} over FP32.
\item
  \textbf{INT8 (8-bit)}: Quantize each gradient vector to 256 discrete
  levels. This requires computing a \textbf{scaling factor} per tensor:
  \(g_{int8} = \text{round}(g / s)\) where \(s = \max(|g|) / 127\). The
  receiver reconstructs \(\hat{g} = g_{int8} \times s\). Provides
  \textbf{4× compression} over FP32 but introduces quantization noise
  proportional to the gradient magnitude.
\item
  \textbf{1-bit SGD}: The extreme case---transmit only the \textbf{sign}
  of each gradient element (+1 or -1). The receiver reconstructs using a
  learned or adaptive scaling factor. Provides \textbf{32× compression}
  over FP32 but introduces substantial noise that typically degrades
  convergence without additional mechanisms.
\end{enumerate}

\textbf{Trade-off}: Each reduction in bit-width introduces quantization
noise. This noise acts as a biased perturbation to the true gradient
direction. For aggressive quantization (INT8 and especially 1-bit), the
systematic bias can prevent convergence unless corrected by
\textbf{Error Feedback} (see below).

\subsection{Sparsification: Transmitting Only Important
Gradients}\label{sec-communication-collective-operations-collective-operations-sparsification-transmitting-important-gradients-2cd2}

An orthogonal approach to quantization is \textbf{sparsification}:
instead of reducing the precision of all gradients, transmit only a
subset of gradient elements.

\textbf{Top-K Sparsification:}

The most common method is \textbf{Top-K compression}: for a gradient
vector \(g \in \mathbb{R}^d\), transmit only the \(K\) elements with the
largest absolute magnitude, setting the rest to zero:

\[\text{TopK}(g) = g \odot \mathbf{1}_{|g| \geq |g|_{(K)}}\]

where \(|g|_{(K)}\) is the \(K\)-th largest element by magnitude. With
\(K = 0.001 \times d\) (keeping only 0.1\% of elements), this achieves
\textbf{1000× compression}.

\textbf{The Convergence Problem:}

Naively discarding small gradients creates a systematic bias. If a
parameter consistently receives small gradients (e.g., 0.01 per step),
it will \emph{never} be updated because 0.01 is always below the Top-K
threshold. Over thousands of steps, these ``lost'' gradients accumulate
to a significant error that prevents the model from converging to the
true optimum.

This is not merely a practical concern---it is a fundamental
mathematical obstacle. Without correction, sparsified SGD is a
\textbf{biased estimator} of the true gradient, and biased gradient
descent can converge to arbitrarily wrong solutions.

\subsection{Error Feedback: The Memory of the
Journey}\label{sec-communication-collective-operations-error-feedback}

We solve the conflict between compression and convergence with
\textbf{Error Feedback}. Like a traveler who can only carry 10kg but has
100kg of gear, we leave the excess behind but \emph{remember} it. We
maintain a local error accumulator \(e_t\) that stores the compression
residual.

\phantomsection\label{callout-definitionux2a-2.10}
\begin{fbx}{callout-definition}{Definition:}{Error Feedback Mechanism}
\phantomsection\label{callout-definition*-2.10}
\textbf{Error Feedback} maintains a local error accumulator \(e_t\) that
stores the compression residual. At each step \(t\):

\[v_t = \text{Compress}(g_t + e_t)\] \[e_{t+1} = (g_t + e_t) - v_t\]

where \(g_t\) is the true gradient, \(v_t\) is the compressed
(transmitted) gradient, and \(e_t\) is the accumulated error from
previous steps.

\end{fbx}

\textbf{Why Error Feedback Guarantees Convergence:}

The key insight is that error feedback makes compression an
\textbf{unbiased estimator over time}. Consider what happens over \(T\)
steps:

\[\sum_{t=1}^{T} v_t = \sum_{t=1}^{T} \left[(g_t + e_t) - e_{t+1}\right] = \sum_{t=1}^{T} g_t + e_1 - e_{T+1}\]

If the error accumulator remains bounded (which it does for reasonable
compression schemes), then as \(T \to \infty\):

\[\frac{1}{T}\sum_{t=1}^{T} v_t \to \frac{1}{T}\sum_{t=1}^{T} g_t\]

The long-run average of transmitted gradients equals the long-run
average of true gradients. \textbf{No gradient information is
permanently lost}---it is merely delayed. Small gradients that are
repeatedly dropped will eventually accumulate in \(e_t\) until they
exceed the compression threshold and get transmitted.

This property---that compression error telescopes across time---is why
error feedback transforms a biased, non-convergent method into an
unbiased, convergent one. Theoretical analysis shows that with error
feedback, compressed SGD converges at the same asymptotic rate as
uncompressed SGD, with only constant-factor slowdowns
(\citeproc{ref-stich2018sparsified}{\textbf{stich2018sparsified?}};
\citeproc{ref-karimireddy2019error}{\textbf{karimireddy2019error?}}).
The following step-by-step trace illustrates how error feedback
preserves gradient information that naive compression would lose:

\phantomsection\label{callout-notebookux2a-2.11}
\begin{fbx}{callout-notebook}{AI Engineer’s Notebook:}{Error Feedback Mechanism}
\phantomsection\label{callout-notebook*-2.11}

\textbf{Scenario}: We have a single parameter receiving small gradients
over 5 training steps. We use aggressive compression that only transmits
values \(\geq 0.5\) (rounding to nearest integer: values in
\([0, 0.5) \to 0\), values in \([0.5, 1.5) \to 1\), etc.).

\textbf{Without Error Feedback} (Naive Compression):

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.0690}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2414}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2184}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2759}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.1954}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Step
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
True Gradient \(g_t\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Transmitted \(v_t\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Cumulative Transmitted
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Cumulative True
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
1 & 0.4 & 0 & 0 & 0.4 \\
2 & 0.3 & 0 & 0 & 0.7 \\
3 & 0.2 & 0 & 0 & 0.9 \\
4 & 0.4 & 0 & 0 & 1.3 \\
5 & 0.3 & 0 & \textbf{0} & \textbf{1.6} \\
\end{longtable}

\textbf{Result}: After 5 steps, we've transmitted \textbf{0} but the
true cumulative gradient is \textbf{1.6}. The parameter never
updates---\textbf{100\% of gradient information is lost}.

\textbf{With Error Feedback}:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 12\tabcolsep) * \real{0.0690}}
  >{\raggedright\arraybackslash}p{(\linewidth - 12\tabcolsep) * \real{0.0805}}
  >{\raggedright\arraybackslash}p{(\linewidth - 12\tabcolsep) * \real{0.0805}}
  >{\raggedright\arraybackslash}p{(\linewidth - 12\tabcolsep) * \real{0.1494}}
  >{\raggedright\arraybackslash}p{(\linewidth - 12\tabcolsep) * \real{0.0805}}
  >{\raggedright\arraybackslash}p{(\linewidth - 12\tabcolsep) * \real{0.3563}}
  >{\raggedright\arraybackslash}p{(\linewidth - 12\tabcolsep) * \real{0.1839}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Step
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\(g_t\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\(e_t\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\(g_t + e_t\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\(v_t\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\(e_{t+1} = (g_t + e_t) - v_t\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Cumulative \(v\)
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
1 & 0.4 & 0.0 & 0.4 & 0 & 0.4 & 0 \\
2 & 0.3 & 0.4 & 0.7 & 1 & −0.3 & 1 \\
3 & 0.2 & −0.3 & −0.1 & 0 & −0.1 & 1 \\
4 & 0.4 & −0.1 & 0.3 & 0 & 0.3 & 1 \\
5 & 0.3 & 0.3 & 0.6 & 1 & −0.4 & \textbf{2} \\
\end{longtable}

\textbf{Result}: After 5 steps, we've transmitted \textbf{2} with a
remaining error of \textbf{−0.4}. The true cumulative is \textbf{1.6},
so transmitted + error = \(2 + (-0.4) = 1.6\) ✓

\textbf{Key Observations}:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{No information is lost}: The sum (transmitted + error buffer)
  always equals the cumulative true gradient.
\item
  \textbf{Small gradients accumulate}: Individual gradients of 0.3--0.4
  were too small to transmit alone, but they accumulated until crossing
  the threshold.
\item
  \textbf{Error oscillates around zero}: The error buffer \(e_t\)
  doesn't grow unboundedly---it oscillates as gradients are ``paid
  back'' through transmission.
\item
  \textbf{Convergence preserved}: Over time, the model receives
  approximately the correct total gradient, just with some delay.
\end{enumerate}

\end{fbx}

\subsection{Compression Trade-offs: Bandwidth
vs.~Convergence}\label{sec-communication-collective-operations-compression-tradeoffs}

Gradient compression is not free---it trades reduced communication for
increased variance in the optimization process.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1770}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2035}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2743}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3451}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Method}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Compression Ratio}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Convergence Impact}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Best Use Case}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
FP16 & 2× & Negligible & Default for all training \\
INT8 + Error FB & 4× & Minor slowdown (\textasciitilde5-10\%) &
Bandwidth-constrained clusters \\
Top-K (1\%) + Error FB & 100× & Moderate slowdown
(\textasciitilde10-20\%) & Cross-datacenter training \\
1-bit + Error FB & 32× & Significant slowdown (\textasciitilde20-30\%) &
Extreme bandwidth constraints \\
\end{longtable}

\textbf{When to Use Compression:}

\begin{itemize}
\tightlist
\item
  \textbf{Use aggressive compression} when communication time dominates
  compute time (high \(T_{comm}/T_{compute}\) ratio). This typically
  occurs with smaller models on large clusters.
\item
  \textbf{Avoid aggressive compression} when compute time
  dominates---the convergence slowdown isn't worth the communication
  savings you're not bottlenecked on.
\item
  \textbf{Always use Error Feedback} with any compression beyond FP16.
  Without it, convergence is not guaranteed.
\end{itemize}

The \(\alpha\)-\(\beta\) analysis from
Section~\ref{sec-communication-collective-operations-collective-operations-network-performance-modeling-0d8e}
helps determine when compression pays off: if your gradients are large
enough to be bandwidth-bound (\(M > n^*\)), compression directly reduces
wall-clock time. If they're latency-bound (\(M < n^*\)), compression
won't help---optimize latency instead.

\section{The Communication Library
Landscape}\label{sec-communication-collective-operations-collective-operations-communication-libraries-nccl-5307}

Three libraries dominate distributed ML communication. Understanding
their trade-offs helps practitioners select the right tool for their
hardware and workload.

\subsection{NCCL: Why It Dominates GPU
Workloads}\label{sec-communication-nccl}

NVIDIA Collective Communications Library (NCCL)\sidenote{\textbf{NCCL
(NVIDIA Collective Communications Library)}: NVIDIA's GPU-optimized
communication library, first released in 2015. NCCL implements
collective operations (AllReduce, AllGather, etc.) with GPU-aware
optimizations that leverage NVLink, NVSwitch, and GPUDirect RDMA. } is
the de-facto standard for multi-GPU training. Its dominance stems from
three GPU-specific optimizations that MPI and Gloo cannot replicate
without hardware vendor support:

\begin{itemize}
\item
  \textbf{Kernel Fusion}\sidenote{\textbf{Kernel Fusion}: A GPU
  optimization technique where multiple operations are combined into a
  single kernel launch, reducing memory round-trips and kernel launch
  overhead. }: NCCL fuses the reduction operator (sum, average) directly
  into the memory copy kernel. Rather than copying data to a buffer,
  reducing, then copying results back, NCCL performs the reduction
  \emph{during} the transfer. This eliminates intermediate memory
  traffic and maximizes HBM bandwidth utilization.
\item
  \textbf{Channel Pipelining}: NCCL opens multiple parallel
  communication channels to saturate all available network interfaces
  simultaneously. A DGX node with 8 NICs can achieve 8× the bandwidth of
  a single-channel implementation by spreading the collective across all
  links.
\item
  \textbf{GPUDirect RDMA}\sidenote{\textbf{GPUDirect RDMA}: NVIDIA
  technology enabling network adapters and storage devices to directly
  access GPU memory without CPU involvement, reducing latency and CPU
  overhead in data transfers. }: The network card reads directly from
  GPU memory via PCIe, bypassing the CPU entirely. Without GPUDirect,
  data must traverse GPU → CPU memory → NIC → network, adding
  microseconds of latency and consuming CPU cycles. GPUDirect eliminates
  this overhead.
\end{itemize}

These optimizations explain why NCCL achieves 2-5× better performance
than generic MPI implementations on GPU clusters.

\subsection{MPI: The HPC Foundation}\label{sec-communication-mpi}

The Message Passing Interface (MPI)\sidenote{\textbf{MPI (Message
Passing Interface)}: A standardized specification for parallel computing
communication, first defined in 1994. MPI defines collective operations,
point-to-point messaging, and process management primitives used across
scientific computing and HPC. } standardized collective operations
decades before deep learning existed. MPI remains relevant for:

\begin{itemize}
\tightlist
\item
  \textbf{CPU-based distributed computing}: For workloads without GPUs,
  MPI implementations (OpenMPI, MPICH, Intel MPI) are mature and
  well-optimized.
\item
  \textbf{Hybrid CPU-GPU clusters}: Some workflows pre-process data on
  CPUs before GPU training.
\item
  \textbf{Portability}: MPI's standardized API works across vendors and
  hardware generations.
\end{itemize}

\textbf{Caveat for ML practitioners}: Standard MPI implementations lack
GPU-awareness. Calling \texttt{MPI\_Allreduce} on GPU memory typically
requires explicit copies to CPU memory, negating GPU performance.
CUDA-aware MPI extensions exist but rarely match NCCL's optimizations.
\textbf{Use NCCL for GPU communication; use MPI when you need CPU
collective operations or cross-platform portability.}

\subsection{Gloo: Cross-Platform
Flexibility}\label{sec-communication-gloo}

Gloo\sidenote{\textbf{Gloo}: Meta's collective communication library
providing CPU-optimized implementations and cross-platform support. Used
as PyTorch's default CPU backend and fallback when NCCL is unavailable.
} is Meta's open-source collective communication library, integrated
into PyTorch as a backend option alongside NCCL.

\begin{itemize}
\tightlist
\item
  \textbf{Strengths}: Cross-platform support (Linux, macOS, Windows),
  CPU optimization, and TCP/IP fallback when RDMA is unavailable.
\item
  \textbf{Use cases}: Development environments, CPU-only training,
  heterogeneous clusters mixing hardware vendors.
\item
  \textbf{Limitations}: Lacks NCCL's GPU-specific optimizations. On GPU
  clusters, Gloo typically achieves 30-50\% of NCCL's throughput.
\end{itemize}

\subsection{Library Selection
Guide}\label{sec-communication-library-selection}

Table~\ref{tbl-library-selection} provides a decision matrix for
choosing the right library based on hardware and workload requirements.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2593}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4630}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2778}}@{}}
\caption{\textbf{Communication Library Selection}: Choose based on
hardware and workload requirements. Most production GPU training uses
NCCL; Gloo serves as a portable
fallback.}\label{tbl-library-selection}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Scenario}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Recommended Library}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Rationale}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Scenario}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Recommended Library}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Rationale}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Multi-GPU training (NVIDIA) & NCCL & GPUDirect, kernel fusion,
NVLink-aware \\
CPU-only distributed training & Gloo or MPI & Mature CPU
optimizations \\
Development/debugging & Gloo & Cross-platform, no CUDA dependency \\
Mixed vendor GPUs & Gloo (fallback) & NCCL is NVIDIA-specific \\
HPC integration & MPI + NCCL & MPI for job launch, NCCL for GPU
collectives \\
\end{longtable}

\section{Fallacies and
Pitfalls}\label{sec-communication-collective-operations-collective-operations-fallacies-pitfalls-9cd0}

\textbf{Fallacy:} \textbf{\emph{Bandwidth is the only metric that
matters.}} For small messages (pipeline parallelism activations, MoE
tokens), \textbf{latency} (\(\alpha\)) dominates. Buying 400G networking
won't help if your message takes 5 μs to serialize in software. The
critical message size \(n^* = \alpha \cdot \beta\) determines which
metric to optimize---below \(n^*\), reduce latency; above it, increase
bandwidth.

\textbf{Pitfall:} \textbf{\emph{Assuming AllReduce works for
everything.}} AllReduce creates a global barrier and assumes all
participants contribute identical data shapes. In \textbf{Expert
Parallelism (MoE)} and \textbf{Recommendation Systems}, where each
worker needs to send distinct data to every other worker, AllReduce is
fundamentally wrong. These workloads require AlltoAll, which has
\(O(N^2)\) logical connections and hits network contention limits at
much smaller cluster sizes.

\textbf{Fallacy:} \textbf{\emph{Ring AllReduce is always optimal.}} Ring
achieves bandwidth-optimal \(2\frac{N-1}{N}\frac{M}{\beta}\) but pays
\(O(N)\) latency. For a 1 MB gradient across 64 GPUs with
\(\alpha = 10\ \mu s\), Tree AllReduce actually wins because Ring's
1,260 μs latency penalty exceeds Tree's 1,000 μs bandwidth penalty. The
crossover formula \(M_{crossover} \approx N \cdot \alpha \cdot \beta\)
determines when to switch algorithms.

\textbf{Pitfall:} \textbf{\emph{Compressing gradients without error
feedback.}} Top-K sparsification can achieve 99\% compression, but
naively discarding small gradients causes divergence. Without error
feedback (\(e_{t+1} = (g_t + e_t) - v_t\)), gradients below the
threshold are lost forever, accumulating systematic bias that eventually
destabilizes training.

\textbf{Fallacy:} \textbf{\emph{Async collectives always hide latency.}}
Python's \texttt{dist.all\_reduce(...,\ async\_op=True)} only returns
control to the CPU. The LogP model distinguishes network latency \(L\)
(overlappable) from processor overhead \(o\) (non-overlappable). If the
GPU compute kernel is shorter than the communication overhead, the GPU
still stalls. You can only hide communication when \(T_{compute} > o\).

\textbf{Pitfall:} \textbf{\emph{Silent data corruption in the network.}}
Networks are not perfect. A bad cable can flip bits. Unlike TCP (which
checksums everything), high-speed RDMA protocols sometimes have weaker
guarantees or buggy NIC firmware. At 10,000 nodes running 24/7, ``rare''
bit flips (1 in \(10^{15}\)) happen multiple times per day, corrupting
gradients without any error signal.

\section{Summary}\label{sec-communication-collective-operations-summary}

This chapter opened with a fundamental asymmetry: computation is local,
but learning is global. We have reframed communication not as a
secondary overhead, but as the \textbf{friction of scale} that governs
the maximum speed of the Machine Learning Fleet. By applying the
\textbf{\(\alpha\)-\(\beta\) and LogP models}, we moved from
``guessing'' bottlenecks to quantifying them, identifying the critical
message size that separates latency-bound software from bandwidth-bound
hardware.

We explored the specialized ``vehicles'' of the datacenter---the
\textbf{Collective Primitives}. We followed the gradient's journey
through \textbf{Ring} and \textbf{Tree AllReduce} algorithms,
understanding that the optimal path depends on cluster size and payload.
Finally, we examined the ``sanctuaries'' of the node hierarchy, where
\textbf{Hierarchical AllReduce} and \textbf{Rail-Optimized Routing}
allow us to cheat the physical limits of the inter-node network.

When even the fastest wires are not enough, \textbf{Gradient
Compression} provides the final squeeze. By using \textbf{Error
Feedback}, we ensure that while the journey may be compressed or
delayed, no vital information is permanently lost, preserving the
mathematical truth of the model's convergence.

\phantomsection\label{callout-takeawaysux2a-2.12}
\begin{fbx}{callout-takeaways}{Takeaways:}{Key Takeaways}
\phantomsection\label{callout-takeaways*-2.12}

\begin{itemize}
\tightlist
\item
  \textbf{The α-β model reveals the bottleneck}: The critical message
  size \(n^* = \alpha \cdot \beta\) determines whether you should
  optimize software latency (small messages) or hardware bandwidth
  (large payloads).
\item
  \textbf{Algorithm choice is scale-dependent}: Ring AllReduce is
  bandwidth-optimal but pays \(O(N)\) latency; Tree AllReduce is
  latency-optimal (\(O(\log N)\)) but bandwidth-inefficient. Use the
  crossover formula \(M_{crossover} \approx N \alpha \beta\) to choose.
\item
  \textbf{Hierarchical algorithms multiply bandwidth}: By performing
  local reductions over fast NVLink before crossing the slow InfiniBand
  bridge, hierarchical collectives effectively multiply inter-node
  bandwidth by the number of GPUs per node.
\item
  \textbf{AlltoAll is the contention king}: Unlike AllReduce, AlltoAll
  creates \(O(N^2)\) logical connections. This makes Expert Parallelism
  (MoE) and Recommendation Systems fundamentally harder to scale than
  dense LLMs.
\item
  \textbf{Error Feedback makes lossy compression safe}: You can throw
  away 99\% of gradients via sparsification, provided you accumulate the
  ``error'' locally and add it to the next step. This turns biased
  estimators into unbiased ones over time.
\item
  \textbf{Topology discovery is not optional}: Modern libraries like
  NCCL dynamically map logical rings to physical wires to avoid ``hot
  spots'' and maximize bisection bandwidth.
\end{itemize}

\end{fbx}

Throughout this chapter, we have seen how these communication traffic
patterns change across our Lighthouse Archetypes.

\phantomsection\label{callout-lighthouseux2a-2.13}
\begin{fbx}{callout-lighthouse}{Lighthouse:}{Communication Archetype Patterns}
\phantomsection\label{callout-lighthouse*-2.13}

The ``Travel Manifest'' for a gradient depends on the system's objective
function and constraint regime:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1507}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2740}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2603}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3151}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Archetype
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Primary Collective
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Dominant Friction
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Optimization Strategy
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Archetype A (LLM)} & AllReduce & Bandwidth (\(\beta\)) &
Hierarchical AllReduce; Rail-optimization \\
\textbf{Archetype B (MoE)} & AllToAll & Latency (\(\alpha\)) \&
Contention & Topology-aware routing; token load-balancing \\
\textbf{Archetype C (Edge)} & P2P / Async & Connectivity \& Latency &
Aggressive quantization; Error Feedback \\
\end{longtable}

\end{fbx}

The communication patterns established in this chapter reveal that
distributed training is fundamentally a network engineering problem
disguised as a machine learning problem. Understanding the
\(\alpha\)-\(\beta\) cost model, collective algorithm selection, and
compression techniques transforms practitioners from users of
distributed frameworks into engineers who can diagnose bottlenecks,
optimize topology configurations, and achieve the scaling efficiency
that determines whether trillion-parameter training runs complete in
weeks or months.

\phantomsection\label{callout-chapter-connectionux2a-2.14}
\begin{fbxSimple}{callout-chapter-connection}{What’s Next:}{From Logic to Resilience}
\phantomsection\label{callout-chapter-connection*-2.14}
We have established the \emph{logic} of the fleet (Parallelism) and the
\emph{traffic patterns} that sustain it (Communication). We have the map
and the vehicles. But in the real world, the ``roads'' are constantly
crumbling. GPUs overheat, networks drop packets, and nodes fail
mid-calculation.

In \textbf{Fault Tolerance}
(\textbf{?@sec-fault-tolerance-reliability}), we examine how to maintain
the illusion of a perfect supercomputer on imperfect hardware. We move
from the logic of movement to the mechanics of survival, exploring
checkpointing, recovery, and elastic training.

\end{fbxSimple}

\phantomsection\label{refs}
\begin{CSLReferences}{1}{0}
\bibitem[\citeproctext]{ref-culler1993logp}
Culler, David, Richard Karp, David Patterson, Abhijit Sahay, Klaus Erik
Schauser, Eunice Santos, Ramesh Subramonian, and Thorsten von Eicken.
1993. {``LogP: Towards a Realistic Model of Parallel Computation.''}
\emph{ACM SIGPLAN Notices} 28 (7): 1--12.
\url{https://doi.org/10.1145/173284.155333}.

\end{CSLReferences}


\backmatter


\end{document}
