% Options for packages loaded elsewhere
% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode,linktoc=all,pdfwindowui,pdfpagemode=FullScreen,pdfpagelayout=TwoPageRight}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  9pt,
  letterpaper,
  abstract,
  titlepage]{scrbook}
\usepackage{xcolor}
\usepackage[left=1in,marginparwidth=2.0666666666667in,textwidth=4.1333333333333in,marginparsep=0.3in]{geometry}
\usepackage{amsmath,amssymb}
\setcounter{secnumdepth}{3}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
% Make \paragraph and \subparagraph free-standing
\makeatletter
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}{
    \@ifstar
      \xxxParagraphStar
      \xxxParagraphNoStar
  }
  \newcommand{\xxxParagraphStar}[1]{\oldparagraph*{#1}\mbox{}}
  \newcommand{\xxxParagraphNoStar}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}{
    \@ifstar
      \xxxSubParagraphStar
      \xxxSubParagraphNoStar
  }
  \newcommand{\xxxSubParagraphStar}[1]{\oldsubparagraph*{#1}\mbox{}}
  \newcommand{\xxxSubParagraphNoStar}[1]{\oldsubparagraph{#1}\mbox{}}
\fi
\makeatother


\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\newsavebox\pandoc@box
\newcommand*\pandocbounded[1]{% scales image to fit in text height/width
  \sbox\pandoc@box{#1}%
  \Gscale@div\@tempa{\textheight}{\dimexpr\ht\pandoc@box+\dp\pandoc@box\relax}%
  \Gscale@div\@tempb{\linewidth}{\wd\pandoc@box}%
  \ifdim\@tempb\p@<\@tempa\p@\let\@tempa\@tempb\fi% select the smaller of both
  \ifdim\@tempa\p@<\p@\scalebox{\@tempa}{\usebox\pandoc@box}%
  \else\usebox{\pandoc@box}%
  \fi%
}
% Set default figure placement to htbp
\def\fps@figure{htbp}
\makeatother
% definitions for citeproc citations
\NewDocumentCommand\citeproctext{}{}
\NewDocumentCommand\citeproc{mm}{%
  \begingroup\def\citeproctext{#2}\cite{#1}\endgroup}
\makeatletter
 % allow citations to break across lines
 \let\@cite@ofmt\@firstofone
 % avoid brackets around text for \cite:
 \def\@biblabel#1{}
 \def\@cite#1#2{{#1\if@tempswa , #2\fi}}
\makeatother
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newenvironment{CSLReferences}[2] % #1 hanging-indent, #2 entry-spacing
 {\begin{list}{}{%
  \setlength{\itemindent}{0pt}
  \setlength{\leftmargin}{0pt}
  \setlength{\parsep}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1
   \setlength{\leftmargin}{\cslhangindent}
   \setlength{\itemindent}{-1\cslhangindent}
  \fi
  % set entry spacing
  \setlength{\itemsep}{#2\baselineskip}}}
 {\end{list}}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{\hfill\break\parbox[t]{\linewidth}{\strut\ignorespaces#1\strut}}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}

% =============================================================================
% LATEX HEADER CONFIGURATION FOR MLSYSBOOK PDF
% =============================================================================
% This file contains all LaTeX package imports, custom commands, and styling
% definitions for the PDF output of the Machine Learning Systems textbook.
%
% Key Features:
% - Harvard crimson branding throughout
% - Custom part/chapter/section styling
% - Professional table formatting with colored headers
% - Margin notes with custom styling
% - TikZ-based part dividers
% - Page numbering (Roman for frontmatter, Arabic for mainmatter)
%
% Note: This file is included via _quarto-pdf.yml and affects PDF output only.
% HTML/EPUB styling is handled separately via CSS files.
% =============================================================================

% =============================================================================
% PACKAGE IMPORTS
% =============================================================================

% Layout and positioning
% \usepackage[outercaption, ragged]{sidecap}  % Commented out to make figure captions inline instead of in margin
\usepackage{adjustbox}      % Adjusting box dimensions
\usepackage{afterpage}      % Execute commands after page break
\usepackage{morefloats}     % Increase number of floats
\usepackage{array}          % Enhanced table column formatting
\usepackage{atbegshi}       % Insert content at page beginning
%\usepackage{changepage}     % Change page dimensions mid-document
\usepackage{emptypage}      % Clear headers/footers on empty pages

% Language and text
\usepackage[english]{babel} % English language support
\usepackage{microtype}      % Improved typography and hyphenation

% Captions and floats
\usepackage{caption}
% Caption styling configuration
%\captionsetup[table]{belowskip=5pt}
\captionsetup{format=plain}
\DeclareCaptionLabelFormat{mylabel}{#1
#2:\hspace{1.0ex}}
\DeclareCaptionFont{ninept}{\fontsize{7pt}{8}\selectfont #1}

% Figure captions: Small font, bold label, ragged right
\captionsetup[figure]{labelfont={bf,ninept},labelsep=space,
belowskip=2pt,aboveskip=6pt,labelformat=mylabel,
justification=raggedright,singlelinecheck=false,font={ninept}}

% Table captions: Small font, bold label, ragged right
\captionsetup[table]{belowskip=6pt,labelfont={bf,ninept},labelsep=none,
labelformat=mylabel,justification=raggedright,singlelinecheck=false,font={ninept}}

% Typography fine-tuning
\emergencystretch=5pt       % Allow extra stretch to avoid overfull boxes

% Utility packages
\usepackage{etoolbox}       % For patching commands and environments

% Page layout and headers
\usepackage{fancyhdr}       % Custom headers and footers
\usepackage{geometry}       % Page dimensions and margins

% Graphics and figures
\usepackage{graphicx}       % Include graphics
\usepackage{float}          % Improved float placement
\usepackage[skins,breakable]{tcolorbox} % Coloured and framed text boxes
\tcbset{before upper=\setlength{\parskip}{3pt}}

% Tables
\usepackage{longtable}      % Multi-page tables

% Fonts and typography
\usepackage{fontspec}       % Font selection for LuaLaTeX
\usepackage{mathptmx}       % Times-like math fonts
\usepackage{newpxtext}      % Palatino-like font for body text

% Colors and visual elements
\usepackage[dvipsnames]{xcolor}  % Extended color support
\usepackage{tikz}           % Programmatic graphics
\usetikzlibrary{positioning}
\usetikzlibrary{calc}
\usepackage{tikzpagenodes}  % TikZ positioning relative to page

% Code listings
\usepackage{listings}       % Code highlighting

% Hyperlinks
\usepackage{hyperref}       % Clickable links in PDF

% Conditional logic
\usepackage{ifthen}         % If-then-else commands

% Math symbols
\usepackage{amsmath}        % AMS math extensions
\usepackage{amssymb}        % AMS math symbols
\usepackage{latexsym}       % Additional LaTeX symbols
\usepackage{pifont}         % Zapf Dingbats symbols
\providecommand{\blacklozenge}{\ding{117}}  % Black diamond symbol

% Lists
\usepackage{enumitem}       % Customizable lists

% Margin notes and sidenotes
\usepackage{marginfix}      % Fixes margin note overflow
\usepackage{marginnote}     % Margin notes
\usepackage{sidenotes}      % Academic-style sidenotes
\renewcommand\raggedrightmarginnote{\sloppy}
\renewcommand\raggedleftmarginnote{\sloppy}

% Typography improvements
\usepackage{ragged2e}       % Better ragged text
\usepackage[all]{nowidow}   % Prevent widows and orphans
\usepackage{needspace}      % Ensure minimum space on page

% Section formatting
\usepackage[explicit]{titlesec}  % Custom section titles
\usepackage{tocloft}        % Table of contents formatting

% QR codes and icons
\usepackage{fontawesome5}   % Font Awesome icons
\usepackage{qrcode}         % QR code generation
\qrset{link, height=15mm}

% =============================================================================
% FLOAT CONFIGURATION
% =============================================================================
% Allow more floats per page to handle figure-heavy chapters
\extrafloats{200}
\setcounter{topnumber}{12}       % Max floats at top of page
\setcounter{bottomnumber}{12}    % Max floats at bottom of page
\setcounter{totalnumber}{24}     % Max floats per page
\setcounter{dbltopnumber}{8}     % Max floats at top of two-column page
\renewcommand{\topfraction}{.95}  % Max fraction of page for top floats
\renewcommand{\bottomfraction}{.95}
\renewcommand{\textfraction}{.05}  % Min fraction of page for text
\renewcommand{\floatpagefraction}{.7}  % Min fraction of float page
\renewcommand{\dbltopfraction}{.95}

% Prevent "Float(s) lost" errors by flushing floats more aggressively
\usepackage{placeins}  % Provides \FloatBarrier

% =============================================================================
% COLOR DEFINITIONS
% =============================================================================
% NOTE: TikZ colors (BlueLine, GreenLine, RedLine, OrangeLine, etc.) are defined
% in the YAML config files under format > pdf > tikz > include-headers.
% Only colors specific to LaTeX packages (not TikZ) are defined here.

% Harvard crimson - primary brand color used throughout
\definecolor{crimson}{HTML}{A51C30}

% Quiz element colors
\definecolor{quiz-question-color1}{RGB}{225,243,248}  % Light blue background
\definecolor{quiz-question-color2}{RGB}{17,158,199}   % Blue border
\definecolor{quiz-answer-color1}{RGB}{250,234,241}    % Light pink background
\definecolor{quiz-answer-color2}{RGB}{152,14,90}      % Magenta border

% =============================================================================
% LIST FORMATTING
% =============================================================================
% Tighter list spacing for academic style
\def\tightlist{}
\setlist{itemsep=1pt, parsep=1pt, topsep=0pt,after={\vspace{0.3\baselineskip}}}
\let\tightlist\relax

\makeatletter
\@ifpackageloaded{framed}{}{\usepackage{framed}}
\@ifpackageloaded{fancyvrb}{}{\usepackage{fancyvrb}}
\makeatother

\makeatletter
%New float "codelisting" has been updated
\AtBeginDocument{%
\floatstyle{ruled}
\newfloat{codelisting}{!htb}{lop}
\floatname{codelisting}{Listing}
\floatplacement{codelisting}{!htb}
\captionsetup[codelisting]{labelfont={bf,ninept},labelformat=mylabel,
  singlelinecheck=false,width=\linewidth,labelsep=none,font={ninept}}%
\renewenvironment{snugshade}{%
   \def\OuterFrameSep{3pt}%
   \def\FrameCommand{\fboxsep=5pt\colorbox{shadecolor}}%
   \MakeFramed{\advance\hsize-\width\FrameRestore}%
   \leftskip 0.5em \rightskip 0.5em%
   \small% decrease font size
   }{\endMakeFramed}%
}
\makeatother

%The space before and after the verbatim environment "Highlighting" has been reduced
\fvset{listparameters=\setlength{\topsep}{0pt}\setlength{\partopsep}{0pt}}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{framesep=0mm,commandchars=\\\{\}}

\makeatletter
\renewcommand\fs@ruled{\def\@fs@cfont{\bfseries}\let\@fs@capt\floatc@ruled
\def\@fs@pre{\hrule height.8pt depth0pt \kern2pt}%
\def\@fs@post{\kern2pt\hrule\relax}%
\def\@fs@mid{\kern2pt\hrule\kern1pt}%space between float and caption
\let\@fs@iftopcapt\iftrue}
\makeatother


% =============================================================================
% HYPHENATION RULES
% =============================================================================
% Explicit hyphenation points for technical terms to avoid bad breaks
\hyphenation{
  light-weight
  light-weight-ed
  de-vel-op-ment
  un-der-stand-ing
  mod-els
  prin-ci-ples
  ex-per-tise
  com-pli-cat-ed
  blue-print
  per‧for‧mance
  com-mu-ni-ca-tion
  par-a-digms
  hy-per-ten-sion
  a-chieved
}

% =============================================================================
% CODE LISTING CONFIGURATION
% =============================================================================
% Settings for code blocks using listings package
\lstset{
breaklines=true,              % Automatic line wrapping
breakatwhitespace=true,       % Break at whitespace only
basicstyle=\ttfamily,         % Monospace font
frame=none,                   % No frame around code
keepspaces=true,              % Preserve spaces
showspaces=false,             % Don't show space characters
showtabs=false,               % Don't show tab characters
columns=flexible,             % Flexible column width
belowskip=0pt,               % Minimal spacing
aboveskip=0pt
}

% =============================================================================
% PAGE GEOMETRY
% =============================================================================
% MIT Press trim size: 7" x 10" (per publisher specifications)
% This is a standard academic textbook format providing good readability
% for technical content with figures and code blocks.
% Wide outer margin accommodates sidenotes/margin notes.
\geometry{
  paperwidth=7in,
  paperheight=10in,
  top=0.875in,
  bottom=0.875in,
  inner=0.875in,              % Inner margin (binding side)
  outer=1.75in,               % Outer margin (includes space for sidenotes)
  footskip=30pt,
  marginparwidth=1.25in,      % Width for margin notes
  twoside                     % Different left/right pages
}

% =============================================================================
% SIDENOTE STYLING
% =============================================================================
% Custom sidenote design with crimson vertical bar
\renewcommand{\thefootnote}{\textcolor{crimson}{\arabic{footnote}}}

% Save original sidenote command
\makeatletter
\@ifundefined{oldsidenote}{
  \let\oldsidenote\sidenote%
}{}
\makeatother

% Redefine sidenote with vertical crimson bar
\renewcommand{\sidenote}[1]{%
  \oldsidenote{%
    \noindent
    \color{crimson!100}                        % Crimson vertical line
    \raisebox{0em}{%
      \rule{0.5pt}{1.5em}                      % Thin vertical line
    }
    \hspace{0.3em}                             % Space after line
    \color{black}                              % Reset text color
    \footnotesize #1                           % Sidenote content
  }%
}

% =============================================================================
% FLOAT HANDLING
% =============================================================================
% Patch LaTeX's output routine to handle float overflow gracefully
% The "Float(s) lost" error occurs in \@doclearpage when \@currlist is not empty
% This patch silently clears pending floats that can't be placed
\makeatletter
\let\orig@doclearpage\@doclearpage
\def\@doclearpage{%
  \ifx\@currlist\@empty\else
    \global\let\@currlist\@empty
    \typeout{Warning: Floats cleared to prevent overflow}%
  \fi
  \orig@doclearpage
}
\makeatother

% Additional safety for structural commands
\let\originalbackmatter\backmatter
\renewcommand{\backmatter}{%
  \clearpage%
  \originalbackmatter%
}

\let\originalfrontmatter\frontmatter
\renewcommand{\frontmatter}{%
  \clearpage%
  \originalfrontmatter%
}

\let\originalmainmatter\mainmatter
\renewcommand{\mainmatter}{%
  \clearpage%
  \originalmainmatter%
}

% =============================================================================
% PAGE HEADERS AND FOOTERS
% =============================================================================
% Ensure chapters use fancy page style (not plain)
\patchcmd{\chapter}{\thispagestyle{plain}}{\thispagestyle{fancy}}{}{}

% Main page style with crimson headers
\pagestyle{fancy}
\fancyhf{}                                              % Clear all
\fancyhead[LE]{\small\color{crimson}\nouppercase{\rightmark}}  % Left even: section
\fancyhead[RO]{\color{crimson}\thepage}                 % Right odd: page number
\fancyhead[LO]{\small\color{crimson}\nouppercase{\leftmark}}   % Left odd: chapter
\fancyhead[RE]{\color{crimson}\thepage}                 % Right even: page number
\renewcommand{\headrulewidth}{0.4pt}                    % Thin header line
\renewcommand{\footrulewidth}{0pt}                      % No footer line

% Plain page style (for chapter openings)
\fancypagestyle{plain}{
  \fancyhf{}
  \fancyfoot[C]{\color{crimson}\thepage}                % Centered page number
  \renewcommand{\headrulewidth}{0pt}
  \renewcommand{\footrulewidth}{0pt}
}

% =============================================================================
% KOMA-SCRIPT FONT ADJUSTMENTS
% =============================================================================
% Apply crimson color to all heading levels
\addtokomafont{disposition}{\rmfamily\color{crimson}}
\addtokomafont{chapter}{\color{crimson}}
\addtokomafont{section}{\color{crimson}}
\addtokomafont{subsection}{\color{crimson}}

% =============================================================================
% ABSTRACT ENVIRONMENT
% =============================================================================
\newenvironment{abstract}{
  \chapter*{\abstractname}
  \addcontentsline{toc}{chapter}{\abstractname}
  \small
}{
  \clearpage
}

% =============================================================================
% HYPERLINK CONFIGURATION
% =============================================================================
% Crimson-colored links throughout, two-page PDF layout
\hypersetup{
  linkcolor=crimson,
  citecolor=crimson,
  urlcolor=crimson,
  pdfpagelayout=TwoPageRight,   % Two-page spread view
  pdfstartview=Fit               % Initial zoom fits page
}

% =============================================================================
% PART SUMMARY SYSTEM
% =============================================================================
% Allows adding descriptive text below part titles
\newcommand{\partsummary}{}     % Empty by default
\newif\ifhaspartsummary%
\haspartsummaryfalse%

\newcommand{\setpartsummary}[1]{%
  \renewcommand{\partsummary}{#1}%
  \haspartsummarytrue%
}

% Additional colors for part page backgrounds
\definecolor{BrownLL}{RGB}{233,222,220}
\definecolor{BlueDD}{RGB}{62,100,125}
\colorlet{BlueDD}{magenta}

% ===============================================================================
% PART STYLING SYSTEM
% ===============================================================================
%
% This system provides three distinct visual styles for book organization:
%
% 1. NUMBERED PARTS (\part{title}) - For main book sections
%    - Roman numerals (I, II, III, etc.) in top right corner
%    - Crimson title with horizontal lines above/below
%    - "Part I" label in sidebar
%    - Used for: foundations, principles, optimization, deployment, etc.
%
% 2. UNNUMBERED PARTS (\part*{title}) - For special sections like "Labs"
%    - Division-style geometric background (left side)
%    - No Roman numerals
%    - Used for: labs section
%
% 3. DIVISIONS (\division{title}) - For major book divisions
%    - Clean geometric background with centered title
%    - Used for: frontmatter, main_content, backmatter
%
% The Lua filter (inject-parts.lua) automatically routes parts by {key:xxx} commands
% to the appropriate LaTeX command based on the key name.
% ===============================================================================

% NUMBERED PARTS: Roman numeral styling for main book sections
\titleformat{\part}[display]
{\thispagestyle{empty}}{}{20pt}{
\begin{tikzpicture}[remember picture,overlay]
%%%
%%
\node[crimson,align=flush right,
inner sep=0,outer sep=0mm,draw=none,%
anchor=east,minimum height=31mm, text width=1.2\textwidth,
yshift=-30mm,font={%
\fontsize{98pt}{104}\selectfont\bfseries}]  (BG) at (current page text area.north east){\thepart};
%
\node[black,inner sep=0mm,draw=none,
anchor=mid,text width=1.2\textwidth,
 minimum height=35mm, align=right,
node distance=7mm,below=of BG,
font={\fontsize{30pt}{34}\selectfont}]
(BGG)  {\hyphenchar\font=-1 \color{black}\MakeUppercase {#1}};
\draw [crimson,line width=3pt] ([yshift=0mm]BGG.north west) -- ([yshift=0mm]BGG.north east);
\draw [crimson,line width=2pt] ([yshift=0mm]BGG.south west) -- ([yshift=0mm]BGG.south east);
%
\node[fill=crimson,text=white,rotate=90,%
anchor=south west,minimum height=15mm,
minimum width=40mm,font={%
\fontsize{20pt}{20}\selectfont\bfseries}](BP)  at
(current page text area.south east)
{{\sffamily Part}~\thepart};
%
\path[red](BP.north west)-|coordinate(PS)(BGG.south west);
%
% Part summary box commented out for cleaner design
% \ifhaspartsummary
% \node[inner sep=4pt,text width=0.7\textwidth,draw=none,fill=BrownLL!40,
% align=justify,font={\fontsize{9pt}{12}\selectfont},anchor=south west]
% at (PS) {\partsummary};
% \fi
\end{tikzpicture}
}[]

\renewcommand{\thepart}{\Roman{part}}

% UNNUMBERED PARTS: Division-style background for special sections
\titleformat{name=\part,numberless}[display]
{\thispagestyle{empty}}{}{20pt}{
\begin{tikzpicture}[remember picture,overlay]
%%%
\coordinate(S1)at([yshift=-200mm]current page.north west);
\draw[draw=none,fill=BlueDD!7](S1)--++(45:16)coordinate(S2)-
|(S2|-current page.north west)--(current page.north west)coordinate(S3)--(S1);
%
\coordinate(E1)at([yshift=-98mm]current page.north west);
\draw[draw=none,fill=BlueDD!15](E1)--(current page.north west)coordinate(E2)
--++(0:98mm)coordinate(E3)--(E1);
%
\coordinate(D1)at([yshift=15mm]current page.south west);
\draw[draw=none,fill=BlueDD!40,opacity=0.5](D1)--++(45:5.5)coordinate(D2)
-|(D2|-current page.north west)--(current page.north west)coordinate(D3)--(D1);
%%%%
\path[red](S2)-|(S2-|current page.east)coordinate(SS2);
%PART
\node[crimson,align=flush right,inner sep=0,outer sep=0mm,draw=none,anchor=south,
font={\fontsize{48pt}{48}\selectfont\bfseries}]  (BG) at ($(S2)!0.5!(SS2)$){\hphantom{Part}};
%%%
\path[green]([yshift=15mm]D2)-|coordinate(TPD)(BG.south east);
\node[inner sep=0mm,draw=none,anchor=south east,%text width=0.9\textwidth,
align=right,font={\fontsize{40pt}{40}\selectfont}]
(BGG) at (TPD)  {\color{crimson}\MakeUppercase {#1}};%\MakeUppercase {}
\end{tikzpicture}
}

% Define \numberedpart command for numbered parts
\newcommand{\numberedpart}[1]{%
\FloatBarrier%  % Flush all pending floats before part break
\clearpage
\thispagestyle{empty}
\stepcounter{part}%
\begin{tikzpicture}[remember picture,overlay]
%%%
%%
\node[crimson,align=flush right,
inner sep=0,outer sep=0mm,draw=none,%
anchor=east,minimum height=31mm, text width=1.2\textwidth,
yshift=-30mm,font={%
\fontsize{98pt}{104}\selectfont\bfseries}]  (BG) at (current page text area.north east){\thepart};
%
\node[black,inner sep=0mm,draw=none,
anchor=mid,text width=1.2\textwidth,
 minimum height=35mm, align=right,
node distance=7mm,below=of BG,
font={\fontsize{30pt}{34}\selectfont}]
(BGG)  {\hyphenchar\font=-1 \color{black}\MakeUppercase {#1}};
\draw [crimson,line width=3pt] ([yshift=0mm]BGG.north west) -- ([yshift=0mm]BGG.north east);
\draw [crimson,line width=2pt] ([yshift=0mm]BGG.south west) -- ([yshift=0mm]BGG.south east);
%
\node[fill=crimson,text=white,rotate=90,%
anchor=south west,minimum height=15mm,
minimum width=40mm,font={%
\fontsize{20pt}{20}\selectfont\bfseries}](BP)  at
(current page text area.south east)
{{\sffamily Part}~\thepart};
%
\path[red](BP.north west)-|coordinate(PS)(BGG.south west);
%
% Part summary box commented out for cleaner design
% \ifhaspartsummary
% \node[inner sep=4pt,text width=0.7\textwidth,draw=none,fill=BrownLL!40,
% align=justify,font={\fontsize{9pt}{12}\selectfont},anchor=south west]
% at (PS) {\partsummary};
% \fi
\end{tikzpicture}
\clearpage
}



% DIVISIONS: Clean geometric styling with subtle tech elements
% Used for frontmatter, main_content, and backmatter divisions
\newcommand{\division}[1]{%
\FloatBarrier%  % Flush all pending floats before division break
\clearpage
\thispagestyle{empty}
\begin{tikzpicture}[remember picture,overlay]

% Clean geometric background (original design)
\coordinate(S1)at([yshift=-200mm]current page.north west);
\draw[draw=none,fill=BlueDD!7](S1)--++(45:16)coordinate(S2)-
|(S2|-current page.north west)--(current page.north west)coordinate(S3)--(S1);

\coordinate(E1)at([yshift=-98mm]current page.north west);
\draw[draw=none,fill=BlueDD!15](E1)--(current page.north west)coordinate(E2)
--++(0:98mm)coordinate(E3)--(E1);

\coordinate(D1)at([yshift=15mm]current page.south west);
\draw[draw=none,fill=BlueDD!40,opacity=0.5](D1)--++(45:5.5)coordinate(D2)
-|(D2|-current page.north west)--(current page.north west)coordinate(D3)--(D1);

% Subtle tech elements - positioned in white areas for better visibility
% Upper right white area - more visible
\draw[crimson!40, line width=0.8pt] ([xshift=140mm,yshift=-60mm]current page.north west) -- ++(40mm,0);
\draw[crimson!40, line width=0.8pt] ([xshift=150mm,yshift=-70mm]current page.north west) -- ++(30mm,0);
\draw[crimson!35, line width=0.7pt] ([xshift=160mm,yshift=-60mm]current page.north west) -- ++(0,-15mm);
\draw[crimson!35, line width=0.7pt] ([xshift=170mm,yshift=-70mm]current page.north west) -- ++(0,10mm);

% Circuit nodes - upper right
\fill[crimson!50] ([xshift=160mm,yshift=-60mm]current page.north west) circle (1.5mm);
\fill[white] ([xshift=160mm,yshift=-60mm]current page.north west) circle (0.8mm);
\fill[crimson!50] ([xshift=170mm,yshift=-70mm]current page.north west) circle (1.3mm);
\fill[white] ([xshift=170mm,yshift=-70mm]current page.north west) circle (0.6mm);

% Lower right white area - enhanced visibility
\draw[crimson!45, line width=0.9pt] ([xshift=140mm,yshift=-190mm]current page.north west) -- ++(45mm,0);
\draw[crimson!45, line width=0.9pt] ([xshift=150mm,yshift=-200mm]current page.north west) -- ++(35mm,0);
\draw[crimson!40, line width=0.8pt] ([xshift=160mm,yshift=-190mm]current page.north west) -- ++(0,-20mm);
\draw[crimson!40, line width=0.8pt] ([xshift=170mm,yshift=-200mm]current page.north west) -- ++(0,15mm);

% Additional connecting lines in lower right
\draw[crimson!35, line width=0.7pt] ([xshift=130mm,yshift=-180mm]current page.north west) -- ++(25mm,0);
\draw[crimson!35, line width=0.7pt] ([xshift=145mm,yshift=-180mm]current page.north west) -- ++(0,-25mm);

% Circuit nodes - lower right (more prominent)
\fill[crimson!55] ([xshift=160mm,yshift=-190mm]current page.north west) circle (1.6mm);
\fill[white] ([xshift=160mm,yshift=-190mm]current page.north west) circle (0.9mm);
\fill[crimson!55] ([xshift=170mm,yshift=-200mm]current page.north west) circle (1.4mm);
\fill[white] ([xshift=170mm,yshift=-200mm]current page.north west) circle (0.7mm);
\fill[crimson!50] ([xshift=145mm,yshift=-180mm]current page.north west) circle (1.2mm);
\fill[white] ([xshift=145mm,yshift=-180mm]current page.north west) circle (0.6mm);

% Title positioned in center - clean and readable
\node[inner sep=0mm,draw=none,anchor=center,text width=0.8\textwidth,
align=center,font={\fontsize{40pt}{40}\selectfont}]
(BGG) at (current page.center)  {\color{crimson}\MakeUppercase {#1}};

\end{tikzpicture}
\clearpage
}

% LAB DIVISIONS: Circuit-style neural network design for lab sections
% Used specifically for lab platform sections (arduino, xiao, grove, etc.)
\newcommand{\labdivision}[1]{%
\FloatBarrier%  % Flush all pending floats before lab division break
\clearpage
\thispagestyle{empty}
\begin{tikzpicture}[remember picture,overlay]
% Circuit background with subtle gradient
\coordinate(S1)at([yshift=-200mm]current page.north west);
\draw[draw=none,fill=BlueDD!5](S1)--++(45:16)coordinate(S2)-
|(S2|-current page.north west)--(current page.north west)coordinate(S3)--(S1);

% TOP AREA: Circuit lines in upper white space
\draw[crimson!50, line width=1.5pt] ([xshift=30mm,yshift=-40mm]current page.north west) -- ++(60mm,0);
\draw[crimson!40, line width=1pt] ([xshift=120mm,yshift=-50mm]current page.north west) -- ++(50mm,0);
\draw[crimson!50, line width=1.5pt] ([xshift=40mm,yshift=-70mm]current page.north west) -- ++(40mm,0);

% Connecting lines in top area
\draw[crimson!30, line width=1pt] ([xshift=60mm,yshift=-40mm]current page.north west) -- ++(0,-20mm);
\draw[crimson!30, line width=1pt] ([xshift=145mm,yshift=-50mm]current page.north west) -- ++(0,10mm);

% Neural nodes in top area
\fill[crimson!70] ([xshift=60mm,yshift=-40mm]current page.north west) circle (2.5mm);
\fill[white] ([xshift=60mm,yshift=-40mm]current page.north west) circle (1.5mm);
\fill[crimson!60] ([xshift=145mm,yshift=-50mm]current page.north west) circle (2mm);
\fill[white] ([xshift=145mm,yshift=-50mm]current page.north west) circle (1mm);
\fill[crimson!80] ([xshift=80mm,yshift=-70mm]current page.north west) circle (2mm);
\fill[white] ([xshift=80mm,yshift=-70mm]current page.north west) circle (1mm);

% BOTTOM AREA: Circuit lines in lower white space
\draw[crimson!50, line width=1.5pt] ([xshift=20mm,yshift=-200mm]current page.north west) -- ++(70mm,0);
\draw[crimson!40, line width=1pt] ([xshift=110mm,yshift=-210mm]current page.north west) -- ++(60mm,0);
\draw[crimson!50, line width=1.5pt] ([xshift=35mm,yshift=-230mm]current page.north west) -- ++(45mm,0);

% Connecting lines in bottom area
\draw[crimson!30, line width=1pt] ([xshift=55mm,yshift=-200mm]current page.north west) -- ++(0,-20mm);
\draw[crimson!30, line width=1pt] ([xshift=140mm,yshift=-210mm]current page.north west) -- ++(0,15mm);

% Neural nodes in bottom area
\fill[crimson!70] ([xshift=55mm,yshift=-200mm]current page.north west) circle (2.5mm);
\fill[white] ([xshift=55mm,yshift=-200mm]current page.north west) circle (1.5mm);
\fill[crimson!60] ([xshift=140mm,yshift=-210mm]current page.north west) circle (2mm);
\fill[white] ([xshift=140mm,yshift=-210mm]current page.north west) circle (1mm);
\fill[crimson!80] ([xshift=80mm,yshift=-230mm]current page.north west) circle (2mm);
\fill[white] ([xshift=80mm,yshift=-230mm]current page.north west) circle (1mm);

% SIDE AREAS: Subtle circuit elements on left and right edges
\draw[crimson!30, line width=1pt] ([xshift=15mm,yshift=-120mm]current page.north west) -- ++(20mm,0);
\draw[crimson!30, line width=1pt] ([xshift=175mm,yshift=-130mm]current page.north west) -- ++(15mm,0);
\fill[crimson!50] ([xshift=25mm,yshift=-120mm]current page.north west) circle (1.5mm);
\fill[white] ([xshift=25mm,yshift=-120mm]current page.north west) circle (0.8mm);
\fill[crimson!50] ([xshift=185mm,yshift=-130mm]current page.north west) circle (1.5mm);
\fill[white] ([xshift=185mm,yshift=-130mm]current page.north west) circle (0.8mm);

% Title positioned in center - CLEAN AREA
\node[inner sep=0mm,draw=none,anchor=center,text width=0.8\textwidth,
align=center,font={\fontsize{44pt}{44}\selectfont\bfseries}]
(BGG) at (current page.center)  {\color{crimson}\MakeUppercase {#1}};

\end{tikzpicture}
\clearpage
}

% Define \lab command for lab styling (different visual treatment)
\newcommand{\lab}[1]{%
\begin{tikzpicture}[remember picture,overlay]
%%%
% Different background pattern for labs
\coordinate(S1)at([yshift=-200mm]current page.north west);
\draw[draw=none,fill=BlueDD!15](S1)--++(45:16)coordinate(S2)-
|(S2|-current page.north west)--(current page.north west)coordinate(S3)--(S1);
%
\coordinate(E1)at([yshift=-98mm]current page.north west);
\draw[draw=none,fill=BlueDD!25](E1)--(current page.north west)coordinate(E2)
--++(0:98mm)coordinate(E3)--(E1);
%
\coordinate(D1)at([yshift=15mm]current page.south west);
\draw[draw=none,fill=BlueDD!60,opacity=0.7](D1)--++(45:5.5)coordinate(D2)
-|(D2|-current page.north west)--(current page.north west)coordinate(D3)--(D1);
%%%%
\path[red](S2)-|(S2-|current page.east)coordinate(SS2);
%LAB - Different styling
\node[crimson,align=flush right,inner sep=0,outer sep=0mm,draw=none,anchor=south,
font={\fontsize{48pt}{48}\selectfont\bfseries}]  (BG) at ($(S2)!0.5!(SS2)$){\hphantom{Workshop}};
%%%
\path[green]([yshift=15mm]D2)-|coordinate(TPD)(BG.south east);
\node[inner sep=0mm,draw=none,anchor=south east,%text width=0.9\textwidth,
align=right,font={\fontsize{40pt}{40}\selectfont}]
(BGG) at (TPD)  {\color{crimson}\MakeUppercase {#1}};%\MakeUppercase {}
\end{tikzpicture}
\thispagestyle{empty}
\clearpage
}

% =============================================================================
% SECTION FORMATTING
% =============================================================================
% All section levels use crimson color and are ragged right

% Section (Large, bold, crimson)
\titleformat{\section}
  {\normalfont\Large\bfseries\color{crimson}\raggedright}
  {\thesection}
  {0.5em}
  {#1}
\titlespacing*{\section}{0pc}{14pt plus 4pt minus 4pt}{6pt plus 2pt minus 2pt}[0pc]

% Subsection (large, bold, crimson)
\titleformat{\subsection}
  {\normalfont\large\bfseries\color{crimson}\raggedright}
  {\thesubsection}
  {0.5em}
  {#1}
\titlespacing*{\subsection}{0pc}{12pt plus 4pt minus 4pt}{5pt plus 1pt minus 2pt}[0pc]

% Subsubsection (normal size, bold, crimson)
\titleformat{\subsubsection}
  {\normalfont\normalsize\bfseries\color{crimson}\raggedright}
  {\thesubsubsection}
  {0.5em}
  {#1}
\titlespacing*{\subsubsection}{0pc}{12pt plus 4pt minus 4pt}{5pt plus 1pt minus 2pt}[0pc]

% Paragraph (run-in, bold, crimson)
\titleformat{\paragraph}[runin]
  {\normalfont\normalsize\bfseries\color{crimson}}
  {\theparagraph}
  {0.5em}
  {#1}
\titlespacing*{\paragraph}{0pc}{6pt plus 2pt minus 2pt}{0.5em}[0pc]

% Subparagraph (run-in, italic, crimson)
\titleformat{\subparagraph}[runin]
  {\normalfont\normalsize\itshape\color{crimson}}
  {\thesubparagraph}
  {0.5em}
  {#1}
\titlespacing*{\subparagraph}{0pc}{6pt plus 2pt minus 2pt}{0.5em}[0pc]

% =============================================================================
% CHAPTER FORMATTING
% =============================================================================
% Numbered chapters: "Chapter X" prefix, huge crimson title
\titleformat{\chapter}[display]
  {\normalfont\huge\bfseries\color{crimson}}
  {\chaptername\ \thechapter}
  {20pt}
  {\Huge #1}
  []

% Unnumbered chapters: no prefix, huge crimson title
\titleformat{name=\chapter,numberless}
  {\normalfont\huge\bfseries\color{crimson}}
  {}
  {0pt}
  {\Huge #1}
  []

\renewcommand{\chaptername}{Chapter}
% =============================================================================
% TABLE OF CONTENTS FORMATTING
% =============================================================================
\setcounter{tocdepth}{2}                      % Show chapters, sections, subsections

% TOC spacing adjustments for number widths and indentation
\setlength{\cftchapnumwidth}{2em}             % Chapter number width
\setlength{\cftsecnumwidth}{2.75em}           % Section number width
\setlength{\cftsubsecnumwidth}{3.25em}        % Subsection number width
\setlength{\cftsubsubsecnumwidth}{4em}        % Subsubsection number width
\setlength{\cftsubsecindent}{4.25em}          % Subsection indent
\setlength{\cftsubsubsecindent}{7.5em}        % Subsubsection indent

% Chapter entries in TOC: bold crimson with "Chapter" prefix
\renewcommand{\cftchapfont}{\bfseries\color{crimson}}
\renewcommand{\cftchappresnum}{\color{crimson}Chapter~}

% Custom formatting for division entries (styled like parts)
\newcommand{\divisionchapter}[1]{%
  \addvspace{12pt}%
  \noindent\hfil\bfseries\color{crimson}#1\hfil\par%
  \addvspace{6pt}%
}

% Adjust TOC spacing for "Chapter" prefix
\newlength{\xtraspace}
\settowidth{\xtraspace}{\cftchappresnum\cftchapaftersnum}
\addtolength{\cftchapnumwidth}{\xtraspace}

% Unnumbered chapters with TOC entry
\newcommand{\likechapter}[1]{%
    \chapter*{#1}
    \addcontentsline{toc}{chapter}{\textcolor{crimson}{#1}}
}

% =============================================================================
% PAGE NUMBERING SYSTEM
% =============================================================================
% Implements traditional book numbering:
% - Roman numerals (i, ii, iii...) for frontmatter
% - Arabic numerals (1, 2, 3...) for mainmatter
% Automatically switches at first numbered chapter
\makeatletter
\newif\if@firstnumbered%
\@firstnumberedtrue%
\newif\if@firstunnumbered%
\@firstunnumberedtrue%

\newcounter{lastRomanPage}
\setcounter{lastRomanPage}{1}

% Start document with Roman numerals (frontmatter)
\AtBeginDocument{
  \pagenumbering{roman}
  \renewcommand{\thepage}{\roman{page}}
}

% Intercept chapter command
\let\old@chapter\chapter%
\renewcommand{\chapter}{%
  \@ifstar{\unnumbered@chapter}{\numbered@chapter}%
}

% Numbered chapters: switch to Arabic on first occurrence
\newcommand{\numbered@chapter}[1]{%
  \if@firstnumbered%
    \cleardoublepage%
    \setcounter{lastRomanPage}{\value{page}}%
    \pagenumbering{arabic}%
    \@firstnumberedfalse%
  \else
    \setcounter{page}{\value{page}}%
  \fi
  \setcounter{sidenote}{1}                    % Reset footnote counter per chapter
  \old@chapter{#1}%
}

% Unnumbered chapters: stay in Roman numerals
\newcommand{\unnumbered@chapter}[1]{%
  \if@firstunnumbered%
    \clearpage
    \setcounter{lastRomanPage}{\value{page}}%
    \pagenumbering{roman}%
    \@firstunnumberedfalse%
  \fi
  \setcounter{sidenote}{1}
  \old@chapter*{#1}%
}
\makeatother

% =============================================================================
% TABLE SIZING AND SPACING
% =============================================================================
% Make tables slightly smaller to fit more content
\AtBeginEnvironment{longtable}{\scriptsize}

% Increase vertical spacing in table cells (default is 1.0)
\renewcommand{\arraystretch}{1.5}

% Prefer placing figures and tables at the top of pages
\makeatletter
\renewcommand{\fps@figure}{t}  % Default placement: top of page
\renewcommand{\fps@table}{t}   % Default placement: top of page
\makeatother

% =============================================================================
% LONGTABLE PAGE BREAKING FIXES (Windows compatibility)
% =============================================================================
% Prevent "Infinite glue shrinkage" errors on Windows LaTeX builds
% by giving longtable more flexibility in page breaking

% Allow more flexible page breaking (vs strict \flushbottom)
\raggedbottom

% Process more rows before attempting page break (default is 20)
\setcounter{LTchunksize}{50}

% Add extra stretch for longtable environments specifically
\AtBeginEnvironment{longtable}{%
  \setlength{\emergencystretch}{3em}%
  \setlength{\parskip}{0pt plus 1pt}%
}

% =============================================================================
% TABLE STYLING - Clean tables with crimson borders
% =============================================================================
% Professional table appearance with:
% - Clean white background (no colored rows)
% - Crimson-colored borders
% - Good spacing for readability
%
% Note: Headers are automatically bolded by Quarto when using **text** in source
\usepackage{booktabs}      % Professional table rules (\toprule, \midrule, \bottomrule)
\usepackage{colortbl}      % For colored borders (\arrayrulecolor)

% Global table styling - crimson borders
\setlength{\arrayrulewidth}{0.5pt}          % Thinner borders than default
%\arrayrulecolor{crimson}                    % Crimson borders matching brand

\setcounter{chapter}{0}

% =============================================================================
% DROP CAPS (Lettrine)
% =============================================================================
% Decorative large first letter at chapter openings, following the tradition
% of Hennessy & Patterson and other MIT Press textbooks.
% Usage in QMD: \lettrine{T}{he first sentence...}
\usepackage{lettrine}
\renewcommand{\LettrineFontHook}{\color{crimson}\bfseries}
\setcounter{DefaultLines}{3}          % Drop cap spans 3 lines
\renewcommand{\DefaultLoversize}{0.1} % Slight oversize for visual weight
\renewcommand{\DefaultLraise}{0}      % No vertical shift
\setlength{\DefaultNindent}{0.5em}    % Indent of continuation text
\setlength{\DefaultSlope}{0pt}        % No slope on continuation

% =============================================================================
% RUNNING HEADERS — Truncation Safety
% =============================================================================
% Long chapter/section titles can overflow the header. These marks truncate
% gracefully so headers stay within the text block.
\renewcommand{\chaptermark}[1]{%
  \markboth{\thechapter.\ #1}{}}
\renewcommand{\sectionmark}[1]{%
  \markright{\thesection\ #1}}

% =============================================================================
% EPIGRAPH ENVIRONMENT
% =============================================================================
% For chapter-opening quotations. Renders as right-aligned italic block
% with attribution in small caps below.
% Usage: \epigraph{Quote text}{Author Name, \textit{Source}}
\newcommand{\bookepigraph}[2]{%
  \vspace{1em}%
  \begin{flushright}%
    \begin{minipage}{0.75\textwidth}%
      \raggedleft\itshape\small #1\\[0.5em]%
      \normalfont\small --- #2%
    \end{minipage}%
  \end{flushright}%
  \vspace{1.5em}%
}

% =============================================================================
% THUMB INDEX TABS
% =============================================================================
% Colored tabs on the outer page edge for quick chapter navigation.
% Each Part gets a different vertical position; chapters within a Part
% share the same tab position. Visible when flipping through the book.
\newcounter{thumbindex}
\setcounter{thumbindex}{0}
\newlength{\thumbtabheight}
\setlength{\thumbtabheight}{16mm}     % Height of each tab
\newlength{\thumbtabwidth}
\setlength{\thumbtabwidth}{8mm}       % Width protruding from edge
\newlength{\thumbtabgap}
\setlength{\thumbtabgap}{1mm}         % Gap between tabs

% Advance to next thumb tab position (call at each \part)
\newcommand{\nextthumb}{%
  \stepcounter{thumbindex}%
}

% Draw the thumb tab on every page (placed in header via fancyhdr)
\newcommand{\drawthumb}{%
  \ifnum\value{thumbindex}>0%
    \begin{tikzpicture}[remember picture,overlay]
      \pgfmathsetmacro{\thumboffset}{%
        20 + (\value{thumbindex}-1) * (16 + 1)}  % mm from top
      \ifodd\value{page}%
        % Odd pages: tab on right edge
        \fill[crimson!80]
          ([yshift=-\thumboffset mm]current page.north east)
          rectangle +(-\thumbtabwidth, -\thumbtabheight);
        \node[white,font=\tiny\bfseries,rotate=90]
          at ([yshift=-\thumboffset mm - 0.5\thumbtabheight,
               xshift=-0.5\thumbtabwidth]current page.north east)
          {\Roman{thumbindex}};
      \else
        % Even pages: tab on left edge
        \fill[crimson!80]
          ([yshift=-\thumboffset mm]current page.north west)
          rectangle +(\thumbtabwidth, -\thumbtabheight);
        \node[white,font=\tiny\bfseries,rotate=-90]
          at ([yshift=-\thumboffset mm - 0.5\thumbtabheight,
               xshift=0.5\thumbtabwidth]current page.north west)
          {\Roman{thumbindex}};
      \fi
    \end{tikzpicture}%
  \fi
}

% Hook into fancyhdr to draw thumb on every content page
\AddToHook{shipout/foreground}{%
  \drawthumb%
}

% =============================================================================
% CROP / BLEED MARKS
% =============================================================================
% For final print submission, uncomment the line below to add crop marks.
% MIT Press production will advise on exact requirements.
% \usepackage[cam,center,width=7.5in,height=10.5in]{crop}

% =============================================================================
% PDF/A ARCHIVAL COMPLIANCE
% =============================================================================
% MIT Press increasingly requires PDF/A for long-term preservation.
% This embeds all fonts and removes transparency.
% Note: pdfx must be loaded early; if it conflicts with hyperref,
% MIT Press production can handle the conversion post-build.
% Uncomment when ready for final submission:
% \usepackage[a-3u]{pdfx}

% =============================================================================
% ENHANCED WIDOW / ORPHAN CONTROL
% =============================================================================
% Prevent single lines at top/bottom of pages and breaks before equations
\clubpenalty=10000          % No orphans (single first line at bottom)
\widowpenalty=10000         % No widows (single last line at top)
\displaywidowpenalty=10000  % No widow before display math
\predisplaypenalty=10000    % No page break just before display math
\postdisplaypenalty=0       % Allow break after display math (natural)
\usepackage{needspace}
\let\Needspace\needspace
\makeatletter
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{plain}
\@ifundefined{c@chapter}{\newfloat{vid}{h}{lovid}}{\newfloat{vid}{h}{lovid}[chapter]}
\floatname{vid}{Video}
\newcommand*\listofvids{\listof{vid}{List of Videos}}
\makeatother
\makeatletter
\@ifpackageloaded{tcolorbox}{}{\usepackage[skins,breakable]{tcolorbox}}
\@ifpackageloaded{fontawesome5}{}{\usepackage{fontawesome5}}
\definecolor{quarto-callout-color}{HTML}{909090}
\definecolor{quarto-callout-note-color}{HTML}{0758E5}
\definecolor{quarto-callout-important-color}{HTML}{CC1914}
\definecolor{quarto-callout-warning-color}{HTML}{EB9113}
\definecolor{quarto-callout-tip-color}{HTML}{00A047}
\definecolor{quarto-callout-caution-color}{HTML}{FC5300}
\definecolor{quarto-callout-color-frame}{HTML}{acacac}
\definecolor{quarto-callout-note-color-frame}{HTML}{4582ec}
\definecolor{quarto-callout-important-color-frame}{HTML}{d9534f}
\definecolor{quarto-callout-warning-color-frame}{HTML}{f0ad4e}
\definecolor{quarto-callout-tip-color-frame}{HTML}{02b875}
\definecolor{quarto-callout-caution-color-frame}{HTML}{fd7e14}
\makeatother
\makeatletter
\@ifpackageloaded{bookmark}{}{\usepackage{bookmark}}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\makeatletter
\@ifpackageloaded{sidenotes}{}{\usepackage{sidenotes}}
\@ifpackageloaded{marginnote}{}{\usepackage{marginnote}}
\makeatother
\newcommand{\fbxIconPath}{assets/images/icons/callouts}
\newcommand{\fbxIconFormat}{pdf}
\makeatletter
\@ifpackageloaded{tcolorbox}{}{\usepackage[many]{tcolorbox}}
\makeatother
%%%% ---foldboxy preamble ----- %%%%%

% Load xstring for string manipulation
\RequirePackage{xstring}

% Icon path and format configuration - can be overridden in filter-metadata
\providecommand{\fbxIconPath}{assets/images/icons/callouts}
\providecommand{\fbxIconFormat}{pdf}

% Helper command to include icon with hyphen-to-underscore conversion
% This ensures consistency: callout-quiz-question -> callout_quiz_question
% Using height= instead of width= ensures consistent header heights across all icons
% regardless of aspect ratio
\newcommand{\fbxIncludeIcon}[2]{%
  \StrSubstitute{#1}{-}{_}[\fbxIconName]%
  \includegraphics[height=#2]{\fbxIconPath/icon_\fbxIconName.\fbxIconFormat}%
}

% Legacy fallback colors (keep for compatibility)
\definecolor{fbx-default-color1}{HTML}{c7c7d0}
\definecolor{fbx-default-color2}{HTML}{a3a3aa}
\definecolor{fbox-color1}{HTML}{c7c7d0}
\definecolor{fbox-color2}{HTML}{a3a3aa}

% arguments: #1 typelabelnummer: #2 titel: #3
\newenvironment{fbx}[3]{%
\begin{tcolorbox}[
  enhanced,
  breakable,
  %fontupper=\fontsize{8pt}{10pt}\selectfont,  % 95% of body text (10pt -> 9.5pt)
  before skip=8pt,  % space above box (increased)
  after skip=8pt,   % space below box (increased)
  attach boxed title to top*={xshift=0pt},
  boxed title style={
  %fuzzy shadow={1pt}{-1pt}{0mm}{0.1mm}{gray},
  arc=1.5pt,
  rounded corners=north,
  sharp corners=south,
  top=6pt,          % Adjusted for ~40px equivalent height
  bottom=5pt,       % Adjusted for ~40px equivalent height
  overlay={
      \node [left,outer sep=0em, black,draw=none,anchor=west,
        rectangle,fill=none,inner sep=0pt]
        at ([xshift=4mm]frame.west) {\fbxIncludeIcon{#1}{4.2mm}};
    },
  },
  colframe=#1-color2,             % Border color (auto-generated from YAML)
  colbacktitle=#1-color1,         % Background color (auto-generated from YAML)
  colback=white,
  coltitle=black,
  titlerule=0mm,
  toprule=0.5pt,
  bottomrule=0.5pt,
  leftrule=2.2pt,
  rightrule=0.5pt,
  outer arc=1.5pt,
  arc=1.5pt,
  left=0.5em,       % increased left padding
  bottomtitle=1.5mm, % increased title bottom margin
  toptitle=1.5mm,    % increased title top margin
  title=\hspace{2.5em}\protect#2\hspace{0.5em}\protect#3, % Protect parameters
  extras middle and last={top=4pt} % increased continuation spacing
]}
{\end{tcolorbox}}


% boxed environment with right border
\newenvironment{fbxSimple}[3]{\begin{tcolorbox}[
  enhanced,
  breakable,
  %fontupper=\fontsize{8pt}{10pt}\selectfont,  % 95% of body text (10pt -> 9.5pt)
  before skip=8pt,  % space above box (increased)
  after skip=8pt,   % space below box (increased)
  attach boxed title to top*={xshift=0pt},
  boxed title style={
  %fuzzy shadow={1pt}{-1pt}{0mm}{0.1mm}{gray},
  arc=1.5pt,
  rounded corners=north,
  sharp corners=south,
  top=6pt,          % Adjusted for ~40px equivalent height
  bottom=5pt,       % Adjusted for ~40px equivalent height
  overlay={
      \node [left,outer sep=0em, black,draw=none,anchor=west,
        rectangle,fill=none,inner sep=0pt]
        at ([xshift=3mm]frame.west) {\fbxIncludeIcon{#1}{4.2mm}};
    },
  },
  colframe=#1-color2,             % Border color (auto-generated from YAML)
  colbacktitle=#1-color1,         % Background color (auto-generated from YAML)
  colback=white,
  coltitle=black,
  titlerule=0mm,
  toprule=0.5pt,
  bottomrule=0.5pt,
  leftrule=2.2pt,
  rightrule=0.5pt,
  outer arc=1.5pt,
  arc=1.5pt,
  left=0.5em,       % increased left padding
  bottomtitle=1.5mm, % increased title bottom margin
  toptitle=1.5mm,    % increased title top margin
  title=\hspace{2.5em}\protect#2\hspace{0.5em}\protect#3, % Protect parameters
  boxsep=1pt,
  extras first={bottom=0pt},
  extras last={top=0pt,bottom=-4pt},
  overlay first={
    \draw[line width=1pt,white] ([xshift=2.2pt]frame.south west)-- ([xshift=-0.5pt]frame.south east);
  },
  overlay last={
    \draw[line width=1pt,white] ([xshift=2.2pt]frame.north west)-- ([xshift=-0.5pt]frame.north east);
   }
]}
{\end{tcolorbox}}

%%%% --- end foldboxy preamble ----- %%%%%
%%==== colors from yaml ===%
\definecolor{callout-definition-color1}{HTML}{F0F4F8}
\definecolor{callout-definition-color2}{HTML}{1B4F72}
\definecolor{callout-resource-slides-color1}{HTML}{E0F2F1}
\definecolor{callout-resource-slides-color2}{HTML}{20B2AA}
\definecolor{callout-quiz-question-color1}{HTML}{F0F0F8}
\definecolor{callout-quiz-question-color2}{HTML}{5B4B8A}
\definecolor{callout-example-color1}{HTML}{F0F8F6}
\definecolor{callout-example-color2}{HTML}{148F77}
\definecolor{callout-resource-videos-color1}{HTML}{E0F2F1}
\definecolor{callout-resource-videos-color2}{HTML}{20B2AA}
\definecolor{callout-resource-exercises-color1}{HTML}{E0F2F1}
\definecolor{callout-resource-exercises-color2}{HTML}{20B2AA}
\definecolor{callout-perspective-color1}{HTML}{F7F8FA}
\definecolor{callout-perspective-color2}{HTML}{4A5568}
\definecolor{callout-colab-color1}{HTML}{FFF5E6}
\definecolor{callout-colab-color2}{HTML}{FF6B35}
\definecolor{callout-checkpoint-color1}{HTML}{FFF8E7}
\definecolor{callout-checkpoint-color2}{HTML}{D97706}
\definecolor{callout-chapter-connection-color1}{HTML}{EFF6FF}
\definecolor{callout-chapter-connection-color2}{HTML}{1E3A5F}
\definecolor{callout-lighthouse-color1}{HTML}{FDF8E6}
\definecolor{callout-lighthouse-color2}{HTML}{B8860B}
\definecolor{callout-quiz-answer-color1}{HTML}{E8F2EA}
\definecolor{callout-quiz-answer-color2}{HTML}{4A7C59}
\definecolor{callout-principle-color1}{HTML}{F3F2FA}
\definecolor{callout-principle-color2}{HTML}{3D3B8E}
\definecolor{callout-theorem-color1}{HTML}{F5F0FA}
\definecolor{callout-theorem-color2}{HTML}{6B46C1}
\definecolor{callout-code-color1}{HTML}{F2F4F8}
\definecolor{callout-code-color2}{HTML}{D1D7E0}
\definecolor{callout-takeaways-color1}{HTML}{FDF2F7}
\definecolor{callout-takeaways-color2}{HTML}{BE185D}
\definecolor{callout-notebook-color1}{HTML}{F2F7FF}
\definecolor{callout-notebook-color2}{HTML}{2C5282}
%=============%

\usepackage{hyphenat}
\usepackage{ifthen}
\usepackage{calc}
\usepackage{calculator}



\usepackage{graphicx}
\usepackage{geometry}
\usepackage{afterpage}
\usepackage{tikz}
\usetikzlibrary{calc}
\usetikzlibrary{fadings}
\usepackage[pagecolor=none]{pagecolor}


% Set the titlepage font families







% Set the coverpage font families

\usepackage{bookmark}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  pdftitle={Introduction to Machine Learning Systems},
  pdfauthor={Vijay Janapa Reddi},
  colorlinks=true,
  linkcolor={Maroon},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}


\title{Introduction to Machine Learning Systems}
\author{Vijay Janapa Reddi}
\date{February 2, 2026}
\begin{document}
%%%%% begin titlepage extension code

  \begin{frontmatter}

\begin{titlepage}
% This is a combination of Pandoc templating and LaTeX
% Pandoc templating https://pandoc.org/MANUAL.html#templates
% See the README for help

\thispagestyle{empty}

\newgeometry{top=-100in}

% Page color

\newcommand{\coverauthorstyle}[1]{{\fontsize{20}{24.0}\selectfont
{#1}}}

\begin{tikzpicture}[remember picture, overlay, inner sep=0pt, outer sep=0pt]

\tikzfading[name=fadeout, inner color=transparent!0,outer color=transparent!100]
\tikzfading[name=fadein, inner color=transparent!100,outer color=transparent!0]
\node[anchor=south west, rotate=0, opacity=1] at ($(current page.south west)+(0.225\paperwidth, 9)$) {
\includegraphics[width=\paperwidth, keepaspectratio]{assets/images/covers/cover-image-transparent-vol1.png}};

% Title
\newcommand{\titlelocationleft}{0.075\paperwidth}
\newcommand{\titlelocationbottom}{0.4\paperwidth}
\newcommand{\titlealign}{left}

\begin{scope}{%
\fontsize{52}{62.4}\selectfont
\node[anchor=north
west, align=left, rotate=0] (Title1) at ($(current page.south west)+(\titlelocationleft,\titlelocationbottom)$)  [text width = 0.9\paperwidth]  {{\nohyphens{Machine
Learning Systems}}};
}
\end{scope}

% Author
\newcommand{\authorlocationleft}{.925\paperwidth}
\newcommand{\authorlocationbottom}{0.175\paperwidth}
\newcommand{\authoralign}{right}

\begin{scope}
{%
\fontsize{20}{24.0}\selectfont
\node[anchor=north
east, align=right, rotate=0] (Author1) at ($(current page.south west)+(\authorlocationleft,\authorlocationbottom)$)  [text width = 6in]  {\coverauthorstyle{Vijay
Janapa Reddi\\}};
}
\end{scope}

% Footer
\newcommand{\footerlocationleft}{0.075\paperwidth}
\newcommand{\footerlocationbottom}{0.475\paperwidth}
\newcommand{\footerlocationalign}{left}

\begin{scope}
{%
\fontsize{25}{30.0}\selectfont
 \node[anchor=north west, align=left, rotate=0] (Footer1) at %
($(current page.south west)+(\footerlocationleft,\footerlocationbottom)$)  [text width = 0.9\paperwidth]  {{\nohyphens{Introduction
to}}};
}
\end{scope}

\end{tikzpicture}
\clearpage
\restoregeometry
%%% TITLE PAGE START

% Set up alignment commands
%Page
\newcommand{\titlepagepagealign}{
\ifthenelse{\equal{left}{right}}{\raggedleft}{}
\ifthenelse{\equal{left}{center}}{\centering}{}
\ifthenelse{\equal{left}{left}}{\raggedright}{}
}


\newcommand{\titleandsubtitle}{
% Title and subtitle
{{\huge{\bfseries{\nohyphens{Introduction to Machine Learning
Systems}}}}\par
}%
}
\newcommand{\titlepagetitleblock}{
\titleandsubtitle
}

\newcommand{\authorstyle}[1]{{\large{#1}}}

\newcommand{\affiliationstyle}[1]{{\large{#1}}}

\newcommand{\titlepageauthorblock}{
{\authorstyle{\nohyphens{Vijay Janapa
Reddi}{\textsuperscript{1}}\textsuperscript{,}{\textsuperscript{,*}}}}}

\newcommand{\titlepageaffiliationblock}{
\hangindent=1em
\hangafter=1
{\affiliationstyle{
{1}.~Harvard University


\vspace{1\baselineskip}
* \textit{Correspondence:}~Vijay Janapa Reddi~vj@eecs.harvard.edu
}}
}
\newcommand{\headerstyled}{%
{}
}
\newcommand{\footerstyled}{%
{\large{}}
}
\newcommand{\datestyled}{%
{February 2, 2026}
}


\newcommand{\titlepageheaderblock}{\headerstyled}

\newcommand{\titlepagefooterblock}{
\footerstyled
}

\newcommand{\titlepagedateblock}{
\datestyled
}

%set up blocks so user can specify order
\newcommand{\titleblock}{{

{\titlepagetitleblock}
}

\vspace{4\baselineskip}
}

\newcommand{\authorblock}{{\titlepageauthorblock}

\vspace{2\baselineskip}
}

\newcommand{\affiliationblock}{{\titlepageaffiliationblock}

\vspace{0pt}
}

\newcommand{\logoblock}{}

\newcommand{\footerblock}{}

\newcommand{\dateblock}{{\titlepagedateblock}

\vspace{0pt}
}

\newcommand{\headerblock}{}

\thispagestyle{empty} % no page numbers on titlepages


\newcommand{\vrulecode}{\textcolor{black}{\rule{\vrulewidth}{\textheight}}}
\newlength{\vrulewidth}
\setlength{\vrulewidth}{2pt}
\newlength{\B}
\setlength{\B}{\ifdim\vrulewidth > 0pt 0.05\textwidth\else 0pt\fi}
\newlength{\minipagewidth}
\ifthenelse{\equal{left}{left} \OR \equal{left}{right} }
{% True case
\setlength{\minipagewidth}{\textwidth - \vrulewidth - \B - 0.1\textwidth}
}{
\setlength{\minipagewidth}{\textwidth - 2\vrulewidth - 2\B - 0.1\textwidth}
}
\ifthenelse{\equal{left}{left} \OR \equal{left}{leftright}}
{% True case
\raggedleft % needed for the minipage to work
\vrulecode
\hspace{\B}
}{%
\raggedright % else it is right only and width is not 0
}
% [position of box][box height][inner position]{width}
% [s] means stretch out vertically; assuming there is a vfill
\begin{minipage}[b][\textheight][s]{\minipagewidth}
\titlepagepagealign
\titleblock

Prof.~Vijay Janapa Reddi

School of Engineering and Applied Sciences

Harvard University

\vspace{80mm}

With heartfelt gratitude to the community for their invaluable
contributions and steadfast support.

\vfill

February 2, 2026

\vfill
\par

\end{minipage}\ifthenelse{\equal{left}{right} \OR \equal{left}{leftright} }{
\hspace{\B}
\vrulecode}{}
\clearpage
%%% TITLE PAGE END
\end{titlepage}
\setcounter{page}{1}
\end{frontmatter}

%%%%% end titlepage extension code

% =============================================================================
% HALF-TITLE PAGE (Volume I)
% =============================================================================
% Standard academic book sequence: half-title -> blank -> title page -> copyright
% The half-title shows only the book title -- no author, no publisher, no date.
\thispagestyle{empty}
\begin{center}
\vspace*{0.3\textheight}
{\fontsize{24pt}{28pt}\selectfont\bfseries\color{crimson} Introduction to\\[0.4em] Machine Learning Systems}\\[2em]
{\large\itshape Volume~I}
\vfill
\end{center}
\clearpage
\thispagestyle{empty}\null\clearpage  % Blank verso (back of half-title)

\renewcommand*\contentsname{Table of contents}
{
\hypersetup{linkcolor=}
\setcounter{tocdepth}{2}
\tableofcontents
}
\listoffigures
\listoftables

\mainmatter
\bookmarksetup{startatroot}

\chapter*{Welcome to Volume I}\label{welcome-to-volume-i}
\addcontentsline{toc}{chapter}{Welcome to Volume I}

\markboth{Welcome to Volume I}{Welcome to Volume I}

\bookmarksetup{startatroot}

\chapter{Benchmarking}\label{sec-benchmarking-ai}

\marginnote{\begin{footnotesize}

\emph{DALL·E 3 Prompt: Photo of a podium set against a tech-themed
backdrop. On each tier of the podium, there are AI chips with intricate
designs. The top chip has a gold medal hanging from it, the second one
has a silver medal, and the third has a bronze medal. Banners with `AI
Olympics' are displayed prominently in the background.}

\end{footnotesize}}

\noindent
\pandocbounded{\includegraphics[keepaspectratio]{contents/vol1/benchmarking/images/png/cover_ai_benchmarking.png}}

\section*{Purpose}\label{purpose}
\addcontentsline{toc}{section}{Purpose}

\markright{Purpose}

\emph{Why do benchmark results so often fail to predict production
performance?}

Compression techniques, hardware acceleration, and data selection
strategies all promise improvement---but \emph{how} do you know if they
actually delivered? Your team compresses a model, optimizes its kernels,
and selects hardware. The benchmarks look excellent: faster inference,
reduced memory, accuracy retained. You deploy to production and the
system fails during a traffic spike because the benchmark measured
throughput under ideal conditions while production runs with variable
inputs and latency constraints the benchmark never captured. This
scenario repeats across the industry because benchmarking is treated as
a checkbox rather than a discipline. A vision model achieves impressive
accuracy on standard datasets but drops dramatically on real-world
images with different characteristics. A vendor reports latency figures
that exclude preprocessing and queuing overhead. An edge device
advertises peak performance that thermal throttling reduces
significantly under sustained workloads. The gap between benchmark and
production is not measurement error but a \emph{fundamental mismatch
between controlled conditions and operational reality}---and
organizations that cannot distinguish between the two make deployment
decisions based on numbers that describe a system they will never
actually run.

\begin{tcolorbox}[enhanced jigsaw, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Learning Objectives}, rightrule=.15mm, breakable, left=2mm, arc=.35mm, toprule=.15mm, opacitybacktitle=0.6, colback=white, colframe=quarto-callout-tip-color-frame, bottomrule=.15mm, colbacktitle=quarto-callout-tip-color!10!white, opacityback=0, leftrule=.75mm, bottomtitle=1mm, toptitle=1mm, coltitle=black, titlerule=0mm]

\begin{itemize}
\tightlist
\item
  Explain how the three-dimensional benchmarking framework (algorithmic,
  system, data) addresses distinct ML evaluation requirements
\item
  Compare training and inference benchmarking approaches through their
  distinct metrics, workloads, and performance characteristics
\item
  Select appropriate benchmark granularity levels (micro, macro,
  end-to-end) based on optimization objectives and development phase
\item
  Apply MLPerf standards to evaluate ML systems across training,
  inference, and power dimensions
\item
  Design benchmark protocols with standardized datasets, metrics, and
  evaluation procedures that ensure reproducible results
\item
  Implement power measurement techniques that define system boundaries
  and enable standardized energy efficiency comparisons
\item
  Critique benchmark limitations including statistical issues,
  deployment gaps, and hardware-dependent performance variations
\end{itemize}

\end{tcolorbox}

\section{Machine Learning Benchmarking
Framework}\label{sec-benchmarking-ai-machine-learning-benchmarking-framework-70b8}

The optimization techniques from preceding chapters all claim
improvements. Data selection strategies (\textbf{?@sec-data-selection})
promise more efficient training. Model compression
(\textbf{?@sec-model-compression}) promises smaller, faster models.
Hardware acceleration (\textbf{?@sec-ai-acceleration}) promises higher
throughput. But \emph{how} do we know these claims hold in production? A
model quantized to INT8 may benchmark 2x faster on a synthetic workload
but show no improvement under real traffic patterns with variable input
sizes and concurrent requests. A pruned model may maintain accuracy on
the test set but fail on edge cases the benchmark never covered.
Benchmarking is the discipline of verifying that optimizations deliver
their promised benefits under realistic conditions.

ML benchmarking operates across three independent dimensions that map
directly to the components of any deployed system. \emph{System
benchmarking} asks: does the hardware deliver promised performance under
realistic workloads, or do memory bandwidth saturation and software
dispatch overhead erode the gains? \emph{Model benchmarking} asks: did
optimization techniques preserve model quality across the full input
distribution, not just on curated test sets? \emph{Data benchmarking}
asks: does the model generalize to real-world data with all its noise,
bias, and distributional shift? Each dimension can independently reveal
problems invisible to the others, and a system that passes all three
provides far stronger deployment confidence than one evaluated along any
single axis.

\phantomsection\label{callout-definitionux2a-1.1}
\begin{fbxSimple}{callout-definition}{Definition:}{Machine Learning Benchmarking}
\phantomsection\label{callout-definition*-1.1}
\textbf{\emph{Machine Learning Benchmarking}} is the empirical
measurement of system performance against \textbf{Representative
Workloads}. It exists to decouple \textbf{Peak Performance} (marketing
specs) from \textbf{Sustained Performance} (real-world capability),
quantifying the gap caused by system overheads like memory bandwidth
saturation and software dispatch latency.

\end{fbxSimple}

Unlike traditional systems where benchmarks represent fixed
specifications, ML benchmarks capture only a snapshot of a shifting
reality.

\phantomsection\label{callout-perspectiveux2a-1.2}
\begin{fbxSimple}{callout-perspective}{Systems Perspective:}{Benchmarks as Moving Targets}
\phantomsection\label{callout-perspective*-1.2}
\textbf{Why ML Benchmarking feels different from CPU Benchmarking:}

In traditional systems (e.g., SPEC CPU), the benchmark is a
\textbf{Rigid Specification}. A sorting algorithm is correct if it sorts
the list. The ``correctness'' is absolute and unchanging.

In ML systems, the benchmark is a \textbf{Soft Specification}. *
\textbf{The Test Set is the Spec}: Correctness is defined by a finite
set of examples (ImageNet). * \textbf{The World Moves}: A model that
scores 99\% on ImageNet (the benchmark) might fail completely on user
photos from 2025.

In H\&P architecture, you design for the benchmark because the benchmark
represents the workload. In AI Engineering, designing solely for the
benchmark is \emph{overfitting}. Robustness comes from acknowledging
that the benchmark is only a proxy for a shifting reality.

\end{fbxSimple}

To make this three-dimensional framework concrete, we ground it in a
running example that threads through the entire chapter. MobileNet
deployment validation spans all three evaluation dimensions,
illustrating \emph{how} each reveals problems the others cannot.

\phantomsection\label{callout-lighthouseux2a-1.3}
\begin{fbxSimple}{callout-lighthouse}{Lighthouse:}{MobileNet Deployment Validation}
\phantomsection\label{callout-lighthouse*-1.3}
Throughout this chapter, we validate the complete optimization pipeline
using \textbf{MobileNet} (introduced in
\textbf{?@sec-dnn-architectures-lighthouse-roster-model-biographies-a763})
as our lighthouse example. MobileNet exemplifies the deployment
challenges where benchmarking determines success or failure:

\textbf{The Optimization Pipeline}:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Model Compression} (\textbf{?@sec-model-compression}): INT8
  quantization reduces MobileNet from 17 MB to 4.3 MB (4x compression)
\item
  \textbf{Hardware Acceleration} (\textbf{?@sec-ai-acceleration}):
  EdgeTPU deployment achieves 2.1ms inference versus 15ms on CPU
\item
  \textbf{Benchmarking Validation} (this chapter): Verify the pipeline
  delivers in practice
\end{enumerate}

\textbf{Three-Dimensional Validation Questions}:

\begin{itemize}
\tightlist
\item
  \textbf{System}: Does EdgeTPU actually achieve 2.1ms, or do
  preprocessing and data transfer add 10ms of overhead?
\item
  \textbf{Model Quality}: Did INT8 quantization preserve accuracy? What
  about edge cases with unusual lighting?
\item
  \textbf{Data}: Does performance hold on real-world smartphone images,
  not just ImageNet test images?
\end{itemize}

Each section of this chapter addresses one dimension of this validation
stack. By the end, you will understand \emph{how} to answer these
questions systematically for any optimization pipeline.

\end{fbxSimple}

Before examining these dimensions in detail, we must establish the
mindset that separates rigorous evaluation from misleading metrics.
Three principles distinguish effective practitioners:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Benchmarks are proxies, not truth.} Every benchmark measures
  specific conditions that may not match your deployment. A system
  achieving 10,000 samples/second in Offline mode might achieve only 200
  QPS in Server mode with latency constraints. Ask: ``What does this
  benchmark NOT measure?''
\item
  \textbf{Goodhart's Law applies everywhere.} ``When a measure becomes a
  target, it ceases to be a good measure.'' Teams that optimize for
  benchmark rankings often produce systems that excel in evaluation but
  fail in production. Benchmark-specific optimizations frequently
  degrade characteristics (robustness, calibration, efficiency) that
  matter for deployment.
\item
  \textbf{End-to-end beats component metrics.} Vendors report component
  latency (5-10ms for model inference), but production latency includes
  preprocessing, queuing, and postprocessing (50-100ms total). A 3x
  inference speedup in isolation might yield only 1.3x end-to-end
  improvement, or worse if the optimization increases memory pressure.
\end{enumerate}

These principles reappear throughout this chapter and are examined in
depth in Section~\ref{sec-benchmarking-ai-fallacies-pitfalls-9781}.

We begin with the historical foundations of
benchmarking\sidenote{\textbf{Benchmark}: From surveying, where a
``bench mark'' was a horizontal cut in stone that served as a reference
point for measuring elevation. Surveyors would rest their leveling staff
on this ``bench'' to ensure consistent measurements. The term entered
computing in the 1970s to describe standardized reference points for
comparing system performance. The metaphor is apt: just as surveyors
need fixed reference points to measure terrain, engineers need
standardized workloads to compare systems. } to understand which lessons
from decades of computing measurement apply to ML, then examine each of
the three dimensions in depth, before showing how integrated
benchmarking brings them together. This structure reflects the
validation sequence practitioners follow: \emph{first verify} hardware
delivers promised performance, \emph{then verify} the model and data
optimizations built atop that hardware actually work.

\section{Historical
Context}\label{sec-benchmarking-ai-historical-context-7350}

Understanding \emph{why} ML benchmarking requires this three-dimensional
approach demands tracing \emph{how} measurement methodologies evolved,
and often failed, over decades of computing history. Each generation of
benchmarks emerged from the limitations of its predecessors, teaching
lessons that directly inform modern ML evaluation.

Because benchmarking intersects with metrics defined across several
chapters, the following note maps cross-chapter connections for
reference.

\begin{tcolorbox}[enhanced jigsaw, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Related Efficiency Metrics}, rightrule=.15mm, breakable, left=2mm, arc=.35mm, toprule=.15mm, opacitybacktitle=0.6, colback=white, colframe=quarto-callout-note-color-frame, bottomrule=.15mm, colbacktitle=quarto-callout-note-color!10!white, opacityback=0, leftrule=.75mm, bottomtitle=1mm, toptitle=1mm, coltitle=black, titlerule=0mm]

While this chapter focuses on system-level benchmarking, comprehensive
evaluation spans multiple dimensions covered elsewhere. For data
selection metrics (PPD, DUE), see \textbf{?@sec-data-selection}. For
model compression evaluation (Accuracy vs.~Compression), see
\textbf{?@sec-model-compression}. For hardware efficiency metrics
(Roofline, TOPS/Watt), see \textbf{?@sec-ai-acceleration}.

\end{tcolorbox}

The evolution from simple performance metrics to ML benchmarking reveals
three methodological shifts, each addressing failures of previous
evaluation paradigms that inform our current approach.

\subsection{Performance
Benchmarks}\label{sec-benchmarking-ai-performance-benchmarks-ea8a}

The evolution from synthetic operations to representative workloads
emerged when early benchmark gaming undermined evaluation validity.
Mainframe benchmarks like Whetstone (1976) and LINPACK (1979) measured
isolated operations, enabling vendors to optimize for narrow tests
rather than practical performance. SPEC CPU (1989) pioneered using real
application workloads to ensure evaluation reflects actual deployment
scenarios. This lesson directly shapes ML benchmarking: optimization
claims from \textbf{?@sec-model-compression} require validation on
representative tasks. \textbf{MLPerf}'s inclusion of real models like
ResNet-50 and BERT ensures benchmarks capture deployment complexity
rather than idealized test cases.

As deployment contexts diversified, benchmarks evolved from
single-dimension to multi-objective evaluation. Graphics benchmarks
measured quality alongside speed; mobile benchmarks evaluated battery
life with performance. The multi-objective challenges from
\textbf{?@sec-introduction}, balancing accuracy, latency, and energy,
manifest directly in modern ML evaluation where no single metric
captures deployment viability.

The shift from isolated components to integrated systems occurred when
distributed computing revealed that component optimization fails to
predict system performance. ML training depends on accelerator compute
(\textbf{?@sec-ai-acceleration}), data pipelines, gradient
synchronization, and storage throughput. MLPerf evaluates complete
workflows, recognizing that performance emerges from component
interactions.

DAWNBench (\citeproc{ref-coleman2017dawnbench}{Coleman et al. 2019})
emerged as an early ML benchmark that pioneered time-to-accuracy
evaluation, directly influencing MLPerf's methodology for measuring
training efficiency. These lessons culminate in MLPerf (2018), which
synthesizes representative workloads, multi-objective evaluation, and
integrated measurement while addressing ML-specific challenges
(\citeproc{ref-ranganathan2024twenty}{Ranganathan and Hölzle 2024}).

\subsection{Energy
Benchmarks}\label{sec-benchmarking-ai-energy-benchmarks-709a}

The multi-objective evaluation paradigm naturally extended to energy
efficiency as computing diversified beyond mainframes with unlimited
power budgets. Mobile devices demanded battery life optimization, while
warehouse-scale systems faced energy costs rivaling hardware expenses.
This shift established energy as a first-class metric alongside
performance, spawning benchmarks like SPEC Power\sidenote{\textbf{SPEC
Power}: Introduced in 2007 to address the growing importance of energy
efficiency in server design, SPEC Power measures performance per watt
across 11 load levels from idle (0\%) through 100\% in 10\% increments.
Results show that modern servers achieve 8-12 SPECpower\_ssj2008 scores
per watt, compared to 1-3 for systems from the mid-2000s, representing
approximately 3-4x efficiency improvement. } for servers,
Green500\sidenote{\textbf{Green500}: Started in 2007 as a counterpart to
the Top500 supercomputer list, Green500 ranks systems by FLOPS per watt
rather than raw performance. The most efficient systems achieve over 60
gigaFLOPS per watt compared to less than 1 gigaFLOPS/watt for early
2000s supercomputers, demonstrating improvements in computational
efficiency. } for supercomputers, and ENERGY
STAR\sidenote{\textbf{ENERGY STAR}: Launched by the EPA in 1992, this
voluntary program has prevented over 4 billion tons of greenhouse gas
emissions and saved consumers over \$500 billion on energy bills.
Computing equipment must meet strict efficiency requirements: ENERGY
STAR computers typically consume 30-65\% less energy than standard
models during operation and sleep modes. } for consumer systems.

Power benchmarking faces ongoing challenges in accounting for diverse
workload patterns and system configurations across computing
environments. The MLPerf Power
(\citeproc{ref-mlperf_power_website}{MLCommons 2024d}) benchmark
introduced specialized methodologies for measuring the energy impact of
machine learning workloads, reflecting the central role energy
efficiency now plays in AI system design.

Energy benchmarking extends beyond hardware power measurement to include
algorithmic efficiency. Model compression techniques (pruning,
quantization, knowledge distillation) often achieve greater energy
savings than hardware improvements alone: INT8 quantization typically
provides 4x inference speedup with 4x energy reduction
(\citeproc{ref-jacob2018quantization}{Jacob et al. 2018}), while pruning
can deliver 8-12x energy reduction (\citeproc{ref-han2016deep}{Han, Mao,
and Dally 2015}). MobileNet architectures achieve 10x energy reduction
versus ResNet through efficient design
(\citeproc{ref-howard2017mobilenets}{Howard et al. 2017}). These
techniques, detailed in \textbf{?@sec-model-compression}, establish that
energy-aware benchmarking must evaluate algorithmic efficiency alongside
hardware power consumption. As AI systems scale, this lesson becomes
increasingly critical for sustainable computing practices.

\subsection{Domain-Specific
Benchmarks}\label{sec-benchmarking-ai-domainspecific-benchmarks-b15f}

Computing diversification necessitated specialized benchmarks tailored
to domain-specific requirements that generic metrics cannot capture.
Domain-specific benchmarks address three categories of specialization:

Deployment constraints shape core metric priorities. Datacenter
workloads optimize for throughput with kilowatt-scale power budgets,
while mobile AI operates within 2-5W thermal envelopes, and IoT devices
require milliwatt-scale operation. These constraints, rooted in
efficiency principles from \textbf{?@sec-introduction}, determine
whether benchmarks prioritize total throughput or energy per operation.

Application requirements impose functional and regulatory constraints
beyond performance. Healthcare AI demands interpretability metrics
alongside accuracy; financial systems require microsecond latency with
audit compliance; autonomous vehicles need safety-critical reliability
(ASIL-D: \textless10\^{}-8 failure/hour). These requirements, connecting
to responsible AI principles, extend evaluation beyond traditional
performance metrics.

Operational conditions determine real-world viability. Autonomous
vehicles face -40°C to +85°C temperatures and degraded sensor inputs;
datacenters handle millions of concurrent requests with network
partitions; industrial IoT endures years-long deployment without
maintenance. Hardware capabilities from \textbf{?@sec-ai-acceleration}
only deliver value when validated under these conditions.

Machine learning presents a prominent example of this transition toward
domain-specific evaluation. Traditional CPU and GPU benchmarks prove
insufficient for assessing ML workloads, which involve complex
interactions between computation, memory bandwidth, and data movement
patterns. MLPerf has standardized performance measurement for machine
learning models across these three categories: MLPerf Training addresses
datacenter deployment constraints with multi-node scaling benchmarks,
MLPerf Inference evaluates latency-critical application requirements
across server to edge deployments, and MLPerf Tiny assesses
ultra-constrained operational conditions for microcontroller
deployments. This tiered structure, summarized in
Table~\ref{tbl-mlperf-suites}, reflects the systematic application of
our three-category framework to ML-specific evaluation needs.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1515}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1288}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3636}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3561}}@{}}
\caption{\textbf{MLPerf Benchmark Suite Variants.} Each variant
addresses a different deployment context, from datacenter-scale training
to ultra-constrained microcontroller inference, targeting specific
operational constraints and measuring metrics relevant to its deployment
scenario.}\label{tbl-mlperf-suites}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{MLPerf Variant}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Target Domain}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Key Constraints}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Primary Metrics}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{MLPerf Variant}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Target Domain}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Key Constraints}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Primary Metrics}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{MLPerf Training} & Datacenter & Multi-node scaling, high
bandwidth interconnects & Time-to-quality, throughput (samples/sec) \\
\textbf{MLPerf Inference} & Server / Edge & Latency SLAs, throughput
requirements & QPS, latency percentiles, accuracy preservation \\
\textbf{MLPerf Tiny} & MCU / IoT & Ultra-constrained (\textless1mW),
limited memory (\textless1MB) & Latency, accuracy, energy per
inference \\
\textbf{MLPerf Power} & Cross-cutting & Energy budgets, thermal
constraints & Performance/Watt, energy per query \\
\end{longtable}

Domain-specific benchmarks capture specialized requirements that general
benchmarks overlook. By systematically addressing deployment
constraints, application requirements, and operational conditions, these
benchmarks drive targeted optimizations in hardware and software while
ensuring that improvements translate to real-world deployment success
rather than narrow laboratory conditions.

This historical progression from general computing benchmarks through
energy-aware measurement to domain-specific evaluation frameworks
provides the foundation for understanding contemporary ML benchmarking
challenges. The lessons learned (representative workloads over synthetic
tests, multi-objective over single metrics, and integrated systems over
isolated components) directly shape how we approach AI system evaluation
today. Table~\ref{tbl-benchmark-evolution} summarizes this progression
and the key lessons each generation contributed.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.0814}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.0465}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2035}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2267}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.4419}}@{}}
\caption{\textbf{Benchmark Evolution.} Evolution of computing benchmarks
from synthetic operations to ML-specific evaluation. Each generation
addressed limitations of its predecessors, culminating in MLPerf's
synthesis of representative workloads, multi-objective metrics, and
integrated system
measurement.}\label{tbl-benchmark-evolution}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Benchmark}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Year}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Primary Focus}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Key Metric(s)}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Lesson for ML Benchmarking}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Benchmark}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Year}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Primary Focus}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Key Metric(s)}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Lesson for ML Benchmarking}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Whetstone} & 1976 & Synthetic floating-point operations & MWIPS
& Gaming synthetic tests undermines evaluation validity \\
\textbf{LINPACK} & 1979 & Linear algebra (matrix operations) & FLOPS &
Isolated operations miss system-level complexity and bottlenecks \\
\textbf{SPEC CPU} & 1989 & Real application workloads & SPECrate,
SPECspeed & Representative workloads reveal true deployment
performance \\
\textbf{SPEC Power} & 2007 & Server energy efficiency & ssj\_ops/Watt
across load levels & Energy efficiency requires multi-load evaluation,
not just peak performance \\
\textbf{Green500} & 2007 & HPC energy efficiency & GFLOPS/Watt &
Efficiency rankings complement raw performance rankings \\
\textbf{MLPerf} & 2018 & ML systems (training + inference) &
Time-to-quality, QPS, latency, accuracy & Synthesizes all lessons:
representative workloads + multi-objective + system \\
\end{longtable}

These lessons culminate in modern ML benchmarking suites, which we
examine next.

\section{System Benchmarking
Suites}\label{sec-benchmarking-ai-system-benchmarking-suites-e946}

The historical evolution from Whetstone to MLPerf reveals three lessons
that shape modern ML evaluation: representative workloads outperform
synthetic tests, multi-objective evaluation surpasses single metrics,
and integrated system measurement beats isolated component analysis.
Machine learning benchmarking must apply all three lessons while
confronting an entirely new challenge: inherent probabilistic
variability.

Unlike traditional workloads with deterministic behavior, ML systems
produce different outputs depending on training data, weight
initialization, and even operation ordering. A CPU benchmark produces
identical results given the same inputs. An ML model's performance
varies with these stochastic factors. This uncertainty fundamentally
changes \emph{what} benchmarks must measure and \emph{how} they must
report results, motivating the three-dimensional evaluation framework we
apply throughout this chapter.

Building on the framework and optimization techniques from previous
chapters, ML benchmarks must evaluate not only computational efficiency
but the interplay between algorithms, hardware, and data. The evolution
of benchmarks reaches its current apex in machine learning, where our
three-dimensional framework reflects decades of computing measurement
evolution. Early machine learning benchmarks focused primarily on
algorithmic performance, measuring how well models could perform
specific tasks (\citeproc{ref-lecun1998gradient}{Lecun et al. 1998}). As
machine learning applications scaled and computational demands grew, the
focus expanded to include system performance and hardware efficiency
(\citeproc{ref-jouppi2017datacenter}{Jouppi et al. 2017}). The role of
data quality then emerged as the third dimension of evaluation
(\citeproc{ref-gebru2021datasheets}{Gebru et al. 2021}).

AI benchmarks differ from traditional performance metrics through their
inherent variability, which introduces accuracy as a new evaluation
dimension alongside deterministic characteristics like computational
speed or energy consumption. The probabilistic nature of machine
learning models means the same system can produce different results
depending on the data it encounters, making accuracy a defining factor
in performance assessment. This distinction adds complexity:
benchmarking AI systems requires measuring not only raw computational
efficiency but also understanding trade-offs between accuracy,
generalization, and resource constraints.

Energy efficiency emerges as a cross-cutting concern that influences all
three dimensions of our framework: algorithmic choices affect
computational complexity and power requirements, hardware capabilities
determine energy-performance trade-offs, and dataset characteristics
influence training energy costs. This evaluation approach departs from
earlier benchmarks that focused on isolated aspects like computational
speed or energy efficiency
(\citeproc{ref-hernandez2020measuring}{Hernandez and Brown 2020}).

This evolution in benchmark complexity mirrors the field's evolving
understanding of \emph{what} drives machine learning system success.
Algorithmic innovations initially dominated progress metrics during the
research phase, but the practical challenges of deploying models at
scale revealed the importance of hardware efficiency
(\citeproc{ref-jouppi2021ten}{Jouppi et al. 2021}). High-profile
failures of machine learning systems in real-world deployments then
highlighted how data quality and representation directly determine
system reliability and fairness
(\citeproc{ref-bender2021stochastic}{Bender et al. 2021}). Understanding
\emph{how} these dimensions interact has become necessary for accurately
assessing machine learning system performance, informing development
decisions, and measuring technological progress.

\subsection{ML Measurement
Challenges}\label{sec-benchmarking-ai-ml-measurement-challenges-60ea}

The unique characteristics of ML systems create measurement challenges
that many traditional benchmarks were not designed for. Unlike
deterministic algorithms that produce identical outputs given the same
inputs, ML systems exhibit inherent variability from multiple sources:
algorithmic randomness from weight initialization and data shuffling,
hardware thermal states affecting clock speeds, system load variations
from concurrent processes, and environmental factors including network
conditions and power management. This variability requires rigorous
statistical methodology to distinguish genuine performance improvements
from measurement noise.

To address this variability, effective benchmark protocols require
multiple experimental runs with different random seeds. Running each
benchmark 5-10 times and reporting statistical measures beyond simple
means (including standard deviations or 95\% confidence intervals)
quantifies result stability and allows practitioners to distinguish
genuine performance improvements from measurement noise.

Empirical studies have shown how inadequate statistical rigor can lead
to misleading conclusions. Many reinforcement learning papers report
improvements that fall within statistical noise
(\citeproc{ref-henderson2018deep}{Henderson et al. 2018}), while GAN
comparisons often lack proper experimental protocols, leading to
inconsistent rankings across different random seeds
(\citeproc{ref-lucic2018gans}{Lucic et al. 2018}). These findings
underscore the importance of establishing measurement protocols that
account for ML's probabilistic nature.

Representative workload selection determines benchmark validity.
Synthetic microbenchmarks often fail to capture the complexity of real
ML workloads where data movement, memory allocation, and dynamic
batching create performance patterns not visible in simplified tests.
Comprehensive benchmarking therefore requires workloads that reflect
actual deployment patterns: variable sequence lengths in language
models, mixed precision training regimes, and realistic data loading
patterns that include preprocessing overhead.

Beyond workload representativeness, the distinction between statistical
significance and practical significance requires careful interpretation.
A small performance improvement might achieve statistical significance
across hundreds of trials but prove operationally irrelevant if it falls
within measurement noise or costs exceed benefits. This creates what we
call \emph{the statistical confidence trap}, where seemingly rigorous
evaluation still misleads.

\phantomsection\label{callout-notebookux2a-1.4}
\begin{fbxSimple}{callout-notebook}{AI Engineer’s Notebook:}{The Statistical Confidence Trap}
\phantomsection\label{callout-notebook*-1.4}
\textbf{Problem}: You are optimizing an image classifier that currently
has \textbf{95\% accuracy}. You deploy a ``compressed'' version and
measure its accuracy on a \textbf{1,000-image} test set. You get
\textbf{94\%}. Did your optimization cause a real regression, or is it
just noise?

\textbf{The Math}:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Expected Errors}: At 95\%, you expect 50 errors. At 94\%, you
  expect 60 errors.
\item
  \textbf{Standard Deviation (\(\sigma\))}: Using the binomial
  distribution \(\sqrt{N p (1-p)}\):

  \[ \sigma \approx \sqrt{1000 \times 0.05 \times 0.95} \approx \mathbf{6.9 \text{ images}} \]
\item
  \textbf{95\% Confidence Interval}:
  \(50 \pm 1.96 \times 6.9 \approx \mathbf{[36, 64]}\).
\end{enumerate}

\textbf{The Systems Conclusion}: Both 50 and 60 fall inside the
\textbf{same confidence interval}. A 1,000-sample test set
\textbf{cannot reliably detect} a 1\% accuracy drop. To distinguish a
1\% change with high confidence, you need \(\approx \mathbf{10,000}\)
samples.

\textbf{The Moral}: Small benchmarks are the ``Laboratory Fallacy.'' In
AI Engineering, your sensors (the test set) must be sized to match the
precision of the change you are trying to measure.

\end{fbxSimple}

Statistical validity is only part of the story. Even statistically
significant improvements can be misleading when they optimize the wrong
metric, a phenomenon known as \emph{Goodhart's Law in action}.

\phantomsection\label{callout-notebookux2a-1.5}
\begin{fbxSimple}{callout-notebook}{AI Engineer’s Notebook:}{Goodhart's Law in Action}
\phantomsection\label{callout-notebook*-1.5}
\textbf{The Metric Trap}: Optimizing for a single metric often degrades
others.

\textbf{Scenario}: You optimize a translation model for \textbf{BLEU
score}.

\begin{itemize}
\tightlist
\item
  \textbf{Original Model}: BLEU = 28.0, Inference = 50ms.
\item
  \textbf{Optimized Model}: BLEU = 28.5 (Better!), Inference = 200ms (4x
  slower).
\end{itemize}

\textbf{The Math}:

\begin{itemize}
\tightlist
\item
  The 0.5 BLEU gain comes from using a larger beam search (beam\_size=10
  vs beam\_size=1).
\item
  \textbf{Cost}: \(10 \times\) more candidate evaluations per step.
\item
  \textbf{Result}: You won the leaderboard but destroyed the product.
\end{itemize}

\textbf{The Systems Conclusion}: Always constrain your optimization.
Maximize Accuracy \emph{subject to} Latency \textless{} 100ms.

\end{fbxSimple}

Current benchmarking paradigms often fall short by measuring narrow task
performance while missing characteristics that determine real-world
system effectiveness. Most existing benchmarks evaluate supervised
learning performance on static datasets, primarily testing pattern
recognition capabilities rather than the adaptability and resilience
required for production deployment. This limitation becomes apparent
when models achieve excellent benchmark performance yet fail under
slightly different conditions or domains. Comprehensive system
evaluation must therefore measure learning efficiency, continual
learning capability, and out-of-distribution generalization alongside
traditional metrics.

These measurement challenges motivate a three-dimensional evaluation
framework:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Model benchmarks} validate whether optimization techniques
  from \textbf{?@sec-model-compression} preserved accuracy, calibration,
  and robustness. A pruned model might maintain top-1 accuracy while
  losing calibration; a quantized model might preserve average-case
  performance while degrading on edge cases.
\item
  \textbf{System benchmarks} validate whether the acceleration
  strategies from \textbf{?@sec-ai-acceleration} deliver their promised
  throughput, latency, and efficiency. Hardware specifications often
  describe theoretical peaks that real workloads never achieve.
\item
  \textbf{Data benchmarks} validate whether data curation strategies
  from \textbf{?@sec-data-selection} produced training sets that enable
  robust generalization. A model might achieve excellent accuracy on
  held-out test data while failing on production inputs with different
  characteristics.
\end{enumerate}

The bulk of this chapter focuses on \textbf{system benchmarking}
(training benchmarks, inference benchmarks, and power measurement)
because these form the foundation of standardized evaluation through
MLPerf. Model and data benchmarking require different methodologies and
are treated in detail in
Section~\ref{sec-benchmarking-ai-model-data-benchmarking-e0ca} after we
establish system evaluation foundations.

\subsection{System
Benchmarks}\label{sec-benchmarking-ai-system-benchmarks-393c}

System benchmarks measure the computational foundation that enables
model capabilities, examining how hardware architectures, memory
systems, and interconnects affect overall performance. This validation
is critical because hardware specifications often describe theoretical
peaks that real workloads never achieve. A GPU advertising 300 TFLOPS
might deliver only 30 TFLOPS on memory-bound transformer inference. This
discrepancy is so common it constitutes \emph{the fallacy of peak
performance}. System benchmarks reveal these gaps by running
standardized ML workloads rather than synthetic microbenchmarks.

\phantomsection\label{callout-perspectiveux2a-1.6}
\begin{fbxSimple}{callout-perspective}{Systems Perspective:}{The Fallacy of Peak Performance}
\phantomsection\label{callout-perspective*-1.6}
\textbf{Marketing vs.~Reality}: Computer architecture has a long history
of ``Peak Performance'' marketing---theoretical numbers that assume
perfect data reuse, zero memory latency, and optimal instruction
scheduling. In reality, real applications rarely achieve more than a
fraction of these peaks. Dave Patterson often refers to peak performance
as ``the performance the manufacturer guarantees you will not exceed.''
For ML systems, this gap is especially wide because of the
\textbf{Memory Wall}. A GPU might advertise 300 TFLOPS, but if your
model is memory-bound, you might only see 10 TFLOPS. Standardized
benchmarks like \textbf{MLPerf} are essential because they force systems
to run \emph{real} models on \emph{real} data, revealing the true
``sustained performance'' that engineers can actually rely on.

\end{fbxSimple}

Armed with this understanding, you can critically evaluate the benchmark
claims you encounter in vendor documentation and marketing materials.

\begin{tcolorbox}[enhanced jigsaw, title=\textcolor{quarto-callout-warning-color}{\faExclamationTriangle}\hspace{0.5em}{Decoding Vendor Benchmark Claims}, rightrule=.15mm, breakable, left=2mm, arc=.35mm, toprule=.15mm, opacitybacktitle=0.6, colback=white, colframe=quarto-callout-warning-color-frame, bottomrule=.15mm, colbacktitle=quarto-callout-warning-color!10!white, opacityback=0, leftrule=.75mm, bottomtitle=1mm, toptitle=1mm, coltitle=black, titlerule=0mm]

When evaluating hardware or software based on vendor-reported
benchmarks, ask these critical questions:

\textbf{What is measured?}

\begin{itemize}
\tightlist
\item
  ``10ms inference latency''---is this model-only, or including
  preprocessing/postprocessing?
\item
  ``1000 TOPS''---at what precision? INT4 TOPS are 4x INT8 TOPS on the
  same hardware
\item
  ``2x faster than competitor''---on which workload? What batch size?
  What precision?
\end{itemize}

\textbf{What is excluded?}

\begin{itemize}
\tightlist
\item
  Memory transfer time between CPU and accelerator
\item
  Model loading and initialization overhead
\item
  Thermal throttling under sustained workloads
\item
  Power consumption at the claimed performance level
\end{itemize}

\textbf{What conditions produced these results?}

\begin{itemize}
\tightlist
\item
  Batch size (larger batches inflate throughput numbers but increase
  latency)
\item
  Precision (FP32 vs.~FP16 vs.~INT8 vs.~INT4)
\item
  Model variant (smaller models benchmark faster but may not meet your
  accuracy needs)
\item
  Thermal state (fresh cold start vs.~sustained operation)
\end{itemize}

\textbf{Translation guide for common claims:}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 2\tabcolsep) * \real{0.3053}}
  >{\raggedright\arraybackslash}p{(\linewidth - 2\tabcolsep) * \real{0.6947}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Vendor Claim}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{What It Often Means}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{``Up to 10,000 images/sec''} & Peak throughput at maximum batch
size, INT8, without preprocessing \\
\textbf{``Sub-millisecond latency''} & Accelerator compute only,
excluding data transfer \\
\textbf{``5x more efficient''} & Per-operation efficiency, not total
system efficiency \\
\textbf{``Optimized for AI''} & May only accelerate specific operations
or precisions \\
\end{longtable}

\end{tcolorbox}

AI computations place significant demands on computational resources,
far exceeding traditional computing workloads. The underlying hardware
infrastructure (general-purpose CPUs, graphics processing units (GPUs),
tensor processing units (TPUs)\sidenote{\textbf{Tensor Processing Unit
(TPU)}: Google's custom ASIC designed specifically for neural network
workloads, first deployed secretly in 2015 and announced in 2016. The
first-generation TPU achieved 15-30x better performance per watt than
contemporary GPUs for inference, while a single TPU v4 pod (4,096 chips)
delivers up to 1.1 exaFLOPS of peak BF16 performance, demonstrating the
capabilities of specialized AI hardware. }, and application-specific
integrated circuits (ASICs)\sidenote{\textbf{Application-Specific
Integrated Circuit (ASIC)}: Custom chips designed for specific
computational tasks, offering superior performance and energy efficiency
compared to general-purpose processors. AI ASICs like Google's TPUs,
Tesla's FSD chips, and Bitcoin mining ASICs can achieve 100-1000x better
efficiency than CPUs for their target applications, but lack the
flexibility for other workloads. }) determines the speed, efficiency,
and scalability of AI solutions. System benchmarks establish
standardized methodologies for evaluating hardware performance across AI
workloads, measuring metrics including computational throughput, memory
bandwidth, power efficiency, and scaling characteristics
(\citeproc{ref-reddi2020mlperf}{Reddi et al. 2019};
\citeproc{ref-mattson2020mlperf}{Mattson et al. 2020}).

These system benchmarks perform two critical functions in the AI
ecosystem. First, they enable developers and organizations to make
informed decisions when selecting hardware platforms for their AI
applications by providing comparative performance data across system
configurations. Evaluation factors include training speed, inference
latency, energy efficiency, and cost-effectiveness. Second, hardware
manufacturers rely on these benchmarks to quantify generational
improvements and guide the development of specialized AI accelerators,
driving advancement in computational capabilities.

\phantomsection\label{callout-definitionux2a-1.7}
\begin{fbxSimple}{callout-definition}{Definition:}{Machine Learning System Benchmarks}
\phantomsection\label{callout-definition*-1.7}
\textbf{\emph{ML System Benchmarks}} standardize the measurement of
\textbf{Infrastructure Efficiency}. By fixing the \textbf{Workload}
(model + dataset) and \textbf{Quality Target}, they isolate the
contribution of hardware and software stacks to \textbf{Throughput} and
\textbf{Latency}, effectively measuring the system's ability to execute
the Silicon Contract.

\end{fbxSimple}

Effective benchmark interpretation requires understanding the
performance characteristics of target hardware. Understanding whether
specific AI workloads are compute-bound or memory-bound provides
essential insight for optimization decisions. Computational intensity,
measured as FLOPS\sidenote{\textbf{FLOPS}: Floating-Point Operations Per
Second, a measure of computational performance indicating how many
floating-point calculations a processor can execute in one second.
Modern AI accelerators achieve high FLOPS ratings: NVIDIA A100 delivers
312 TFLOPS (trillion FLOPS) for FP16/BF16 Tensor Core operations (624
TFLOPS with structured sparsity), while high-end CPUs achieve 1-10
TFLOPS. FLOPS measurements help compare hardware capabilities and
determine computational bottlenecks in ML workloads. } per byte of data
movement, determines performance limits. Consider an NVIDIA A100 GPU
with 312 TFLOPS of tensor performance (TF32/FP16 with Tensor Cores; FP32
is 19.5 TFLOPS) and 2.0 TB/s memory bandwidth (SXM variant), yielding an
arithmetic intensity threshold of 153 FLOPS/byte. The architectural
foundations for understanding these hardware characteristics, including
the \textbf{roofline model} for analyzing compute-bound versus
memory-bound workloads, are established in
\textbf{?@sec-ai-acceleration}, which provides context for interpreting
system benchmark results.

High-intensity operations like dense matrix multiplication in certain AI
model operations (typically \textgreater150 FLOPS/byte) achieve
near-peak computational throughput on the A100. For example, a ResNet-50
forward pass on large batch sizes (256+) achieves arithmetic intensity
of \textasciitilde300 FLOPS/byte, enabling 85-90\% of peak tensor
performance (approximately 280 TFLOPS achieved vs 312 TFLOPS
theoretical) (\citeproc{ref-nvidia2020a100}{Choquette et al. 2021}).
Conversely, low-intensity operations like activation functions and
certain lightweight operations (\textless10 FLOPS/byte) become memory
bandwidth limited, utilizing only a fraction of the GPU's computational
capacity. A BERT inference with batch size 1 achieves only 8 FLOPS/byte
arithmetic intensity, limiting performance to 16 TFLOPS (2.0 TB/s × 8
FLOPS/byte), representing just 5\% of peak computational capability.

This quantitative analysis, formalized in roofline
models\sidenote{\textbf{Roofline Model}: Named for its visual appearance
resembling a house roofline, this performance model was developed by
Samuel Williams at UC Berkeley in 2009. The plot reveals whether
workloads are compute-bound (sloped region, limited by peak FLOPS) or
memory-bound (flat region, limited by bandwidth). The ``ridge point''
where the slope meets the flat line marks the critical arithmetic
intensity threshold. The architectural metaphor is apt: performance
cannot exceed the roofline, just as you cannot build above a roof. },
provides a systematic framework that guides both algorithm design and
hardware selection by clearly identifying the dominant performance
constraints for specific workloads. Understanding these quantitative
relationships allows engineers to predict performance bottlenecks
accurately and optimize both model architectures and deployment
strategies accordingly. For instance, increasing batch size from 1 to 32
for transformer inference can shift operations from memory-bound (8
FLOPS/byte) to compute-bound (150 FLOPS/byte), improving GPU utilization
from 4\% to 65\% (\citeproc{ref-pope2022efficiently}{Pope et al. 2022}).

The following worked example applies \emph{roofline analysis for BERT
inference} to demonstrate how these principles translate into concrete
deployment predictions.

\phantomsection\label{callout-notebookux2a-1.8}
\begin{fbxSimple}{callout-notebook}{AI Engineer’s Notebook:}{Roofline Analysis for BERT Inference}
\phantomsection\label{callout-notebook*-1.8}
\textbf{Problem}: You need to deploy BERT-Base for inference on an A100
GPU. Management expects high GPU utilization. What performance should
you predict, and how can you improve it?

\subsubsection*{Step 1: Hardware Limits}\label{step-1-hardware-limits}
\addcontentsline{toc}{subsubsection}{Step 1: Hardware Limits}

\begin{itemize}
\tightlist
\item
  Peak compute: 312 TFLOPS (Tensor Core, TF32/FP16)
\item
  Memory bandwidth: 2.0 TB/s
\item
  Ridge point: 312 ÷ 2.0 = 153 FLOPS/byte
\end{itemize}

Any workload with arithmetic intensity below 153 FLOPS/byte is
memory-bound; above is compute-bound.

\subsubsection*{Step 2: BERT-Base
Characteristics}\label{step-2-bert-base-characteristics}
\addcontentsline{toc}{subsubsection}{Step 2: BERT-Base Characteristics}

\begin{itemize}
\tightlist
\item
  Parameters: 110M = 440 MB (FP32)
\item
  FLOPs per inference: \textasciitilde22 billion (forward pass with
  sequence length \(S=128\))
\item
  Data movement: \textasciitilde440 MB (must load all weights from
  memory)
\item
  Arithmetic intensity: 22 × 10⁹ ÷ 440 × 10⁶ = 50 FLOPS/byte
\end{itemize}

\subsubsection*{Step 3: Performance
Prediction}\label{step-3-performance-prediction}
\addcontentsline{toc}{subsubsection}{Step 3: Performance Prediction}

Since 50 \textless{} 153, BERT at batch=1 is \textbf{memory-bound}:

\[\text{Achievable perf}\] = 50 FLOPS/byte × 2.0 TB/s = \textbf{100
TFLOPS}

\[\text{GPU utilization}\] = 100 ÷ 312 = \textbf{32\%}

\subsubsection*{Step 4: Optimization via
Batching}\label{step-4-optimization-via-batching}
\addcontentsline{toc}{subsubsection}{Step 4: Optimization via Batching}

Increase batch size to 32:

\begin{itemize}
\tightlist
\item
  Same 440 MB of weights, but 32× more compute
\item
  New FLOPs: 22 × 10⁹ × 32 = 704 × 10⁹
\item
  New intensity: 704 × 10⁹ ÷ 440 × 10⁶ = 1600 FLOPS/byte
\end{itemize}

Since 1600 \textgreater{} 153, batch=32 is \textbf{compute-bound}:

\[\text{Achievable perf}\] ≈ 0.85 × 312 = \textbf{265 TFLOPS}

\[\text{GPU utilization} \approx 85\%\]

\textbf{The Systems Insight}: Batch size transforms memory-bound
inference (32\% utilization) into compute-bound inference (85\%
utilization). But batching increases latency because you must wait to
accumulate requests. This is the fundamental throughput-latency tradeoff
that MLPerf scenarios capture: SingleStream (batch=1, latency-optimized)
versus Offline (maximum batch, throughput-optimized).

\end{fbxSimple}

System benchmarks evaluate performance across scales, ranging from
single-chip configurations to large distributed systems, and AI
workloads including both training and inference tasks. This evaluation
approach ensures that benchmarks accurately reflect real-world
deployment scenarios and deliver insights that inform both hardware
selection decisions and system architecture design.
Figure~\ref{fig-imagenet-gpus} reveals the striking correlation between
GPU adoption and ImageNet classification error rates from 2010 to 2014:
as GPU entries surged from 0 to 110, error rates plummeted from 28.2\%
to 7.3\%, demonstrating how hardware capabilities and algorithmic
advances drive progress in tandem.

\begin{figure}[htb]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/15d47d45dfe68677a60a31768aa63be502158f89.pdf}}

}

\caption{\label{fig-imagenet-gpus}\textbf{GPU Adoption and Error
Reduction}: As GPU entries in ImageNet surged from 0 to 110 between 2010
and 2014, top-5 error rates dropped from 28.2\% to 7.3\%, demonstrating
the co-evolution of hardware capabilities and algorithmic advances.}

\end{figure}%

The ImageNet example demonstrates \emph{how} hardware advances enable
algorithmic breakthroughs, but effective system benchmarking requires
understanding the relationship between workload characteristics and
hardware utilization. Modern AI systems rarely achieve theoretical peak
performance due to interactions between computational patterns, memory
hierarchies, and system architectures. This gap between theoretical and
achieved performance shapes how we design meaningful system benchmarks.

Realistic hardware utilization patterns are essential for actionable
benchmark design. Different AI workloads interact with hardware
architectures in different ways, creating utilization patterns that vary
based on model architecture, batch size, and precision choices. GPU
utilization varies from 85\% for well-optimized ResNet-50 training with
batch size 64 to only 15\% with batch size 1
(\citeproc{ref-you2019scaling}{You et al. 2019}) due to insufficient
parallelism. Memory bandwidth utilization ranges from 20\% for
parameter-heavy transformer models to 90\% for activation-heavy
convolutional networks, directly impacting achievable performance across
different precision levels.

Energy efficiency adds another dimension to system benchmarking.
Performance per watt varies by three orders of magnitude across
computing platforms, making energy efficiency a key benchmark dimension
for production deployments. Utilization significantly impacts
efficiency: underutilized GPUs consume disproportionate power while
delivering minimal performance, creating substantial efficiency
penalties that affect operational costs and environmental impact.

Distributed system performance introduces additional complexity beyond
single-machine evaluation. Multi-node training involves communication
bottlenecks, network topology effects, and coordination overhead that
single-node benchmarks cannot capture. For deployments spanning multiple
machines, specialized distributed benchmarking methodologies, including
scaling efficiency measurement and network performance profiling, become
essential. These distributed benchmarking approaches are critical for
scaling ML systems across multiple machines but require dedicated
treatment beyond the scope of single-node evaluation.

Within the single-machine scope of this book, multi-GPU benchmarking
focuses on intra-node communication patterns, memory bandwidth
utilization across accelerators, and the efficiency of gradient
synchronization within shared-memory systems. Modern workstations with
4-8 GPUs connected via NVLink or PCIe provide substantial parallelism
while avoiding the network communication challenges that characterize
multi-node deployments.

These hardware utilization insights directly inform benchmark design
principles. Effective system benchmarks must evaluate performance across
realistic utilization scenarios rather than focusing solely on peak
theoretical capabilities. This approach ensures that benchmark results
translate to practical deployment guidance, enabling engineers to make
informed decisions about hardware selection, system configuration, and
optimization strategies.

\subsection{Community-Driven
Standardization}\label{sec-benchmarking-ai-communitydriven-standardization-5c56}

Evaluating systems across the three dimensions (model quality, system
performance, and data representativeness) demands consistent measurement
standards that individual organizations cannot establish alone. When one
team measures inference latency with preprocessing included and another
excludes it, when accuracy benchmarks use different data splits, or when
power measurements employ different system boundaries, meaningful
comparison becomes impossible. The proliferation of benchmarks across
our three dimensions creates fragmentation that only community-driven
standardization can resolve. While early computing benchmarks primarily
measured simple metrics like processor speed and memory bandwidth,
modern benchmarks must evaluate sophisticated aspects of system
performance, from complex power consumption profiles to highly
specialized application-specific capabilities. This evolution in scope
and complexity necessitates comprehensive validation and consensus from
the computing community, particularly in rapidly evolving fields like
machine learning.

The lasting impact of any benchmark depends critically on its acceptance
by the broader research community, where technical excellence alone is
insufficient for adoption. Benchmarks developed without broad community
input often fail to gain meaningful traction, frequently missing
critical metrics that leading research groups consider essential.
Successful benchmarks emerge through collaborative development involving
academic institutions, industry partners, and domain experts. This
inclusive approach ensures benchmarks evaluate capabilities most crucial
for advancing the field, while balancing theoretical and practical
considerations.

In contrast, benchmarks developed through extensive collaboration among
respected institutions carry the authority necessary to drive widespread
adoption, while those perceived as advancing particular corporate
interests face skepticism and limited acceptance. The remarkable success
of ImageNet demonstrates how sustained community engagement through
workshops and challenges establishes long-term viability and lasting
impact. This community-driven development creates a foundation for
formal standardization, where organizations like IEEE and ISO transform
these benchmarks into official standards.

The standardization process provides crucial infrastructure for
benchmark formalization and adoption. IEEE working groups
(\citeproc{ref-ieee_working_groups}{Sharman et al. 2022}) transform
community-developed benchmarking methodologies into formal industry
standards, establishing precise specifications for measurement and
reporting. The IEEE 2416-2019 (\citeproc{ref-ieee_2416_2019}{Peyghami
and Blaabjerg 2020}) standard for system power modeling exemplifies this
process, codifying best practices developed through community consensus.
Similarly, ISO/IEC technical committees (\citeproc{ref-iso_tc}{ISO
2024}) develop international standards for benchmark validation and
certification, ensuring consistent evaluation across global research and
industry communities. These organizations bridge the gap between
community-driven innovation and formal standardization, providing
frameworks that enable reliable comparison of results across different
institutions and geographic regions.

Successful community benchmarks establish clear governance structures
for managing their evolution. Through rigorous version control systems
and detailed change documentation, benchmarks maintain backward
compatibility while incorporating new advances. This governance includes
formal processes for proposing, reviewing, and implementing changes,
ensuring that benchmarks remain relevant while maintaining stability.
Modern benchmarks increasingly emphasize reproducibility requirements,
incorporating automated verification systems and standardized evaluation
environments.

Open access accelerates benchmark adoption and ensures consistent
implementation. Projects that provide open-source reference
implementations, comprehensive documentation, validation suites, and
containerized evaluation environments reduce barriers to entry. This
standardization enables research groups to evaluate solutions using
uniform methods and metrics. Without coordinated implementation
frameworks, organizations might interpret benchmarks inconsistently,
compromising result reproducibility and meaningful comparison across
studies.

The most successful benchmarks strike a careful balance between academic
rigor and industry practicality. Academic involvement ensures
theoretical soundness and comprehensive evaluation methodology, while
industry participation grounds benchmarks in practical constraints and
real-world applications. This balance proves particularly crucial in
machine learning benchmarks, where theoretical advances must translate
to practical improvements in deployed systems
(\citeproc{ref-patterson2021carbon}{Patterson et al. 2021}). These
evaluation methodology principles guide both training and inference
benchmark design throughout this chapter.

Community consensus establishes enduring benchmark relevance, while
fragmentation impedes scientific progress. Through collaborative
development and transparent operation, benchmarks evolve into
authoritative standards for measuring advancement. The most successful
benchmarks in energy efficiency and domain-specific applications share
this foundation of community development and governance, demonstrating
how collective expertise and shared purpose create lasting impact in
rapidly advancing fields.

While community standards ensure reproducibility, they do not prescribe
the level of detail at which measurements should be taken. The choice of
granularity, from individual operations to complete systems,
fundamentally shapes what insights benchmarks can provide.

\section{Benchmarking
Granularity}\label{sec-benchmarking-ai-benchmarking-granularity-3855}

Community-driven standardization establishes common measurement
protocols, but a second design decision remains: at what level of detail
should evaluation occur? Standardization answers ``how do we measure
consistently?'' while granularity answers ``what exactly do we
measure?'' Each validation dimension can be assessed at different
scales, from individual operations to complete workflows, with each
granularity level revealing different kinds of problems:

\begin{itemize}
\tightlist
\item
  \textbf{Micro benchmarks} isolate individual components: kernel
  execution time, memory bandwidth utilization, single-layer accuracy.
  These diagnose \emph{where} problems occur.
\item
  \textbf{Macro benchmarks} evaluate subsystems: full model training
  convergence, inference pipeline throughput, dataset bias metrics.
  These reveal \emph{what} problems exist.
\item
  \textbf{End-to-end benchmarks} measure complete workflows:
  request-to-response latency including preprocessing, training
  time-to-accuracy including data loading, model performance on
  production data distributions. These show \emph{whether} the system
  works.
\end{itemize}

The optimization techniques from Part III operate at different
granularities (kernel fusion targets micro performance, pruning affects
macro model behavior, data curation determines end-to-end
generalization) and validation must match. A micro benchmark might show
kernel speedup while a macro benchmark reveals memory bottlenecks that
negate the gain; an end-to-end benchmark might expose data pipeline
stalls invisible at any other level.

Figure~\ref{fig-granularity} breaks down the ML system stack into three
distinct evaluation layers, each revealing different performance
characteristics. At the application level, end-to-end benchmarks assess
overall system performance including data preprocessing, model training,
and inference. At the model layer, benchmarks evaluate efficiency and
accuracy, measuring how well models generalize to new data and their
computational efficiency during training and inference. At the
infrastructure layer, benchmarking examines individual hardware and
software components like GPUs or TPUs.

\begin{figure}[htb]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/c63e86e787092d32c132f1c1818203440ef96499.pdf}}

}

\caption{\label{fig-granularity}\textbf{Benchmarking Granularity}:
Four-panel block diagram showing micro, model, application, and
end-to-end evaluation layers. Each panel maps a distinct scope of
assessment, from isolated kernel operations through full-system
deployment, enabling targeted optimization at every level of the ML
stack.}

\end{figure}%

\textbf{Micro Benchmarks.} While end-to-end benchmarks reveal overall
system behavior, optimization requires pinpointing exactly which
operations consume time and energy. Micro-benchmarks serve this
diagnostic purpose by isolating individual tensor operations, the
mathematical primitives whose hardware optimization we examined in
\textbf{?@sec-ai-acceleration}.

Consider debugging a slow inference pipeline: macro benchmarks might
show unacceptable latency, but only micro-benchmarks reveal whether the
bottleneck lies in convolutions, attention mechanisms, or memory copies.
This diagnostic precision makes micro-benchmarks essential for the
targeted optimization that transforms theoretical hardware capabilities
into realized performance gains. These benchmarks isolate individual
tasks to provide detailed insights into the computational demands of
particular system elements, from neural network layers to optimization
techniques to activation functions.

A key area of micro-benchmarking focuses on tensor
operations\sidenote{\textbf{Tensor Operations}: From Latin ``tendere''
(to stretch), tensors were introduced by mathematicians Ricci-Curbastro
and Levi-Civita in 1900 to describe quantities that transform
predictably under coordinate changes. In ML, ``tensor'' refers to
multi-dimensional arrays: vectors (1D), matrices (2D), and higher-order
tensors. Operations like GEMM and convolution form the computational
backbone of neural networks, with modern accelerators achieving
order-of-magnitude gains for optimized tensor primitives. }, which are
the computational core of deep learning. Libraries like cuDNN
(\citeproc{ref-chetlur2014cudnn}{Chetlur et al.
2014})\sidenote{\textbf{cuDNN}: CUDA Deep Neural Network library,
NVIDIA's GPU-accelerated library of primitives for deep neural networks.
Released in 2014, cuDNN provides highly optimized implementations for
convolutions, pooling, normalization, and activation layers, often
yielding multi-fold speedups over naive implementations and becoming a
widely used standard for GPU-accelerated deep learning. } by NVIDIA
provide benchmarks for measuring fundamental computations such as
convolutions and matrix multiplications across different hardware
configurations. These measurements help developers understand how their
hardware handles the core mathematical operations that dominate ML
workloads.

But measuring these operations correctly requires discipline. The
following \emph{micro-benchmarking rules} prevent common measurement
errors that can invalidate results entirely.

\phantomsection\label{callout-perspectiveux2a-1.9}
\begin{fbxSimple}{callout-perspective}{Systems Perspective:}{Micro-Benchmarking Rules}
\phantomsection\label{callout-perspective*-1.9}

To avoid measuring hardware artifacts instead of kernel performance,
follow the \textbf{Systems Detective's Rules}:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{The Warm-up Rule}: Never measure the first 10-50 iterations.
  Modern hardware uses \textbf{DVFS (Dynamic Voltage and Frequency
  Scaling)} and \textbf{Turbo Boost}. A ``cold'' GPU may take 100ms to
  ramp from 300MHz to 1.5GHz. Your first batch will appear 5x slower
  than reality.
\item
  \textbf{The Variance Rule}: Report the \textbf{Coefficient of
  Variation (CV)} (\(CV = \sigma / \mu\)). If \(CV > 0.05\) (5\%), your
  measurement is noisy. This usually indicates background OS jitter,
  thermal throttling, or memory contention.
\item
  \textbf{The ``Speed of Light'' (SOL) Check}: Compare your achieved
  throughput against the \textbf{Roofline}. If your kernel achieves 10
  TFLOPS on an H100 (peak \textasciitilde989 TFLOPS for FP8 dense, or
  \textasciitilde4,000 TFLOPS FP8 with sparsity), don't just optimize
  the code---ask \emph{why} the utilization is so low. Is it a
  \textbf{Kernel Launch Latency} issue (too many small kernels)?
\item
  \textbf{The Flush Rule}: When measuring memory bandwidth, ensure you
  flush the L2 cache between runs, or your ``bandwidth'' will reflect
  cache speed (\textasciitilde5-10 TB/s) rather than DRAM speed
  (\textasciitilde1-2 TB/s).
\end{enumerate}

\end{fbxSimple}

With these measurement principles established, we can now examine how to
diagnose specific bottlenecks using the Iron Law framework.

\phantomsection\label{callout-notebookux2a-1.10}
\begin{fbxSimple}{callout-notebook}{AI Engineer’s Notebook:}{Measuring the Iron Law Terms}
\phantomsection\label{callout-notebook*-1.10}

\textbf{From Theory to Trace}: How to map the Iron Law equation to a
profiler timeline (like Nsight Systems or PyTorch Profiler).

\textbf{1. Measuring the Data Term (\(\frac{D_{vol}}{BW}\))}

\begin{itemize}
\tightlist
\item
  \textbf{Signal:} Look for the \textbf{``Memory Throughput''} or
  \textbf{``DRAM Bandwidth''} line.
\item
  \textbf{Calculation:}
  \(\text{Effective BW} = \frac{\text{Total Bytes Transferred}}{\text{Kernel Duration}}\).
\item
  \textbf{Diagnosis:} If \(\text{Effective BW} \approx \text{Peak BW}\)
  (e.g., \textgreater1.6 TB/s on A100), your kernel is \textbf{Memory
  Bound}. Optimizing compute (Ops) will do nothing.
\end{itemize}

\textbf{2. Measuring the Throughput Term (\(Util\))}

\begin{itemize}
\tightlist
\item
  \textbf{Signal:} Look for \textbf{``SM Active''} or \textbf{``Compute
  Throughput''}.
\item
  \textbf{Calculation:}
  \(\text{Achieved TFLOPS} = \frac{\text{FLOP Count}}{\text{Kernel Duration}}\).
\item
  \textbf{Diagnosis:} If
  \(\text{Achieved TFLOPS} \ll \text{Peak TFLOPS}\) AND
  \(\text{Memory BW} \ll \text{Peak BW}\), you are in the
  \textbf{``Utilization Trap''}---likely Latency Bound (kernels too
  small) or Grid Bound (not enough threads).
\end{itemize}

\textbf{3. Measuring the Latency Term (\(Latency_{fixed}\))}

\begin{itemize}
\tightlist
\item
  \textbf{Signal:} Look for \textbf{Gaps} (empty space) between colored
  kernel bars on the timeline.
\item
  \textbf{Calculation:}
  \(\text{Overhead Ratio} = \frac{\text{Gap Duration}}{\text{Kernel Duration} + \text{Gap Duration}}\).
\item
  \textbf{Diagnosis:} A ``Sawtooth'' pattern (Compute, Gap, Compute,
  Gap) indicates high software overhead. You need \textbf{Operator
  Fusion} (Ch 7) or \textbf{CUDA Graphs} to remove the gaps.
\end{itemize}

\end{fbxSimple}

While benchmarks like MLPerf tell you \emph{how fast} a system is,
micro-benchmarking tools tell you \emph{why} it is slow. To perform this
diagnosis, engineers use kernel-level profilers that peer inside the
execution of individual operations.

\textbf{1. Framework Profilers (e.g., PyTorch Profiler)} These tools
capture the ``logical'' execution flow. They answer:

\begin{itemize}
\tightlist
\item
  ``Which layer is taking the most time?''
\item
  ``Are my CPU and GPU synchronized or overlapped?''
\item
  ``Is the data loader keeping up?''
\item
  \emph{Key Metric}: \textbf{Step Time Breakdown} (Data Loading
  vs.~Compute vs.~Communication).
\end{itemize}

\textbf{2. Kernel Profilers (e.g., NVIDIA Nsight Systems / Compute)}
These tools capture the ``physical'' execution on the hardware. They
answer:

\begin{itemize}
\tightlist
\item
  ``Is this matrix multiplication Compute-Bound or Memory-Bound?''
\item
  ``Are we hitting 100\% occupancy on the Streaming Multiprocessors?''
\item
  ``Are memory coalescing rules being respected?''
\item
  \emph{Key Metric}: \textbf{Roofline Analysis} (FLOPS vs.~Memory
  Bandwidth).
\end{itemize}

\textbf{The Workflow}: Start with the Framework Profiler to find the
slow layer (e.g., ``The Attention Block is slow''). Then, use the Kernel
Profiler to diagnose the physics (e.g., ``The Softmax kernel is
memory-bound because it's reading too many bytes per FLOP''). This
targeted approach avoids the ``optimization without measurement'' trap.

Micro-benchmarks also examine activation functions and neural network
layers in isolation. This includes measuring the performance of various
activation functions like ReLU, Sigmoid\sidenote{\textbf{Sigmoid}: From
Greek ``sigma'' (the S-shaped letter σ), this activation function S(x) =
1/(1+e\^{}(-x)) maps real numbers to (0,1), producing the characteristic
S-curve that gave it its name. Introduced to neural networks in the
1980s, sigmoid dominated early deep learning but fell from favor due to
vanishing gradients and expensive exponential operations. It persists in
output layers for binary classification and as gates in LSTM cells. },
and Tanh\sidenote{\textbf{Tanh}: Hyperbolic tangent, borrowing the
``tangent'' concept from trigonometry but applied to hyperbolas rather
than circles. The function tanh(x) = (e\^{}x -
e\textsuperscript{(-x))/(e}x + e\^{}(-x)) maps inputs to (-1, 1),
providing zero-centered outputs that often yield stronger gradients than
sigmoid in hidden layers. ``Hyperbolic'' functions were introduced by
Vincenzo Riccati in the 1760s. } under controlled conditions, and
evaluating the computational efficiency of distinct neural network
components such as LSTM\sidenote{\textbf{LSTM (Long Short-Term Memory)}:
A type of recurrent neural network architecture introduced by Hochreiter
and Schmidhuber in 1997, designed to solve the vanishing gradient
problem in traditional RNNs. LSTMs use gates (forget, input, output) to
control information flow, enabling them to learn dependencies over
hundreds of time steps, making them crucial for sequence modeling before
the Transformer era. } cells or Transformer blocks when processing
standardized inputs.

DeepBench (\citeproc{ref-deepbench_github}{Alosco et al. 2021}),
developed by Baidu, was one of the first to demonstrate the value of
comprehensive micro-benchmarking. It evaluates these fundamental
operations across different hardware platforms, providing detailed
performance data that helps developers optimize their deep learning
implementations. By isolating and measuring individual operations,
DeepBench enables precise comparison of hardware platforms and
identification of potential performance bottlenecks.

\textbf{Macro Benchmarks.} While micro-benchmarks examine individual
operations like tensor computations and layer performance, macro
benchmarks evaluate complete machine learning models. This shift from
component-level to model-level assessment provides insights into how
architectural choices and component interactions affect overall model
behavior. For instance, while micro-benchmarks might show optimal
performance for individual convolutional layers, macro-benchmarks reveal
how these layers work together within a complete convolutional neural
network.

Macro-benchmarks measure multiple performance dimensions that emerge
only at the model level. These include prediction accuracy, which shows
how well the model generalizes to new data; memory consumption patterns
across different batch sizes and sequence lengths; throughput under
varying computational loads; and latency across different hardware
configurations. Understanding these metrics helps developers make
informed decisions about model architecture, optimization strategies,
and deployment configurations.

The assessment of complete models occurs under standardized conditions
using established datasets and tasks. For example, computer vision
models might be evaluated on ImageNet
(\citeproc{ref-imagenet_website}{Blaivas and Blaivas 2020}), measuring
both computational efficiency and prediction accuracy. Natural language
processing models might be assessed on translation tasks, examining how
they balance quality and speed across different language pairs.

Several industry-standard benchmarks enable consistent model evaluation
across platforms. MLPerf Inference
(\citeproc{ref-mlperf_inference_website}{MLCommons 2024b}) provides
comprehensive testing suites adapted for different computational
environments (\citeproc{ref-reddi2020mlperf}{Reddi et al. 2019}). MLPerf
Mobile (\citeproc{ref-mlperf_mobile_website}{MLCommons 2024c}) focuses
on mobile device constraints (\citeproc{ref-janapa2022mlperf}{Janapa
Reddi et al. 2022}), while MLPerf Tiny
(\citeproc{ref-mlperf_tiny_website}{MLCommons 2024e}) addresses
microcontroller deployments (\citeproc{ref-banbury2021mlperf}{Banbury et
al. 2021}). For embedded systems, EEMBC's MLMark emphasizes both
performance and power efficiency. The AI-Benchmark
(\citeproc{ref-ai_benchmark_website}{Ignatov and Timofte 2024}) suite
specializes in mobile platforms, evaluating models across diverse tasks
from image recognition to face parsing.

\textbf{End-to-End Benchmarks.} End-to-end benchmarks provide an
inclusive evaluation that extends beyond the boundaries of the ML model
itself. Rather than focusing solely on a machine learning model's
computational efficiency or accuracy, these benchmarks encompass the
entire pipeline of an AI system. This includes initial ETL
(Extract-Transform-Load) or ELT (Extract-Load-Transform) data
processing, the core model's performance, post-processing of results,
and critical infrastructure components like storage and network systems.

Data processing is the foundation of all AI systems, transforming raw
data into a format suitable for model training or inference. In ETL
pipelines, data undergoes extraction from source systems, transformation
through cleaning and feature engineering, and loading into model-ready
formats. These preprocessing steps' efficiency, scalability, and
accuracy significantly impact overall system performance. End-to-end
benchmarks must assess standardized datasets through these pipelines to
ensure data preparation doesn't become a bottleneck.

The post-processing phase also plays an important role, involving the
interpretation of raw model outputs, conversion of scores into
meaningful categories, filtering of results based on predefined tasks,
or integration with other systems. For instance, a computer vision
system might need to post-process detection boundaries, apply confidence
thresholds, and format results for downstream applications. In
real-world deployments, this phase delivers actionable insights.

Beyond core AI operations, infrastructure components heavily influence
overall performance and user experience. Storage solutions, whether
cloud-based, on-premises, or hybrid, can significantly impact data
retrieval and storage times, especially with vast AI datasets. Network
interactions, vital for distributed systems, can become performance
bottlenecks if not optimized. End-to-end benchmarks must evaluate these
components under specified environmental conditions to ensure
reproducible measurements of the entire system.

To date, there are no public, end-to-end benchmarks that fully account
for data storage, network, and compute performance. While MLPerf
Training and Inference approach end-to-end evaluation, they primarily
focus on model performance rather than real-world deployment scenarios.
Nonetheless, they provide valuable baseline metrics for assessing AI
system capabilities.

Given the inherent specificity of end-to-end benchmarking, organizations
typically perform these evaluations internally by instrumenting
production deployments. This allows engineers to develop result
interpretation guidelines based on realistic workloads, but given the
sensitivity and specificity of the information, these benchmarks rarely
appear in public settings.

\textbf{Granularity Trade-offs and Selection Criteria.}
Table~\ref{tbl-benchmark-comparison} reveals how different challenges
emerge at different stages of an AI system's lifecycle. Each
benchmarking approach provides unique insights: micro-benchmarks help
engineers optimize specific components like GPU kernel implementations
or data loading operations, macro-benchmarks guide model architecture
decisions and algorithm selection, while end-to-end benchmarks reveal
system-level bottlenecks in production environments.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.0833}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3167}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3000}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3000}}@{}}
\caption{\textbf{Benchmarking Granularity Levels.} Different benchmark
scopes target distinct stages of ML system development. Micro-benchmarks
isolate individual operations for low-level optimization,
macro-benchmarks evaluate complete models to guide architectural
choices, and end-to-end benchmarks assess full system performance in
production environments.}\label{tbl-benchmark-comparison}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Component}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Micro Benchmarks}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Macro Benchmarks}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{End-to-End Benchmarks}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Component}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Micro Benchmarks}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Macro Benchmarks}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{End-to-End Benchmarks}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Focus} & Individual operations & Complete models & Full system
pipeline \\
\textbf{Scope} & Tensor ops, layers, activations & Model architecture,
training, inference & ETL, model, infrastructure \\
\textbf{Example} & Conv layer performance on cuDNN & ResNet-50 on
ImageNet & Production recommendation system \\
\textbf{Advantages} & Precise bottleneck identification, Component
optimization & Model architecture comparison, Standardized evaluation &
Realistic performance assessment, System-wide insights \\
\textbf{Challenges} & May miss interaction effects & Limited
infrastructure insights & Complex to standardize, Often proprietary \\
\textbf{Typical Use} & Hardware selection, Operation optimization &
Model selection, Research comparison & Production system evaluation \\
\end{longtable}

Figure~\ref{fig-benchmark-tradeoffs} visualizes the core trade-off
between diagnostic power and real-world representativeness across
benchmark granularity levels. This relationship illustrates why
comprehensive ML system evaluation requires multiple benchmark types:
micro-benchmarks provide precise optimization guidance for isolated
components, while end-to-end benchmarks capture the complex interactions
that emerge in production systems. The optimal benchmarking strategy
combines insights from all three levels to balance detailed component
analysis with realistic system-wide assessment.

\begin{figure}[htb]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/63e02bb8820c97fc0aa4c12354a0c6445d16f0da.pdf}}

}

\caption{\label{fig-benchmark-tradeoffs}\textbf{Isolation
vs.~Representativeness}: The core trade-off in benchmarking granularity.
Micro-benchmarks provide high diagnostic precision but limited
real-world relevance, while end-to-end benchmarks capture realistic
system behavior but offer less precise component-level insights.
Effective ML system evaluation requires strategic combination of all
three levels.}

\end{figure}%

Component interaction often produces unexpected behaviors that
single-level benchmarks miss. While micro-benchmarks might show
excellent performance for individual operations and macro-benchmarks
might demonstrate strong model accuracy, end-to-end evaluation can
reveal that data preprocessing creates unexpected bottlenecks during
high-traffic periods. These system-level insights remain hidden when
components undergo isolated testing, motivating systematic approaches
that connect granularity choices to concrete implementation decisions.

Having established that benchmarks operate at multiple granularities, we
now examine the fundamental components that every benchmark, regardless
of granularity, must specify. These components define what is measured,
how measurements are taken, and how results are reported.

\section{Benchmark
Components}\label{sec-benchmarking-ai-benchmark-components-97cc}

The granularity level established above directly shapes how benchmark
components are instantiated. Micro-benchmarks measuring tensor
operations require synthetic inputs that isolate specific computational
patterns, enabling precise performance characterization of individual
kernels as discussed in \textbf{?@sec-ai-acceleration}. Macro-benchmarks
evaluating complete models demand representative datasets like ImageNet
that capture realistic task complexity while enabling standardized
comparison across architectures. End-to-end benchmarks assessing
production systems must incorporate real-world data characteristics
including distribution shift, noise, and edge cases absent from curated
evaluation sets. Similarly, evaluation metrics shift focus across
granularity levels: micro-benchmarks emphasize FLOPS and memory
bandwidth utilization, macro-benchmarks balance accuracy and inference
speed, while end-to-end benchmarks prioritize system reliability and
operational efficiency under load. Understanding this systematic
variation ensures that component choices align with evaluation
objectives rather than applying uniform approaches across different
benchmarking scales.

Having established how benchmark granularity shapes evaluation scope
(from micro-benchmarks isolating tensor operations to end-to-end
assessments of complete systems), we now examine how these conceptual
levels translate into concrete benchmark implementations. The components
discussed abstractly above must be instantiated through specific choices
about tasks, datasets, models, and metrics. This implementation process
follows a systematic workflow that ensures reproducible and meaningful
evaluation regardless of the chosen granularity level.

An AI benchmark provides this structured framework for evaluating
artificial intelligence systems. While individual benchmarks vary
significantly in their specific focus and granularity, they share common
implementation components that enable consistent evaluation and
comparison across different approaches.

The essential components interconnect to form a complete evaluation
pipeline. Figure~\ref{fig-benchmark-components} illustrates how task
definition, dataset selection, model selection, and evaluation metrics
build upon each other, creating a progression from problem specification
through deployment assessment.

\begin{figure}[htb]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/d0d11425754987b979b3574c7786d7988ad08cf5.pdf}}

}

\caption{\label{fig-benchmark-components}\textbf{Anomaly Detection
Pipeline}: Nine-stage benchmark workflow applied to an industrial audio
anomaly detection task. The pipeline progresses from problem definition
through dataset selection, model training, quantization, and ARM
embedded deployment, illustrating how each benchmark component feeds the
next.}

\end{figure}%

Effective benchmark design must account for the optimization techniques
established in preceding chapters. Quantization and pruning affect model
accuracy-efficiency trade-offs, requiring benchmarks that measure both
speedup and accuracy preservation simultaneously. Hardware acceleration
techniques influence arithmetic intensity and memory bandwidth
utilization, necessitating roofline model analysis to interpret results
correctly. Understanding these optimization foundations enables
benchmark selection that validates claimed improvements rather than
measuring artificial scenarios.

\subsection{Problem
Definition}\label{sec-benchmarking-ai-problem-definition-79e4}

A benchmark implementation begins with a formal specification of the
machine learning task and its evaluation criteria. In machine learning,
tasks represent well-defined problems that AI systems must solve.
Consider the anomaly detection system depicted in
Figure~\ref{fig-benchmark-components}, which processes audio signals to
identify deviations from normal operation patterns. This industrial
monitoring application exemplifies how formal task specifications
translate into practical implementations.

The formal definition of any benchmark task encompasses both the
computational problem and its evaluation framework. While the specific
tasks vary significantly by domain, well-established categories have
emerged across major fields of AI research. Natural language processing
tasks, for example, include machine translation, question answering
(\citeproc{ref-hirschberg2015advances}{Hirschberg and Manning 2015}),
and text classification. Computer vision similarly employs standardized
tasks such as object detection, image segmentation, and facial
recognition (\citeproc{ref-everingham2010pascal}{Everingham et al.
2009}).

Every benchmark task specification must define three essential elements.
The input specification determines what data the system processes; in
the anomaly detection example, this means audio waveform data. The
output specification describes the required system response, such as the
binary classification of normal versus anomalous patterns. The
performance specification establishes quantitative requirements for
accuracy, processing speed, and resource utilization.

Task design directly impacts the benchmark's ability to evaluate AI
systems effectively. The audio anomaly detection example clearly
illustrates this relationship through its specific requirements:
processing continuous signal data, adapting to varying noise conditions,
and operating within strict time constraints. These practical
constraints create a detailed framework for assessing model performance,
ensuring evaluations reflect real-world operational demands.

The implementation of a benchmark proceeds systematically from this
foundational task definition. Each subsequent phase, from dataset
selection through deployment, builds directly upon these initial
specifications, ensuring that evaluations maintain consistency while
addressing the defined requirements across different approaches and
implementations.

\subsection{Standardized
Datasets}\label{sec-benchmarking-ai-standardized-datasets-123f}

Building directly upon the problem definition established in the
previous phase, standardized datasets provide the essential foundation
for training and evaluating models. These carefully curated collections
ensure all models undergo testing under identical conditions, enabling
direct comparisons across different approaches and architectures. In
Figure~\ref{fig-benchmark-components}, the audio anomaly detection
example demonstrates how waveform data serves as the standardized input
for evaluating detection performance.

In computer vision, datasets such as ImageNet
(\citeproc{ref-imagenet_website}{Blaivas and Blaivas 2020})
(\citeproc{ref-deng2009imagenet}{Deng et al. 2009}), COCO
(\citeproc{ref-coco_website}{Lin et al. 2014a})
(\citeproc{ref-lin2014microsoft}{Lin et al. 2014b}), and CIFAR-10
(\citeproc{ref-cifar10_website}{LeCun, Xiao, and Krizhevsky
2025})\sidenote{\textbf{CIFAR-10}: A dataset of 60,000 32×32 color
images across 10 classes (airplane, automobile, bird, cat, deer, dog,
frog, horse, ship, truck), collected by Alex Krizhevsky, Vinod Nair, and
Geoffrey Hinton at the University of Toronto in 2009. Despite its small
image size, CIFAR-10 became fundamental for comparing deep learning
architectures, with top-1 error rates improving from 18.5\% with
traditional methods to 2.6\% with modern deep networks. }
(\citeproc{ref-krizhevsky2009learning}{Krizhevsky, Hinton, et al. 2009})
serve as reference standards. For natural language processing,
collections such as SQuAD (\citeproc{ref-squad_website}{Rajpurkar et al.
2016a})\sidenote{\textbf{SQuAD}: Stanford Question Answering Dataset,
introduced in 2016, containing 100,000+ question-answer pairs based on
Wikipedia articles. SQuAD became the gold standard for evaluating
reading comprehension, with human performance at 86.8\% F1 score and
leading AI systems achieving over 90\% by 2018, marking the first time
machines exceeded human performance on this benchmark. }
(\citeproc{ref-rajpurkar2016squad}{Rajpurkar et al. 2016b}), GLUE
(\citeproc{ref-glue_website}{Wang et al. 2018a})\sidenote{\textbf{GLUE}:
General Language Understanding Evaluation, a collection of nine English
sentence understanding tasks including sentiment analysis, textual
entailment, and similarity. Introduced in 2018, GLUE provided
standardized evaluation with a human baseline of 87.1\% and became
obsolete when BERT achieved 80.2\% in 2019, leading to the more
challenging SuperGLUE benchmark. } (\citeproc{ref-wang2018glue}{Wang et
al. 2018b}), and WikiText (\citeproc{ref-wikitext_website}{Merity 2016})
(\citeproc{ref-merity2016pointer}{Merity et al. 2016}) fulfill similar
functions. These datasets encompass a range of complexities and edge
cases to thoroughly evaluate machine learning systems.

As Figure~\ref{fig-benchmark-components} illustrates in its workflow
progression, strategic dataset selection shapes all subsequent
implementation steps and ultimately determines the benchmark's
effectiveness. In the audio anomaly detection example, the dataset must
include representative waveform samples of normal operation alongside
comprehensive examples of various anomalous conditions. Notable examples
include datasets like ToyADMOS for industrial manufacturing anomalies
and Google Speech Commands for general sound recognition. Regardless of
the specific dataset chosen, the data volume must suffice for both model
training and validation, while incorporating real-world signal
characteristics and noise patterns that reflect deployment conditions.

The selection of benchmark datasets directly shapes experimental
outcomes and model evaluation. Effective datasets must balance two key
requirements: accurately representing real-world challenges while
maintaining sufficient complexity to differentiate model performance
meaningfully. While research often utilizes simplified datasets like
ToyADMOS\sidenote{\textbf{ToyADMOS}: A dataset for anomaly detection in
machine operating sounds, developed by NTT Communications in 2019
containing audio recordings from toy car and toy conveyor belt
operations. The dataset includes 1,000+ normal samples and 300+
anomalous samples per machine type, designed to standardize acoustic
anomaly detection research with reproducible experimental conditions. }
(\citeproc{ref-koizumi2019toyadmos}{Koizumi et al. 2019}), these
controlled environments, though valuable for methodological development,
may not fully capture real-world deployment complexities.

\subsection{Model
Selection}\label{sec-benchmarking-ai-model-selection-01e6}

Following dataset specification, the benchmark process advances
systematically to model architecture selection and implementation. This
critical phase establishes performance baselines and determines the
optimal modeling approach for the specific task at hand. The selection
process directly builds upon the architectural foundations established
in \textbf{?@sec-dnn-architectures} and must account for the
framework-specific considerations discussed in
\textbf{?@sec-ai-frameworks}. Examine the model selection stage in
Figure~\ref{fig-benchmark-components} to see how this phase connects
dataset specification to training code development.

Baseline models serve as the reference points for evaluating novel
approaches. These span from basic implementations, including linear
regression for continuous predictions and logistic regression for
classification tasks, to advanced architectures with proven success in
comparable domains. The choice of baseline depends critically on the
deployment framework: a PyTorch implementation may exhibit different
performance characteristics than its TensorFlow equivalent due to
framework-specific optimizations and operator implementations. In
natural language processing applications, advanced language models like
BERT\sidenote{\textbf{BERT}: Bidirectional Encoder Representations from
Transformers, introduced by Google in 2018, revolutionized natural
language processing by pre-training on vast text corpora using masked
language modeling. BERT-Large contains 340 million parameters and
achieved state-of-the-art results on 11 NLP tasks, establishing the
foundation for modern language models like GPT and ChatGPT. } have
emerged as standard benchmarks for comparative analysis. The
architectural details of transformers and their performance
characteristics are thoroughly covered in
\textbf{?@sec-dnn-architectures}.

Selecting the right baseline model requires careful evaluation of
architectures against benchmark requirements. This selection process
directly informs the development of training code, which is the
cornerstone of benchmark reproducibility. The training implementation
must thoroughly document all aspects of the model pipeline, from data
preprocessing through training procedures, enabling precise replication
of model behavior across research teams.

With model architecture selected, model development follows two primary
optimization paths: training and inference. During training
optimization, efforts concentrate on achieving target accuracy metrics
while operating within computational constraints. The training
implementation must demonstrate consistent achievement of performance
thresholds under specified conditions.

In parallel, the inference optimization path addresses deployment
considerations, particularly the transition from development to
production environments. A key example involves precision reduction
through numerical optimization techniques, progressing from
high-precision to lower-precision representations to improve deployment
efficiency. This process demands careful calibration to maintain model
accuracy while reducing resource requirements. The benchmark must detail
both the quantization methodology and verification procedures that
confirm preserved performance.

The intersection of these two optimization paths with real-world
constraints shapes overall deployment strategy. Comprehensive benchmarks
must therefore specify requirements for both training and inference
scenarios, ensuring models maintain consistent performance from
development through deployment. This crucial connection between
development and production metrics naturally leads to the establishment
of evaluation criteria.

The optimization process must balance four key objectives: model
accuracy, computational speed, memory utilization, and energy
efficiency. Following our three-dimensional benchmarking framework, this
optimization landscape requires evaluation metrics that quantify
performance across algorithmic, system, and data dimensions. As models
transition from development to deployment, these metrics serve as
critical tools for guiding optimization decisions and validating
performance enhancements.

\subsection{Evaluation
Metrics}\label{sec-benchmarking-ai-evaluation-metrics-6bac}

Building upon the optimization framework established through model
selection, evaluation metrics\sidenote{\textbf{Metric}: From Greek
``metron'' (measure), the root of ``meter,'' ``geometry,'' and
``symmetry.'' In mathematics, a metric is any function that defines
distance between elements. ML borrowed this term broadly to mean any
quantitative measure of model behavior. The Greek heritage reminds us
that meaningful measurement requires both a standard unit and a
consistent method, principles that benchmarking formalizes. These
metrics establish objective standards for comparing different
approaches, allowing researchers and practitioners to gauge solution
effectiveness. } provide the quantitative measures needed to assess
machine learning model performance.

Understanding the landscape of ML benchmarking requires organizing
metrics into a coherent taxonomy.
\{\#sec-benchmarking-ai-metric-taxonomy-d4cd\}
Table~\ref{tbl-metric-taxonomy} categorizes metrics by what each
measures and when it should be applied:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1628}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3256}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2558}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2558}}@{}}
\caption{\textbf{ML Benchmarking Metric Taxonomy.} Metrics organized by
category, showing the appropriate unit and primary use case for
each.}\label{tbl-metric-taxonomy}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Category}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Metric}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Unit}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Primary Use Case}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Category}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Metric}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Unit}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Primary Use Case}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Accuracy} & Top-1/Top-5 Accuracy & Percentage &
Classification \\
& mAP (mean Average Precision) & 0-1 score & Object detection \\
& BLEU/ROUGE & 0-100 score & NLP generation \\
& Perplexity & Score (lower = better) & Language modeling \\
\textbf{Throughput} & Samples/second & Samples/s & Batch inference \\
& Tokens/second & Tokens/s & LLM inference \\
& Time-to-train & Hours/days & Training benchmarks \\
\textbf{Latency} & p50 latency & Milliseconds & Median response time \\
& p99 latency & Milliseconds & Tail latency (SLA) \\
& First-token latency & Milliseconds & LLM responsiveness \\
\textbf{Efficiency} & Samples/second/watt & Samples/s/W & Energy
efficiency \\
& Accuracy/FLOP & \%/PFLOP & Algorithmic efficiency \\
& TCO per inference & \$/inference & Economic efficiency \\
\end{longtable}

\textbf{Key distinctions}:

\begin{itemize}
\tightlist
\item
  \textbf{Throughput vs.~Latency}: \textbf{Throughput} measures
  aggregate capacity (ideal for batch processing), while
  \textbf{latency} measures individual request timing (critical for
  interactive applications). These metrics can conflict; maximizing
  throughput often increases latency through batching.
\item
  \textbf{Mean vs.~Percentile Latency}: Mean latency can hide
  problematic tail behavior. A system with 10ms mean latency might have
  500ms p99 latency, failing SLA requirements. In production, you should
  report percentiles (p50, p95, p99) in addition to means.
\item
  \textbf{Single vs.~Compound Metrics}: Compound metrics like
  samples/second/watt combine multiple dimensions but can obscure
  individual bottlenecks. Report both atomic metrics and relevant
  compound metrics.
\end{itemize}

The selection of appropriate metrics represents a critical aspect of
benchmark design, as they must align with task objectives while
providing meaningful insights into model behavior across both training
and deployment scenarios. Importantly, metric computation can vary
between frameworks. The training methodologies from
\textbf{?@sec-ai-training} demonstrate how different frameworks handle
loss computation and gradient accumulation differently, affecting
reported metrics.

Task-specific metrics quantify a model's performance on its intended
function. For example, classification tasks employ metrics including
accuracy (overall correct predictions), precision (positive prediction
accuracy), recall (positive case detection rate), and F1 score
(precision-recall harmonic mean)
(\citeproc{ref-sokolova2009systematic}{Sokolova and Lapalme 2009}).
Regression problems utilize error measurements like Mean Squared Error
(MSE) and Mean Absolute Error (MAE) to assess prediction accuracy.
Domain-specific applications often require specialized metrics; for
example, machine translation uses the BLEU score\sidenote{\textbf{BLEU
Score}: Bilingual Evaluation Understudy, introduced by IBM in 2002,
measures machine translation quality by comparing n-gram overlap between
machine and human reference translations. BLEU scores range from 0-100,
with scores above 30 considered useful, above 50 good, and above 60 high
quality. Google Translate achieved BLEU scores of 40+ on major language
pairs by 2016. } to evaluate the semantic and syntactic similarity
between machine-generated and human reference translations
(\citeproc{ref-papineni2002bleu}{Papineni et al. 2001}).

However, as models transition from research to production deployment,
implementation metrics become equally important. Model size, measured in
parameters or memory footprint, directly affects deployment feasibility
across different hardware platforms. Processing latency, typically
measured in milliseconds per inference, determines whether the model
meets real-time requirements. Energy consumption, measured in watts or
joules per inference, indicates operational efficiency. These practical
considerations reflect the growing need for solutions that balance
accuracy with computational efficiency. The operational challenges of
maintaining these metrics in production environments are explored in
deployment strategies
(\textbf{?@sec-machine-learning-operations-mlops}).

Consequently, the selection of appropriate metrics requires careful
consideration of both task requirements and deployment constraints. A
single metric rarely captures all relevant aspects of performance in
real-world scenarios. For instance, in anomaly detection systems, high
accuracy alone may not indicate good performance if the model generates
frequent false alarms. Similarly, a fast model with poor accuracy fails
to provide practical value.

This multi-metric evaluation approach appears in our anomaly detection
system, which reports performance across multiple dimensions: model size
(270 Kparameters), processing speed (10.4 ms/inference), and detection
accuracy (0.86 AUC\sidenote{\textbf{AUC (Area Under the Curve)}: A
performance metric for binary classification that measures the area
under the Receiver Operating Characteristic (ROC) curve, representing
the trade-off between true positive and false positive rates. AUC values
range from 0 to 1, where 0.5 indicates random performance, 0.7-0.8 is
acceptable, 0.8-0.9 is excellent, and above 0.9 is outstanding
discrimination ability. }). This combination of metrics ensures the
model meets both technical and operational requirements in real-world
deployment scenarios.

\subsection{Benchmark
Harness}\label{sec-benchmarking-ai-benchmark-harness-09ea}

While evaluation metrics provide the measurement framework, a benchmark
harness implements the systematic infrastructure for evaluating model
performance under controlled conditions. This critical component ensures
reproducible testing by managing how inputs are delivered to the system
under test and how measurements are collected, effectively transforming
theoretical metrics into quantifiable measurements.

The harness design should align with the intended deployment scenario
and usage patterns. For server deployments, the harness implements
request patterns that simulate real-world traffic, typically generating
inputs using a Poisson distribution\sidenote{\textbf{Poisson
Distribution}: Named after French mathematician Siméon Denis Poisson,
who derived it in 1837 while studying wrongful convictions in the French
court system. The distribution models events occurring independently at
a constant average rate, applicable to server request arrivals,
radioactive decay, and phone calls. For server workloads, the
probability of k requests in time t follows P(k) = (λt)\^{}k *
e\^{}(-λt) / k!, enabling realistic load modeling at 10-1000 requests
per second. } to model random but statistically consistent server
workloads. The harness manages concurrent requests and varying load
intensities to evaluate system behavior under different operational
conditions.

For embedded and mobile applications, the harness generates input
patterns that reflect actual deployment conditions. This might involve
sequential image injection for mobile vision applications or
synchronized multi-sensor streams for autonomous systems. Such precise
input generation and timing control ensures the system experiences
realistic operational patterns, revealing performance characteristics
that would emerge in actual device deployment.

The harness must also accommodate different throughput models. Batch
processing scenarios require the ability to evaluate system performance
on large volumes of parallel inputs, while real-time applications need
precise timing control for sequential processing. In the embedded
implementation phase, the harness must support precise measurement of
inference time and energy consumption per operation.

Reproducibility demands that the harness maintain consistent testing
conditions across different evaluation runs. This includes controlling
environmental factors such as background processes, thermal conditions,
and power states that might affect performance measurements. The harness
must also provide mechanisms for collecting and logging performance
metrics without significantly impacting the system under test.

\subsection{System
Specifications}\label{sec-benchmarking-ai-system-specifications-6e80}

Complementing the benchmark harness that controls test execution, system
specifications are fundamental components of machine learning benchmarks
that directly impact model performance, training time, and experimental
reproducibility. These specifications encompass the complete
computational environment, ensuring that benchmarking results can be
properly contextualized, compared, and reproduced by other researchers.

Hardware specifications typically include:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Processor type and speed (e.g., CPU model, clock rate)
\item
  GPUs, or TPUs, including model, memory capacity, and quantity if used
  for distributed training
\item
  Memory capacity and type (e.g., RAM size, DDR4)
\item
  Storage type and capacity (e.g., SSD, HDD)
\item
  Network configuration, if relevant for distributed computing
\end{enumerate}

Software specifications generally include:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Operating system and version
\item
  Programming language and version
\item
  Machine learning frameworks and libraries (e.g., TensorFlow, PyTorch)
  with version numbers
\item
  Compiler information and optimization flags
\item
  Custom software or scripts used in the benchmark process
\item
  Environment management tools and configuration (e.g., Docker
  containers\sidenote{\textbf{Docker}: Containerization platform that
  packages applications and their dependencies into lightweight,
  portable containers, helping produce consistent execution across
  different environments. Widely adopted in ML benchmarking since 2013,
  Docker reduces ``works on my machine'' problems by providing
  controlled runtime environments, with MLPerf and other benchmark
  suites distributing official Docker images to support reproducible
  results. }, virtual environments)
\end{enumerate}

The precise documentation of these specifications is essential for
experimental validity and reproducibility. This documentation enables
other researchers to replicate the benchmark environment with high
fidelity, provides critical context for interpreting performance
metrics, and facilitates understanding of resource requirements and
scaling characteristics across different models and tasks.

In many cases, benchmarks may include results from multiple hardware
configurations to provide a more comprehensive view of model performance
across different computational environments. This approach is
particularly valuable as it highlights the trade-offs between model
complexity, computational resources, and performance.

As the field evolves, hardware and software specifications increasingly
incorporate detailed energy consumption metrics and computational
efficiency measures, such as FLOPS/watt and total power usage over
training time. This expansion reflects growing concerns about the
environmental impact of large-scale machine learning models and supports
the development of more sustainable AI practices. Comprehensive
specification documentation thus serves multiple purposes: enabling
reproducibility, supporting fair comparisons, and advancing both the
technical and environmental aspects of machine learning research.

\subsection{Run Rules}\label{sec-benchmarking-ai-run-rules-c33f}

Beyond the technical infrastructure, run rules establish the procedural
framework that ensures benchmark results can be reliably replicated by
researchers and practitioners, complementing the technical environment
defined by system specifications. These guidelines are essential for
validating research claims, building upon existing work, and advancing
machine learning. Central to reproducibility in AI benchmarks is the
management of controlled randomness, the systematic handling of
stochastic processes such as weight initialization and data shuffling
that ensures consistent, verifiable results.

Comprehensive documentation of hyperparameters forms a critical
component of reproducibility. Hyperparameters are configuration settings
that control how models learn, such as learning rates and batch sizes,
which must be documented for reproducibility. Given that minor
hyperparameter adjustments can significantly impact model performance,
their precise documentation is essential. Benchmarks mandate the
preservation and sharing of training and evaluation datasets. When
direct data sharing is restricted by privacy or licensing constraints,
benchmarks must provide detailed specifications for data preprocessing
and selection criteria, enabling researchers to construct comparable
datasets or understand the characteristics of the original experimental
data.

Code provenance and availability constitute another vital aspect of
reproducibility guidelines. Contemporary benchmarks typically require
researchers to publish implementation code in version-controlled
repositories, encompassing not only the model implementation but also
comprehensive scripts for data preprocessing, training, and evaluation.
Advanced benchmarks often provide containerized environments that
encapsulate all dependencies and configurations. Detailed experimental
logging is mandatory, including systematic recording of training
metrics, model checkpoints, and documentation of any experimental
adjustments.

These reproducibility guidelines serve multiple essential functions:
they enhance transparency, enable rigorous peer review, and accelerate
scientific progress in AI research. By following these protocols, the
research community can effectively verify results, iterate on successful
approaches, and identify methodological limitations. In machine
learning, where methods evolve rapidly, these reproducibility practices
form the foundation for reliable and progressive research.

\subsection{Result
Interpretation}\label{sec-benchmarking-ai-result-interpretation-cd29}

Building on the foundation established by run rules, result
interpretation guidelines provide the essential framework for
understanding and contextualizing benchmark outcomes. These guidelines
help researchers and practitioners draw meaningful conclusions from
benchmark results, ensuring fair and informative comparisons between
different models or approaches.
\{\#sec-benchmarking-ai-benchmark-result-interpretation-framework-16a7\}

\phantomsection\label{callout-exampleux2a-1.11}
\begin{fbxSimple}{callout-example}{Example:}{Benchmarking a Vision Model for Edge Deployment}
\phantomsection\label{callout-example*-1.11}
\textbf{Scenario}: A team validates MobileNetV2 for a wildlife camera
trap running on a Raspberry Pi 4.

\textbf{Benchmark Results}:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1912}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2353}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2941}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2794}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Precision}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Latency (ms)}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Accuracy (Top-1)}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Model Size (MB)}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{FP32} & 120 ms & 71.8\% & 14.0 \\
\textbf{INT8} & 35 ms & 71.2\% & 3.5 \\
\end{longtable}

\textbf{Interpretation}: The 3.4x speedup and 4x size reduction from
quantization come at a minimal cost of 0.6\% accuracy drop. For a
battery-powered real-time system, INT8 is the clear choice, enabling 28
FPS processing compared to 8 FPS with FP32.

\end{fbxSimple}

Before drawing conclusions from benchmark results, practitioners should
systematically evaluate several dimensions that affect the validity and
applicability of reported numbers:

\textbf{Question 1: Is the comparison fair?}

\begin{itemize}
\tightlist
\item
  Same model architecture, or different? (Comparing ResNet-50
  vs.~MobileNet conflates architecture and optimization)
\item
  Same precision (FP32 vs.~INT8)? Precision differences can explain 2-4x
  performance gaps
\item
  Same batch size? Larger batches favor throughput; smaller batches
  favor latency
\item
  Same hardware generation? Avoid comparing A100 results to H100 without
  normalization
\end{itemize}

\textbf{Question 2: What is excluded?}

\begin{itemize}
\tightlist
\item
  Data loading and preprocessing time (often significant)
\item
  Model loading and initialization (critical for cold-start scenarios)
\item
  Communication overhead in distributed settings
\item
  Memory transfer between CPU and accelerator
\item
  Queue wait time under load
\end{itemize}

\textbf{Question 3: What conditions produced these results?}

\begin{itemize}
\tightlist
\item
  Thermal state (cold vs.~sustained operation)
\item
  Power limit settings (default vs.~maximum)
\item
  Batch size (optimal for benchmark vs.~realistic for deployment)
\item
  Input characteristics (benchmark data vs.~production data
  distribution)
\end{itemize}

\textbf{Question 4: Are the statistics meaningful?}

\begin{itemize}
\tightlist
\item
  How many runs? (Minimum 5, preferably 10+)
\item
  What variance is reported? (Standard deviation, confidence intervals)
\item
  Are outliers included or excluded?
\item
  Is the system in steady-state, or are startup effects included?
\end{itemize}

Applying these questions to a concrete vendor claim illustrates how
incomplete specifications obscure real performance.

\phantomsection\label{callout-notebookux2a-1.12}
\begin{fbxSimple}{callout-notebook}{AI Engineer’s Notebook:}{Interpreting a Benchmark Claim}
\phantomsection\label{callout-notebook*-1.12}
\textbf{Problem}: A vendor claims ``Our system achieves 10,000
images/second on ResNet-50.'' Should you trust this number for your
deployment planning?

\textbf{Critical Questions}:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{What batch size?} Batch 256 achieves high throughput but 256ms
  latency; batch 1 achieves low latency but lower throughput.
\item
  \textbf{What precision?} INT8 is 2-4x faster than FP32 but may have
  accuracy implications.
\item
  \textbf{What is included?} Pure inference, or including preprocessing?
\item
  \textbf{What accuracy?} Matching the original 76.1\% Top-1, or
  degraded?
\end{enumerate}

\textbf{A Complete Specification}: ``10,000 images/second on ResNet-50
at batch size 32, INT8 precision, 76.0\% Top-1 accuracy, including JPEG
decoding, on NVIDIA H100 at 700W TDP.''

\textbf{The Systems Insight}: Understanding whether a performance
difference is meaningful requires both statistical rigor AND contextual
validation. A benchmark number without these details is a marketing
claim, not an engineering specification.

\end{fbxSimple}

Result interpretation requires careful consideration of real-world
applications and context. While a 1\% improvement in accuracy might be
crucial for medical diagnostics or financial systems, other applications
might prioritize inference speed or model efficiency over marginal
accuracy gains. Understanding these context-specific requirements is
essential for meaningful interpretation of benchmark results. Users must
also recognize inherent benchmark limitations, as no single evaluation
framework can encompass all possible use cases. Common limitations
include dataset biases, task-specific characteristics, and constraints
of evaluation metrics.

Modern benchmarks often necessitate multi-dimensional analysis across
various performance metrics. For instance, when a model demonstrates
superior accuracy but requires substantially more computational
resources, interpretation guidelines help practitioners evaluate these
trade-offs based on their specific constraints and requirements. The
guidelines also address the critical issue of benchmark overfitting,
where models might be excessively optimized for specific benchmark tasks
at the expense of real-world generalization. To mitigate this risk,
guidelines often recommend evaluating model performance on related but
distinct tasks and considering practical deployment scenarios.

These comprehensive interpretation frameworks ensure that benchmarks
serve their intended purpose: providing standardized performance
measurements while enabling nuanced understanding of model capabilities.
This balanced approach supports evidence-based decision-making in both
research contexts and practical machine learning applications.

\subsection{Example
Benchmark}\label{sec-benchmarking-ai-example-benchmark-229f}

To illustrate how these components work together in practice, a complete
benchmark run evaluates system performance by synthesizing multiple
components under controlled conditions to produce reproducible
measurements. Figure~\ref{fig-benchmark-components} illustrates this
integration through an audio anomaly detection system. It shows how
performance metrics are systematically measured and reported within a
framework that encompasses problem definition, datasets, model
selection, evaluation criteria, and standardized run rules.

The benchmark measures several key performance dimensions. For
computational resources, the system reports a model size of 270
Kparameters and requires 10.4 milliseconds per inference. For task
effectiveness, it achieves a detection accuracy of 0.86 AUC (Area Under
Curve) in distinguishing normal from anomalous audio patterns. For
operational efficiency, it consumes 516 µJ of energy per inference.

The relative importance of these metrics varies by deployment context.
Energy consumption per inference is critical for battery-powered devices
but less consequential for systems with constant power supply. Model
size constraints differ significantly between cloud deployments with
abundant resources and embedded devices with limited memory. Processing
speed requirements depend on whether the system must operate in
real-time or can process data in batches.

The benchmark reveals inherent trade-offs between performance metrics in
machine learning systems. For instance, reducing the model size from 270
Kparameters might improve processing speed and energy efficiency but
could decrease the 0.86 AUC detection accuracy. These interconnected
metrics contribute to overall system performance in the deployment
phase.

Ultimately, whether these measurements constitute a ``passing''
benchmark depends on the specific requirements of the intended
application. The benchmark framework provides the structure and
methodology for consistent evaluation, while the acceptance criteria
must align with deployment constraints and performance requirements.

The anomaly detection example illustrates benchmarking for a general
inference scenario. However, models optimized through compression
techniques require specialized evaluation that captures the unique
trade-offs between size reduction and capability preservation.

\subsection{Compression
Benchmarks}\label{sec-benchmarking-ai-compression-benchmarks-9cf0}

Extending beyond general benchmarking principles, as machine learning
models continue to grow in size and complexity, neural network
compression has emerged as a critical optimization technique for
deployment across resource-constrained environments. Compression
benchmarking methodologies evaluate the effectiveness of techniques
including pruning, quantization, knowledge distillation, and
architecture optimization. These specialized benchmarks measure the core
trade-offs between model size reduction, accuracy preservation, and
computational efficiency improvements.

Model compression benchmarks assess multiple dimensions simultaneously.
The primary dimension involves size reduction metrics that evaluate
parameters (counting), memory footprint (bytes), and storage
requirements (compressed file size). Effective compression achieves
significant reduction while maintaining accuracy: MobileNetV2 achieves
approximately 72\% ImageNet top-1 accuracy with 3.4 million parameters
versus ResNet-50's 76\% accuracy with 25.6 million parameters,
representing a 7.5x efficiency improvement in the parameter-to-accuracy
ratio.

Beyond basic size metrics, sparsity evaluation frameworks distinguish
between structured and unstructured pruning efficiency. Structured
pruning removes entire neurons or filters, achieving consistent speedups
but typically lower compression ratios (2-4x). Unstructured pruning
eliminates individual weights, achieving higher compression ratios
(10-100x) but requiring specialized sparse computation support for
speedup realization. Benchmark protocols must specify hardware platform
and software implementation to ensure meaningful sparse acceleration
measurements.

Complementing sparsity techniques, quantization benchmarking protocols
evaluate precision reduction techniques across multiple data types. INT8
quantization typically provides 4x memory reduction and 2-4x inference
speedup while maintaining 99\%+ accuracy preservation for most computer
vision models. Mixed-precision approaches achieve optimal efficiency by
applying different precision levels to different layers: critical layers
retain FP16 precision while computation-heavy layers utilize INT8 or
INT4, enabling fine-grained efficiency optimization.

Another critical dimension involves knowledge transfer effectiveness
metrics that measure performance relationships between different model
sizes. Successful knowledge transfer achieves 90-95\% of larger model
accuracy while reducing model size by 5-10x. Compact models can
demonstrate this approach, achieving high performance with significantly
fewer parameters and faster inference, illustrating the potential for
efficiency without significant capability loss.

Finally, acceleration factor measurements for optimized models reveal
the practical benefits across different hardware platforms. Optimized
models achieve varying speedup factors: sparse models deliver 2-5x
speedup on CPUs, reduced-precision models achieve 2-8x speedup on mobile
processors, and efficient architectures provide 5-20x speedup on
specialized edge accelerators. These hardware-specific measurements
ensure efficiency benchmarks reflect real deployment scenarios.

Efficiency-aware benchmarking addresses critical gaps in traditional
evaluation frameworks. Current benchmark suites like MLPerf focus
primarily on dense, unoptimized models that do not represent production
deployments, where optimized models are ubiquitous. Future benchmarking
frameworks should include efficiency model divisions specifically
evaluating optimized architectures, reduced-precision inference, and
compact models to accurately reflect real deployment practices and guide
efficiency research toward practical impact.

\subsection{Mobile and Edge
Benchmarks}\label{sec-benchmarking-ai-mobile-edge-benchmarks-fd4f}

Mobile and edge deployments face constraints fundamentally different
from cloud environments, requiring specialized benchmarking approaches
that capture the unique trade-offs in resource-constrained settings.
These constraints form an interdependent triangle of power consumption,
inference latency, and model accuracy, where improving any two typically
degrades the third.

\textbf{The Power-Latency-Accuracy Triangle.} Edge deployment requires
navigating trade-offs that cloud deployments can largely ignore:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.1944}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4028}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4028}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Constraint}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Cloud Impact}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Edge Impact}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Power} & Operational cost (\textasciitilde\$0.10/kWh) & Hard
limit (battery capacity) \\
\textbf{Latency} & User experience metric & Safety-critical deadline \\
\textbf{Accuracy} & Primary optimization target & Constrained by
power/latency \\
\end{longtable}

\textbf{Concrete example}: A smartphone camera AI for real-time object
detection must process 30 frames/second (33ms/frame) while consuming
\textless1W to avoid excessive battery drain and thermal throttling. A
MobileNetV3 model achieving 75\% accuracy at 15ms/frame and 0.8W meets
these constraints; a ResNet-50 achieving 80\% accuracy at 45ms/frame and
2.5W does not, despite being ``better'' by accuracy-only benchmarks. An
\emph{edge benchmark reality check} exposes these gaps between marketed
specifications and sustained operational behavior. The gap between
\emph{peak} and \emph{sustained} performance is particularly dangerous:
a vendor may report burst-mode numbers that halve under thermal
throttling within minutes, making \emph{benchmarking the edge} a
fundamentally different exercise than benchmarking the cloud.

\phantomsection\label{callout-exampleux2a-1.13}
\begin{fbxSimple}{callout-example}{Example:}{Benchmarking the Edge}
\phantomsection\label{callout-example*-1.13}
\textbf{The Scenario}: You are selecting a device for a smart doorbell.
The vendor claims their chip runs ``AI at 1 Watt.''

\textbf{The Benchmark}: You run a continuous object detection loop.

\textbf{The Reality}:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Minute 0-1}: The chip runs fast (30 FPS) at 1W.
\item
  \textbf{Minute 2}: The chip heats up.
\item
  \textbf{Minute 5}: \textbf{Thermal Throttling} kicks in. The clock
  speed drops by 50\% to prevent melting.
\item
  \textbf{Steady State}: The chip stabilizes at 15 FPS.
\end{enumerate}

\textbf{The Conclusion}: The ``Benchmark'' was 30 FPS. The ``Product
Reality'' is 15 FPS. If you designed your user experience for 30 FPS,
your product is broken. Always benchmark \textbf{sustained performance},
not just peak.

\end{fbxSimple}

This pattern of optimistic marketing claims versus production reality is
endemic to edge hardware. The following checklist provides a systematic
approach to cutting through the noise.

\phantomsection\label{callout-perspectiveux2a-1.14}
\begin{fbxSimple}{callout-perspective}{Systems Perspective:}{Edge Benchmark Reality Check}
\phantomsection\label{callout-perspective*-1.14}

When evaluating edge hardware claims:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Peak vs.~Sustained}: Snapdragon 8 Gen 3 advertises 35 TOPS
  peak but delivers 20 TOPS sustained under thermal throttling. Always
  benchmark under sustained workloads (\textgreater30 seconds minimum).
\item
  \textbf{Power at idle vs.~active}: A device consuming 50mW idle and 2W
  active may report ``2W'' for marketing, but if your application runs
  inference 1\% of the time, effective power draw is
  \textasciitilde70mW---not 2W.
\item
  \textbf{Thermal envelope}: Edge devices typically target 3-5W thermal
  design power (TDP). Exceeding this triggers throttling within seconds.
  Benchmark reports omitting thermal conditions are incomplete.
\item
  \textbf{End-to-end vs.~accelerator-only}: NPU benchmarks often exclude
  data transfer overhead. Moving image data from camera to NPU and back
  can exceed inference time for small models.
\end{enumerate}

\end{fbxSimple}

\textbf{Heterogeneous Processor Coordination.} Mobile SoCs integrate
heterogeneous processors (CPU, GPU, DSP, NPU) requiring specialized
benchmarking that captures workload distribution complexity while
accounting for thermal and battery constraints. Effective processor
coordination achieves 3-5x performance improvements through intelligent
work distribution:

\begin{itemize}
\tightlist
\item
  \textbf{CPU}: Best for control flow, small batches, sequential
  processing
\item
  \textbf{GPU}: Best for parallel floating-point, batch processing,
  general ML
\item
  \textbf{DSP}: Best for fixed-point signal processing, always-on
  detection
\item
  \textbf{NPU}: Best for specific neural network architectures with
  INT8/INT4 precision
\end{itemize}

Benchmarks must evaluate workload placement decisions. A voice assistant
might use the DSP for always-on wake-word detection (5mW continuous),
switch to NPU for speech recognition (200mW burst), and use CPU for
language understanding (100mW). Single-processor benchmarks miss these
orchestration dynamics.

\textbf{Battery and Thermal Benchmarking.} Battery impact varies
dramatically by use case: computational photography consumes 2-5W during
active capture, while background AI for activity recognition requires
5-50mW for acceptable all-day endurance. Effective battery benchmarking
requires:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Workload duty cycle specification}: What fraction of time is
  inference active?
\item
  \textbf{Background power measurement}: System power when model is
  loaded but idle
\item
  \textbf{Inference energy per operation}: Joules per inference, not
  just watts during inference
\item
  \textbf{Thermal throttling characterization}: Performance over
  5-minute sustained workloads
\end{enumerate}

\textbf{Edge-Cloud Coordination.} Mobile benchmarking must also evaluate
5G/WiFi edge-cloud coordination, with URLLC\sidenote{\textbf{URLLC
(Ultra-Reliable Low-Latency Communication)}: 5G service category
requiring 99.999\% reliability and \textless1ms latency for
mission-critical applications like remote surgery and autonomous
vehicles. Achieving both simultaneously requires edge computing
placement within 10km of users. ML inference at the edge must meet these
guarantees, driving specialized model architectures and redundant
deployment patterns. } demanding \textless1ms latency for critical
applications. This introduces new benchmarking dimensions:

\begin{itemize}
\tightlist
\item
  \textbf{Network latency variability}: 4G/5G latency ranges from 10ms
  to 100ms+ depending on congestion
\item
  \textbf{Fallback behavior}: How does the system behave when
  connectivity fails?
\item
  \textbf{Workload splitting}: What computation runs locally
  vs.~remotely?
\item
  \textbf{Privacy constraints}: What data can be transmitted for cloud
  inference?
\end{itemize}

Automotive deployments add ASIL validation, multi-sensor fusion, and
-40°C to +85°C environmental testing. These unique requirements
necessitate comprehensive frameworks evaluating sustained performance
under thermal constraints, battery efficiency across usage patterns, and
connectivity-dependent behavior, extending beyond isolated peak
measurements.

\section{Training vs.~Inference
Evaluation}\label{sec-benchmarking-ai-training-vs-inference-evaluation-a3be}

With benchmark components and evaluation granularities established, we
now address the core of hardware acceleration validation. The same
neural network behaves fundamentally differently depending on whether it
is learning or predicting, and the hardware acceleration strategies from
\textbf{?@sec-ai-acceleration} must be validated for both phases.
Training seeks optimal parameters through iterative refinement,
processing billions of examples over hours or days, stressing memory
bandwidth, multi-GPU scaling, and sustained throughput. Inference
applies those parameters to individual inputs, often within millisecond
deadlines, stressing latency consistency, cold-start time, and power
efficiency.

These contrasting objectives create evaluation requirements so different
that separate benchmarking frameworks emerged for each: MLPerf Training
and MLPerf Inference. The following sections detail how each framework
validates the hardware acceleration claims from preceding chapters,
revealing whether theoretical TFLOPS translate to practical
time-to-train or queries-per-second.

The training methodologies from \textbf{?@sec-ai-training} focus on
iterative optimization over large datasets, while deployment strategies
from \textbf{?@sec-machine-learning-operations-mlops} prioritize
consistent, low-latency serving. Understanding this distinction is
necessary before we examine each phase's specialized metrics, as the
differences cascade through metric selection, resource allocation, and
scaling behavior.

These fundamental differences manifest across several dimensions.
Training involves iterative optimization with bidirectional computation
(forward and backward passes), while inference performs single forward
passes with fixed model parameters. ResNet-50 training requires 8GB GPU
memory for gradients and optimizer states compared to 0.5GB for
inference-only forward passes. At scale, training GPT-3 utilized 1024
A100 GPUs for months, while inference deploys single models across
thousands of concurrent requests with millisecond response requirements.

The phases also prioritize different performance characteristics.
Training prioritizes throughput and convergence speed, measured in
samples processed per unit time and training completion time. BERT-Large
training achieves optimal performance at batch size 512 with 32-hour
convergence time, while BERT inference optimizes for \textless10ms
latency per query with batch size 1-4. Training can sacrifice latency
for throughput (processing 10,000 samples/second), while inference
sacrifices throughput for latency consistency. This distinction extends
to resource utilization: multi-node training scales efficiently with
batch sizes 4096-32,768, achieving 90\% compute utilization, whereas
inference must respond to individual requests with minimal latency,
constraining batch sizes to 1-16 for real-time applications and
resulting in 15-40\% GPU utilization.

Memory and optimization strategies diverge accordingly. Training
requires simultaneous access to parameters, gradients, optimizer states,
and activations, creating 3-4x memory overhead compared to inference.
Mixed-precision training (FP16/FP32) reduces memory usage by 50\% while
maintaining convergence, whereas inference can utilize INT8 quantization
for 4x memory reduction with minimal accuracy loss. Training employs
gradient compression, mixed-precision training, and progressive pruning
during optimization, while inference optimization utilizes post-training
quantization, knowledge distillation, and neural architecture search.

Energy costs follow different patterns. Training energy costs are
amortized across model lifetime and measured in total energy per trained
model; estimates for large training runs can reach the scale of
thousands of megawatt-hours (GPT-3 has been estimated at roughly 1,287
MWh) (\citeproc{ref-patterson2021carbon}{Patterson et al. 2021}).
Inference energy costs accumulate per query and can become a dominant
operational consideration at scale. A durable way to reason about
per-query energy is the identity (E = P \times t). For example, a 300 W
accelerator running a 10 ms inference consumes (300 \times 0.01 = 3)
joules, which is about (0.00083) Wh; at 100 ms, that becomes about
(0.0083) Wh.

This comparative framework guides benchmark design by highlighting which
metrics matter most for each phase and how evaluation methodologies
should differ to capture phase-specific performance characteristics.
Training benchmarks emphasize convergence time and scaling efficiency,
while inference benchmarks prioritize latency consistency and resource
efficiency across diverse deployment scenarios.

\section{Training
Benchmarks}\label{sec-benchmarking-ai-training-benchmarks-96da}

Training benchmarks divide into three categories: convergence metrics
that measure learning progress, throughput metrics that measure
computational efficiency, and scalability metrics that measure
distributed performance. We examine each in turn.

Training benchmarks validate whether hardware acceleration delivers
promised training throughput. The GPU clusters, TPU pods, and
distributed training strategies examined in
\textbf{?@sec-ai-acceleration} all claim dramatic speedups, and training
benchmarks reveal which claims hold under realistic workloads. They
evaluate how hardware configurations, data loading mechanisms, and
distributed training strategies actually perform when training
production-scale models.

These benchmarks are vital because training represents the largest
capital expenditure in ML systems. A cluster that costs \$10M should
demonstrably outperform a \$2M cluster on training time-to-accuracy, but
only rigorous benchmarking reveals whether the 5x cost delivers
proportional value or falls victim to scaling inefficiencies, memory
bottlenecks, or communication overhead.

For instance, large-scale models like OpenAI's
GPT-3\sidenote{\textbf{GPT-3}: OpenAI's 2020 language model with 175
billion parameters, trained on 300 billion tokens using 10,000 NVIDIA
V100 GPUs for several months at an estimated cost of \$4.6 million or
more (Lambda Labs estimate; actual cost likely higher due to scaling
inefficiencies). GPT-3 demonstrated emergent abilities like few-shot
learning and in-context reasoning, establishing the paradigm of scaling
laws where larger models consistently outperform smaller ones across
diverse language tasks. } (\citeproc{ref-brown2020language}{Brown et al.
2020}), which consists of 175 billion parameters trained on
approximately 570GB of filtered CommonCrawl text (from a
\textasciitilde45TB raw dataset, combined with other sources to form 300
billion training tokens), highlight the immense computational demands of
modern training. Standardized \emph{ML training benchmarks} provide
systematic evaluation of the underlying systems to ensure that hardware
and software configurations can meet these unprecedented demands
efficiently.

\phantomsection\label{callout-definitionux2a-1.15}
\begin{fbxSimple}{callout-definition}{Definition:}{ML Training Benchmarks}
\phantomsection\label{callout-definition*-1.15}
\textbf{\emph{ML Training Benchmarks}} measure the \textbf{Rate of
Convergence} per unit of resource (time, energy, cost). They validate
the system's ability to sustain high \textbf{Arithmetic Intensity}
across distributed accelerators while managing the \textbf{Communication
Overhead} of gradient synchronization.

\end{fbxSimple}

Beyond computational demands, efficient data storage and delivery play a
major role in the training process. Loading an entire image dataset into
memory is typically infeasible, so practitioners rely on data loaders
from ML frameworks. Successful model training depends on timely and
efficient data delivery, making it essential to benchmark data
pipelines, preprocessing speed, and storage retrieval times to
understand their impact on training performance. Hardware selection
represents another key factor, as different configurations can
significantly impact training time. Training benchmarks evaluate CPU,
GPU, memory, and network utilization during the training phase to guide
system optimizations and uncover bottlenecks or inefficiencies in
resource utilization.

In many cases, using a single hardware accelerator is insufficient to
meet the computational demands of large-scale model training. Machine
learning models are often trained in data centers with multiple GPUs or
TPUs, where distributed computing enables parallel processing across
nodes. Training benchmarks assess how efficiently the system scales
across multiple nodes, manages data sharding, and handles challenges
like node failures or drop-offs during training.

MLPerf Training (\citeproc{ref-mlperf_training_website}{MLCommons
2024f}) provides the standardized framework referenced throughout this
analysis of training benchmarks.

\subsection{Training Benchmark
Motivation}\label{sec-benchmarking-ai-training-benchmark-motivation-f365}

From a systems perspective, training machine learning models represents
a computationally intensive process that requires careful optimization
of resources. Training benchmarks serve as essential tools for
evaluating system efficiency, identifying bottlenecks, and ensuring that
machine learning systems can scale effectively. They provide a
standardized approach to measuring how various system components,
including hardware accelerators, memory, storage, and network
infrastructure, affect training performance.

Training benchmarks allow researchers and engineers to push the
state-of-the-art, optimize configurations, improve scalability, and
reduce overall resource consumption by evaluating these factors.
Figure~\ref{fig-mlperf-training-improve} demonstrates that performance
improvements in progressive versions of MLPerf Training benchmarks have
consistently outpaced Moore's Law, with ResNet training speedups
exceeding 30x over five years. This exponential improvement shows that
what gets measured gets improved, showcasing the rapid evolution of ML
computing through standardized benchmarking.

\begin{figure}[htb]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/cae12187e745b99a2456b2b33a19e07858a3f2cb.pdf}}

}

\caption{\label{fig-mlperf-training-improve}\textbf{MLPerf Training
Progress}: Standardized benchmarks reveal that machine learning training
performance consistently surpasses Moore's Law, indicating substantial
gains from systems-level optimizations. These trends emphasize how
focused measurement and iterative improvement drive rapid advancements
in ML training efficiency and scalability. Source:
(\citeproc{ref-tschand2024mlperf}{Tschand et al. 2024}).}

\end{figure}%

\textbf{Importance of Training Benchmarks.} As machine learning models
grow in complexity, training becomes increasingly demanding in terms of
compute power, memory, and data storage. The ability to measure and
compare training efficiency is critical to ensuring that systems can
effectively handle large-scale workloads. Training benchmarks provide a
structured methodology for assessing performance across different
hardware platforms, software frameworks, and optimization techniques.

A primary challenge in training machine learning models is efficient
allocation of computational resources. Training a large-scale language
model such as GPT-3, which consists of 175 billion parameters and
requires processing terabytes of data, places enormous demands on modern
computing infrastructure. Without standardized benchmarks, it is
difficult to determine whether a system fully utilizes its resources or
whether inefficiencies (slow data loading, underutilized accelerators,
excessive memory overhead) limit performance.

Training benchmarks help uncover such inefficiencies by measuring key
performance indicators, including system throughput, time-to-accuracy,
and hardware utilization. Recall from \textbf{?@sec-ai-acceleration}
that GPUs achieve approximately 15,700 GFLOPS for mixed-precision
operations while TPUs deliver 275,000 INT8 operations per second for
specialized tensor workloads. Training benchmarks allow us to measure
whether these theoretical hardware capabilities translate to actual
training speedups under realistic conditions. These benchmarks allow
practitioners to analyze \emph{whether} accelerators are being leveraged
effectively or \emph{whether} specific bottlenecks, such as memory
bandwidth constraints from hardware limitations
(\textbf{?@sec-ai-acceleration}), are reducing overall system
performance. For example, a system using TF32 precision1 may achieve
higher throughput than one using FP32, but if TF32 introduces numerical
instability that increases the number of iterations required to reach
the target accuracy, the overall training time may be longer. By
providing insights into these factors, benchmarks support the design of
more efficient training workflows that maximize hardware potential while
minimizing unnecessary computation.

\textbf{Hardware and Software Optimization.} The performance of machine
learning training is heavily influenced by the choice of hardware and
software. Training benchmarks guide system designers in selecting
optimal configurations by measuring how different architectures,
including GPUs, TPUs, and emerging AI accelerators, handle computational
workloads. These benchmarks also evaluate how well deep learning
frameworks, such as TensorFlow and PyTorch, optimize performance across
different hardware setups.

For example, the MLPerf Training benchmark suite is widely used to
compare the performance of different accelerator architectures on tasks
such as image classification, natural language processing, and
recommendation systems. By running standardized benchmarks across
multiple hardware configurations, engineers can determine whether
certain accelerators are better suited for specific training workloads.
This information is particularly valuable in large-scale data centers
and cloud computing environments, where selecting the right combination
of hardware and software can lead to significant performance gains and
cost savings.

Beyond hardware selection, training benchmarks also inform software
optimizations. Machine learning frameworks implement various low-level
optimizations, including mixed-precision
training\sidenote{\textbf{Mixed-Precision Training}: A training
technique that uses both 16-bit (FP16) and 32-bit (FP32) floating-point
representations to accelerate training while maintaining model accuracy.
Introduced by NVIDIA in 2017, mixed precision can achieve 1.5-2x
speedups on modern GPUs with Tensor Cores while reducing memory usage by
\textasciitilde40\%, enabling larger batch sizes and faster convergence
for large models. }, memory-efficient data loading, and distributed
training strategies, that can significantly impact system performance.
Benchmarks help quantify the impact of these optimizations, ensuring
that training systems are configured for maximum efficiency.

\textbf{Scalability and Efficiency.} As machine learning workloads
continue to grow, efficient scaling across distributed computing
environments has become a key concern. Many modern deep learning models
are trained across multiple GPUs or TPUs, requiring efficient
parallelization strategies to ensure that additional computing resources
lead to meaningful performance improvements. Training benchmarks measure
how well a system scales by evaluating system throughput, memory
efficiency, and overall training time as additional computational
resources are introduced.

Effective scaling is not always guaranteed. While adding more GPUs or
TPUs should, in theory, reduce training time, issues such as
communication overhead, data synchronization latency, and memory
bottlenecks can limit scaling efficiency. Training benchmarks help
identify these challenges by quantifying how performance scales with
increasing hardware resources. A well-designed system should exhibit
near-linear scaling, where doubling the number of GPUs results in a
near-halving of training time. However, real-world inefficiencies often
prevent perfect scaling, and benchmarks provide the necessary insights
to optimize system design accordingly.

Another crucial factor in training efficiency is time-to-accuracy, which
measures how quickly a model reaches a target accuracy level. This
metric bridges the algorithmic and system dimensions of our framework,
connecting model convergence characteristics with computational
efficiency. By leveraging training benchmarks, system designers can
assess whether their infrastructure is capable of handling large-scale
workloads efficiently while maintaining training stability and accuracy.

\textbf{Cost and Energy Factors.} The computational cost of training
large-scale models has risen sharply in recent years, making
cost-efficiency a critical consideration. Training a model such as GPT-3
can require millions of dollars in cloud computing resources, making it
imperative to evaluate cost-effectiveness across different hardware and
software configurations. Training benchmarks provide a means to quantify
the cost per training run by analyzing computational expenses, cloud
pricing models, and energy consumption.

Beyond financial cost, energy efficiency has become an increasingly
important metric. Large-scale training runs consume vast amounts of
electricity, contributing to significant carbon emissions. Benchmarks
help evaluate energy efficiency by measuring power consumption per unit
of training progress, allowing organizations to identify sustainable
approaches to AI development.

For example, MLPerf includes an energy benchmarking component that
tracks the power consumption of various hardware accelerators during
training. This allows researchers to compare different computing
platforms not only in terms of raw performance but also in terms of
their environmental impact. By integrating energy efficiency metrics
into benchmarking studies, organizations can design AI systems that
balance computational power with sustainability goals.

\textbf{Fair ML Systems Comparison.} One of the primary functions of
training benchmarks is to establish a standardized framework for
comparing ML systems. Given the wide variety of hardware architectures,
deep learning frameworks, and optimization techniques available today,
ensuring fair and reproducible comparisons is necessary.

Standardized benchmarks provide a common evaluation methodology,
allowing researchers and practitioners to assess how different training
systems perform under identical conditions. MLPerf Training benchmarks
enable vendor-neutral comparisons by defining strict evaluation criteria
for deep learning tasks such as image classification, language modeling,
and recommendation systems. This ensures that performance results are
meaningful and not skewed by differences in dataset preprocessing,
hyperparameter tuning, or implementation details.

Reproducibility concerns in machine learning research are addressed by
providing clearly defined evaluation methodologies. Results can be
consistently reproduced across different computing environments,
enabling researchers to make informed decisions when selecting hardware,
software, and training methodologies while driving systematic progress
in AI systems development.

\subsection{Training
Metrics}\label{sec-benchmarking-ai-training-metrics-0f1a}

Evaluating the performance of machine learning training requires a set
of well-defined metrics that go beyond conventional algorithmic
measures. From a systems perspective, training benchmarks assess how
efficiently and effectively a machine learning model can be trained to a
predefined accuracy threshold. Metrics such as throughput, scalability,
and energy efficiency are only meaningful in relation to whether the
model successfully reaches its target accuracy. Without this constraint,
optimizing for raw speed or resource utilization may lead to misleading
conclusions.

Training benchmarks, such as MLPerf Training, define specific accuracy
targets for different machine learning tasks, ensuring that performance
measurements are made in a fair and reproducible manner. A system that
trains a model quickly but fails to reach the required accuracy is not
considered a valid benchmark result. Conversely, a system that achieves
the best possible accuracy but takes an excessive amount of time or
resources may not be practically useful. Effective benchmarking requires
balancing speed, efficiency, and accuracy convergence.

\textbf{Time and Throughput.}

One of the primary metrics for evaluating training efficiency is the
time required to reach a predefined accuracy threshold. Training time
(\(T_{\text{train}}\)) measures how long a model takes to converge to an
acceptable performance level, reflecting the overall computational
efficiency of the system. It is formally defined as: \[
T_{\text{train}} = \arg\min_{t} \big\{ \text{accuracy}(t) \geq \text{target accuracy} \big\}
\]

This metric ensures that benchmarking focuses on how quickly and
effectively a system can achieve meaningful results.

Throughput\sidenote{\textbf{Throughput}: A compound of ``through'' and
``put'' (to place), originally from manufacturing where it measured
units passing through a production line per unit time. The term entered
computing in the 1960s batch-processing era to describe jobs completed
per hour. In ML systems, throughput measures samples processed, tokens
generated, or inferences completed per second, the rate at which work
flows through the computational pipeline. }, often expressed as the
number of training samples processed per second, provides an additional
measure of system performance:

\[
\text{Throughput} = \frac{N_{\text{samples}}}{T_{\text{train}}}
\] where \(N_{\text{samples}}\) is the total number of training samples
processed. Throughput alone does not guarantee meaningful results, as a
model may process a large number of samples quickly without necessarily
reaching the desired accuracy.

For example, in MLPerf Training, the benchmark for ResNet-50 may require
reaching an accuracy target like 75.9\% top-1 on the ImageNet dataset. A
system that processes 10,000 images per second but fails to achieve this
accuracy is not considered a valid benchmark result, while a system that
processes fewer images per second but converges efficiently is
preferable. This highlights why throughput should be evaluated in
relation to time-to-accuracy rather than as an independent performance
measure.

\textbf{Scalability and Parallelism.}

As machine learning models increase in size, training workloads often
require distributed computing across multiple processors or
accelerators. Scalability measures how effectively training performance
improves as more computational resources are added. An ideal system
should exhibit near-linear scaling, where doubling the number of GPUs or
TPUs leads to a proportional reduction in training time. However,
real-world performance is often constrained by factors such as
communication overhead, memory bandwidth limitations, and inefficiencies
in parallelization strategies.

When training large-scale models such as GPT-3, OpenAI employed
approximately 10,000 NVIDIA V100 GPUs in a distributed training setup.
Google's systems have demonstrated similar scaling challenges with their
4,096-node TPU v4 clusters, where adding computational resources
provides more raw power but performance improvements are constrained by
network communication overhead between nodes. Benchmarks such as MLPerf
quantify how well a system scales across multiple GPUs, providing
insights into where inefficiencies arise in distributed training.

Parallelism in training is categorized into data
parallelism\sidenote{\textbf{Data Parallelism}: A training strategy that
distributes batches across multiple GPUs, with each GPU computing
gradients on its portion before synchronization. Single-machine
multi-GPU implementations are covered in \textbf{?@sec-ai-training}. },
model parallelism\sidenote{\textbf{Model Parallelism}: Partitioning a
model across multiple GPUs when it exceeds single-GPU memory capacity.
Required for very large models that cannot fit on a single accelerator.
}, and pipeline parallelism, each presenting distinct challenges. Data
parallelism, the most commonly used strategy, involves splitting the
training dataset across multiple compute nodes. The efficiency of this
approach depends on synchronization mechanisms and gradient
communication overhead. In contrast, model parallelism partitions the
neural network itself, requiring efficient coordination between
processors. Benchmarks evaluate how well a system manages these
parallelism strategies without degrading accuracy convergence.

A key metric for evaluating parallelism is \emph{scaling efficiency},
which quantifies how much of the added computational capacity translates
into actual speedup.

\phantomsection\label{callout-notebookux2a-1.16}
\begin{fbxSimple}{callout-notebook}{AI Engineer’s Notebook:}{Scaling Efficiency Calculation}
\phantomsection\label{callout-notebook*-1.16}
\textbf{Problem}: Your team trains ResNet-50 on ImageNet. Single-GPU
training takes 24 hours. With 8 GPUs, training takes 4 hours. Is this
good scaling? Where did the efficiency go?

\subsubsection*{Step 1: Define Scaling
Efficiency}\label{step-1-define-scaling-efficiency}
\addcontentsline{toc}{subsubsection}{Step 1: Define Scaling Efficiency}

For \textbf{strong scaling} (fixed problem size, more processors):

\[\text{Scaling Efficiency}(N) = \frac{T(1)}{N \times T(N)} \times 100\%\]

Where \(T(1)\) is single-GPU time, \(T(N)\) is N-GPU time, and \(N\) is
GPU count.

\subsubsection*{Step 2: Calculate
Efficiency}\label{step-2-calculate-efficiency}
\addcontentsline{toc}{subsubsection}{Step 2: Calculate Efficiency}

Efficiency(8) = 24 hours / (8 × 4 hours) × 100\% = 24/32 = \textbf{75\%}

With perfect scaling, 8 GPUs would complete in 3 hours (24/8). The
actual 4 hours represents 75\% efficiency.

\subsubsection*{Step 3: Account for the 25\%
Loss}\label{step-3-account-for-the-25-loss}
\addcontentsline{toc}{subsubsection}{Step 3: Account for the 25\% Loss}

The ``missing'' 25\% decomposes into measurable overhead:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3218}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2759}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4023}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Source}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Typical Contribution}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Measurement}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Gradient synchronization} & 10-15\% & AllReduce time per step \\
\textbf{Memory copy (CPU↔GPU)} & 3-5\% & Data transfer profiling \\
\textbf{Load imbalance} & 2-5\% & Per-GPU step time variance \\
\textbf{Batch size effects} & 2-5\% & Larger batches converge
differently \\
\end{longtable}

\subsubsection*{Step 4: The Systems
Insight}\label{step-4-the-systems-insight}
\addcontentsline{toc}{subsubsection}{Step 4: The Systems Insight}

Scaling efficiency decreases as \(N\) grows because communication
overhead scales with GPU count while per-GPU compute shrinks. At 8 GPUs,
75\% efficiency is typical. At 64 GPUs, efficiency often drops to
50-60\%. At 1000+ GPUs, even 30-40\% efficiency requires sophisticated
optimization.

This is why MLPerf reports both raw performance AND scaling efficiency:
a system achieving 2x throughput at 50\% efficiency may be worse than
1.5x throughput at 90\% efficiency, depending on your cost constraints.

\end{fbxSimple}

\textbf{Resource Utilization.} The efficiency of machine learning
training depends not only on speed and scalability but also on how well
available hardware resources are utilized. Compute utilization measures
the extent to which processing units, such as GPUs or TPUs, are actively
engaged during training. Low utilization may indicate bottlenecks in
data movement, memory access, or inefficient workload scheduling.

For instance, when training BERT on a TPU cluster, researchers observed
that input pipeline inefficiencies were limiting overall throughput.
Although the TPUs had high raw compute power, the system was not keeping
them fully utilized due to slow data retrieval from storage. By
profiling the resource utilization, engineers identified the bottleneck
and optimized the input pipeline using TFRecord and data prefetching,
leading to improved performance.

Memory bandwidth is another critical factor, as deep learning models
require frequent access to large volumes of data during training. If
memory bandwidth becomes a limiting factor, increasing compute power
alone will not improve training speed. Benchmarks assess how well models
leverage available memory, ensuring that data transfer rates between
storage, main memory, and processing units do not become performance
bottlenecks.

I/O performance also plays a significant role in training efficiency,
particularly when working with large datasets that cannot fit entirely
in memory. Benchmarks evaluate the efficiency of data loading pipelines,
including preprocessing operations, caching mechanisms, and storage
retrieval speeds. Systems that fail to optimize data loading can
experience significant slowdowns, regardless of computational power.

\textbf{Energy Efficiency and Cost.} Training large-scale machine
learning models requires substantial computational resources, leading to
significant energy consumption and financial costs. Energy efficiency
metrics quantify the power usage of training workloads, helping identify
systems that optimize computational efficiency while minimizing energy
waste. The increasing focus on sustainability has led to the inclusion
of energy-based benchmarks, such as those in MLPerf Training, which
measure power consumption per training run. To understand what these
energy benchmarks actually measure, we must decompose \emph{why INT8
saves energy} at the hardware level.

\phantomsection\label{callout-notebookux2a-1.17}
\begin{fbxSimple}{callout-notebook}{AI Engineer’s Notebook:}{Why INT8 Saves Energy}
\phantomsection\label{callout-notebook*-1.17}
Understanding WHY quantization reduces energy consumption requires
decomposing energy into its physical sources. Two dominant factors
determine inference energy: compute operations and memory access.

\textbf{Compute Energy (per multiply-accumulate operation)}:

\begin{longtable}[]{@{}lrr@{}}
\toprule\noalign{}
\textbf{Precision} & \textbf{Multiplier Energy} & \textbf{Relative
Cost} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{FP32} & \textasciitilde3.7 pJ & 1.0× \\
\textbf{FP16} & \textasciitilde1.1 pJ & 0.3× \\
\textbf{INT8} & \textasciitilde0.2 pJ & 0.05× \\
\end{longtable}

An 8-bit multiplier uses \textasciitilde20× less energy than a 32-bit
floating-point multiplier because transistor count scales roughly with
bit-width squared, and switching energy scales with transistor count.

\textbf{Memory Access Energy (per byte)}:

\begin{longtable}[]{@{}lrr@{}}
\toprule\noalign{}
\textbf{Memory Level} & \textbf{Energy per Byte} & \textbf{Relative
Cost} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Register} & \textasciitilde0.01 pJ & 1× \\
\textbf{L1 Cache} & \textasciitilde0.5 pJ & 50× \\
\textbf{L2 Cache} & \textasciitilde2 pJ & 200× \\
\textbf{DRAM} & \textasciitilde10 pJ & 1000× \\
\end{longtable}

Memory access dominates: reading one byte from DRAM costs 1000× more
energy than a register access.

\textbf{Combined Effect for MobileNet Inference}:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2847}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2639}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2639}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1875}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Component}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{FP32 (17 MB)}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{INT8 (4.3 MB)}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Savings}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Model load from DRAM} & 170 μJ & 43 μJ & 4× \\
\textbf{Compute (300M MACs)} & 1,110 μJ & 60 μJ & 18× \\
\textbf{Total} & \textbf{1,280 μJ} & \textbf{103 μJ} & \textbf{12×} \\
\end{longtable}

\textbf{The Systems Insight}: INT8 quantization provides 4× memory
reduction (obvious) but \textasciitilde18× compute energy reduction
(less obvious). The combined effect explains why quantized models on
edge devices achieve dramatic battery life improvements, not just from
faster inference, but from fundamentally lower energy per operation.

\end{fbxSimple}

Training GPT-3 was estimated to consume 1,287 MWh of electricity
(\citeproc{ref-patterson2021carbon}{Patterson et al. 2021}). If a system
can achieve the same accuracy with fewer training iterations, it
directly reduces energy consumption. Energy-aware benchmarks help guide
the development of hardware and training strategies that optimize power
efficiency while maintaining accuracy targets.

Cost considerations extend beyond electricity usage to include hardware
expenses, cloud computing costs, and infrastructure maintenance.
Training benchmarks provide insights into the cost-effectiveness of
different hardware and software configurations by measuring training
time in relation to resource expenditure. Organizations can use these
benchmarks to balance performance and budget constraints when selecting
training infrastructure.

\textbf{Fault Tolerance and Robustness.} Training workloads often run
for extended periods, sometimes spanning days or weeks, making fault
tolerance an essential consideration. A robust system must handle
unexpected failures (hardware malfunctions, network disruptions, and
memory errors) without compromising accuracy convergence.

In large-scale cloud-based training, node failures are common due to
hardware instability. If a GPU node in a distributed cluster fails,
training must continue without corrupting the model. MLPerf Training
includes evaluations of fault-tolerant training strategies, such as
checkpointing, where models periodically save their progress. This
ensures that failures do not require restarting the entire training
process.

\textbf{Reproducibility and Standardization.} For benchmarks to be
meaningful, results must be reproducible across different runs, hardware
platforms, and software frameworks. Variability in training results can
arise due to stochastic processes, hardware differences, and software
optimizations. Ensuring reproducibility requires standardizing
evaluation protocols, controlling for randomness in model
initialization, and enforcing consistency in dataset processing.

MLPerf Training enforces strict reproducibility requirements, ensuring
that accuracy results remain stable across multiple training runs. When
NVIDIA submitted benchmark results for MLPerf, they had to demonstrate
that their ResNet-50 ImageNet training time remained consistent across
different GPUs. This ensures that benchmarks measure true system
performance rather than noise from randomness.

\subsection{Training Performance
Evaluation}\label{sec-benchmarking-ai-training-performance-evaluation-bdc3}

Evaluating the performance of machine learning training systems involves
more than just measuring how fast a model can be trained. A
comprehensive benchmarking approach considers multiple dimensions, each
capturing a different aspect of system behavior. The specific metrics
used depend on the goals of the evaluation, whether those are optimizing
speed, improving resource efficiency, reducing energy consumption, or
ensuring robustness and reproducibility.

Table~\ref{tbl-training-metrics} summarizes the core categories and
associated metrics commonly used to benchmark system-level training
performance, providing a framework for understanding how training
systems behave under different workloads and configurations.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.1806}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.5463}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2731}}@{}}
\caption{\textbf{Training Benchmark Dimensions.} Key categories and
metrics for evaluating machine learning training systems beyond simple
speed, covering resource efficiency, reproducibility, and overall
performance tradeoffs across different training approaches and
infrastructure
configurations.}\label{tbl-training-metrics}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Category}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Key Metrics}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Example Benchmark Use}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Category}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Key Metrics}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Example Benchmark Use}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Training Time and Throughput} & Time-to-accuracy (seconds,
minutes, hours); Throughput (samples/sec) & Comparing training speed
across different GPU architectures \\
\textbf{Scalability and Parallelism} & Scaling efficiency (\% of ideal
speedup); Communication overhead (latency, bandwidth) & Analyzing
distributed training performance for large models \\
\textbf{Resource Utilization} & Compute utilization (\% GPU/TPU usage);
Memory bandwidth (GB/s); I/O efficiency (data loading speed) &
Optimizing data pipelines to improve GPU utilization \\
\textbf{Energy Efficiency and Cost} & Energy consumption per run (MWh,
kWh); Performance per watt (TOPS/W) & Evaluating energy-efficient
training strategies \\
\textbf{Fault Tolerance and Robustness} & Checkpoint overhead (time per
save); Recovery success rate (\%) & Assessing failure recovery in
cloud-based training systems \\
\textbf{Reproducibility and Standardization} & Variance across runs (\%
difference in accuracy, training time); Framework consistency
(TensorFlow vs.~PyTorch vs.~JAX) & Ensuring consistency in benchmark
results across hardware \\
\end{longtable}

Training time and throughput are often the first metrics considered when
evaluating system performance. Time-to-accuracy, the duration required
for a model to achieve a specified accuracy level, is a practical and
widely used benchmark. Throughput, typically measured in samples per
second, provides insight into how efficiently data is processed during
training. For example, when comparing a ResNet-50 model trained on
NVIDIA A100 versus V100 GPUs, the A100 generally offers higher
throughput and faster convergence. However, it is important to ensure
that increased throughput does not come at the expense of convergence
quality, especially when reduced numerical precision (e.g., TF32) is
used to speed up computation.

As model sizes continue to grow, scalability becomes a critical
performance dimension. Efficient use of multiple GPUs or TPUs is
essential for training large models such as GPT-3 or T5. In this
context, scaling efficiency and communication overhead are key metrics.
A system might scale linearly up to 64 GPUs, but beyond that,
performance gains may taper off due to increased synchronization and
communication costs. Benchmarking tools that monitor interconnect
bandwidth and gradient aggregation latency can reveal how well a system
handles distributed training.

Resource utilization complements these measures by examining how
effectively a system leverages its compute and memory resources. Metrics
such as GPU utilization, memory bandwidth, and data loading efficiency
help identify performance bottlenecks. For instance, a BERT pretraining
task that exhibits only moderate GPU utilization may be constrained by
an underperforming data pipeline. Optimizations like sharding input
files or prefetching data into device memory can often resolve these
inefficiencies.

In addition to raw performance, energy efficiency and cost have become
increasingly important considerations. Training large models at scale
can consume significant power, raising environmental and financial
concerns. Metrics such as energy consumed per training run and
performance per watt (e.g., TOPS/W) help evaluate the sustainability of
different hardware and system configurations. For example, while two
systems may reach the same accuracy in the same amount of time, the one
that uses significantly less energy may be preferred for long-term
deployment.

Fault tolerance and robustness address how well a system performs under
non-ideal conditions, which are common in real-world deployments.
Training jobs frequently encounter hardware failures, preemptions, or
network instability. Metrics like checkpoint overhead and recovery
success rate provide insight into the resilience of a training system.
In practice, checkpointing can introduce non-trivial overhead. For
example, pausing training every 30 minutes to write a full checkpoint
may reduce overall throughput by 5-10\%. Systems must strike a balance
between failure recovery and performance impact.

Finally, reproducibility and standardization ensure that benchmark
results are consistent, interpretable, and transferable. Even minor
differences in software libraries, initialization seeds, or
floating-point behavior can affect training outcomes. Comparing the same
model across frameworks, such as comparing PyTorch with Automatic Mixed
Precision to TensorFlow with XLA, can reveal variation in convergence
rates or final accuracy. Reliable benchmarking requires careful control
of these variables, along with repeated runs to assess statistical
variance.

Together, these dimensions provide a holistic view of training
performance. They help researchers, engineers, and system designers move
beyond simplistic comparisons and toward a more nuanced understanding of
how machine learning systems behave under realistic conditions. As
established in our statistical rigor framework earlier, measuring these
dimensions accurately requires systematic methodology that distinguishes
between true performance differences and statistical noise, accounting
for factors like GPU boost clock\sidenote{\textbf{GPU Boost Clock}:
NVIDIA's dynamic frequency scaling technology that automatically
increases GPU core and memory clocks above base frequencies when thermal
and power conditions allow. Boost clocks can increase performance by
10-30\% in cool conditions but decrease under sustained workloads,
causing benchmark variability. For example, RTX 4090 base clock is 2230
MHz but can boost to 2520 MHz when cool. } behavior and thermal
throttling\sidenote{\textbf{Thermal Throttling}: A protection mechanism
that reduces processor frequency when temperatures exceed safe operating
limits (typically 83-90°C for GPUs, 100-105°C for CPUs). Thermal
throttling can reduce performance by 20-50\% during sustained AI
workloads, making thermal management crucial for consistent benchmark
results. Modern systems implement sophisticated thermal monitoring with
temperature sensors every few millimeters across the chip. } that can
significantly impact measurements.

\subsubsection{Training Benchmark
Pitfalls}\label{sec-benchmarking-ai-training-benchmark-pitfalls-8778}

Despite the availability of well-defined benchmarking methodologies,
certain misconceptions and flawed evaluation practices often lead to
misleading conclusions. Understanding these pitfalls is important for
interpreting benchmark results correctly.

\textbf{Overemphasis on Raw Throughput.} A common mistake in training
benchmarks is assuming that higher throughput always translates to
better training performance. It is possible to artificially increase
throughput by using lower numerical precision, reducing synchronization,
or even bypassing certain computations. However, these optimizations do
not necessarily lead to faster convergence.

For example, a system using TF32 precision may achieve higher throughput
than one using FP32, but if TF32 introduces numerical instability that
increases the number of iterations required to reach the target
accuracy, the overall training time may be longer. The correct way to
evaluate throughput is in relation to time-to-accuracy, ensuring that
speed optimizations do not come at the expense of convergence
efficiency.

\textbf{Isolated Single-Node Performance.} Benchmarking training
performance on a single node without considering distributed scaling can
lead to misleading conclusions. A GPU may demonstrate excellent
throughput when used independently, but when deployed in large clusters
like Google's 4,096-node TPU v4 configurations, communication overhead
and synchronization constraints significantly diminish these efficiency
gains.

For instance, a system optimized for single-node performance may employ
memory optimizations that do not generalize to multi-node environments.
Large-scale models such as GPT-3 require efficient gradient
synchronization across thousands of nodes, making comprehensive
scalability assessment essential. Google's experience with 4,096-node
TPU clusters demonstrates that gradient synchronization challenges
become dominant performance factors at this scale.

\textbf{Ignoring Failures and Interference.} Many benchmarks assume an
idealized training environment where hardware failures, memory
corruption, network instability, or interference from other processes do
not occur. However, real-world training jobs often experience unexpected
failures and workload interference that require checkpointing, recovery
mechanisms, and resource management.

A system optimized for ideal-case performance but lacking fault
tolerance and interference handling may achieve impressive benchmark
results under controlled conditions, but frequent failures, inefficient
recovery, and resource contention could make it impractical for
large-scale deployment. Effective benchmarking should consider
checkpointing overhead, failure recovery efficiency, and the impact of
interference from other processes rather than assuming perfect execution
conditions.

\textbf{Linear Scaling Assumption.} When evaluating distributed
training, it is often assumed that increasing the number of GPUs or TPUs
will result in proportional speedups. In practice, communication
bottlenecks, memory contention, and synchronization overheads lead to
diminishing returns as more compute nodes are added.

For example, training a model across 1,000 GPUs does not necessarily
provide 100 times the speed of training on 10 GPUs. At a certain scale,
gradient communication costs become a limiting factor, offsetting the
benefits of additional parallelism. Proper benchmarking should assess
scalability efficiency rather than assuming idealized linear
improvements.

\textbf{Ignoring Reproducibility.} Benchmark results are often reported
without verifying their reproducibility across different hardware and
software frameworks. Even minor variations in floating-point arithmetic,
memory layouts, or optimization strategies can introduce statistical
differences in training time and accuracy.

For example, a benchmark run on TensorFlow with XLA optimizations may
exhibit different convergence characteristics compared to the same model
trained using PyTorch with Automatic Mixed Precision (AMP). Proper
benchmarking requires evaluating results across multiple frameworks to
ensure that software-specific optimizations do not distort performance
comparisons.

Training benchmarks provide valuable insights into machine learning
system performance, but their interpretation requires careful
consideration of real-world constraints. High throughput does not
necessarily mean faster training if it compromises accuracy convergence.
Similarly, scaling efficiency must be evaluated holistically, taking
into account both computational efficiency and communication overhead.

Avoiding common benchmarking pitfalls and employing structured
evaluation methodologies allows practitioners to better optimize
training workflows, design efficient AI systems, and develop scalable
machine learning infrastructure. As models increase in complexity,
benchmarking methodologies must evolve to reflect real-world challenges,
ensuring that benchmarks remain meaningful and actionable.

Training benchmarks validate that hardware can efficiently learn model
parameters, but a model trained efficiently still requires validation of
its deployment performance. The transition from training to inference
evaluation involves fundamental shifts in what matters: metrics change
from time-to-accuracy to latency percentiles, workload patterns shift
from sustained batch processing to variable request arrival, and
optimization targets move from maximizing throughput to maintaining
consistent response times. These differences necessitate separate
benchmarking frameworks tailored to deployment realities.

Where training asks ``how quickly can we reach the target accuracy?''
inference asks an entirely different question: ``how reliably can we
serve predictions under production constraints?'' This shift in
perspective reshapes every aspect of evaluation methodology.

\section{Inference
Benchmarks}\label{sec-benchmarking-ai-inference-benchmarks-2c1f}

Where training benchmarks ask ``how quickly can we learn?'' inference
benchmarks ask ``how reliably can we serve?'' This shift in focus
changes nearly every aspect of evaluation. Training tolerates variable
iteration times as long as convergence proceeds; inference requires
consistent latency because users experience every slow response.
Training optimizes for aggregate throughput across hours or days;
inference must handle unpredictable request patterns with
millisecond-level guarantees. Training typically runs on dedicated
high-performance hardware; inference spans environments from datacenter
GPUs to mobile phones to microcontrollers.

Inference benchmarks validate whether hardware acceleration and model
compression translate to production-ready serving performance. This is
where the optimization chapters converge: the accelerated hardware from
\textbf{?@sec-ai-acceleration} runs compressed models from
\textbf{?@sec-model-compression} to deliver real-time predictions.
Inference benchmarks reveal whether theoretical speedups become actual
latency reductions under realistic deployment conditions.

Unlike training benchmarks that tolerate high latency in exchange for
throughput, inference benchmarks enforce strict latency constraints that
expose system weaknesses: cold-start delays, memory fragmentation,
thermal throttling, and the gap between batch-optimized throughput and
single-request latency. A system achieving excellent training benchmark
scores may fail inference benchmarks if it cannot maintain consistent
sub-10ms response times under variable load.

As deep learning models grow in complexity, efficient inference becomes
a critical challenge, particularly for real-time applications like
autonomous driving, healthcare diagnostics, and conversational AI.
Serving large-scale language models involves handling billions of
parameters while maintaining acceptably low latency. Inference
benchmarks evaluate how well hardware and model optimizations work
together across deployment environments from cloud data centers to edge
devices.

\phantomsection\label{callout-definitionux2a-1.18}
\begin{fbxSimple}{callout-definition}{Definition:}{ML Inference Benchmarks}
\phantomsection\label{callout-definition*-1.18}
\textbf{\emph{ML Inference Benchmarks}} quantify the system's ability to
meet \textbf{Latency Constraints} under load. Unlike training benchmarks
which optimize for pure throughput, inference benchmarks measure the
\textbf{Tail Latency} (p99) and \textbf{Jitter} of the serving stack,
validating its suitability for interactive applications.

\end{fbxSimple}

Unlike training, which is typically conducted in large-scale data
centers with ample computational resources, inference must be optimized
for dramatically diverse deployment scenarios, including mobile devices,
IoT systems, and embedded processors. Efficient inference depends on
multiple interconnected factors, such as optimized data pipelines, model
optimization techniques, and hardware acceleration. Benchmarks help
evaluate how well these optimizations improve real-world deployment
performance.

Building on these optimization requirements, hardware selection plays an
important role in inference efficiency. While GPUs and TPUs are widely
used for training, inference workloads often require specialized
accelerators like NPUs (Neural Processing Units)\sidenote{NPUs
(introduced in \textbf{?@sec-ml-system-architecture}) require different
benchmarking approaches than GPUs. Peak TOPS ratings (1-15 TOPS for
mobile NPUs) vary by precision and operator; meaningful benchmarks must
measure end-to-end model latency rather than synthetic peak throughput.
Power consumption benchmarks are equally critical since NPUs target
100-1000x better energy efficiency than GPUs. },
FPGAs\sidenote{\textbf{Field-Programmable Gate Array (FPGA)}:
Reconfigurable silicon chips that can be programmed after manufacturing
to implement custom digital circuits. Unlike fixed ASICs, FPGAs offer
flexibility to optimize for different algorithms, achieving 10-100x
better energy efficiency than CPUs for specific ML workloads while
maintaining adaptability to algorithm changes. }, and dedicated
inference chips such as Google's Edge TPU\sidenote{\textbf{Edge TPU}:
Google's ultra-low-power AI accelerator designed for edge devices,
consuming only 2 watts while delivering 4 TOPS of performance. Each Edge
TPU is optimized for TensorFlow Lite models and costs around \$25,
making distributed AI deployment economically viable at massive scale.
}. Inference benchmarks evaluate the utilization and performance of
these hardware components, helping practitioners choose the right
configurations for their deployment needs.

Scaling inference workloads across cloud servers, edge platforms, mobile
devices, and tinyML systems introduces additional complexity.
Figure~\ref{fig-power-differentials} reveals the staggering power
consumption differentials among these systems, spanning from microwatts
in tiny embedded devices to megawatts in datacenter training clusters.
Inference benchmarks evaluate the trade-offs between latency, cost, and
energy efficiency, thereby assisting organizations in making informed
deployment decisions.

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{contents/vol1/benchmarking/benchmarking_files/figure-pdf/fig-power-differentials-output-1.pdf}}

}

\caption{\label{fig-power-differentials}\textbf{Power Consumption
Differentials}: Power usage spans six orders of magnitude across ML
system types, from milliwatts in tinyML devices through watts at the
edge to kilowatts in datacenter inference and hundreds of kilowatts for
training clusters. These differentials shape deployment trade-offs
between latency, cost, and energy efficiency.}

\end{figure}%

MLPerf's inference benchmarks provide standardized evaluation across
deployment scenarios from cloud to edge devices.

\subsection{Inference Benchmark
Motivation}\label{sec-benchmarking-ai-inference-benchmark-motivation-eee1}

Deploying machine learning models for inference introduces challenges
distinct from training. While training optimizes large-scale computation
over extensive datasets, inference must deliver predictions efficiently
and at scale in real-world environments. Inference benchmarks evaluate
deployment-specific performance challenges, identifying bottlenecks that
emerge when models transition from development to production serving.

The motivating factors for inference benchmarks parallel those discussed
for training benchmarks (hardware optimization, scalability, cost, and
fair comparison) but differ in their specific concerns. Where training
benchmarks evaluate how well accelerators handle sustained batch
computation, inference benchmarks must assess diverse hardware ranging
from datacenter GPUs to mobile NPUs and embedded processors. Software
optimization frameworks like TensorRT\sidenote{\textbf{TensorRT}:
NVIDIA's high-performance inference optimizer and runtime library that
accelerates deep learning models on NVIDIA GPUs. Introduced in 2016,
TensorRT applies graph optimizations, kernel fusion, and precision
calibration to achieve 1.5-7x speedups over naive implementations,
supporting FP16, INT8, and sparse matrix operations. }, ONNX
Runtime\sidenote{\textbf{ONNX Runtime}: Microsoft's cross-platform,
high-performance ML inferencing and training accelerator supporting the
Open Neural Network Exchange (ONNX) format. Released in 2018, it enables
models trained in any framework to run efficiently across different
hardware (CPU, GPU, NPU) with optimizations like graph fusion and memory
pattern optimization. }, and TVM\sidenote{\textbf{TVM}: An open-source
deep learning compiler stack that optimizes tensor programs for diverse
hardware backends including CPUs, GPUs, and specialized accelerators.
Developed at the University of Washington, TVM uses machine learning to
automatically generate optimized code, achieving performance competitive
with hand-tuned libraries while supporting new hardware architectures. }
apply techniques such as operator fusion\sidenote{\textbf{Operator
Fusion}: A compiler optimization technique that combines multiple neural
network operations into single kernels to reduce memory bandwidth
requirements and improve cache efficiency. For example, fusing
convolution with batch normalization and ReLU can eliminate intermediate
memory writes, achieving 20-40\% speedups in inference workloads. },
numerical precision adjustments, and kernel tuning that are specific to
inference workloads. Benchmarks allow developers to measure the impact
of such techniques on latency, throughput, and power efficiency,
ensuring that optimizations translate into real-world improvements
without degrading model accuracy.

Scalability concerns also shift character. Training scales by adding
GPUs to reduce time-to-accuracy on a fixed workload, whereas inference
must scale dynamically in response to fluctuating user demand, handling
traffic spikes without violating latency guarantees. Cold-start
performance, the time required for a model to load and begin processing
queries, becomes a distinct inference concern with no training analog.
Applications that load models on demand, such as serverless AI
deployments, are particularly sensitive to this overhead.

The cost and energy profile of inference differs fundamentally from
training. Training costs are incurred once and amortized over the
model's lifetime, while inference costs accumulate continuously as
models serve production traffic. Running an inefficient model at scale
can significantly increase cloud compute expenses, and on
battery-powered devices, excessive computation directly impacts
usability. Benchmarks that measure cost per inference request and
efficiency per watt help organizations optimize for both performance and
sustainability across deployment platforms.

MLPerf Inference extends the standardized comparison principles
established for training benchmarks to deployment scenarios, defining
evaluation criteria for tasks such as image classification, object
detection, and speech recognition across different hardware platforms.
This ensures that inference performance comparisons remain meaningful
and reproducible while accounting for deployment-specific constraints
like latency requirements and energy efficiency
(\citeproc{ref-reddi2020mlperf}{Reddi et al. 2019}).

\subsection{Inference
Metrics}\label{sec-benchmarking-ai-inference-metrics-78d4}

Evaluating the performance of inference systems requires a distinct set
of metrics from those used for training. Where the training metrics
established earlier emphasize throughput, scalability, and
time-to-accuracy, inference benchmarks focus on latency consistency,
resource efficiency, and deployment practicality across environments
ranging from cloud data centers handling millions of requests to mobile
and edge devices operating under strict power and memory constraints.
The following metrics capture how efficiently a trained model can
process inputs and generate predictions at scale.

\textbf{Latency and Tail Latency.} Latency (introduced in
\textbf{?@sec-ml-system-architecture}) is one of the most critical
performance metrics for inference, particularly in real-time
applications where delays can negatively impact user experience or
system safety. Latency refers to the time taken for an inference system
to process an input and produce a prediction. While the average latency
of a system is useful, it does not capture performance in high-demand
scenarios where occasional delays can degrade reliability.

To account for this, benchmarks often measure tail
latency\sidenote{\textbf{Tail Latency}: The worst-case response times
(typically 95th or 99th percentile) that determine user experience in
production systems. While average latency might be 50ms, 99th percentile
could be 500ms due to garbage collection, thermal throttling, or
resource contention. Production SLAs are set based on tail latency, not
averages. }, which reflects the worst-case delays in a system. These are
typically reported as the 95th percentile (p95) or 99th percentile (p99)
latency, meaning that 95\% or 99\% of inferences are completed within a
given time. For applications such as autonomous driving or real-time
trading, maintaining low tail latency is essential to avoid
unpredictable delays that could lead to catastrophic outcomes.

These measurements form the basis for Service Level Objectives (SLOs)
and Service Level Agreements (SLAs), which formalize performance
expectations.

\phantomsection\label{callout-definitionux2a-1.19}
\begin{fbxSimple}{callout-definition}{Definition:}{SLO vs. SLA}
\phantomsection\label{callout-definition*-1.19}
\textbf{\emph{Service Level Objective (SLO)}} is the internal
engineering target for reliability, typically defined as a strict upper
bound on \textbf{Tail Latency} or error rate.

\textbf{Service Level Agreement (SLA)} is the external contractual
obligation, typically looser than the SLO, which defines the financial
consequences of violating reliability guarantees.

\end{fbxSimple}

Understanding this distinction is crucial: your engineering team
optimizes for SLOs while your business commits to SLAs. Choosing the
wrong metric to optimize can waste engineering effort or violate
customer guarantees.

\phantomsection\label{callout-checkpointux2a-1.20}
\begin{fbxSimple}{callout-checkpoint}{Checkpoint:}{Metric Selection}
\phantomsection\label{callout-checkpoint*-1.20}

The metric shapes the optimization.

\textbf{The Golden Rules}

\begin{itemize}
\tightlist
\item[$\square$]
  \textbf{Throughput vs.~Latency}: Are you optimizing for cost
  (Throughput) or user experience (Latency)? You cannot maximize both
  simultaneously.
\item[$\square$]
  \textbf{Tail Latency}: Do you measure p99? (Averages hide the failures
  that drive users away).
\item[$\square$]
  \textbf{End-to-End}: Does your ``inference latency'' include
  preprocessing? (If not, your benchmark is a lie).
\end{itemize}

\end{fbxSimple}

Tail latency's connection to user experience at scale becomes critical
in production systems serving millions of users. Even small P99 latency
degradations create compounding effects across large user bases: if 1\%
of requests experience 10x latency (e.g., 1000ms instead of 100ms), this
affects 10,000 users per million requests, potentially leading to
timeout errors, poor user experience, and customer churn. Search engines
and recommendation systems demonstrate this sensitivity: industry
studies have shown that latency increases on the order of hundreds of
milliseconds can reduce engagement by 10-20\% and conversions by
measurable percentages, making sub-100ms response times a common target
for interactive services.

Service level objectives (SLOs) in production systems therefore focus on
tail latency rather than mean latency to ensure consistent user
experience. Typical production SLOs specify P95 \textless{} 100ms and
P99 \textless{} 500ms for interactive services, recognizing that
occasional slow responses have disproportionate impact on user
satisfaction. Large-scale systems like Netflix and Uber optimize for
P99.9 latency to handle traffic spikes and infrastructure variations
that affect service reliability.

\textbf{End-to-End vs.~Component Latency.} A critical distinction in
inference benchmarking is between component latency (time spent in model
computation) and end-to-end latency (total time from request arrival to
response delivery). Many benchmarks report only model inference time,
obscuring significant overhead that determines actual user experience.

Consider Table~\ref{tbl-latency-breakdown}, which quantifies a typical
latency breakdown for an inference request:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3768}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2464}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3768}}@{}}
\caption{\textbf{Inference Latency Breakdown.} Different components
contribute to end-to-end latency, with model inference often
representing only a fraction of total request
time.}\label{tbl-latency-breakdown}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Component}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Typical Range}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Notes}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Component}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Typical Range}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Notes}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Network round-trip} & 10-100 ms & Varies by region \\
\textbf{Request parsing} & 0.1-1 ms & JSON/protobuf \\
\textbf{Input preprocessing} & 1-50 ms & Tokenization, image resize \\
\textbf{Queue wait time} & 0-1000+ ms & Load-dependent \\
\textbf{Model inference} & 5-100 ms & The ``benchmark'' \\
\textbf{Output postprocessing} & 0.5-10 ms & Decoding, format \\
\textbf{Response serialization} & 0.1-1 ms & JSON/protobuf \\
\end{longtable}

\textbf{Implications for benchmarking:}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Preprocessing dominance}: For vision models, image decoding
  and resizing can exceed model inference time, especially for
  high-resolution inputs
\item
  \textbf{Queue effects}: Under load, queuing delay often dominates,
  making isolated model benchmarks misleading
\item
  \textbf{Network realities}: Edge deployment benchmarks should include
  network latency to cloud fallback scenarios
\item
  \textbf{First-token vs.~total latency}: For LLMs, first-token latency
  (time to generate the first output token) differs significantly from
  total generation time
\end{enumerate}

These component-level contributions explain why optimizing any single
stage yields diminishing returns on end-to-end performance, an
\emph{optimization ceiling} formalized by \textbf{Amdahl's Law}.

\phantomsection\label{callout-notebookux2a-1.21}
\begin{fbxSimple}{callout-notebook}{AI Engineer’s Notebook:}{Amdahl's Law: Optimization Ceiling}
\phantomsection\label{callout-notebook*-1.21}
The latency breakdown reveals why aggressive model optimization often
yields disappointing end-to-end results. Consider a vision pipeline
where preprocessing (JPEG decode, resize, normalize) consumes 8ms and
inference consumes 10ms. Optimizing inference by 5x (from 10ms to 2ms)
reduces total latency from 18ms to only 10ms, a 1.8x improvement rather
than 5x.

Amdahl's Law formalizes this ceiling: if preprocessing consumes fraction
\(f\) of total latency, then even infinitely fast inference yields at
most \(1/f\) speedup. With preprocessing at 45\% of latency (\(f\) =
0.45), the maximum achievable speedup is \(1/f\) ≈ 2.2× regardless of
model optimization.

This principle has direct implications for benchmarking interpretation.
A 3× inference speedup reported in isolation might translate to only
1.5× end-to-end improvement in production. Comprehensive benchmarks must
either include preprocessing in measurements or clearly state that
reported speedups apply only to the inference component.

\end{fbxSimple}

These mathematical constraints highlight why rigorous benchmarking
methodology matters. Before interpreting any benchmark result, verify
that the measurement approach itself is sound.

\phantomsection\label{callout-checkpointux2a-1.22}
\begin{fbxSimple}{callout-checkpoint}{Checkpoint:}{Benchmarking Methodology}
\phantomsection\label{callout-checkpoint*-1.22}

Bad benchmarks optimize the wrong things.

\textbf{Best Practices}

\begin{itemize}
\tightlist
\item[$\square$]
  \textbf{Representative Data}: Does your benchmark use real data
  distributions, or ``clean'' academic datasets? (Data determines
  performance).
\item[$\square$]
  \textbf{Warm-up}: Did you discard the first 50 runs? (JIT compilation
  and caching skew initial results).
\item[$\square$]
  \textbf{Isolation}: Are you running on a dedicated machine? (Noisy
  neighbors invalidate benchmarks).
\end{itemize}

\end{fbxSimple}

Comprehensive latency reporting therefore requires specifying which
components are included, measuring under realistic load conditions, and
distinguishing component from end-to-end metrics.

\textbf{Throughput and Batch Efficiency.} While latency measures the
speed of individual inference requests, throughput measures how many
inference requests a system can process per second. It is typically
expressed in queries per second (QPS) or frames per second (FPS) for
vision tasks. Some inference systems operate on a single-instance basis,
where each input is processed independently as soon as it arrives. Other
systems process multiple inputs in parallel using batch inference, which
can significantly improve efficiency by leveraging hardware
optimizations.

For example, cloud-based services handling millions of queries per
second benefit from batch inference, where large groups of inputs are
processed together to maximize computational efficiency. In contrast,
applications like robotics, interactive AI, and augmented reality
require low-latency single-instance inference, where the system must
respond immediately to each new input.

Benchmarks must consider both single-instance and batch throughput to
provide a comprehensive understanding of inference performance across
different deployment scenarios.

\textbf{Precision and Accuracy Trade-offs.} Optimizing inference
performance often involves reducing numerical precision, which can
significantly accelerate computation while reducing memory and energy
consumption. However, lower-precision calculations can introduce
accuracy degradation, making it essential to benchmark the trade-offs
between speed and predictive quality.

Inference benchmarks evaluate how well models perform under different
numerical settings, such as FP32\sidenote{\textbf{FP32}: 32-bit
floating-point format providing high numerical precision with
approximately 7 decimal digits of accuracy. Standard for research and
training, FP32 operations consume maximum memory and computational
resources but ensure numerical stability. Modern GPUs achieve 15-20
TFLOPS in FP32, serving as the baseline for precision comparisons. },
FP16\sidenote{\textbf{FP16}: 16-bit floating-point format that halves
memory usage compared to FP32 while maintaining reasonable numerical
precision. Widely supported by modern AI accelerators, FP16 can achieve
2-4x speedups over FP32 with minimal accuracy loss for most deep
learning models, making it the preferred format for inference and
mixed-precision training. }, and INT8\sidenote{\textbf{INT8}: 8-bit
integer format providing maximum memory and computational efficiency,
requiring only 25\% of FP32 storage. Post-training precision reduction
to INT8 can achieve 4x memory reduction and 2-4x speedup on specialized
hardware, but requires careful calibration to minimize accuracy
degradation, typically maintaining 95-99\% of original model
performance. }. Many modern AI accelerators support mixed-precision
inference, allowing systems to dynamically adjust numerical
representation based on workload requirements. Model compression
techniques\sidenote{Model compression (detailed in
\textbf{?@sec-model-compression}) encompasses precision reduction,
structural optimization, and knowledge transfer. For benchmarking,
compression impact must be measured across multiple dimensions: accuracy
degradation, inference speedup, memory reduction, and energy savings. A
technique achieving 10x size reduction with only 1\% accuracy loss may
still be unsuitable if latency does not improve proportionally. }
further improve efficiency, but their impact on model accuracy varies
depending on the task and dataset. Benchmarks help determine whether
these optimizations are viable for deployment, ensuring that
improvements in efficiency do not come at the cost of unacceptable
accuracy loss.

\textbf{Memory Footprint and Model Size.} Beyond computational
optimizations, memory footprint is another critical consideration for
inference systems, particularly for devices with limited resources.
Efficient inference depends not only on speed but also on memory usage.
Unlike training, where large models can be distributed across powerful
GPUs or TPUs, inference often requires models to run within strict
memory budgets. The total model size determines how much storage is
required for deployment, while RAM usage reflects the working memory
needed during execution. Some models require large memory bandwidth to
efficiently transfer data between processing units, which can become a
bottleneck if the hardware lacks sufficient capacity.

Inference benchmarks evaluate these factors to ensure that models can be
deployed effectively across a range of devices. A model that achieves
high accuracy but exceeds memory constraints may be impractical for
real-world use. To address this, various compression techniques are
often applied to reduce model size while maintaining accuracy.
Benchmarks help assess whether these optimizations strike the right
balance between memory efficiency and predictive performance.

\textbf{Cold-Start and Model Load Time.} Once memory requirements are
optimized, cold-start performance becomes critical for ensuring
inference systems are ready to respond quickly upon deployment. In many
deployment scenarios, models are not always kept in memory but instead
loaded on demand when needed. This can introduce significant delays,
particularly in serverless AI environments\sidenote{\textbf{Serverless
AI}: Cloud computing paradigm where ML models are deployed as functions
that automatically scale from zero to handle incoming requests, with
users paying only for actual inference time. Serverless platforms can
support ML inference, but cold-start latencies can be significant for
large models and may impact user experience compared to always-on
deployments. }, where resources are allocated dynamically based on
incoming requests. Cold-start performance measures how quickly a system
can transition from idle to active execution, ensuring that inference is
available without excessive wait times.

Model load time refers to the duration required to load a trained model
into memory before it can process inputs. In some cases, particularly on
resource-limited devices, models must be reloaded frequently to free up
memory for other applications. The time taken for the first inference
request is also an important consideration, as it reflects the total
delay users experience when interacting with an AI-powered service.
Benchmarks help quantify these delays, ensuring that inference systems
can meet real-world responsiveness requirements.

\textbf{Dynamic Workload Scaling.} While cold-start latency addresses
initial responsiveness, scalability ensures that inference systems can
handle fluctuating workloads and concurrent demands over time. Inference
workloads must scale effectively across different usage patterns. In
cloud-based AI services, this means efficiently handling millions of
concurrent users, while on mobile or embedded devices, it involves
managing multiple AI models running simultaneously without overloading
the system.

Scalability measures how well inference performance improves when
additional computational resources are allocated. In some cases, adding
more GPUs or TPUs increases throughput significantly, but in other
scenarios, bottlenecks such as memory bandwidth limitations or network
latency may limit scaling efficiency. Benchmarks also assess how well a
system balances multiple concurrent models in real-world deployment,
where different AI-powered features may need to run at the same time
without interference.

For cloud-based AI, benchmarks evaluate how efficiently a system handles
fluctuating demand, ensuring that inference servers can dynamically
allocate resources without compromising latency. In mobile and embedded
AI, efficient multi-model execution is essential for running multiple
AI-powered features simultaneously without degrading system performance.

\textbf{Energy Consumption and Efficiency.} Since inference workloads
run continuously in production, power consumption and energy efficiency
are critical considerations. Mobile and edge devices face the most acute
constraints, where battery life and thermal limits restrict available
computational resources. Even in large-scale cloud environments, power
efficiency directly impacts operational costs and sustainability goals.

The energy required for a single inference is often measured in joules
per inference, reflecting how efficiently a system processes inputs
while minimizing power draw. In cloud-based inference, efficiency is
commonly expressed as queries per second per watt (QPS/W) to quantify
how well a system balances performance and energy consumption. For
mobile AI applications, optimizing inference power consumption extends
battery life and allows models to run efficiently on
resource-constrained devices. Reducing energy use also plays a key role
in making large-scale AI systems more environmentally sustainable,
ensuring that computational advancements align with energy-conscious
deployment strategies.

\subsection{Inference Performance
Evaluation}\label{sec-benchmarking-ai-inference-performance-evaluation-6793}

Evaluating inference performance is a critical step in understanding how
well machine learning systems meet the demands of real-world
applications. Unlike training, which is typically conducted offline,
inference systems must process inputs and generate predictions
efficiently across a wide range of deployment scenarios. Metrics such as
latency, throughput, memory usage, and energy efficiency provide a
structured way to measure system performance and identify areas for
improvement.

Table~\ref{tbl-inference-metrics} highlights key metrics for evaluating
inference systems and their relevance to different deployment contexts.
While each metric offers unique insights, it is important to approach
inference benchmarking holistically. Trade-offs between metrics,
including speed versus accuracy and throughput versus power consumption,
are common, and understanding these trade-offs is essential for
effective system design.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2000}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4387}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3613}}@{}}
\caption{\textbf{Inference Performance Metrics.} Latency, throughput,
and resource usage metrics provide a quantitative basis for optimizing
deployed machine learning systems and selecting appropriate hardware
configurations, balancing speed, cost, and accuracy in production
applications.}\label{tbl-inference-metrics}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Category}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Key Metrics}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Example Benchmark Use}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Category}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Key Metrics}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Example Benchmark Use}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Latency and Tail Latency} & Mean latency (ms/request); Tail
latency (p95, p99, p99.9) & Evaluating real-time performance for
safety-critical AI \\
\textbf{Throughput and Efficiency} & Queries per second (QPS); Frames
per second (FPS); Batch throughput & Comparing large-scale cloud
inference systems \\
\textbf{Numerical Precision Impact} & Accuracy degradation (FP32
vs.~INT8); Speedup from reduced precision & Balancing accuracy
vs.~efficiency in optimized inference \\
\textbf{Memory Footprint} & Model size (MB/GB); RAM usage (MB); Memory
bandwidth utilization & Assessing feasibility for edge and mobile
deployments \\
\textbf{Cold-Start and Load Time} & Model load time (s); First inference
latency (s) & Evaluating responsiveness in serverless AI \\
\textbf{Scalability} & Efficiency under load; Multi-model serving
performance & Measuring robustness for dynamic, high-demand systems \\
\textbf{Power and Energy Efficiency} & Power consumption (Watts);
Performance per Watt (QPS/W) & Optimizing energy use for mobile and
sustainable AI \\
\end{longtable}

\textbf{Inference Systems Considerations.} Inference systems face unique
challenges depending on where and how they are deployed. Real-time
applications, such as self-driving cars or voice assistants, require low
latency to ensure timely responses, while large-scale cloud deployments
focus on maximizing throughput to handle millions of queries. Edge
devices, on the other hand, are constrained by memory and power, making
efficiency critical.

Understanding the trade-offs between metrics is central to evaluating
inference performance. For example, optimizing for high throughput might
increase latency, making a system unsuitable for real-time applications.
Similarly, reducing numerical precision improves power efficiency and
speed but may lead to minor accuracy degradation. A thoughtful
evaluation must balance these trade-offs to align with the intended
application.

The deployment environment also plays a significant role in determining
evaluation priorities. Cloud-based systems often prioritize scalability
and adaptability to dynamic workloads, while mobile and edge systems
require careful attention to memory usage and energy efficiency. These
differing priorities mean that benchmarks must be tailored to the
context of the system's use, rather than relying on one-size-fits-all
evaluations.

Ultimately, evaluating inference performance requires a holistic
approach. Focusing on a single metric, such as latency or energy
efficiency, provides an incomplete picture. Instead, all relevant
dimensions must be considered together to ensure that the system meets
its functional, resource, and performance goals in a balanced way.

\textbf{Context-Dependent Metrics.} Different deployment scenarios
require distinctly different metric priorities, as the operational
constraints and success criteria vary dramatically across contexts.
Understanding these priorities allows engineers to focus benchmarking
efforts effectively and interpret results within appropriate decision
frameworks. Table~\ref{tbl-metric-priorities} illustrates how
performance priorities shift across five major deployment contexts,
revealing the systematic relationship between operational constraints
and optimization targets.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.1884}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.1449}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.1594}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.1522}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.3551}}@{}}
\caption{\textbf{Performance Metric Priorities by Deployment Context.}
Different operational environments demand distinct optimization focuses,
reflecting varying constraints and success criteria. These priorities
guide both benchmark selection and result
interpretation.}\label{tbl-metric-priorities}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Deployment Context}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Primary Priority}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Secondary Priority}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Tertiary Priority}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Key Design Constraint}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Deployment Context}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Primary Priority}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Secondary Priority}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Tertiary Priority}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Key Design Constraint}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Real-Time Applications} & Latency (p95 \textless{} 50ms) &
Reliability (99.9\%) & Memory Footprint & User experience demands
immediate response \\
\textbf{Cloud-Scale Services} & Throughput (QPS) & Cost Efficiency &
Average Latency & Business viability requires massive scale \\
\textbf{Edge/Mobile Devices} & Power Consumption & Memory Footprint &
Latency & Battery life and resource limits dominate \\
\textbf{Training Workloads} & Training Time & GPU Utilization & Memory
Efficiency & Research velocity enables faster experimentation \\
\textbf{Scientific/Medical} & Accuracy & Reliability & Explainability &
Correctness cannot be compromised for performance \\
\end{longtable}

Operational constraints drive performance optimization strategies
through a clear hierarchy. As Table~\ref{tbl-metric-priorities} reveals,
real-time applications exemplify latency-critical deployments where user
experience depends on immediate system response. Autonomous vehicle
perception systems must process sensor data within strict timing
deadlines, making p95 latency more important than peak throughput. The
table shows reliability as the secondary priority because system
failures in autonomous vehicles carry safety implications that transcend
performance concerns.

Conversely, cloud-scale services prioritize aggregate throughput to
handle millions of concurrent users, accepting higher average latency in
exchange for improved cost efficiency per query. The progression from
throughput to cost efficiency to latency reflects economic realities:
cloud providers must optimize for revenue per server while maintaining
acceptable user experience. Notice how the same metric (latency) ranks
as primary for real-time applications but tertiary for cloud services,
demonstrating the context-dependent nature of performance evaluation.

Edge and mobile deployments face distinctly different constraints, where
battery life and thermal limitations dominate design decisions. A
smartphone AI assistant that improves throughput by 50\% but increases
power consumption by 30\% represents a net regression, as reduced
battery life directly impacts user satisfaction. Training workloads
present another distinct optimization landscape, where research
productivity depends on experiment turnaround time, making GPU
utilization efficiency and memory bandwidth critical for enabling larger
model exploration.

Scientific and medical applications establish accuracy and reliability
as non-negotiable requirements, with performance optimization serving
these primary objectives rather than substituting for them. A medical
diagnostic system achieving 99.2\% accuracy at 10ms latency provides
superior value compared to 98.8\% accuracy at 5ms latency, demonstrating
how context-specific priorities guide meaningful performance evaluation.

This prioritization framework fundamentally shapes benchmark
interpretation and optimization strategies. Achieving 2x throughput
improvement represents significant value for cloud deployments but
provides minimal benefit for battery-powered edge devices where 20\%
power reduction delivers superior operational impact.

\subsubsection{Inference Benchmark
Pitfalls}\label{sec-benchmarking-ai-inference-benchmark-pitfalls-1962}

Even with well-defined metrics, benchmarking inference systems can be
challenging. Missteps during the evaluation process often lead to
misleading conclusions. Students and practitioners should be aware of
common pitfalls when analyzing inference performance.

\textbf{Overemphasis on Average Latency.}

While average latency provides a baseline measure of response time, it
fails to capture how a system performs under peak load. In real-world
scenarios, worst-case latency, which is captured through metrics such as
p95\sidenote{\textbf{P95 Latency}: The 95th percentile latency
measurement, meaning 95\% of requests complete within this time while
5\% take longer. For example, if p95 latency is 100ms, then 19 out of 20
requests finish within 100ms. P95 is widely used in SLA agreements
because it captures typical user experience while acknowledging that
some requests will naturally take longer due to system variability. } or
p99\sidenote{\textbf{P99 Latency}: The 99th percentile latency
measurement, indicating that 99\% of requests complete within this time
while only 1\% experience longer delays. P99 latency is crucial for
user-facing applications where even rare slow responses significantly
impact user satisfaction. For instance, if a web service handles 1
million requests daily, p99 latency determines the experience for 10,000
users. } tail latency, can significantly impact system reliability. For
instance, a conversational AI system may fail to provide timely
responses if occasional latency spikes exceed acceptable thresholds.

\textbf{Ignoring Memory and Energy Constraints.} A model with excellent
throughput or latency may be unsuitable for mobile or edge deployments
if it requires excessive memory or power. For example, an inference
system designed for cloud environments might fail to operate efficiently
on a battery-powered device. Proper benchmarks must consider memory
footprint and energy consumption to ensure practicality across
deployment contexts.

\textbf{Ignoring Cold-Start Performance.} In serverless environments,
where models are loaded on demand, cold-start
latency\sidenote{\textbf{Cold-Start Latency}: The initialization time
required when a system or service starts from a completely idle state,
including time to load libraries, initialize models, and allocate
memory. In serverless AI deployments, cold-start latencies range from
100ms for simple models to 10+ seconds for large language models,
significantly impacting user experience compared to warm instances that
respond in milliseconds. } is a critical factor. Ignoring the time it
takes to initialize a model and process the first request can result in
unrealistic expectations for responsiveness. Evaluating both model load
time and first-inference latency ensures that systems are designed to
meet real-world responsiveness requirements.

\textbf{Isolated Metrics Evaluation.} Benchmarking inference systems
often involves balancing competing metrics. For example, maximizing
batch throughput might degrade latency, while aggressive precision
reduction could reduce accuracy. Focusing on a single metric without
considering its impact on others can lead to incomplete or misleading
evaluations.

Numerical precision optimization exemplifies this challenge particularly
well. Individual accelerator benchmarks show INT8 operations achieving
4x higher TOPS\sidenote{\textbf{TOPS (Tera Operations Per Second)}: A
measure of computational throughput indicating trillions of operations
per second, commonly used for AI accelerator performance. Modern AI
chips achieve 100-1000 TOPS for INT8 operations: NVIDIA H100 delivers
2000 TOPS INT8, Apple M2 Neural Engine provides 15.8 TOPS, while edge
devices like Google Edge TPU achieve 4 TOPS. Higher TOPS enable faster
AI inference and training. However, when these accelerators deploy in
complete training systems, the chip-level advantage often disappears due
to increased convergence time, precision conversion overhead, and
mixed-precision coordination complexity. The ``4x faster''
micro-benchmark translates into slower end-to-end training,
demonstrating why isolated hardware metrics cannot substitute for
holistic system evaluation. Balanced approaches like FP16
mixed-precision often provide superior system-level performance despite
lower peak TOPS measurements. Comprehensive benchmarks must account for
these cross-metric interactions and system-level complexities. } (Tera
Operations Per Second) compared to FP32, creating compelling performance
narratives.

\textbf{Linear Scaling Assumption.} The linear scaling pitfall discussed
for training benchmarks applies equally to inference, though the
bottlenecks differ. Where training scaling is limited primarily by
gradient synchronization overhead, inference scaling encounters
bottlenecks from memory bandwidth saturation, thermal throttling under
sustained load, and request routing overhead in distributed serving. As
discussed in \textbf{?@sec-ai-acceleration}, these limitations arise
from fundamental hardware constraints and interconnect architectures.
Benchmarks that assume linear scaling behavior may overestimate system
performance, particularly in distributed deployments.

\textbf{Ignoring Application Requirements.} Generic benchmarking results
may fail to account for the specific needs of an application. For
instance, a benchmark optimized for cloud inference might be irrelevant
for edge devices, where energy and memory constraints dominate.
Tailoring benchmarks to the deployment context ensures that results are
meaningful and actionable.

\textbf{Statistical Significance and Noise.} Distinguishing meaningful
performance improvements from measurement noise requires proper
statistical analysis. Following the evaluation methodology principles
established earlier, MLPerf addresses measurement variability by
requiring multiple benchmark runs and reporting percentile-based metrics
rather than single measurements (\citeproc{ref-reddi2020mlperf}{Reddi et
al. 2019}). For instance, MLPerf Inference reports 99th percentile
latency alongside mean performance, capturing both typical behavior and
worst-case scenarios that single-run measurements might miss. This
approach recognizes that system performance naturally varies due to
factors like thermal throttling, memory allocation patterns, and
background processes.

Inference benchmarks are essential tools for understanding system
performance, but their utility depends on careful and holistic
evaluation. Metrics like latency, throughput, memory usage, and energy
efficiency provide valuable insights, but their importance varies
depending on the application and deployment context. Students should
approach benchmarking as a process of balancing multiple priorities,
rather than optimizing for a single metric.

Avoiding common pitfalls and considering the trade-offs between
different metrics allows practitioners to design inference systems that
are reliable, efficient, and suitable for real-world deployment. The
ultimate goal of benchmarking is to guide system improvements that align
with the demands of the intended application.

\subsection{MLPerf Inference
Benchmarks}\label{sec-benchmarking-ai-mlperf-inference-benchmarks-e878}

The MLPerf Inference benchmark, developed by
MLCommons\sidenote{\textbf{MLCommons}: Non-profit consortium (founded
2018 as MLPerf) establishing industry-standard ML benchmarks. Members
include Google, NVIDIA, Intel, and leading universities. MLPerf Training
measures time-to-accuracy; MLPerf Inference measures throughput/latency.
Results reveal 10× performance differences between vendors, driving
competitive innovation while enabling apples-to-apples hardware
comparisons. }, provides a standardized framework for evaluating machine
learning inference performance across a range of deployment
environments. Initially, MLPerf started with a single inference
benchmark, but as machine learning systems expanded into diverse
applications, it became clear that a one-size-fits-all benchmark was
insufficient. Different inference scenarios, including cloud-based AI
services and resource-constrained embedded devices, demanded tailored
evaluations. This realization led to the development of a family of
MLPerf inference benchmarks, each designed to assess performance within
a specific deployment setting.

\textbf{MLPerf Inference.} MLPerf Inference
(\citeproc{ref-mlperf_inference_website}{MLCommons 2024b}) serves as the
baseline benchmark, originally designed to evaluate large-scale
inference systems. It primarily focuses on data center and cloud-based
inference workloads, where high throughput, low latency, and efficient
resource utilization are essential. The benchmark assesses performance
across a range of deep learning models, including image classification,
object detection, natural language processing, and recommendation
systems. This version of MLPerf is a widely used reference point for
comparing AI accelerators, GPUs, TPUs, and CPUs in high-performance
computing environments.

Major technology companies regularly reference MLPerf results for
hardware procurement decisions. When evaluating hardware for
recommendation systems infrastructure, MLPerf benchmark scores on
DLRM\sidenote{\textbf{DLRM (Deep Learning Recommendation Model)}:
Facebook's neural network architecture for personalized recommendations,
released in 2019, combining categorical features through embedding
tables with continuous features through multi-layer perceptrons. DLRM
models can contain 100+ billion parameters with embedding tables
consuming terabytes of memory, requiring specialized hardware
optimization for the sparse matrix operations that dominate
recommendation system workloads. } (Deep Learning Recommendation Model)
workloads can inform choices between different accelerator generations.
Across generations, benchmark results often show substantial throughput
improvements, although the magnitude depends on workload, software
stack, and system configuration. This illustrates how standardized
benchmarks can translate into consequential infrastructure decisions.

These standardized evaluations provide invaluable comparisons, but the
cost of comprehensive benchmarking limits who can participate and how
thoroughly systems are evaluated.

\phantomsection\label{callout-perspectiveux2a-1.23}
\begin{fbxSimple}{callout-perspective}{Systems Perspective:}{The Cost of Comprehensive Benchmarking}
\phantomsection\label{callout-perspective*-1.23}
While benchmarking is essential for ML system development, it comes with
substantial costs that limit participation to well-resourced
organizations. Submitting to MLPerf can require significant engineering
effort and significant hardware and cloud compute time. A comprehensive
MLPerf Training submission can involve months of engineering time for
optimization, tuning, and validation across multiple hardware
configurations, and can require compute budgets that reach six figures
in dollars depending on the scope.

This cost barrier explains why MLPerf submissions are dominated by major
technology companies and hardware vendors, while smaller organizations
rely on published results rather than conducting their own comprehensive
evaluations. The high barrier to entry motivates the need for more
lightweight, internal benchmarking practices that organizations can use
to make informed decisions without the expense of full-scale
standardized benchmarking.

\end{fbxSimple}

\textbf{MLPerf Mobile.} MLPerf Mobile
(\citeproc{ref-mlperf_mobile_website}{MLCommons 2024c}) extends MLPerf's
evaluation framework to smartphones and other mobile devices. Unlike
cloud-based inference, mobile inference operates under strict power and
memory constraints, requiring models to be optimized for efficiency
without sacrificing responsiveness. The benchmark measures latency and
responsiveness for real-time AI tasks, such as camera-based scene
detection, speech recognition, and augmented reality applications.
MLPerf Mobile has become an industry standard for assessing AI
performance on flagship smartphones and mobile AI chips, helping
developers optimize models for on-device AI workloads.

\textbf{MLPerf Client.} MLPerf Client
(\citeproc{ref-mlperf_client_website}{MLCommons 2024a}) focuses on
inference performance on consumer computing devices, such as laptops,
desktops, and workstations. This benchmark addresses local AI workloads
that run directly on personal devices, eliminating reliance on cloud
inference. Tasks such as real-time video editing, speech-to-text
transcription, and AI-enhanced productivity applications fall under this
category. Unlike cloud-based benchmarks, MLPerf Client evaluates how AI
workloads interact with general-purpose hardware, such as CPUs, discrete
GPUs, and integrated Neural Processing Units (NPUs), making it relevant
for consumer and enterprise AI applications.

\textbf{MLPerf Tiny.} MLPerf Tiny
(\citeproc{ref-mlperf_tiny_website}{MLCommons 2024e}) was created to
benchmark embedded and ultra-low-power AI systems, such as IoT devices,
wearables, and microcontrollers. Unlike other MLPerf benchmarks, which
assess performance on powerful accelerators, MLPerf Tiny evaluates
inference on devices with limited compute, memory, and power resources.
This benchmark is particularly relevant for applications such as smart
sensors, AI-driven automation, and real-time industrial monitoring,
where models must run efficiently on hardware with minimal processing
capabilities. MLPerf Tiny helps developers optimize models for
constrained environments and has become the standard benchmark for edge
AI performance.

\textbf{MLPerf Execution Scenarios.} Beyond hardware platforms, MLPerf
defines four execution scenarios that characterize how inference
requests arrive. These scenarios capture fundamentally different traffic
patterns that require distinct optimization strategies, connecting
benchmarking methodology directly to deployment reality.

\textbf{SingleStream} processes one request at a time, measuring latency
for sequential inference. This scenario models mobile and embedded
applications where a single user interacts with the device: a smartphone
camera app classifying images, a voice assistant processing speech, or a
wearable detecting gestures. The key metric is per-request latency, and
batching provides no benefit since requests arrive only after the
previous result is consumed. Optimization focuses on preprocessing
efficiency and power consumption rather than throughput.

\textbf{MultiStream} processes multiple synchronized input streams
simultaneously, modeling scenarios like autonomous vehicles with
multiple cameras that must be processed together for spatial fusion.
Unlike SingleStream's sequential requests, MultiStream requires
processing frames from all sensors within tight timing deadlines
(typically 33ms for 30 FPS). The key constraint is synchronization: all
streams must complete before the planning module can act. Optimization
focuses on jitter handling and meeting hard deadlines rather than
average throughput.

\textbf{Server} generates requests following a Poisson distribution,
simulating cloud API traffic where requests arrive independently and
unpredictably. This scenario models web services handling millions of
queries from different users. Unlike SingleStream's guaranteed
sequential arrival, Server traffic creates queuing dynamics where
multiple requests compete for resources. The key metrics are throughput
(queries per second) and tail latency (p99), and dynamic batching can
improve efficiency by grouping requests that arrive within a time
window. Optimization balances throughput against latency SLOs.

\textbf{Offline} provides all inputs upfront, measuring maximum
throughput when latency constraints are removed. This scenario models
batch processing pipelines: overnight data processing, scientific
computing, or pre-computing recommendations. With no latency
requirement, systems can use maximum batch sizes to saturate hardware
utilization. The key metric is pure throughput (samples per second), and
optimization focuses entirely on hardware efficiency.

Table~\ref{tbl-mlperf-scenarios} maps these execution scenarios to their
deployment contexts and optimization strategies:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1379}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3017}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3103}}@{}}
\caption{\textbf{MLPerf Execution Scenarios.} The four MLPerf inference
scenarios map to distinct deployment contexts, each requiring different
optimization strategies. SingleStream and MultiStream prioritize
latency, Server balances throughput and latency, and Offline maximizes
throughput. Matching the scenario to deployment context determines which
benchmark results are
relevant.}\label{tbl-mlperf-scenarios}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Scenario}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Context}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Strategy}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Focus}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Scenario}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Context}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Strategy}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Focus}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{SingleStream} & Mobile apps, embedded devices & No batching
(batch=1) & Preprocessing, power efficiency \\
\textbf{MultiStream} & Autonomous driving, video analytics &
Synchronized sensor fusion & Jitter handling, deadline guarantees \\
\textbf{Server} & Cloud APIs, web services & Dynamic batching with
timeout & Throughput-latency tradeoff tuning \\
\textbf{Offline} & Batch processing, data pipelines & Maximum batch size
& Throughput, hardware utilization \\
\end{longtable}

These scenarios explain why the same hardware can report dramatically
different benchmark numbers. An accelerator achieving 10,000
samples/second in Offline mode might achieve only 200 queries/second in
Server mode with p99 latency constraints, because Server mode includes
queuing overhead and cannot use maximum batch sizes. When evaluating
hardware for a specific application, selecting the appropriate scenario
ensures benchmark results predict production performance. To demonstrate
scenario-based validation concretely, we return to our \emph{MobileNet
on EdgeTPU} lighthouse.

\phantomsection\label{callout-lighthouseux2a-1.24}
\begin{fbxSimple}{callout-lighthouse}{Lighthouse:}{MobileNet on EdgeTPU}
\phantomsection\label{callout-lighthouse*-1.24}
Completing our MobileNet lighthouse example, we validate the hardware
acceleration claims from \textbf{?@sec-ai-acceleration} using MLPerf
Tiny scenarios.

\emph{Note: The following values are illustrative, based on typical
EdgeTPU and Cortex-M7 performance characteristics. Actual results vary
with clock frequency, thermal conditions, and specific implementation.
Always benchmark your specific configuration.}

\textbf{Hardware acceleration claim}: EdgeTPU achieves
\textasciitilde2ms inference for INT8 MobileNetV2, approximately 8×
speedup over ARM Cortex-M7 CPU (\textasciitilde15ms).

\textbf{Validation protocol} (SingleStream scenario):

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.1364}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.1818}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2045}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2273}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2500}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Metric}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{CPU (Cortex-M7)}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{EdgeTPU}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Claimed}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Validated?}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Inference latency} & \textasciitilde15ms & \textasciitilde2ms &
8× faster & ✓ \\
\textbf{End-to-end latency} & \textasciitilde18ms & \textasciitilde6ms &
--- & \textasciitilde3× faster \\
\textbf{Power consumption} & \textasciitilde120mW & \textasciitilde500mW
& --- & \textasciitilde4× higher \\
\textbf{Energy per inference} & \textasciitilde1.8mJ &
\textasciitilde1.0mJ & --- & \textasciitilde1.8× more efficient \\
\end{longtable}

\textbf{What this reveals}: The 8× inference speedup is real, but
end-to-end improvement is only \textasciitilde3× because preprocessing
(image capture, resize, normalize) runs on the CPU in both cases.
Additionally, EdgeTPU consumes more power but completes faster, yielding
better energy efficiency per inference.

\textbf{The deployment decision}: For battery-powered devices running
infrequently (doorbell camera: \textasciitilde100 inferences/day), CPU
is more power-efficient overall because the device spends most time in
sleep mode. For continuous operation (real-time video analytics: 30
FPS), EdgeTPU's per-inference energy efficiency dominates.

This illustrates why benchmarking requires matching the MLPerf scenario
to your deployment context: SingleStream validates mobile applications;
Offline benchmarks would give different conclusions optimized for
throughput rather than latency.

\end{fbxSimple}

\section{Power Measurement
Techniques}\label{sec-benchmarking-ai-power-measurement-techniques-bcc2}

Training benchmarks measure learning speed; inference benchmarks measure
serving speed. But speed alone provides an incomplete picture. A system
achieving record throughput while consuming kilowatts of power may prove
impractical for edge deployment or economically unsustainable at
datacenter scale. Power measurement completes the evaluation triad by
quantifying the energy cost of performance, enabling the efficiency
comparisons that increasingly determine deployment decisions.

This third dimension is critical because \textbf{?@sec-ai-acceleration}
established TOPS/Watt as a primary design objective alongside raw TOPS.
Power benchmarks validate whether efficiency-optimized accelerators
actually deliver their promised energy savings. Power claims are
particularly susceptible to gaming: a chip advertising ``10 TOPS at
0.5W'' might achieve that ratio only at minimal utilization; under
sustained load, thermal throttling and voltage scaling may deliver 3
TOPS at 2W. Power benchmarks expose these gaps.

However, measuring power consumption in machine learning systems
presents challenges distinct from measuring time or throughput. Power
varies with temperature, workload phase, and system configuration in
ways that performance metrics do not. Table~\ref{tbl-power} quantifies
how energy demands of ML models vary dramatically across deployment
environments, spanning multiple orders of magnitude from TinyML devices
consuming mere microwatts to data center racks requiring kilowatts. This
wide spectrum illustrates the fundamental challenge in creating
standardized benchmarking methodologies
(\citeproc{ref-henderson2020towards}{Henderson et al. 2020}).

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.1875}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4844}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3281}}@{}}
\caption{\textbf{Power Consumption Spectrum.} Machine learning
deployments span a wide range of power demands, from microwatt-scale
TinyML devices to kilowatt-scale server racks. This variability
challenges standardized energy efficiency benchmarking and requires
scale-appropriate measurement
techniques.}\label{tbl-power}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Category}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Device Type}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Power Consumption}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Category}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Device Type}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Power Consumption}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Tiny} & Neural Decision Processor (NDP) & 150 µW \\
\textbf{Tiny} & M7 Microcontroller & 25 mW \\
\textbf{Mobile} & Raspberry Pi 4 & 3.5 W \\
\textbf{Mobile} & Smartphone & 4 W \\
\textbf{Edge} & Smart Camera & 10-15 W \\
\textbf{Edge} & Edge Server & 65-95 W \\
\textbf{Cloud} & ML Server Node & 300-500 W \\
\textbf{Cloud} & ML Server Rack & 4-10 kW \\
\end{longtable}

This dramatic range in power requirements, which spans over four orders
of magnitude, presents significant challenges for measurement and
benchmarking. Consequently, creating a unified methodology requires
careful consideration of each scale's unique characteristics. For
example, accurately measuring microwatt-level consumption in TinyML
devices demands different instrumentation and techniques than monitoring
kilowatt-scale server racks. Any comprehensive benchmarking framework
must accommodate these vastly different scales while ensuring
measurements remain consistent, fair, and reproducible across diverse
hardware configurations.

\subsection{Power Measurement
Boundaries}\label{sec-benchmarking-ai-power-measurement-boundaries-982c}

To address these measurement challenges, we must understand how power
consumption is measured at different system scales, from TinyML devices
to full-scale data center inference nodes.
Figure~\ref{fig-power-diagram} illustrates distinct measurement
boundaries for each scenario: components shown in green indicate what is
included in energy accounting, while components shown with red dashed
outlines are excluded from power measurements.

\begin{figure}[htb]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/a89134adf5fb2767722b831d039b146c4c995485.pdf}}

}

\caption{\label{fig-power-diagram}\textbf{Power Measurement Boundaries}:
MLPerf defines system boundaries for power measurement, ranging from
single-chip devices to full data center nodes, to enable fair
comparisons of energy efficiency across diverse hardware platforms.
These boundaries delineate which components' power consumption is
included in reported metrics, impacting the interpretation of
performance results. Source: (\citeproc{ref-tschand2024mlperf}{Tschand
et al. 2024}).}

\end{figure}%

The diagram is organized into three categories, Tiny, Inference, and
Training examples, each reflecting different measurement scopes based on
system architecture and deployment environment. In TinyML systems, the
entire low-power SoC, including compute, memory, and basic
interconnects, typically falls within the measurement boundary.
Inference nodes introduce more complexity, incorporating multiple SoCs,
local storage, accelerators, and memory, while often excluding remote
storage and off-chip components. Training deployments span multiple
racks, where only selected elements, including compute nodes and network
switches, are measured, while storage systems, cooling infrastructure,
and parts of the interconnect fabric are often excluded.

System-level power measurement offers a more holistic view than
measuring individual components in isolation. While component-level
metrics (e.g., accelerator or processor power) are valuable for
performance tuning, real-world ML workloads involve intricate
interactions between compute units, memory systems, and supporting
infrastructure. For instance, analysis of Google's TensorFlow Mobile
workloads shows that data movement accounts for 57.3\% of total
inference energy consumption
(\citeproc{ref-BoroumandASPLOS2018}{Boroumand et al. 2018}),
highlighting how memory-bound operations can dominate system power
usage.

Shared infrastructure presents additional challenges. In data centers,
resources such as cooling systems and power delivery are shared across
workloads, complicating attribution of energy use to specific ML tasks.
Cooling alone can account for 20-30\% of total facility power
consumption, making it a major factor in energy efficiency assessments
(\citeproc{ref-barroso2022datacenter}{Barroso, Clidaras, and Hölzle
2013}). Even at the edge, components like memory and I/O interfaces may
serve both ML and non-ML functions, further blurring measurement
boundaries.

Shared infrastructure complexity is further compounded by dynamic power
management techniques that modern systems employ to optimize energy
efficiency. Dynamic voltage and frequency scaling (DVFS) adjusts
processor voltage and clock frequency based on workload demands,
enabling significant power reductions during periods of lower
computational intensity. Advanced DVFS implementations using on-chip
switching regulators can achieve substantial energy savings
(\citeproc{ref-kim2008system}{Kim et al. 2008}), causing power
consumption to vary by 30-50\% for the same ML model depending on system
load and concurrent activity. This variability affects not only the
compute components but also the supporting infrastructure, as reduced
processor activity can lower cooling requirements and overall facility
power draw.

Support infrastructure, particularly cooling systems, is a major
component of total energy consumption in large-scale deployments. Data
centers must maintain operational temperatures, typically between
20-25°C, to ensure system reliability. Cooling overhead is captured in
the Power Usage Effectiveness (PUE) metric, which ranges from 1.1 in
highly efficient facilities to over 2.0 in less optimized ones
(\citeproc{ref-barroso2019datacenter}{Barroso, Hölzle, and Ranganathan
2019}). The interaction between compute workloads and cooling
infrastructure creates complex dependencies; for example, power
management techniques like DVFS not only reduce direct processor power
consumption but also decrease heat generation, creating cascading
effects on cooling requirements. Even edge devices require basic thermal
management.

\subsection{Computational Efficiency vs.~Power
Consumption}\label{sec-benchmarking-ai-computational-efficiency-vs-power-consumption-d01e}

The relationship between computational performance and energy efficiency
is one of the most important tradeoffs in modern ML system design. As
systems push for higher performance, they often encounter diminishing
returns in energy efficiency due to fundamental physical limitations in
semiconductor scaling and power delivery
(\citeproc{ref-koomey2011web}{Koomey et al. 2011}). This relationship is
particularly evident in processor frequency scaling, where increasing
clock frequency by 20\% typically yields only modest performance
improvements (around 5\%) while dramatically increasing power
consumption by up to 50\%, reflecting the cubic relationship between
voltage, frequency, and power consumption
(\citeproc{ref-le2010dynamic}{Le Sueur and Heiser 2010}).

In deployment scenarios with strict energy constraints, particularly
battery-powered edge devices and mobile applications, optimizing this
performance-energy tradeoff becomes essential for practical viability.
Model optimization techniques offer promising approaches to achieve
better efficiency without significant accuracy degradation. Numerical
precision optimization techniques, which reduce computational
requirements while maintaining model quality, demonstrate this tradeoff
effectively. Research shows that reduced-precision computation can
maintain model accuracy within 1-2\% of the original while delivering
3-4x improvements in both inference speed and energy efficiency.

These optimization strategies span three interconnected dimensions:
accuracy, computational performance, and energy efficiency. Advanced
optimization methods enable fine-tuned control over this tradeoff space.
Similarly, model optimization and compression techniques require careful
balancing of accuracy losses against efficiency gains. The optimal
operating point among these factors depends heavily on deployment
requirements and constraints; mobile applications typically prioritize
energy efficiency to extend battery life, while cloud-based services
might optimize for accuracy even at higher power consumption costs,
leveraging economies of scale and dedicated cooling infrastructure.

Energy efficiency metrics now occupy a central position in AI system
evaluation. Power measurement standards such as MLPerf Power
(\citeproc{ref-tschand2024mlperf}{Tschand et al. 2024}) provide
standardized frameworks for comparing energy efficiency across hardware
platforms and deployment scenarios. These standards enable engineers to
systematically balance performance, power consumption, and environmental
impact when selecting hardware and optimization strategies.

\subsection{Standardized Power
Measurement}\label{sec-benchmarking-ai-standardized-power-measurement-7fae}

While power measurement techniques, such as SPEC Power
(\citeproc{ref-spec_power_website}{Lockhart et al. 2025}), have long
existed for general computing systems
(\citeproc{ref-lange2009identifying}{Lange 2009}), machine learning
workloads present unique challenges that require specialized measurement
approaches. Machine learning systems exhibit distinct power consumption
patterns characterized by phases of intense computation interspersed
with data movement and preprocessing operations. These patterns vary
significantly across different types of models and tasks. A large
language model's power profile looks very different from that of a
computer vision inference task.

Direct power measurement requires careful consideration of sampling
rates and measurement windows. For example, certain neural network
architectures create short, intense power spikes during complex
computations, requiring high-frequency sampling (\textgreater{} 1 KHz)
to capture accurately. In contrast, CNN inference tends to show more
consistent power draw patterns that can be captured with lower sampling
rates. The measurement duration must also account for ML-specific
behaviors like warm-up periods, where initial inferences may consume
more power due to cache population and pipeline initialization.

Memory access patterns in ML workloads significantly impact power
consumption measurements. While traditional compute benchmarks might
focus primarily on processor power, ML systems often spend substantial
energy moving data between memory hierarchies. For example,
recommendation models like DLRM can spend more energy on memory access
than computation. This requires measurement approaches that can capture
both compute and memory subsystem power consumption.

Accelerator-specific considerations further complicate power
measurement. Many ML systems employ specialized hardware like GPUs,
TPUs, or NPUs. These accelerators often have their own power management
schemes and can operate independently of the main system processor.
Accurate measurement requires capturing power consumption across all
relevant compute units while maintaining proper time synchronization.
This is particularly challenging in heterogeneous systems that may
dynamically switch between different compute resources based on workload
characteristics or power constraints.

The scale of ML workloads also influences measurement methodology. In
multi-GPU configurations, power measurement must account for both
compute power and the energy cost of gradient synchronization. For
multi-node deployments, additional network infrastructure power
consumption becomes significant. Similarly, edge ML deployments must
consider both active inference power and the energy cost of model
updates or data preprocessing.

Batch size and throughput considerations add another layer of
complexity. Unlike traditional computing workloads, ML systems often
process inputs in batches to improve computational efficiency. However,
the relationship between batch size and power consumption is non-linear.
While larger batches generally improve compute efficiency, they also
increase memory pressure and peak power requirements. Measurement
methodologies must therefore capture power consumption across different
batch sizes to provide a complete efficiency profile.

System idle states require special attention in ML workloads,
particularly in edge scenarios where systems operate intermittently,
actively processing when new data arrives, then entering low-power
states between inferences. A wake-word detection TinyML system, for
instance, might only actively process audio for a small fraction of its
operating time, making idle power consumption a critical factor in
overall efficiency.

Temperature effects play a crucial role in ML system power measurement.
Sustained ML workloads can cause significant temperature increases,
triggering thermal throttling and changing power consumption patterns.
This is especially relevant in edge devices where thermal constraints
may limit sustained performance. Measurement methodologies must account
for these thermal effects and their impact on power consumption,
particularly during extended benchmarking runs.

\subsection{MLPerf Power Case
Study}\label{sec-benchmarking-ai-mlperf-power-case-study-a554}

MLPerf Power (\citeproc{ref-tschand2024mlperf}{Tschand et al. 2024}) is
a standard methodology for measuring energy efficiency in machine
learning systems. This comprehensive benchmarking framework provides
accurate assessment of power consumption across diverse ML deployments.
At the datacenter level, it measures power usage in large-scale AI
workloads, where energy consumption optimization directly impacts
operational costs. For edge computing, it evaluates power efficiency in
consumer devices like smartphones and laptops, where battery life
constraints are critical. In tiny inference scenarios, it assesses
energy consumption for ultra-low-power AI systems, particularly IoT
sensors and microcontrollers operating with strict power budgets.

The MLPerf Power methodology applies the standardized evaluation
principles discussed earlier, adapting to various hardware architectures
from general-purpose CPUs to specialized AI accelerators. Meaningful
cross-platform comparisons are ensured while maintaining measurement
integrity across different computing scales.

The benchmark has accumulated thousands of reproducible measurements
submitted by industry organizations, demonstrating their latest hardware
capabilities and the sector-wide focus on energy-efficient AI
technology. Figure~\ref{fig-power-trends} traces energy efficiency
evolution across system scales through successive MLPerf versions,
revealing critical performance trends in datacenter, edge, and tiny
deployments.

\begin{figure}[htb]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/73dd9774e00dc51357319fd1d2bd2690bd2b4c6b.pdf}}

}

\caption{\label{fig-power-trends}\textbf{Energy Efficiency Gains}:
Successive MLPerf inference benchmark versions show energy efficiency
(samples per watt) improving up to 378x for datacenter workloads and
1070x for tinyML deployments across successive releases. Standardized
measurement protocols enable meaningful cross-platform comparisons,
driving sector-wide progress toward sustainable AI. Source:
(\citeproc{ref-tschand2024mlperf}{Tschand et al. 2024}).}

\end{figure}%

Analysis of these MLPerf Power trends reveals two significant patterns:
first, a plateauing of energy efficiency improvements across all three
scales for traditional ML workloads, and second, a dramatic increase in
energy efficiency specifically for generative AI applications. This
dichotomy suggests both the maturation of optimization techniques for
conventional ML tasks and the rapid innovation occurring in the
generative AI space. These trends underscore the dual challenges facing
the field: developing novel approaches to break through efficiency
plateaus while ensuring sustainable scaling practices for increasingly
powerful generative AI models.

The measurement techniques examined above, from timing protocols to
power instrumentation, provide the raw data for benchmarking. However,
converting measurements into meaningful comparisons requires
understanding systematic sources of error and bias.

\section{Benchmarking Limitations and Best
Practices}\label{sec-benchmarking-ai-benchmarking-limitations-best-practices-9d65}

The preceding sections established what benchmarks measure: training
throughput, inference latency, power efficiency, and their validation
through MLPerf. Knowing what to measure, however, is insufficient
without understanding what benchmarks \emph{cannot} capture.

Every benchmark makes simplifying assumptions. Training benchmarks
assume fixed datasets and reproducible random seeds. Inference
benchmarks assume steady-state operation and well-formed inputs. Power
benchmarks assume controlled thermal environments. These assumptions
enable standardized comparison but diverge from production realities
where data distributions shift, request patterns spike, and thermal
throttling occurs.

The gap between benchmark performance and production reality has
derailed countless deployments where teams optimized for the wrong
signals. This section examines four categories of benchmarking
limitations (statistical, deployment-related, system design, and
organizational) that determine whether benchmark results translate to
deployment success. Each limitation includes actionable practices that
effective practitioners employ to bridge the benchmark-to-production
gap.

\subsection{Statistical \& Methodological
Issues}\label{sec-benchmarking-ai-statistical-methodological-issues-7aa5}

The foundation of reliable benchmarking rests on sound statistical
methodology. Benchmark results are only as reliable as the measurements
that produce them. Three fundamental issues undermine this foundation if
left unaddressed.

\textbf{Incomplete problem coverage} represents one of the most
fundamental limitations. Many benchmarks, while useful for controlled
comparisons, fail to capture the full diversity of real-world
applications. Common image classification datasets such as CIFAR-10
(\citeproc{ref-cifar10_website}{LeCun, Xiao, and Krizhevsky 2025})
contain a limited variety of images. Models that perform well on these
datasets may struggle when applied to more complex, real-world scenarios
with greater variability in lighting, perspective, and object
composition. This gap between benchmark tasks and real-world complexity
means strong benchmark performance provides limited guarantees about
practical deployment success.

\textbf{Statistical insignificance} arises when benchmark evaluations
are conducted on too few data samples or trials. For example, testing an
optical character recognition (OCR) system on a small dataset may not
accurately reflect its performance on large-scale, noisy text documents.
Without sufficient trials and diverse input distributions, benchmarking
results may be misleading or fail to capture true system reliability.
The statistical confidence intervals around benchmark scores often go
unreported, obscuring whether measured differences represent genuine
improvements or measurement noise.

\textbf{Reproducibility} represents a major ongoing challenge. Benchmark
results can vary significantly depending on factors such as hardware
configurations, software versions, and system dependencies. Small
differences in compilers, numerical precision, or library updates can
lead to inconsistent performance measurements across different
environments. To mitigate this issue, MLPerf addresses reproducibility
by providing reference implementations, standardized test environments,
and strict submission guidelines. Even with these efforts, achieving
true consistency across diverse hardware platforms remains an ongoing
challenge. The proliferation of optimization libraries, framework
versions, and compiler flags creates a vast configuration space where
slight variations produce different results.

\subsection{Laboratory-to-Deployment Performance
Gaps}\label{sec-benchmarking-ai-laboratorytodeployment-performance-gaps-16c8}

Statistical rigor ensures that benchmark measurements are accurate. But
accurate measurements of the wrong thing still lead to deployment
failures. Beyond statistical methodology, benchmarks must align with
practical deployment objectives.

\textbf{Misalignment with real-world goals} occurs when benchmarks
emphasize metrics such as speed, accuracy, and throughput, while
practical AI deployments require balancing multiple objectives including
power efficiency, cost, and robustness. A model that achieves
state-of-the-art accuracy on a benchmark may be impractical for
deployment if it consumes excessive energy or requires expensive
hardware. Similarly, optimizing for average-case performance on
benchmark datasets may neglect tail-latency requirements that determine
user experience in production systems. The multi-objective nature of
real deployment, encompassing resource constraints, operational costs,
maintenance complexity, and business requirements, extends far beyond
the single-metric optimization that most benchmarks reward.

\subsection{System Design
Challenges}\label{sec-benchmarking-ai-system-design-challenges-9ed2}

Statistical methodology and deployment alignment address how we measure
and what we optimize for. A third category of limitations emerges from
the physical systems being measured. Hardware behavior depends on
environmental conditions, architectural compatibility, and operational
context in ways that complicate fair comparison.

\textbf{Environmental Conditions.} Environmental conditions in AI
benchmarking refer to the physical and operational circumstances under
which experiments are conducted. These conditions, while often
overlooked in benchmark design, can significantly influence benchmark
results and impact the reproducibility of experiments. Physical
environmental factors include ambient temperature, humidity, air
quality, and altitude. These elements can affect hardware performance in
subtle but measurable ways. For instance, elevated temperatures may lead
to thermal throttling in processors, potentially reducing computational
speed and affecting benchmark outcomes. Similarly, variations in
altitude can impact cooling system efficiency and hard drive performance
due to changes in air pressure.

Beyond physical factors, operational environmental factors encompass the
broader system context in which benchmarks are executed. This includes
background processes running on the system, network conditions, and
power supply stability. The presence of other active programs or
services can compete for computational resources, potentially altering
the performance characteristics of the model under evaluation. Ensuring
the validity and reproducibility of benchmark results requires
documenting and controlling these environmental conditions to the extent
possible. This may involve conducting experiments in
temperature-controlled environments, monitoring and reporting ambient
conditions, standardizing the operational state of benchmark systems,
and documenting any background processes or system loads.

When controlling all environmental variables is impractical, as in
distributed or cloud-based benchmarking, detailed reporting of
conditions becomes necessary. This information allows other researchers
to account for potential variations when interpreting or reproducing
results, and informs the development of robust models capable of
consistent performance across varying operational conditions.

\textbf{Hardware Lottery.} A critical issue in benchmarking is what has
been described as the hardware lottery\sidenote{\textbf{Hardware
Lottery}: The phenomenon where algorithmic progress is heavily
influenced by which approaches happen to align well with available
hardware. For example, the Transformer architecture succeeded partly
because its matrix multiplication operations perfectly match GPU
capabilities, while equally valid architectures like graph neural
networks remain underexplored due to poor GPU mapping. This suggests
some ``breakthrough'' algorithms may simply be hardware-compatible
rather than fundamentally superior. }, a concept introduced by
(\citeproc{ref-hooker2021hardware}{Hooker 2021}). The success of a
machine learning model is often dictated not only by its architecture
and training data but also by how well it aligns with the underlying
hardware used for inference. Some models perform exceptionally well not
because they are inherently better but because they are optimized for
the parallel processing capabilities of GPUs or TPUs. Other promising
architectures may be overlooked because they do not map efficiently to
dominant hardware platforms.

This dependence on hardware compatibility introduces subtle but
significant biases into benchmarking results. A model that is highly
efficient on a specific GPU may perform poorly on a CPU or a custom AI
accelerator. For instance, Figure~\ref{fig-hw-lottery} compares the
performance of models across different hardware platforms. The
multi-hardware models show comparable results to ``MobileNetV3 Large
min'' on both the CPU \texttt{uint8} and GPU configurations. However,
these multi-hardware models demonstrate significant performance
improvements over the MobileNetV3 Large baseline when run on the EdgeTPU
and DSP hardware. This emphasizes the variable efficiency of
multi-hardware models in specialized computing environments.

\begin{figure}[htb]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/0f12237f29f34be7f170e5e94b1b0013536cae9a.pdf}}

}

\caption{\label{fig-hw-lottery}\textbf{Hardware-Dependent Accuracy}:
Model performance varies significantly across hardware platforms,
indicating that architectural efficiency is not solely determined by
design but also by hardware compatibility. Multi-hardware models exhibit
comparable accuracy to MobileNetV3 Large on CPU and GPU configurations,
yet achieve substantial gains on EdgeTPU and DSP, emphasizing the
importance of hardware-aware model optimization for specialized
computing environments. Source: (\citeproc{ref-chu2021discovering}{Chu
et al. 2021}).}

\end{figure}%

Without careful benchmarking across diverse hardware configurations, the
field risks favoring architectures that ``win'' the hardware lottery
rather than selecting models based on their intrinsic strengths. This
bias can shape research directions, influence funding allocation, and
impact the design of next-generation AI systems. In extreme cases, it
may even stifle innovation by discouraging exploration of alternative
architectures that do not align with current hardware trends.

\subsection{Organizational \& Strategic
Issues}\label{sec-benchmarking-ai-organizational-strategic-issues-d25a}

The preceding limitations arise from technical challenges: statistical
noise, deployment misalignment, environmental variance, and hardware
compatibility. A fourth category emerges from human factors. Competitive
pressures and research incentives create systematic biases in how
benchmarks are used and interpreted. These organizational dynamics
require governance mechanisms and community standards to maintain
benchmark integrity.

\textbf{Benchmark Engineering.} While the hardware lottery is an
unintended consequence of hardware trends, benchmark engineering is an
intentional practice where models or systems are explicitly optimized to
excel on specific benchmark tests. This practice can lead to misleading
performance claims and results that do not generalize beyond the
benchmarking environment.

Benchmark engineering occurs when AI developers fine-tune
hyperparameters, preprocessing techniques, or model architectures
specifically to maximize benchmark scores rather than improve real-world
performance. For example, an object detection model might be carefully
optimized to achieve record-low latency on a benchmark but fail when
deployed in dynamic, real-world environments with varying lighting,
motion blur, and occlusions. Similarly, a language model might be tuned
to excel on benchmark datasets but struggle when processing
conversational speech with informal phrasing and code-switching.

The pressure to achieve high benchmark scores is often driven by
competition, marketing, and research recognition. Benchmarks are
frequently used to rank AI models and systems, creating an incentive to
optimize specifically for them. While this can drive technical
advancements, it also risks prioritizing benchmark-specific
optimizations at the expense of broader generalization. This phenomenon
exemplifies Goodhart's Law\sidenote{\textbf{Goodhart's Law}: Originally
articulated by British economist Charles Goodhart in 1975, this
principle states: ``When a measure becomes a target, it ceases to be a
good measure.'' In ML systems benchmarking, this captures the
fundamental tension between using benchmarks as indicators of system
quality and the tendency for practitioners to optimize specifically for
benchmark scores rather than underlying performance characteristics. As
benchmarks become targets for optimization, they progressively lose
their value as meaningful proxies for real-world system effectiveness.
}.

\textbf{Bias and Over-Optimization.} Several strategies can ensure that
benchmarks remain useful and fair. Transparency is paramount: benchmark
submissions should include detailed documentation on any optimizations
applied, ensuring that improvements are clearly distinguished from
benchmark-specific tuning. Researchers and developers should report both
benchmark performance and real-world deployment results to provide a
complete picture of a system's capabilities. Diversifying evaluation
methodologies provides additional protection. Instead of relying on a
single static benchmark, AI systems should be evaluated across multiple,
continuously updated benchmarks that reflect real-world complexity. This
reduces the risk of models being overfitted to a single test set and
encourages general-purpose improvements rather than narrow
optimizations.

Standardization and third-party verification can also help mitigate
bias. By establishing industry-wide benchmarking standards and requiring
independent third-party audits of results, the AI community can improve
the reliability and credibility of benchmarking outcomes. Third-party
verification ensures that reported results are reproducible across
different settings and helps prevent unintentional benchmark gaming.
Complementing controlled evaluations, application-specific testing
remains essential: AI models should be assessed not only on benchmark
datasets but also in practical deployment environments. An autonomous
driving model, for instance, should be tested in a variety of weather
conditions and urban settings rather than being judged solely on
controlled benchmark datasets. Finally, benchmarks should test AI models
on multiple hardware configurations to ensure that performance is not
being driven solely by compatibility with a specific platform, reducing
the risk of the hardware lottery.

\textbf{Benchmark Evolution.} A persistent challenge in benchmarking is
that benchmarks are rarely static. As AI systems evolve, so must the
benchmarks that evaluate them. What defines ``good performance'' today
may be less relevant tomorrow as models, hardware, and application
requirements change. While benchmarks are essential for tracking
progress, they can also become outdated, leading to over-optimization
for old metrics rather than real-world performance improvements.

This evolution is evident in the history of AI benchmarks. Early model
benchmarks, for instance, focused heavily on image classification and
object detection, as these were some of the first widely studied deep
learning tasks. However, as AI expanded into natural language
processing, recommendation systems, and generative AI, it became clear
that these early benchmarks no longer reflected the most important
challenges in the field. In response, new benchmarks emerged to measure
language understanding (\citeproc{ref-wang2018glue}{Wang et al. 2018b},
\citeproc{ref-wang2019superglue}{2019}) and generative AI
(\citeproc{ref-liang2022helm}{Liang et al. 2022}).

Benchmark evolution extends beyond the addition of new tasks to
encompass new dimensions of performance measurement. While traditional
AI benchmarks emphasized accuracy and throughput, modern applications
demand evaluation across multiple criteria: fairness, robustness,
scalability, and energy efficiency. Figure~\ref{fig-sciml-graph}
illustrates this complexity through scientific applications, which span
orders of magnitude in their performance requirements. For instance,
Large Hadron Collider sensors must process data at rates approaching
10\(^{14}\) bytes per second (equivalent to about 100 terabytes per
second) with nanosecond-scale computation times, while mobile
applications operate at 10\(^{4}\) bytes per second with longer
computational windows. This range of requirements necessitates
specialized benchmarks. For example, edge AI applications require
benchmarks like MLPerf that specifically evaluate performance under
resource constraints and scientific application domains need their own
``Fast ML for Science'' benchmarks
(\citeproc{ref-duarte2022fastml}{Duarte et al. 2022}).

\begin{figure}[htb]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/8de0089ab58f82faeba7c4c0b092e3c0f2c4d8af.pdf}}

}

\caption{\label{fig-sciml-graph}\textbf{Performance Spectrum}:
Scientific applications and edge devices demand vastly different
computational resources, spanning multiple orders of magnitude in data
rates and latency requirements. Consequently, traditional benchmarks
focused solely on accuracy are insufficient; specialized evaluation
metrics and benchmarks like MLPerf become essential for optimizing AI
systems across diverse deployment scenarios. Source:
(\citeproc{ref-duarte2022fastml}{Duarte et al. 2022}).}

\end{figure}%

The need for evolving benchmarks also presents a challenge: stability
versus adaptability. On the one hand, benchmarks must remain stable for
long enough to allow meaningful comparisons over time. If benchmarks
change too frequently, it becomes difficult to track long-term progress
and compare new results with historical performance. On the other hand,
failing to update benchmarks leads to stagnation, where models are
optimized for outdated tasks rather than advancing the field. Striking
the right balance between benchmark longevity and adaptation is an
ongoing challenge for the AI community.

Evolving benchmarks remains essential for meaningful progress
measurement. Without updates, benchmarks become detached from real-world
needs, and researchers optimize for artificial test cases rather than
practical challenges. The transition from ImageNet-era accuracy
benchmarks to multi-dimensional evaluations spanning fairness,
robustness, and energy efficiency illustrates this evolution in
practice.

\subsection{MLPerf as Industry
Standard}\label{sec-benchmarking-ai-mlperf-industry-standard-05a1}

MLPerf has played a crucial role in improving benchmarking by reducing
bias, increasing generalizability, and ensuring benchmarks evolve
alongside AI advancements. One of its key contributions is the
standardization of benchmarking environments. By providing reference
implementations, clearly defined rules, and reproducible test
environments, MLPerf ensures that performance results are consistent
across different hardware and software platforms, reducing variability
in benchmarking outcomes.

Recognizing that AI is deployed in a variety of real-world settings,
MLPerf has also introduced different categories of inference benchmarks
that align with our three-dimensional framework. The inclusion of MLPerf
Inference, MLPerf Mobile, MLPerf Client, and MLPerf Tiny reflects an
effort to evaluate models across different deployment constraints while
maintaining the systematic evaluation principles established throughout
this chapter.

Beyond providing a structured benchmarking framework, MLPerf is
continuously evolving to keep pace with the rapid progress in AI. New
tasks are incorporated into benchmarks to reflect emerging challenges,
such as generative AI models and energy-efficient computing, ensuring
that evaluations remain relevant and forward-looking. By regularly
updating its benchmarking methodologies, MLPerf helps prevent benchmarks
from becoming outdated or encouraging overfitting to legacy performance
metrics.

By prioritizing fairness, transparency, and adaptability, MLPerf ensures
that benchmarking remains a meaningful tool for guiding AI research and
deployment. Instead of simply measuring raw speed or accuracy, MLPerf's
evolving benchmarks aim to capture the complexities of real-world AI
performance, ultimately fostering more reliable, efficient, and
impactful AI systems.

\section{Model and Data
Benchmarking}\label{sec-benchmarking-ai-model-data-benchmarking-e0ca}

The preceding sections validated hardware acceleration through training
throughput, inference latency, and power efficiency, completing the
system dimension of our three-dimensional benchmarking framework. But
hardware validation alone cannot ensure deployment success. The
optimization pipeline from Part III also included model compression
(\textbf{?@sec-model-compression}) and data selection
(\textbf{?@sec-data-selection}), each requiring its own validation. A
system that achieves excellent hardware benchmarks may still fail if
compression degraded model quality or if training data does not
represent production distributions.

System benchmarks validated hardware claims; limitations analysis
revealed what benchmarks cannot capture. This section completes the
validation stack by addressing the remaining two dimensions of our
three-dimensional framework: model and data. Model benchmarks verify
that compression preserved accuracy and critical model properties. Data
benchmarks verify that training data enables robust generalization.
Together with system benchmarks, they determine whether the entire
optimization pipeline, from data curation through hardware deployment,
actually delivers.

Real-world performance emerges from the interaction of all three
dimensions. A compressed model running on accelerated hardware trained
on biased data will fail despite excellent system benchmarks.
Comprehensive evaluation requires validating all three dimensions.

\subsection{Model
Benchmarking}\label{sec-benchmarking-ai-model-benchmarking-4847}

Model benchmarks validate whether compression techniques from
\textbf{?@sec-model-compression} preserved the properties that matter
for deployment. This extends beyond top-line accuracy: a pruned model
might maintain ImageNet accuracy while losing robustness to adversarial
inputs; a quantized model might preserve average-case performance while
degrading on rare but critical edge cases; a distilled model might match
the teacher's accuracy while losing calibration. Historically,
benchmarks focused almost exclusively on accuracy, but compression makes
multi-dimensional evaluation essential.

Figure~\ref{fig-imagenet-challenge} traces the dramatic reduction in
error rates on the ImageNet Large Scale Visual Recognition Challenge
(ILSVRC) (\citeproc{ref-ilsvrc_website}{Russakovsky et al. 2015a})
classification task, from 25.8\% in 2010 to a mere 3.57\% by 2015.
Starting from the baseline models in 2010 and 2011, the introduction of
AlexNet\sidenote{\textbf{AlexNet}: Developed by Alex Krizhevsky, Ilya
Sutskever, and Geoffrey Hinton at the University of Toronto, this
8-layer neural network revolutionized computer vision in 2012. With 60
million parameters trained on two GTX 580 GPUs, AlexNet introduced key
innovations in neural network design that became standard techniques in
modern AI. } in 2012 marked an improvement, reducing the error rate from
25.8\% to 16.4\%. Subsequent models like ZFNet, VGGNet, GoogleNet, and
ResNet\sidenote{\textbf{ResNet}: Microsoft's Residual Networks,
introduced in 2015 by Kaiming He and colleagues, solved the vanishing
gradient problem with skip connections, enabling networks with 152+
layers. ResNet-50 became the de facto standard for transfer learning,
while an ensemble of ResNet models achieved 3.57\% top-5 error on
ImageNet (single-model ResNet-152 achieved 4.49\%), exceeding the
estimated 5.1\% human error rate from Andrej Karpathy's analysis. }
continued this trend, with ResNet achieving an error rate of 3.57\% by
2015 (\citeproc{ref-russakovsky2015imagenet}{Russakovsky et al. 2015b}).
This progression established the baselines against which model
compression techniques are evaluated---a pruned ResNet must demonstrate
how much accuracy it sacrifices for a given efficiency gain.

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{contents/vol1/benchmarking/benchmarking_files/figure-pdf/fig-imagenet-challenge-output-1.pdf}}

}

\caption{\label{fig-imagenet-challenge}\textbf{ImageNet Challenge
Progression}: Neural networks have reduced error rates from 28.2\% in
2010 to 3.57\% by 2015, highlighting the impact of architectural
advancements on classification accuracy. These milestones establish the
baselines against which compression techniques are evaluated.}

\end{figure}%

\textbf{Accuracy Metrics and Their Blind Spots.} The most common model
metrics (accuracy, precision, recall, F1) each reveal different aspects
of model behavior while hiding others.

\textbf{Top-k accuracy} measures whether the correct label appears in
the model's top k predictions. Top-1 accuracy is strict; top-5 is
lenient. The gap between them reveals model uncertainty: a model with
75\% top-1 but 95\% top-5 accuracy ``knows'' the answer is among a few
candidates but struggles to commit. For deployment, the acceptable gap
depends on whether downstream systems can use ranked predictions or
require single answers.

\textbf{Precision and recall} matter when classes are imbalanced or
errors have asymmetric costs. A fraud detection model with 99\% accuracy
might have 10\% recall on actual fraud (catching only 1 in 10 fraudulent
transactions), a catastrophic failure despite high accuracy. Precision
(of predicted positives, how many are correct?) and recall (of actual
positives, how many were found?) expose these failures that accuracy
hides.

\textbf{Aggregate metrics hide subgroup failures.} A model achieving
95\% overall accuracy might achieve 60\% on a critical demographic
subgroup. The Gender Shades project
(\citeproc{ref-buolamwini2018gender}{Buolamwini and Gebru 2018})
revealed commercial facial recognition systems performing significantly
worse on darker-skinned individuals, a disparity invisible to aggregate
benchmarks. Disaggregated evaluation across deployment-relevant
subgroups is essential; \textbf{?@sec-responsible-engineering} examines
fairness evaluation systematically.

\textbf{Calibration: When Confidence Scores Matter.} For many deployment
scenarios, \emph{how confident} the model is matters as much as
\emph{what} it predicts. A
well-calibrated\sidenote{\textbf{Calibration}: From Latin ``calibrare,''
derived from Arabic ``qalib'' (a mold or form for casting metal). The
term originally described adjusting measuring instruments against known
standards. In ML, calibration ensures predicted probabilities match
empirical frequencies: a model predicting ``80\% confident'' should be
correct 80\% of the time across many such predictions. The surveying
metaphor recurs: just as instruments must be calibrated against
reference points, model confidence must be validated against ground
truth. } model's confidence scores correspond to actual correctness
probability: when it says ``90\% confident,'' it should be correct 90\%
of the time.

Calibration failures create downstream problems. An overconfident model
triggers unnecessary human review (predicted 95\% confidence but wrong
30\% of the time). An underconfident model fails to automate decisions
it could handle (predicted 70\% confidence but correct 95\% of the
time). Expected Calibration Error (ECE) measures the gap between
confidence and accuracy across confidence bins; reliability diagrams
visualize this correspondence.

Compression frequently degrades calibration even when preserving
accuracy. A quantized model might maintain 94\% accuracy while becoming
overconfident, predicting 90\%+ confidence on examples it gets wrong.
This occurs because quantization affects the softmax distribution's
shape, compressing probability mass toward the top prediction. Post-hoc
calibration techniques (temperature scaling, Platt scaling) can
partially correct this, but only if calibration is measured.

\textbf{Compression Validation: The Efficiency-Quality Frontier.} Model
compression (\textbf{?@sec-model-compression}) trades model capacity for
efficiency. Validation must answer: did we achieve an acceptable
trade-off, or did compression damage capabilities that matter?

\textbf{Pareto frontier\sidenote{\textbf{Pareto Frontier}: Named after
Italian economist Vilfredo Pareto (1848-1923), who observed that 80\% of
Italy's land was owned by 20\% of the population. In optimization, a
Pareto frontier contains all solutions where improving one objective
requires degrading another. For model compression, this frontier reveals
the fundamental accuracy-efficiency trade-off: any point on the frontier
represents a valid design choice, while points below the frontier are
strictly dominated by better alternatives. A compressed model achieving
92\% accuracy at 10ms latency is valuable if no alternative achieves
higher accuracy at equal-or-lower latency. It's wasteful if an
alternative achieves 94\% at 8ms. } evaluation} determines whether a
compressed model represents a good trade-off. Plot accuracy against your
efficiency metric (latency, model size, energy). Models on the Pareto
frontier cannot improve one metric without degrading the other; models
below the frontier are dominated by better alternatives.

\textbf{Different compression techniques fail differently.} Quantization
(reducing numerical precision) typically preserves average-case
performance while degrading on inputs near decision boundaries---exactly
the edge cases that often matter most. Pruning (removing weights or
structures) loses capacity for rare features---potentially fine for
common cases but catastrophic for tail scenarios. Distillation (training
smaller models to mimic larger ones) can lose calibration even when
matching accuracy. Validation must probe these specific failure modes,
not just measure aggregate accuracy.

\textbf{Acceptable degradation depends on deployment context.} A 2\%
accuracy drop might be acceptable for a recommendation system (users
tolerate imperfect suggestions) but unacceptable for medical diagnosis
(each error has significant consequences). Define accuracy thresholds
before compression, then validate against them. Our \emph{MobileNet INT8
compression} lighthouse illustrates the complete validation protocol.

\phantomsection\label{callout-lighthouseux2a-1.25}
\begin{fbxSimple}{callout-lighthouse}{Lighthouse:}{MobileNet INT8 Compression}
\phantomsection\label{callout-lighthouse*-1.25}
Returning to our MobileNet lighthouse example, consider the complete
validation protocol for INT8 quantization:

\textbf{Pre-compression baseline}: MobileNetV2 achieves 71.8\% top-1
accuracy on ImageNet at 3.5M parameters (14 MB FP32).

\textbf{Post-compression metrics} (INT8 quantization to 4.3 MB):

\begin{longtable}[]{@{}lrrl@{}}
\toprule\noalign{}
\textbf{Metric} & \textbf{FP32} & \textbf{INT8} &
\textbf{Acceptable?} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Top-1 accuracy} & 71.8\% & 70.9\% & ✓ (\textless2\% drop) \\
\textbf{Top-5 accuracy} & 91.0\% & 90.4\% & ✓ \\
\textbf{Calibration ECE} & 0.031 & 0.089 & ⚠ (degraded) \\
\textbf{Edge-case accuracy} & 68.2\% & 61.4\% & ⚠ (10\% drop) \\
\end{longtable}

\textbf{Understanding ECE (Expected Calibration Error)}: ECE measures
whether predicted confidence matches actual accuracy. When a model
predicts ``90\% confident,'' it should be correct 90\% of the time.
Interpretation thresholds:

\begin{itemize}
\tightlist
\item
  ECE \textless{} 0.05: Well-calibrated; confidence scores are reliable
  for threshold-based decisions
\item
  0.05 \textless{} ECE \textless{} 0.10: Moderate calibration; use
  confidence scores with caution
\item
  ECE \textgreater{} 0.10: Poorly calibrated; confidence scores are
  unreliable
\end{itemize}

The INT8 model's ECE of 0.089 indicates borderline calibration:
confidence scores are becoming unreliable for automated decision
thresholds.

\textbf{Edge-case definition}: Images with \textgreater50\% occlusion,
\textless100 lux lighting, or \textgreater30° rotation from training
distribution (approximately 5\% of real-world inputs).

\textbf{What this reveals}: Average-case accuracy looks acceptable
(0.9\% drop), but calibration degraded significantly and edge-case
accuracy dropped 10\%. If the deployment context uses confidence
thresholds (e.g., ``only act if confidence \textgreater{} 85\%'') or
encounters many edge cases (unusual lighting, partial occlusions), INT8
MobileNet may fail despite passing aggregate benchmarks.

\textbf{The fix}: Apply temperature scaling post-hoc to restore
calibration. Temperature scaling learns a single scalar \(T\) to divide
logits before softmax: \(\text{softmax}(z_i / T)\). Typical values:
\(T = 1.5\)--\(2.5\) for quantized models. Additionally, add edge-case
examples to the test set to monitor that specific failure mode
continuously.

\end{fbxSimple}

The Lottery Ticket Hypothesis
(\textbf{?@sec-model-compression-lottery-ticket-hypothesis-a5bd})
provides concrete benchmarking data illustrating what Pareto-efficient
compression looks like. Through iterative pruning, researchers
discovered that sparse subnetworks (``winning tickets'') can match dense
model performance: ResNet-18 subnetworks at 10-20\% of original size
achieve 93.2\% accuracy versus 94.1\% for the full model on CIFAR-10---a
0.9 percentage point drop for 80-90\% size reduction
(\citeproc{ref-frankle2018lottery}{Frankle and Carbin 2019}). BERT-base
winning tickets retain 97\% of original performance with 90\% fewer
parameters, requiring 5-8x less training time to converge.

These numbers reveal the shape of compression trade-offs: the ResNet
result shows diminishing returns (the last 80\% of parameters contribute
only 0.9\% accuracy), while BERT demonstrates that aggressive pruning
can preserve nearly all capability for the right architecture.
Compression validation should establish similar trade-off curves for
your specific model and task, identifying where your model sits on the
Pareto frontier and whether further compression yields meaningful
efficiency gains or merely degrades quality.

\textbf{Large Language Model Benchmarks.} The compression evaluation
framework applies well to models with well-defined accuracy metrics:
classification accuracy, detection mAP, segmentation IoU. Large language
models present unique benchmarking challenges because their outputs
resist such clean quantification. Unlike classification tasks where
ground truth is well-defined, language model evaluation must assess
open-ended generation quality, factual accuracy, reasoning capability,
and safety---dimensions that resist simple quantification.

\textbf{Knowledge benchmarks} like MMLU\sidenote{\textbf{MMLU}:
Introduced by Hendrycks et al.
(\citeproc{ref-hendrycks2021mmlu}{Hendrycks et al. 2020}), MMLU tests
models on 57 academic subjects from elementary mathematics to
professional medicine and law. With 15,908 questions total, it revealed
that GPT-3 achieved 43.9\% accuracy while human experts average 89.8\%.
} (Massive Multitask Language Understanding) evaluate factual knowledge
across 57 subjects. Scores range from 25\% (random guessing) to 90\%+
for frontier models; human experts average 89.8\%. However, MMLU's
multiple-choice format tests recognition rather than generation,
potentially overestimating real-world capability since a model might
select the correct answer from options while being unable to produce it
unprompted.

\textbf{Holistic benchmarks} like HELM\sidenote{\textbf{HELM}:
Stanford's comprehensive evaluation framework introduced in 2022,
testing 30+ language models across scenarios including question
answering, summarization, and reasoning. } (Holistic Evaluation of
Language Models) address single-metric limitations by evaluating across
7 dimensions: accuracy, calibration, robustness, fairness, bias,
toxicity, and efficiency. This reveals trade-offs invisible to
accuracy-only evaluation; a model achieving high accuracy may exhibit
poor calibration or elevated toxicity. The same multi-dimensional
principle from classification (accuracy alone is insufficient) applies
with greater force to generative models.

\textbf{Generation-specific metrics} capture properties absent from
discriminative benchmarks:

\begin{itemize}
\tightlist
\item
  \textbf{Perplexity}\sidenote{\textbf{Perplexity}: From Latin
  ``perplexus'' (entangled, confused), entering English in the 15th
  century to describe mental bewilderment. In information theory, Claude
  Shannon adapted the concept in the 1940s as 2\^{}H(p) where H is
  entropy, measuring how ``confused'' or uncertain a probability
  distribution is. For language models, perplexity quantifies how
  surprised the model is by test text. A perplexity of 1 means perfect
  prediction; 100 means the model is as confused as randomly choosing
  among 100 equally likely words. } measures how well a model predicts
  held-out text (lower is better). A perplexity of 10 means the model is
  ``10-way confused'' on average. Useful for comparing models on the
  same corpus, but does not directly measure generation quality.
\end{itemize}

\begin{itemize}
\item
  \textbf{First-token latency} (time to first generated token) dominates
  user-perceived responsiveness for interactive applications. This
  metric is dominated by prompt processing, proportional to input
  length.
\item
  \textbf{Tokens per second} measures generation throughput. Modern LLMs
  achieve 20-100 tokens/second depending on model size and hardware. For
  a 500-word response (\textasciitilde750 tokens), 25 vs 100
  tokens/second means 30 seconds vs 7.5 seconds.
\item
  \textbf{Time-to-first-token vs inter-token latency} capture different
  bottlenecks requiring different optimizations. Batching improves
  throughput but typically increases first-token latency, a trade-off
  invisible if only one metric is measured.
\end{itemize}

\textbf{Benchmark contamination} is a unique LLM failure mode. Models
trained on web-scale corpora may encounter benchmark questions during
pretraining, inflating scores through memorization rather than
capability (\citeproc{ref-xu2024benchmarking}{Xu et al. 2024}). Studies
estimate 4-15\% contamination rates for popular benchmarks, with
contaminated examples showing 10-20\% higher accuracy. Mitigation
strategies include temporal holdouts (benchmarks from content published
after training cutoff), dynamic benchmarks (continuously generated
evaluation instances), and contamination detection (testing whether
models recall exact benchmark phrasing).

\subsection{Data
Benchmarking}\label{sec-benchmarking-ai-data-benchmarking-22f9}

Model benchmarks validate whether compression preserved model quality.
But model quality depends fundamentally on the data used to train and
evaluate it. A perfectly preserved model trained on biased or
unrepresentative data will still fail in production. Data benchmarks
validate whether the efficiency strategies from
\textbf{?@sec-data-selection}---active learning, curriculum design, data
augmentation, and synthetic data generation---actually produced training
sets that enable robust deployment. This is often the last validation to
fail and the hardest to diagnose: a model achieving excellent accuracy
on held-out test data may collapse on production inputs that the
training data never adequately represented.

Contemporary AI development reveals that data quality often determines
performance boundaries more than model architecture. This recognition
elevated data benchmarking from afterthought to critical discipline. For
data selection metrics (PPD, DUE) and benchmarks (DataPerf), see
\textbf{?@sec-data-selection}.

\subsubsection*{Coverage Metrics: What Does the Data
Represent?}\label{coverage-metrics-what-does-the-data-represent}
\addcontentsline{toc}{subsubsection}{Coverage Metrics: What Does the
Data Represent?}

The first question data benchmarking must answer is whether the training
data actually represents the inputs the model will encounter. A model
cannot learn patterns it has never seen, and the ways training data can
\emph{fail} to represent deployment reality are often subtle.

Consider class balance: a fraud detection dataset with 99\% legitimate
transactions and 1\% fraud might produce a model that achieves 99\%
accuracy by simply labeling everything legitimate. The model is useless,
but the accuracy metric looks excellent. Imbalance ratios above 10:1
typically require mitigation through oversampling, class weighting, or
threshold adjustment. More insidious is subgroup imbalance \emph{within}
classes---a dataset might have balanced positive and negative examples
overall, but negative examples might be drawn predominantly from one
demographic group, creating disparities invisible to aggregate class
balance metrics.

Feature coverage presents an even harder challenge because it requires
domain knowledge about what variations matter. A computer vision model
trained exclusively on daytime images will fail on nighttime inputs; a
natural language model trained on formal text will fail on colloquial
language. Unlike class balance, which can be computed from labels alone,
feature coverage requires understanding the deployment context.
\emph{What lighting conditions will the camera encounter? What dialects
will users speak? What edge cases exist in production that test sets
never capture?} These questions have no algorithmic answer---they demand
collaboration between ML engineers and domain experts who understand the
deployment environment.

For applications affecting people, demographic representation becomes a
coverage dimension with ethical implications. Training data must
represent the deployment population across relevant dimensions: age,
gender, ethnicity, geography, language. A facial recognition system
trained predominantly on one demographic group will systematically
underperform on others, even if aggregate accuracy metrics look
acceptable. The challenge is that demographic metadata is often
unavailable or unreliable, making representation gaps difficult to
detect and measure.

\subsubsection*{Quality Metrics: Can the Labels Be
Trusted?}\label{quality-metrics-can-the-labels-be-trusted}
\addcontentsline{toc}{subsubsection}{Quality Metrics: Can the Labels Be
Trusted?}

Even when training data covers the right inputs, the labels themselves
may be unreliable. Studies consistently find 3-6\% label error rates in
major datasets, including ImageNet
(\citeproc{ref-northcutt2021pervasive}{Northcutt, Athalye, and Mueller
2021}). These errors are not merely noise---they become learned ground
truth. A model trained on data where wolves are occasionally labeled as
dogs will learn that some wolves \emph{are} dogs. The benchmark will
report this as correct behavior because the model matches the
(incorrect) labels.

For small datasets, manual audit of a random sample can estimate label
accuracy. For large datasets, confident learning techniques identify
likely mislabeled examples by finding cases where model predictions
systematically disagree with labels. But detection is only the first
step; correction requires human review, and scaling human review to
millions of examples presents its own challenges.

Inter-annotator agreement provides a different lens on label quality by
measuring consistency across human labelers. Cohen's kappa or Fleiss'
kappa quantify agreement beyond what chance would produce. When
agreement falls below 0.6 on tasks with clear ground truth, something is
wrong---either the labeling guidelines are ambiguous, the task is
inherently subjective, or labeler quality varies significantly.
Agreement below 0.4 is problematic for any supervised learning
application because the training signal itself is incoherent.

The distinction between random and systematic errors matters enormously
for their downstream effects. Random label noise partially averages out
during training---if different examples are mislabeled in different
directions, the model learns the central tendency. But systematic errors
(consistently mislabeling a particular subclass) are learned as ground
truth. A dataset where all wolves photographed in snow are labeled
``dogs'' will produce a model that calls snowy wolves dogs, and no
amount of additional data fixes this without correcting the systematic
error at its source.

\subsubsection*{Distribution Alignment: Will It
Generalize?}\label{distribution-alignment-will-it-generalize}
\addcontentsline{toc}{subsubsection}{Distribution Alignment: Will It
Generalize?}

The final category of data benchmarking asks whether models will
generalize from training conditions to deployment reality. This is where
the gap between benchmark performance and production performance most
frequently emerges.

The standard assumption underlying held-out evaluation---that test data
comes from the same distribution as training data---is routinely
violated in practice. Test sets constructed years after training data
may reflect distribution drift as the world changes. Test sets from
different geographic regions may reflect population shift. A model
achieving 95\% accuracy on a held-out test set may drop to 70\% when
deployed to a region or time period the test set did not represent.
Standard held-out evaluation overestimates deployment performance
whenever the i.i.d. (independent and identically distributed) assumption
fails.

The true test is train/production alignment, and this is far harder to
measure because production data differs from training data in ways that
held-out test sets often fail to capture. Production images come from
different cameras with different characteristics. Production users come
from different populations with different behaviors. Production inputs
include edge cases that curated test sets systematically exclude. The
WILDS\sidenote{\textbf{WILDS}: A curated benchmark of 10 datasets
spanning distribution shifts encountered in real-world ML deployments,
introduced by Stanford researchers in 2021. WILDS tests include hospital
patient population shifts (Camelyon17), wildlife camera location changes
(iWildCam), and satellite imagery temporal drift (PovertyMap). The
benchmark quantifies the gap between in-distribution and
out-of-distribution performance, revealing that models achieving 97\%
in-distribution accuracy can drop to 70\% under realistic shifts. }
benchmark (\citeproc{ref-koh2021wilds}{Koh et al. 2021}) was designed
specifically to evaluate models under realistic distribution
shifts---hospital systems with different patient populations, wildlife
cameras at different locations, satellite imagery from different time
periods. The results are sobering: models achieving 90\%+ accuracy on
in-distribution test sets may drop to 60\% under these realistic shifts.

Given these challenges, shift detection methods become essential for
production monitoring. Statistical tests like the Kolmogorov-Smirnov
test or Maximum Mean Discrepancy (MMD) on input features can detect
covariate shift---when the distribution of inputs changes even if the
relationship between inputs and outputs remains stable. Monitoring model
confidence distributions can detect when the model encounters inputs
unlike anything in training. The goal is early detection: identifying
distribution shift before it causes catastrophic performance
degradation, enabling intervention through model updates, data
collection, or deployment constraints.

These distribution alignment challenges highlight a fundamental tension
in ML development: should we fix the data and iterate on models, or fix
the model and iterate on data? Figure~\ref{fig-model-vs-data} contrasts
model-centric and data-centric AI approaches. The model-centric paradigm
treats datasets as fixed while iterating on architectures; the
data-centric approach fixes architectures while systematically improving
data quality. Research increasingly demonstrates that methodical dataset
enhancement can yield superior performance gains compared to model
refinements alone---challenging the conventional emphasis on
architectural innovation.

\begin{figure}[htb]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/4252dc8f5f880f997a38318e783863907dca54aa.pdf}}

}

\caption{\label{fig-model-vs-data}\textbf{Development Paradigms}:
Model-centric AI prioritizes architectural innovation with fixed
datasets, while data-centric AI systematically improves dataset quality
(annotations, diversity, and bias) with consistent model architectures
to achieve performance gains. Modern research indicates that strategic
data enhancement often yields greater improvements than solely refining
model complexity.}

\end{figure}%

Data quality's primacy in AI development reflects a fundamental shift in
understanding: superior datasets, not just sophisticated models, produce
more reliable and robust AI systems. Initiatives like DataPerf and
DataComp\sidenote{\textbf{DataComp}: A benchmark for studying
data-centric approaches to multimodal learning, introduced in 2023 by
researchers from multiple institutions. DataComp provides a standardized
framework where participants compete to create the best training dataset
for CLIP-style models while keeping the model architecture and training
code fixed. Results demonstrated that careful data filtering can yield
models matching or exceeding those trained on 10x larger unfiltered
datasets, quantifying data quality's impact on system performance. }
have emerged to systematically evaluate how dataset improvements affect
model performance. For instance, DataComp
(\citeproc{ref-gadre2024datacomp}{Gadre et al. 2023}) demonstrated that
models trained on a carefully curated 30\% subset of data achieved
better results than those trained on the complete dataset, challenging
the assumption that more data automatically leads to better performance
(\citeproc{ref-northcutt2021pervasive}{Northcutt, Athalye, and Mueller
2021}).

A significant challenge in data benchmarking emerges from dataset
saturation. When models achieve near-perfect accuracy on benchmarks like
ImageNet, it becomes crucial to distinguish whether performance gains
represent genuine advances in AI capability or merely optimization to
existing test sets. Figure~\ref{fig-dataset-saturation} captures how AI
systems have surpassed human performance across various applications
over the past decade, from handwriting recognition in the early 2000s to
reading comprehension and language understanding by 2020.

\begin{figure}[htb]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/0d395b9567855f6c990f1614048bec9ca6770f06.pdf}}

}

\caption{\label{fig-dataset-saturation}\textbf{Dataset Saturation}: AI
systems surpass human performance on five benchmark capabilities:
handwriting recognition, speech recognition, image recognition, reading
comprehension, and language understanding, each crossing the human
baseline between 1998 and 2020. This saturation underscores the need for
dynamic benchmarks that remain challenging as model capabilities
improve. Source: (\citeproc{ref-kiela2021dynabench}{Kiela et al.
2021}).}

\end{figure}%

\textbf{Dataset Saturation and Dynamic Benchmarks.}
Figure~\ref{fig-dataset-saturation} raises a fundamental methodological
question: when models surpass human performance on benchmarks, does this
reflect genuine capability advances or optimization to static evaluation
sets? MNIST illustrates the concern: certain test images, though nearly
illegible to humans, were assigned specific labels during dataset
creation in 1994. Models correctly predicting these labels may be
memorizing dataset artifacts rather than learning digit recognition. The
question ``Are we done with ImageNet?''
(\citeproc{ref-beyer2020we}{Beyer et al. 2020}) generalizes this
concern.

Dynamic benchmarking approaches like
Dynabench\sidenote{\textbf{Dynabench}: A platform for dynamic,
human-in-the-loop benchmark generation introduced by Facebook AI
Research in 2021. Unlike static benchmarks where models can overfit to
fixed test sets, Dynabench continuously generates new adversarial
examples by having humans craft inputs that fool current best models.
This creates an evolving evaluation where benchmark difficulty scales
with model capability, addressing the saturation problem where 95\%+
accuracy on static benchmarks may reflect memorization rather than
genuine understanding. } (\citeproc{ref-kiela2021dynabench}{Kiela et al.
2021}) address saturation by continuously evolving test data based on
model performance, ensuring that benchmarks remain challenging as
capabilities improve. However, dynamic benchmarks complement rather than
replace the coverage, quality, and distribution metrics described above:
they prevent saturation but do not diagnose its causes.

\subsection{Holistic System-Model-Data
Evaluation}\label{sec-benchmarking-ai-holistic-systemmodeldata-evaluation-8fda}

We have now examined all three dimensions of our benchmarking framework:
system benchmarks that validate hardware performance, model benchmarks
that verify compression preserved quality, and data benchmarks that
assess training set representativeness. AI benchmarking has
traditionally evaluated these dimensions as separate entities. However,
real-world AI performance emerges from their interaction, and optimizing
one dimension can expose weaknesses in another.

Consider a concrete failure cascade: a team achieves excellent MLPerf
Inference scores by deploying an INT8-quantized model on optimized
hardware. System benchmarks pass. But the quantized model was validated
only on ImageNet-distributed test data; deployment reveals accuracy
degradation on factory-floor images with different lighting
characteristics. Model quality benchmarks would have caught the
quantization sensitivity. Further investigation shows the training data
contained no images with industrial lighting---a data quality gap that
no amount of system or model optimization can address.

This interdependence means that benchmark results from one dimension can
be invalidated by failures in another:

\begin{itemize}
\tightlist
\item
  \textbf{System success + Model failure}: Hardware delivers promised
  throughput, but compression degraded accuracy below deployment
  thresholds
\item
  \textbf{System success + Data failure}: Fast inference on
  representative inputs, but training data bias causes failures on
  demographic subgroups
\item
  \textbf{Model success + System failure}: Accurate predictions, but
  latency variance under load violates SLA requirements
\item
  \textbf{Model success + Data failure}: High accuracy on held-out test
  set, but distribution shift in production causes silent degradation
\end{itemize}

This interdependence is precisely the AI Triad introduced in
\textbf{?@sec-introduction} (\textbf{?@fig-ai-triad}): System
corresponds to Machine, Model corresponds to Algorithm, and Data remains
Data. Holistic evaluation requires not just passing benchmarks in each
dimension, but verifying that assumptions made in one dimension hold
across the others. The Part III optimization pipeline (data → model →
hardware) creates implicit dependencies that benchmarking must validate
explicitly.

As AI continues to evolve, benchmarking methodologies must advance in
tandem. Evaluating AI performance through the lens of systems, models,
and data ensures that benchmarks drive improvements not just in
accuracy, but also in efficiency, fairness, and robustness. This
holistic perspective provides essential validation before deployment.
The DAM Taxonomy provides a diagnostic framework for identifying which
component limits performance. Table~\ref{tbl-dam-bottleneck} formalizes
this approach by crossing each AI Triad component with the three
fundamental bottleneck types (see \textbf{?@sec-appendix-dam} for the
full ``Check the DAM'' flowchart and troubleshooting guide).

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.0751}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3006}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3006}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3237}}@{}}
\caption{\textbf{DAM x Bottleneck Diagnostic Matrix.} Each cell
describes a performance constraint symptom, and the row identifies which
AI Triad component to address. When performance stalls, ask:
\emph{``Where is the flow blocked? Check the
DAM.''}}\label{tbl-dam-bottleneck}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Component}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Compute-Bound}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Memory-Bound}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{I/O-Bound}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Component}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Compute-Bound}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Memory-Bound}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{I/O-Bound}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Data} & Preprocessing too slow (augmentation, tokenization) &
Dataset exceeds RAM (spills to disk) & Storage cannot feed GPU (disk
throughput limit) \\
\textbf{Algorithm} & Model too large for hardware (FLOPs exceed
capacity) & Activations exceed memory (batch size limited) & Gradient
sync slower than compute (distributed training) \\
\textbf{Machine} & GPU utilization saturated (need faster accelerator) &
Memory bandwidth saturated (need more HBM bandwidth) & Network/PCIe
bandwidth saturated (need faster links) \\
\end{longtable}

The diagnostic power of this matrix becomes clear when benchmarks reveal
unexpected results. If system benchmarks show low GPU utilization
despite adequate hardware (Machine row, Compute-Bound column: ``GPU
utilization saturated'' is \emph{not} the symptom), the bottleneck
likely lies elsewhere, perhaps in the data pipeline that cannot feed
samples fast enough (Data row, I/O-Bound column). Systematic diagnosis
using this matrix prevents the common mistake of optimizing the wrong
component.

Yet validation under controlled laboratory conditions differs
fundamentally from validation under production reality, where data
distributions shift, workloads spike unpredictably, and system
components interact in ways that isolated benchmarks cannot capture.

\section{Production
Considerations}\label{sec-benchmarking-ai-production-considerations-084b}

The three-dimensional framework validated hardware performance, model
quality, and data representativeness under controlled conditions. But a
system that passes all three benchmark categories can still fail in
production. Laboratory benchmarks assume stable data distributions,
uniform request patterns, and isolated execution. Production
environments violate all three assumptions continuously. This gap
between benchmark success and deployment success motivates a final
benchmarking concern: how do we validate systems under conditions that
match operational reality?

\textbf{From Laboratory to Production.} Laboratory benchmarks answer:
``What is my system capable of under ideal conditions?'' Production
validation answers: ``Is my system performing correctly right now, under
real conditions?''

This distinction matters because production ML systems face challenges
absent from controlled evaluation:

\begin{itemize}
\tightlist
\item
  \textbf{Silent degradation}: Models can produce plausible but
  incorrect outputs without obvious error signals, requiring continuous
  validation against labeled samples. A recommendation system returning
  ``reasonable'' but suboptimal suggestions has no built-in error
  indicator.
\item
  \textbf{Dynamic workloads}: Real traffic patterns vary dramatically
  from benchmark conditions. A system benchmarked at steady 1,000 QPS
  may fail when flash traffic events spike to 10,000 QPS, revealing that
  benchmark ``throughput'' assumed uniform request arrival, not bursty
  production patterns.
\item
  \textbf{Data distribution shift}: Production data evolves over time,
  diverging from training distributions in ways that degrade model
  performance. An image classifier trained on professional photos
  degrades gradually as users submit smartphone images with different
  lighting, angles, and compression artifacts.
\item
  \textbf{Multi-objective constraints}: Production requires simultaneous
  optimization across accuracy, latency, cost, and resource utilization,
  objectives that benchmark rankings treat independently but production
  treats as constraints.
\end{itemize}

\textbf{Bridging Benchmark to Deployment.} Before deployment, validate
your benchmarking conclusions against production-representative
conditions. The following \emph{pre-deployment benchmark checklist}
summarizes the key validation steps:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3125}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2917}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3958}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Benchmark Assumption}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Production Reality}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Validation Approach}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Uniform request arrival} & Bursty traffic patterns & Load test
with production trace replay \\
\textbf{Clean, preprocessed inputs} & Variable quality inputs & Evaluate
on production data sample \\
\textbf{Warm system state} & Cold starts, cache misses & Measure
cold-start performance \\
\textbf{Isolated execution} & Resource contention & Benchmark under
realistic system load \\
\textbf{Fixed model version} & A/B testing, gradual rollout & Establish
baseline for comparison \\
\end{longtable}

\begin{tcolorbox}[enhanced jigsaw, title=\textcolor{quarto-callout-important-color}{\faExclamation}\hspace{0.5em}{Pre-Deployment Benchmark Checklist}, rightrule=.15mm, breakable, left=2mm, arc=.35mm, toprule=.15mm, opacitybacktitle=0.6, colback=white, colframe=quarto-callout-important-color-frame, bottomrule=.15mm, colbacktitle=quarto-callout-important-color!10!white, opacityback=0, leftrule=.75mm, bottomtitle=1mm, toptitle=1mm, coltitle=black, titlerule=0mm]

Before deploying a model based on benchmark results:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Replay production traces}: Use logged request patterns to
  validate throughput/latency under realistic conditions
\item
  \textbf{Test with production data}: Sample recent production inputs
  (respecting privacy) to verify accuracy holds
\item
  \textbf{Stress test edge cases}: Identify worst-case inputs and verify
  graceful degradation
\item
  \textbf{Establish monitoring baselines}: Document expected metric
  ranges for anomaly detection
\item
  \textbf{Define rollback criteria}: Specify quantitative thresholds
  that trigger automatic rollback
\end{enumerate}

\end{tcolorbox}

\textbf{Production Monitoring as Continuous Benchmarking.} Production
monitoring extends benchmarking from a one-time gate to a continuous
process. The same principles apply (standardized metrics, reproducible
measurement, statistical rigor) but the context shifts from ``will this
work?'' to ``is this working?''

These production monitoring challenges---including A/B testing
frameworks, canary deployment strategies, shadow scoring, and continuous
validation pipelines---are examined comprehensively in
\textbf{?@sec-machine-learning-operations-mlops}. That chapter extends
the benchmarking principles established here into the dynamic
operational contexts that characterize real-world ML system deployment,
establishing infrastructure for detecting silent failures, tracking
performance degradation, and validating system behavior under production
conditions. Where this chapter asks ``how fast is my system under
controlled conditions?'',
\textbf{?@sec-machine-learning-operations-mlops} asks ``is my system
performing correctly right now?''---transitioning from offline
evaluation to continuous production verification.

\section{Fallacies and
Pitfalls}\label{sec-benchmarking-ai-fallacies-pitfalls-9781}

Benchmarking creates false confidence through standardized measurement
that obscures deployment realities. Teams assume controlled evaluations
predict production performance, but real systems face variability,
resource constraints, and multi-objective trade-offs that benchmarks
cannot capture. These fallacies waste engineering effort and produce
systems optimized for evaluation rather than deployment.

\paragraph*{\texorpdfstring{Fallacy: \emph{Benchmark performance
directly translates to real-world application
performance.}}{Fallacy: Benchmark performance directly translates to real-world application performance.}}\label{fallacy-benchmark-performance-directly-translates-to-real-world-application-performance.}
\addcontentsline{toc}{paragraph}{Fallacy: \emph{Benchmark performance
directly translates to real-world application performance.}}

Engineers select systems based on benchmark rankings. In production,
performance degrades significantly. As
Section~\ref{sec-benchmarking-ai-ml-measurement-challenges-60ea}
demonstrates, ML systems exhibit inherent variability from data quality
issues, distribution shifts, and resource constraints absent in
controlled evaluation. A language model achieving 92\% benchmark
accuracy drops to 78-82\% accuracy in production when processing
user-generated text with spelling errors, informal language, and
domain-specific terminology. An inference system with 15ms mean latency
on MLPerf experiences 150-200ms p99 latency in production due to
concurrent load, garbage collection pauses, and network variability.
Teams relying solely on benchmark rankings systematically underestimate
deployment complexity, leading to failed launches and costly
re-engineering.

\paragraph*{\texorpdfstring{Pitfall: \emph{Optimizing exclusively for
benchmark metrics without considering broader system
requirements.}}{Pitfall: Optimizing exclusively for benchmark metrics without considering broader system requirements.}}\label{pitfall-optimizing-exclusively-for-benchmark-metrics-without-considering-broader-system-requirements.}
\addcontentsline{toc}{paragraph}{Pitfall: \emph{Optimizing exclusively
for benchmark metrics without considering broader system requirements.}}

Teams optimize aggressively to improve benchmark rankings. In
deployment, benchmark-specific optimizations degrade critical
characteristics. A team reduces inference latency from 12ms to 8ms
through aggressive quantization, improving MLPerf ranking by 15
positions while degrading calibration such that prediction confidence
scores become unreliable for downstream decision-making. Another team
achieves 2.1\% ImageNet accuracy improvement through extensive
hyperparameter tuning but the optimized model consumes 40\% more energy
and exhibits 25\% worse performance on out-of-distribution images from
production cameras. This exemplifies Goodhart's Law: when benchmark
scores become optimization targets, they cease to be meaningful measures
of system quality. Organizations rewarding benchmark rankings over
deployment success systematically produce systems that excel in
evaluation but fail in production.

\paragraph*{\texorpdfstring{Fallacy: \emph{Single-metric evaluation
provides sufficient insight into system
performance.}}{Fallacy: Single-metric evaluation provides sufficient insight into system performance.}}\label{fallacy-single-metric-evaluation-provides-sufficient-insight-into-system-performance.}
\addcontentsline{toc}{paragraph}{Fallacy: \emph{Single-metric evaluation
provides sufficient insight into system performance.}}

Engineers evaluate systems using one primary metric. Production requires
balancing multiple competing objectives that single metrics obscure. As
established in Section~\ref{sec-benchmarking-ai-inference-metrics-78d4},
modern inference systems demand evaluation across accuracy, latency,
throughput, energy, and robustness dimensions. A recommendation model
achieving 94\% accuracy with 180ms p99 latency fails service-level
objectives requiring p99 \textless{} 100ms despite excellent accuracy.
Conversely, a system optimized for 1,200 QPS throughput achieves this
rate while consuming 420W versus 180W for a slightly slower system at
1,000 QPS. For battery-powered edge devices, the 18\% throughput loss
enables 2.3x longer operation time. Different stakeholders prioritize
different metrics: ML engineers focus on accuracy, infrastructure teams
on throughput and cost, product managers on latency percentiles.
Single-metric optimization systematically produces systems that excel on
one dimension while failing deployment requirements on others.

\paragraph*{\texorpdfstring{Pitfall: \emph{Using outdated benchmarks
that no longer reflect current challenges and
requirements.}}{Pitfall: Using outdated benchmarks that no longer reflect current challenges and requirements.}}\label{pitfall-using-outdated-benchmarks-that-no-longer-reflect-current-challenges-and-requirements.}
\addcontentsline{toc}{paragraph}{Pitfall: \emph{Using outdated
benchmarks that no longer reflect current challenges and requirements.}}

Teams continue using established benchmarks long after they cease to
provide meaningful discrimination. Benchmark saturation occurs when
multiple approaches achieve near-identical performance, eliminating
useful comparison. ImageNet top-5 classification error decreased from
26.2\% in 2012 to 3.57\% by 2015, with the competition ending in 2017
when 29 of 38 teams exceeded 95\% accuracy; further optimization beyond
this threshold provides marginal value for most applications. Similarly,
MNIST achieves 99.8\% accuracy with simple models, yet teams still
report improvements at the third decimal place. As discussed in
Section~\ref{sec-benchmarking-ai-statistical-methodological-issues-7aa5},
statistical confidence intervals around these measurements often exceed
the claimed improvements. Changing deployment contexts compound the
problem: benchmarks designed for server hardware become misleading for
edge devices with 10x memory constraints and 100x power budgets.
Effective benchmarking requires retiring saturated benchmarks and
developing evaluation frameworks matching current deployment realities.

\paragraph*{\texorpdfstring{Pitfall: \emph{Applying research-oriented
benchmarks to evaluate production system performance without accounting
for operational
constraints.}}{Pitfall: Applying research-oriented benchmarks to evaluate production system performance without accounting for operational constraints.}}\label{pitfall-applying-research-oriented-benchmarks-to-evaluate-production-system-performance-without-accounting-for-operational-constraints.}
\addcontentsline{toc}{paragraph}{Pitfall: \emph{Applying
research-oriented benchmarks to evaluate production system performance
without accounting for operational constraints.}}

Teams use academic benchmarks designed for research comparisons to
evaluate production systems. Research benchmarks assume unlimited
computational resources, optimal data quality, and idealized conditions
absent in production. As established in
Section~\ref{sec-benchmarking-ai-laboratorytodeployment-performance-gaps-16c8},
production systems face concurrent user loads, varying input quality,
network latency, and system failures that degrade performance. A system
achieving 800 QPS throughput in isolated benchmarks sustains only
400-500 QPS under production load with 90\% utilization due to queue
contention and garbage collection pauses. Research benchmarks report
model inference time (5-10ms) while production end-to-end latency
includes preprocessing, queuing, and postprocessing overhead totaling
50-100ms. Production systems require 99.9\% availability (43 minutes
downtime per month) and graceful degradation under failures,
characteristics research benchmarks ignore. Effective production
evaluation requires operational metrics: sustained throughput under
load, recovery time from failures, and complete latency breakdown from
request to response.

\section{Summary}\label{sec-benchmarking-ai-summary-5b23}

Benchmarking completes Part III's optimization pipeline by validating
whether the efficiency gains from data selection
(\textbf{?@sec-data-selection}), model compression
(\textbf{?@sec-model-compression}), and hardware acceleration
(\textbf{?@sec-ai-acceleration}) actually deliver in practice. Working
backward through the optimization stack (hardware first, then model
quality, then data representativeness), the three-dimensional framework
catches failures at each layer before they cascade to production.

The validation sequence reflects how problems manifest: hardware issues
surface immediately (wrong throughput, thermal throttling), model
quality issues emerge under evaluation (accuracy degradation,
calibration loss), and data issues often reveal themselves only in
production (distribution shift, demographic bias). System benchmarks
like MLPerf Training and Inference validate hardware claims with
standardized workloads. Model quality benchmarks verify that compression
preserved critical properties beyond top-line accuracy. Data benchmarks
expose representativeness gaps that no amount of hardware optimization
can compensate for.

\phantomsection\label{callout-takeawaysux2a-1.26}
\begin{fbxSimple}{callout-takeaways}{Takeaways:}{Key Takeaways}
\phantomsection\label{callout-takeaways*-1.26}

\begin{itemize}
\tightlist
\item
  \textbf{Benchmarks are proxies, not truth}: Standardized results like
  MLPerf provide comparative baselines, but production performance
  depends on your specific data distribution, load patterns, and SLA
  constraints.
\item
  \textbf{Apply the DAM Taxonomy for Diagnosis}: When benchmark results
  fall short, use the DAM framework to isolate the bottleneck: is it
  \textbf{Data} (Information: I/O bandwidth, preprocessing overhead),
  \textbf{Algorithm} (Logic: arithmetic intensity, model complexity), or
  \textbf{Machine} (Physics: peak throughput, thermal throttling, memory
  bandwidth)?
\item
  \textbf{The tail determines the user experience}: Average latency
  obscures performance failures. Benchmarking for interactive systems
  must report p95 and p99 tail latencies to ensure SLO compliance under
  load.
\item
  \textbf{Amdahl's Law sets the optimization ceiling}: Model speedup is
  limited by the non-model fraction of the pipeline. If preprocessing
  consumes 50\% of the latency, even an infinite-speed model can only
  achieve 2x total system improvement.
\item
  \textbf{Precision is an energy lever}: INT8 quantization provides 4x
  memory reduction but can deliver 10-20x energy reduction by shifting
  the balance from energy-intensive DRAM access to efficient integer
  arithmetic.
\end{itemize}

\end{fbxSimple}

Benchmarking discipline separates engineering from guesswork. The
practitioners who rigorously validate their optimizations---measuring
wall-clock latency rather than trusting FLOP counts, profiling tail
latencies rather than averages, testing on production-representative
data rather than convenient benchmarks---build systems that perform as
expected when deployed. As AI systems become increasingly influential in
critical applications, this measurement rigor determines whether
optimization claims translate into real-world impact.

\phantomsection\label{callout-chapter-connectionux2a-1.27}
\begin{fbxSimple}{callout-chapter-connection}{What’s Next:}{From Lab to Live}
\phantomsection\label{callout-chapter-connection*-1.27}
We have validated our optimizations in the lab, but a benchmark is a
map, not the territory. Production reality includes traffic bursts, data
drift, and cascading failures that no static benchmark can capture. In
Part IV, we leave the controlled environment of the benchmark for the
chaotic reality of production, beginning with
\textbf{?@sec-model-serving-systems} where systems must survive contact
with the real world.

\end{fbxSimple}

\FloatBarrier\clearpage

\setpartsummary{This part covers the operational reality of ML systems. It spans serving infrastructure, MLOps practices, and responsible engineering, addressing the challenges of maintaining reliable, ethical, and scalable systems in production.}

\addtocontents{toc}{\par\addvspace{12pt}\noindent\hfil\bfseries\color{crimson}Part~I~Deployment and Operations\color{black}\hfil\par\addvspace{6pt}}

\numberedpart{Deployment and Operations}

\haspartsummaryfalse

\phantomsection\label{refs}
\begin{CSLReferences}{1}{0}
\bibitem[\citeproctext]{ref-deepbench_github}
Alosco, Michael L., Megan L. Mariani, Charles H. Adler, Laura J. Balcer,
Charles Bernick, Rhoda Au, Sarah J. Banks, et al. 2021. {``Developing
Methods to Detect and Diagnose Chronic Traumatic Encephalopathy During
Life: Rationale, Design, and Methodology for the DIAGNOSE CTE Research
Project.''} \emph{Alzheimer's Research \&Amp; Therapy} 13 (1): 136.
\url{https://doi.org/10.1186/s13195-021-00872-x}.

\bibitem[\citeproctext]{ref-banbury2021mlperf}
Banbury, Colby, Vijay Janapa Reddi, Peter Torelli, Jeremy Holleman, Nat
Jeffries, Csaba Kiraly, Pietro Montino, et al. 2021. {``MLPerf Tiny
Benchmark.''} \emph{arXiv Preprint arXiv:2106.07597}, June.
\url{http://arxiv.org/abs/2106.07597v4}.

\bibitem[\citeproctext]{ref-barroso2022datacenter}
Barroso, Luiz André, Jimmy Clidaras, and Urs Hölzle. 2013. \emph{The
Datacenter as a Computer: An Introduction to the Design of
Warehouse-Scale Machines}. Springer International Publishing.
\url{https://doi.org/10.1007/978-3-031-01741-4}.

\bibitem[\citeproctext]{ref-barroso2019datacenter}
Barroso, Luiz André, Urs Hölzle, and Parthasarathy Ranganathan. 2019.
\emph{The Datacenter as a Computer: Designing Warehouse-Scale Machines}.
Springer International Publishing.
\url{https://doi.org/10.1007/978-3-031-01761-2}.

\bibitem[\citeproctext]{ref-bender2021stochastic}
Bender, Emily M., Timnit Gebru, Angelina McMillan-Major, and Shmargaret
Shmitchell. 2021. {``On the Dangers of Stochastic Parrots: Can Language
Models Be Too Big? 🦜.''} In \emph{Proceedings of the 2021 ACM
Conference on Fairness, Accountability, and Transparency}, 610--23. ACM.
\url{https://doi.org/10.1145/3442188.3445922}.

\bibitem[\citeproctext]{ref-beyer2020we}
Beyer, Lucas, Olivier J. Hénaff, Alexander Kolesnikov, Xiaohua Zhai, and
Aäron van den Oord. 2020. {``Are We Done with ImageNet?''} \emph{arXiv
Preprint arXiv:2006.07159}, June.
\url{http://arxiv.org/abs/2006.07159v1}.

\bibitem[\citeproctext]{ref-imagenet_website}
Blaivas, Laura, and Michael Blaivas. 2020. {``Are Convolutional Neural
Networks Trained on
\textless Scp\textgreater ImageNet\textless/Scp\textgreater{} Images
Wearing
\textless Scp\textgreater rose-Colored\textless/Scp\textgreater{}
Glasses?: A Quantitative Comparison of
\textless Scp\textgreater ImageNet\textless/Scp\textgreater, Computed
Tomographic, Magnetic Resonance, Chest
\textless Scp\textgreater x-Ray\textless/Scp\textgreater, and
\textless Scp\textgreater point-of-Care\textless/Scp\textgreater{}
Ultrasound Images for Quality.''} \emph{Journal of Ultrasound in
Medicine} 40 (2): 377--83. \url{https://doi.org/10.1002/jum.15413}.

\bibitem[\citeproctext]{ref-BoroumandASPLOS2018}
Boroumand, Amirali, Saugata Ghose, Youngsok Kim, Rachata
Ausavarungnirun, Eric Shiu, Rahul Thakur, Daehyun Kim, et al. 2018.
{``Google Workloads for Consumer Devices: Mitigating Data Movement
Bottlenecks.''} In \emph{Proceedings of the Twenty-Third International
Conference on Architectural Support for Programming Languages and
Operating Systems}, 316--31. ASPLOS '18. ACM.
\url{https://doi.org/10.1145/3173162.3173177}.

\bibitem[\citeproctext]{ref-brown2020language}
Brown, Tom B., Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan,
Prafulla Dhariwal, Arvind Neelakantan, et al. 2020. {``Language Models
Are Few-Shot Learners.''} Edited by Hugo Larochelle, Marc'Aurelio
Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin.
\emph{Advances in Neural Information Processing Systems} 33 (May):
1877--1901. \url{https://doi.org/10.48550/arxiv.2005.14165}.

\bibitem[\citeproctext]{ref-buolamwini2018gender}
Buolamwini, Joy, and Timnit Gebru. 2018. {``Gender Shades:
Intersectional Accuracy Disparities in Commercial Gender
Classification.''} In \emph{Conference on Fairness, Accountability and
Transparency}, 77--91. PMLR.
\url{http://proceedings.mlr.press/v81/buolamwini18a.html}.

\bibitem[\citeproctext]{ref-chetlur2014cudnn}
Chetlur, Sharan, Cliff Woolley, Philippe Vandermersch, Jonathan Cohen,
John Tran, Bryan Catanzaro, and Evan Shelhamer. 2014. {``cuDNN:
Efficient Primitives for Deep Learning.''} \emph{arXiv Preprint
arXiv:1410.0759}, October. \url{http://arxiv.org/abs/1410.0759v3}.

\bibitem[\citeproctext]{ref-nvidia2020a100}
Choquette, Jack, Wishwesh Gandhi, Olivier Giroux, Nick Stam, and Ronny
Krashinsky. 2021. {``NVIDIA A100 Tensor Core GPU: Performance and
Innovation.''} \emph{IEEE Micro} 41 (2): 29--35.
\url{https://doi.org/10.1109/mm.2021.3061394}.

\bibitem[\citeproctext]{ref-chu2021discovering}
Chu, Grace, Okan Arikan, Gabriel Bender, Weijun Wang, Achille Brighton,
Pieter-Jan Kindermans, Hanxiao Liu, Berkin Akin, Suyog Gupta, and Andrew
Howard. 2021. {``Discovering Multi-Hardware Mobile Models via
Architecture Search.''} In \emph{2021 IEEE/CVF Conference on Computer
Vision and Pattern Recognition Workshops (CVPRW)}, 3016--25. IEEE.
\url{https://doi.org/10.1109/cvprw53098.2021.00337}.

\bibitem[\citeproctext]{ref-coleman2017dawnbench}
Coleman, Cody, Daniel Kang, Deepak Narayanan, Luigi Nardi, Tian Zhao,
Jian Zhang, Peter Bailis, Kunle Olukotun, Chris Ré, and Matei Zaharia.
2019. {``Analysis of DAWNBench, a Time-to-Accuracy Machine Learning
Performance Benchmark.''} \emph{ACM SIGOPS Operating Systems Review} 53
(1): 14--25. \url{https://doi.org/10.1145/3352020.3352024}.

\bibitem[\citeproctext]{ref-deng2009imagenet}
Deng, Jia, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei.
2009. {``ImageNet: A Large-Scale Hierarchical Image Database.''} In
\emph{2009 IEEE Conference on Computer Vision and Pattern Recognition},
248--55. Ieee; IEEE. \url{https://doi.org/10.1109/cvpr.2009.5206848}.

\bibitem[\citeproctext]{ref-duarte2022fastml}
Duarte, Javier, Nhan Tran, Ben Hawks, Christian Herwig, Jules Muhizi,
Shvetank Prakash, and Vijay Janapa Reddi. 2022. {``FastML Science
Benchmarks: Accelerating Real-Time Scientific Edge Machine Learning.''}
\emph{arXiv Preprint arXiv:2207.07958}, July.
\url{http://arxiv.org/abs/2207.07958v1}.

\bibitem[\citeproctext]{ref-everingham2010pascal}
Everingham, Mark, Luc Van Gool, Christopher K. I. Williams, John Winn,
and Andrew Zisserman. 2009. {``The Pascal Visual Object Classes (VOC)
Challenge.''} \emph{International Journal of Computer Vision} 88 (2):
303--38. \url{https://doi.org/10.1007/s11263-009-0275-4}.

\bibitem[\citeproctext]{ref-frankle2018lottery}
Frankle, Jonathan, and Michael Carbin. 2019. {``The Lottery Ticket
Hypothesis: Finding Sparse, Trainable Neural Networks.''} In
\emph{International Conference on Learning Representations}.
\url{https://openreview.net/forum?id=rJl-b3RcF7}.

\bibitem[\citeproctext]{ref-gadre2024datacomp}
Gadre, Samir Yitzhak, Gabriel Ilharco, Alex Fang, Jonathan Hayase,
Georgios Smber, Thao Nguyen, Ryan Marten, et al. 2023. {``DataComp: In
Search of the Next Generation of Multimodal Datasets.''} \emph{Advances
in Neural Information Processing Systems} 36:
27092--112.\href{\%0A\%20\%20\%20\%20https://proceedings.neurips.cc/paper/_files/paper/2023/file/56332d41d4b195a35c5b5d9a4a6c9e30-Paper-Datasets/_and/_Benchmarks.pdf\%0A\%20\%20}{https://proceedings.neurips.cc/paper\textbackslash\_files/paper/2023/file/56332d41d4b195a35c5b5d9a4a6c9e30-Paper-Datasets\textbackslash\_and\textbackslash\_Benchmarks.pdf
}.

\bibitem[\citeproctext]{ref-gebru2021datasheets}
Gebru, Timnit, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman
Vaughan, Hanna Wallach, Hal Daumé III, and Kate Crawford. 2021.
{``Datasheets for Datasets.''} \emph{Communications of the ACM} 64 (12):
86--92. \url{https://doi.org/10.1145/3458723}.

\bibitem[\citeproctext]{ref-han2016deep}
Han, Song, Huizi Mao, and William J. Dally. 2015. {``Deep Compression:
Compressing Deep Neural Networks with Pruning, Trained Quantization and
Huffman Coding,''} October. \url{http://arxiv.org/abs/1510.00149v5}.

\bibitem[\citeproctext]{ref-henderson2020towards}
Henderson, Peter, Jieru Hu, Joshua Romoff, Emma Brunskill, Dan Jurafsky,
and Joelle Pineau. 2020. {``Towards the Systematic Reporting of the
Energy and Carbon Footprints of Machine Learning.''} \emph{CoRR}
abs/2002.05651 (248): 1--43.
\url{https://doi.org/10.48550/arxiv.2002.05651}.

\bibitem[\citeproctext]{ref-henderson2018deep}
Henderson, Peter, Riashat Islam, Philip Bachman, Joelle Pineau, Doina
Precup, and David Meger. 2018. {``Deep Reinforcement Learning That
Matters.''} \emph{Proceedings of the AAAI Conference on Artificial
Intelligence} 32 (1): 3207--14.
\url{https://doi.org/10.1609/aaai.v32i1.11694}.

\bibitem[\citeproctext]{ref-hendrycks2021mmlu}
Hendrycks, Dan, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika,
Dawn Song, and Jacob Steinhardt. 2020. {``Measuring Massive Multitask
Language Understanding,''} September.
\url{http://arxiv.org/abs/2009.03300v3}.

\bibitem[\citeproctext]{ref-hernandez2020measuring}
Hernandez, Danny, and Tom B. Brown. 2020. {``Measuring the Algorithmic
Efficiency of Neural Networks.''} \emph{arXiv Preprint
arXiv:2007.03051}, May. \url{https://doi.org/10.48550/arxiv.2005.04305}.

\bibitem[\citeproctext]{ref-hirschberg2015advances}
Hirschberg, Julia, and Christopher D. Manning. 2015. {``Advances in
Natural Language Processing.''} \emph{Science} 349 (6245): 261--66.
\url{https://doi.org/10.1126/science.aaa8685}.

\bibitem[\citeproctext]{ref-hooker2021hardware}
Hooker, Sara. 2021. {``The Hardware Lottery.''} \emph{Communications of
the ACM} 64 (12): 58--65. \url{https://doi.org/10.1145/3467017}.

\bibitem[\citeproctext]{ref-howard2017mobilenets}
Howard, Andrew G., Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun
Wang, Tobias Weyand, Marco Andreetto, and Hartwig Adam. 2017.
{``MobileNets: Efficient Convolutional Neural Networks for Mobile Vision
Applications.''} \emph{ArXiv Preprint} abs/1704.04861 (April).
\url{http://arxiv.org/abs/1704.04861v1}.

\bibitem[\citeproctext]{ref-ai_benchmark_website}
Ignatov, Andrey, and Radu Timofte. 2024. {``AI Benchmark.''}
\url{https://ai-benchmark.com/}.

\bibitem[\citeproctext]{ref-iso_tc}
ISO. 2024. {``ISO/IEC JTC 1/SC 42 Artificial Intelligence.''}
\url{https://www.iso.org/committee/45020.html}.

\bibitem[\citeproctext]{ref-jacob2018quantization}
Jacob, Benoit, Skirmantas Kligys, Bo Chen, Menglong Zhu, Matthew Tang,
Andrew Howard, Hartwig Adam, and Dmitry Kalenichenko. 2018.
{``Quantization and Training of Neural Networks for Efficient
Integer-Arithmetic-Only Inference.''} In \emph{2018 IEEE/CVF Conference
on Computer Vision and Pattern Recognition}, 2704--13. IEEE.
\url{https://doi.org/10.1109/cvpr.2018.00286}.

\bibitem[\citeproctext]{ref-janapa2022mlperf}
Janapa Reddi, Vijay et al. 2022. {``MLPerf Mobile V2. 0: An
Industry-Standard Benchmark Suite for Mobile Machine Learning.''} In
\emph{Proceedings of Machine Learning and Systems}, 4:806--23.

\bibitem[\citeproctext]{ref-jouppi2021ten}
Jouppi, Norman P., Doe Hyun Yoon, Matthew Ashcraft, Mark Gottscho,
Thomas B. Jablin, George Kurian, James Laudon, et al. 2021. {``Ten
Lessons from Three Generations Shaped Google's TPUv4i : Industrial
Product.''} In \emph{2021 ACM/IEEE 48th Annual International Symposium
on Computer Architecture (ISCA)}, 64:1--14. 5. IEEE.
\url{https://doi.org/10.1109/isca52012.2021.00010}.

\bibitem[\citeproctext]{ref-jouppi2017datacenter}
Jouppi, Norman P., Cliff Young, Nishant Patil, David Patterson, Gaurav
Agrawal, Raminder Bajwa, Sarah Bates, et al. 2017. {``In-Datacenter
Performance Analysis of a Tensor Processing Unit.''} In
\emph{Proceedings of the 44th Annual International Symposium on Computer
Architecture}, 1--12. ISCA '17. New York, NY, USA: ACM.
\url{https://doi.org/10.1145/3079856.3080246}.

\bibitem[\citeproctext]{ref-kiela2021dynabench}
Kiela, Douwe, Max Bartolo, Yixin Nie, Divyansh Kaushik, Atticus Geiger,
Zhengxuan Wu, Bertie Vidgen, et al. 2021. {``Dynabench: Rethinking
Benchmarking in NLP.''} In \emph{Proceedings of the 2021 Conference of
the North American Chapter of the Association for Computational
Linguistics: Human Language Technologies}, 9:4110--24. Online:
Association for Computational Linguistics.
\url{https://doi.org/10.18653/v1/2021.naacl-main.324}.

\bibitem[\citeproctext]{ref-kim2008system}
Kim, Wonyoung, Meeta S. Gupta, Gu-Yeon Wei, and David Brooks. 2008.
{``System Level Analysis of Fast, Per-Core DVFS Using on-Chip Switching
Regulators.''} In \emph{2008 IEEE 14th International Symposium on High
Performance Computer Architecture}, 123--34. HPCA '08. IEEE.
\url{https://doi.org/10.1109/hpca.2008.4658633}.

\bibitem[\citeproctext]{ref-koh2021wilds}
Koh, Pang Wei, Shiori Sagawa, Henrik Marklund, Sang Michael Xie, Marvin
Zhang, Akshay Balsubramani, Weihua Hu, et al. 2021. {``WILDS: A
Benchmark of in-the-Wild Distribution Shifts.''} In \emph{Proceedings of
the 38th International Conference on Machine Learning, ICML 2021, 18-24
July 2021, Virtual Event}, edited by Marina Meila and Tong Zhang,
139:5637--64. Proceedings of Machine Learning Research. PMLR.
\url{http://proceedings.mlr.press/v139/koh21a.html}.

\bibitem[\citeproctext]{ref-koizumi2019toyadmos}
Koizumi, Yuma, Shoichiro Saito, Hisashi Uematsu, Noboru Harada, and
Keisuke Imoto. 2019. {``ToyADMOS: A Dataset of Miniature-Machine
Operating Sounds for Anomalous Sound Detection.''} In \emph{2019 IEEE
Workshop on Applications of Signal Processing to Audio and Acoustics
(WASPAA)}, 313--17. IEEE; IEEE.
\url{https://doi.org/10.1109/waspaa.2019.8937164}.

\bibitem[\citeproctext]{ref-koomey2011web}
Koomey, Jonathan, Stephen Berard, Marla Sanchez, and Henry Wong. 2011.
{``Implications of Historical Trends in the Electrical Efficiency of
Computing.''} \emph{IEEE Annals of the History of Computing} 33 (3):
46--54. \url{https://doi.org/10.1109/mahc.2010.28}.

\bibitem[\citeproctext]{ref-krizhevsky2009learning}
Krizhevsky, Alex, Geoffrey Hinton, et al. 2009. {``Learning Multiple
Layers of Features from Tiny Images.''}

\bibitem[\citeproctext]{ref-lange2009identifying}
Lange, Klaus-Dieter. 2009. {``Identifying Shades of Green: The SPECpower
Benchmarks.''} \emph{Computer} 42 (3): 95--97.
\url{https://doi.org/10.1109/mc.2009.84}.

\bibitem[\citeproctext]{ref-le2010dynamic}
Le Sueur, Etienne, and Gernot Heiser. 2010. {``Dynamic Voltage and
Frequency Scaling: The Laws of Diminishing Returns.''} In
\emph{Proceedings of the 2010 International Conference on Power Aware
Computing and Systems}, 1--8.

\bibitem[\citeproctext]{ref-cifar10_website}
LeCun, Yann, Han Xiao, and Alex Krizhevsky. 2025. {``A Collection of
Foundational Image Classification Datasets (MNIST, Fashion-MNIST,
CIFAR-10, CIFAR-100).''} \emph{IEEE DataPort}.
\url{https://doi.org/10.21227/8TDH-NW85}.

\bibitem[\citeproctext]{ref-lecun1998gradient}
Lecun, Y., L. Bottou, Y. Bengio, and P. Haffner. 1998. {``Gradient-Based
Learning Applied to Document Recognition.''} \emph{Proceedings of the
IEEE} 86 (11): 2278--2324. \url{https://doi.org/10.1109/5.726791}.

\bibitem[\citeproctext]{ref-liang2022helm}
Liang, Percy, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu,
Michihiro Yasunaga, Yian Zhang, et al. 2022. {``Holistic Evaluation of
Language Models.''} \emph{arXiv Preprint arXiv:2211.09110}, November.
\url{http://arxiv.org/abs/2211.09110v2}.

\bibitem[\citeproctext]{ref-coco_website}
Lin, Tsung-Yi, Michael Maire, Serge Belongie, James Hays, Pietro Perona,
Deva Ramanan, Piotr Dollár, and C. Lawrence Zitnick. 2014a. {``Microsoft
COCO: Common Objects in Context.''} In \emph{Computer Vision -- ECCV
2014}, 740--55. Springer International Publishing.
\url{https://doi.org/10.1007/978-3-319-10602-1/_48}.

\bibitem[\citeproctext]{ref-lin2014microsoft}
---------. 2014b. {``Microsoft COCO: Common Objects in Context.''} In
\emph{Computer Vision -- ECCV 2014}, 740--55. Springer; Springer
International Publishing.
\url{https://doi.org/10.1007/978-3-319-10602-1/_48}.

\bibitem[\citeproctext]{ref-spec_power_website}
Lockhart, Catherine M., Elizabeth Powers, Brian Sweet, Patrick P.
Gleason, and Diana Brixner. 2025. {``AMCP Real-World Evidence Standards:
Overcoming Barriers to Using Real-World Evidence in US Payer
Decision-Making.''} \emph{Journal of Managed Care \&Amp; Specialty
Pharmacy} 31 (12): 1230--36.
\url{https://doi.org/10.18553/jmcp.2025.25108}.

\bibitem[\citeproctext]{ref-lucic2018gans}
Lucic, Mario, Karol Kurach, Marcin Michalski, Sylvain Gelly, and Olivier
Bousquet. 2018. {``Are GANs Created Equal? A Large-Scale Study.''} In
\emph{Advances in Neural Information Processing Systems}. Vol. 31.
\url{https://proceedings.neurips.cc/paper/2018/file/e46e7bb42968e44f4b3e72f703b6de8f-Paper.pdf}.

\bibitem[\citeproctext]{ref-mattson2020mlperf}
Mattson, Peter, Vijay Janapa Reddi, Christine Cheng, Cody Coleman, Greg
Diamos, David Kanter, Paulius Micikevicius, et al. 2020. {``MLPerf: An
Industry Standard Benchmark Suite for Machine Learning Performance.''}
\emph{IEEE Micro} 40 (2): 8--16.
\url{https://doi.org/10.1109/mm.2020.2974843}.

\bibitem[\citeproctext]{ref-wikitext_website}
Merity, Stephen. 2016. {``The WikiText Long Term Dependency Language
Modeling Dataset.''}
\url{https://www.salesforce.com/blog/the-wikitext-long-term-dependency-language-modeling-dataset/}.

\bibitem[\citeproctext]{ref-merity2016pointer}
Merity, Stephen, Caiming Xiong, James Bradbury, and Richard Socher.
2016. {``Pointer Sentinel Mixture Models.''} \emph{arXiv Preprint
arXiv:1609.07843}, September. \url{http://arxiv.org/abs/1609.07843v1}.

\bibitem[\citeproctext]{ref-mlperf_client_website}
MLCommons. 2024a. {``MLPerf Client Benchmark.''}
\url{https://mlcommons.org/en/inference-edge/}.

\bibitem[\citeproctext]{ref-mlperf_inference_website}
---------. 2024b. {``MLPerf Inference Benchmark.''}
\url{https://mlcommons.org/en/inference-datacenter/}.

\bibitem[\citeproctext]{ref-mlperf_mobile_website}
---------. 2024c. {``MLPerf Mobile Benchmark.''}
\url{https://mlcommons.org/en/mlperf-mobile/}.

\bibitem[\citeproctext]{ref-mlperf_power_website}
---------. 2024d. {``MLPerf Power Measurement.''}
\url{https://mlcommons.org/}.

\bibitem[\citeproctext]{ref-mlperf_tiny_website}
---------. 2024e. {``MLPerf Tiny Benchmark.''}
\url{https://mlcommons.org/en/inference-tiny/}.

\bibitem[\citeproctext]{ref-mlperf_training_website}
---------. 2024f. {``MLPerf Training Benchmark.''}
\url{https://mlcommons.org/benchmarks/training/}.

\bibitem[\citeproctext]{ref-northcutt2021pervasive}
Northcutt, Curtis G., Anish Athalye, and Jonas Mueller. 2021.
{``Pervasive Label Errors in Test Sets Destabilize Machine Learning
Benchmarks.''} \emph{arXiv Preprint arXiv:2103.14749} 34 (March):
19075--90. \url{https://doi.org/10.48550/arxiv.2103.14749}.

\bibitem[\citeproctext]{ref-papineni2002bleu}
Papineni, Kishore, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2001.
{``BLEU: A Method for Automatic Evaluation of Machine Translation.''} In
\emph{Proceedings of the 40th Annual Meeting on Association for
Computational Linguistics - ACL '02}, 311. Association for Computational
Linguistics. \url{https://doi.org/10.3115/1073083.1073135}.

\bibitem[\citeproctext]{ref-patterson2021carbon}
Patterson, David, Joseph Gonzalez, Quoc Le, Chen Liang, Lluis-Miquel
Munguia, Daniel Rothchild, David So, Maud Texier, and Jeff Dean. 2021.
{``Carbon Emissions and Large Neural Network Training.''} \emph{arXiv
Preprint arXiv:2104.10350}, April.
\url{http://arxiv.org/abs/2104.10350v3}.

\bibitem[\citeproctext]{ref-ieee_2416_2019}
Peyghami, Saeed, and Frede Blaabjerg. 2020. {``Availability Modeling in
Power Converters Considering Components Aging.''} \emph{IEEE
Transactions on Energy Conversion} 35 (4): 1981--84.
\url{https://doi.org/10.1109/tec.2020.3018631}.

\bibitem[\citeproctext]{ref-pope2022efficiently}
Pope, Reiner, Sholto Douglas, Aakanksha Chowdhery, Jacob Devlin, James
Bradbury, Anselm Levskaya, Jonathan Heek, Kefan Xiao, Shivani Agrawal,
and Jeff Dean. 2022. {``Efficiently Scaling Transformer Inference.''}
\emph{arXiv Preprint arXiv:2211.05102}, November.
\url{http://arxiv.org/abs/2211.05102v1}.

\bibitem[\citeproctext]{ref-squad_website}
Rajpurkar, Pranav, Jian Zhang, Konstantin Lopyrev, and Percy Liang.
2016a. {``The Stanford Question Answering Dataset.''}
\url{https://rajpurkar.github.io/SQuAD-explorer/}.

\bibitem[\citeproctext]{ref-rajpurkar2016squad}
---------. 2016b. {``SQuAD: 100,000+ Questions for Machine Comprehension
of Text.''} \emph{arXiv Preprint arXiv:1606.05250}, June, 2383--92.
\url{https://doi.org/10.18653/v1/d16-1264}.

\bibitem[\citeproctext]{ref-ranganathan2024twenty}
Ranganathan, Parthasarathy, and Urs Hölzle. 2024. {``Twenty Five Years
of Warehouse-Scale Computing.''} \emph{IEEE Micro} 44 (5): 11--22.
\url{https://doi.org/10.1109/mm.2024.3409469}.

\bibitem[\citeproctext]{ref-reddi2020mlperf}
Reddi, Vijay Janapa, Christine Cheng, David Kanter, Peter Mattson,
Guenther Schmuelling, Carole-Jean Wu, Brian Anderson, et al. 2019.
{``MLPerf Inference Benchmark.''} \emph{arXiv Preprint
arXiv:1911.02549}, November, 446--59.
\url{https://doi.org/10.1109/isca45697.2020.00045}.

\bibitem[\citeproctext]{ref-ilsvrc_website}
Russakovsky, Olga, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh,
Sean Ma, Zhiheng Huang, et al. 2015a. {``ImageNet Large Scale Visual
Recognition Challenge.''} \emph{International Journal of Computer
Vision} 115 (3): 211--52.
\url{https://doi.org/10.1007/s11263-015-0816-y}.

\bibitem[\citeproctext]{ref-russakovsky2015imagenet}
---------, et al. 2015b. {``ImageNet Large Scale Visual Recognition
Challenge.''} \emph{International Journal of Computer Vision} 115 (3):
211--52. \url{https://doi.org/10.1007/s11263-015-0816-y}.

\bibitem[\citeproctext]{ref-ieee_working_groups}
Sharman, James E., Isabella Tan, George S. Stergiou, Carolina Lombardi,
Francesca Saladini, Mark Butlin, Raj Padwal, et al. 2022. {``Automated
{`Oscillometric'} Blood Pressure Measuring Devices: How They Work and
What They Measure.''} \emph{Journal of Human Hypertension} 37 (2):
93--100. \url{https://doi.org/10.1038/s41371-022-00693-x}.

\bibitem[\citeproctext]{ref-sokolova2009systematic}
Sokolova, Marina, and Guy Lapalme. 2009. {``A Systematic Analysis of
Performance Measures for Classification Tasks.''} \emph{Information
Processing \&Amp; Management} 45 (4): 427--37.
\url{https://doi.org/10.1016/j.ipm.2009.03.002}.

\bibitem[\citeproctext]{ref-tschand2024mlperf}
Tschand, Arya, Arun Tejusve Raghunath Rajan, Sachin Idgunji, Anirban
Ghosh, Jeremy Holleman, Csaba Kiraly, Pawan Ambalkar, et al. 2024.
{``MLPerf Power: Benchmarking the Energy Efficiency of Machine Learning
Systems from Microwatts to Megawatts for Sustainable AI.''} \emph{arXiv
Preprint arXiv:2410.12032}, October.
\url{http://arxiv.org/abs/2410.12032v2}.

\bibitem[\citeproctext]{ref-wang2019superglue}
Wang, Alex, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian
Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. 2019.
{``SuperGLUE: A Stickier Benchmark for General-Purpose Language
Understanding Systems.''} \emph{arXiv Preprint arXiv:1905.00537}, May.
\url{http://arxiv.org/abs/1905.00537v3}.

\bibitem[\citeproctext]{ref-glue_website}
Wang, Alex, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and
Samuel Bowman. 2018a. {``GLUE: A Multi-Task Benchmark and Analysis
Platform for Natural Language Understanding.''} In \emph{Proceedings of
the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural
Networks for NLP}, 353--55. Association for Computational Linguistics.
\url{https://doi.org/10.18653/v1/w18-5446}.

\bibitem[\citeproctext]{ref-wang2018glue}
Wang, Alex, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and
Samuel R. Bowman. 2018b. {``GLUE: A Multi-Task Benchmark and Analysis
Platform for Natural Language Understanding.''} \emph{arXiv Preprint
arXiv:1804.07461}, April. \url{http://arxiv.org/abs/1804.07461v3}.

\bibitem[\citeproctext]{ref-xu2024benchmarking}
Xu, Ruijie, Zengzhi Wang, Run-Ze Fan, and Pengfei Liu. 2024.
{``Benchmarking Benchmark Leakage in Large Language Models.''}
\emph{arXiv Preprint arXiv:2404.18824}, April.
\url{http://arxiv.org/abs/2404.18824v1}.

\bibitem[\citeproctext]{ref-you2019scaling}
You, Yang, Zhao Zhang, Cho-Jui Hsieh, James Demmel, and Kurt Keutzer.
2019. {``Scaling SGD Batch Size to 32K for ImageNet Training.''} In
\emph{Proceedings of Machine Learning and Systems}.

\end{CSLReferences}


\backmatter

\clearpage


\end{document}
