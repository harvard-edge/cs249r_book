% Options for packages loaded elsewhere
% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode,linktoc=all,pdfwindowui,pdfpagemode=FullScreen,pdfpagelayout=TwoPageRight}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  9pt,
  letterpaper,
  abstract,
  titlepage]{scrbook}
\usepackage{xcolor}
\usepackage[left=1in,marginparwidth=2.0666666666667in,textwidth=4.1333333333333in,marginparsep=0.3in]{geometry}
\usepackage{amsmath,amssymb}
\setcounter{secnumdepth}{3}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
% Make \paragraph and \subparagraph free-standing
\makeatletter
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}{
    \@ifstar
      \xxxParagraphStar
      \xxxParagraphNoStar
  }
  \newcommand{\xxxParagraphStar}[1]{\oldparagraph*{#1}\mbox{}}
  \newcommand{\xxxParagraphNoStar}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}{
    \@ifstar
      \xxxSubParagraphStar
      \xxxSubParagraphNoStar
  }
  \newcommand{\xxxSubParagraphStar}[1]{\oldsubparagraph*{#1}\mbox{}}
  \newcommand{\xxxSubParagraphNoStar}[1]{\oldsubparagraph{#1}\mbox{}}
\fi
\makeatother

\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{242,244,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{#1}}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.82,0.10,0.26}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{#1}}
\newcommand{\BuiltInTok}[1]{\textcolor[rgb]{0.88,0.40,0.10}{#1}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.41,0.45,0.49}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.86,0.20,0.18}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.58,0.00,0.30}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{#1}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.75,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.88,0.40,0.10}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.42,0.37,0.78}{#1}}
\newcommand{\ImportTok}[1]{\textcolor[rgb]{0.00,0.45,0.70}{\textbf{#1}}}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.73,0.49,0.84}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.10,0.10,0.10}{#1}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{#1}}
\newcommand{\RegionMarkerTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.12,0.55,0.30}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{#1}}}}

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\newsavebox\pandoc@box
\newcommand*\pandocbounded[1]{% scales image to fit in text height/width
  \sbox\pandoc@box{#1}%
  \Gscale@div\@tempa{\textheight}{\dimexpr\ht\pandoc@box+\dp\pandoc@box\relax}%
  \Gscale@div\@tempb{\linewidth}{\wd\pandoc@box}%
  \ifdim\@tempb\p@<\@tempa\p@\let\@tempa\@tempb\fi% select the smaller of both
  \ifdim\@tempa\p@<\p@\scalebox{\@tempa}{\usebox\pandoc@box}%
  \else\usebox{\pandoc@box}%
  \fi%
}
% Set default figure placement to htbp
\def\fps@figure{htbp}
\makeatother
% definitions for citeproc citations
\NewDocumentCommand\citeproctext{}{}
\NewDocumentCommand\citeproc{mm}{%
  \begingroup\def\citeproctext{#2}\cite{#1}\endgroup}
\makeatletter
 % allow citations to break across lines
 \let\@cite@ofmt\@firstofone
 % avoid brackets around text for \cite:
 \def\@biblabel#1{}
 \def\@cite#1#2{{#1\if@tempswa , #2\fi}}
\makeatother
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newenvironment{CSLReferences}[2] % #1 hanging-indent, #2 entry-spacing
 {\begin{list}{}{%
  \setlength{\itemindent}{0pt}
  \setlength{\leftmargin}{0pt}
  \setlength{\parsep}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1
   \setlength{\leftmargin}{\cslhangindent}
   \setlength{\itemindent}{-1\cslhangindent}
  \fi
  % set entry spacing
  \setlength{\itemsep}{#2\baselineskip}}}
 {\end{list}}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{\hfill\break\parbox[t]{\linewidth}{\strut\ignorespaces#1\strut}}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}

% =============================================================================
% LATEX HEADER CONFIGURATION FOR MLSYSBOOK PDF
% =============================================================================
% This file contains all LaTeX package imports, custom commands, and styling
% definitions for the PDF output of the Machine Learning Systems textbook.
%
% Key Features:
% - Harvard crimson branding throughout
% - Custom part/chapter/section styling
% - Professional table formatting with colored headers
% - Margin notes with custom styling
% - TikZ-based part dividers
% - Page numbering (Roman for frontmatter, Arabic for mainmatter)
%
% Note: This file is included via _quarto-pdf.yml and affects PDF output only.
% HTML/EPUB styling is handled separately via CSS files.
% =============================================================================

% =============================================================================
% PACKAGE IMPORTS
% =============================================================================

% Layout and positioning
% \usepackage[outercaption, ragged]{sidecap}  % Commented out to make figure captions inline instead of in margin
\usepackage{adjustbox}      % Adjusting box dimensions
\usepackage{afterpage}      % Execute commands after page break
\usepackage{morefloats}     % Increase number of floats
\usepackage{array}          % Enhanced table column formatting
\usepackage{atbegshi}       % Insert content at page beginning
%\usepackage{changepage}     % Change page dimensions mid-document
\usepackage{emptypage}      % Clear headers/footers on empty pages

% Language and text
\usepackage[english]{babel} % English language support
\usepackage{microtype}      % Improved typography and hyphenation

% Captions and floats
\usepackage{caption}
% Caption styling configuration
%\captionsetup[table]{belowskip=5pt}
\captionsetup{format=plain}
\DeclareCaptionLabelFormat{mylabel}{#1
#2:\hspace{1.0ex}}
\DeclareCaptionFont{ninept}{\fontsize{7pt}{8}\selectfont #1}

% Figure captions: Small font, bold label, ragged right
\captionsetup[figure]{labelfont={bf,ninept},labelsep=space,
belowskip=2pt,aboveskip=6pt,labelformat=mylabel,
justification=raggedright,singlelinecheck=false,font={ninept}}

% Table captions: Small font, bold label, ragged right
\captionsetup[table]{belowskip=6pt,labelfont={bf,ninept},labelsep=none,
labelformat=mylabel,justification=raggedright,singlelinecheck=false,font={ninept}}

% Typography fine-tuning
\emergencystretch=5pt       % Allow extra stretch to avoid overfull boxes

% Utility packages
\usepackage{etoolbox}       % For patching commands and environments

% Page layout and headers
\usepackage{fancyhdr}       % Custom headers and footers
\usepackage{geometry}       % Page dimensions and margins

% Graphics and figures
\usepackage{graphicx}       % Include graphics
\usepackage{float}          % Improved float placement
\usepackage[skins,breakable]{tcolorbox} % Coloured and framed text boxes
\tcbset{before upper=\setlength{\parskip}{3pt}}

% Tables
\usepackage{longtable}      % Multi-page tables

% Fonts and typography
\usepackage{fontspec}       % Font selection for LuaLaTeX
\usepackage{mathptmx}       % Times-like math fonts
\usepackage{newpxtext}      % Palatino-like font for body text

% Colors and visual elements
\usepackage[dvipsnames]{xcolor}  % Extended color support
\usepackage{tikz}           % Programmatic graphics
\usetikzlibrary{positioning}
\usetikzlibrary{calc}
\usepackage{tikzpagenodes}  % TikZ positioning relative to page

% Code listings
\usepackage{listings}       % Code highlighting

% Hyperlinks
\usepackage{hyperref}       % Clickable links in PDF

% Conditional logic
\usepackage{ifthen}         % If-then-else commands

% Math symbols
\usepackage{amsmath}        % AMS math extensions
\usepackage{amssymb}        % AMS math symbols
\usepackage{latexsym}       % Additional LaTeX symbols
\usepackage{pifont}         % Zapf Dingbats symbols
\providecommand{\blacklozenge}{\ding{117}}  % Black diamond symbol

% Lists
\usepackage{enumitem}       % Customizable lists

% Margin notes and sidenotes
\usepackage{marginfix}      % Fixes margin note overflow
\usepackage{marginnote}     % Margin notes
\usepackage{sidenotes}      % Academic-style sidenotes
\renewcommand\raggedrightmarginnote{\sloppy}
\renewcommand\raggedleftmarginnote{\sloppy}

% Typography improvements
\usepackage{ragged2e}       % Better ragged text
\usepackage[all]{nowidow}   % Prevent widows and orphans
\usepackage{needspace}      % Ensure minimum space on page

% Section formatting
\usepackage[explicit]{titlesec}  % Custom section titles
\usepackage{tocloft}        % Table of contents formatting

% QR codes and icons
\usepackage{fontawesome5}   % Font Awesome icons
\usepackage{qrcode}         % QR code generation
\qrset{link, height=15mm}

% =============================================================================
% FLOAT CONFIGURATION
% =============================================================================
% Allow more floats per page to handle figure-heavy chapters
\extrafloats{200}
\setcounter{topnumber}{12}       % Max floats at top of page
\setcounter{bottomnumber}{12}    % Max floats at bottom of page
\setcounter{totalnumber}{24}     % Max floats per page
\setcounter{dbltopnumber}{8}     % Max floats at top of two-column page
\renewcommand{\topfraction}{.95}  % Max fraction of page for top floats
\renewcommand{\bottomfraction}{.95}
\renewcommand{\textfraction}{.05}  % Min fraction of page for text
\renewcommand{\floatpagefraction}{.7}  % Min fraction of float page
\renewcommand{\dbltopfraction}{.95}

% Prevent "Float(s) lost" errors by flushing floats more aggressively
\usepackage{placeins}  % Provides \FloatBarrier

% =============================================================================
% COLOR DEFINITIONS
% =============================================================================
% Harvard crimson - primary brand color used throughout
\definecolor{crimson}{HTML}{A51C30}

% Quiz element colors
\definecolor{quiz-question-color1}{RGB}{225,243,248}  % Light blue background
\definecolor{quiz-question-color2}{RGB}{17,158,199}   % Blue border
\definecolor{quiz-answer-color1}{RGB}{250,234,241}    % Light pink background
\definecolor{quiz-answer-color2}{RGB}{152,14,90}      % Magenta border

% =============================================================================
% LIST FORMATTING
% =============================================================================
% Tighter list spacing for academic style
\def\tightlist{}
\setlist{itemsep=1pt, parsep=1pt, topsep=0pt,after={\vspace{0.3\baselineskip}}}
\let\tightlist\relax

\makeatletter
\@ifpackageloaded{framed}{}{\usepackage{framed}}
\@ifpackageloaded{fancyvrb}{}{\usepackage{fancyvrb}}
\makeatother

\makeatletter
%New float "codelisting" has been updated
\AtBeginDocument{%
\floatstyle{ruled}
\newfloat{codelisting}{!htb}{lop}
\floatname{codelisting}{Listing}
\floatplacement{codelisting}{!htb}
\captionsetup[codelisting]{labelfont={bf,ninept},labelformat=mylabel,
  singlelinecheck=false,width=\linewidth,labelsep=none,font={ninept}}%
\renewenvironment{snugshade}{%
   \def\OuterFrameSep{3pt}%
   \def\FrameCommand{\fboxsep=5pt\colorbox{shadecolor}}%
   \MakeFramed{\advance\hsize-\width\FrameRestore}%
   \leftskip 0.5em \rightskip 0.5em%
   \small% decrease font size
   }{\endMakeFramed}%
}
\makeatother

%The space before and after the verbatim environment "Highlighting" has been reduced
\fvset{listparameters=\setlength{\topsep}{0pt}\setlength{\partopsep}{0pt}}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{framesep=0mm,commandchars=\\\{\}}

\makeatletter
\renewcommand\fs@ruled{\def\@fs@cfont{\bfseries}\let\@fs@capt\floatc@ruled
\def\@fs@pre{\hrule height.8pt depth0pt \kern2pt}%
\def\@fs@post{\kern2pt\hrule\relax}%
\def\@fs@mid{\kern2pt\hrule\kern1pt}%space between float and caption
\let\@fs@iftopcapt\iftrue}
\makeatother


% =============================================================================
% HYPHENATION RULES
% =============================================================================
% Explicit hyphenation points for technical terms to avoid bad breaks
\hyphenation{
  light-weight
  light-weight-ed
  de-vel-op-ment
  un-der-stand-ing
  mod-els
  prin-ci-ples
  ex-per-tise
  com-pli-cat-ed
  blue-print
  per‧for‧mance
  com-mu-ni-ca-tion
  par-a-digms
  hy-per-ten-sion
  a-chieved
}

% =============================================================================
% CODE LISTING CONFIGURATION
% =============================================================================
% Settings for code blocks using listings package
\lstset{
breaklines=true,              % Automatic line wrapping
breakatwhitespace=true,       % Break at whitespace only
basicstyle=\ttfamily,         % Monospace font
frame=none,                   % No frame around code
keepspaces=true,              % Preserve spaces
showspaces=false,             % Don't show space characters
showtabs=false,               % Don't show tab characters
columns=flexible,             % Flexible column width
belowskip=0pt,               % Minimal spacing
aboveskip=0pt
}

% =============================================================================
% PAGE GEOMETRY
% =============================================================================
% MIT Press trim size: 7" x 10" (per publisher specifications)
% This is a standard academic textbook format providing good readability
% for technical content with figures and code blocks.
% Wide outer margin accommodates sidenotes/margin notes.
\geometry{
  paperwidth=7in,
  paperheight=10in,
  top=0.875in,
  bottom=0.875in,
  inner=0.875in,              % Inner margin (binding side)
  outer=1.75in,               % Outer margin (includes space for sidenotes)
  footskip=30pt,
  marginparwidth=1.25in,      % Width for margin notes
  twoside                     % Different left/right pages
}

% =============================================================================
% SIDENOTE STYLING
% =============================================================================
% Custom sidenote design with crimson vertical bar
\renewcommand{\thefootnote}{\textcolor{crimson}{\arabic{footnote}}}

% Save original sidenote command
\makeatletter
\@ifundefined{oldsidenote}{
  \let\oldsidenote\sidenote%
}{}
\makeatother

% Redefine sidenote with vertical crimson bar
\renewcommand{\sidenote}[1]{%
  \oldsidenote{%
    \noindent
    \color{crimson!100}                        % Crimson vertical line
    \raisebox{0em}{%
      \rule{0.5pt}{1.5em}                      % Thin vertical line
    }
    \hspace{0.3em}                             % Space after line
    \color{black}                              % Reset text color
    \footnotesize #1                           % Sidenote content
  }%
}

% =============================================================================
% FLOAT HANDLING
% =============================================================================
% Patch LaTeX's output routine to handle float overflow gracefully
% The "Float(s) lost" error occurs in \@doclearpage when \@currlist is not empty
% This patch silently clears pending floats that can't be placed
\makeatletter
\let\orig@doclearpage\@doclearpage
\def\@doclearpage{%
  \ifx\@currlist\@empty\else
    \global\let\@currlist\@empty
    \typeout{Warning: Floats cleared to prevent overflow}%
  \fi
  \orig@doclearpage
}
\makeatother

% Additional safety for structural commands
\let\originalbackmatter\backmatter
\renewcommand{\backmatter}{%
  \clearpage%
  \originalbackmatter%
}

\let\originalfrontmatter\frontmatter
\renewcommand{\frontmatter}{%
  \clearpage%
  \originalfrontmatter%
}

\let\originalmainmatter\mainmatter
\renewcommand{\mainmatter}{%
  \clearpage%
  \originalmainmatter%
}

% =============================================================================
% PAGE HEADERS AND FOOTERS
% =============================================================================
% Ensure chapters use fancy page style (not plain)
\patchcmd{\chapter}{\thispagestyle{plain}}{\thispagestyle{fancy}}{}{}

% Main page style with crimson headers
\pagestyle{fancy}
\fancyhf{}                                              % Clear all
\fancyhead[LE]{\small\color{crimson}\nouppercase{\rightmark}}  % Left even: section
\fancyhead[RO]{\color{crimson}\thepage}                 % Right odd: page number
\fancyhead[LO]{\small\color{crimson}\nouppercase{\leftmark}}   % Left odd: chapter
\fancyhead[RE]{\color{crimson}\thepage}                 % Right even: page number
\renewcommand{\headrulewidth}{0.4pt}                    % Thin header line
\renewcommand{\footrulewidth}{0pt}                      % No footer line

% Plain page style (for chapter openings)
\fancypagestyle{plain}{
  \fancyhf{}
  \fancyfoot[C]{\color{crimson}\thepage}                % Centered page number
  \renewcommand{\headrulewidth}{0pt}
  \renewcommand{\footrulewidth}{0pt}
}

% =============================================================================
% KOMA-SCRIPT FONT ADJUSTMENTS
% =============================================================================
% Apply crimson color to all heading levels
\addtokomafont{disposition}{\rmfamily\color{crimson}}
\addtokomafont{chapter}{\color{crimson}}
\addtokomafont{section}{\color{crimson}}
\addtokomafont{subsection}{\color{crimson}}

% =============================================================================
% ABSTRACT ENVIRONMENT
% =============================================================================
\newenvironment{abstract}{
  \chapter*{\abstractname}
  \addcontentsline{toc}{chapter}{\abstractname}
  \small
}{
  \clearpage
}

% =============================================================================
% HYPERLINK CONFIGURATION
% =============================================================================
% Crimson-colored links throughout, two-page PDF layout
\hypersetup{
  linkcolor=crimson,
  citecolor=crimson,
  urlcolor=crimson,
  pdfpagelayout=TwoPageRight,   % Two-page spread view
  pdfstartview=Fit               % Initial zoom fits page
}

% =============================================================================
% PART SUMMARY SYSTEM
% =============================================================================
% Allows adding descriptive text below part titles
\newcommand{\partsummary}{}     % Empty by default
\newif\ifhaspartsummary%
\haspartsummaryfalse%

\newcommand{\setpartsummary}[1]{%
  \renewcommand{\partsummary}{#1}%
  \haspartsummarytrue%
}

% Additional colors for part page backgrounds
\definecolor{BrownLL}{RGB}{233,222,220}
\definecolor{BlueDD}{RGB}{62,100,125}
\colorlet{BlueDD}{magenta}

% ===============================================================================
% PART STYLING SYSTEM
% ===============================================================================
%
% This system provides three distinct visual styles for book organization:
%
% 1. NUMBERED PARTS (\part{title}) - For main book sections
%    - Roman numerals (I, II, III, etc.) in top right corner
%    - Crimson title with horizontal lines above/below
%    - "Part I" label in sidebar
%    - Used for: foundations, principles, optimization, deployment, etc.
%
% 2. UNNUMBERED PARTS (\part*{title}) - For special sections like "Labs"
%    - Division-style geometric background (left side)
%    - No Roman numerals
%    - Used for: labs section
%
% 3. DIVISIONS (\division{title}) - For major book divisions
%    - Clean geometric background with centered title
%    - Used for: frontmatter, main_content, backmatter
%
% The Lua filter (inject-parts.lua) automatically routes parts by {key:xxx} commands
% to the appropriate LaTeX command based on the key name.
% ===============================================================================

% NUMBERED PARTS: Roman numeral styling for main book sections
\titleformat{\part}[display]
{\thispagestyle{empty}}{}{20pt}{
\begin{tikzpicture}[remember picture,overlay]
%%%
%%
\node[crimson,align=flush right,
inner sep=0,outer sep=0mm,draw=none,%
anchor=east,minimum height=31mm, text width=1.2\textwidth,
yshift=-30mm,font={%
\fontsize{98pt}{104}\selectfont\bfseries}]  (BG) at (current page text area.north east){\thepart};
%
\node[black,inner sep=0mm,draw=none,
anchor=mid,text width=1.2\textwidth,
 minimum height=35mm, align=right,
node distance=7mm,below=of BG,
font={\fontsize{30pt}{34}\selectfont}]
(BGG)  {\hyphenchar\font=-1 \color{black}\MakeUppercase {#1}};
\draw [crimson,line width=3pt] ([yshift=0mm]BGG.north west) -- ([yshift=0mm]BGG.north east);
\draw [crimson,line width=2pt] ([yshift=0mm]BGG.south west) -- ([yshift=0mm]BGG.south east);
%
\node[fill=crimson,text=white,rotate=90,%
anchor=south west,minimum height=15mm,
minimum width=40mm,font={%
\fontsize{20pt}{20}\selectfont\bfseries}](BP)  at
(current page text area.south east)
{{\sffamily Part}~\thepart};
%
\path[red](BP.north west)-|coordinate(PS)(BGG.south west);
%
% Part summary box commented out for cleaner design
% \ifhaspartsummary
% \node[inner sep=4pt,text width=0.7\textwidth,draw=none,fill=BrownLL!40,
% align=justify,font={\fontsize{9pt}{12}\selectfont},anchor=south west]
% at (PS) {\partsummary};
% \fi
\end{tikzpicture}
}[]

\renewcommand{\thepart}{\Roman{part}}

% UNNUMBERED PARTS: Division-style background for special sections
\titleformat{name=\part,numberless}[display]
{\thispagestyle{empty}}{}{20pt}{
\begin{tikzpicture}[remember picture,overlay]
%%%
\coordinate(S1)at([yshift=-200mm]current page.north west);
\draw[draw=none,fill=BlueDD!7](S1)--++(45:16)coordinate(S2)-
|(S2|-current page.north west)--(current page.north west)coordinate(S3)--(S1);
%
\coordinate(E1)at([yshift=-98mm]current page.north west);
\draw[draw=none,fill=BlueDD!15](E1)--(current page.north west)coordinate(E2)
--++(0:98mm)coordinate(E3)--(E1);
%
\coordinate(D1)at([yshift=15mm]current page.south west);
\draw[draw=none,fill=BlueDD!40,opacity=0.5](D1)--++(45:5.5)coordinate(D2)
-|(D2|-current page.north west)--(current page.north west)coordinate(D3)--(D1);
%%%%
\path[red](S2)-|(S2-|current page.east)coordinate(SS2);
%PART
\node[crimson,align=flush right,inner sep=0,outer sep=0mm,draw=none,anchor=south,
font={\fontsize{48pt}{48}\selectfont\bfseries}]  (BG) at ($(S2)!0.5!(SS2)$){\hphantom{Part}};
%%%
\path[green]([yshift=15mm]D2)-|coordinate(TPD)(BG.south east);
\node[inner sep=0mm,draw=none,anchor=south east,%text width=0.9\textwidth,
align=right,font={\fontsize{40pt}{40}\selectfont}]
(BGG) at (TPD)  {\color{crimson}\MakeUppercase {#1}};%\MakeUppercase {}
\end{tikzpicture}
}

% Define \numberedpart command for numbered parts
\newcommand{\numberedpart}[1]{%
\FloatBarrier%  % Flush all pending floats before part break
\clearpage
\thispagestyle{empty}
\stepcounter{part}%
\begin{tikzpicture}[remember picture,overlay]
%%%
%%
\node[crimson,align=flush right,
inner sep=0,outer sep=0mm,draw=none,%
anchor=east,minimum height=31mm, text width=1.2\textwidth,
yshift=-30mm,font={%
\fontsize{98pt}{104}\selectfont\bfseries}]  (BG) at (current page text area.north east){\thepart};
%
\node[black,inner sep=0mm,draw=none,
anchor=mid,text width=1.2\textwidth,
 minimum height=35mm, align=right,
node distance=7mm,below=of BG,
font={\fontsize{30pt}{34}\selectfont}]
(BGG)  {\hyphenchar\font=-1 \color{black}\MakeUppercase {#1}};
\draw [crimson,line width=3pt] ([yshift=0mm]BGG.north west) -- ([yshift=0mm]BGG.north east);
\draw [crimson,line width=2pt] ([yshift=0mm]BGG.south west) -- ([yshift=0mm]BGG.south east);
%
\node[fill=crimson,text=white,rotate=90,%
anchor=south west,minimum height=15mm,
minimum width=40mm,font={%
\fontsize{20pt}{20}\selectfont\bfseries}](BP)  at
(current page text area.south east)
{{\sffamily Part}~\thepart};
%
\path[red](BP.north west)-|coordinate(PS)(BGG.south west);
%
% Part summary box commented out for cleaner design
% \ifhaspartsummary
% \node[inner sep=4pt,text width=0.7\textwidth,draw=none,fill=BrownLL!40,
% align=justify,font={\fontsize{9pt}{12}\selectfont},anchor=south west]
% at (PS) {\partsummary};
% \fi
\end{tikzpicture}
\clearpage
}



% DIVISIONS: Clean geometric styling with subtle tech elements
% Used for frontmatter, main_content, and backmatter divisions
\newcommand{\division}[1]{%
\FloatBarrier%  % Flush all pending floats before division break
\clearpage
\thispagestyle{empty}
\begin{tikzpicture}[remember picture,overlay]

% Clean geometric background (original design)
\coordinate(S1)at([yshift=-200mm]current page.north west);
\draw[draw=none,fill=BlueDD!7](S1)--++(45:16)coordinate(S2)-
|(S2|-current page.north west)--(current page.north west)coordinate(S3)--(S1);

\coordinate(E1)at([yshift=-98mm]current page.north west);
\draw[draw=none,fill=BlueDD!15](E1)--(current page.north west)coordinate(E2)
--++(0:98mm)coordinate(E3)--(E1);

\coordinate(D1)at([yshift=15mm]current page.south west);
\draw[draw=none,fill=BlueDD!40,opacity=0.5](D1)--++(45:5.5)coordinate(D2)
-|(D2|-current page.north west)--(current page.north west)coordinate(D3)--(D1);

% Subtle tech elements - positioned in white areas for better visibility
% Upper right white area - more visible
\draw[crimson!40, line width=0.8pt] ([xshift=140mm,yshift=-60mm]current page.north west) -- ++(40mm,0);
\draw[crimson!40, line width=0.8pt] ([xshift=150mm,yshift=-70mm]current page.north west) -- ++(30mm,0);
\draw[crimson!35, line width=0.7pt] ([xshift=160mm,yshift=-60mm]current page.north west) -- ++(0,-15mm);
\draw[crimson!35, line width=0.7pt] ([xshift=170mm,yshift=-70mm]current page.north west) -- ++(0,10mm);

% Circuit nodes - upper right
\fill[crimson!50] ([xshift=160mm,yshift=-60mm]current page.north west) circle (1.5mm);
\fill[white] ([xshift=160mm,yshift=-60mm]current page.north west) circle (0.8mm);
\fill[crimson!50] ([xshift=170mm,yshift=-70mm]current page.north west) circle (1.3mm);
\fill[white] ([xshift=170mm,yshift=-70mm]current page.north west) circle (0.6mm);

% Lower right white area - enhanced visibility
\draw[crimson!45, line width=0.9pt] ([xshift=140mm,yshift=-190mm]current page.north west) -- ++(45mm,0);
\draw[crimson!45, line width=0.9pt] ([xshift=150mm,yshift=-200mm]current page.north west) -- ++(35mm,0);
\draw[crimson!40, line width=0.8pt] ([xshift=160mm,yshift=-190mm]current page.north west) -- ++(0,-20mm);
\draw[crimson!40, line width=0.8pt] ([xshift=170mm,yshift=-200mm]current page.north west) -- ++(0,15mm);

% Additional connecting lines in lower right
\draw[crimson!35, line width=0.7pt] ([xshift=130mm,yshift=-180mm]current page.north west) -- ++(25mm,0);
\draw[crimson!35, line width=0.7pt] ([xshift=145mm,yshift=-180mm]current page.north west) -- ++(0,-25mm);

% Circuit nodes - lower right (more prominent)
\fill[crimson!55] ([xshift=160mm,yshift=-190mm]current page.north west) circle (1.6mm);
\fill[white] ([xshift=160mm,yshift=-190mm]current page.north west) circle (0.9mm);
\fill[crimson!55] ([xshift=170mm,yshift=-200mm]current page.north west) circle (1.4mm);
\fill[white] ([xshift=170mm,yshift=-200mm]current page.north west) circle (0.7mm);
\fill[crimson!50] ([xshift=145mm,yshift=-180mm]current page.north west) circle (1.2mm);
\fill[white] ([xshift=145mm,yshift=-180mm]current page.north west) circle (0.6mm);

% Title positioned in center - clean and readable
\node[inner sep=0mm,draw=none,anchor=center,text width=0.8\textwidth,
align=center,font={\fontsize{40pt}{40}\selectfont}]
(BGG) at (current page.center)  {\color{crimson}\MakeUppercase {#1}};

\end{tikzpicture}
\clearpage
}

% LAB DIVISIONS: Circuit-style neural network design for lab sections
% Used specifically for lab platform sections (arduino, xiao, grove, etc.)
\newcommand{\labdivision}[1]{%
\FloatBarrier%  % Flush all pending floats before lab division break
\clearpage
\thispagestyle{empty}
\begin{tikzpicture}[remember picture,overlay]
% Circuit background with subtle gradient
\coordinate(S1)at([yshift=-200mm]current page.north west);
\draw[draw=none,fill=BlueDD!5](S1)--++(45:16)coordinate(S2)-
|(S2|-current page.north west)--(current page.north west)coordinate(S3)--(S1);

% TOP AREA: Circuit lines in upper white space
\draw[crimson!50, line width=1.5pt] ([xshift=30mm,yshift=-40mm]current page.north west) -- ++(60mm,0);
\draw[crimson!40, line width=1pt] ([xshift=120mm,yshift=-50mm]current page.north west) -- ++(50mm,0);
\draw[crimson!50, line width=1.5pt] ([xshift=40mm,yshift=-70mm]current page.north west) -- ++(40mm,0);

% Connecting lines in top area
\draw[crimson!30, line width=1pt] ([xshift=60mm,yshift=-40mm]current page.north west) -- ++(0,-20mm);
\draw[crimson!30, line width=1pt] ([xshift=145mm,yshift=-50mm]current page.north west) -- ++(0,10mm);

% Neural nodes in top area
\fill[crimson!70] ([xshift=60mm,yshift=-40mm]current page.north west) circle (2.5mm);
\fill[white] ([xshift=60mm,yshift=-40mm]current page.north west) circle (1.5mm);
\fill[crimson!60] ([xshift=145mm,yshift=-50mm]current page.north west) circle (2mm);
\fill[white] ([xshift=145mm,yshift=-50mm]current page.north west) circle (1mm);
\fill[crimson!80] ([xshift=80mm,yshift=-70mm]current page.north west) circle (2mm);
\fill[white] ([xshift=80mm,yshift=-70mm]current page.north west) circle (1mm);

% BOTTOM AREA: Circuit lines in lower white space
\draw[crimson!50, line width=1.5pt] ([xshift=20mm,yshift=-200mm]current page.north west) -- ++(70mm,0);
\draw[crimson!40, line width=1pt] ([xshift=110mm,yshift=-210mm]current page.north west) -- ++(60mm,0);
\draw[crimson!50, line width=1.5pt] ([xshift=35mm,yshift=-230mm]current page.north west) -- ++(45mm,0);

% Connecting lines in bottom area
\draw[crimson!30, line width=1pt] ([xshift=55mm,yshift=-200mm]current page.north west) -- ++(0,-20mm);
\draw[crimson!30, line width=1pt] ([xshift=140mm,yshift=-210mm]current page.north west) -- ++(0,15mm);

% Neural nodes in bottom area
\fill[crimson!70] ([xshift=55mm,yshift=-200mm]current page.north west) circle (2.5mm);
\fill[white] ([xshift=55mm,yshift=-200mm]current page.north west) circle (1.5mm);
\fill[crimson!60] ([xshift=140mm,yshift=-210mm]current page.north west) circle (2mm);
\fill[white] ([xshift=140mm,yshift=-210mm]current page.north west) circle (1mm);
\fill[crimson!80] ([xshift=80mm,yshift=-230mm]current page.north west) circle (2mm);
\fill[white] ([xshift=80mm,yshift=-230mm]current page.north west) circle (1mm);

% SIDE AREAS: Subtle circuit elements on left and right edges
\draw[crimson!30, line width=1pt] ([xshift=15mm,yshift=-120mm]current page.north west) -- ++(20mm,0);
\draw[crimson!30, line width=1pt] ([xshift=175mm,yshift=-130mm]current page.north west) -- ++(15mm,0);
\fill[crimson!50] ([xshift=25mm,yshift=-120mm]current page.north west) circle (1.5mm);
\fill[white] ([xshift=25mm,yshift=-120mm]current page.north west) circle (0.8mm);
\fill[crimson!50] ([xshift=185mm,yshift=-130mm]current page.north west) circle (1.5mm);
\fill[white] ([xshift=185mm,yshift=-130mm]current page.north west) circle (0.8mm);

% Title positioned in center - CLEAN AREA
\node[inner sep=0mm,draw=none,anchor=center,text width=0.8\textwidth,
align=center,font={\fontsize{44pt}{44}\selectfont\bfseries}]
(BGG) at (current page.center)  {\color{crimson}\MakeUppercase {#1}};

\end{tikzpicture}
\clearpage
}

% Define \lab command for lab styling (different visual treatment)
\newcommand{\lab}[1]{%
\begin{tikzpicture}[remember picture,overlay]
%%%
% Different background pattern for labs
\coordinate(S1)at([yshift=-200mm]current page.north west);
\draw[draw=none,fill=BlueDD!15](S1)--++(45:16)coordinate(S2)-
|(S2|-current page.north west)--(current page.north west)coordinate(S3)--(S1);
%
\coordinate(E1)at([yshift=-98mm]current page.north west);
\draw[draw=none,fill=BlueDD!25](E1)--(current page.north west)coordinate(E2)
--++(0:98mm)coordinate(E3)--(E1);
%
\coordinate(D1)at([yshift=15mm]current page.south west);
\draw[draw=none,fill=BlueDD!60,opacity=0.7](D1)--++(45:5.5)coordinate(D2)
-|(D2|-current page.north west)--(current page.north west)coordinate(D3)--(D1);
%%%%
\path[red](S2)-|(S2-|current page.east)coordinate(SS2);
%LAB - Different styling
\node[crimson,align=flush right,inner sep=0,outer sep=0mm,draw=none,anchor=south,
font={\fontsize{48pt}{48}\selectfont\bfseries}]  (BG) at ($(S2)!0.5!(SS2)$){\hphantom{Workshop}};
%%%
\path[green]([yshift=15mm]D2)-|coordinate(TPD)(BG.south east);
\node[inner sep=0mm,draw=none,anchor=south east,%text width=0.9\textwidth,
align=right,font={\fontsize{40pt}{40}\selectfont}]
(BGG) at (TPD)  {\color{crimson}\MakeUppercase {#1}};%\MakeUppercase {}
\end{tikzpicture}
\thispagestyle{empty}
\clearpage
}

% =============================================================================
% SECTION FORMATTING
% =============================================================================
% All section levels use crimson color and are ragged right

% Section (Large, bold, crimson)
\titleformat{\section}
  {\normalfont\Large\bfseries\color{crimson}\raggedright}
  {\thesection}
  {0.5em}
  {#1}
\titlespacing*{\section}{0pc}{14pt plus 4pt minus 4pt}{6pt plus 2pt minus 2pt}[0pc]

% Subsection (large, bold, crimson)
\titleformat{\subsection}
  {\normalfont\large\bfseries\color{crimson}\raggedright}
  {\thesubsection}
  {0.5em}
  {#1}
\titlespacing*{\subsection}{0pc}{12pt plus 4pt minus 4pt}{5pt plus 1pt minus 2pt}[0pc]

% Subsubsection (normal size, bold, crimson)
\titleformat{\subsubsection}
  {\normalfont\normalsize\bfseries\color{crimson}\raggedright}
  {\thesubsubsection}
  {0.5em}
  {#1}
\titlespacing*{\subsubsection}{0pc}{12pt plus 4pt minus 4pt}{5pt plus 1pt minus 2pt}[0pc]

% Paragraph (run-in, bold, crimson, ends with period)
\titleformat{\paragraph}[runin]
  {\normalfont\normalsize\bfseries\color{crimson}}
  {\theparagraph}
  {0.5em}
  {#1}
  [\textbf{.}]
  \titlespacing*{\paragraph}{0pc}{6pt plus 2pt minus 2pt}{0.5em}[0pc]

% Subparagraph (run-in, italic, crimson, ends with period)
\titleformat{\subparagraph}[runin]
  {\normalfont\normalsize\itshape\color{crimson}}
  {\thesubparagraph}
  {0.5em}
  {#1}
  [\textbf{.}]
  \titlespacing*{\subparagraph}{0pc}{6pt plus 2pt minus 2pt}{0.5em}[0pc]

% =============================================================================
% CHAPTER FORMATTING
% =============================================================================
% Numbered chapters: "Chapter X" prefix, huge crimson title
\titleformat{\chapter}[display]
  {\normalfont\huge\bfseries\color{crimson}}
  {\chaptername\ \thechapter}
  {20pt}
  {\Huge #1}
  []

% Unnumbered chapters: no prefix, huge crimson title
\titleformat{name=\chapter,numberless}
  {\normalfont\huge\bfseries\color{crimson}}
  {}
  {0pt}
  {\Huge #1}
  []

\renewcommand{\chaptername}{Chapter}
% =============================================================================
% TABLE OF CONTENTS FORMATTING
% =============================================================================
\setcounter{tocdepth}{2}                      % Show chapters, sections, subsections

% TOC spacing adjustments for number widths and indentation
\setlength{\cftchapnumwidth}{2em}             % Chapter number width
\setlength{\cftsecnumwidth}{2.75em}           % Section number width
\setlength{\cftsubsecnumwidth}{3.25em}        % Subsection number width
\setlength{\cftsubsubsecnumwidth}{4em}        % Subsubsection number width
\setlength{\cftsubsecindent}{4.25em}          % Subsection indent
\setlength{\cftsubsubsecindent}{7.5em}        % Subsubsection indent

% Chapter entries in TOC: bold crimson with "Chapter" prefix
\renewcommand{\cftchapfont}{\bfseries\color{crimson}}
\renewcommand{\cftchappresnum}{\color{crimson}Chapter~}

% Custom formatting for division entries (styled like parts)
\newcommand{\divisionchapter}[1]{%
  \addvspace{12pt}%
  \noindent\hfil\bfseries\color{crimson}#1\hfil\par%
  \addvspace{6pt}%
}

% Adjust TOC spacing for "Chapter" prefix
\newlength{\xtraspace}
\settowidth{\xtraspace}{\cftchappresnum\cftchapaftersnum}
\addtolength{\cftchapnumwidth}{\xtraspace}

% Unnumbered chapters with TOC entry
\newcommand{\likechapter}[1]{%
    \chapter*{#1}
    \addcontentsline{toc}{chapter}{\textcolor{crimson}{#1}}
}

% =============================================================================
% PAGE NUMBERING SYSTEM
% =============================================================================
% Implements traditional book numbering:
% - Roman numerals (i, ii, iii...) for frontmatter
% - Arabic numerals (1, 2, 3...) for mainmatter
% Automatically switches at first numbered chapter
\makeatletter
\newif\if@firstnumbered%
\@firstnumberedtrue%
\newif\if@firstunnumbered%
\@firstunnumberedtrue%

\newcounter{lastRomanPage}
\setcounter{lastRomanPage}{1}

% Start document with Roman numerals (frontmatter)
\AtBeginDocument{
  \pagenumbering{roman}
  \renewcommand{\thepage}{\roman{page}}
}

% Intercept chapter command
\let\old@chapter\chapter%
\renewcommand{\chapter}{%
  \@ifstar{\unnumbered@chapter}{\numbered@chapter}%
}

% Numbered chapters: switch to Arabic on first occurrence
\newcommand{\numbered@chapter}[1]{%
  \if@firstnumbered%
    \cleardoublepage%
    \setcounter{lastRomanPage}{\value{page}}%
    \pagenumbering{arabic}%
    \@firstnumberedfalse%
  \else
    \setcounter{page}{\value{page}}%
  \fi
  \setcounter{sidenote}{1}                    % Reset footnote counter per chapter
  \old@chapter{#1}%
}

% Unnumbered chapters: stay in Roman numerals
\newcommand{\unnumbered@chapter}[1]{%
  \if@firstunnumbered%
    \clearpage
    \setcounter{lastRomanPage}{\value{page}}%
    \pagenumbering{roman}%
    \@firstunnumberedfalse%
  \fi
  \setcounter{sidenote}{1}
  \old@chapter*{#1}%
}
\makeatother

% =============================================================================
% TABLE SIZING AND SPACING
% =============================================================================
% Make tables slightly smaller to fit more content
\AtBeginEnvironment{longtable}{\scriptsize}

% Increase vertical spacing in table cells (default is 1.0)
\renewcommand{\arraystretch}{1.3}

% Prefer placing tables at the top of pages
\makeatletter
\renewcommand{\fps@table}{t}  % Default placement: top of page
\makeatother

% =============================================================================
% LONGTABLE PAGE BREAKING FIXES (Windows compatibility)
% =============================================================================
% Prevent "Infinite glue shrinkage" errors on Windows LaTeX builds
% by giving longtable more flexibility in page breaking

% Allow more flexible page breaking (vs strict \flushbottom)
\raggedbottom

% Process more rows before attempting page break (default is 20)
\setcounter{LTchunksize}{50}

% Add extra stretch for longtable environments specifically
\AtBeginEnvironment{longtable}{%
  \setlength{\emergencystretch}{3em}%
  \setlength{\parskip}{0pt plus 1pt}%
}

% =============================================================================
% TABLE STYLING - Clean tables with crimson borders
% =============================================================================
% Professional table appearance with:
% - Clean white background (no colored rows)
% - Crimson-colored borders
% - Good spacing for readability
%
% Note: Headers are automatically bolded by Quarto when using **text** in source
\usepackage{booktabs}      % Professional table rules (\toprule, \midrule, \bottomrule)
\usepackage{colortbl}      % For colored borders (\arrayrulecolor)

% Global table styling - crimson borders
\setlength{\arrayrulewidth}{0.5pt}          % Thinner borders than default
%\arrayrulecolor{crimson}                    % Crimson borders matching brand

\setcounter{chapter}{0}
\usepackage{needspace}
\let\Needspace\needspace
\makeatletter
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{plain}
\@ifundefined{c@chapter}{\newfloat{vid}{h}{lovid}}{\newfloat{vid}{h}{lovid}[chapter]}
\floatname{vid}{Video}
\newcommand*\listofvids{\listof{vid}{List of Videos}}
\makeatother
\makeatletter
\@ifpackageloaded{tcolorbox}{}{\usepackage[skins,breakable]{tcolorbox}}
\@ifpackageloaded{fontawesome5}{}{\usepackage{fontawesome5}}
\definecolor{quarto-callout-color}{HTML}{909090}
\definecolor{quarto-callout-note-color}{HTML}{0758E5}
\definecolor{quarto-callout-important-color}{HTML}{CC1914}
\definecolor{quarto-callout-warning-color}{HTML}{EB9113}
\definecolor{quarto-callout-tip-color}{HTML}{00A047}
\definecolor{quarto-callout-caution-color}{HTML}{FC5300}
\definecolor{quarto-callout-color-frame}{HTML}{acacac}
\definecolor{quarto-callout-note-color-frame}{HTML}{4582ec}
\definecolor{quarto-callout-important-color-frame}{HTML}{d9534f}
\definecolor{quarto-callout-warning-color-frame}{HTML}{f0ad4e}
\definecolor{quarto-callout-tip-color-frame}{HTML}{02b875}
\definecolor{quarto-callout-caution-color-frame}{HTML}{fd7e14}
\makeatother
\makeatletter
\@ifpackageloaded{bookmark}{}{\usepackage{bookmark}}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\makeatletter
\@ifpackageloaded{sidenotes}{}{\usepackage{sidenotes}}
\@ifpackageloaded{marginnote}{}{\usepackage{marginnote}}
\makeatother
\newcommand{\fbxIconPath}{assets/images/icons/callouts}
\newcommand{\fbxIconFormat}{pdf}
\makeatletter
\@ifpackageloaded{tcolorbox}{}{\usepackage[many]{tcolorbox}}
\makeatother
%%%% ---foldboxy preamble ----- %%%%%

% Load xstring for string manipulation
\RequirePackage{xstring}

% Icon path and format configuration - can be overridden in filter-metadata
\providecommand{\fbxIconPath}{assets/images/icons/callouts}
\providecommand{\fbxIconFormat}{pdf}

% Helper command to include icon with hyphen-to-underscore conversion
% This ensures consistency: callout-quiz-question -> callout_quiz_question
\newcommand{\fbxIncludeIcon}[2]{%
  \StrSubstitute{#1}{-}{_}[\fbxIconName]%
  \includegraphics[width=#2]{\fbxIconPath/icon_\fbxIconName.\fbxIconFormat}%
}

% Legacy fallback colors (keep for compatibility)
\definecolor{fbx-default-color1}{HTML}{c7c7d0}
\definecolor{fbx-default-color2}{HTML}{a3a3aa}
\definecolor{fbox-color1}{HTML}{c7c7d0}
\definecolor{fbox-color2}{HTML}{a3a3aa}

% arguments: #1 typelabelnummer: #2 titel: #3
\newenvironment{fbx}[3]{%
\begin{tcolorbox}[
  enhanced,
  breakable,
  %fontupper=\fontsize{8pt}{10pt}\selectfont,  % 95% of body text (10pt -> 9.5pt)
  before skip=8pt,  % space above box (increased)
  after skip=8pt,   % space below box (increased)
  attach boxed title to top*={xshift=0pt},
  boxed title style={
  %fuzzy shadow={1pt}{-1pt}{0mm}{0.1mm}{gray},
  arc=1.5pt,
  rounded corners=north,
  sharp corners=south,
  top=6pt,          % Adjusted for ~40px equivalent height
  bottom=5pt,       % Adjusted for ~40px equivalent height
  overlay={
      \node [left,outer sep=0em, black,draw=none,anchor=west,
        rectangle,fill=none,inner sep=0pt]
        at ([xshift=4mm]frame.west) {\fbxIncludeIcon{#1}{4.2mm}};
    },
  },
  colframe=#1-color2,             % Border color (auto-generated from YAML)
  colbacktitle=#1-color1,         % Background color (auto-generated from YAML)
  colback=white,
  coltitle=black,
  titlerule=0mm,
  toprule=0.5pt,
  bottomrule=0.5pt,
  leftrule=2.2pt,
  rightrule=0.5pt,
  outer arc=1.5pt,
  arc=1.5pt,
  left=0.5em,       % increased left padding
  bottomtitle=1.5mm, % increased title bottom margin
  toptitle=1.5mm,    % increased title top margin
  title=\hspace{2.5em}\protect#2\hspace{0.2em}\protect#3, % Protect parameters
  extras middle and last={top=4pt} % increased continuation spacing
]}
{\end{tcolorbox}}


% boxed environment with right border
\newenvironment{fbxSimple}[3]{\begin{tcolorbox}[
  enhanced,
  breakable,
  %fontupper=\fontsize{8pt}{10pt}\selectfont,  % 95% of body text (10pt -> 9.5pt)
  before skip=8pt,  % space above box (increased)
  after skip=8pt,   % space below box (increased)
  attach boxed title to top*={xshift=0pt},
  boxed title style={
  %fuzzy shadow={1pt}{-1pt}{0mm}{0.1mm}{gray},
  arc=1.5pt,
  rounded corners=north,
  sharp corners=south,
  top=6pt,          % Adjusted for ~40px equivalent height
  bottom=5pt,       % Adjusted for ~40px equivalent height
  overlay={
      \node [left,outer sep=0em, black,draw=none,anchor=west,
        rectangle,fill=none,inner sep=0pt]
        at ([xshift=3mm]frame.west) {\fbxIncludeIcon{#1}{4.2mm}};
    },
  },
  colframe=#1-color2,             % Border color (auto-generated from YAML)
  colbacktitle=#1-color1,         % Background color (auto-generated from YAML)
  colback=white,
  coltitle=black,
  titlerule=0mm,
  toprule=0.5pt,
  bottomrule=0.5pt,
  leftrule=2.2pt,
  rightrule=0.5pt,
  outer arc=1.5pt,
  arc=1.5pt,
  left=0.5em,       % increased left padding
  bottomtitle=1.5mm, % increased title bottom margin
  toptitle=1.5mm,    % increased title top margin
  title=\hspace{2.5em}\protect#2\hspace{0.2em}\protect#3, % Protect parameters
  boxsep=1pt,
  extras first={bottom=0pt},
  extras last={top=0pt,bottom=-4pt},
  overlay first={
    \draw[line width=1pt,white] ([xshift=2.2pt]frame.south west)-- ([xshift=-0.5pt]frame.south east);
  },
  overlay last={
    \draw[line width=1pt,white] ([xshift=2.2pt]frame.north west)-- ([xshift=-0.5pt]frame.north east);
   }
]}
{\end{tcolorbox}}

%%%% --- end foldboxy preamble ----- %%%%%
%%==== colors from yaml ===%
\definecolor{callout-notebook-color1}{HTML}{F2F7FF}
\definecolor{callout-notebook-color2}{HTML}{2C5282}
\definecolor{callout-resource-slides-color1}{HTML}{E0F2F1}
\definecolor{callout-resource-slides-color2}{HTML}{20B2AA}
\definecolor{callout-checkpoint-color1}{HTML}{E8F5E9}
\definecolor{callout-checkpoint-color2}{HTML}{2E7D32}
\definecolor{callout-quiz-answer-color1}{HTML}{E8F2EA}
\definecolor{callout-quiz-answer-color2}{HTML}{4a7c59}
\definecolor{callout-lighthouse-color1}{HTML}{FDF8E6}
\definecolor{callout-lighthouse-color2}{HTML}{B8860B}
\definecolor{callout-example-color1}{HTML}{F0F8F6}
\definecolor{callout-example-color2}{HTML}{148F77}
\definecolor{callout-colab-color1}{HTML}{FFF5E6}
\definecolor{callout-colab-color2}{HTML}{FF6B35}
\definecolor{callout-definition-color1}{HTML}{F0F4F8}
\definecolor{callout-definition-color2}{HTML}{1B4F72}
\definecolor{callout-resource-exercises-color1}{HTML}{E0F2F1}
\definecolor{callout-resource-exercises-color2}{HTML}{20B2AA}
\definecolor{callout-code-color1}{HTML}{F2F4F8}
\definecolor{callout-code-color2}{HTML}{D1D7E0}
\definecolor{callout-chapter-connection-color1}{HTML}{FDF2F7}
\definecolor{callout-chapter-connection-color2}{HTML}{A51C30}
\definecolor{callout-quiz-question-color1}{HTML}{F0F0F8}
\definecolor{callout-quiz-question-color2}{HTML}{5B4B8A}
\definecolor{callout-resource-videos-color1}{HTML}{E0F2F1}
\definecolor{callout-resource-videos-color2}{HTML}{20B2AA}
\definecolor{callout-perspective-color1}{HTML}{F7F8FA}
\definecolor{callout-perspective-color2}{HTML}{4A5568}
%=============%

\usepackage{hyphenat}
\usepackage{ifthen}
\usepackage{calc}
\usepackage{calculator}



\usepackage{graphicx}
\usepackage{geometry}
\usepackage{afterpage}
\usepackage{tikz}
\usetikzlibrary{calc}
\usetikzlibrary{fadings}
\usepackage[pagecolor=none]{pagecolor}


% Set the titlepage font families







% Set the coverpage font families

\usepackage{bookmark}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  pdftitle={Machine Learning Systems},
  pdfauthor={Vijay Janapa Reddi},
  colorlinks=true,
  linkcolor={Maroon},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}


\title{Machine Learning Systems}
\usepackage{etoolbox}
\makeatletter
\providecommand{\subtitle}[1]{% add subtitle to \maketitle
  \apptocmd{\@title}{\par {\large #1 \par}}{}{}
}
\makeatother
\subtitle{Volume I: Introduction}
\author{Vijay Janapa Reddi}
\date{January 28, 2026}
\begin{document}
%%%%% begin titlepage extension code

  \begin{frontmatter}

\begin{titlepage}
% This is a combination of Pandoc templating and LaTeX
% Pandoc templating https://pandoc.org/MANUAL.html#templates
% See the README for help

\thispagestyle{empty}

\newgeometry{top=-100in}

% Page color

\newcommand{\coverauthorstyle}[1]{{\fontsize{20}{24.0}\selectfont
{#1}}}

\begin{tikzpicture}[remember picture, overlay, inner sep=0pt, outer sep=0pt]

\tikzfading[name=fadeout, inner color=transparent!0,outer color=transparent!100]
\tikzfading[name=fadein, inner color=transparent!100,outer color=transparent!0]
\node[anchor=south west, rotate=0, opacity=1] at ($(current page.south west)+(0.225\paperwidth, 9)$) {
\includegraphics[width=\paperwidth, keepaspectratio]{assets/images/covers/cover-image-transparent-vol1.png}};

% Title
\newcommand{\titlelocationleft}{0.075\paperwidth}
\newcommand{\titlelocationbottom}{0.4\paperwidth}
\newcommand{\titlealign}{left}

\begin{scope}{%
\fontsize{52}{62.4}\selectfont
\node[anchor=north
west, align=left, rotate=0] (Title1) at ($(current page.south west)+(\titlelocationleft,\titlelocationbottom)$)  [text width = 0.9\paperwidth]  {{\nohyphens{Machine
Learning Systems}}};
}
\end{scope}

% Author
\newcommand{\authorlocationleft}{.925\paperwidth}
\newcommand{\authorlocationbottom}{0.150\paperwidth}
\newcommand{\authoralign}{right}

\begin{scope}
{%
\fontsize{20}{24.0}\selectfont
\node[anchor=north
east, align=right, rotate=0] (Author1) at ($(current page.south west)+(\authorlocationleft,\authorlocationbottom)$)  [text width = 6in]  {\coverauthorstyle{Vijay\\Janapa
Reddi\\}};
}
\end{scope}

% Footer
\newcommand{\footerlocationleft}{0.075\paperwidth}
\newcommand{\footerlocationbottom}{0.475\paperwidth}
\newcommand{\footerlocationalign}{left}

\begin{scope}
{%
\fontsize{25}{30.0}\selectfont
 \node[anchor=north west, align=left, rotate=0] (Footer1) at %
($(current page.south west)+(\footerlocationleft,\footerlocationbottom)$)  [text width = 0.9\paperwidth]  {{\nohyphens{Volume
I: Introduction}}};
}
\end{scope}

\end{tikzpicture}
\clearpage
\restoregeometry
%%% TITLE PAGE START

% Set up alignment commands
%Page
\newcommand{\titlepagepagealign}{
\ifthenelse{\equal{left}{right}}{\raggedleft}{}
\ifthenelse{\equal{left}{center}}{\centering}{}
\ifthenelse{\equal{left}{left}}{\raggedright}{}
}


\newcommand{\titleandsubtitle}{
% Title and subtitle
{{\huge{\bfseries{\nohyphens{Machine Learning Systems}}}}\par
}%

\vspace{\betweentitlesubtitle}
{
{\large{\textit{\nohyphens{Volume I: Introduction}}}}\par
}}
\newcommand{\titlepagetitleblock}{
\titleandsubtitle
}

\newcommand{\authorstyle}[1]{{\large{#1}}}

\newcommand{\affiliationstyle}[1]{{\large{#1}}}

\newcommand{\titlepageauthorblock}{
{\authorstyle{\nohyphens{Vijay Janapa
Reddi}{\textsuperscript{1}}\textsuperscript{,}{\textsuperscript{,*}}}}}

\newcommand{\titlepageaffiliationblock}{
\hangindent=1em
\hangafter=1
{\affiliationstyle{
{1}.~Harvard University


\vspace{1\baselineskip}
* \textit{Correspondence:}~Vijay Janapa Reddi~vj@eecs.harvard.edu
}}
}
\newcommand{\headerstyled}{%
{}
}
\newcommand{\footerstyled}{%
{\large{}}
}
\newcommand{\datestyled}{%
{January 28, 2026}
}


\newcommand{\titlepageheaderblock}{\headerstyled}

\newcommand{\titlepagefooterblock}{
\footerstyled
}

\newcommand{\titlepagedateblock}{
\datestyled
}

%set up blocks so user can specify order
\newcommand{\titleblock}{\newlength{\betweentitlesubtitle}
\setlength{\betweentitlesubtitle}{0.05\textheight}
{

{\titlepagetitleblock}
}

\vspace{4\baselineskip}
}

\newcommand{\authorblock}{{\titlepageauthorblock}

\vspace{2\baselineskip}
}

\newcommand{\affiliationblock}{{\titlepageaffiliationblock}

\vspace{0pt}
}

\newcommand{\logoblock}{}

\newcommand{\footerblock}{}

\newcommand{\dateblock}{{\titlepagedateblock}

\vspace{0pt}
}

\newcommand{\headerblock}{}

\thispagestyle{empty} % no page numbers on titlepages


\newcommand{\vrulecode}{\textcolor{black}{\rule{\vrulewidth}{\textheight}}}
\newlength{\vrulewidth}
\setlength{\vrulewidth}{2pt}
\newlength{\B}
\setlength{\B}{\ifdim\vrulewidth > 0pt 0.05\textwidth\else 0pt\fi}
\newlength{\minipagewidth}
\ifthenelse{\equal{left}{left} \OR \equal{left}{right} }
{% True case
\setlength{\minipagewidth}{\textwidth - \vrulewidth - \B - 0.1\textwidth}
}{
\setlength{\minipagewidth}{\textwidth - 2\vrulewidth - 2\B - 0.1\textwidth}
}
\ifthenelse{\equal{left}{left} \OR \equal{left}{leftright}}
{% True case
\raggedleft % needed for the minipage to work
\vrulecode
\hspace{\B}
}{%
\raggedright % else it is right only and width is not 0
}
% [position of box][box height][inner position]{width}
% [s] means stretch out vertically; assuming there is a vfill
\begin{minipage}[b][\textheight][s]{\minipagewidth}
\titlepagepagealign
\titleblock

Prof.~Vijay Janapa Reddi

School of Engineering and Applied Sciences

Harvard University

\vspace{80mm}

With heartfelt gratitude to the community for their invaluable
contributions and steadfast support.

\vfill

January 28, 2026

\vfill
\par

\end{minipage}\ifthenelse{\equal{left}{right} \OR \equal{left}{leftright} }{
\hspace{\B}
\vrulecode}{}
\clearpage
%%% TITLE PAGE END
\end{titlepage}
\setcounter{page}{1}
\end{frontmatter}

%%%%% end titlepage extension code

\renewcommand*\contentsname{Table of contents}
{
\hypersetup{linkcolor=}
\setcounter{tocdepth}{2}
\tableofcontents
}

\mainmatter
\bookmarksetup{startatroot}

\chapter*{Welcome to Volume I}\label{welcome-to-volume-i}
\addcontentsline{toc}{chapter}{Welcome to Volume I}

\markboth{Welcome to Volume I}{Welcome to Volume I}

\section*{What You Will Learn}\label{what-you-will-learn}
\addcontentsline{toc}{section}{What You Will Learn}

\markright{What You Will Learn}

Volume I progresses through four stages:

\begin{itemize}
\tightlist
\item
  \textbf{Part I: Foundations} --- Build your conceptual foundation with
  mental models that underpin all effective systems work.
\item
  \textbf{Part II: Build} --- Engineer complete workflows from data
  pipelines through training infrastructure.
\item
  \textbf{Part III: Optimize} --- Transform theoretical understanding
  into systems that run efficiently in resource-constrained
  environments.
\item
  \textbf{Part IV: Deploy} --- Navigate serving, operations, and
  responsible engineering practices.
\end{itemize}

\section*{Prerequisites}\label{prerequisites}
\addcontentsline{toc}{section}{Prerequisites}

\markright{Prerequisites}

This volume assumes:

\begin{itemize}
\tightlist
\item
  \textbf{Programming proficiency} in Python with familiarity in NumPy
\item
  \textbf{Mathematics foundations} in linear algebra, calculus, and
  probability at the undergraduate level
\item
  Prior ML experience is helpful but not required;
  \textbf{?@sec-deep-learning-systems-foundations} provides essential
  background
\end{itemize}

\section*{Support Our Mission}\label{support-our-mission}
\addcontentsline{toc}{section}{Support Our Mission}

\markright{Support Our Mission}

\section*{Continue Your Journey}\label{continue-your-journey}
\addcontentsline{toc}{section}{Continue Your Journey}

\markright{Continue Your Journey}

\section*{Listen to the AI Podcast}\label{listen-to-the-ai-podcast}
\addcontentsline{toc}{section}{Listen to the AI Podcast}

\markright{Listen to the AI Podcast}

\section*{Want to Help Out?}\label{want-to-help-out}
\addcontentsline{toc}{section}{Want to Help Out?}

\markright{Want to Help Out?}

This is a collaborative project, and your input matters. If you'd like
to contribute, check out our
\href{https://github.com/harvard-edge/cs249r_book/blob/dev/docs/contribute.md}{contribution
guidelines}. Feedback, corrections, and new ideas are welcome. Simply
file a GitHub
\href{https://github.com/harvard-edge/cs249r_book/issues}{issue}.

\bookmarksetup{startatroot}

\chapter{Model Serving Systems}\label{sec-model-serving-systems}

\marginnote{\begin{footnotesize}

\emph{DALL·E 3 Prompt: An illustration of a machine learning serving
system, depicted as a high-speed assembly line in a modern factory. On
one end, raw data (images, text documents, sensor readings) enters on
conveyor belts. In the center, a glowing neural network processes inputs
through preprocessing stations, a central inference engine, and
postprocessing units. Workers monitor latency gauges and throughput
meters. Some requests wait in small batches while others stream through
individually. The scene conveys speed, precision, and the transformation
from raw input to useful predictions, with visible clocks emphasizing
the time-critical nature of serving.}

\end{footnotesize}}

\noindent
\pandocbounded{\includegraphics[keepaspectratio]{contents/vol1/serving/images/png/cover_serving.png}}

\section*{Purpose}\label{purpose}
\addcontentsline{toc}{section}{Purpose}

\markright{Purpose}

\emph{Why does serving invert every optimization priority that made
training successful?}

Training and serving want opposite things. Training maximizes
throughput---large batches, long durations, latency spikes absorbed into
hour-long epochs. Serving minimizes latency---individual requests
answered in milliseconds, where one slow response means a frustrated
user. Training amortizes expensive hardware across billions of examples;
serving pays per-request, where inefficiency compounds into operational
cost. Training checkpoints and restarts; serving must never fail
visibly. This inversion explains why models that train beautifully often
serve poorly: batch sizes that maximized GPU utilization become latency
violations; preprocessing bottlenecks ignored during training dominate
serving time. The skills that optimize training actively harm serving.

\begin{tcolorbox}[enhanced jigsaw, breakable, colframe=quarto-callout-tip-color-frame, toprule=.15mm, coltitle=black, bottomtitle=1mm, bottomrule=.15mm, opacitybacktitle=0.6, arc=.35mm, colbacktitle=quarto-callout-tip-color!10!white, toptitle=1mm, left=2mm, titlerule=0mm, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Learning Objectives}, rightrule=.15mm, leftrule=.75mm, opacityback=0, colback=white]

\begin{itemize}
\item
  Contrast training and serving system priorities by explaining the
  inversion from throughput optimization to latency minimization
\item
  Decompose request latency into preprocessing, inference, and
  postprocessing phases to identify optimization bottlenecks
\item
  Apply Little's Law and M/M/1 queuing models to predict serving system
  latency under varying load conditions
\item
  Perform capacity planning to meet percentile latency SLOs while
  accounting for traffic variance and fault tolerance requirements
\item
  Identify sources of training-serving skew and select prevention
  strategies appropriate to deployment contexts
\item
  Select batching strategies (dynamic, continuous, none) based on
  traffic patterns and latency constraints
\item
  Evaluate runtime and precision tradeoffs to meet deployment cost and
  performance requirements
\end{itemize}

\end{tcolorbox}

\section{The Serving
Paradigm}\label{sec-model-serving-systems-serving-paradigm-9634}

Serving marks the transition from model development to production
deployment, where the \textbf{Iron Law of ML Systems}
(\textbf{?@sec-ai-training-iron-law-training-performance-a53f})
undergoes a fundamental shift. In training, we optimized for high
Throughput and Bandwidth to process massive datasets. In serving, the
\textbf{Latency term} (\(\text{Latency}_{\text{fixed}}\))---the
irreducible overhead of request scheduling, network round-trips, and
system orchestration---suddenly becomes the dominant constraint. This
chapter explores how to re-engineer the system to minimize this fixed
`tax' on every prediction.

Serving introduces an inversion that reshapes the priorities established
in prior chapters. Training optimizes for samples processed per hour
over days of computation. Serving must deliver predictions within
milliseconds under unpredictable load. \textbf{?@sec-benchmarking-ai}
established techniques for measuring throughput and accuracy under
controlled conditions; production serving faces traffic patterns that no
benchmark could anticipate. \textbf{?@sec-model-compression} provided
quantization methods that reduced model size; serving must validate that
those optimizations preserve accuracy under real traffic distributions.
This inversion from throughput to latency, from controlled to
unpredictable, from offline to real time defines the serving challenge.

\phantomsection\label{callout-definitionux2a-1.1}
\begin{fbx}{callout-definition}{Definition: }{Model Serving}
\phantomsection\label{callout-definition*-1.1}
\textbf{Model Serving} refers to the process of exposing trained machine
learning models for \emph{real-time prediction}, requiring systems that
transform raw inputs into useful outputs while meeting \emph{latency
constraints}, maintaining \emph{consistency} with training behavior, and
achieving \emph{cost-effective resource utilization}.

\end{fbx}

Serving systems must execute a complete inference pipeline under latency
constraints, not just the neural network computation.
Figure~\ref{fig-serving-inference-pipeline} illustrates this pipeline:
raw inputs flow through preprocessing (traditional computing), neural
network inference (deep learning), and postprocessing (traditional
computing) before producing final outputs. Each stage contributes to
total latency, and bottlenecks can occur anywhere in the pipeline.

\begin{figure}[htb]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/0c1fec7aefaa056568ed13a2ab2a93c020eece1d.pdf}}

}

\caption{\label{fig-serving-inference-pipeline}\textbf{The Inference
Pipeline}: ML serving systems transform raw inputs into final outputs
through sequential stages---preprocessing, neural network computation,
and postprocessing. The neural network represents just one component;
preprocessing and postprocessing rely on traditional computing and often
dominate total latency in optimized systems.}

\end{figure}%

This chapter explains how to build systems that orchestrate this
pipeline efficiently. The chapter proceeds in three parts. \textbf{Part
1} establishes system fundamentals: serving architectures, server
anatomy, and the protocols that connect clients to models. \textbf{Part
2} follows the request lifecycle: where latency accumulates across
preprocessing, inference, and postprocessing, then how queuing dynamics
govern system behavior under load. \textbf{Part 3} addresses
optimization: model lifecycle management ensures models are ready to
serve, batching strategies maximize throughput, LLM-specific techniques
handle generative workloads, runtime selection tunes performance, and
economics translate these choices into infrastructure decisions.

\subsection{Static vs Dynamic
Inference}\label{sec-model-serving-systems-static-vs-dynamic-inference-e864}

The first architectural decision in any serving system is whether
predictions happen before or during user requests
(\citeproc{ref-google2024staticdynamic}{Google 2024}). This choice
shapes system design, cost structure, and capability boundaries.

\textbf{Static inference} (also called offline or batch inference)
pre-computes predictions for anticipated inputs and stores them for
retrieval. Consider a recommendation system that generates predictions
for all user-item pairs nightly. When a user requests recommendations,
the system retrieves pre-computed results from a lookup table rather
than running inference. This approach eliminates inference latency
entirely since results already exist, enables quality verification
before deployment, and reduces serving costs. However, static inference
cannot handle novel inputs that were not anticipated during the batch
computation and introduces hours or days of latency when models update.

\textbf{Dynamic inference} (also called online or real-time inference)
computes predictions on demand when requests arrive. This handles any
input, including rare edge cases and novel combinations, and immediately
reflects model updates. The cost is strict latency requirements that may
force simpler models and more intensive monitoring infrastructure.

For our ResNet-50 image classifier, consider two deployment scenarios. A
\textbf{static approach} suits a photo organization app that
pre-classifies all images in a user's library overnight---with 10,000
photos and 5ms inference each, batch processing takes \textasciitilde50
seconds total, and users see instant classification when browsing. A
\textbf{dynamic approach} suits a content moderation API that must
classify user-uploaded images in real-time, with each image requiring
the full preprocessing→inference→postprocessing pipeline and a 100ms
latency budget. Most production image classification systems use a
\textbf{hybrid approach}: frequently requested images (popular products,
known memes) are pre-classified and cached, while novel uploads trigger
dynamic inference.

The choice between static and dynamic serving has direct economic
implications. Stricter latency requirements directly translate into
higher infrastructure costs, creating a tradeoff that serving system
architects must navigate carefully.

\phantomsection\label{callout-notebook-1.1}
\begin{fbx}{callout-notebook}{AI Engineer’s Notebook 1.1: }{Engineering Economics: The Cost of Latency}
\phantomsection\label{callout-notebook-1.1}
Latency constraints directly dictate infrastructure costs. Consider a
GPU server renting for \$4/hour.

\textbf{Scenario A (Low Latency):} Batch size 1. * Latency: 5ms. *
Throughput: 200 req/s. * Cost per million queries: \textbf{\$5.55}.

\textbf{Scenario B (High Throughput):} Batch size 8. * Latency: 10ms
(doubled due to batching overhead). * Throughput: 800 req/s (quadrupled
due to parallel efficiency). * Cost per million queries:
\textbf{\$1.38}.

\textbf{The Trade-off:} Reducing latency from 10ms to 5ms increases the
hardware bill by \textbf{400\%}. Engineers must quantify whether that
5ms speedup generates enough business value to justify the 4x cost
increase.

\end{fbx}

Most production systems combine both approaches. Common queries hit a
cache populated by batch inference while uncommon requests trigger
dynamic computation. Understanding this spectrum is essential because it
determines which subsequent optimization strategies apply. Static
inference optimizes for throughput during batch computation and storage
efficiency for serving. Dynamic inference optimizes for per-request
latency under concurrent load, which requires understanding where time
goes within each request.

The static-versus-dynamic decision is just the first of several
architectural choices that shape serving system design. Equally
important is \emph{where} the model executes, since deployment context
fundamentally constrains every subsequent optimization.

\subsection{The Spectrum of Serving
Architectures}\label{sec-model-serving-systems-spectrum-serving-architectures-8966}

While ``serving'' often implies a networked server processing API
requests, the architectural pattern varies fundamentally by deployment
environment. Understanding this spectrum is essential for Volume I's
focus on mastering the ML node, since the same model may require
radically different serving strategies depending on where it executes.

\textbf{1. Networked Serving (Cloud/Datacenter)}

The model runs as a standalone service (microservice). The primary
interface is the network (HTTP/gRPC). Optimization focuses on
\textbf{throughput} (batching) and \textbf{concurrency}.

\begin{itemize}
\tightlist
\item
  \emph{Key Constraint:} Network bandwidth and serialization cost.
\item
  \emph{Typical Hardware:} NVIDIA GPUs (V100, A100, H100), Google TPUs,
  AWS Inferentia.
\item
  \emph{Cold Start:} Seconds to minutes (container startup, model
  loading, warmup).
\end{itemize}

\textbf{2. Application-Embedded Serving (Mobile/Edge)}

The model runs within the user application process (e.g., a smartphone
app using CoreML or TensorFlow Lite). There is no ``server.'' The
interface is a function call. Optimization focuses on \textbf{energy}
and \textbf{responsiveness} (SingleStream).

\begin{itemize}
\tightlist
\item
  \emph{Key Advantage:} \textbf{Zero-Copy Inference}. When data moves
  through a system, each copy consumes CPU cycles and memory bandwidth.
  In cloud serving, a camera frame might be copied four times: from
  network buffer to application memory, then to a preprocessing buffer,
  then to GPU-accessible memory, and finally to GPU VRAM. Mobile NPUs
  can eliminate most of these copies by sharing memory directly with the
  camera hardware. The camera writes pixels into a buffer that the NPU
  reads directly, avoiding the CPU entirely. This reduces both latency
  (no copy operations) and energy (memory copies consume significant
  power). The mechanism requires hardware support: the camera, CPU, and
  NPU must share a unified memory architecture, which modern mobile SoCs
  like Apple's M-series and Qualcomm Snapdragon provide.
\item
  \emph{Typical Hardware:} Mobile NPUs (Apple Neural Engine, Qualcomm
  Hexagon), embedded GPUs (Jetson).
\item
  \emph{Cold Start:} Milliseconds (model already in app memory); first
  inference may trigger JIT compilation (100-500ms).
\item
  \emph{Power Budget:} 1-5W sustained, with thermal throttling after
  prolonged inference.
\end{itemize}

\textbf{3. Bare-Metal Serving (TinyML)}

The model is compiled into the firmware of a microcontroller. There is
no operating system or dynamic memory allocator. ``Serving'' is a tight
loop reading sensors and invoking the interpreter. Optimization focuses
on \textbf{static memory usage} (fitting in SRAM).

\begin{itemize}
\tightlist
\item
  \emph{Key Difference:} All memory is pre-allocated (Tensor Arena).
  Dynamic batching is impossible.
\item
  \emph{Typical Hardware:} ARM Cortex-M series, ESP32, specialized
  TinyML accelerators.
\item
  \emph{Cold Start:} Microseconds (model weights in flash, tensor arena
  pre-allocated).
\item
  \emph{Power Budget:} Microwatts to milliwatts; battery operation for
  months or years.
\end{itemize}

Table~\ref{tbl-serving-spectrum} summarizes how these deployment
contexts shape serving system design:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2584}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2584}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2584}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2022}}@{}}
\caption{\textbf{Serving Architecture Spectrum}: The deployment context
fundamentally shapes every aspect of serving system design. Cloud
systems optimize for throughput with dynamic batching; mobile systems
optimize for energy with fixed batch-1; TinyML systems operate under
extreme memory and power constraints with no dynamic
allocation.}\label{tbl-serving-spectrum}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Characteristic}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Cloud/Datacenter}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Mobile/Edge}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{TinyML}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Characteristic}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Cloud/Datacenter}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Mobile/Edge}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{TinyML}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Latency Target} \textbf{Batch Size} \textbf{Memory}
\textbf{Power} \textbf{Update Mechanism} \textbf{Failure Mode}
\textbf{Monitoring} & 10-100ms 1-128 (dynamic) 16-80GB VRAM 300-700W
Container deploy Retry/failover Full telemetry & 20-50ms 1 (fixed) 2-8GB
shared 1-10W App store update Graceful degradation Limited analytics &
1-100ms 1 (fixed) 256KB-2MB SRAM 1-100mW Firmware OTA Silent or reset
Heartbeat only \\
\end{longtable}

\begin{tcolorbox}[enhanced jigsaw, breakable, colframe=quarto-callout-note-color-frame, toprule=.15mm, coltitle=black, bottomtitle=1mm, bottomrule=.15mm, opacitybacktitle=0.6, arc=.35mm, colbacktitle=quarto-callout-note-color!10!white, toptitle=1mm, left=2mm, titlerule=0mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{ResNet-50 Across the Serving Spectrum}, rightrule=.15mm, leftrule=.75mm, opacityback=0, colback=white]

The same ResNet-50 architecture requires dramatically different serving
strategies across deployment contexts:

\textbf{Cloud (V100 GPU):}

\begin{itemize}
\tightlist
\item
  Model format: TensorRT FP16 engine (49MB)
\item
  Inference: 1.4ms at batch-1, 14ms at batch-16
\item
  Throughput: 1,143 images/second (batched)
\item
  Memory: 2GB VRAM (model + activations for batch-32)
\end{itemize}

\textbf{Mobile (Pixel 6 NPU):}

\begin{itemize}
\tightlist
\item
  Model format: TensorFlow Lite INT8 (25MB)
\item
  Inference: 12ms at batch-1 (NPU), 45ms (CPU fallback)
\item
  Throughput: \textasciitilde80 images/second (single-stream)
\item
  Memory: 150MB peak (shared with app)
\item
  Energy: 0.8mJ per inference (NPU), 4.2mJ (CPU)
\end{itemize}

\textbf{TinyML (Cortex-M7):}

\begin{itemize}
\tightlist
\item
  Model format: Not feasible---ResNet-50 requires 98MB weights
\item
  Alternative: MobileNetV2-0.35 quantized to INT8 (1.4MB)
\item
  Inference: 120ms at batch-1
\item
  Throughput: \textasciitilde8 images/second
\item
  Memory: 320KB tensor arena (fits in 512KB SRAM)
\item
  Energy: 12mJ per inference
\end{itemize}

\textbf{Key insight}: The ``same model'' claim is misleading---each
deployment requires not just different optimization but often different
architectures entirely. TinyML serving cannot use ResNet-50; it requires
architectures designed for the constraints from the start.

\end{tcolorbox}

\subsection{The Load Balancer
Layer}\label{sec-model-serving-systems-load-balancer-layer-9c4d}

The preceding spectrum focused on how deployment context shapes serving
constraints. For cloud and datacenter deployments, where multiple
replicas serve the same model, an additional infrastructure layer is
required: the load balancer. Production serving systems place load
balancers between clients and model servers, providing three essential
functions for serving infrastructure.

\textbf{Request Distribution} routes incoming requests to available
model replicas using algorithms like round-robin or least-connections.
For latency-sensitive ML serving, algorithms that route away from slow
or overloaded replicas improve tail latency.

\textbf{Health Monitoring} continuously verifies that replicas are ready
to serve, routing traffic away from unhealthy instances. For ML systems,
health checks must verify not just process liveness but model readiness,
confirming that weights are loaded and warmup is complete.

\textbf{Deployment Support} enables safe model updates by gradually
shifting traffic between versions.
\textbf{?@sec-machine-learning-operations-mlops} examines deployment
strategies including canary testing, blue-green deployments, and shadow
mode validation.

For single-machine serving with multiple model instances, such as
running several ONNX Runtime sessions, the framework and operating
system handle request queuing. The full complexity of load balancing
becomes essential when scaling to distributed inference systems, where
multiple machines serve the same model. The implementation details of
request distribution algorithms and multi-replica architectures belong
to that distributed context.

\textbf{Impact on Queuing Analysis}: When capacity planning considers
``the server'' in this chapter, it means the single machine's model
serving capacity. The queuing dynamics analyzed in
Section~\ref{sec-model-serving-systems-queuing-theory-tail-latency-29a6}
apply to understanding single-machine behavior and determining when
scaling to multiple machines becomes necessary.

While load balancers distribute requests across replicas, achieving
predictable latency also requires controlling what happens \emph{within}
each machine. The operating system environment introduces its own
sources of variability.

\subsection{Deterministic Latency and Resource
Isolation}\label{sec-model-serving-systems-deterministic-latency-resource-isolation-4d1c}

An inference server does not operate in isolation. On a single machine,
the operating system manages multiple competing processes---logging
agents, monitoring tools, and system interrupts---which can
intermittently steal CPU cycles from the inference pipeline. These
``noisy neighbors'' are a primary source of \textbf{latency jitter},
where the time required to process identical requests varies
significantly, causing the 99th percentile (P99) latency to spike even
when the hardware is under-utilized.

To achieve deterministic performance on a single node, systems engineers
employ three primary isolation techniques:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{CPU Affinity (Pinning)}: Restricting the inference server's
  threads to specific physical CPU cores. This prevents the operating
  system from context-switching the server's processes, ensuring that
  the ``preprocessing'' stage of the pipeline always has immediate
  access to computational resources.
\item
  \textbf{Memory Locking (\texttt{mlock})}: Instructing the OS to lock
  the model weights and KV caches in physical RAM. This prevents the
  system from ``paging out'' model data to slow disk storage during
  periods of high memory pressure, ensuring consistent microsecond-scale
  access times.
\item
  \textbf{Interrupt Shielding}: Configuring the system to route network
  and storage interrupts to CPU cores not used by the inference runner.
  This ensures that a burst of incoming network traffic does not
  interrupt the GPU's command stream, protecting the ``heartbeat'' of
  the inference execution.
\end{enumerate}

Understanding these isolation principles transforms a simple ``model
script'' into a \textbf{deterministic service}, a transition essential
for safety-critical applications like autonomous driving or real-time
industrial control.

\section{Serving System
Architecture}\label{sec-model-serving-systems-serving-system-architecture-4879}

The preceding section established the architectural spectrum from cloud
to TinyML and the infrastructure layers that route requests to models.
Building a high-performance serving system requires coordinating
multiple software components to minimize overhead and maximize hardware
utilization. This section examines the internal architecture of
inference servers and the protocols that connect them to clients.

\subsection{Anatomy of an Inference
Server}\label{sec-model-serving-systems-anatomy-inference-server-f12e}

While model optimization focuses on the mathematical artifact, model
serving requires a specialized software architecture to manage
high-frequency request streams and hardware utilization. An inference
server\sidenote{\textbf{Inference Server}: The concept emerged from
Google's TensorFlow Serving (\citeproc{ref-olston2017tensorflow}{Olston
et al. 2017}), open-sourced February 2016, which pioneered the
separation of model logic from serving infrastructure. NVIDIA's Triton
(\citeproc{ref-nvidia2024triton}{Savard et al. 2024}), originally
TensorRT Inference Server with GA release in March 2019, extended this
to multi-framework support. These servers implement dynamic batching
that can improve GPU utilization by up to 70\% compared to naive
single-request serving. The architecture mirrors the separation of
concerns in traditional web servers like Apache (1995) and nginx (2004),
applying decades of distributed systems knowledge to ML deployment. }
(such as NVIDIA Triton, TensorFlow Serving, or TorchServe) is not a
simple wrapper around a model script; it is a high-performance scheduler
that manages concurrency, memory, and data movement.

Understanding the internal anatomy of these servers reveals how they
bridge the gap between irregular user traffic and the highly regular,
batch-oriented requirements of accelerators.

\subsection{The Request
Pipeline}\label{sec-model-serving-systems-request-pipeline-8fb1}

Every request traverses a multi-stage pipeline designed to maximize
hardware throughput while minimizing latency overhead.
Figure~\ref{fig-server-anatomy} visualizes this internal flow.

\begin{figure}[htb]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/8a932a7889e17bd554bde27128be8cb72039e5c3.pdf}}

}

\caption{\label{fig-server-anatomy}\textbf{Inference Server Anatomy}:
Modern inference servers organize request processing into a decoupled
pipeline. The Network Ingress handles high-concurrency protocols
(HTTP/gRPC), the Queue buffers bursts of traffic, and the Dynamic
Batcher aggregates individual requests into optimized tensors. The
Inference Runner manages the low-level execution on the hardware
accelerator, ensuring the GPU remains utilized through asynchronous
execution.}

\end{figure}%

The server architecture serves three critical functions:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Concurrency Management}: Servers use asynchronous event loops
  or thread pools to handle thousands of concurrent client connections
  without blocking. This ensures that network I/O wait times do not idle
  the accelerator.
\item
  \textbf{Request Transformation}: The server handles the conversion of
  network payloads (JSON/Protobuf) into the specific tensor formats
  required by the optimized model runtime. Image tensors, for example,
  can be stored as NCHW (batch, channels, height, width) or NHWC (batch,
  height, width, channels). PyTorch and TensorRT prefer NCHW because it
  places channel data contiguously, enabling efficient convolution on
  GPUs. TensorFlow defaults to NHWC, which is more efficient on CPUs. A
  format mismatch between client and server silently corrupts inference:
  the model interprets pixel rows as color channels, producing garbage
  outputs without raising errors.
\item
  \textbf{Model Management}: Servers manage the lifecycle of models,
  including loading weights into VRAM, managing versioning, and ensuring
  that ``warm-up'' inferences are completed before exposing the model to
  live traffic.
\end{enumerate}

Of these components, the scheduler deserves special attention because it
embodies the fundamental serving tradeoff between throughput and
latency.

\subsection{The Scheduler: Where Throughput Meets
Latency}\label{sec-model-serving-systems-scheduler-throughput-meets-latency-d022}

The \textbf{Scheduler} is the ``brain'' of the inference server. It
implements the dynamic batching logic discussed in
Section~\ref{sec-model-serving-systems-throughput-optimization-18d1}.
The scheduler must decide: ``Should I run this one request now to
minimize its latency, or wait 5 milliseconds for a second request to
arrive and process them together to maximize throughput?''

Systems designers use the \textbf{Batching Window} parameter to tune
this trade-off. A window of 0ms optimizes for pure latency (no
batching), while a window of 10-50ms is common for high-throughput cloud
services. This decision determines the ``duty cycle'' of the GPU---the
percentage of time the hardware is actually computing vs.~waiting for
work.

\subsection{Interface Protocols and
Serialization}\label{sec-model-serving-systems-interface-protocols-serialization-5510}

The mechanism used to transport data between client and server directly
affects the latency budget. While model inference is often highly
optimized, the cost of moving data into the model---serialization and
network protocol overhead---can become the dominant bottleneck,
especially for lightweight models where inference time is small.

\subsubsection{The Serialization
Bottleneck}\label{sec-model-serving-systems-serialization-bottleneck-e41f}

Text-based formats like JSON are ubiquitous but computationally
expensive. Parsing a JSON object requires reading every byte, validating
syntax, and converting text representations into machine-native types.
For high-throughput systems, this consumes CPU cycles that could
otherwise be used for request handling or preprocessing.

Binary formats like Protocol Buffers (Protobuf) or FlatBuffers reduce
this overhead by designing the wire format to map directly to in-memory
data structures. This enables ``zero-copy'' deserialization in optimal
cases, where the network buffer can be used directly without allocating
new memory.

\subsubsection{REST vs
gRPC}\label{sec-model-serving-systems-rest-vs-grpc-c7b7}

Two dominant paradigms define modern serving interfaces, each with
distinct system characteristics:

\textbf{REST (Representational State Transfer)} typically uses HTTP/1.1
and JSON. It is universally supported, human-readable, and stateless,
making it the default choice for public-facing APIs. However, standard
HTTP/1.1 requires a new TCP handshake for each request (unless
keep-alive is carefully tuned), and JSON serialization adds significant
latency for numerical data like tensors.

\textbf{gRPC (gRPC Remote Procedure Call)}\sidenote{\textbf{gRPC}:
Open-sourced by Google in February 2015, gRPC evolved from Stubby,
Google's internal RPC framework that had been handling tens of billions
of calls per second across their datacenters since approximately 2001.
The combination of HTTP/2 multiplexing and Protocol Buffers binary
serialization achieves roughly 10x lower serialization overhead than
REST/JSON, making it the de facto standard for latency-sensitive ML
inference APIs. } uses HTTP/2 and Protobuf. HTTP/2 enables multiplexing
multiple requests over a single persistent TCP connection, eliminating
handshake latency and allowing efficient binary streaming. Protobuf
provides strict type safety and efficient binary serialization, making
it the standard for internal service-to-service communication where
latency is critical.

\begin{tcolorbox}[enhanced jigsaw, breakable, colframe=quarto-callout-note-color-frame, toprule=.15mm, coltitle=black, bottomtitle=1mm, bottomrule=.15mm, opacitybacktitle=0.6, arc=.35mm, colbacktitle=quarto-callout-note-color!10!white, toptitle=1mm, left=2mm, titlerule=0mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Benchmarks: JSON vs Protobuf Serialization}, rightrule=.15mm, leftrule=.75mm, opacityback=0, colback=white]

Consider a request payload containing 1,000 floating point numbers
(e.g., an embedding vector).

\begin{itemize}
\tightlist
\item
  \textbf{JSON}: Uses \textasciitilde9 KB on the wire. Requires
  \textasciitilde50\(\mu\)s to parse.
\item
  \textbf{Protobuf}: Uses \textasciitilde4 KB on the wire. Requires
  \textasciitilde5\(\mu\)s to parse.
\end{itemize}

For a system processing 10,000 requests per second, switching to
Protobuf saves nearly half a core of CPU time just in serialization
overhead. This 10\(\times\) efficiency gain makes gRPC essential for
high-throughput internal microservices.

\end{tcolorbox}

\textbf{System Choice}: Use REST for public APIs to maximize developer
accessibility. Use gRPC for high-performance internal communication to
minimize the serialization tax.

The architectural components and protocols examined so far describe
\emph{how} serving systems are built. Understanding \emph{why} certain
configurations perform better requires analyzing what happens to
individual requests as they traverse these components.

\section{The Request
Lifecycle}\label{sec-model-serving-systems-request-lifecycle-d9c6}

With the serving architecture established, we now trace what happens to
a single request as it flows through the system. Understanding where
time goes within each request is essential for effective optimization:
you cannot improve what you do not measure.

\subsection{The Latency
Budget}\label{sec-model-serving-systems-latency-budget-ef40}

For dynamic inference systems, the fundamental difference from training
lies in optimization objectives. Training optimizes throughput:
maximizing samples processed per hour over days of computation. A
training job that processes 1000 samples per second is successful
regardless of how long any individual sample takes. Serving inverts this
priority, optimizing latency (introduced in
\textbf{?@sec-ml-system-architecture}) per request under strict time
constraints. A serving system with 1000ms per-request latency has
failed, even if it achieves excellent throughput.

This shift has concrete implications for system design
(\citeproc{ref-gujarati2020serving}{Gujarati et al. 2020}). The metrics
that matter change from aggregate throughput to latency distributions.
Mean latency tells you little about user experience; p50, p95, and p99
latencies reveal how the system performs across the full range of
requests. If your mean latency is 50ms but p99 is 2 seconds, one in a
hundred users waits 40 times longer than average. For consumer-facing
applications, these tail latencies often determine user satisfaction and
retention.\sidenote{\textbf{Tail Latency Impact}: Research at Google and
Amazon in the mid-2000s established that users are more sensitive to
latency variance than mean latency. Industry experience suggests that
latency increases of 100ms can measurably impact user engagement and
conversion rates for e-commerce applications, though the magnitude
varies by context. This is why service level objectives (SLOs) typically
specify percentile targets rather than averages. }

\phantomsection\label{callout-definitionux2a-1.2}
\begin{fbx}{callout-definition}{Definition: }{Latency Budget}
\phantomsection\label{callout-definition*-1.2}
\textbf{Latency Budget} refers to the maximum time allowed for a serving
request to complete, decomposed into allocations for
\emph{preprocessing}, \emph{inference}, and \emph{postprocessing}
phases. Effective latency budgeting requires understanding where time is
consumed and allocating resources accordingly.

\end{fbx}

Every serving request decomposes into three phases that each consume
part of the latency budget. Preprocessing transforms raw input such as
image bytes or text strings into model-ready tensors. Inference executes
the model computation. Postprocessing transforms model outputs into
user-facing responses.

A common misconception is that faster hardware automatically means
faster serving. In practice, preprocessing and postprocessing often
dominate total latency. Studies of production systems show preprocessing
consuming 60 to 70 percent of total request time when inference runs on
optimized accelerators (\citeproc{ref-nvidia2024tritontutorial}{NVIDIA
2025}). Optimizing only the inference phase yields diminishing returns
when the surrounding pipeline remains bottlenecked on CPU operations.

\subsection{Latency Distribution
Analysis}\label{sec-model-serving-systems-latency-distribution-analysis-b0f8}

Understanding where time goes requires instrumenting each phase
independently. Consider what happens when our ResNet-50 classifier
receives a JPEG image:

\begin{tcolorbox}[enhanced jigsaw, breakable, colframe=quarto-callout-note-color-frame, toprule=.15mm, coltitle=black, bottomtitle=1mm, bottomrule=.15mm, opacitybacktitle=0.6, arc=.35mm, colbacktitle=quarto-callout-note-color!10!white, toptitle=1mm, left=2mm, titlerule=0mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{ResNet-50: Latency Budget Breakdown}, rightrule=.15mm, leftrule=.75mm, opacityback=0, colback=white]

A typical serving request for our ResNet-50 classifier shows the
following latency distribution:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2561}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3537}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1585}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2073}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Phase}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Operation}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Time}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Percentage}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Preprocessing} \textbf{Preprocessing} \textbf{Preprocessing}
\textbf{Data Transfer} \textbf{Inference} \textbf{Postprocessing}
\textbf{Total} & JPEG decode Resize to 224×224 Normalize (mean/std)
CPU→GPU copy \textbf{ResNet-50 forward pass} Softmax + top-5 & 3.0ms
1.0ms 0.5ms 0.5ms \textbf{5.0ms} 0.1ms \textbf{10.1ms} & 30\% 10\% 5\%
5\% \textbf{50\%} \textasciitilde0\% \textbf{100\%} \\
\end{longtable}

Key insight: \textbf{Preprocessing consumes 45\% of latency} despite
model inference being the computationally intensive phase. With TensorRT
optimization reducing inference to 2ms, preprocessing would dominate at
75\%.

\end{tcolorbox}

This breakdown reveals why straightforward optimization efforts often
fail. Engineers focus on model optimization (quantization, pruning)
because that is where ML expertise applies, but the actual bottleneck is
image decoding running on CPU.

\phantomsection\label{callout-perspectiveux2a-1.3}
\begin{fbx}{callout-perspective}{Systems Perspective: }{The Quantitative Approach to Serving}
\phantomsection\label{callout-perspective*-1.3}
\textbf{Amdahl's Law at Work}: Preprocessing (4.5ms) and data transfer
(0.5ms) consume 50\% of total latency. Optimizing the model 10× faster
(5ms → 0.5ms) yields only 1.8× end-to-end speedup---from 10.1ms to
5.6ms. This is why focusing exclusively on model optimization
(quantization, pruning) often disappoints: the bottleneck is elsewhere.

\textbf{DSA Efficiency}: General-purpose CPUs achieve only 1-2\% of peak
performance at batch-1 because instruction overhead dominates. DSAs like
TPUs and Tensor Cores replace complex logic with dense MAC arrays,
achieving 10-100× higher arithmetic intensity. This makes hardware
acceleration a requirement for economically viable serving.

\textbf{Engineering Implication}: Profile before optimizing. If
preprocessing dominates, GPU-accelerated pipelines (NVIDIA DALI) may
outperform model quantization.

\end{fbx}

Moving preprocessing to GPU can reduce total latency by 6x in some
pipelines by eliminating CPU-GPU data transfers between stages
(\citeproc{ref-nvidia2024tritontutorial}{NVIDIA 2025}). Effective
optimization targets the largest time consumers first.

\subsubsection{The Killer Microseconds
Problem}\label{sec-model-serving-systems-killer-microseconds-problem-bc00}

Barroso, Patterson, and colleagues identified a critical gap in how
systems handle latency at different time scales
(\citeproc{ref-barroso2017attack}{Barroso et al. 2017}). Modern systems
efficiently handle nanosecond-scale events (CPU cache access, DRAM
reads) through hardware mechanisms like out-of-order execution, and
millisecond-scale events (disk I/O, network calls) through software
techniques like threading and asynchronous I/O. But microsecond-scale
events fall into an uncomfortable middle ground where neither approach
works well.

ML serving lives squarely in this microsecond regime. Individual
inference calls complete in 1 to 10ms, but the surrounding operations
such as serialization, memory allocation, network stack processing, and
encryption each add microseconds that compound into significant
overhead. Google's analysis found that a significant fraction, often 20
percent or more, of datacenter CPU cycles are consumed by this
``datacenter tax'' rather than useful computation. For serving systems,
this means a 2μs network fabric can become 100μs end-to-end through
software overhead, context switching costs of 5 to 10μs can exceed the
inference time for small models, and memory allocation patterns in
preprocessing can add unpredictable microsecond delays. These overheads
explain why serving latency often exceeds the sum of its measured parts,
and why system-level optimization through memory pooling, zero-copy data
paths, and kernel bypass matters as much as model optimization.

The latency budget framework provides a systematic approach to
optimization. First, measure each phase to identify the true bottleneck.
Then allocate engineering effort proportionally to where time is
actually spent. Finally, consider architectural changes such as GPU
preprocessing or batching strategies that can shift work between phases.

\subsection{Resolution and Input Size
Tradeoffs}\label{sec-model-serving-systems-resolution-input-size-tradeoffs-155d}

Input resolution affects both preprocessing and inference latency, but
the relationship differs depending on whether the system is
compute-bound (limited by arithmetic throughput) or memory-bound
(limited by data movement). A compute-bound system slows proportionally
to increased computation; a memory-bound system may show minimal
slowdown if activation tensors still fit in fast memory.
\textbf{?@sec-ai-acceleration} covers this distinction in depth through
roofline model analysis; understanding it is essential for making
informed resolution decisions.

For compute-bound models, Equation~\ref{eq-resolution-throughput}
formalizes how throughput scales inversely with resolution squared:

\begin{equation}\phantomsection\label{eq-resolution-throughput}{\frac{T(r_2)}{T(r_1)} = \left(\frac{r_1}{r_2}\right)^2}\end{equation}

Doubling resolution from 224 to 448 theoretically yields 4x slowdown
(measured: 3.6x due to fixed overhead amortization). However, at high
resolutions, models transition from compute-bound to memory-bound as
activation tensors exceed cache capacity.
Table~\ref{tbl-resolution-bottleneck} quantifies this transition for
ResNet-50, showing how arithmetic intensity decreases with resolution:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2099}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2716}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2840}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2099}}@{}}
\caption{\textbf{Resolution and Compute Bottleneck}: ResNet-50
arithmetic intensity decreases with resolution as activation sizes grow.
For a V100 PCIe (14 TFLOPS FP32, 900 GB/s bandwidth), the ridge point is
approximately 16 FLOPS/byte. At 224x224, compute dominates; by 512x512,
memory bandwidth becomes the limiting
factor.}\label{tbl-resolution-bottleneck}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Resolution}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Activation Size}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Arith. Intensity}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Bottleneck}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Resolution}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Activation Size}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Arith. Intensity}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Bottleneck}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{224×224} \textbf{384×384} \textbf{512×512} \textbf{640×640} &
12.5MB 36.8MB 65.5MB 102.4MB & 85 FLOPS/byte 49 FLOPS/byte 28 FLOPS/byte
18 FLOPS/byte & Compute Transitional Memory BW Memory BW \\
\end{longtable}

\subsubsection{Deployment-Specific Resolution
Decisions}\label{sec-model-serving-systems-deploymentspecific-resolution-decisions-1d76}

Different deployment contexts have distinct resolution requirements.
Mobile applications often accept lower resolution such as 224×224 for
object detection in camera viewfinders, where latency and battery life
dominate. Medical imaging requires high resolution of 512×512 or higher
for diagnostic accuracy, with relaxed latency requirements. Autonomous
vehicles use multiple resolutions for different tasks, with low
resolution for detection and high resolution crops for recognition.
Cloud APIs typically receive resolution set by client upload and must
handle a range gracefully. This variability makes cloud APIs ideal
candidates for adaptive resolution strategies, where the system selects
resolution dynamically based on content characteristics.

\subsubsection{Adaptive
Resolution}\label{sec-model-serving-systems-adaptive-resolution-ffe9}

Production systems can select resolution dynamically based on content.
One approach runs a lightweight classifier at 128×128 to categorize
content type, then selects task-appropriate resolution with documents at
512×512, landscapes at 224×224, and faces at 384×384. This achieves 1.4×
throughput improvement with 99.2 percent accuracy retention versus fixed
high resolution. This pattern trades preprocessing cost from running the
lightweight classifier for inference savings on the main model.

The latency analysis so far has focused on sequential processing: one
request completing before the next begins. But the preprocessing,
inference, and postprocessing stages use different hardware resources.
This separation creates an opportunity to process multiple requests
simultaneously.

\subsection{Utilization and Request
Pipelining}\label{sec-model-serving-systems-utilization-request-pipelining-c61c}

The preceding analysis examined where time goes within individual
pipeline stages. But optimizing each stage in isolation misses a
critical opportunity: the stages use different hardware resources. The
latency budget analysis in
Section~\ref{sec-model-serving-systems-latency-budget-ef40} reveals that
model inference is only one component of the request lifecycle. From a
hardware perspective, the primary goal of a serving system is to
maximize the \textbf{duty cycle} of the accelerator---the percentage of
time the GPU is performing useful computation.

In a serialized serving system, the hardware sits idle during network
I/O and CPU-based preprocessing. High-performance serving systems use
\textbf{Request Pipelining} to overlap these stages, ensuring the GPU is
fed a continuous stream of tensors.

\subsection{Overlapping I/O and
Compute}\label{sec-model-serving-systems-overlapping-io-compute-e18c}

Figure~\ref{fig-serving-pipeline-timing} contrasts serial execution with
pipelined execution. In the serial case (A), each request must complete
its entire lifecycle (Network \(\rightarrow\) CPU Preprocessing
\(\rightarrow\) GPU Inference \(\rightarrow\) Postprocessing) before the
next request begins. Even with a fast GPU, the system throughput is
limited by the slowest stage, and the GPU remains idle for more than
50\% of the time.

\begin{figure}[htb]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/60498cb297addf6051a9175f005f4356dce61257.pdf}}

}

\caption{\label{fig-serving-pipeline-timing}\textbf{Request Pipelining}:
Pipelining hides latency by overlapping independent operations across
different hardware resources. In pipelined execution (B), the CPU
processes the next request's data while the GPU executes the current
request's inference. This increases the GPU duty cycle toward 100\%,
effectively doubling or tripling throughput on the same hardware without
changing the model.}

\end{figure}%

Pipelining is enabled by \textbf{Asynchronous I/O} and
\textbf{Concurrency Models}. Instead of waiting for a GPU kernel to
finish, the server's CPU thread submits the work to the GPU's command
queue and immediately begins preprocessing the next incoming request.

\subsection{The Systems Metric: Hardware Duty
Cycle}\label{sec-model-serving-systems-systems-metric-hardware-duty-cycle-b147}

In the ``Quantitative Approach'' to ML systems, we define the efficiency
of a serving system by its ability to saturate the bottleneck resource.
For most ML systems, this is the GPU's compute cores or memory
bandwidth.

\[\text{System Efficiency} = \frac{\sum T_{\text{compute}}}{\text{Wall Clock Time} \times \text{Resource Count}}\]

If a ResNet-50 request takes 10ms total (5ms GPU, 5ms CPU), a serial
system achieves only 50\% efficiency. By pipelining just two requests,
efficiency approaches 100\% (assuming the CPU can keep up with the GPU).
If the CPU is too slow to feed the GPU, the system becomes
\textbf{CPU-bound}, and further model optimization provides zero
throughput gain, a direct application of Amdahl's Law (introduced in
\textbf{?@sec-ml-system-architecture}) to serving: if preprocessing
consumes 50\% of latency, maximum speedup is 2x regardless of how fast
the model runs.

\subsection{Postprocessing}\label{sec-model-serving-systems-postprocessing-3b24}

Preprocessing and inference produce raw tensors, but these
floating-point arrays carry no inherent meaning to applications or
users. The final phase of the request lifecycle, postprocessing,
transforms these tensors into actionable predictions: a 0.95 probability
becomes a confident ``dog'' label, a sequence of token IDs becomes
readable text, or a bounding box tensor becomes a highlighted region in
an image. While often overlooked in system design, postprocessing
significantly impacts both latency and the usefulness of predictions.

\subsubsection{From Logits to
Predictions}\label{sec-model-serving-systems-logits-predictions-09df}

Classification models output logits or probabilities across classes.
Converting these to predictions involves several potential steps
including argmax selection that chooses the highest-probability class,
thresholding that applies confidence thresholds before returning
predictions, top-k extraction that returns multiple high-probability
classes with scores, and calibration that adjusts raw probabilities to
better reflect true likelihoods.

\begin{tcolorbox}[enhanced jigsaw, breakable, colframe=quarto-callout-note-color-frame, toprule=.15mm, coltitle=black, bottomtitle=1mm, bottomrule=.15mm, opacitybacktitle=0.6, arc=.35mm, colbacktitle=quarto-callout-note-color!10!white, toptitle=1mm, left=2mm, titlerule=0mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{ResNet-50: Postprocessing Pipeline}, rightrule=.15mm, leftrule=.75mm, opacityback=0, colback=white]

For ResNet-50 image classification, typical postprocessing includes:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Transform raw logits to calibrated probabilities}
\CommentTok{\# Input: logits tensor of shape (batch\_size, 1000) {-} one score per ImageNet class}
\NormalTok{probs }\OperatorTok{=}\NormalTok{ torch.softmax(}
\NormalTok{    logits, dim}\OperatorTok{={-}}\DecValTok{1}
\NormalTok{)  }\CommentTok{\# Normalize to sum=1; \textasciitilde{}0.05ms on GPU}

\CommentTok{\# Extract top{-}5 predictions for multi{-}class response}
\CommentTok{\# topk returns (values, indices) sorted by probability}
\NormalTok{top5\_probs, top5\_indices }\OperatorTok{=}\NormalTok{ probs.topk(}\DecValTok{5}\NormalTok{)  }\CommentTok{\# \textasciitilde{}0.02ms; GPU operation}

\CommentTok{\# Map class indices to human{-}readable labels}
\CommentTok{\# IMAGENET\_CLASSES: list of 1000 class names from synset mapping}
\NormalTok{labels }\OperatorTok{=}\NormalTok{ [}
\NormalTok{    IMAGENET\_CLASSES[i] }\ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in}\NormalTok{ top5\_indices}
\NormalTok{]  }\CommentTok{\# \textasciitilde{}0.01ms; CPU lookup}

\CommentTok{\# Format response with predictions and metadata for API contract}
\NormalTok{response }\OperatorTok{=}\NormalTok{ \{}
    \StringTok{"predictions"}\NormalTok{: [}
\NormalTok{        \{}\StringTok{"label"}\NormalTok{: label, }\StringTok{"confidence"}\NormalTok{: }\BuiltInTok{float}\NormalTok{(prob)\}}
        \ControlFlowTok{for}\NormalTok{ label, prob }\KeywordTok{in} \BuiltInTok{zip}\NormalTok{(labels, top5\_probs)}
\NormalTok{    ],}
    \StringTok{"model\_version"}\NormalTok{: }\StringTok{"resnet50{-}v2.1"}\NormalTok{,  }\CommentTok{\# Enable client{-}side version tracking}
    \StringTok{"inference\_time\_ms"}\NormalTok{: }\FloatTok{5.2}\NormalTok{,  }\CommentTok{\# Observability for latency monitoring}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\textbf{Total postprocessing time}: \textasciitilde0.1ms (negligible
compared to preprocessing and inference)

\end{tcolorbox}

Each step adds latency but improves response utility. Calibration in
particular can add significant computation but is essential when
downstream systems make decisions based on confidence scores.

\subsubsection{Output
Formatting}\label{sec-model-serving-systems-output-formatting-f78a}

Production systems rarely return raw predictions. Outputs must conform
to API contracts, often requiring JSON serialization with specific
schema, confidence score formatting and thresholding, error handling for
edge cases such as no confident prediction or out-of-distribution input,
and metadata attachment including model version, inference time, and
feature attributions.

The latency budget analysis reveals where time goes within a single
request. But production systems do not process requests in isolation:
they must handle hundreds or thousands of concurrent requests competing
for finite resources. Understanding this concurrency requires a
different analytical framework.

\section{Queuing Theory and Tail
Latency}\label{sec-model-serving-systems-queuing-theory-tail-latency-29a6}

The request lifecycle analysis explains where time goes within a single
request, but production systems must handle many concurrent requests
competing for finite resources. Understanding why latency degrades under
load requires queuing theory, the mathematical framework that explains
how requests wait for service in any system with finite capacity. These
principles apply to web servers and ML inference alike, and explain the
counterintuitive behavior that causes well-provisioned systems to
violate latency SLOs when load increases modestly.

\subsection{Queuing
Fundamentals}\label{sec-model-serving-systems-queuing-fundamentals-10d3}

Two mathematical foundations govern serving system behavior: Little's
Law, which relates queue depth to throughput, and the M/M/1 model, which
predicts how latency degrades under load. Together, they provide the
quantitative framework for capacity planning.

\subsection{Little's
Law}\label{sec-model-serving-systems-littles-law-9352}

The most fundamental result in queuing theory is Little's
Law,\sidenote{\textbf{Little's Law}: Proven by John D.C. Little in 1961
(\citeproc{ref-little1961proof}{Little 1961}), this theorem establishes
that \(L = \lambda W\) holds for any stable queuing system regardless of
arrival patterns, service distributions, or scheduling policies. The
remarkable generality makes it one of the most useful results in
operations research. For serving systems, it enables capacity planning
from observable metrics: measuring queue depth and arrival rate directly
yields average latency without instrumenting individual requests. }
which Equation~\ref{eq-littles-law} expresses as a simple relationship
between three quantities in any stable system:

\begin{equation}\phantomsection\label{eq-littles-law}{L = \lambda \cdot W}\end{equation}

where \(L\) is the average number of requests in the system, \(\lambda\)
is the arrival rate (requests per second), and \(W\) is the average time
each request spends in the system. This relationship holds regardless of
arrival distribution, service time distribution, or scheduling policy.

Little's Law has immediate practical implications. If your inference
service averages 10ms per request (\(W = 0.01\)s) and you observe 50
concurrent requests in the system on average (\(L = 50\)), then your
arrival rate must be \(\lambda = L/W = 5000\) requests per second.
Conversely, if you need to limit concurrent requests to 10 (perhaps due
to GPU memory constraints), and your service time is 10ms, you can
sustain at most 1000 requests per second.

\subsection{The Utilization-Latency
Relationship}\label{sec-model-serving-systems-utilizationlatency-relationship-a2f0}

For a system with Poisson arrivals and exponential service times (the
M/M/1 queue model), the average time in system follows:

\begin{equation}\phantomsection\label{eq-mm1-wait}{W = \frac{1}{\mu - \lambda} = \frac{\text{service time}}{1 - \rho}}\end{equation}

where \(\mu\) is the service rate (requests per second the server can
handle), and \(\rho = \lambda/\mu\) is the utilization (fraction of time
the server is busy).

This equation reveals why serving systems exhibit nonlinear behavior. At
50\% utilization, average wait time is \(2\times\) the service time. At
80\% utilization, it is \(5\times\). At 90\% utilization, it is
\(10\times\). Small increases in load near capacity cause
disproportionate latency increases.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3086}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3457}}@{}}
\caption{\textbf{Utilization-Latency Relationship}: Average wait time as
a multiple of service time for an M/M/1 queue. At 50\% utilization, wait
time is 2x service time; at 90\%, it reaches 10x. This nonlinear growth
explains why systems that perform well at moderate load suddenly violate
SLOs when traffic increases: moving from 80\% to 90\% utilization
doubles wait time.}\label{tbl-utilization-latency}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Utilization (\(\rho\))}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Wait Time Multiple}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Example (5ms service)}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Utilization (\(\rho\))}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Wait Time Multiple}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Example (5ms service)}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
50\% 70\% 80\% 90\% 95\% & 2.0× 3.3× 5.0× 10.0× 20.0× & 10ms 17ms 25ms
50ms 100ms \\
\end{longtable}

The M/M/1 model assumes exponentially distributed service times, but ML
inference typically has near-constant service time for fixed batch
sizes, making the M/D/1 (deterministic service) model more accurate in
practice. We use M/M/1 here because it yields closed-form solutions and
produces conservative estimates. Table~\ref{tbl-utilization-latency}
reveals how average wait time grows rapidly as utilization approaches
100\%. For M/D/1 queues, average wait time is approximately half of
M/M/1 at the same utilization, which matters for capacity planning:
M/M/1 analysis will slightly over-provision, erring on the side of
meeting SLOs rather than violating them.\sidenote{\textbf{Kendall
Notation}: The M/M/1 notation was introduced by British statistician
David Kendall in 1953 and follows the pattern A/S/c
(Arrivals/Service/servers). ``M'' stands for ``Markovian'' (memoryless,
meaning exponential distributions), honoring Russian mathematician
Andrey Markov (1856-1922). ``D'' means deterministic. So M/M/1 describes
a single server with exponential arrivals and service times, while M/D/1
has deterministic service. ML inference is closer to M/D/1 since
inference time is nearly constant, but M/M/1 yields conservative
estimates suitable for capacity planning. }

\subsection{Multi-Server
Considerations}\label{sec-model-serving-systems-multiserver-considerations-00fc}

The preceding analysis focuses on a single ML node---one machine serving
inference requests. This scope aligns with Volume I's focus on mastering
the fundamental unit of ML systems. Understanding single-node queuing
dynamics is prerequisite to effective scaling: you cannot optimize a
distributed system without first understanding the behavior of its
components.

\textbf{When Single-Node Analysis Applies}: M/M/1 analysis remains the
foundation for:

\begin{itemize}
\tightlist
\item
  \textbf{Right-sizing individual nodes}: Determining whether a single
  GPU can meet latency SLOs at expected traffic
\item
  \textbf{Identifying the scaling trigger}: Calculating when traffic
  exceeds single-node capacity
\item
  \textbf{Cost-effective provisioning}: Avoiding premature scale-out
  that wastes resources
\end{itemize}

For traffic exceeding single-node capacity, production systems deploy
multiple replicas behind a load balancer. The M/M/c queuing model
extends M/M/1 to c parallel servers, showing that multiple replicas
dramatically improve tail latency: the probability of all servers being
simultaneously slow drops exponentially with server count. At c=4
replicas, p99 latency can be 3× lower than the single-server case at the
same total throughput.

\textbf{Scope Boundary}: This chapter establishes single-node serving
foundations. Distributed inference systems---model sharding across GPUs,
tensor parallelism, pipeline parallelism---introduce coordination
overhead and consistency challenges that require advanced scaling
principles.

\subsection{Tail
Latency}\label{sec-model-serving-systems-tail-latency-5376}

Production SLOs\sidenote{\textbf{SLO (Service Level Objective)}: Emerged
from telecommunications in the 1980s, formalized in ITIL (IT
Infrastructure Library) standards in the 1990s. SLOs define measurable
targets (e.g., ``p99 latency \textless{} 100ms''), while SLAs (Service
Level Agreements) are contractual commitments with penalties. Google's
Site Reliability Engineering (SRE) practices, documented in their 2016
book, popularized ``error budgets'' derived from SLOs: if your SLO is
99.9\% availability, you have a 0.1\% error budget to spend on
deployments and experiments. } typically specify percentile targets
(p95, p99) rather than averages because tail latency determines user
experience for the slowest requests (\citeproc{ref-dean2013tail}{Dean
and Barroso 2013}). For an M/M/1 queue, the p99 latency follows:

\begin{equation}\phantomsection\label{eq-p99-latency}{W_{p99} \approx \text{service time} \cdot \left(1 + \frac{\ln(100 \cdot \rho)}{1 - \rho}\right)}\end{equation}

At 70 percent utilization, p99 latency is approximately fifteen times
the service time, while average latency is only 3.3 times. This explains
why systems that seem healthy with low average latency can have
unacceptable tail latency, since the average hides the experience of the
unluckiest requests.

\subsubsection{The Tail at Scale
Problem}\label{sec-model-serving-systems-tail-scale-problem-958d}

Dean and Barroso's analysis reveals why tail latency becomes critical as
systems scale beyond single machines (\citeproc{ref-dean2013tail}{Dean
and Barroso 2013}). The key insight is that when requests fan out to
multiple servers, the probability of experiencing at least one slow
response grows rapidly with server count. This ``tail at scale'' effect
makes individual server tail latency critical for overall system
performance.

For single-machine serving, this principle has two implications. First,
tail latency on individual machines matters because it will compound
when systems eventually scale. Second, the tail-tolerant techniques
described below (hedging, graceful degradation) provide value even on
single machines and become essential at scale.

\textbf{Tail-Tolerant Techniques}: Request hedging sends redundant
requests after a timeout, accepting whichever response arrives first.
Backup requests and load balancing away from slow servers directly
address latency variance. These techniques apply to single-machine
serving with multiple GPU streams or model replicas, and become
essential when scaling to distributed inference systems.

\phantomsection\label{callout-notebook-1.2}
\begin{fbx}{callout-notebook}{AI Engineer’s Notebook 1.2: }{Worked Example: ResNet-50 Capacity Planning}
\phantomsection\label{callout-notebook-1.2}
Consider designing a ResNet-50 serving system with these requirements:

\begin{itemize}
\tightlist
\item
  \textbf{Target p99 latency}: 50ms
\item
  \textbf{Peak expected traffic}: 5,000 requests per second
\item
  \textbf{Service time} (TensorRT FP16): 5ms
\end{itemize}

\textbf{Step 1: Find safe utilization}

Applying Equation~\ref{eq-p99-latency} to constrain
\(W_{p99} \leq 50\)ms with 5ms service time and solving for \(\rho\):

\[5\text{ms} \cdot \left(1 + \frac{\ln(100 \cdot \rho)}{1 - \rho}\right) \leq 50\text{ms}\]

This yields \(\rho \leq 0.72\) (72\% maximum utilization).

\textbf{Step 2: Calculate required service rate}

\[\mu_{\text{required}} = \frac{\lambda}{\rho_{\text{safe}}} = \frac{5000}{0.72} = 6944 \text{ requests/second}\]

\textbf{Step 3: Determine GPU count}

Single V100 throughput at batch=16: 1,143 images/second

\[\text{GPUs needed} = \frac{6944}{1143} = 6.1 \rightarrow 7 \text{ GPUs}\]

\textbf{Step 4: Add headroom for variance}

Production systems add 30\% headroom for traffic spikes and variance:

\[\text{Final count} = 7 \times 1.3 = 9.1 \rightarrow 10 \text{ GPUs}\]

\textbf{Step 5: Verify fault tolerance (N+1 redundancy)}

The 30\% headroom addresses traffic variance, but production systems
also need fault tolerance. With 10 GPUs, losing one leaves 9 GPUs
handling 5,000 QPS:

\[\text{Utilization after failure} = \frac{5000 / 1143}{9} = 48.6\%\]

This remains well below the 72\% safe utilization threshold, confirming
N+1 redundancy is satisfied. For stricter fault tolerance requirements,
N+2 redundancy (tolerating two simultaneous failures) would require
11-12 GPUs.

\textbf{Result}: Provision 10 V100 GPUs to serve 5,000 QPS at 50ms p99
latency with N+1 fault tolerance.

\end{fbx}

The queuing analysis explains the capacity planning approach detailed in
Section~\ref{sec-model-serving-systems-capacity-planning-96a3} and
connects directly to the MLPerf Server scenario.
\textbf{?@sec-benchmarking-ai} explains how MLPerf measures throughput
only for requests meeting the latency SLO: a system achieving 10,000 QPS
but violating the SLO on 5\% of requests reports only 9,500 valid QPS.

\subsection{Tail-Tolerant
Techniques}\label{sec-model-serving-systems-tailtolerant-techniques-066e}

Rather than eliminating all sources of latency variability, which is
often impractical, production systems employ techniques that tolerate
variability while still meeting SLOs (\citeproc{ref-dean2013tail}{Dean
and Barroso 2013}; \citeproc{ref-dean2012rapid}{Dean 2012}). These
techniques treat latency variance as a given and design around it.

\subsubsection{Hedged
Requests}\label{sec-model-serving-systems-hedged-requests-293b}

When a request has not completed within the expected time, send a
duplicate request to another server.\sidenote{\textbf{Hedging}: Borrowed
from finance, where ``hedging'' means reducing risk by making offsetting
bets. The term derives from the literal hedge (a boundary of shrubs)
that protects a garden. Financial hedging dates to the 1600s Dutch tulip
markets. Google's Jeff Dean introduced ``hedged requests'' in his
influential 2013 ``Tail at Scale'' paper, applying the financial concept
to distributed systems: send redundant requests to protect against the
risk of slow responses. } The client uses whichever response arrives
first and cancels the other. For ML serving, this means maintaining
multiple model replicas and routing slow requests to alternative
replicas. The overhead is modest: if you hedge at the 95th percentile,
only 5\% of requests generate duplicates, increasing load by just 5\%
while dramatically reducing tail latency.

\textbf{Cancellation Complexity}: A critical implementation detail is
that CUDA kernels cannot be interrupted mid-execution. When a hedged
request completes, the duplicate must be cancelled, but if inference has
already begun on the GPU, cancellation approaches include checking a
cancellation flag before launching inference, accepting wasted compute
for the in-flight kernel, or using request prioritization to
deprioritize the duplicate. Since hedging typically applies only to the
slowest 5 percent of requests, the overhead from occasional wasted
compute remains acceptable.

\subsubsection{Tied
Requests}\label{sec-model-serving-systems-tied-requests-25de}

Send the request to multiple servers simultaneously, but include a tag
allowing servers to cancel execution once another server begins
processing. This eliminates the delay of waiting to detect a slow
response before hedging. For inference servers with significant startup
overhead from model loading and memory allocation, tied requests ensure
at least one server begins immediately.

\subsubsection{Canary
Requests}\label{sec-model-serving-systems-canary-requests-483c}

For requests that fan out to many backends, first send the request to a
small subset of 1 to 2 servers.\sidenote{\textbf{Canary}: From the
practice of using canary birds in coal mines from the early 1900s
through the 1980s. Miners brought caged canaries underground because the
birds' high metabolic rate made them sensitive to carbon monoxide and
methane, dying before gas concentrations became lethal to humans. In
software, ``canary'' describes any small-scale test that detects
problems before they affect the full system, whether canary deployments,
canary requests, or canary tests. } If these return within expected
time, send to the remainder. If the canary is slow, the system can take
corrective action by retrying elsewhere or using cached results before
committing to the full fan-out. This prevents a single slow backend from
stalling an entire distributed inference request.

\subsubsection{Graceful
Degradation}\label{sec-model-serving-systems-graceful-degradation-df79}

When load exceeds capacity, return approximate results rather than
timing out. For classification, return cached predictions for similar
inputs. For generative models, return shorter outputs. For ensemble
systems, return predictions from a subset of models. This maintains
responsiveness at the cost of some accuracy, which users often prefer to
outright failures.

\subsubsection{Admission
Control}\label{sec-model-serving-systems-admission-control-f796}

When traffic exceeds capacity, accepting all requests can trigger
widespread SLO violations. Admission control proactively rejects
requests when queue depth exceeds a threshold, returning immediate 503
responses rather than accepting requests that are likely to timeout.
This sacrifices throughput to protect latency for admitted requests.

\textbf{Setting the Threshold}: A practical starting point is 2 to 3
times service time multiplied by the number of workers. For a system
with 4 workers and 10ms service time, this yields a queue depth
threshold of 80 to 120 requests. Adaptive admission control adjusts
thresholds based on observed p99 latency, tightening when latency
increases above target and relaxing when latency remains healthy.

\textbf{Retry Storm Prevention}: A subtle failure mode occurs when all
replicas are overloaded simultaneously. If the load balancer retries
rejected requests at other replicas that are also overloaded, retry
traffic amplifies the overload. Coordinated load shedding addresses this
by sharing load information across replicas, enabling system-wide
decisions about which requests to accept. When global load exceeds
capacity, replicas collectively reject the same fraction of requests
rather than each rejecting independently and triggering retries.

These techniques become essential at scale when fan-out amplification
makes individual server tail latency visible to users. Single-machine
serving systems can implement hedged and tied requests across GPU
streams or model replicas. The queuing analysis here assumes FIFO
processing, but production systems often implement priority scheduling
such as deadline-aware or shortest-job-first approaches to further
reduce tail latency for heterogeneous workloads
(\citeproc{ref-harchol2013performance}{Harchol-Balter 2013}).

The tail-tolerant techniques examined in this section optimize the flow
of requests through a functioning serving system. But the queuing
analysis assumes a critical precondition: that models are loaded,
initialized, and producing correct predictions. In production, this
assumption fails regularly: during deployments, new instances must load
models from scratch; during scaling events, cold start latency affects
the first requests to new replicas; and when preprocessing pipelines
diverge from training, accuracy silently degrades. The next section
examines these lifecycle challenges that must be solved before queuing
optimization becomes relevant.

\section{Model Lifecycle
Management}\label{sec-model-serving-systems-model-lifecycle-management-ff2e}

The queuing analysis from previous sections assumes two prerequisites:
models are loaded and ready to process requests, and predictions match
what was validated during development. Production systems often violate
both assumptions. Cold start latency can exceed inference time by orders
of magnitude during scaling events. Subtle preprocessing differences
between training and serving pipelines cause accuracy degradation that
no amount of queuing optimization can address. This section examines the
challenges that threaten these foundational assumptions.

\subsection{Training-Serving
Skew}\label{sec-model-serving-systems-trainingserving-skew-7b99}

A model that performed well during validation may silently degrade when
deployed. This degradation, invisible to latency monitoring and
exception tracking, represents one of the most subtle failure modes in
production ML.

\phantomsection\label{callout-definitionux2a-1.4}
\begin{fbx}{callout-definition}{Definition: }{Training-Serving Skew}
\phantomsection\label{callout-definition*-1.4}
\textbf{Training-Serving Skew} occurs when a model's performance in
production degrades relative to its \emph{training validation metrics},
typically caused by \emph{discrepancies} between the training and
serving data pipelines or environments.

\end{fbx}

\textbf{?@sec-machine-learning-operations-mlops} provides comprehensive
coverage of skew diagnosis, monitoring, and organizational prevention
strategies. Here we focus on the \emph{serving-specific} manifestation:
\textbf{preprocessing divergence}. This occurs when the real-time
inference pipeline processes raw data differently than the batch
training pipeline, a common failure mode when training uses
Python/Pandas while serving uses C++/Java or optimized inference
servers. Unlike data drift (which
\textbf{?@sec-machine-learning-operations-mlops} addresses through
monitoring), preprocessing divergence is deterministic and preventable
through careful engineering.

\begin{tcolorbox}[enhanced jigsaw, breakable, colframe=quarto-callout-note-color-frame, toprule=.15mm, coltitle=black, bottomtitle=1mm, bottomrule=.15mm, opacitybacktitle=0.6, arc=.35mm, colbacktitle=quarto-callout-note-color!10!white, toptitle=1mm, left=2mm, titlerule=0mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{ResNet-50: Image Preprocessing Skew}, rightrule=.15mm, leftrule=.75mm, opacityback=0, colback=white]

For ResNet-50 serving, common sources of skew include:

\textbf{Resize interpolation}: Training uses PIL.BILINEAR while OpenCV
defaults to cv2.INTER\_LINEAR. These produce pixel-level differences
that can shift accuracy by 0.5-1\%.

\textbf{Color space handling}: JPEG loading in different libraries may
produce BGR vs RGB ordering. If the model trained on RGB but serves BGR
inputs, predictions are essentially random.

\textbf{Normalization constants}: ImageNet normalization uses specific
mean/std values. Using \texttt{mean={[}0.5,\ 0.5,\ 0.5{]}} instead of
\texttt{mean={[}0.485,\ 0.456,\ 0.406{]}} shifts inputs out of the
training distribution.

\textbf{Prevention}: The safest approach is to export the exact
preprocessing code used during training and run it identically in
serving, or use a framework like NVIDIA DALI that can help standardize
preprocessing across training and serving environments.

\end{tcolorbox}

\subsection{Model Loading and
Initialization}\label{sec-model-serving-systems-model-loading-initialization-cc5a}

With preprocessing pipelines designed to avoid training-serving skew,
the next challenge is getting models ready to serve. Before processing
any request, models must load from storage into memory and prepare for
inference (\citeproc{ref-romero2021infaas}{Romero et al. 2021}). This
initialization phase adds latency that affects system responsiveness
during deployments, scaling events, and recovery from failures.

\phantomsection\label{callout-definitionux2a-1.5}
\begin{fbx}{callout-definition}{Definition: }{Cold Start}
\phantomsection\label{callout-definition*-1.5}
\textbf{Cold Start} refers to the latency penalty incurred by the first
request(s) processed by a new model instance, caused by
\emph{initialization overheads} such as \emph{weight loading},
\emph{runtime compilation}, and \emph{memory allocation}.

\end{fbx}

Understanding cold start dynamics enables designing systems that meet
latency requirements from the moment they begin serving traffic.

\subsection{Cold Start
Anatomy}\label{sec-model-serving-systems-cold-start-anatomy-2924}

Cold start latency compounds from multiple sources, each adding to the
time between deployment and serving readiness. Weight loading reads
model parameters from disk or network storage. Graph compilation
performs just-in-time compilation of operations for the specific
hardware. Memory allocation reserves GPU memory for activations and
intermediate values. Warmup\sidenote{\textbf{Warmup}: The computing
metaphor derives from physical warming, where engines and machines
perform better after reaching operating temperature. In JIT-compiled
systems like the JVM (1990s), ``warmup'' specifically refers to the
period when the runtime gathers profiling data and compiles hot paths.
For ML serving, warmup serves a dual purpose: triggering lazy memory
allocation and populating CPU/GPU caches with frequently-accessed data,
ensuring the first real user request does not pay these one-time costs.
} execution performs initial inferences that populate caches and trigger
lazy initialization.

\begin{tcolorbox}[enhanced jigsaw, breakable, colframe=quarto-callout-note-color-frame, toprule=.15mm, coltitle=black, bottomtitle=1mm, bottomrule=.15mm, opacitybacktitle=0.6, arc=.35mm, colbacktitle=quarto-callout-note-color!10!white, toptitle=1mm, left=2mm, titlerule=0mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{ResNet-50: Cold Start Timeline}, rightrule=.15mm, leftrule=.75mm, opacityback=0, colback=white]

Loading ResNet-50 for production serving involves the following cold
start phases:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.1471}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.5098}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Phase}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Duration}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Notes}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Weight loading (SSD)} \textbf{Weight loading (S3)} \textbf{CUDA
context} \textbf{TensorRT compilation} \textbf{Warmup (10 inferences)}
\textbf{Total (local, optimized)} \textbf{Total (cloud, first deploy)} &
0.5s 3-5s 0.3-0.5s 15-30s 0.2s \textbf{\textsubscript{1.5s\textbf{
}}35s} & 98MB FP32 weights from local storage Network latency dominates
for cloud storage GPU driver initialization and memory setup Converts
PyTorch model to optimized engine Triggers remaining lazy initialization
With pre-compiled TensorRT engine, warm container Including compilation
from cold state \\
\end{longtable}

\textbf{Key insight}: Pre-compiling models and storing the optimized
engine eliminates the 30-second compilation phase on subsequent
deployments.

\textbf{CUDA Context}: Before any GPU operation, the CUDA runtime must
establish a \emph{context}: a data structure that tracks memory
allocations, loaded kernels, and device state. Creating a context
requires communicating with the GPU driver and allocating GPU memory for
internal bookkeeping. This one-time cost (0.3-0.5s) affects every new
process that uses the GPU. CUDA 11+ introduced lazy initialization that
defers some setup until first use, reducing apparent startup time but
shifting cost to the first inference.

\textbf{CUDA MPS (Multi-Process Service)}: Normally, each process
creates its own CUDA context, and the GPU time-slices between contexts.
MPS allows multiple processes to share a single context, eliminating
redundant initialization and enabling concurrent kernel execution. For
serving systems running multiple model replicas, MPS can reduce
aggregate cold start time and improve GPU utilization. The trade-off is
reduced isolation: a crash in one process can affect others sharing the
MPS server.

\end{tcolorbox}

Without warmup, the first real request triggers compilation and memory
allocation mid-inference, often causing timeout failures. A request that
normally takes 5ms might require 500ms during cold start, violating SLOs
and degrading user experience.

\subsection{Loading
Strategies}\label{sec-model-serving-systems-loading-strategies-eb38}

Different loading strategies trade off cold start duration against
serving performance and memory efficiency.

\textbf{Full loading} reads the entire model into memory before serving
begins. This maximizes inference speed since all weights are immediately
available, but extends cold start duration and limits model size to
available memory. Full loading is appropriate when cold start latency is
acceptable and models comfortably fit in memory.

\textbf{Memory mapping} maps model files directly into the address
space, loading pages on demand as accessed. This reduces cold start time
since inference can begin before the full model loads, but causes
unpredictable latency as pages fault in during initial requests. Memory
mapping works well for infrequently accessed model components but can
cause latency spikes if critical weights are not preloaded.

\textbf{Lazy initialization} defers compilation and allocation until
first use. This minimizes startup time but shifts latency to the first
request. Production systems often combine lazy initialization with
synthetic warmup requests to trigger initialization before real traffic
arrives.

\subsection{Model Caching
Infrastructure}\label{sec-model-serving-systems-model-caching-infrastructure-4f1a}

Production systems cache model weights at the infrastructure level to
reduce cold start for common deployment scenarios:

\textbf{Container Image Embedding}: Bundle model weights directly in the
container image. This produces a single deployment artifact and
eliminates network fetches at startup, but creates large images (often
10-50GB) that slow container pulls and consume registry storage. Best
for models that rarely update.

\textbf{Shared Filesystem}: Mount a network filesystem (EFS, GCS FUSE)
containing model weights. Multiple replicas share cached weights, and
updates propagate immediately without redeployment. Network latency
affects cold start, and filesystem availability becomes a critical
dependency. Best for organizations with many models and frequent
updates.

\textbf{Node-Local SSD Cache}: Pre-populate local SSDs on inference
nodes with frequently-used models. Provides fast loading (500MB/s+ for
NVMe) without network dependency, but requires cache management to
handle model updates and capacity limits. Best for high-traffic models
where cold start latency is critical.

The choice depends on model update frequency: infrequent updates favor
container embedding, frequent updates favor shared filesystem, and
performance-critical deployments benefit from local caching with
background refresh.

\subsection{Multi-Model
Serving}\label{sec-model-serving-systems-multimodel-serving-a9c1}

Production systems often serve multiple models from a single machine,
whether different model versions for A/B testing, ensemble components,
or entirely different models sharing infrastructure. GPU memory becomes
the limiting resource, requiring careful management strategies.

Strategies for multi-model serving include time-multiplexing that loads
one model at a time and swaps based on request routing, memory sharing
where models share GPU memory to limit concurrent execution but enable
more models, and model virtualization where frameworks like Triton
manage model lifecycle by loading and unloading based on traffic
patterns (\citeproc{ref-nvidia2024triton}{Savard et al. 2024}). The
choice depends on request patterns. If models receive traffic evenly,
concurrent loading works. If traffic is bursty and model-specific,
time-multiplexing with intelligent preloading reduces average latency
while maximizing GPU utilization.

\subsubsection{Multi-Stream
Execution}\label{sec-model-serving-systems-multistream-execution-1b1f}

When multiple models or multiple instances of the same model must run
concurrently on a single GPU, the hardware must partition resources
between them. NVIDIA's Multi-Instance GPU technology enables
hardware-level isolation, dividing an A100 into up to 7 independent GPU
instances, each with dedicated memory and compute resources. MIG is
available on A100, A30 (up to 4 instances), H100, H200, and newer data
center GPUs. For older GPUs such as V100 or T4, CUDA stream scheduling
provides time-multiplexed sharing without hardware isolation.

The choice depends on whether consistent latency with MIG or maximum
utilization with shared streams is the priority.

\subsubsection{Model Swapping and Host
Memory}\label{sec-model-serving-systems-model-swapping-host-memory-c54f}

When the aggregate size of all models exceeds GPU memory capacity, the
serving system must swap models between host memory (DRAM) and device
memory (VRAM) on demand. This introduces a new latency component
determined by the PCIe bus bandwidth.

For a 10 GB model on PCIe Gen4 x16 (32 GB/s theoretical bandwidth),
loading takes at least:
\[ T_{\text{load}} = \frac{10 \text{ GB}}{32 \text{ GB/s}} \approx 312 \text{ ms} \]

To mitigate this, systems use \textbf{Pinned Memory} (page-locked host
memory). By default, the operating system can move (``page'') any memory
region to disk when RAM is under pressure. This creates a problem for
GPU transfers: if the GPU's DMA (Direct Memory Access) engine begins
reading a memory region that gets paged out mid-transfer, the transfer
fails or stalls. To avoid this, the CPU must first copy data to a
temporary pinned buffer before the GPU can safely read it, adding both
latency and CPU overhead.

Pinning memory instructs the OS to keep that region permanently in
physical RAM. The GPU's DMA engine can then transfer data directly from
the pinned region at full PCIe bandwidth without CPU involvement. The
trade-off is that pinned memory reduces the RAM available for other
processes and cannot be reclaimed under memory pressure. For model
serving, the performance gain (2-3× faster transfers) typically
justifies pinning model weights and frequently-used input buffers, while
leaving less critical memory pageable.

The lifecycle management strategies examined so far ensure models are
ready to serve: loaded into memory, warmed up, and producing predictions
consistent with training. With these prerequisites satisfied, the
queuing dynamics from
Section~\ref{sec-model-serving-systems-queuing-theory-tail-latency-29a6}
become relevant. The next optimization opportunity lies in how requests
are grouped for processing, which directly affects both the throughput
and latency terms in our queuing equations.

\section{Throughput
Optimization}\label{sec-model-serving-systems-throughput-optimization-18d1}

With models loaded, initialized, and ready to serve, the next
optimization opportunity lies in how requests are grouped for
processing. Batching\sidenote{\textbf{Batch}: From Old French ``bache''
(a quantity baked at one time), the term entered computing in the 1950s
to describe jobs processed together without human interaction, as
contrasted with interactive computing. IBM's batch processing systems of
the 1960s would collect punch cards overnight and process them
sequentially. The ML usage preserves this core meaning: group samples
together for efficient processing, trading individual response time for
aggregate throughput. } differs fundamentally between training and
serving (\citeproc{ref-crankshaw2017clipper}{Crankshaw et al. 2017}).
Training batches maximize throughput, processing hundreds or thousands
of samples together with no concern for individual sample latency.
Serving batches must balance throughput against individual request
latency, typically processing single digits of requests together while
ensuring no request waits too long.

\phantomsection\label{callout-definitionux2a-1.6}
\begin{fbx}{callout-definition}{Definition: }{Dynamic Batching}
\phantomsection\label{callout-definition*-1.6}
\textbf{Dynamic Batching} refers to a serving strategy that collects
incoming requests within a \emph{time window} and processes them
together, trading individual request latency for improved
\emph{throughput} and \emph{hardware utilization}. The window size and
maximum batch size parameters control this tradeoff.

\end{fbx}

\subsection{Why Batching
Helps}\label{sec-model-serving-systems-batching-helps-f1dc}

Modern accelerators achieve peak efficiency only at sufficient batch
sizes (\citeproc{ref-shen2019nexus}{Shen et al. 2019}). A single
inference request leaves most compute units idle because GPUs are
designed for parallel execution across thousands of threads. Batching
amortizes fixed costs across multiple requests and enables parallel
execution across the batch dimension.

Two fixed costs dominate at small batch sizes. \textbf{Kernel launch
overhead}\sidenote{\textbf{Kernel}: From Old English ``cyrnel'' meaning
seed or grain, the essential core of something. In operating systems
(1960s), the kernel is the core that manages hardware resources. CUDA
borrowed this term around 2007 for GPU functions because they represent
the computational ``core'' of parallel algorithms. Unlike OS kernels
that run continuously, GPU kernels are discrete units of parallel work
launched by the CPU and executed across thousands of GPU threads
simultaneously. } is the time for the CPU to prepare and submit work to
the GPU. Each layer in a neural network typically requires a separate
kernel launch: the CPU must assemble kernel parameters, copy them to
GPU-accessible memory, and signal the GPU to begin execution. This
overhead is typically 5-20μs per kernel, independent of batch size.
ResNet-50 has approximately 50 layers, so kernel launch alone adds
250-1000μs per inference. At batch size 1, this overhead may exceed the
actual compute time; at batch size 32, the same overhead is amortized
across 32 images. \textbf{Weight loading} reads model parameters from
GPU memory (VRAM) to the compute units. At batch size 1, the GPU reads
all weights to process one image; at batch size 32, the same weight read
processes 32 images, achieving 32× better memory efficiency.

\phantomsection\label{callout-notebook-1.3}
\begin{fbx}{callout-notebook}{AI Engineer’s Notebook 1.3: }{Worked Example: ResNet-50 Batching Efficiency}
\phantomsection\label{callout-notebook-1.3}
The throughput-latency tradeoff for ResNet-50 on a V100 GPU illustrates
the power of batching:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.1717}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2222}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2424}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.1717}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.1616}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Batch Size}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Inference Time}*
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Per-Image Compute}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Throughput}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{GPU Util.}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
1 4 8 16 32 & 5.0ms 7.2ms 9.1ms 14.0ms 25.0ms & 5.0ms 1.8ms 1.1ms 0.9ms
0.8ms & 200 img/s 556 img/s 879 img/s 1,143 img/s 1,280 img/s & 15\%
42\% 65\% 85\% 95\% \\
\end{longtable}

*Times shown are pure inference time, excluding queue wait.
Section~\ref{sec-model-serving-systems-traffic-patterns-batching-strategy-2e6b}
analyzes how user-perceived latency includes batching window wait.

\textbf{Key insight}: Batch size 32 achieves 6.4× higher throughput than
batch size 1. However, user-perceived latency includes both queue wait
and inference time. With a 10ms batching window and 25ms inference,
total latency reaches 35ms versus 5ms at batch size 1.

\end{fbx}

The table reveals the throughput-latency tradeoff in stark terms: larger
batches dramatically improve hardware efficiency but increase
per-request latency. In practice, the optimal batch size depends on both
the latency Service Level Objective (SLO) and the arrival rate of
requests. The following analysis shows how to find the sweet spot.

\phantomsection\label{callout-perspectiveux2a-1.7}
\begin{fbx}{callout-perspective}{Systems Perspective: }{Napkin Math: The Batching Sweet Spot}
\phantomsection\label{callout-perspective*-1.7}
\textbf{Problem}: You are serving a ResNet-50 model. At batch=1, the GPU
is mostly idle (15\% utilization). You want to increase throughput to
save money, but you have a \textbf{20 ms} latency budget.

\textbf{The Math}: 1. \textbf{Baseline (Batch 1)}: Inference = \textbf{5
ms}. Throughput = \textbf{200 img/s}. 2. \textbf{Optimized (Batch 8)}: -
\textbf{Wait Time}: You set a \textbf{5 ms} batching window to collect
requests. - \textbf{Inference Time}: Batch 8 inference takes \textbf{9
ms}. - \textbf{User Latency}:
\(5 \text{ ms (wait)} + 9 \text{ ms (compute)} = \mathbf{14 \text{ ms}}\).
- \textbf{Throughput}:
\(8 \text{ img} / 14 \text{ ms} \approx \mathbf{570 \text{ img/s}}\).

\textbf{The Systems Conclusion}: By accepting a \textbf{3x increase in
latency} (5ms \(\rightarrow\) 14ms), you have achieved nearly \textbf{3x
higher throughput} on the same hardware. As long as 14ms is under your
20ms budget, this is ``free'' capacity. This trade-off is the
fundamental lever of serving economics.

\end{fbx}

The efficiency gains from batching come at a cost: requests must wait
for the batch to form. This creates a fundamental tension between
throughput optimization (larger batches) and latency minimization
(immediate processing). Understanding the different batching strategies
and their tradeoffs is essential for tuning this balance.

\subsection{Static vs Dynamic
Batching}\label{sec-model-serving-systems-static-vs-dynamic-batching-fd0a}

\textbf{Static batching} waits for a fixed batch size before processing.
Simple to implement but problematic in practice: during low traffic,
requests wait indefinitely for a full batch. During high traffic, large
batches increase per-request latency.

\textbf{Dynamic batching} collects requests within a time window,
processing whatever has arrived when the window closes
(\citeproc{ref-olston2017tensorflow}{Olston et al. 2017}). This bounds
maximum wait time regardless of traffic level. The window size
represents a direct tradeoff: shorter windows reduce latency but
sacrifice throughput; longer windows improve throughput but increase
latency.

Typical configurations use windows of 5-50ms with maximum batch sizes of
8-32 for latency-sensitive applications. The optimal configuration
depends on request arrival patterns, model characteristics, and latency
requirements.

\subsection{Dynamic Batching Latency-Throughput
Trade-offs}\label{sec-model-serving-systems-dynamic-batching-latencythroughput-tradeoffs-986d}

Dynamic batching introduces a fundamental tension between throughput
optimization and latency constraints. Understanding this tradeoff
quantitatively enables systematic configuration decisions rather than
trial-and-error tuning.

\phantomsection\label{callout-perspectiveux2a-1.8}
\begin{fbx}{callout-perspective}{Systems Perspective: }{Napkin Math: The Little's Law of Queues}
\phantomsection\label{callout-perspective*-1.8}
\textbf{The Problem}: Why does your latency spike when traffic
increases?

\textbf{The Physics}: Little's Law (\(L = \lambda W\)) governs all
stable queues. * \(L\): Average number of items in the system (Queue
Depth). * \(\lambda\): Average arrival rate (Requests/Second). * \(W\):
Average time an item spends in the system (Latency).

\textbf{The Equation}:
\[ \text{Latency (W)} = \frac{\text{Queue Depth (L)}}{\text{Throughput (}\lambda\text{)}} \]

\textbf{The Systems Conclusion}: If your hardware is saturated
(throughput \(\lambda\) is maxed out), any increase in traffic increases
queue depth (\(L\)). Since \(\lambda\) cannot grow, \textbf{Latency
(\(W\)) must grow linearly with queue depth}. This is why
\textbf{Admission Control} (rejecting requests when \(L\) is high) is
the \emph{only} way to preserve latency during an overload.

\end{fbx}

Equation~\ref{eq-batching-latency} decomposes the total user-perceived
latency for a batched request into two components:

\begin{equation}\phantomsection\label{eq-batching-latency}{L_{\text{total}} = L_{\text{wait}} + L_{\text{compute}}(b)}\end{equation}

where \(L_{\text{wait}}\) is the time spent waiting in the batching
queue and \(L_{\text{compute}}(b)\) is the inference time for batch size
\(b\). The batching window \(T\) bounds wait time
(\(L_{\text{wait}} \leq T\)), while batch size affects compute time
through GPU utilization characteristics.

\subsubsection{Queue Waiting Time
Analysis}\label{sec-model-serving-systems-queue-waiting-time-analysis-8d5c}

For Poisson arrivals with rate \(\lambda\) and batching window \(T\),
requests arrive uniformly within the window. A request arriving at time
\(t\) within the window waits \(T - t\) for the batch to close.
Equation~\ref{eq-avg-wait} shows that the average wait time is simply
half the window:

\begin{equation}\phantomsection\label{eq-avg-wait}{E[L_{\text{wait}}] = \frac{T}{2}}\end{equation}

This simple relationship has direct implications. A 20ms batching window
adds 10ms average latency regardless of batch size achieved. If your
latency SLO is 50ms and inference takes 5ms, the batching window
consumes 20\% of your latency budget before any computation begins.

\subsubsection{Batch Size
Distribution}\label{sec-model-serving-systems-batch-size-distribution-bd6b}

The number of requests collected during window \(T\) follows a Poisson
distribution with mean \(\lambda T\).
Equation~\ref{eq-batch-distribution} formalizes this relationship:

\begin{equation}\phantomsection\label{eq-batch-distribution}{P(\text{batch size} = k) = \frac{(\lambda T)^k e^{-\lambda T}}{k!}}\end{equation}

Table~\ref{tbl-batch-variability} quantifies this variability, showing
how batch size fluctuates for different traffic levels with a fixed 10ms
window:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2065}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.1848}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.1522}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.1848}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2391}}@{}}
\caption{\textbf{Batch Size Variability}: At low traffic, batching
windows frequently contain zero requests (wasted GPU cycles). At
moderate traffic, batch sizes fluctuate significantly around the mean.
High traffic provides more stable batching but still sees 13\% of
batches exceeding twice the mean
size.}\label{tbl-batch-variability}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Arrival Rate}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Mean Batch}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Std Dev}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{P(batch=0)}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{P(batch≥2×mean)}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Arrival Rate}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Mean Batch}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Std Dev}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{P(batch=0)}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{P(batch≥2×mean)}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{50 QPS} \textbf{200 QPS} \textbf{500 QPS} \textbf{1000 QPS} &
0.5 2.0 5.0 10.0 & 0.7 1.4 2.2 3.2 & 61\% 14\% 0.7\% 0.005\% & 1\% 9\%
12\% 13\% \\
\end{longtable}

\subsubsection{Throughput Maximization
Strategy}\label{sec-model-serving-systems-throughput-maximization-strategy-27f5}

Throughput optimization requires maximizing the number of requests
processed per unit time. For a system with service time \(S(b)\) for
batch size \(b\), throughput follows Equation~\ref{eq-batch-throughput}:

\begin{equation}\phantomsection\label{eq-batch-throughput}{\text{Throughput}(b) = \frac{b}{T + S(b)}}\end{equation}

The numerator increases linearly with batch size while the denominator
increases sub-linearly (due to GPU parallelism). This creates an optimal
batch size that balances these competing effects.

For ResNet-50 on a V100 GPU, service time scales as
\(S(b) = 5\text{ms} + 0.6b\) (5ms fixed overhead plus 0.6ms per
additional image in the batch). With \(T = 10\)ms batching window:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.1828}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2043}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.2151}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.1828}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.1828}}@{}}
\caption{\textbf{Batching Throughput Analysis}: ResNet-50 throughput on
V100 with 10ms batching window. Throughput increases 14.6x from batch
size 1 to 32 (64 to 935 img/s), but total latency more than doubles
(15.6ms to 34.2ms). The optimal configuration depends on whether the
latency SLO or throughput target is the binding
constraint.}\label{tbl-batching-throughput}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Batch Size}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Service Time}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Total Latency}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Throughput}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Efficiency}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Batch Size}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Service Time}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Total Latency}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Throughput}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Efficiency}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
1 4 8 16 32 & 5.6ms 7.4ms 9.8ms 14.6ms 24.2ms & 15.6ms 17.4ms 19.8ms
24.6ms 34.2ms & 64 img/s 230 img/s 404 img/s 650 img/s 935 img/s & Low
Moderate Good High Maximum \\
\end{longtable}

\phantomsection\label{callout-notebook-1.4}
\begin{fbx}{callout-notebook}{AI Engineer’s Notebook 1.4: }{Engineering Calculation: The Batching Sweet Spot}
\phantomsection\label{callout-notebook-1.4}
\textbf{The Iron Law Connection:} In serving, we maximize throughput by
amortizing the \textbf{Latency (Overhead)} term.
\[ \text{Time}_{\text{total}} = \frac{\text{Ops}}{\text{Peak} \cdot \text{Util}} + \text{Latency}_{\text{fixed}} \]

\textbf{Deriving the Sweet Spot:} * \textbf{Case 1 (Batch 1):} Overhead
(5ms) \(\approx\) Compute (0.6ms). Efficiency \(\approx 10\%\). The GPU
is mostly waiting. * \textbf{Case 2 (Batch 32):} Overhead (5ms) \(\ll\)
Compute (19.2ms). Efficiency \(\approx 80\%\). The GPU is crunching
numbers.

\textbf{The Golden Rule:} Increase batch size until the \textbf{Latency
Term} becomes negligible (\textless{} 10\% of total time). Beyond this
point, you gain minimal throughput but pay a linear latency penalty.

\end{fbx}

\subsubsection{Latency-Constrained
Optimization}\label{sec-model-serving-systems-latencyconstrained-optimization-8f66}

When latency SLOs provide the binding constraint, the optimization
problem becomes finding the maximum batch size that meets the SLO. For
SLO \(L_{\text{SLO}}\) and average wait time \(T/2\),
Equation~\ref{eq-latency-constrained-batch} defines the maximum
allowable batch size:

\begin{equation}\phantomsection\label{eq-latency-constrained-batch}{b_{\text{max}} = \max\{b : \frac{T}{2} + S(b) \leq L_{\text{SLO}}\}}\end{equation}

Consider a 50ms p95 latency SLO for ResNet-50 serving:

\textbf{Scenario 1: Conservative window (T = 5ms)} - Average wait: 2.5ms
- Latency budget for inference: 47.5ms - Maximum batch size: 71 (but
typically capped at 32 for memory) - Achieved throughput:
\textasciitilde1,140 img/s (batch=32)

\textbf{Scenario 2: Aggressive window (T = 25ms)} - Average wait: 12.5ms
- Latency budget for inference: 37.5ms - Maximum batch size: 48 -
Achieved throughput: \textasciitilde1,280 img/s (batch=48)

The aggressive window achieves only 12\% higher throughput but increases
average latency by 10ms and p99 latency by 25ms. Examine
Table~\ref{tbl-batching-throughput}: for latency-sensitive applications,
the conservative window provides better user experience at modest
throughput cost.

\subsubsection{SLO Violation
Analysis}\label{sec-model-serving-systems-slo-violation-analysis-4f72}

Batch size variability causes SLO violations even when mean latency
appears safe. The p99 latency includes both worst-case wait time (full
window) and worst-case batch size (governed by Poisson tail).
Equation~\ref{eq-p99-batch-latency} captures this relationship:

\begin{equation}\phantomsection\label{eq-p99-batch-latency}{L_{p99} \approx T + S(b_{p99})}\end{equation}

where \(b_{p99}\) is the 99th percentile batch size. For
\(\lambda = 500\) QPS and \(T = 10\)ms:

\begin{itemize}
\tightlist
\item
  Mean batch size: 5
\item
  p99 batch size: 11 (from Poisson distribution)
\item
  Mean latency: \(5\text{ms} + 9.0\text{ms} = 14\text{ms}\)
\item
  p99 latency: \(10\text{ms} + 11.6\text{ms} = 21.6\text{ms}\)
\end{itemize}

The p99 latency is 1.54× the mean, reflecting both wait time variance
and batch size variance. Systems that provision based on mean latency
will experience SLO violations.

\subsubsection{Adaptive Batching
Windows}\label{sec-model-serving-systems-adaptive-batching-windows-c404}

Fixed batching windows waste latency budget during high traffic when
large batches form quickly. Listing~\ref{lst-adaptive-batching}
demonstrates how adaptive strategies adjust the window based on queue
depth.

\begin{codelisting}

\caption{\label{lst-adaptive-batching}\textbf{Adaptive Batching Window}:
Dynamically adjusts batch timeout based on queue depth and arrival rate,
reducing average latency by 27\% compared to fixed windows while
maintaining throughput.}

\centering{

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ adaptive\_batching\_window(queue\_depth, arrival\_rate, slo\_ms):}
    \CommentTok{"""Compute optimal batching window based on current system state."""}
\NormalTok{    target\_batch\_size }\OperatorTok{=} \DecValTok{16}  \CommentTok{\# Optimal batch for GPU utilization}

    \CommentTok{\# Fast path: batch ready, close immediately to minimize latency}
    \ControlFlowTok{if}\NormalTok{ queue\_depth }\OperatorTok{\textgreater{}=}\NormalTok{ target\_batch\_size:}
        \ControlFlowTok{return} \DecValTok{0}

    \CommentTok{\# Compute maximum allowable wait from SLO constraint}
    \CommentTok{\# Reserve 30\% of latency budget for batching, remainder for inference}
\NormalTok{    max\_wait }\OperatorTok{=}\NormalTok{ slo\_ms }\OperatorTok{*} \FloatTok{0.3}

    \CommentTok{\# Estimate time to accumulate target batch at current arrival rate}
    \ControlFlowTok{if}\NormalTok{ arrival\_rate }\OperatorTok{\textgreater{}} \DecValTok{0}\NormalTok{:}
\NormalTok{        requests\_needed }\OperatorTok{=}\NormalTok{ target\_batch\_size }\OperatorTok{{-}}\NormalTok{ queue\_depth}
\NormalTok{        estimated\_wait }\OperatorTok{=}\NormalTok{ requests\_needed }\OperatorTok{/}\NormalTok{ arrival\_rate}
        \CommentTok{\# Return minimum of estimated wait and SLO{-}constrained maximum}
        \ControlFlowTok{return} \BuiltInTok{min}\NormalTok{(estimated\_wait, max\_wait)}

    \ControlFlowTok{return}\NormalTok{ (}
\NormalTok{        max\_wait  }\CommentTok{\# Low traffic: use full budget to accumulate batch}
\NormalTok{    )}
\end{Highlighting}
\end{Shaded}

}

\end{codelisting}%

This approach reduces average wait time during high traffic while
maintaining batch sizes. For traffic varying between 200-1000 QPS:

\begin{itemize}
\tightlist
\item
  Fixed window (10ms): Average latency 15ms, throughput 650 img/s
\item
  Adaptive window: Average latency 11ms (27\% reduction), throughput 680
  img/s (5\% improvement)
\end{itemize}

The interplay between window size and batch limits creates a space of
possible configurations, each representing a different balance between
throughput and latency.

\textbf{Throughput-Latency Pareto Frontier}

The batching configuration space forms a Pareto frontier where improving
throughput requires accepting higher latency.
Table~\ref{tbl-pareto-batching} traces this frontier across five
representative configurations:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 10\tabcolsep) * \real{0.1579}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 10\tabcolsep) * \real{0.1404}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 10\tabcolsep) * \real{0.1579}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 10\tabcolsep) * \real{0.1579}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 10\tabcolsep) * \real{0.1491}}
  >{\raggedright\arraybackslash}p{(\linewidth - 10\tabcolsep) * \real{0.2018}}@{}}
\caption{\textbf{Batching Pareto Frontier}: Each configuration
represents a different point on the throughput-latency trade-off curve.
Moving from 2ms to 50ms windows improves throughput by only 52\% while
increasing p99 latency by 5.4×. Diminishing returns make aggressive
batching costly for latency-sensitive
applications.}\label{tbl-pareto-batching}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Window (ms)}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Max Batch}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Avg Latency}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{p99 Latency}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Throughput}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Configuration}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Window (ms)}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Max Batch}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Avg Latency}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{p99 Latency}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Throughput}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Configuration}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
2 5 10 20 50 & 16 32 32 64 128 & 8ms 10ms 15ms 23ms 38ms & 18ms 22ms
35ms 52ms 98ms & 890 img/s 1,140 img/s 1,240 img/s 1,310 img/s 1,350
img/s & Ultra-low latency Balanced Moderate latency Throughput-optimized
Maximum throughput \\
\end{longtable}

\subsubsection{Practical Configuration
Guidelines}\label{sec-model-serving-systems-practical-configuration-guidelines-9791}

Based on quantitative analysis, principled batching configuration
follows these guidelines. Start with the latency budget by allocating 20
to 30 percent of SLO to batching wait time. Estimate traffic using the
p95 arrival rate rather than average to account for traffic spikes.
Calculate the maximum window as
\(T_{\text{max}} = 0.3 \times L_{\text{SLO}}\). Determine the batch size
limit from GPU memory and p99 latency constraints. Monitor the actual
distribution since batch size variance indicates whether traffic
assumptions hold.

For ResNet-50 with 50ms SLO and 500 QPS traffic:

\begin{itemize}
\tightlist
\item
  Latency budget for batching: 15ms
\item
  Maximum window: 15ms
\item
  Expected batch size: 7.5
\item
  Maximum batch size: 32 (memory limit)
\item
  Configuration: \(T = 12\)ms, \(b_{\text{max}} = 32\)
\item
  Predicted p99 latency: 43ms (within SLO)
\item
  Predicted throughput: 1,180 img/s
\end{itemize}

\subsection{Continuous
Batching}\label{sec-model-serving-systems-continuous-batching-8bb6}

Autoregressive models like language models generate outputs token by
token, creating a batching challenge that differs from single-pass
models like ResNet-50. Traditional batching processes all sequences in a
batch for all generation steps, wasting compute when sequences complete
at different times (\citeproc{ref-yu2022orca}{Yu et al. 2022}). If one
sequence in a batch of 8 finishes after 10 tokens while others need 100
tokens, 87.5\% of the compute for that sequence slot is wasted. This
inefficiency matters as language models grow to dominate production
inference workloads.

Continuous batching (also called iteration-level batching) addresses
this waste by allowing new requests to join a batch between generation
steps and completed sequences to exit (\citeproc{ref-kwon2023vllm}{Kwon
et al. 2023}). Rather than forming static batches that persist for the
entire generation process, the system manages batch composition
dynamically at each decoding iteration.

The mechanism works as follows: when a sequence generates its
end-of-sequence token, its slot becomes immediately available. A waiting
request can fill that slot for the next iteration rather than waiting
for the entire batch to complete. Similarly, the system can add new
requests to available slots without interrupting ongoing generation.
This dynamic approach maintains high GPU utilization even when sequence
lengths vary dramatically.

Systems implementing continuous batching, such as vLLM and TensorRT-LLM,
achieve 2-4× higher throughput than traditional static batching
(\citeproc{ref-agrawal2024sarathi}{Agrawal et al. 2025}). The
improvement comes from two sources: eliminating wasted compute on
completed sequences and reducing average wait time for new requests. For
production language model serving where response lengths vary from
single tokens to thousands, continuous batching has become essential for
cost-effective deployment.

Memory management adds complexity to continuous batching. As sequences
enter and exit the batch, the key-value cache that stores attention
context must be dynamically allocated and freed. Consider what happens
when sequences of varying lengths share GPU memory: a 100-token sequence
completes and releases its cache, but a new 150-token sequence cannot
use that space because it needs a larger contiguous block. Over time,
small unusable gaps accumulate between allocated regions, eventually
preventing new sequences from starting even when total free memory
appears sufficient. This \emph{memory fragmentation} can waste 40 to 50
percent of available memory in naive implementations, severely limiting
the concurrent batch size that determines throughput.

\textbf{PagedAttention},\sidenote{\textbf{PagedAttention}: Introduced by
Kwon et al.~at SOSP 2023, this algorithm directly applies operating
system virtual memory concepts to GPU memory management for LLMs. Before
PagedAttention, researchers found that existing systems wasted 60-80\%
of KV cache memory due to fragmentation and over-reservation. By
borrowing paging and copy-on-write mechanisms from OS design,
PagedAttention reduces waste to under 4\%, enabling 2-4x higher
throughput on the same hardware. This technique has become the de facto
standard in production LLM serving systems. } introduced in vLLM, solves
this fragmentation problem by applying operating system virtual memory
concepts to GPU memory (\citeproc{ref-kwon2023vllm}{Kwon et al. 2023}).
Instead of allocating one contiguous block per sequence, PagedAttention
divides the KV cache into fixed-size \emph{pages} (typically 16 tokens
each). A sequence's cache consists of pointers to non-contiguous pages
scattered across GPU memory. When a sequence completes, its pages return
to a free list and can be reused by any new sequence, regardless of
length. This approach achieves near-zero fragmentation: vLLM reports
memory utilization above 95\% compared to 50-60\% for contiguous
allocation schemes. The overhead is modest (one pointer lookup per page
during attention computation), making PagedAttention the standard for
production LLM serving.

\begin{tcolorbox}[enhanced jigsaw, breakable, colframe=quarto-callout-note-color-frame, toprule=.15mm, coltitle=black, bottomtitle=1mm, bottomrule=.15mm, opacitybacktitle=0.6, arc=.35mm, colbacktitle=quarto-callout-note-color!10!white, toptitle=1mm, left=2mm, titlerule=0mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{LLM Serving: Beyond the Fundamentals}, rightrule=.15mm, leftrule=.75mm, opacityback=0, colback=white]

Language model serving introduces challenges beyond the batching and
memory principles established here. The key-value cache that stores
attention context scales with sequence length and batch size, often
exceeding the model weights themselves in memory consumption. Techniques
like speculative decoding use small draft models to propose multiple
tokens that the target model verifies in parallel, achieving 2-3×
latency reduction for interactive applications. Weight-only quantization
(INT4 weights with FP16 activations) proves more effective than
activation quantization for memory-bandwidth-bound LLM inference.

These LLM-specific optimizations build directly on the foundations this
chapter establishes: queuing theory governs request scheduling, batching
tradeoffs determine throughput-latency curves, and precision selection
follows the same accuracy-efficiency principles. The serving
fundamentals apply universally; LLM serving adds domain-specific
techniques atop this foundation. Advanced treatments provide detailed
coverage of KV cache optimization, including advanced techniques for
multi-tenant serving and distributed inference.

\end{tcolorbox}

While continuous batching represents the state of the art for LLM
serving, not all deployment scenarios benefit from batching at all. The
sophisticated techniques examined so far, from dynamic batching windows
to PagedAttention, optimize for high-throughput server workloads. But
these techniques introduce complexity and latency overhead that may not
be justified for all deployment contexts. A fundamental question
remains: when does batching hurt rather than help?

\subsection{When Not to
Batch}\label{sec-model-serving-systems-batch-a464}

Some scenarios require single-request processing. Ultra-low latency
requirements where p99 latency must stay under 10ms make any batching
delay unacceptable. Highly variable request sizes where inputs vary
dramatically in size cause batching to create padding overhead that
wastes compute. Memory constraints where models already consume most GPU
memory mean batch activations may cause out-of-memory errors.

\subsection{Session Affinity
Constraints}\label{sec-model-serving-systems-session-affinity-constraints-8b1f}

When requests from the same user or session should route to the same
replica, batching becomes constrained. Session affinity, also called
sticky sessions, matters for three main reasons.

\textbf{KV-Cache Reuse}: For conversational AI, the key-value cache from
previous turns dramatically speeds up multi-turn conversations. Routing
a follow-up request to a different replica forfeits this cached context,
increasing latency by 2 to 5 times for long conversations.

\textbf{User-Specific Models}: Some systems serve personalized models or
adapters per user. Routing requests to the replica that has already
loaded that user's adapter avoids repeated loading overhead.

\textbf{Stateful Preprocessing}: When preprocessing maintains state
through tokenizer caches or session-specific normalization, routing to a
different replica requires rebuilding this state.

The tension with batching is clear since strict affinity constrains
which requests can be batched together, potentially reducing batch sizes
and GPU utilization. Production systems often implement soft affinity
where requests prefer their assigned replica but can overflow to others
when that replica is overloaded. This preserves most affinity benefits
while maintaining load balance.

\subsection{Traffic Patterns and Batching
Strategy}\label{sec-model-serving-systems-traffic-patterns-batching-strategy-2e6b}

The optimal batching strategy depends critically on how requests arrive.
Different deployment contexts exhibit fundamentally different arrival
patterns, each requiring distinct batching approaches. The MLPerf
inference benchmark codifies these patterns into four scenarios that
directly map to real-world deployments, as
\textbf{?@sec-benchmarking-ai} explains in detail.

\subsubsection{Server Traffic (Poisson
Arrivals)}\label{sec-model-serving-systems-server-traffic-poisson-arrivals-d20b}

Cloud APIs and web services typically receive requests following a
Poisson process,\sidenote{\textbf{Poisson Process}: A stochastic model
where events occur continuously and independently at a constant average
rate. Named after French mathematician Simeon Denis Poisson (1781-1840),
this model accurately describes many real-world arrival patterns
including web requests and API calls. The key property for serving
systems is that inter-arrival times are exponentially distributed,
meaning the probability of long gaps between requests decays
exponentially, which is why batching windows can be tuned
probabilistically. } where arrivals are independent and uniformly
distributed over time. Equation~\ref{eq-poisson-batch} expresses the
expected batch size for Poisson arrivals with rate \(\lambda\) and
batching window \(T\):

\begin{equation}\phantomsection\label{eq-poisson-batch}{E[\text{batch size}] = \lambda \cdot T}\end{equation}

The variance equals the mean (a property of Poisson distributions), so
batch sizes fluctuate significantly at moderate traffic. With
\(\lambda = 200\) requests/second and \(T = 10\)ms, expected batch size
is 2, but 16\% of windows will have zero requests (wasted compute
cycles) while others may have 4 or more.

The optimal batching window balances waiting cost against throughput
benefit. Equation~\ref{eq-optimal-window} defines this optimum:

\begin{equation}\phantomsection\label{eq-optimal-window}{T_{\text{optimal}} = \min\left(L - S, \sqrt{\frac{S}{\lambda}}\right)}\end{equation}

where \(L\) is the latency SLO and \(S\) is the service time. A perhaps
surprising result emerges from this equation: as traffic increases, the
optimal window decreases while achieved batch sizes still grow.
Table~\ref{tbl-traffic-adaptive} demonstrates this phenomenon across
four traffic levels.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2346}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2593}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2593}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2222}}@{}}
\caption{\textbf{Traffic-Adaptive Batching}: Higher traffic enables
shorter windows while still achieving larger batches. The optimal window
decreases even as batch sizes grow because more requests arrive per unit
time.}\label{tbl-traffic-adaptive}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Arrival Rate}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Optimal Window}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Avg Batch Size}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{p99 Latency}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Arrival Rate}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Optimal Window}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Avg Batch Size}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{p99 Latency}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{100 QPS} \textbf{500 QPS} \textbf{1,000 QPS} \textbf{5,000 QPS}
& 20ms 8ms 5ms 2ms & 2.0 4.0 5.0 10.0 & 45ms 42ms 38ms 35ms \\
\end{longtable}

\subsubsection{Streaming Traffic (Correlated
Arrivals)}\label{sec-model-serving-systems-streaming-traffic-correlated-arrivals-44f6}

Autonomous vehicles, video analytics, and robotics systems receive
inputs from multiple synchronized sensors. Rather than independent
arrivals, frames from all cameras for a given timestamp must be
processed together as a batch.

\begin{tcolorbox}[enhanced jigsaw, breakable, colframe=quarto-callout-note-color-frame, toprule=.15mm, coltitle=black, bottomtitle=1mm, bottomrule=.15mm, opacitybacktitle=0.6, arc=.35mm, colbacktitle=quarto-callout-note-color!10!white, toptitle=1mm, left=2mm, titlerule=0mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Multi-Camera Autonomous Vehicle Serving}, rightrule=.15mm, leftrule=.75mm, opacityback=0, colback=white]

Consider a vehicle with 6 cameras capturing at 30 FPS, requiring spatial
fusion:

\textbf{Timeline for processing frame set N:}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 2\tabcolsep) * \real{0.1528}}
  >{\raggedright\arraybackslash}p{(\linewidth - 2\tabcolsep) * \real{0.5000}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Time}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Event}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
T = 0ms T = 8ms T = 10ms T = 15ms T = 15ms T = 25ms T = 32ms & Cameras
begin capturing frame N Camera 1 frame arrives Cameras 2-5 frames arrive
Camera 6 arrives (jitter) Batch inference begins (6 images) Inference
complete Result ready for planning module \\
\end{longtable}

\textbf{Key constraints:}

\begin{itemize}
\tightlist
\item
  Hard deadline: 33ms per frame set (real-time requirement)
\item
  Batch size: Fixed at 6 (one per camera)
\item
  Synchronization budget: 12ms of 33ms total (36\% for jitter tolerance)
\item
  Timeout policy: If camera frame not received by T+20ms, use previous
  frame
\end{itemize}

Unlike Poisson traffic where dynamic batching optimizes throughput,
streaming traffic requires synchronization policies that handle sensor
jitter while meeting hard deadlines.

\end{tcolorbox}

\subsubsection{Single-User Traffic (Sequential
Arrivals)}\label{sec-model-serving-systems-singleuser-traffic-sequential-arrivals-3c8a}

Mobile and embedded applications serve one user at a time, with requests
arriving only after the previous result is consumed. Batch size is
typically 1, eliminating batching optimization entirely but raising
different challenges centered on energy efficiency and thermal
management.

\begin{tcolorbox}[enhanced jigsaw, breakable, colframe=quarto-callout-note-color-frame, toprule=.15mm, coltitle=black, bottomtitle=1mm, bottomrule=.15mm, opacitybacktitle=0.6, arc=.35mm, colbacktitle=quarto-callout-note-color!10!white, toptitle=1mm, left=2mm, titlerule=0mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{ResNet-50: Mobile Single-User Serving (Pixel 6 NPU)}, rightrule=.15mm, leftrule=.75mm, opacityback=0, colback=white]

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3333}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2000}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1733}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2667}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Phase}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Duration}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Energy}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Notes}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Camera buffer read} \textbf{JPEG decode (CPU)} \textbf{Resize +
Normalize} \textbf{NPU inference} \textbf{Post-process + UI}
\textbf{Total} & 8ms 15ms 5ms 12ms 5ms \textbf{45ms} & 0.08mJ 1.5mJ
0.4mJ 0.8mJ 0.2mJ \textbf{3.0mJ} & System API Single-threaded CPU
preprocessing 82\% utilization Result rendering 22 FPS sustained \\
\end{longtable}

\textbf{Key metrics for ML node serving:}

\begin{itemize}
\tightlist
\item
  \textbf{Energy per inference}: 3.0mJ enables \textasciitilde9,000
  inferences per 10Wh battery (typical smartphone)
\item
  \textbf{Thermal budget}: At 3.0mJ/45ms = 67mW sustained, indefinite
  operation without throttling
\item
  \textbf{NPU vs CPU tradeoff}: CPU fallback uses 4.2mJ (1.4× energy) at
  85ms (1.9× latency)
\item
  \textbf{Memory footprint}: 150MB peak (model + activations), competing
  with app memory
\end{itemize}

\textbf{Critical insight}: Even at batch size 1, the mobile NPU achieves
82\% utilization because its compute capacity matches single-image
workloads. This differs from datacenter GPUs, which achieve only 15\%
utilization at batch size 1 because their massive parallelism requires
larger batches to saturate.

\end{tcolorbox}

\subsubsection{Mobile Serving
Constraints}\label{sec-model-serving-systems-mobile-serving-constraints-eb68}

Unlike cloud serving where cost dominates, mobile serving faces three
related constraints that shape optimization strategy:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Energy Budget}: Each inference depletes battery. A photo app
  running continuous inference at 22 FPS drains 240mW---acceptable for
  active use but problematic for background processing. The optimization
  target shifts from throughput to energy-per-inference.
\item
  \textbf{Thermal Throttling}: Sustained high-power operation triggers
  thermal management. When the SoC reaches thermal limits (typically
  45°C junction), the OS reduces NPU frequency by 30-50\%, degrading
  both latency and throughput. Bursty workloads that allow cooling
  between bursts outperform sustained maximum throughput.
\item
  \textbf{Memory Pressure}: Mobile apps compete for limited shared
  memory. A 150MB model footprint is acceptable for a dedicated camera
  app but problematic when the model must coexist with other app
  components. Memory-mapped model loading
  (Section~\ref{sec-model-serving-systems-loading-strategies-eb38})
  helps by loading pages on demand rather than requiring the full model
  in memory.
\end{enumerate}

These constraints make mobile serving optimization fundamentally
different from cloud optimization. The goal is not maximum throughput
but \textbf{sustainable performance}---maintaining acceptable latency
without thermal throttling or excessive battery drain.

Table~\ref{tbl-traffic-patterns-summary} maps the four MLPerf scenarios
to their deployment contexts and optimal batching strategies, providing
a decision framework for serving system design.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2111}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2444}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2111}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3111}}@{}}
\caption{\textbf{Traffic Patterns and Batching Strategies}: The four
MLPerf inference scenarios map to distinct deployment contexts. Server
traffic (cloud APIs) uses dynamic batching with timeout; MultiStream
(autonomous driving) uses synchronized sensor fusion; SingleStream
(mobile) processes requests individually; Offline (batch processing)
maximizes batch size for
throughput.}\label{tbl-traffic-patterns-summary}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{MLPerf} \textbf{Scenario}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Deployment} \textbf{Context}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Batch} \textbf{Strategy}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Optimization} \textbf{Focus}
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{MLPerf} \textbf{Scenario}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Deployment} \textbf{Context}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Batch} \textbf{Strategy}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Optimization} \textbf{Focus}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Server} & Cloud APIs, web services & Dynamic batching with
timeout & Window tuning, utilization-latency curve \\
\textbf{MultiStream} & Autonomous driving, video analytics &
Synchronized sensor fusion & Jitter handling, deadline guarantees \\
\textbf{SingleStream} & Mobile apps, embedded devices & No batching
(batch=1) & Preprocessing, power efficiency \\
\textbf{Offline} & Batch processing, data pipelines & Maximum batch size
& Throughput, hardware utilization \\
\end{longtable}

\section{LLM Serving}\label{sec-model-serving-systems-llm-serving-b8bf}

The traffic patterns and batching strategies examined in the previous
section share a common assumption: models produce a single output per
request, whether a classification label, a bounding box, or an embedding
vector. Large language models break this assumption, generating tokens
incrementally over hundreds or thousands of iterations and creating a
different latency profile. The p50, p95, and p99 metrics that govern
classification serving apply differently when a single request takes 2
to 3 seconds to complete but must feel responsive throughout. While the
foundational principles of queuing theory, batching tradeoffs, and
latency budgets apply universally, LLMs require additional metrics,
different optimization strategies, and unique memory management
techniques.

\subsection{Performance Metrics: TTFT and
TPOT}\label{sec-model-serving-systems-performance-metrics-ttft-tpot-b009}

Generative models produce a stream of tokens rather than a single output
tensor. This streaming nature requires a two-part metric system that
reflects the internal state transition from ``prefill'' (processing
input) to ``decode'' (generating output).

\phantomsection\label{callout-definitionux2a-1.9}
\begin{fbx}{callout-definition}{Definition: }{LLM Performance Metrics}
\phantomsection\label{callout-definition*-1.9}
\textbf{Time to First Token (TTFT)} measures the latency from the moment
a request is submitted until the first output token appears. This metric
is governed by the compute-bound prefill phase and determines the
responsiveness of an interactive application.

\textbf{Time Per Output Token (TPOT)} measures the time to generate each
subsequent token. This metric is governed by the memory-bandwidth-bound
decode phase and determines the perceived fluidity of the generation.

\end{fbx}

These two metrics---TTFT and TPOT---capture fundamentally different user
experience aspects. A fast TTFT provides immediate responsiveness (the
system starts answering quickly), while a fast TPOT provides fluid
generation (the answer streams smoothly). Production systems must
optimize both, typically with different techniques since they are
governed by different hardware constraints.

\phantomsection\label{callout-lighthouseux2a-1.10}
\begin{fbx}{callout-lighthouse}{Lighthouse Example: }{Lighthouse Example: LLM Serving Latency Targets}
\phantomsection\label{callout-lighthouse*-1.10}

A production-grade LLM service typically targets the following SLOs:

\begin{itemize}
\tightlist
\item
  \textbf{TTFT}: \textless{} 500 ms (for a 1000-token prompt)
\item
  \textbf{TPOT}: \textless{} 50 ms (equivalent to \textasciitilde20
  tokens/second, faster than human reading speed)
\item
  \textbf{Throughput}: \textgreater{} 1000 tokens/second aggregate
  across all users
\end{itemize}

\end{fbx}

\subsection{Decoding
Strategies}\label{sec-model-serving-systems-decoding-strategies-afe8}

Generative models require decoding strategies that trade off quality,
diversity, and latency. The choice of decoding strategy can dramatically
affect both output quality and computational cost.

\textbf{Greedy decoding} selects the highest-probability token at each
step. Fast but often produces repetitive, low-quality outputs because it
cannot recover from early mistakes.

\textbf{Beam search} maintains multiple candidate sequences, selecting
the highest-scoring complete sequence. Produces higher-quality outputs
but multiplies computation by the beam width.

\textbf{Sampling} with temperature, top-k, and top-p parameters
introduces randomness for diversity
(\citeproc{ref-holtzman2020curious}{Holtzman et al. 2020}). Temperature
scales logits before softmax. Top-k limits sampling to the k
highest-probability tokens. Top-p, also called nucleus sampling, limits
sampling to tokens comprising probability mass p.

The choice presents latency tradeoffs
(\citeproc{ref-meister2020beam}{Meister, Cotterell, and Vieira 2020}).
Beam search with width 5 takes roughly 5× the compute of greedy
decoding. Sampling adds minimal overhead but requires careful parameter
tuning to balance quality and coherence.

\subsection{Streaming
Responses}\label{sec-model-serving-systems-streaming-responses-469b}

Rather than waiting for complete generation, production LLM systems
return tokens as they are produced. This improves perceived latency
since users see output beginning quickly, but requires infrastructure
support for chunked HTTP responses and client-side incremental
rendering. Streaming changes the latency profile: TTFT determines when
output starts appearing, while TPOT determines the perceived generation
speed.

\subsection{Memory and KV
Cache}\label{sec-model-serving-systems-memory-kv-cache-d1ea}

Generative inference requires managing the \textbf{KV Cache}, a stateful
memory structure that grows with sequence length. Unlike traditional
models where memory usage is constant per batch, LLM memory usage is
dynamic:

\begin{itemize}
\tightlist
\item
  \textbf{State Accumulation}: Each generated token adds to the context
  window, consuming additional GPU memory.
\item
  \textbf{Fragmentation}: Variable-length sequences can lead to memory
  fragmentation if not managed explicitly.
\end{itemize}

The continuous batching and PagedAttention techniques covered in
Section~\ref{sec-model-serving-systems-continuous-batching-8bb6} address
these challenges. Advanced techniques including prefix caching and
speculative decoding are covered in specialized coverage of large-scale
systems.

\section{Inference Runtime
Selection}\label{sec-model-serving-systems-inference-runtime-selection-5eef}

The batching strategies and LLM-specific techniques examined in
preceding sections determine \emph{how} requests are grouped and
processed. But these strategies assume an underlying execution engine
that actually runs the model computations. The execution environment
directly affects whether the latency budgets established earlier are
achievable. The inference runtime, the software layer that orchestrates
tensor operations and manages hardware resources, can vary by an order
of magnitude in performance for identical models. Choosing appropriately
requires understanding the tradeoffs between framework-native serving,
general-purpose optimization, and specialized inference engines.

\subsection{Framework-Native
Serving}\label{sec-model-serving-systems-frameworknative-serving-da62}

PyTorch and TensorFlow models can serve directly using their native
runtimes. This approach maximizes compatibility (any model that trains
will serve) and simplifies the deployment pipeline (no export or
conversion step). However, framework runtimes include training
functionality that adds overhead, and default execution paths may not
exploit hardware-specific optimizations.

TorchScript and TensorFlow SavedModel formats enable ahead-of-time
compilation and graph optimization, improving over eager execution while
maintaining framework compatibility. These formats represent the first
step toward deployment optimization without abandoning the familiar
framework ecosystem.

\subsection{General-Purpose
Optimization}\label{sec-model-serving-systems-generalpurpose-optimization-9f11}

ONNX Runtime provides a hardware-agnostic optimization layer
(\citeproc{ref-onnxruntime2024}{Microsoft 2024}). Models export to ONNX
format, then ONNX Runtime applies graph optimizations and selects
execution providers for the target hardware. This enables single-format
deployment across CPUs, GPUs, and specialized accelerators.

\subsection{Specialized Inference
Engines}\label{sec-model-serving-systems-specialized-inference-engines-1924}

TensorRT\sidenote{\textbf{TensorRT}: NVIDIA's inference optimization SDK
that applies layer fusion, kernel auto-tuning, and precision calibration
to neural networks. Unlike framework-native runtimes that preserve
training-time graph structure, TensorRT rebuilds the computation graph
for the specific target GPU during a build phase. This GPU-specific
compilation means TensorRT engines are not portable across GPU
architectures, requiring separate builds for V100, A100, and H100
deployments. The build phase can take minutes but produces engines that
often achieve 2-5x speedup over PyTorch. } (NVIDIA GPUs), OpenVINO
(Intel hardware), and similar engines optimize specifically for their
target hardware (\citeproc{ref-nvidia2024tensorrt}{Khan, Yoon, and
Bhandarkar 2024}; \citeproc{ref-chen2018tvm}{0001 et al. 2018}). They
apply aggressive optimizations that framework-native runtimes cannot
safely perform:

\textbf{Layer fusion} combines multiple sequential operations into a
single GPU kernel. Consider a common pattern: convolution → batch
normalization → ReLU activation. Without fusion, this requires three
kernel launches, three round-trips to GPU memory (write conv output,
read for batchnorm, write batchnorm output, read for ReLU), and three
sets of intermediate tensors. Fusion combines all three into one kernel
that reads inputs once, computes the combined result in registers, and
writes final outputs once. This eliminates kernel launch overhead
(15-60μs saved per fusion) and reduces memory traffic by 2-3×. TensorRT
automatically detects and fuses common patterns; a typical ResNet-50
reduces from \textasciitilde50 kernels to \textasciitilde15 after
fusion.

\textbf{Kernel auto-tuning} selects the fastest algorithm for each
operation on the specific GPU. A single convolution can be implemented
using dozens of algorithms (direct, FFT-based, Winograd, various tiling
strategies), each optimal for different input sizes and GPU
architectures. Auto-tuning benchmarks each candidate and caches the
winner, trading compilation time for runtime performance.

These optimizations typically achieve 2-5× speedup over framework-native
serving but require explicit export and may not support all operations.

\begin{tcolorbox}[enhanced jigsaw, breakable, colframe=quarto-callout-note-color-frame, toprule=.15mm, coltitle=black, bottomtitle=1mm, bottomrule=.15mm, opacitybacktitle=0.6, arc=.35mm, colbacktitle=quarto-callout-note-color!10!white, toptitle=1mm, left=2mm, titlerule=0mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{ResNet-50: Runtime Comparison}, rightrule=.15mm, leftrule=.75mm, opacityback=0, colback=white]

Performance comparison for ResNet-50 inference on V100 GPU (batch size
1):

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2368}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1842}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1842}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3684}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Runtime}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Latency}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Speedup}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Notes}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
PyTorch (eager) TorchScript ONNX Runtime TensorRT FP32 TensorRT FP16
TensorRT INT8 & 8.5ms 6.2ms 5.1ms 2.8ms 1.4ms 0.9ms & 1.0× 1.4× 1.7×
3.0× 6.1× 9.4× & Baseline, no optimization JIT compilation
Cross-platform NVIDIA-specific Tensor Core acceleration Requires
calibration \\
\end{longtable}

\textbf{Key insight}: The 9.4× speedup from TensorRT INT8 comes at the
cost of: (1) quantization calibration data, (2) potential accuracy loss
(\textless1\% for ResNet-50), and (3) NVIDIA-specific deployment.

\end{tcolorbox}

The optimization-compatibility tradeoff is inherent. More aggressive
optimization yields better performance but increases deployment
complexity and may introduce numerical differences from training. The
choice depends on latency requirements, deployment constraints, and
available engineering resources.

\subsection{Runtime
Configuration}\label{sec-model-serving-systems-runtime-configuration-992f}

Beyond runtime selection, configuration choices impact serving
performance including thread pools that control parallelism for CPU
inference, memory allocation strategies that choose between
pre-allocating buffers versus dynamic allocation, execution providers
that select and prioritize hardware backends, and graph optimization
level that trades compilation time for runtime performance. Production
deployments require experimentation to find optimal configurations for
specific models and hardware combinations. A systematic approach tests
key parameters and measures their impact on latency distributions.

\subsection{Precision Selection for
Serving}\label{sec-model-serving-systems-precision-selection-serving-55ba}

Numerical precision directly trades accuracy for throughput, connecting
to the quantization techniques covered in
\textbf{?@sec-model-compression}. While \textbf{?@sec-model-compression}
focuses on training-time quantization, serving introduces additional
considerations including calibration requirements, layer sensitivity,
and dynamic precision selection.

\subsubsection{Precision-Throughput
Relationship}\label{sec-model-serving-systems-precisionthroughput-relationship-7a7a}

For memory-bandwidth-bound operations, reducing precision proportionally
increases throughput by reducing data movement.
Equation~\ref{eq-precision-throughput} quantifies the theoretical
maximum speedup from precision reduction:

\begin{equation}\phantomsection\label{eq-precision-throughput}{\frac{\text{Throughput}_{\text{INT8}}}{\text{Throughput}_{\text{FP32}}} = \frac{32}{8} = 4\times \text{ (theoretical maximum)}}\end{equation}

In practice, GPU compute pipelines and Tensor Core alignment
requirements limit achieved speedup to 2.5-3.5x for INT8 versus FP32.
Tensor Cores require specific alignment: INT8 operations need tensor
dimensions divisible by 16, while FP16 requires divisibility by 8.
\textbf{?@sec-ai-acceleration} provides the detailed Tensor Core
architecture that explains these alignment constraints.

\begin{tcolorbox}[enhanced jigsaw, breakable, colframe=quarto-callout-note-color-frame, toprule=.15mm, coltitle=black, bottomtitle=1mm, bottomrule=.15mm, opacitybacktitle=0.6, arc=.35mm, colbacktitle=quarto-callout-note-color!10!white, toptitle=1mm, left=2mm, titlerule=0mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{ResNet-50: Precision Tradeoffs on V100}, rightrule=.15mm, leftrule=.75mm, opacityback=0, colback=white]

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 10\tabcolsep) * \real{0.1619}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 10\tabcolsep) * \real{0.1333}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 10\tabcolsep) * \real{0.1238}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 10\tabcolsep) * \real{0.1429}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 10\tabcolsep) * \real{0.2286}}
  >{\raggedright\arraybackslash}p{(\linewidth - 10\tabcolsep) * \real{0.1714}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Precision}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Latency}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Memory}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Accuracy}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Tensor Core Util.}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Calibration}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{FP32} \textbf{FP16} \textbf{INT8 (PTQ)} \textbf{INT8 (QAT)} &
2.8ms 1.4ms 0.9ms 0.9ms & 98MB 49MB 25MB 25MB & 76.13\% 76.13\% 75.80\%
76.05\% & 0\% 85\% 92\% 92\% & None None 1,000 samples Full
retraining \\
\end{longtable}

\textbf{Key observations:}

\begin{itemize}
\tightlist
\item
  INT8 achieves 3.1× speedup but loses 0.33\% accuracy with
  post-training quantization (PTQ)
\item
  Quantization-aware training (QAT) recovers most accuracy but requires
  retraining
\item
  FP16 provides 2× speedup with no accuracy loss for most models
\end{itemize}

\end{tcolorbox}

\subsubsection{Layer
Sensitivity}\label{sec-model-serving-systems-layer-sensitivity-520b}

Not all layers tolerate reduced precision equally.
Equation~\ref{eq-quant-error} captures how quantization error for a
layer scales with weight magnitude and gradient sensitivity:

\begin{equation}\phantomsection\label{eq-quant-error}{\epsilon_{\text{quant}} \propto \alpha \cdot \|W\|_2 \cdot 2^{-b}}\end{equation}

where \(\alpha\) is a layer-specific sensitivity coefficient,
\(\|W\|_2\) is the weight L2 norm, and \(b\) is the bit width. This
explains observed patterns where first convolutional layers with high
gradients and large sensitivity coefficients are precision-sensitive and
often kept at FP16, middle layers with stable gradients and low
sensitivity coefficients tolerate INT8 well, and final classification
layers with small weights but high task sensitivity benefit from FP16 or
higher precision.

\subsubsection{Calibration
Requirements}\label{sec-model-serving-systems-calibration-requirements-caba}

Post-training quantization requires a calibration dataset to determine
optimal scale factors for INT8 conversion. Production experience shows
that calibration data must be representative of actual serving traffic,
not just training data. Using ImageNet validation images to calibrate a
model serving wildlife camera images resulted in 3.2\% accuracy
degradation in one production system.

\subsubsection{Dynamic Precision
Selection}\label{sec-model-serving-systems-dynamic-precision-selection-c724}

Advanced serving systems select precision per request based on runtime
conditions. If the system is ahead of latency SLO, it uses higher
precision for better accuracy. For low-confidence INT8 results, it
recomputes at FP16. Different customer tiers may receive different
precision levels. This pattern enables adaptive quality-latency
tradeoffs while maximizing throughput during normal operation.

The precision decision has direct infrastructure consequences: INT8
inference achieves roughly 3x higher throughput than FP32, meaning a
workload requiring 30 GPUs at FP32 needs only 10 at INT8. This 3x
reduction in hardware translates directly to a 3x reduction in operating
costs. The connection between model-level optimization and
infrastructure economics is why precision selection cannot be treated as
purely a model concern, and why the next section examines cost and
capacity planning in detail.

\section{Economics and Capacity
Planning}\label{sec-model-serving-systems-economics-capacity-planning-3e7e}

The runtime and precision choices from the previous section determine
per-inference performance. Production deployment requires translating
these technical choices into infrastructure decisions. Serving costs
scale with request volume, unlike training costs that scale with dataset
size and model complexity (\citeproc{ref-zhang2019mark}{Zhang et al.
2019}). Cost structure analysis enables decisions that balance
performance requirements against budget constraints.

\subsection{Cost Per
Inference}\label{sec-model-serving-systems-cost-per-inference-8949}

Total serving cost decomposes into several components including compute
time for GPU or CPU per inference, memory for accelerator memory
required to hold model and activations, data transfer for network
bandwidth for request and response payloads, and orchestration overhead
for container runtime, load balancing, and monitoring. For GPU
inference, compute time dominates when utilization is high. When
utilization is low, memory cost dominates because the GPU is reserved
but idle. Serving infrastructure should maximize GPU utilization through
batching, multi-model serving, or right-sized instance selection.

\begin{tcolorbox}[enhanced jigsaw, breakable, colframe=quarto-callout-note-color-frame, toprule=.15mm, coltitle=black, bottomtitle=1mm, bottomrule=.15mm, opacitybacktitle=0.6, arc=.35mm, colbacktitle=quarto-callout-note-color!10!white, toptitle=1mm, left=2mm, titlerule=0mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{ResNet-50: Cost Analysis}, rightrule=.15mm, leftrule=.75mm, opacityback=0, colback=white]

Consider serving ResNet-50 on AWS infrastructure (US-East region,
on-demand pricing as of 2024):

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3182}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1818}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1932}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2841}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Instance Type}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Cost/Hour}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Throughput}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
\textbf{Cost per 1M Images}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{c5.xlarge (CPU)} \textbf{g4dn.xlarge (T4 GPU)}
\textbf{p3.2xlarge (V100 GPU)} & \$0.17 \$0.53 \$3.06 & 50 img/s 400
img/s 1,200 img/s & \$0.94 \$0.37 \$0.71 \\
\end{longtable}

\textbf{Key insight}: The T4 GPU instance achieves the lowest cost per
inference despite higher hourly cost, because GPU throughput
dramatically exceeds CPU throughput. The V100 is only cost-effective at
very high sustained traffic where its higher throughput justifies the 6x
price increase. Note that cloud pricing varies by region and changes
over time; consult current pricing for production planning.

\end{tcolorbox}

\subsection{GPU vs CPU
Economics}\label{sec-model-serving-systems-gpu-vs-cpu-economics-eb06}

GPUs provide significant speedup for parallel operations but cost more
per hour (\citeproc{ref-wu2019machine}{Wu et al. 2019}). The crossover
point depends on model characteristics and latency requirements.

CPU inference makes economic sense when models are small with few
parameters and simple operations, latency requirements are relaxed with
hundreds of milliseconds acceptable, request volume is low or highly
variable, and models use operations that do not parallelize well. GPU
inference makes economic sense when models are large with
parallel-friendly operations, latency requirements are strict at tens of
milliseconds, request volume is high and consistent, and batching can
achieve high utilization.

\subsubsection{Scaling
Responsiveness}\label{sec-model-serving-systems-scaling-responsiveness-30c0}

Beyond steady-state costs, startup time affects scaling economics. CPU
instances typically start in 30 to 60 seconds while GPU instances take 2
to 5 minutes including driver initialization, model loading, and warmup.
For variable traffic patterns, this startup latency can be more
important than cost per inference. If traffic spikes arrive faster than
GPU instances can scale, latency SLOs will be violated despite having
sufficient eventual capacity.

This asymmetry suggests different scaling strategies where CPU instances
enable reactive scaling by responding to current demand while GPU
instances often require predictive scaling by provisioning based on
anticipated demand. For bursty workloads, a hybrid approach uses
always-on GPU capacity for baseline load plus CPU overflow capacity for
spikes, trading higher per-inference cost during spikes for better
responsiveness.

\subsection{Capacity
Planning}\label{sec-model-serving-systems-capacity-planning-96a3}

The GPU versus CPU decision establishes the cost per inference, but
determining how much infrastructure to provision requires combining cost
analysis with the queuing theory foundations from
Section~\ref{sec-model-serving-systems-queuing-theory-tail-latency-29a6}.
Capacity planning translates latency requirements and traffic
projections into infrastructure specifications. Key inputs include
traffic patterns such as peak request rate, daily and weekly cycles, and
growth projections, latency SLOs including p50, p95, and p99 targets,
and model characteristics such as inference time distribution at various
batch sizes. From these inputs, queuing theory determines required
capacity (\citeproc{ref-harchol2013performance}{Harchol-Balter 2013}).
The equations developed in
Section~\ref{sec-model-serving-systems-queuing-theory-tail-latency-29a6}
provide the mathematical foundation where Equation~\ref{eq-mm1-wait}
shows how latency scales with utilization, while
Equation~\ref{eq-p99-latency} enables calculating p99 latency for
capacity planning.

The relationship between utilization and latency is nonlinear as shown
in the utilization-latency table. At 70 percent utilization, p99 latency
is approximately fifteen times service time. At 90 percent utilization,
it reaches approximately 46 times service time. This nonlinearity
explains why systems that seem healthy with low average latency can
suddenly violate SLOs when traffic increases modestly.

The worked example in
Section~\ref{sec-model-serving-systems-queuing-theory-tail-latency-29a6}
demonstrates the complete capacity planning process by starting from a
50ms p99 SLO and 5,000 QPS target, deriving the safe utilization
threshold of 72 percent, calculating required service rate of 6,944 QPS,
and determining GPU count with headroom of 10 V100s. Production systems
typically provision for peak load plus 30 percent headroom, using
auto-scaling to reduce costs during low-traffic periods while meeting
latency objectives during peaks.

\subsection{Production Case Study: Serving
Llama-3-8B}\label{sec-model-serving-systems-production-case-study-serving-llama38b-0499}

To apply the principles of latency budgeting, memory management, and
hardware efficiency, we analyze the production profile of a modern Large
Language Model (LLM) serving workload. This case study demonstrates how
physical constraints---memory bandwidth and PCIe capacity---translate
directly into service-level metrics and unit economics.

\subsubsection{Workload
Profile}\label{sec-model-serving-systems-workload-profile-a380}

\begin{itemize}
\tightlist
\item
  \textbf{Model}: Llama-3-8B (quantized to 4-bit AWQ).
\item
  \textbf{Hardware}: 1\(\times\) NVIDIA H100 SXM5 GPU (80 GB HBM3, 3.35
  TB/s bandwidth).
\item
  \textbf{Request Characteristics}: 1,000-token input prompt (Prefill),
  256-token generated response (Decode).
\item
  \textbf{Target SLOs}: TTFT \(<\) 200 ms, TPOT \(<\) 20 ms.
\end{itemize}

\subsubsection{Latency
Deconstruction}\label{sec-model-serving-systems-latency-deconstruction-217e}

The end-to-end request latency is governed by the two-phase execution
model of autoregressive transformers.

\textbf{1. Prefill Phase (Time to First Token)}

The model processes the 1,000-token prompt in parallel. On an H100, this
compute-bound operation achieves approximately 10,000 tokens per second.
*
\(T_{\text{prefill}} = \frac{1000 \text{ tokens}}{10000 \text{ tokens/s}} = 100 \text{ ms}\).
* Accounting for 20 ms of system overhead (network ingress,
tokenization), the \textbf{TTFT is 120 ms}, comfortably within the 200
ms SLO.

\textbf{2. Decode Phase (Time Per Output Token)}

The model generates 256 tokens sequentially. This phase is
memory-bandwidth bound; the system must read the entire 3.5 GB weight
tensor from VRAM to generate a single token.

\phantomsection\label{callout-perspectiveux2a-1.11}
\begin{fbx}{callout-perspective}{Systems Perspective: }{The Physics of Token Generation}
\phantomsection\label{callout-perspective*-1.11}
\textbf{The Memory Wall for Generative AI}: Because the decode phase has
an arithmetic intensity of \(\approx 1\) FLOP/byte (we read every weight
to generate just one token), performance is strictly limited by memory
bandwidth (\(BW\)), not compute.

\[ T_{\text{token}} \approx \frac{\text{Model Size (Bytes)}}{\text{Memory Bandwidth (Bytes/s)}} \]

\textbf{The Engineering Implication}: For Llama-3-8B (3.5 GB int4), an
A100 80GB (2 TB/s HBM2e) generates tokens at \(\approx 1.7\) ms/token.
An H100 SXM5 (3.35 TB/s) improves this to \(\approx 1\) ms/token. Adding
more \emph{compute cores} yields \textbf{zero} latency improvement; only
faster memory (or smaller models) can speed up generation.

\end{fbx}

\begin{itemize}
\tightlist
\item
  \(T_{\text{token}} \approx \frac{3.5 \text{ GB}}{3.35 \text{ TB/s}} \approx 1 \text{ ms}\)
  (theoretical limit).
\item
  Accounting for kernel launch overhead and attention computation,
  realized \(T_{\text{token}}\) is approximately 10 ms.
\item
  Total decode time =
  \(256 \text{ tokens} \times 10 \text{ ms/token} = 2.56 \text{ seconds}\).
\item
  \textbf{TPOT is 10 ms}, well within the 20 ms ``fluidity'' SLO.
\end{itemize}

\subsubsection{Memory \&
Throughput}\label{sec-model-serving-systems-memory-throughput-63dd}

With 4-bit weights occupying 3.5 GB, the remaining \textasciitilde76 GB
of VRAM is available for the \textbf{KV Cache}. Using
\textbf{PagedAttention}, we can allocate this memory with near-zero
fragmentation.

\begin{itemize}
\tightlist
\item
  Each token requires approximately 0.5 MB of KV cache (32 layers
  \(\times\) 4096 dim \(\times\) 2 vectors \(\times\) 2-byte precision).
\item
  Total cache capacity
  \(\approx \frac{72 \text{ GB}}{0.5 \text{ MB/token}} \approx 144,000 \text{ tokens}\).
\item
  At 1,256 tokens per request (input + output), the GPU can handle a
  \textbf{concurrent batch size of \textasciitilde114 requests}.
\end{itemize}

\subsubsection{Unit
Economics}\label{sec-model-serving-systems-unit-economics-b685}

For an H100 SXM5 instance at approximately \$3.00 per hour (specialized
cloud providers; hyperscaler rates vary from \$2-13 per hour as of
2024): * Total tokens per hour =
\(114 \text{ batch} \times \frac{3600 \text{ s/hr}}{2.68 \text{ s/req}} \times 1256 \text{ tokens/req} \approx 190 \text{ million tokens/hour}\).
* \textbf{Cost per million tokens}:
\(\frac{\$3.00}{190} \approx \mathbf{\$0.015}\).

This analysis highlights that for LLMs, \textbf{memory capacity} (the
size of the KV cache) is the primary determinant of throughput and cost,
while \textbf{memory bandwidth} is the primary determinant of latency.

This case study applies the core principles developed throughout this
chapter: latency budgets decompose into prefill and decode phases,
queuing theory governs batch sizing and capacity planning, and hardware
constraints in the form of memory bandwidth and capacity determine
achievable performance and cost. The quantitative framework established
here enables principled engineering decisions, but only when applied
correctly. Common misconceptions cause even experienced engineers to
misapply these principles in practice.

\section{Fallacies and
Pitfalls}\label{sec-model-serving-systems-fallacies-pitfalls-336b}

Serving inverts training priorities in unexpected ways. Intuitions from
batch processing fail under latency constraints and variable load,
causing wasted effort, violated SLOs, and silent accuracy degradation in
production.

\textbf{Fallacy:} \emph{Faster model inference automatically means
faster end-to-end serving.}

Engineers assume model inference dominates serving latency. In
production, preprocessing and postprocessing often consume 45 to 70
percent of total request time when inference runs on optimized
accelerators, as
Section~\ref{sec-model-serving-systems-latency-budget-ef40}
demonstrates. A team reducing inference from 5ms to 2ms achieves only 23
percent end-to-end improvement if preprocessing remains at 8ms. Amdahl's
Law formalizes this: if preprocessing consumes 50 percent of latency,
even infinitely fast inference yields at most 2× speedup. Teams that
optimize inference without profiling the complete request path waste
engineering effort while the actual bottleneck remains unaddressed.

\textbf{Pitfall:} \emph{Running serving infrastructure at high
utilization to maximize cost efficiency.}

Teams target 90 percent utilization to minimize idle capacity. In
production, latency degrades nonlinearly as utilization approaches
capacity. Equation~\ref{eq-mm1-wait} shows that at 90 percent
utilization, average wait time reaches 10× service time. Moving from 70
percent to 90 percent utilization cuts infrastructure costs by 22
percent but triples average latency. For a 5ms inference service, p99
latency jumps from 25ms to 50ms. Systems provisioned for average load
violate SLOs precisely when traffic increases during business-critical
periods. Production systems targeting 60 to 70 percent utilization at
peak load maintain the latency headroom needed to absorb traffic spikes.

\textbf{Fallacy:} \emph{Training accuracy guarantees serving accuracy.}

Engineers assume identical model weights preserve validation set
performance. In production, preprocessing differences silently shift
inputs outside the training distribution.
Section~\ref{sec-model-serving-systems-trainingserving-skew-7b99} shows
how training-serving skew causes accuracy degradation despite identical
weights: PIL versus OpenCV resize interpolation differs subtly, float64
versus float32 normalization produces different values, or feature
computation timing changes. A model achieving 95 percent validation
accuracy drops to 90 percent in production from these preprocessing
mismatches. Standard monitoring checking exceptions and latency
violations fails to detect this silent degradation. Production systems
require either identical preprocessing code for training and serving, or
statistical monitoring comparing input distributions to catch drift
before accuracy degrades.

\textbf{Pitfall:} \emph{Using average latency to evaluate serving system
performance.}

Engineers monitor average latency because it trends smoothly and is
simple to compute. In production, averages hide the slowest requests
that determine user satisfaction. A system with 10ms average latency
might have 200ms p99 latency, meaning 1 percent of users experience 20×
worse performance.
Section~\ref{sec-model-serving-systems-tail-latency-5376} explains how
queuing variability causes this divergence: at 70 percent utilization
with 5ms service time, average latency is 17ms but p99 reaches 75ms.
Production SLOs specify percentile targets (p95, p99) precisely because
averages mask tail behavior. Systems reporting only averages pass
monitoring checks while violating user experience standards.

\textbf{Fallacy:} \emph{Larger batch sizes always improve throughput.}

Engineers maximize batch size assuming GPU saturation improves
efficiency. In practice, throughput gains diminish while latency grows
unbounded beyond the optimal point. For ResNet-50 on V100, increasing
batch size from 16 to 32 improves throughput only 12 percent while
nearly doubling inference time from 14ms to 25ms. At batch size 64,
memory fragmentation reduces effective utilization to 60 percent despite
the GPU appearing busy, and variable input sizes create padding
overhead.
Section~\ref{sec-model-serving-systems-dynamic-batching-latencythroughput-tradeoffs-986d}
shows optimal batch size depends on latency SLOs, memory constraints,
and traffic patterns: for 50ms p99 targets, batch sizes above 32
routinely violate SLOs.

\textbf{Pitfall:} \emph{Calibrating quantized models with training data
rather than production traffic.}

Teams calibrate with training data because it is readily available and
produced validation accuracy. In production, traffic distribution often
differs from training data, making calibration scale factors suboptimal.
Post-training quantization determines INT8 scale factors by measuring
activation ranges on calibration data, but this assumes production
inputs match the calibration distribution. One production system
experienced 3.2 percent accuracy loss serving wildlife camera images
after calibrating with ImageNet validation data.
\textbf{?@sec-model-compression} shows quantization error scales with
activation range: miscalibration amplifies errors precisely on
out-of-distribution inputs. Effective quantization requires calibrating
with representative samples of actual serving traffic.

\textbf{Pitfall:} \emph{Cold start latency only matters for the first
request.}

Engineers optimize steady-state latency assuming most requests hit warm
instances. In production, cold starts affect any request arriving after
inactivity, after model updates, or during auto-scaling. Systems with
bursty traffic experience cold starts on 10 to 30 percent of requests
during scale-up events. ResNet-50 with TensorRT requires 30 seconds for
compilation if the optimized engine is not cached; during a traffic
spike triggering 10 new instances, 300 seconds of user-facing latency is
added across the first requests to each instance.
Section~\ref{sec-model-serving-systems-model-loading-initialization-cc5a}
shows cold start compounds weight loading, CUDA context initialization,
and warmup. Systems ignoring cold start meet SLOs during steady state
but violate them during scale-up events and deployment windows.

\section{Summary}\label{sec-model-serving-systems-summary-9635}

Serving marks the transition from model development to production
deployment, where the optimization priorities that governed training
must be inverted. The shift from throughput maximization to latency
minimization transforms every system design decision. The queuing theory
foundations established here reveal why this inversion is not merely a
change in metrics but a change in the governing mathematics: the
nonlinear relationship between utilization and latency means that
systems behaving well at moderate load can suddenly violate SLOs when
traffic increases modestly. Little's Law and the M/M/1 wait time
equations provide the quantitative foundation for capacity planning,
replacing intuition-based provisioning with engineering rigor.

Effective serving optimization requires understanding the complete
request path rather than focusing exclusively on model inference.
Interface protocols like gRPC and efficient serialization formats
minimize the ``tax'' of data movement, while preprocessing often
consumes 45 to 70 percent of total latency when inference runs on
optimized accelerators. The microsecond-scale overheads identified by
Barroso, Patterson, and colleagues explain why serving latency often
exceeds the sum of its measured parts, and why system-level optimization
matters as much as model optimization. Training-serving skew represents
another dimension of this complexity, silently degrading accuracy when
preprocessing logic differs between training and production environments
in ways that traditional testing cannot detect.

The traffic pattern analysis reveals how deployment context shapes
batching strategy and system design. Server workloads with Poisson
arrivals optimize dynamic batching windows, autonomous vehicles with
streaming sensor data require synchronized batch formation, and mobile
applications with single-user patterns eliminate batching entirely. The
MLPerf scenarios codify these patterns for standardized benchmarking,
connecting the serving principles established here to the measurement
frameworks explored in \textbf{?@sec-benchmarking-ai}. Precision
selection and runtime optimization extend the quantization techniques
from \textbf{?@sec-model-compression} and Tensor Core capabilities from
\textbf{?@sec-ai-acceleration} into the serving domain. Finally, the
translation of these technical metrics into unit economics, as shown by
the Llama-3 case study, demonstrates how engineering decisions regarding
batching, precision, and hardware selection directly determine the
financial viability of deployment.

\begin{tcolorbox}[enhanced jigsaw, breakable, colframe=quarto-callout-important-color-frame, toprule=.15mm, coltitle=black, bottomtitle=1mm, bottomrule=.15mm, opacitybacktitle=0.6, arc=.35mm, colbacktitle=quarto-callout-important-color!10!white, toptitle=1mm, left=2mm, titlerule=0mm, title=\textcolor{quarto-callout-important-color}{\faExclamation}\hspace{0.5em}{Key Takeaways}, rightrule=.15mm, leftrule=.75mm, opacityback=0, colback=white]

\begin{itemize}
\item
  \textbf{Serving inverts training priorities}: Training optimizes
  throughput (samples/hour); serving optimizes latency (ms/request).
  Different objectives require different system designs.
\item
  \textbf{Queuing theory governs capacity planning}: At 80\%
  utilization, wait time is 5× service time; at 90\%, it's 10×. Small
  load increases cause disproportionate latency spikes.
\item
  \textbf{Preprocessing dominates optimized systems}: When model
  inference is fast (5ms), preprocessing (image decode, tokenization)
  consumes 45--70\% of total latency. Optimize the pipeline, not just
  the model.
\item
  \textbf{Batching strategy depends on traffic pattern}: Poisson
  arrivals (web APIs) use dynamic batching; streaming sensors use
  synchronized batches; mobile apps eliminate batching entirely.
\item
  \textbf{Training-serving skew can degrade accuracy undetected}:
  Different preprocessing between training and serving (e.g., resize
  interpolation, normalization order) shifts inputs outside the training
  distribution, causing accuracy degradation that conventional
  monitoring cannot detect. Use identical code paths.
\item
  \textbf{KV cache optimization is critical for large language models}:
  Generation is memory-bound. PagedAttention and continuous batching can
  improve throughput 2--4× over naive serving.
\end{itemize}

\end{tcolorbox}

The serving foundations established here provide the infrastructure for
operational deployment. We have optimized the single node for
milliseconds. But a single node is fragile. What happens when the server
crashes? When the model drifts? When we need to update the code without
dropping a single request? In \textbf{Ops}
(\textbf{?@sec-machine-learning-operations-mlops}), we scale our
perspective from the single request to the system lifecycle, building
the automated machinery that keeps the factory running 24/7.

\phantomsection\label{refs}
\begin{CSLReferences}{1}{0}
\bibitem[\citeproctext]{ref-chen2018tvm}
0001, Tianqi Chen, Thierry Moreau, Ziheng Jiang, Lianmin Zheng, Eddie Q.
Yan, Haichen Shen, Meghan Cowan, et al. 2018. {``TVM: An Automated
End-to-End Optimizing Compiler for Deep Learning.''} In \emph{OSDI},
578--94.
\url{https://www.usenix.org/conference/osdi18/presentation/chen}.

\bibitem[\citeproctext]{ref-agrawal2024sarathi}
Agrawal, Arney, Nitin Kedia, Ashish Panwar, Jayashree Mohan, Nipun
Kwatra, Bhargav S. Gulavani, Alexey Tumanov, and Ramachandran Ramjee.
2025. {``Efficient LLM Inference via Chunked Prefills.''} \emph{ACM
SIGOPS Operating Systems Review} 59 (1): 9--16.
\url{https://doi.org/10.1145/3759441.3759444}.

\bibitem[\citeproctext]{ref-barroso2017attack}
Barroso, Luiz, Mike Marty, David Patterson, and Parthasarathy
Ranganathan. 2017. {``Attack of the Killer Microseconds.''}
\emph{Communications of the ACM} 60 (4): 48--54.
\url{https://doi.org/10.1145/3015146}.

\bibitem[\citeproctext]{ref-crankshaw2017clipper}
Crankshaw, Daniel, Xin Wang, Guilio Zhou, Michael J. Franklin, Joseph E.
Gonzalez, and Ion Stoica. 2017. {``Clipper: A Low-Latency Online
Prediction Serving System.''} In \emph{14th USENIX Symposium on
Networked Systems Design and Implementation (NSDI 17)}, 613--27. USENIX
Association.

\bibitem[\citeproctext]{ref-dean2012rapid}
Dean, Jeffrey. 2012. {``Achieving Rapid Response Times in Large Online
Services.''} Berkeley AMPLab Cloud Seminar.
\url{https://research.google/pubs/pub44875/}.

\bibitem[\citeproctext]{ref-dean2013tail}
Dean, Jeffrey, and Luiz André Barroso. 2013. {``The Tail at Scale.''}
\emph{Communications of the ACM} 56 (2): 74--80.
\url{https://doi.org/10.1145/2408776.2408794}.

\bibitem[\citeproctext]{ref-google2024staticdynamic}
Google. 2024. {``Static Vs. Dynamic Inference.''} Google Machine
Learning Crash
Course.\href{\%0A\%20\%20\%20\%20https://developers.google.com/machine-learning/crash-course/production-ml-systems/static-vs-dynamic-inference\%0A\%20\%20}{https://developers.google.com/machine-learning/crash-course/production-ml-systems/static-vs-dynamic-inference
}.

\bibitem[\citeproctext]{ref-gujarati2020serving}
Gujarati, Arpan, Reza Karber, Safraz Musaev, Weiyang Liu, Anurag
Narayanan, Shi Quan Zhong, Mahmut Kandemir, Vyas Sekar, and Alexander
Zadorozhny. 2020. {``Serving DNNs Like Clockwork: Performance
Predictability from the Bottom Up.''} In \emph{14th USENIX Symposium on
Operating Systems Design and Implementation (OSDI 20)}, 443--62. USENIX
Association.

\bibitem[\citeproctext]{ref-harchol2013performance}
Harchol-Balter, Mor. 2013. \emph{Performance Modeling and Design of
Computer Systems: Queueing Theory in Action}. Cambridge University
Press. \url{https://doi.org/10.1017/cbo9781139226424}.

\bibitem[\citeproctext]{ref-holtzman2020curious}
Holtzman, Ari, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi 0001.
2020. {``The Curious Case of Neural Text Degeneration.''} In
\emph{ICLR}. \url{https://openreview.net/forum?id=rygGQyrFvH}.

\bibitem[\citeproctext]{ref-nvidia2024tensorrt}
Khan, Z, SC Yoon, and SM Bhandarkar. 2024. {``Deep Learning Model
Compression and Hardware Acceleration for High-Performance Foreign
Material Detection on Poultry Meat Using NIR Hyperspectral Imaging.''}
\emph{Sensors (Basel, Switzerland)} 25 (3).
\url{https://doi.org/10.3390/s25030970}.

\bibitem[\citeproctext]{ref-kwon2023vllm}
Kwon, Woosuk, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody
Hao Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica. 2023. {``Efficient
Memory Management for Large Language Model Serving with
PagedAttention.''} In \emph{Proceedings of the 29th Symposium on
Operating Systems Principles}, 611--26. ACM; ACM.
\url{https://doi.org/10.1145/3600006.3613165}.

\bibitem[\citeproctext]{ref-little1961proof}
Little, John D. C. 1961. {``A Proof for the Queuing Formula:
{\(L = \lambda W\)}.''} \emph{Operations Research} 9 (3): 383--87.
\url{https://doi.org/10.1287/opre.9.3.383}.

\bibitem[\citeproctext]{ref-meister2020beam}
Meister, Clara, Ryan Cotterell, and Tim Vieira. 2020. {``If Beam Search
Is the Answer, What Was the Question?''} In \emph{Proceedings of the
2020 Conference on Empirical Methods in Natural Language Processing
(EMNLP)}, 2173--85. Association for Computational Linguistics.
\url{https://doi.org/10.18653/v1/2020.emnlp-main.170}.

\bibitem[\citeproctext]{ref-onnxruntime2024}
Microsoft. 2024. {``ONNX Runtime: Cross-Platform Inference and Training
Machine-Learning Accelerator.''} GitHub.
\url{https://github.com/microsoft/onnxruntime}.

\bibitem[\citeproctext]{ref-nvidia2024tritontutorial}
NVIDIA. 2025. {``Productionizing GPU Inference on EKS with KServe and
NVIDIA Triton.''} \emph{American International Journal of Computer
Science and Technology} 7 (6).
\url{https://doi.org/10.63282/3117-5481/aijcst-v7i6p104}.

\bibitem[\citeproctext]{ref-olston2017tensorflow}
Olston, Christopher, Noah Fiedel, Kiril Gorovoy, Jeremiah Harmsen, Li
Lao, Fangwei Li, Vinu Rajashekhar, Sukriti Ramesh, and Jordan Soyke.
2017. {``TensorFlow-Serving: Flexible, High-Performance ML Serving.''}
\emph{CoRR}. \url{http://arxiv.org/abs/1712.06139}.

\bibitem[\citeproctext]{ref-romero2021infaas}
Romero, Francisco, Qian Li 0027, Neeraja J. Yadwadkar, and Christos
Kozyrakis. 2021. {``INFaaS: Automated Model-Less Inference Serving.''}
In \emph{2021 USENIX Annual Technical Conference (USENIX ATC 21)},
397--411. USENIX Association.
\url{https://www.usenix.org/conference/atc21/presentation/romero}.

\bibitem[\citeproctext]{ref-nvidia2024triton}
Savard, Claire, Nicholas Manganelli, Burt Holzman, Lindsey Gray, Alexx
Perloff, Kevin Pedro, Kevin Stenson, and Keith Ulmer. 2024.
{``Optimizing High-Throughput Inference on Graph Neural Networks at
Shared Computing Facilities with the NVIDIA Triton Inference Server.''}
\emph{Comput. Softw. Big Sci.} 8 (1): 14.
\url{https://doi.org/10.1007/S41781-024-00123-2}.

\bibitem[\citeproctext]{ref-shen2019nexus}
Shen, Haichen, Lequn Chen, Yuchen Jin, Liangyu Zhao, Bingyu Kong,
Matthai Philipose, Arvind Krishnamurthy, and Ravi Sundaram. 2019.
{``Nexus: A GPU Cluster Engine for Accelerating DNN-Based Video
Analysis.''} In \emph{Proceedings of the 27th ACM Symposium on Operating
Systems Principles}, 322--37. ACM; ACM.
\url{https://doi.org/10.1145/3341301.3359658}.

\bibitem[\citeproctext]{ref-wu2019machine}
Wu, Carole-Jean, David Brooks, Kevin Chen, Douglas Chen, Sy Choudhury,
Marat Dukhan, Kim Hazelwood, et al. 2019. {``Machine Learning at
Facebook: Understanding Inference at the Edge.''} In \emph{2019 IEEE
International Symposium on High Performance Computer Architecture
(HPCA)}, 331--44. IEEE. \url{https://doi.org/10.1109/hpca.2019.00048}.

\bibitem[\citeproctext]{ref-yu2022orca}
Yu, Gyeong-In, Joo Seong Jeong, Geon-Woo Kim, Soojeong Kim, and
Byung-Gon Chun. 2022. {``Orca: A Distributed Serving System for
Transformer-Based Generative Models.''} In \emph{16th USENIX Symposium
on Operating Systems Design and Implementation (OSDI 22)}, 521--38.
USENIX Association.
\url{https://www.usenix.org/conference/osdi22/presentation/yu}.

\bibitem[\citeproctext]{ref-zhang2019mark}
Zhang, Chengliang, Minchen Yu, Wei Wang 0030, and Feng Yan 0001. 2019.
{``MArk: Exploiting Cloud Services for Cost-Effective, SLO-Aware Machine
Learning Inference Serving.''} In \emph{2019 USENIX Annual Technical
Conference (USENIX ATC 19)}, 1049--62. USENIX Association.
\url{https://www.usenix.org/conference/atc19/presentation/zhang-chengliang}.

\end{CSLReferences}


\backmatter


\end{document}
