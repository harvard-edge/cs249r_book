# Image Classification {.unnumbered}

![*DALL·E prompt - 1950s style cartoon illustration based on a real image by Marcelo Rovai*](./images/png/ini-DALLE.png)

## Overview

We are increasingly facing an artificial intelligence (AI) revolution, where, as [Gartner](https://www.researchgate.net/figure/Gartner-2023-Artificial-intelligence-emerging-technologies-impact-radar-T-Nguyen-2023_fig1_372048156) states, **Edge AI and Computer Vision** have a very high impact potential, and **it is for now**!

\clearpage

When we look into Machine Learning (ML) applied to vision, the first concept that greets us is **Image Classification**, a kind of ML's *Hello World* that is both simple and profound!

The Seeed Studio XIAOML Kit provides a comprehensive hardware solution centered around the[ XIAO ESP32-S3 Sense](https://www.seeedstudio.com/xiao-series-page), featuring an integrated **OV3660** camera and SD card support. Those features make the XIAO ESP32S3 Sense an excellent starting point for exploring TinyML vision AI.

In this Lab, we will explore Image Classification using the non-code tool **SenseCraft AI** and explore a more detailed development with **Edge Impulse Studio** and **Arduino IDE**. 

::: callout-tip

## Learning Objectives

- **Deploy Pre-trained Models** using SenseCraft AI Studio for immediate computer vision applications

- **Collect and Manage Image Datasets** for custom classification tasks with proper data organization

- **Train Custom Image Classification Models** using transfer learning with MobileNet V2 architecture

- **Optimize Models for Edge Deployment** through quantization and memory-efficient preprocessing

- **Implement Post-processing Pipelines,** including GPIO control and real-time inference integration

- **Compare Development Approaches** between no-code and advanced ML platforms for embedded applications
:::

## Image Classification

Image classification is a fundamental task in computer vision that involves categorizing entire images into one of several predefined classes. This process entails analyzing the visual content of an image and assigning it a label from a fixed set of categories based on the dominant object or scene it depicts.

Image classification is crucial in various applications, ranging from organizing and searching through large databases of images in digital libraries and social media platforms to enabling autonomous systems to comprehend their surroundings. Common architectures that have significantly advanced the field of image classification include Convolutional Neural Networks (CNNs), such as AlexNet, VGGNet, and ResNet. These models have demonstrated remarkable accuracy on challenging datasets, such as [ImageNet](https://www.image-net.org/index.php), by learning hierarchical representations of visual data.

As the cornerstone of many computer vision systems, image classification drives innovation, laying the groundwork for more complex tasks like object detection and image segmentation, and facilitating a deeper understanding of visual data across various industries. So, let's start exploring the [Person Classification](https://sensecraft.seeed.cc/ai/view-model/60768-person-classification?tab=public) model ("Person - No Person"), a ready-to-use computer vision application on the **[SenseCraft AI](https://sensecraft.seeed.cc/ai/device/local/32)**.

\noindent
![](./images/png/person-class.png){width=85% fig-align="center"}

### Image Classification on the SenseCraft AI Workspace

Start by connecting the XIAOML Kit (or just the XIAO ESP32S3 Sense, disconnected from the Expansion Board) to the computer via USB-C, and then open the [SenseCraft AI Workspace](https://sensecraft.seeed.cc/ai/device/local/32) to connect it. 

\noindent
![](./images/png/connection.png){width=85% fig-align="center"}

Once connected, select the option `[Select Model...]` and enter in the search window: "*Person Classification*". From the options available, select the one trained over the MobileNet V2 (passing the mouse over the models will open a pop-up window with its main characteristics). 

\noindent
![](./images/png/model_selection.png){width=85% fig-align="center"}

Click on the chosen model and confirm the deployment. A new firmware for the model should start uploading to our device.

> Note that the percentage of models downloaded and firmware uploaded will be displayed. If not, try disconnecting the device, then reconnect it and press the boot button. 

After the model is uploaded successfully, we can view the live feed from the XIAO camera and the classification result (`Person` or `Not a Person`) in the **Preview** area, along with the inference details displayed in the **Device Logger**. 

> Note that we can also select our **Inference Frame Interval**, from "Real-Time" (Default) to 10 seconds, and the **Mode** (UART, I2C, etc) as the data is shared by the device (the default is UART via USB). 

\noindent
![](./images/png/person-detect-inference.png){width=85% fig-align="center"}

At the Device Logger, we can see that the latency of the model is from 52 to 78 ms for pre-processing and around 532ms for inference, which will give us a total time of a little less than 600ms, or about **1.7 Frames per second (FPS)**. 

> To run the Mobilenet V2 0.35, the XIAO had a peak current of 160mA at 5.23V, resulting in a **power consumption of 830mW**.

### Post-Processing

An essential step in an Image Classification project pipeline is to define what we want to do with the inference result. So, imagine that we will use the XIAO to automatically turn on the room lights if a person is detected. 

\noindent
![](./images/png/pipeline.png){width=75% fig-align="center"}

With the SebseCraft AI, we can do it on the `Output -> GPIO` section. Click on the Add icon to trigger the action when event conditions are met. A pop-up window will open, where you can define the action to be taken. For example, if a person is detected with a confidence of more than 60% the internal `LED` should be ON. In a real scenario, a GPIO, for example, `D0`, `D1`, `D2`, `D11`, or `D12`, would be used to trigger a relay to turn on a light. 

\noindent
![](./images/png/action.png){width=65% fig-align="center"} 

Once confirmed, the created **Trigger Action** will be shown. Press `Send` to upload the command to the XIAO. 

\noindent
![](./images/png/trigger.png){width=85% fig-align="center"}

Now, pointing the XIAO at a person will make the internal LED go ON. 

\noindent
![](./images/png/person-trigger.png){width=65% fig-align="center"}

> We will explore more trigger actions and post-processing techniques further in this lab. 

## An Image Classification Project 

Let's create a simple Image Classification project using SenseCraft AI Studio. Below, we can see a typical machine learning pipeline that will be used in our project.

\noindent
![](./images/png/pipeline-sensecraft.png){width=85% fig-align="center"}

On SenseCraft AI Studio: Let's open the tab [Training](https://sensecraft.seeed.cc/ai/training):

\noindent
![](./images/png/img-class-project.png){width=85% fig-align="center"}

The default is to train a `Classification` model with a WebCam if it is available. Let's select the `XIAOESP32S3 Sense` instead. Pressing the green button `[Connect]` will cause a Pop-Up window to appear. Select the corresponding Port and press the blue button `[Connect]`.

\noindent
![](./images/png/train-img-class.png){width=85% fig-align="center"}

The image streamed from the Grove Vision AI V2 will be displayed.

### The Goal

The first step, as we can see in the ML pipeline, is to define a goal. Let's imagine that we have an industrial installation that should automatically sort wheels and boxes.

\noindent
![](./images/png/distr-line.png){width=75% fig-align="center"}

So, let's simulate it, classifying, for example, a toy `box` and a toy `wheel`. We should also include a 3rd class of images, `background`, where there are no objects in the scene.

\noindent
![](./images/png/classes_img_class.png){width=75% fig-align="center"}

### Data Collection 

Let's create the classes, following, for example, an alphabetical order:

- Class1: background
- Class 2: box
- Class 3: wheel

\noindent
![](./images/png/classes.png){width=75% fig-align="center"}

Select one of the classes and keep pressing the green button (`Hold to Record`) under the preview area. The collected images (and their counting) will appear on the Image Samples Screen. Carefully and slowly, move the camera to capture different angles of the object. To modify the position or interfere with the image, release the green button, rearrange the object, and then hold it again to resume the capture. 

\noindent
![](./images/png/collect-imaages.png){width=85% fig-align="center"}

After collecting the images, review them and delete any incorrect ones.

\noindent
![](./images/png/clean_dataset.png){width=75% fig-align="center"}

Collect around **50 images** from each class and go to Training Step.

> Note that it is possible to download the collected images to be used in another application, for example, with the Edge Impulse Studio.

### Training 

Confirm if the correct device is selected (`XIAO ESP32S3 Sense`) and press `[Start Training]`

\noindent
![](./images/png/train-setup.png){width=85% fig-align="center"}

### Test 

After training, the inference result can be previewed.

>  Note that the model is not running on the device. We are, in fact, only capturing the images with the device and performing a **live preview** using the training model, which is running in the Studio.

\noindent
![](./images/png/img-class-infer.png){width=85% fig-align="center"}

Now is the time to really deploy the model in the device.

### Deployment 

Select the trained model and  `XIAO ESP32S3 Sense` at the `Supported Devices` window. And press `[Deploy to device]`.

\noindent
![](./images/png/select-to-deploy.png){width=85% fig-align="center"}

The SeneCrafit AI will redirect us to the **Vision Workplace** tab. `Confirm` the deployment, select the Port, and `Connect` it.

\noindent
![](./images/png/upload-model.png){width=85% fig-align="center"}

The model will be flashed into the device. After an automatic reset, the model will start running on the device. On the Device Logger, we can see that the inference has a **latency of approximately 426 ms**, plus a **pre-processing of around 110ms**, corresponding to a **frame rate of 1.8 frames per second (FPS)**.

Also, note that in **Settings**,  it is possible to adjust the model's confidence.

\noindent
![](./images/png/infer.png){width=85% fig-align="center"}

> To run the Image Classification Model, the XIAO ESP32S3 had a peak current of 14mA at 5.23V, resulting in a **power consumption of 730mW**.

As before, in the **Output --> GPIO**, we can turn the GPIOs or the Internal LED ON based on the detected class. For example, the LED will be turned ON when the wheel is detected.

\noindent
![](./images/png/led-wheel.png){width=65% fig-align="center"}

### Saving the Model 

It is possible to save the model in the SenseCraft AI Studio. The Studio will retain all our models for later deployment. For that, return to the `Training` tab and select the button `[Save to SenseCraft`]:

\noindent
![](./images/png/save-model.png){width=75% fig-align="center"}

Follow the instructions to enter the model's name, description, image, and other details. 

\noindent
![](./images/png/model-saved.png){width=85% fig-align="center"}

Note that the trained model (an Int8 MobileNet V2 with a size of 320KB) can be downloaded for further use or even analysis, for example, using [Netron](https://github.com/lutzroeder/netron). Note that the model uses images of size 224x224x3 as its Input Tensor. In the next step, we will use different hyperparameters on the Edge Impulse Studio. 

\noindent
![](./images/png/netron.png){width=85% fig-align="center"}

Also, the model can be deployed again to the device at any time. Automatically, the **Workspace** will be open on the SenseCraft AI. 

## Image Classification Project from a Dataset

The primary objective of our project is to train a model and perform inference on the XIAO ESP32S3 Sense. For training, we should find some data **(in fact, tons of data!)**.

*But as we alheady know, first of all, we need a goal! What do we want to classify?*

With TinyML, a set of techniques associated with machine learning inference on embedded devices, we should limit the classification to three or four categories due to limitations (mainly memory). We can, for example, train the images captured for the Box versus Wheel, which can be downloaded from the SenseCraft AI Studio. 

> Alternatively, we can use a completely new dataset, such as one that differentiates apples from bananas and potatoes, or other categories. If possible, try finding a specific dataset that includes images from those categories. [Kaggle fruit-and-vegetable-image-recognition](https://www.kaggle.com/kritikseth/fruit-and-vegetable-image-recognition) is a good start. 

Let's download the dataset captured in the previous section. Open the menu (3 dots) on each of the captured classes and select `Export Data`.

\noindent
![](./images/png/export-dataset.png){width=75% fig-align="center"}

The dataset will be downloaded to the computer as a .ZIP file, with one file for each class. Save them in your working folder and unzip them. You should have three folders, one for each class. 

\noindent
![](./images/png/dataset-pc.png){width=85% fig-align="center"}

> Optionally, you can add some fresh images, using, for example, the code discussed in the setup lab.

## Training the model with Edge Impulse Studio

We will use the Edge Impulse Studio to train our model. [Edge Impulse](https://www.edgeimpulse.com/) is a leading development platform for machine learning on edge devices.

Enter your account credentials (or create a free account) at Edge Impulse. Next, create a new project:

### Data Acquisition

Next, go to the **Data acquisition** section and there, select `+ Add data`. A pop-up window will appear.  Select `UPLOAD DATA`. 

\noindent
![](./images/png/get-dataset.png){width=85% fig-align="center"}

After selection, a new Pop-Up window will appear, asking to update the data. 

- In Upload mode: `select a folder` and press `[Choose Files]`. 
- Go to the folder that contains one of the classes and press `[Upload]`

\noindent
![](./images/png/select-folder.png){width=85% fig-align="center"}

- You will return automatically to the Upload data window. 
- Select `Automatically split between training and testing`
- And enter the label of the images that are in the folder. 
- Select `[Upload data]`
- At this point, the files will start to be uploaded, and after that, another Pop-Up window will appear asking if you are building an object detection project. Select `[no]`

\noindent
![](./images/png/up-data-2.png){width=85% fig-align="center"}

Repeat the procedure for all classes. **Do not forget to change the label's name**. If you forget and the images are uploaded, please note that they will be mixed in the Studio. Do not worry, you can manually move the data between classes further. 

Close the Upload Data window and return to the **Data aquisition** page. We can see that all dataset was uploaded. Note that on the upper panel, we can see that we have 158 items, all of which are balanced. Also, 19% of the images were left for testing. 

\noindent
![](./images/png/dataset.png){width=85% fig-align="center"}

### Impulse Design

> An impulse takes raw data (in this case, images), extracts features (resizes pictures), and then uses a learning block to classify new data.

Classifying images is the most common application of deep learning, but a substantial amount of data is required to accomplish this task. We have around 50 images for each category. Is this number enough? Not at all! We will need thousands of images to "teach" or "model" each class, allowing us to differentiate them. However, we can resolve this issue by retraining a previously trained model using thousands of images. We refer to this technique as **"Transfer Learning" (TL)**. With TL, we can fine-tune a pre-trained image classification model on our data, achieving good performance even with relatively small image datasets, as in our case.

\noindent
![](./images/png/TL.png){width=85% fig-align="center"}

With TL, we can fine-tune a pre-trained image classification model on our data, performing well even with relatively small image datasets (our case).

So, starting from the raw images, we will resize them $(96\times 96)$ Pixels are fed to our Transfer Learning block. Let's create an Inpulse. 

> At this point, we can also define our target device to monitor our "budget" (memory and latency). The XIAO ESP32S3 is not officially supported by Edge Impulse, so let's consider the Espressif ESP-EYE, which is similar but slower. 

\noindent
![](./images/png/impulse.png){width=85% fig-align="center"}

Save the Impulse, as shown above, and go to the **Image** section.  

### Pre-processing (Feature Generation) 

Besides resizing the images, we can convert them to grayscale or retain their original RGB color depth. Let's select `[RGB]` in the `Image` section. Doing that, each data sample will have a dimension of 27,648 features (96x96x3). Pressing `[Save Parameters]` will open a new tab, `Generate Features`. Press the button `[Generate Features]`to generate the features.

### Model Design, Training, and Test 

In 2007, Google introduced [MobileNetV1](https://research.googleblog.com/2017/06/mobilenets-open-source-models-for.html). In 2018, [MobileNetV2: Inverted Residuals and Linear Bottlenecks](https://arxiv.org/abs/1801.04381), was launched, and, in 2019, the V3. The Mobilinet is a family of general-purpose computer vision neural networks explicitly designed for mobile devices to support classification, detection, and other applications. MobileNets are small, low-latency, low-power models parameterized to meet the resource constraints of various use cases.

Although the base MobileNet architecture is already compact and has low latency, a specific use case or application may often require the model to be even smaller and faster. MobileNets introduce a straightforward parameter, **α** (alpha), called the width multiplier to construct these smaller, less computationally expensive models. The role of the width multiplier α is to thin a network uniformly at each layer.

Edge Impulse Studio has available MobileNet V1 (96x96 images) and V2 (96x96 and 160x160 images), with several different **α** values (from 0.05 to 1.0). For example, you will get the highest accuracy with V2, 160x160 images, and α=1.0. Of course, there is a trade-off. The higher the accuracy, the more memory (around 1.3M RAM and 2.6M ROM) will be needed to run the model, implying more latency. The smaller footprint will be obtained at another extreme with MobileNet V1 and α=0.10 (around 53.2K RAM and 101K ROM).

> We will use the **MobileNet V2 0.35** as our base model (but a model with a greater alpha can be used here). The final layer of our model, preceding the output layer, will have 16 neurons with a 10% dropout rate for preventing overfitting.

Another necessary technique to use with deep learning is **data augmentation**. Data augmentation is a method that can help improve the accuracy of machine learning models by creating additional artificial data. A data augmentation system makes small, random changes to your training data during the training process (such as flipping, cropping, or rotating the images).

Under the hood, here you can see how Edge Impulse implements a data Augmentation policy on your data:

```cpp
# Implements the data augmentation policy
def augment_image(image, label):
    # Flips the image randomly
    image = tf.image.random_flip_left_right(image)

    # Increase the image size, then randomly crop it down to
    # the original dimensions
    resize_factor = random.uniform(1, 1.2)
    new_height = math.floor(resize_factor * INPUT_SHAPE[0])
    new_width = math.floor(resize_factor * INPUT_SHAPE[1])
    image = tf.image.resize_with_crop_or_pad(image, new_height,
                                             new_width)
    image = tf.image.random_crop(image, size=INPUT_SHAPE)

    # Vary the brightness of the image
    image = tf.image.random_brightness(image, max_delta=0.2)

    return image, label
```

Now, let's us define the hyperparameters:

- Epochs: 20,
- Bach Size: 32
- Learning Rate: 0.0005
- Validation size: 20%

And, so, we have as a training result:

\noindent
![](./images/png/train-result.png){width=85% fig-align="center"}

The model profile predicts **233 KB of RAM and 546 KB of Flash**, indicating no problem with the Xiao ESP32S3, which has 8 MB of PSRAM. Additionally, the Studio indicates a **latency of around 1160 ms**, which is very high. However, this is to be expected, given that we are using the ESP-EYE, whose CPU is an Extensa LX6, and the ESP32S3 uses a newer and more powerful Xtensa LX7. 

> With the test data, we also achieved 100% accuracy, even with a quantized INT8 model. This result is not typical in real projects, but our project here is relatively simple, with 2 objects that are very distinctive from each other. 

### Model Deployment 

WE can deploy the model as an Arduino Library or get the **trained model to test it on the SenseCraft AI**. Let's go with this second option. 

On the **Dashboard**, it is possible to download the trained model in several different formats. Let's download `TensorFlow Lite (int8 quantized)`, which has a size of 623KB. 

\noindent
![](./images/png/deploy-1.png){width=85% fig-align="center"}

### Deploy the model on the SenseCraft AI Studio 

On **SenseCraft AI Studio**, go to the `Workspace` tab, select `XIAO ESP32S3`, the corresponding Port, and connect the device. 

You should see the last model that was uploaded to the device. Select the green button `[Upload Model]`. A pop-up window will ask for the **model name**, the **model file,** and to enter the class names (**objects**).  We should use labels following alphabetical order: `0: background`, `1: box`, and `2: wheel`, and then press `[Send]`.

\noindent
![](./images/png/model-tflite-infer.png){width=85% fig-align="center"}

After a few seconds, the model will be uploaded ("flashed") to our device, and the camera image will appear in real-time on the **Preview** Sector. The Classification result will be displayed under the image preview. It is also possible to select the `Confidence Threshold` of your inference using the cursor on **Settings**.

On the **Device Logger**, we can view the Serial Monitor, where we can observe the latency, which is approximately 81 ms for pre-processing and 205 ms for inference, **corresponding to a frame rate of 3.4 frames per second (FPS)**, what is double of we got, training the model on SenseCraft, because we are working with smaller images (96x96 versus 224x224). 

> The total latency is around **4 times faster** than the estimation made in Edge Impulse Studio on an Xtensa LX6 CPU; now we are performing the inference on an Xtensa LX7 CPU.

\noindent
![](./images/png/tfmodel-infer.png){width=85% fig-align="center"}

## Post-Processing

It is possible to obtain the output of a model inference, including Latency, Class ID, and Confidence, as shown on the Device Logger in SenseCraft AI. This allows us to utilize the **XIAO ESP32S3 Sense as an AI sensor**. In other words, we can retrieve the model data using different communication protocols such as MQTT, UART, I2C, or SPI, depending on our project requirements.

> The idea is similar to what we have done on the [Seeed Grove Vision AI V2 Image Classification Post-Processing Lab](https://www.mlsysbook.ai/contents/labs/seeed/grove_vision_ai_v2/image_classification/image_classification#sec-image-classification-postprocessing-553f). 

Below is an example of a connection using the I2C bus. 

![As a Sensor | Seeed Studio Wiki](https://files.seeedstudio.com/wiki/SenseCraft_AI/img2/iic_connection.png){width=75% fig-align="center"}

Please refer to [Seeed Studio Wiki](https://wiki.seeedstudio.com/sensecraft-ai/tutorials/sensecraft-ai-output-libraries-xiao/) for more information. 

## Summary

The XIAO ESP32S3 Sense is a remarkably capable and flexible platform for image classification applications. Through this lab, we've explored two distinct development approaches that cater to different skill levels and project requirements.

- The **SenseCraft AI Studio** provides an accessible entry point with its **no-code interface**, enabling rapid prototyping and deployment of pre-trained models like person detection. With real-time inference and integrated post-processing capabilities, it demonstrates how AI can be deployed without extensive programming or ML knowledge.

- For more advanced applications, **Edge Impulse Studio** offers comprehensive machine learning pipeline tools, including custom dataset management, transfer learning with several pre-trained models, such as MobileNet, and model optimization. 

Key insights from this lab include the importance of image resolution trade-offs, the effectiveness of transfer learning for small datasets, and the practical considerations of edge AI deployment, such as power consumption and memory constraints.

The Lab demonstrates fundamental TinyML principles that extend beyond this specific hardware: resource-constrained inference, real-time processing requirements, and the complete pipeline from data collection through model deployment to practical applications. With built-in post-processing capabilities including GPIO control and communication protocols, the XIAO serves as more than just an inference engine—it becomes a complete AI sensor platform.

This foundation in image classification prepares you for more complex computer vision tasks while showcasing how modern edge AI makes sophisticated computer vision accessible, cost-effective, and deployable in real-world embedded applications ranging from industrial automation to smart home systems.

## Resources

- [Getting Started with the XIAO ESP32S3](https://wiki.seeedstudio.com/xiao_esp32s3_getting_started/)
- [SenseCraft AI Studio Home](https://sensecraft.seeed.cc/ai/home)
- [SenseCraft Vision Workspace](https://sensecraft.seeed.cc/ai/device/local/32)
- [Dataset example](https://www.kaggle.com/kritikseth/fruit-and-vegetable-image-recognition)
- [Edge Impulse Project](https://studio.edgeimpulse.com/public/757065/live)
- [XIAO as an AI Sensor](https://wiki.seeedstudio.com/sensecraft-ai/tutorials/sensecraft-ai-output-libraries-xiao/)
- [Seeed Arduino SSCMA Library](https://github.com/Seeed-Studio/Seeed_Arduino_SSCMA)
