<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.31">
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
<title>ML Systems Textbook</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>

<script src="../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../">
<link href="../../../contents/core/benchmarking/benchmarking.html" rel="next">
<link href="../../../contents/core/optimizations/optimizations.html" rel="prev">
<link href="../../../assets/images/icons/favicon.png" rel="icon" type="image/png">
<script src="../../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-92151ed919028c7172e396588fd5eb2d.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-dark-92151ed919028c7172e396588fd5eb2d.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-92151ed919028c7172e396588fd5eb2d.css" rel="stylesheet" class="quarto-color-scheme-extra" id="quarto-text-highlighting-styles">
<script src="../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../site_libs/bootstrap/bootstrap-e20ddbae1c31dc3d16ae49e0de1a1121.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../../site_libs/bootstrap/bootstrap-dark-c4b04d4abe0a732bdbc9f57301cfc3b1.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="light">
<link href="../../../site_libs/bootstrap/bootstrap-e20ddbae1c31dc3d16ae49e0de1a1121.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme-extra" id="quarto-bootstrap" data-mode="light">
<link href="../../../site_libs/quarto-contrib/foldbox/foldbox.css" rel="stylesheet">
<script src="../../../site_libs/quarto-contrib/glightbox/glightbox.min.js"></script>
<link href="../../../site_libs/quarto-contrib/glightbox/glightbox.min.css" rel="stylesheet">
<link href="../../../site_libs/quarto-contrib/glightbox/lightbox.css" rel="stylesheet"><script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "/"
  ],
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-M21L0CBCVN"></script><script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-M21L0CBCVN', { 'anonymize_ip': true});
</script><script type="application/json" class="js-hypothesis-config">
{
  "theme": "clean",
  "openSidebar": false
}
</script><script async="" src="https://hypothes.is/embed.js"></script><script>
  window.document.addEventListener("DOMContentLoaded", function (_event) {
    document.body.classList.add('hypothesis-enabled');
  });
</script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="">
<link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&amp;family=JetBrains+Mono:wght@400;500&amp;display=swap" rel="stylesheet">
<link rel="manifest" href="../../../site.webmanifest">
<link rel="apple-touch-icon" href="../../../assets/images/icons/favicon.png">
<meta name="theme-color" content="#A51C30">
<script type="module" src="../../../tools/scripts/socratiQ/bundle.js" defer=""></script><script src="../../../assets/scripts/sidebar-auto-collapse.js" defer=""></script><script src="../../../assets/scripts/version-link.js" defer=""></script><script src="../../../assets/scripts/subscribe-modal.js" defer=""></script><style>
.callout-code {
  --color1: #F2F4F8;
  --color2: #D1D7E0;
}
.callout-quiz-question {
  --color1: #F0F0F8;
  --color2: #5B4B8A;
}
.callout-definition {
  --color1: #F0F4F8;
  --color2: #1B4F72;
}
.callout-chapter-connection {
  --color1: #FDF2F7;
  --color2: #A51C30;
}
.callout-resource-slides {
  --color1: #E0F2F1;
  --color2: #20B2AA;
}
.callout-resource-exercises {
  --color1: #E0F2F1;
  --color2: #20B2AA;
}
.callout-colab {
  --color1: #FFF5E6;
  --color2: #FF6B35;
}
.callout-quiz-answer {
  --color1: #E8F2EA;
  --color2: #4a7c59;
}
.callout-resource-videos {
  --color1: #E0F2F1;
  --color2: #20B2AA;
}
.callout-example {
  --color1: #F0F8F6;
  --color2: #148F77;
}
</style>
<style>
details.callout-code > summary::before {
  background-image: url("../../../assets/images/icons/callouts/icon_callout_code.png");
}
details.callout-quiz-question > summary::before {
  background-image: url("../../../assets/images/icons/callouts/icon_callout_quiz_question.png");
}
details.callout-definition > summary::before {
  background-image: url("../../../assets/images/icons/callouts/icon_callout_definition.png");
}
details.callout-chapter-connection > summary::before {
  background-image: url("../../../assets/images/icons/callouts/icon_callout_chapter_connection.png");
}
details.callout-resource-slides > summary::before {
  background-image: url("../../../assets/images/icons/callouts/icon_callout_resource_slides.png");
}
details.callout-resource-exercises > summary::before {
  background-image: url("../../../assets/images/icons/callouts/icon_callout_resource_exercises.png");
}
details.callout-colab > summary::before {
  background-image: url("../../../assets/images/icons/callouts/icon_callout_colab.png");
}
details.callout-quiz-answer > summary::before {
  background-image: url("../../../assets/images/icons/callouts/icon_callout_quiz_answer.png");
}
details.callout-resource-videos > summary::before {
  background-image: url("../../../assets/images/icons/callouts/icon_callout_resource_videos.png");
}
details.callout-example > summary::before {
  background-image: url("../../../assets/images/icons/callouts/icon_callout_example.png");
}
</style>
<script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script><script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script><script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>
<meta property="og:title" content="ML Systems Textbook">
<meta property="og:image" content="https://mlsysbook.ai/book/contents/core/hw_acceleration/assets/images/covers/cover-hardcover-book.png">
<meta property="og:site_name" content="Machine Learning Systems">
<meta property="og:locale" content="en_US">
<meta name="twitter:title" content="ML Systems Textbook">
<meta name="twitter:image" content="https://mlsysbook.ai/book/contents/core/hw_acceleration/assets/images/covers/cover-hardcover-book.png">
<meta name="twitter:card" content="summary_large_image">
</head>
<body class="nav-sidebar floating nav-fixed slimcontent quarto-light"><script id="quarto-html-before-body" type="application/javascript">
    const toggleBodyColorMode = (bsSheetEl) => {
      const mode = bsSheetEl.getAttribute("data-mode");
      const bodyEl = window.document.querySelector("body");
      if (mode === "dark") {
        bodyEl.classList.add("quarto-dark");
        bodyEl.classList.remove("quarto-light");
      } else {
        bodyEl.classList.add("quarto-light");
        bodyEl.classList.remove("quarto-dark");
      }
    }
    const toggleBodyColorPrimary = () => {
      const bsSheetEl = window.document.querySelector("link#quarto-bootstrap:not([rel=disabled-stylesheet])");
      if (bsSheetEl) {
        toggleBodyColorMode(bsSheetEl);
      }
    }
    const setColorSchemeToggle = (alternate) => {
      const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
      for (let i=0; i < toggles.length; i++) {
        const toggle = toggles[i];
        if (toggle) {
          if (alternate) {
            toggle.classList.add("alternate");
          } else {
            toggle.classList.remove("alternate");
          }
        }
      }
    };
    const toggleColorMode = (alternate) => {
      // Switch the stylesheets
      const primaryStylesheets = window.document.querySelectorAll('link.quarto-color-scheme:not(.quarto-color-alternate)');
      const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
      manageTransitions('#quarto-margin-sidebar .nav-link', false);
      if (alternate) {
        // note: dark is layered on light, we don't disable primary!
        enableStylesheet(alternateStylesheets);
        for (const sheetNode of alternateStylesheets) {
          if (sheetNode.id === "quarto-bootstrap") {
            toggleBodyColorMode(sheetNode);
          }
        }
      } else {
        disableStylesheet(alternateStylesheets);
        enableStylesheet(primaryStylesheets)
        toggleBodyColorPrimary();
      }
      manageTransitions('#quarto-margin-sidebar .nav-link', true);
      // Switch the toggles
      setColorSchemeToggle(alternate)
      // Hack to workaround the fact that safari doesn't
      // properly recolor the scrollbar when toggling (#1455)
      if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
        manageTransitions("body", false);
        window.scrollTo(0, 1);
        setTimeout(() => {
          window.scrollTo(0, 0);
          manageTransitions("body", true);
        }, 40);
      }
    }
    const disableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        stylesheet.rel = 'disabled-stylesheet';
      }
    }
    const enableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        if(stylesheet.rel !== 'stylesheet') { // for Chrome, which will still FOUC without this check
          stylesheet.rel = 'stylesheet';
        }
      }
    }
    const manageTransitions = (selector, allowTransitions) => {
      const els = window.document.querySelectorAll(selector);
      for (let i=0; i < els.length; i++) {
        const el = els[i];
        if (allowTransitions) {
          el.classList.remove('notransition');
        } else {
          el.classList.add('notransition');
        }
      }
    }
    const isFileUrl = () => {
      return window.location.protocol === 'file:';
    }
    const hasAlternateSentinel = () => {
      let styleSentinel = getColorSchemeSentinel();
      if (styleSentinel !== null) {
        return styleSentinel === "alternate";
      } else {
        return false;
      }
    }
    const setStyleSentinel = (alternate) => {
      const value = alternate ? "alternate" : "default";
      if (!isFileUrl()) {
        window.localStorage.setItem("quarto-color-scheme", value);
      } else {
        localAlternateSentinel = value;
      }
    }
    const getColorSchemeSentinel = () => {
      if (!isFileUrl()) {
        const storageValue = window.localStorage.getItem("quarto-color-scheme");
        return storageValue != null ? storageValue : localAlternateSentinel;
      } else {
        return localAlternateSentinel;
      }
    }
    const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
      const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
      const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
      let newTheme = '';
      if(authorPrefersDark) {
        newTheme = isAlternate ? baseTheme : alternateTheme;
      } else {
        newTheme = isAlternate ? alternateTheme : baseTheme;
      }
      const changeGiscusTheme = () => {
        // From: https://github.com/giscus/giscus/issues/336
        const sendMessage = (message) => {
          const iframe = document.querySelector('iframe.giscus-frame');
          if (!iframe) return;
          iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
        }
        sendMessage({
          setConfig: {
            theme: newTheme
          }
        });
      }
      const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
      if (isGiscussLoaded) {
        changeGiscusTheme();
      }
    };
    const authorPrefersDark = false;
    const queryPrefersDark = window.matchMedia('(prefers-color-scheme: dark)');
    const darkModeDefault = queryPrefersDark.matches;
      document.querySelector('link#quarto-text-highlighting-styles.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
      document.querySelector('link#quarto-bootstrap.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
    let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
    // Dark / light mode switch
    window.quartoToggleColorScheme = () => {
      // Read the current dark / light value
      let toAlternate = !hasAlternateSentinel();
      toggleColorMode(toAlternate);
      setStyleSentinel(toAlternate);
      toggleGiscusIfUsed(toAlternate, darkModeDefault);
      window.dispatchEvent(new Event('resize'));
    };
    queryPrefersDark.addEventListener("change", e => {
      if(window.localStorage.getItem("quarto-color-scheme") !== null)
        return;
      const alternate = e.matches
      toggleColorMode(alternate);
      localAlternateSentinel = e.matches ? 'alternate' : 'default'; // this is used alongside local storage!
      toggleGiscusIfUsed(alternate, darkModeDefault);
    });
    // Switch to dark mode if need be
    if (hasAlternateSentinel()) {
      toggleColorMode(true);
    } else {
      toggleColorMode(false);
    }
  </script>

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top"><nav class="navbar navbar-expand-lg " data-bs-theme="dark"><div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="../../../index.html" class="navbar-brand navbar-brand-logo">
    <img src="../../../assets/images/icons/favicon.png" alt="" class="navbar-logo"></a>
    <a class="navbar-brand" href="../../../index.html">
    <span class="navbar-title">Machine Learning Systems</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
<li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-textbook" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Textbook</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-textbook">
<li>
    <a class="dropdown-item" href="../../../../book/"><i class="bi bi-book-half" role="img">
</i> 
 <span class="dropdown-text">Textbook</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../../tinytorch/"><i class="bi bi-fire" role="img">
</i> 
 <span class="dropdown-text">TinyTorch</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../../kits/"><i class="bi bi-cpu" role="img">
</i> 
 <span class="dropdown-text">Hardware Kits</span></a>
  </li>  
        <li><hr class="dropdown-divider"></li>
        <li>
    <a class="dropdown-item" href="../../../../labs/"><i class="bi bi-lightbulb" role="img">
</i> 
 <span class="dropdown-text">Labs (Coming 2026)</span></a>
  </li>  
    </ul>
</li>
</ul>
<ul class="navbar-nav navbar-nav-scroll ms-auto">
<li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-downloads" role="link" data-bs-toggle="dropdown" aria-expanded="false">
      <i class="bi bi-download" role="img">
</i> 
 <span class="menu-text">Downloads</span>
    </a>
    <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="nav-menu-downloads">
<li>
    <a class="dropdown-item" href="../../../assets/downloads/Machine-Learning-Systems.pdf" target="_blank"><i class="bi bi-file-pdf" role="img">
</i> 
 <span class="dropdown-text">Textbook PDF</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../assets/downloads/Machine-Learning-Systems.epub" target="_blank"><i class="bi bi-journal-text" role="img">
</i> 
 <span class="dropdown-text">Textbook EPUB</span></a>
  </li>  
    </ul>
</li>
  <li class="nav-item">
    <a class="nav-link" href="https://github.com/harvard-edge/cs249r_book#support-this-work" target="_blank"> <i class="bi bi-star" role="img">
</i> 
<span class="menu-text">Star</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://opencollective.com/mlsysbook" target="_blank"> <i class="bi bi-heart" role="img">
</i> 
<span class="menu-text">Support</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../#subscribe"> <i class="bi bi-envelope" role="img">
</i> 
<span class="menu-text">Subscribe</span></a>
  </li>  
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-github" role="link" data-bs-toggle="dropdown" aria-expanded="false">
      <i class="bi bi-github" role="img">
</i> 
 <span class="menu-text">GitHub</span>
    </a>
    <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="nav-menu-github">
<li>
    <a class="dropdown-item" href="https://github.com/harvard-edge/cs249r_book" target="_blank"><i class="bi bi-pencil" role="img">
</i> 
 <span class="dropdown-text">Edit this page</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://github.com/harvard-edge/cs249r_book/issues/new" target="_blank"><i class="bi bi-bug" role="img">
</i> 
 <span class="dropdown-text">Report an issue</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://github.com/harvard-edge/cs249r_book/discussions" target="_blank"><i class="bi bi-chat" role="img">
</i> 
 <span class="dropdown-text">Discussions</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://github.com/harvard-edge/cs249r_book" target="_blank"><i class="bi bi-code" role="img">
</i> 
 <span class="dropdown-text">View source</span></a>
  </li>  
    </ul>
</li>
</ul>
</div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav><nav class="quarto-secondary-nav"><div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../../contents/core/efficient_ai/efficient_ai.html">Performance Engineering</a></li><li class="breadcrumb-item"><a href="../../../contents/core/hw_acceleration/hw_acceleration.html">AI Acceleration</a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
    </div>
  </nav><div id="quarto-announcement" data-announcement-id="b43e2aeb169c88acb08fe42121c141fd" class="alert alert-primary hidden">
<i class="bi bi-megaphone quarto-announcement-icon"></i><div class="quarto-announcement-content">
<p>ðŸŽ‰ <strong>Happy New Year!</strong> New navbar with dropdown menus. Try them out!<br> ðŸ”¥ <strong>TinyTorch:</strong> Build your own ML framework from scratch. <a href="https://mlsysbook.ai/tinytorch">Start â†’</a><br> ðŸ“¦ <strong>Hardware Kits:</strong> Arduino, Seeed &amp; Raspberry Pi labs. <a href="https://mlsysbook.ai/kits">Explore â†’</a><br> ðŸ“¬ <strong>Newsletter:</strong> ML Systems insights &amp; updates. <a href="#subscribe">Subscribe â†’</a></p>
</div>
<i class="bi bi-x-lg quarto-announcement-action"></i>
</div>
</header><!-- content --><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto"><div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Homepage</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/frontmatter/foreword.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Foreword</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/frontmatter/about/about.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">About the Book</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/frontmatter/acknowledgements/acknowledgements.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Acknowledgements</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/frontmatter/socratiq/socratiq.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">SocratiQ AI</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Systems Foundations</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/introduction/introduction.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduction</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/ml_systems/ml_systems.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">ML Systems</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/dl_primer/dl_primer.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">DL Primer</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/dnn_architectures/dnn_architectures.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">DNN Architectures</span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">Design Principles</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/workflow/workflow.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">AI Workflow</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/data_engineering/data_engineering.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Data Engineering</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/frameworks/frameworks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">AI Frameworks</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/training/training.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">AI Training</span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true">
 <span class="menu-text">Performance Engineering</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/efficient_ai/efficient_ai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Efficient AI</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/optimizations/optimizations.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Model Optimizations</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/hw_acceleration/hw_acceleration.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">AI Acceleration</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/benchmarking/benchmarking.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Benchmarking AI</span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true">
 <span class="menu-text">Robust Deployment</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/ops/ops.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">ML Operations</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/ondevice_learning/ondevice_learning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">On-Device Learning</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/privacy_security/privacy_security.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Security &amp; Privacy</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/robust_ai/robust_ai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Robust AI</span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true">
 <span class="menu-text">Trustworthy Systems</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/responsible_ai/responsible_ai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Responsible AI</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/sustainable_ai/sustainable_ai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Sustainable AI</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/ai_for_good/ai_for_good.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">AI for Good</span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true">
 <span class="menu-text">Frontiers of ML Systems</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/frontiers/frontiers.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">AGI Systems</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/conclusion/conclusion.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Conclusion</span></a>
  </div>
</li>
      </ul>
</li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="true">
 <span class="menu-text">Glossary</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-7" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/backmatter/glossary/glossary.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Complete Glossary</span></a>
  </div>
</li>
      </ul>
</li>
    </ul>
</div>
</nav><div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active" data-toc-expanded="99"><h2 id="toc-title">On this page</h2>
   
  <ul>
<li>
<a href="#sec-ai-acceleration" id="toc-sec-ai-acceleration" class="nav-link active" data-scroll-target="#sec-ai-acceleration">AI Acceleration</a>
  <ul>
<li><a href="#purpose" id="toc-purpose" class="nav-link" data-scroll-target="#purpose">Purpose</a></li>
  <li><a href="#sec-ai-acceleration-ai-hardware-acceleration-fundamentals-2096" id="toc-sec-ai-acceleration-ai-hardware-acceleration-fundamentals-2096" class="nav-link" data-scroll-target="#sec-ai-acceleration-ai-hardware-acceleration-fundamentals-2096">AI Hardware Acceleration Fundamentals</a></li>
  <li>
<a href="#sec-ai-acceleration-evolution-hardware-specialization-1d21" id="toc-sec-ai-acceleration-evolution-hardware-specialization-1d21" class="nav-link" data-scroll-target="#sec-ai-acceleration-evolution-hardware-specialization-1d21">Evolution of Hardware Specialization</a>
  <ul class="collapse">
<li><a href="#sec-ai-acceleration-specialized-computing-1a77" id="toc-sec-ai-acceleration-specialized-computing-1a77" class="nav-link" data-scroll-target="#sec-ai-acceleration-specialized-computing-1a77">Specialized Computing</a></li>
  <li><a href="#sec-ai-acceleration-parallel-computing-graphics-processing-66b1" id="toc-sec-ai-acceleration-parallel-computing-graphics-processing-66b1" class="nav-link" data-scroll-target="#sec-ai-acceleration-parallel-computing-graphics-processing-66b1">Parallel Computing and Graphics Processing</a></li>
  <li><a href="#sec-ai-acceleration-emergence-domainspecific-architectures-e045" id="toc-sec-ai-acceleration-emergence-domainspecific-architectures-e045" class="nav-link" data-scroll-target="#sec-ai-acceleration-emergence-domainspecific-architectures-e045">Emergence of Domain-Specific Architectures</a></li>
  <li><a href="#sec-ai-acceleration-machine-learning-hardware-specialization-a17f" id="toc-sec-ai-acceleration-machine-learning-hardware-specialization-a17f" class="nav-link" data-scroll-target="#sec-ai-acceleration-machine-learning-hardware-specialization-a17f">Machine Learning Hardware Specialization</a></li>
  </ul>
</li>
  <li>
<a href="#sec-ai-acceleration-ai-compute-primitives-8471" id="toc-sec-ai-acceleration-ai-compute-primitives-8471" class="nav-link" data-scroll-target="#sec-ai-acceleration-ai-compute-primitives-8471">AI Compute Primitives</a>
  <ul class="collapse">
<li>
<a href="#sec-ai-acceleration-vector-operations-729f" id="toc-sec-ai-acceleration-vector-operations-729f" class="nav-link" data-scroll-target="#sec-ai-acceleration-vector-operations-729f">Vector Operations</a>
  <ul class="collapse">
<li><a href="#sec-ai-acceleration-highlevel-framework-operations-9248" id="toc-sec-ai-acceleration-highlevel-framework-operations-9248" class="nav-link" data-scroll-target="#sec-ai-acceleration-highlevel-framework-operations-9248">High-Level Framework Operations</a></li>
  <li><a href="#sec-ai-acceleration-sequential-scalar-execution-d063" id="toc-sec-ai-acceleration-sequential-scalar-execution-d063" class="nav-link" data-scroll-target="#sec-ai-acceleration-sequential-scalar-execution-d063">Sequential Scalar Execution</a></li>
  <li><a href="#sec-ai-acceleration-parallel-vector-execution-cdaa" id="toc-sec-ai-acceleration-parallel-vector-execution-cdaa" class="nav-link" data-scroll-target="#sec-ai-acceleration-parallel-vector-execution-cdaa">Parallel Vector Execution</a></li>
  <li><a href="#sec-ai-acceleration-vector-processing-history-c631" id="toc-sec-ai-acceleration-vector-processing-history-c631" class="nav-link" data-scroll-target="#sec-ai-acceleration-vector-processing-history-c631">Vector Processing History</a></li>
  </ul>
</li>
  <li>
<a href="#sec-ai-acceleration-matrix-operations-508d" id="toc-sec-ai-acceleration-matrix-operations-508d" class="nav-link" data-scroll-target="#sec-ai-acceleration-matrix-operations-508d">Matrix Operations</a>
  <ul class="collapse">
<li><a href="#sec-ai-acceleration-matrix-operations-neural-networks-527a" id="toc-sec-ai-acceleration-matrix-operations-neural-networks-527a" class="nav-link" data-scroll-target="#sec-ai-acceleration-matrix-operations-neural-networks-527a">Matrix Operations in Neural Networks</a></li>
  <li><a href="#sec-ai-acceleration-types-matrix-computations-neural-networks-b497" id="toc-sec-ai-acceleration-types-matrix-computations-neural-networks-b497" class="nav-link" data-scroll-target="#sec-ai-acceleration-types-matrix-computations-neural-networks-b497">Types of Matrix Computations in Neural Networks</a></li>
  <li><a href="#sec-ai-acceleration-matrix-operations-hardware-acceleration-514a" id="toc-sec-ai-acceleration-matrix-operations-hardware-acceleration-514a" class="nav-link" data-scroll-target="#sec-ai-acceleration-matrix-operations-hardware-acceleration-514a">Matrix Operations Hardware Acceleration</a></li>
  <li><a href="#sec-ai-acceleration-historical-foundations-matrix-computation-402e" id="toc-sec-ai-acceleration-historical-foundations-matrix-computation-402e" class="nav-link" data-scroll-target="#sec-ai-acceleration-historical-foundations-matrix-computation-402e">Historical Foundations of Matrix Computation</a></li>
  </ul>
</li>
  <li>
<a href="#sec-ai-acceleration-special-function-units-ed00" id="toc-sec-ai-acceleration-special-function-units-ed00" class="nav-link" data-scroll-target="#sec-ai-acceleration-special-function-units-ed00">Special Function Units</a>
  <ul class="collapse">
<li><a href="#sec-ai-acceleration-nonlinear-functions-fdce" id="toc-sec-ai-acceleration-nonlinear-functions-fdce" class="nav-link" data-scroll-target="#sec-ai-acceleration-nonlinear-functions-fdce">Non-Linear Functions</a></li>
  <li><a href="#sec-ai-acceleration-hardware-implementation-nonlinear-functions-004d" id="toc-sec-ai-acceleration-hardware-implementation-nonlinear-functions-004d" class="nav-link" data-scroll-target="#sec-ai-acceleration-hardware-implementation-nonlinear-functions-004d">Hardware Implementation of Non-Linear Functions</a></li>
  <li><a href="#sec-ai-acceleration-hardware-acceleration-08e3" id="toc-sec-ai-acceleration-hardware-acceleration-08e3" class="nav-link" data-scroll-target="#sec-ai-acceleration-hardware-acceleration-08e3">Hardware Acceleration</a></li>
  <li><a href="#sec-ai-acceleration-sfus-history-a1b6" id="toc-sec-ai-acceleration-sfus-history-a1b6" class="nav-link" data-scroll-target="#sec-ai-acceleration-sfus-history-a1b6">SFUs History</a></li>
  </ul>
</li>
  <li>
<a href="#sec-ai-acceleration-compute-units-execution-models-f406" id="toc-sec-ai-acceleration-compute-units-execution-models-f406" class="nav-link" data-scroll-target="#sec-ai-acceleration-compute-units-execution-models-f406">Compute Units and Execution Models</a>
  <ul class="collapse">
<li><a href="#sec-ai-acceleration-mapping-primitives-execution-units-ccb6" id="toc-sec-ai-acceleration-mapping-primitives-execution-units-ccb6" class="nav-link" data-scroll-target="#sec-ai-acceleration-mapping-primitives-execution-units-ccb6">Mapping Primitives to Execution Units</a></li>
  <li><a href="#sec-ai-acceleration-evolution-simd-simt-architectures-e1fd" id="toc-sec-ai-acceleration-evolution-simd-simt-architectures-e1fd" class="nav-link" data-scroll-target="#sec-ai-acceleration-evolution-simd-simt-architectures-e1fd">Evolution from SIMD to SIMT Architectures</a></li>
  <li><a href="#sec-ai-acceleration-tensor-cores-771f" id="toc-sec-ai-acceleration-tensor-cores-771f" class="nav-link" data-scroll-target="#sec-ai-acceleration-tensor-cores-771f">Tensor Cores</a></li>
  <li><a href="#sec-ai-acceleration-processing-elements-daa1" id="toc-sec-ai-acceleration-processing-elements-daa1" class="nav-link" data-scroll-target="#sec-ai-acceleration-processing-elements-daa1">Processing Elements</a></li>
  <li><a href="#sec-ai-acceleration-systolic-arrays-6fa8" id="toc-sec-ai-acceleration-systolic-arrays-6fa8" class="nav-link" data-scroll-target="#sec-ai-acceleration-systolic-arrays-6fa8">Systolic Arrays</a></li>
  <li><a href="#sec-ai-acceleration-numerics-ai-acceleration-f7be" id="toc-sec-ai-acceleration-numerics-ai-acceleration-f7be" class="nav-link" data-scroll-target="#sec-ai-acceleration-numerics-ai-acceleration-f7be">Numerics in AI Acceleration</a></li>
  <li><a href="#sec-ai-acceleration-architectural-integration-01b6" id="toc-sec-ai-acceleration-architectural-integration-01b6" class="nav-link" data-scroll-target="#sec-ai-acceleration-architectural-integration-01b6">Architectural Integration</a></li>
  </ul>
</li>
  <li><a href="#sec-ai-acceleration-costperformance-analysis-e925" id="toc-sec-ai-acceleration-costperformance-analysis-e925" class="nav-link" data-scroll-target="#sec-ai-acceleration-costperformance-analysis-e925">Cost-Performance Analysis</a></li>
  </ul>
</li>
  <li>
<a href="#sec-ai-acceleration-ai-memory-systems-0057" id="toc-sec-ai-acceleration-ai-memory-systems-0057" class="nav-link" data-scroll-target="#sec-ai-acceleration-ai-memory-systems-0057">AI Memory Systems</a>
  <ul class="collapse">
<li>
<a href="#sec-ai-acceleration-understanding-ai-memory-wall-3ea9" id="toc-sec-ai-acceleration-understanding-ai-memory-wall-3ea9" class="nav-link" data-scroll-target="#sec-ai-acceleration-understanding-ai-memory-wall-3ea9">Understanding the AI Memory Wall</a>
  <ul class="collapse">
<li><a href="#sec-ai-acceleration-quantifying-computememory-performance-gap-1526" id="toc-sec-ai-acceleration-quantifying-computememory-performance-gap-1526" class="nav-link" data-scroll-target="#sec-ai-acceleration-quantifying-computememory-performance-gap-1526">Quantifying the Compute-Memory Performance Gap</a></li>
  <li><a href="#sec-ai-acceleration-memory-access-patterns-ml-workloads-a960" id="toc-sec-ai-acceleration-memory-access-patterns-ml-workloads-a960" class="nav-link" data-scroll-target="#sec-ai-acceleration-memory-access-patterns-ml-workloads-a960">Memory Access Patterns in ML Workloads</a></li>
  <li><a href="#sec-ai-acceleration-irregular-memory-access-c6ec" id="toc-sec-ai-acceleration-irregular-memory-access-c6ec" class="nav-link" data-scroll-target="#sec-ai-acceleration-irregular-memory-access-c6ec">Irregular Memory Access</a></li>
  </ul>
</li>
  <li>
<a href="#sec-ai-acceleration-memory-hierarchy-1839" id="toc-sec-ai-acceleration-memory-hierarchy-1839" class="nav-link" data-scroll-target="#sec-ai-acceleration-memory-hierarchy-1839">Memory Hierarchy</a>
  <ul class="collapse">
<li><a href="#sec-ai-acceleration-onchip-memory-72d1" id="toc-sec-ai-acceleration-onchip-memory-72d1" class="nav-link" data-scroll-target="#sec-ai-acceleration-onchip-memory-72d1">On-Chip Memory</a></li>
  <li><a href="#sec-ai-acceleration-offchip-memory-ecdb" id="toc-sec-ai-acceleration-offchip-memory-ecdb" class="nav-link" data-scroll-target="#sec-ai-acceleration-offchip-memory-ecdb">Off-Chip Memory</a></li>
  </ul>
</li>
  <li><a href="#sec-ai-acceleration-memory-bandwidth-architectural-tradeoffs-435c" id="toc-sec-ai-acceleration-memory-bandwidth-architectural-tradeoffs-435c" class="nav-link" data-scroll-target="#sec-ai-acceleration-memory-bandwidth-architectural-tradeoffs-435c">Memory Bandwidth and Architectural Trade-offs</a></li>
  <li>
<a href="#sec-ai-acceleration-hostaccelerator-communication-bb7a" id="toc-sec-ai-acceleration-hostaccelerator-communication-bb7a" class="nav-link" data-scroll-target="#sec-ai-acceleration-hostaccelerator-communication-bb7a">Host-Accelerator Communication</a>
  <ul class="collapse">
<li><a href="#sec-ai-acceleration-data-transfer-patterns-689a" id="toc-sec-ai-acceleration-data-transfer-patterns-689a" class="nav-link" data-scroll-target="#sec-ai-acceleration-data-transfer-patterns-689a">Data Transfer Patterns</a></li>
  <li><a href="#sec-ai-acceleration-data-transfer-mechanisms-4109" id="toc-sec-ai-acceleration-data-transfer-mechanisms-4109" class="nav-link" data-scroll-target="#sec-ai-acceleration-data-transfer-mechanisms-4109">Data Transfer Mechanisms</a></li>
  <li><a href="#sec-ai-acceleration-data-transfer-overheads-fbc9" id="toc-sec-ai-acceleration-data-transfer-overheads-fbc9" class="nav-link" data-scroll-target="#sec-ai-acceleration-data-transfer-overheads-fbc9">Data Transfer Overheads</a></li>
  </ul>
</li>
  <li>
<a href="#sec-ai-acceleration-model-memory-pressure-f95e" id="toc-sec-ai-acceleration-model-memory-pressure-f95e" class="nav-link" data-scroll-target="#sec-ai-acceleration-model-memory-pressure-f95e">Model Memory Pressure</a>
  <ul class="collapse">
<li><a href="#sec-ai-acceleration-multilayer-perceptrons-0bbc" id="toc-sec-ai-acceleration-multilayer-perceptrons-0bbc" class="nav-link" data-scroll-target="#sec-ai-acceleration-multilayer-perceptrons-0bbc">Multilayer Perceptrons</a></li>
  <li><a href="#sec-ai-acceleration-convolutional-neural-networks-3085" id="toc-sec-ai-acceleration-convolutional-neural-networks-3085" class="nav-link" data-scroll-target="#sec-ai-acceleration-convolutional-neural-networks-3085">Convolutional Neural Networks</a></li>
  <li><a href="#sec-ai-acceleration-transformer-networks-638c" id="toc-sec-ai-acceleration-transformer-networks-638c" class="nav-link" data-scroll-target="#sec-ai-acceleration-transformer-networks-638c">Transformer Networks</a></li>
  </ul>
</li>
  <li><a href="#sec-ai-acceleration-ml-accelerators-implications-c962" id="toc-sec-ai-acceleration-ml-accelerators-implications-c962" class="nav-link" data-scroll-target="#sec-ai-acceleration-ml-accelerators-implications-c962">ML Accelerators Implications</a></li>
  </ul>
</li>
  <li>
<a href="#sec-ai-acceleration-hardware-mapping-fundamentals-neural-networks-f9a9" id="toc-sec-ai-acceleration-hardware-mapping-fundamentals-neural-networks-f9a9" class="nav-link" data-scroll-target="#sec-ai-acceleration-hardware-mapping-fundamentals-neural-networks-f9a9">Hardware Mapping Fundamentals for Neural Networks</a>
  <ul class="collapse">
<li>
<a href="#sec-ai-acceleration-computation-placement-23d2" id="toc-sec-ai-acceleration-computation-placement-23d2" class="nav-link" data-scroll-target="#sec-ai-acceleration-computation-placement-23d2">Computation Placement</a>
  <ul class="collapse">
<li><a href="#sec-ai-acceleration-computation-placement-definition-e130" id="toc-sec-ai-acceleration-computation-placement-definition-e130" class="nav-link" data-scroll-target="#sec-ai-acceleration-computation-placement-definition-e130">Computation Placement Definition</a></li>
  <li><a href="#sec-ai-acceleration-computation-placement-importance-e7e9" id="toc-sec-ai-acceleration-computation-placement-importance-e7e9" class="nav-link" data-scroll-target="#sec-ai-acceleration-computation-placement-importance-e7e9">Computation Placement Importance</a></li>
  <li><a href="#sec-ai-acceleration-effective-computation-placement-099d" id="toc-sec-ai-acceleration-effective-computation-placement-099d" class="nav-link" data-scroll-target="#sec-ai-acceleration-effective-computation-placement-099d">Effective Computation Placement</a></li>
  </ul>
</li>
  <li>
<a href="#sec-ai-acceleration-memory-allocation-e095" id="toc-sec-ai-acceleration-memory-allocation-e095" class="nav-link" data-scroll-target="#sec-ai-acceleration-memory-allocation-e095">Memory Allocation</a>
  <ul class="collapse">
<li><a href="#sec-ai-acceleration-memory-allocation-definition-e740" id="toc-sec-ai-acceleration-memory-allocation-definition-e740" class="nav-link" data-scroll-target="#sec-ai-acceleration-memory-allocation-definition-e740">Memory Allocation Definition</a></li>
  <li><a href="#sec-ai-acceleration-memory-challenges-different-workloads-e87c" id="toc-sec-ai-acceleration-memory-challenges-different-workloads-e87c" class="nav-link" data-scroll-target="#sec-ai-acceleration-memory-challenges-different-workloads-e87c">Memory Challenges for Different Workloads</a></li>
  </ul>
</li>
  <li>
<a href="#sec-ai-acceleration-combinatorial-complexity-ea33" id="toc-sec-ai-acceleration-combinatorial-complexity-ea33" class="nav-link" data-scroll-target="#sec-ai-acceleration-combinatorial-complexity-ea33">Combinatorial Complexity</a>
  <ul class="collapse">
<li><a href="#sec-ai-acceleration-exploring-configuration-space-f010" id="toc-sec-ai-acceleration-exploring-configuration-space-f010" class="nav-link" data-scroll-target="#sec-ai-acceleration-exploring-configuration-space-f010">Exploring the Configuration Space</a></li>
  <li><a href="#sec-ai-acceleration-ordering-computation-execution-7251" id="toc-sec-ai-acceleration-ordering-computation-execution-7251" class="nav-link" data-scroll-target="#sec-ai-acceleration-ordering-computation-execution-7251">Ordering Computation and Execution</a></li>
  <li><a href="#sec-ai-acceleration-parallelization-across-processing-elements-90d6" id="toc-sec-ai-acceleration-parallelization-across-processing-elements-90d6" class="nav-link" data-scroll-target="#sec-ai-acceleration-parallelization-across-processing-elements-90d6">Parallelization Across Processing Elements</a></li>
  <li><a href="#sec-ai-acceleration-memory-placement-data-movement-fd52" id="toc-sec-ai-acceleration-memory-placement-data-movement-fd52" class="nav-link" data-scroll-target="#sec-ai-acceleration-memory-placement-data-movement-fd52">Memory Placement and Data Movement</a></li>
  <li><a href="#sec-ai-acceleration-mapping-search-space-e9b6" id="toc-sec-ai-acceleration-mapping-search-space-e9b6" class="nav-link" data-scroll-target="#sec-ai-acceleration-mapping-search-space-e9b6">Mapping Search Space</a></li>
  </ul>
</li>
  </ul>
</li>
  <li>
<a href="#sec-ai-acceleration-dataflow-optimization-strategies-ce52" id="toc-sec-ai-acceleration-dataflow-optimization-strategies-ce52" class="nav-link" data-scroll-target="#sec-ai-acceleration-dataflow-optimization-strategies-ce52">Dataflow Optimization Strategies</a>
  <ul class="collapse">
<li>
<a href="#sec-ai-acceleration-building-blocks-mapping-strategies-4932" id="toc-sec-ai-acceleration-building-blocks-mapping-strategies-4932" class="nav-link" data-scroll-target="#sec-ai-acceleration-building-blocks-mapping-strategies-4932">Building Blocks of Mapping Strategies</a>
  <ul class="collapse">
<li><a href="#sec-ai-acceleration-data-movement-patterns-3b06" id="toc-sec-ai-acceleration-data-movement-patterns-3b06" class="nav-link" data-scroll-target="#sec-ai-acceleration-data-movement-patterns-3b06">Data Movement Patterns</a></li>
  <li><a href="#sec-ai-acceleration-memoryefficient-tensor-layouts-e250" id="toc-sec-ai-acceleration-memoryefficient-tensor-layouts-e250" class="nav-link" data-scroll-target="#sec-ai-acceleration-memoryefficient-tensor-layouts-e250">Memory-Efficient Tensor Layouts</a></li>
  <li><a href="#sec-ai-acceleration-kernel-fusion-7faf" id="toc-sec-ai-acceleration-kernel-fusion-7faf" class="nav-link" data-scroll-target="#sec-ai-acceleration-kernel-fusion-7faf">Kernel Fusion</a></li>
  <li><a href="#sec-ai-acceleration-memoryefficient-tiling-strategies-9fce" id="toc-sec-ai-acceleration-memoryefficient-tiling-strategies-9fce" class="nav-link" data-scroll-target="#sec-ai-acceleration-memoryefficient-tiling-strategies-9fce">Memory-Efficient Tiling Strategies</a></li>
  </ul>
</li>
  <li>
<a href="#sec-ai-acceleration-applying-mapping-strategies-neural-networks-3110" id="toc-sec-ai-acceleration-applying-mapping-strategies-neural-networks-3110" class="nav-link" data-scroll-target="#sec-ai-acceleration-applying-mapping-strategies-neural-networks-3110">Applying Mapping Strategies to Neural Networks</a>
  <ul class="collapse">
<li><a href="#sec-ai-acceleration-convolutional-neural-networks-1e47" id="toc-sec-ai-acceleration-convolutional-neural-networks-1e47" class="nav-link" data-scroll-target="#sec-ai-acceleration-convolutional-neural-networks-1e47">Convolutional Neural Networks</a></li>
  <li><a href="#sec-ai-acceleration-transformer-architectures-8f25" id="toc-sec-ai-acceleration-transformer-architectures-8f25" class="nav-link" data-scroll-target="#sec-ai-acceleration-transformer-architectures-8f25">Transformer Architectures</a></li>
  <li><a href="#sec-ai-acceleration-multilayer-perceptrons-eb18" id="toc-sec-ai-acceleration-multilayer-perceptrons-eb18" class="nav-link" data-scroll-target="#sec-ai-acceleration-multilayer-perceptrons-eb18">Multi-Layer Perceptrons</a></li>
  </ul>
</li>
  <li>
<a href="#sec-ai-acceleration-hybrid-mapping-strategies-3e8c" id="toc-sec-ai-acceleration-hybrid-mapping-strategies-3e8c" class="nav-link" data-scroll-target="#sec-ai-acceleration-hybrid-mapping-strategies-3e8c">Hybrid Mapping Strategies</a>
  <ul class="collapse">
<li><a href="#sec-ai-acceleration-layerspecific-mapping-1102" id="toc-sec-ai-acceleration-layerspecific-mapping-1102" class="nav-link" data-scroll-target="#sec-ai-acceleration-layerspecific-mapping-1102">Layer-Specific Mapping</a></li>
  </ul>
</li>
  <li><a href="#sec-ai-acceleration-hardware-implementations-hybrid-strategies-c0e8" id="toc-sec-ai-acceleration-hardware-implementations-hybrid-strategies-c0e8" class="nav-link" data-scroll-target="#sec-ai-acceleration-hardware-implementations-hybrid-strategies-c0e8">Hardware Implementations of Hybrid Strategies</a></li>
  </ul>
</li>
  <li>
<a href="#sec-ai-acceleration-compiler-support-172e" id="toc-sec-ai-acceleration-compiler-support-172e" class="nav-link" data-scroll-target="#sec-ai-acceleration-compiler-support-172e">Compiler Support</a>
  <ul class="collapse">
<li><a href="#sec-ai-acceleration-compiler-design-differences-ml-workloads-0698" id="toc-sec-ai-acceleration-compiler-design-differences-ml-workloads-0698" class="nav-link" data-scroll-target="#sec-ai-acceleration-compiler-design-differences-ml-workloads-0698">Compiler Design Differences for ML Workloads</a></li>
  <li><a href="#sec-ai-acceleration-ml-compilation-pipeline-7676" id="toc-sec-ai-acceleration-ml-compilation-pipeline-7676" class="nav-link" data-scroll-target="#sec-ai-acceleration-ml-compilation-pipeline-7676">ML Compilation Pipeline</a></li>
  <li>
<a href="#sec-ai-acceleration-graph-optimization-f888" id="toc-sec-ai-acceleration-graph-optimization-f888" class="nav-link" data-scroll-target="#sec-ai-acceleration-graph-optimization-f888">Graph Optimization</a>
  <ul class="collapse">
<li><a href="#sec-ai-acceleration-computation-graph-optimization-a028" id="toc-sec-ai-acceleration-computation-graph-optimization-a028" class="nav-link" data-scroll-target="#sec-ai-acceleration-computation-graph-optimization-a028">Computation Graph Optimization</a></li>
  <li><a href="#sec-ai-acceleration-implementation-ai-compilers-1df9" id="toc-sec-ai-acceleration-implementation-ai-compilers-1df9" class="nav-link" data-scroll-target="#sec-ai-acceleration-implementation-ai-compilers-1df9">Implementation in AI Compilers</a></li>
  <li><a href="#sec-ai-acceleration-graph-optimization-importance-9ccb" id="toc-sec-ai-acceleration-graph-optimization-importance-9ccb" class="nav-link" data-scroll-target="#sec-ai-acceleration-graph-optimization-importance-9ccb">Graph Optimization Importance</a></li>
  </ul>
</li>
  <li>
<a href="#sec-ai-acceleration-kernel-selection-df01" id="toc-sec-ai-acceleration-kernel-selection-df01" class="nav-link" data-scroll-target="#sec-ai-acceleration-kernel-selection-df01">Kernel Selection</a>
  <ul class="collapse">
<li><a href="#sec-ai-acceleration-implementation-ai-compilers-c917" id="toc-sec-ai-acceleration-implementation-ai-compilers-c917" class="nav-link" data-scroll-target="#sec-ai-acceleration-implementation-ai-compilers-c917">Implementation in AI Compilers</a></li>
  <li><a href="#sec-ai-acceleration-kernel-selection-importance-3c3f" id="toc-sec-ai-acceleration-kernel-selection-importance-3c3f" class="nav-link" data-scroll-target="#sec-ai-acceleration-kernel-selection-importance-3c3f">Kernel Selection Importance</a></li>
  </ul>
</li>
  <li>
<a href="#sec-ai-acceleration-memory-planning-fb9f" id="toc-sec-ai-acceleration-memory-planning-fb9f" class="nav-link" data-scroll-target="#sec-ai-acceleration-memory-planning-fb9f">Memory Planning</a>
  <ul class="collapse">
<li><a href="#sec-ai-acceleration-implementation-ai-compilers-2ae0" id="toc-sec-ai-acceleration-implementation-ai-compilers-2ae0" class="nav-link" data-scroll-target="#sec-ai-acceleration-implementation-ai-compilers-2ae0">Implementation in AI Compilers</a></li>
  <li><a href="#sec-ai-acceleration-memory-planning-importance-e987" id="toc-sec-ai-acceleration-memory-planning-importance-e987" class="nav-link" data-scroll-target="#sec-ai-acceleration-memory-planning-importance-e987">Memory Planning Importance</a></li>
  </ul>
</li>
  <li>
<a href="#sec-ai-acceleration-computation-scheduling-7ccd" id="toc-sec-ai-acceleration-computation-scheduling-7ccd" class="nav-link" data-scroll-target="#sec-ai-acceleration-computation-scheduling-7ccd">Computation Scheduling</a>
  <ul class="collapse">
<li><a href="#sec-ai-acceleration-implementation-ai-compilers-ff25" id="toc-sec-ai-acceleration-implementation-ai-compilers-ff25" class="nav-link" data-scroll-target="#sec-ai-acceleration-implementation-ai-compilers-ff25">Implementation in AI Compilers</a></li>
  <li><a href="#sec-ai-acceleration-computation-scheduling-importance-04a1" id="toc-sec-ai-acceleration-computation-scheduling-importance-04a1" class="nav-link" data-scroll-target="#sec-ai-acceleration-computation-scheduling-importance-04a1">Computation Scheduling Importance</a></li>
  <li><a href="#sec-ai-acceleration-code-generation-85c8" id="toc-sec-ai-acceleration-code-generation-85c8" class="nav-link" data-scroll-target="#sec-ai-acceleration-code-generation-85c8">Code Generation</a></li>
  </ul>
</li>
  <li><a href="#sec-ai-acceleration-compilationruntime-support-0206" id="toc-sec-ai-acceleration-compilationruntime-support-0206" class="nav-link" data-scroll-target="#sec-ai-acceleration-compilationruntime-support-0206">Compilation-Runtime Support</a></li>
  </ul>
</li>
  <li>
<a href="#sec-ai-acceleration-runtime-support-f94f" id="toc-sec-ai-acceleration-runtime-support-f94f" class="nav-link" data-scroll-target="#sec-ai-acceleration-runtime-support-f94f">Runtime Support</a>
  <ul class="collapse">
<li><a href="#sec-ai-acceleration-runtime-architecture-differences-ml-systems-932e" id="toc-sec-ai-acceleration-runtime-architecture-differences-ml-systems-932e" class="nav-link" data-scroll-target="#sec-ai-acceleration-runtime-architecture-differences-ml-systems-932e">Runtime Architecture Differences for ML Systems</a></li>
  <li><a href="#sec-ai-acceleration-dynamic-kernel-execution-33fc" id="toc-sec-ai-acceleration-dynamic-kernel-execution-33fc" class="nav-link" data-scroll-target="#sec-ai-acceleration-dynamic-kernel-execution-33fc">Dynamic Kernel Execution</a></li>
  <li><a href="#sec-ai-acceleration-runtime-kernel-selection-1ffe" id="toc-sec-ai-acceleration-runtime-kernel-selection-1ffe" class="nav-link" data-scroll-target="#sec-ai-acceleration-runtime-kernel-selection-1ffe">Runtime Kernel Selection</a></li>
  <li><a href="#sec-ai-acceleration-kernel-scheduling-utilization-99d6" id="toc-sec-ai-acceleration-kernel-scheduling-utilization-99d6" class="nav-link" data-scroll-target="#sec-ai-acceleration-kernel-scheduling-utilization-99d6">Kernel Scheduling and Utilization</a></li>
  </ul>
</li>
  <li>
<a href="#sec-ai-acceleration-multichip-ai-acceleration-38d7" id="toc-sec-ai-acceleration-multichip-ai-acceleration-38d7" class="nav-link" data-scroll-target="#sec-ai-acceleration-multichip-ai-acceleration-38d7">Multi-Chip AI Acceleration</a>
  <ul class="collapse">
<li><a href="#sec-ai-acceleration-chipletbased-architectures-a890" id="toc-sec-ai-acceleration-chipletbased-architectures-a890" class="nav-link" data-scroll-target="#sec-ai-acceleration-chipletbased-architectures-a890">Chiplet-Based Architectures</a></li>
  <li>
<a href="#sec-ai-acceleration-multigpu-systems-f017" id="toc-sec-ai-acceleration-multigpu-systems-f017" class="nav-link" data-scroll-target="#sec-ai-acceleration-multigpu-systems-f017">Multi-GPU Systems</a>
  <ul class="collapse">
<li><a href="#sec-ai-acceleration-communication-overhead-amdahls-law-analysis-8eb4" id="toc-sec-ai-acceleration-communication-overhead-amdahls-law-analysis-8eb4" class="nav-link" data-scroll-target="#sec-ai-acceleration-communication-overhead-amdahls-law-analysis-8eb4">Communication Overhead and Amdahlâ€™s Law Analysis</a></li>
  </ul>
</li>
  <li><a href="#sec-ai-acceleration-tpu-pods-fd33" id="toc-sec-ai-acceleration-tpu-pods-fd33" class="nav-link" data-scroll-target="#sec-ai-acceleration-tpu-pods-fd33">TPU Pods</a></li>
  <li><a href="#sec-ai-acceleration-waferscale-ai-6420" id="toc-sec-ai-acceleration-waferscale-ai-6420" class="nav-link" data-scroll-target="#sec-ai-acceleration-waferscale-ai-6420">Wafer-Scale AI</a></li>
  <li><a href="#sec-ai-acceleration-ai-systems-scaling-trajectory-ad73" id="toc-sec-ai-acceleration-ai-systems-scaling-trajectory-ad73" class="nav-link" data-scroll-target="#sec-ai-acceleration-ai-systems-scaling-trajectory-ad73">AI Systems Scaling Trajectory</a></li>
  <li>
<a href="#sec-ai-acceleration-computation-memory-scaling-changes-2bb1" id="toc-sec-ai-acceleration-computation-memory-scaling-changes-2bb1" class="nav-link" data-scroll-target="#sec-ai-acceleration-computation-memory-scaling-changes-2bb1">Computation and Memory Scaling Changes</a>
  <ul class="collapse">
<li><a href="#sec-ai-acceleration-multichip-execution-mapping-8ec9" id="toc-sec-ai-acceleration-multichip-execution-mapping-8ec9" class="nav-link" data-scroll-target="#sec-ai-acceleration-multichip-execution-mapping-8ec9">Multi-chip Execution Mapping</a></li>
  <li><a href="#sec-ai-acceleration-distributed-access-memory-allocation-d970" id="toc-sec-ai-acceleration-distributed-access-memory-allocation-d970" class="nav-link" data-scroll-target="#sec-ai-acceleration-distributed-access-memory-allocation-d970">Distributed Access Memory Allocation</a></li>
  <li><a href="#sec-ai-acceleration-data-movement-constraints-a823" id="toc-sec-ai-acceleration-data-movement-constraints-a823" class="nav-link" data-scroll-target="#sec-ai-acceleration-data-movement-constraints-a823">Data Movement Constraints</a></li>
  <li><a href="#sec-ai-acceleration-compilers-runtimes-adaptation-0d70" id="toc-sec-ai-acceleration-compilers-runtimes-adaptation-0d70" class="nav-link" data-scroll-target="#sec-ai-acceleration-compilers-runtimes-adaptation-0d70">Compilers and Runtimes Adaptation</a></li>
  </ul>
</li>
  <li>
<a href="#sec-ai-acceleration-execution-models-adaptation-344b" id="toc-sec-ai-acceleration-execution-models-adaptation-344b" class="nav-link" data-scroll-target="#sec-ai-acceleration-execution-models-adaptation-344b">Execution Models Adaptation</a>
  <ul class="collapse">
<li><a href="#sec-ai-acceleration-crossaccelerator-scheduling-4ac0" id="toc-sec-ai-acceleration-crossaccelerator-scheduling-4ac0" class="nav-link" data-scroll-target="#sec-ai-acceleration-crossaccelerator-scheduling-4ac0">Cross-Accelerator Scheduling</a></li>
  <li><a href="#sec-ai-acceleration-crossaccelerator-coordination-8aa4" id="toc-sec-ai-acceleration-crossaccelerator-coordination-8aa4" class="nav-link" data-scroll-target="#sec-ai-acceleration-crossaccelerator-coordination-8aa4">Cross-Accelerator Coordination</a></li>
  <li><a href="#sec-ai-acceleration-crossaccelerator-execution-management-87a9" id="toc-sec-ai-acceleration-crossaccelerator-execution-management-87a9" class="nav-link" data-scroll-target="#sec-ai-acceleration-crossaccelerator-execution-management-87a9">Cross-Accelerator Execution Management</a></li>
  <li><a href="#sec-ai-acceleration-computation-placement-adaptation-9a3c" id="toc-sec-ai-acceleration-computation-placement-adaptation-9a3c" class="nav-link" data-scroll-target="#sec-ai-acceleration-computation-placement-adaptation-9a3c">Computation Placement Adaptation</a></li>
  </ul>
</li>
  <li><a href="#sec-ai-acceleration-navigating-multichip-ai-complexities-83bd" id="toc-sec-ai-acceleration-navigating-multichip-ai-complexities-83bd" class="nav-link" data-scroll-target="#sec-ai-acceleration-navigating-multichip-ai-complexities-83bd">Navigating Multi-Chip AI Complexities</a></li>
  </ul>
</li>
  <li>
<a href="#sec-ai-acceleration-heterogeneous-soc-ai-acceleration-b1bb" id="toc-sec-ai-acceleration-heterogeneous-soc-ai-acceleration-b1bb" class="nav-link" data-scroll-target="#sec-ai-acceleration-heterogeneous-soc-ai-acceleration-b1bb">Heterogeneous SoC AI Acceleration</a>
  <ul class="collapse">
<li><a href="#sec-ai-acceleration-mobile-soc-architecture-evolution-6ca8" id="toc-sec-ai-acceleration-mobile-soc-architecture-evolution-6ca8" class="nav-link" data-scroll-target="#sec-ai-acceleration-mobile-soc-architecture-evolution-6ca8">Mobile SoC Architecture Evolution</a></li>
  <li><a href="#sec-ai-acceleration-strategies-dynamic-workload-distribution-a421" id="toc-sec-ai-acceleration-strategies-dynamic-workload-distribution-a421" class="nav-link" data-scroll-target="#sec-ai-acceleration-strategies-dynamic-workload-distribution-a421">Strategies for Dynamic Workload Distribution</a></li>
  <li><a href="#sec-ai-acceleration-power-thermal-management-6c00" id="toc-sec-ai-acceleration-power-thermal-management-6c00" class="nav-link" data-scroll-target="#sec-ai-acceleration-power-thermal-management-6c00">Power and Thermal Management</a></li>
  <li><a href="#sec-ai-acceleration-automotive-heterogeneous-ai-systems-deda" id="toc-sec-ai-acceleration-automotive-heterogeneous-ai-systems-deda" class="nav-link" data-scroll-target="#sec-ai-acceleration-automotive-heterogeneous-ai-systems-deda">Automotive Heterogeneous AI Systems</a></li>
  <li><a href="#sec-ai-acceleration-software-stack-challenges-255c" id="toc-sec-ai-acceleration-software-stack-challenges-255c" class="nav-link" data-scroll-target="#sec-ai-acceleration-software-stack-challenges-255c">Software Stack Challenges</a></li>
  </ul>
</li>
  <li><a href="#sec-ai-acceleration-fallacies-pitfalls-dc1f" id="toc-sec-ai-acceleration-fallacies-pitfalls-dc1f" class="nav-link" data-scroll-target="#sec-ai-acceleration-fallacies-pitfalls-dc1f">Fallacies and Pitfalls</a></li>
  <li><a href="#sec-ai-acceleration-summary-a5f8" id="toc-sec-ai-acceleration-summary-a5f8" class="nav-link" data-scroll-target="#sec-ai-acceleration-summary-a5f8">Summary</a></li>
  <li><a href="#self-check-answers" id="toc-self-check-answers" class="nav-link" data-scroll-target="#self-check-answers">Self-Check Answers</a></li>
  </ul>
</li>
  </ul></nav>
    </div>
<!-- main -->
<main class="content page-columns page-full" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../../contents/core/efficient_ai/efficient_ai.html">Performance Engineering</a></li><li class="breadcrumb-item"><a href="../../../contents/core/hw_acceleration/hw_acceleration.html">AI Acceleration</a></li></ol></nav></header><section id="sec-ai-acceleration" class="level1 page-columns page-full"><h1>AI Acceleration</h1>
<div class="{layout-narrow} page-columns page-full">

<div class="no-row-height column-margin column-container"><div class="">
<p><em>DALLÂ·E 3 Prompt: Create an intricate and colorful representation of a System on Chip (SoC) design in a rectangular format. Showcase a variety of specialized machine learning accelerators and chiplets, all integrated into the processor. Provide a detailed view inside the chip, highlighting the rapid movement of electrons. Each accelerator and chiplet should be designed to interact with neural network neurons, layers, and activations, emphasizing their processing speed. Depict the neural networks as a network of interconnected nodes, with vibrant data streams flowing between the accelerator pieces, showcasing the enhanced computation speed.</em></p>
</div></div><p> <img src="images/png/cover_ai_hardware.png" class="img-fluid"></p>
</div>
<section id="purpose" class="level2 unnumbered page-columns page-full"><h2 class="unnumbered anchored" data-anchor-id="purpose">Purpose</h2>
<p><em>What makes specialized hardware acceleration not just beneficial but essential for practical machine learning deployment, and why does this represent a fundamental shift in how we approach computational system design?</em></p>
<p>Practical machine learning systems depend entirely on hardware acceleration. Without specialized processors, computational demands remain economically and physically infeasible. General-purpose CPUs achieve only 100 GFLOPS<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> for neural network operations <span class="citation" data-cites="sze2017efficient">(<a href="#ref-sze2017efficient" role="doc-biblioref">Sze et al. 2017a</a>)</span>, while modern training workloads require trillions of operations per second, creating a performance gap that traditional scaling cannot bridge. Hardware acceleration transforms computationally impossible tasks into practical deployments, enabling entirely new application categories. Engineers working with modern AI systems must understand acceleration principles to harness 100-1000<span class="math inline">\(\times\)</span> performance improvements that make real-time inference, large-scale training, and edge deployment economically viable.</p>
<div class="no-row-height column-margin column-container"><div id="fn1"><p><sup>1</sup>&nbsp;<strong>GFLOPS (Giga Floating-Point Operations Per Second)</strong>: A measure of computational throughput representing one billion floating-point operations per second. TOPS (Tera Operations Per Second) represents one trillion operations per second, typically used for integer operations in AI accelerators.</p></div></div><div class="callout callout-style-default callout-tip callout-titled" title="Learning Objectives">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Learning Objectives
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><p>Trace the evolution of hardware acceleration from floating-point coprocessors to modern AI accelerators and explain the architectural principles driving this progression</p></li>
<li><p>Classify AI compute primitives (vector operations, matrix multiplication, systolic arrays) and analyze their implementation in contemporary accelerators</p></li>
<li><p>Evaluate memory hierarchy designs for AI accelerators and predict their impact on performance bottlenecks using bandwidth and energy consumption metrics</p></li>
<li><p>Design mapping strategies for neural network layers onto specialized hardware architectures, considering dataflow patterns and resource utilization trade-offs</p></li>
<li><p>Apply compiler optimization techniques (graph optimization, kernel fusion, memory planning) to transform high-level ML models into efficient hardware execution plans</p></li>
<li><p>Compare multi-chip scaling approaches (chiplets, multi-GPU, distributed systems) and assess their suitability for different AI workload characteristics</p></li>
<li><p>Critique common misconceptions about hardware acceleration and identify potential pitfalls in accelerator selection and deployment strategies</p></li>
</ul>
</div>
</div>
</section><section id="sec-ai-acceleration-ai-hardware-acceleration-fundamentals-2096" class="level2 page-columns page-full"><h2 class="anchored" data-anchor-id="sec-ai-acceleration-ai-hardware-acceleration-fundamentals-2096">AI Hardware Acceleration Fundamentals</h2>
<p>Modern machine learning systems challenge the architectural assumptions underlying general-purpose processors. While software optimization techniques examined in the preceding chapter provide systematic approaches to algorithmic efficiency through precision reduction, structural pruning, and execution refinements, they operate within the constraints of existing computational substrates. Conventional CPUs achieve utilization rates of merely 5-10% when executing typical machine learning workloads <span class="citation" data-cites="gholami2024ai">(<a href="#ref-gholami2024ai" role="doc-biblioref">Gholami et al. 2024</a>)</span>, due to architectural misalignments between sequential processing models and the highly parallel, data-intensive nature of neural network computations.</p>
<div class="no-row-height column-margin column-container"></div><p>This performance gap has driven a shift toward domain-specific hardware acceleration within computer architecture. Hardware acceleration complements software optimization, addressing efficiency limitations through architectural redesign rather than algorithmic modification. The co-evolution of machine learning algorithms and specialized computing architectures has enabled the transition from computationally prohibitive research conducted on high-performance computing systems to ubiquitous deployment across diverse computing environments, from hyperscale data centers to resource-constrained edge devices.</p>
<p>Hardware acceleration for machine learning systems sits at the intersection of computer systems engineering, computer architecture, and applied machine learning. For practitioners developing production systems, architectural selection decisions regarding accelerator technologies encompassing graphics processing units, tensor processing units, and neuromorphic processors directly determine system-level performance characteristics, energy efficiency profiles, and implementation complexity. Deployed systems in domains such as natural language processing, computer vision, and autonomous systems demonstrate performance improvements spanning two to three orders of magnitude relative to general-purpose implementations.</p>
<p>This chapter examines hardware acceleration principles and methodologies for machine learning systems. The analysis begins with the historical evolution of domain-specific computing architectures, showing how design patterns from floating-point coprocessors to graphics processing units inform contemporary AI acceleration strategies. We then address the computational primitives that characterize machine learning workloads, including matrix multiplication, vector operations, and nonlinear activation functions, and analyze the architectural mechanisms through which specialized hardware optimizes these operations via innovations such as systolic array architectures and tensor processing cores.</p>
<p>Memory hierarchy design plays a critical role in acceleration effectiveness, given that data movement energy costs typically exceed computational energy by more than two orders of magnitude. This analysis covers memory architecture design principles, from on-chip SRAM buffer optimization to high-bandwidth memory interfaces, and examines approaches to minimizing energy-intensive data movement patterns. We also address compiler optimization and runtime system support, which determine the extent to which theoretical hardware capabilities translate into measurable system performance.</p>
<p>The chapter concludes with scaling methodologies for systems requiring computational capacity beyond single-chip implementations. Multi-chip architectures, ranging from chiplet-based integration to distributed warehouse-scale systems, introduce trade-offs between computational parallelism and inter-chip communication overhead. Through detailed analysis of contemporary systems including NVIDIA GPU architectures, Google Tensor Processing Units, and emerging neuromorphic computing platforms, we establish the theoretical foundations and practical considerations necessary for effective deployment of AI acceleration across diverse system contexts.</p>
<div id="quiz-question-sec-ai-acceleration-ai-hardware-acceleration-fundamentals-2096" class="callout callout-quiz-question">
<details class="callout-quiz-question fbx-default closebutton"><summary><strong>Self-Check: Question 1.1</strong></summary><div>
<ol type="1">
<li>
<p>What is the primary reason for the shift from general-purpose processors to domain-specific hardware in machine learning systems?</p>
<ol type="a">
<li>To reduce the cost of hardware components</li>
<li>To improve the parallel processing capabilities and efficiency</li>
<li>To simplify the design of machine learning algorithms</li>
<li>To increase the utilization of existing software optimizations</li>
</ol>
</li>
<li><p>True or False: Hardware acceleration in machine learning systems only focuses on improving computational speed, not energy efficiency.</p></li>
<li><p>How do architectural selection decisions impact system-level performance in machine learning systems?</p></li>
<li>
<p>Which of the following architectural innovations is used to optimize matrix multiplication in machine learning workloads?</p>
<ol type="a">
<li>Floating-point coprocessors</li>
<li>Sequential processing models</li>
<li>Systolic array architectures</li>
<li>High-bandwidth memory interfaces</li>
</ol>
</li>
<li><p>In a production system, what trade-offs might you consider when choosing between single-chip and multi-chip architectures for AI acceleration?</p></li>
</ol>
<p><a href="#quiz-answer-sec-ai-acceleration-ai-hardware-acceleration-fundamentals-2096" class="question-label">See Answers â†’</a></p>
</div></details>
</div>
</section><section id="sec-ai-acceleration-evolution-hardware-specialization-1d21" class="level2 page-columns page-full"><h2 class="anchored" data-anchor-id="sec-ai-acceleration-evolution-hardware-specialization-1d21">Evolution of Hardware Specialization</h2>
<p>Computing architectures follow a recurring pattern: as computational workloads grow in complexity, general-purpose processors become increasingly inefficient, prompting the development of specialized hardware accelerators. The need for higher computational efficiency, reduced energy consumption, and optimized execution of domain-specific workloads drives this transition. Machine learning acceleration represents the latest stage in this ongoing evolution, following a trajectory observed in prior domains such as floating-point arithmetic, graphics processing, and digital signal processing.</p>
<p>This evolutionary progression provides context for understanding how modern ML accelerators including GPUs with tensor cores (specialized units that accelerate matrix operations), Googleâ€™s TPUs<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a>, and Appleâ€™s Neural Engine emerged from established architectural principles. These technologies enable widely deployed applications such as real-time language translation, image recognition, and personalized recommendations. The architectural strategies enabling such capabilities derive from decades of hardware specialization research and development.</p>
<div class="no-row-height column-margin column-container"><div id="fn2"><p><sup>2</sup>&nbsp;<strong>TPU Origins</strong>: Google secretly developed the Tensor Processing Unit (TPU) starting in 2013 when they realized CPUs couldnâ€™t handle the computational demands of their neural networks. The TPUv1, deployed in 2015, delivered 15-30<span class="math inline">\(\times\)</span> better performance per watt than contemporary GPUs for inference. This breakthrough significantly changed how the industry approached AI hardware, proving that domain-specific architectures could dramatically outperform general-purpose processors for neural network workloads.</p></div></div><p>Hardware specialization forms the foundation of this transition, enhancing performance and efficiency by optimizing frequently executed computational patterns through dedicated circuit implementations. While this approach yields significant gains, it introduces trade-offs in flexibility, silicon area utilization, and programming complexity. As computing demands continue to evolve, specialized accelerators must balance these factors to deliver sustained improvements in efficiency and performance.</p>
<p>The evolution of hardware specialization provides perspective for understanding modern machine learning accelerators. Many principles that shaped the development of early floating-point and graphics accelerators now inform the design of AI-specific hardware. Examining these past trends offers a framework for analyzing contemporary approaches to AI acceleration and anticipating future developments in specialized computing.</p>
<section id="sec-ai-acceleration-specialized-computing-1a77" class="level3 page-columns page-full"><h3 class="anchored" data-anchor-id="sec-ai-acceleration-specialized-computing-1a77">Specialized Computing</h3>
<p>The transition toward specialized computing architectures stems from the limitations of general-purpose processors. Early computing systems relied on central processing units (CPUs) to execute all computational tasks sequentially, following a one-size-fits-all approach. As computing workloads diversified and grew in complexity, certain operations, especially floating-point arithmetic, emerged as performance bottlenecks that could not be efficiently handled by CPUs alone. These inefficiencies prompted the development of specialized hardware architectures designed to accelerate specific computational patterns <span class="citation" data-cites="flynn1966very">(<a href="#ref-flynn1966very" role="doc-biblioref">Flynn 1966</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-flynn1966very" class="csl-entry" role="listitem">
Flynn, M. J. 1966. <span>â€œVery High-Speed Computing Systems.â€</span> <em>Proceedings of the IEEE</em> 54 (12): 1901â€“9. <a href="https://doi.org/10.1109/proc.1966.5273">https://doi.org/10.1109/proc.1966.5273</a>.
</div><div id="fn3"><p><sup>3</sup>&nbsp;<strong>Intel 8087 Impact</strong>: The 8087 coprocessor cost hundreds of dollars (up to $700-795 according to various accounts, about $2,100-2,400 today) but transformed scientific computing. CAD workstations that took hours for complex calculations could complete them in minutes. This success created the entire coprocessor market and established the economic model for specialized hardware that persists today: charge premium prices for dramatic performance improvements in specific domains.</p></div><div id="ref-fisher_8087_1981" class="csl-entry" role="listitem">
Fisher, Lawrence D. 1981. <span>â€œThe 8087 Numeric Data Processor.â€</span> <em>IEEE Computer</em> 14 (7): 19â€“29. <a href="https://doi.org/10.1109/MC.1981.1653991">https://doi.org/10.1109/MC.1981.1653991</a>.
</div></div><p>One of the earliest examples of hardware specialization was the Intel 8087 mathematics coprocessor<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a>, introduced in 1980. This floating-point unit (FPU) was designed to offload arithmetic-intensive computations from the main CPU, dramatically improving performance for scientific and engineering applications. The 8087 demonstrated unprecedented efficiency, achieving performance gains of up to 100Ã— for floating-point operations compared to software-based implementations on general-purpose processors <span class="citation" data-cites="fisher_8087_1981">(<a href="#ref-fisher_8087_1981" role="doc-biblioref">Fisher 1981</a>)</span>. This milestone established a principle in computer architecture: carefully designed hardware specialization could provide order-of-magnitude improvements for well-defined, computationally intensive tasks.</p>
<p>The success of floating-point coprocessors<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a> led to their eventual integration into mainstream processors. The Intel 486DX, released in 1989, incorporated an on-chip floating-point unit, eliminating the requirement for an external coprocessor. This integration improved processing efficiency and established a recurring pattern in computer architecture: successful specialized functions become standard features in subsequent generations of general-purpose processors <span class="citation" data-cites="patterson2021computer">(<a href="#ref-patterson2021computer" role="doc-biblioref">Patterson and Hennessy 2021</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="fn4"><p><sup>4</sup>&nbsp;<strong>Coprocessor</strong>: A specialized secondary processor designed to handle specific tasks that the main CPU performs poorly. The 8087 math coprocessor was the first successful example, followed by graphics coprocessors (GPUs) and network processors. Modern â€œacceleratorsâ€ are essentially evolved coprocessors. The term changed as these chips became more powerful than host CPUs for their target workloads. Todayâ€™s AI accelerators follow the same pattern but often eclipse CPU performance.</p></div><div id="ref-patterson2021computer" class="csl-entry" role="listitem">
Patterson, David A, and John L. Hennessy. 2021. <em>Computer Organization and Design: The Hardware/Software Interface</em>. 5th ed. Morgan Kaufmann.
</div></div><p>Early floating-point acceleration established principles that continue to influence modern hardware specialization:</p>
<ol type="1">
<li>Identification of computational bottlenecks through workload analysis</li>
<li>Development of specialized circuits for frequent operations</li>
<li>Creation of efficient hardware-software interfaces</li>
<li>Progressive integration of proven specialized functions</li>
</ol>
<p>This progression from domain-specific specialization to general-purpose integration has shaped modern computing architectures. As computational workloads expanded beyond arithmetic operations, these core principles were applied to new domains, such as graphics processing, digital signal processing, and ultimately, machine learning acceleration. Each domain introduced specialized architectures tailored to their unique computational requirements, establishing hardware specialization as an approach for advancing computing performance and efficiency in increasingly complex workloads.</p>
<p>The evolution of specialized computing hardware follows a consistent trajectory, wherein architectural innovations are introduced to address emerging computational bottlenecks and are subsequently incorporated into mainstream computing platforms. As illustrated in <a href="#fig-timeline" class="quarto-xref">Figure&nbsp;1</a>, each computing era produced accelerators that addressed the dominant workload characteristics of the period. These developments have advanced architectural efficiency and shaped the foundation upon which contemporary machine learning systems operate. The computational capabilities required for tasks such as real-time language translation, personalized recommendations, and on-device inference depend on foundational principles and architectural innovations established in earlier domains, including floating-point computation, graphics processing, and digital signal processing.</p>
<div id="fig-timeline" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-pos="htb" data-fig-env="figure">
<figure class="quarto-float quarto-float-fig figure"><div aria-describedby="fig-timeline-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="284c5b21bbcb3bac7a47ffbf3fc71aecca5b7daa.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="Figure&nbsp;1: Hardware Specialization Trajectory: Computing architectures progressively incorporate specialized accelerators to address emerging performance bottlenecks and workload demands, mirroring a historical pattern from floating-point units to graphics processors and, ultimately, machine learning accelerators. This evolution reflects a strategy for improving computational efficiency by tailoring hardware to specific task characteristics and advancing increasingly complex applications."><img src="hw_acceleration_files/mediabag/284c5b21bbcb3bac7a47ffbf3fc71aecca5b7daa.svg" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-timeline-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: <strong>Hardware Specialization Trajectory</strong>: Computing architectures progressively incorporate specialized accelerators to address emerging performance bottlenecks and workload demands, mirroring a historical pattern from floating-point units to graphics processors and, ultimately, machine learning accelerators. This evolution reflects a strategy for improving computational efficiency by tailoring hardware to specific task characteristics and advancing increasingly complex applications.
</figcaption></figure>
</div>
</section><section id="sec-ai-acceleration-parallel-computing-graphics-processing-66b1" class="level3 page-columns page-full"><h3 class="anchored" data-anchor-id="sec-ai-acceleration-parallel-computing-graphics-processing-66b1">Parallel Computing and Graphics Processing</h3>
<p>The principles established through floating-point acceleration provided a blueprint for addressing emerging computational challenges. As computing applications diversified, new computational patterns emerged that exceeded the capabilities of general-purpose processors. This expansion of specialized computing manifested across multiple domains, each contributing unique insights to hardware acceleration strategies.</p>
<p>Graphics processing emerged as a primary driver of hardware specialization in the 1990s. Early graphics accelerators focused on specific operations like bitmap transfers and polygon filling. The introduction of programmable graphics pipelines with NVIDIAâ€™s GeForce 256 in 1999 represented a significant advancement in specialized computing. Graphics Processing Units (GPUs) demonstrated how parallel processing architectures could efficiently handle data-parallel workloads, achieving 50-100<span class="math inline">\(\times\)</span> speedups in 3D rendering tasks like texture mapping and vertex transformation. By 2004, high-end GPUs could process over 100 million polygons per second <span class="citation" data-cites="owens2008gpu">(<a href="#ref-owens2008gpu" role="doc-biblioref">Owens et al. 2008</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-lyons2011understanding" class="csl-entry" role="listitem">
Lyons, Richard G. 2011. <em>Understanding Digital Signal Processing</em>. 3rd ed. Prentice Hall.
</div></div><p>Concurrently, Digital Signal Processing (DSP) processors established parallel data path architectures with specialized multiply-accumulate units and circular buffers optimized for filtering and transform operations. Texas Instrumentsâ€™ TMS32010 (1983) demonstrated how domain-specific instruction sets could dramatically improve performance for signal processing applications <span class="citation" data-cites="lyons2011understanding">(<a href="#ref-lyons2011understanding" role="doc-biblioref">Lyons 2011</a>)</span>.</p>
<p>Network processing introduced additional patterns of specialization. Network processors developed unique architectures to handle packet processing at line rate, incorporating multiple processing cores, specialized packet manipulation units, and sophisticated memory management systems. Intelâ€™s IXP2800 network processor demonstrated how multiple levels of hardware specialization could be combined to address complex processing requirements.</p>
<p>These diverse domains of specialization exhibit several common characteristics:</p>
<ol type="1">
<li>Identification of domain-specific computational patterns</li>
<li>Development of specialized processing elements and memory hierarchies</li>
<li>Creation of domain-specific programming models</li>
<li>Progressive evolution toward more flexible architectures</li>
</ol>
<p>This period of expanding specialization demonstrated that hardware acceleration strategies could address diverse computational requirements across multiple domains. The GPUâ€™s success in parallelizing 3D graphics pipelines enabled its subsequent adoption for training deep neural networks, exemplified by AlexNet<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a> in 2012, which executed on consumer-grade NVIDIA GPUs. DSP innovations in low-power signal processing facilitated real-time inference on edge devices, including voice assistants and wearables. These domains informed ML hardware designs and established that accelerators could be deployed across both cloud and embedded contexts, principles that continue to influence contemporary AI ecosystem development.</p>
<div class="no-row-height column-margin column-container"><div id="fn5"><p><sup>5</sup>&nbsp;<strong>AlexNetâ€™s GPU Revolution</strong>: AlexNetâ€™s breakthrough wasnâ€™t just algorithmic. It proved GPUs could train deep networks 10<span class="math inline">\(\times\)</span> faster than CPUs <span class="citation" data-cites="krizhevsky2012alexnet">(<a href="#ref-krizhevsky2012alexnet" role="doc-biblioref">Krizhevsky, Sutskever, and Hinton 2017</a>)</span>. The team split the 8-layer network across two NVIDIA GTX 580s (512 cores each), reducing training time from weeks to days. This success triggered the â€œdeep learning gold rushâ€ and established NVIDIA as the default AI hardware company, with GPU sales for data centers growing from $200 million to $47 billion by 2024. Modern GPUs like the NVIDIA H100 contains 16,896 streaming processors, demonstrating the massive scaling in parallel processing capability since AlexNetâ€™s era.</p><div id="ref-krizhevsky2012alexnet" class="csl-entry" role="listitem">
Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E. Hinton. 2017. <span>â€œImageNet Classification with Deep Convolutional Neural Networks.â€</span> <em>Communications of the ACM</em> 60 (6): 84â€“90. <a href="https://doi.org/10.1145/3065386">https://doi.org/10.1145/3065386</a>.
</div></div></div></section><section id="sec-ai-acceleration-emergence-domainspecific-architectures-e045" class="level3 page-columns page-full"><h3 class="anchored" data-anchor-id="sec-ai-acceleration-emergence-domainspecific-architectures-e045">Emergence of Domain-Specific Architectures</h3>
<p>The emergence of domain-specific architectures (DSA)<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a> marks a shift in computer system design, driven by two factors: the breakdown of traditional scaling laws and the increasing computational demands of specialized workloads. The slowdown of Mooreâ€™s Law<a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a>, which previously ensured predictable enhancements in transistor density every 18 to 24 months, and the end of Dennard scaling<a href="#fn8" class="footnote-ref" id="fnref8" role="doc-noteref"><sup>8</sup></a>, which permitted frequency increases without corresponding power increases, created a performance and efficiency bottleneck in general-purpose computing. As John Hennessy and David Patterson noted in their 2017 Turing Lecture <span class="citation" data-cites="HennessyPatterson2017Turing">(<a href="#ref-HennessyPatterson2017Turing" role="doc-biblioref">Hennessy and Patterson 2019</a>)</span>, these limitations signaled the onset of a new era in computer architecture, one centered on domain-specific solutions that optimize hardware for specialized workloads.</p>
<div class="no-row-height column-margin column-container"><div id="fn6"><p><sup>6</sup>&nbsp;<strong>Domain-Specific Architectures (DSA)</strong>: Computing architectures optimized for specific application domains rather than general-purpose computation. Unlike CPUs designed for flexibility, DSAs sacrifice programmability for dramatic efficiency gains. Googleâ€™s TPU achieves 15-30<span class="math inline">\(\times\)</span> better performance per watt than GPUs for neural networks, while video codecs provide 100-1000<span class="math inline">\(\times\)</span> improvements over software decoding. The 2018 Turing Award recognized this shift as the defining trend in modern computer architecture.</p></div><div id="fn7"><p><sup>7</sup>&nbsp;<strong>Mooreâ€™s Law</strong>: Intel co-founder Gordon Mooreâ€™s 1965 observation that transistor density doubles every 18-24 months. This exponential scaling drove computing progress for 50 years, enabling everything from smartphones to supercomputers. However, physical limits around 2005 slowed this pace dramatically. Modern 3&nbsp;nm chips cost $20 billion to develop versus $3 million in 1999, forcing the industry toward specialized architectures.</p></div><div id="fn8"><p><sup>8</sup>&nbsp;<strong>Dennard Scaling</strong>: Robert Dennardâ€™s 1974 principle that as transistors shrink, their power density remains constant, allowing higher frequencies without increased power consumption. This enabled CPUs to reach 3+ GHz by 2005. However, quantum effects and leakage current ended Dennard scaling around 2005, forcing architects to prioritize efficiency over raw speed and leading to the multi-core revolution.</p></div><div id="ref-HennessyPatterson2017Turing" class="csl-entry" role="listitem">
Hennessy, John L., and David A. Patterson. 2019. <span>â€œA New Golden Age for Computer Architecture.â€</span> <em>Communications of the ACM</em> 62 (2): 48â€“60. <a href="https://doi.org/10.1145/3282307">https://doi.org/10.1145/3282307</a>.
</div></div><p>Historically, improvements in processor performance depended on semiconductor process scaling and increasing clock speeds. However, as power density limitations restricted further frequency scaling, and as transistor miniaturization encountered increasing physical and economic constraints, architects explored alternative approaches to sustain computational growth. This resulted in a shift toward domain-specific architectures, which dedicate silicon resources to optimize computation for specific application domains, trading flexibility for efficiency.</p>
<p>Domain-specific architectures achieve superior performance and energy efficiency through several key principles:</p>
<ol type="1">
<li><p><strong>Customized data paths</strong>: Design processing paths specifically optimized for target application patterns, enabling direct hardware execution of common operations. For example, matrix multiplication units in AI accelerators implement systolic arraysâ€”grid-like networks of processing elements that rhythmically compute and pass data through neighboring unitsâ€”tailored for neural network computations.</p></li>
<li><p><strong>Specialized memory hierarchies</strong>: Optimize memory systems around domain-specific access patterns and data reuse characteristics. This includes custom cache configurations, prefetching logic, and memory controllers tuned for expected workloads.</p></li>
<li><p><strong>Reduced instruction overhead</strong>: Implement domain-specific instruction sets that minimize decode and dispatch complexity by encoding common operation sequences into single instructions. This improves both performance and energy efficiency.</p></li>
<li><p><strong>Direct hardware implementation</strong>: Create dedicated circuit blocks that natively execute frequently used operations without software intervention. This eliminates instruction processing overhead and maximizes throughput.</p></li>
</ol>
<p>These principles achieve compelling demonstration in modern smartphones. Modern smartphones can decode 4K video at 60 frames per second while consuming only a few watts of power, despite video processing requiring billions of operations per second. This efficiency is achieved through dedicated hardware video codecs that implement industry standards such as H.264/AVC (introduced in 2003) and H.265/HEVC (finalized in 2013) <span class="citation" data-cites="sullivan2012overview">(<a href="#ref-sullivan2012overview" role="doc-biblioref">Sullivan et al. 2012</a>)</span>. These specialized circuits provide 100â€“1000<span class="math inline">\(\times\)</span> improvements in both performance and power efficiency compared to software-based decoding on general-purpose processors.</p>
<div class="no-row-height column-margin column-container"><div id="ref-sullivan2012overview" class="csl-entry" role="listitem">
Sullivan, Gary J., Jens-Rainer Ohm, Woo-Jin Han, and Thomas Wiegand. 2012. <span>â€œOverview of the High Efficiency Video Coding (HEVC) Standard.â€</span> <em>IEEE Transactions on Circuits and Systems for Video Technology</em> 22 (12): 1649â€“68. <a href="https://doi.org/10.1109/tcsvt.2012.2221191">https://doi.org/10.1109/tcsvt.2012.2221191</a>.
</div><div id="ref-Shang2018GenomicsAccel" class="csl-entry" role="listitem">
Shang, J., G. Wang, and Y. Liu. 2018. <span>â€œAccelerating Genomic Data Analysis with Domain-Specific Architectures.â€</span> <em>IEEE Transactions on Computers</em> 67 (7): 965â€“78. <a href="https://doi.org/10.1109/TC.2018.2799212">https://doi.org/10.1109/TC.2018.2799212</a>.
</div><div id="fn9"><p><sup>9</sup>&nbsp;<strong>Application-Specific Integrated Circuits (ASICs)</strong>: Custom silicon chips designed for a single application, offering maximum efficiency by eliminating unused features. Bitcoin mining ASICs achieve 100,000<span class="math inline">\(\times\)</span> better energy efficiency than CPUs for SHA-256 hashing. However, their inflexibility means they become worthless if algorithms change. An estimated $5 billion in Ethereum mining ASICs became obsolete when Ethereum switched to proof-of-stake in September 2022.</p></div><div id="ref-Taylor2017ASICMining" class="csl-entry" role="listitem">
Bedford Taylor, Michael. 2017. <span>â€œThe Evolution of Bitcoin Hardware.â€</span> <em>Computer</em> 50 (9): 58â€“66. <a href="https://doi.org/10.1109/mc.2017.3571056">https://doi.org/10.1109/mc.2017.3571056</a>.
</div></div><p>The trend toward specialization continues to accelerate, with new architectures emerging for an expanding range of domains. Genomics processing benefits from custom accelerators that optimize sequence alignment and variant calling, reducing the time required for DNA analysis <span class="citation" data-cites="Shang2018GenomicsAccel">(<a href="#ref-Shang2018GenomicsAccel" role="doc-biblioref">Shang, Wang, and Liu 2018</a>)</span>. Similarly, blockchain computation has produced application-specific integrated circuits (ASICs)<a href="#fn9" class="footnote-ref" id="fnref9" role="doc-noteref"><sup>9</sup></a> optimized for cryptographic hashing, substantially increasing the efficiency of mining operations <span class="citation" data-cites="Taylor2017ASICMining">(<a href="#ref-Taylor2017ASICMining" role="doc-biblioref">Bedford Taylor 2017</a>)</span>. These examples demonstrate that domain-specific architecture represents a fundamental transformation in computing systems, offering tailored solutions that address the growing complexity and diversity of modern computational workloads.</p>
</section><section id="sec-ai-acceleration-machine-learning-hardware-specialization-a17f" class="level3 page-columns page-full"><h3 class="anchored" data-anchor-id="sec-ai-acceleration-machine-learning-hardware-specialization-a17f">Machine Learning Hardware Specialization</h3>
<p>Machine learning constitutes a computational domain with unique characteristics that have driven the development of specialized hardware architectures. Unlike traditional computing workloads that exhibit irregular memory access patterns and diverse instruction streams, neural networks are characterized by predictable patterns: dense matrix multiplications, regular data flow, and tolerance for reduced precision. These characteristics enable specialized hardware optimizations that would be ineffective for general-purpose computing but provide substantial speedups for ML workloads.</p>
<div id="callout-definition*-1.1" class="callout callout-definition" title="ML Accelerator">
<p></p><details class="callout-definition fbx-default closebutton" open=""><summary><strong>Definition: </strong>ML Accelerator</summary><div>
<strong><em>Machine Learning Accelerators</em></strong> are specialized computing hardware optimized for the <em>computational patterns</em> of neural networks, achieving superior <em>performance per watt</em> through <em>parallel processing</em>, <em>specialized memory hierarchies</em>, and <em>reduced-precision arithmetic</em>.
</div></details><p></p>
</div>
<p>Machine learning computational requirements reveal limitations in traditional processors. CPUs achieve only 5-10% utilization on neural network workloads, delivering approximately 100 GFLOPS<a href="#fn10" class="footnote-ref" id="fnref10" role="doc-noteref"><sup>10</sup></a> while consuming hundreds of watts. This inefficiency results from architectural mismatches: CPUs optimize for single-thread performance and irregular memory access, while neural networks require massive parallelism and predictable data streams. The memory bandwidth<a href="#fn11" class="footnote-ref" id="fnref11" role="doc-noteref"><sup>11</sup></a> constraint becomes particularly severe: a single neural network layer may require accessing gigabytes of parameters, overwhelming CPU cache hierarchies<a href="#fn12" class="footnote-ref" id="fnref12" role="doc-noteref"><sup>12</sup></a> designed for kilobyte-scale working sets.</p>
<div class="no-row-height column-margin column-container"><div id="fn10"><p><sup>10</sup>&nbsp;<strong>GFLOPS (Giga Floating-Point Operations Per Second)</strong>: A measure of computational throughput representing one billion floating-point operations per second. TOPS (Tera Operations Per Second) represents one trillion operations per second, typically used for integer operations in AI accelerators.</p></div><div id="fn11"><p><sup>11</sup>&nbsp;<strong>Memory Bandwidth</strong>: The rate at which data can be transferred between memory and processors, measured in GB/s or TB/s. AI workloads are often bandwidth-bound rather than compute-bound. NVIDIA H100 provides 3.35 TB/s (approximately 40<span class="math inline">\(\times\)</span> faster than typical DDR5-4800 configurations at ~80 GB/s) because neural networks require constant weight access, making memory bandwidth the primary bottleneck in many AI applications.</p></div><div id="fn12"><p><sup>12</sup>&nbsp;<strong>Cache Hierarchy</strong>: Multi-level memory system with L1, L2, and L3 caches providing progressively larger capacity but higher latency. CPUs optimize for 32-64KB L1 caches with &lt;1ns access time, but neural networks need gigabytes of weights that cannot fit in cache, causing frequent expensive DRAM accesses (100ns latency) and degrading performance from 90%+ cache hit rates to &lt;10%.</p></div></div><p>The energy economics of data movement influence accelerator design. Accessing data from DRAM requires approximately 640 picojoules while performing a multiply-accumulate operation consumes only 3.7&nbsp;pJ, approximately a 173Ã— penalty (specific values vary by technology node and design) that establishes minimizing data movement as the primary optimization target. This disparity explains the progression from repurposed graphics processors to purpose-built neural network accelerators. GPUs achieve 15,000+ GFLOPS through massive parallelism but encounter efficiency challenges from their graphics heritage. TPUs and other custom accelerators achieve utilization above 85% by implementing systolic arrays and other architectures that maximize data reuse while minimizing movement.</p>
<p>Training and inference present distinct computational profiles that influence accelerator design. Training requires high-precision arithmetic (FP32 or FP16) for gradient computation and weight updates, bidirectional data flow for backpropagation<a href="#fn13" class="footnote-ref" id="fnref13" role="doc-noteref"><sup>13</sup></a>, and large memory capacity for storing activations. Inference can exploit reduced precision (INT8 or INT4), requires only forward computation, and prioritizes latency over throughput<a href="#fn14" class="footnote-ref" id="fnref14" role="doc-noteref"><sup>14</sup></a>. These differences drive specialized architectures: training accelerators maximize FLOPS and memory bandwidth, while inference accelerators optimize for energy efficiency and deterministic latency.</p>
<div class="no-row-height column-margin column-container"><div id="fn13"><p><sup>13</sup>&nbsp;<strong>Backpropagation</strong>: The key training algorithm that computes gradients by propagating errors backwards through the network using the chain rule. Unlike forward inference which only needs current layer outputs, backpropagation requires storing all intermediate activations from forward pass, increasing memory requirements 2-3<span class="math inline">\(\times\)</span> and necessitating bidirectional data flow that complicates accelerator design.</p></div><div id="fn14"><p><sup>14</sup>&nbsp;<strong>Latency vs Throughput</strong>: Latency measures response time for a single request (milliseconds), while throughput measures requests processed per unit time (requests/second). Training optimizes throughput to process large batches efficiently, while inference prioritizes latency for real-time responses. A GPU might achieve 1000 images/second (high throughput) but take 50ms per image (high latency), making it unsuitable for real-time applications requiring &lt;10ms response times.</p></div></div><p>Deployment context shapes architectural choices. Datacenter accelerators accept 700-watt power budgets to maximize throughput for training massive models. Edge devices must deliver real-time inference within milliwatt constraints, driving architectures that eliminate every unnecessary data movement. Mobile processors balance performance with battery life, while automotive systems prioritize deterministic response times for safety-critical applications. This diversity has produced a rich ecosystem of specialized accelerators, each optimized for specific deployment scenarios and computational requirements.</p>
<p>In data centers, training accelerators such as NVIDIA H100 and Google TPUv4 reduce model development from weeks to days through massive parallelism and high-bandwidth memory systems. These systems prioritize raw computational throughput, accepting 700-watt power consumption to achieve petaflop-scale performance. The economics support this trade-offâ€”reducing training time from months to days can reduce millions in operational costs and accelerate time-to-market for AI applications.</p>
<p>At the opposite extreme, edge deployment requires different optimization strategies. Processing-in-memory architectures eliminate data movement by integrating compute directly with memory. Dynamic voltage scaling reduces power by 50-90% during low-intensity operations. Neuromorphic designs process only changing inputs, achieving 1000Ã— power reduction for temporal workloads. These techniques enable sophisticated AI models to operate continuously on battery power, supporting applications from smartphone photography to autonomous sensors that function for years without external power.</p>
<p>The success of application-specific accelerators demonstrates that no single architecture can efficiently address all ML workloads. The 156 billion edge devices projected by 2030 will require architectures optimized for energy efficiency and real-time guarantees, while cloud-scale training will continue advancing the boundaries of computational throughput. This diversity drives continued innovation in specialized architectures, each optimized for its specific deployment context and computational requirements.</p>
<p>The evolution of specialized hardware architectures illustrates a principle in computing systems: as computational patterns emerge and mature, hardware specialization follows to achieve optimal performance and energy efficiency. This progression appears clearly in machine learning acceleration, where domain-specific architectures have evolved to meet the increasing computational demands of machine learning models. Unlike general-purpose processors, which prioritize flexibility, specialized accelerators optimize execution for well-defined workloads, balancing performance, energy efficiency, and integration with software frameworks.</p>
<p><a href="#tbl-hw-evolution" class="quarto-xref">Table&nbsp;1</a> summarizes key milestones in the evolution of hardware specialization, showing how each era produced architectures tailored to the prevailing computational demands. While these accelerators initially emerged to optimize domain-specific workloads, including floating-point operations, graphics rendering, and media processing, they also introduced architectural strategies that persist in contemporary systems. The specialization principles outlined in earlier generations now underpin the design of modern AI accelerators. Understanding this historical trajectory provides context for analyzing how hardware specialization continues to enable scalable, efficient execution of machine learning workloads across diverse deployment environments.</p>
<div id="tbl-hw-evolution" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure"><figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-hw-evolution-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;1: <strong>Hardware Specialization Trends</strong>: Successive computing eras progressively integrate specialized hardware to accelerate prevalent workloads, moving from general-purpose CPUs to domain-specific architectures and ultimately to customizable AI accelerators. This evolution reflects a fundamental principle: tailoring hardware to computational patterns improves performance and energy efficiency, driving innovation in machine learning systems.
</figcaption><div aria-describedby="tbl-hw-evolution-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<colgroup>
<col style="width: 8%">
<col style="width: 25%">
<col style="width: 31%">
<col style="width: 32%">
</colgroup>
<thead><tr class="header">
<th style="text-align: right;"><strong>Era</strong></th>
<th style="text-align: left;"><strong>Computational Pattern</strong></th>
<th style="text-align: left;"><strong>Architecture Examples</strong></th>
<th style="text-align: left;"><strong>Characteristics</strong></th>
</tr></thead>
<tbody>
<tr class="odd">
<td style="text-align: right;"><strong>1980s</strong></td>
<td style="text-align: left;">Floating-Point &amp; Signal Processing</td>
<td style="text-align: left;">FPU, DSP</td>
<td style="text-align: left;"><ul>
<li>
Single-purpose engines
</li>
<li>
Focused instruction sets
</li>
<li>
Coprocessor interfaces
</li>
</ul></td>
</tr>
<tr class="even">
<td style="text-align: right;"><strong>1990s</strong></td>
<td style="text-align: left;">3D Graphics &amp; Multimedia</td>
<td style="text-align: left;">GPU, SIMD Units</td>
<td style="text-align: left;"><ul>
<li>
Many identical compute units
</li>
<li>
Regular data patterns
</li>
<li>
Wide memory interfaces
</li>
</ul></td>
</tr>
<tr class="odd">
<td style="text-align: right;"><strong>2000s</strong></td>
<td style="text-align: left;">Real-time Media Coding</td>
<td style="text-align: left;">Media Codecs, Network Processors</td>
<td style="text-align: left;"><ul>
<li>
Fixed-function pipelines
</li>
<li>
High throughput processing
</li>
<li>
Power-performance optimization
</li>
</ul></td>
</tr>
<tr class="even">
<td style="text-align: right;"><strong>2010s</strong></td>
<td style="text-align: left;">Deep Learning Tensor Operations</td>
<td style="text-align: left;">TPU, GPU Tensor Cores</td>
<td style="text-align: left;"><ul>
<li>
Matrix multiplication units
</li>
<li>
Massive parallelism
</li>
<li>
Memory bandwidth optimization
</li>
</ul></td>
</tr>
<tr class="odd">
<td style="text-align: right;"><strong>2020s</strong></td>
<td style="text-align: left;">Application-Specific Acceleration</td>
<td style="text-align: left;">ML Engines, Smart NICs, Domain Accelerators</td>
<td style="text-align: left;"><ul>
<li>
Workload-specific datapaths
</li>
<li>
Customized memory hierarchies
</li>
<li>
Application-optimized designs
</li>
</ul></td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p>This historical progression reveals a recurring pattern: each wave of hardware specialization responded to a computational bottleneck, whether graphics rendering, media encoding, or neural network inference. What distinguishes the 2020s is not just specialization, but its pervasiveness: AI accelerators now underpin everything from product recommendations on YouTube to object detection in autonomous vehicles. Unlike earlier accelerators, todayâ€™s AI hardware must integrate tightly with dynamic software frameworks and scale across cloud-to-edge deployments. The table illustrates not just the past but also the trajectory toward increasingly tailored, high-impact computing platforms.</p>
<p>For AI acceleration, this transition has introduced challenges that extend well beyond hardware design. Machine learning accelerators must integrate seamlessly into ML workflows by aligning with optimizations at multiple levels of the computing stack. They must operate effectively with widely adopted frameworks such as TensorFlow, PyTorch, and JAX, ensuring that deployment is smooth and consistent across varied hardware platforms. Compiler and runtime support become necessary; advanced optimization techniques, such as graph-level transformations, kernel fusion, and memory scheduling, are critical for using the full potential of these specialized accelerators.</p>
<p>Scalability drives additional complexity as AI accelerators deploy across diverse environments from high-throughput data centers to resource-constrained edge and mobile devices, requiring tailored performance tuning and energy efficiency strategies. Integration into heterogeneous computing<a href="#fn15" class="footnote-ref" id="fnref15" role="doc-noteref"><sup>15</sup></a> environments demands interoperability that enables specialized units to coordinate effectively with conventional CPUs and GPUs in distributed systems.</p>
<div class="no-row-height column-margin column-container"><div id="fn15"><p><sup>15</sup>&nbsp;<strong>Heterogeneous Computing</strong>: Computing systems that combine different types of processors (CPUs, GPUs, TPUs, FPGAs) to optimize performance for diverse workloads. Modern data centers mix x86 CPUs for control tasks, GPUs for training, and TPUs for inference. Programming heterogeneous systems requires frameworks like OpenCL or CUDA that can coordinate execution across different architectures, but offers 10-100<span class="math inline">\(\times\)</span> efficiency gains by matching each task to optimal hardware.</p></div></div><p>AI accelerators represent a system-level transformation that requires tight hardware-software coupling. This transformation manifests in three specific computational patterns, compute primitives, that drive accelerator design decisions. Understanding these primitives determines the architectural features that enable 100-1000<span class="math inline">\(\times\)</span> performance improvements through coordinated hardware specialization and software optimization strategies examined in subsequent sections.</p>
<p>The evolution from floating-point coprocessors to AI accelerators reveals a consistent pattern: computational bottlenecks drive specialized hardware development. Where the Intel 8087 addressed floating-point operations that consumed 80% of scientific computing time, modern AI workloads present an even more extreme case. Matrix multiplications and convolutions constitute over 95% of neural network computation. This concentration of computational demand creates unprecedented opportunities for specialization, explaining why AI accelerators achieve 100-1000<span class="math inline">\(\times\)</span> performance improvements over general-purpose processors.</p>
<p>The specialization principles established through decades of hardware evolution identifying dominant operations, creating dedicated datapaths, and optimizing memory access patterns now guide AI accelerator design. However, neural networks introduce unique characteristics that demand new architectural approaches: massive parallelism in matrix operations, predictable data access patterns enabling prefetching, and tolerance for reduced precision that allows aggressive optimization. Understanding these computational patterns, which we term AI compute primitives, helps comprehend how modern accelerators transform the theoretical efficiency gains from <strong><a href="../optimizations/optimizations.html#sec-model-optimizations">Chapter 10: Model Optimizations</a></strong> into practical performance improvements. These hardware-software optimizations become critical in deployment scenarios ranging from <strong><a href="../ml_systems/ml_systems.html#sec-ml-systems">Chapter 2: ML Systems</a></strong> edge devices to cloud-scale inference systems.</p>
<p>Before examining these computational primitives in detail, we need to understand the architectural organization that enables their efficient execution. Modern AI accelerators achieve their dramatic performance improvements through a carefully orchestrated hierarchy of specialized components operating in concert. The architecture comprises three subsystems, each addressing distinct aspects of the computational challenge.</p>
<p>The processing substrate consists of an array of processing elements, each containing dedicated computational units optimized for specific operations: tensor cores execute matrix multiplication, vector units perform element-wise operations, and special function units compute activation functions. These processing elements are organized in a grid topology that enables massive parallelism, with dozens to hundreds of units operating simultaneously on different portions of the computation, exploiting the data-level parallelism inherent in neural network workloads.</p>
<p>The memory hierarchy forms an equally critical architectural component. High-bandwidth memory provides the aggregate throughput required to sustain these numerous processing elements, while a multi-level cache hierarchy from shared L2 caches down to per-element L1 caches and scratchpads minimizes the energy cost of data movement. This hierarchical organization embodies a design principle: in AI accelerators, data movement typically consumes more energy than computation itself, necessitating architectural strategies that prioritize data reuse by maintaining frequently accessed values, including weights and partial results, in proximity to compute units.</p>
<p>The host interface establishes connectivity between the specialized accelerator and the broader computing system, enabling coordination between general-purpose CPUs that manage program control flow and the accelerator that executes computationally intensive neural network operations. This architectural partitioning reflects specialization at the system level: CPUs address control flow, conditional logic, and system coordination, while accelerators focus on the regular, massively parallel arithmetic operations that dominate neural network execution.</p>
<p><a href="#fig-accelerator-anatomy" class="quarto-xref">Figure&nbsp;2</a> illustrates this architectural organization, showing how specialized compute units, hierarchical memory subsystems, and host connectivity integrate to form a system optimized for AI workloads.</p>
<div id="fig-accelerator-anatomy" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-pos="htb" data-fig-env="figure">
<figure class="quarto-float quarto-float-fig figure"><div aria-describedby="fig-accelerator-anatomy-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="1559765a6107b67182c20d4134c01f5bf2e29375.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-2" title="Figure&nbsp;2: Anatomy of a Modern AI Accelerator: AI accelerators integrate specialized processing elements containing tensor cores, vector units, and special function units, supported by a hierarchical memory system from high-bandwidth memory down to local caches. This architecture maximizes data reuse and parallel execution while minimizing energy-intensive data movement, forming the foundation for 100-1000Ã— performance improvements over general-purpose processors."><img src="hw_acceleration_files/mediabag/1559765a6107b67182c20d4134c01f5bf2e29375.svg" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-accelerator-anatomy-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: <strong>Anatomy of a Modern AI Accelerator</strong>: AI accelerators integrate specialized processing elements containing tensor cores, vector units, and special function units, supported by a hierarchical memory system from high-bandwidth memory down to local caches. This architecture maximizes data reuse and parallel execution while minimizing energy-intensive data movement, forming the foundation for 100-1000Ã— performance improvements over general-purpose processors.
</figcaption></figure>
</div>
<div id="quiz-question-sec-ai-acceleration-evolution-hardware-specialization-1d21" class="callout callout-quiz-question">
<details class="callout-quiz-question fbx-default closebutton"><summary><strong>Self-Check: Question 1.2</strong></summary><div>
<ol type="1">
<li>
<p>Which of the following best describes the primary motivation for the development of specialized hardware accelerators in computing?</p>
<ol type="a">
<li>To reduce the cost of general-purpose processors</li>
<li>To increase the flexibility of computing systems</li>
<li>To handle increasingly complex computational workloads efficiently</li>
<li>To simplify the programming models for developers</li>
</ol>
</li>
<li><p>Explain how the evolution of specialized hardware has influenced the design of modern machine learning accelerators.</p></li>
<li><p>True or False: The integration of specialized functions into general-purpose processors is a common trend observed in the evolution of computing architectures.</p></li>
<li>
<p>What is a key trade-off introduced by the use of specialized hardware accelerators?</p>
<ol type="a">
<li>Increased flexibility in programming</li>
<li>Reduced programming complexity</li>
<li>Higher energy consumption</li>
<li>Reduced silicon area utilization</li>
</ol>
</li>
<li><p>In a production system, how might the choice of hardware accelerators impact the deployment of machine learning models?</p></li>
</ol>
<p><a href="#quiz-answer-sec-ai-acceleration-evolution-hardware-specialization-1d21" class="question-label">See Answers â†’</a></p>
</div></details>
</div>
</section></section><section id="sec-ai-acceleration-ai-compute-primitives-8471" class="level2 page-columns page-full"><h2 class="anchored" data-anchor-id="sec-ai-acceleration-ai-compute-primitives-8471">AI Compute Primitives</h2>
<p>Understanding how hardware evolved toward AI-specific designs requires examining the computational patterns that drove this specialization. The transition from general-purpose CPUs achieving 100 GFLOPS to specialized accelerators delivering 100,000+ GFLOPS reflects architectural optimization for specific computational patterns that dominate machine learning workloads. These patterns, which we term compute primitives, appear repeatedly across all neural network architectures regardless of application domain or model size.</p>
<p>Modern neural networks are built upon a small number of core computational patterns. Regardless of the layer typeâ€”whether fully connected, convolutional, or attention-based layersâ€”the underlying operation typically involves multiplying input values by learned weights and accumulating the results. This repeated multiply-accumulate process dominates neural network execution and defines the arithmetic foundation of AI workloads. The regularity and frequency of these operations have led to the development of AI compute primitives: hardware-level abstractions optimized to execute these core computations with high efficiency.</p>
<p>Neural networks exhibit highly structured, data-parallel computations that enable architectural specialization. Building on the parallelization principles established in <a href="#sec-ai-acceleration-parallel-computing-graphics-processing-66b1" class="quarto-xref">Section&nbsp;1.2.2</a>, these patterns emphasize predictable data reuse and fixed operation sequences. AI compute primitives distill these patterns into reusable architectural units that support high-throughput and energy-efficient execution.</p>
<p>This decomposition is illustrated in <a href="#lst-dense_layer_def" class="quarto-xref">Listing&nbsp;1</a>, which defines a dense layer at the framework level.</p>
<div id="lst-dense_layer_def" class="listing quarto-float quarto-figure quarto-figure-left anchored">
<figure class="quarto-float quarto-float-lst figure"><figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-lst" id="lst-dense_layer_def-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Listing&nbsp;1: <strong>Dense Layer Definition</strong>: Defines a dense layer using a high-level API, illustrating how neural networks implement parallel transformations across input tensors.
</figcaption><div aria-describedby="lst-dense_layer_def-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="sourceCode" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>dense <span class="op">=</span> Dense(<span class="dv">512</span>)(input_tensor)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</figure>
</div>
<p>This high-level call expands into mathematical operations is shown in <a href="#lst-dense_expansion" class="quarto-xref">Listing&nbsp;2</a>.</p>
<div id="lst-dense_expansion" class="listing quarto-float quarto-figure quarto-figure-left anchored">
<figure class="quarto-float quarto-float-lst figure"><figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-lst" id="lst-dense_expansion-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Listing&nbsp;2: <strong>Layer Computation</strong>: Neural networks compute each layerâ€™s output via weighted input summation followed by an activation function transformation.
</figcaption><div aria-describedby="lst-dense_expansion-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="sourceCode" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> matmul(input_weights) <span class="op">+</span> bias</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> activation(output)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</figure>
</div>
<p>At the processor level, the computation reduces to nested loops that multiply inputs and weights, sum the results, and apply a nonlinear function, as shown in <a href="#lst-loop_level_dense" class="quarto-xref">Listing&nbsp;3</a>.</p>
<div id="lst-loop_level_dense" class="listing quarto-float quarto-figure quarto-figure-left anchored">
<figure class="quarto-float quarto-float-lst figure"><figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-lst" id="lst-loop_level_dense-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Listing&nbsp;3: <strong>Nested Loops</strong>: Computes output values through sequential matrix multiplications and bias additions, followed by activation function application to produce final outputs.
</figcaption><div aria-describedby="lst-loop_level_dense-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="sourceCode" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> n <span class="kw">in</span> <span class="bu">range</span>(batch_size):</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> m <span class="kw">in</span> <span class="bu">range</span>(output_size):</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">sum</span> <span class="op">=</span> bias[m]</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> k <span class="kw">in</span> <span class="bu">range</span>(input_size):</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>            <span class="bu">sum</span> <span class="op">+=</span> <span class="bu">input</span>[n, k] <span class="op">*</span> weights[k, m]</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>        output[n, m] <span class="op">=</span> activation(<span class="bu">sum</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</figure>
</div>
<p>This transformation reveals four computational characteristics: data-level parallelism enabling simultaneous execution, structured matrix operations defining computational workloads, predictable data movement patterns driving memory optimization, and frequent nonlinear transformations motivating specialized function units.</p>
<p>The design of AI compute primitives follows three architectural criteria. First, the primitive must be used frequently enough to justify dedicated hardware resources. Second, its specialized implementation must offer substantial performance or energy efficiency gains relative to general-purpose alternatives. Third, the primitive must remain stable across generations of neural network architectures to ensure long-term applicability. These considerations shape the inclusion of primitives such as vector operations, matrix operations, and special function units in modern ML accelerators. Together, they serve as the architectural foundation for efficient and scalable neural network execution.</p>
<section id="sec-ai-acceleration-vector-operations-729f" class="level3 page-columns page-full"><h3 class="anchored" data-anchor-id="sec-ai-acceleration-vector-operations-729f">Vector Operations</h3>
<p>Vector operations provide the first level of hardware acceleration by processing multiple data elements simultaneously. This parallelism exists at multiple scales, from individual neurons to entire layers, making vector processing essential for efficient neural network execution. Framework-level code translates to hardware instructions, revealing the critical role of vector processing in neural accelerators.</p>
<section id="sec-ai-acceleration-highlevel-framework-operations-9248" class="level4"><h4 class="anchored" data-anchor-id="sec-ai-acceleration-highlevel-framework-operations-9248">High-Level Framework Operations</h4>
<p>Machine learning frameworks hide hardware complexity through high-level abstractions. These abstractions decompose into progressively lower-level operations, revealing opportunities for hardware acceleration. One such abstraction is shown in <a href="#lst-linear_layer_highlevel" class="quarto-xref">Listing&nbsp;4</a>, which illustrates the execution flow of a linear layer.</p>
<div id="lst-linear_layer_highlevel" class="listing quarto-float quarto-figure quarto-figure-left anchored">
<figure class="quarto-float quarto-float-lst figure"><figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-lst" id="lst-linear_layer_highlevel-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Listing&nbsp;4: <strong>Linear Layer</strong>: Neural networks transform input data into a higher-dimensional space using linear mappings to enable complex feature extraction.
</figcaption><div aria-describedby="lst-linear_layer_highlevel-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="sourceCode" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>layer <span class="op">=</span> nn.Linear(<span class="dv">256</span>, <span class="dv">512</span>)  <span class="co"># 256 inputs to</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="co"># 512 outputs</span></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> layer(input_tensor)  <span class="co"># Process a batch of inputs</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</figure>
</div>
<p>This abstraction represents a fully connected layer that transforms input features through learned weights. To understand how hardware acceleration opportunities emerge, <a href="#lst-linear_math_internal" class="quarto-xref">Listing&nbsp;5</a> shows how the framework translates this high-level expression into mathematical operations.</p>
<div id="lst-linear_math_internal" class="listing quarto-float quarto-figure quarto-figure-left anchored">
<figure class="quarto-float quarto-float-lst figure"><figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-lst" id="lst-linear_math_internal-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Listing&nbsp;5: <strong>Fully Connected Layer</strong>: Each output is computed as a weighted sum of all inputs plus a bias, followed by an activation function transformation. Linear transformations enable complex model architectures in neural networks.
</figcaption><div aria-describedby="lst-linear_math_internal-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="sourceCode" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>Z <span class="op">=</span> matmul(weights, <span class="bu">input</span>) <span class="op">+</span> bias  <span class="co"># Each output needs all inputs</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> activation(Z)  <span class="co"># Transform each result</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</figure>
</div>
<p>These mathematical operations further decompose into explicit computational steps during processor execution. <a href="#lst-loop_linear_layer" class="quarto-xref">Listing&nbsp;6</a> illustrates the nested loops that implement these multiply-accumulate operations.</p>
<div id="lst-loop_linear_layer" class="listing quarto-float quarto-figure quarto-figure-left anchored">
<figure class="quarto-float quarto-float-lst figure"><figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-lst" id="lst-loop_linear_layer-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Listing&nbsp;6: <strong>Linear Layer Computation</strong>: Each output neuron is computed by summing weighted inputs from all features, followed by an activation function application. Understanding this process helps in grasping the fundamental building blocks of neural networks.
</figcaption><div aria-describedby="lst-loop_linear_layer-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="sourceCode" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> batch <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">32</span>):            <span class="co"># Process 32 samples at once</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> out_neuron <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">512</span>):  <span class="co"># Compute each output neuron</span></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">sum</span> <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> in_feature <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">256</span>): <span class="co"># Each output needs</span></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>                                      <span class="co"># all inputs</span></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>            <span class="bu">sum</span> <span class="op">+=</span> <span class="bu">input</span>[batch, in_feature] <span class="op">*</span></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>                         weights[out_neuron, in_feature]</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>        output[batch, out_neuron] <span class="op">=</span> activation(<span class="bu">sum</span> <span class="op">+</span></span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>                                    bias[out_neuron])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</figure>
</div>
</section><section id="sec-ai-acceleration-sequential-scalar-execution-d063" class="level4"><h4 class="anchored" data-anchor-id="sec-ai-acceleration-sequential-scalar-execution-d063">Sequential Scalar Execution</h4>
<p>Traditional scalar processors execute these operations sequentially, processing individual values one at a time. For the linear layer example above with a batch of 32 samples, computing the outputs requires over 4 million multiply-accumulate operations. Each operation involves loading an input value and a weight value, multiplying them, and accumulating the result. This sequential approach becomes highly inefficient when processing the massive number of identical operations required by neural networks.</p>
<p>Recognizing this inefficiency, modern processors leverage vector processing to transform execution patterns fundamentally.</p>
</section><section id="sec-ai-acceleration-parallel-vector-execution-cdaa" class="level4 page-columns page-full"><h4 class="anchored" data-anchor-id="sec-ai-acceleration-parallel-vector-execution-cdaa">Parallel Vector Execution</h4>
<p>Vector processing units achieve this transformation by operating on multiple data elements simultaneously. <a href="#lst-riscv_vector_mac" class="quarto-xref">Listing&nbsp;7</a> demonstrates this approach using RISC-V<a href="#fn16" class="footnote-ref" id="fnref16" role="doc-noteref"><sup>16</sup></a> assembly code that showcases modern vector processing capabilities.</p>
<div class="no-row-height column-margin column-container"><div id="fn16"><p><sup>16</sup>&nbsp;<strong>RISC-V for AI</strong>: RISC-V, the open-source instruction set architecture from UC Berkeley (2010), is becoming important for AI accelerators because itâ€™s freely customizable. Companies like SiFive and Google have created RISC-V chips with custom AI extensions. Unlike proprietary architectures, RISC-V allows hardware designers to add specialized ML instructions without licensing fees, potentially democratizing AI hardware development beyond the current duopoly of x86 and ARM.</p></div></div><div id="lst-riscv_vector_mac" class="listing quarto-float quarto-figure quarto-figure-left anchored">
<figure class="quarto-float quarto-float-lst figure"><figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-lst" id="lst-riscv_vector_mac-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Listing&nbsp;7: <strong>Vectorized Multiply-Accumulate Loop</strong>: This loop showcases how RISC-V vector instructions enable efficient batch processing by performing 8 multiply-add operations simultaneously, reducing computational latency in neural network training. <em>Source: RISC-V Architecture Manual</em>
</figcaption><div aria-describedby="lst-riscv_vector_mac-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="sourceCode" id="cb7"><pre class="sourceCode c code-with-copy"><code class="sourceCode c"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>vsetvli t0<span class="op">,</span> a0<span class="op">,</span> e32   # Process <span class="dv">8</span> elements at once</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>loop_batch<span class="op">:</span></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>    loop_neuron<span class="op">:</span></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>        vxor<span class="op">.</span>vv v0<span class="op">,</span> v0<span class="op">,</span> v0    # Clear <span class="dv">8</span> accumulators</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>        loop_feature<span class="op">:</span></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>            vle32<span class="op">.</span>v v1<span class="op">,</span> <span class="op">(</span>in_ptr<span class="op">)</span>    # Load <span class="dv">8</span> inputs together</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>            vle32<span class="op">.</span>v v2<span class="op">,</span> <span class="op">(</span>wt_ptr<span class="op">)</span>    # Load <span class="dv">8</span> weights together</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>            vfmacc<span class="op">.</span>vv v0<span class="op">,</span> v1<span class="op">,</span> v2    # <span class="dv">8</span> multiply<span class="op">-</span>adds at once</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>            add in_ptr<span class="op">,</span> in_ptr<span class="op">,</span> <span class="dv">32</span>  # Move to next <span class="dv">8</span> inputs</span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>            add wt_ptr<span class="op">,</span> wt_ptr<span class="op">,</span> <span class="dv">32</span>  # Move to next <span class="dv">8</span> weights</span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>            bnez feature_cnt<span class="op">,</span> loop_feature</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</figure>
</div>
<p>This vector implementation processes eight data elements in parallel, reducing both computation time and energy consumption. Vector load instructions transfer eight values simultaneously, maximizing memory bandwidth utilization. The vector multiply-accumulate instruction processes eight pairs of values in parallel, dramatically reducing the total instruction count from over 4 million to approximately 500,000.</p>
<p>To clarify how vector instructions map to common deep learning patterns, <a href="#tbl-vector" class="quarto-xref">Table&nbsp;2</a> introduces key vector operations and their typical applications in neural network computation. These operations, such as reduction, gather, scatter, and masked operations, are frequently encountered in layers like pooling, embedding lookups, and attention mechanisms. This terminology is necessary for interpreting how low-level vector hardware accelerates high-level machine learning workloads.</p>
<div id="tbl-vector" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure"><figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-vector-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;2: <strong>Vector Operations</strong>: Neural network layers frequently utilize core vector operations such as reduction, gather, and scatter to accelerate computation and efficiently process data in parallel; these operations clarify how low-level hardware optimizations map to high-level machine learning algorithms. These operations enable efficient implementation of common layers like pooling, embedding lookups, and attention mechanisms within deep learning models.
</figcaption><div aria-describedby="tbl-vector-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<colgroup>
<col style="width: 22%">
<col style="width: 41%">
<col style="width: 35%">
</colgroup>
<thead><tr class="header">
<th style="text-align: left;"><strong>Vector Operation</strong></th>
<th style="text-align: left;"><strong>Description</strong></th>
<th style="text-align: left;"><strong>Neural Network Application</strong></th>
</tr></thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><strong>Reduction</strong></td>
<td style="text-align: left;">Combines elements across a vector (e.g., sum, max)</td>
<td style="text-align: left;">Pooling layers, attention score computation</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Gather</strong></td>
<td style="text-align: left;">Loads multiple non-consecutive memory elements</td>
<td style="text-align: left;">Embedding lookups, sparse operations</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>Scatter</strong></td>
<td style="text-align: left;">Writes to multiple non-consecutive memory locations</td>
<td style="text-align: left;">Gradient updates for embeddings</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Masked operations</strong></td>
<td style="text-align: left;">Selectively operates on vector elements</td>
<td style="text-align: left;">Attention masks, padding handling</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>Vector-scalar broadcast</strong></td>
<td style="text-align: left;">Applies scalar to all vector elements</td>
<td style="text-align: left;">Bias addition, scaling operations</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p>Vector processing efficiency gains extend beyond instruction count reduction. Memory bandwidth utilization improves as vector loads transfer multiple values per operation. Energy efficiency increases because control logic is shared across multiple operations. These improvements compound across the deep layers of modern neural networks, where billions of operations execute for each forward pass.</p>
</section><section id="sec-ai-acceleration-vector-processing-history-c631" class="level4 page-columns page-full"><h4 class="anchored" data-anchor-id="sec-ai-acceleration-vector-processing-history-c631">Vector Processing History</h4>
<p>The principles underlying vector operations have long been central to high-performance computing. In the 1970s and 1980s, vector processors emerged as an architectural solution for scientific computing, weather modeling, and physics simulations, where large arrays of data required efficient parallel processing. Early systems such as the Cray-1<a href="#fn17" class="footnote-ref" id="fnref17" role="doc-noteref"><sup>17</sup></a>, one of the first commercially successful supercomputers, introduced dedicated vector units to perform arithmetic operations on entire data vectors in a single instruction. These vector units dramatically improved computational throughput compared to traditional scalar execution <span class="citation" data-cites="jordan1982guide">(<a href="#ref-jordan1982guide" role="doc-biblioref">Jordan 1982</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="fn17"><p><sup>17</sup>&nbsp;<strong>Cray-1 Vector Legacy</strong>: The Cray-1 (1975) cost $8.8 million (approximately $40-45 million in 2024 dollars) but could perform 160 million floating-point operations per secondâ€”1000x faster than typical computers. Its 64-element vector registers and pipelined vector units established the architectural template that modern AI accelerators still follow: process many data elements simultaneously with specialized hardware pipelines.</p></div><div id="ref-jordan1982guide" class="csl-entry" role="listitem">
Jordan, T. L. 1982. <span>â€œA Guide to Parallel Computation and Some Cray-1 Experiences.â€</span> In <em>Parallel Computations</em>, 1â€“50. Elsevier. <a href="https://doi.org/10.1016/b978-0-12-592101-5.50006-3">https://doi.org/10.1016/b978-0-12-592101-5.50006-3</a>.
</div></div><p>These concepts have reemerged in machine learning, where neural networks exhibit structure well suited to vectorized execution. The same operations, such as vector addition, multiplication, and reduction, that once accelerated numerical simulations now drive the execution of machine learning workloads. While the scale and specialization of modern AI accelerators differ from their historical predecessors, the underlying architectural principles remain the same. The resurgence of vector processing in neural network acceleration highlights its utility for achieving high computational efficiency.</p>
<p>Vector operations establish the foundation for neural network acceleration by enabling efficient parallel processing of independent data elements. While vector operations excel at element-wise transformations like activation functions, neural networks also require structured computations that combine multiple input features to produce output features, transformations that naturally express themselves as matrix operations. This need for coordinated computation across multiple dimensions simultaneously leads to the next architectural primitive: matrix operations.</p>
</section></section><section id="sec-ai-acceleration-matrix-operations-508d" class="level3 page-columns page-full"><h3 class="anchored" data-anchor-id="sec-ai-acceleration-matrix-operations-508d">Matrix Operations</h3>
<p>Matrix operations form the computational workhorse of neural networks, transforming high-dimensional data through structured patterns of weights, activations, and gradients <span class="citation" data-cites="Goodfellow-et-al-2016">(<a href="#ref-Goodfellow-et-al-2016" role="doc-biblioref">Goodfellow, Courville, and Bengio 2013</a>)</span>. While vector operations process elements independently, matrix operations orchestrate computations across multiple dimensions simultaneously. These operations reveal patterns that drive hardware acceleration strategies.</p>
<div class="no-row-height column-margin column-container"></div><section id="sec-ai-acceleration-matrix-operations-neural-networks-527a" class="level4"><h4 class="anchored" data-anchor-id="sec-ai-acceleration-matrix-operations-neural-networks-527a">Matrix Operations in Neural Networks</h4>
<p>Neural network computations decompose into hierarchical matrix operations. As shown in <a href="#lst-linear_matrix_hierarchy" class="quarto-xref">Listing&nbsp;8</a>, a linear layer demonstrates this hierarchy by transforming input features into output neurons over a batch.</p>
<div id="lst-linear_matrix_hierarchy" class="listing quarto-float quarto-figure quarto-figure-left anchored">
<figure class="quarto-float quarto-float-lst figure"><figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-lst" id="lst-linear_matrix_hierarchy-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Listing&nbsp;8: <strong>Matrix Operations</strong>: Neural networks perform transformations using matrix multiplications and biases to achieve output predictions. Training requires careful management of input batches and activation functions to optimize model performance.
</figcaption><div aria-describedby="lst-linear_matrix_hierarchy-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="sourceCode" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>layer <span class="op">=</span> nn.Linear(<span class="dv">256</span>, <span class="dv">512</span>)  <span class="co"># Layer transforms 256 inputs to</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="co"># 512 outputs</span></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> layer(input_batch)  <span class="co"># Process a batch of 32 samples</span></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Framework Internal: Core operations</span></span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>Z <span class="op">=</span> matmul(weights, <span class="bu">input</span>)  <span class="co"># Matrix: transforms [256 x 32]</span></span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a><span class="co"># input to [512 x 32] output</span></span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>Z <span class="op">=</span> Z <span class="op">+</span> bias  <span class="co"># Vector: adds bias to each</span></span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a><span class="co"># output independently</span></span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> relu(Z)  <span class="co"># Vector: applies activation to</span></span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a><span class="co"># each element independently</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</figure>
</div>
<p>This computation demonstrates the scale of matrix operations in neural networks. Each output neuron (512 total) must process all input features (256 total) for every sample in the batch (32 samples). The weight matrix alone contains <span class="math inline">\(256 \times 512 = 131,072\)</span> parameters that define these transformations, illustrating why efficient matrix multiplication becomes crucial for performance.</p>
<p>Neural networks employ matrix operations across diverse architectural patterns beyond simple linear layers.</p>
</section><section id="sec-ai-acceleration-types-matrix-computations-neural-networks-b497" class="level4 page-columns page-full"><h4 class="anchored" data-anchor-id="sec-ai-acceleration-types-matrix-computations-neural-networks-b497">Types of Matrix Computations in Neural Networks</h4>
<p>Matrix operations appear consistently across modern neural architectures, as illustrated in <a href="#lst-matrix_patterns" class="quarto-xref">Listing&nbsp;9</a>. Convolution operations are transformed into matrix multiplications through the im2col technique<a href="#fn18" class="footnote-ref" id="fnref18" role="doc-noteref"><sup>18</sup></a>, enabling efficient execution on hardware optimized for matrix operations.</p>
<div class="no-row-height column-margin column-container"><div id="fn18"><p><sup>18</sup>&nbsp;<strong>Im2col (Image-to-Column)</strong>: A preprocessing technique that converts convolution operations into matrix multiplications by unfolding image patches into column vectors. A 3Ã—3 convolution on a 224Ã—224 image creates a matrix with ~50,000 columns, enabling efficient GEMM execution but increasing memory usage 9Ã— due to overlapping patches. This transformation explains why convolutions are actually matrix operations in modern ML accelerators.</p></div></div><div id="lst-matrix_patterns" class="listing quarto-float quarto-figure quarto-figure-left anchored">
<figure class="quarto-float quarto-float-lst figure"><figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-lst" id="lst-matrix_patterns-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Listing&nbsp;9: <strong>Linear Layers</strong>: Layer transformations combine input features to produce hidden representations. Matrix operations in neural networks enable efficient feature extraction and transformation, forming the backbone of many machine learning architectures.
</figcaption><div aria-describedby="lst-matrix_patterns-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="sourceCode" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>hidden <span class="op">=</span> matmul(weights, inputs)</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="co"># weights: [out_dim x in_dim], inputs: [in_dim x batch]</span></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Result combines all inputs for each output</span></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Attention Mechanisms - Multiple matrix operations</span></span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>Q <span class="op">=</span> matmul(Wq, inputs)</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Project inputs to query space [query_dim x batch]</span></span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>K <span class="op">=</span> matmul(Wk, inputs)</span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Project inputs to key space[key_dim x batch]</span></span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a>attention <span class="op">=</span> matmul(Q, K.T)</span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Compare all queries with all keys [query_dim x key_dim]</span></span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Convolutions - Matrix multiply after reshaping</span></span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a>patches <span class="op">=</span> im2col(<span class="bu">input</span>)</span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert [H x W x C] image to matrix of patches</span></span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> matmul(kernel, patches)</span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Apply kernels to all patches simultaneously</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</figure>
</div>
<p>This pervasive pattern of matrix multiplication has direct implications for hardware design. The need for efficient matrix operations drives the development of specialized hardware architectures that can handle these computations at scale. The following sections explore how modern AI accelerators implement matrix operations, focusing on their architectural features and performance optimizations.</p>
</section><section id="sec-ai-acceleration-matrix-operations-hardware-acceleration-514a" class="level4 page-columns page-full"><h4 class="anchored" data-anchor-id="sec-ai-acceleration-matrix-operations-hardware-acceleration-514a">Matrix Operations Hardware Acceleration</h4>
<p>The computational demands of matrix operations have driven specialized hardware optimizations. Modern processors implement dedicated matrix units that extend beyond vector processing capabilities. An example of such matrix acceleration is shown in <a href="#lst-matrix_unit" class="quarto-xref">Listing&nbsp;10</a>.</p>
<div id="lst-matrix_unit" class="listing quarto-float quarto-figure quarto-figure-left anchored">
<figure class="quarto-float quarto-float-lst figure"><figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-lst" id="lst-matrix_unit-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Listing&nbsp;10: <strong>Matrix Unit Operation</strong>: Enables efficient block-wise matrix multiplication and accumulation in hardware-accelerated systems, showcasing how specialized units streamline computational tasks essential for AI/ML operations.
</figcaption><div aria-describedby="lst-matrix_unit-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="sourceCode" id="cb10"><pre class="sourceCode c code-with-copy"><code class="sourceCode c"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>mload mr1<span class="op">,</span> <span class="op">(</span>weight_ptr<span class="op">)</span>     # Load e<span class="op">.</span>g<span class="op">.,</span> <span class="dv">16</span><span class="er">x16</span> block of</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>                            <span class="pp"># </span><span class="er">weight matrix</span></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>mload mr2<span class="op">,</span> <span class="op">(</span>input_ptr<span class="op">)</span>      # Load corresponding input block</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>matmul<span class="op">.</span>mm mr3<span class="op">,</span> mr1<span class="op">,</span> mr2     # Multiply and accumulate entire</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>                            <span class="pp"># </span><span class="er">blocks at once</span></span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>mstore <span class="op">(</span>output_ptr<span class="op">),</span> mr3    # Store computed output block</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</figure>
</div>
<p>This matrix processing unit can handle <span class="math inline">\(16\times16\)</span> blocks of the linear layer computation described earlier, processing 256 multiply-accumulate operations simultaneously compared to the 8 operations possible with vector processing. These matrix operations complement vectorized computation by enabling structured many-to-many transformations. The interplay between matrix and vector operations shapes the efficiency of neural network execution.</p>
<p>Matrix operations provide computational capabilities for neural networks through coordinated parallel processing across multiple dimensions (see <a href="#tbl-matrix" class="quarto-xref">Table&nbsp;3</a>). While they enable transformations such as attention mechanisms and convolutions, their performance depends on efficient data handling. Conversely, vector operations are optimized for one-to-one transformations like activation functions and layer normalization. The distinction between these operations highlights the importance of dataflow patterns in neural accelerator design, examined next <span class="citation" data-cites="Hwu2011GPU">(<a href="#ref-Hwu2011GPU" role="doc-biblioref">Hwu 2011</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-Hwu2011GPU" class="csl-entry" role="listitem">
Hwu, Wen-mei W. 2011. <span>â€œIntroduction.â€</span> In <em>GPU Computing Gems Emerald Edition</em>, xixâ€“xx. Elsevier. <a href="https://doi.org/10.1016/b978-0-12-384988-5.00064-4">https://doi.org/10.1016/b978-0-12-384988-5.00064-4</a>.
</div></div><div id="tbl-matrix" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure"><figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-matrix-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;3: <strong>Operation Characteristics</strong>: Matrix operations excel at many-to-many transformations common in neural network layers, while vector operations efficiently handle one-to-one transformations like activation functions and normalization. Understanding these distinctions guides the selection of appropriate computational primitives for different machine learning tasks and impacts system performance.
</figcaption><div aria-describedby="tbl-matrix-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<colgroup>
<col style="width: 17%">
<col style="width: 18%">
<col style="width: 25%">
<col style="width: 36%">
</colgroup>
<thead><tr class="header">
<th style="text-align: left;"><strong>Operation Type</strong></th>
<th style="text-align: left;"><strong>Best For</strong></th>
<th style="text-align: left;"><strong>Examples</strong></th>
<th style="text-align: left;"><strong>Key Characteristic</strong></th>
</tr></thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><li>
Layer transformations
</li></td>
<td style="text-align: left;"></td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Matrix Operations</strong></td>
<td style="text-align: left;">Many-to-many transforms</td>
<td style="text-align: left;">
<li>
Attention computation
</li>
<li>
Convolutions
</li>
<li>
Activation functions
</li>
</td>
<td style="text-align: left;">Each output depends on multiple inputs</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>Vector Operations</strong></td>
<td style="text-align: left;">One-to-one transforms</td>
<td style="text-align: left;">
<li>
Layer normalization
</li>
<li>
Element-wise gradients
</li>
</td>
<td style="text-align: left;">Each output depends only on corresponding input</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
</section><section id="sec-ai-acceleration-historical-foundations-matrix-computation-402e" class="level4 page-columns page-full"><h4 class="anchored" data-anchor-id="sec-ai-acceleration-historical-foundations-matrix-computation-402e">Historical Foundations of Matrix Computation</h4>
<p>Matrix operations have long served as a cornerstone of computational mathematics, with applications extending from numerical simulations to graphics processing <span class="citation" data-cites="Golub1996Matrix">(<a href="#ref-Golub1996Matrix" role="doc-biblioref">Golub and Loan 1996</a>)</span>. The structured nature of matrix multiplications and transformations made them natural targets for acceleration in early computing architectures. In the 1980s and 1990s, specialized digital signal processors (DSPs) and graphics processing units (GPUs) optimized for matrix computations played a critical role in accelerating workloads such as image processing, scientific computing, and 3D rendering <span class="citation" data-cites="owens2008gpu">(<a href="#ref-owens2008gpu" role="doc-biblioref">Owens et al. 2008</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-Golub1996Matrix" class="csl-entry" role="listitem">
Golub, Gene H., and Charles F. Van Loan. 1996. <em>Matrix Computations</em>. Johns Hopkins University Press.
</div><div id="ref-owens2008gpu" class="csl-entry" role="listitem">
Owens, J. D., M. Houston, D. Luebke, S. Green, J. E. Stone, and J. C. Phillips. 2008. <span>â€œGPU Computing.â€</span> <em>Proceedings of the IEEE</em> 96 (5): 879â€“99. <a href="https://doi.org/10.1109/jproc.2008.917757">https://doi.org/10.1109/jproc.2008.917757</a>.
</div></div><p>The widespread adoption of machine learning has reinforced the importance of efficient matrix computation. Neural networks, fundamentally built on matrix multiplications and tensor operations, have driven the development of dedicated hardware architectures that extend beyond traditional vector processing. Modern tensor processing units (TPUs) and AI accelerators implement matrix multiplication at scale, reflecting the same architectural principles that once underpinned early scientific computing and graphics workloads. The resurgence of matrix-centric architectures highlights the deep connection between classical numerical computing and contemporary AI acceleration.</p>
<p>While matrix operations provide the computational backbone for neural networks, they represent only part of the acceleration challenge. Neural networks also depend critically on non-linear transformations that cannot be efficiently expressed through linear algebra alone.</p>
</section></section><section id="sec-ai-acceleration-special-function-units-ed00" class="level3 page-columns page-full"><h3 class="anchored" data-anchor-id="sec-ai-acceleration-special-function-units-ed00">Special Function Units</h3>
<p>While vector and matrix operations efficiently handle the linear transformations in neural networks, non-linear functions present unique computational challenges that require dedicated hardware solutions. Special Function Units (SFUs) provide hardware acceleration for these essential computations, completing the set of fundamental processing primitives needed for efficient neural network execution.</p>
<section id="sec-ai-acceleration-nonlinear-functions-fdce" class="level4 page-columns page-full"><h4 class="anchored" data-anchor-id="sec-ai-acceleration-nonlinear-functions-fdce">Non-Linear Functions</h4>
<p>Non-linear functions play a fundamental role in machine learning by enabling neural networks to model complex relationships <span class="citation" data-cites="Goodfellow-et-al-2016">(<a href="#ref-Goodfellow-et-al-2016" role="doc-biblioref">Goodfellow, Courville, and Bengio 2013</a>)</span>. <a href="#lst-nonlinear_layer" class="quarto-xref">Listing&nbsp;11</a> illustrates a typical neural network layer sequence.</p>
<div class="no-row-height column-margin column-container"><div id="ref-Goodfellow-et-al-2016" class="csl-entry" role="listitem">
Goodfellow, Ian J., Aaron Courville, and Yoshua Bengio. 2013. <span>â€œScaling up Spike-and-Slab Models for Unsupervised Feature Learning.â€</span> <em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em> 35 (8): 1902â€“14. <a href="https://doi.org/10.1109/tpami.2012.273">https://doi.org/10.1109/tpami.2012.273</a>.
</div></div><div id="lst-nonlinear_layer" class="listing quarto-float quarto-figure quarto-figure-left anchored">
<figure class="quarto-float quarto-float-lst figure"><figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-lst" id="lst-nonlinear_layer-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Listing&nbsp;11: <strong>Non-Linear Transformations</strong>: Neural networks process input data through a sequence of linear transformations followed by non-linear activations to capture complex patterns. This layer sequence enhances model expressiveness and learning capabilities.
</figcaption><div aria-describedby="lst-nonlinear_layer-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="sourceCode" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>layer <span class="op">=</span> nn.Sequential(</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>    nn.Linear(<span class="dv">256</span>, <span class="dv">512</span>), nn.ReLU(), nn.BatchNorm1d(<span class="dv">512</span>)</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> layer(input_tensor)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</figure>
</div>
<p>This sequence introduces multiple non-linear transformations that extend beyond simple matrix operations. <a href="#lst-nonlinear_math" class="quarto-xref">Listing&nbsp;12</a> demonstrates how the framework decomposes these operations into their mathematical components.</p>
<div id="lst-nonlinear_math" class="listing quarto-float quarto-figure quarto-figure-left anchored">
<figure class="quarto-float quarto-float-lst figure"><figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-lst" id="lst-nonlinear_math-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Listing&nbsp;12: <strong>Non-linear Transformations</strong>: Neural networks apply linear and non-linear operations to transform input data into meaningful features for learning. Machine learning models leverage these transformations to capture complex patterns in data efficiently.
</figcaption><div aria-describedby="lst-nonlinear_math-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="sourceCode" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>Z <span class="op">=</span> matmul(weights, <span class="bu">input</span>) <span class="op">+</span> bias  <span class="co"># Linear transformation</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>H <span class="op">=</span> <span class="bu">max</span>(<span class="dv">0</span>, Z)  <span class="co"># ReLU activation</span></span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>mean <span class="op">=</span> reduce_mean(H, axis<span class="op">=</span><span class="dv">0</span>)  <span class="co"># BatchNorm statistics</span></span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>var <span class="op">=</span> reduce_mean((H <span class="op">-</span> mean) <span class="op">**</span> <span class="dv">2</span>)  <span class="co"># Variance computation</span></span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> gamma <span class="op">*</span> (H <span class="op">-</span> mean) <span class="op">/</span> sqrt(var <span class="op">+</span> eps) <span class="op">+</span> beta</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Normalization</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</figure>
</div>
</section><section id="sec-ai-acceleration-hardware-implementation-nonlinear-functions-004d" class="level4 page-columns page-full"><h4 class="anchored" data-anchor-id="sec-ai-acceleration-hardware-implementation-nonlinear-functions-004d">Hardware Implementation of Non-Linear Functions</h4>
<p>The computational complexity of these operations becomes apparent when examining their implementation on traditional processors. These seemingly simple mathematical operations translate into complex sequences of instructions. Consider the computation of batch normalization: calculating the square root requires multiple iterations of numerical approximation, while exponential functions in operations like softmax need series expansion or lookup tables <span class="citation" data-cites="Ioffe2015">(<a href="#ref-Ioffe2015" role="doc-biblioref">Ioffe and Szegedy 2015</a>)</span>. Even a simple ReLU activation introduces branching logic that can disrupt instruction pipelining (see <a href="#lst-traditional_overhead" class="quarto-xref">Listing&nbsp;13</a> for an example).</p>
<div class="no-row-height column-margin column-container"><div id="ref-Ioffe2015" class="csl-entry" role="listitem">
Ioffe, Sergey, and Christian Szegedy. 2015. <span>â€œBatch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift.â€</span> <em>International Conference on Machine Learning (ICML)</em>, February, 448â€“56. <a href="http://arxiv.org/abs/1502.03167v3">http://arxiv.org/abs/1502.03167v3</a>.
</div></div><div id="lst-traditional_overhead" class="listing quarto-float quarto-figure quarto-figure-left anchored">
<figure class="quarto-float quarto-float-lst figure"><figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-lst" id="lst-traditional_overhead-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Listing&nbsp;13: <strong>ReLU and BatchNorm Operations</strong>: Neural networks process input data through conditional operations that can disrupt instruction pipelining and multiple passes required for normalization, highlighting efficiency challenges in traditional implementations. Source: IEEE Spectrum
</figcaption><div aria-describedby="lst-traditional_overhead-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="sourceCode" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> batch <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">32</span>):</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> feature <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">512</span>):</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>       <span class="co"># ReLU: Requires branch prediction and potential</span></span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>       <span class="co"># pipeline stalls</span></span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>       z <span class="op">=</span> matmul_output[batch, feature]</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>       h <span class="op">=</span> <span class="bu">max</span>(<span class="fl">0.0</span>, z)    <span class="co"># Conditional operation</span></span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>       <span class="co"># BatchNorm: Multiple passes over data</span></span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a>       mean_sum[feature] <span class="op">+=</span> h    <span class="co"># First pass for mean</span></span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a>       var_sum[feature] <span class="op">+=</span> h <span class="op">*</span> h <span class="co"># Additional pass for variance</span></span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a>       temp[batch, feature] <span class="op">=</span> h  <span class="co"># Extra memory storage needed</span></span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Normalization requires complex arithmetic</span></span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> feature <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">512</span>):</span>
<span id="cb13-16"><a href="#cb13-16" aria-hidden="true" tabindex="-1"></a>    mean <span class="op">=</span> mean_sum[feature] <span class="op">/</span> batch_size</span>
<span id="cb13-17"><a href="#cb13-17" aria-hidden="true" tabindex="-1"></a>    var <span class="op">=</span> (var_sum[feature] <span class="op">/</span> batch_size) <span class="op">-</span> mean <span class="op">*</span> mean</span>
<span id="cb13-18"><a href="#cb13-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-19"><a href="#cb13-19" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Square root computation: Multiple iterations</span></span>
<span id="cb13-20"><a href="#cb13-20" aria-hidden="true" tabindex="-1"></a>    scale <span class="op">=</span> gamma[feature] <span class="op">/</span> sqrt(var <span class="op">+</span> eps)  <span class="co"># Iterative</span></span>
<span id="cb13-21"><a href="#cb13-21" aria-hidden="true" tabindex="-1"></a>                                              <span class="co"># approximation</span></span>
<span id="cb13-22"><a href="#cb13-22" aria-hidden="true" tabindex="-1"></a>    shift <span class="op">=</span> beta[feature] <span class="op">-</span> mean <span class="op">*</span> scale</span>
<span id="cb13-23"><a href="#cb13-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-24"><a href="#cb13-24" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Additional pass over data for final computation</span></span>
<span id="cb13-25"><a href="#cb13-25" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> batch <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">32</span>):</span>
<span id="cb13-26"><a href="#cb13-26" aria-hidden="true" tabindex="-1"></a>        output[batch, feature] <span class="op">=</span> temp[batch, feature] <span class="op">*</span></span>
<span id="cb13-27"><a href="#cb13-27" aria-hidden="true" tabindex="-1"></a>                                 scale <span class="op">+</span> shift</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</figure>
</div>
<p>These operations introduce several key inefficiencies:</p>
<ol type="1">
<li>Multiple passes over data, increasing memory bandwidth requirements</li>
<li>Complex arithmetic requiring many instruction cycles</li>
<li>Conditional operations that can cause pipeline stalls</li>
<li>Additional memory storage for intermediate results</li>
<li>Poor utilization of vector processing units</li>
</ol>
<p>More specifically, each operation introduces distinct challenges. Batch normalization requires multiple passes through data: one for mean computation, another for variance, and a final pass for output transformation. Each pass loads and stores data through the memory hierarchy. Operations that appear simple in mathematical notation often expand into many instructions. The square root computation typically requires 10-20 iterations of numerical methods like Newton-Raphson approximation for suitable precision <span class="citation" data-cites="Goldberg1991">(<a href="#ref-Goldberg1991" role="doc-biblioref">Goldberg 1991</a>)</span>. Conditional operations like ReLUâ€™s max function require branch instructions that can stall the processorâ€™s pipeline. The implementation needs temporary storage for intermediate values, increasing memory usage and bandwidth consumption. While vector units excel at regular computations, functions like exponentials and square roots often require scalar operations that cannot fully utilize vector processing capabilities.</p>
<div class="no-row-height column-margin column-container"><div id="ref-Goldberg1991" class="csl-entry" role="listitem">
Goldberg, David. 1991. <span>â€œWhat Every Computer Scientist Should Know about Floating-Point Arithmetic.â€</span> <em>ACM Computing Surveys</em> 23 (1): 5â€“48. <a href="https://doi.org/10.1145/103162.103163">https://doi.org/10.1145/103162.103163</a>.
</div></div></section><section id="sec-ai-acceleration-hardware-acceleration-08e3" class="level4 page-columns page-full"><h4 class="anchored" data-anchor-id="sec-ai-acceleration-hardware-acceleration-08e3">Hardware Acceleration</h4>
<p>SFUs address these inefficiencies through dedicated hardware implementation. Modern ML accelerators include specialized circuits that transform these complex operations into single-cycle or fixed-latency computations. The accelerator can load a vector of values and apply non-linear functions directly, eliminating the need for multiple passes and complex instruction sequences as shown in <a href="#lst-sfu_vector_ops" class="quarto-xref">Listing&nbsp;14</a>.</p>
<div id="lst-sfu_vector_ops" class="listing quarto-float quarto-figure quarto-figure-left anchored">
<figure class="quarto-float quarto-float-lst figure"><figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-lst" id="lst-sfu_vector_ops-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Listing&nbsp;14: <strong>Hardware Acceleration</strong>: Single-cycle non-linear operations enable efficient vector processing in ML accelerators, showcasing how specialized hardware reduces computational latency.
</figcaption><div aria-describedby="lst-sfu_vector_ops-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="sourceCode" id="cb14"><pre class="sourceCode c code-with-copy"><code class="sourceCode c"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>vld<span class="op">.</span>v v1<span class="op">,</span> <span class="op">(</span>input_ptr<span class="op">)</span>    # Load vector of values</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>vrelu<span class="op">.</span>v v2<span class="op">,</span> v1           # Single<span class="op">-</span>cycle ReLU on entire vector</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>vsigm<span class="op">.</span>v v3<span class="op">,</span> v1           # Fixed<span class="op">-</span>latency sigmoid computation</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>vtanh<span class="op">.</span>v v4<span class="op">,</span> v1           # Direct hardware tanh implementation</span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>vrsqrt<span class="op">.</span>v v5<span class="op">,</span> v1          # Fast reciprocal square root</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</figure>
</div>
<p>Each SFU implements a specific function through specialized circuitry. For instance, a ReLU unit performs the comparison and selection in dedicated logic, eliminating branching overhead. Square root operations use hardware implementations of algorithms like Newton-Raphson with fixed iteration counts, providing guaranteed latency. Exponential and logarithmic functions often combine small lookup tables with hardware interpolation circuits <span class="citation" data-cites="Lauterbach2019">(<a href="#ref-Lauterbach2019" role="doc-biblioref">Costa et al. 2019</a>)</span>. Using these custom instructions, the SFU implementation eliminates multiple passes over data, removes complex arithmetic sequences, and maintains high computational efficiency. <a href="#tbl-sfu" class="quarto-xref">Table&nbsp;4</a> shows the various hardware implementations and their typical latencies.</p>
<div class="no-row-height column-margin column-container"><div id="ref-Lauterbach2019" class="csl-entry" role="listitem">
Costa, Tiago, Chen Shi, Kevin Tien, and Kenneth L. Shepard. 2019. <span>â€œA CMOS 2D Transmit Beamformer with Integrated PZT Ultrasound Transducers for Neuromodulation.â€</span> In <em>2019 IEEE Custom Integrated Circuits Conference (CICC)</em>, 1â€“4. IEEE. <a href="https://doi.org/10.1109/cicc.2019.8780236">https://doi.org/10.1109/cicc.2019.8780236</a>.
</div></div><div id="tbl-sfu" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure"><figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-sfu-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;4: <strong>Special Function Units</strong>: Dedicated hardware implementations of common mathematical functionsâ€”like relu, sigmoid, and reciprocal square rootâ€”accelerate machine learning computations by eliminating software overhead and enabling parallel processing of vector data. Typical latencies of 1â€“2 cycles per function demonstrate the performance gains achieved through specialized circuitry instead of general-purpose arithmetic.
</figcaption><div aria-describedby="tbl-sfu-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<colgroup>
<col style="width: 21%">
<col style="width: 20%">
<col style="width: 36%">
<col style="width: 20%">
</colgroup>
<thead><tr class="header">
<th style="text-align: left;"><strong>Function Unit</strong></th>
<th style="text-align: left;"><strong>Operation</strong></th>
<th style="text-align: left;"><strong>Implementation Strategy</strong></th>
<th style="text-align: right;"><strong>Typical Latency</strong></th>
</tr></thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><strong>Activation Unit</strong></td>
<td style="text-align: left;">ReLU, sigmoid, tanh</td>
<td style="text-align: left;">Piece-wise approximation circuits</td>
<td style="text-align: right;">1-2 cycles</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Statistics Unit</strong></td>
<td style="text-align: left;">Mean, variance</td>
<td style="text-align: left;">Parallel reduction trees</td>
<td style="text-align: right;">log(N) cycles</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>Exponential Unit</strong></td>
<td style="text-align: left;">exp, log</td>
<td style="text-align: left;">Table lookup + hardware interpolation</td>
<td style="text-align: right;">2-4 cycles</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Root/Power Unit</strong></td>
<td style="text-align: left;">sqrt, rsqrt</td>
<td style="text-align: left;">Fixed-iteration Newton-Raphson</td>
<td style="text-align: right;">4-8 cycles</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
</section><section id="sec-ai-acceleration-sfus-history-a1b6" class="level4 page-columns page-full"><h4 class="anchored" data-anchor-id="sec-ai-acceleration-sfus-history-a1b6">SFUs History</h4>
<p>The need for efficient non-linear function evaluation has shaped computer architecture for decades. Early processors incorporated hardware support for complex mathematical functions, such as logarithms and trigonometric operations, to accelerate workloads in scientific computing and signal processing <span class="citation" data-cites="Smith1997">(<a href="#ref-Smith1997" role="doc-biblioref">Smith 1997</a>)</span>. In the 1970s and 1980s, floating-point co-processors were introduced to handle complex mathematical operations separately from the main CPU <span class="citation" data-cites="palmer_8087_1981">(<a href="#ref-palmer_8087_1981" role="doc-biblioref">Palmer 1980</a>)</span>. In the 1990s, instruction set extensions such as Intelâ€™s SSE and ARMâ€™s NEON provided dedicated hardware for vectorized mathematical transformations, improving efficiency for multimedia and signal processing applications.</p>
<div class="no-row-height column-margin column-container"><div id="ref-Smith1997" class="csl-entry" role="listitem">
Smith, Steven W. 1997. <em>The Scientist and Engineerâ€™s Guide to Digital Signal Processing</em>. California Technical Publishing. <a href="https://www.dspguide.com/">https://www.dspguide.com/</a>.
</div><div id="ref-palmer_8087_1981" class="csl-entry" role="listitem">
Palmer, John F. 1980. <span>â€œThe INTEL 8087 Numeric Data Processor.â€</span> In <em>Proceedings of the May 19-22, 1980, National Computer Conference on - AFIPS â€™80</em>, 887. ACM Press. <a href="https://doi.org/10.1145/1500518.1500674">https://doi.org/10.1145/1500518.1500674</a>.
</div></div><p>Machine learning workloads have reintroduced a strong demand for specialized functional units, as activation functions, normalization layers, and exponential transformations are fundamental to neural network computations. Rather than relying on iterative software approximations, modern AI accelerators implement fast, fixed-latency SFUs for these operations, mirroring historical trends in scientific computing. The reemergence of dedicated special function units underscores the ongoing cycle in hardware evolution, where domain-specific requirements drive the reinvention of classical architectural concepts in new computational paradigms.</p>
<p>The combination of vector, matrix, and special function units provides the computational foundation for modern AI accelerators. However, the effective utilization of these processing primitives depends critically on data movement and access patterns. This leads us to examine the architectures, hierarchies, and strategies that enable efficient data flow in neural network execution.</p>
</section></section><section id="sec-ai-acceleration-compute-units-execution-models-f406" class="level3 page-columns page-full"><h3 class="anchored" data-anchor-id="sec-ai-acceleration-compute-units-execution-models-f406">Compute Units and Execution Models</h3>
<p>The vector operations, matrix operations, and special function units examined previously represent the fundamental computational primitives in AI accelerators. Modern AI processors package these primitives into distinct execution units, such as SIMD units, tensor cores, and processing elements, which define how computations are structured and exposed to users. Understanding this organization reveals both the theoretical capabilities and practical performance characteristics that developers can leverage in contemporary AI accelerators.</p>
<section id="sec-ai-acceleration-mapping-primitives-execution-units-ccb6" class="level4"><h4 class="anchored" data-anchor-id="sec-ai-acceleration-mapping-primitives-execution-units-ccb6">Mapping Primitives to Execution Units</h4>
<p>The progression from computational primitives to execution units follows a structured hierarchy that reflects the increasing complexity and specialization of AI accelerators:</p>
<ul>
<li>Vector operations â†’ SIMD/SIMT units that enable parallel processing of independent data elements</li>
<li>Matrix operations â†’ Tensor cores and systolic arrays that provide structured matrix multiplication</li>
<li>Special functions â†’ Dedicated hardware units integrated within processing elements</li>
</ul>
<p>Each execution unit combines these computational primitives with specialized memory and control mechanisms, optimizing both performance and energy efficiency. This structured packaging allows hardware vendors to expose standardized programming interfaces while implementing diverse underlying architectures tailored to specific workload requirements. The choice of execution unit significantly influences overall system efficiency, affecting data locality, compute density, and workload adaptability. Subsequent sections examine how these execution units operate within AI accelerators to maximize performance across different machine learning tasks.</p>
</section><section id="sec-ai-acceleration-evolution-simd-simt-architectures-e1fd" class="level4 page-columns page-full"><h4 class="anchored" data-anchor-id="sec-ai-acceleration-evolution-simd-simt-architectures-e1fd">Evolution from SIMD to SIMT Architectures</h4>
<p>Single Instruction Multiple Data (SIMD)<a href="#fn19" class="footnote-ref" id="fnref19" role="doc-noteref"><sup>19</sup></a> execution applies identical operations to multiple data elements in parallel, minimizing instruction overhead while maximizing data throughput. This execution model is widely used to accelerate workloads with regular, independent data parallelism, such as neural network computations. The ARM Scalable Vector Extension (SVE) provides a representative example of how modern architectures implement SIMD operations efficiently, as illustrated in <a href="#lst-arm_sve_vector" class="quarto-xref">Listing&nbsp;15</a>.</p>
<div class="no-row-height column-margin column-container"><div id="fn19"><p><sup>19</sup>&nbsp;<strong>SIMD Evolution</strong>: SIMD originated in Flynnâ€™s 1966 taxonomy for scientific computing, but neural networks transformed it from a niche HPC concept to mainstream necessity. Modern CPUs have 512-bit SIMD units (AVX-512), but AI pushed development of SIMT (Single Instruction, Multiple Thread) where thousands of lightweight threads execute in parallelâ€”GPU architectures now coordinate 65,536+ threads simultaneously, impossible with traditional SIMD.</p></div></div><div id="lst-arm_sve_vector" class="listing quarto-float quarto-figure quarto-figure-left anchored">
<figure class="quarto-float quarto-float-lst figure"><figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-lst" id="lst-arm_sve_vector-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Listing&nbsp;15: <strong>Vector Operation</strong>: Vector multiplication and addition operations enable efficient parallel processing in machine learning models. <em>Source: ARM Documentation</em>
</figcaption><div aria-describedby="lst-arm_sve_vector-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="sourceCode" id="cb15"><pre class="sourceCode c code-with-copy"><code class="sourceCode c"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>ptrue p0<span class="op">.</span>s              # Create predicate <span class="cf">for</span> vector length</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>ld1w z0<span class="op">.</span>s<span class="op">,</span> p0<span class="op">/</span>z<span class="op">,</span> <span class="op">[</span>x0<span class="op">]</span>   # Load vector of inputs</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>fmul z1<span class="op">.</span>s<span class="op">,</span> z0<span class="op">.</span>s<span class="op">,</span> z0<span class="op">.</span>s   # Multiply elements</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>fadd z2<span class="op">.</span>s<span class="op">,</span> z1<span class="op">.</span>s<span class="op">,</span> z0<span class="op">.</span>s   # Add elements</span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>st1w z2<span class="op">.</span>s<span class="op">,</span> p0<span class="op">,</span> <span class="op">[</span>x1<span class="op">]</span>     # Store results</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</figure>
</div>
<p>Processor architectures continue to expand SIMD capabilities to accommodate increasing computational demands. Intelâ€™s Advanced Matrix Extensions (AMX) <span class="citation" data-cites="intel2021amx">(<a href="#ref-intel2021amx" role="doc-biblioref">Corporation 2021</a>)</span> and ARMâ€™s SVE2 architecture <span class="citation" data-cites="stephens2017arm">(<a href="#ref-stephens2017arm" role="doc-biblioref">Stephens et al. 2017</a>)</span> provide flexible SIMD execution, enabling software to scale across different hardware implementations.</p>
<div class="no-row-height column-margin column-container"><div id="ref-intel2021amx" class="csl-entry" role="listitem">
Corporation, Intel. 2021. <span>â€œIntel Advanced Matrix Extensions (Intel AMX).â€</span> In <em>Intel Architecture Instruction Set Extensions Programming Reference</em>. Intel Corporation.<a href="https://software.intel.com/content/www/us/en/develop/download/intel-architecture-instruction-set-extensions-programming-reference.html%0A%20%20">https://software.intel.com/content/www/us/en/develop/download/intel-architecture-instruction-set-extensions-programming-reference.html </a>.
</div><div id="ref-stephens2017arm" class="csl-entry" role="listitem">
Stephens, Nigel, Stuart Biles, Matthias Boettcher, Jacob Eapen, Mbou Eyole, Giacomo Gabrielli, Matt Horsnell, et al. 2017. <span>â€œThe ARM Scalable Vector Extension.â€</span> <em>IEEE Micro</em> 37 (2): 26â€“39. <a href="https://doi.org/10.1109/mm.2017.35">https://doi.org/10.1109/mm.2017.35</a>.
</div><div id="ref-lindholm2008nvidia" class="csl-entry" role="listitem">
Lindholm, Erik, John Nickolls, Stuart Oberman, and John Montrym. 2008. <span>â€œNVIDIA Tesla: A Unified Graphics and Computing Architecture.â€</span> <em>IEEE Micro</em> 28 (2): 39â€“55. <a href="https://doi.org/10.1109/mm.2008.31">https://doi.org/10.1109/mm.2008.31</a>.
</div><div id="fn20"><p><sup>20</sup>&nbsp;<strong>Streaming Multiprocessor (SM)</strong>: NVIDIAâ€™s fundamental GPU compute unit containing multiple CUDA cores, tensor cores, shared memory, and schedulers. Each SM manages 2048+ threads organized into 64 warps (32 threads each), enabling massive parallelism. NVIDIA H100 contains 132 SMs with 128 streaming processors each, totaling 16,896 cores. SMs execute threads in SIMT fashion, with all threads in a warp sharing the same instruction but processing different data.</p></div><div id="fn21"><p><sup>21</sup>&nbsp;<strong>Warp</strong>: NVIDIAâ€™s fundamental execution unit of 32 threads that execute the same instruction simultaneously in lock-step. All threads in a warp share instruction fetch and decode, maximizing instruction throughput. If threads diverge (different control flow), the warp becomes inefficient by serializing execution paths. Modern GPUs achieve best performance when threads in a warp access consecutive memory addresses, enabling memory coalescing.</p></div></div><p>To address these limitations, SIMT extends SIMD principles by enabling parallel execution across multiple independent threads, each maintaining its own program counter and architectural state <span class="citation" data-cites="lindholm2008nvidia">(<a href="#ref-lindholm2008nvidia" role="doc-biblioref">Lindholm et al. 2008</a>)</span>. This model maps naturally to matrix computations, where each thread processes different portions of a workload while still benefiting from shared instruction execution. In NVIDIAâ€™s GPU architectures, each Streaming Multiprocessor (SM)<a href="#fn20" class="footnote-ref" id="fnref20" role="doc-noteref"><sup>20</sup></a> coordinates thousands of threads executing in parallel, allowing for efficient scaling of neural network computations, as demonstrated in <a href="#lst-cuda_simt" class="quarto-xref">Listing&nbsp;16</a>. Threads are organized into warps<a href="#fn21" class="footnote-ref" id="fnref21" role="doc-noteref"><sup>21</sup></a>, which are the fundamental execution units that enable SIMT efficiency.</p>
<div id="lst-cuda_simt" class="listing quarto-float quarto-figure quarto-figure-left anchored">
<figure class="quarto-float quarto-float-lst figure"><figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-lst" id="lst-cuda_simt-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Listing&nbsp;16: <strong>SIMT Execution</strong>: Each thread processes a unique output element in parallel, demonstrating how SIMT enables efficient matrix multiplication on GPUs.
</figcaption><div aria-describedby="lst-cuda_simt-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="sourceCode" id="cb16"><pre class="sourceCode c code-with-copy"><code class="sourceCode c"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>__global__ <span class="dt">void</span> matrix_multiply<span class="op">(</span><span class="dt">float</span><span class="op">*</span> C<span class="op">,</span> <span class="dt">float</span><span class="op">*</span> A<span class="op">,</span> <span class="dt">float</span><span class="op">*</span></span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>                                B<span class="op">,</span> <span class="dt">int</span> N<span class="op">)</span> <span class="op">{</span></span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>    <span class="co">// Each thread processes one output element</span></span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>    <span class="dt">int</span> row <span class="op">=</span> blockIdx<span class="op">.</span>y <span class="op">*</span> blockDim<span class="op">.</span>y <span class="op">+</span> threadIdx<span class="op">.</span>y<span class="op">;</span></span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>    <span class="dt">int</span> col <span class="op">=</span> blockIdx<span class="op">.</span>x <span class="op">*</span> blockDim<span class="op">.</span>x <span class="op">+</span> threadIdx<span class="op">.</span>x<span class="op">;</span></span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a>    <span class="dt">float</span> sum <span class="op">=</span> <span class="fl">0.0</span><span class="bu">f</span><span class="op">;</span></span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> <span class="op">(</span><span class="dt">int</span> k <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> k <span class="op">&lt;</span> N<span class="op">;</span> k<span class="op">++)</span> <span class="op">{</span></span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a>        <span class="co">// Threads in a warp execute in parallel</span></span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a>        sum <span class="op">+=</span> A<span class="op">[</span>row <span class="op">*</span> N <span class="op">+</span> k<span class="op">]</span> <span class="op">*</span> B<span class="op">[</span>k <span class="op">*</span> N <span class="op">+</span> col<span class="op">];</span></span>
<span id="cb16-11"><a href="#cb16-11" aria-hidden="true" tabindex="-1"></a>    <span class="op">}</span></span>
<span id="cb16-12"><a href="#cb16-12" aria-hidden="true" tabindex="-1"></a>    C<span class="op">[</span>row <span class="op">*</span> N <span class="op">+</span> col<span class="op">]</span> <span class="op">=</span> sum<span class="op">;</span></span>
<span id="cb16-13"><a href="#cb16-13" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</figure>
</div>
<p>SIMT execution allows neural network computations to scale efficiently across thousands of threads while maintaining flexibility for divergent execution paths. Similar execution models appear in AMDâ€™s RDNA and Intelâ€™s Xe architectures, reinforcing SIMT as a fundamental mechanism for AI acceleration.</p>
</section><section id="sec-ai-acceleration-tensor-cores-771f" class="level4 page-columns page-full"><h4 class="anchored" data-anchor-id="sec-ai-acceleration-tensor-cores-771f">Tensor Cores</h4>
<p>While SIMD and SIMT units provide efficient execution of vector operations, neural networks rely heavily on matrix computations that require specialized execution units for structured multi-dimensional processing. The energy economics of matrix operations drive this specialization: traditional scalar processing requires multiple DRAM accesses per operation, consuming 640&nbsp;pJ per fetch, while tensor cores amortize this energy cost across entire matrix blocks. Tensor processing units extend SIMD and SIMT principles by enabling efficient matrix operations through dedicated hardware blocks that execute matrix multiplications and accumulations on entire matrix blocks in a single operation. Tensor cores transform the energy profile from 173Ã— memory-bound inefficiency to compute-optimized execution where the 3.7&nbsp;pJ multiply-accumulate operation dominates the energy budget rather than data movement.</p>
<p>Tensor cores<a href="#fn22" class="footnote-ref" id="fnref22" role="doc-noteref"><sup>22</sup></a>, implemented in architectures such as NVIDIAâ€™s Ampere GPUs, provide an example of this approach. They expose matrix computation capabilities through specialized instructions, such as the tensor core operation shown in <a href="#lst-tensor_core_op" class="quarto-xref">Listing&nbsp;17</a> on the NVIDIA A100 GPU.</p>
<div class="no-row-height column-margin column-container"><div id="fn22"><p><sup>22</sup>&nbsp;<strong>Tensor Core Breakthrough</strong>: NVIDIA introduced tensor cores in the V100 (2017) to accelerate the <span class="math inline">\(4\times 4\)</span> matrix operations common in neural networks. The A100â€™s third-generation tensor cores achieve 312 TFLOPS for FP16 tensor operationsâ€”20<span class="math inline">\(\times\)</span> faster than traditional CUDA cores. This single innovation enabled training of models like GPT-3 that would have been impossible with conventional hardware, fundamentally changing the scale of AI research.</p></div></div><div id="lst-tensor_core_op" class="listing quarto-float quarto-figure quarto-figure-left anchored">
<figure class="quarto-float quarto-float-lst figure"><figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-lst" id="lst-tensor_core_op-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Listing&nbsp;17: <strong>Tensor Core Operation</strong>: Matrix multiplications are performed in parallel across entire matrix blocks, optimizing computational efficiency for neural network training.
</figcaption><div aria-describedby="lst-tensor_core_op-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="sourceCode" id="cb17"><pre class="sourceCode c code-with-copy"><code class="sourceCode c"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>Tensor Core Operation <span class="op">(</span>NVIDIA A100<span class="op">):</span></span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>mma<span class="op">.</span>sync<span class="op">.</span>aligned<span class="op">.</span>m16n16k16<span class="op">.</span>f16<span class="op">.</span>f16</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a>  <span class="op">{</span>d0<span class="op">,</span>d1<span class="op">,</span>d2<span class="op">,</span>d3<span class="op">},</span>     <span class="co">// Destination registers</span></span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a>  <span class="op">{</span>a0<span class="op">,</span>a1<span class="op">,</span>a2<span class="op">,</span>a3<span class="op">},</span>     <span class="co">// Source matrix A</span></span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a>  <span class="op">{</span>b0<span class="op">,</span>b1<span class="op">,</span>b2<span class="op">,</span>b3<span class="op">},</span>     <span class="co">// Source matrix B</span></span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a>  <span class="op">{</span>c0<span class="op">,</span>c1<span class="op">,</span>c2<span class="op">,</span>c3<span class="op">}</span>      <span class="co">// Accumulator</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</figure>
</div>
<p>A single tensor core instruction processes an entire matrix block while maintaining intermediate results in local registers, significantly improving computational efficiency compared to implementations based on scalar or vector operations. This structured approach enables hardware to achieve high throughput while reducing the burden of explicit loop unrolling and data management at the software level.</p>
<p>Tensor processing unit architectures differ based on design priorities. NVIDIAâ€™s Ampere architecture incorporates tensor cores optimized for general-purpose deep learning acceleration. Googleâ€™s TPUv4 utilizes large-scale matrix units arranged in systolic arrays to maximize sustained training throughput. Appleâ€™s M1 neural engine<a href="#fn23" class="footnote-ref" id="fnref23" role="doc-noteref"><sup>23</sup></a> integrates smaller matrix processors optimized for mobile inference workloads, while Intelâ€™s Sapphire Rapids architecture introduces AMX tiles designed for high-performance datacenter applications.</p>
<div class="no-row-height column-margin column-container"><div id="fn23"><p><sup>23</sup>&nbsp;<strong>Appleâ€™s Neural Engine Strategy</strong>: Apple introduced the Neural Engine in September 2017â€™s A11 chip to enable on-device ML without draining battery life. The M1â€™s 16-core Neural Engine delivers 11 TOPS while the entire M1 chip has a 20-watt system TDPâ€”enabling real-time features like live text recognition and voice processing without cloud connectivity. This â€œprivacy through hardwareâ€ approach influenced the entire industry to prioritize edge AI capabilities.</p></div></div><p>The increasing specialization of AI hardware has driven significant performance improvements in deep learning workloads. <a href="#fig-ai-performance" class="quarto-xref">Figure&nbsp;3</a> illustrates the trajectory of AI accelerator performance in NVIDIA GPUs, highlighting the transition from general-purpose floating-point execution units to highly optimized tensor processing cores.</p>
<div id="fig-ai-performance" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure"><div aria-describedby="fig-ai-performance-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="images/png/int8_tops_nvidia.png" class="lightbox" data-gallery="quarto-lightbox-gallery-3" title="Figure&nbsp;3: GPU Performance Scaling: NVIDIA GPUs experienced a 10\times increase in integer 8-bit TOPS (tera operations per second) over a decade, driven by architectural innovations transitioning from floating-point to tensor core acceleration. This trend reflects the growing specialization of hardware for deep learning workloads and the increasing demand for efficient inference capabilities."><img src="images/png/int8_tops_nvidia.png" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-ai-performance-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: <strong>GPU Performance Scaling</strong>: NVIDIA GPUs experienced a <span class="math inline">\(10\times\)</span> increase in integer 8-bit TOPS (tera operations per second) over a decade, driven by architectural innovations transitioning from floating-point to tensor core acceleration. This trend reflects the growing specialization of hardware for deep learning workloads and the increasing demand for efficient inference capabilities.
</figcaption></figure>
</div>
</section><section id="sec-ai-acceleration-processing-elements-daa1" class="level4 page-columns page-full"><h4 class="anchored" data-anchor-id="sec-ai-acceleration-processing-elements-daa1">Processing Elements</h4>
<p>The highest level of execution unit organization integrates multiple tensor cores with local memory into processing elements (PEs). A processing element serves as a fundamental building block in many AI accelerators, combining different computational units to efficiently execute neural network operations. Each PE typically includes vector units for element-wise operations, tensor cores for matrix computation, special function units for non-linear transformations, and dedicated memory resources to optimize data locality and minimize data movement overhead.</p>
<p>Processing elements play an essential role in AI hardware by balancing computational density with memory access efficiency. Their design varies across different architectures to support diverse workloads and scalability requirements. Graphcoreâ€™s Intelligence Processing Unit (IPU) distributes computation across 1,472 tiles, each containing independent processing elements optimized for fine-grained parallelism <span class="citation" data-cites="Graphcore2020">(<a href="#ref-Graphcore2020" role="doc-biblioref">Graphcore 2020</a>)</span>. Cerebras extends this approach in the CS-2 system, integrating 850,000 processing elements across a wafer-scale device to accelerate sparse computations. Teslaâ€™s D1 processor arranges processing elements with substantial local memory, optimizing throughput and latency for real-time autonomous vehicle workloads <span class="citation" data-cites="Tesla2021">(<a href="#ref-Tesla2021" role="doc-biblioref">Quinnell 2024</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-Graphcore2020" class="csl-entry" role="listitem">
Graphcore. 2020. <span>â€œThe Colossus MK2 IPU Processor.â€</span> <em>Graphcore Technical Paper</em>.
</div><div id="ref-Tesla2021" class="csl-entry" role="listitem">
Quinnell, Eric. 2024. <span>â€œTesla Transport Protocol over Ethernet (TTPoE): A New Lossy, Exa-Scale Fabric for the Dojo AI Supercomputer.â€</span> In <em>2024 IEEE Hot Chips 36 Symposium (HCS)</em>, 1â€“23. IEEE. <a href="https://doi.org/10.1109/hcs61935.2024.10664947">https://doi.org/10.1109/hcs61935.2024.10664947</a>.
</div></div><p>Processing elements provide the structural foundation for large-scale AI acceleration. Their efficiency depends not only on computational capability but also on interconnect strategies and memory hierarchy design. The next sections explore how these architectural choices impact performance across different AI workloads.</p>
<p>Tensor processing units have enabled substantial efficiency gains in AI workloads by using hardware-accelerated matrix computation. Their role continues to evolve as architectures incorporate support for advanced execution techniques, including structured sparsity and workload-specific optimizations. The effectiveness of these units, however, depends not only on their computational capabilities but also on how they interact with memory hierarchies and data movement mechanisms, which are examined in subsequent sections.</p>
</section><section id="sec-ai-acceleration-systolic-arrays-6fa8" class="level4 page-columns page-full"><h4 class="anchored" data-anchor-id="sec-ai-acceleration-systolic-arrays-6fa8">Systolic Arrays</h4>
<p>While tensor cores package matrix operations into structured computational units, systolic arrays provide an alternative approach optimized for continuous data flow and operand reuse. The fundamental motivation for systolic architectures stems from the energy efficiency constraints discussed earlierâ€”minimizing the impact of memory access penalties through architectural design. A systolic array arranges processing elements in a grid pattern, where data flows rhythmically between neighboring units in a synchronized manner, enabling each operand to participate in multiple computations as it propagates through the array. This structured movement minimizes external memory accesses by maximizing local data reuseâ€”a single weight value can contribute to dozens of operations as it moves through the processing elements, fundamentally transforming the energy profile from memory-bound to compute-efficient execution.</p>
<p>The concept of systolic arrays was first introduced by Kung and Leiserson<a href="#fn24" class="footnote-ref" id="fnref24" role="doc-noteref"><sup>24</sup></a>, who formalized their use in parallel computing architectures for efficient matrix operations <span class="citation" data-cites="Kung1982">(<a href="#ref-Kung1982" role="doc-biblioref">Kung 1982</a>)</span>. Unlike general-purpose execution units, systolic arrays exploit spatial and temporal locality by reusing operands as they propagate through the grid. Googleâ€™s TPU exemplifies this architectural approach. In the TPUv4, a <span class="math inline">\(128\times128\)</span> systolic array of multiply-accumulate units processes matrix operations by streaming data through the array in a pipelined manner, as shown in <a href="#fig-systolic-array" class="quarto-xref">Figure&nbsp;4</a>.</p>
<div class="no-row-height column-margin column-container"><div id="fn24"><p><sup>24</sup>&nbsp;<strong>Systolic Array Renaissance</strong>: H.T. Kung and Charles Leiserson introduced systolic arrays at CMU in 1979 for VLSI signal processing, but the concept languished for decades due to programming complexity. Googleâ€™s 2016 TPU resurrection proved these â€œheartbeatâ€ architectures could deliver massive efficiency gains for neural networksâ€”the TPUv1â€™s <span class="math inline">\(256\times 256\)</span> systolic array achieved 92 TOPS for 8-bit integer operations while consuming just 40 watts, making systolic arrays the dominant AI architecture today.</p></div><div id="ref-Kung1982" class="csl-entry" role="listitem">
Kung. 1982. <span>â€œWhy Systolic Architectures?â€</span> <em>Computer</em> 15 (1): 37â€“46. <a href="https://doi.org/10.1109/mc.1982.1653825">https://doi.org/10.1109/mc.1982.1653825</a>.
</div></div><p>The systolic array architecture achieves computational efficiency through synchronized data movement across a structured grid of processing elements. Systolic arrays organize computation around four fundamental components:</p>
<ol type="1">
<li>
<strong>Control Unit</strong>: Coordinates timing and data distribution across the array, maintaining synchronized operation throughout the computational grid</li>
<li>Data Streams: Input matrices propagate through coordinated pathwaysâ€”matrix A elements traverse horizontally while matrix B elements flow vertically through the processing grid</li>
<li>
<strong>Processing Element Grid</strong>: Individual processing elements execute multiply-accumulate operations on streaming data, generating partial results that accumulate toward the final computation</li>
<li>
<strong>Output Collection</strong>: Results aggregate at designated output boundaries where accumulated partial sums form complete matrix elements</li>
</ol>
<p>The synchronized data flow ensures that matrix element A[i,k] encounters corresponding B[k,j] elements at precise temporal intervals, executing the multiply-accumulate operations required for matrix multiplication C[i,j] = Î£ A[i,k] Ã— B[k,j]. This systematic reuse of operands across multiple processing elements substantially reduces memory bandwidth requirements by eliminating redundant data fetches from external memory subsystems.</p>
<p>Consider the multiplication of 2Ã—2 matrices A and B within a systolic array. During the first computational cycle, element A[0,0]=2 propagates horizontally while B[0,0]=1 moves vertically, converging at processing element PE(0,0) to execute the multiplication 2Ã—1=2. In the subsequent cycle, the same A[0,0]=2 advances to PE(0,1) where it encounters B[0,1]=3, computing 2Ã—3=6. Concurrently, A[0,1]=4 enters PE(0,0) to engage with the next B matrix element. This coordinated data movement enables systematic operand reuse across multiple computational operations, eliminating redundant memory accesses and exemplifying the fundamental efficiency principle underlying systolic array architectures.</p>
<div id="fig-systolic-array" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-pos="htb" data-fig-env="figure">
<figure class="quarto-float quarto-float-fig figure"><div aria-describedby="fig-systolic-array-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="39ea304c244f2c4267e1f8d4d12d87e8b95dee80.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-4" title="Figure&nbsp;4: Systolic Array Dataflow: Processing elements within the array execute matrix operations by streaming data in a pipelined manner, maximizing operand reuse and minimizing memory access compared to traditional memory-compute architectures. This spatial and temporal locality enables efficient parallel computation, as exemplified by the multiply-accumulate units in Googleâ€™s tpuv4."><img src="hw_acceleration_files/mediabag/39ea304c244f2c4267e1f8d4d12d87e8b95dee80.svg" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-systolic-array-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4: <strong>Systolic Array Dataflow</strong>: Processing elements within the array execute matrix operations by streaming data in a pipelined manner, maximizing operand reuse and minimizing memory access compared to traditional memory-compute architectures. This spatial and temporal locality enables efficient parallel computation, as exemplified by the multiply-accumulate units in Googleâ€™s tpuv4.
</figcaption></figure>
</div>
<p>Each processing element in the array performs a multiply-accumulate operation in every cycle:</p>
<ol type="1">
<li>Receives an input activation from above</li>
<li>Receives a weight value from the left</li>
<li>Multiplies these values and adds to its running sum</li>
<li>Passes the input activation downward and the weight value rightward to neighboring elements</li>
</ol>
<p>This structured computation model minimizes data movement between global memory and processing elements, improving both efficiency and scalability. As systolic arrays operate in a streaming fashion, they are particularly effective for high-throughput workloads such as deep learning training and inference.</p>
<p>While the diagram in <a href="#fig-systolic-array" class="quarto-xref">Figure&nbsp;4</a> illustrates one common systolic array implementation, systolic architectures vary significantly across different accelerator designs. Training-focused architectures like Googleâ€™s TPU employ large arrays optimized for high computational throughput, while inference-oriented designs found in edge devices prioritize energy efficiency with smaller configurations.</p>
<p>The fundamental principle remains consistent: data flows systematically through processing elements, with inputs moving horizontally and vertically to compute partial sums in a synchronized fashion. However, as detailed in <a href="#sec-ai-acceleration-understanding-ai-memory-wall-3ea9" class="quarto-xref">Section&nbsp;1.4.1</a>, practical effectiveness is ultimately constrained by memory bandwidth bottlenecks.</p>
<p>A 128Ã—128 systolic array capable of 16,384 operations per cycle requires continuous data feed to maintain utilizationâ€”each cycle demands fresh input activations and weight parameters that must traverse from off-chip memory through on-chip buffers to the array edges. The TPUâ€™s 1,200 GB/s on-chip bandwidth enables high utilization, but even this substantial bandwidth becomes limiting when processing large transformer models where memory requirements exceed on-chip capacity.</p>
<p>Recall from <strong><a href="../optimizations/optimizations.html#sec-model-optimizations">Chapter 10: Model Optimizations</a></strong> that quantization reduces model memory footprint by converting FP32 weights to INT8 representationsâ€”this optimization directly addresses the memory bandwidth constraints identified here. Converting 32-bit floating-point weights to 8-bit integers reduces memory traffic by 4Ã—, transforming bandwidth-bound operations into compute-bound workloads where systolic arrays can achieve higher utilization. Similarly, structured pruning removes entire rows or columns of weight matrices, reducing both the data volume that must traverse memory hierarchies and the computation required. These algorithmic optimizations prove valuable precisely because they target the memory bottleneck that limits accelerator performance in practice.</p>
</section><section id="sec-ai-acceleration-numerics-ai-acceleration-f7be" class="level4"><h4 class="anchored" data-anchor-id="sec-ai-acceleration-numerics-ai-acceleration-f7be">Numerics in AI Acceleration</h4>
<p>The efficiency of AI accelerators is not determined by computational power alone but also by the precision of numerical representations. The choice of numerical format shapes the balance between accuracy, throughput, and energy consumption, influencing how different execution units, such as SIMD and SIMT units, tensor cores, and systolic arrays, are designed and deployed.</p>
<section id="sec-ai-acceleration-precision-tradeoffs-8fa8" class="level5"><h5 class="anchored" data-anchor-id="sec-ai-acceleration-precision-tradeoffs-8fa8">Precision Trade-offs</h5>
<p>Numerical precision represents a critical design parameter in modern AI accelerators. While higher precision formats provide mathematical stability and accuracy, they come with substantial costs in terms of power consumption, memory bandwidth, and computational throughput. Finding the optimal precision point has become a central challenge in AI hardware architecture.</p>
<p>Early deep learning models primarily relied on single-precision floating point (FP32) for both training and inference. While FP32 offers sufficient dynamic range and precision for stable learning, it imposes high computational and memory costs, limiting efficiency, especially as model sizes increase. Over time, hardware architectures evolved to support lower precision formats such as half-precision floating point (FP16) and bfloat16 (BF16), which reduce memory usage and increase computational throughput while maintaining sufficient accuracy for deep learning tasks. More recently, integer formats (INT8, INT4) have gained prominence in inference workloads, where small numerical representations significantly improve energy efficiency without compromising model accuracy beyond acceptable limits.</p>
<p>The transition from high-precision to lower-precision formats is deeply integrated into hardware execution models. As detailed in <a href="#sec-ai-acceleration-evolution-simd-simt-architectures-e1fd" class="quarto-xref">Section&nbsp;1.3.4.2</a>, SIMD and SIMT units provide flexible support for multiple precisions. Tensor cores (<a href="#sec-ai-acceleration-tensor-cores-771f" class="quarto-xref">Section&nbsp;1.3.4.3</a>) accelerate computation using reduced-precision arithmetic, while systolic arrays (<a href="#sec-ai-acceleration-systolic-arrays-6fa8" class="quarto-xref">Section&nbsp;1.3.4.5</a>) optimize performance by minimizing memory bandwidth constraints through low-precision formats that maximize operand reuse.</p>
<p>Despite the advantages of reduced precision, deep learning models cannot always rely solely on low-bit representations. To address this challenge, modern AI accelerators implement mixed-precision computing, where different numerical formats are used at different stages of execution. These precision choices have important implications for model fairness and reliability. For example, matrix multiplications may be performed in FP16 or BF16, while accumulations are maintained in FP32 to prevent precision loss. Similarly, inference engines leverage INT8 arithmetic while preserving key activations in higher precision when necessary.</p>
</section><section id="sec-ai-acceleration-mixedprecision-computing-656f" class="level5"><h5 class="anchored" data-anchor-id="sec-ai-acceleration-mixedprecision-computing-656f">Mixed-Precision Computing</h5>
<p>Modern AI accelerators increasingly support mixed-precision execution, allowing different numerical formats to be used at various stages of computation. Training workloads often leverage FP16 or BF16 for matrix multiplications, while maintaining FP32 accumulations to preserve precision. Inference workloads, by contrast, optimize for INT8 or even INT4, achieving high efficiency while retaining acceptable accuracy.</p>
<p>This shift toward precision diversity is evident in the evolution of AI hardware. Early architectures such as NVIDIA Volta provided limited support for lower precision beyond FP16, whereas later architectures, including Turing and Ampere, expanded the range of supported formats. Ampere GPUs introduced TF32 as a hybrid between FP32 and FP16, alongside broader support for BF16, INT8, and INT4. <a href="#tbl-nvidia-numerics" class="quarto-xref">Table&nbsp;5</a> illustrates this trend.</p>
<div id="tbl-nvidia-numerics" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure"><figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-nvidia-numerics-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;5: <strong>Precision Support Evolution</strong>: GPU architectures progressively expanded support for lower-precision data types, enabling performance gains and efficiency improvements in AI workloads. Early architectures primarily utilized FP32, while later generations incorporated FP16, BF16, INT8, and INT4 to accelerate both training and inference tasks.
</figcaption><div aria-describedby="tbl-nvidia-numerics-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<colgroup>
<col style="width: 17%">
<col style="width: 10%">
<col style="width: 37%">
<col style="width: 33%">
</colgroup>
<thead><tr class="header">
<th style="text-align: left;"><strong>Architecture</strong></th>
<th style="text-align: right;"><strong>Year</strong></th>
<th style="text-align: right;"><strong>Supported Tensor Core Precisions</strong></th>
<th style="text-align: right;"><strong>Supported CUDA Core Precisions</strong></th>
</tr></thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><strong>Volta</strong></td>
<td style="text-align: right;">2017</td>
<td style="text-align: right;">FP16</td>
<td style="text-align: right;">FP64, FP32, FP16</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Turing</strong></td>
<td style="text-align: right;">2018</td>
<td style="text-align: right;">FP16, INT8</td>
<td style="text-align: right;">FP64, FP32, FP16, INT8</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>Ampere</strong></td>
<td style="text-align: right;">2020</td>
<td style="text-align: right;">FP64, TF32, bfloat16, FP16, INT8, INT4</td>
<td style="text-align: right;">FP64, FP32, FP16, bfloat16, INT8</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p><a href="#tbl-nvidia-numerics" class="quarto-xref">Table&nbsp;5</a> highlights how newer architectures incorporate a growing diversity of numerical formats, reflecting the need for greater flexibility across different AI workloads. This trend suggests that future AI accelerators will continue expanding support for adaptive precision, optimizing both computational efficiency and model accuracy.</p>
<p>The precision format used in hardware design has far-reaching implications. By adopting lower-precision formats, the data transferred between execution units and memory is reduced, leading to decreased memory bandwidth requirements and storage. Tensor cores and systolic arrays can process more lower-precision elements in parallel, thereby increasing the effective throughput in terms of FLOPs. Energy efficiency is also improved, as integer-based computations (e.g., INT8) require lower power compared to floating-point arithmeticâ€”a clear advantage for inference workloads.</p>
<p>As AI models continue to scale in size, accelerator architectures are evolving to support more efficient numerical formats. Future designs are expected to incorporate adaptive precision techniques, dynamically adjusting computation precision based on workload characteristics. This evolution promises further optimization of deep learning performance while striking an optimal balance between accuracy and energy efficiency.</p>
</section></section><section id="sec-ai-acceleration-architectural-integration-01b6" class="level4"><h4 class="anchored" data-anchor-id="sec-ai-acceleration-architectural-integration-01b6">Architectural Integration</h4>
<p>The organization of computational primitives into execution units determines the efficiency of AI accelerators. While SIMD, tensor cores, and systolic arrays serve as fundamental building blocks, their integration into full-chip architectures varies significantly across different AI processors. The choice of execution units, their numerical precision support, and their connectivity impact how effectively hardware can scale for deep learning workloads.</p>
<p>Modern AI processors exhibit a range of design trade-offs based on their intended applications. Some architectures, such as NVIDIAâ€™s A100, integrate large numbers of tensor cores optimized for FP16-based training, while Googleâ€™s TPUv4 prioritizes high-throughput BF16 matrix multiplications. Inference-focused processors, such as Intelâ€™s Sapphire Rapids, incorporate INT8-optimized tensor cores to maximize efficiency. The Apple M1, designed for mobile workloads, employs smaller processing elements optimized for low-power FP16 execution. These design choices reflect the growing flexibility in numerical precision and execution unit organization, as discussed in the previous section.</p>
<p><a href="#tbl-execution-units" class="quarto-xref">Table&nbsp;6</a> summarizes the execution unit configurations across contemporary AI processors.</p>
<div id="tbl-execution-units" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure"><figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-execution-units-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;6: <strong>AI Processor Configurations</strong>: Modern AI processors prioritize different execution unit characteristics to optimize performance for specific workloads; NVIDIA A100 leverages wide SIMD and tensor cores for training, Google TPUv4 emphasizes high-throughput BF16 matrix multiplication, and Intel Sapphire Rapids focuses on INT8-optimized inference, while apple M1 prioritizes low-power FP16 execution on smaller processing elements. These variations in SIMD width, tensor core size, and processing element count reflect the growing diversity in AI hardware architectures and their targeted applications.
</figcaption><div aria-describedby="tbl-execution-units-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<colgroup>
<col style="width: 18%">
<col style="width: 14%">
<col style="width: 21%">
<col style="width: 22%">
<col style="width: 20%">
</colgroup>
<thead><tr class="header">
<th style="text-align: right;"><strong>Processor</strong></th>
<th style="text-align: right;"><strong>SIMD Width</strong></th>
<th style="text-align: right;"><strong>Tensor Core Size</strong></th>
<th style="text-align: right;"><strong>Processing Elements</strong></th>
<th style="text-align: left;"><strong>Primary Workloads</strong></th>
</tr></thead>
<tbody>
<tr class="odd">
<td style="text-align: right;"><strong>NVIDIA A100</strong></td>
<td style="text-align: right;">1024-bit</td>
<td style="text-align: right;">
<span class="math inline">\(4\times4\times4\)</span> FP16</td>
<td style="text-align: right;">108 SMs</td>
<td style="text-align: left;">Training, HPC</td>
</tr>
<tr class="even">
<td style="text-align: right;"><strong>Google TPUv4</strong></td>
<td style="text-align: right;">128-wide</td>
<td style="text-align: right;">
<span class="math inline">\(128\times128\)</span> BF16</td>
<td style="text-align: right;">2 cores/chip</td>
<td style="text-align: left;">Training</td>
</tr>
<tr class="odd">
<td style="text-align: right;"><strong>Intel Sapphire</strong></td>
<td style="text-align: right;">512-bit AVX</td>
<td style="text-align: right;">
<span class="math inline">\(32\times32\)</span> INT8/BF16</td>
<td style="text-align: right;">56 cores</td>
<td style="text-align: left;">Inference</td>
</tr>
<tr class="even">
<td style="text-align: right;"><strong>Apple M1</strong></td>
<td style="text-align: right;">128-bit NEON</td>
<td style="text-align: right;">
<span class="math inline">\(16\times16\)</span> FP16</td>
<td style="text-align: right;">8 NPU cores</td>
<td style="text-align: left;">Mobile inference</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p><a href="#tbl-execution-units" class="quarto-xref">Table&nbsp;6</a> highlights how execution unit configurations vary across architectures to optimize for different deep learning workloads. Training accelerators prioritize high-throughput floating-point tensor operations, whereas inference processors focus on low-precision integer execution for efficiency. Meanwhile, mobile accelerators balance precision and power efficiency to meet real-time constraints.</p>
</section></section><section id="sec-ai-acceleration-costperformance-analysis-e925" class="level3"><h3 class="anchored" data-anchor-id="sec-ai-acceleration-costperformance-analysis-e925">Cost-Performance Analysis</h3>
<p>While architectural specifications define computational potential, practical deployment decisions require understanding cost-performance trade-offs across different accelerator options. However, raw computational metrics alone provide an incomplete pictureâ€”the fundamental constraint in modern AI acceleration is not compute capacity but data movement efficiency.</p>
<p>The energy differential established earlierâ€”where memory access costs dominate computationâ€”drives the entire specialized hardware revolution. This disparity explains why GPUs with high memory bandwidth achieve 40-60% utilization, while TPUs with systolic arrays achieve 85% utilization by minimizing data movement.</p>
<p><a href="#tbl-accelerator-economics" class="quarto-xref">Table&nbsp;7</a> provides concrete cost-performance data for representative accelerators, but the economic analysis must account for utilization efficiency and energy consumption patterns that determine real-world performance.</p>
<div id="tbl-accelerator-economics" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure"><figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-accelerator-economics-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;7: <strong>Accelerator Cost-Performance Comparison</strong>: Hardware costs must be evaluated against computational capabilities to determine optimal deployment strategies. While newer accelerators like H100 offer better price-performance ratios, total cost of ownership includes power consumption, cooling requirements, and infrastructure costs that significantly impact operational economics. *TPU pricing estimated from cloud rates.
</figcaption><div aria-describedby="tbl-accelerator-economics-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<colgroup>
<col style="width: 17%">
<col style="width: 19%">
<col style="width: 20%">
<col style="width: 19%">
<col style="width: 20%">
</colgroup>
<thead><tr class="header">
<th style="text-align: right;"><strong>Accelerator</strong></th>
<th style="text-align: right;"><strong>List Price (USD)</strong></th>
<th style="text-align: right;"><strong>Peak FLOPS (FP16)</strong></th>
<th style="text-align: right;"><strong>Memory Bandwidth</strong></th>
<th style="text-align: right;"><strong>Price/Performance</strong></th>
</tr></thead>
<tbody>
<tr class="odd">
<td style="text-align: right;"><strong>NVIDIA V100</strong></td>
<td style="text-align: right;">~$9,000 (2017-19)</td>
<td style="text-align: right;">125 TFLOPS</td>
<td style="text-align: right;">900 GB/s</td>
<td style="text-align: right;">$72/TFLOP</td>
</tr>
<tr class="even">
<td style="text-align: right;"><strong>NVIDIA A100</strong></td>
<td style="text-align: right;">$15,000</td>
<td style="text-align: right;">312 TFLOPS (FP16)</td>
<td style="text-align: right;">1,935 GB/s</td>
<td style="text-align: right;">$48/TFLOP</td>
</tr>
<tr class="odd">
<td style="text-align: right;"><strong>NVIDIA H100</strong></td>
<td style="text-align: right;">$25,000-30,000</td>
<td style="text-align: right;">756 TFLOPS (TF32)</td>
<td style="text-align: right;">3,350 GB/s</td>
<td style="text-align: right;">$33/TFLOP</td>
</tr>
<tr class="even">
<td style="text-align: right;"><strong>Google TPUv4</strong></td>
<td style="text-align: right;">~$8,000*</td>
<td style="text-align: right;">275 TFLOPS (BF16)</td>
<td style="text-align: right;">1,200 GB/s</td>
<td style="text-align: right;">$29/TFLOP</td>
</tr>
<tr class="odd">
<td style="text-align: right;"><strong>Intel Gaudi 2</strong></td>
<td style="text-align: right;">$12,000</td>
<td style="text-align: right;">200 TFLOPS (INT8)</td>
<td style="text-align: right;">800 GB/s</td>
<td style="text-align: right;">$60/TFLOP</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p>A startup training large language models faces the choice between 8 V100s ($72K) providing 1,000 TFLOPS or 4 A100s ($60K) delivering 1,248 TFLOPS. However, performance analysis reveals the true performance storyâ€”transformer training with its arithmetic intensity of 0.5-2 FLOPS/byte makes both configurations memory-bandwidth bound rather than compute-bound. The A100â€™s 1,935 GB/s bandwidth delivers 2.15Ã— higher sustainable performance than V100â€™s 900 GB/s, making the effective performance gain 115% rather than the 25% suggested by peak FLOPS. When combined with 17% lower hardware cost and 30% better energy efficiency (400&nbsp;W vs 300&nbsp;W per effective TFLOP), the A100 configuration provides compelling economic advantages that compound over multi-year deployments.</p>
<p>These cost dynamics explain the rapid adoption of newer accelerators despite higher unit prices. The H100â€™s $33/TFLOP represents a 54% improvement over V100â€™s $72/TFLOP, but more importantly, its 3,350 GB/s bandwidth enables 3.7Ã— higher memory throughput per dollarâ€”the metric that determines real-world transformer performance. Cloud deployment further complicates the analysis, as providers typically charge $2-4/hour for high-end accelerators, making the break-even point between purchase and rental highly dependent on utilization patterns and energy costs that can account for 60-70% of total operational expenses over a three-year lifecycle.</p>
<p>Framework selection significantly impacts these economic decisionsâ€”detailed hardware-framework optimization strategies are covered in <strong><a href="../frameworks/frameworks.html#sec-ai-frameworks">Chapter 7: AI Frameworks</a></strong>, while performance evaluation methodologies are discussed in <strong><a href="../benchmarking/benchmarking.html#sec-benchmarking-ai">Chapter 12: Benchmarking AI</a></strong>.</p>
<p>While execution units define the compute potential of an accelerator, their effectiveness is fundamentally constrained by data movement and memory hierarchy. Achieving high utilization of compute resources requires efficient memory systems that minimize data transfer overhead and optimize locality. Understanding these constraints reveals why memory architecture becomes as critical as computational design in AI acceleration.</p>
<div id="quiz-question-sec-ai-acceleration-ai-compute-primitives-8471" class="callout callout-quiz-question">
<details class="callout-quiz-question fbx-default closebutton"><summary><strong>Self-Check: Question 1.3</strong></summary><div>
<ol type="1">
<li>
<p>What is the primary role of AI compute primitives in neural network execution?</p>
<ol type="a">
<li>To optimize the execution of core computational patterns in neural networks</li>
<li>To provide a high-level programming interface for machine learning frameworks</li>
<li>To replace general-purpose CPUs in all computing tasks</li>
<li>To ensure compatibility across different neural network architectures</li>
</ol>
</li>
<li><p>Explain how vector operations enhance the efficiency of neural network computations in AI accelerators.</p></li>
<li><p>The hardware component that performs non-linear transformations like ReLU and sigmoid in a single cycle is known as the ____.</p></li>
<li><p>Order the following computational steps for executing a dense layer in a neural network: (1) Apply activation function, (2) Multiply inputs by weights, (3) Add bias.</p></li>
<li>
<p>Which of the following is NOT a characteristic of AI compute primitives?</p>
<ol type="a">
<li>They are frequently used in neural network computations.</li>
<li>They offer significant energy efficiency gains.</li>
<li>They are designed to replace all general-purpose computing tasks.</li>
<li>They remain stable across different neural network architectures.</li>
</ol>
</li>
</ol>
<p><a href="#quiz-answer-sec-ai-acceleration-ai-compute-primitives-8471" class="question-label">See Answers â†’</a></p>
</div></details>
</div>
</section></section><section id="sec-ai-acceleration-ai-memory-systems-0057" class="level2 page-columns page-full"><h2 class="anchored" data-anchor-id="sec-ai-acceleration-ai-memory-systems-0057">AI Memory Systems</h2>
<p>The execution units examined in previous sectionsâ€”SIMD units, tensor cores, and systolic arraysâ€”provide impressive computational throughput: modern accelerators achieve 100 to 1000 TFLOPS for neural network operations. Yet these theoretical capabilities remain unrealized in practice when memory subsystems cannot supply data at sufficient rates. This fundamental constraint, termed the AI memory wall, represents the dominant bottleneck in real-world accelerator performance.</p>
<p>Unlike conventional workloads, ML models require frequent access to large volumes of parameters, activations, and intermediate results, leading to substantial memory bandwidth demandsâ€”a challenge that intersects with the data management strategies covered in <strong><a href="../data_engineering/data_engineering.html#sec-data-engineering">Chapter 6: Data Engineering</a></strong>. Modern AI hardware addresses these challenges through advanced memory hierarchies, efficient data movement techniques, and compression strategies that promote efficient execution and improved AI acceleration.</p>
<p>This section examines memory system design through four interconnected perspectives. First, we quantify the growing disparity between computational throughput and memory bandwidth, revealing why the AI memory wall represents the dominant performance constraint in modern accelerators. Second, we explore how memory hierarchies balance competing demands for speed, capacity, and energy efficiency through carefully structured tiers from on-chip SRAM to off-chip DRAM. Third, we analyze communication patterns between host systems and accelerators, exposing transfer bottlenecks that limit end-to-end performance. Finally, we examine how different neural network architecturesâ€”multilayer perceptrons, convolutional networks, and transformersâ€”create distinct memory pressure patterns that inform hardware design decisions and optimization strategies.</p>
<section id="sec-ai-acceleration-understanding-ai-memory-wall-3ea9" class="level3 page-columns page-full"><h3 class="anchored" data-anchor-id="sec-ai-acceleration-understanding-ai-memory-wall-3ea9">Understanding the AI Memory Wall</h3>
<p>The AI memory wall represents the fundamental bottleneck constraining modern accelerator performanceâ€”the growing disparity between computational throughput and memory bandwidth that prevents accelerators from achieving their theoretical capabilities. While compute units can execute millions of operations per second through specialized primitives like vector operations and matrix multiplications, they depend entirely on memory systems to supply the continuous stream of weights, activations, and intermediate results these operations require.</p>
<section id="sec-ai-acceleration-quantifying-computememory-performance-gap-1526" class="level4 page-columns page-full"><h4 class="anchored" data-anchor-id="sec-ai-acceleration-quantifying-computememory-performance-gap-1526">Quantifying the Compute-Memory Performance Gap</h4>
<p>The severity of this constraint becomes apparent when examining scaling trends. Over the past 20 years, peak computational capabilities have scaled at 3.0Ã— every two years, while DRAM bandwidth has grown at only 1.6Ã— during the same period <span class="citation" data-cites="gholami2024ai">(<a href="#ref-gholami2024ai" role="doc-biblioref">Gholami et al. 2024</a>)</span>. This divergence creates an exponentially widening gap where accelerators possess massive computational power but cannot access data quickly enough to utilize it. Modern hardware exemplifies this imbalance: an NVIDIA H100 delivers 989 TFLOPS but only 3.35 TB/s memory bandwidth <span class="citation" data-cites="nvidia2022h100">(<a href="#ref-nvidia2022h100" role="doc-biblioref">Choquette 2023</a>)</span>, requiring 295 operations per byte to achieve full utilizationâ€”far exceeding the 1-10 operations per byte typical in neural networks.</p>
<div class="no-row-height column-margin column-container"><div id="ref-gholami2024ai" class="csl-entry" role="listitem">
Gholami, Amir, Zhewei Yao, Sehoon Kim, Coleman Hooper, Michael W. Mahoney, and Kurt Keutzer. 2024. <span>â€œAI and Memory Wall.â€</span> <em>IEEE Micro</em> 44 (3): 33â€“39. <a href="https://doi.org/10.1109/mm.2024.3373763">https://doi.org/10.1109/mm.2024.3373763</a>.
</div></div><p>The memory wall manifests through three critical constraints. First, the energy disparityâ€”accessing DRAM consumes 640&nbsp;pJ compared to 3.7&nbsp;pJ for computation <span class="citation" data-cites="Horowitz2014">(<a href="#ref-Horowitz2014" role="doc-biblioref">Horowitz 2014</a>)</span>, creating a 173Ã— energy penalty that often limits performance due to power budgets rather than computational capacity. Second, the bandwidth limitationâ€”even TB/s memory systems cannot feed thousands of parallel compute units continuously, forcing 50-70% idle time in typical workloads. Third, the latency hierarchyâ€”off-chip memory access requires hundreds of cycles, creating pipeline stalls that cascade through parallel execution units.</p>
<p>As illustrated in <a href="#fig-compute-memory-imbalance" class="quarto-xref">Figure&nbsp;5</a>, this â€œAI Memory Wallâ€ continues to widen, making memory bandwidth rather than compute the primary constraint in AI acceleration.</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-compute-memory-imbalance" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure"><div aria-describedby="fig-compute-memory-imbalance-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="hw_acceleration_files/figure-html/fig-compute-memory-imbalance-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-5" title="Figure&nbsp;5: AI Memory Wall: The growing disparity between compute performance and memory bandwidth emphasizes the increasing challenge in sustaining peak computational efficiency due to memory constraints. Over the past 20 years, while computational capabilities have advanced rapidly, memory bandwidth has not kept pace, leading to potential bottlenecks in data-intensive applications."><img src="hw_acceleration_files/figure-html/fig-compute-memory-imbalance-1.png" class="img-fluid figure-img" width="672"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-compute-memory-imbalance-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5: <strong>AI Memory Wall</strong>: The growing disparity between compute performance and memory bandwidth emphasizes the increasing challenge in sustaining peak computational efficiency due to memory constraints. Over the past 20 years, while computational capabilities have advanced rapidly, memory bandwidth has not kept pace, leading to potential bottlenecks in data-intensive applications.
</figcaption></figure>
</div>
</div>
</div>
<p>Beyond performance limitations, memory access imposes a significant energy cost. Fetching data from off-chip DRAM consumes far more energy than performing arithmetic operations <span class="citation" data-cites="Horowitz2014">(<a href="#ref-Horowitz2014" role="doc-biblioref">Horowitz 2014</a>)</span>. This inefficiency is particularly evident in machine learning models, where large parameter sizes, frequent memory accesses, and non-uniform data movement patterns exacerbate memory bottlenecks. The energy differential drives architectural decisionsâ€”Googleâ€™s TPU achieves 30-83<span class="math inline">\(\times\)</span> better energy efficiency than contemporary GPUs by minimizing data movement through systolic arrays and large on-chip memory. These design choices demonstrate that energy constraints, not computational limits, often determine practical deployment feasibility.</p>
<div class="no-row-height column-margin column-container"><div id="ref-Horowitz2014" class="csl-entry" role="listitem">
Horowitz, Mark. 2014. <span>â€œ1.1 Computingâ€™s Energy Problem (and What We Can Do about It).â€</span> In <em>2014 IEEE International Solid-State Circuits Conference Digest of Technical Papers (ISSCC)</em>. IEEE. <a href="https://doi.org/10.1109/isscc.2014.6757323">https://doi.org/10.1109/isscc.2014.6757323</a>.
</div></div></section><section id="sec-ai-acceleration-memory-access-patterns-ml-workloads-a960" class="level4 page-columns page-full"><h4 class="anchored" data-anchor-id="sec-ai-acceleration-memory-access-patterns-ml-workloads-a960">Memory Access Patterns in ML Workloads</h4>
<p>Machine learning workloads place substantial demands on memory systems due to the large volume of data involved in computation. Unlike traditional compute-bound applications, where performance is often dictated by the speed of arithmetic operations, ML workloads are characterized by high data movement requirements. The efficiency of an accelerator is not solely determined by its computational throughput but also by its ability to continuously supply data to processing units without introducing stalls or delays.</p>
<p>A neural network processes multiple types of data throughout its execution, each with distinct memory access patterns:</p>
<ul>
<li>
<strong>Model parameters (weights and biases)</strong>: Machine learning models, particularly those used in large-scale applications such as natural language processing and computer vision, often contain millions to billions of parameters. Storing and accessing these weights efficiently is essential for maintaining throughput.</li>
<li>
<strong>Intermediate activations</strong>: During both training and inference, each layer produces intermediate results that must be temporarily stored and retrieved for subsequent operations. These activations can contribute significantly to memory overhead, particularly in deep architectures.</li>
<li>
<strong>Gradients (during training)</strong>: Backpropagation requires storing and accessing gradients for every parameter, further increasing the volume of data movement between compute units and memory.</li>
</ul>
<p>As models increase in size and complexity, improvements in memory capacity and bandwidth become essential. Although specialized compute units accelerate operations like matrix multiplications, their overall performance depends on the continuous, efficient delivery of data to the processing elements. In large-scale applications, such as natural language processing and computer vision, models often incorporate millions to billions of parameters <span class="citation" data-cites="Brown2020">(<a href="#ref-Brown2020" role="doc-biblioref">Brown et al. 2020</a>)</span>. Consequently, achieving high performance necessitates minimizing delays and stalls caused by inefficient data movement between memory and compute units <span class="citation" data-cites="Narayanan2021 Huang2019">(<a href="#ref-Narayanan2021" role="doc-biblioref">Narayanan et al. 2021</a>; <a href="#ref-Huang2019" role="doc-biblioref">Xingyu 2019</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-Huang2019" class="csl-entry" role="listitem">
Xingyu, Huang et al. 2019. <span>â€œAddressing the Memory Bottleneck in AI Accelerators.â€</span> <em>IEEE Micro</em>.
</div></div><p>One way to quantify this challenge is by comparing the data transfer time with the time required for computations. Specifically, we define the memory transfer time as <span class="math display">\[
T_{\text{mem}} = \frac{M_{\text{total}}}{B_{\text{mem}}},
\]</span> where <span class="math inline">\(M_{\text{total}}\)</span> is the total data volume and <span class="math inline">\(B_{\text{mem}}\)</span> is the available memory bandwidth. In contrast, the compute time is given by <span class="math display">\[
T_{\text{compute}} = \frac{\text{FLOPs}}{P_{\text{peak}}},
\]</span> with the number of floating-point operations (FLOPs) divided by the peak hardware throughput, <span class="math inline">\(P_{\text{peak}}\)</span>. When <span class="math inline">\(T_{\text{mem}} &gt; T_{\text{compute}}\)</span>, the system becomes memory-bound, meaning that the processing elements spend more time waiting for data than performing computations. This imbalance demonstrates the need for memory-optimized architectures and efficient data movement strategies to sustain high performance.</p>
<p><a href="#fig-memory-wall" class="quarto-xref">Figure&nbsp;6</a> demonstrates the emerging challenge between model growth and hardware memory capabilities, illustrating the â€œAI Memory Wall.â€ The figure tracks AI model sizes (red dots) and hardware memory bandwidth (blue dots) over time on a log scale. Model parameters have grown exponentially, from AlexNetâ€™s ~62.3M parameters in 2012 to Gemini 1â€™s trillion-scale parameters in 2023, as shown by the steeper red trend line. In contrast, hardware memory bandwidth, represented by successive generations of NVIDIA GPUs (~100-200 GB/s) and Google TPUs (~2-3 TB/s), has increased more gradually (blue trend line). The expanding shaded region between these trends corresponds to the â€œAI Memory Wall,â€ which will be an architectural challenge where model scaling outpaces available memory bandwidth. This growing disparity necessitates increasingly sophisticated memory management and model optimization techniques to maintain computational efficiency.</p>
</section><section id="sec-ai-acceleration-irregular-memory-access-c6ec" class="level4 page-columns page-full"><h4 class="anchored" data-anchor-id="sec-ai-acceleration-irregular-memory-access-c6ec">Irregular Memory Access</h4>
<p>Unlike traditional computing workloads, where memory access follows well-structured and predictable patterns, machine learning models often exhibit irregular memory access behaviors that make efficient data retrieval a challenge. These irregularities arise due to the nature of ML computations, where memory access patterns are influenced by factors such as batch size, layer type, and sparsity. As a result, standard caching mechanisms and memory hierarchies often struggle to optimize performance, leading to increased memory latency and inefficient bandwidth utilization.</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-memory-wall" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure"><div aria-describedby="fig-memory-wall-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="hw_acceleration_files/figure-html/fig-memory-wall-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-6" title="Figure&nbsp;6: AI Memory Wall: The figure emphasizes the growing disparity between model sizes and hardware memory bandwidths, illustrating the challenge in sustaining performance as models become more complex."><img src="hw_acceleration_files/figure-html/fig-memory-wall-1.png" class="img-fluid figure-img" width="672"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-memory-wall-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6: <strong>AI Memory Wall</strong>: The figure emphasizes the growing disparity between model sizes and hardware memory bandwidths, illustrating the challenge in sustaining performance as models become more complex.
</figcaption></figure>
</div>
</div>
</div>
<p>To better understand how ML workloads differ from traditional computing workloads, it is useful to compare their respective memory access patterns (<a href="#tbl-traditional-vs-ml-mem" class="quarto-xref">Table&nbsp;8</a>). Traditional workloads, such as scientific computing, general-purpose CPU applications, and database processing, typically exhibit well-defined memory access characteristics that benefit from standard caching and prefetching techniques. ML workloads, on the other hand, introduce highly dynamic access patterns that challenge conventional memory optimization strategies.</p>
<p>One key source of irregularity in ML workloads stems from batch size and execution order. The way input data is processed in batches directly affects memory reuse, creating a complex optimization challenge. Small batch sizes decrease the likelihood of reusing cached activations and weights, resulting in frequent memory fetches from slower, off-chip memory. Larger batch sizes can improve reuse and amortize memory access costs, but simultaneously place higher demands on available memory bandwidth, potentially creating congestion at different memory hierarchy levels. This delicate balance requires careful consideration of model architecture and available hardware resources.</p>
<div id="tbl-traditional-vs-ml-mem" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure"><figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-traditional-vs-ml-mem-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;8: <strong>Memory Access Characteristics</strong>: Traditional workloads exhibit predictable, sequential memory access, benefiting from standard caching, while machine learning workloads introduce irregular and dynamic patterns due to sparsity and data dependencies that challenge conventional memory optimization techniques. Understanding these differences is crucial for designing memory systems that efficiently support the unique demands of modern AI applications.
</figcaption><div aria-describedby="tbl-traditional-vs-ml-mem-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<colgroup>
<col style="width: 20%">
<col style="width: 42%">
<col style="width: 36%">
</colgroup>
<thead><tr class="header">
<th style="text-align: left;"><strong>Feature</strong></th>
<th style="text-align: left;"><strong>Traditional Computing Workloads</strong></th>
<th style="text-align: left;"><strong>Machine Learning Workloads</strong></th>
</tr></thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><strong>Memory Access Pattern</strong></td>
<td style="text-align: left;">Regular and predictable (e.g., sequential reads, structured patterns)</td>
<td style="text-align: left;">Irregular and dynamic (e.g., sparsity, attention mechanisms)</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Cache Locality</strong></td>
<td style="text-align: left;">High temporal and spatial locality</td>
<td style="text-align: left;">Often low locality, especially in large models</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>Data Reuse</strong></td>
<td style="text-align: left;">Structured loops with frequent data reuse</td>
<td style="text-align: left;">Sparse and dynamic reuse depending on layer type</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Data Dependencies</strong></td>
<td style="text-align: left;">Well-defined dependencies allow efficient prefetching</td>
<td style="text-align: left;">Variable dependencies based on network structure</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>Workload Example</strong></td>
<td style="text-align: left;">Scientific computing (e.g., matrix factorizations, physics simulations)</td>
<td style="text-align: left;">Neural networks (e.g., CNNs, Transformers, sparse models)</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Memory Bottleneck</strong></td>
<td style="text-align: left;">DRAM latency, cache misses</td>
<td style="text-align: left;">Off-chip bandwidth constraints, memory fragmentation</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>Impact on Energy Consumption</strong></td>
<td style="text-align: left;">Moderate, driven by FLOP-heavy execution</td>
<td style="text-align: left;">High, dominated by data movement costs</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p>Different neural network layers interact with memory in distinct ways beyond batch size considerations. Convolutional layers benefit from spatial locality, as neighboring pixels in an image are processed together, enabling efficient caching of small weight kernels. Conversely, fully connected layers require frequent access to large weight matrices, often leading to more randomized memory access patterns that poorly align with standard caching policies. Transformers introduce additional complexity, as attention mechanisms demand accessing large key-value pairs stored across varied memory locations. The dynamic nature of sequence length and attention span renders traditional prefetching strategies ineffective, resulting in unpredictable memory latencies.</p>
<p>Another significant factor contributing to irregular memory access is sparsity<a href="#fn25" class="footnote-ref" id="fnref25" role="doc-noteref"><sup>25</sup></a> in neural networks. Many modern ML models employ techniques such as weight pruning, activation sparsity, and structured sparsity to reduce computational overhead. However, these optimizations often lead to non-uniform memory access, as sparse representations necessitate fetching scattered elements rather than sequential blocks, making hardware caching less effective. Models that incorporate dynamic computation paths, such as Mixture of Experts and Adaptive Computation Time, introduce highly non-deterministic memory access patterns, where the active neurons or model components can vary with each inference step. This variability challenges efficient prefetching and caching strategies.</p>
<div class="no-row-height column-margin column-container"><div id="fn25"><p><sup>25</sup>&nbsp;<strong>Sparsity in Neural Networks</strong>: The property that most weights or activations in a neural network are zero or near-zero, enabling computational and memory optimizations. Natural sparsity occurs when ReLU activations zero out 50-90% of values, while artificial sparsity results from pruning techniques that remove 90-99% of weights with minimal accuracy loss. Sparse networks can be 10-100<span class="math inline">\(\times\)</span> smaller and faster, but require specialized hardware support (like NVIDIAâ€™s 2:4 sparsity in A100) or software optimization to realize benefits, as standard dense hardware performs zero multiplications inefficiently.</p></div></div><p>These irregularities have significant consequences. ML workloads often experience reduced cache efficiency, as activations and weights may not be accessed in predictable sequences. This leads to increased reliance on off-chip memory traffic, which slows down execution and consumes more energy. Irregular access patterns contribute to memory fragmentation, where the way data is allocated and retrieved results in inefficient utilization of available memory resources. The combined effect is that ML accelerators frequently encounter memory bottlenecks that limit their ability to fully utilize available compute power.</p>
</section></section><section id="sec-ai-acceleration-memory-hierarchy-1839" class="level3 page-columns page-full"><h3 class="anchored" data-anchor-id="sec-ai-acceleration-memory-hierarchy-1839">Memory Hierarchy</h3>
<p>To address the memory challenges in ML acceleration, hardware designers implement sophisticated memory hierarchies that balance speed, capacity, and energy efficiency. Understanding this hierarchy is essential before examining how different ML architectures utilize memory resources. Unlike general-purpose computing, where memory access patterns are often unpredictable, ML workloads exhibit structured reuse patterns that can be optimized through careful organization of data across multiple memory levels.</p>
<p>At the highest level, large-capacity but slow storage devices provide long-term model storage. At the lowest level, high-speed registers and caches ensure that compute units can access operands with minimal latency. Between these extremes, intermediate memory levels, such as scratchpad memory, high-bandwidth memory, and off-chip DRAM, offer trade-offs between performance and capacity.</p>
<p><a href="#tbl-memory-heirarchy" class="quarto-xref">Table&nbsp;9</a> summarizes the key characteristics of different memory levels in modern AI accelerators. Each level in the hierarchy has distinct latency, bandwidth, and capacity properties, which directly influence how neural network data, such as weights, activations, and intermediate results, should be allocated.</p>
<div id="tbl-memory-heirarchy" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure"><figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-memory-heirarchy-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;9: <strong>Memory Hierarchy Trade-Offs</strong>: AI accelerators leverage a multi-level memory hierarchy to balance performance and capacity, optimizing data access for computationally intensive machine learning tasks. Each level provides distinct latency, bandwidth, and capacity characteristics that dictate how neural network componentsâ€”weights, activations, and intermediate resultsâ€”should be strategically allocated to minimize bottlenecks and maximize throughput.
</figcaption><div aria-describedby="tbl-memory-heirarchy-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<colgroup>
<col style="width: 23%">
<col style="width: 13%">
<col style="width: 9%">
<col style="width: 9%">
<col style="width: 42%">
</colgroup>
<thead><tr class="header">
<th style="text-align: left;"><strong>Memory Level</strong></th>
<th style="text-align: right;"><strong>Approx. Latency</strong></th>
<th style="text-align: left;"><strong>Bandwidth</strong></th>
<th style="text-align: left;"><strong>Capacity</strong></th>
<th style="text-align: left;"><strong>Example Use in Deep Learning</strong></th>
</tr></thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><strong>Registers</strong></td>
<td style="text-align: right;">~1 cycle</td>
<td style="text-align: left;">Highest</td>
<td style="text-align: left;">Few values</td>
<td style="text-align: left;">Storing operands for immediate computation</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>L1/L2 Cache (SRAM)</strong></td>
<td style="text-align: right;">~1-10 ns</td>
<td style="text-align: left;">High</td>
<td style="text-align: left;">KBs-MBs</td>
<td style="text-align: left;">Caching frequently accessed activations and small weight blocks</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>Scratchpad Memory</strong></td>
<td style="text-align: right;">~5-20 ns</td>
<td style="text-align: left;">High</td>
<td style="text-align: left;">MBs</td>
<td style="text-align: left;">Software-managed storage for intermediate computations</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>High-Bandwidth Memory (HBM)</strong></td>
<td style="text-align: right;">~100 ns</td>
<td style="text-align: left;">Very High</td>
<td style="text-align: left;">GBs</td>
<td style="text-align: left;">Storing large model parameters and activations for high-speed access</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>Off-Chip DRAM (DDR, GDDR, LPDDR)</strong></td>
<td style="text-align: right;">~50-150 ns</td>
<td style="text-align: left;">Moderate</td>
<td style="text-align: left;">GBs-TBs</td>
<td style="text-align: left;">Storing entire model weights that do not fit on-chip</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Flash Storage (SSD/NVMe)</strong></td>
<td style="text-align: right;">~100 Âµs - 1 ms</td>
<td style="text-align: left;">Low</td>
<td style="text-align: left;">TBs</td>
<td style="text-align: left;">Storing pre-trained models and checkpoints for later loading</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<section id="sec-ai-acceleration-onchip-memory-72d1" class="level4 page-columns page-full"><h4 class="anchored" data-anchor-id="sec-ai-acceleration-onchip-memory-72d1">On-Chip Memory</h4>
<p>Each level of the memory hierarchy serves a distinct role in AI acceleration, with different trade-offs in speed, capacity, and accessibility. Registers, located within compute cores, provide the fastest access but can only store a few operands at a time. These are best utilized for immediate computations, where the operands needed for an operation can be loaded and consumed within a few cycles. However, because register storage is so limited, frequent memory accesses are required to fetch new operands and store intermediate results.</p>
<p>To reduce the need for constant data movement between registers and external memory, small but fast caches serve as an intermediary buffer. These caches store recently accessed activations, weights, and intermediate values, ensuring that frequently used data remains available with minimal delay. However, the size of caches is limited, making them insufficient for storing full feature maps or large weight tensors in machine learning models. As a result, only the most frequently used portions of a modelâ€™s parameters or activations can reside here at any given time.</p>
<p>For larger working datasets, many AI accelerators include scratchpad memory, which offers more storage than caches but with a crucial difference: it allows explicit software control over what data is stored and when it is evicted. Unlike caches, which rely on hardware-based eviction policies, scratchpad memory enables machine learning workloads to retain key values such as activations and filter weights for multiple layers of computation. This capability is particularly useful in models like convolutional neural networks, where the same input feature maps and filter weights are reused across multiple operations. By keeping this data in scratchpad memory rather than reloading it from external memory, accelerators can significantly reduce unnecessary memory transfers and improve overall efficiency <span class="citation" data-cites="Chen2016">(<a href="#ref-Chen2016" role="doc-biblioref">Chen, Emer, and Sze 2017</a>)</span>.</p>
<div class="no-row-height column-margin column-container"></div></section><section id="sec-ai-acceleration-offchip-memory-ecdb" class="level4 page-columns page-full"><h4 class="anchored" data-anchor-id="sec-ai-acceleration-offchip-memory-ecdb">Off-Chip Memory</h4>
<p>Beyond on-chip memory, high-bandwidth memory provides rapid access to larger model parameters and activations that do not fit within caches or scratchpad buffers. HBM achieves its high performance by stacking multiple memory dies and using wide memory interfaces, allowing it to transfer large amounts of data with minimal latency compared to traditional DRAM. Because of its high bandwidth and lower latency, HBM is often used to store entire layers of machine learning models that must be accessed quickly during execution. However, its cost and power consumption limit its use primarily to high-performance AI accelerators, making it less common in power-constrained environments such as edge devices.</p>
<p>When a machine learning model exceeds the capacity of on-chip memory and HBM, it must rely on off-chip DRAM, such as DDR, GDDR, or LPDDR. While DRAM offers significantly greater storage capacity, its access latency is higher, meaning that frequent retrievals from DRAM can introduce execution bottlenecks. To make effective use of DRAM, models must be structured so that only the necessary portions of weights and activations are retrieved at any given time, minimizing the impact of long memory fetch times.</p>
<p>At the highest level of the hierarchy, flash storage and solid-state drives (SSDs) store large pre-trained models, datasets, and checkpointed weights. These storage devices offer large capacities but are too slow for real-time execution, requiring models to be loaded into faster memory tiers before computation begins. For instance, in training scenarios, checkpointed models stored in SSDs must be loaded into DRAM or HBM before resuming computation, as direct execution from SSDs would be too slow to maintain efficient accelerator utilization <span class="citation" data-cites="Narayanan2021">(<a href="#ref-Narayanan2021" role="doc-biblioref">Narayanan et al. 2021</a>)</span>.</p>
<div class="no-row-height column-margin column-container"></div><p>The memory hierarchy balances competing objectives of speed, capacity, and energy efficiency. However, moving data through multiple memory levels introduces bottlenecks that limit accelerator performance. Data transfers between memory levels incur latency costs, particularly for off-chip accesses. Limited bandwidth restricts data flow between memory tiers. Memory capacity constraints force constant data movement as models exceed local storage. These constraints make memory bandwidth the fundamental determinant of real-world accelerator performance.</p>
</section></section><section id="sec-ai-acceleration-memory-bandwidth-architectural-tradeoffs-435c" class="level3"><h3 class="anchored" data-anchor-id="sec-ai-acceleration-memory-bandwidth-architectural-tradeoffs-435c">Memory Bandwidth and Architectural Trade-offs</h3>
<p>Building on the memory wall analysis established in <a href="#sec-ai-acceleration-understanding-ai-memory-wall-3ea9" class="quarto-xref">Section&nbsp;1.4.1</a>, this section quantifies how specific bandwidth characteristics impact system performance across different deployment scenarios.</p>
<p>Modern accelerators exhibit distinct bandwidth-capacity trade-offs: NVIDIA H100 GPUs provide 3.35 TB/s HBM3 bandwidth with 80&nbsp;GB capacity, optimizing for flexibility across diverse workloads. Googleâ€™s TPUv4 delivers 1.2 TB/s bandwidth with 128&nbsp;MB on-chip memory, prioritizing energy efficiency for tensor operations. This 3:1 bandwidth advantage enables H100 to handle memory-intensive models like large language models, while TPUâ€™s lower bandwidth suffices for compute-intensive inference due to superior data reuse.</p>
<p>Different neural network operations achieve varying bandwidth utilization: transformer attention mechanisms achieve only 20-40% of peak memory bandwidth due to irregular access patterns, convolutional layers achieve 60-85% through predictable spatial access patterns, and fully connected layers approach 90% when batch sizes exceed 128.</p>
<p>As established earlier, on-chip memory access consumes 5-10 pJ per access, while external DRAM requires 640 pJ per accessâ€”a 65-125<span class="math inline">\(\times\)</span> energy penalty. AI accelerators minimize DRAM access through three key strategies: weight stationarity (keeping model parameters in on-chip memory), input stationarity (buffering input activations locally), and output stationarity (accumulating partial sums on-chip).</p>
<p>Memory bandwidth scaling follows different trajectories across accelerator designs:</p>
<ul>
<li>
<strong>GPU scaling</strong>: Bandwidth increases linearly with memory channels, from 900 GB/s (A100) to 3,350 GB/s (H100), enabling larger model support</li>
<li>
<strong>TPU scaling</strong>: Bandwidth optimization through systolic array design achieves 900 GB/s with 35% lower power than GPU alternatives</li>
<li>
<strong>Mobile accelerator scaling</strong>: Appleâ€™s M3 Neural Engine achieves 400 GB/s unified memory bandwidth while consuming &lt;5&nbsp;W through aggressive voltage scaling</li>
</ul>
<p>HBM memory costs $8-15 per GB compared to $0.05 per GB for DDR5, creating 160-300<span class="math inline">\(\times\)</span> cost differences. High-bandwidth accelerators require 40-80&nbsp;GB HBM for competitive performance, adding $320-1,200 to manufacturing costs. Edge accelerators sacrifice bandwidth (50-200 GB/s) to achieve sub-$100 cost targets while maintaining sufficient performance for inference workloads.</p>
<p>These bandwidth characteristics directly influence deployment decisions: cloud training prioritizes raw bandwidth for maximum model capacity, edge inference optimizes bandwidth efficiency for energy constraints, and mobile deployment balances bandwidth with cost limitations. While these hardware-specific optimizations are fundamental, the integrated system-level efficiency approaches that combine hardware acceleration with software optimization techniques are comprehensively covered in <strong><a href="../efficient_ai/efficient_ai.html#sec-efficient-ai">Chapter 9: Efficient AI</a></strong>. The deployment of these optimizations across different system contextsâ€”from mobile devices in <strong><a href="../ml_systems/ml_systems.html#sec-ml-systems">Chapter 2: ML Systems</a></strong> to production workflows in <strong><a href="../ops/ops.html#sec-ml-operations">Chapter 13: ML Operations</a></strong>â€”determines their real-world impact.</p>
</section><section id="sec-ai-acceleration-hostaccelerator-communication-bb7a" class="level3 page-columns page-full"><h3 class="anchored" data-anchor-id="sec-ai-acceleration-hostaccelerator-communication-bb7a">Host-Accelerator Communication</h3>
<p>Machine learning accelerators, such as GPUs and TPUs, achieve high computational throughput through parallel execution. However, their efficiency is fundamentally constrained by data movement between the host (CPU) and accelerator memory. Unlike general-purpose workloads that operate entirely within a CPUâ€™s memory subsystem, AI workloads require frequent data transfers between CPU main memory and the accelerator, introducing latency, consuming bandwidth, and affecting overall performance.</p>
<p>Host-accelerator data movement follows a structured sequence, as illustrated in <a href="#fig-host-accelerator-data-movement" class="quarto-xref">Figure&nbsp;7</a>. Before computation begins, data is copied from CPU memory to the acceleratorâ€™s memory. The CPU then issues execution instructions, and the accelerator processes the data in parallel. Once computation completes, the results are stored in accelerator memory and transferred back to the CPU. Each step introduces potential inefficiencies that must be managed to optimize performance.</p>
<div id="fig-host-accelerator-data-movement" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-pos="htb" data-fig-env="figure">
<figure class="quarto-float quarto-float-fig figure"><div aria-describedby="fig-host-accelerator-data-movement-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="afa606b806077d9110fe0e84f1fc5f145d86c1d8.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-7" title="Figure&nbsp;7: Host-Accelerator Data Transfer: AI workloads require frequent data movement between CPU memory and accelerators; this figure details the sequential steps of copying input data, executing computation, and transferring results, each introducing potential performance bottlenecks. Understanding this data transfer sequence is crucial for optimizing AI system performance and minimizing latency."><img src="hw_acceleration_files/mediabag/afa606b806077d9110fe0e84f1fc5f145d86c1d8.svg" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-host-accelerator-data-movement-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;7: <strong>Host-Accelerator Data Transfer</strong>: AI workloads require frequent data movement between CPU memory and accelerators; this figure details the sequential steps of copying input data, executing computation, and transferring results, each introducing potential performance bottlenecks. Understanding this data transfer sequence is crucial for optimizing AI system performance and minimizing latency.
</figcaption></figure>
</div>
<p>The key challenges in host-accelerator data movement include latency, bandwidth constraints, and synchronization overheads. Optimizing data transfers through efficient memory management and interconnect technologies is essential for maximizing accelerator utilization.</p>
<section id="sec-ai-acceleration-data-transfer-patterns-689a" class="level4 page-columns page-full"><h4 class="anchored" data-anchor-id="sec-ai-acceleration-data-transfer-patterns-689a">Data Transfer Patterns</h4>
<p>The efficiency of ML accelerators depends not only on their computational power but also on the continuous supply of data. Even high-performance GPUs and TPUs remain underutilized if data transfers are inefficient. Host and accelerator memory exist as separate domains, requiring explicit transfers over interconnects such as PCIe, NVLink, or proprietary links. Ineffective data movement can cause execution stalls, making transfer optimization critical.</p>
<p><a href="#fig-host-accelerator-data-movement" class="quarto-xref">Figure&nbsp;7</a> illustrates this structured sequence. In step (1), data is copied from CPU memory to accelerator memory, as GPUs cannot directly access host memory at high speeds. A direct memory access (DMA)<a href="#fn26" class="footnote-ref" id="fnref26" role="doc-noteref"><sup>26</sup></a> engine typically handles this transfer without consuming CPU cycles. In step (2), the CPU issues execution commands via APIs like CUDA, ROCm, or OpenCL. Step (3) involves parallel execution on the accelerator, where stalls can occur if data is not available when needed. Finally, in step (4), computed results are copied back to CPU memory for further processing.</p>
<div class="no-row-height column-margin column-container"><div id="fn26"><p><sup>26</sup>&nbsp;<strong>Direct Memory Access (DMA)</strong>: Hardware mechanism that enables devices to transfer data to/from memory without CPU intervention. First introduced in 1981 with IBMâ€™s PC, DMA engines free the CPU to perform other tasks while data moves between system memory and accelerators. Modern GPUs contain multiple DMA engines achieving 32 GB/s (PCIe 4.0) to 900 GB/s (NVLink) transfer rates. This asynchronous capability is crucial for AI workloads where data movement can overlap with computation, improving overall system utilization.</p></div></div><p>Latency and bandwidth limitations significantly impact AI workloads. PCIe, with a peak bandwidth of 32 GB/s (PCIe 4.0), is much slower than an acceleratorâ€™s high-bandwidth memory, which can exceed 1 TB/s. Large data transfers exacerbate bottlenecks, particularly in deep learning tasks. Additionally, synchronization overheads arise when computation must wait for data transfers to complete. Efficient scheduling and overlapping transfers with execution are essential to mitigate these inefficiencies.</p>
</section><section id="sec-ai-acceleration-data-transfer-mechanisms-4109" class="level4"><h4 class="anchored" data-anchor-id="sec-ai-acceleration-data-transfer-mechanisms-4109">Data Transfer Mechanisms</h4>
<p>The movement of data between the host (CPU) and the accelerator (GPU, TPU, or other AI hardware) depends on the interconnect technology that links the two processing units. The choice of interconnect determines the bandwidth available for transfers, the latency of communication, and the overall efficiency of host-accelerator execution. The most commonly used transfer mechanisms include PCIe (Peripheral Component Interconnect Express), NVLink, Direct Memory Access, and Unified Memory Architectures. Each of these plays a crucial role in optimizing the four-step data movement process illustrated in <a href="#fig-host-accelerator-data-movement" class="quarto-xref">Figure&nbsp;7</a>.</p>
<section id="sec-ai-acceleration-pcie-interface-c5b4" class="level5"><h5 class="anchored" data-anchor-id="sec-ai-acceleration-pcie-interface-c5b4">PCIe Interface</h5>
<p>Most accelerators communicate with the CPU via PCIe, the industry-standard interconnect for data movement. PCIe 4.0 provides up to 32 GB/s bandwidth, while PCIe 5.0 doubles this to 64 GB/s. However, this is still significantly lower than HBM bandwidth within accelerators, making PCIe a bottleneck for large AI workloads.</p>
<p>PCIe also introduces latency overheads due to its packet-based communication and memory-mapped I/O model. Frequent small transfers are inefficient, so batching data movement reduces overhead. Computation commands, issued over PCIe, further contribute to latency, requiring careful optimization of execution scheduling.</p>
</section><section id="sec-ai-acceleration-nvlink-interface-312b" class="level5"><h5 class="anchored" data-anchor-id="sec-ai-acceleration-nvlink-interface-312b">NVLink Interface</h5>
<p>To address the bandwidth limitations of PCIe, NVIDIA developed NVLink, a proprietary high-speed interconnect that provides significantly higher bandwidth between GPUs and, in some configurations, between the CPU and GPU. Unlike PCIe, which operates as a shared bus, NVLink enables direct point-to-point communication between connected devices, reducing contention and improving efficiency for AI workloads.</p>
<p>For host-accelerator transfers, NVLink can be used in step (1) to transfer input data from main memory to GPU memory at speeds far exceeding PCIe, with bandwidths reaching up to 600 GB/s in NVLink 4.0. This significantly reduces the data movement bottleneck, allowing accelerators to access input data with lower latency. In multi-GPU configurations, NVLink also accelerates peer-to-peer transfers, allowing accelerators to exchange data without routing through main memory, thereby optimizing step (3) of the computation process.</p>
<p>Although NVLink offers substantial performance benefits, it is not universally available. Unlike PCIe, which is an industry standard across all accelerators, NVLink is specific to NVIDIA hardware, limiting its applicability to systems designed with NVLink-enabled GPUs.</p>
</section><section id="sec-ai-acceleration-dma-data-transfers-a1a7" class="level5"><h5 class="anchored" data-anchor-id="sec-ai-acceleration-dma-data-transfers-a1a7">DMA for Data Transfers</h5>
<p>In conventional memory transfers, the CPU issues load/store instructions, consuming processing cycles. DMA offloads this task, enabling asynchronous data movement without CPU intervention.</p>
<p>During data transfers, the CPU initiates a DMA request, allowing data to be copied to accelerator memory in the background. Similarly, result transfers back to main memory occur without blocking execution. This enables overlapping computation with data movement, reducing idle time and improving accelerator utilization.</p>
<p>DMA is essential for enabling asynchronous data movement, which allows transfers to overlap with computation. Instead of waiting for transfers to complete before execution begins, AI workloads can stream data into the accelerator while earlier computations are still in progress, reducing idle time and improving accelerator utilization.</p>
</section><section id="sec-ai-acceleration-unified-memory-b18f" class="level5"><h5 class="anchored" data-anchor-id="sec-ai-acceleration-unified-memory-b18f">Unified Memory</h5>
<p>While PCIe, NVLink, and DMA optimize explicit memory transfers, some AI workloads require a more flexible memory model that eliminates the need for manual data copying. Unified Memory provides an abstraction that allows both the host and accelerator to access a single, shared memory space, automatically handling data movement when needed.</p>
<p>With Unified Memory, data does not need to be explicitly copied between CPU and GPU memory before execution. Instead, when a computation requires a memory region that is currently located in host memory, the system automatically migrates it to the accelerator, handling step (1) transparently. Similarly, when computed results are accessed by the CPU, step (4) occurs automatically, eliminating the need for manual memory management.</p>
<p>Although Unified Memory simplifies programming, it introduces performance trade-offs. Since memory migrations occur on demand, they can lead to unpredictable latencies, particularly if large datasets need to be transferred frequently. Additionally, since Unified Memory is implemented through page migration techniques, small memory accesses can trigger excessive data movement, further reducing efficiency.</p>
<p>For AI workloads that require fine-grained memory control, explicit data transfers using PCIe, NVLink, and DMA often provide better performance. However, for applications where ease of development is more important than absolute speed, Unified Memory offers a convenient alternative.</p>
</section></section><section id="sec-ai-acceleration-data-transfer-overheads-fbc9" class="level4"><h4 class="anchored" data-anchor-id="sec-ai-acceleration-data-transfer-overheads-fbc9">Data Transfer Overheads</h4>
<p>Host-accelerator data movement introduces overheads that impact AI workload execution. Unlike on-chip memory accesses, which occur at nanosecond latencies, host-accelerator transfers traverse system interconnects, adding latency, bandwidth constraints, and synchronization delays.</p>
<p>Interconnect latency affects transfer speed, with PCIe, the standard host-accelerator link, incurring significant overhead due to packet-based transactions and memory-mapped I/O. This makes frequent small transfers inefficient. Faster alternatives like NVLink reduce latency and improve bandwidth but are limited to specific hardware ecosystems.</p>
<p>Synchronization delays further contribute to inefficiencies. Synchronous transfers block execution until data movement completes, ensuring data consistency but introducing idle time. Asynchronous transfers allow computation and data movement to overlap, reducing stalls but requiring careful coordination to avoid execution mismatches.</p>
<p>These factors, including interconnect latency, bandwidth limitations, and synchronization overheads, determine AI workload efficiency. While optimization techniques mitigate these limitations, understanding these fundamental transfer mechanics is essential for improving performance.</p>
</section></section><section id="sec-ai-acceleration-model-memory-pressure-f95e" class="level3 page-columns page-full"><h3 class="anchored" data-anchor-id="sec-ai-acceleration-model-memory-pressure-f95e">Model Memory Pressure</h3>
<p>Machine learning models impose varying memory access patterns that significantly influence accelerator performance. The way data is transferred between the host and accelerator, how frequently memory is accessed, and the efficiency of caching mechanisms all determine overall execution efficiency. While multilayer perceptrons (MLPs), convolutional neural networks (CNNs), and transformer networks each require large parameter sets, their distinct memory demands necessitate tailored optimization strategies for accelerators. Understanding these differences provides insight into why different hardware architectures exhibit varying levels of efficiency across workloads.</p>
<section id="sec-ai-acceleration-multilayer-perceptrons-0bbc" class="level4 page-columns page-full"><h4 class="anchored" data-anchor-id="sec-ai-acceleration-multilayer-perceptrons-0bbc">Multilayer Perceptrons</h4>
<p>MLPs, also referred to as fully connected networks, are among the simplest neural architectures. Each layer consists of a dense matrix multiplication, requiring every neuron to interact with all neurons in the preceding layer. This results in high memory bandwidth demands, particularly for weights, as every input activation contributes to a large set of computations.</p>
<p>From a memory perspective, MLPs rely on large, dense weight matrices that frequently exceed on-chip memory capacity, necessitating off-chip memory accesses. Since accelerators cannot directly access host memory at high speed, data transfers must be explicitly managed via interconnects such as PCIe or NVLink. These transfers introduce latency and consume bandwidth, affecting execution efficiency.</p>
<p>Despite their bandwidth-heavy nature, MLPs exhibit regular and predictable memory access patterns, making them amenable to optimizations such as prefetching and streaming memory accesses. Dedicated AI accelerators mitigate transfer overhead by staging weight matrices in fast SRAM caches and overlapping data movement with computation through direct memory access engines, reducing execution stalls. These optimizations allow accelerators to sustain high throughput even when handling large parameter sets <span class="citation" data-cites="Chen2016">(<a href="#ref-Chen2016" role="doc-biblioref">Chen, Emer, and Sze 2017</a>)</span>.</p>
<div class="no-row-height column-margin column-container"></div></section><section id="sec-ai-acceleration-convolutional-neural-networks-3085" class="level4 page-columns page-full"><h4 class="anchored" data-anchor-id="sec-ai-acceleration-convolutional-neural-networks-3085">Convolutional Neural Networks</h4>
<p>Convolutional Neural Networks (CNNs) are widely used in image processing and computer vision tasks. Unlike MLPs, which require dense matrix multiplications, CNNs process input feature maps using small filter kernels that slide across the image. This localized computation structure results in high spatial data reuse, where the same input pixels contribute to multiple convolutions.</p>
<p>CNN accelerators benefit from on-chip memory optimizations, as convolution filters exhibit extensive reuse, allowing weights to be stored in fast local SRAM instead of frequently accessing off-chip memory. However, activation maps require careful management due to their size. Since accessing main memory over interconnects like PCIe introduces latency and bandwidth bottlenecks, CNN accelerators employ tiling techniques to divide feature maps into smaller regions that fit within on-chip buffers. This minimizes costly external memory transfers, improving overall efficiency <span class="citation" data-cites="Chen2016">(<a href="#ref-Chen2016" role="doc-biblioref">Chen, Emer, and Sze 2017</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-Chen2016" class="csl-entry" role="listitem">
Chen, Yu-Hsin, Joel Emer, and Vivienne Sze. 2017. <span>â€œUsing Dataflow to Optimize Energy Efficiency of Deep Neural Network Accelerators.â€</span> <em>IEEE Micro</em> 37 (3): 12â€“21. <a href="https://doi.org/10.1109/mm.2017.54">https://doi.org/10.1109/mm.2017.54</a>.
</div></div><p>While CNN workloads are more memory-efficient than MLPs, managing intermediate activations remains a challenge. Accelerators use hierarchical caching strategies and DMA engines to optimize memory movement, ensuring that computations are not stalled by inefficient host-accelerator data transfers. These memory optimizations help CNN accelerators maintain high throughput by reducing reliance on off-chip memory bandwidth <span class="citation" data-cites="Chen2016">(<a href="#ref-Chen2016" role="doc-biblioref">Chen, Emer, and Sze 2017</a>)</span>.</p>
</section><section id="sec-ai-acceleration-transformer-networks-638c" class="level4 page-columns page-full"><h4 class="anchored" data-anchor-id="sec-ai-acceleration-transformer-networks-638c">Transformer Networks</h4>
<p>Transformers have become the dominant architecture for natural language processing and are increasingly used in other domains such as vision and speech recognition. Unlike CNNs, which rely on local computations, transformers perform global attention mechanisms, where each token in an input sequence can interact with all other tokens. This leads to irregular and bandwidth-intensive memory access patterns, as large key-value matrices must be fetched and updated frequently.</p>
<p>These models are particularly challenging for accelerators due to their massive parameter sizes, which often exceed on-chip memory capacity. As a result, frequent memory transfers between host and accelerator introduce substantial latency overheads, particularly when relying on interconnects such as PCIe. Unified Memory architectures can mitigate some of these issues by dynamically handling data movement, but they introduce additional latency due to unpredictable on-demand memory migrations. Because transformers are memory-bound rather than compute-bound, accelerators optimized for them rely on high-bandwidth memory, tensor tiling, and memory partitioning to sustain performance <span class="citation" data-cites="Brown2020">(<a href="#ref-Brown2020" role="doc-biblioref">Brown et al. 2020</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-Brown2020" class="csl-entry" role="listitem">
Brown, Tom B., Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, et al. 2020. <span>â€œLanguage Models Are Few-Shot Learners.â€</span> <em>NeurIPS</em>, May. <a href="http://arxiv.org/abs/2005.14165v4">http://arxiv.org/abs/2005.14165v4</a>.
</div><div id="ref-Narayanan2021" class="csl-entry" role="listitem">
Narayanan, Deepak, Mohammad Shoeybi, Jared Casper, Patrick LeGresley, Mostofa Patwary, Vijay Anand Korthikanti, Dmitri Vainbrand, et al. 2021. <span>â€œEfficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM.â€</span> <em>NeurIPS</em>, April. <a href="http://arxiv.org/abs/2104.04473v5">http://arxiv.org/abs/2104.04473v5</a>.
</div></div><p>Additionally, attention caching mechanisms and specialized tensor layouts reduce redundant memory fetches, improving execution efficiency. Given the bandwidth limitations of traditional interconnects, NVLink-enabled architectures offer significant advantages for large-scale transformer training, as they provide higher throughput and lower latency compared to PCIe. DMA-based asynchronous memory transfers enable overlapping computation with data movement, reducing execution stalls <span class="citation" data-cites="Narayanan2021">(<a href="#ref-Narayanan2021" role="doc-biblioref">Narayanan et al. 2021</a>)</span>.</p>
</section></section><section id="sec-ai-acceleration-ml-accelerators-implications-c962" class="level3"><h3 class="anchored" data-anchor-id="sec-ai-acceleration-ml-accelerators-implications-c962">ML Accelerators Implications</h3>
<p>The diverse memory requirements of MLPs, CNNs, and Transformers highlight the need to tailor memory architectures to specific workloads. <a href="#tbl-model-mem-compare" class="quarto-xref">Table&nbsp;10</a> compares the memory access patterns across these different models.</p>
<div id="tbl-model-mem-compare" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure"><figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-model-mem-compare-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;10: <strong>ML Model Memory Access</strong>: Different machine learning models exhibit distinct memory access patterns and bottlenecks due to variations in weight size, activation reuse, and data sparsity; these characteristics significantly impact hardware accelerator design and performance optimization. Transformers demand high bandwidth and capacity due to their massive, sparsely accessed weights, while cnns benefit from spatial locality and high activation reuse, reducing memory pressure.
</figcaption><div aria-describedby="tbl-model-mem-compare-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<colgroup>
<col style="width: 14%">
<col style="width: 14%">
<col style="width: 17%">
<col style="width: 25%">
<col style="width: 25%">
</colgroup>
<thead><tr class="header">
<th style="text-align: left;"><strong>Model Type</strong></th>
<th style="text-align: left;"><strong>Weight Size</strong></th>
<th style="text-align: left;"><strong>Activation Reuse</strong></th>
<th style="text-align: left;"><strong>Memory Access Pattern</strong></th>
<th style="text-align: left;"><strong>Primary Bottleneck</strong></th>
</tr></thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><strong>MLP (Dense)</strong></td>
<td style="text-align: left;">Large, dense</td>
<td style="text-align: left;">Low</td>
<td style="text-align: left;">Regular, sequential (streamed)</td>
<td style="text-align: left;">Bandwidth (off-chip)</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>CNN</strong></td>
<td style="text-align: left;">Small, reused</td>
<td style="text-align: left;">High</td>
<td style="text-align: left;">Spatial locality</td>
<td style="text-align: left;">Feature map movement</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>Transformer</strong></td>
<td style="text-align: left;">Massive, sparse</td>
<td style="text-align: left;">Low</td>
<td style="text-align: left;">Irregular, high-bandwidth</td>
<td style="text-align: left;">Memory capacity + Interconnect</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p>Each model type presents unique challenges that directly impact accelerator design. MLPs benefit from fast streaming access to dense weight matrices, making memory bandwidth a critical factor in performance, especially when transferring large weights from host memory to accelerator memory. CNNs, with their high activation reuse and structured memory access patterns, can leverage on-chip caching and tiling strategies to minimize off-chip memory transfers. Transformers, however, impose significant demands on both bandwidth and capacity, as attention mechanisms require frequent access to large key-value matrices, leading to high interconnect traffic and increased memory pressure.</p>
<p>To address these challenges, modern AI accelerators incorporate multi-tier memory hierarchies that balance speed, capacity, and energy efficiency. On-chip SRAM caches and scratchpad memories store frequently accessed data, while high-bandwidth external memory provides scalability for large models. Efficient interconnects, such as NVLink, help alleviate host-accelerator transfer bottlenecks, particularly in transformer workloads where memory movement constraints can dominate execution time.</p>
<p>As ML workloads continue to grow in complexity, memory efficiency becomes as critical as raw compute power. The analysis reveals how memory systems dominate accelerator performance: the 173Ã— energy penalty for DRAM access creates a fundamental bottleneck, carefully structured memory hierarchies can improve effective bandwidth by 10-100Ã—, and different neural network architectures create distinct memory pressure patterns. These constraintsâ€”from bandwidth limitations to communication overheadsâ€”determine whether theoretical computational capabilities translate into real-world performance. Having established how memory systems constrain accelerator effectiveness, we now examine how mapping strategies systematically address these limitations.</p>
<div id="quiz-question-sec-ai-acceleration-ai-memory-systems-0057" class="callout callout-quiz-question">
<details class="callout-quiz-question fbx-default closebutton"><summary><strong>Self-Check: Question 1.4</strong></summary><div>
<ol type="1">
<li>
<p>What is the primary constraint that defines the AI memory wall?</p>
<ol type="a">
<li>The limited number of compute units available in accelerators.</li>
<li>The cost of high-bandwidth memory compared to traditional DRAM.</li>
<li>The energy consumption of arithmetic operations compared to memory access.</li>
<li>The disparity between computational throughput and memory bandwidth.</li>
</ol>
</li>
<li><p>Explain how memory hierarchies in AI accelerators balance speed, capacity, and energy efficiency.</p></li>
<li><p>The energy penalty for accessing ____ is significantly higher than for computation, influencing AI accelerator design.</p></li>
<li>
<p>Which neural network architecture is most likely to be constrained by memory capacity and interconnect bandwidth?</p>
<ol type="a">
<li>Transformer Networks</li>
<li>Convolutional Neural Networks (CNNs)</li>
<li>Multilayer Perceptrons (MLPs)</li>
<li>Recurrent Neural Networks (RNNs)</li>
</ol>
</li>
<li><p>In a system design scenario, how might you address the memory bottlenecks imposed by transformer networks?</p></li>
</ol>
<p><a href="#quiz-answer-sec-ai-acceleration-ai-memory-systems-0057" class="question-label">See Answers â†’</a></p>
</div></details>
</div>
</section></section><section id="sec-ai-acceleration-hardware-mapping-fundamentals-neural-networks-f9a9" class="level2 page-columns page-full"><h2 class="anchored" data-anchor-id="sec-ai-acceleration-hardware-mapping-fundamentals-neural-networks-f9a9">Hardware Mapping Fundamentals for Neural Networks</h2>
<p>The memory system challenges examined in the previous sectionâ€”bandwidth limitations, hierarchical access costs, and model-specific pressure patternsâ€”directly determine how effectively neural networks execute on accelerators. A systolic array with 1,200 GB/s on-chip bandwidth and sophisticated memory hierarchies delivers no performance benefit if computations are mapped without considering these memory access patterns. As established in <a href="#sec-ai-acceleration-understanding-ai-memory-wall-3ea9" class="quarto-xref">Section&nbsp;1.4.1</a>, the extreme energy penalty for memory access means that mapping strategies must prioritize data reuse and locality above all other considerations. This reality drives the need for systematic mapping approaches that coordinate computation placement, memory allocation, and data movement to exploit hardware capabilities while respecting memory constraints.</p>
<p>Efficient execution of machine learning models on specialized AI acceleration hardware requires a structured approach to computation, ensuring that available resources are fully utilized while minimizing performance bottlenecks. These mapping considerations become particularly critical in distributed training scenarios, as explored in <strong><a href="../training/training.html#sec-ai-training">Chapter 8: AI Training</a></strong>. Unlike general-purpose processors, which rely on dynamic task scheduling, AI accelerators operate under a structured execution model that maximizes throughput by carefully assigning computations to processing elements. This process, known as mapping, dictates how computations are distributed across hardware resources, influencing execution speed, memory access patterns, and overall efficiency.</p>
<div id="callout-definition*-1.2" class="callout callout-definition" title="Mapping in AI Acceleration">
<p></p><details class="callout-definition fbx-default closebutton" open=""><summary><strong>Definition: </strong>Mapping in AI Acceleration</summary><div>
<strong><em>Mapping in AI Acceleration</em></strong> is the process of assigning ML <em>computations</em> to <em>hardware units</em> through <em>spatial allocation</em>, <em>temporal scheduling</em>, and <em>memory-aware placement</em> to maximize execution efficiency and resource utilization.
</div></details><p></p>
</div>
<p>Mapping machine learning models onto AI accelerators presents several challenges due to hardware constraints and the diversity of model architectures. Given the hierarchical memory system of modern accelerators, mapping strategies must carefully manage when and where data is accessed to minimize latency and power overhead while ensuring that compute units remain actively engaged. Poor mapping decisions can lead to underutilized compute resources, excessive data movement, and increased execution time, ultimately reducing overall efficiency.</p>
<p>To understand the complexity of this challenge, consider an analogy: mapping a neural network to an accelerator is like planning a massive, factory-wide assembly process. You have thousands of workers (processing elements) and a complex set of tasks (computations). You must decide which worker does which task (computation placement), where to store the parts they need (memory allocation), and the exact sequence of operations to minimize time spent walking around (dataflow). A small change in the plan can lead to massive differences in factory output. Just as a poorly organized factory might have workers idle while others are overwhelmed, or materials stored too far from where theyâ€™re needed, a poorly mapped neural network can leave processing elements underutilized while creating memory bottlenecks that stall the entire system.</p>
<p>Mapping encompasses three interrelated aspects that form the foundation of effective AI accelerator design.</p>
<ul>
<li>
<strong>Computation Placement</strong>: Systematically assigns operations (e.g., matrix multiplications, convolutions) to processing elements to maximize parallelism and reduce idle time.</li>
<li>
<strong>Memory Allocation</strong>: Carefully determines where model parameters, activations, and intermediate results reside within the memory hierarchy to optimize access efficiency.</li>
<li>
<strong>Dataflow and Execution Scheduling</strong>: Structures the movement of data between compute units to reduce bandwidth bottlenecks and ensure smooth, continuous execution.</li>
</ul>
<p>Effective mapping strategies minimize off-chip memory accesses, maximize compute utilization, and efficiently manage data movement across different levels of the memory hierarchy.</p>
<div class="callout callout-style-default callout-note callout-titled" title="The Role of the Compiler">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
The Role of the Compiler
</div>
</div>
<div class="callout-body-container callout-body">
<p>Developers rarely perform this complex mapping manually. Instead, a specialized <strong>compiler</strong> (like NVIDIAâ€™s NVCC or Googleâ€™s XLA) takes the high-level model from the framework and automatically explores the mapping search space to find an optimal execution plan for the target hardware. The compiler is the crucial software layer that translates the modelâ€™s computational graph into an efficient hardware-specific dataflow, balancing the three interrelated aspects of computation placement, memory allocation, and execution scheduling described above. This compiler support is examined in detail in <a href="#sec-ai-acceleration-compiler-support-172e" class="quarto-xref">Section&nbsp;1.7</a>.</p>
</div>
</div>
<p>The following sections explore the key mapping choices that influence execution efficiency and lay the groundwork for optimization strategies that refine these decisions.</p>
<section id="sec-ai-acceleration-computation-placement-23d2" class="level3 page-columns page-full"><h3 class="anchored" data-anchor-id="sec-ai-acceleration-computation-placement-23d2">Computation Placement</h3>
<p>Modern AI accelerators are designed to execute machine learning models with massive parallelism, using thousands to millions of processing elements to perform computations simultaneously. However, simply having many compute units is not enough. How computations are assigned to these units determines overall efficiency.</p>
<p>Without careful placement, some processing elements may sit idle while others are overloaded, leading to wasted resources, increased memory traffic, and reduced performance. Computation placement is the process of strategically mapping operations onto available hardware resources to sustain high throughput, minimize stalls, and optimize execution efficiency.</p>
<section id="sec-ai-acceleration-computation-placement-definition-e130" class="level4 page-columns page-full"><h4 class="anchored" data-anchor-id="sec-ai-acceleration-computation-placement-definition-e130">Computation Placement Definition</h4>
<p>AI accelerators contain thousands to millions of processing elements, making computation placement a large-scale problem. Modern GPUs, such as the NVIDIA H100, feature over 16,000 streaming processors and more than 500 specialized tensor cores, each designed to accelerate matrix operations <span class="citation" data-cites="nvidia2022h100">(<a href="#ref-nvidia2022h100" role="doc-biblioref">Choquette 2023</a>)</span>. TPUs utilize systolic arrays composed of thousands of interconnected multiply-accumulate (MAC) units, while wafer-scale processors like Cerebrasâ€™ CS-2 push parallelism even further, integrating over 850,000 cores on a single chip <span class="citation" data-cites="Cerebras2021">(<a href="#ref-Cerebras2021" role="doc-biblioref">Systems 2021b</a>)</span>. In these architectures, even minor inefficiencies in computation placement can lead to significant performance losses, as idle cores or excessive memory movement compound across the system.</p>
<div class="no-row-height column-margin column-container"><div id="ref-nvidia2022h100" class="csl-entry" role="listitem">
Choquette, Jack. 2023. <span>â€œNVIDIA Hopper H100 GPU: Scaling Performance.â€</span> <em>IEEE Micro</em> 43 (3): 9â€“17. <a href="https://doi.org/10.1109/mm.2023.3256796">https://doi.org/10.1109/mm.2023.3256796</a>.
</div></div><p>Computation placement ensures that all processing elements contribute effectively to execution. This means that workloads must be distributed in a way that avoids imbalanced execution, where some processing elements sit idle while others remain overloaded. Similarly, placement must minimize unnecessary data movement, as excessive memory transfers introduce latency and power overheads that degrade system performance.</p>
<p>Neural network computations vary significantly based on the model architecture, influencing how placement strategies are applied. For example, in a CNN, placement focuses on dividing image regions across processing elements to maximize parallelism. A <span class="math inline">\(256\times256\)</span> image processed through thousands of GPU cores might be broken into small tiles, each mapped to a different processing unit to execute convolutional operations simultaneously. In contrast, a transformer-based model requires placement strategies that accommodate self-attention mechanisms, where each token in a sequence interacts with all others, leading to irregular and memory-intensive computation patterns. Meanwhile, Graph Neural Networks (GNNs) introduce additional complexity, as computations depend on sparse and dynamic graph structures that require adaptive workload distribution <span class="citation" data-cites="Zheng2020">(<a href="#ref-Zheng2020" role="doc-biblioref">Zheng et al. 2020</a>)</span>.</p>
<div class="no-row-height column-margin column-container"></div><p>Because computation placement directly impacts resource utilization, execution speed, and power efficiency, it is one of the most critical factors in AI acceleration. A well-placed computation can reduce latency by orders of magnitude, while a poorly placed one can render thousands of processing units underutilized. The next section explores why efficient computation placement is essential and the consequences of suboptimal mapping strategies.</p>
</section><section id="sec-ai-acceleration-computation-placement-importance-e7e9" class="level4 page-columns page-full"><h4 class="anchored" data-anchor-id="sec-ai-acceleration-computation-placement-importance-e7e9">Computation Placement Importance</h4>
<p>While computation placement is a hardware-driven process, its importance is fundamentally shaped by the structure of neural network workloads. Different types of machine learning models exhibit distinct computation patterns, which directly influence how efficiently they can be mapped onto accelerators. Without careful placement, workloads can become unbalanced, memory access patterns can become inefficient, and the overall performance of the system can degrade significantly.</p>
<p>For models with structured computation patterns, such as CNNs, computation placement is relatively straightforward. CNNs process images using filters that are applied to small, localized regions, meaning their computations can be evenly distributed across processing elements. Because these operations are highly parallelizable, CNNs benefit from spatial partitioning, where the input is divided into tiles that are processed independently. This structured execution makes CNNs well-suited for accelerators that favor regular dataflows, minimizing the complexity of placement decisions.</p>
<p>However, for models with irregular computation patterns, such as transformers and GNNs, computation placement becomes significantly more challenging. Transformers, which rely on self-attention mechanisms, require each token in a sequence to interact with all others, resulting in non-uniform computation demands. Unlike CNNs, where each processing element performs a similar amount of work, transformers introduce workload imbalance, where certain operations, including the computation of attention scores, require far more computation than others. Without careful placement, this imbalance can lead to stalls, where some processing elements remain idle while others struggle to keep up.</p>
<p>The challenge is even greater in graph neural networks (GNNs), where computation depends on sparse and dynamically changing graph structures. Unlike CNNs, which operate on dense and regularly structured data, GNNs must process nodes and edges with highly variable degrees of connectivity. Some regions of a graph may require significantly more computation than others, making workload balancing across processing elements difficult <span class="citation" data-cites="Zheng2020">(<a href="#ref-Zheng2020" role="doc-biblioref">Zheng et al. 2020</a>)</span>. If computations are not placed strategically, some compute units will sit idle while others remain overloaded, leading to underutilization and inefficiencies in execution.</p>
<div class="no-row-height column-margin column-container"></div><p>Poor computation placement adversely affects AI execution by creating workload imbalance, inducing excessive data movement, and causing execution stalls and bottlenecks. An uneven distribution of computations can lead to idle processing elements, preventing full hardware utilization and diminishing throughput. Inefficient execution assignment increases memory traffic by necessitating frequent data transfers between memory hierarchies, introducing latency and raising power consumption. Finally, such misallocation can cause operations to wait on data dependencies, resulting in pipeline inefficiencies that ultimately lower overall system performance.</p>
<p>Computation placement ensures that models execute efficiently given their unique computational structure. A well-placed workload reduces execution time, memory overhead, and power consumption, while a poorly placed one can lead to stalled execution pipelines and inefficient resource utilization. The next section explores the key considerations that must be addressed to ensure that computation placement is both efficient and adaptable to different model architectures.</p>
</section><section id="sec-ai-acceleration-effective-computation-placement-099d" class="level4"><h4 class="anchored" data-anchor-id="sec-ai-acceleration-effective-computation-placement-099d">Effective Computation Placement</h4>
<p>Computation placement is a balancing act between hardware constraints and workload characteristics. To achieve high efficiency, placement strategies must account for parallelism, memory access, and workload variability while ensuring that processing elements remain fully utilized. Poor placement leads to imbalanced execution, increased data movement, and performance degradation, making it essential to consider key factors when designing placement strategies.</p>
<p>As summarized in <a href="#tbl-placement-challenges" class="quarto-xref">Table&nbsp;11</a>, computation placement faces several critical challenges that impact execution efficiency. Effective mapping strategies must address these challenges by balancing workload distribution, minimizing data movement, and optimizing communication across processing elements.</p>
<div id="tbl-placement-challenges" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure"><figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-placement-challenges-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;11: <strong>Computation Placement Challenges</strong>: Effective neural network deployment requires strategic allocation of computations to processing elements, balancing workload distribution, data movement costs, and hardware constraints to maximize execution efficiency and avoid performance bottlenecks. Understanding these challenges guides the design of mapping strategies that optimize resource utilization and minimize communication overhead.
</figcaption><div aria-describedby="tbl-placement-challenges-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<colgroup>
<col style="width: 14%">
<col style="width: 46%">
<col style="width: 37%">
</colgroup>
<thead><tr class="header">
<th style="text-align: left;"><strong>Challenge</strong></th>
<th style="text-align: left;"><strong>Impact on Execution</strong></th>
<th style="text-align: left;"><strong>Key Considerations for Placement</strong></th>
</tr></thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><strong>Workload Imbalance</strong></td>
<td style="text-align: left;">Some processing elements finish early while others remain overloaded, leading to idle compute resources.</td>
<td style="text-align: left;">Distribute operations evenly to prevent stalls and ensure full utilization of PEs.</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Irregular Computation Patterns</strong></td>
<td style="text-align: left;">Models like transformers and GNNs introduce non-uniform computation demands, making static placement difficult.</td>
<td style="text-align: left;">Use adaptive placement strategies that adjust execution based on workload characteristics.</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>Excessive Data Movement</strong></td>
<td style="text-align: left;">Frequent memory transfers introduce latency and increase power consumption.</td>
<td style="text-align: left;">Keep frequently used data close to the compute units and minimize off-chip memory accesses.</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Limited Interconnect Bandwidth</strong></td>
<td style="text-align: left;">Poorly placed operations can create congestion, slowing data movement between PEs.</td>
<td style="text-align: left;">Optimize spatial and temporal placement to reduce communication overhead.</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>Model-Specific Execution Needs</strong></td>
<td style="text-align: left;">CNNs, transformers, and GNNs require different execution patterns, making a single placement strategy ineffective.</td>
<td style="text-align: left;">Tailor placement strategies to match the computational structure of each model type.</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p>Each of these challenges highlights a core trade-off in computation placement: maximizing parallelism while minimizing memory overhead. For CNNs, placement strategies prioritize structured tiling to maintain efficient data reuse. For transformers, placement must ensure balanced execution across attention layers. For GNNs, placement must dynamically adjust to sparse computation patterns.</p>
<p>Beyond model-specific needs, effective computation placement must also be scalable. As models grow in size and complexity, placement strategies must adapt dynamically rather than relying on static execution patterns. Future AI accelerators increasingly integrate runtime-aware scheduling mechanisms, where placement is optimized based on real-time workload behavior rather than predetermined execution plans.</p>
<p>Effective computation placement requires balancing hardware capabilities with model characteristics. The next section explores how computation placement interacts with memory allocation and data movement, ensuring that AI accelerators operate at peak efficiency.</p>
</section></section><section id="sec-ai-acceleration-memory-allocation-e095" class="level3 page-columns page-full"><h3 class="anchored" data-anchor-id="sec-ai-acceleration-memory-allocation-e095">Memory Allocation</h3>
<p>Efficient memory allocation is essential for high-performance AI acceleration. As AI models grow in complexity, accelerators must manage vast amounts of data movementâ€”loading model parameters, storing intermediate activations, and handling gradient computations. The way this data is allocated across the memory hierarchy directly affects execution efficiency, power consumption, and overall system throughput.</p>
<section id="sec-ai-acceleration-memory-allocation-definition-e740" class="level4 page-columns page-full"><h4 class="anchored" data-anchor-id="sec-ai-acceleration-memory-allocation-definition-e740">Memory Allocation Definition</h4>
<p>While computation placement determines where operations are executed, memory allocation defines where data is stored and how it is accessed throughout execution. All AI accelerators rely on hierarchical memory systems, ranging from on-chip caches and scratchpads to HBM and DRAM. Poor memory allocation can lead to excessive off-chip memory accesses, increasing bandwidth contention and slowing down execution. Since AI accelerators operate at teraflop and petaflop scales, inefficient memory access patterns can result in substantial performance bottlenecks.</p>
<p>The primary goal of memory allocation is to minimize latency and reduce power consumption by keeping frequently accessed data as close as possible to the processing elements. Different hardware architectures implement memory hierarchies tailored for AI workloads. GPUs rely on a mix of global memory, shared memory, and registers, requiring careful tiling strategies to optimize locality <span class="citation" data-cites="nvidia2020ampere">(<a href="#ref-nvidia2020ampere" role="doc-biblioref">Qi, Kantarci, and Liu 2017</a>)</span>. TPUs use on-chip SRAM scratchpads, where activations and weights must be efficiently preloaded to sustain systolic array execution <span class="citation" data-cites="jouppi_tpu_2017">(<a href="#ref-jouppi_tpu_2017" role="doc-biblioref">Norman P. Jouppi et al. 2017b</a>)</span>. Wafer-scale processors, with their hundreds of thousands of cores, demand sophisticated memory partitioning strategies to avoid excessive interconnect traffic <span class="citation" data-cites="Cerebras2021">(<a href="#ref-Cerebras2021" role="doc-biblioref">Systems 2021b</a>)</span>. In all cases, the effectiveness of memory allocation determines the overall throughput, power efficiency, and scalability of AI execution.</p>
<div class="no-row-height column-margin column-container"><div id="ref-jouppi_tpu_2017" class="csl-entry" role="listitem">
Jouppi, Norman P., Cliff Young, Nishant Patil, David Patterson, Gaurav Agrawal, Raminder Bajwa, Sarah Bates, et al. 2017b. <span>â€œIn-Datacenter Performance Analysis of a Tensor Processing Unit.â€</span> In <em>Proceedings of the 44th Annual International Symposium on Computer Architecture</em>, 1â€“12. ACM. <a href="https://doi.org/10.1145/3079856.3080246">https://doi.org/10.1145/3079856.3080246</a>.
</div><div id="ref-Cerebras2021" class="csl-entry" role="listitem">
â€”â€”â€”. 2021b. <span>â€œWafer-Scale Deep Learning Acceleration with the Cerebras CS-2.â€</span> <em>Cerebras Technical Paper</em>.
</div></div><p>Memory allocation directly impacts AI acceleration efficiency through data storage and access patterns. Unlike general-purpose computing, where memory management is abstracted by caches and dynamic allocation, AI accelerators require explicit data placement strategies to sustain high throughput and avoid unnecessary stalls. This is particularly evident in systolic arrays (<a href="#fig-systolic-array" class="quarto-xref">Figure&nbsp;4</a>), where the rhythmic data flow between processing elements depends on precisely timed memory access patterns. In TPUâ€™s systolic arrays, for instance, weights must be preloaded into on-chip scratchpads and streamed through the array in perfect synchronization with input activations to maintain the pipelined computation flow. When memory is not allocated efficiently, AI workloads suffer from latency overhead, excessive power consumption, and bottlenecks that limit computational performance.</p>
</section><section id="sec-ai-acceleration-memory-challenges-different-workloads-e87c" class="level4 page-columns page-full"><h4 class="anchored" data-anchor-id="sec-ai-acceleration-memory-challenges-different-workloads-e87c">Memory Challenges for Different Workloads</h4>
<p>Neural network architectures have varying memory demands, which influence the importance of proper allocation. CNNs rely on structured and localized data access patterns, meaning that inefficient memory allocation can lead to redundant data loads and cache inefficiencies <span class="citation" data-cites="chen2016eyeriss">(<a href="#ref-chen2016eyeriss" role="doc-biblioref">Chen et al. 2016</a>)</span>. In contrast, transformer models require frequent access to large model parameters and intermediate activations, making them highly sensitive to memory bandwidth constraints. GNNs introduce even greater challenges, as their irregular and sparse data structures result in unpredictable memory access patterns that can lead to inefficient use of memory resources. Poor memory allocation has three major consequences for AI execution:</p>
<div class="no-row-height column-margin column-container"></div><ol type="1">
<li>
<strong>Increased Memory Latency</strong>: When frequently accessed data is not stored in the right location, accelerators must retrieve it from higher-latency memory, slowing down execution.</li>
<li>
<strong>Higher Power Consumption</strong>: Off-chip memory accesses consume significantly more energy than on-chip storage, leading to inefficiencies at scale.</li>
<li>
<strong>Reduced Computational Throughput</strong>: If data is not available when needed, processing elements remain idle, reducing the overall performance of the system.</li>
</ol>
<p>As AI models continue to grow in size and complexity, the importance of scalable and efficient memory allocation increases. Memory limitations can dictate how large of a model can be deployed on a given accelerator, affecting feasibility and performance.</p>
<div id="tbl-memory-allocation" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure"><figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-memory-allocation-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;12: <strong>Memory Allocation Challenges</strong>: Efficient memory management in AI accelerators balances data access speed with hardware constraints, mitigating performance bottlenecks caused by latency, bandwidth limitations, and irregular data patterns. Addressing these challenges is critical for deploying complex models, such as transformers and graphs, which have variable and demanding memory requirements.
</figcaption><div aria-describedby="tbl-memory-allocation-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<colgroup>
<col style="width: 17%">
<col style="width: 38%">
<col style="width: 43%">
</colgroup>
<thead><tr class="header">
<th style="text-align: left;"><strong>Challenge</strong></th>
<th style="text-align: left;"><strong>Impact on Execution</strong></th>
<th style="text-align: left;"><strong>Key Considerations for Allocation</strong></th>
</tr></thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><strong>High Memory Latency</strong></td>
<td style="text-align: left;">Slow data access delays execution and reduces throughput.</td>
<td style="text-align: left;">Prioritize placing frequently accessed data in faster memory locations.</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Limited On-Chip Storage</strong></td>
<td style="text-align: left;">Small local memory constrains the amount of data available near compute units.</td>
<td style="text-align: left;">Allocate storage efficiently to maximize data availability without exceeding hardware limits.</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>High Off-Chip Bandwidth Demand</strong></td>
<td style="text-align: left;">Frequent access to external memory increases delays and power consumption.</td>
<td style="text-align: left;">Reduce unnecessary memory transfers by carefully managing when and how data is moved.</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Irregular Memory Access Patterns</strong></td>
<td style="text-align: left;">Some models require accessing data unpredictably, leading to inefficient memory usage.</td>
<td style="text-align: left;">Organize memory layout to align with access patterns and minimize unnecessary data movement.</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>Model-Specific Memory Needs</strong></td>
<td style="text-align: left;">Different models require different allocation strategies to optimize performance.</td>
<td style="text-align: left;">Tailor allocation decisions based on the structure and execution characteristics of the workload.</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p>As summarized in <a href="#tbl-memory-allocation" class="quarto-xref">Table&nbsp;12</a>, memory allocation in AI accelerators must address several key challenges that influence execution efficiency. Effective allocation strategies mitigate high latency, bandwidth limitations, and irregular access patterns by carefully managing data placement and movement. Ensuring that frequently accessed data is stored in faster memory locations while minimizing unnecessary transfers is essential for maintaining performance and energy efficiency.</p>
<p>Each of these challenges requires careful memory management to balance execution efficiency with hardware constraints. While structured models may benefit from well-defined memory layouts that facilitate predictable access, others, like transformer-based and graph-based models, require more adaptive allocation strategies to handle variable and complex memory demands. Beyond workload-specific considerations, memory allocation must also be scalable. As model sizes continue to grow, accelerators must dynamically manage memory resources rather than relying on static allocation schemes. Ensuring that frequently used data is accessible when needed without overwhelming memory capacity is essential for maintaining high efficiency.</p>
</section></section><section id="sec-ai-acceleration-combinatorial-complexity-ea33" class="level3 page-columns page-full"><h3 class="anchored" data-anchor-id="sec-ai-acceleration-combinatorial-complexity-ea33">Combinatorial Complexity</h3>
<p>The efficient execution of machine learning models on AI accelerators requires careful consideration of placement and allocation. Placement involves spatial assignment of computations and data, while allocation covers temporal distribution of resources. These decisions are interdependent, and each introduces trade-offs that impact performance, energy efficiency, and scalability. <a href="#tbl-combinatorial-complexity" class="quarto-xref">Table&nbsp;13</a> outlines the fundamental trade-offs between computation placement and resource allocation in AI accelerators. Placement decisions influence parallelism, memory access patterns, and communication overhead, while allocation strategies determine how resources are distributed over time to balance execution efficiency. The interplay between these factors shapes overall performance, requiring a careful balance to avoid bottlenecks such as excessive synchronization, memory congestion, or underutilized compute resources. Optimizing these trade-offs is essential for ensuring that AI accelerators operate at peak efficiency.</p>
<p>Each of these dimensions requires balancing trade-offs between placement and allocation. For instance, spatially distributing computations across multiple processing elements can increase throughput; however, if data allocation is not optimized, memory bandwidth limitations may introduce bottlenecks. Likewise, allocating resources for fine-grained computations may enhance flexibility but, without appropriate placement strategies, may lead to excessive synchronization overhead.</p>
<div id="tbl-combinatorial-complexity" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure"><figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-combinatorial-complexity-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;13: <strong>Placement-Allocation Trade-Offs</strong>: AI accelerator performance depends on strategically mapping computations to hardware and allocating resources over time, balancing parallelism, memory access, and execution efficiency to avoid bottlenecks. Careful consideration of these interdependent factors is essential for maximizing throughput and minimizing energy consumption in machine learning systems.
</figcaption><div aria-describedby="tbl-combinatorial-complexity-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<colgroup>
<col style="width: 15%">
<col style="width: 42%">
<col style="width: 41%">
</colgroup>
<thead><tr class="header">
<th style="text-align: left;"><strong>Dimension</strong></th>
<th style="text-align: left;"><strong>Placement Considerations</strong></th>
<th style="text-align: left;"><strong>Allocation Considerations</strong></th>
</tr></thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><strong>Computational Granularity</strong></td>
<td style="text-align: left;">Fine-grained placement enables greater parallelism but increases synchronization overhead.</td>
<td style="text-align: left;">Coarse-grained allocation reduces synchronization overhead but may limit flexibility.</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Spatial vs.&nbsp;Temporal Mapping</strong></td>
<td style="text-align: left;">Spatial placement enhances parallel execution but can lead to resource contention and memory congestion.</td>
<td style="text-align: left;">Temporal allocation balances resource sharing but may reduce overall throughput.</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>Memory and Data Locality</strong></td>
<td style="text-align: left;">Placing data closer to compute units minimizes latency but may reduce overall memory availability.</td>
<td style="text-align: left;">Allocating data across multiple memory levels increases capacity but introduces higher access costs.</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Communication and Synchronization</strong></td>
<td style="text-align: left;">Co-locating compute units reduces communication latency but may introduce contention.</td>
<td style="text-align: left;">Allocating synchronization mechanisms mitigates stalls but can introduce additional overhead.</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>Dataflow and Execution Ordering</strong></td>
<td style="text-align: left;">Static placement simplifies execution but limits adaptability to workload variations.</td>
<td style="text-align: left;">Dynamic allocation improves adaptability but adds scheduling complexity.</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p>Because AI accelerator architectures impose constraints on both where computations execute and how resources are assigned over time, selecting an effective mapping strategy necessitates a coordinated approach to placement and allocation. Understanding how these trade-offs influence execution efficiency is essential for optimizing performance on AI accelerators.</p>
<section id="sec-ai-acceleration-exploring-configuration-space-f010" class="level4"><h4 class="anchored" data-anchor-id="sec-ai-acceleration-exploring-configuration-space-f010">Exploring the Configuration Space</h4>
<p>The efficiency of AI accelerators is determined not only by their computational capabilities but also by how neural network computations are mapped to hardware resources. Mapping defines how computations are assigned to processing elements, how data is placed and moved through the memory hierarchy, and how execution is scheduled. The choices made in this process significantly impact performance, influencing compute utilization, memory bandwidth efficiency, and energy consumption.</p>
<p>Mapping machine learning models to hardware presents a large and complex design space. Unlike traditional computational workloads, model execution involves multiple interacting factors, including computation, data movement, parallelism, and scheduling, each introducing constraints and trade-offs. The hierarchical memory structure of accelerators, as discussed in the Memory Systems section, further complicates this process by imposing limits on bandwidth, latency, and data reuse. As a result, effective mapping strategies must carefully balance competing objectives to maximize efficiency.</p>
<p>At the heart of this design space lie three interconnected aspects: data placement, computation scheduling, and data movement timing. Data placement refers to the allocation of data across various memory hierarchies, such as on-chip buffers, caches, and off-chip DRAM, and its effective management is critical because it influences both latency and energy consumption. Inefficient placement often results in frequent, costly memory accesses, whereas strategic placement ensures that data used regularly remains in fast-access storage. Computation scheduling governs the order in which operations execute, impacting compute efficiency and memory access patterns; for instance, some execution orders may optimize parallelism while introducing synchronization overheads, and others may improve data locality at the expense of throughput. Meanwhile, timing in data movement is equally essential, as transferring data between memory levels incurs significant latency and energy costs. Efficient mapping strategies thus focus on minimizing unnecessary transfers by reusing data and overlapping communication with computation to enhance overall performance.</p>
<p>These factors define a vast combinatorial design space, where small variations in mapping decisions can lead to large differences in performance and energy efficiency. A poor mapping strategy can result in underutilized compute resources, excessive data movement, or imbalanced workloads, creating bottlenecks that degrade overall efficiency. Conversely, a well-designed mapping maximizes both throughput and resource utilization, making efficient use of available hardware.</p>
<p>Because of the interconnected nature of mapping decisions, there is no single optimal solutionâ€”different workloads and hardware architectures demand different approaches. The next sections examine the structure of this design space and how different mapping choices shape the execution of machine learning workloads.</p>
<p>Mapping machine learning computations onto specialized hardware requires balancing multiple constraints, including compute efficiency, memory bandwidth, and execution scheduling. The challenge arises from the vast number of possible ways to assign computations to processing elements, order execution, and manage data movement. Each decision contributes to a high-dimensional search space, where even minor variations in mapping choices can significantly impact performance.</p>
<p>Unlike traditional workloads with predictable execution patterns, machine learning models introduce diverse computational structures that require flexible mappings adapted to data reuse, parallelization opportunities, and memory constraints. The search space grows combinatorially, making exhaustive search infeasible. To understand this complexity, three sources emerge of variation:</p>
</section><section id="sec-ai-acceleration-ordering-computation-execution-7251" class="level4 page-columns page-full"><h4 class="anchored" data-anchor-id="sec-ai-acceleration-ordering-computation-execution-7251">Ordering Computation and Execution</h4>
<p>Machine learning workloads are often structured as nested loops, iterating over various dimensions of computation. For instance, a matrix multiplication kernel may loop over batch size (<span class="math inline">\(N\)</span>), input features (<span class="math inline">\(C\)</span>), and output features (<span class="math inline">\(K\)</span>). The order in which these loops execute has a profound effect on data locality, reuse patterns, and computational efficiency.</p>
<p>The number of ways to arrange <span class="math inline">\(d\)</span> loops follows a factorial growth pattern: <span class="math display">\[
\mathcal{O} = d!
\]</span> which scales rapidly. A typical convolutional layer may involve up to seven loop dimensions, leading to: <span class="math display">\[
7! = 5,040 \text{ possible execution orders.}
\]</span></p>
<p>When considering multiple memory levels, the search space expands as: <span class="math display">\[
(d!)^l
\]</span> where <span class="math inline">\(l\)</span> is the number of memory hierarchy levels. This rapid expansion highlights why execution order optimization is crucialâ€”poor loop ordering can lead to excessive memory traffic, while an optimized order improves cache utilization <span class="citation" data-cites="sze2017efficient">(<a href="#ref-sze2017efficient" role="doc-biblioref">Sze et al. 2017a</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-sze2017efficient" class="csl-entry" role="listitem">
Sze, Vivienne, Yu-Hsin Chen, Tien-Ju Yang, and Joel Emer. 2017a. <span>â€œEfficient Processing of Deep Neural Networks: A Tutorial and Survey.â€</span> <em>Proceedings of the IEEE</em> 105 (12): 2295â€“2329. <a href="https://doi.org/10.1109/jproc.2017.2761740">https://doi.org/10.1109/jproc.2017.2761740</a>.
</div></div></section><section id="sec-ai-acceleration-parallelization-across-processing-elements-90d6" class="level4"><h4 class="anchored" data-anchor-id="sec-ai-acceleration-parallelization-across-processing-elements-90d6">Parallelization Across Processing Elements</h4>
<p>Modern AI accelerators leverage thousands of processing elements to maximize parallelism, but determining which computations should be parallelized is non-trivial. Excessive parallelization can introduce synchronization overheads and increased bandwidth demands, while insufficient parallelization leads to underutilized hardware.</p>
<p>The number of ways to distribute computations among parallel units follows the binomial coefficient: <span class="math display">\[
\mathcal{P} = \frac{d!}{(d-k)!}
\]</span> where <span class="math inline">\(d\)</span> is the number of loops, and <span class="math inline">\(k\)</span> is the number selected for parallel execution. For a six-loop computation where three loops are chosen for parallel execution, the number of valid configurations is: <span class="math display">\[
\frac{6!}{(6-3)!} = 120.
\]</span></p>
<p>Even for a single layer, there can be hundreds of valid parallelization strategies, each affecting data synchronization, memory contention, and overall compute efficiency. Expanding this across multiple layers and model architectures further magnifies the complexity.</p>
</section><section id="sec-ai-acceleration-memory-placement-data-movement-fd52" class="level4"><h4 class="anchored" data-anchor-id="sec-ai-acceleration-memory-placement-data-movement-fd52">Memory Placement and Data Movement</h4>
<p>The hierarchical memory structure of AI accelerators introduces additional constraints, as data must be efficiently placed across registers, caches, shared memory, and off-chip DRAM. Data placement impacts latency, bandwidth consumption, and energy efficiencyâ€”frequent access to slow memory creates bottlenecks, while optimized placement reduces costly memory transfers.</p>
<p>The number of ways to allocate data across memory levels follows an exponential growth function: <span class="math display">\[
\mathcal{M} = n^{d \times l}
\]</span> where:</p>
<ul>
<li>
<span class="math inline">\(n\)</span> = number of placement choices per level,</li>
<li>
<span class="math inline">\(d\)</span> = number of computational dimensions,</li>
<li>
<span class="math inline">\(l\)</span> = number of memory hierarchy levels.</li>
</ul>
<p>For a model with:</p>
<ul>
<li>
<span class="math inline">\(d = 5\)</span> computational dimensions,</li>
<li>
<span class="math inline">\(l = 3\)</span> memory levels,</li>
<li>
<span class="math inline">\(n = 4\)</span> possible placement choices per level,</li>
</ul>
<p>the number of possible memory allocations is: <span class="math display">\[
4^{5 \times 3} = 4^{15} = 1,073,741,824.
\]</span></p>
<p>This highlights how even a single layer may have over a billion possible memory configurations, making manual optimization impractical.</p>
</section><section id="sec-ai-acceleration-mapping-search-space-e9b6" class="level4"><h4 class="anchored" data-anchor-id="sec-ai-acceleration-mapping-search-space-e9b6">Mapping Search Space</h4>
<p>By combining the complexity from computation ordering, parallelization, and memory placement, the total mapping search space can be approximated as: <span class="math display">\[
\mathcal{S} = \left( n^d \times d! \times \frac{d!}{(d-k)!} \right)^l
\]</span> where:</p>
<ul>
<li>
<span class="math inline">\(n^d\)</span> represents memory placement choices,</li>
<li>
<span class="math inline">\(d!\)</span> accounts for computation ordering choices,</li>
<li>
<span class="math inline">\(\frac{d!}{(d-k)!}\)</span> captures parallelization possibilities,</li>
<li>
<span class="math inline">\(l\)</span> is the number of memory hierarchy levels.</li>
</ul>
<p>This equation illustrates the exponential growth of the search space, making brute-force search infeasible for all but the simplest cases.</p>
<div id="quiz-question-sec-ai-acceleration-hardware-mapping-fundamentals-neural-networks-f9a9" class="callout callout-quiz-question">
<details class="callout-quiz-question fbx-default closebutton"><summary><strong>Self-Check: Question 1.5</strong></summary><div>
<ol type="1">
<li>
<p>Which of the following best describes the primary goal of mapping in AI acceleration?</p>
<ol type="a">
<li>Optimizing execution efficiency by aligning computations with hardware resources.</li>
<li>Minimizing the energy consumption of the accelerator.</li>
<li>Maximizing the number of processing elements used at any time.</li>
<li>Ensuring all computations are executed in parallel.</li>
</ol>
</li>
<li><p>True or False: Effective computation placement on AI accelerators always requires manual intervention by developers.</p></li>
<li><p>Why is data locality critical in the mapping of neural networks onto AI accelerators?</p></li>
<li><p>Order the following steps in the mapping process for neural networks on AI accelerators: (1) Data placement, (2) Computation scheduling, (3) Data movement timing.</p></li>
<li><p>In a production system, how might poor computation placement affect the performance of AI accelerators?</p></li>
</ol>
<p><a href="#quiz-answer-sec-ai-acceleration-hardware-mapping-fundamentals-neural-networks-f9a9" class="question-label">See Answers â†’</a></p>
</div></details>
</div>
</section></section></section><section id="sec-ai-acceleration-dataflow-optimization-strategies-ce52" class="level2 page-columns page-full"><h2 class="anchored" data-anchor-id="sec-ai-acceleration-dataflow-optimization-strategies-ce52">Dataflow Optimization Strategies</h2>
<p>Mapping strategies establish <em>where</em> computations execute and <em>where</em> data resides within an acceleratorâ€™s architecture, but they do not specify <em>how</em> data flows through processing elements during execution. A systolic array might process a matrix multiplication with weights stored in local memory, but the order in which weights, inputs, and outputs move through the array fundamentally determines memory bandwidth consumption and energy efficiency. These dataflow patternsâ€”termed optimization strategiesâ€”represent the critical implementation dimension that translates abstract mapping decisions into concrete execution plans.</p>
<p>The choice among weight-stationary, input-stationary, and output-stationary approaches directly impacts whether an accelerator operates in the compute-bound or memory-bound region. Understanding these trade-offs is essential because compilers (<a href="#sec-ai-acceleration-compiler-support-172e" class="quarto-xref">Section&nbsp;1.7</a>) and runtime systems (<a href="#sec-ai-acceleration-runtime-support-f94f" class="quarto-xref">Section&nbsp;1.8</a>) must select appropriate dataflow patterns based on computational characteristics and memory hierarchy capabilities analyzed in <a href="#sec-ai-acceleration-memory-hierarchy-1839" class="quarto-xref">Section&nbsp;1.4.2</a>.</p>
<p>Efficiently mapping machine learning computations onto hardware is a complex challenge due to the vast number of possible configurations. As models grow in complexity, the number of potential mappings increases exponentially. Even for a single layer, there are thousands of ways to order computation loops, hundreds of parallelization strategies, and an exponentially growing number of memory placement choices. This combinatorial explosion makes exhaustive search impractical.</p>
<p>To overcome this challenge, AI accelerators rely on structured mapping strategies that systematically balance computational efficiency, data locality, and parallel execution. Rather than evaluating every possible configuration, these approaches use a combination of heuristic, analytical, and machine learning-based techniques to find high-performance mappings efficiently.</p>
<p>The key to effective mapping lies in understanding and applying a set of core techniques that optimize data movement, memory access, and computation. These building blocks of mapping strategies provide a structured foundation for efficient execution, explored in the next section.</p>
<section id="sec-ai-acceleration-building-blocks-mapping-strategies-4932" class="level3 page-columns page-full"><h3 class="anchored" data-anchor-id="sec-ai-acceleration-building-blocks-mapping-strategies-4932">Building Blocks of Mapping Strategies</h3>
<p>To navigate the complexity of mapping decisions, a set of foundational techniques is leveraged that optimizes execution across data movement, memory access, and computation efficiency. These techniques provide the necessary structure for mapping strategies that maximize hardware performance while minimizing bottlenecks.</p>
<p>Key techniques include data movement strategies, which determine where data is staged during computation in order to reduce redundant transfers, such as in weight stationary, output stationary, and input stationary approaches. Memory-aware tensor layouts also play an important role by influencing memory access patterns and cache efficiency through the organization of data in formats such as row-major or channel-major.</p>
<p>Other strategies involve kernel fusion, a method that minimizes redundant memory writes by combining multiple operations into a single computational step. Tiling is employed as a technique that partitions large computations into smaller, memory-friendly blocks to improve cache efficiency and reduce memory bandwidth requirements. Finally, balancing computation and communication is essential for managing the trade-offs between parallel execution and memory access to achieve high throughput.</p>
<p>Each of these building blocks plays a crucial role in structuring high-performance execution, forming the basis for both heuristic and model-driven optimization techniques. The next section explores how these strategies are adapted to different types of AI models.</p>
<section id="sec-ai-acceleration-data-movement-patterns-3b06" class="level4 page-columns page-full"><h4 class="anchored" data-anchor-id="sec-ai-acceleration-data-movement-patterns-3b06">Data Movement Patterns</h4>
<p>While computational mapping determines where and when operations occur, its success depends heavily on how efficiently data is accessed and transferred across the memory hierarchy. As discussed in <a href="#sec-ai-acceleration-irregular-memory-access-c6ec" class="quarto-xref">Section&nbsp;1.4.1.3</a>, machine learning workloads exhibit irregular access patterns that challenge standard caching mechanisms. This irregularity makes data movement strategy critical to overall system performance.</p>
<p>Even when computational units are mapped efficiently, poor data movement strategies can severely degrade performance, leading to frequent memory stalls and underutilized hardware resources. If data cannot be supplied to processing elements at the required rate, computational units remain idle, increasing latency, memory traffic, and energy consumption <span class="citation" data-cites="chen2016eyeriss">(<a href="#ref-chen2016eyeriss" role="doc-biblioref">Chen et al. 2016</a>)</span>.</p>
<div class="no-row-height column-margin column-container"></div><p>To illustrate the impact of data movement inefficiencies, consider a typical matrix multiplication operation shown in <a href="#lst-matmul_data_movement" class="quarto-xref">Listing&nbsp;18</a>, which forms the backbone of many machine learning models.</p>
<div id="lst-matmul_data_movement" class="listing quarto-float quarto-figure quarto-figure-left anchored">
<figure class="quarto-float quarto-float-lst figure"><figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-lst" id="lst-matmul_data_movement-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Listing&nbsp;18: <strong>Matrix Multiplication</strong>: Data movement bottlenecks can lead to underutilized hardware resources, illustrating the importance of efficient data flow in optimizing machine learning model performance. Via This operation
</figcaption><div aria-describedby="lst-matmul_data_movement-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="sourceCode" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="co">## Matrix multiplication where:</span></span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a><span class="co">## weights: [512 x 256] - model parameters</span></span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a><span class="co">## input:   [256 x 32]  - batch of activations</span></span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a><span class="co">## Z:       [512 x 32]  - output activations</span></span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a><span class="co">## Computing each output element Z[i,j]:</span></span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">512</span>):</span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">32</span>):</span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> k <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">256</span>):</span>
<span id="cb18-10"><a href="#cb18-10" aria-hidden="true" tabindex="-1"></a>            Z[i, j] <span class="op">+=</span> weights[i, k] <span class="op">*</span> <span class="bu">input</span>[k, j]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</figure>
</div>
<p>This computation reveals several critical dataflow challenges. The first challenge is the number of memory accesses required. For each output <span class="math inline">\(Z[i, j]\)</span>, the computation must fetch an entire row of weights from the weight matrix and a full column of activations from the input matrix. Since the weight matrix contains 512 rows and the input matrix contains 32 columns, this results in repeated memory accesses that place a significant burden on memory bandwidth.</p>
<p>The second challenge comes from weight reuse. The same weights are applied to multiple inputs, meaning that an ideal mapping strategy should maximize weight locality to avoid redundant memory fetches. Without proper reuse, the accelerator would waste bandwidth loading the same weights multiple times <span class="citation" data-cites="chen2018tvm">(<a href="#ref-chen2018tvm" role="doc-biblioref">Tianqi et al. 2018</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-chen2018tvm" class="csl-entry" role="listitem">
Tianqi, Chen et al. 2018. <span>â€œTVM: An Automated End-to-End Optimizing Compiler for Deep Learning.â€</span> <em>13th USENIX Symposium on Operating Systems Design and Implementation (OSDI 18)</em>, 578â€“94.
</div></div><p>The third challenge involves the accumulation of intermediate results. Since each element in <span class="math inline">\(Z[i,j]\)</span> requires contributions from 256 different weight-input pairs, partial sums must be stored and retrieved before the final value is computed. If these intermediate values are stored inefficiently, the system will require frequent memory accesses, further increasing bandwidth demands.</p>
<p>A natural way to mitigate these challenges is to leverage SIMD and SIMT execution models, which allow multiple values to be fetched in parallel. However, even with these optimizations, data movement remains a bottleneck. The issue is not just how quickly data is retrieved but how often it must be moved and where it is placed within the memory hierarchy <span class="citation" data-cites="han2016eie">(<a href="#ref-han2016eie" role="doc-biblioref">Han et al. 2016</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-han2016eie" class="csl-entry" role="listitem">
Han, Song, Xingyu Liu, Huizi Mao, Jing Pu, Ardavan Pedram, Mark A. Horowitz, and William J. Dally. 2016. <span>â€œEIE: Efficient Inference Engine on Compressed Deep Neural Network.â€</span> In <em>2016 ACM/IEEE 43rd Annual International Symposium on Computer Architecture (ISCA)</em>, 243â€“54. IEEE. <a href="https://doi.org/10.1109/isca.2016.30">https://doi.org/10.1109/isca.2016.30</a>.
</div></div><p>Given that data movement is 100-1000Ã— more expensive than computation, the single most important goal of an accelerator is to minimize memory access. Dataflow strategies are the architectural patterns designed to achieve this by maximizing data reuse. The question is: which data is most valuable to keep local? This directly addresses the AI Memory Wall challenge examined in <a href="#sec-ai-acceleration-understanding-ai-memory-wall-3ea9" class="quarto-xref">Section&nbsp;1.4.1</a>, where the extreme energy penalty for memory access dominates system performance.</p>
<p>To address these constraints, accelerators implement dataflow strategies that determine which data remains fixed in memory and which data is streamed dynamically. These strategies represent different answers to the fundamental question of data locality: weight-stationary keeps model parameters local, input-stationary maintains activation data, and output-stationary preserves intermediate results. Each approach trades off different memory access patterns to maximize data reuse and minimize the energy-intensive transfers that constitute the primary bottleneck in AI acceleration.</p>
<section id="sec-ai-acceleration-weight-stationary-156a" class="level5"><h5 class="anchored" data-anchor-id="sec-ai-acceleration-weight-stationary-156a">Weight Stationary</h5>
<p>The Weight Stationary strategy keeps weights fixed in local memory, while input activations and partial sums are streamed through the system. Weight stationary approaches prove particularly beneficial in CNNs and matrix multiplications, where the same set of weights is applied across multiple inputs. By ensuring weights remain stationary, this method reduces redundant memory fetches, which helps alleviate bandwidth bottlenecks and improves energy efficiency.</p>
<p>A key advantage of the weight stationary approach is that it maximizes weight reuse, reducing the frequency of memory accesses to external storage. Since weight parameters are often shared across multiple computations, keeping them in local memory eliminates unnecessary data movement, lowering the overall energy cost of computation. This makes it particularly effective for architectures where weights represent the dominant memory overhead, such as systolic arrays and custom accelerators designed for machine learning.</p>
<p>A simplified Weight Stationary implementation for matrix multiplication is illustrated in <a href="#lst-weight_stationary" class="quarto-xref">Listing&nbsp;19</a>.</p>
<div id="lst-weight_stationary" class="listing quarto-float quarto-figure quarto-figure-left anchored">
<figure class="quarto-float quarto-float-lst figure"><figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-lst" id="lst-weight_stationary-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Listing&nbsp;19: <strong>Weight Stationary Matrix Multiplication</strong>: Weight stationary matrix multiplication keeps weights fixed in local memory while input activations stream through, demonstrating how it maximizes weight reuse to reduce energy costs.
</figcaption><div aria-describedby="lst-weight_stationary-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="sourceCode" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="co">## Weight Stationary Matrix Multiplication</span></span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a><span class="co">## - Weights remain fixed in local memory</span></span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a><span class="co">## - Input activations stream through</span></span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a><span class="co">## - Partial sums accumulate for final output</span></span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> weight_block <span class="kw">in</span> weights:  <span class="co"># Load and keep weights stationary</span></span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a>    load_to_local(weight_block)  <span class="co"># Fixed in local storage</span></span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> input_block <span class="kw">in</span> inputs:  <span class="co"># Stream inputs dynamically</span></span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> output_block <span class="kw">in</span> outputs:  <span class="co"># Compute results</span></span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a>            output_block <span class="op">+=</span> compute(weight_block, input_block)</span>
<span id="cb19-11"><a href="#cb19-11" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Reuse weights across inputs</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</figure>
</div>
<p>In weight stationary execution, weights are loaded once into local memory and remain fixed throughout the computation, while inputs are streamed dynamically, thereby reducing redundant memory accesses. At the same time, partial sums are accumulated in an efficient manner that minimizes unnecessary data movement, ensuring that the system maintains high throughput and energy efficiency.</p>
<p>By keeping weights fixed in local storage, memory bandwidth requirements are significantly reduced, as weights do not need to be reloaded for each new computation. Instead, the system efficiently reuses the stored weights across multiple input activations, allowing for high throughput execution. This makes weight stationary dataflow highly effective for workloads with heavy weight reuse patterns, such as CNNs and matrix multiplications.</p>
<p>However, while this strategy reduces weight-related memory traffic, it introduces trade-offs in input and output movement. Since inputs must be streamed dynamically while weights remain fixed, the efficiency of this approach depends on how well input activations can be delivered to the computational units without causing stalls. Additionally, partial sums, which represent intermediate results, must be carefully accumulated to avoid excessive memory traffic. The total performance gain depends on the size of available on-chip memory, as storing larger weight matrices locally can become a constraint in models with millions or billions of parameters.</p>
<p>The weight stationary strategy is well-suited for workloads where weights exhibit high reuse and memory bandwidth is a limiting factor. It is commonly employed in CNNs, systolic arrays, and matrix multiplication kernels, where structured weight reuse leads to significant performance improvements. However, for models where input or output reuse is more critical, alternative dataflow strategies, such as output stationary or input stationary, may provide better trade-offs.</p>
</section><section id="sec-ai-acceleration-output-stationary-54e5" class="level5 page-columns page-full"><h5 class="anchored" data-anchor-id="sec-ai-acceleration-output-stationary-54e5">Output Stationary</h5>
<p>The Output Stationary strategy keeps partial sums fixed in local memory, while weights and input activations stream through the system. This approach is particularly effective for fully connected layers, systolic arrays, and other operations where an output element accumulates contributions from multiple weight-input pairs. By keeping partial sums stationary, this method reduces redundant memory writes, minimizing bandwidth consumption and improving energy efficiency <span class="citation" data-cites="chen2016eyeriss">(<a href="#ref-chen2016eyeriss" role="doc-biblioref">Chen et al. 2016</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-chen2016eyeriss" class="csl-entry" role="listitem">
Chen, Yu-Hsin, Tushar Krishna, Joel S. Emer, and Vivienne Sze. 2016. <span>â€œEyeriss: A Spatial Architecture for Energy-Efficient Dataflow for Convolutional Neural Networks.â€</span> <em>IEEE Journal of Solid-State Circuits</em> 51 (1): 186â€“98. <a href="https://doi.org/10.1109/JSSC.2015.2488709">https://doi.org/10.1109/JSSC.2015.2488709</a>.
</div></div><p>A key advantage of the output stationary approach is that it optimizes accumulation efficiency, ensuring that each output element is computed as efficiently as possible before being written to memory. Unlike Weight Stationary, which prioritizes weight reuse, Output Stationary execution is designed to minimize memory bandwidth overhead caused by frequent writes of intermediate results. This makes it well-suited for workloads where accumulation dominates the computational pattern, such as fully connected layers and matrix multiplications in transformer-based models.</p>
<p><a href="#lst-output_stationary" class="quarto-xref">Listing&nbsp;20</a> shows a simplified Output Stationary implementation for matrix multiplication.</p>
<div id="lst-output_stationary" class="listing quarto-float quarto-figure quarto-figure-left anchored">
<figure class="quarto-float quarto-float-lst figure"><figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-lst" id="lst-output_stationary-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Listing&nbsp;20: <strong>Output Stationary Execution</strong>: Accumulates partial sums locally to reduce memory writes and enhance efficiency during matrix multiplication, making it ideal for transformer-based models.
</figcaption><div aria-describedby="lst-output_stationary-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="sourceCode" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="co">## - Partial sums remain in local memory</span></span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a><span class="co">## - Weights and input activations stream through dynamically</span></span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a><span class="co">## - Final outputs are written only once</span></span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> output_block <span class="kw">in</span> outputs:  <span class="co"># Keep partial sums stationary</span></span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a>    accumulator <span class="op">=</span> <span class="dv">0</span>  <span class="co"># Initialize accumulation buffer</span></span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> weight_block, input_block <span class="kw">in</span> <span class="bu">zip</span>(weights, inputs):</span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a>        accumulator <span class="op">+=</span> compute(weight_block, input_block)</span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Accumulate partial sums</span></span>
<span id="cb20-10"><a href="#cb20-10" aria-hidden="true" tabindex="-1"></a>    store_output(accumulator)  <span class="co"># Single write to memory</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</figure>
</div>
<p>This implementation follows the core principles of output stationary execution:</p>
<ul>
<li>Partial sums are kept in local memory throughout the computation.</li>
<li>Weights and inputs are streamed dynamically, ensuring that intermediate results remain locally accessible.</li>
<li>Final outputs are written back to memory only once, reducing unnecessary memory traffic.</li>
</ul>
<p>By accumulating partial sums locally, this approach eliminates excessive memory writes, improving overall system efficiency. In architectures such as systolic arrays, where computation progresses through a grid of processing elements, keeping partial sums stationary aligns naturally with structured accumulation workflows, reducing synchronization overhead.</p>
<p>However, while Output Stationary reduces memory write traffic, it introduces trade-offs in weight and input movement. Since weights and activations must be streamed dynamically, the efficiency of this approach depends on how well data can be fed into the system without causing stalls. Additionally, parallel implementations must carefully synchronize updates to partial sums, especially in architectures where multiple processing elements contribute to the same output.</p>
<p>The Output Stationary strategy is most effective for workloads where accumulation is the dominant operation and minimizing intermediate memory writes is critical. It is commonly employed in fully connected layers, attention mechanisms, and systolic arrays, where structured accumulation leads to significant performance improvements. However, for models where input reuse is more critical, alternative dataflow strategies, such as Input Stationary, may provide better trade-offs.</p>
</section><section id="sec-ai-acceleration-input-stationary-6c7b" class="level5"><h5 class="anchored" data-anchor-id="sec-ai-acceleration-input-stationary-6c7b">Input Stationary</h5>
<p>The Input Stationary strategy keeps input activations fixed in local memory, while weights and partial sums stream through the system. This approach is particularly effective for batch processing, transformer models, and sequence-based architectures, where input activations are reused across multiple computations. By ensuring that activations remain in local memory, this method reduces redundant input fetches, improving data locality and minimizing memory traffic.</p>
<p>A key advantage of the Input Stationary approach is that it maximizes input reuse, reducing the frequency of memory accesses for activations. Since many models, especially those in NLP and recommendation systems, process the same input data across multiple computations, keeping inputs stationary eliminates unnecessary memory transfers, thereby lowering energy consumption. This strategy is particularly useful when dealing with large batch sizes, where a single batch of input activations contributes to multiple weight transformations.</p>
<p>A simplified Input Stationary implementation for matrix multiplication is illustrated in <a href="#lst-input_stationary" class="quarto-xref">Listing&nbsp;21</a>.</p>
<div id="lst-input_stationary" class="listing quarto-float quarto-figure quarto-figure-left anchored">
<figure class="quarto-float quarto-float-lst figure"><figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-lst" id="lst-input_stationary-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Listing&nbsp;21: <strong>Input Stationary</strong>: This approach keeps input activations stationary while dynamically streaming weights to maximize memory reuse and reduce energy consumption.
</figcaption><div aria-describedby="lst-input_stationary-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="sourceCode" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="co">## - Input activations remain in local memory</span></span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a><span class="co">## - Weights stream through dynamically</span></span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a><span class="co">## - Partial sums accumulate and are written out</span></span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> input_block <span class="kw">in</span> inputs:  <span class="co"># Keep input activations stationary</span></span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a>    load_to_local(input_block)  <span class="co"># Fixed in local storage</span></span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> weight_block <span class="kw">in</span> weights:  <span class="co"># Stream weights dynamically</span></span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> output_block <span class="kw">in</span> outputs:  <span class="co"># Compute results</span></span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"></a>            output_block <span class="op">+=</span> compute(weight_block, input_block)</span>
<span id="cb21-10"><a href="#cb21-10" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Reuse inputs across weights</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</figure>
</div>
<p>This implementation follows the core principles of input stationary execution:</p>
<ul>
<li>Input activations are loaded into local memory and remain fixed during computation.</li>
<li>
<strong>Weights are streamed dynamically</strong>, ensuring efficient application across multiple inputs.</li>
<li>
<strong>Partial sums are accumulated and written out</strong>, optimizing memory bandwidth usage.</li>
</ul>
<p>By keeping input activations stationary, this strategy minimizes redundant memory accesses to input data, significantly reducing external memory bandwidth requirements. This is particularly beneficial in transformer architectures, where each token in an input sequence is used across multiple attention heads and layers. Additionally, in batch processing scenarios, keeping input activations in local memory improves data locality, making it well-suited for fully connected layers and matrix multiplications.</p>
<p>However, while Input Stationary reduces memory traffic for activations, it introduces trade-offs in weight and output movement. Since weights must be streamed dynamically while inputs remain fixed, the efficiency of this approach depends on how well weights can be delivered to the computational units without causing stalls. Additionally, partial sums must be accumulated efficiently before being written back to memory, which may require additional buffering mechanisms.</p>
<p>The Input Stationary strategy is most effective for workloads where input activations exhibit high reuse, and memory bandwidth for inputs is a critical constraint. It is commonly employed in transformers, recurrent networks, and batch processing workloads, where structured input reuse leads to significant performance improvements. However, for models where output accumulation is more critical, alternative dataflow strategies, such as Output Stationary, may provide better trade-offs.</p>
</section></section><section id="sec-ai-acceleration-memoryefficient-tensor-layouts-e250" class="level4 page-columns page-full"><h4 class="anchored" data-anchor-id="sec-ai-acceleration-memoryefficient-tensor-layouts-e250">Memory-Efficient Tensor Layouts</h4>
<p>Efficient execution of machine learning workloads depends not only on how data moves (dataflow strategies) but also on how data is stored and accessed in memory. Tensor layouts, which refers to the arrangement of multidimensional data in memory, can significantly impact memory access efficiency, cache performance, and computational throughput. Poorly chosen layouts can lead to excessive memory stalls, inefficient cache usage, and increased data movement costs.</p>
<p>In AI accelerators, tensor layout optimization is particularly important because data is frequently accessed in patterns dictated by the underlying hardware architecture. Choosing the right layout ensures that memory accesses align with hardware-friendly access patterns, minimizing overhead from costly memory transactions <span class="citation" data-cites="nvidia2021cudnn">(<a href="#ref-nvidia2021cudnn" role="doc-biblioref">C. NVIDIA 2025</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-nvidia2021cudnn" class="csl-entry" role="listitem">
NVIDIA, Corporation. 2025. <span>â€œCost-Effective Deep Learning Infrastructure with NVIDIA GPU.â€</span> <em>Kathmandu University Journal of Science, Engineering and Technology</em> 19 (1). <a href="https://doi.org/10.70530/kuset.v19i1.587">https://doi.org/10.70530/kuset.v19i1.587</a>.
</div><div id="ref-xla2020" class="csl-entry" role="listitem">
He, Xuzhen. 2023a. <span>â€œAccelerated Linear Algebra Compiler for Computationally Efficient Numerical Models: Success and Potential Area of Improvement.â€</span> <em>PLOS ONE</em> 18 (2): e0282265. <a href="https://doi.org/10.1371/journal.pone.0282265">https://doi.org/10.1371/journal.pone.0282265</a>.
</div></div><p>While developers can sometimes manually specify tensor layouts, the choice is often determined automatically by machine learning frameworks (e.g., TensorFlow, PyTorch, JAX), compilers, or AI accelerator runtimes. Low-level optimization tools such as cuDNN (for NVIDIA GPUs), XLA (for TPUs), and MLIR (for custom accelerators) may rearrange tensor layouts dynamically to optimize performance <span class="citation" data-cites="xla2020">(<a href="#ref-xla2020" role="doc-biblioref">He 2023a</a>)</span>. In high-level frameworks, layout transformations are typically applied transparently, but developers working with custom kernels or low-level libraries (e.g., CUDA, Metal, or OpenCL) may have direct control over tensor format selection.</p>
<p>For example, in PyTorch, users can manually modify layouts using tensor.permute() or tensor.contiguous() to ensure efficient memory access <span class="citation" data-cites="paszke2019pytorch">(<a href="#ref-paszke2019pytorch" role="doc-biblioref">Paszke et al. 2019</a>)</span>. In TensorFlow, layout optimizations are often applied internally by the XLA compiler, choosing between NHWC (row-major) and NCHW (channel-major) based on the target hardware <span class="citation" data-cites="tensorflow2022">(<a href="#ref-tensorflow2022" role="doc-biblioref">Brain 2022</a>)</span>. Hardware-aware machine learning libraries, such as cuDNN for GPUs or OneDNN for CPUs, enforce specific memory layouts to maximize cache locality and SIMD efficiency. Ultimately, while developers may have some control over tensor layout selection, most layout decisions are driven by the compiler and runtime system, ensuring that tensors are stored in memory in a way that best suits the underlying hardware.</p>
<div class="no-row-height column-margin column-container"><div id="ref-paszke2019pytorch" class="csl-entry" role="listitem">
Paszke, Adam et al. 2019. <span>â€œPyTorch: An Imperative Style, High-Performance Deep Learning Library.â€</span> <em>NeurIPS</em>.
</div><div id="ref-tensorflow2022" class="csl-entry" role="listitem">
â€”â€”â€”. 2022. <em>TensorFlow Documentation</em>. <a href="https://www.tensorflow.org/">https://www.tensorflow.org/</a>.
</div></div><section id="sec-ai-acceleration-rowmajor-layout-741f" class="level5 page-columns page-full"><h5 class="anchored" data-anchor-id="sec-ai-acceleration-rowmajor-layout-741f">Row-Major Layout</h5>
<p>Row-major layout refers to the way multi-dimensional tensors are stored in memory, where elements are arranged row by row, ensuring that all values in a given row are placed contiguously before moving to the next row. This storage format is widely used in general-purpose CPUs and some machine learning frameworks because it aligns naturally with sequential memory access patterns, making it more cache-efficient for certain types of operations <span class="citation" data-cites="oneDNN2021">(<a href="#ref-oneDNN2021" role="doc-biblioref">Intel 2021</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-oneDNN2021" class="csl-entry" role="listitem">
Intel, Corporation. 2021. <em>oneDNN: Intelâ€™s Deep Learning Neural Network Library</em>. <a href="https://github.com/oneapi-src/oneDNN">https://github.com/oneapi-src/oneDNN</a>.
</div></div><p>To understand how row-major layout works, consider a single RGB image represented as a tensor of shape (Height, Width, Channels). If the image has a size of <span class="math inline">\(3\times 3\)</span> pixels with 3 channels (RGB), the corresponding tensor is structured as (3, 3, 3). The values are stored in memory as follows: <span class="math display">\[\begin{gather*}
I(0,0,0), I(0,0,1), I(0,0,2), I(0,1,0), I(0,1,1), \\
I(0,1,2), I(0,2,0), I(0,2,1), I(0,2,2), \ldots
\end{gather*}\]</span></p>
<p>Each row is stored contiguously, meaning all pixel values in the first row are placed sequentially in memory before moving on to the second row. This ordering is advantageous because CPUs and cache hierarchies are optimized for sequential memory access. When data is accessed in a row-wise fashion, such as when applying element-wise operations like activation functions or basic arithmetic transformations, memory fetches are efficient, and cache utilization is maximized <span class="citation" data-cites="sodani2017knl">(<a href="#ref-sodani2017knl" role="doc-biblioref">Sodani 2015</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-sodani2017knl" class="csl-entry" role="listitem">
Sodani, Avinash. 2015. <span>â€œKnights Landing (KNL): 2nd Generation Intel Xeon Phi Processor.â€</span> In <em>2015 IEEE Hot Chips 27 Symposium (HCS)</em>, 1â€“24. IEEE. <a href="https://doi.org/10.1109/hotchips.2015.7477467">https://doi.org/10.1109/hotchips.2015.7477467</a>.
</div></div><p>The efficiency of row-major storage becomes particularly evident in CPU-based machine learning workloads, where operations such as batch normalization, matrix multiplications, and element-wise arithmetic frequently process rows of data sequentially. Since modern CPUs employ cache prefetching mechanisms, a row-major layout allows the next required data values to be preloaded into cache ahead of execution, reducing memory latency and improving overall computational throughput.</p>
<p>However, row-major layout can introduce inefficiencies when performing operations that require accessing data across channels rather than across rows. Consider a convolutional layer that applies a filter across multiple channels of an input image. Since channel values are interleaved in row-major storage, the convolution operation must jump across memory locations to fetch all the necessary channel values for a given pixel. These strided memory accesses can be costly on hardware architectures that rely on vectorized execution and coalesced memory access, such as GPUs and TPUs.</p>
<p>Despite these limitations, row-major layout remains a dominant storage format in CPU-based machine learning frameworks. TensorFlow, for instance, defaults to the NHWC (row-major) format on CPUs, ensuring that cache locality is optimized for sequential processing. However, when targeting GPUs, frameworks often rearrange data dynamically to take advantage of more efficient memory layouts, such as channel-major storage, which aligns better with parallelized computation.</p>
</section><section id="sec-ai-acceleration-channelmajor-layout-d6a9" class="level5 page-columns page-full"><h5 class="anchored" data-anchor-id="sec-ai-acceleration-channelmajor-layout-d6a9">Channel-Major Layout</h5>
<p>In contrast to row-major layout, channel-major layout arranges data in memory such that all values for a given channel are stored together before moving to the next channel. This format is particularly beneficial for GPUs, TPUs, and other AI accelerators, where vectorized operations and memory coalescing significantly impact computational efficiency.</p>
<p>To understand how channel-major layout works, consider the same RGB image tensor of size (Height, Width, Channels) = (3, 3, 3). Instead of storing pixel values row by row, the data is structured channel-first in memory as follows: <span class="math display">\[\begin{gather*}
I(0,0,0), I(1,0,0), I(2,0,0), I(0,1,0), I(1,1,0), I(2,1,0), \ldots, \\
I(0,0,1), I(1,0,1), I(2,0,1), \ldots, I(0,0,2), I(1,0,2), I(2,0,2), \ldots
\end{gather*}\]</span></p>
<p>In this format, all red channel values for the entire image are stored first, followed by all green values, and then all blue values. This ordering allows hardware accelerators to efficiently load and process data across channels in parallel, which is crucial for convolution operations and SIMD (Single Instruction, Multiple Data) execution models <span class="citation" data-cites="chetlur2014cudnn">(<a href="#ref-chetlur2014cudnn" role="doc-biblioref">Chetlur et al. 2014</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-chetlur2014cudnn" class="csl-entry" role="listitem">
Chetlur, Sharan, Cliff Woolley, Philippe Vandermersch, Jonathan Cohen, John Tran, Bryan Catanzaro, and Evan Shelhamer. 2014. <span>â€œcuDNN: Efficient Primitives for Deep Learning.â€</span> <em>arXiv Preprint arXiv:1410.0759</em>, October. <a href="http://arxiv.org/abs/1410.0759v3">http://arxiv.org/abs/1410.0759v3</a>.
</div></div><p>The advantage of channel-major layout becomes clear when performing convolutions in machine learning models. Convolutional layers process images by applying a shared set of filters across all channels. When the data is stored in a channel-major format, a convolution kernel can load an entire channel efficiently, reducing the number of scattered memory fetches. This reduces memory latency, improves throughput, and enhances data locality for matrix multiplications, which are fundamental to machine learning workloads.</p>
<p>Because GPUs and TPUs rely on memory coalescing<a href="#fn27" class="footnote-ref" id="fnref27" role="doc-noteref"><sup>27</sup></a>, a technique in which consecutive threads fetch contiguous memory addresses, channel-major layout aligns naturally with the way these processors execute parallel computations. For example, in NVIDIA GPUs, each thread in a warp (a group of threads executed simultaneously) processes different elements of the same channel, ensuring that memory accesses are efficient and reducing the likelihood of strided memory accesses, which can degrade performance.</p>
<div class="no-row-height column-margin column-container"><div id="fn27"><p><sup>27</sup>&nbsp;<strong>Memory Coalescing</strong>: Hardware optimization where consecutive threads in a warp access consecutive memory addresses, enabling the memory controller to combine multiple requests into a single efficient transaction. Uncoalesced access (threads accessing scattered addresses) can reduce GPU memory bandwidth by 10-20<span class="math inline">\(\times\)</span>. This is why tensor layouts and data organization are crucial for GPU performanceâ€”poorly structured data causes expensive scattered memory access patterns.</p></div></div><p>Despite its advantages in machine learning accelerators, channel-major layout can introduce inefficiencies when running on general-purpose CPUs. Since CPUs optimize for sequential memory access, storing all values for a single channel before moving to the next disrupts cache locality for row-wise operations. This is why many machine learning frameworks (e.g., TensorFlow, PyTorch) default to row-major (NHWC) on CPUs and channel-major (NCHW) on GPUsâ€”optimizing for the strengths of each hardware type.</p>
<p>Modern AI frameworks and compilers often transform tensor layouts dynamically depending on the execution environment. For instance, TensorFlow and PyTorch automatically switch between NHWC<a href="#fn28" class="footnote-ref" id="fnref28" role="doc-noteref"><sup>28</sup></a> and NCHW based on whether a model is running on a CPU, GPU, or TPU, ensuring that the memory layout aligns with the most efficient execution path.</p>
<div class="no-row-height column-margin column-container"><div id="fn28"><p><sup>28</sup>&nbsp;<strong>NHWC vs NCHW</strong>: Tensor layout formats where letters indicate dimension order: N(batch), H(height), W(width), C(channels). NHWC stores data row-by-row with channels interleaved (CPU-friendly), while NCHW groups all values for each channel together (GPU-friendly). A 224Ã—224 RGB image in NHWC stores as [R1,G1,B1,R2,G2,B2,â€¦] while NCHW stores as [R1,R2,â€¦,G1,G2,â€¦,B1,B2,â€¦]. This seemingly minor difference can impact performance by 2-5<span class="math inline">\(\times\)</span> depending on hardware.</p></div></div></section><section id="sec-ai-acceleration-comparing-rowmajor-channelmajor-layouts-e410" class="level5"><h5 class="anchored" data-anchor-id="sec-ai-acceleration-comparing-rowmajor-channelmajor-layouts-e410">Comparing Row-Major and Channel-Major Layouts</h5>
<p>Both row-major (NHWC) and channel-major (NCHW) layouts serve distinct purposes in machine learning workloads, with their efficiency largely determined by the hardware architecture, memory access patterns, and computational requirements. The choice of layout directly influences cache utilization, memory bandwidth efficiency, and processing throughput. <a href="#tbl-major" class="quarto-xref">Table&nbsp;14</a> summarizes the differences between row-major (NHWC) and channel-major (NCHW) layouts in terms of performance trade-offs and hardware compatibility.</p>
<div id="tbl-major" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure"><figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-major-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;14: <strong>Data Layout Strategies</strong>: Row-major (NHWC) and channel-major (NCHW) layouts optimize memory access patterns for different hardware architectures; NHWC suits cpus and element-wise operations, while NCHW accelerates GPU and TPU-based convolution operations. Choosing the appropriate layout significantly impacts performance by maximizing cache utilization and memory bandwidth efficiency.
</figcaption><div aria-describedby="tbl-major-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<colgroup>
<col style="width: 20%">
<col style="width: 38%">
<col style="width: 40%">
</colgroup>
<thead><tr class="header">
<th style="text-align: left;"><strong>Feature</strong></th>
<th style="text-align: left;"><strong>Row-Major (NHWC)</strong></th>
<th style="text-align: left;"><strong>Channel-Major (NCHW)</strong></th>
</tr></thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><strong>Memory Storage Order</strong></td>
<td style="text-align: left;">Pixels are stored row-by-row, channel interleaved</td>
<td style="text-align: left;">All values for a given channel are stored together first</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Best for</strong></td>
<td style="text-align: left;">CPUs, element-wise operations</td>
<td style="text-align: left;">GPUs, TPUs, convolution operations</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>Cache Efficiency</strong></td>
<td style="text-align: left;">High cache locality for sequential row access</td>
<td style="text-align: left;">Optimized for memory coalescing across channels</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Convolution Performance</strong></td>
<td style="text-align: left;">Requires strided memory accesses (inefficient on GPUs)</td>
<td style="text-align: left;">Efficient for GPU convolution kernels</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>Memory Fetching</strong></td>
<td style="text-align: left;">Good for operations that process rows sequentially</td>
<td style="text-align: left;">Optimized for SIMD execution across channels</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Default in Frameworks</strong></td>
<td style="text-align: left;">Default on CPUs (e.g., TensorFlow NHWC)</td>
<td style="text-align: left;">Default on GPUs (e.g., cuDNN prefers NCHW)</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p>The decision to use row-major (NHWC) or channel-major (NCHW) layouts is not always made manually by developers. Instead, machine learning frameworks and AI compilers often determine the optimal layout dynamically based on the target hardware and operation type. CPUs tend to favor NHWC due to cache-friendly sequential memory access, while GPUs perform better with NCHW, which reduces memory fetch overhead for machine learning computations.</p>
<p>In practice, modern AI compilers such as TensorFlowâ€™s XLA and PyTorchâ€™s TorchScript perform automatic layout transformations, converting tensors between NHWC and NCHW as needed to optimize performance across different processing units. This ensures that machine learning models achieve the highest possible throughput without requiring developers to manually specify tensor layouts.</p>
</section></section><section id="sec-ai-acceleration-kernel-fusion-7faf" class="level4 page-columns page-full"><h4 class="anchored" data-anchor-id="sec-ai-acceleration-kernel-fusion-7faf">Kernel Fusion</h4>
<p>One of the most impactful optimization techniques in AI acceleration involves reducing the overhead of intermediate data movement between operations. This section examines how kernel fusion transforms multiple separate computations into unified operations, dramatically improving memory efficiency and execution performance. We first analyze the memory bottlenecks created by intermediate writes, then explore how fusion techniques eliminate these inefficiencies.</p>
<section id="sec-ai-acceleration-intermediate-memory-write-f140" class="level5 page-columns page-full"><h5 class="anchored" data-anchor-id="sec-ai-acceleration-intermediate-memory-write-f140">Intermediate Memory Write</h5>
<p>Optimizing memory access is a fundamental challenge in AI acceleration. While AI models rely on high-throughput computation, their performance is often constrained by memory bandwidth and intermediate memory writes rather than pure arithmetic operations. Every time an operation produces an intermediate result that must be written to memory and later read back, execution stalls occur due to data movement overhead.</p>
<p>Building on software optimization techniques from <strong><a href="../optimizations/optimizations.html#sec-model-optimizations">Chapter 10: Model Optimizations</a></strong> and memory bandwidth constraints established in <a href="#sec-ai-acceleration-understanding-ai-memory-wall-3ea9" class="quarto-xref">Section&nbsp;1.4.1</a>, kernel fusion represents the critical bridge between software optimization and hardware acceleration. Many AI workloads introduce unnecessary intermediate memory writes, leading to increased memory bandwidth consumption and reduced execution efficiency <span class="citation" data-cites="nvidia2017gpu">(<a href="#ref-nvidia2017gpu" role="doc-biblioref">Ye et al. 2025</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-nvidia2017gpu" class="csl-entry" role="listitem">
Ye, Zirui, Bei Yao, Haoran Zheng, Li Tao, Ripeng Wang, Yankui Chang, Zhi Chen, Yingming Zhao, Wei Wei, and Xie George Xu. 2025. <span>â€œUncertainty Quantification for CT Dosimetry Based on 10&nbsp;281 Subjects Using Automatic Image Segmentation and Fast Monte Carlo Calculations.â€</span> <em>Medical Physics</em> 52 (6): 4910â€“23. <a href="https://doi.org/10.1002/mp.17796">https://doi.org/10.1002/mp.17796</a>.
</div></div><p><a href="#lst-naive_execution" class="quarto-xref">Listing&nbsp;22</a> illustrates a naÃ¯ve execution model in which each operation is treated as a separate kernel, meaning that each intermediate result is written to memory and then read back for the next operation.</p>
<div id="lst-naive_execution" class="listing quarto-float quarto-figure quarto-figure-left anchored">
<figure class="quarto-float quarto-float-lst figure"><figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-lst" id="lst-naive_execution-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Listing&nbsp;22: <strong>NaÃ¯ve Execution</strong>: Each step writes intermediate results to memory before processing the next, leading to increased bandwidth usage and reduced efficiency. <em>Source: NVIDIA GPU Technology Conference 2017</em>[nvidia2017gpu]
</figcaption><div aria-describedby="lst-naive_execution-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="sourceCode" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a><span class="co">## Input tensor</span></span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> torch.randn(<span class="dv">1024</span>, <span class="dv">1024</span>).cuda()</span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a><span class="co">## Step-by-step execution (naÃ¯ve approach)</span></span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a>X1 <span class="op">=</span> torch.relu(X)  <span class="co"># Intermediate tensor stored</span></span>
<span id="cb22-8"><a href="#cb22-8" aria-hidden="true" tabindex="-1"></a><span class="co"># in memory</span></span>
<span id="cb22-9"><a href="#cb22-9" aria-hidden="true" tabindex="-1"></a>X2 <span class="op">=</span> torch.batch_norm(X1)  <span class="co"># Another intermediate tensor stored</span></span>
<span id="cb22-10"><a href="#cb22-10" aria-hidden="true" tabindex="-1"></a>Y <span class="op">=</span> <span class="fl">2.0</span> <span class="op">*</span> X2 <span class="op">+</span> <span class="fl">1.0</span>  <span class="co"># Final result</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</figure>
</div>
<p>Each operation produces an intermediate tensor that must be written to memory and retrieved for the next operation. On large tensors, this overhead of moving data can outweigh the computational cost of the operations <span class="citation" data-cites="shazeer2018mesh">(<a href="#ref-shazeer2018mesh" role="doc-biblioref">Shazeer et al. 2018</a>)</span>. <a href="#tbl-memory-footprint" class="quarto-xref">Table&nbsp;15</a> illustrates the memory overhead in a naÃ¯ve execution model. While only the final result <span class="math inline">\(Y\)</span> is needed, storing multiple intermediate tensors creates unnecessary memory traffic and inefficient memory usage. This data movement bottleneck significantly impacts performance, making memory optimization crucial for AI accelerators.</p>
<div class="no-row-height column-margin column-container"><div id="ref-shazeer2018mesh" class="csl-entry" role="listitem">
Shazeer, Noam, Youlong Cheng, Niki Parmar, Dustin Tran, Ashish Vaswani, Penporn Koanantakool, Peter Hawkins, et al. 2018. <span>â€œMesh-TensorFlow: Deep Learning for Supercomputers.â€</span> <em>arXiv Preprint arXiv:1811.02084</em>, November. <a href="http://arxiv.org/abs/1811.02084v1">http://arxiv.org/abs/1811.02084v1</a>.
</div></div><div id="tbl-memory-footprint" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure"><figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-memory-footprint-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;15: <strong>Intermediate Tensor Storage</strong>: NaÃ¯ve execution models require substantial memory to store intermediate tensors generated by each operation; for a 1024x1024 tensor, this table shows that storing these intermediate resultsâ€”even if only the final output is neededâ€”quadruples the total memory footprint from 4 MB to 16 MB. Minimizing this intermediate data storage is crucial for improving memory efficiency and accelerating AI computations.
</figcaption><div aria-describedby="tbl-memory-footprint-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<colgroup>
<col style="width: 26%">
<col style="width: 63%">
</colgroup>
<thead><tr class="header">
<th style="text-align: left;"><strong>Tensor</strong></th>
<th style="text-align: right;"><strong>Size (MB) for 1024 <span class="math inline">\(\times\)</span> 1024 Tensor</strong></th>
</tr></thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><strong>X</strong></td>
<td style="text-align: right;">4 MB</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Xâ€™</strong></td>
<td style="text-align: right;">4 MB</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>Xâ€™â€™</strong></td>
<td style="text-align: right;">4 MB</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Y</strong></td>
<td style="text-align: right;">4 MB</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>Total Memory</strong></td>
<td style="text-align: right;">16 MB</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p>Even though only the final result <span class="math inline">\(Y\)</span> is needed, three additional intermediate tensors consume extra memory without contributing to final output storage. This excessive memory usage limits scalability and wastes memory bandwidth, particularly in AI accelerators where minimizing data movement is critical.</p>
</section><section id="sec-ai-acceleration-kernel-fusion-memory-efficiency-f227" class="level5 page-columns page-full"><h5 class="anchored" data-anchor-id="sec-ai-acceleration-kernel-fusion-memory-efficiency-f227">Kernel Fusion for Memory Efficiency</h5>
<p>Kernel fusion is a key optimization technique that aims to minimize intermediate memory writes, reducing the memory footprint and bandwidth consumption of machine learning workloads <span class="citation" data-cites="jia2018beyond">(<a href="#ref-jia2018beyond" role="doc-biblioref">Zhihao Jia, Zaharia, and Aiken 2018</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-jia2018beyond" class="csl-entry" role="listitem">
Jia, Zhihao, Matei Zaharia, and Alex Aiken. 2018. <span>â€œBeyond Data and Model Parallelism for Deep Neural Networks.â€</span> <em>arXiv Preprint arXiv:1807.05358</em>, July. <a href="http://arxiv.org/abs/1807.05358v1">http://arxiv.org/abs/1807.05358v1</a>.
</div></div><p>Kernel fusion involves merging multiple computation steps into a single, optimized operation, eliminating the need for storing and reloading intermediate tensors. Instead of executing each layer or element-wise operation separately, in which each step writes its output to memory before the next step begins, fusion enables direct data propagation between operations, keeping computations within high-speed registers or local memory.</p>
<p>A common machine learning sequence might involve applying a nonlinear activation function (e.g., ReLU), followed by batch normalization, and then scaling the values for input to the next layer. In a naÃ¯ve implementation, each of these steps generates an intermediate tensor, which is written to memory, read back, and then modified again: <span class="math display">\[
X' = \text{ReLU}(X)
X'' = \text{BatchNorm}(X')
Y = \alpha \cdot X'' + \beta
\]</span></p>
<p>With kernel fusion, these operations are combined into a single computation step, allowing the entire transformation to occur without generating unnecessary intermediate tensors: <span class="math display">\[
Y = \alpha \cdot \text{BatchNorm}\big(\text{ReLU}(X)\big) + \beta
\]</span></p>
<p><a href="#tbl-fusion-benefits" class="quarto-xref">Table&nbsp;16</a> highlights the impact of operation fusion on memory efficiency. By keeping intermediate results in registers or local memory rather than writing them to main memory, fusion significantly reduces memory traffic. This optimization is especially beneficial on highly parallel architectures like GPUs and TPUs, where minimizing memory accesses translates directly into improved execution throughput. Compared to the naÃ¯ve execution model, fused execution eliminates the need for storing intermediate tensors, dramatically lowering the total memory footprint and improving overall efficiency.</p>
<div id="tbl-fusion-benefits" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure"><figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-fusion-benefits-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;16: <strong>Operation Fusion Benefits</strong>: Fused execution reduces memory usage by eliminating the need to store intermediate tensors, directly improving efficiency on memory-bound hardware like gpus and tpus. This table quantifies the memory savings, showing a reduction from 16 MB in naÃ¯ve execution to 4 MB with fused operations.
</figcaption><div aria-describedby="tbl-fusion-benefits-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<colgroup>
<col style="width: 25%">
<col style="width: 39%">
<col style="width: 34%">
</colgroup>
<thead><tr class="header">
<th style="text-align: left;"><strong>Execution Model</strong></th>
<th style="text-align: left;"><strong>Intermediate Tensors Stored</strong></th>
<th style="text-align: right;"><strong>Total Memory Usage (MB)</strong></th>
</tr></thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><strong>NaÃ¯ve Execution</strong></td>
<td style="text-align: left;">Xâ€™, Xâ€™â€™</td>
<td style="text-align: right;">16 MB</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Fused Execution</strong></td>
<td style="text-align: left;">None</td>
<td style="text-align: right;">4 MB</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
</section><section id="sec-ai-acceleration-performance-benefits-constraints-1b74" class="level5 page-columns page-full"><h5 class="anchored" data-anchor-id="sec-ai-acceleration-performance-benefits-constraints-1b74">Performance Benefits and Constraints</h5>
<p>Kernel fusion brings several key advantages that enhance memory efficiency and computation throughput. By reducing memory accesses, fused kernels ensure that intermediate values stay within registers instead of being repeatedly written to and read from memory. This significantly lowers memory traffic, which is one of the primary bottlenecks in machine learning workloads. GPUs and TPUs, in particular, benefit from kernel fusion because high-bandwidth memory is a scarce resource, and reducing memory transactions leads to better utilization of compute units <span class="citation" data-cites="nvidia2020ampere">(<a href="#ref-nvidia2020ampere" role="doc-biblioref">Qi, Kantarci, and Liu 2017</a>)</span>.</p>
<div class="no-row-height column-margin column-container"></div><p>However, not all operations can be fused. Element-wise operations, such as ReLU, batch normalization, and simple arithmetic transformations, are ideal candidates for fusion since their computations depend only on single elements from the input tensor. In contrast, operations with complex data dependencies, such as matrix multiplications and convolutions, involve global data movement, making direct fusion impractical. These operations require values from multiple input elements to compute a single output, which prevents them from being executed as a single fused kernel.</p>
<p>Another major consideration is register pressure. Fusing multiple operations means all temporary values must be kept in registers rather than memory. While this eliminates redundant memory writes, it also increases register demand. If a fused kernel exceeds the available registers per thread, the system must spill excess values into shared memory, introducing additional latency and potentially negating the benefits of fusion. On GPUs, where thread occupancy (the number of threads that can run in parallel) is limited by available registers, excessive fusion can reduce parallelism, leading to diminishing returns.</p>
<p>Different AI accelerators and compilers handle fusion in distinct ways. NVIDIA GPUs, for example, favor warp-level parallelism, where element-wise fusion is straightforward. TPUs, on the other hand, prioritize systolic array execution, which is optimized for matrix-matrix operations rather than element-wise fusion <span class="citation" data-cites="nvidia2020ampere">(<a href="#ref-nvidia2020ampere" role="doc-biblioref">Qi, Kantarci, and Liu 2017</a>)</span>. AI compilers such as XLA (TensorFlow), TorchScript (PyTorch), TensorRT (NVIDIA), and MLIR automatically detect fusion opportunities and apply heuristics to balance memory savings and execution efficiency <span class="citation" data-cites="xla2021">(<a href="#ref-xla2021" role="doc-biblioref">He 2023b</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-nvidia2020ampere" class="csl-entry" role="listitem">
Qi, Xuan, Burak Kantarci, and Chen Liu. 2017. <span>â€œGPU-Based Acceleration of SDN Controllers.â€</span> In <em>Network as a Service for Next Generation Internet</em>, 339â€“56. Institution of Engineering; Technology. <a href="https://doi.org/10.1049/pbte073e%5C_ch14">https://doi.org/10.1049/pbte073e\_ch14</a>.
</div><div id="ref-xla2021" class="csl-entry" role="listitem">
â€”â€”â€”. 2023b. <span>â€œAccelerated Linear Algebra Compiler for Computationally Efficient Numerical Models: Success and Potential Area of Improvement.â€</span> <em>PLOS ONE</em> 18 (2): e0282265. <a href="https://doi.org/10.1371/journal.pone.0282265">https://doi.org/10.1371/journal.pone.0282265</a>.
</div></div><p>Despite its advantages, fusion is not always beneficial. Some AI frameworks allow developers to disable fusion selectively, especially when debugging performance issues or making frequent model modifications. The decision to fuse operations must consider trade-offs between memory efficiency, register usage, and hardware execution constraints to ensure that fusion leads to tangible performance improvements.</p>
</section></section><section id="sec-ai-acceleration-memoryefficient-tiling-strategies-9fce" class="level4 page-columns page-full"><h4 class="anchored" data-anchor-id="sec-ai-acceleration-memoryefficient-tiling-strategies-9fce">Memory-Efficient Tiling Strategies</h4>
<p>While modern AI accelerators offer high computational throughput, their performance is often limited by memory bandwidth rather than raw processing power. If data cannot be supplied to processing units fast enough, execution stalls occur, leading to wasted cycles and inefficient hardware utilization.</p>
<p>Tiling is a technique used to mitigate this issue by restructuring computations into smaller, memory-friendly subproblems. Instead of processing entire matrices or tensors at once, which leads to excessive memory traffic, tiling partitions computations into smaller blocks (tiles) that fit within fast local memory (e.g., caches, shared memory, or registers) <span class="citation" data-cites="lam1991cache">(<a href="#ref-lam1991cache" role="doc-biblioref">Lam, Rothberg, and Wolf 1991</a>)</span>. By doing so, tiling increases data reuse, minimizes memory fetches, and improves overall computational efficiency.</p>
<div class="no-row-height column-margin column-container"><div id="ref-lam1991cache" class="csl-entry" role="listitem">
Lam, Monica D., Edward E. Rothberg, and Michael E. Wolf. 1991. <span>â€œThe Cache Performance and Optimizations of Blocked Algorithms.â€</span> In <em>Proceedings of the Fourth International Conference on Architectural Support for Programming Languages and Operating Systems - ASPLOS-IV</em>, 63â€“74. ACM Press. <a href="https://doi.org/10.1145/106972.106981">https://doi.org/10.1145/106972.106981</a>.
</div></div><p>A classic example of inefficient memory access is matrix multiplication, which is widely used in AI models. Without tiling, the naÃ¯ve approach results in repeated memory accesses for the same data, leading to unnecessary bandwidth consumption (<a href="#lst-naive_matmul" class="quarto-xref">Listing&nbsp;23</a>).</p>
<div id="lst-naive_matmul" class="listing quarto-float quarto-figure quarto-figure-left anchored">
<figure class="quarto-float quarto-float-lst figure"><figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-lst" id="lst-naive_matmul-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Listing&nbsp;23: NaÃ¯ve matrix multiplication without tiling
</figcaption><div aria-describedby="lst-naive_matmul-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="sourceCode" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(N):</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(N):</span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> k <span class="kw">in</span> <span class="bu">range</span>(N):</span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a>            C[i, j] <span class="op">+=</span> A[i, k] <span class="op">*</span> B[k, j]  <span class="co"># Repeatedly fetching</span></span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a>            <span class="co"># A[i, k] and B[k, j]</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</figure>
</div>
<p>Each iteration requires loading elements from matrices <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> multiple times from memory, causing excessive data movement. As the size of the matrices increases, the memory bottleneck worsens, limiting performance.</p>
<p>Tiling addresses this problem by ensuring that smaller portions of matrices are loaded into fast memory, reused efficiently, and only written back to main memory when necessary. This technique is especially crucial in AI accelerators, where memory accesses dominate execution time. By breaking up large matrices into smaller tiles, as illustrated in <a href="#fig-tiling-diagram" class="quarto-xref">Figure&nbsp;8</a>, computation can be performed more efficiently on hardware by maximizing data reuse in fast memory. In the following sections, the fundamental principles emerge of tiling, its different strategies, and the key trade-offs involved in selecting an effective tiling approach.</p>
<div id="fig-tiling-diagram" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-pos="htb" data-fig-env="figure">
<figure class="quarto-float quarto-float-fig figure"><div aria-describedby="fig-tiling-diagram-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="074576e90626519d8d3a924f58fb346edced7416.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-8" title="Figure&nbsp;8: Matrix Tiling: Partitioning large matrices into smaller tiles optimizes data reuse and reduces memory access overhead during computation. This technique improves performance on AI accelerators by enabling efficient loading and processing of data in fast memory, minimizing transfers from slower main memory."><img src="hw_acceleration_files/mediabag/074576e90626519d8d3a924f58fb346edced7416.svg" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-tiling-diagram-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;8: <strong>Matrix Tiling</strong>: Partitioning large matrices into smaller tiles optimizes data reuse and reduces memory access overhead during computation. This technique improves performance on AI accelerators by enabling efficient loading and processing of data in fast memory, minimizing transfers from slower main memory.
</figcaption></figure>
</div>
<section id="sec-ai-acceleration-tiling-fundamentals-e9e6" class="level5"><h5 class="anchored" data-anchor-id="sec-ai-acceleration-tiling-fundamentals-e9e6">Tiling Fundamentals</h5>
<p>Tiling is based on a simple but powerful principle: instead of operating on an entire data structure at once, computations are divided into smaller tiles that fit within the available fast memory. By structuring execution around these tiles, data reuse is maximized, reducing redundant memory accesses and improving overall efficiency.</p>
<p>Consider matrix multiplication, a key operation in machine learning workloads. The operation computes the output matrix <span class="math inline">\(C\)</span> from two input matrices <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span>: <span class="math display">\[
C = A \times B
\]</span> where each element <span class="math inline">\(C[i,j]\)</span> is computed as: <span class="math display">\[
C[i,j] = \sum_{k} A[i,k] \times B[k,j]
\]</span></p>
<p>A naÃ¯ve implementation follows this formula directly (<a href="#lst-naive_matmul_repeat" class="quarto-xref">Listing&nbsp;24</a>).</p>
<div id="lst-naive_matmul_repeat" class="listing quarto-float quarto-figure quarto-figure-left anchored">
<figure class="quarto-float quarto-float-lst figure"><figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-lst" id="lst-naive_matmul_repeat-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Listing&nbsp;24: <strong>NaÃ¯ve Matrix Multiplication</strong>: This code directly implements matrix multiplication using nested loops, showing how each element in the output matrix is computed as a sum of products from corresponding elements in the input matrices.
</figcaption><div aria-describedby="lst-naive_matmul_repeat-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="sourceCode" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(N):</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(N):</span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> k <span class="kw">in</span> <span class="bu">range</span>(N):</span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a>            C[i, j] <span class="op">+=</span> A[i, k] <span class="op">*</span> B[k, j]  <span class="co"># Repeatedly fetching</span></span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a>            <span class="co"># A[i, k] and B[k, j]</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</figure>
</div>
<p>At first glance, this approach seems correctâ€”it computes the desired result and follows the mathematical definition. However, the issue lies in how memory is accessed. Every time the innermost loop runs, it fetches an element from matrix <span class="math inline">\(A\)</span> and matrix <span class="math inline">\(B\)</span> from memory, performs a multiplication, and updates an element in matrix <span class="math inline">\(C\)</span>. Because matrices are large, the processor frequently reloads the same values from memory, even though they were just used in previous computations.</p>
<p>This unnecessary data movement is expensive. Fetching values from main memory (DRAM) is hundreds of times slower than accessing values stored in on-chip cache or registers. If the same values must be reloaded multiple times instead of being stored in fast memory, execution slows down significantly.</p>
</section><section id="sec-ai-acceleration-performance-benefits-tiling-e7bd" class="level5"><h5 class="anchored" data-anchor-id="sec-ai-acceleration-performance-benefits-tiling-e7bd">Performance Benefits of Tiling</h5>
<p>Instead of computing one element at a time and constantly moving data in and out of slow memory, tiling processes submatrices (tiles) at a time, keeping frequently used values in fast memory. The idea is to divide the matrices into smaller blocks that fit within the processorâ€™s cache or shared memory, ensuring that once a block is loaded, it is reused multiple times before moving to the next one.</p>
<p><a href="#lst-tiled_matmul" class="quarto-xref">Listing&nbsp;25</a> illustrates a tiled version of matrix multiplication, which improves memory locality by processing blocks of data.</p>
<div id="lst-tiled_matmul" class="listing quarto-float quarto-figure quarto-figure-left anchored">
<figure class="quarto-float quarto-float-lst figure"><figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-lst" id="lst-tiled_matmul-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Listing&nbsp;25: <strong>Tiled Matrix Multiplication</strong>: This approach divides matrices into smaller blocks to optimize memory usage by reusing data within processor cache, thereby improving computational efficiency.
</figcaption><div aria-describedby="lst-tiled_matmul-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="sourceCode" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a>TILE_SIZE <span class="op">=</span> <span class="dv">32</span>  <span class="co"># Choose a tile size based on</span></span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a><span class="co"># hardware constraints</span></span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, N, TILE_SIZE):</span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, N, TILE_SIZE):</span>
<span id="cb25-6"><a href="#cb25-6" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> k <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, N, TILE_SIZE):</span>
<span id="cb25-7"><a href="#cb25-7" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Compute the submatrix</span></span>
<span id="cb25-8"><a href="#cb25-8" aria-hidden="true" tabindex="-1"></a>            <span class="co"># C[i:i+TILE_SIZE, j:j+TILE_SIZE]</span></span>
<span id="cb25-9"><a href="#cb25-9" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> ii <span class="kw">in</span> <span class="bu">range</span>(i, i <span class="op">+</span> TILE_SIZE):</span>
<span id="cb25-10"><a href="#cb25-10" aria-hidden="true" tabindex="-1"></a>                <span class="cf">for</span> jj <span class="kw">in</span> <span class="bu">range</span>(j, j <span class="op">+</span> TILE_SIZE):</span>
<span id="cb25-11"><a href="#cb25-11" aria-hidden="true" tabindex="-1"></a>                    <span class="cf">for</span> kk <span class="kw">in</span> <span class="bu">range</span>(k, k <span class="op">+</span> TILE_SIZE):</span>
<span id="cb25-12"><a href="#cb25-12" aria-hidden="true" tabindex="-1"></a>                        C[ii, jj] <span class="op">+=</span> A[ii, kk] <span class="op">*</span> B[kk, jj]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</figure>
</div>
<p>This restructuring significantly improves performance for three main reasons:</p>
<ol type="1">
<li><p><strong>Better Memory Reuse</strong>: Instead of fetching elements from <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> repeatedly from slow memory, this approach loads a small tile of data into fast memory, performs multiple computations using it, and only then moves on to the next tile. This minimizes redundant memory accesses.</p></li>
<li><p><strong>Reduced Memory Bandwidth Usage</strong>: Since each tile is used multiple times before being evicted, memory traffic is reduced. Instead of repeatedly accessing DRAM, most required data is available in L1/L2 cache or shared memory, leading to faster execution.</p></li>
<li><p><strong>Increased Compute Efficiency</strong>: Processors spend less time waiting for data and more time performing useful computations. In architectures like GPUs and TPUs, where thousands of parallel processing units operate simultaneously, tiling ensures that data is read and processed in a structured manner, avoiding unnecessary stalls.</p></li>
</ol>
<p>This technique is particularly effective in AI accelerators, where machine learning workloads consist of large matrix multiplications and tensor transformations. Without tiling, these workloads quickly become memory-bound, meaning performance is constrained by how fast data can be retrieved rather than by the raw computational power of the processor.</p>
</section><section id="sec-ai-acceleration-tiling-methods-6257" class="level5"><h5 class="anchored" data-anchor-id="sec-ai-acceleration-tiling-methods-6257">Tiling Methods</h5>
<p>While the general principle of tiling remains the same, which involves partitioning large computations into smaller subproblems to improve memory reuse, there are different ways to apply tiling based on the structure of the computation and hardware constraints. The two primary tiling strategies are spatial tiling and temporal tiling. These strategies optimize different aspects of computation and memory access, and in practice, they are often combined to achieve the best performance.</p>
<section id="sec-ai-acceleration-spatial-tiling-247e" class="level6"><h6 class="anchored" data-anchor-id="sec-ai-acceleration-spatial-tiling-247e">Spatial Tiling</h6>
<p>Spatial tiling focuses on partitioning data structures into smaller blocks that fit within the fast memory of the processor. This approach ensures that each tile is fully processed before moving to the next, reducing redundant memory accesses. Spatial tiling is widely used in operations such as matrix multiplication, convolutions, and attention mechanisms in transformer models.</p>
<p>Spatial tiling is illustrated in <a href="#lst-tiled_spatial" class="quarto-xref">Listing&nbsp;26</a>, where the computation proceeds over blocks of the input matrices.</p>
<div id="lst-tiled_spatial" class="listing quarto-float quarto-figure quarto-figure-left anchored">
<figure class="quarto-float quarto-float-lst figure"><figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-lst" id="lst-tiled_spatial-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Listing&nbsp;26: <strong>Spatial Tiling</strong>: Reduces redundant memory accesses by processing matrix tiles sequentially.
</figcaption><div aria-describedby="lst-tiled_spatial-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="sourceCode" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a>TILE_SIZE <span class="op">=</span> <span class="dv">32</span>  <span class="co"># Tile size chosen based on available</span></span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a><span class="co"># fast memory</span></span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, N, TILE_SIZE):</span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, N, TILE_SIZE):</span>
<span id="cb26-6"><a href="#cb26-6" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> k <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, N, TILE_SIZE):</span>
<span id="cb26-7"><a href="#cb26-7" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Process a submatrix (tile) at a time</span></span>
<span id="cb26-8"><a href="#cb26-8" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> ii <span class="kw">in</span> <span class="bu">range</span>(i, i <span class="op">+</span> TILE_SIZE):</span>
<span id="cb26-9"><a href="#cb26-9" aria-hidden="true" tabindex="-1"></a>                <span class="cf">for</span> jj <span class="kw">in</span> <span class="bu">range</span>(j, j <span class="op">+</span> TILE_SIZE):</span>
<span id="cb26-10"><a href="#cb26-10" aria-hidden="true" tabindex="-1"></a>                    <span class="cf">for</span> kk <span class="kw">in</span> <span class="bu">range</span>(k, k <span class="op">+</span> TILE_SIZE):</span>
<span id="cb26-11"><a href="#cb26-11" aria-hidden="true" tabindex="-1"></a>                        C[ii, jj] <span class="op">+=</span> A[ii, kk] <span class="op">*</span> B[kk, jj]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</figure>
</div>
<p>In this implementation, each tile of <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> is loaded into cache or shared memory before processing, ensuring that the same data does not need to be fetched repeatedly from slower memory. The tile is fully used before moving to the next block, minimizing redundant memory accesses. Since data is accessed in a structured, localized way, cache efficiency improves significantly.</p>
<p>Spatial tiling is particularly beneficial when dealing with large tensors that do not fit entirely in fast memory. By breaking them into smaller tiles, computations remain localized, avoiding excessive data movement between memory levels. This technique is widely used in AI accelerators where machine learning workloads involve large-scale tensor operations that require careful memory management to achieve high performance.</p>
</section><section id="sec-ai-acceleration-temporal-tiling-563b" class="level6"><h6 class="anchored" data-anchor-id="sec-ai-acceleration-temporal-tiling-563b">Temporal Tiling</h6>
<p>While spatial tiling optimizes how data is partitioned, temporal tiling focuses on reorganizing the computation itself to improve data reuse over time. Many machine learning workloads involve operations where the same data is accessed repeatedly across multiple iterations. Without temporal tiling, this often results in redundant memory fetches, leading to inefficiencies. Temporal tiling, also known as loop blocking, restructures the computation to ensure that frequently used data stays in fast memory for as long as possible before moving on to the next computation.</p>
<p>A classic example where temporal tiling is beneficial is convolutional operations, where the same set of weights is applied to multiple input regions. Without loop blocking, these weights might be loaded from memory multiple times for each computation. With temporal tiling, the computation is reordered so that the weights remain in fast memory across multiple inputs, reducing unnecessary memory fetches and improving overall efficiency.</p>
<p><a href="#lst-loop_blocking" class="quarto-xref">Listing&nbsp;27</a> illustrates a simplified example of loop blocking in matrix multiplication.</p>
<div id="lst-loop_blocking" class="listing quarto-float quarto-figure quarto-figure-left anchored">
<figure class="quarto-float quarto-float-lst figure"><figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-lst" id="lst-loop_blocking-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Listing&nbsp;27: <strong>Temporal Tiling</strong>: Reduces redundant memory accesses by caching weights in fast memory across multiple matrix multiplications.
</figcaption><div aria-describedby="lst-loop_blocking-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="sourceCode" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, N, TILE_SIZE):</span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, N, TILE_SIZE):</span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> k <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, N, TILE_SIZE):</span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Load tile into fast memory before computation</span></span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a>            A_tile <span class="op">=</span> A[i:i<span class="op">+</span>TILE_SIZE, k:k<span class="op">+</span>TILE_SIZE]</span>
<span id="cb27-6"><a href="#cb27-6" aria-hidden="true" tabindex="-1"></a>            B_tile <span class="op">=</span> B[k:k<span class="op">+</span>TILE_SIZE, j:j<span class="op">+</span>TILE_SIZE]</span>
<span id="cb27-7"><a href="#cb27-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-8"><a href="#cb27-8" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> ii <span class="kw">in</span> <span class="bu">range</span>(TILE_SIZE):</span>
<span id="cb27-9"><a href="#cb27-9" aria-hidden="true" tabindex="-1"></a>                <span class="cf">for</span> jj <span class="kw">in</span> <span class="bu">range</span>(TILE_SIZE):</span>
<span id="cb27-10"><a href="#cb27-10" aria-hidden="true" tabindex="-1"></a>                    <span class="cf">for</span> kk <span class="kw">in</span> <span class="bu">range</span>(TILE_SIZE):</span>
<span id="cb27-11"><a href="#cb27-11" aria-hidden="true" tabindex="-1"></a>                        C[i<span class="op">+</span>ii, j<span class="op">+</span>jj] <span class="op">+=</span> A_tile[ii, kk] <span class="op">*</span></span>
<span id="cb27-12"><a href="#cb27-12" aria-hidden="true" tabindex="-1"></a>                                         B_tile[kk, jj]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</figure>
</div>
<p>Temporal tiling improves performance by ensuring that the data loaded into fast memory is used multiple times before being evicted. In this implementation, small tiles of matrices <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are explicitly loaded into temporary storage before performing computations, reducing memory fetch overhead. This restructuring allows the computation to process an entire tile before moving to the next, thereby reducing the number of times data must be loaded from slower memory.</p>
<p>This technique is particularly useful in workloads where certain values are used repeatedly, such as convolutions, recurrent neural networks (RNNs), and self-attention mechanisms in transformers. By applying loop blocking, AI accelerators can significantly reduce memory stalls and improve execution throughput.</p>
</section></section><section id="sec-ai-acceleration-tiling-challenges-tradeoffs-e9c9" class="level5"><h5 class="anchored" data-anchor-id="sec-ai-acceleration-tiling-challenges-tradeoffs-e9c9">Tiling Challenges and Trade-offs</h5>
<p>While tiling significantly improves performance by optimizing memory reuse and reducing redundant memory accesses, it introduces several challenges and trade-offs. Selecting the right tile size is a critical decision, as it directly affects computational efficiency and memory bandwidth usage. If the tile size is too small, the benefits of tiling diminish, as memory fetches still dominate execution time. On the other hand, if the tile size is too large, it may exceed the available fast memory, causing cache thrashing and performance degradation.</p>
<p>Load balancing is another key concern. In architectures such as GPUs and TPUs, computations are executed in parallel across thousands of processing units. If tiles are not evenly distributed, some units may remain idle while others are overloaded, leading to suboptimal utilization of computational resources. Effective tile scheduling ensures that parallel execution remains balanced and efficient.</p>
<p>Data movement overhead is also an important consideration. Although tiling reduces the number of slow memory accesses, transferring tiles between different levels of memory still incurs a cost. This is especially relevant in hierarchical memory systems, where accessing data from cache is much faster than accessing it from DRAM. Efficient memory prefetching and scheduling strategies are required to minimize latency and ensure that data is available when needed.</p>
<p>Beyond spatial and temporal tiling, hybrid approaches combine elements of both strategies to achieve optimal performance. Hybrid tiling adapts to workload-specific constraints by dynamically adjusting tile sizes or reordering computations based on real-time execution conditions. For example, some AI accelerators use spatial tiling for matrix multiplications while employing temporal tiling for weight reuse in convolutional layers.</p>
<p>Other methods exist for optimizing memory usage and computational efficiency beyond tiling. Techniques such as register blocking, double buffering, and hierarchical tiling extend the basic tiling principles to further optimize execution. AI compilers and runtime systems, such as TensorFlow XLA, TVM, and MLIR, automatically select tiling strategies based on hardware constraints, enabling fine-tuned performance optimization without manual intervention.</p>
<p><a href="#tbl-tiling-strategies" class="quarto-xref">Table&nbsp;17</a> provides a comparative overview of spatial, temporal, and hybrid tiling approaches, highlighting their respective benefits and trade-offs.</p>
<div id="tbl-tiling-strategies" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure"><figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-tiling-strategies-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;17: <strong>Tiling Strategies</strong>: Spatial, temporal, and hybrid tiling optimize memory access patterns for improved performance; spatial tiling maximizes data reuse within fast memory, temporal tiling exploits loop structure for reduced accesses, and hybrid tiling combines both approaches to balance computational efficiency and memory bandwidth. These techniques are crucial for AI compilers and runtime systems to automatically optimize model execution on diverse hardware.
</figcaption><div aria-describedby="tbl-tiling-strategies-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<colgroup>
<col style="width: 9%">
<col style="width: 36%">
<col style="width: 28%">
<col style="width: 25%">
</colgroup>
<thead><tr class="header">
<th style="text-align: left;"><strong>Aspect</strong></th>
<th style="text-align: left;"><strong>Spatial Tiling (Data Tiling)</strong></th>
<th style="text-align: left;"><strong>Temporal Tiling (Loop Blocking)</strong></th>
<th style="text-align: left;"><strong>Hybrid Tiling</strong></th>
</tr></thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><strong>Primary Goal</strong></td>
<td style="text-align: left;">Reduce memory accesses by keeping data in fast memory longer</td>
<td style="text-align: left;">Increase data reuse across loop iterations</td>
<td style="text-align: left;">Adapt dynamically to workload constraints</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Optimization Focus</strong></td>
<td style="text-align: left;">Partitioning data structures into smaller, memory-friendly blocks</td>
<td style="text-align: left;">Reordering computations to maximize reuse before eviction</td>
<td style="text-align: left;">Balancing spatial and temporal reuse strategies</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>Memory Usage</strong></td>
<td style="text-align: left;">Improves cache locality and reduces DRAM access</td>
<td style="text-align: left;">Keeps frequently used data in fast memory for multiple iterations</td>
<td style="text-align: left;">Minimizes data movement while ensuring high reuse</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Common Use Cases</strong></td>
<td style="text-align: left;">Matrix multiplications, CNNs, self-attention in transformers</td>
<td style="text-align: left;">Convolutions, recurrent neural networks (RNNs), iterative computations</td>
<td style="text-align: left;">AI accelerators with hierarchical memory, mixed workloads</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>Performance Gains</strong></td>
<td style="text-align: left;">Reduced memory bandwidth requirements, better cache utilization</td>
<td style="text-align: left;">Lower memory fetch latency, improved data locality</td>
<td style="text-align: left;">Maximized efficiency across multiple hardware types</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Challenges</strong></td>
<td style="text-align: left;">Requires careful tile size selection, inefficient for workloads with minimal spatial reuse</td>
<td style="text-align: left;">Can increase register pressure, requires loop restructuring</td>
<td style="text-align: left;">Complexity in tuning tile size and execution order dynamically</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>Best When</strong></td>
<td style="text-align: left;">Data is large and needs to be partitioned for efficient processing</td>
<td style="text-align: left;">The same data is accessed multiple times across iterations</td>
<td style="text-align: left;">Both data partitioning and iteration-based reuse are important</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p>As machine learning models continue to grow in size and complexity, tiling remains a critical tool for improving hardware efficiency, ensuring that AI accelerators operate at their full potential. While manual tiling strategies can provide substantial benefits, modern compilers and hardware-aware optimization techniques further enhance performance by automatically selecting the most effective tiling strategies for a given workload.</p>
</section></section></section><section id="sec-ai-acceleration-applying-mapping-strategies-neural-networks-3110" class="level3 page-columns page-full"><h3 class="anchored" data-anchor-id="sec-ai-acceleration-applying-mapping-strategies-neural-networks-3110">Applying Mapping Strategies to Neural Networks</h3>
<p>While these foundational mapping techniques apply broadly, their effectiveness varies based on the computational structure, data access patterns, and parallelization opportunities of different neural network architectures. Each architecture imposes distinct constraints on data movement, memory hierarchy, and computation scheduling, requiring tailored mapping strategies to optimize performance.</p>
<p>A structured approach to mapping is essential to address the combinatorial explosion of choices that arise when assigning computations to AI accelerators. Rather than treating each model as a separate optimization problem, we recognize that the same fundamental principles apply across different architecturesâ€”only their priority shifts based on workload characteristics. The goal is to systematically select and apply mapping strategies that maximize efficiency for different types of machine learning models.</p>
<p>These principles apply to three representative AI workloads, each characterized by distinct computational demands. CNNs benefit from spatial data reuse, making weight-stationary execution and the application of tiling techniques especially effective. In contrast, Transformers are inherently memory-bound and rely on strategies such as efficient KV-cache management, fused attention mechanisms, and highly parallel execution to mitigate memory traffic. MLPs, which involve substantial matrix multiplication operations, demand the use of structured tiling, optimized weight layouts, and memory-aware execution to enhance overall performance.</p>
<p>Despite their differences, each of these models follows a common set of mapping principles, with variations in how optimizations are prioritized. The following table provides a structured mapping between different optimization strategies and their suitability for CNNs, Transformers, and MLPs. This table serves as a roadmap for selecting appropriate mapping strategies for different machine learning workloads.</p>
<table class="caption-top table">
<colgroup>
<col style="width: 12%">
<col style="width: 9%">
<col style="width: 8%">
<col style="width: 7%">
<col style="width: 60%">
</colgroup>
<thead><tr class="header">
<th style="text-align: left;"><strong>Optimization Technique</strong></th>
<th style="text-align: left;"><strong>CNNs</strong></th>
<th style="text-align: left;"><strong>Transformers</strong></th>
<th style="text-align: left;"><strong>MLPs</strong></th>
<th style="text-align: left;"><strong>Rationale</strong></th>
</tr></thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><strong>Dataflow Strategy</strong></td>
<td style="text-align: left;">Weight Stationary</td>
<td style="text-align: left;">Activation Stationary</td>
<td style="text-align: left;">Weight Stationary</td>
<td style="text-align: left;">CNNs reuse filters across spatial locations; Transformers reuse activations (KV-cache); MLPs reuse weights across batches.</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Memory-Aware Tensor Layouts</strong></td>
<td style="text-align: left;">NCHW (Channel-Major)</td>
<td style="text-align: left;">NHWC (Row-Major)</td>
<td style="text-align: left;">NHWC</td>
<td style="text-align: left;">CNNs favor channel-major for convolution efficiency; Transformers and MLPs prioritize row-major for fast memory access.</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>Kernel Fusion</strong></td>
<td style="text-align: left;">Convolution + Activation</td>
<td style="text-align: left;">Fused Attention</td>
<td style="text-align: left;">GEMM Fusion</td>
<td style="text-align: left;">CNNs optimize convolution+activation fusion; Transformers fuse attention mechanisms; MLPs benefit from fused matrix multiplications.</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Tiling for Memory Efficiency</strong></td>
<td style="text-align: left;">Spatial Tiling</td>
<td style="text-align: left;">Temporal Tiling</td>
<td style="text-align: left;">Blocked Tiling</td>
<td style="text-align: left;">CNNs tile along spatial dimensions; Transformers use loop blocking to improve sequence memory efficiency; MLPs use blocked tiling for large matrix multiplications.</td>
</tr>
</tbody>
</table>
<p>This table highlights that each machine learning model benefits from a different combination of optimization techniques, reinforcing the importance of tailoring execution strategies to the computational and memory characteristics of the workload.</p>
<p>In the following sections, we explore how these optimizations apply to each network type, explaining how CNNs, Transformers, and MLPs leverage specific mapping strategies to improve execution efficiency and hardware utilization.</p>
<section id="sec-ai-acceleration-convolutional-neural-networks-1e47" class="level4"><h4 class="anchored" data-anchor-id="sec-ai-acceleration-convolutional-neural-networks-1e47">Convolutional Neural Networks</h4>
<p>CNNs are characterized by their structured spatial computations, where small filters (or kernels) are repeatedly applied across an input feature map. This structured weight reuse makes weight stationary execution the most effective strategy for CNNs. Keeping filter weights in fast memory while streaming activations ensures that weights do not need to be repeatedly fetched from slower external memory, significantly reducing memory bandwidth demands. Since each weight is applied to multiple spatial locations, weight stationary execution maximizes arithmetic intensity and minimizes redundant memory transfers.</p>
<p>Memory-aware tensor layouts also play a critical role in CNN execution. Convolution operations benefit from a channel-major memory format, often represented as NCHW (batch, channels, height, width). This layout aligns with the access patterns of convolutions, enabling efficient memory coalescing on accelerators such as GPUs and TPUs. By storing data in a format that optimizes cache locality, accelerators can fetch contiguous memory blocks efficiently, reducing latency and improving throughput.</p>
<p>Kernel fusion is another important optimization for CNNs. In a typical machine learning pipeline, convolution operations are often followed by activation functions such as ReLU and batch normalization. Instead of treating these operations as separate computational steps, fusing them into a single kernel reduces intermediate memory writes and improves execution efficiency. This optimization minimizes memory bandwidth pressure by keeping intermediate values in registers rather than writing them to memory and fetching them back in subsequent steps.</p>
<p>Given the size of input images and feature maps, tiling is necessary to ensure that computations fit within fast memory hierarchies. Spatial tiling, where input feature maps are processed in smaller subregions, allows for efficient utilization of on-chip memory while avoiding excessive off-chip memory transfers. This technique ensures that input activations, weights, and intermediate outputs remain within high-speed caches or shared memory as long as possible, reducing memory stalls and improving overall performance.</p>
<p>Together, these optimizations ensure that CNNs make efficient use of available compute resources by maximizing weight reuse, optimizing memory access patterns, reducing redundant memory writes, and structuring computation to fit within fast memory constraints.</p>
</section><section id="sec-ai-acceleration-transformer-architectures-8f25" class="level4"><h4 class="anchored" data-anchor-id="sec-ai-acceleration-transformer-architectures-8f25">Transformer Architectures</h4>
<p>Unlike CNNs, which rely on structured spatial computations, Transformers process variable-length sequences and rely heavily on attention mechanisms. The primary computational bottleneck in Transformers is memory bandwidth, as attention mechanisms require frequent access to stored key-value pairs across multiple query vectors. Given this access pattern, activation stationary execution is the most effective strategy. By keeping key-value activations in fast memory and streaming query vectors dynamically, activation reuse is maximized while minimizing redundant memory fetches. This approach is critical in reducing bandwidth overhead, especially in long-sequence tasks such as natural language processing.</p>
<p>Memory layout optimization is equally important for Transformers. Unlike CNNs, which benefit from channel-major layouts, Transformers require efficient access to sequences of activations, making a row-major format (NHWC) the preferred choice. This layout ensures that activations are accessed contiguously in memory, reducing cache misses and improving memory coalescing for matrix multiplications.</p>
<p>Kernel fusion plays a key role in optimizing Transformer execution. In self-attention, multiple computational steps, such as query-key dot products, softmax normalization, and weighted summation, can be fused into a single operation. Fused attention kernels eliminate intermediate memory writes by computing attention scores and performing weighted summations within a single execution step. This optimization significantly reduces memory traffic, particularly for large batch sizes and long sequences.</p>
<p>Due to the nature of sequence processing, tiling must be adapted to improve memory efficiency. Instead of spatial tiling, which is effective for CNNs, Transformers benefit from temporal tiling, where computations are structured to process sequence blocks efficiently. This method ensures that activations are loaded into fast memory in manageable chunks, reducing excessive memory transfers. Temporal tiling is particularly beneficial for long-sequence models, where the memory footprint of key-value activations grows significantly. By tiling sequences into smaller segments, memory locality is improved, enabling efficient cache utilization and reducing bandwidth pressure.</p>
<p>These optimizations collectively address the primary bottlenecks in Transformer models by prioritizing activation reuse, structuring memory layouts for efficient batched computations, fusing attention operations to reduce intermediate memory writes, and employing tiling techniques suited to sequence-based processing.</p>
</section><section id="sec-ai-acceleration-multilayer-perceptrons-eb18" class="level4 page-columns page-full"><h4 class="anchored" data-anchor-id="sec-ai-acceleration-multilayer-perceptrons-eb18">Multi-Layer Perceptrons</h4>
<p>MLPs primarily consist of fully connected layers, where large matrices of weights and activations are multiplied to produce output representations. Given this structure, weight stationary execution is the most effective strategy for MLPs. Similar to CNNs, MLPs benefit from keeping weights in local memory while streaming activations dynamically, as this ensures that weight matrices, which are typically reused across multiple activations in a batch, do not need to be frequently reloaded.</p>
<p>The preferred memory layout for MLPs aligns with that of Transformers, as matrix multiplications are more efficient when using a row-major (NHWC) format. Since activation matrices are processed in batches, this layout ensures that input activations are accessed efficiently without introducing memory fragmentation. By aligning tensor storage with compute-friendly memory access patterns, cache utilization is improved, reducing memory stalls.</p>
<p>Kernel fusion in MLPs is primarily applied to General Matrix Multiplication (GEMM)<a href="#fn29" class="footnote-ref" id="fnref29" role="doc-noteref"><sup>29</sup></a> operations. Since dense layers are often followed by activation functions and bias additions, fusing these operations into a single computation step reduces memory traffic. GEMM fusion ensures that activations, weights, and biases are processed within a single optimized kernel, avoiding unnecessary memory writes and reloads.</p>
<div class="no-row-height column-margin column-container"><div id="fn29"><p><sup>29</sup>&nbsp;<strong>General Matrix Multiplication (GEMM)</strong>: The fundamental operation C = Î±AB + Î²C that underlies most neural network computations. GEMM accounts for 90-95% of computation time in training deep networks and is the target of most AI hardware optimization. Optimized GEMM libraries like cuBLAS (NVIDIA), oneDNN (Intel), and CLBlast achieve 80-95% of theoretical peak performance through techniques like register blocking, vectorization, and hierarchical tiling. Modern AI accelerators are essentially specialized GEMM engines with additional support for activation functions and data movement.</p></div></div><p>To further improve memory efficiency, MLPs rely on blocked tiling strategies, where large matrix multiplications are divided into smaller sub-blocks that fit within the acceleratorâ€™s shared memory. This method ensures that frequently accessed portions of matrices remain in fast memory throughout computation, reducing external memory accesses. By structuring computations in a way that balances memory utilization with efficient parallel execution, blocked tiling minimizes bandwidth limitations and maximizes throughput.</p>
<p>These optimizations ensure that MLPs achieve high computational efficiency by structuring execution around weight reuse, optimizing memory layouts for dense matrix operations, reducing redundant memory writes through kernel fusion, and employing blocked tiling strategies to maximize on-chip memory utilization.</p>
</section></section><section id="sec-ai-acceleration-hybrid-mapping-strategies-3e8c" class="level3 page-columns page-full"><h3 class="anchored" data-anchor-id="sec-ai-acceleration-hybrid-mapping-strategies-3e8c">Hybrid Mapping Strategies</h3>
<p>While general mapping strategies provide a structured framework for optimizing machine learning models, real-world architectures often involve diverse computational requirements that cannot be effectively addressed with a single, fixed approach. Hybrid mapping strategies allow AI accelerators to dynamically apply different optimizations to specific layers or components within a model, ensuring that each computation is executed with maximum efficiency.</p>
<p>Machine learning models typically consist of multiple layer types, each exhibiting distinct memory access patterns, data reuse characteristics, and parallelization opportunities. By tailoring mapping strategies to these specific properties, hybrid approaches achieve higher computational efficiency, improved memory bandwidth utilization, and reduced data movement overhead compared to a uniform mapping approach <span class="citation" data-cites="sze2020efficient">(<a href="#ref-sze2020efficient" role="doc-biblioref">Sze et al. 2017b</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-sze2020efficient" class="csl-entry" role="listitem">
Sze, Vivienne, Yu-Hsin Chen, Tien-Ju Yang, and Joel S. Emer. 2017b. <span>â€œEfficient Processing of Deep Neural Networks: A Tutorial and Survey.â€</span> <em>Proceedings of the IEEE</em> 105 (12): 2295â€“2329. <a href="https://doi.org/10.1109/jproc.2017.2761740">https://doi.org/10.1109/jproc.2017.2761740</a>.
</div></div><section id="sec-ai-acceleration-layerspecific-mapping-1102" class="level4 page-columns page-full"><h4 class="anchored" data-anchor-id="sec-ai-acceleration-layerspecific-mapping-1102">Layer-Specific Mapping</h4>
<p>Hybrid mapping strategies are particularly beneficial in models that combine spatially localized computations, such as convolutions, with fully connected operations, such as dense layers or attention mechanisms. These operations possess distinct characteristics that require different mapping strategies for optimal performance.</p>
<p>In convolutional neural networks, hybrid strategies are frequently employed to optimize performance. Specifically, weight stationary execution is applied to convolutional layers, ensuring that filters remain in local memory while activations are streamed dynamically. For fully connected layers, output stationary execution is utilized to minimize redundant memory writes during matrix multiplications. Additionally, kernel fusion is integrated to combine activation functions, batch normalization, and element wise operations into a single computational step, thereby reducing intermediate memory traffic. Collectively, these approaches enhance computational efficiency and memory utilization, contributing to the overall performance of the network.</p>
<p>Transformers employ several strategies to enhance performance by optimizing memory usage and computational efficiency. Specifically, they use activation stationary mapping in self-attention layers to maximize the reuse of stored key-value pairs, thereby reducing memory fetches. In feedforward layers, weight stationary mapping is applied to ensure that large weight matrices are efficiently reused across computations. Additionally, these models incorporate fused attention kernels that integrate softmax and weighted summation into a single computation step, significantly enhancing execution speed <span class="citation" data-cites="dao2022flashattention">(<a href="#ref-dao2022flashattention" role="doc-biblioref">Jacobs et al. 2002</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-dao2022flashattention" class="csl-entry" role="listitem">
Jacobs, David, Bas Rokers, Archisman Rudra, and Zili Liu. 2002. <span>â€œFragment Completion in Humans and Machines.â€</span> In <em>Advances in Neural Information Processing Systems 14</em>, 35:27â€“34. The MIT Press. <a href="https://doi.org/10.7551/mitpress/1120.003.0008">https://doi.org/10.7551/mitpress/1120.003.0008</a>.
</div></div><p>For multilayer perceptrons, hybrid mapping strategies are employed to optimize performance through a combination of techniques that enhance both memory efficiency and computational throughput. Specifically, weight stationary execution is utilized to maximize the reuse of weights across activations, ensuring that these frequently accessed parameters remain readily available and reduce redundant memory accesses. In addition, blocked tiling strategies are implemented for large matrix multiplications, which significantly improve cache locality by partitioning the computation into manageable sub-blocks that fit within fast memory. Complementing these approaches, general matrix multiplication fusion is applied, effectively reducing memory stalls by merging consecutive matrix multiplication operations with subsequent functional transformations. Collectively, these optimizations illustrate how tailored mapping strategies can systematically balance memory constraints with computational demands in multilayer perceptron architectures.</p>
<p>Hybrid mapping strategies are widely employed in vision transformers, which seamlessly integrate convolutional and self-attention operations. In these models, the patch embedding layer performs a convolution-like operation that benefits from weight stationary mapping <span class="citation" data-cites="Dosovitskiy2020ViT">(<a href="#ref-Dosovitskiy2020ViT" role="doc-biblioref">Dosovitskiy et al. 2020</a>)</span>. The self-attention layers, on the other hand, require activation stationary execution to efficiently reuse the key-value cache across multiple queries. Additionally, the MLP component leverages general matrix multiplication fusion and blocked tiling to execute dense matrix multiplications efficiently. This layer-specific optimization framework effectively balances memory locality with computational efficiency, rendering vision transformers particularly well-suited for AI accelerators.</p>
<div class="no-row-height column-margin column-container"><div id="ref-Dosovitskiy2020ViT" class="csl-entry" role="listitem">
Dosovitskiy, Alexey, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, et al. 2020. <span>â€œAn Image Is Worth 16x16 Words: Transformers for Image Recognition at Scale.â€</span> <em>International Conference on Learning Representations (ICLR)</em>, October. <a href="http://arxiv.org/abs/2010.11929v2">http://arxiv.org/abs/2010.11929v2</a>.
</div></div></section></section><section id="sec-ai-acceleration-hardware-implementations-hybrid-strategies-c0e8" class="level3"><h3 class="anchored" data-anchor-id="sec-ai-acceleration-hardware-implementations-hybrid-strategies-c0e8">Hardware Implementations of Hybrid Strategies</h3>
<p>Several modern AI accelerators incorporate hybrid mapping strategies to optimize execution by tailoring layer-specific techniques to the unique computational requirements of diverse neural network architectures. For example, Google TPUs employ weight stationary mapping for convolutional layers and activation stationary mapping for attention layers within transformer models, ensuring that the most critical data remains in fast memory. Likewise, NVIDIA GPUs leverage fused kernels alongside hybrid memory layouts, which enable the application of different mapping strategies within the same model to maximize performance. In addition, Graphcore IPUs dynamically select execution strategies on a per-layer basis to optimize memory access, thereby enhancing overall computational efficiency.</p>
<p>These real-world implementations illustrate how hybrid mapping strategies bridge the gap between different types of machine learning computations, ensuring that each layer executes with maximum efficiency. However, hardware support is essential for these techniques to be practical. Accelerators must provide architectural features such as programmable memory hierarchies, efficient interconnects, and specialized execution pipelines to fully exploit hybrid mapping.</p>
<p>Hybrid mapping provides a flexible and efficient approach to deep learning execution, enabling AI accelerators to adapt to the diverse computational requirements of modern architectures. By selecting the optimal mapping technique for each layer, hybrid strategies help reduce memory bandwidth constraints, improve data locality, and maximize parallelism.</p>
<p>While hybrid mapping strategies offer an effective way to optimize computations at a layer-specific level, they remain static design-time optimizations. In real-world AI workloads, execution conditions can change dynamically due to varying input sizes, memory contention, or hardware resource availability. Machine learning compilers and runtime systems extend these mapping techniques by introducing dynamic scheduling, memory optimizations, and automatic tuning mechanisms. These systems ensure that hybrid strategies are not just predefined execution choices, but rather adaptive mechanisms that allow deep learning workloads to operate efficiently across different accelerators and deployment environments. In the next section, we explore how machine learning compilers and runtime stacks enable these adaptive optimizations through just-in-time scheduling, memory-aware execution, and workload balancing strategies.</p>
<div id="quiz-question-sec-ai-acceleration-dataflow-optimization-strategies-ce52" class="callout callout-quiz-question">
<details class="callout-quiz-question fbx-default closebutton"><summary><strong>Self-Check: Question 1.6</strong></summary><div>
<ol type="1">
<li>
<p>Which of the following dataflow strategies keeps weights fixed in local memory while streaming input activations through the system?</p>
<ol type="a">
<li>Weight Stationary</li>
<li>Input Stationary</li>
<li>Output Stationary</li>
<li>Activation Stationary</li>
</ol>
</li>
<li><p>True or False: In an output stationary dataflow strategy, input activations are kept fixed in local memory.</p></li>
<li><p>What are the trade-offs of using an input stationary strategy in a transformer model?</p></li>
<li>
<p>In a system design scenario, which dataflow strategy would be most effective for a CNN with high weight reuse?</p>
<ol type="a">
<li>Activation Stationary</li>
<li>Output Stationary</li>
<li>Input Stationary</li>
<li>Weight Stationary</li>
</ol>
</li>
<li><p>How might you decide between using a weight stationary or output stationary strategy in a new AI model?</p></li>
</ol>
<p><a href="#quiz-answer-sec-ai-acceleration-dataflow-optimization-strategies-ce52" class="question-label">See Answers â†’</a></p>
</div></details>
</div>
</section></section><section id="sec-ai-acceleration-compiler-support-172e" class="level2 page-columns page-full"><h2 class="anchored" data-anchor-id="sec-ai-acceleration-compiler-support-172e">Compiler Support</h2>
<p>The performance of machine learning acceleration depends not only on hardware capabilities but also on how efficiently models are translated into executable operations. These optimization techniques, including kernel fusion, tiling, memory scheduling, and data movement strategies, are essential for maximizing efficiency. However, these optimizations must be systematically applied before execution to ensure they align with hardware constraints and computational requirements.</p>
<p>This process exemplifies the hardware-software co-design principle established in <a href="#sec-ai-acceleration-ai-hardware-acceleration-fundamentals-2096" class="quarto-xref">Section&nbsp;1.1</a>, where machine learning compilers bridge high-level model representations with low-level hardware execution. The compiler optimizes models by restructuring computations, selecting efficient execution kernels, and maximizing hardware utilization <span class="citation" data-cites="chen_tvmlang_2018">(<a href="#ref-chen_tvmlang_2018" role="doc-biblioref">0001 et al. 2018a</a>)</span>. Unlike traditional compilers designed for general-purpose computing, ML workloads require specialized approaches for tensor computations and parallel execution.</p>
<div class="no-row-height column-margin column-container"></div><section id="sec-ai-acceleration-compiler-design-differences-ml-workloads-0698" class="level3 page-columns page-full"><h3 class="anchored" data-anchor-id="sec-ai-acceleration-compiler-design-differences-ml-workloads-0698">Compiler Design Differences for ML Workloads</h3>
<p>Machine learning workloads introduce unique challenges that traditional compilers were not designed to handle. Unlike conventional software execution, which primarily involves sequential or multi-threaded program flow, machine learning models are expressed as computation graphs that describe large-scale tensor operations. These graphs require specialized optimizations that traditional compilers cannot efficiently apply <span class="citation" data-cites="cui_mlcompilers_2019">(<a href="#ref-cui_mlcompilers_2019" role="doc-biblioref">Cui, Li, and Xie 2019</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-cui_mlcompilers_2019" class="csl-entry" role="listitem">
Cui, Hongyi, Jiajun Li, and Peng et al. Xie. 2019. <span>â€œA Survey on Machine Learning Compilers: Taxonomy, Challenges, and Future Directions.â€</span> <em>ACM Computing Surveys</em> 52 (4): 1â€“39.
</div></div><p><a href="#tbl-ml-vs-traditional-compilers" class="quarto-xref">Table&nbsp;18</a> outlines the fundamental differences between traditional compilers and those designed for machine learning workloads. While traditional compilers optimize linear program execution through techniques like instruction scheduling and register allocation, ML compilers focus on optimizing computation graphs for efficient tensor operations. This distinction is critical, as ML compilers must incorporate domain-specific transformations such as kernel fusion, memory-aware scheduling, and hardware-accelerated execution plans to achieve high performance on specialized accelerators like GPUs and TPUs.</p>
<p>This comparison highlights why machine learning models require a different compilation approach. Instead of optimizing instruction-level execution, machine learning compilers must transform entire computation graphs, apply tensor-aware memory optimizations, and schedule operations across thousands of parallel processing elements. These requirements make traditional compiler techniques insufficient for modern deep learning workloads.</p>
<div id="tbl-ml-vs-traditional-compilers" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure"><figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-ml-vs-traditional-compilers-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;18: <strong>Compiler Optimization Priorities</strong>: Traditional and machine learning compilers diverge in their optimization targets; traditional compilers prioritize efficient execution of sequential code, while ML compilers focus on optimizing tensor operations within computation graphs for specialized hardware. This table clarifies how ML compilers incorporate domain-specific transformationsâ€”like kernel fusion and memory-aware schedulingâ€”to achieve high performance on accelerators, unlike the instruction scheduling and register allocation techniques used in conventional software compilation.
</figcaption><div aria-describedby="tbl-ml-vs-traditional-compilers-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<colgroup>
<col style="width: 18%">
<col style="width: 39%">
<col style="width: 41%">
</colgroup>
<thead><tr class="header">
<th style="text-align: left;"><strong>Aspect</strong></th>
<th style="text-align: left;"><strong>Traditional Compiler</strong></th>
<th style="text-align: left;"><strong>Machine Learning Compiler</strong></th>
</tr></thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><strong>Input Representation</strong></td>
<td style="text-align: left;">Linear program code (C, Python)</td>
<td style="text-align: left;">Computational graph (ML models)</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Execution Model</strong></td>
<td style="text-align: left;">Sequential or multi-threaded execution</td>
<td style="text-align: left;">Massively parallel tensor-based execution</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>Optimization Priorities</strong></td>
<td style="text-align: left;">Instruction scheduling, loop unrolling, register allocation</td>
<td style="text-align: left;">Graph transformations, kernel fusion, memory-aware execution</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Memory Management</strong></td>
<td style="text-align: left;">Stack and heap memory allocation</td>
<td style="text-align: left;">Tensor layout transformations, tiling, memory-aware scheduling</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>Target Hardware</strong></td>
<td style="text-align: left;">CPUs (general-purpose execution)</td>
<td style="text-align: left;">GPUs, TPUs, and custom accelerators</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Compilation Output</strong></td>
<td style="text-align: left;">CPU-specific machine code</td>
<td style="text-align: left;">Hardware-specific execution plan (kernels, memory scheduling)</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
</section><section id="sec-ai-acceleration-ml-compilation-pipeline-7676" class="level3 page-columns page-full"><h3 class="anchored" data-anchor-id="sec-ai-acceleration-ml-compilation-pipeline-7676">ML Compilation Pipeline</h3>
<p>Machine learning models, as defined in modern frameworks, are initially represented in a high-level computation graph that describes operations on tensors. However, these representations are not directly executable on hardware accelerators such as GPUs, TPUs, and custom AI chips. To achieve efficient execution, models must go through a compilation process that transforms them into optimized execution plans suited for the target hardware <span class="citation" data-cites="tensorflow_xla_2020">(<a href="#ref-tensorflow_xla_2020" role="doc-biblioref">Brain 2020</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-tensorflow_xla_2020" class="csl-entry" role="listitem">
Brain, Google. 2020. <span>â€œXLA: Optimizing Compiler for Machine Learning.â€</span> <em>TensorFlow Blog</em>. <a href="https://www.tensorflow.org/xla">https://www.tensorflow.org/xla</a>.
</div></div><p>The machine learning compilation workflow consists of several key stages, each responsible for applying specific optimizations that ensure minimal memory overhead, maximum parallel execution, and optimal compute utilization. These stages include:</p>
<ol type="1">
<li>
<strong>Graph Optimization</strong>: The computation graph is restructured to eliminate inefficiencies.</li>
<li>
<strong>Kernel Selection</strong>: Each operation is mapped to an optimized hardware-specific implementation.</li>
<li>
<strong>Memory Planning</strong>: Tensor layouts and memory access patterns are optimized to reduce bandwidth consumption.</li>
<li>
<strong>Computation Scheduling</strong>: Workloads are distributed across parallel processing elements to maximize hardware utilization.</li>
<li>
<strong>Code Generation</strong>: The optimized execution plan is translated into machine-specific instructions for execution.</li>
</ol>
<p>At each stage, the compiler applies theoretical optimizations discussed earlier, including kernel fusion, tiling, data movement strategies, and computation placement, ensuring that these optimizations are systematically incorporated into the final execution plan.</p>
<p>By understanding this workflow, we can see how machine learning acceleration is realized not just through hardware improvements but also through compiler-driven software optimizations.</p>
</section><section id="sec-ai-acceleration-graph-optimization-f888" class="level3 page-columns page-full"><h3 class="anchored" data-anchor-id="sec-ai-acceleration-graph-optimization-f888">Graph Optimization</h3>
<p>AI accelerators provide specialized hardware to speed up computation, but raw model representations are not inherently optimized for execution on these accelerators. Machine learning frameworks define models using high-level computation graphs, where nodes represent operations (such as convolutions, matrix multiplications, and activations), and edges define data dependencies. However, if executed as defined, these graphs often contain redundant operations, inefficient memory access patterns, and suboptimal execution sequences that can prevent the hardware from operating at peak efficiency.</p>
<p>For example, in a Transformer model, the self-attention mechanism involves repeated accesses to the same key-value pairs across multiple attention heads. If compiled naÃ¯vely, the model may reload the same data multiple times, leading to excessive memory traffic <span class="citation" data-cites="shoeybi_megatron_2020">(<a href="#ref-shoeybi_megatron_2020" role="doc-biblioref">Shoeybi et al. 2019a</a>)</span>. Similarly, in a CNN, applying batch normalization and activation functions as separate operations after each convolution leads to unnecessary intermediate memory writes, increasing memory bandwidth usage. These inefficiencies are addressed during graph optimization, where the compiler restructures the computation graph to eliminate unnecessary operations and improve memory locality <span class="citation" data-cites="chen_tvmlang_2018">(<a href="#ref-chen_tvmlang_2018" role="doc-biblioref">0001 et al. 2018a</a>)</span>.</p>
<div class="no-row-height column-margin column-container"></div><p>The graph optimization phase of compilation is responsible for transforming this high-level computation graph into an optimized execution plan before it is mapped to hardware. Rather than requiring manual optimization, the compiler systematically applies transformations that improve data movement, reduce redundant computations, and restructure operations for efficient parallel execution <span class="citation" data-cites="nvidia_tensorRT_2021">(<a href="#ref-nvidia_tensorRT_2021" role="doc-biblioref">NVIDIA 2021</a>)</span>.</p>
<p>At this stage, the compiler is still working at a hardware-agnostic level, focusing on high-level restructuring that improves efficiency before more hardware-specific optimizations are applied later.</p>
<section id="sec-ai-acceleration-computation-graph-optimization-a028" class="level4"><h4 class="anchored" data-anchor-id="sec-ai-acceleration-computation-graph-optimization-a028">Computation Graph Optimization</h4>
<p>Graph optimization transforms the computation graph through a series of structured techniques designed to enhance execution efficiency. One key technique is kernel fusion, which merges consecutive operations to eliminate unnecessary memory writes and reduce the number of kernel launches. This approach is particularly effective in convolutional neural networks, where fusing convolution, batch normalization, and activation functions notably accelerates processing. Another important technique is computation reordering, which adjusts the execution order of operations to improve data locality and maximize parallel execution. For instance, in Transformer models, such reordering enables the reuse of cached key-value pairs rather than reloading them repeatedly from memory, thereby reducing latency.</p>
<p>Additionally, redundant computation elimination plays an important role. By identifying and removing duplicate or unnecessary operations, this method is especially beneficial in models with residual connections where common subexpressions might otherwise be redundantly computed. Memory-aware dataflow adjustments enhance overall performance by refining tensor layouts and optimizing memory movement. For example, tiling matrix multiplications to meet the structural requirements of systolic arrays in TPUs ensures that hardware resources are utilized optimally. This combined approach not only reduces unnecessary processing but also aligns data storage and movement with the acceleratorâ€™s strengths, leading to efficient execution across diverse AI workloads. Together, these techniques prepare the model for acceleration by minimizing overhead and ensuring an optimal balance between computational and memory resources.</p>
</section><section id="sec-ai-acceleration-implementation-ai-compilers-1df9" class="level4"><h4 class="anchored" data-anchor-id="sec-ai-acceleration-implementation-ai-compilers-1df9">Implementation in AI Compilers</h4>
<p>Modern AI compilers perform graph optimization through the use of automated pattern recognition and structured rewrite rules, systematically transforming computation graphs to maximize efficiency without manual intervention. For example, Googleâ€™s XLA (Accelerated Linear Algebra) in TensorFlow applies graph-level transformations such as fusion and layout optimizations that streamline execution on TPUs and GPUs. Similarly, TVM (Tensor Virtual Machine) not only refines tensor layouts and adjusts computational structures but also tunes execution strategies across diverse hardware backends, which is particularly beneficial for deploying models on embedded Tiny ML devices with strict memory constraints.</p>
<p>NVIDIAâ€™s TensorRT, another specialized deep learning compiler, focuses on minimizing kernel launch overhead by fusing operations and optimizing execution scheduling on GPUs, thereby improving utilization and reducing inference latency in large-scale convolutional neural network applications. Additionally, MLIR (Multi-Level Intermediate Representation) facilitates flexible graph optimization across various AI accelerators by enabling multi-stage transformations that improve execution order and memory access patterns, thus easing the transition of models from CPU-based implementations to accelerator-optimized versions. These compilers preserve the mathematical integrity of the models while rewriting the computation graph to ensure that the subsequent hardware-specific optimizations can be effectively applied.</p>
</section><section id="sec-ai-acceleration-graph-optimization-importance-9ccb" class="level4"><h4 class="anchored" data-anchor-id="sec-ai-acceleration-graph-optimization-importance-9ccb">Graph Optimization Importance</h4>
<p>Graph optimization enables AI accelerators to operate at peak efficiency. Without this phase, even the most optimized hardware would be underutilized, as models would be executed in a way that introduces unnecessary memory stalls, redundant computations, and inefficient data movement. By systematically restructuring computation graphs, the compiler arranges operations for efficient execution that mitigates bottlenecks before mapping to hardware, minimizes memory movement to keep tensors in high-speed memory, and optimizes parallel execution to reduce unnecessary serialization while enhancing hardware utilization. For instance, without proper graph optimization, a large Transformer model running on an edge device may experience excessive memory stalls due to suboptimal data access patterns; however, through effective graph restructuring, the model can operate with significantly reduced memory bandwidth consumption and latency, thus enabling real-time inference on devices with constrained resources.</p>
<p>With the computation graph now fully optimized, the next step in compilation is kernel selection, where the compiler determines which hardware-specific implementation should be used for each operation. This ensures that the structured execution plan is translated into optimized low-level instructions for the target accelerator.</p>
</section></section><section id="sec-ai-acceleration-kernel-selection-df01" class="level3 page-columns page-full"><h3 class="anchored" data-anchor-id="sec-ai-acceleration-kernel-selection-df01">Kernel Selection</h3>
<p>At this stage, the compiler translates the abstract operations in the computation graph into optimized low-level functions, ensuring that execution is performed as efficiently as possible given the constraints of the target accelerator. A kernel is a specialized implementation of a computational operation designed to run efficiently on a particular hardware architecture. Most accelerators, including GPUs, TPUs, and custom AI chips, provide multiple kernel implementations for the same operation, each optimized for different execution scenarios. Choosing the right kernel for each operation is essential for maximizing computational throughput, minimizing memory stalls, and ensuring that the acceleratorâ€™s specialized processing elements are fully utilized <span class="citation" data-cites="nvidia_tensorRT_2021">(<a href="#ref-nvidia_tensorRT_2021" role="doc-biblioref">NVIDIA 2021</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-chen_tvmlang_2018" class="csl-entry" role="listitem">
0001, Tianqi Chen, Thierry Moreau, Ziheng Jiang, Lianmin Zheng, Eddie Q. Yan, Haichen Shen, Meghan Cowan, et al. 2018a. <span>â€œTVM: An Automated End-to-End Optimizing Compiler for Deep Learning.â€</span> In <em>13th USENIX Symposium on Operating Systems Design and Implementation (OSDI 18)</em>, 578â€“94. <a href="https://www.usenix.org/conference/osdi18/presentation/chen">https://www.usenix.org/conference/osdi18/presentation/chen</a>.
</div></div><p>Kernel selection builds upon the graph optimization phase, ensuring that the structured execution plan is mapped to the most efficient implementation available. While graph optimization eliminates inefficiencies at the model level, kernel selection ensures that each individual operation is executed using the most efficient hardware-specific routine. The effectiveness of this process directly impacts the modelâ€™s overall performance, as poor kernel choices can nullify the benefits of prior optimizations by introducing unnecessary computation overhead or memory bottlenecks <span class="citation" data-cites="chen_tvmlang_2018">(<a href="#ref-chen_tvmlang_2018" role="doc-biblioref">0001 et al. 2018a</a>)</span>.</p>
<p>In a Transformer model, the matrix multiplications that dominate self-attention computations can be executed using different strategies depending on the available hardware. On a CPU, a general-purpose matrix multiplication routine is typically employed, exploiting vectorized execution to improve efficiency. In contrast, on a GPU, the compiler may select an implementation that leverages tensor cores to accelerate matrix multiplications using mixed-precision arithmetic. When the model is deployed on a TPU, the operation can be mapped onto a systolic array, ensuring that data flows through the accelerator in a manner that maximizes reuse and minimizes off-chip memory accesses. Additionally, for inference workloads, an integer arithmetic kernel may be preferable, as it facilitates computations in INT8 instead of floating-point precision, thereby reducing power consumption without significantly compromising accuracy.</p>
<p>In many cases, compilers do not generate custom kernels from scratch but instead select from vendor-optimized kernel libraries that provide highly tuned implementations for different architectures. For instance, cuDNN and cuBLAS offer optimized kernels for deep learning on NVIDIA GPUs, while oneDNN provides optimized execution for Intel architectures. Similarly, ACL (Arm Compute Library) is optimized for Arm-based devices, and Eigen and BLIS provide efficient CPU-based implementations of deep learning operations. These libraries allow the compiler to choose pre-optimized, high-performance kernels rather than having to reinvent execution strategies for each hardware platform.</p>
<section id="sec-ai-acceleration-implementation-ai-compilers-c917" class="level4 page-columns page-full"><h4 class="anchored" data-anchor-id="sec-ai-acceleration-implementation-ai-compilers-c917">Implementation in AI Compilers</h4>
<p>AI compilers use heuristics, profiling, and cost models to determine the best kernel for each operation. These strategies ensure that each computation is executed in a way that maximizes throughput and minimizes memory bottlenecks.</p>
<p>In rule-based selection, the compiler applies predefined heuristics based on the known capabilities of the hardware. For instance, XLA, the compiler used in TensorFlow, automatically selects tensor core-optimized kernels for NVIDIA GPUs when mixed-precision execution is enabled. These predefined rules allow the compiler to make fast, reliable decisions about which kernel to use without requiring extensive analysis.</p>
<p>Profile-guided selection takes a more dynamic approach, benchmarking different kernel options and choosing the one that performs best for a given workload. TVM, an open-source AI compiler, uses AutoTVM to empirically evaluate kernel performance, tuning execution strategies based on real-world execution times. By testing different kernels before deployment, profile-guided selection helps ensure that operations are assigned to the most efficient implementation under actual execution conditions.</p>
<p>Another approach, cost model-based selection, relies on performance predictions to estimate execution time and memory consumption for various kernels before choosing the most efficient one. MLIR, a compiler infrastructure designed for machine learning workloads, applies this technique to determine the most effective tiling and memory access strategies <span class="citation" data-cites="mlir_framework_2021">(<a href="#ref-mlir_framework_2021" role="doc-biblioref">Lattner et al. 2020</a>)</span>. By modeling how different kernels interact with the acceleratorâ€™s compute units and memory hierarchy, the compiler can select the kernel that minimizes execution cost while maximizing performance.</p>
<div class="no-row-height column-margin column-container"><div id="ref-mlir_framework_2021" class="csl-entry" role="listitem">
Lattner, Chris, Mehdi Amini, Uday Bondhugula, Albert Cohen, Andy Davis, Jacques Pienaar, River Riddle, Tatiana Shpeisman, Nicolas Vasilache, and Oleksandr Zinenko. 2020. <span>â€œMLIR: A Compiler Infrastructure for the End of Mooreâ€™s Law.â€</span> <em>arXiv Preprint arXiv:2002.11054</em>, February. <a href="http://arxiv.org/abs/2002.11054v2">http://arxiv.org/abs/2002.11054v2</a>.
</div></div><p>Many AI compilers also incorporate precision-aware kernel selection, where the selected kernel is optimized for specific numerical formats such as FP32, FP16, BF16, or INT8. Training workloads often prioritize higher precision (FP32, BF16) to maintain model accuracy, whereas inference workloads favor lower precision (FP16, INT8) to increase speed and reduce power consumption. For example, an NVIDIA GPU running inference with TensorRT can dynamically select FP16 or INT8 kernels based on a modelâ€™s accuracy constraints. This trade-off between precision and performance is a key aspect of kernel selection, especially when deploying models in resource-constrained environments.</p>
<p>Some compilers go beyond static kernel selection and implement adaptive kernel tuning, where execution strategies are adjusted at runtime based on the systemâ€™s workload and available resources. AutoTVM in TVM measures kernel performance across different workloads and dynamically refines execution strategies. TensorRT applies real-time optimizations based on batch size, memory constraints, and GPU load, adjusting kernel selection dynamically. Googleâ€™s TPU compiler takes a similar approach, optimizing kernel selection based on cloud resource availability and execution environment constraints.</p>
</section><section id="sec-ai-acceleration-kernel-selection-importance-3c3f" class="level4"><h4 class="anchored" data-anchor-id="sec-ai-acceleration-kernel-selection-importance-3c3f">Kernel Selection Importance</h4>
<p>The efficiency of AI acceleration depends not only on how computations are structured but also on how they are executed. Even the best-designed computation graph will fail to achieve peak performance if the selected kernels do not fully utilize the hardwareâ€™s capabilities.</p>
<p>Proper kernel selection allows models to execute using the most efficient algorithms available for the given hardware, ensuring that memory is accessed in a way that avoids unnecessary stalls and that specialized acceleration features, such as tensor cores or systolic arrays, are leveraged wherever possible. Selecting an inappropriate kernel can lead to underutilized compute resources, excessive memory transfers, and increased power consumption, all of which limit the performance of AI accelerators.</p>
<p>For instance, if a Transformer model running on a GPU is assigned a non-tensor-core kernel for its matrix multiplications, it may execute at only a fraction of the possible performance. Conversely, if a model designed for FP32 execution is forced to run on an INT8-optimized kernel, it may experience significant numerical instability, degrading accuracy. These choices illustrate why kernel selection is as much about maintaining numerical correctness as it is about optimizing performance.</p>
<p>With kernel selection complete, the next stage in compilation involves execution scheduling and memory management, where the compiler determines how kernels are launched and how data is transferred between different levels of the memory hierarchy. These final steps in the compilation pipeline ensure that computations run with maximum parallelism while minimizing the overhead of data movement. As kernel selection determines what to execute, execution scheduling and memory management dictate when and how those kernels are executed, ensuring that AI accelerators operate at peak efficiency.</p>
</section></section><section id="sec-ai-acceleration-memory-planning-fb9f" class="level3 page-columns page-full"><h3 class="anchored" data-anchor-id="sec-ai-acceleration-memory-planning-fb9f">Memory Planning</h3>
<p>The memory planning phase ensures that data is allocated and accessed in a way that minimizes memory bandwidth consumption, reduces latency, and maximizes cache efficiency <span class="citation" data-cites="zhang2020optimizing">(<a href="#ref-zhang2020optimizing" role="doc-biblioref">Zhang, Li, and Ouyang 2020</a>)</span>. Even with the most optimized execution plan, a model can still suffer from severe performance degradation if memory is not managed efficiently.</p>
<div class="no-row-height column-margin column-container"><div id="ref-zhang2020optimizing" class="csl-entry" role="listitem">
Zhang, Y., J. Li, and H. Ouyang. 2020. <span>â€œOptimizing Memory Access for Deep Learning Workloads.â€</span> <em>IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems</em> 39 (11): 2345â€“58.
</div></div><p>Machine learning workloads are often memory-intensive. They require frequent movement of large tensors between different levels of the memory hierarchy. The compiler must determine how tensors are stored, how they are accessed, and how intermediate results are handled to ensure that memory does not become a bottleneck.</p>
<p>The memory planning phase focuses on optimizing tensor layouts, memory access patterns, and buffer reuse to prevent unnecessary stalls and memory contention during execution. In this phase, tensors are arranged in a memory-efficient format that aligns with hardware access patterns, thereby minimizing the need for format conversions. Additionally, memory accesses are structured to reduce cache misses and stalls, which in turn lowers overall bandwidth consumption. Buffer reuse is also a critical aspect, as it reduces redundant memory allocations by intelligently managing intermediate results. Together, these strategies ensure that data is efficiently placed and accessed, thereby enhancing both computational performance and energy efficiency in AI workloads.</p>
<section id="sec-ai-acceleration-implementation-ai-compilers-2ae0" class="level4 page-columns page-full"><h4 class="anchored" data-anchor-id="sec-ai-acceleration-implementation-ai-compilers-2ae0">Implementation in AI Compilers</h4>
<p>Memory planning is a complex problem because AI models must balance memory availability, reuse, and access efficiency while operating across multiple levels of the memory hierarchy. AI compilers use several key strategies to manage memory effectively and prevent unnecessary data movement.</p>
<p>The first step in memory planning is tensor layout optimization, where the compiler determines how tensors should be arranged in memory to maximize locality and prevent unnecessary data format conversions. Different hardware accelerators have different preferred storage layoutsâ€”for instance, NVIDIA GPUs often use row-major storage (NHWC format), while TPUs favor channel-major layouts (NCHW format) to optimize memory coalescing <span class="citation" data-cites="abadi2016tensorflow">(<a href="#ref-abadi2016tensorflow" role="doc-biblioref">Abadi et al. 2016</a>)</span>. The compiler automatically transforms tensor layouts based on the expected access patterns of the target hardware, ensuring that memory accesses are aligned for maximum efficiency.</p>
<div class="no-row-height column-margin column-container"><div id="ref-abadi2016tensorflow" class="csl-entry" role="listitem">
Abadi, M. et al. 2016. <span>â€œTensorFlow: A System for Large-Scale Machine Learning.â€</span> <em>12th USENIX Symposium on Operating Systems Design and Implementation (OSDI 16)</em>, 265â€“83.
</div><div id="ref-moreau2018relay" class="csl-entry" role="listitem">
Jones, Gareth A. 2018. <span>â€œJoining Dessins Together.â€</span> <em>arXiv Preprint arXiv:1810.03960</em>, October. <a href="http://arxiv.org/abs/1810.03960v1">http://arxiv.org/abs/1810.03960v1</a>.
</div></div><p>Beyond layout optimization, memory planning also includes buffer allocation and reuse, where the compiler minimizes memory footprint by reusing intermediate storage whenever possible. Deep learning workloads generate many temporary tensors, such as activations and gradients, which can quickly overwhelm on-chip memory if not carefully managed. Instead of allocating new memory for each tensor, the compiler analyzes the computation graph to identify opportunities for buffer reuse, ensuring that intermediate values are stored and overwritten efficiently <span class="citation" data-cites="moreau2018relay">(<a href="#ref-moreau2018relay" role="doc-biblioref">Jones 2018</a>)</span>.</p>
<p>Another critical aspect of memory planning is minimizing data movement between different levels of the memory hierarchy. AI accelerators typically have a mix of high-speed on-chip memory (such as caches or shared SRAM) and larger, but slower, external DRAM. If tensor data is repeatedly moved between these memory levels, the model may become memory-bound, reducing computational efficiency. To prevent this, compilers use tiling strategies that break large computations into smaller, memory-friendly chunks, allowing execution to fit within fast, local memory and reducing the need for costly off-chip memory accesses.</p>
</section><section id="sec-ai-acceleration-memory-planning-importance-e987" class="level4"><h4 class="anchored" data-anchor-id="sec-ai-acceleration-memory-planning-importance-e987">Memory Planning Importance</h4>
<p>Without proper memory planning, even the most optimized computation graph and kernel selection will fail to deliver high performance. Excessive memory transfers, inefficient memory layouts, and redundant memory allocations can all lead to bottlenecks that prevent AI accelerators from reaching their peak throughput.</p>
<p>For instance, a CNN running on a GPU may achieve high computational efficiency in theory, but if its convolutional feature maps are stored in an incompatible format, for example, if it uses a row-major layout that necessitates conversion to a channel-friendly format such as NCHW or a variant like NHCW, constant tensor format conversions can introduce significant overhead. Similarly, a Transformer model deployed on an edge device may struggle to meet real-time inference requirements if memory is not carefully planned, leading to frequent off-chip memory accesses that increase latency and power consumption.</p>
<p>Through careful management of tensor placement, optimizing memory access patterns, and reducing unnecessary data movement, memory planning guarantees efficient operation of AI accelerators, leading to tangible performance improvements in real-world applications.</p>
</section></section><section id="sec-ai-acceleration-computation-scheduling-7ccd" class="level3 page-columns page-full"><h3 class="anchored" data-anchor-id="sec-ai-acceleration-computation-scheduling-7ccd">Computation Scheduling</h3>
<p>With graph optimization completed, kernels selected, and memory planning finalized, the next step in the compilation pipeline is computation scheduling. This phase determines when and where each computation should be executed, ensuring that workloads are efficiently distributed across available processing elements while avoiding unnecessary stalls and resource contention <span class="citation" data-cites="Rajbhandari2020 Zheng2020">(<a href="#ref-Rajbhandari2020" role="doc-biblioref">Rajbhandari et al. 2020</a>; <a href="#ref-Zheng2020" role="doc-biblioref">Zheng et al. 2020</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-Rajbhandari2020" class="csl-entry" role="listitem">
Rajbhandari, Samyam, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. 2020. <span>â€œZeRO: Memory Optimization Towards Training Trillion Parameter Models.â€</span> <em>Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis (SC)</em>. <a href="https://doi.org/10.5555/3433701.3433721">https://doi.org/10.5555/3433701.3433721</a>.
</div><div id="ref-Zheng2020" class="csl-entry" role="listitem">
Zheng, Lianmin, Ziheng Jia, Yida Gao, Jiacheng Lin, Song Han, Xuehai Geng, Eric Zhao, and Tianqi Wu. 2020. <span>â€œAnsor: Generating High-Performance Tensor Programs for Deep Learning.â€</span> <em>USENIX Symposium on Operating Systems Design and Implementation (OSDI)</em>, 863â€“79.
</div><div id="ref-Jia2019" class="csl-entry" role="listitem">
Jia, Ziheng, Nathan Tillman, Luis Vega, Po-An Ouyang, Matei Zaharia, and Joseph E. Gonzalez. 2019. <span>â€œOptimizing DNN Computation with Relaxed Graph Substitutions.â€</span> <em>Conference on Machine Learning and Systems (MLSys)</em>.
</div></div><p>AI accelerators achieve high performance through massive parallelism, but without an effective scheduling strategy, computational units may sit idle, memory bandwidth may be underutilized, and execution efficiency may degrade. Computation scheduling is responsible for ensuring that all processing elements remain active, execution dependencies are managed correctly, and workloads are distributed optimally <span class="citation" data-cites="Jia2019">(<a href="#ref-Jia2019" role="doc-biblioref">Ziheng Jia et al. 2019</a>)</span>.</p>
<p>In the scheduling phase, parallel execution, synchronization, and resource allocation are managed systematically. Task partitioning decomposes extensive computations into smaller, manageable tasks that can be distributed efficiently among multiple compute cores. Execution order optimization then determines the most effective sequence for launching these operations, maximizing hardware performance while reducing execution stalls. Additionally, resource allocation and synchronization are orchestrated to ensure that compute cores, memory bandwidth, and shared caches are utilized effectively, avoiding contention. Through these coordinated strategies, computation scheduling achieves optimal hardware utilization, minimizes memory access delays, and supports a streamlined and efficient execution process.</p>
<section id="sec-ai-acceleration-implementation-ai-compilers-ff25" class="level4 page-columns page-full"><h4 class="anchored" data-anchor-id="sec-ai-acceleration-implementation-ai-compilers-ff25">Implementation in AI Compilers</h4>
<p>Computation scheduling is highly dependent on the underlying hardware architecture, as different AI accelerators have unique execution models that must be considered when determining how workloads are scheduled. AI compilers implement several key strategies to optimize scheduling for efficient execution.</p>
<p>One of the most fundamental aspects of scheduling is task partitioning, where the compiler divides large computational graphs into smaller, manageable units that can be executed in parallel. On GPUs, this typically means mapping matrix multiplications and convolutions to thousands of CUDA cores, while on TPUs, tasks are partitioned to fit within systolic arrays that operate on structured data flows <span class="citation" data-cites="norrie2021design">(<a href="#ref-norrie2021design" role="doc-biblioref">Norrie et al. 2021</a>)</span>. In CPUs, partitioning is often focused on breaking computations into vectorized chunks that align with SIMD execution. The goal is to map workloads to available processing units efficiently, ensuring that each core remains active throughout execution.</p>
<div class="no-row-height column-margin column-container"><div id="ref-norrie2021design" class="csl-entry" role="listitem">
Norrie, Thomas, Nishant Patil, Doe Hyun Yoon, George Kurian, Sheng Li, James Laudon, Cliff Young, Norman Jouppi, and David Patterson. 2021. <span>â€œThe Design Process for Googleâ€™s Training Chips: TPUv2 and TPUv3.â€</span> <em>IEEE Micro</em> 41 (2): 56â€“63. <a href="https://doi.org/10.1109/mm.2021.3058217">https://doi.org/10.1109/mm.2021.3058217</a>.
</div><div id="ref-Shoeybi2019" class="csl-entry" role="listitem">
â€”â€”â€”. 2019b. <span>â€œMegatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism.â€</span> <em>arXiv Preprint arXiv:1909.08053</em>, September. <a href="http://arxiv.org/abs/1909.08053v4">http://arxiv.org/abs/1909.08053v4</a>.
</div></div><p>Scheduling involves optimizing execution order to minimize dependencies and maximize throughput beyond task partitioning. Many AI models include operations that can be computed independently (e.g., different batches in a batch processing pipeline) alongside operations that have strict dependencies (e.g., recurrent layers in an RNN). AI compilers analyze these dependencies and attempt to rearrange execution where possible, reducing idle time and improving parallel efficiency. For example, in Transformer models, scheduling may prioritize preloading attention matrices into memory while earlier layers are still executing, ensuring that data is ready when needed <span class="citation" data-cites="Shoeybi2019">(<a href="#ref-Shoeybi2019" role="doc-biblioref">Shoeybi et al. 2019b</a>)</span>.</p>
<p>Another crucial aspect of computation scheduling is resource allocation and synchronization, where the compiler determines how compute cores share memory and coordinate execution. Modern AI accelerators often support overlapping computation and data transfers, meaning that while one task executes, the next task can begin fetching its required data. Compilers take advantage of this by scheduling tasks in a way that hides memory latency, ensuring that execution remains compute-bound rather than memory-bound <span class="citation" data-cites="Chen2018">(<a href="#ref-Chen2018" role="doc-biblioref">0001 et al. 2018b</a>)</span>. TensorRT and XLA, for example, employ streaming execution strategies where multiple kernels are launched in parallel, and synchronization is carefully managed to prevent execution stalls <span class="citation" data-cites="GoogleXLA">(<a href="#ref-GoogleXLA" role="doc-biblioref">Google 2025</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-Chen2018" class="csl-entry" role="listitem">
â€”â€”â€”, et al. 2018b. <span>â€œTVM: An Automated End-to-End Optimizing Compiler for Deep Learning.â€</span> In <em>OSDI</em>, 578â€“94. <a href="https://www.usenix.org/conference/osdi18/presentation/chen">https://www.usenix.org/conference/osdi18/presentation/chen</a>.
</div><div id="ref-GoogleXLA" class="csl-entry" role="listitem">
Google. 2025. <span>â€œXLA: Optimizing Compiler for Machine Learning.â€</span> <a href="https://tensorflow.org/xla">https://tensorflow.org/xla</a>.
</div></div></section><section id="sec-ai-acceleration-computation-scheduling-importance-04a1" class="level4"><h4 class="anchored" data-anchor-id="sec-ai-acceleration-computation-scheduling-importance-04a1">Computation Scheduling Importance</h4>
<p>Without effective scheduling, even the most optimized model can suffer from underutilized compute resources, memory bottlenecks, and execution inefficiencies. Poor scheduling decisions can lead to idle processing elements, forcing expensive compute cores to wait for data or synchronization events before continuing execution.</p>
<p>For instance, a CNN running on a GPU may have highly optimized kernels and efficient memory layouts, but if its execution is not scheduled correctly, compute units may remain idle between kernel launches, reducing throughput. Similarly, a Transformer model deployed on a TPU may perform matrix multiplications efficiently but could experience performance degradation if attention layers are not scheduled to overlap efficiently with memory transfers.</p>
<p>Effective computation scheduling occupies a central role in the orchestration of parallel workloads, ensuring that processing elements are utilized to their fullest capacity while preventing idle coresâ€”a critical aspect for maximizing overall throughput. By strategically overlapping computation with data movement, the scheduling mechanism effectively conceals memory latency, thereby preventing operational stalls during data retrieval. By resolving execution dependencies with precision, it minimizes waiting periods and enhances the concurrent progression of computation and data transfer. This systematic integration of scheduling and data handling serves to not only elevate performance but also exemplify the rigorous engineering principles that underpin modern accelerator design.</p>
</section><section id="sec-ai-acceleration-code-generation-85c8" class="level4 page-columns page-full"><h4 class="anchored" data-anchor-id="sec-ai-acceleration-code-generation-85c8">Code Generation</h4>
<p>Unlike the previous phases, which required AI-specific optimizations, code generation follows many of the same principles as traditional compilers. This process includes instruction selection, register allocation, and final optimization passes, ensuring that execution makes full use of hardware-specific features such as vectorized execution, memory prefetching, and instruction reordering.</p>
<p>For CPUs and GPUs, AI compilers typically generate machine code or optimized assembly instructions, while for TPUs, FPGAs<a href="#fn30" class="footnote-ref" id="fnref30" role="doc-noteref"><sup>30</sup></a>, and other accelerators, the output may be optimized bytecode or execution graphs that are interpreted by the hardwareâ€™s runtime system.</p>
<div class="no-row-height column-margin column-container"><div id="fn30"><p><sup>30</sup>&nbsp;<strong>FPGA (Field-Programmable Gate Array)</strong>: Reconfigurable hardware containing programmable logic blocks and routing that can implement custom digital circuits after manufacturing. Unlike fixed ASICs, FPGAs can be reprogrammed for different algorithms, offering flexibility between software and hardware efficiency. Intelâ€™s FPGA-based AI chips achieve 2-10<span class="math inline">\(\times\)</span> better performance per watt than GPUs for specific workloads, but require specialized hardware description languages (Verilog/VHDL) and longer development cycles, limiting adoption compared to GPU programming.</p></div></div><p>At this point, the compilation pipeline is complete: the original high-level model representation has been transformed into an optimized, executable format tailored for efficient execution on the target hardware. The combination of graph transformations, kernel selection, memory-aware execution, and parallel scheduling ensures that AI accelerators run workloads with maximum efficiency, minimal memory overhead, and optimal computational throughput.</p>
</section></section><section id="sec-ai-acceleration-compilationruntime-support-0206" class="level3"><h3 class="anchored" data-anchor-id="sec-ai-acceleration-compilationruntime-support-0206">Compilation-Runtime Support</h3>
<p>The compiler plays a fundamental role in AI acceleration, transforming high-level machine learning models into optimized execution plans tailored to the constraints of specialized hardware. Throughout this section, we have seen how graph optimization restructures computation, kernel selection maps operations to hardware-efficient implementations, memory planning optimizes data placement, and computation scheduling ensures efficient parallel execution. Each of these phases is crucial in enabling AI models to fully leverage modern accelerators, ensuring high throughput, minimal memory overhead, and efficient execution pipelines.</p>
<p>However, compilation alone is not enough to guarantee efficient execution in real-world AI workloads. While compilers statically optimize computation based on known model structures and hardware capabilities, AI execution environments are often dynamic and unpredictable. Batch sizes fluctuate, hardware resources may be shared across multiple workloads, and accelerators must adapt to real-time performance constraints. In these cases, a static execution plan is insufficient, and runtime management becomes critical in ensuring that models execute optimally under real-world conditions.</p>
<p>This transition from static compilation to adaptive execution is where AI runtimes come into play. Runtimes provide dynamic memory allocation, real-time kernel selection, workload scheduling, and multi-chip coordination, allowing AI models to adapt to varying execution conditions while maintaining efficiency. In the next section, we explore how AI runtimes extend the capabilities of compilers, enabling models to run effectively in diverse and scalable deployment scenarios.</p>
<div id="quiz-question-sec-ai-acceleration-compiler-support-172e" class="callout callout-quiz-question">
<details class="callout-quiz-question fbx-default closebutton"><summary><strong>Self-Check: Question 1.7</strong></summary><div>
<ol type="1">
<li>
<p>Which of the following is a primary focus of machine learning compilers compared to traditional compilers?</p>
<ol type="a">
<li>Graph transformations and kernel fusion</li>
<li>Instruction scheduling and register allocation</li>
<li>Loop unrolling and memory allocation</li>
<li>Sequential program optimization</li>
</ol>
</li>
<li><p>Explain why kernel fusion is important in machine learning compilers.</p></li>
<li><p>Order the following stages in the ML compilation pipeline: (1) Graph Optimization, (2) Memory Planning, (3) Kernel Selection, (4) Computation Scheduling.</p></li>
<li>
<p>In a production system, what trade-offs might you consider when selecting kernels for ML model execution?</p>
<ol type="a">
<li>Precision versus performance</li>
<li>All of the above</li>
<li>Execution speed versus memory usage</li>
<li>Power consumption versus accuracy</li>
</ol>
</li>
</ol>
<p><a href="#quiz-answer-sec-ai-acceleration-compiler-support-172e" class="question-label">See Answers â†’</a></p>
</div></details>
</div>
</section></section><section id="sec-ai-acceleration-runtime-support-f94f" class="level2 page-columns page-full"><h2 class="anchored" data-anchor-id="sec-ai-acceleration-runtime-support-f94f">Runtime Support</h2>
<p>While compilers optimize AI models before execution, real-world deployment introduces dynamic and unpredictable conditions that static compilation alone cannot fully address <span class="citation" data-cites="nvidia_tensorRT_2021">(<a href="#ref-nvidia_tensorRT_2021" role="doc-biblioref">NVIDIA 2021</a>)</span>. AI workloads operate in varied execution environments, where factors such as fluctuating batch sizes, shared hardware resources, memory contention, and latency constraints necessitate real-time adaptation. Precompiled execution plans, optimized for a fixed set of assumptions, may become suboptimal when actual runtime conditions change.</p>
<div class="no-row-height column-margin column-container"><div id="ref-nvidia_tensorRT_2021" class="csl-entry" role="listitem">
NVIDIA. 2021. <span>â€œTensorRT: High-Performance Deep Learning Inference Library.â€</span> <em>NVIDIA Developer Blog</em>. <a href="https://developer.nvidia.com/tensorrt">https://developer.nvidia.com/tensorrt</a>.
</div></div><p>To bridge this gap, AI runtimes provide a dynamic layer of execution management, extending the optimizations performed at compile time with real-time decision-making. Unlike traditional compiled programs that execute a fixed sequence of instructions, AI workloads require adaptive control over memory allocation, kernel execution, and resource scheduling. AI runtimes continuously monitor execution conditions and make on-the-fly adjustments to ensure that machine learning models fully utilize available hardware while maintaining efficiency and performance guarantees.</p>
<p>At a high level, AI runtimes manage three critical aspects of execution:</p>
<ol type="1">
<li>
<strong>Kernel Execution Management</strong>: AI runtimes dynamically select and dispatch computation kernels based on the current system state, ensuring that workloads are executed with minimal latency.</li>
<li>
<strong>Memory Adaptation and Allocation</strong>: Since AI workloads frequently process large tensors with varying memory footprints, runtimes adjust memory allocation dynamically to prevent bottlenecks and excessive data movement <span class="citation" data-cites="deepmind_gpipe_2019">(<a href="#ref-deepmind_gpipe_2019" role="doc-biblioref">Huang et al. 2019</a>)</span>.</li>
<li>
<strong>Execution Scaling</strong>: AI runtimes handle workload distribution across multiple accelerators, supporting large-scale execution in multi-chip, multi-node, or cloud environments <span class="citation" data-cites="mirhoseini_device_placement_2017">(<a href="#ref-mirhoseini_device_placement_2017" role="doc-biblioref">Mirhoseini et al. 2017</a>)</span>.</li>
</ol>
<div class="no-row-height column-margin column-container"><div id="ref-deepmind_gpipe_2019" class="csl-entry" role="listitem">
Huang, Yanping et al. 2019. <span>â€œGPipe: Efficient Training of Giant Neural Networks Using Pipeline Parallelism.â€</span> In <em>Advances in Neural Information Processing Systems (NeurIPS)</em>.
</div><div id="ref-mirhoseini_device_placement_2017" class="csl-entry" role="listitem">
Mirhoseini, Azalia et al. 2017. <span>â€œDevice Placement Optimization with Reinforcement Learning.â€</span> <em>International Conference on Machine Learning (ICML)</em>.
</div></div><p>By dynamically handling these execution aspects, AI runtimes complement compiler-based optimizations, ensuring that models continue to perform efficiently under varying runtime conditions. The next section explores how AI runtimes differ from traditional software runtimes, highlighting why machine learning workloads require fundamentally different execution strategies compared to conventional CPU-based programs.</p>
<section id="sec-ai-acceleration-runtime-architecture-differences-ml-systems-932e" class="level3"><h3 class="anchored" data-anchor-id="sec-ai-acceleration-runtime-architecture-differences-ml-systems-932e">Runtime Architecture Differences for ML Systems</h3>
<p>Traditional software runtimes are designed for managing general-purpose program execution, primarily handling sequential and multi-threaded workloads on CPUs. These runtimes allocate memory, schedule tasks, and optimize execution at the level of individual function calls and instructions. In contrast, AI runtimes are specialized for machine learning workloads, which require massively parallel computation, large-scale tensor operations, and dynamic memory management.</p>
<p><a href="#tbl-runtime-comparison" class="quarto-xref">Table&nbsp;19</a> highlights the fundamental differences between traditional and AI runtimes. One of the key distinctions lies in execution flow. Traditional software runtimes operate on a predictable, structured execution model where function calls and CPU threads follow a predefined control path. AI runtimes, however, execute computational graphs, requiring complex scheduling decisions that account for dependencies between tensor operations, parallel kernel execution, and efficient memory access.</p>
<div id="tbl-runtime-comparison" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure"><figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-runtime-comparison-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;19: <strong>Runtime Execution Models</strong>: Traditional and AI runtimes diverge in their execution approaches; traditional runtimes prioritize sequential or multi-threaded instruction processing, while AI runtimes leverage massively parallel tensor operations for accelerated computation on machine learning workloads. This distinction necessitates specialized AI runtime architectures designed for efficient parallelization and memory management of large-scale tensor data.
</figcaption><div aria-describedby="tbl-runtime-comparison-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<colgroup>
<col style="width: 23%">
<col style="width: 31%">
<col style="width: 44%">
</colgroup>
<thead><tr class="header">
<th style="text-align: left;"><strong>Aspect</strong></th>
<th style="text-align: left;"><strong>Traditional Runtime</strong></th>
<th style="text-align: left;"><strong>AI Runtime</strong></th>
</tr></thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><strong>Execution Model</strong></td>
<td style="text-align: left;">Sequential or multi-threaded execution</td>
<td style="text-align: left;">Massively parallel tensor execution</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Task Scheduling</strong></td>
<td style="text-align: left;">CPU thread management</td>
<td style="text-align: left;">Kernel dispatch across accelerators</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>Memory Management</strong></td>
<td style="text-align: left;">Static allocation (stack/heap)</td>
<td style="text-align: left;">Dynamic tensor allocation, buffer reuse</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Optimization Priorities</strong></td>
<td style="text-align: left;">Low-latency instruction execution</td>
<td style="text-align: left;">Minimizing memory stalls, maximizing parallel execution</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>Adaptability</strong></td>
<td style="text-align: left;">Mostly static execution plan</td>
<td style="text-align: left;">Adapts to batch size and hardware availability</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Target Hardware</strong></td>
<td style="text-align: left;">CPUs (general-purpose execution)</td>
<td style="text-align: left;">GPUs, TPUs, and custom accelerators</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p>Memory management is another major differentiator. Traditional software runtimes handle small, frequent memory allocations, optimizing for cache efficiency and low-latency access. AI runtimes, in contrast, must dynamically allocate, reuse, and optimize large tensors, ensuring that memory access patterns align with accelerator-friendly execution. Poor memory management in AI workloads can lead to performance bottlenecks, particularly due to excessive off-chip memory transfers and inefficient cache usage.</p>
<p>AI runtimes are inherently designed for adaptability. While traditional runtimes often follow a mostly static execution plan, AI workloads typically operate in highly variable execution environments, such as cloud-based accelerators or multi-tenant hardware. As a result, AI runtimes must continuously adjust batch sizes, reallocate compute resources, and manage real-time scheduling decisions to maintain high throughput and minimize execution delays.</p>
<p>These distinctions demonstrate why AI runtimes require fundamentally different execution strategies compared to traditional software runtimes. Rather than simply managing CPU processes, AI runtimes must oversee large-scale tensor execution, multi-device coordination, and real-time workload adaptation to ensure that machine learning models can run efficiently under diverse and ever-changing deployment conditions.</p>
</section><section id="sec-ai-acceleration-dynamic-kernel-execution-33fc" class="level3"><h3 class="anchored" data-anchor-id="sec-ai-acceleration-dynamic-kernel-execution-33fc">Dynamic Kernel Execution</h3>
<p>Dynamic kernel execution is the process of mapping machine learning models to hardware and optimizing runtime execution. While static compilation provides a solid foundation, efficient execution of machine learning workloads requires real-time adaptation to fluctuating conditions such as available memory, data sizes, and computational loads. The runtime functions as an intermediary that continuously adjusts execution strategies to match both the constraints of the underlying hardware and the characteristics of the workload.</p>
<p>When mapping a machine learning model to hardware, individual computational operations, including matrix multiplications, convolutions, and activation functions, must be assigned to the most appropriate processing units. This mapping is not fixed; it must be modified during runtime in response to changes in input data, memory availability, and overall system load. Dynamic kernel execution allows the runtime to make real-time decisions regarding kernel selection, execution order, and memory management, ensuring that workloads remain efficient despite these changing conditions.</p>
<p>For example, consider an AI accelerator executing a deep neural network (DNN) for image classification. If an incoming batch of high-resolution images requires significantly more memory than expected, a statically planned execution may cause cache thrashing or excessive off-chip memory accesses. Instead, a dynamic runtime can adjust tiling strategies on the fly, breaking down tensor operations into smaller tiles that fit within the high-speed on-chip memory. This prevents memory stalls and ensures optimal utilization of caches.</p>
<p>Similarly, when running a transformer-based NLP model, the sequence length of input text may vary between inference requests. A static execution plan optimized for a fixed sequence length may lead to underutilization of compute resources when processing shorter sequences or excessive memory pressure with longer sequences. Dynamic kernel execution can mitigate this by selecting different kernel implementations based on the actual sequence length, dynamically adjusting memory allocations and execution strategies to maintain efficiency.</p>
<p>Overlapping computation with memory movement is a vital strategy to mitigate performance bottlenecks. AI workloads often encounter delays due to memory-bound issues, where data movement between memory hierarchies limits computation speed. To combat this, AI runtimes implement techniques like asynchronous execution and double buffering, ensuring that computations proceed without waiting for memory transfers to complete. In a large-scale model, for instance, image data can be prefetched while computations are performed on the previous batch, thus maintaining a steady flow of data and avoiding pipeline stalls.</p>
<p>Another practical example is the execution of convolutional layers in a CNN on a GPU. If multiple convolution kernels need to be scheduled, a static scheduling approach may lead to inefficient resource utilization due to variation in layer sizes and compute requirements. By dynamically scheduling kernel execution, AI runtimes can prioritize smaller kernels when compute units are partially occupied, improving hardware utilization. For instance, in NVIDIAâ€™s TensorRT runtime, fusion of small kernels into larger execution units is done dynamically to avoid launch overhead, optimizing latency-sensitive inference tasks.</p>
<p>Dynamic kernel execution plays an essential role in ensuring that machine learning models are executed efficiently. By dynamically adjusting execution strategies in response to real-time system conditions, AI runtimes optimize both training and inference performance across various hardware platforms.</p>
</section><section id="sec-ai-acceleration-runtime-kernel-selection-1ffe" class="level3 page-columns page-full"><h3 class="anchored" data-anchor-id="sec-ai-acceleration-runtime-kernel-selection-1ffe">Runtime Kernel Selection</h3>
<p>While compilers may perform an initial selection of kernels based on static analysis of the machine learning model and hardware target, AI runtimes often need to override these decisions during execution. Real-time factors, such as available memory, hardware utilization, and workload priorities, may differ significantly from the assumptions made during compilation. By dynamically selecting and switching kernels at runtime, AI runtimes can adapt to these changing conditions, ensuring that models continue to perform efficiently.</p>
<p>For instance, consider transformer-based language models, where a significant portion of execution time is spent on matrix multiplications. The AI runtime must determine the most efficient way to execute these operations based on the current system state. If the model is running on a GPU with specialized Tensor Cores, the runtime may switch from a standard FP32 kernel to an FP16 kernel to take advantage of hardware acceleration <span class="citation" data-cites="shoeybi_megatron_2020">(<a href="#ref-shoeybi_megatron_2020" role="doc-biblioref">Shoeybi et al. 2019a</a>)</span>. Conversely, if the lower precision of FP16 causes unacceptable numerical instability, the runtime can opt for mixed-precision execution, selectively using FP32 where higher precision is necessary.</p>
<div class="no-row-height column-margin column-container"><div id="ref-shoeybi_megatron_2020" class="csl-entry" role="listitem">
Shoeybi, Mohammad, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. 2019a. <span>â€œMegatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism.â€</span> <em>arXiv Preprint arXiv:1909.08053</em>, September. <a href="http://arxiv.org/abs/1909.08053v4">http://arxiv.org/abs/1909.08053v4</a>.
</div></div><p>Memory constraints also influence kernel selection. When memory bandwidth is limited, the runtime may adjust its execution strategy, reordering operations or changing the tiling strategy to fit computations into the available cache rather than relying on slower main memory. For example, a large matrix multiplication may be broken into smaller chunks, ensuring that the computation fits into the on-chip memory of the GPU, reducing overall latency.</p>
<p>Additionally, batch size can influence kernel selection. For workloads that handle a mix of small and large batches, the AI runtime may choose a latency-optimized kernel for small batches and a throughput-optimized kernel for large-scale batch processing. This adjustment ensures that the model continues to operate efficiently across different execution scenarios, without the need for manual tuning.</p>
</section><section id="sec-ai-acceleration-kernel-scheduling-utilization-99d6" class="level3 page-columns page-full"><h3 class="anchored" data-anchor-id="sec-ai-acceleration-kernel-scheduling-utilization-99d6">Kernel Scheduling and Utilization</h3>
<p>Once the AI runtime selects an appropriate kernel, the next step is scheduling it in a way that maximizes parallelism and resource utilization. Unlike traditional task schedulers, which are designed to manage CPU threads, AI runtimes must coordinate a much larger number of tasks across parallel execution units such as GPU cores, tensor processing units, or custom AI accelerators <span class="citation" data-cites="google_tpu_2017">(<a href="#ref-google_tpu_2017" role="doc-biblioref">Norman P. Jouppi et al. 2017a</a>)</span>. Effective scheduling ensures that these computational resources are kept fully engaged, preventing bottlenecks and maximizing throughput.</p>
<div class="no-row-height column-margin column-container"><div id="ref-google_tpu_2017" class="csl-entry" role="listitem">
Jouppi, Norman P et al. 2017a. <span>â€œIn-Datacenter Performance Analysis of a Tensor Processing Unit.â€</span> <em>Proceedings of the 44th Annual International Symposium on Computer Architecture (ISCA)</em>.
</div></div><p>For example, in image recognition models that use convolutional layers, operations can be distributed across multiple processing units, enabling different filters to run concurrently. This parallelization ensures that the available hardware is fully utilized, speeding up execution. Similarly, batch normalization and activation functions must be scheduled efficiently to avoid unnecessary delays. If these operations are not interleaved with other computations, they may block the pipeline and reduce overall throughput.</p>
<p>Efficient kernel scheduling can also be influenced by real-time memory management . AI runtimes ensure that intermediate data, such as feature maps in deep neural networks, are preloaded into cache before they are needed. This proactive management helps prevent delays caused by waiting for data to be loaded from slower memory tiers, ensuring continuous execution.</p>
<p>These techniques enable AI runtimes to ensure optimal resource utilization and efficient parallel computation, which are essential for the high-performance execution of machine learning models, particularly in environments that require scaling across multiple hardware accelerators.</p>
<p>The compiler and runtime systems examined thus far optimize execution within single acceleratorsâ€”managing computation mapping, memory hierarchies, and kernel scheduling. While these single-chip optimizations achieve impressive performance gains, modern AI workloads increasingly exceed what any individual chip can deliver. Training GPT-3 would require running a single H100 continuously for 10 years, consuming 314 sextillion floating-point operations. Real-time inference serving for global applications demands throughput beyond any single acceleratorâ€™s capacity. These computational requirements, rooted in the scaling laws from <strong><a href="../efficient_ai/efficient_ai.html#sec-efficient-ai">Chapter 9: Efficient AI</a></strong>, necessitate a fundamental shift from single-chip optimization to distributed acceleration strategies.</p>
<div id="quiz-question-sec-ai-acceleration-runtime-support-f94f" class="callout callout-quiz-question">
<details class="callout-quiz-question fbx-default closebutton"><summary><strong>Self-Check: Question 1.8</strong></summary><div>
<ol type="1">
<li>
<p>Which of the following best describes a key function of AI runtimes in machine learning systems?</p>
<ol type="a">
<li>Static memory allocation</li>
<li>Sequential task execution</li>
<li>Dynamic kernel execution management</li>
<li>Fixed execution plans</li>
</ol>
</li>
<li><p>How do AI runtimes differ from traditional software runtimes in terms of memory management?</p></li>
<li><p>Order the following tasks in AI runtime management: (1) Memory adaptation, (2) Kernel execution management, (3) Execution scaling.</p></li>
<li>
<p>In a production system, what might be a consequence of poor dynamic kernel execution management?</p>
<ol type="a">
<li>Improved parallel execution</li>
<li>Increased latency and resource underutilization</li>
<li>Reduced memory requirements</li>
<li>Enhanced sequential processing</li>
</ol>
</li>
</ol>
<p><a href="#quiz-answer-sec-ai-acceleration-runtime-support-f94f" class="question-label">See Answers â†’</a></p>
</div></details>
</div>
</section></section><section id="sec-ai-acceleration-multichip-ai-acceleration-38d7" class="level2 page-columns page-full"><h2 class="anchored" data-anchor-id="sec-ai-acceleration-multichip-ai-acceleration-38d7">Multi-Chip AI Acceleration</h2>
<p>The transition from single-chip to multi-chip architectures represents more than simple replicationâ€”it requires rethinking how computations distribute across processors, how data flows between chips, and how systems maintain coherence at scale. Where single-chip optimization focuses on maximizing utilization within fixed resources, multi-chip systems must balance computational distribution against communication overhead, memory coherence costs, and synchronization complexity. These challenges fundamentally transform the optimization landscape, requiring new abstractions and techniques beyond those developed for individual accelerators.</p>
<p>Modern AI workloads increasingly demand computational resources that exceed the capabilities of single-chip accelerators. This section examines how AI systems scale from individual processors to multi-chip architectures, analyzing the motivation behind different scaling approaches and their impact on system design. These scaling considerations are fundamental to the distributed training strategies covered in <strong><a href="../training/training.html#sec-ai-training">Chapter 8: AI Training</a></strong> and the operational challenges discussed in <strong><a href="../ops/ops.html#sec-ml-operations">Chapter 13: ML Operations</a></strong>. The security implications of distributed acceleration, particularly around model protection and data privacy, are examined in <strong><a href="../privacy_security/privacy_security.html#sec-security-privacy">Chapter 15: Security & Privacy</a></strong>. By understanding this progression, we can better appreciate how each component of the AI hardware stack, ranging from compute units to memory systems, must adapt to support large-scale machine learning workloads.</p>
<p>The scaling of AI systems follows a natural progression, starting with integration within a single package through chiplet architectures, extending to multi-GPU configurations within a server, expanding to distributed accelerator pods, and culminating in wafer-scale integration. Each approach presents unique trade-offs between computational density, communication overhead, and system complexity. For instance, chiplet architectures maintain high-speed interconnects within a package, while distributed systems sacrifice communication latency for massive parallelism.</p>
<p>Understanding these scaling strategies is essential for several reasons. First, it provides insight into how different hardware architectures address the growing computational demands of AI workloads. Second, it reveals the fundamental challenges that arise when extending beyond single-chip execution, such as managing inter-chip communication and coordinating distributed computation. Finally, it establishes the foundation for subsequent discussions on how mapping strategies, compilation techniques, and runtime systems evolve to support efficient execution at scale.</p>
<p>The progression begins with chiplet architectures, which represent the most tightly integrated form of multi-chip scaling.</p>
<section id="sec-ai-acceleration-chipletbased-architectures-a890" class="level3 page-columns page-full"><h3 class="anchored" data-anchor-id="sec-ai-acceleration-chipletbased-architectures-a890">Chiplet-Based Architectures</h3>
<p>Chiplet<a href="#fn31" class="footnote-ref" id="fnref31" role="doc-noteref"><sup>31</sup></a> architectures achieve this scaling by partitioning large designs into smaller, modular dies that are interconnected within a single package, as illustrated in <a href="#fig-AMD_chiplet_based" class="quarto-xref">Figure&nbsp;9</a>.</p>
<div class="no-row-height column-margin column-container"><div id="fn31"><p><sup>31</sup>&nbsp;<strong>Chiplet</strong>: Small, specialized semiconductor dies that are connected together within a single package to create larger, more complex processors. AMDâ€™s EPYC processors use up to 8 chiplets connected via Infinity Fabric, achieving yields above 80% versus 20% for equivalent monolithic designs. This modular approach reduces manufacturing costs and enables mixing different technologiesâ€”compute chiplets in 7&nbsp;nm with I/O chiplets in 14&nbsp;nmâ€”optimizing each function independently.</p></div></div><div id="fig-AMD_chiplet_based" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure"><div aria-describedby="fig-AMD_chiplet_based-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="images/png/chiplets.png" class="lightbox" data-gallery="quarto-lightbox-gallery-9" title="Figure&nbsp;9: Chiplet Interconnect: Modern AI accelerators partition large designs into smaller chiplets and connect them via high-bandwidth interconnects, enabling scalability beyond monolithic die limitations and improving manufacturing yields. HBM stacks provide fast access to data, crucial for the memory-intensive workloads common in machine learning."><img src="images/png/chiplets.png" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-AMD_chiplet_based-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;9: <strong>Chiplet Interconnect</strong>: Modern AI accelerators partition large designs into smaller chiplets and connect them via high-bandwidth interconnects, enabling scalability beyond monolithic die limitations and improving manufacturing yields. HBM stacks provide fast access to data, crucial for the memory-intensive workloads common in machine learning.
</figcaption></figure>
</div>
<p>Modern AI accelerators, such as AMDâ€™s Instinct MI300, take this approach by integrating multiple compute chiplets alongside memory chiplets, linked by high-speed die-to-die interconnects. This modular design allows manufacturers to bypass the manufacturing limits of monolithic chips while still achieving high-density compute.</p>
<p>However, even within a single package, scaling is not without challenges. Inter-chiplet communication latency, memory coherence<a href="#fn32" class="footnote-ref" id="fnref32" role="doc-noteref"><sup>32</sup></a>, and thermal management become critical factors as more chiplets are integrated. Unlike traditional multi-chip systems, chiplet-based designs must carefully balance latency-sensitive workloads across multiple dies without introducing excessive bottlenecks.</p>
<div class="no-row-height column-margin column-container"><div id="fn32"><p><sup>32</sup>&nbsp;<strong>Memory Coherence</strong>: Ensuring all processors in a system see the same consistent view of shared memory when multiple cores/chips access the same data. Traditional cache coherence protocols like MESI add 10-50&nbsp;ns latency for multi-core CPUs. For AI accelerators with thousands of cores, coherence becomes prohibitively expensiveâ€”most ML hardware instead uses explicit memory management where programmers control data placement and synchronization manually.</p></div></div></section><section id="sec-ai-acceleration-multigpu-systems-f017" class="level3 page-columns page-full"><h3 class="anchored" data-anchor-id="sec-ai-acceleration-multigpu-systems-f017">Multi-GPU Systems</h3>
<p>Beyond chiplet-based designs, AI workloads often require multiple discrete GPUs working together. In multi-GPU systems, each accelerator has its own dedicated memory and compute resources, but they must efficiently share data and synchronize execution.</p>
<p>A common example is NVIDIA DGX systems, which integrate multiple GPUs connected via NVLink or PCIe. This architecture enables workloads to be split across GPUs, typically using data parallelism (where each GPU processes a different batch of data) or model parallelism (where different GPUs handle different parts of a neural network) <span class="citation" data-cites="Ben-Nun2019data">(<a href="#ref-Ben-Nun2019data" role="doc-biblioref">Ben-Nun and Hoefler 2019</a>)</span>. These parallelization strategies are explored in depth in <strong><a href="../training/training.html#sec-ai-training">Chapter 8: AI Training</a></strong>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-Ben-Nun2019data" class="csl-entry" role="listitem">
Ben-Nun, Tal, and Torsten Hoefler. 2019. <span>â€œDemystifying Parallel and Distributed Deep Learning: An in-Depth Concurrency Analysis.â€</span> <em>ACM Computing Surveys</em> 52 (4): 1â€“43. <a href="https://doi.org/10.1145/3320060">https://doi.org/10.1145/3320060</a>.
</div></div><p>As illustrated in <a href="#fig-multi-gpu" class="quarto-xref">Figure&nbsp;10</a>, NVSwitch interconnects enable high-speed communication between GPUs, reducing bottlenecks in distributed training. However, scaling up the number of GPUs introduces fundamental distributed coordination challenges that become the dominant performance constraint. The arithmetic intensity of transformer training (0.5-2 FLOPS/byte) forces frequent gradient synchronization across GPUs, where AllReduce operations must aggregate 175 billion parameters in GPT-3 scale models. NVSwitch provides 600 GB/s bidirectional bandwidth, but even this substantial interconnect becomes bandwidth-bound when 8 H100 GPUs simultaneously exchange gradients, creating a 4.8 TB/s aggregate demand that exceeds available capacity. The coordination complexity compounds exponentiallyâ€”while two GPUs require a single communication channel, eight GPUs need 28 interconnect paths, and fault tolerance requirements mandate redundant communication patterns. Memory consistency protocols further complicate coordination as different GPUs may observe weight updates at different times, requiring sophisticated synchronization primitives that can add 10-50Î¼s latency per training stepâ€”seemingly small delays that aggregate to hours of training time across million-iteration runs.</p>
<div id="fig-multi-gpu" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-pos="htb" data-fig-env="figure">
<figure class="quarto-float quarto-float-fig figure"><div aria-describedby="fig-multi-gpu-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="98ab56e9be393ccdfd3a8bbee67360ac9fd51a2e.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-10" title="Figure&nbsp;10: Multi-GPU Scaling: NVSwitch interconnects enable high-bandwidth, low-latency communication between GPUs, overcoming PCIe bottlenecks for distributed training of large models. Scaling GPU count introduces challenges in maintaining memory consistency and efficiently scheduling workloads across interconnected devices."><img src="hw_acceleration_files/mediabag/98ab56e9be393ccdfd3a8bbee67360ac9fd51a2e.svg" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-multi-gpu-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;10: <strong>Multi-GPU Scaling</strong>: NVSwitch interconnects enable high-bandwidth, low-latency communication between GPUs, overcoming PCIe bottlenecks for distributed training of large models. Scaling GPU count introduces challenges in maintaining memory consistency and efficiently scheduling workloads across interconnected devices.
</figcaption></figure>
</div>
<section id="sec-ai-acceleration-communication-overhead-amdahls-law-analysis-8eb4" class="level4"><h4 class="anchored" data-anchor-id="sec-ai-acceleration-communication-overhead-amdahls-law-analysis-8eb4">Communication Overhead and Amdahlâ€™s Law Analysis</h4>
<p>The fundamental limitation of distributed AI training stems from Amdahlâ€™s Law, which quantifies how communication overhead constrains parallel speedup regardless of available compute power. For distributed neural network training, communication overhead during gradient synchronization creates a sequential bottleneck that limits scalability even with infinite parallelism.</p>
<p>The maximum speedup achievable with distributed training is bound by Amdahlâ€™s Law: <span class="math display">\[
\text{Speedup} = \frac{1}{(1-P) + \frac{P}{N}}
\]</span> where <span class="math inline">\(P\)</span> is the fraction of work that can be parallelized and <span class="math inline">\(N\)</span> is the number of processors. However, for AI training, communication overhead introduces additional sequential time: <span class="math display">\[
\text{Speedup}_{\text{AI}} = \frac{1}{(1-P) + \frac{P}{N} + \frac{C}{N}}
\]</span> where <span class="math inline">\(C\)</span> represents the communication overhead fraction.</p>
<p>Consider training a 175&nbsp;B parameter model with 1000 H100 GPUs as a concrete example:</p>
<ul>
<li>
<strong>Computation time per iteration</strong>: 100&nbsp;ms of forward/backward passes</li>
<li>
<strong>Communication time</strong>: AllReduce of 175&nbsp;B parameters (700&nbsp;GB in FP32) across 1000 GPUs</li>
<li>
<strong>Available bandwidth</strong>: 600 GB/s per NVSwitch link</li>
<li>
<strong>Communication overhead</strong>: <span class="math inline">\(\frac{700\text{GB}}{600\text{GB/s}} \times \log_2(1000) \approx 11.6\text{ms}\)</span>
</li>
</ul>
<p>Even if only 5% of training requires communication (P = 0.95), the maximum speedup is: <span class="math display">\[
\text{Speedup} = \frac{1}{0.05 + \frac{0.95}{1000} + \frac{0.116}{100}} \approx 8.3\text{x}
\]</span></p>
<p>This demonstrates why adding more GPUs beyond ~100 provides diminishing returns for large model training.</p>
<p>Communication requirements scale superlinearly with model size and linearly with the number of parameters. Modern transformer models require gradient synchronization across all parameters during each training step:</p>
<ul>
<li>
<strong>GPT-3 (175&nbsp;B parameters)</strong>: 700&nbsp;GB gradient exchange per step</li>
<li>
<strong>GPT-4 (estimated 1.8&nbsp;T parameters)</strong>: ~7&nbsp;TB gradient exchange per step</li>
<li>
<strong>Future 10&nbsp;T parameter models</strong>: ~40&nbsp;TB gradient exchange per step</li>
</ul>
<p>Even with advanced interconnects like NVLink 4.0 (1.8&nbsp;TB/s), gradient synchronization for 10&nbsp;T parameter models would require 22+ seconds per training step, making distributed training impractical without algorithmic innovations like gradient compression or asynchronous updates.</p>
<p>Multi-GPU systems face additional bottlenecks from memory bandwidth competition. When 8 H100 GPUs simultaneously access HBM during gradient computation, the effective memory bandwidth per GPU drops from 3.35&nbsp;TB/s to approximately 2.1&nbsp;TB/s due to memory controller contention and NUMA effects. This 37% reduction in memory performance compounds communication overhead, further limiting scalability.</p>
<p>Understanding Amdahlâ€™s Law guides optimization strategies:</p>
<ol type="1">
<li>
<strong>Gradient Compression</strong>: Reduce communication volume by 10-100<span class="math inline">\(\times\)</span> through sparsification and quantization</li>
<li>
<strong>Pipeline Parallelism</strong>: Overlap communication with computation to hide gradient synchronization latency</li>
<li>
<strong>Model Parallelism</strong>: Partition models across devices to reduce gradient synchronization requirements</li>
<li>
<strong>Asynchronous Updates</strong>: Relax consistency requirements to eliminate synchronization barriers</li>
</ol>
<p>These techniques modify the effective value of <span class="math inline">\(P\)</span> and <span class="math inline">\(C\)</span> in Amdahlâ€™s equation, enabling better scaling behavior at the cost of algorithmic complexity.</p>
</section></section><section id="sec-ai-acceleration-tpu-pods-fd33" class="level3 page-columns page-full"><h3 class="anchored" data-anchor-id="sec-ai-acceleration-tpu-pods-fd33">TPU Pods</h3>
<p>As models and datasets continue to expand, training and inference workloads must extend beyond single-server configurations. This scaling requirement has led to the development of sophisticated distributed systems where multiple accelerators communicate across networks. Googleâ€™s TPU Pods represent a pioneering approach to this challenge, interconnecting hundreds of TPUs to function as a unified system <span class="citation" data-cites="Jouppi2020tpuv4">(<a href="#ref-Jouppi2020tpuv4" role="doc-biblioref">Norman P. Jouppi et al. 2020</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-Jouppi2020tpuv4" class="csl-entry" role="listitem">
Jouppi, Norman P., Doe Hyun Yoon, George Kurian, Sheng Li, Nishant Patil, James Laudon, Cliff Young, and David Patterson. 2020. <span>â€œA Domain-Specific Supercomputer for Training Deep Neural Networks.â€</span> <em>Communications of the ACM</em> 63 (7): 67â€“78. <a href="https://doi.org/10.1145/3360307">https://doi.org/10.1145/3360307</a>.
</div></div><p>The architectural design of TPU Pods differs fundamentally from traditional multi-GPU systems. While multi-GPU configurations typically rely on NVLink or PCIe connections within a single machine, TPU Pods employ high-bandwidth optical links to interconnect accelerators at data center scale. This design implements a 2D torus interconnect topology, enabling efficient data exchange between accelerators while minimizing communication bottlenecks as workloads scale across nodes.</p>
<p>The effectiveness of this architecture is demonstrated in its performance scaling capabilities. As illustrated in <a href="#fig-tpu-pod-perf" class="quarto-xref">Figure&nbsp;11</a>, TPU Pod performance exhibits near-linear scaling when running ResNet-50, from quarter-pod to full-pod configurations. The system achieves a remarkable 33.0<span class="math inline">\(\times\)</span> speedup when scaled to 1024 chips compared to a 16-TPU baseline. This scaling efficiency is particularly noteworthy in larger configurations, where performance continues to scale strongly even as the system expands from 128 to 1024 chips.</p>
<div id="fig-tpu-pod-perf" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure"><div aria-describedby="fig-tpu-pod-perf-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="images/png/tpu-pod-perf.png" class="lightbox" data-gallery="quarto-lightbox-gallery-11" title="Figure&nbsp;11: Scaling Efficiency of TPU Pods: Increasing the number of TPU chips within a pod maintains near-linear performance gains on ResNet-50, achieving a 33.0\times speedup from 16 to 1024 chips. This efficient scaling provides the effectiveness of the 2D torus interconnect and high-bandwidth optical links in minimizing communication bottlenecks as workloads expand across multiple accelerators."><img src="images/png/tpu-pod-perf.png" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-tpu-pod-perf-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;11: <strong>Scaling Efficiency of TPU Pods</strong>: Increasing the number of TPU chips within a pod maintains near-linear performance gains on ResNet-50, achieving a 33.0<span class="math inline">\(\times\)</span> speedup from 16 to 1024 chips. This efficient scaling provides the effectiveness of the 2D torus interconnect and high-bandwidth optical links in minimizing communication bottlenecks as workloads expand across multiple accelerators.
</figcaption></figure>
</div>
<p>However, distributing AI workloads across an entire data center introduces distributed coordination challenges that fundamentally differ from single-node systems. The 2D torus interconnect, while providing high bisection bandwidth, creates communication bottlenecks when training large transformer models that require AllReduce operations across all 1,024 TPUs. Each parameter gradient must traverse multiple hops through the torus network, with worst-case communication requiring 32 hops between distant TPUs, creating latency penalties that compound with model size.</p>
<p>The distributed memory architecture exacerbates coordination complexityâ€”unlike multi-GPU systems with shared host memory, each TPU node maintains independent memory spaces, forcing explicit data marshaling and synchronization protocols. Network partition tolerance becomes critical as optical link failures can split the pod into disconnected islands, requiring sophisticated consensus algorithms to maintain training consistency.</p>
<p>The energy cost of coordination also scales dramatically: moving data across the podâ€™s optical interconnect consumes 1000<span class="math inline">\(\times\)</span> more energy than on-chip communication within individual TPUs, transforming distributed training into a careful balance between computation parallelism and communication efficiency where AllReduce bandwidth, not compute capacity, determines overall training throughput.</p>
</section><section id="sec-ai-acceleration-waferscale-ai-6420" class="level3 page-columns page-full"><h3 class="anchored" data-anchor-id="sec-ai-acceleration-waferscale-ai-6420">Wafer-Scale AI</h3>
<p>At the frontier of AI scaling, wafer-scale<a href="#fn33" class="footnote-ref" id="fnref33" role="doc-noteref"><sup>33</sup></a> integration represents a paradigm shiftâ€”abandoning traditional multi-chip architectures in favor of a single, massive AI processor. Rather than partitioning computation across discrete chips, this approach treats an entire silicon wafer as a unified compute fabric, eliminating the inefficiencies of inter-chip communication.</p>
<div class="no-row-height column-margin column-container"><div id="fn33"><p><sup>33</sup>&nbsp;<strong>Wafer-Scale Integration</strong>: Using an entire 300&nbsp;mm silicon wafer as a single processor instead of cutting it into individual chips. Cerebras WSE-3 contains 4 trillion transistors across 850,000 coresâ€”125<span class="math inline">\(\times\)</span> more than the largest GPUs. Manufacturing challenges include 100% yield requirements (solved with redundant cores) and cooling 23&nbsp;kW of power. This approach eliminates inter-chip communication delays but costs $2-3 million per wafer versus $40,000 for equivalent GPU clusters.</p></div><div id="ref-Cerebras2021wse2" class="csl-entry" role="listitem">
Systems, Cerebras. 2021a. <span>â€œThe Wafer-Scale Engine 2: Scaling AI Compute Beyond GPUs.â€</span> <em>Cerebras White Paper</em>. <a href="https://cerebras.ai/product-chip/">https://cerebras.ai/product-chip/</a>.
</div></div><p>As shown in <a href="#fig-processor-trends" class="quarto-xref">Figure&nbsp;12</a>, Cerebrasâ€™ Wafer-Scale Engine (WSE) processors break away from the historical transistor scaling trends of CPUs, GPUs, and TPUs. While these architectures have steadily increased transistor counts along an exponential trajectory, WSE introduces an entirely new scaling paradigm, integrating trillions of transistors onto a single waferâ€”far surpassing even the most advanced GPUs and TPUs. With WSE-3, this trajectory continues, pushing wafer-scale AI to unprecedented levels <span class="citation" data-cites="Cerebras2021wse2">(<a href="#ref-Cerebras2021wse2" role="doc-biblioref">Systems 2021a</a>)</span>.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-processor-trends" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure"><div aria-describedby="fig-processor-trends-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="hw_acceleration_files/figure-html/fig-processor-trends-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-12" title="Figure&nbsp;12: Wafer-Scale Integration: Wafer-scale AI processors integrate trillions of transistors onto a single wafer, offering ultra-fast on-die communication to surpass traditional multi-chip architectures and achieve unprecedented performance levels."><img src="hw_acceleration_files/figure-html/fig-processor-trends-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="672"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-processor-trends-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;12: <strong>Wafer-Scale Integration</strong>: Wafer-scale AI processors integrate trillions of transistors onto a single wafer, offering ultra-fast on-die communication to surpass traditional multi-chip architectures and achieve unprecedented performance levels.
</figcaption></figure>
</div>
</div>
</div>
<p>The fundamental advantage of wafer-scale AI is its ultra-fast, on-die communication. Unlike chiplets, GPUs, or TPU Pods, where data must traverse physical boundaries between separate devices, wafer-scale AI enables near-instantaneous data transfer across its vast compute array. This architecture drastically reduces communication latency, unlocking performance levels that are unachievable with conventional multi-chip systems.</p>
<p>However, achieving this level of integration introduces formidable engineering challenges. Thermal dissipation, fault tolerance, and manufacturing yield become major constraints when fabricating a processor of this scale. These sustainability challenges, including energy consumption and resource utilization, are examined in <strong><a href="../sustainable_ai/sustainable_ai.html#sec-sustainable-ai">Chapter 18: Sustainable AI</a></strong>. Unlike distributed TPU systems, which mitigate failures by dynamically re-routing workloads, wafer-scale AI must incorporate built-in redundancy mechanisms to tolerate localized defects in the silicon. Successfully addressing these challenges is essential to realizing the full potential of wafer-scale computing as the next frontier in AI acceleration.</p>
</section><section id="sec-ai-acceleration-ai-systems-scaling-trajectory-ad73" class="level3"><h3 class="anchored" data-anchor-id="sec-ai-acceleration-ai-systems-scaling-trajectory-ad73">AI Systems Scaling Trajectory</h3>
<p><a href="#tbl-scaling-trajectory" class="quarto-xref">Table&nbsp;20</a> illustrates the progressive scaling of AI acceleration, from single-chip processors to increasingly complex architectures such as chiplet-based designs, multi-GPU systems, TPU Pods, and wafer-scale AI. Each step in this evolution introduces new challenges related to data movement, memory access, interconnect efficiency, and workload distribution. While chiplets enable modular scaling within a package, they introduce latency and memory coherence issues. Multi-GPU systems rely on high-speed interconnects like NVLink but face synchronization and communication bottlenecks. TPU Pods push scalability further by distributing workloads across clusters, yet they must contend with interconnect congestion and workload partitioning. At the extreme end, wafer-scale AI integrates an entire wafer into a single computational unit, presenting unique challenges in thermal management and fault tolerance.</p>
<div id="tbl-scaling-trajectory" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure"><figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-scaling-trajectory-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;20: <strong>AI Acceleration Trends</strong>: Scaling AI systems provides increasing challenges in data movement and memory access, driving architectural innovations from chiplets to wafer-scale integration. Each approach introduces unique trade-offs between modularity, latency, and complexity, demanding careful consideration of interconnect efficiency and workload distribution.
</figcaption><div aria-describedby="tbl-scaling-trajectory-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<colgroup>
<col style="width: 19%">
<col style="width: 32%">
<col style="width: 46%">
</colgroup>
<thead><tr class="header">
<th style="text-align: left;"><strong>Scaling Approach</strong></th>
<th style="text-align: left;"><strong>Key Feature</strong></th>
<th style="text-align: left;"><strong>Challenges</strong></th>
</tr></thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><strong>Chiplets</strong></td>
<td style="text-align: left;">Modular scaling within a package</td>
<td style="text-align: left;">Inter-chiplet latency, memory coherence</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Multi-GPU</strong></td>
<td style="text-align: left;">External GPU interconnects (NVLink)</td>
<td style="text-align: left;">Synchronization overhead, communication bottlenecks</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>TPU Pods</strong></td>
<td style="text-align: left;">Distributed accelerator clusters</td>
<td style="text-align: left;">Interconnect congestion, workload partitioning</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Wafer-Scale AI</strong></td>
<td style="text-align: left;">Entire wafer as a single processor</td>
<td style="text-align: left;">Thermal dissipation, fault tolerance</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
</section><section id="sec-ai-acceleration-computation-memory-scaling-changes-2bb1" class="level3 page-columns page-full"><h3 class="anchored" data-anchor-id="sec-ai-acceleration-computation-memory-scaling-changes-2bb1">Computation and Memory Scaling Changes</h3>
<p>As AI systems scale from single-chip accelerators to multi-chip architectures, the fundamental challenges in computation and memory evolve. In a single accelerator, execution is primarily optimized for localityâ€”ensuring that computations are mapped efficiently to available processing elements while minimizing memory access latency. However, as AI systems extend beyond a single chip, the scope of these optimizations expands significantly. Computation must now be distributed across multiple accelerators, and memory access patterns become constrained by interconnect bandwidth and communication overhead.</p>
<section id="sec-ai-acceleration-multichip-execution-mapping-8ec9" class="level4"><h4 class="anchored" data-anchor-id="sec-ai-acceleration-multichip-execution-mapping-8ec9">Multi-chip Execution Mapping</h4>
<p>In single-chip AI accelerators, computation placement is concerned with mapping workloads to PEs, vector units, and tensor cores. Mapping strategies aim to maximize data locality, ensuring that computations access nearby memory to reduce costly data movement.</p>
<p>As AI systems scale to multi-chip execution, computation placement must consider several critical factors. Workloads need to be partitioned across multiple accelerators, which requires explicit coordination of execution order and dependencies. This division is essential due to the inherent latency associated with cross-chip communication, which contrasts sharply with single-chip systems that benefit from shared on-chip memory. Accordingly, computation scheduling must be interconnect-aware to manage these delays effectively. Additionally, achieving load balancing across accelerators is vital; an uneven distribution of tasks can result in some accelerators remaining underutilized while others operate at full capacity, ultimately hindering overall system performance.</p>
<p>For example, in multi-GPU training, computation mapping must ensure that each GPU has a balanced portion of the workload while minimizing expensive cross-GPU communication. Similarly, in TPU Pods, mapping strategies must align with the torus interconnect topology, ensuring that computation is placed to minimize long-distance data transfers.</p>
<p>Thus, while computation placement in single-chip systems is a local optimization problem, in multi-chip architectures, it becomes a global optimization challenge where execution efficiency depends on minimizing inter-chip communication and balancing workload distribution.</p>
</section><section id="sec-ai-acceleration-distributed-access-memory-allocation-d970" class="level4 page-columns page-full"><h4 class="anchored" data-anchor-id="sec-ai-acceleration-distributed-access-memory-allocation-d970">Distributed Access Memory Allocation</h4>
<p>Memory allocation strategies in single-chip AI accelerators are designed to minimize off-chip memory accesses by using on-chip caches, SRAM, and HBM. Techniques such as tiling, data reuse, and kernel fusion ensure that computations make efficient use of fast local memory.</p>
<p>In multi-chip AI systems, each accelerator manages its own local memory, which necessitates the explicit allocation of model parameters, activations, and intermediate data across the devices. Unlike single-chip execution where data is fetched once and reused, multi-chip setups require deliberate strategies to minimize redundant data transfers, as data must be communicated between accelerators. Additionally, when overlapping data is processed by multiple accelerators, the synchronization of shared data can introduce significant overhead that must be carefully managed to ensure efficient execution.</p>
<p>For instance, in multi-GPU deep learning, gradient synchronization across GPUs is a memory-intensive operation that must be optimized to avoid network congestion <span class="citation" data-cites="Shallue2019measuring">(<a href="#ref-Shallue2019measuring" role="doc-biblioref">Shallue et al. 2019</a>)</span>. In wafer-scale AI, memory allocation must account for fault tolerance and redundancy mechanisms, ensuring that defective regions of the wafer do not disrupt execution.</p>
<div class="no-row-height column-margin column-container"><div id="ref-Shallue2019measuring" class="csl-entry" role="listitem">
Shallue, Christopher J, Jaehoon Lee, Joseph Antognini, Jascha Sohl-Dickstein, Roy Frostig, and George E Dahl. 2019. <span>â€œMeasuring the Effects of Data Parallelism on Neural Network Training.â€</span> <em>Journal of Machine Learning Research</em> 20: 1â€“49. <a href="http://jmlr.org/papers/v20/18-789.html">http://jmlr.org/papers/v20/18-789.html</a>.
</div></div><p>Thus, while memory allocation in single-chip accelerators focuses on local cache efficiency, in multi-chip architectures, it must be explicitly coordinated across accelerators to balance memory bandwidth, minimize redundant transfers, and reduce synchronization overhead.</p>
</section><section id="sec-ai-acceleration-data-movement-constraints-a823" class="level4"><h4 class="anchored" data-anchor-id="sec-ai-acceleration-data-movement-constraints-a823">Data Movement Constraints</h4>
<p>In single-chip AI accelerators, data movement optimization is largely focused on minimizing on-chip memory access latency. Techniques such as weight stationarity, input stationarity, and tiling ensure that frequently used data remains close to the execution units, reducing off-chip memory traffic.</p>
<p>In multi-chip architectures, data movement transcends being merely an intra-chip issue and becomes a significant system-wide bottleneck. Scaling introduces several critical challenges, foremost among them being inter-chip bandwidth constraints; communication links such as PCIe, NVLink, and TPU interconnects operate at speeds that are considerably slower than those of on-chip memory accesses. Additionally, when accelerators share model parameters or intermediate computations, the resulting data synchronization overhead, which encompass latency and contention, can markedly impede execution. Finally, optimizing collective communication is essential for workloads that require frequent data exchanges, such as gradient updates in deep learning training, where minimizing synchronization penalties is imperative for achieving efficient system performance.</p>
<p>For example, in TPU Pods, systolic execution models ensure that data moves in structured patterns, reducing unnecessary off-chip transfers. In multi-GPU inference, techniques like asynchronous data fetching and overlapping computation with communication help mitigate inter-chip latency.</p>
<p>Thus, while data movement optimization in single-chip systems focuses on cache locality and tiling, in multi-chip architectures, the primary challenge is reducing inter-chip communication overhead to maximize efficiency.</p>
</section><section id="sec-ai-acceleration-compilers-runtimes-adaptation-0d70" class="level4"><h4 class="anchored" data-anchor-id="sec-ai-acceleration-compilers-runtimes-adaptation-0d70">Compilers and Runtimes Adaptation</h4>
<p>As AI acceleration extends beyond a single chip, compilers and runtimes must adapt to manage computation placement, memory organization, and execution scheduling across multiple accelerators. The fundamental principles of locality, parallelism, and efficient scheduling remain essential, but their implementation requires new strategies for distributed execution.</p>
<p>One of the primary challenges in scaling AI execution is computation placement. In a single-chip accelerator, workloads are mapped to processing elements, vector units, and tensor cores with an emphasis on minimizing on-chip data movement and maximizing parallel execution. However, in a multi-chip system, computation must be partitioned hierarchically, where workloads are distributed not just across cores within a chip, but also across multiple accelerators. Compilers handle this by implementing interconnect-aware scheduling, optimizing workload placement to minimize costly inter-chip communication.</p>
<p>Similarly, memory management evolves as scaling extends beyond a single accelerator. In a single-chip system, local caching, HBM reuse, and efficient tiling strategies ensure that frequently accessed data remains close to computation units. However, in a multi-chip system, each accelerator has its own independent memory, requiring explicit memory partitioning and coordination. Compilers optimize memory layouts for distributed execution, while runtimes introduce data prefetching and caching mechanisms to reduce inter-chip memory access overhead.</p>
<p>Beyond computation and memory, data movement becomes a major bottleneck at scale. In a single-chip accelerator, efficient on-chip caching and minimized DRAM accesses ensure that data is reused efficiently. However, in a multi-chip system, communication-aware execution becomes critical, requiring compilers to generate execution plans that overlap computation with data transfers. Runtimes handle inter-chip synchronization, ensuring that workloads are not stalled by waiting for data to arrive from remote accelerators.</p>
<p>Finally, execution scheduling must be extended for global coordination. In single-chip AI execution, scheduling is primarily concerned with parallelism and maximizing compute occupancy within the accelerator. However, in a multi-chip system, scheduling must balance workload distribution across accelerators while taking interconnect bandwidth and synchronization latency into account. Runtimes manage this complexity by implementing adaptive scheduling strategies that dynamically adjust execution plans based on system state and network congestion.</p>
<p><a href="#tbl-scaling-adaptations" class="quarto-xref">Table&nbsp;21</a> summarizes these key adaptations, highlighting how compilers and runtimes extend their capabilities to efficiently support multi-chip AI execution.</p>
<p>Thus, while the fundamentals of AI acceleration remain intact, compilers and runtimes must extend their functionality to operate efficiently across distributed systems. The next section will explore how mapping strategies evolve to further optimize multi-chip AI execution.</p>
<div id="tbl-scaling-adaptations" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure"><figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-scaling-adaptations-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;21: <strong>Multi-Chip Adaptations</strong>: Efficient AI execution on multiple accelerators requires coordinated adjustments to computation placement, memory management, and scheduling to balance workload distribution and minimize communication overhead. Compilers and runtimes extend their capabilities to dynamically adapt to system state and network congestion, enabling scalable and performant multi-chip AI systems.
</figcaption><div aria-describedby="tbl-scaling-adaptations-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<colgroup>
<col style="width: 22%">
<col style="width: 31%">
<col style="width: 45%">
</colgroup>
<thead><tr class="header">
<th style="text-align: left;"><strong>Aspect</strong></th>
<th style="text-align: left;"><strong>Single-Chip AI Accelerator</strong></th>
<th style="text-align: left;"><strong>Multi-Chip AI System &amp; How Compilers/Runtimes Adapt</strong></th>
</tr></thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><strong>Computation Placement</strong></td>
<td style="text-align: left;">Local PEs, tensor cores, vector units</td>
<td style="text-align: left;">Hierarchical mapping, interconnect-aware scheduling</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Memory Management</strong></td>
<td style="text-align: left;">Caching, HBM reuse, local tiling</td>
<td style="text-align: left;">Distributed allocation, prefetching, caching</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>Data Movement</strong></td>
<td style="text-align: left;">On-chip reuse, minimal DRAM access</td>
<td style="text-align: left;">Communication-aware execution, overlap transfers</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Execution Scheduling</strong></td>
<td style="text-align: left;">Parallelism, compute occupancy</td>
<td style="text-align: left;">Global scheduling, interconnect-aware balancing</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
</section></section><section id="sec-ai-acceleration-execution-models-adaptation-344b" class="level3"><h3 class="anchored" data-anchor-id="sec-ai-acceleration-execution-models-adaptation-344b">Execution Models Adaptation</h3>
<p>As AI accelerators scale beyond a single chip, execution models must evolve to account for the complexities introduced by distributed computation, memory partitioning, and inter-chip communication. In single-chip accelerators, execution is optimized for local processing elements, with scheduling strategies that balance parallelism, locality, and data reuse. However, in multi-chip AI systems, execution must now be coordinated across multiple accelerators, introducing new challenges in workload scheduling, memory coherence, and interconnect-aware execution.</p>
<p>This section explores how execution models change as AI acceleration scales, focusing on scheduling, memory coordination, and runtime management in multi-chip systems.</p>
<section id="sec-ai-acceleration-crossaccelerator-scheduling-4ac0" class="level4"><h4 class="anchored" data-anchor-id="sec-ai-acceleration-crossaccelerator-scheduling-4ac0">Cross-Accelerator Scheduling</h4>
<p>In single-chip AI accelerators, execution scheduling is primarily aimed at optimizing parallelism within the processor. This involves ensuring that workloads are effectively mapped to tensor cores, vector units, and special function units by employing techniques designed to enhance data locality and resource utilization. For instance, static scheduling uses a predetermined execution order that is carefully optimized for locality and reuse, while dynamic scheduling adapts in real time to variations in workload demands. Additionally, pipeline execution divides computations into stages, thereby maximizing hardware utilization by maintaining a continuous flow of operations.</p>
<p>In contrast, scheduling in multi-chip architectures must address the additional challenges posed by inter-chip dependencies. Workload partitioning in such systems involves distributing tasks across various accelerators such that each receives an optimal share of the workload, all while minimizing the overhead caused by excessive communication. Interconnect-aware scheduling is essential to align execution timing with the constraints of inter-chip bandwidth, thus preventing performance stalls. Latency hiding techniques also play a critical role, as they enable the overlapping of computation with communication, effectively reducing waiting times.</p>
<p>For example, in multi-GPU inference scenarios, execution scheduling is implemented in a way that allows data to be prefetched concurrently with computation, thereby mitigating memory stalls. Similarly, TPU Pods leverage the systolic array model to tightly couple execution scheduling with data flow, ensuring that each TPU core receives its required data precisely when needed. Therefore, while single-chip execution scheduling is focused largely on maximizing internal parallelism, multi-chip systems require a more holistic approach that explicitly manages communication overhead and synchronizes workload distribution across accelerators.</p>
</section><section id="sec-ai-acceleration-crossaccelerator-coordination-8aa4" class="level4"><h4 class="anchored" data-anchor-id="sec-ai-acceleration-crossaccelerator-coordination-8aa4">Cross-Accelerator Coordination</h4>
<p>In single-chip AI accelerators, memory coordination is managed through sophisticated local caching strategies that keep frequently used data in close proximity to the execution units. Techniques such as tiling, kernel fusion, and data reuse are employed to reduce the dependency on slower memory hierarchies, thereby enhancing performance and reducing latency.</p>
<p>In contrast, multi-chip architectures present a distributed memory coordination challenge that necessitates more deliberate management. Each accelerator in such a system possesses its own independent memory, which must be organized through explicit memory partitioning to minimize cross-chip data accesses. Additionally, ensuring consistency and synchronization of shared data across accelerators is essential to maintain computational correctness. Efficient communication mechanisms must also be implemented to schedule data transfers in a way that limits overhead associated with synchronization delays.</p>
<p>For instance, in distributed deep learning training, model parameters must be synchronized across multiple GPUs using methods such as all-reduce, where gradients are aggregated across accelerators while reducing communication latency. In wafer-scale AI, memory coordination must further address fault-tolerant execution, ensuring that defective areas do not compromise overall system performance. Consequently, while memory coordination in single-chip systems is primarily concerned with cache optimization, multi-chip architectures require management of distributed memory access, synchronization, and communication to achieve efficient execution.</p>
</section><section id="sec-ai-acceleration-crossaccelerator-execution-management-87a9" class="level4"><h4 class="anchored" data-anchor-id="sec-ai-acceleration-crossaccelerator-execution-management-87a9">Cross-Accelerator Execution Management</h4>
<p>Execution in single-chip AI accelerators is managed by AI runtimes that handle workload scheduling, memory allocation, and hardware execution. These runtimes optimize execution at the kernel level, ensuring that computations are executed efficiently within the available resources.</p>
<p>In multi-chip AI systems, runtimes must incorporate a strategy for distributed execution orchestration. This approach ensures that both computation and memory access are seamlessly coordinated across multiple accelerators, enabling efficient utilization of hardware resources and minimizing bottlenecks associated with data transfers.</p>
<p>These systems require robust mechanisms for cross-chip workload synchronization. Careful management of dependencies and timely coordination between accelerators are essential to prevent stalls in execution that may arise from delays in inter-chip communication. Such synchronization is critical for maintaining the flow of computation, particularly in environments where latency can significantly impact overall performance.</p>
<p>Finally, adaptive execution models play a pivotal role in contemporary multi-chip architectures. These models dynamically adjust execution plans based on current hardware availability and communication constraints, ensuring that the system can respond to changing conditions and optimize performance in real time. Together, these strategies provide a resilient framework for managing the complexities of distributed AI execution.</p>
<p>For example, in Googleâ€™s TPU Pods, the TPU runtime is responsible for scheduling computations across multiple TPU cores, ensuring that workloads are executed in a way that minimizes communication bottlenecks. In multi-GPU frameworks like PyTorch and TensorFlow, runtime execution must synchronize operations across GPUs, ensuring that data is transferred efficiently while maintaining execution order.</p>
<p>Thus, while single-chip runtimes focus on optimizing execution within a single processor, multi-chip runtimes must handle system-wide execution, balancing computation, memory, and interconnect performance.</p>
</section><section id="sec-ai-acceleration-computation-placement-adaptation-9a3c" class="level4"><h4 class="anchored" data-anchor-id="sec-ai-acceleration-computation-placement-adaptation-9a3c">Computation Placement Adaptation</h4>
<p>As AI systems expand beyond single-chip execution, computation placement must adapt to account for inter-chip workload distribution and interconnect efficiency. In single-chip accelerators, compilers optimize placement by mapping workloads to tensor cores, vector units, and PEs, ensuring maximum parallelism while minimizing on-chip data movement. However, in multi-chip systems, placement strategies must address interconnect bandwidth constraints, synchronization latency, and hierarchical workload partitioning across multiple accelerators.</p>
<p><a href="#tbl-computation-placement" class="quarto-xref">Table&nbsp;22</a> highlights these adaptations. To reduce expensive cross-chip communication, compilers now implement interconnect-aware workload partitioning, strategically assigning computations to accelerators based on communication cost. For instance, in multi-GPU training, compilers optimize placement to minimize NVLink or PCIe traffic, whereas TPU Pods leverage the torus interconnect topology to enhance data exchanges.</p>
<div id="tbl-computation-placement" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure"><figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-computation-placement-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;22: <strong>Computation Placement Strategies</strong>: Multi-chip AI systems necessitate hierarchical workload mapping to minimize communication overhead; compilers adapt single-chip optimization techniques by considering interconnect bandwidth and latency when assigning computations to accelerators. This table contrasts computation placement in single-chip systemsâ€”local to processing elementsâ€”with multi-chip systems, where placement strategies prioritize efficient data exchange across accelerators.
</figcaption><div aria-describedby="tbl-computation-placement-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<colgroup>
<col style="width: 19%">
<col style="width: 28%">
<col style="width: 51%">
</colgroup>
<thead><tr class="header">
<th style="text-align: left;"><strong>Aspect</strong></th>
<th style="text-align: left;"><strong>Single-Chip AI Accelerator</strong></th>
<th style="text-align: left;"><strong>Multi-Chip AI System &amp; How Compilers/Runtimes Adapt</strong></th>
</tr></thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><strong>Computation Placement</strong></td>
<td style="text-align: left;">Local PEs, tensor cores, vector units</td>
<td style="text-align: left;">Hierarchical mapping, interconnect-aware scheduling</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Workload Distribution</strong></td>
<td style="text-align: left;">Optimized within a single chip</td>
<td style="text-align: left;">Partitioning across accelerators, minimizing inter-chip communication</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>Synchronization</strong></td>
<td style="text-align: left;">Managed within local execution units</td>
<td style="text-align: left;">Runtimes dynamically balance workloads, adjust execution plans</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p>Runtimes complement this by dynamically managing execution workloads, adjusting placement in real-time to balance loads across accelerators. Unlike static compilation, which assumes a fixed hardware topology, AI runtimes continuously monitor system conditions and migrate tasks as needed to prevent bottlenecks. This ensures efficient execution even in environments with fluctuating workload demands or varying hardware availability.</p>
<p>Thus, computation placement at scale builds upon local execution optimizations while introducing new challenges in inter-chip coordination, communication-aware execution, and dynamic load balancingâ€”challenges that extend to how memory hierarchies must adapt to support efficient execution across multi-chip architectures.</p>
</section></section><section id="sec-ai-acceleration-navigating-multichip-ai-complexities-83bd" class="level3"><h3 class="anchored" data-anchor-id="sec-ai-acceleration-navigating-multichip-ai-complexities-83bd">Navigating Multi-Chip AI Complexities</h3>
<p>The evolution of AI hardware, from single-chip accelerators to multi-chip systems and wafer-scale integration, highlights the increasing complexity of efficiently executing large-scale machine learning workloads. Scaling AI systems introduces new challenges in computation placement, memory management, and data movement. While the fundamental principles of AI acceleration remain consistent, their implementation must adapt to the constraints of distributed execution, interconnect bandwidth limitations, and synchronization overhead.</p>
<p>Multi-chip AI architectures represent a significant step forward in addressing the computational demands of modern machine learning models. By distributing workloads across multiple accelerators, these systems offer increased performance, memory capacity, and scalability. However, realizing these benefits requires careful consideration of how computations are mapped to hardware, how memory is partitioned and accessed, and how execution is scheduled across a distributed system.</p>
<p>While we an overview of the key concepts and challenges in multi-chip AI acceleration as they extend beyond a single system, there is still much more to explore. As AI models continue to grow in size and complexity, new architectural innovations, mapping strategies, and runtime optimizations will be needed to sustain efficient execution. These emerging trends and future directions continue to evolve rapidly in the field. The ongoing development of AI hardware and software reflects a broader trend in computing, where specialization and domain-specific architectures are becoming increasingly important for addressing the unique demands of emerging workloads.</p>
<p>Understanding the principles and trade-offs involved in multi-chip AI acceleration enables machine learning engineers and system designers to make informed decisions about how to best deploy and optimize their models. Whether training large language models on TPU pods or deploying computer vision applications on multi-GPU systems, the ability to efficiently map computations to hardware will continue to be a critical factor in realizing the full potential of AI.</p>
<div id="quiz-question-sec-ai-acceleration-multichip-ai-acceleration-38d7" class="callout callout-quiz-question">
<details class="callout-quiz-question fbx-default closebutton"><summary><strong>Self-Check: Question 1.9</strong></summary><div>
<ol type="1">
<li>
<p>What is a primary challenge when transitioning from single-chip to multi-chip AI architectures?</p>
<ol type="a">
<li>Maximizing utilization within fixed resources</li>
<li>Increasing the clock speed of individual chips</li>
<li>Reducing the size of individual processors</li>
<li>Balancing computational distribution against communication overhead</li>
</ol>
</li>
<li><p>Explain how memory coherence challenges differ between chiplet-based architectures and traditional multi-chip systems.</p></li>
<li><p>True or False: In multi-GPU systems, increasing the number of GPUs always leads to linear performance gains.</p></li>
<li><p>The fundamental limitation of distributed AI training is that communication overhead constrains parallel speedup according to established _____ principles.</p></li>
<li>
<p>In a multi-accelerator system design, what is a critical factor that affects performance scaling?</p>
<ol type="a">
<li>The number of CPUs in the system</li>
<li>The efficiency of the specialized interconnect architecture</li>
<li>The use of PCIe interconnects</li>
<li>The clock speed of individual accelerators</li>
</ol>
</li>
</ol>
<p><a href="#quiz-answer-sec-ai-acceleration-multichip-ai-acceleration-38d7" class="question-label">See Answers â†’</a></p>
</div></details>
</div>
</section></section><section id="sec-ai-acceleration-heterogeneous-soc-ai-acceleration-b1bb" class="level2 page-columns page-full"><h2 class="anchored" data-anchor-id="sec-ai-acceleration-heterogeneous-soc-ai-acceleration-b1bb">Heterogeneous SoC AI Acceleration</h2>
<p>The multi-chip architectures examined in previous sections focused primarily on maximizing computational throughput for data center workloads, where power budgets extend to kilowatts and cooling infrastructure supports rack-scale deployments. However, the hardware acceleration principles establishedâ€”specialized compute units, memory hierarchy optimization, and workload mapping strategiesâ€”must adapt dramatically when deploying AI systems in mobile and edge environments. A smartphone operates within a 2 to 5 watt power budget, autonomous vehicles require deterministic real-time guarantees, and IoT sensors must function for years on battery power. These constraints necessitate heterogeneous System-on-Chip (SoC) architectures that coordinate multiple specialized processors within a single chip while meeting stringent power, thermal, and latency requirements fundamentally different from data center deployments.</p>
<p>The mobile AI revolution has fundamentally transformed how we think about AI acceleration, moving beyond homogeneous data center architectures to heterogeneous System-on-Chip (SoC) designs that coordinate multiple specialized processors. Modern smartphones, automotive systems, and IoT devices integrate CPU cores, GPU shaders, digital signal processors (DSPs), and dedicated neural processing units (NPUs) within a single chip, requiring sophisticated orchestration to achieve optimal performance under strict power and thermal constraints.</p>
<section id="sec-ai-acceleration-mobile-soc-architecture-evolution-6ca8" class="level3 page-columns page-full"><h3 class="anchored" data-anchor-id="sec-ai-acceleration-mobile-soc-architecture-evolution-6ca8">Mobile SoC Architecture Evolution</h3>
<p>Qualcommâ€™s Snapdragon AI Engine exemplifies heterogeneous computing for mobile AI, coordinating Kryo CPU cores, Adreno GPU, Hexagon DSP, and dedicated NPU<a href="#fn34" class="footnote-ref" id="fnref34" role="doc-noteref"><sup>34</sup></a> across a shared memory hierarchy. The Snapdragon 8 Gen 3 achieves 73 TOPS through intelligent workload distributionâ€”computer vision kernels execute on the GPUâ€™s parallel shaders, audio processing leverages the DSPâ€™s specialized arithmetic units, while transformer attention mechanisms utilize the NPUâ€™s optimized matrix engines. This coordination requires millisecond-precision scheduling to meet real-time constraints while managing thermal throttling and battery life optimization.</p>
<div class="no-row-height column-margin column-container"><div id="fn34"><p><sup>34</sup>&nbsp;<strong>Neural Processing Unit (NPU)</strong>: Specialized processors designed specifically for neural network inference, distinct from general-purpose GPUs. Apple introduced the first consumer NPU in the A11 chip (2017), achieving 600 billion operations per second while consuming less than 1 watt. Modern NPUs like Appleâ€™s M3 Neural Engine deliver 18 TOPS for on-device AI tasks like real-time image processing, voice recognition, and computational photography. NPUs excel at low-power, fixed-function AI workloads but lack the programmability of GPUs for diverse ML research.</p></div></div><p>While Qualcommâ€™s approach emphasizes diverse processor specialization, Appleâ€™s vertically integrated strategy demonstrates how tight hardware-software co-design enables even more sophisticated heterogeneous execution. The M2 chipâ€™s 16-core Neural Engine (15.8 TOPS) coordinates with the 10-core GPU and 8-core CPU through a unified memory architecture that eliminates data copying overhead. The Neural Engineâ€™s specialized matrix multiplication units handle transformer layers, while the GPUâ€™s Metal Performance Shaders accelerate convolutional operations, and the CPU manages control flow and dynamic layer selection. This fine-grained coordination enables real-time language translation and on-device image generation while maintaining millisecond response times.</p>
<p>Beyond these vertically integrated solutions from Qualcomm and Apple, ARMâ€™s IP licensing model offers a fundamentally different approach that enables SoC designers to customize processor combinations based on target applications. The Mali-G78 GPUâ€™s 24 cores can be paired with Ethos-N78 NPU for balanced general-purpose and AI acceleration, while the Cortex-M55 microcontroller integrates Ethos-U55 microNPU for ultra-low-power edge applications. This modular flexibility allows automotive SoCs to emphasize deterministic real-time processing while smartphone SoCs optimize for interactive performance and battery efficiency.</p>
</section><section id="sec-ai-acceleration-strategies-dynamic-workload-distribution-a421" class="level3"><h3 class="anchored" data-anchor-id="sec-ai-acceleration-strategies-dynamic-workload-distribution-a421">Strategies for Dynamic Workload Distribution</h3>
<p>With multiple specialized processors available on heterogeneous SoCs, the critical challenge becomes intelligently distributing neural network operations across these resources to maximize performance while respecting power and latency constraints.</p>
<p>Modern neural networks require intelligent partitioning across heterogeneous processors based on operation characteristics and current system state. Convolutional layers with regular data access patterns typically execute efficiently on GPU shader cores, while fully connected layers with irregular sparsity patterns may perform better on general-purpose CPU cores with large caches. Attention mechanisms in transformers benefit from NPU matrix engines when sequences are long, but may execute more efficiently on CPU when sequence lengths are small due to the NPU setup overhead.</p>
<p>Beyond static operation-to-processor mapping, heterogeneous SoCs implement dynamic processor selection based on multiple constraints:</p>
<ul>
<li>
<strong>Power Budget</strong>: During battery operation, the system may route computations to lower-power DSP cores rather than high-performance GPU cores</li>
<li>
<strong>Thermal State</strong>: When approaching thermal limits, workloads shift from power-hungry NPU to more efficient CPU execution</li>
<li>
<strong>Latency Requirements</strong>: Safety-critical automotive applications prioritize deterministic CPU execution over potentially faster but variable NPU processing</li>
<li>
<strong>Concurrent Workload Interference</strong>: Multiple AI applications may require load balancing across available processors to maintain Quality of Service</li>
</ul>
<p>Compounding the processor selection challenge, shared memory architectures require sophisticated arbitration when multiple processors access LPDDR simultaneously. The Snapdragon 8 Gen 3â€™s memory controller implements priority-based scheduling where camera processing receives higher priority than background AI tasks, ensuring real-time video processing while background neural networks adapt their execution patterns to available memory bandwidth. This arbitration becomes critical during memory-intensive operations like large language model inference, where parameter streaming from DRAM must be carefully coordinated across processors.</p>
</section><section id="sec-ai-acceleration-power-thermal-management-6c00" class="level3"><h3 class="anchored" data-anchor-id="sec-ai-acceleration-power-thermal-management-6c00">Power and Thermal Management</h3>
<p>Mobile AI workloads must maintain high performance while operating within strict power budgets and thermal envelopesâ€”constraints that require sophisticated coordination across heterogeneous processors.</p>
<p>Heterogeneous SoCs implement coordinated DVFS across multiple processors to optimize the power-performance envelope. When one processor increases frequency to meet latency demands, the system may reduce voltage on other processors to maintain total power budget. This coordination becomes complex in AI workloads where computational phases may shift rapidly between processorsâ€”the system must predict upcoming workload transitions to preemptively adjust operating points while avoiding voltage/frequency oscillations that degrade efficiency.</p>
<p>When DVFS alone cannot maintain the power envelope, mobile SoCs implement thermal throttling through intelligent task migration rather than simple frequency reduction. When the NPU approaches thermal limits during intensive neural network processing, the runtime system can migrate layers to the GPU or CPU while maintaining computational throughput. This approach preserves performance during thermal events, though it requires sophisticated workload characterization to predict execution time and power consumption across different processors.</p>
<p>Beyond real-time power and thermal management, mobile AI systems must also adapt their computational strategies based on battery state and charging status. During low battery conditions, the system may switch from high-accuracy models to efficient approximations, migrate workloads from power-hungry NPU to energy-efficient DSP, or reduce inference frequency while maintaining application responsiveness. Conversely, during charging, the system can enable higher-performance models and increase processing frequency to deliver enhanced user experiences.</p>
</section><section id="sec-ai-acceleration-automotive-heterogeneous-ai-systems-deda" class="level3"><h3 class="anchored" data-anchor-id="sec-ai-acceleration-automotive-heterogeneous-ai-systems-deda">Automotive Heterogeneous AI Systems</h3>
<p>Automotive applications introduce unique heterogeneous computing challenges that combine mobile-style power efficiency with hard real-time guarantees and functional safety requirementsâ€”a combination that demands fundamentally different architectural approaches.</p>
<p>Automotive SoCs must guarantee deterministic inference latency for safety-critical functions while supporting advanced driver assistance systems (ADAS). The Snapdragon Ride platform coordinates multiple AI accelerators across safety domainsâ€”redundant processing elements ensure functional safety compliance while high-performance accelerators handle perception, planning, and control algorithms. This architecture requires temporal isolation between safety-critical and convenience functions, implemented through hardware partitioning and time-triggered scheduling.</p>
<p>These safety requirements become even more complex when considering that modern vehicles integrate multiple AI-enabled SoCs for different domainsâ€”vision processing SoCs handle camera-based perception, radar processing SoCs manage RF sensor data, while central compute platforms coordinate high-level decision making. These distributed systems must maintain temporal coherence across sensor modalities with microsecond-precision timing, requiring specialized inter-SoC communication protocols and distributed synchronization mechanisms.</p>
<p>Extending beyond the vehicleâ€™s internal sensors, vehicle-to-everything (V2X) communication adds another layer of heterogeneous processing where AI algorithms must coordinate local sensor processing with information received from other vehicles and infrastructure. This requires ultra-low latency processing chains where 5G modems, AI accelerators, and control systems operate within millisecond deadlines while maintaining functional safety requirements.</p>
</section><section id="sec-ai-acceleration-software-stack-challenges-255c" class="level3"><h3 class="anchored" data-anchor-id="sec-ai-acceleration-software-stack-challenges-255c">Software Stack Challenges</h3>
<p>The architectural sophistication of heterogeneous SoCs creates substantial software development challenges that span programming models, memory management, and runtime optimization.</p>
<p>Programming heterogeneous SoCs requires frameworks that abstract processor differences while exposing performance-critical optimization opportunities. OpenCL and Vulkan provide cross-processor execution, but achieving optimal performance requires processor-specific optimizations that complicate portable development. Modern ML frameworks like TensorFlow Lite and PyTorch Mobile implement automatic processor selection, but developers still need to understand heterogeneous execution patterns to achieve optimal results.</p>
<p>Complicating the programming challenge further, heterogeneous SoCs with shared memory architectures require sophisticated memory management that considers processor-specific caching behaviors, memory access patterns, and coherency requirements. CPU caches may interfere with GPU memory access patterns, while NPU direct memory access (DMA) operations must be synchronized with CPU cache operations to maintain data consistency.</p>
<p>To address the complexity of manual optimization across these dimensions, advanced heterogeneous SoCs implement machine learning-based runtime optimization that learns from execution patterns to improve processor selection, thermal management, and power optimization. These systems collect telemetry on workload characteristics, processor utilization, and power consumption to build models that predict optimal execution strategies for new workloads.</p>
<p>This heterogeneous approach to AI acceleration represents the future of computing, where no single processor architecture can optimally handle the diverse computational patterns in modern AI applications. Understanding these coordination challenges is essential for developing efficient mobile AI systems that deliver high performance while meeting the strict power, thermal, and real-time constraints of edge deployment scenarios.</p>
<p>However, the complexity of these heterogeneous systems creates numerous opportunities for misconception and suboptimal design decisions. The following fallacies and pitfalls highlight common misunderstandings that can undermine acceleration strategies.</p>
<div id="quiz-question-sec-ai-acceleration-heterogeneous-soc-ai-acceleration-b1bb" class="callout callout-quiz-question">
<details class="callout-quiz-question fbx-default closebutton"><summary><strong>Self-Check: Question 1.10</strong></summary><div>
<ol type="1">
<li>
<p>What is a primary reason for using heterogeneous SoC architectures in mobile AI systems?</p>
<ol type="a">
<li>To increase computational throughput without power constraints.</li>
<li>To maximize data center workload efficiency.</li>
<li>To simplify the design process by using a single type of processor.</li>
<li>To coordinate multiple specialized processors within strict power and thermal limits.</li>
</ol>
</li>
<li><p>Explain how dynamic workload distribution strategies in heterogeneous SoCs help manage power and thermal constraints.</p></li>
<li><p>Order the following steps in managing workload distribution on a heterogeneous SoC: (1) Assess system power budget, (2) Evaluate processor thermal state, (3) Allocate tasks based on constraints, (4) Monitor performance and adjust.</p></li>
<li>
<p>Which of the following best describes a challenge in programming heterogeneous SoCs?</p>
<ol type="a">
<li>Managing memory coherency across diverse processors.</li>
<li>Ensuring all processors use the same programming model.</li>
<li>Achieving optimal performance without considering processor-specific optimizations.</li>
<li>Implementing a single execution strategy for all tasks.</li>
</ol>
</li>
<li><p>In a production system, what trade-offs might you consider when implementing AI acceleration on a heterogeneous SoC for an autonomous vehicle?</p></li>
</ol>
<p><a href="#quiz-answer-sec-ai-acceleration-heterogeneous-soc-ai-acceleration-b1bb" class="question-label">See Answers â†’</a></p>
</div></details>
</div>
</section></section><section id="sec-ai-acceleration-fallacies-pitfalls-dc1f" class="level2"><h2 class="anchored" data-anchor-id="sec-ai-acceleration-fallacies-pitfalls-dc1f">Fallacies and Pitfalls</h2>
<p>Hardware acceleration involves complex interactions between specialized architectures, software stacks, and workload characteristics that create significant opportunities for misunderstanding optimal deployment strategies. The impressive performance numbers often associated with AI accelerators can mask important constraints and trade-offs that determine real-world effectiveness across different deployment scenarios.</p>
<p><strong>Fallacy:</strong> <em>More specialized hardware always provides better performance than general-purpose alternatives.</em></p>
<p>This belief assumes that specialized accelerators automatically outperform general-purpose processors for all AI workloads. Specialized hardware achieves peak performance only when workloads match the architectural assumptions and optimization targets. Models with irregular memory access patterns, small batch sizes, or dynamic computation graphs may perform better on flexible general-purpose processors than on specialized accelerators designed for dense, regular computations. The overhead of data movement, format conversion, and synchronization can eliminate the benefits of specialized computation. Effective hardware selection requires matching workload characteristics to architectural strengths rather than assuming specialization always wins.</p>
<p><strong>Pitfall:</strong> <em>Ignoring memory bandwidth limitations when selecting acceleration strategies.</em></p>
<p>Many practitioners focus on computational throughput metrics without considering memory bandwidth constraints that often limit real-world performance. AI accelerators with impressive computational capabilities can be severely bottlenecked by insufficient memory bandwidth, leading to poor hardware utilization. The ratio between computation intensity and memory access requirements determines whether an accelerator can achieve its theoretical performance. This oversight leads to expensive hardware deployments that fail to deliver expected performance improvements because the workload is memory-bound rather than compute-bound.</p>
<p><strong>Fallacy:</strong> <em>Hardware acceleration benefits scale linearly with additional accelerators.</em></p>
<p>This misconception drives teams to expect proportional performance gains when adding more accelerators to their systems. Multi-accelerator setups introduce communication overhead, synchronization costs, and load balancing challenges that can severely limit scaling efficiency. Small models may not provide enough parallel work to utilize multiple accelerators effectively, while large models may be limited by communication bandwidth between devices. Distributed training and inference face additional challenges from gradient aggregation, model partitioning, and coordination overhead that create non-linear scaling relationships.</p>
<p><strong>Pitfall:</strong> <em>Vendor-specific optimizations without considering long-term portability and flexibility.</em></p>
<p>Organizations often optimize exclusively for specific hardware vendors to achieve maximum performance without considering the implications for system flexibility and future migration. Deep integration with vendor-specific libraries, custom kernels, and proprietary optimization tools creates lock-in that complicates hardware upgrades, vendor changes, or multi-vendor deployments. While vendor-specific optimizations can provide significant performance benefits, they should be balanced against the need for system portability and the ability to adapt to evolving hardware landscapes. Maintaining some level of hardware abstraction preserves strategic flexibility while still capturing most performance benefits.</p>
<div id="quiz-question-sec-ai-acceleration-fallacies-pitfalls-dc1f" class="callout callout-quiz-question">
<details class="callout-quiz-question fbx-default closebutton"><summary><strong>Self-Check: Question 1.11</strong></summary><div>
<ol type="1">
<li>
<p>Which of the following scenarios would most likely benefit from using general-purpose processors over specialized hardware accelerators?</p>
<ol type="a">
<li>A workload with irregular memory access patterns and dynamic computation graphs.</li>
<li>A workload with dense, regular computations and large batch sizes.</li>
<li>A workload that requires high computational throughput with minimal memory access.</li>
<li>A workload optimized for a specific vendorâ€™s proprietary libraries.</li>
</ol>
</li>
<li><p>True or False: Adding more accelerators to a system will always result in linear performance improvements.</p></li>
<li><p>Explain why memory bandwidth limitations can undermine the performance benefits of AI accelerators.</p></li>
<li><p>The belief that hardware acceleration benefits scale linearly with additional accelerators is a common ____. This misconception overlooks the communication and synchronization overheads that limit scaling efficiency.</p></li>
<li><p>In a production system, what trade-offs should be considered when optimizing for vendor-specific hardware?</p></li>
</ol>
<p><a href="#quiz-answer-sec-ai-acceleration-fallacies-pitfalls-dc1f" class="question-label">See Answers â†’</a></p>
</div></details>
</div>
</section><section id="sec-ai-acceleration-summary-a5f8" class="level2"><h2 class="anchored" data-anchor-id="sec-ai-acceleration-summary-a5f8">Summary</h2>
<p>Hardware acceleration has emerged as the critical enabler that transforms machine learning from academic curiosity to practical reality, fundamentally reshaping how we design both computational systems and the algorithms that run on them. The evolution from general-purpose processors to specialized AI accelerators represents more than just incremental improvementâ€”it reflects a paradigm shift toward domain-specific computing where hardware and software are co-designed to optimize specific computational patterns. The journey from CPUs through GPUs to specialized TPUs, NPUs, and wafer-scale systems demonstrates how understanding workload characteristics drives architectural innovation, creating opportunities for orders-of-magnitude performance improvements through targeted specialization.</p>
<p>The technical challenges of AI acceleration span multiple layers of the computing stack, from low-level memory hierarchy optimization to high-level compiler transformations and runtime orchestration. Memory bandwidth limitations create fundamental bottlenecks that require sophisticated techniques like data tiling, kernel fusion, and hierarchy-aware scheduling to overcome. Mapping neural network computations to hardware involves complex trade-offs between different dataflow patterns, memory allocation strategies, and execution scheduling approaches that must balance computational efficiency with resource utilization.</p>
<p>Building on these foundational concepts, the emergence of multi-chip and distributed acceleration systems introduces additional complexities around communication overhead, memory coherence, and workload partitioning that require careful system-level optimization.</p>
<div class="callout callout-style-default callout-important callout-titled" title="Key Takeaways">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Key Takeaways
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>Specialized AI accelerators achieve performance gains through domain-specific architectures optimized for tensor operations and dataflow patterns</li>
<li>Memory hierarchy management is often the primary bottleneck in AI acceleration, requiring sophisticated data movement optimization strategies</li>
<li>Hardware-software co-design enables order-of-magnitude improvements by aligning algorithm characteristics with architectural capabilities</li>
<li>Multi-chip scaling introduces distributed computing challenges that require new approaches to communication, synchronization, and resource management</li>
</ul>
</div>
</div>
<p>The principles of hardware acceleration established here provide the foundation for understanding how benchmarking methodologies evaluate accelerator performance and how deployment strategies must account for hardware constraints and capabilities. As AI models continue growing in complexity and computational requirements, the ability to effectively leverage specialized hardware becomes increasingly critical for practical system deployment, influencing everything from energy efficiency and cost optimization to the feasibility of real-time inference and large-scale training across diverse application domains.</p>


<div id="quiz-question-sec-ai-acceleration-summary-a5f8" class="callout callout-quiz-question">
<details class="callout-quiz-question fbx-default closebutton"><summary><strong>Self-Check: Question 1.12</strong></summary><div>
<ol type="1">
<li>
<p>Which of the following best describes the primary advantage of hardware-software co-design in AI accelerators?</p>
<ol type="a">
<li>Reduced hardware costs</li>
<li>Simplified software development</li>
<li>Improved computational efficiency</li>
<li>Increased general-purpose applicability</li>
</ol>
</li>
<li><p>Explain how memory hierarchy management can become a bottleneck in AI acceleration and how it can be mitigated.</p></li>
<li><p>Order the following steps in optimizing a multi-chip AI acceleration system: (1) Workload partitioning, (2) Communication overhead reduction, (3) Memory coherence management.</p></li>
</ol>
<p><a href="#quiz-answer-sec-ai-acceleration-summary-a5f8" class="question-label">See Answers â†’</a></p>
</div></details>
</div>
</section><section id="self-check-answers" class="level2"><h2 class="anchored" data-anchor-id="self-check-answers">Self-Check Answers</h2>
<div id="quiz-answer-sec-ai-acceleration-ai-hardware-acceleration-fundamentals-2096" class="callout callout-quiz-answer">
<details class="callout-quiz-answer fbx-answer closebutton"><summary><strong>Self-Check: Answer 1.1</strong></summary><div>
<ol type="1">
<li>
<p><strong>What is the primary reason for the shift from general-purpose processors to domain-specific hardware in machine learning systems?</strong></p>
<ol type="a">
<li>To reduce the cost of hardware components</li>
<li>To improve the parallel processing capabilities and efficiency</li>
<li>To simplify the design of machine learning algorithms</li>
<li>To increase the utilization of existing software optimizations</li>
</ol>
<p><em>Answer</em>: The correct answer is B. To improve the parallel processing capabilities and efficiency. This shift is driven by the need to address the architectural misalignments of general-purpose processors with the parallel, data-intensive nature of ML workloads.</p>
<p><em>Learning Objective</em>: Understand the motivations behind using domain-specific hardware in ML systems.</p>
</li>
<li>
<p><strong>True or False: Hardware acceleration in machine learning systems only focuses on improving computational speed, not energy efficiency.</strong></p>
<p><em>Answer</em>: False. Hardware acceleration also aims to improve energy efficiency, as data movement energy costs typically exceed computational energy by more than two orders of magnitude.</p>
<p><em>Learning Objective</em>: Recognize the dual goals of hardware acceleration: improving computational speed and energy efficiency.</p>
</li>
<li>
<p><strong>How do architectural selection decisions impact system-level performance in machine learning systems?</strong></p>
<p><em>Answer</em>: Architectural selection decisions impact system-level performance by determining the efficiency of computations, energy usage, and implementation complexity. For example, choosing between GPUs, TPUs, or neuromorphic processors affects how well a system can handle specific ML workloads. This is important because it influences the overall effectiveness and cost-efficiency of deploying ML systems.</p>
<p><em>Learning Objective</em>: Analyze the impact of different hardware architectures on ML system performance.</p>
</li>
<li>
<p><strong>Which of the following architectural innovations is used to optimize matrix multiplication in machine learning workloads?</strong></p>
<ol type="a">
<li>Floating-point coprocessors</li>
<li>Sequential processing models</li>
<li>Systolic array architectures</li>
<li>High-bandwidth memory interfaces</li>
</ol>
<p><em>Answer</em>: The correct answer is C. Systolic array architectures. These are used to optimize matrix multiplication by efficiently managing data flow and computation.</p>
<p><em>Learning Objective</em>: Identify architectural innovations that optimize key ML operations.</p>
</li>
<li>
<p><strong>In a production system, what trade-offs might you consider when choosing between single-chip and multi-chip architectures for AI acceleration?</strong></p>
<p><em>Answer</em>: When choosing between single-chip and multi-chip architectures, trade-offs include balancing computational parallelism with inter-chip communication overhead. Single-chip solutions may offer lower latency and simpler integration, while multi-chip architectures can provide greater computational capacity but may introduce complexity and communication delays. This is important because it affects the scalability and performance of AI systems in different deployment contexts.</p>
<p><em>Learning Objective</em>: Evaluate trade-offs in architectural choices for AI acceleration in production systems.</p>
</li>
</ol>
<p><a href="#quiz-question-sec-ai-acceleration-ai-hardware-acceleration-fundamentals-2096" class="answer-label">â† Back to Questions</a></p>
</div></details>
</div>
<div id="quiz-answer-sec-ai-acceleration-evolution-hardware-specialization-1d21" class="callout callout-quiz-answer">
<details class="callout-quiz-answer fbx-answer closebutton"><summary><strong>Self-Check: Answer 1.2</strong></summary><div>
<ol type="1">
<li>
<p><strong>Which of the following best describes the primary motivation for the development of specialized hardware accelerators in computing?</strong></p>
<ol type="a">
<li>To reduce the cost of general-purpose processors</li>
<li>To increase the flexibility of computing systems</li>
<li>To handle increasingly complex computational workloads efficiently</li>
<li>To simplify the programming models for developers</li>
</ol>
<p><em>Answer</em>: The correct answer is C. To handle increasingly complex computational workloads efficiently. Specialized hardware accelerators are developed to optimize performance and energy efficiency for specific tasks, addressing the limitations of general-purpose processors.</p>
<p><em>Learning Objective</em>: Understand the motivations behind the shift from general-purpose processors to specialized hardware accelerators.</p>
</li>
<li>
<p><strong>Explain how the evolution of specialized hardware has influenced the design of modern machine learning accelerators.</strong></p>
<p><em>Answer</em>: The evolution of specialized hardware, such as FPUs and GPUs, has informed the design of modern ML accelerators by demonstrating the benefits of optimizing hardware for specific computational patterns. This approach has led to significant performance and efficiency gains in executing neural network workloads, which are characterized by predictable data flows and parallelism. For example, tensor cores in GPUs are specifically designed for matrix operations, a common pattern in ML.</p>
<p><em>Learning Objective</em>: Analyze the influence of historical hardware specialization on the design of contemporary ML accelerators.</p>
</li>
<li>
<p><strong>True or False: The integration of specialized functions into general-purpose processors is a common trend observed in the evolution of computing architectures.</strong></p>
<p><em>Answer</em>: True. This is true because successful specialized functions, like floating-point units, are often integrated into general-purpose processors to enhance their capabilities and efficiency over time.</p>
<p><em>Learning Objective</em>: Recognize the trend of integrating specialized functions into general-purpose processors in computing history.</p>
</li>
<li>
<p><strong>What is a key trade-off introduced by the use of specialized hardware accelerators?</strong></p>
<ol type="a">
<li>Increased flexibility in programming</li>
<li>Reduced programming complexity</li>
<li>Higher energy consumption</li>
<li>Reduced silicon area utilization</li>
</ol>
<p><em>Answer</em>: The correct answer is D. Reduced silicon area utilization. Specialized hardware accelerators optimize performance for specific tasks, which can lead to trade-offs in flexibility and silicon area utilization, as they are not as versatile as general-purpose processors.</p>
<p><em>Learning Objective</em>: Identify trade-offs associated with the use of specialized hardware accelerators in computing systems.</p>
</li>
<li>
<p><strong>In a production system, how might the choice of hardware accelerators impact the deployment of machine learning models?</strong></p>
<p><em>Answer</em>: The choice of hardware accelerators can significantly impact the deployment of ML models by affecting performance, energy efficiency, and scalability. For example, using TPUs can accelerate training and inference tasks, reducing time-to-market and operational costs. However, it may also require adjustments in software frameworks and programming models to fully leverage the hardwareâ€™s capabilities. This choice must balance performance gains with integration and development costs.</p>
<p><em>Learning Objective</em>: Evaluate the impact of hardware accelerator choices on the deployment and operation of machine learning models in production systems.</p>
</li>
</ol>
<p><a href="#quiz-question-sec-ai-acceleration-evolution-hardware-specialization-1d21" class="answer-label">â† Back to Questions</a></p>
</div></details>
</div>
<div id="quiz-answer-sec-ai-acceleration-ai-compute-primitives-8471" class="callout callout-quiz-answer">
<details class="callout-quiz-answer fbx-answer closebutton"><summary><strong>Self-Check: Answer 1.3</strong></summary><div>
<ol type="1">
<li>
<p><strong>What is the primary role of AI compute primitives in neural network execution?</strong></p>
<ol type="a">
<li>To optimize the execution of core computational patterns in neural networks</li>
<li>To provide a high-level programming interface for machine learning frameworks</li>
<li>To replace general-purpose CPUs in all computing tasks</li>
<li>To ensure compatibility across different neural network architectures</li>
</ol>
<p><em>Answer</em>: The correct answer is A. To optimize the execution of core computational patterns in neural networks. AI compute primitives are designed to efficiently handle the multiply-accumulate operations that dominate neural network workloads. Options B, C, and D do not accurately describe the role of compute primitives.</p>
<p><em>Learning Objective</em>: Understand the function and importance of AI compute primitives in optimizing neural network computations.</p>
</li>
<li>
<p><strong>Explain how vector operations enhance the efficiency of neural network computations in AI accelerators.</strong></p>
<p><em>Answer</em>: Vector operations enhance efficiency by processing multiple data elements simultaneously, reducing computation time and energy consumption. For example, vector processing units can perform multiple multiply-add operations in parallel, maximizing memory bandwidth utilization and improving throughput. This is important because it enables high-performance execution of neural network layers, which rely on data-parallel computations.</p>
<p><em>Learning Objective</em>: Analyze the role of vector operations in improving computational efficiency in AI systems.</p>
</li>
<li>
<p><strong>The hardware component that performs non-linear transformations like ReLU and sigmoid in a single cycle is known as the ____. </strong></p>
<p><em>Answer</em>: Special Function Unit. This unit is designed to efficiently handle non-linear functions, reducing computational latency and improving performance in neural networks.</p>
<p><em>Learning Objective</em>: Recall the specific hardware components used for non-linear operations in AI accelerators.</p>
</li>
<li>
<p><strong>Order the following computational steps for executing a dense layer in a neural network: (1) Apply activation function, (2) Multiply inputs by weights, (3) Add bias.</strong></p>
<p><em>Answer</em>: The correct order is: (2) Multiply inputs by weights, (3) Add bias, (1) Apply activation function. This sequence reflects the typical computation in a dense layer, where inputs are first transformed by weights, then adjusted by biases, and finally passed through an activation function.</p>
<p><em>Learning Objective</em>: Understand the sequence of operations in neural network layer computations.</p>
</li>
<li>
<p><strong>Which of the following is NOT a characteristic of AI compute primitives?</strong></p>
<ol type="a">
<li>They are frequently used in neural network computations.</li>
<li>They offer significant energy efficiency gains.</li>
<li>They are designed to replace all general-purpose computing tasks.</li>
<li>They remain stable across different neural network architectures.</li>
</ol>
<p><em>Answer</em>: The correct answer is C. They are designed to replace all general-purpose computing tasks. AI compute primitives are specifically optimized for neural network tasks and do not replace all general-purpose computing tasks. Options A, B, and D accurately describe characteristics of AI compute primitives.</p>
<p><em>Learning Objective</em>: Identify the characteristics and limitations of AI compute primitives in machine learning systems.</p>
</li>
</ol>
<p><a href="#quiz-question-sec-ai-acceleration-ai-compute-primitives-8471" class="answer-label">â† Back to Questions</a></p>
</div></details>
</div>
<div id="quiz-answer-sec-ai-acceleration-ai-memory-systems-0057" class="callout callout-quiz-answer">
<details class="callout-quiz-answer fbx-answer closebutton"><summary><strong>Self-Check: Answer 1.4</strong></summary><div>
<ol type="1">
<li>
<p><strong>What is the primary constraint that defines the AI memory wall?</strong></p>
<ol type="a">
<li>The limited number of compute units available in accelerators.</li>
<li>The cost of high-bandwidth memory compared to traditional DRAM.</li>
<li>The energy consumption of arithmetic operations compared to memory access.</li>
<li>The disparity between computational throughput and memory bandwidth.</li>
</ol>
<p><em>Answer</em>: The correct answer is D. The disparity between computational throughput and memory bandwidth. This is correct because the AI memory wall is the fundamental bottleneck due to the growing gap between the two, limiting accelerator performance.</p>
<p><em>Learning Objective</em>: Understand the concept of the AI memory wall and its implications on accelerator performance.</p>
</li>
<li>
<p><strong>Explain how memory hierarchies in AI accelerators balance speed, capacity, and energy efficiency.</strong></p>
<p><em>Answer</em>: Memory hierarchies balance these factors by using multiple levels of memory, each optimized for different trade-offs. Registers and caches provide fast access for frequently used data, while larger but slower memories like DRAM offer greater capacity for less frequently accessed data. This structure minimizes latency and energy consumption while maximizing data availability for compute units.</p>
<p><em>Learning Objective</em>: Analyze how memory hierarchies are structured to optimize AI accelerator performance.</p>
</li>
<li>
<p><strong>The energy penalty for accessing ____ is significantly higher than for computation, influencing AI accelerator design.</strong></p>
<p><em>Answer</em>: DRAM. The energy penalty for accessing DRAM is significantly higher than for computation, influencing AI accelerator design to minimize off-chip memory access.</p>
<p><em>Learning Objective</em>: Recall the energy implications of different memory access types in AI systems.</p>
</li>
<li>
<p><strong>Which neural network architecture is most likely to be constrained by memory capacity and interconnect bandwidth?</strong></p>
<ol type="a">
<li>Transformer Networks</li>
<li>Convolutional Neural Networks (CNNs)</li>
<li>Multilayer Perceptrons (MLPs)</li>
<li>Recurrent Neural Networks (RNNs)</li>
</ol>
<p><em>Answer</em>: The correct answer is A. Transformer Networks. This is because transformers have massive parameter sizes and irregular access patterns, which create significant demands on both memory capacity and interconnect bandwidth.</p>
<p><em>Learning Objective</em>: Identify how different neural network architectures impose distinct memory constraints.</p>
</li>
<li>
<p><strong>In a system design scenario, how might you address the memory bottlenecks imposed by transformer networks?</strong></p>
<p><em>Answer</em>: To address memory bottlenecks in transformer networks, one could use high-bandwidth memory to reduce latency, employ high-speed interconnects for faster data transfer, and optimize data movement with DMA engines. Additionally, leveraging attention caching and tensor tiling can minimize redundant memory accesses, improving overall efficiency.</p>
<p><em>Learning Objective</em>: Evaluate strategies to mitigate memory bottlenecks in specific neural network architectures.</p>
</li>
</ol>
<p><a href="#quiz-question-sec-ai-acceleration-ai-memory-systems-0057" class="answer-label">â† Back to Questions</a></p>
</div></details>
</div>
<div id="quiz-answer-sec-ai-acceleration-hardware-mapping-fundamentals-neural-networks-f9a9" class="callout callout-quiz-answer">
<details class="callout-quiz-answer fbx-answer closebutton"><summary><strong>Self-Check: Answer 1.5</strong></summary><div>
<ol type="1">
<li>
<p><strong>Which of the following best describes the primary goal of mapping in AI acceleration?</strong></p>
<ol type="a">
<li>Optimizing execution efficiency by aligning computations with hardware resources.</li>
<li>Minimizing the energy consumption of the accelerator.</li>
<li>Maximizing the number of processing elements used at any time.</li>
<li>Ensuring all computations are executed in parallel.</li>
</ol>
<p><em>Answer</em>: The correct answer is A. Optimizing execution efficiency by aligning computations with hardware resources. This is correct because mapping aims to maximize resource utilization and minimize memory access costs by strategically placing computations.</p>
<p><em>Learning Objective</em>: Understand the primary objectives of mapping in AI acceleration.</p>
</li>
<li>
<p><strong>True or False: Effective computation placement on AI accelerators always requires manual intervention by developers.</strong></p>
<p><em>Answer</em>: False. This is false because specialized compilers are typically used to automate the mapping process, exploring the search space to find optimal execution plans.</p>
<p><em>Learning Objective</em>: Recognize the role of compilers in automating computation placement on AI accelerators.</p>
</li>
<li>
<p><strong>Why is data locality critical in the mapping of neural networks onto AI accelerators?</strong></p>
<p><em>Answer</em>: Data locality is critical because it minimizes latency and power consumption by keeping frequently accessed data close to processing elements. For example, in specialized matrix processing architectures, data must be preloaded into on-chip scratchpads to maintain efficient execution. This is important because poor data locality can lead to excessive memory access, increasing latency and energy use.</p>
<p><em>Learning Objective</em>: Explain the importance of data locality in neural network mapping.</p>
</li>
<li>
<p><strong>Order the following steps in the mapping process for neural networks on AI accelerators: (1) Data placement, (2) Computation scheduling, (3) Data movement timing.</strong></p>
<p><em>Answer</em>: The correct order is: (1) Data placement, (3) Data movement timing, (2) Computation scheduling. Data placement determines where data is stored, data movement timing manages the transfer between memory levels, and computation scheduling organizes the execution order.</p>
<p><em>Learning Objective</em>: Understand the sequential steps involved in the mapping process for neural networks.</p>
</li>
<li>
<p><strong>In a production system, how might poor computation placement affect the performance of AI accelerators?</strong></p>
<p><em>Answer</em>: Poor computation placement can lead to underutilized processing elements, increased data movement, and execution stalls. For example, if computations are not evenly distributed, some elements may remain idle while others are overloaded. This is important because it can significantly degrade system throughput and efficiency.</p>
<p><em>Learning Objective</em>: Analyze the impact of computation placement on AI accelerator performance in practical scenarios.</p>
</li>
</ol>
<p><a href="#quiz-question-sec-ai-acceleration-hardware-mapping-fundamentals-neural-networks-f9a9" class="answer-label">â† Back to Questions</a></p>
</div></details>
</div>
<div id="quiz-answer-sec-ai-acceleration-dataflow-optimization-strategies-ce52" class="callout callout-quiz-answer">
<details class="callout-quiz-answer fbx-answer closebutton"><summary><strong>Self-Check: Answer 1.6</strong></summary><div>
<ol type="1">
<li>
<p><strong>Which of the following dataflow strategies keeps weights fixed in local memory while streaming input activations through the system?</strong></p>
<ol type="a">
<li>Weight Stationary</li>
<li>Input Stationary</li>
<li>Output Stationary</li>
<li>Activation Stationary</li>
</ol>
<p><em>Answer</em>: The correct answer is A. Weight Stationary. This strategy keeps weights in local memory to maximize reuse, reducing redundant memory fetches and improving energy efficiency.</p>
<p><em>Learning Objective</em>: Understand the basic concept of weight stationary dataflow strategy.</p>
</li>
<li>
<p><strong>True or False: In an output stationary dataflow strategy, input activations are kept fixed in local memory.</strong></p>
<p><em>Answer</em>: False. In output stationary dataflow, partial sums are kept fixed in local memory, while weights and input activations stream through the system.</p>
<p><em>Learning Objective</em>: Distinguish between different dataflow strategies and their memory usage.</p>
</li>
<li>
<p><strong>What are the trade-offs of using an input stationary strategy in a transformer model?</strong></p>
<p><em>Answer</em>: Input stationary strategies keep input activations fixed, reducing redundant fetches and improving data locality. However, it requires efficient streaming of weights and partial sums, which can be challenging if memory bandwidth is limited. This strategy is beneficial in transformer models where input reuse is high.</p>
<p><em>Learning Objective</em>: Analyze the trade-offs of input stationary strategies in specific AI models.</p>
</li>
<li>
<p><strong>In a system design scenario, which dataflow strategy would be most effective for a CNN with high weight reuse?</strong></p>
<ol type="a">
<li>Activation Stationary</li>
<li>Output Stationary</li>
<li>Input Stationary</li>
<li>Weight Stationary</li>
</ol>
<p><em>Answer</em>: The correct answer is D. Weight Stationary. CNNs benefit from weight stationary strategies due to their structured weight reuse across spatial locations, reducing memory bandwidth demands.</p>
<p><em>Learning Objective</em>: Apply dataflow strategy concepts to real-world AI systems.</p>
</li>
<li>
<p><strong>How might you decide between using a weight stationary or output stationary strategy in a new AI model?</strong></p>
<p><em>Answer</em>: The decision depends on the modelâ€™s computational pattern and memory constraints. Weight stationary is ideal for models with high weight reuse, like CNNs, while output stationary suits models where accumulation dominates, like fully connected layers. Consider memory bandwidth, reuse patterns, and hardware capabilities.</p>
<p><em>Learning Objective</em>: Evaluate dataflow strategies based on model characteristics and system constraints.</p>
</li>
</ol>
<p><a href="#quiz-question-sec-ai-acceleration-dataflow-optimization-strategies-ce52" class="answer-label">â† Back to Questions</a></p>
</div></details>
</div>
<div id="quiz-answer-sec-ai-acceleration-compiler-support-172e" class="callout callout-quiz-answer">
<details class="callout-quiz-answer fbx-answer closebutton"><summary><strong>Self-Check: Answer 1.7</strong></summary><div>
<ol type="1">
<li>
<p><strong>Which of the following is a primary focus of machine learning compilers compared to traditional compilers?</strong></p>
<ol type="a">
<li>Graph transformations and kernel fusion</li>
<li>Instruction scheduling and register allocation</li>
<li>Loop unrolling and memory allocation</li>
<li>Sequential program optimization</li>
</ol>
<p><em>Answer</em>: The correct answer is A. Graph transformations and kernel fusion. This is correct because ML compilers optimize computation graphs for efficient tensor operations, unlike traditional compilers that focus on linear code execution.</p>
<p><em>Learning Objective</em>: Understand the primary optimization focus of ML compilers compared to traditional compilers.</p>
</li>
<li>
<p><strong>Explain why kernel fusion is important in machine learning compilers.</strong></p>
<p><em>Answer</em>: Kernel fusion is important because it merges consecutive operations to reduce memory writes and kernel launches, enhancing execution efficiency. For example, in CNNs, fusing convolution, batch normalization, and activation functions accelerates processing. This is important because it minimizes redundant data movement and optimizes parallel execution.</p>
<p><em>Learning Objective</em>: Explain the role and benefits of kernel fusion in optimizing ML models.</p>
</li>
<li>
<p><strong>Order the following stages in the ML compilation pipeline: (1) Graph Optimization, (2) Memory Planning, (3) Kernel Selection, (4) Computation Scheduling.</strong></p>
<p><em>Answer</em>: The correct order is: (1) Graph Optimization, (3) Kernel Selection, (2) Memory Planning, (4) Computation Scheduling. Graph optimization restructures the computation graph, kernel selection maps operations to efficient implementations, memory planning optimizes data placement, and computation scheduling determines execution timing.</p>
<p><em>Learning Objective</em>: Understand the sequence of stages in the ML compilation pipeline and their roles.</p>
</li>
<li>
<p><strong>In a production system, what trade-offs might you consider when selecting kernels for ML model execution?</strong></p>
<ol type="a">
<li>Precision versus performance</li>
<li>All of the above</li>
<li>Execution speed versus memory usage</li>
<li>Power consumption versus accuracy</li>
</ol>
<p><em>Answer</em>: The correct answer is B. All of the above. Kernel selection involves trade-offs between precision, power consumption, execution speed, and memory usage, impacting overall model performance and resource efficiency.</p>
<p><em>Learning Objective</em>: Identify trade-offs involved in kernel selection for ML model execution.</p>
</li>
</ol>
<p><a href="#quiz-question-sec-ai-acceleration-compiler-support-172e" class="answer-label">â† Back to Questions</a></p>
</div></details>
</div>
<div id="quiz-answer-sec-ai-acceleration-runtime-support-f94f" class="callout callout-quiz-answer">
<details class="callout-quiz-answer fbx-answer closebutton"><summary><strong>Self-Check: Answer 1.8</strong></summary><div>
<ol type="1">
<li>
<p><strong>Which of the following best describes a key function of AI runtimes in machine learning systems?</strong></p>
<ol type="a">
<li>Static memory allocation</li>
<li>Sequential task execution</li>
<li>Dynamic kernel execution management</li>
<li>Fixed execution plans</li>
</ol>
<p><em>Answer</em>: The correct answer is C. Dynamic kernel execution management. AI runtimes dynamically manage kernel execution to adapt to real-time system conditions, unlike static memory allocation or fixed execution plans.</p>
<p><em>Learning Objective</em>: Understand the dynamic execution management role of AI runtimes.</p>
</li>
<li>
<p><strong>How do AI runtimes differ from traditional software runtimes in terms of memory management?</strong></p>
<p><em>Answer</em>: AI runtimes dynamically allocate and manage large tensors, optimizing memory access for parallel execution, unlike traditional runtimes that use static allocation for small, frequent memory operations. This is important because it prevents bottlenecks and excessive data movement in AI workloads.</p>
<p><em>Learning Objective</em>: Explain the differences in memory management between AI and traditional runtimes.</p>
</li>
<li>
<p><strong>Order the following tasks in AI runtime management: (1) Memory adaptation, (2) Kernel execution management, (3) Execution scaling.</strong></p>
<p><em>Answer</em>: The correct order is: (2) Kernel execution management, (1) Memory adaptation, (3) Execution scaling. AI runtimes first manage kernel execution based on system state, then adapt memory allocation, and finally scale execution across accelerators.</p>
<p><em>Learning Objective</em>: Understand the sequence of tasks managed by AI runtimes.</p>
</li>
<li>
<p><strong>In a production system, what might be a consequence of poor dynamic kernel execution management?</strong></p>
<ol type="a">
<li>Improved parallel execution</li>
<li>Increased latency and resource underutilization</li>
<li>Reduced memory requirements</li>
<li>Enhanced sequential processing</li>
</ol>
<p><em>Answer</em>: The correct answer is B. Increased latency and resource underutilization. Poor dynamic kernel execution management can lead to inefficient resource use and higher latency due to suboptimal adaptation to runtime conditions.</p>
<p><em>Learning Objective</em>: Analyze the impact of dynamic kernel execution management on system performance.</p>
</li>
</ol>
<p><a href="#quiz-question-sec-ai-acceleration-runtime-support-f94f" class="answer-label">â† Back to Questions</a></p>
</div></details>
</div>
<div id="quiz-answer-sec-ai-acceleration-multichip-ai-acceleration-38d7" class="callout callout-quiz-answer">
<details class="callout-quiz-answer fbx-answer closebutton"><summary><strong>Self-Check: Answer 1.9</strong></summary><div>
<ol type="1">
<li>
<p><strong>What is a primary challenge when transitioning from single-chip to multi-chip AI architectures?</strong></p>
<ol type="a">
<li>Maximizing utilization within fixed resources</li>
<li>Increasing the clock speed of individual chips</li>
<li>Reducing the size of individual processors</li>
<li>Balancing computational distribution against communication overhead</li>
</ol>
<p><em>Answer</em>: The correct answer is D. Balancing computational distribution against communication overhead. This is correct because multi-chip architectures require careful management of how computations and data are distributed across multiple chips, which introduces communication and synchronization challenges. Options A, B, and C do not address the unique challenges of multi-chip systems.</p>
<p><em>Learning Objective</em>: Understand the primary challenges in scaling AI systems from single-chip to multi-chip architectures.</p>
</li>
<li>
<p><strong>Explain how memory coherence challenges differ between chiplet-based architectures and traditional multi-chip systems.</strong></p>
<p><em>Answer</em>: In chiplet-based architectures, memory coherence challenges arise from the need to maintain a consistent view of memory across multiple chiplets within a single package, which requires high-speed interconnects and careful latency management. Traditional multi-chip systems face similar challenges but often with higher latency and complexity due to separate packages. For example, chiplet designs must balance latency-sensitive workloads without introducing excessive bottlenecks, whereas traditional systems may rely on more explicit memory management strategies. This is important because efficient memory coherence is critical for performance in both architectures.</p>
<p><em>Learning Objective</em>: Analyze the differences in memory coherence challenges between chiplet-based and traditional multi-chip systems.</p>
</li>
<li>
<p><strong>True or False: In multi-GPU systems, increasing the number of GPUs always leads to linear performance gains.</strong></p>
<p><em>Answer</em>: False. This is false because increasing the number of GPUs introduces coordination and communication challenges that can limit scalability. For example, the need for frequent gradient synchronization in large models can create bottlenecks that prevent linear scaling. In practice, the coordination complexity and communication overhead can significantly impact performance gains.</p>
<p><em>Learning Objective</em>: Understand the limitations of scaling performance in multi-GPU systems.</p>
</li>
<li>
<p><strong>The fundamental limitation of distributed AI training is that communication overhead constrains parallel speedup according to established _____ principles.</strong></p>
<p><em>Answer</em>: scaling. These scaling principles quantify the impact of communication overhead on the potential speedup of parallel systems, highlighting the limitations in scalability due to sequential bottlenecks.</p>
<p><em>Learning Objective</em>: Recall the key principle that limits scalability in distributed AI training.</p>
</li>
<li>
<p><strong>In a multi-accelerator system design, what is a critical factor that affects performance scaling?</strong></p>
<ol type="a">
<li>The number of CPUs in the system</li>
<li>The efficiency of the specialized interconnect architecture</li>
<li>The use of PCIe interconnects</li>
<li>The clock speed of individual accelerators</li>
</ol>
<p><em>Answer</em>: The correct answer is B. The efficiency of the specialized interconnect architecture. This is correct because multi-accelerator systems rely on efficient interconnect topologies to enable optimal data exchange between accelerators, minimizing communication bottlenecks as workloads scale. Options A, C, and D are less relevant to the specific scaling challenges faced by multi-accelerator systems.</p>
<p><em>Learning Objective</em>: Understand the role of interconnect topology in scaling performance in TPU Pods.</p>
</li>
</ol>
<p><a href="#quiz-question-sec-ai-acceleration-multichip-ai-acceleration-38d7" class="answer-label">â† Back to Questions</a></p>
</div></details>
</div>
<div id="quiz-answer-sec-ai-acceleration-heterogeneous-soc-ai-acceleration-b1bb" class="callout callout-quiz-answer">
<details class="callout-quiz-answer fbx-answer closebutton"><summary><strong>Self-Check: Answer 1.10</strong></summary><div>
<ol type="1">
<li>
<p><strong>What is a primary reason for using heterogeneous SoC architectures in mobile AI systems?</strong></p>
<ol type="a">
<li>To increase computational throughput without power constraints.</li>
<li>To maximize data center workload efficiency.</li>
<li>To simplify the design process by using a single type of processor.</li>
<li>To coordinate multiple specialized processors within strict power and thermal limits.</li>
</ol>
<p><em>Answer</em>: The correct answer is D. To coordinate multiple specialized processors within strict power and thermal limits. This is correct because mobile AI systems operate under stringent constraints that require efficient coordination of diverse processors. Options A, B, and C do not address the specific challenges of mobile environments.</p>
<p><em>Learning Objective</em>: Understand the motivation behind using heterogeneous SoC architectures in constrained environments.</p>
</li>
<li>
<p><strong>Explain how dynamic workload distribution strategies in heterogeneous SoCs help manage power and thermal constraints.</strong></p>
<p><em>Answer</em>: Dynamic workload distribution strategies allocate tasks to processors based on current power and thermal conditions. For example, during high power demand, tasks may shift from power-hungry NPUs to more efficient CPUs. This is important because it ensures system performance while maintaining operational constraints.</p>
<p><em>Learning Objective</em>: Analyze how dynamic workload distribution in heterogeneous SoCs addresses power and thermal challenges.</p>
</li>
<li>
<p><strong>Order the following steps in managing workload distribution on a heterogeneous SoC: (1) Assess system power budget, (2) Evaluate processor thermal state, (3) Allocate tasks based on constraints, (4) Monitor performance and adjust.</strong></p>
<p><em>Answer</em>: The correct order is: (1) Assess system power budget, (2) Evaluate processor thermal state, (3) Allocate tasks based on constraints, (4) Monitor performance and adjust. This sequence ensures that tasks are allocated efficiently while continuously adapting to changing system conditions.</p>
<p><em>Learning Objective</em>: Understand the process of dynamic workload management in heterogeneous SoCs.</p>
</li>
<li>
<p><strong>Which of the following best describes a challenge in programming heterogeneous SoCs?</strong></p>
<ol type="a">
<li>Managing memory coherency across diverse processors.</li>
<li>Ensuring all processors use the same programming model.</li>
<li>Achieving optimal performance without considering processor-specific optimizations.</li>
<li>Implementing a single execution strategy for all tasks.</li>
</ol>
<p><em>Answer</em>: The correct answer is A. Managing memory coherency across diverse processors. This is a challenge because each processor may have different caching and memory access patterns, requiring careful synchronization. Options B, C, and D do not accurately capture the complexity of programming heterogeneous systems.</p>
<p><em>Learning Objective</em>: Identify challenges in software development for heterogeneous SoCs.</p>
</li>
<li>
<p><strong>In a production system, what trade-offs might you consider when implementing AI acceleration on a heterogeneous SoC for an autonomous vehicle?</strong></p>
<p><em>Answer</em>: Trade-offs include balancing real-time processing needs with power efficiency. For example, safety-critical tasks may require deterministic CPU execution, while less critical tasks can run on NPUs. This is important because it affects both performance and energy consumption, crucial for vehicle operation.</p>
<p><em>Learning Objective</em>: Evaluate trade-offs in deploying AI acceleration on heterogeneous SoCs in automotive applications.</p>
</li>
</ol>
<p><a href="#quiz-question-sec-ai-acceleration-heterogeneous-soc-ai-acceleration-b1bb" class="answer-label">â† Back to Questions</a></p>
</div></details>
</div>
<div id="quiz-answer-sec-ai-acceleration-fallacies-pitfalls-dc1f" class="callout callout-quiz-answer">
<details class="callout-quiz-answer fbx-answer closebutton"><summary><strong>Self-Check: Answer 1.11</strong></summary><div>
<ol type="1">
<li>
<p><strong>Which of the following scenarios would most likely benefit from using general-purpose processors over specialized hardware accelerators?</strong></p>
<ol type="a">
<li>A workload with irregular memory access patterns and dynamic computation graphs.</li>
<li>A workload with dense, regular computations and large batch sizes.</li>
<li>A workload that requires high computational throughput with minimal memory access.</li>
<li>A workload optimized for a specific vendorâ€™s proprietary libraries.</li>
</ol>
<p><em>Answer</em>: The correct answer is A. A workload with irregular memory access patterns and dynamic computation graphs. General-purpose processors are better suited for workloads that do not align with the architectural assumptions of specialized hardware, such as irregular memory access patterns and dynamic computation graphs. Specialized hardware is optimized for dense, regular computations.</p>
<p><em>Learning Objective</em>: Understand the conditions under which general-purpose processors may outperform specialized hardware.</p>
</li>
<li>
<p><strong>True or False: Adding more accelerators to a system will always result in linear performance improvements.</strong></p>
<p><em>Answer</em>: False. This is false because multi-accelerator setups introduce communication overhead, synchronization costs, and load balancing challenges that can limit scaling efficiency. Performance gains are often non-linear due to these factors.</p>
<p><em>Learning Objective</em>: Challenge the misconception that performance scales linearly with additional hardware.</p>
</li>
<li>
<p><strong>Explain why memory bandwidth limitations can undermine the performance benefits of AI accelerators.</strong></p>
<p><em>Answer</em>: Memory bandwidth limitations can bottleneck AI accelerators by preventing them from achieving their theoretical computational throughput. If the memory cannot supply data at the rate needed by the accelerator, the hardware remains underutilized. This is important because it highlights the need to balance computational power with memory access capabilities to achieve optimal performance.</p>
<p><em>Learning Objective</em>: Analyze how memory bandwidth constraints affect the real-world performance of AI accelerators.</p>
</li>
<li>
<p><strong>The belief that hardware acceleration benefits scale linearly with additional accelerators is a common ____. This misconception overlooks the communication and synchronization overheads that limit scaling efficiency.</strong></p>
<p><em>Answer</em>: fallacy. This misconception overlooks the communication and synchronization overheads that limit scaling efficiency.</p>
<p><em>Learning Objective</em>: Identify and understand common misconceptions in hardware acceleration scaling.</p>
</li>
<li>
<p><strong>In a production system, what trade-offs should be considered when optimizing for vendor-specific hardware?</strong></p>
<p><em>Answer</em>: Optimizing for vendor-specific hardware can provide significant performance benefits but may lead to vendor lock-in, complicating future upgrades or migrations. This is important because maintaining flexibility and portability can be crucial for long-term system evolution and adaptation to new technologies.</p>
<p><em>Learning Objective</em>: Evaluate the trade-offs between performance optimization and system flexibility in hardware selection.</p>
</li>
</ol>
<p><a href="#quiz-question-sec-ai-acceleration-fallacies-pitfalls-dc1f" class="answer-label">â† Back to Questions</a></p>
</div></details>
</div>
<div id="quiz-answer-sec-ai-acceleration-summary-a5f8" class="callout callout-quiz-answer">
<details class="callout-quiz-answer fbx-answer closebutton"><summary><strong>Self-Check: Answer 1.12</strong></summary><div>
<ol type="1">
<li>
<p><strong>Which of the following best describes the primary advantage of hardware-software co-design in AI accelerators?</strong></p>
<ol type="a">
<li>Reduced hardware costs</li>
<li>Simplified software development</li>
<li>Improved computational efficiency</li>
<li>Increased general-purpose applicability</li>
</ol>
<p><em>Answer</em>: The correct answer is C. Improved computational efficiency. This is correct because hardware-software co-design aligns algorithm characteristics with architectural capabilities, leading to significant performance improvements. Other options do not directly address the efficiency gains from co-design.</p>
<p><em>Learning Objective</em>: Understand the benefits of hardware-software co-design in AI accelerators.</p>
</li>
<li>
<p><strong>Explain how memory hierarchy management can become a bottleneck in AI acceleration and how it can be mitigated.</strong></p>
<p><em>Answer</em>: Memory hierarchy management becomes a bottleneck due to limited bandwidth and latency issues. Techniques like data tiling, kernel fusion, and hierarchy-aware scheduling can mitigate these by optimizing data movement and reducing memory access latency. This is important because efficient memory management is critical for maximizing the performance of AI accelerators.</p>
<p><em>Learning Objective</em>: Analyze the challenges and solutions related to memory hierarchy management in AI acceleration.</p>
</li>
<li>
<p><strong>Order the following steps in optimizing a multi-chip AI acceleration system: (1) Workload partitioning, (2) Communication overhead reduction, (3) Memory coherence management.</strong></p>
<p><em>Answer</em>: The correct order is: (1) Workload partitioning, (3) Memory coherence management, (2) Communication overhead reduction. Workload partitioning is the initial step to distribute tasks across chips. Memory coherence management ensures data consistency across distributed memory. Finally, reducing communication overhead optimizes data transfer between chips.</p>
<p><em>Learning Objective</em>: Understand the process of optimizing multi-chip AI acceleration systems.</p>
</li>
</ol>
<p><a href="#quiz-question-sec-ai-acceleration-summary-a5f8" class="answer-label">â† Back to Questions</a></p>
</div></details>
</div>

</section></section><a onclick="window.scrollTo(0, 0); return false;" role="button" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> Back to top</a></main><!-- /main --><script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    // Ensure there is a toggle, if there isn't float one in the top right
    if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
      const a = window.document.createElement('a');
      a.classList.add('top-right');
      a.classList.add('quarto-color-scheme-toggle');
      a.href = "";
      a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
      const i = window.document.createElement("i");
      i.classList.add('bi');
      a.appendChild(i);
      window.document.body.appendChild(a);
    }
    setColorSchemeToggle(hasAlternateSentinel())
    const icon = "î§‹";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/mlsysbook\.ai\/book\/");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
            // target, if specified
            link.setAttribute("target", "_blank");
            if (link.getAttribute("rel") === null) {
              link.setAttribute("rel", "noopener");
            }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
  });
  </script><nav class="page-navigation"><div class="nav-page nav-page-previous">
      <a href="../../../contents/core/optimizations/optimizations.html" class="pagination-link" aria-label="Model Optimizations">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">Model Optimizations</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../../../contents/core/benchmarking/benchmarking.html" class="pagination-link" aria-label="Benchmarking AI">
        <span class="nav-page-text">Benchmarking AI</span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer"><div class="nav-footer">
    <div class="nav-footer-left">
<p>Â© 2024-2025 Harvard University. Licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC-BY-NC-SA 4.0</a></p>
</div>   
    <div class="nav-footer-center">
<p>Written, edited and curated by Prof.&nbsp;Vijay Janapa Reddi (Harvard University)</p>
</div>
    <div class="nav-footer-right">
      <ul class="footer-items list-unstyled">
<li class="nav-item compact">
    <a class="nav-link" href="https://github.com/harvard-edge/cs249r_book">
      <i class="bi bi-github" role="img" aria-label="View source on GitHub">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/harvard-edge/cs249r_book">
      <i class="bi bi-star" role="img" aria-label="Star this repository">
</i> 
    </a>
  </li>  
</ul>
</div>
  </div>
</footer><script>var lightboxQuarto = GLightbox({"closeEffect":"zoom","descPosition":"bottom","loop":false,"openEffect":"zoom","selector":".lightbox"});
(function() {
  let previousOnload = window.onload;
  window.onload = () => {
    if (previousOnload) {
      previousOnload();
    }
    lightboxQuarto.on('slide_before_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      const href = trigger.getAttribute('href');
      if (href !== null) {
        const imgEl = window.document.querySelector(`a[href="${href}"] img`);
        if (imgEl !== null) {
          const srcAttr = imgEl.getAttribute("src");
          if (srcAttr && srcAttr.startsWith("data:")) {
            slideConfig.href = srcAttr;
          }
        }
      } 
    });
  
    lightboxQuarto.on('slide_after_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(slideNode);
      }
    });
  
  };
  
})();
          </script>


</body></html>