
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>05. Autograd &#8212; Tinyüî•Torch</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.8ecb98da25f57f5357bf6f572d296f466b2cfe2517ffebfabe82451661e28f02.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="../_static/custom.css?v=009d37f4" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script type="module" src="https://cdn.jsdelivr.net/npm/mermaid@10.6.1/dist/mermaid.esm.min.mjs"></script>
    <script type="module" src="https://cdn.jsdelivr.net/npm/@mermaid-js/layout-elk@0.2.0/dist/mermaid-layout-elk.esm.min.mjs"></script>
    <script type="module">import mermaid from "https://cdn.jsdelivr.net/npm/mermaid@10.6.1/dist/mermaid.esm.min.mjs";import elkLayouts from "https://cdn.jsdelivr.net/npm/@mermaid-js/layout-elk@0.2.0/dist/mermaid-layout-elk.esm.min.mjs";mermaid.registerLayoutLoaders(elkLayouts);mermaid.initialize({startOnLoad:false});</script>
    <script src="https://cdn.jsdelivr.net/npm/d3@7.9.0/dist/d3.min.js"></script>
    <script type="module">import mermaid from "https://cdn.jsdelivr.net/npm/mermaid@10.6.1/dist/mermaid.esm.min.mjs";

const defaultStyle = document.createElement('style');
defaultStyle.textContent = `pre.mermaid {
    /* Same as .mermaid-container > pre */
    display: block;
    width: 100%;
}

pre.mermaid > svg {
    /* Same as .mermaid-container > pre > svg */
    height: 500px;
    width: 100%;
    max-width: 100% !important;
}
`;
document.head.appendChild(defaultStyle);

const fullscreenStyle = document.createElement('style');
fullscreenStyle.textContent = `.mermaid-container {
    display: flex;
    flex-direction: row;
    width: 100%;
}

.mermaid-container > pre {
    display: block;
    width: 100%;
}

.mermaid-container > pre > svg {
    height: 500px;
    width: 100%;
    max-width: 100% !important;
}

.mermaid-fullscreen-btn {
    width: 28px;
    height: 28px;
    background: rgba(255, 255, 255, 0.95);
    border: 1px solid rgba(0, 0, 0, 0.3);
    border-radius: 4px;
    cursor: pointer;
    display: flex;
    align-items: center;
    justify-content: center;
    transition: all 0.2s;
    box-shadow: 0 2px 6px rgba(0, 0, 0, 0.2);
    font-size: 14px;
    line-height: 1;
    padding: 0;
    color: #333;
}

.mermaid-fullscreen-btn:hover {
    opacity: 100% !important;
    background: rgba(255, 255, 255, 1);
    box-shadow: 0 3px 10px rgba(0, 0, 0, 0.3);
    transform: scale(1.1);
}

.mermaid-fullscreen-btn.dark-theme {
    background: rgba(50, 50, 50, 0.95);
    border: 1px solid rgba(255, 255, 255, 0.3);
    color: #e0e0e0;
}

.mermaid-fullscreen-btn.dark-theme:hover {
    background: rgba(60, 60, 60, 1);
    box-shadow: 0 3px 10px rgba(255, 255, 255, 0.2);
}

.mermaid-fullscreen-modal {
    display: none;
    position: fixed !important;
    top: 0 !important;
    left: 0 !important;
    width: 95vw;
    height: 100vh;
    background: rgba(255, 255, 255, 0.98);
    z-index: 9999;
    padding: 20px;
    overflow: auto;
}

.mermaid-fullscreen-modal.dark-theme {
    background: rgba(0, 0, 0, 0.98);
}

.mermaid-fullscreen-modal.active {
    display: flex;
    align-items: center;
    justify-content: center;
}

.mermaid-container-fullscreen {
    position: relative;
    width: 95vw;
    height: 90vh;
    max-width: 95vw;
    max-height: 90vh;
    background: white;
    border-radius: 8px;
    padding: 20px;
    box-shadow: 0 10px 40px rgba(0, 0, 0, 0.3);
    overflow: auto;
    display: flex;
    align-items: center;
    justify-content: center;
}

.mermaid-container-fullscreen.dark-theme {
    background: #1a1a1a;
    box-shadow: 0 10px 40px rgba(0, 0, 0, 0.8);
}

.mermaid-container-fullscreen pre.mermaid {
    width: 100%;
    height: 100%;
    display: flex;
    align-items: center;
    justify-content: center;
}

.mermaid-container-fullscreen .mermaid svg {
    height: 100% !important;
    width: 100% !important;
    cursor: grab;
}

.mermaid-fullscreen-close {
    position: fixed !important;
    top: 20px !important;
    right: 20px !important;
    width: 40px;
    height: 40px;
    background: rgba(255, 255, 255, 0.95);
    border: 1px solid rgba(0, 0, 0, 0.2);
    border-radius: 50%;
    cursor: pointer;
    z-index: 10000;
    display: flex;
    align-items: center;
    justify-content: center;
    box-shadow: 0 4px 12px rgba(0, 0, 0, 0.3);
    transition: all 0.2s;
    font-size: 24px;
    line-height: 1;
    color: #333;
}

.mermaid-fullscreen-close:hover {
    background: white;
    box-shadow: 0 6px 16px rgba(0, 0, 0, 0.4);
    transform: scale(1.1);
}

.mermaid-fullscreen-close.dark-theme {
    background: rgba(50, 50, 50, 0.95);
    border: 1px solid rgba(255, 255, 255, 0.2);
    color: #e0e0e0;
}

.mermaid-fullscreen-close.dark-theme:hover {
    background: rgba(60, 60, 60, 1);
    box-shadow: 0 6px 16px rgba(255, 255, 255, 0.2);
}

.mermaid-fullscreen-modal .mermaid-fullscreen-btn {
    display: none !important;
}`;
document.head.appendChild(fullscreenStyle);

// Detect if page has dark background
const isDarkTheme = () => {
    const bgColor = window.getComputedStyle(document.body).backgroundColor;
    const match = bgColor.match(/rgb\((\d+),\s*(\d+),\s*(\d+)/);
    if (match) {
        const r = parseInt(match[1]);
        const g = parseInt(match[2]);
        const b = parseInt(match[3]);
        const brightness = (r * 299 + g * 587 + b * 114) / 1000;
        return brightness < 128;
    }
    return false;
};

const load = async () => {
    await mermaid.run();

    const all_mermaids = document.querySelectorAll(".mermaid");
    const mermaids_processed = document.querySelectorAll(".mermaid[data-processed='true']");

    if ("False" === "True") {
        const mermaids_to_add_zoom = -1 === -1 ? all_mermaids.length : -1;
        if(mermaids_to_add_zoom > 0) {
            var svgs = d3.selectAll("");
            if(all_mermaids.length !== mermaids_processed.length) {
                setTimeout(load, 200);
                return;
            } else if(svgs.size() !== mermaids_to_add_zoom) {
                setTimeout(load, 200);
                return;
            } else {
                svgs.each(function() {
                    var svg = d3.select(this);
                    svg.html("<g class='wrapper'>" + svg.html() + "</g>");
                    var inner = svg.select("g");
                    var zoom = d3.zoom().on("zoom", function(event) {
                        inner.attr("transform", event.transform);
                    });
                    svg.call(zoom);
                });
            }
        }
    } else if(all_mermaids.length !== mermaids_processed.length) {
        // Wait for mermaid to process all diagrams
        setTimeout(load, 200);
        return;
    }

    const darkTheme = isDarkTheme();

    // Stop here if not adding fullscreen capability
    if ("True" !== "True") return;

    const modal = document.createElement('div');
    modal.className = 'mermaid-fullscreen-modal' + (darkTheme ? ' dark-theme' : '');
    modal.setAttribute('role', 'dialog');
    modal.setAttribute('aria-modal', 'true');
    modal.setAttribute('aria-label', 'Fullscreen diagram viewer');
    modal.innerHTML = `
        <button class="mermaid-fullscreen-close${darkTheme ? ' dark-theme' : ''}" aria-label="Close fullscreen">‚úï</button>
        <div class="mermaid-container-fullscreen${darkTheme ? ' dark-theme' : ''}"></div>
    `;
    document.body.appendChild(modal);

    const modalContent = modal.querySelector('.mermaid-container-fullscreen');
    const closeBtn = modal.querySelector('.mermaid-fullscreen-close');

    let previousScrollOffset = [window.scrollX, window.scrollY];

    const closeModal = () => {
        modal.classList.remove('active');
        modalContent.innerHTML = '';
        document.body.style.overflow = ''
        window.scrollTo({left: previousScrollOffset[0], top: previousScrollOffset[1], behavior: 'instant'});
    };

    closeBtn.addEventListener('click', closeModal);
    modal.addEventListener('click', (e) => {
        if (e.target === modal) closeModal();
    });
    document.addEventListener('keydown', (e) => {
        if (e.key === 'Escape' && modal.classList.contains('active')) {
            closeModal();
        }
    });

    const allButtons = [];

    document.querySelectorAll('.mermaid').forEach((mermaidDiv) => {
        if (mermaidDiv.parentNode.classList.contains('mermaid-container') ||
            mermaidDiv.closest('.mermaid-fullscreen-modal')) {
            return;
        }

        const container = document.createElement('div');
        container.className = 'mermaid-container';
        mermaidDiv.parentNode.insertBefore(container, mermaidDiv);
        container.appendChild(mermaidDiv);

        const fullscreenBtn = document.createElement('button');
        fullscreenBtn.className = 'mermaid-fullscreen-btn' + (darkTheme ? ' dark-theme' : '');
        fullscreenBtn.setAttribute('aria-label', 'View diagram in fullscreen');
        fullscreenBtn.textContent = '‚õ∂';
        fullscreenBtn.style.opacity = '50%';

        // Calculate dynamic position based on diagram's margin and padding
        const diagramStyle = window.getComputedStyle(mermaidDiv);
        const marginTop = parseFloat(diagramStyle.marginTop) || 0;
        const marginRight = parseFloat(diagramStyle.marginRight) || 0;
        const paddingTop = parseFloat(diagramStyle.paddingTop) || 0;
        const paddingRight = parseFloat(diagramStyle.paddingRight) || 0;
        fullscreenBtn.style.top = `${marginTop + paddingTop + 4}px`;
        fullscreenBtn.style.right = `${marginRight + paddingRight + 4}px`;

        fullscreenBtn.addEventListener('click', () => {
            previousScrollOffset = [window.scroll, window.scrollY];
            const clone = mermaidDiv.cloneNode(true);
            modalContent.innerHTML = '';
            modalContent.appendChild(clone);

            const svg = clone.querySelector('svg');
            if (svg) {
                svg.removeAttribute('width');
                svg.removeAttribute('height');
                svg.style.width = '100%';
                svg.style.height = 'auto';
                svg.style.maxWidth = '100%';
                svg.style.sdisplay = 'block';

                if ("False" === "True") {
                    setTimeout(() => {
                        const g = svg.querySelector('g');
                        if (g) {
                            var svgD3 = d3.select(svg);
                            svgD3.html("<g class='wrapper'>" + svgD3.html() + "</g>");
                            var inner = svgD3.select("g");
                            var zoom = d3.zoom().on("zoom", function(event) {
                                inner.attr("transform", event.transform);
                            });
                            svgD3.call(zoom);
                        }
                    }, 100);
                }
            }

            modal.classList.add('active');
            document.body.style.overflow = 'hidden';
        });

        container.appendChild(fullscreenBtn);
        allButtons.push(fullscreenBtn);
    });

    // Update theme classes when theme changes
    const updateTheme = () => {
        const dark = isDarkTheme();
        allButtons.forEach(btn => {
            if (dark) {
                btn.classList.add('dark-theme');
            } else {
                btn.classList.remove('dark-theme');
            }
        });
        if (dark) {
            modal.classList.add('dark-theme');
            modalContent.classList.add('dark-theme');
            closeBtn.classList.add('dark-theme');
        } else {
            modal.classList.remove('dark-theme');
            modalContent.classList.remove('dark-theme');
            closeBtn.classList.remove('dark-theme');
        }
    };

    // Watch for theme changes
    const observer = new MutationObserver(updateTheme);
    observer.observe(document.documentElement, {
        attributes: true,
        attributeFilter: ['class', 'style', 'data-theme']
    });
    observer.observe(document.body, {
        attributes: true,
        attributeFilter: ['class', 'style']
    });
};

window.addEventListener("load", load);
</script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'modules/05_autograd_ABOUT';</script>
    <script src="../_static/ml-timeline.js?v=76e9b3e3"></script>
    <script src="../_static/wip-banner.js?v=04a7e74d"></script>
    <script src="../_static/marimo-badges.js?v=e6289128"></script>
    <script src="../_static/sidebar-link.js?v=404b701b"></script>
    <script src="../_static/hero-carousel.js?v=10341d2a"></script>
    <script src="../_static/subscribe-modal.js?v=42919b64"></script>
    <link rel="icon" href="../_static/favicon.svg"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="06. Optimizers" href="06_optimizers_ABOUT.html" />
    <link rel="prev" title="04. Loss Functions" href="04_losses_ABOUT.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo-tinytorch.png" class="logo__image only-light" alt="Tinyüî•Torch - Home"/>
    <script>document.write(`<img src="../_static/logo-tinytorch.png" class="logo__image only-dark" alt="Tinyüî•Torch - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">üöÄ Getting Started</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../getting-started.html">Complete Guide</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">üèó Foundation Tier (01-07)</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../tiers/foundation.html">üìñ Tier Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="01_tensor_ABOUT.html">01. Tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="02_activations_ABOUT.html">02. Activations</a></li>
<li class="toctree-l1"><a class="reference internal" href="03_layers_ABOUT.html">03. Layers</a></li>
<li class="toctree-l1"><a class="reference internal" href="04_losses_ABOUT.html">04. Losses</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">05. Autograd</a></li>
<li class="toctree-l1"><a class="reference internal" href="06_optimizers_ABOUT.html">06. Optimizers</a></li>
<li class="toctree-l1"><a class="reference internal" href="07_training_ABOUT.html">07. Training</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">üèõÔ∏è Architecture Tier (08-13)</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../tiers/architecture.html">üìñ Tier Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="08_dataloader_ABOUT.html">08. DataLoader</a></li>
<li class="toctree-l1"><a class="reference internal" href="09_spatial_ABOUT.html">09. Convolutions</a></li>
<li class="toctree-l1"><a class="reference internal" href="10_tokenization_ABOUT.html">10. Tokenization</a></li>
<li class="toctree-l1"><a class="reference internal" href="11_embeddings_ABOUT.html">11. Embeddings</a></li>
<li class="toctree-l1"><a class="reference internal" href="12_attention_ABOUT.html">12. Attention</a></li>
<li class="toctree-l1"><a class="reference internal" href="13_transformers_ABOUT.html">13. Transformers</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">‚è±Ô∏è Optimization Tier (14-19)</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../tiers/optimization.html">üìñ Tier Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="14_profiling_ABOUT.html">14. Profiling</a></li>
<li class="toctree-l1"><a class="reference internal" href="15_quantization_ABOUT.html">15. Quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="16_compression_ABOUT.html">16. Compression</a></li>
<li class="toctree-l1"><a class="reference internal" href="17_memoization_ABOUT.html">17. Memoization</a></li>
<li class="toctree-l1"><a class="reference internal" href="18_acceleration_ABOUT.html">18. Acceleration</a></li>
<li class="toctree-l1"><a class="reference internal" href="19_benchmarking_ABOUT.html">19. Benchmarking</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">üèÖ Capstone Competition</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../tiers/olympics.html">üìñ Competition Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="20_capstone_ABOUT.html">20. Torch Olympics</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">üß≠ Course Orientation</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../chapters/00-introduction.html">Course Structure</a></li>
<li class="toctree-l1"><a class="reference internal" href="../prerequisites.html">Prerequisites &amp; Resources</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapters/learning-journey.html">Learning Journey</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapters/milestones.html">Historical Milestones</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq.html">FAQ</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">üõ†Ô∏è TITO CLI Reference</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../tito/overview.html">Command Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tito/modules.html">Module Workflow</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tito/milestones.html">Milestone System</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tito/data.html">Progress &amp; Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tito/troubleshooting.html">Troubleshooting</a></li>
<li class="toctree-l1"><a class="reference internal" href="../datasets.html">Datasets Guide</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">ü§ù Community</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../community.html">Ecosystem</a></li>
<li class="toctree-l1"><a class="reference internal" href="../resources.html">Learning Resources</a></li>
<li class="toctree-l1"><a class="reference internal" href="../credits.html">Credits &amp; Acknowledgments</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/modules/05_autograd_ABOUT.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>05. Autograd</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#overview">Overview</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#learning-objectives">Learning Objectives</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#build-use-reflect">Build ‚Üí Use ‚Üí Reflect</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#implementation-guide">Implementation Guide</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#function-base-class-foundation-of-gradient-computation">Function Base Class - Foundation of Gradient Computation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#addbackward-gradient-rules-for-addition">AddBackward - Gradient Rules for Addition</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mulbackward-gradient-rules-for-multiplication">MulBackward - Gradient Rules for Multiplication</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#matmulbackward-gradient-rules-for-matrix-multiplication">MatmulBackward - Gradient Rules for Matrix Multiplication</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#enhanced-tensor-with-backward-method">Enhanced Tensor with backward() Method</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#complete-neural-network-example">Complete Neural Network Example</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#getting-started">Getting Started</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#prerequisites">Prerequisites</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#development-workflow">Development Workflow</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#testing">Testing</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#comprehensive-test-suite">Comprehensive Test Suite</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#test-coverage-areas">Test Coverage Areas</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#inline-testing-mathematical-verification">Inline Testing &amp; Mathematical Verification</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#manual-testing-examples">Manual Testing Examples</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#systems-thinking-questions">Systems Thinking Questions</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#computational-graph-memory-and-construction">Computational Graph Memory and Construction</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#reverse-mode-vs-forward-mode-autodiff">Reverse-Mode vs Forward-Mode Autodiff</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#gradient-accumulation-and-memory-management">Gradient Accumulation and Memory Management</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#real-world-applications">Real-World Applications</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#how-your-implementation-maps-to-pytorch">How Your Implementation Maps to PyTorch</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mathematical-foundations">Mathematical Foundations</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#performance-characteristics">Performance Characteristics</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#ready-to-build">Ready to Build?</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section id="autograd">
<h1>05. Autograd<a class="headerlink" href="#autograd" title="Link to this heading">#</a></h1>
<p><strong>FOUNDATION TIER</strong> | Difficulty: ‚≠ê‚≠ê‚≠ê‚≠ê (4/4) | Time: 8-10 hours</p>
<section id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Link to this heading">#</a></h2>
<p>Build the automatic differentiation engine that makes neural network training possible. This module implements reverse-mode autodiff by enhancing the existing Tensor class with gradient tracking capabilities and creating Function classes that encode gradient computation rules for each operation. You‚Äôll implement the mathematical foundation that transforms TinyTorch from a static computation library into a dynamic, trainable ML framework where calling backward() on any tensor automatically computes gradients throughout the entire computation graph.</p>
<pre  class="mermaid">
        graph BT
    x[&quot;x&lt;br/&gt;Tensor&lt;br/&gt;(input)&quot;] --&gt; mul[&quot;*&lt;br/&gt;MulBackward&quot;]
    w[&quot;w&lt;br/&gt;Tensor&lt;br/&gt;(weight)&quot;] --&gt; mul
    mul --&gt; add[&quot;+&lt;br/&gt;AddBackward&quot;]
    b[&quot;b&lt;br/&gt;Tensor&lt;br/&gt;(bias)&quot;] --&gt; add
    add --&gt; relu[&quot;ReLU&lt;br/&gt;ReLUBackward&quot;]
    relu --&gt; loss[&quot;Loss&lt;br/&gt;Function&quot;]

    loss -.backward().-&gt; relu
    relu -.‚àÇL/‚àÇrelu.-&gt; add
    add -.‚àÇL/‚àÇadd.-&gt; mul
    add -.‚àÇL/‚àÇb.-&gt; b
    mul -.‚àÇL/‚àÇx.-&gt; x
    mul -.‚àÇL/‚àÇw.-&gt; w

    style loss fill:#ffcdd2
    style x fill:#c5e1a5
    style w fill:#c5e1a5
    style b fill:#c5e1a5
    style mul fill:#e1f5fe
    style add fill:#e1f5fe
    style relu fill:#e1f5fe
    </pre><p><strong>Computational Graph Example</strong>: Forward pass (solid arrows) builds the graph, backward pass (dotted arrows) propagates gradients.</p>
</section>
<section id="learning-objectives">
<h2>Learning Objectives<a class="headerlink" href="#learning-objectives" title="Link to this heading">#</a></h2>
<p>By the end of this module, you will be able to:</p>
<ul class="simple">
<li><p><strong>Understand computational graph construction</strong>: Learn how autodiff systems dynamically build directed acyclic graphs during forward pass that track operation dependencies for gradient flow</p></li>
<li><p><strong>Implement Function base class with gradient rules</strong>: Create the Function architecture where each operation (AddBackward, MulBackward, MatmulBackward) implements its specific chain rule derivative computation</p></li>
<li><p><strong>Enhance Tensor class with backward() method</strong>: Add gradient tracking attributes (requires_grad, grad, _grad_fn) and implement reverse-mode differentiation that traverses computation graphs</p></li>
<li><p><strong>Analyze memory overhead and accumulation</strong>: Understand how computation graphs store intermediate values, when gradients accumulate vs. reset, and memory-computation trade-offs in gradient checkpointing</p></li>
<li><p><strong>Connect to PyTorch‚Äôs autograd architecture</strong>: Recognize how your Function classes mirror torch.autograd.Function and understand the enhanced Tensor approach vs. deprecated Variable wrapper pattern</p></li>
</ul>
</section>
<section id="build-use-reflect">
<h2>Build ‚Üí Use ‚Üí Reflect<a class="headerlink" href="#build-use-reflect" title="Link to this heading">#</a></h2>
<p>This module follows TinyTorch‚Äôs <strong>Build ‚Üí Use ‚Üí Reflect</strong> framework:</p>
<ol class="arabic simple">
<li><p><strong>Build</strong>: Implement Function base class and operation-specific gradient functions (AddBackward, MulBackward, MatmulBackward, SumBackward), enhance Tensor class with backward() method, create enable_autograd() that activates gradient tracking</p></li>
<li><p><strong>Use</strong>: Apply automatic differentiation to mathematical expressions, compute gradients for neural network parameters (weights and biases), verify gradient correctness against manual chain rule calculations</p></li>
<li><p><strong>Reflect</strong>: How does computation graph memory scale with network depth? Why does backward pass take 2-3x forward pass time despite similar operations? When does gradient accumulation help vs. hurt training?</p></li>
</ol>
</section>
<section id="implementation-guide">
<h2>Implementation Guide<a class="headerlink" href="#implementation-guide" title="Link to this heading">#</a></h2>
<section id="function-base-class-foundation-of-gradient-computation">
<h3>Function Base Class - Foundation of Gradient Computation<a class="headerlink" href="#function-base-class-foundation-of-gradient-computation" title="Link to this heading">#</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">tinytorch.core.tensor</span><span class="w"> </span><span class="kn">import</span> <span class="n">Tensor</span>

<span class="k">class</span><span class="w"> </span><span class="nc">Function</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Base class for differentiable operations.</span>

<span class="sd">    Each operation (add, multiply, matmul) inherits from Function</span>
<span class="sd">    and implements the apply() method that computes gradients.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">tensors</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Store input tensors needed for backward pass.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">saved_tensors</span> <span class="o">=</span> <span class="n">tensors</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">next_functions</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="c1"># Build computation graph connections</span>
        <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">tensors</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)</span> <span class="ow">and</span> <span class="n">t</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">:</span>
                <span class="k">if</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="s1">&#39;_grad_fn&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">next_functions</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">t</span><span class="o">.</span><span class="n">_grad_fn</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">apply</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">grad_output</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Compute gradients for inputs using chain rule.</span>

<span class="sd">        Args:</span>
<span class="sd">            grad_output: Gradient flowing backward from output</span>

<span class="sd">        Returns:</span>
<span class="sd">            Tuple of gradients for each input tensor</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">&quot;Each Function must implement apply()&quot;</span><span class="p">)</span>

<span class="c1"># Usage: Every operation creates a Function subclass</span>
<span class="c1"># that remembers inputs and knows how to compute gradients</span>
</pre></div>
</div>
</section>
<section id="addbackward-gradient-rules-for-addition">
<h3>AddBackward - Gradient Rules for Addition<a class="headerlink" href="#addbackward-gradient-rules-for-addition" title="Link to this heading">#</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">AddBackward</span><span class="p">(</span><span class="n">Function</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Gradient computation for tensor addition.</span>

<span class="sd">    Mathematical Rule: If z = a + b, then ‚àÇz/‚àÇa = 1 and ‚àÇz/‚àÇb = 1</span>
<span class="sd">    Gradient flows unchanged to both inputs.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">apply</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">grad_output</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Addition distributes gradients equally to both inputs.&quot;&quot;&quot;</span>
        <span class="n">a</span><span class="p">,</span> <span class="n">b</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">saved_tensors</span>
        <span class="n">grad_a</span> <span class="o">=</span> <span class="n">grad_b</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)</span> <span class="ow">and</span> <span class="n">a</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">:</span>
            <span class="n">grad_a</span> <span class="o">=</span> <span class="n">grad_output</span>  <span class="c1"># ‚àÇ(a+b)/‚àÇa = 1</span>

        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)</span> <span class="ow">and</span> <span class="n">b</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">:</span>
            <span class="n">grad_b</span> <span class="o">=</span> <span class="n">grad_output</span>  <span class="c1"># ‚àÇ(a+b)/‚àÇb = 1</span>

        <span class="k">return</span> <span class="n">grad_a</span><span class="p">,</span> <span class="n">grad_b</span>

<span class="c1"># Example: z = x + y computes dz/dx = 1, dz/dy = 1</span>
</pre></div>
</div>
</section>
<section id="mulbackward-gradient-rules-for-multiplication">
<h3>MulBackward - Gradient Rules for Multiplication<a class="headerlink" href="#mulbackward-gradient-rules-for-multiplication" title="Link to this heading">#</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">MulBackward</span><span class="p">(</span><span class="n">Function</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Gradient computation for element-wise multiplication.</span>

<span class="sd">    Mathematical Rule: If z = a * b, then ‚àÇz/‚àÇa = b and ‚àÇz/‚àÇb = a</span>
<span class="sd">    Each input&#39;s gradient equals grad_output times the OTHER input.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">apply</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">grad_output</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Product rule: gradient = grad_output * other_input.&quot;&quot;&quot;</span>
        <span class="n">a</span><span class="p">,</span> <span class="n">b</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">saved_tensors</span>
        <span class="n">grad_a</span> <span class="o">=</span> <span class="n">grad_b</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)</span> <span class="ow">and</span> <span class="n">a</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">:</span>
            <span class="n">grad_a</span> <span class="o">=</span> <span class="n">grad_output</span> <span class="o">*</span> <span class="n">b</span><span class="o">.</span><span class="n">data</span>  <span class="c1"># ‚àÇ(a*b)/‚àÇa = b</span>

        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)</span> <span class="ow">and</span> <span class="n">b</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">:</span>
            <span class="n">grad_b</span> <span class="o">=</span> <span class="n">grad_output</span> <span class="o">*</span> <span class="n">a</span><span class="o">.</span><span class="n">data</span>  <span class="c1"># ‚àÇ(a*b)/‚àÇb = a</span>

        <span class="k">return</span> <span class="n">grad_a</span><span class="p">,</span> <span class="n">grad_b</span>

<span class="c1"># Example: z = x * y computes dz/dx = y, dz/dy = x</span>
</pre></div>
</div>
</section>
<section id="matmulbackward-gradient-rules-for-matrix-multiplication">
<h3>MatmulBackward - Gradient Rules for Matrix Multiplication<a class="headerlink" href="#matmulbackward-gradient-rules-for-matrix-multiplication" title="Link to this heading">#</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">MatmulBackward</span><span class="p">(</span><span class="n">Function</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Gradient computation for matrix multiplication.</span>

<span class="sd">    Mathematical Rule: If Z = A @ B, then:</span>
<span class="sd">    - ‚àÇZ/‚àÇA = grad_Z @ B.T</span>
<span class="sd">    - ‚àÇZ/‚àÇB = A.T @ grad_Z</span>

<span class="sd">    Dimension check: A(m√ók) @ B(k√ón) = Z(m√ón)</span>
<span class="sd">    Backward: grad_Z(m√ón) @ B.T(n√ók) = grad_A(m√ók) ‚úì</span>
<span class="sd">              A.T(k√óm) @ grad_Z(m√ón) = grad_B(k√ón) ‚úì</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">apply</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">grad_output</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Matrix multiplication gradients involve transposing inputs.&quot;&quot;&quot;</span>
        <span class="n">a</span><span class="p">,</span> <span class="n">b</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">saved_tensors</span>
        <span class="n">grad_a</span> <span class="o">=</span> <span class="n">grad_b</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)</span> <span class="ow">and</span> <span class="n">a</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">:</span>
            <span class="c1"># ‚àÇ(A@B)/‚àÇA = grad_output @ B.T</span>
            <span class="n">b_T</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="n">b</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">grad_a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">grad_output</span><span class="p">,</span> <span class="n">b_T</span><span class="p">)</span>

        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)</span> <span class="ow">and</span> <span class="n">b</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">:</span>
            <span class="c1"># ‚àÇ(A@B)/‚àÇB = A.T @ grad_output</span>
            <span class="n">a_T</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">grad_b</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">a_T</span><span class="p">,</span> <span class="n">grad_output</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">grad_a</span><span class="p">,</span> <span class="n">grad_b</span>

<span class="c1"># Core operation for neural network weight gradients</span>
</pre></div>
</div>
<hr class="docutils" />
<p><strong>‚úì CHECKPOINT 1: Computational Graph Construction Complete</strong></p>
<p>You‚Äôve implemented the Function base class and gradient rules for core operations:</p>
<ul class="simple">
<li><p>‚úÖ Function base class with apply() method</p></li>
<li><p>‚úÖ AddBackward, MulBackward, MatmulBackward, SumBackward</p></li>
<li><p>‚úÖ Understanding of chain rule for each operation</p></li>
</ul>
<p><strong>What you can do now</strong>: Build computation graphs during forward pass that track operation dependencies.</p>
<p><strong>Next milestone</strong>: Enhance Tensor class to automatically call these Functions during backward pass.</p>
<p><strong>Progress</strong>: ~40% through Module 05 (~3-4 hours) | Still to come: Tensor.backward() implementation (~4-6 hours)</p>
</section>
<hr class="docutils" />
<section id="enhanced-tensor-with-backward-method">
<h3>Enhanced Tensor with backward() Method<a class="headerlink" href="#enhanced-tensor-with-backward-method" title="Link to this heading">#</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">enable_autograd</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Enhance Tensor class with automatic differentiation capabilities.</span>

<span class="sd">    This function monkey-patches Tensor operations to track gradients:</span>
<span class="sd">    - Replaces __add__, __mul__, matmul with gradient-tracking versions</span>
<span class="sd">    - Adds backward() method for reverse-mode differentiation</span>
<span class="sd">    - Adds zero_grad() method for resetting gradients</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">gradient</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Compute gradients via reverse-mode autodiff.</span>

<span class="sd">        Traverses computation graph backwards, applying chain rule</span>
<span class="sd">        at each operation to propagate gradients to all inputs.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">:</span>
            <span class="k">return</span>

        <span class="c1"># Initialize gradient for scalar outputs</span>
        <span class="k">if</span> <span class="n">gradient</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">size</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
                <span class="n">gradient</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;backward() requires gradient for non-scalars&quot;</span><span class="p">)</span>

        <span class="c1"># Accumulate gradient</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">grad</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">grad</span> <span class="o">+=</span> <span class="n">gradient</span>

        <span class="c1"># Propagate through computation graph</span>
        <span class="n">grad_fn</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;_grad_fn&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">grad_fn</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">grads</span> <span class="o">=</span> <span class="n">grad_fn</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">gradient</span><span class="p">)</span>

            <span class="c1"># Recursively call backward on parent tensors</span>
            <span class="k">for</span> <span class="n">tensor</span><span class="p">,</span> <span class="n">grad</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">grad_fn</span><span class="o">.</span><span class="n">saved_tensors</span><span class="p">,</span> <span class="n">grads</span><span class="p">):</span>
                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)</span> <span class="ow">and</span> <span class="n">tensor</span><span class="o">.</span><span class="n">requires_grad</span> <span class="ow">and</span> <span class="n">grad</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="n">tensor</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">grad</span><span class="p">)</span>

    <span class="c1"># Install backward() method on Tensor class</span>
    <span class="n">Tensor</span><span class="o">.</span><span class="n">backward</span> <span class="o">=</span> <span class="n">backward</span>

<span class="c1"># Usage: enable_autograd() activates gradients globally</span>
</pre></div>
</div>
</section>
<section id="complete-neural-network-example">
<h3>Complete Neural Network Example<a class="headerlink" href="#complete-neural-network-example" title="Link to this heading">#</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">tinytorch.core.autograd</span><span class="w"> </span><span class="kn">import</span> <span class="n">enable_autograd</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">tinytorch.core.tensor</span><span class="w"> </span><span class="kn">import</span> <span class="n">Tensor</span>

<span class="n">enable_autograd</span><span class="p">()</span>  <span class="c1"># Activate gradient tracking</span>

<span class="c1"># Forward pass builds computation graph automatically</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">([[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">]],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">W1</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">([[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">]],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">b1</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">([[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">]],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Each operation stores its Function for backward pass</span>
<span class="n">h1</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">W1</span><span class="p">)</span> <span class="o">+</span> <span class="n">b1</span>  <span class="c1"># h1._grad_fn = AddBackward</span>
                         <span class="c1"># h1 contains MatmulBackward + AddBackward</span>

<span class="n">W2</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">([[</span><span class="mf">0.3</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.5</span><span class="p">]],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">h1</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">W2</span><span class="p">)</span>   <span class="c1"># output._grad_fn = MatmulBackward</span>
<span class="n">loss</span> <span class="o">=</span> <span class="p">(</span><span class="n">output</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>  <span class="c1"># loss._grad_fn = SumBackward</span>

<span class="c1"># Backward pass traverses graph in reverse, computing all gradients</span>
<span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

<span class="c1"># All parameters now have gradients</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;x.grad shape: </span><span class="si">{</span><span class="n">x</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>    <span class="c1"># (1, 3)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;W1.grad shape: </span><span class="si">{</span><span class="n">W1</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>  <span class="c1"># (3, 2)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;b1.grad shape: </span><span class="si">{</span><span class="n">b1</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>  <span class="c1"># (1, 2)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;W2.grad shape: </span><span class="si">{</span><span class="n">W2</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>  <span class="c1"># (2, 1)</span>
</pre></div>
</div>
<hr class="docutils" />
<p><strong>‚úì CHECKPOINT 2: Automatic Differentiation Working</strong></p>
<p>You‚Äôve completed the core autograd implementation:</p>
<ul class="simple">
<li><p>‚úÖ Function classes with gradient computation rules</p></li>
<li><p>‚úÖ Enhanced Tensor with backward() method</p></li>
<li><p>‚úÖ Computational graph traversal in reverse order</p></li>
<li><p>‚úÖ Gradient accumulation and propagation</p></li>
</ul>
<p><strong>What you can do now</strong>: Train any neural network by calling loss.backward() to compute all parameter gradients automatically.</p>
<p><strong>Next milestone</strong>: Apply autograd to complete networks in Module 06 (Optimizers) and Module 07 (Training).</p>
<p><strong>Progress</strong>: ~80% through Module 05 (~7-8 hours) | Still to come: Testing &amp; systems analysis (~1-2 hours)</p>
</section>
</section>
<hr class="docutils" />
<section id="getting-started">
<h2>Getting Started<a class="headerlink" href="#getting-started" title="Link to this heading">#</a></h2>
<section id="prerequisites">
<h3>Prerequisites<a class="headerlink" href="#prerequisites" title="Link to this heading">#</a></h3>
<p>Ensure you understand the mathematical building blocks:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># Activate TinyTorch environment</span>
<span class="nb">source</span><span class="w"> </span>scripts/activate-tinytorch

<span class="c1"># Verify prerequisite modules</span>
tito<span class="w"> </span><span class="nb">test</span><span class="w"> </span>tensor
tito<span class="w"> </span><span class="nb">test</span><span class="w"> </span>activations
tito<span class="w"> </span><span class="nb">test</span><span class="w"> </span>layers
tito<span class="w"> </span><span class="nb">test</span><span class="w"> </span>losses
</pre></div>
</div>
</section>
<section id="development-workflow">
<h3>Development Workflow<a class="headerlink" href="#development-workflow" title="Link to this heading">#</a></h3>
<ol class="arabic simple">
<li><p><strong>Open the development file</strong>: <code class="docutils literal notranslate"><span class="pre">modules/05_autograd/autograd.py</span></code></p></li>
<li><p><strong>Implement Function base class</strong>: Create gradient computation foundation with saved_tensors and apply() method</p></li>
<li><p><strong>Build operation Functions</strong>: Implement AddBackward, MulBackward, SubBackward, DivBackward, MatmulBackward gradient rules</p></li>
<li><p><strong>Add backward() to Tensor</strong>: Implement reverse-mode differentiation with gradient accumulation and graph traversal</p></li>
<li><p><strong>Create enable_autograd()</strong>: Monkey-patch Tensor operations to track gradients and build computation graphs</p></li>
<li><p><strong>Extend to activations and losses</strong>: Add ReLUBackward, SigmoidBackward, MSEBackward, CrossEntropyBackward gradient functions</p></li>
<li><p><strong>Export and verify</strong>: <code class="docutils literal notranslate"><span class="pre">tito</span> <span class="pre">module</span> <span class="pre">complete</span> <span class="pre">05</span> <span class="pre">&amp;&amp;</span> <span class="pre">tito</span> <span class="pre">test</span> <span class="pre">autograd</span></code></p></li>
</ol>
</section>
</section>
<section id="testing">
<h2>Testing<a class="headerlink" href="#testing" title="Link to this heading">#</a></h2>
<section id="comprehensive-test-suite">
<h3>Comprehensive Test Suite<a class="headerlink" href="#comprehensive-test-suite" title="Link to this heading">#</a></h3>
<p>Run the full test suite to verify mathematical correctness:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># TinyTorch CLI (recommended)</span>
tito<span class="w"> </span><span class="nb">test</span><span class="w"> </span>autograd

<span class="c1"># Direct pytest execution</span>
python<span class="w"> </span>-m<span class="w"> </span>pytest<span class="w"> </span>tests/05_autograd/<span class="w"> </span>-v

<span class="c1"># Run specific test categories</span>
python<span class="w"> </span>-m<span class="w"> </span>pytest<span class="w"> </span>tests/05_autograd/test_gradient_flow.py<span class="w"> </span>-v
python<span class="w"> </span>-m<span class="w"> </span>pytest<span class="w"> </span>tests/05_autograd/test_batched_matmul_backward.py<span class="w"> </span>-v
</pre></div>
</div>
</section>
<section id="test-coverage-areas">
<h3>Test Coverage Areas<a class="headerlink" href="#test-coverage-areas" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>‚úÖ <strong>Function Classes</strong>: Verify AddBackward, MulBackward, MatmulBackward compute correct gradients according to mathematical definitions</p></li>
<li><p>‚úÖ <strong>Backward Pass</strong>: Test gradient flow through multi-layer computation graphs with multiple operation types</p></li>
<li><p>‚úÖ <strong>Chain Rule Application</strong>: Ensure composite functions (f(g(x))) correctly apply chain rule: df/dx = (df/dg) √ó (dg/dx)</p></li>
<li><p>‚úÖ <strong>Gradient Accumulation</strong>: Verify gradients accumulate correctly when multiple paths lead to same tensor</p></li>
<li><p>‚úÖ <strong>Broadcasting Gradients</strong>: Test gradient unbroadcasting when operations involve tensors of different shapes</p></li>
<li><p>‚úÖ <strong>Neural Network Integration</strong>: Validate seamless gradient computation through layers, activations, and loss functions</p></li>
</ul>
</section>
<section id="inline-testing-mathematical-verification">
<h3>Inline Testing &amp; Mathematical Verification<a class="headerlink" href="#inline-testing-mathematical-verification" title="Link to this heading">#</a></h3>
<p>The module includes comprehensive mathematical validation:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Example inline test output</span>
<span class="err">üî¨</span> <span class="n">Unit</span> <span class="n">Test</span><span class="p">:</span> <span class="n">Function</span> <span class="n">Classes</span><span class="o">...</span>
<span class="err">‚úÖ</span> <span class="n">AddBackward</span> <span class="n">gradient</span> <span class="n">computation</span> <span class="n">correct</span>
<span class="err">‚úÖ</span> <span class="n">MulBackward</span> <span class="n">gradient</span> <span class="n">computation</span> <span class="n">correct</span>
<span class="err">‚úÖ</span> <span class="n">MatmulBackward</span> <span class="n">gradient</span> <span class="n">computation</span> <span class="n">correct</span>
<span class="err">‚úÖ</span> <span class="n">SumBackward</span> <span class="n">gradient</span> <span class="n">computation</span> <span class="n">correct</span>
<span class="err">üìà</span> <span class="n">Progress</span><span class="p">:</span> <span class="n">Function</span> <span class="n">Classes</span> <span class="err">‚úì</span>

<span class="c1"># Mathematical verification with known derivatives</span>
<span class="err">üî¨</span> <span class="n">Unit</span> <span class="n">Test</span><span class="p">:</span> <span class="n">Tensor</span> <span class="n">Autograd</span> <span class="n">Enhancement</span><span class="o">...</span>
<span class="err">‚úÖ</span> <span class="n">Simple</span> <span class="n">gradient</span><span class="p">:</span> <span class="n">d</span><span class="p">(</span><span class="mi">3</span><span class="n">x</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">/</span><span class="n">dx</span> <span class="o">=</span> <span class="mi">3</span> <span class="err">‚úì</span>
<span class="err">‚úÖ</span> <span class="n">Matrix</span> <span class="n">multiplication</span> <span class="n">gradients</span> <span class="n">match</span> <span class="n">analytical</span> <span class="n">solution</span> <span class="err">‚úì</span>
<span class="err">‚úÖ</span> <span class="n">Multi</span><span class="o">-</span><span class="n">operation</span> <span class="n">chain</span> <span class="n">rule</span> <span class="n">application</span> <span class="n">correct</span> <span class="err">‚úì</span>
<span class="err">‚úÖ</span> <span class="n">Gradient</span> <span class="n">accumulation</span> <span class="n">works</span> <span class="n">correctly</span> <span class="err">‚úì</span>
<span class="err">üìà</span> <span class="n">Progress</span><span class="p">:</span> <span class="n">Autograd</span> <span class="n">Enhancement</span> <span class="err">‚úì</span>

<span class="c1"># Integration test</span>
<span class="err">üß™</span> <span class="n">Integration</span> <span class="n">Test</span><span class="p">:</span> <span class="n">Multi</span><span class="o">-</span><span class="n">layer</span> <span class="n">Neural</span> <span class="n">Network</span><span class="o">...</span>
<span class="err">‚úÖ</span> <span class="n">Forward</span> <span class="k">pass</span> <span class="n">builds</span> <span class="n">computation</span> <span class="n">graph</span> <span class="n">correctly</span>
<span class="err">‚úÖ</span> <span class="n">Backward</span> <span class="k">pass</span> <span class="n">computes</span> <span class="n">gradients</span> <span class="k">for</span> <span class="nb">all</span> <span class="n">parameters</span>
<span class="err">‚úÖ</span> <span class="n">Gradient</span> <span class="n">shapes</span> <span class="n">match</span> <span class="n">parameter</span> <span class="n">shapes</span>
<span class="err">‚úÖ</span> <span class="n">Complex</span> <span class="n">operations</span> <span class="p">(</span><span class="n">matmul</span> <span class="o">+</span> <span class="n">add</span> <span class="o">+</span> <span class="n">mul</span> <span class="o">+</span> <span class="nb">sum</span><span class="p">)</span> <span class="n">work</span> <span class="n">correctly</span>
</pre></div>
</div>
</section>
<section id="manual-testing-examples">
<h3>Manual Testing Examples<a class="headerlink" href="#manual-testing-examples" title="Link to this heading">#</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">tinytorch.core.autograd</span><span class="w"> </span><span class="kn">import</span> <span class="n">enable_autograd</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">tinytorch.core.tensor</span><span class="w"> </span><span class="kn">import</span> <span class="n">Tensor</span>

<span class="n">enable_autograd</span><span class="p">()</span>

<span class="c1"># Test 1: Power rule - d(x^2)/dx = 2x</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">([</span><span class="mf">3.0</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="n">x</span>  <span class="c1"># y = x¬≤</span>
<span class="n">y</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;d(x¬≤)/dx at x=3: </span><span class="si">{</span><span class="n">x</span><span class="o">.</span><span class="n">grad</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>  <span class="c1"># Should be 6.0 ‚úì</span>

<span class="c1"># Test 2: Product rule - d(uv)/dx = u&#39;v + uv&#39;</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">([</span><span class="mf">2.0</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">u</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="n">x</span>      <span class="c1"># u = x¬≤, du/dx = 2x</span>
<span class="n">v</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="n">x</span> <span class="o">*</span> <span class="n">x</span>  <span class="c1"># v = x¬≥, dv/dx = 3x¬≤</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">u</span> <span class="o">*</span> <span class="n">v</span>      <span class="c1"># y = x‚Åµ, dy/dx = 5x‚Å¥</span>
<span class="n">y</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;d(x‚Åµ)/dx at x=2: </span><span class="si">{</span><span class="n">x</span><span class="o">.</span><span class="n">grad</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>  <span class="c1"># Should be 80.0 ‚úì</span>

<span class="c1"># Test 3: Chain rule - d(f(g(x)))/dx = f&#39;(g(x)) √ó g&#39;(x)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">([</span><span class="mf">2.0</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">g</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="n">x</span>           <span class="c1"># g(x) = x¬≤, g&#39;(x) = 2x</span>
<span class="n">f</span> <span class="o">=</span> <span class="n">g</span> <span class="o">+</span> <span class="n">g</span> <span class="o">+</span> <span class="n">g</span>       <span class="c1"># f(g) = 3g, f&#39;(g) = 3</span>
<span class="n">f</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="c1"># df/dx = f&#39;(g) √ó g&#39;(x) = 3 √ó 2x = 6x = 12</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;d(3x¬≤)/dx at x=2: </span><span class="si">{</span><span class="n">x</span><span class="o">.</span><span class="n">grad</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>  <span class="c1"># Should be 12.0 ‚úì</span>

<span class="c1"># Test 4: Gradient accumulation in multi-path graphs</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">([</span><span class="mf">1.0</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">y1</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">x</span>  <span class="c1"># Path 1: dy1/dx = 1 + 1 = 2</span>
<span class="n">y2</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="mi">3</span>  <span class="c1"># Path 2: dy2/dx = 3</span>
<span class="n">z</span> <span class="o">=</span> <span class="n">y1</span> <span class="o">+</span> <span class="n">y2</span> <span class="c1"># z = (x+x) + (3x) = 5x, dz/dx = 5</span>
<span class="n">z</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;dz/dx with multiple paths: </span><span class="si">{</span><span class="n">x</span><span class="o">.</span><span class="n">grad</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>  <span class="c1"># Should be 5.0 ‚úì</span>
</pre></div>
</div>
</section>
</section>
<section id="systems-thinking-questions">
<h2>Systems Thinking Questions<a class="headerlink" href="#systems-thinking-questions" title="Link to this heading">#</a></h2>
<section id="computational-graph-memory-and-construction">
<h3>Computational Graph Memory and Construction<a class="headerlink" href="#computational-graph-memory-and-construction" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Graph Building</strong>: How do operations dynamically construct the computational graph during forward pass? What data structures represent the graph?</p></li>
<li><p><strong>Memory Overhead</strong>: Each Function stores saved_tensors for backward pass. For a ResNet-50 with 50 layers, estimate memory overhead relative to parameters</p></li>
<li><p><strong>Graph Lifetime</strong>: When is the computation graph built? When is it freed? What happens if you call backward() twice without recreating the graph?</p></li>
<li><p><strong>Dynamic vs Static Graphs</strong>: PyTorch builds graphs dynamically (define-by-run) while TensorFlow 1.x used static graphs (define-then-run). What are the trade-offs for debugging, memory, and compilation?</p></li>
</ul>
</section>
<section id="reverse-mode-vs-forward-mode-autodiff">
<h3>Reverse-Mode vs Forward-Mode Autodiff<a class="headerlink" href="#reverse-mode-vs-forward-mode-autodiff" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Computational Complexity</strong>: For function f: ‚Ñù‚Åø ‚Üí ‚Ñù·µê, forward-mode costs O(n) passes, reverse-mode costs O(m) passes. Why do neural networks always use reverse-mode?</p></li>
<li><p><strong>Neural Network Case</strong>: For loss: ‚Ñù·¥∫ ‚Üí ‚Ñù¬π where N=millions of parameters and m=1, what‚Äôs the speedup of reverse-mode vs forward-mode?</p></li>
<li><p><strong>Jacobian Computation</strong>: Forward-mode computes Jacobian-vector products (JVP), reverse-mode computes vector-Jacobian products (VJP). When does each matter?</p></li>
<li><p><strong>Second-Order Derivatives</strong>: Computing Hessians (gradients of gradients) for Newton‚Äôs method requires running autodiff twice. What‚Äôs the memory cost?</p></li>
</ul>
</section>
<section id="gradient-accumulation-and-memory-management">
<h3>Gradient Accumulation and Memory Management<a class="headerlink" href="#gradient-accumulation-and-memory-management" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Intermediate Value Storage</strong>: Backward pass requires values from forward pass (saved_tensors). For 100-layer ResNet, what percentage of memory is computation graph vs parameters?</p></li>
<li><p><strong>Gradient Checkpointing</strong>: Trade computation for memory by recomputing forward pass values during backward. When does this make sense? What‚Äôs the time-memory trade-off?</p></li>
<li><p><strong>Gradient Accumulation</strong>: Processing batch as 4 mini-batches with gradient accumulation uses less memory than single large batch. Why? Does it change training dynamics?</p></li>
<li><p><strong>In-Place Operations</strong>: <code class="docutils literal notranslate"><span class="pre">x</span> <span class="pre">+=</span> <span class="pre">y</span></code> can corrupt gradients by overwriting values needed for backward pass. How do frameworks detect and prevent this?</p></li>
</ul>
</section>
<section id="real-world-applications">
<h3>Real-World Applications<a class="headerlink" href="#real-world-applications" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Deep Learning Training</strong>: Every neural network from ResNets to GPT-4 relies on automatic differentiation for computing weight gradients during backpropagation</p></li>
<li><p><strong>Probabilistic Programming</strong>: Bayesian inference frameworks (Pyro, Stan) use autodiff to compute gradients of log-probability with respect to latent variables</p></li>
<li><p><strong>Robotics and Control</strong>: Trajectory optimization uses autodiff to compute gradients of cost functions with respect to control inputs for gradient-based planning</p></li>
<li><p><strong>Physics Simulations</strong>: Differentiable physics engines use autodiff for inverse problems like inferring material properties from observed motion</p></li>
</ul>
</section>
<section id="how-your-implementation-maps-to-pytorch">
<h3>How Your Implementation Maps to PyTorch<a class="headerlink" href="#how-your-implementation-maps-to-pytorch" title="Link to this heading">#</a></h3>
<p><strong>What you just built:</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Your TinyTorch autograd implementation</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">tinytorch.core.tensor</span><span class="w"> </span><span class="kn">import</span> <span class="n">Tensor</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">tinytorch.core.autograd</span><span class="w"> </span><span class="kn">import</span> <span class="n">AddBackward</span><span class="p">,</span> <span class="n">MulBackward</span>

<span class="c1"># Forward pass with gradient tracking</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">([[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">]],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">w</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">([[</span><span class="mf">0.5</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.7</span><span class="p">]],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>  <span class="c1"># Builds computation graph</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

<span class="c1"># Backward pass computes gradients</span>
<span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>  <span class="c1"># YOUR implementation traverses graph</span>
<span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>  <span class="c1"># Gradients you computed</span>
<span class="nb">print</span><span class="p">(</span><span class="n">w</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>How PyTorch does it:</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># PyTorch equivalent</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>

<span class="c1"># Forward pass with gradient tracking</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">]],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">w</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">0.5</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.7</span><span class="p">]],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">x</span> <span class="o">@</span> <span class="n">w</span>  <span class="c1"># Builds computation graph (same concept)</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

<span class="c1"># Backward pass computes gradients</span>
<span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>  <span class="c1"># PyTorch autograd engine</span>
<span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>  <span class="c1"># Same gradient values</span>
<span class="nb">print</span><span class="p">(</span><span class="n">w</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>Key Insight</strong>: Your <code class="docutils literal notranslate"><span class="pre">Function</span></code> classes (AddBackward, MulBackward, MatmulBackward) implement the <strong>exact same gradient computation rules</strong> that PyTorch uses internally. When you call <code class="docutils literal notranslate"><span class="pre">loss.backward()</span></code>, both implementations traverse the computation graph in reverse topological order, applying the chain rule via each Function‚Äôs backward method.</p>
<p><strong>What‚Äôs the SAME?</strong></p>
<ul class="simple">
<li><p><strong>Computational graph architecture</strong>: Tensor operations create Function nodes</p></li>
<li><p><strong>Gradient computation</strong>: Chain rule via reverse-mode autodiff</p></li>
<li><p><strong>API design</strong>: <code class="docutils literal notranslate"><span class="pre">requires_grad</span></code>, <code class="docutils literal notranslate"><span class="pre">.backward()</span></code>, <code class="docutils literal notranslate"><span class="pre">.grad</span></code> attribute</p></li>
<li><p><strong>Function pattern</strong>: <code class="docutils literal notranslate"><span class="pre">forward()</span></code> computes output, <code class="docutils literal notranslate"><span class="pre">backward()</span></code> computes gradients</p></li>
<li><p><strong>Tensor enhancement</strong>: Gradients stored directly in Tensor (modern PyTorch style, not Variable wrapper)</p></li>
</ul>
<p><strong>What‚Äôs different in production PyTorch?</strong></p>
<ul class="simple">
<li><p><strong>Backend</strong>: C++/CUDA implementation ~100-1000√ó faster</p></li>
<li><p><strong>Memory optimization</strong>: Graph nodes pooled and reused across iterations</p></li>
<li><p><strong>Optimized gradients</strong>: Hand-tuned gradient formulas (e.g., fused operations)</p></li>
<li><p><strong>Advanced features</strong>: Higher-order gradients, gradient checkpointing, JIT compilation</p></li>
</ul>
<p><strong>Why this matters</strong>: When you debug PyTorch training and encounter <code class="docutils literal notranslate"><span class="pre">RuntimeError:</span> <span class="pre">element</span> <span class="pre">0</span> <span class="pre">of</span> <span class="pre">tensors</span> <span class="pre">does</span> <span class="pre">not</span> <span class="pre">require</span> <span class="pre">grad</span></code>, you understand this is checking the computation graph structure you implemented. When gradients are <code class="docutils literal notranslate"><span class="pre">None</span></code>, you know backward() hasn‚Äôt been called or the tensor isn‚Äôt connected to the loss‚Äîconcepts from YOUR implementation.</p>
<p><strong>Production usage example</strong>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># PyTorch production code (after TinyTorch)</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">784</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>  <span class="c1"># Uses torch.Tensor with requires_grad=True</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>

<span class="c1"># Training loop - same workflow you built</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>  <span class="c1"># Forward pass builds graph</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
<span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>  <span class="c1"># Backward pass (YOUR implementation&#39;s logic)</span>
<span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>  <span class="c1"># Update using .grad (YOUR gradients)</span>
</pre></div>
</div>
<p>After implementing autograd yourself, you understand that <code class="docutils literal notranslate"><span class="pre">loss.backward()</span></code> traverses the computation graph you built during forward pass, calling each operation‚Äôs gradient function (AddBackward, MatmulBackward, etc.) in reverse order‚Äîexactly like your implementation.</p>
</section>
<section id="mathematical-foundations">
<h3>Mathematical Foundations<a class="headerlink" href="#mathematical-foundations" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Chain Rule</strong>: ‚àÇf/‚àÇx = (‚àÇf/‚àÇu)(‚àÇu/‚àÇx) for composite functions f(u(x)) - the mathematical foundation of backpropagation</p></li>
<li><p><strong>Computational Graphs as DAGs</strong>: Directed acyclic graphs where nodes are operations and edges are data dependencies enable topological ordering for backward pass</p></li>
<li><p><strong>Jacobians and Matrix Calculus</strong>: For vector-valued functions, gradients are Jacobian matrices. Matrix multiplication gradient rules come from Jacobian chain rule</p></li>
<li><p><strong>Dual Numbers</strong>: Alternative autodiff implementation using numbers with infinitesimals: a + bŒµ where Œµ¬≤ = 0</p></li>
</ul>
</section>
<section id="performance-characteristics">
<h3>Performance Characteristics<a class="headerlink" href="#performance-characteristics" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Time Complexity</strong>: Backward pass takes roughly 2-3x forward pass time (not 1x!) because matmul gradients need two matmuls (grad_x = grad_y &#64; W.T, grad_W = x.T &#64; grad_y)</p></li>
<li><p><strong>Space Complexity</strong>: Computation graph memory scales with number of operations in forward pass, typically 1-2x parameter memory for deep networks</p></li>
<li><p><strong>Numerical Stability</strong>: Gradients can vanish (‚Üí0) or explode (‚Üí‚àû) in deep networks. What causes this? How do residual connections and layer normalization help?</p></li>
<li><p><strong>Sparse Gradients</strong>: Embedding layers produce sparse gradients (most entries zero). Specialized gradient accumulation saves memory</p></li>
</ul>
<div class="tip admonition">
<p class="admonition-title">Systems Reality Check</p>
<p><strong>Production Context</strong>: PyTorch‚Äôs autograd engine processes billions of gradient computations per second using optimized C++ gradient functions, memory pooling, and compiled graph perf. Your Python implementation demonstrates the mathematical principles but runs ~100-1000x slower.</p>
<p><strong>Performance Note</strong>: For ResNet-50 (25M parameters), the computational graph stores ~100MB of intermediate activations during forward pass. Gradient checkpointing reduces this to ~10MB by recomputing activations, trading 30% extra computation for 90% memory savings - critical for training larger models on limited GPU memory.</p>
<p><strong>Architecture Evolution</strong>: PyTorch originally used separate Variable wrapper but merged it into Tensor in v0.4.0 (2018) for cleaner API. Your implementation follows this modern enhanced-Tensor approach, not the deprecated Variable pattern.</p>
</div>
</section>
</section>
<section id="ready-to-build">
<h2>Ready to Build?<a class="headerlink" href="#ready-to-build" title="Link to this heading">#</a></h2>
<p>You‚Äôre about to implement the mathematical foundation that makes modern AI possible. Automatic differentiation is the invisible engine powering every neural network, from simple classifiers to GPT and diffusion models. Before autodiff, researchers manually derived gradient formulas for each layer and loss function - tedious, error-prone, and severely limiting research progress. Automatic differentiation changed everything.</p>
<p>Understanding autodiff from first principles will give you deep insight into how deep learning really works. You‚Äôll implement the Function base class that encodes gradient rules, enhance the Tensor class with backward() that traverses computation graphs, and see why reverse-mode autodiff enables efficient training of billion-parameter models. This is where mathematics meets software engineering to create something truly powerful.</p>
<p>The enhanced Tensor approach you‚Äôll build mirrors modern PyTorch (post-v0.4) where gradients are native Tensor capabilities, not external wrappers. You‚Äôll understand why computation graphs consume memory proportional to network depth, why backward pass takes 2-3x forward pass time, and why gradient checkpointing trades computation for memory. These insights are critical for training large models efficiently.</p>
<p>Take your time with each Function class, verify gradients match manual chain rule calculations, and enjoy building the heart of machine learning. This module transforms TinyTorch from a static math library into a trainable ML framework - the moment everything comes alive.</p>
<p>Choose your preferred way to engage with this module:</p>
<div class="sd-container-fluid sd-sphinx-override sd-mb-4 docutils">
<div class="sd-row sd-row-cols-1 sd-row-cols-xs-1 sd-row-cols-sm-2 sd-row-cols-md-3 sd-row-cols-lg-3 docutils">
<div class="sd-col sd-d-flex-row docutils">
<div class="sd-card sd-sphinx-override sd-w-100 sd-shadow-sm sd-card-hover docutils">
<div class="sd-card-body docutils">
<div class="sd-card-title sd-font-weight-bold docutils">
üöÄ Launch Binder</div>
<p class="sd-card-text">Run this module interactively in your browser. No installation required!</p>
</div>
<a class="sd-stretched-link sd-hide-link-text reference external" href="https://mybinder.org/v2/gh/mlsysbook/TinyTorch/main?filepath=modules/05_autograd/autograd.ipynb"><span>https://mybinder.org/v2/gh/mlsysbook/TinyTorch/main?filepath=modules/05_autograd/autograd.ipynb</span></a></div>
</div>
<div class="sd-col sd-d-flex-row docutils">
<div class="sd-card sd-sphinx-override sd-w-100 sd-shadow-sm sd-card-hover docutils">
<div class="sd-card-body docutils">
<div class="sd-card-title sd-font-weight-bold docutils">
‚ö° Open in Colab</div>
<p class="sd-card-text">Use Google Colab for GPU access and cloud compute power.</p>
</div>
<a class="sd-stretched-link sd-hide-link-text reference external" href="https://colab.research.google.com/github/mlsysbook/TinyTorch/blob/main/modules/05_autograd/autograd.ipynb"><span>https://colab.research.google.com/github/mlsysbook/TinyTorch/blob/main/modules/05_autograd/autograd.ipynb</span></a></div>
</div>
<div class="sd-col sd-d-flex-row docutils">
<div class="sd-card sd-sphinx-override sd-w-100 sd-shadow-sm sd-card-hover docutils">
<div class="sd-card-body docutils">
<div class="sd-card-title sd-font-weight-bold docutils">
üìñ View Source</div>
<p class="sd-card-text">Browse the Python source code and understand the implementation.</p>
</div>
<a class="sd-stretched-link sd-hide-link-text reference external" href="https://github.com/mlsysbook/TinyTorch/blob/main/modules/05_autograd/autograd.py"><span>https://github.com/mlsysbook/TinyTorch/blob/main/modules/05_autograd/autograd.py</span></a></div>
</div>
</div>
</div>
<div class="tip admonition">
<p class="admonition-title">üíæ Save Your Progress</p>
<p><strong>Binder sessions are temporary!</strong> Download your completed notebook when done, or switch to local development for persistent work.</p>
</div>
<hr class="docutils" />
<div class="prev-next-area">
<a class="left-prev" href="../modules/04_losses/ABOUT.html" title="previous page">‚Üê Previous Module</a>
<a class="right-next" href="../modules/06_optimizers/ABOUT.html" title="next page">Next Module ‚Üí</a>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./modules"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="04_losses_ABOUT.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">04. Loss Functions</p>
      </div>
    </a>
    <a class="right-next"
       href="06_optimizers_ABOUT.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">06. Optimizers</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#overview">Overview</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#learning-objectives">Learning Objectives</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#build-use-reflect">Build ‚Üí Use ‚Üí Reflect</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#implementation-guide">Implementation Guide</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#function-base-class-foundation-of-gradient-computation">Function Base Class - Foundation of Gradient Computation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#addbackward-gradient-rules-for-addition">AddBackward - Gradient Rules for Addition</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mulbackward-gradient-rules-for-multiplication">MulBackward - Gradient Rules for Multiplication</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#matmulbackward-gradient-rules-for-matrix-multiplication">MatmulBackward - Gradient Rules for Matrix Multiplication</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#enhanced-tensor-with-backward-method">Enhanced Tensor with backward() Method</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#complete-neural-network-example">Complete Neural Network Example</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#getting-started">Getting Started</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#prerequisites">Prerequisites</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#development-workflow">Development Workflow</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#testing">Testing</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#comprehensive-test-suite">Comprehensive Test Suite</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#test-coverage-areas">Test Coverage Areas</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#inline-testing-mathematical-verification">Inline Testing &amp; Mathematical Verification</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#manual-testing-examples">Manual Testing Examples</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#systems-thinking-questions">Systems Thinking Questions</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#computational-graph-memory-and-construction">Computational Graph Memory and Construction</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#reverse-mode-vs-forward-mode-autodiff">Reverse-Mode vs Forward-Mode Autodiff</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#gradient-accumulation-and-memory-management">Gradient Accumulation and Memory Management</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#real-world-applications">Real-World Applications</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#how-your-implementation-maps-to-pytorch">How Your Implementation Maps to PyTorch</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mathematical-foundations">Mathematical Foundations</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#performance-characteristics">Performance Characteristics</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#ready-to-build">Ready to Build?</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Prof. Vijay Janapa Reddi (Harvard University)
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      ¬© Copyright 2025.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>