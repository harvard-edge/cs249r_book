#!/usr/bin/env python3
"""
Generate changelog entries with AI-powered summaries.

This script generates changelog entries by analyzing Git changes since the last
publication. It can generate simple change counts or AI-powered summaries.
"""

import argparse
import os
import sys
import re
from datetime import datetime
from collections import defaultdict
import yaml

# Global variables
chapter_order = []
LAB_STRUCTURE = None # Added for lab organization

def load_lab_structure(quarto_file="book/config/_quarto-html.yml"):
    """Load lab structure from quarto HTML config file."""
    global LAB_STRUCTURE
    
    if not os.path.exists(quarto_file):
        print(f"‚ö†Ô∏è Quarto config file not found: {quarto_file}")
        return None
    
    try:
        with open(quarto_file, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
        
        # Extract lab sections from the sidebar structure
        lab_sections = {}
        
        if 'website' in config and 'sidebar' in config['website']:
            for i, section in enumerate(config['website']['sidebar']):
                if isinstance(section, dict):
                    # Look for lab-related sections
                    section_id = section.get('id', '')
                    section_title = section.get('section', '')
                    
                    # Check if this is a lab section or contains lab sections
                    if any(keyword in section_id.lower() for keyword in ['arduino', 'seeed', 'grove', 'raspberry', 'shared', 'labs']):
                        lab_sections[section_title] = []
                        
                        # Extract file paths from contents
                        if 'contents' in section:
                            for item in section['contents']:
                                if isinstance(item, dict) and 'href' in item:
                                    file_path = item['href']
                                    # Convert to the actual file path format used in git
                                    if file_path.startswith('contents/'):
                                        file_path = f"book/{file_path}"
                                    lab_sections[section_title].append(file_path)
                    
                    # Also check if this section contains nested lab sections
                    elif 'contents' in section:
                        for item in section['contents']:
                            if isinstance(item, dict):
                                item_id = item.get('id', '')
                                item_title = item.get('section', '')
                                
                                # Check if this nested item is a lab section
                                if any(keyword in item_id.lower() for keyword in ['arduino', 'seeed', 'grove', 'raspberry', 'shared', 'labs']):
                                    lab_sections[item_title] = []
                                    
                                    # Extract file paths from nested contents
                                    if 'contents' in item:
                                        for nested_item in item['contents']:
                                            if isinstance(nested_item, dict) and 'href' in nested_item:
                                                file_path = nested_item['href']
                                                # Convert to the actual file path format used in git
                                                if file_path.startswith('contents/'):
                                                    file_path = f"book/{file_path}"
                                                lab_sections[item_title].append(file_path)
        
        LAB_STRUCTURE = lab_sections
        print(f"‚úÖ Loaded lab structure with {len(lab_sections)} groups")
        return lab_sections
        
    except Exception as e:
        print(f"‚ùå Error loading lab structure: {e}")
        import traceback
        traceback.print_exc()
        return None

def get_lab_group_for_file(file_path):
    """Determine which lab group a file belongs to based on the structure."""
    if not LAB_STRUCTURE:
        return None
    
    # Normalize the file path for comparison
    normalized_path = file_path.replace('book/', '')
    
    for group_name, files in LAB_STRUCTURE.items():
        for group_file in files:
            # Convert group file path to normalized format
            group_file_normalized = group_file.replace('book/', '')
            
            if normalized_path == group_file_normalized:
                return group_name
    
    return None

def organize_labs_by_structure(lab_entries):
    """Organize lab entries according to the structure from quarto config."""
    if not LAB_STRUCTURE:
        # Fallback to flat list if no structure loaded
        return lab_entries
    
    # Group lab entries by their hardware platform
    lab_groups = defaultdict(list)
    
    for entry in lab_entries:
        # Extract file path from the entry (assuming format: "**Title**: Updated content...")
        # We need to match this with the actual file paths
        # For now, we'll use a simple heuristic based on the title
        if "Arduino" in entry or "nicla" in entry.lower():
            lab_groups["Arduino"].append(entry)
        elif "Seeed" in entry or "xiao" in entry.lower():
            lab_groups["Seeed XIAO ESP32S3"].append(entry)
        elif "Grove" in entry or "grove" in entry.lower():
            lab_groups["Grove Vision"].append(entry)
        elif "Raspberry" in entry or "raspi" in entry.lower() or "pi " in entry.lower():
            lab_groups["Raspberry Pi"].append(entry)
        elif "Shared" in entry or "shared" in entry.lower() or "kws_feature" in entry.lower() or "dsp_spectral" in entry.lower():
            lab_groups["Shared"].append(entry)
        elif "Hands-on" in entry or "labs" in entry.lower():
            lab_groups["Hands-on Labs"].append(entry)
        else:
            # Default to a general labs group
            lab_groups["Other Labs"].append(entry)
    
    # Sort each group by impact level and build the organized output
    organized_labs = []
    
    # Use the order from the quarto config
    for group_name in LAB_STRUCTURE.keys():
        if group_name in lab_groups:
            sorted_entries = sort_by_impact_level(lab_groups[group_name])
            if sorted_entries:
                # Calculate total changes for this group
                total_changes = sum(int(re.search(r'(\d+) changes', entry).group(1)) 
                                  for entry in sorted_entries 
                                  if re.search(r'(\d+) changes', entry))
                
                organized_labs.append(f"- **{group_name}**: Updated content with {total_changes} changes")
                for entry in sorted_entries:
                    # Extract just the title and changes, remove the group prefix
                    title_match = re.search(r'\*\*(.*?)\*\*: Updated content with (\d+) changes', entry)
                    if title_match:
                        title = title_match.group(1)
                        changes = title_match.group(2)
                        organized_labs.append(f"  - {title}: Updated content with {changes} changes")
    
    return organized_labs

def extract_chapter_title_from_file(file_path):
    """Extract chapter title from the actual file content by reading the first header."""
    try:
        if not os.path.exists(file_path):
            return None
        
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # Look for the first header (starts with #)
        lines = content.split('\n')
        for line in lines:
            line = line.strip()
            if line.startswith('# '):
                # Extract title from # Title format
                title = line[2:].strip()
                return title
            elif line.startswith('## '):
                # Extract title from ## Title format
                title = line[3:].strip()
                return title
        
        # If no header found, fall back to filename
        basename = os.path.basename(file_path)
        if basename.endswith('.qmd'):
            title = basename[:-4]  # Remove .qmd extension
        else:
            title = basename
        
        return title.replace('_', ' ').title()
        
    except Exception as e:
        print(f"‚ö†Ô∏è Error reading file {file_path}: {e}")
        # Fall back to filename
        basename = os.path.basename(file_path)
        if basename.endswith('.qmd'):
            title = basename[:-4]
        else:
            title = basename
        return title.replace('_', ' ').title()

def load_chapter_order(quarto_file=None):
    """Load chapter order from quarto config file."""
    global chapter_order
    
    if not quarto_file:
        quarto_file = "book/config/_quarto-pdf.yml"
    
    if not os.path.exists(quarto_file):
        print(f"‚ö†Ô∏è Quarto config file not found: {quarto_file}")
        return
    
    try:
        with open(quarto_file, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
        
        def find_chapters(obj):
            chapters = []
            if isinstance(obj, dict):
                for key, value in obj.items():
                    if key == 'contents':
                        chapters.extend(extract_qmd_paths(value))
                    else:
                        chapters.extend(find_chapters(value))
            elif isinstance(obj, list):
                for item in obj:
                    chapters.extend(find_chapters(item))
            return chapters
        
        def extract_qmd_paths(items):
            paths = []
            for item in items:
                if isinstance(item, dict):
                    if 'href' in item:
                        href = item['href']
                        if href.endswith('.qmd'):
                            paths.append(href)
                    if 'contents' in item:
                        paths.extend(extract_qmd_paths(item['contents']))
            return paths
        
        chapter_order = find_chapters(config)
        print(f"üìö Loaded {len(chapter_order)} chapters from {quarto_file}")
        
    except Exception as e:
        print(f"‚ùå Error loading chapter order: {e}")
        chapter_order = []

def call_ollama(prompt, model="gemma2:9b", url="http://localhost:11434"):
    """Call Ollama API to generate AI summaries."""
    try:
        import requests
        import json
        
        payload = {
            "model": model,
            "prompt": prompt,
            "stream": False
        }
        
        response = requests.post(f"{url}/api/generate", json=payload, timeout=30)
        if response.status_code == 200:
            result = response.json()
            return result.get('response', '').strip()
        else:
            print(f"‚ö†Ô∏è Ollama API error: {response.status_code}")
            return None
    except Exception as e:
        print(f"‚ö†Ô∏è Error calling Ollama: {e}")
        return None

def generate_ai_summary(chapter_title, commit_messages, file_path, verbose=False):
    """Generate AI summary for a file based on commit messages."""
    if not commit_messages.strip():
        return f"Updated content with minor changes"
    
    # Create a prompt for AI summary
    prompt = f"""Based on these Git commit messages for {chapter_title} ({file_path}), generate a brief, informative summary of what was updated. Focus on the most important changes and improvements.

Commit messages:
{commit_messages}

Generate a concise summary (1-2 sentences) that describes the key updates:"""
    
    if verbose:
        print(f"ü§ñ Generating AI summary for {chapter_title}...")
    
    ai_summary = call_ollama(prompt)
    
    if ai_summary:
        return ai_summary
    else:
        # Fallback to simple summary
        commit_count = len([msg for msg in commit_messages.split('\n') if msg.strip()])
        return f"Updated content with {commit_count} changes"

def run_git_command(cmd, verbose=False, retries=3):
    """Run a git command and return the output."""
    import subprocess
    
    for attempt in range(retries):
        try:
            result = subprocess.run(cmd, capture_output=True, text=True, check=True)
            if verbose:
                print(f"  üîß {' '.join(cmd)}")
            return result.stdout
        except subprocess.CalledProcessError as e:
            if verbose:
                print(f"  ‚ùå {' '.join(cmd)} failed: {e}")
            if attempt == retries - 1:
                print(f"‚ùå Git command failed after {retries} attempts: {' '.join(cmd)}")
                return ""
            continue
    return ""

def extract_chapter_title(file_path):
    """Extract chapter title from file content or path."""
    # First try to extract from actual file content
    title_from_file = extract_chapter_title_from_file(file_path)
    if title_from_file:
        return title_from_file
    
    # Fallback to path-based extraction
    # Try exact path match first
    normalized_file_path = normalized_path(file_path)
    for chapter_path in chapter_order:
        if normalized_file_path.endswith(normalized_path(chapter_path)):
            # Extract title from the chapter path
            basename = os.path.basename(chapter_path)
            if basename.endswith('.qmd'):
                title = basename[:-4]  # Remove .qmd extension
            else:
                title = basename
            
            # Convert to title case and handle special cases
            title = title.replace('_', ' ').title()
            
            # Add chapter number if available
            for i, ch in enumerate(chapter_order, 1):
                if normalized_file_path.endswith(normalized_path(ch)):
                    if "introduction" in title.lower():
                        return f"Chapter 1: Introduction"
                    elif "ml_systems" in title.lower():
                        return f"Chapter 2: ML Systems"
                    elif "dl_primer" in title.lower():
                        return f"Chapter 3: DL Primer"
                    elif "dnn_architectures" in title.lower():
                        return f"Chapter 4: DNN Architectures"
                    elif "workflow" in title.lower():
                        return f"Chapter 5: AI Workflow"
                    elif "data_engineering" in title.lower():
                        return f"Chapter 6: Data Engineering"
                    elif "frameworks" in title.lower():
                        return f"Chapter 7: AI Frameworks"
                    elif "training" in title.lower():
                        return f"Chapter 8: AI Training"
                    elif "efficient_ai" in title.lower():
                        return f"Chapter 9: Efficient AI"
                    elif "optimizations" in title.lower():
                        return f"Chapter 10: Model Optimizations"
                    elif "hw_acceleration" in title.lower():
                        return f"Chapter 11: AI Acceleration"
                    elif "benchmarking" in title.lower():
                        return f"Chapter 12: Benchmarking AI"
                    elif "ops" in title.lower():
                        return f"Chapter 13: ML Operations"
                    elif "ondevice_learning" in title.lower():
                        return f"Chapter 14: On-Device Learning"
                    elif "privacy_security" in title.lower():
                        return f"Chapter 15: Security & Privacy"
                    elif "responsible_ai" in title.lower():
                        return f"Chapter 16: Responsible AI"
                    elif "sustainable_ai" in title.lower():
                        return f"Chapter 17: Sustainable AI"
                    elif "robust_ai" in title.lower():
                        return f"Chapter 18: Robust AI"
                    elif "ai_for_good" in title.lower():
                        return f"Chapter 19: AI for Good"
                    elif "conclusion" in title.lower():
                        return f"Chapter 20: Conclusion"
                    else:
                        return f"Chapter {i}: {title}"
    
    # Final fallback: extract from file path
    basename = os.path.basename(file_path)
    if basename.endswith('.qmd'):
        title = basename[:-4]
    else:
        title = basename
    
    return title.replace('_', ' ').title()

def sort_by_impact_level(updates):
    """Sort updates by impact level (number of changes)."""
    def extract_impact_level(update):
        # Extract impact bars from the start of each update
        match = re.search(r'(\d+) changes', update)
        return int(match.group(1)) if match else 0
    
    return sorted(updates, key=extract_impact_level, reverse=True)

def get_changes_in_dev_since(date_start, date_end=None, verbose=False):
    """Get all changes in dev branch since a specific date."""
    cmd = ["git", "log", "--pretty=format:", "--numstat", f"--since={date_start}"]
    if date_end:
        cmd.append(f"--until={date_end}")
    cmd.extend(["origin/dev", "--"])
    return run_git_command(cmd, verbose=verbose)

def get_commit_messages_for_file(file_path, since, until=None, verbose=False):
    """Get commit messages for a specific file since a date."""
    cmd = ["git", "log", "--pretty=format:%s", f"--since={since}"]
    if until:
        cmd.append(f"--until={until}")
    cmd.extend(["origin/dev", "--", file_path])
    return run_git_command(cmd, verbose=verbose)

def format_friendly_date(date_str):
    """Format date string to friendly format."""
    try:
        # Try ISO format first (2023-09-16T22:16:31-04:00)
        if 'T' in date_str:
            dt = datetime.fromisoformat(date_str)
        else:
            # Fallback to space-separated format
            dt = datetime.strptime(date_str, "%Y-%m-%d %H:%M:%S %z")
        return dt.strftime("%B %d at %I:%M %p")
    except:
        return date_str

def normalized_path(path):
    """Normalize path for comparison."""
    return os.path.normpath(path).lower()

def generate_entry(start_date, end_date=None, verbose=False, is_latest=False, ai_mode=False, ollama_url="http://localhost:11434", ollama_model="gemma2:9b"):
    """Generate a changelog entry for the specified time period."""
    if verbose:
        print(f"üìÅ Processing changes from {start_date} to {end_date or 'now'}")
    print(f"üîç Analyzing Git changes...")
    changes = get_changes_in_dev_since(start_date, end_date, verbose=verbose)
    if not changes.strip():
        print("  ‚ö†Ô∏è No changes found in specified period")
        return None

    print("üìä Categorizing changes by file...")
    changes_by_file = defaultdict(lambda: [0, 0])
    for line in changes.splitlines():
        parts = line.split("\t")
        if len(parts) != 3:
            continue
        added, removed, file_path = parts
        added = int(added) if added.isdigit() else 0
        removed = int(removed) if removed.isdigit() else 0
        changes_by_file[file_path][0] += added
        changes_by_file[file_path][1] += removed

    current_date = datetime.now().strftime('%B %d at %I:%M %p') if not end_date else format_friendly_date(end_date)
    entry = f"### üìÖ {current_date}\n\n"

    frontmatter, chapters, labs, appendix = [], [], [], []

    ordered_files = sorted(
        changes_by_file,
        key=lambda f: next(
            (i for i, ch in enumerate(chapter_order) if normalized_path(f).endswith(normalized_path(ch))),
            float('inf')
        )
    )

    total_files = len(ordered_files)
    print(f"üìù Processing {total_files} changed files...")
    
    # Filter for only content files (qmd files in book content directories)
    content_files = []
    for file_path in ordered_files:
        # Only include .qmd files in book content directories
        if (file_path.endswith('.qmd') and 
            ('book/contents/' in file_path or 
             'contents/' in file_path or
             file_path.startswith('contents/'))):
            content_files.append(file_path)
    
    total_files = len(content_files)
    print(f"üìù Processing {total_files} content files (filtered from {len(ordered_files)} total files)...")
    
    for idx, file_path in enumerate(content_files, 1):
        added, removed = changes_by_file[file_path]
        total = added + removed
        if verbose:
            print(f"üîç Summarizing {file_path} ({added}+ / {removed}-) [{idx}/{total_files}]")
        else:
            print(f"  üìÑ [{idx}/{total_files}] {os.path.basename(file_path)} ({added}+ {removed}-)")
        
        # Skip references
        if "references.qmd" in file_path:
            continue
            
        commit_msgs = get_commit_messages_for_file(file_path, start_date, end_date, verbose=verbose)
        
        # Skip if no meaningful commits
        if not commit_msgs.strip():
            if verbose:
                print(f"‚è≠Ô∏è Skipping {file_path} - no meaningful changes")
            continue
            
        print(f"    üìù Generating summary...")
        
        # Generate summary based on AI mode
        chapter_title = extract_chapter_title(file_path)
        if ai_mode:
            summary_text = generate_ai_summary(chapter_title, commit_msgs, file_path, verbose=verbose)
            summary = f"- **{chapter_title}**: {summary_text}"
        else:
            # Create simple summary based on file path and commit count
            commit_count = len([msg for msg in commit_msgs.split('\n') if msg.strip()])
            summary_text = f"Updated content with {commit_count} changes"
            summary = f"- **{chapter_title}**: {summary_text}"
        
        # Show the generated summary
        print(f"      üìù {summary_text}")
        
        # Categorize by content type
        if "contents/frontmatter/" in file_path:
            frontmatter.append(summary)
        elif "contents/labs/" in file_path:
            labs.append(summary)
        elif "contents/appendix/" in file_path:
            appendix.append(summary)
        else:
            chapters.append(summary)

    print(f"üìã Organizing into sections...")
    print(f"  üìÑ Frontmatter: {len(frontmatter)} entries")
    print(f"  üìñ Chapters: {len(chapters)} entries")
    print(f"  üßë‚Äçüíª Labs: {len(labs)} entries")
    print(f"  üìö Appendix: {len(appendix)} entries")

    # Determine if sections should be open or closed
    details_state = ""  # Always closed for better UX

    # Add sections in order: Frontmatter, Chapters, Labs, Appendix
    if frontmatter:
        entry += f"<details {details_state}>\n<summary>**üìÑ Frontmatter**</summary>\n\n" + "\n".join(sort_by_impact_level(frontmatter)) + "\n\n</details>\n\n"
    if chapters:
        entry += f"<details {details_state}>\n<summary>**üìñ Chapters**</summary>\n\n" + "\n".join(sort_by_impact_level(chapters)) + "\n\n</details>\n\n"
    if labs:
        # Organize labs according to the structure from quarto config
        organized_labs = organize_labs_by_structure(labs)
        entry += f"<details {details_state}>\n<summary>**üßë‚Äçüíª Labs**</summary>\n\n" + "\n".join(organized_labs) + "\n\n</details>\n\n"
    if appendix:
        entry += f"<details {details_state}>\n<summary>**üìö Appendix**</summary>\n\n" + "\n".join(sort_by_impact_level(appendix)) + "\n\n</details>\n"

    # If no content sections were added, return None (empty entry)
    if not frontmatter and not chapters and not labs and not appendix:
        print("  ‚ö†Ô∏è No meaningful content changes found - skipping entry")
        return None

    print("‚úÖ Entry generation complete")
    return entry

def generate_changelog(mode="incremental", verbose=False, ai_mode=False, ollama_url="http://localhost:11434", ollama_model="gemma2:9b"):
    """Generate changelog entries."""
    print("üîÑ Starting Git data fetch...")
    print("  üì¶ Fetching gh-pages branch...")
    run_git_command(["git", "fetch", "origin", "gh-pages:refs/remotes/origin/gh-pages"], verbose=verbose)
    print("  üì¶ Fetching dev branch...")
    run_git_command(["git", "fetch", "origin", "dev:refs/remotes/origin/dev"], verbose=verbose)
    print("‚úÖ Git data fetch complete")

    def get_latest_gh_pages_commit():
        print("üîç Looking for latest publication commit...")
        # Look for actual publication commits, not administrative ones
        output = run_git_command(["git", "log", "--pretty=format:%H %aI", "--grep=Built site for gh-pages", "origin/gh-pages"], verbose=verbose)
        if output.strip():
            first_line = output.split('\n')[0]
            parts = first_line.split(" ", 1)
            result = (parts[0], parts[1]) if len(parts) == 2 else (None, None)
            if result[0]:
                print(f"  üìÖ Found latest commit: {result[0][:8]} from {result[1]}")
            return result
        print("  ‚ö†Ô∏è No publication commits found")
        return (None, None)

    def get_all_gh_pages_commits():
        print("üîç Scanning all publication commits...")
        # Look for actual publication commits, not administrative ones
        output = run_git_command(["git", "log", "--pretty=format:%H %aI", "--grep=Built site for gh-pages", "origin/gh-pages"], verbose=verbose)
        commits = []
        for line in output.splitlines():
            parts = line.split(" ", 1)
            if len(parts) == 2:
                commits.append((parts[0], parts[1]))
        print(f"  üìä Found {len(commits)} publication commits")
        return commits

    def extract_year_from_date(date_str):
        try:
            # Try ISO format first (2023-09-16T22:16:31-04:00)
            return datetime.fromisoformat(date_str.replace('Z', '+00:00')).year
        except:
            try:
                # Try the old format as fallback
                return datetime.strptime(date_str, "%Y-%m-%d %H:%M:%S %z").year
            except:
                return datetime.now().year

    latest_commit, latest_date = get_latest_gh_pages_commit()

    if mode == "full":
        if verbose:
            print("üîÅ Running full regeneration...")
        commits = get_all_gh_pages_commits()
        
        # Group commits by date (YYYY-MM-DD) to merge same-day publishes
        def extract_date_only(date_str):
            try:
                # Try ISO format first (2023-09-16T22:16:31-04:00)
                return datetime.fromisoformat(date_str.replace('Z', '+00:00')).strftime("%Y-%m-%d")
            except:
                try:
                    # Try the old format as fallback
                    return datetime.strptime(date_str, "%Y-%m-%d %H:%M:%S %z").strftime("%Y-%m-%d")
                except:
                    return date_str.split()[0]  # fallback to first part
        
        # Group commits by publication date
        commits_by_date = defaultdict(list)
        for commit, date in commits:
            date_key = extract_date_only(date)
            commits_by_date[date_key].append((commit, date))
        
        # Sort dates and get unique publication periods
        unique_dates = sorted(commits_by_date.keys(), reverse=True)  # newest first
        print(f"üìä Found {len(unique_dates)} unique publication dates...")
        
        # Group entries by year
        entries_by_year = defaultdict(list)
        
        for date_key in unique_dates:
            # Get the latest commit for this date
            latest_commit_for_date = commits_by_date[date_key][0]
            entry = generate_entry(latest_commit_for_date[1], verbose=verbose, ai_mode=ai_mode, ollama_url=ollama_url, ollama_model=ollama_model)
            if entry:
                year = extract_year_from_date(latest_commit_for_date[1])
                entries_by_year[year].append(entry)
        
        # Build output with year headers
        output_sections = []
        for year in sorted(entries_by_year.keys(), reverse=True):
            year_header = f"## {year} Updates"
            year_entries = "\n\n".join(entries_by_year[year])
            output_sections.append(f"{year_header}\n\n{year_entries}")
        
        return "\n\n---\n\n".join(output_sections) + "\n"
        
    else:
        if verbose:
            print("‚ö° Running update mode...")
        print(f"üìÖ Processing changes since: {format_friendly_date(latest_date) if latest_date else 'beginning'}")
        entry = generate_entry(latest_date, verbose=verbose, is_latest=True, ai_mode=ai_mode, ollama_url=ollama_url, ollama_model=ollama_model)
        if not entry:
            return "_No updates found._"
        
        # Extract year from the latest date instead of using current year
        if latest_date:
            current_year = extract_year_from_date(latest_date)
        else:
            current_year = datetime.now().year
        year_header = f"## {current_year} Updates"
        return f"{year_header}\n\n{entry}"

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Generate changelog entries with AI-powered summaries.")
    
    # Changelog mode arguments
    parser.add_argument("--full", action="store_true", help="Regenerate the entire changelog from scratch.")
    parser.add_argument("--incremental", action="store_true", help="Add new entries since last gh-pages publish.")
    
    # General options
    parser.add_argument("-t", "--test", action="store_true", help="Run without writing to file.")
    parser.add_argument("-v", "--verbose", action="store_true", help="Verbose output.")
    parser.add_argument("-q", "--quarto-config", type=str, help="Path to quarto config file (default: book/config/_quarto-pdf.yml)")
    
    # AI options
    parser.add_argument("--ai-mode", action="store_true", help="Enable AI-generated summaries instead of simple change counts.")
    parser.add_argument("--ollama-url", default="http://localhost:11434", help="Ollama API URL for AI summaries.")
    parser.add_argument("--ollama-model", default="gemma2:9b", help="Ollama model to use for AI summaries.")

    args = parser.parse_args()
    
    # Load configuration
    try:
        load_chapter_order(args.quarto_config)
        load_lab_structure("book/config/_quarto-html.yml") # Load lab structure from HTML config
        
        # Print configuration header
        print("=" * 60)
        print("üìù CHANGELOG GENERATION CONFIG")
        print("=" * 60)
        print(f"üéØ Mode: {'FULL' if args.full else 'UPDATE'}")
        print(f"üîß Test Mode: {'ON' if args.test else 'OFF'}")
        print(f"üì¢ Verbose: {'ON' if args.verbose else 'OFF'}")
        print(f"ü§ñ AI Mode: {'ON' if args.ai_mode else 'OFF'}")
        if args.ai_mode:
            print(f"ü§ñ AI Model: {args.ollama_model}")
            print(f"ü§ñ AI URL: {args.ollama_url}")
        print(f"üìã Features: Impact bars, importance sorting, specific summaries")
        print("=" * 60)
        print()
        
        print("üöÄ Starting changelog generation...")
        
        # Determine mode
        mode = "full" if args.full else "incremental"
        
        # Generate changelog
        new_entry = generate_changelog(mode=mode, verbose=args.verbose, ai_mode=args.ai_mode, ollama_url=args.ollama_url, ollama_model=args.ollama_model)
        
        if args.test:
            # Display the generated content for test mode
            print("üß™ TEST MODE - Generated changelog entry:")
            print("=" * 60)
            print(new_entry)
            print("=" * 60)
            print(f"üìä Content length: {len(new_entry)} characters")
        else:
            # Write to CHANGELOG.md
            if new_entry and new_entry != "_No updates found._":
                # Read existing changelog
                changelog_file = "CHANGELOG.md"
                existing_content = ""
                if os.path.exists(changelog_file):
                    with open(changelog_file, 'r', encoding='utf-8') as f:
                        existing_content = f.read()
                
                # Insert new entry at the top
                if existing_content.strip():
                    updated_content = new_entry + "\n\n---\n\n" + existing_content
                else:
                    updated_content = new_entry
                
                # Write back to file
                with open(changelog_file, 'w', encoding='utf-8') as f:
                    f.write(updated_content)
                
                print(f"‚úÖ Changelog updated: {changelog_file}")
                print(f"üìä File size: {len(updated_content)} characters")
            else:
                print("‚ÑπÔ∏è No changes to add to changelog")
        
    except KeyboardInterrupt:
        print("\n‚ö†Ô∏è Process interrupted by user")
        sys.exit(1)
    except Exception as e:
        print(f"‚ùå Error: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1) 