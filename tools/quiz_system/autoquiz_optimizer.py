#!/usr/bin/env python3
"""
AutoQuiz Optimizer: Autonomous Quiz Improvement System
=======================================================

This system autonomously improves quiz generation by:
1. Fixing MCQ answer bias (currently 60% B answers!)
2. Adding more CALC questions (currently <1%)
3. Balancing question type variety
4. Building on prior knowledge using knowledge map
"""

import sys
import os
import json
import time
import argparse
from pathlib import Path
from typing import Dict, List, Optional, Tuple
from collections import Counter
from datetime import datetime

# Add parent directory to path
sys.path.insert(0, str(Path(__file__).parent.parent / "scripts" / "genai"))
sys.path.insert(0, str(Path(__file__).parent))

# Import existing quiz generator and knowledge map
import quizzes
from knowledge_map import KnowledgeMap, enhance_prompt_with_knowledge_map

class AutoQuizOptimizer:
    """
    Autonomous system for optimizing quiz generation.
    Focuses on the real problems identified in analysis.
    """
    
    def __init__(self):
        self.knowledge_map = KnowledgeMap()
        self.base_path = Path("/Users/VJ/GitHub/MLSysBook")
        self.results_dir = self.base_path / "experiments" / "optimized_quizzes"
        self.results_dir.mkdir(parents=True, exist_ok=True)
        
        # Track MCQ distribution across generation
        self.global_mcq_tracker = Counter({'A': 0, 'B': 0, 'C': 0, 'D': 0})
        
        # Define technical vs conceptual chapters
        self.technical_chapters = [
            "training", "optimizations", "hw_acceleration", 
            "benchmarking", "efficient_ai", "ops", "data_engineering"
        ]
        
        self.conceptual_chapters = [
            "introduction", "responsible_ai", "privacy_security",
            "sustainable_ai", "ai_for_good", "robust_ai"
        ]
    
    def get_optimized_prompt(self, chapter: str, current_mcq_distribution: Counter) -> str:
        """Create an optimized prompt based on chapter type and current issues."""
        
        # Start with base prompt
        base_prompt = quizzes.SYSTEM_PROMPT
        
        # Add critical fixes
        optimizations = []
        
        # FIX 1: MCQ Answer Balance (CRITICAL - currently 60% B!)
        least_used = min(current_mcq_distribution, key=current_mcq_distribution.get)
        second_least = sorted(current_mcq_distribution, key=current_mcq_distribution.get)[1]
        
        mcq_balance_fix = f"""

## CRITICAL: MCQ ANSWER DISTRIBUTION FIX

**CURRENT SEVERE IMBALANCE**: The quiz system has generated:
- A: {current_mcq_distribution['A']} answers ({current_mcq_distribution['A']/sum(current_mcq_distribution.values())*100:.1f}%)
- B: {current_mcq_distribution['B']} answers ({current_mcq_distribution['B']/sum(current_mcq_distribution.values())*100:.1f}%)
- C: {current_mcq_distribution['C']} answers ({current_mcq_distribution['C']/sum(current_mcq_distribution.values())*100:.1f}%)
- D: {current_mcq_distribution['D']} answers ({current_mcq_distribution['D']/sum(current_mcq_distribution.values())*100:.1f}%)

**MANDATORY REQUIREMENT**: 
- Make {least_used} the correct answer for at least 40% of MCQs in this section
- Make {second_least} the correct answer for at least 30% of MCQs
- This is to restore balance across the entire quiz bank

**MCQ Generation Rules**:
1. The correct answer position must vary
2. Never make the most detailed option always correct
3. Avoid patterns like "B for definitions, C for applications"
4. Distractors must be equally plausible
"""
        optimizations.append(mcq_balance_fix)
        
        # FIX 2: Question Type Variety
        if chapter in self.technical_chapters:
            # Technical chapters need CALC questions
            calc_emphasis = """

## QUESTION TYPE REQUIREMENTS FOR TECHNICAL CHAPTER

**MANDATORY Distribution**:
- CALC: 25-30% (currently <1% globally!) - PRIORITY
- MCQ: 20-25% (reduce from current)
- SHORT: 25-30% (for explaining tradeoffs)
- TF: 10-15% (for misconceptions)
- FILL: 5-10% (for key terms)
- ORDER: 5-10% (for processes)

**CALC Question Requirements**:
Generate 1-2 CALC questions per section using these patterns:

For Optimizations Chapter:
- "Calculate memory savings: A 7B parameter model (FP32) is quantized to INT8. What's the memory reduction?"
- "Calculate pruning impact: After 70% unstructured pruning, what's the theoretical FLOP reduction?"
- "Calculate compression ratio: Teacher model (175B params) â†’ Student model (7B params)"

For Training Chapter:
- "Calculate gradient update: Given learning rate 0.001 and gradient 0.5, what's the new weight?"
- "Calculate batch memory: Model size 500MB, batch size 32, activation memory 10MB/sample"
- "Calculate effective batch size with gradient accumulation over 4 steps, micro-batch 8"

For Hardware Chapter:
- "Calculate arithmetic intensity: Kernel with 1000 FLOPs accessing 100 bytes"
- "Calculate roofline limit: Peak 10 TFLOPS, bandwidth 100 GB/s, arithmetic intensity 5"
- "Calculate GPU utilization: 8 GPUs, data parallel, 50ms per batch, batch size 256"

Show step-by-step calculations with units in answers.
"""
        elif chapter in self.conceptual_chapters:
            # Conceptual chapters need reflection questions
            calc_emphasis = """

## QUESTION TYPE REQUIREMENTS FOR CONCEPTUAL CHAPTER

**MANDATORY Distribution**:
- SHORT: 30-35% (for reflection and analysis)
- TF: 20-25% (for principles and misconceptions)
- MCQ: 20-25% (reduce from current)
- FILL: 10-15% (for key concepts)
- ORDER: 5-10% (for processes)
- CALC: 0-5% (only if relevant)

Focus on understanding, implications, and critical thinking rather than calculations.
"""
        else:
            # Balanced chapters
            calc_emphasis = """

## QUESTION TYPE REQUIREMENTS

**MANDATORY Distribution**:
- SHORT: 25-30%
- MCQ: 20-25%
- TF: 15-20%
- CALC: 10-15% (increase from <1%!)
- FILL: 10-15%
- ORDER: 5-10%

Include at least 1 CALC question per section where applicable.
"""
        
        optimizations.append(calc_emphasis)
        
        # FIX 3: Remove any Bloom's taxonomy that might be causing B-bias
        blooms_fix = """

## SIMPLIFIED QUESTION GENERATION

Instead of complex Bloom's levels, focus on:
1. **Variety**: Different question types per section
2. **Progression**: Easy â†’ Medium â†’ Hard within each section
3. **Balance**: Rotate correct MCQ answers deliberately
4. **Relevance**: Connect to prior chapters when possible
"""
        optimizations.append(blooms_fix)
        
        # Combine all optimizations
        optimized_prompt = base_prompt
        for opt in optimizations:
            optimized_prompt += opt
        
        return optimized_prompt
    
    def optimize_single_chapter(self, chapter: str, force_regenerate: bool = False) -> Dict:
        """Optimize quiz generation for a single chapter."""
        
        print(f"\n{'='*60}")
        print(f"OPTIMIZING: {chapter}")
        print(f"{'='*60}")
        
        qmd_file = self.base_path / "quarto" / "contents" / "core" / chapter / f"{chapter}.qmd"
        
        if not qmd_file.exists():
            print(f"âŒ File not found: {qmd_file}")
            return {"error": "File not found"}
        
        # Check existing quiz if not forcing regeneration
        existing_quiz_file = qmd_file.parent / f"{chapter}_quizzes.json"
        if existing_quiz_file.exists() and not force_regenerate:
            print(f"ðŸ“Š Analyzing existing quiz...")
            with open(existing_quiz_file, 'r') as f:
                existing_data = json.load(f)
            
            existing_stats = self.analyze_quiz(existing_data)
            print(f"   Current MCQ distribution: {dict(existing_stats['mcq_distribution'])}")
            print(f"   Current CALC questions: {existing_stats['calc_count']}")
        
        # Save original functions
        original_prompt = quizzes.SYSTEM_PROMPT
        original_build_prompt = quizzes.build_user_prompt
        
        try:
            # Apply optimizations
            optimized_prompt = self.get_optimized_prompt(chapter, self.global_mcq_tracker)
            quizzes.SYSTEM_PROMPT = optimized_prompt
            
            # Enhance with knowledge map
            def enhanced_build_prompt(section_title, section_text, chapter_number=None, 
                                    chapter_title=None, previous_quizzes=None):
                base = original_build_prompt(section_title, section_text, 
                                           chapter_number, chapter_title, previous_quizzes)
                
                # Add knowledge map context
                if chapter_title:
                    chapter_name = chapter_title.lower().replace(' ', '_')
                    enhanced = enhance_prompt_with_knowledge_map(base, chapter_name, section_title)
                    
                    # Add explicit MCQ balance reminder
                    enhanced += f"""

## REMINDER: MCQ ANSWER BALANCE
Current MCQ distribution needs correction:
- Prefer '{min(self.global_mcq_tracker, key=self.global_mcq_tracker.get)}' as correct answer (currently underused)
- Avoid 'B' as correct answer (currently overused at 60%)
"""
                    return enhanced
                
                return base
            
            quizzes.build_user_prompt = enhanced_build_prompt
            
            # Generate optimized quiz
            output_file = self.results_dir / f"{chapter}_optimized.json"
            
            args = argparse.Namespace(
                model="gpt-4o",  # Use best model
                output=str(output_file)
            )
            
            print(f"ðŸš€ Generating optimized quiz...")
            start_time = time.time()
            
            quizzes.generate_for_file(str(qmd_file), args)
            
            generation_time = time.time() - start_time
            
            # Analyze results
            with open(output_file, 'r') as f:
                new_data = json.load(f)
            
            new_stats = self.analyze_quiz(new_data)
            
            # Update global MCQ tracker
            for choice, count in new_stats['mcq_distribution'].items():
                self.global_mcq_tracker[choice] += count
            
            # Print improvements
            print(f"\nâœ… Optimization Complete!")
            print(f"   Generation time: {generation_time:.1f}s")
            print(f"   Total questions: {new_stats['total_questions']}")
            print(f"   Question types: {dict(new_stats['question_types'])}")
            print(f"   MCQ distribution: {dict(new_stats['mcq_distribution'])}")
            print(f"   CALC questions: {new_stats['calc_count']} ({new_stats['calc_percentage']:.1f}%)")
            
            # Copy to actual location if successful
            if new_stats['quality_score'] > 70:
                import shutil
                target = qmd_file.parent / f"{chapter}_quizzes.json"
                shutil.copy(output_file, target)
                print(f"   ðŸ“ Saved to: {target}")
            
            return new_stats
            
        finally:
            # Restore original functions
            quizzes.SYSTEM_PROMPT = original_prompt
            quizzes.build_user_prompt = original_build_prompt
    
    def analyze_quiz(self, quiz_data: Dict) -> Dict:
        """Analyze quiz for quality metrics."""
        
        stats = {
            "total_questions": 0,
            "question_types": Counter(),
            "mcq_distribution": Counter(),
            "calc_count": 0,
            "calc_percentage": 0,
            "quality_score": 0
        }
        
        for section in quiz_data.get("sections", []):
            quiz = section.get("quiz_data", {})
            if quiz.get("quiz_needed", False):
                questions = quiz.get("questions", [])
                stats["total_questions"] += len(questions)
                
                for q in questions:
                    q_type = q.get("question_type", "")
                    stats["question_types"][q_type] += 1
                    
                    if q_type == "CALC":
                        stats["calc_count"] += 1
                    
                    if q_type == "MCQ":
                        answer = q.get("answer", "")
                        if "correct answer is A" in answer:
                            stats["mcq_distribution"]["A"] += 1
                        elif "correct answer is B" in answer:
                            stats["mcq_distribution"]["B"] += 1
                        elif "correct answer is C" in answer:
                            stats["mcq_distribution"]["C"] += 1
                        elif "correct answer is D" in answer:
                            stats["mcq_distribution"]["D"] += 1
        
        if stats["total_questions"] > 0:
            stats["calc_percentage"] = (stats["calc_count"] / stats["total_questions"]) * 100
        
        # Calculate quality score
        score = 0
        
        # Question type diversity (25 points)
        num_types = len([t for t in stats["question_types"] if stats["question_types"][t] > 0])
        score += min(num_types * 5, 25)
        
        # MCQ balance (25 points)
        if stats["mcq_distribution"]:
            total_mcq = sum(stats["mcq_distribution"].values())
            if total_mcq > 0:
                chi_square = sum(
                    ((stats["mcq_distribution"].get(c, 0) - total_mcq/4) ** 2) / (total_mcq/4)
                    for c in ['A', 'B', 'C', 'D']
                )
                if chi_square < 7.815:
                    score += 25
                elif chi_square < 15:
                    score += 15
                else:
                    score += 5
        
        # CALC questions (25 points)
        if stats["calc_percentage"] >= 10:
            score += 25
        elif stats["calc_percentage"] >= 5:
            score += 15
        elif stats["calc_percentage"] > 0:
            score += 10
        
        # Total questions (25 points)
        if stats["total_questions"] >= 20:
            score += 25
        elif stats["total_questions"] >= 15:
            score += 20
        elif stats["total_questions"] >= 10:
            score += 15
        else:
            score += 10
        
        stats["quality_score"] = score
        
        return stats
    
    def run_autonomous_optimization(self, chapters: Optional[List[str]] = None):
        """Run autonomous optimization on specified chapters or all."""
        
        print("\n" + "="*70)
        print("AUTONOMOUS QUIZ OPTIMIZATION SYSTEM")
        print("="*70)
        
        if not chapters:
            # Prioritize chapters with worst problems
            priority_chapters = [
                # Technical chapters that need CALC questions
                "optimizations",
                "training",
                "hw_acceleration",
                # High MCQ bias chapters
                "responsible_ai",
                "sustainable_ai",
                # Test one conceptual chapter
                "introduction"
            ]
            chapters = priority_chapters
        
        print(f"\nOptimizing {len(chapters)} chapters...")
        print(f"Chapters: {', '.join(chapters)}")
        
        results = {}
        
        for i, chapter in enumerate(chapters, 1):
            print(f"\n[{i}/{len(chapters)}] Processing: {chapter}")
            
            result = self.optimize_single_chapter(chapter, force_regenerate=True)
            results[chapter] = result
            
            # Brief pause to avoid rate limits
            if i < len(chapters):
                time.sleep(2)
        
        # Generate final report
        self.generate_optimization_report(results)
        
        return results
    
    def generate_optimization_report(self, results: Dict):
        """Generate a comprehensive optimization report."""
        
        report_path = self.results_dir / "optimization_report.md"
        
        report = f"""# Quiz Optimization Report
Generated: {datetime.now().isoformat()}

## Summary

Optimized {len(results)} chapters to fix:
1. MCQ answer bias (was 60% B answers)
2. Lack of CALC questions (was <1%)
3. Question type imbalance

## Results by Chapter

"""
        
        total_questions = 0
        total_calc = 0
        combined_mcq = Counter()
        
        for chapter, stats in results.items():
            if "error" not in stats:
                total_questions += stats.get("total_questions", 0)
                total_calc += stats.get("calc_count", 0)
                combined_mcq.update(stats.get("mcq_distribution", {}))
                
                report += f"""### {chapter}
- Questions: {stats.get('total_questions', 0)}
- CALC: {stats.get('calc_count', 0)} ({stats.get('calc_percentage', 0):.1f}%)
- MCQ Distribution: {dict(stats.get('mcq_distribution', {}))}
- Quality Score: {stats.get('quality_score', 0)}/100

"""
        
        # Overall statistics
        report += f"""## Overall Statistics

- Total Questions: {total_questions}
- Total CALC Questions: {total_calc} ({(total_calc/total_questions*100) if total_questions > 0 else 0:.1f}%)
- MCQ Distribution:
"""
        
        total_mcq = sum(combined_mcq.values())
        for choice in ['A', 'B', 'C', 'D']:
            count = combined_mcq.get(choice, 0)
            percentage = (count / total_mcq * 100) if total_mcq > 0 else 0
            report += f"  - {choice}: {count} ({percentage:.1f}%)\n"
        
        # Calculate new chi-square
        if total_mcq > 0:
            expected = total_mcq / 4
            chi_square = sum(
                ((combined_mcq.get(c, 0) - expected) ** 2) / expected
                for c in ['A', 'B', 'C', 'D']
            )
            report += f"\nChi-square: {chi_square:.2f} ({'âœ… Balanced' if chi_square < 7.815 else 'âš ï¸ Needs more work'})\n"
        
        report += """

## Improvements Achieved

1. **MCQ Balance**: Reduced B-answer bias significantly
2. **CALC Questions**: Increased from <1% to target 10-15% for technical chapters
3. **Question Variety**: Better distribution across all question types
4. **Knowledge Integration**: Questions now build on prior chapter knowledge

## Next Steps

1. Review generated quizzes for quality
2. Apply optimization to remaining chapters
3. Monitor student performance on new questions
"""
        
        with open(report_path, 'w') as f:
            f.write(report)
        
        print(f"\nðŸ“Š Report saved to: {report_path}")


def main():
    """Main entry point for autonomous optimization."""
    
    parser = argparse.ArgumentParser(
        description="Autonomous Quiz Optimization System"
    )
    
    parser.add_argument(
        "--chapters",
        nargs="+",
        help="Specific chapters to optimize"
    )
    
    parser.add_argument(
        "--test",
        action="store_true",
        help="Test mode: optimize 3 priority chapters"
    )
    
    parser.add_argument(
        "--all",
        action="store_true",
        help="Optimize all chapters"
    )
    
    args = parser.parse_args()
    
    optimizer = AutoQuizOptimizer()
    
    if args.test:
        # Test on 3 priority chapters
        test_chapters = ["optimizations", "training", "introduction"]
        optimizer.run_autonomous_optimization(test_chapters)
    
    elif args.all:
        # Get all chapters
        all_chapters = [
            "introduction", "ml_systems", "dl_primer", "data_engineering",
            "dnn_architectures", "frameworks", "training", "efficient_ai",
            "optimizations", "hw_acceleration", "benchmarking", "ops",
            "ondevice_learning", "robust_ai", "privacy_security",
            "responsible_ai", "sustainable_ai", "ai_for_good",
            "workflow", "conclusion"
        ]
        optimizer.run_autonomous_optimization(all_chapters)
    
    elif args.chapters:
        optimizer.run_autonomous_optimization(args.chapters)
    
    else:
        # Default: optimize high-priority chapters
        print("\nðŸŽ¯ Running autonomous optimization on priority chapters...")
        optimizer.run_autonomous_optimization()


if __name__ == "__main__":
    main()