%comment{This file was created with betterbib v5.0.11.}


@techreport{/content/paper/876367e3-en,
 author = {Oecd},
 title = {A blueprint for building national compute capacity for artificial intelligence},
 year = {2023},
 number = {350},
 doi = {10.1787/876367e3-en},
 url = {https://doi.org/10.1787/876367e3-en},
 source = {Crossref},
 institution = {Organisation for Economic Co-Operation and Development (OECD)},
 issn = {2071-6826},
 month = feb,
}

@inproceedings{10.1145/1993744.1993791,
 author = {Li, Chao and Qouneh, Amer and Li, Tao},
 title = {Characterizing and analyzing renewable energy driven data centers},
 year = {2011},
 booktitle = {Proceedings of the ACM SIGMETRICS joint international conference on Measurement and modeling of computer systems},
 location = {San Jose, California, USA},
 publisher = {ACM},
 address = {New York, NY, USA},
 series = {SIGMETRICS '11},
 pages = {131--132},
 doi = {10.1145/1993744.1993791},
 isbn = {9781450308144},
 url = {https://doi.org/10.1145/1993744.1993791},
 abstract = {An increasing number of data centers today start to incorporate renewable energy solutions to cap their carbon footprint. However, the impact of renewable energy on large-scale data center design is still not well understood. In this paper, we model and evaluate data centers driven by intermittent renewable energy. Using real-world data center and renewable energy source traces, we show that renewable power utilization and load tuning frequency are two critical metrics for designing sustainable high-performance data centers. Our characterization reveals that load power fluctuation together with the intermittent renewable power supply introduce unnecessary tuning activities, which can increase the management overhead and degrade the performance of renewable energy driven data centers.},
 numpages = {2},
 keywords = {data center, power variation, renewable energy, load tuning},
 source = {Crossref},
 month = jun,
}

@article{10242251,
 author = {Eshraghian, Jason K. and Ward, Max and Neftci, Emre O. and Wang, Xinxin and Lenz, Gregor and Dwivedi, Girish and Bennamoun, Mohammed and Jeong, Doo Seok and Lu, Wei D.},
 title = {Training Spiking Neural Networks Using Lessons From Deep Learning},
 year = {2023},
 journal = {Proc. IEEE},
 volume = {111},
 number = {9},
 pages = {1016--1054},
 doi = {10.1109/jproc.2023.3308088},
 bdsk-url-1 = {https://doi.org/10.1109/JPROC.2023.3308088},
 source = {Crossref},
 url = {https://doi.org/10.1109/jproc.2023.3308088},
 publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
 issn = {0018-9219, 1558-2256},
 month = sep,
}

@techreport{7437cc5b-f961-36e9-a745-ba32105a94d0,
 author = {Nakano, Jane},
 title = {The Geopolitics of Critical Minerals Supply Chains},
 year = {2021},
 pages = {II--III},
 url = {http://www.jstor.org/stable/resrep30033.1},
 urldate = {2023-12-08},
 institution = {Center for Strategic and International Studies (CSIS)},
}

@article{9563954,
 author = {Thompson, Neil C. and Greenewald, Kristjan and Lee, Keeheon and Manso, Gabriel F.},
 title = {Deep Learning's Diminishing Returns: {The} Cost of Improvement is Becoming Unsustainable},
 year = {2021},
 journal = {IEEE Spectr.},
 volume = {58},
 number = {10},
 pages = {50--55},
 doi = {10.1109/mspec.2021.9563954},
 source = {Crossref},
 url = {https://doi.org/10.1109/mspec.2021.9563954},
 publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
 issn = {0018-9235, 1939-9340},
 month = oct,
}

@article{a4e2073d2c6e4fcb9b7459c2e44716f1,
 author = {Monyei, Chukwuka G. and Jenkins, Kirsten E.H.},
 title = {Electrons have no identity: {Setting} right misrepresentations in Google and Apple{\textquoteright}s clean energy purchasing},
 year = {2018},
 month = dec,
 journal = {Energy Research \&amp; Social Science},
 publisher = {Elsevier BV},
 volume = {46},
 pages = {48--51},
 doi = {10.1016/j.erss.2018.06.015},
 issn = {2214-6296},
 abstract = {Aside dedicated generation, transmission and distribution networks, the hype around corporations and other entities purchasing so called clean energy may be considered a deliberate accounting misrepresentation. To illustrate this case in this short perspective, we begin by explaining the technical difficulties of remaining {\textquotedblleft}renewables pure{\textquotedblright}. We then give case studies of two organisations {\textendash} Apple Inc. and Google LLC {\textendash} who are, arguably, at fault of making such claims. The method is a simple, non-systematic comparison between what is technically possible, and what is claimed to be possible. Given that incongruous renewables claims have the potential to further impoverish vulnerable households who must bear the financial costs of renewables integration, we conclude that a successful decarbonisation pathway must not have selective winners or losers.},
 keywords = {electricity generation, clean energy, power purchase agreements, renewables},
 language = {English},
 source = {Crossref},
 url = {https://doi.org/10.1016/j.erss.2018.06.015},
}

@inproceedings{abadi2016deep,
 author = {Abadi, Martin and Chu, Andy and Goodfellow, Ian and McMahan, H. Brendan and Mironov, Ilya and Talwar, Kunal and Zhang, Li},
 title = {Deep Learning with Differential Privacy},
 year = {2016},
 booktitle = {Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security},
 publisher = {ACM},
 address = {New York, NY, USA},
 series = {CCS '16},
 pages = {308--318},
 date-added = {2023-11-22 18:06:03 -0500},
 date-modified = {2023-11-22 18:08:42 -0500},
 keywords = {deep learning, differential privacy},
 doi = {10.1145/2976749.2978318},
 source = {Crossref},
 url = {https://doi.org/10.1145/2976749.2978318},
 month = oct,
}

@inproceedings{abadi2016tensorflow,
 author = {Yu, Yuan and Abadi, Mart{\'\i}n and Barham, Paul and Brevdo, Eugene and Burrows, Mike and Davis, Andy and Dean, Jeff and Ghemawat, Sanjay and Harley, Tim and Hawkins, Peter and Isard, Michael and Kudlur, Manjunath and Monga, Rajat and Murray, Derek and Zheng, Xiaoqiang},
 title = {Dynamic control flow in large-scale machine learning},
 year = {2018},
 booktitle = {Proceedings of the Thirteenth EuroSys Conference},
 pages = {265--283},
 doi = {10.1145/3190508.3190551},
 source = {Crossref},
 url = {https://doi.org/10.1145/3190508.3190551},
 publisher = {ACM},
 month = apr,
}

@inproceedings{Abdelkader_2020,
 author = {Abdelkader, Ahmed and Curry, Michael J. and Fowl, Liam and Goldstein, Tom and Schwarzschild, Avi and Shu, Manli and Studer, Christoph and Zhu, Chen},
 title = {Headless Horseman: {Adversarial} Attacks on Transfer Learning Models},
 year = {2020},
 booktitle = {ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
 date-added = {2023-11-22 16:28:31 -0500},
 date-modified = {2023-11-22 16:29:33 -0500},
 doi = {10.1109/icassp40776.2020.9053181},
 source = {Crossref},
 url = {https://doi.org/10.1109/icassp40776.2020.9053181},
 publisher = {IEEE},
 month = may,
}

@article{adagrad,
 author = {Duchi, John and Hazan, Elad and Singer, Yoram},
 title = {Adaptive Subgradient Methods for Online Learning and Stochastic Optimization},
 year = {2011},
 journal = {J. Mach. Learn. Res.},
 url = {http://jmlr.org/papers/v12/duchi11a.html},
}

@misc{adam,
 author = {Kingma, Diederik P. and Ba, Jimmy},
 editor = {Bengio, Yoshua and LeCun, Yann},
 title = {Adam: {A} Method for Stochastic Optimization},
 year = {2017},
 booktitle = {3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings},
 url = {http://arxiv.org/abs/1412.6980},
 eprint = {1412.6980},
 archiveprefix = {arXiv},
 primaryclass = {cs.LG},
 timestamp = {Thu, 25 Jul 2019 14:25:37 +0200},
 biburl = {https://dblp.org/rec/journals/corr/KingmaB14.bib},
 bibsource = {dblp computer science bibliography, https://dblp.org},
}

@misc{adelta,
 author = {Zeiler, Matthew D.},
 title = {Reinforcement and Systemic Machine Learning for Decision Making},
 year = {2012},
 eprint = {1212.5701},
 archiveprefix = {arXiv},
 primaryclass = {cs.LG},
 doi = {10.1002/9781118266502.ch6},
 source = {Crossref},
 url = {https://doi.org/10.1002/9781118266502.ch6},
 publisher = {Wiley},
 isbn = {9780470919996, 9781118266502},
 pages = {119--149},
 month = jul,
}

@inproceedings{adolf2016fathom,
 author = {Adolf, Robert and Rama, Saketh and Reagen, Brandon and Wei, Gu-yeon and Brooks, David},
 title = {Fathom: {Reference} workloads for modern deep learning methods},
 year = {2016},
 booktitle = {2016 IEEE International Symposium on Workload Characterization (IISWC)},
 pages = {1--10},
 organization = {IEEE},
 doi = {10.1109/iiswc.2016.7581275},
 source = {Crossref},
 url = {https://doi.org/10.1109/iiswc.2016.7581275},
 publisher = {IEEE},
 month = sep,
}

@article{afib,
 author = {Guo, Yutao and Wang, Hao and Zhang, Hui and Liu, Tong and Liang, Zhaoguang and Xia, Yunlong and Yan, Li and Xing, Yunli and Shi, Haili and Li, Shuyan and Liu, Yanxia and Liu, Fan and Feng, Mei and Chen, Yundai and Lip, Gregory Y.H. and Guo, Yutao and Lip, Gregory Y.H. and Lane, Deirdre A. and Chen, Yundai and Wang, Liming and Eckstein, Jens and Thomas, G Neil and Tong, Liu and Mei, Feng and Xuejun, Liu and Xiaoming, Li and Zhaoliang, Shan and Xiangming, Shi and Wei, Zhang and Yunli, Xing and Jing, Wen and Fan, Wu and Sitong, Yang and Xiaoqing, Jin and Bo, Yang and Xiaojuan, Bai and Yuting, Jiang and Yangxia, Liu and Yingying, Song and Zhongju, Tan and Li, Yang and Tianzhu, Luan and Chunfeng, Niu and Lili, Zhang and Shuyan, Li and Zulu, Wang and Bing, Xv and Liming, Liu and Yuanzhe, Jin and Yunlong, Xia and Xiaohong, Chen and Fang, Wu and Lina, Zhong and yihong, Sun and shujie, Jia and Jing, Li and Nan, Li and shijun, Li and huixia, Liu and Rong, Li and Fan, Liu and qingfeng, Ge and tianyun, Guan and Yuan, Wen and Xin, Li and Yan, Ren and xiaoping, Chen and ronghua, Chen and Yun, Shi and yulan, Zhao and haili, Shi and yujie, Zhao and quanchun, Wang and weidong, Sun and Lin, Wei and Chan, Esther and Guangliang, Shan and Chen, Yao and Wei, Zong and Dandi, Chen and Xiang, Han and Anding, Xu and Xiaohan, Fan and Ziqiang, Yu and Xiang, Gu and Fulin, Ge},
 title = {Mobile Photoplethysmographic Technology to Detect Atrial Fibrillation},
 year = {2019},
 journal = {J. Am. Coll. Cardiol.},
 volume = {74},
 number = {19},
 pages = {2365--2375},
 doi = {10.1016/j.jacc.2019.08.019},
 bdsk-url-1 = {https://doi.org/10.1016/j.jacc.2019.08.019},
 source = {Crossref},
 url = {https://doi.org/10.1016/j.jacc.2019.08.019},
 publisher = {Elsevier BV},
 issn = {0735-1097},
 month = nov,
}

@inproceedings{agarwal2018reductions,
 author = {Agarwal, Alekh and Beygelzimer, Alina and Dud{\'\i}k, Miroslav and Langford, John and Wallach, Hanna},
 title = {A reductions approach to fair classification},
 year = {2018},
 booktitle = {International conference on machine learning},
 pages = {60--69},
 organization = {PMLR},
}

@inproceedings{agrawal2003side,
 author = {Agrawal, Dakshi and Baktir, Selcuk and Karakoyunlu, Deniz and Rohatgi, Pankaj and Sunar, Berk},
 title = {{Trojan} Detection using {IC} Fingerprinting},
 year = {2007},
 booktitle = {2007 IEEE Symposium on Security and Privacy (SP '07)},
 pages = {29--45},
 organization = {Springer},
 doi = {10.1109/sp.2007.36},
 source = {Crossref},
 url = {https://doi.org/10.1109/sp.2007.36},
 publisher = {IEEE},
 month = may,
}

@incollection{ai_health_rise,
 author = {Bohr, Adam and Memarzadeh, Kaveh},
 title = {The rise of artificial intelligence in healthcare applications},
 year = {2020},
 month = jun,
 journal = {Artificial Intelligence in Healthcare},
 pages = {25--60},
 doi = {10.1016/b978-0-12-818438-7.00002-2},
 source = {Crossref},
 url = {https://doi.org/10.1016/b978-0-12-818438-7.00002-2},
 booktitle = {Artificial Intelligence in Healthcare},
 publisher = {Elsevier},
}

@misc{al2016theano,
 author = {Team, The Theano Development and Al-Rfou, Rami and Alain, Guillaume and Almahairi, Amjad and Angermueller, Christof and Bahdanau, Dzmitry and Ballas, Nicolas and Bastien, Fr\'ed\'eric and Bayer, Justin and Belikov, Anatoly and Belopolsky, Alexander and Bengio, Yoshua and Bergeron, Arnaud and Bergstra, James and Bisson, Valentin and Snyder, Josh Bleecher and Bouchard, Nicolas and Boulanger-Lewandowski, Nicolas and Bouthillier, Xavier and de Br\'ebisson, Alexandre and Breuleux, Olivier and Carrier, Pierre-Luc and Cho, Kyunghyun and Chorowski, Jan and Christiano, Paul and Cooijmans, Tim and C\^ot\'e, Marc-Alexandre and C\^ot\'e, Myriam and Courville, Aaron and Dauphin, Yann N. and Delalleau, Olivier and Demouth, Julien and Desjardins, Guillaume and Dieleman, Sander and Dinh, Laurent and Ducoffe, M\'elanie and Dumoulin, Vincent and Kahou, Samira Ebrahimi and Erhan, Dumitru and Fan, Ziye and Firat, Orhan and Germain, Mathieu and Glorot, Xavier and Goodfellow, Ian and Graham, Matt and Gulcehre, Caglar and Hamel, Philippe and Harlouchet, Iban and Heng, Jean-Philippe and Hidasi, Bal\'azs and Honari, Sina and Jain, Arjun and Jean, S\'ebastien and Jia, Kai and Korobov, Mikhail and Kulkarni, Vivek and Lamb, Alex and Lamblin, Pascal and Larsen, Eric and Laurent, C\'esar and Lee, Sean and Lefrancois, Simon and Lemieux, Simon and L\'eonard, Nicholas and Lin, Zhouhan and Livezey, Jesse A. and Lorenz, Cory and Lowin, Jeremiah and Ma, Qianli and Manzagol, Pierre-Antoine and Mastropietro, Olivier and McGibbon, Robert T. and Memisevic, Roland and van Merri\"enboer, Bart and Michalski, Vincent and Mirza, Mehdi and Orlandi, Alberto and Pal, Christopher and Pascanu, Razvan and Pezeshki, Mohammad and Raffel, Colin and Renshaw, Daniel and Rocklin, Matthew and Romero, Adriana and Roth, Markus and Sadowski, Peter and Salvatier, John and Savard, Fran\c{c}ois and Schl\"uter, Jan and Schulman, John and Schwartz, Gabriel and Serban, Iulian Vlad and Serdyuk, Dmitriy and Shabanian, Samira and Simon, \'Etienne and Spieckermann, Sigurd and Subramanyam, S. Ramana and Sygnowski, Jakub and Tanguay, J\'er\'emie and van Tulder, Gijs and Turian, Joseph and Urban, Sebastian and Vincent, Pascal and Visin, Francesco and de Vries, Harm and Warde-Farley, David and Webb, Dustin J. and Willson, Matthew and Xu, Kelvin and Xue, Lijun and Yao, Li and Zhang, Saizheng and Zhang, Ying},
 title = {Theano: {A} Python framework for fast computation of mathematical expressions},
 year = {2016},
 archiveprefix = {arXiv},
 eprint = {1605.02688},
 primaryclass = {cs.SC},
}

@article{Aledhari_Razzak_Parizi_Saeed_2020,
 author = {Aledhari, Mohammed and Razzak, Rehma and Parizi, Reza M. and Saeed, Fahad},
 title = {Federated Learning: {A} Survey on Enabling Technologies, Protocols, and Applications},
 year = {2020},
 journal = {\#IEEE\_O\_ACC\#},
 volume = {8},
 pages = {140699--140725},
 doi = {10.1109/access.2020.3013541},
 bdsk-url-1 = {https://doi.org/10.1109/access.2020.3013541},
 source = {Crossref},
 url = {https://doi.org/10.1109/access.2020.3013541},
 publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
 issn = {2169-3536},
}

@article{alghamdi2022beyond,
 author = {Alghamdi, Wael and Hsu, Hsiang and Jeong, Haewon and Wang, Hao and Michalak, Peter and Asoodeh, Shahab and Calmon, Flavio},
 title = {Beyond Adult and {COMPAS:} {Fair} multi-class prediction via information projection},
 year = {2022},
 journal = {Adv. Neur. In.},
 volume = {35},
 pages = {38747--38760},
}

@article{aljundi_gradient_nodate,
 author = {Aljundi, Rahaf and Lin, Min and Goujaud, Baptiste and Bengio, Yoshua},
 title = {Gradient based sample selection for online continual learning},
 file = {Aljundi et al. - Gradient based sample selection for online continu.pdf:/Users/alex/Zotero/storage/GPHM4KY7/Aljundi et al. - Gradient based sample selection for online continu.pdf:application/pdf},
 language = {en},
}

@online{alleghenycounty2018,
 author = {of Human Services, Allegheny County Department},
 title = {{Allegheny} Family Screening Tool: {Algorithmic} Risk Assessment in Child Protective Services},
 year = {2018},
 url = {https://www.alleghenycountyanalytics.us/wp-content/uploads/2018/10/17-ACDHS-11_AFST_102518.pdf},
 organization = {Allegheny County Analytics},
}

@inproceedings{altayeb2022classifying,
 author = {Altayeb, Moez and Zennaro, Marco and Rovai, Marcelo},
 title = {Classifying mosquito wingbeat sound using {TinyML}},
 year = {2022},
 booktitle = {Proceedings of the 2022 ACM Conference on Information Technology for Social Good},
 pages = {132--137},
 doi = {10.1145/3524458.3547258},
 source = {Crossref},
 url = {https://doi.org/10.1145/3524458.3547258},
 publisher = {ACM},
 month = sep,
}

@inproceedings{amiel2006fault,
 author = {Amiel, Frederic and Clavier, Christophe and Tunstall, Michael},
 title = {Fault analysis of {DPA}-resistant algorithms},
 year = {2006},
 booktitle = {International Workshop on Fault Diagnosis and Tolerance in Cryptography},
 pages = {223--236},
 date-added = {2023-11-22 16:45:05 -0500},
 date-modified = {2023-11-22 16:45:55 -0500},
 organization = {Springer},
}

@misc{amodei_ai_2018,
 author = {Amodei, Dario and Hernandez, Danny},
 title = {Mastering Windows Server{\textregistered} 2016},
 year = {2018},
 month = jul,
 journal = {OpenAI Blog},
 url = {https://doi.org/10.1002/9781119549277.ch3},
 bdsk-url-1 = {https://openai.com/research/ai-and-compute},
 doi = {10.1002/9781119549277.ch3},
 source = {Crossref},
 publisher = {Wiley},
 isbn = {9781119404972, 9781119549277},
 pages = {115--156},
}

@article{amodei2016concrete,
 author = {Van Noorden, Richard},
 title = {{ArXiv} preprint server plans multimillion-dollar overhaul},
 year = {2016},
 journal = {Nature},
 doi = {10.1038/534602a},
 number = {7609},
 source = {Crossref},
 url = {https://doi.org/10.1038/534602a},
 volume = {534},
 publisher = {Springer Science and Business Media LLC},
 issn = {0028-0836, 1476-4687},
 pages = {602--602},
 month = jun,
}

@misc{anthony2020carbontracker,
 author = {Anthony, Lasse F. Wolff and Kanding, Benjamin and Selvan, Raghavendra},
 year = {2020},
 month = jul,
 note = {arXiv:2007.03051},
 howpublished = {ICML Workshop on Challenges in Deploying and monitoring Machine Learning Systems},
}

@inproceedings{antol2015vqa,
 author = {Antol, Stanislaw and Agrawal, Aishwarya and Lu, Jiasen and Mitchell, Margaret and Batra, Dhruv and Zitnick, C. Lawrence and Parikh, Devi},
 title = {{VQA:} {Visual} Question Answering},
 year = {2015},
 booktitle = {2015 IEEE International Conference on Computer Vision (ICCV)},
 pages = {2425--2433},
 doi = {10.1109/iccv.2015.279},
 source = {Crossref},
 url = {https://doi.org/10.1109/iccv.2015.279},
 publisher = {IEEE},
 month = dec,
}

@inproceedings{antonakakis2017understanding,
 author = {Antonakakis, Manos and April, Tim and Bailey, Michael and Bernhard, Matt and Bursztein, Elie and Cochran, Jaime and Durumeric, Zakir and Halderman, J Alex and Invernizzi, Luca and Kallitsis, Michalis and others},
 title = {Understanding the mirai botnet},
 year = {2017},
 booktitle = {26th USENIX security symposium (USENIX Security 17)},
 pages = {1093--1110},
}

@article{app112211073,
 author = {Kwon, Jisu and Park, Daejin},
 title = {{Hardware/Software} Co-Design for {TinyML} Voice-Recognition Application on Resource Frugal Edge Devices},
 year = {2021},
 journal = {Applied Sciences},
 volume = {11},
 number = {22},
 doi = {10.3390/app112211073},
 issn = {2076-3417},
 url = {https://doi.org/10.3390/app112211073},
 article-number = {11073},
 bdsk-url-1 = {https://www.mdpi.com/2076-3417/11/22/11073},
 bdsk-url-2 = {https://doi.org/10.3390/app112211073},
 source = {Crossref},
 publisher = {MDPI AG},
 pages = {11073},
 month = nov,
}

@article{Ardila_Branson_Davis_Henretty_Kohler_Meyer_Morais_Saunders_Tyers_Weber_2020,
 author = {Ardila, Rosana and Branson, Megan and Davis, Kelly and Henretty, Michael and Kohler, Michael and Meyer, Josh and Morais, Reuben and Saunders, Lindsay and Tyers, Francis M. and Weber, Gregor},
 title = {Common Voice: {A} Massively-Multilingual Speech Corpus},
 year = {2020},
 month = may,
 journal = {Proceedings of the 12th Conference on Language Resources and Evaluation},
 pages = {4218--4222},
}

@inproceedings{Asonov2004Keyboard,
 author = {Asonov, D. and Agrawal, R.},
 title = {Keyboard acoustic emanations},
 year = {2004},
 booktitle = {IEEE Symposium on Security and Privacy, 2004. Proceedings. 2004},
 pages = {3--11},
 date-added = {2023-11-22 17:05:39 -0500},
 date-modified = {2023-11-22 17:06:45 -0500},
 organization = {IEEE},
 doi = {10.1109/secpri.2004.1301311},
 source = {Crossref},
 url = {https://doi.org/10.1109/secpri.2004.1301311},
 publisher = {IEEE},
}

@article{ateniese2015hacking,
 author = {Ateniese, Giuseppe and Mancini, Luigi V. and Spognardi, Angelo and Villani, Antonio and Vitali, Domenico and Felici, Giovanni},
 title = {Hacking smart machines with smarter ones: {How} to extract meaningful data from machine learning classifiers},
 year = {2015},
 journal = {Int. J. Secur. Netw.},
 volume = {10},
 number = {3},
 pages = {137},
 date-added = {2023-11-22 16:14:42 -0500},
 date-modified = {2023-11-22 16:15:42 -0500},
 doi = {10.1504/ijsn.2015.071829},
 source = {Crossref},
 url = {https://doi.org/10.1504/ijsn.2015.071829},
 publisher = {Inderscience Publishers},
 issn = {1747-8405, 1747-8413},
}

@inproceedings{athalye2018obfuscated,
 author = {Athalye, Anish and Carlini, Nicholas and Wagner, David},
 title = {Obfuscated gradients give a false sense of security: {Circumventing} defenses to adversarial examples},
 year = {2018},
 booktitle = {International conference on machine learning},
 pages = {274--283},
 organization = {PMLR},
}

@misc{awq,
 author = {Lin and Tang, Tang and Yang, Dang and Gan, Han},
 title = {{AWQ:} {Activation-aware} Weight Quantization for {LLM} Compression and Acceleration},
 year = {2023},
 journal = {arXiv},
 doi = {10.48550/arXiv.2306.00978},
 url = {https://arxiv.org/abs/2306.00978},
 urldate = {2023-10-03},
 bdsk-url-1 = {https://arxiv.org/abs/2306.00978},
 bdsk-url-2 = {https://doi.org/10.48550/arXiv.2306.00978},
}

@misc{bailey_enabling_2018,
 author = {Bailey, Brian},
 title = {Enabling Cheaper Design},
 year = {2018},
 month = sep,
 journal = {Semiconductor Engineering},
 url = {https://semiengineering.com/enabling-cheaper-design/},
 urldate = {2023-11-07},
 abstract = {Enabling Cheaper Design, At what point does cheaper design enable a significant growth in custom semiconductor content? Not everyone is onboard with the idea.},
 language = {en-US},
 bdsk-url-1 = {https://semiengineering.com/enabling-cheaper-design/},
}

@article{bains2020business,
 author = {Bains, Sunny},
 title = {The business of building brains},
 year = {2020},
 journal = {Nature Electronics},
 volume = {3},
 number = {7},
 pages = {348--351},
 doi = {10.1038/s41928-020-0449-1},
 source = {Crossref},
 url = {https://doi.org/10.1038/s41928-020-0449-1},
 publisher = {Springer Science and Business Media LLC},
 issn = {2520-1131},
 month = jul,
}

@inproceedings{bamoumen2022tinyml,
 author = {Bamoumen, Hatim and Temouden, Anas and Benamar, Nabil and Chtouki, Yousra},
 title = {How {TinyML} Can be Leveraged to Solve Environmental Problems: {A} Survey},
 year = {2022},
 booktitle = {2022 International Conference on Innovation and Intelligence for Informatics, Computing, and Technologies (3ICT)},
 pages = {338--343},
 organization = {IEEE},
 doi = {10.1109/3ict56508.2022.9990661},
 source = {Crossref},
 url = {https://doi.org/10.1109/3ict56508.2022.9990661},
 publisher = {IEEE},
 month = nov,
}

@article{banbury2020benchmarking,
 author = {Banbury, Colby R and Reddi, Vijay Janapa and Lam, Max and Fu, William and Fazel, Amin and Holleman, Jeremy and Huang, Xinyuan and Hurtado, Robert and Kanter, David and Lokhmotov, Anton and others},
 title = {Benchmarking tinyml systems: {Challenges} and direction},
 year = {2020},
 journal = {arXiv preprint arXiv:2003.04821},
}

@article{bank2023autoencoders,
 author = {Bank, Dor and Koenigstein, Noam and Giryes, Raja},
 title = {Autoencoders},
 year = {2023},
 journal = {Machine Learning for Data Science Handbook: Data Mining and Knowledge Discovery Handbook},
 publisher = {Springer},
 pages = {353--374},
}

@inproceedings{barenghi2010low,
 author = {Barenghi, Alessandro and Bertoni, Guido M. and Breveglieri, Luca and Pellicioli, Mauro and Pelosi, Gerardo},
 title = {Low voltage fault attacks to {AES}},
 year = {2010},
 booktitle = {2010 IEEE International Symposium on Hardware-Oriented Security and Trust (HOST)},
 pages = {7--12},
 date-added = {2023-11-22 16:42:05 -0500},
 date-modified = {2023-11-22 16:43:09 -0500},
 organization = {IEEE},
 doi = {10.1109/hst.2010.5513121},
 source = {Crossref},
 url = {https://doi.org/10.1109/hst.2010.5513121},
 publisher = {IEEE},
 month = jun,
}

@book{barroso2019datacenter,
 author = {Barroso, Luiz Andr\'e and H\"olzle, Urs and Ranganathan, Parthasarathy},
 title = {The Datacenter as a Computer},
 year = {2019},
 publisher = {Springer International Publishing},
 doi = {10.1007/978-3-031-01761-2},
 source = {Crossref},
 url = {https://doi.org/10.1007/978-3-031-01761-2},
 subtitle = {Designing Warehouse-Scale Machines},
 issn = {1935-3235, 1935-3243},
 isbn = {9783031006333, 9783031017612},
}

@techreport{bashmakov2022climate,
 author = {Bashmakov, Igor and Nilsson, Lars and Acquaye, Adolf and Bataille, Christopher and Cullen, Jonathan and de la Rue du Can, St\'ephane and Fischedick, Manfred and Geng, Yong and Tanaka, Kanako},
 title = {Climate Change 2022: {Mitigation} of Climate Change. Contribution of Working Group {III} to the Sixth Assessment Report of the Intergovernmental Panel on Climate Change, Chapter 11},
 year = {2022},
 institution = {Office of Scientific and Technical Information (OSTI)},
 doi = {10.2172/1973106},
 source = {Crossref},
 url = {https://doi.org/10.2172/1973106},
 month = apr,
}

@inproceedings{bau2017network,
 author = {Bau, David and Zhou, Bolei and Khosla, Aditya and Oliva, Aude and Torralba, Antonio},
 title = {Network Dissection: {Quantifying} Interpretability of Deep Visual Representations},
 year = {2017},
 booktitle = {2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
 pages = {6541--6549},
 doi = {10.1109/cvpr.2017.354},
 source = {Crossref},
 url = {https://doi.org/10.1109/cvpr.2017.354},
 publisher = {IEEE},
 month = jul,
}

@misc{bayes_hyperparam,
 author = {Snoek, Jasper and Larochelle, Hugo and Adams, Ryan P.},
 title = {Practical {Bayesian} Optimization of Machine Learning Algorithms},
 year = {2012},
 eprint = {1206.2944},
 archiveprefix = {arXiv},
 primaryclass = {stat.ML},
}

@article{beck1998beyond,
 author = {Beck, Nathaniel and Jackman, Simon},
 title = {Beyond Linearity by Default: {Generalized} Additive Models},
 year = {1998},
 journal = {Am. J. Polit. Sci.},
 publisher = {JSTOR},
 pages = {596},
 doi = {10.2307/2991772},
 number = {2},
 source = {Crossref},
 url = {https://doi.org/10.2307/2991772},
 volume = {42},
 issn = {0092-5853},
 month = apr,
}

@article{Bender_Friedman_2018,
 author = {Bender, Emily M. and Friedman, Batya},
 title = {Data Statements for Natural Language Processing: {Toward} Mitigating System Bias and Enabling Better Science},
 year = {2018},
 journal = {Transactions of the Association for Computational Linguistics},
 volume = {6},
 pages = {587--604},
 doi = {10.1162/tacl_a_00041},
 bdsk-url-1 = {https://doi.org/10.1162/tacl\_a\_00041},
 source = {Crossref},
 url = {https://doi.org/10.1162/tacl_a_00041},
 publisher = {MIT Press - Journals},
 issn = {2307-387X},
 month = dec,
}

@article{beyer2020we,
 author = {Beyer, Lucas and H\'enaff, Olivier J and Kolesnikov, Alexander and Zhai, Xiaohua and Oord, A\"aron van den},
 title = {Are we done with imagenet?},
 year = {2020},
 journal = {arXiv preprint arXiv:2006.07159},
}

@inproceedings{bhagoji2018practical,
 author = {Bhagoji, Arjun Nitin and He, Warren and Li, Bo and Song, Dawn},
 title = {Practical black-box attacks on deep neural networks using efficient query mechanisms},
 year = {2018},
 booktitle = {Proceedings of the European conference on computer vision (ECCV)},
 pages = {154--169},
}

@inproceedings{Biega2020Oper,
 author = {Biega, Asia J. and Potash, Peter and Daum\'e, Hal and Diaz, Fernando and Finck, Mich\`ele},
 title = {Operationalizing the Legal Principle of Data Minimization for Personalization},
 year = {2020},
 booktitle = {Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval},
 publisher = {ACM},
 address = {New York, NY, USA},
 series = {SIGIR '20},
 pages = {399--408},
 date-added = {2023-11-22 17:57:23 -0500},
 date-modified = {2023-11-22 17:59:54 -0500},
 keywords = {data minimization, privacy, gdpr, recommender systems, purpose limitation, personalization},
 doi = {10.1145/3397271.3401034},
 source = {Crossref},
 url = {https://doi.org/10.1145/3397271.3401034},
 month = jul,
}

@misc{bigbatch,
 author = {You, Yang and Zhang, Zhao and Hsieh, Cho-Jui and Demmel, James and Keutzer, Kurt},
 title = {{ImageNet} Training in Minutes},
 year = {2018},
 eprint = {1709.05011},
 archiveprefix = {arXiv},
 primaryclass = {cs.CV},
}

@article{biggio2012poisoning,
 author = {Biggio, Battista and Nelson, Blaine and Laskov, Pavel},
 title = {Poisoning attacks against support vector machines},
 year = {2012},
 journal = {arXiv preprint arXiv:1206.6389},
 date-added = {2023-11-22 16:21:35 -0500},
 date-modified = {2023-11-22 16:22:06 -0500},
}

@article{biggio2014pattern,
 author = {Biggio, Battista and Fumera, Giorgio and Roli, Fabio},
 title = {Pattern Recognition Systems Under Attack: {Design} Issues And Research Challenges},
 year = {2014},
 journal = {Int. J. Pattern Recogn.},
 publisher = {World Scientific Pub Co Pte Lt},
 volume = {28},
 number = {07},
 pages = {1460002},
 doi = {10.1142/s0218001414600027},
 source = {Crossref},
 url = {https://doi.org/10.1142/s0218001414600027},
 issn = {0218-0014, 1793-6381},
 month = oct,
}

@article{biggs2021natively,
 author = {Biggs, John and Myers, James and Kufel, Jedrzej and Ozer, Emre and Craske, Simon and Sou, Antony and Ramsdale, Catherine and Williamson, Ken and Price, Richard and White, Scott},
 title = {A natively flexible 32-bit Arm microprocessor},
 year = {2021},
 journal = {Nature},
 publisher = {Springer Science and Business Media LLC},
 volume = {595},
 number = {7868},
 pages = {532--536},
 doi = {10.1038/s41586-021-03625-w},
 source = {Crossref},
 url = {https://doi.org/10.1038/s41586-021-03625-w},
 issn = {0028-0836, 1476-4687},
 month = jul,
}

@article{binkert2011gem5,
 author = {Binkert, Nathan and Beckmann, Bradford and Black, Gabriel and Reinhardt, Steven K. and Saidi, Ali and Basu, Arkaprava and Hestness, Joel and Hower, Derek R. and Krishna, Tushar and Sardashti, Somayeh and Sen, Rathijit and Sewell, Korey and Shoaib, Muhammad and Vaish, Nilay and Hill, Mark D. and Wood, David A.},
 title = {The gem5 simulator},
 year = {2011},
 journal = {ACM SIGARCH Computer Architecture News},
 publisher = {Association for Computing Machinery (ACM)},
 volume = {39},
 number = {2},
 pages = {1--7},
 doi = {10.1145/2024716.2024718},
 source = {Crossref},
 url = {https://doi.org/10.1145/2024716.2024718},
 issn = {0163-5964},
 month = may,
}

@misc{blalock_what_2020,
 author = {Blalock, Davis and Ortiz, Jose Javier Gonzalez and Frankle, Jonathan and Guttag, John},
 title = {What is the State of Neural Network Pruning?},
 year = {2020},
 month = mar,
 publisher = {arXiv},
 doi = {10.48550/arXiv.2003.03033},
 url = {http://arxiv.org/abs/2003.03033},
 urldate = {2023-10-20},
 note = {arXiv:2003.03033 [cs, stat]},
 abstract = {Neural network pruning{\textemdash}the task of reducing the size of a network by removing parameters{\textemdash}has been the subject of a great deal of work in recent years. We provide a meta-analysis of the literature, including an overview of approaches to pruning and consistent findings in the literature. After aggregating results across 81 papers and pruning hundreds of models in controlled conditions, our clearest finding is that the community suffers from a lack of standardized benchmarks and metrics. This deficiency is substantial enough that it is hard to compare pruning techniques to one another or determine how much progress the field has made over the past three decades. To address this situation, we identify issues with current practices, suggest concrete remedies, and introduce ShrinkBench, an open-source framework to facilitate standardized evaluations of pruning methods. We use ShrinkBench to compare various pruning techniques and show that its comprehensive evaluation can prevent common pitfalls when comparing pruning methods.},
 file = {arXiv Fulltext PDF:/Users/jeffreyma/Zotero/storage/MA4QGZ6E/Blalock et al. - 2020 - What is the State of Neural Network Pruning.pdf:application/pdf;arXiv.org Snapshot:/Users/jeffreyma/Zotero/storage/8DFKG4GL/2003.html:text/html},
 keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
 bdsk-url-1 = {http://arxiv.org/abs/2003.03033},
 bdsk-url-2 = {https://doi.org/10.48550/arXiv.2003.03033},
}

@inproceedings{bondi2018spot,
 author = {Bondi, Elizabeth and Kapoor, Ashish and Dey, Debadeepta and Piavis, James and Shah, Shital and Hannaford, Robert and Iyer, Arvind and Joppa, Lucas and Tambe, Milind},
 title = {Near Real-Time Detection of Poachers from Drones in {AirSim}},
 year = {2018},
 booktitle = {Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence},
 volume = {32},
 number = {1},
 doi = {10.24963/ijcai.2018/847},
 source = {Crossref},
 url = {https://doi.org/10.24963/ijcai.2018/847},
 publisher = {International Joint Conferences on Artificial Intelligence Organization},
 month = jul,
}

@inproceedings{bourtoule2021machine,
 author = {Bourtoule, Lucas and Chandrasekaran, Varun and Choquette-Choo, Christopher A. and Jia, Hengrui and Travers, Adelin and Zhang, Baiwu and Lie, David and Papernot, Nicolas},
 title = {Machine Unlearning},
 year = {2021},
 booktitle = {2021 IEEE Symposium on Security and Privacy (SP)},
 pages = {141--159},
 organization = {IEEE},
 doi = {10.1109/sp40001.2021.00019},
 source = {Crossref},
 url = {https://doi.org/10.1109/sp40001.2021.00019},
 publisher = {IEEE},
 month = may,
}

@article{breier2018deeplaser,
 author = {Breier, Jakub and Hou, Xiaolu and Jap, Dirmanto and Ma, Lei and Bhasin, Shivam and Liu, Yang},
 title = {Deeplaser: {Practical} fault attack on deep neural networks},
 year = {2018},
 journal = {arXiv preprint arXiv:1806.05859},
}

@inproceedings{Breier2018Practical,
 author = {Breier, Jakub and Hou, Xiaolu and Jap, Dirmanto and Ma, Lei and Bhasin, Shivam and Liu, Yang},
 title = {Practical Fault Attack on Deep Neural Networks},
 booktitle = {Proceedings of the 2018 ACM SIGSAC Conference on Computer and Communications Security},
 publisher = {ACM},
 address = {New York, NY, USA},
 series = {CCS '18},
 pages = {2204--2206},
 date-added = {2023-11-22 16:51:23 -0500},
 date-modified = {2023-11-22 16:53:46 -0500},
 keywords = {fault attacks, deep learning security, adversarial attacks},
 doi = {10.1145/3243734.3278519},
 source = {Crossref},
 url = {https://doi.org/10.1145/3243734.3278519},
 year = {2018},
 month = oct,
}

@article{bricken2023towards,
 author = {Davarzani, Samaneh and Saucier, David and Talegaonkar, Purva and Parker, Erin and Turner, Alana and Middleton, Carver and Carroll, Will and Ball, John E. and Gurbuz, Ali and Chander, Harish and Burch, Reuben F. and Smith, Brian K. and Knight, Adam and Freeman, Charles},
 title = {Closing the Wearable Gap: {Foot{\textendash}ankle} kinematic modeling via deep learning models based on a smart sock wearable},
 year = {2023},
 journal = {Wearable Technologies},
 doi = {10.1017/wtc.2023.3},
 source = {Crossref},
 url = {https://doi.org/10.1017/wtc.2023.3},
 volume = {4},
 publisher = {Cambridge University Press (CUP)},
 issn = {2631-7176},
}

@inproceedings{brown_language_2020,
 author = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
 title = {Language Models are Few-Shot Learners},
 year = {2020},
 journal = {Adv Neural Inf Process Syst},
 booktitle = {Advances in Neural Information Processing Systems},
 publisher = {Curran Associates, Inc.},
 volume = {33},
 pages = {1877--1901},
 url = {https://proceedings.neurips.cc/paper_files/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html},
 urldate = {2023-11-07},
 abstract = {We demonstrate that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even becoming competitive with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks. We also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora.},
 bdsk-url-1 = {https://proceedings.neurips.cc/paper\_files/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html},
}

@article{brown2020language,
 author = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
 title = {Language models are few-shot learners},
 year = {2020},
 journal = {Adv Neural Inf Process Syst},
 volume = {33},
 pages = {1877--1901},
}

@inproceedings{buolamwini2018genderShades,
 author = {Buolamwini, Joy and Gebru, Timnit},
 title = {Gender shades: {Intersectional} accuracy disparities in commercial gender classification},
 year = {2018},
 booktitle = {Conference on fairness, accountability and transparency},
 pages = {77--91},
 organization = {PMLR},
}

@article{Burnet1989Spycatcher,
 author = {Burnet, David and Thomas, Richard},
 title = {Spycatcher: {The} Commodification of Truth},
 year = {1989},
 journal = {J. Law Soc.},
 volume = {16},
 number = {2},
 pages = {210},
 date-added = {2023-11-22 17:03:00 -0500},
 date-modified = {2023-11-22 17:04:44 -0500},
 doi = {10.2307/1410360},
 source = {Crossref},
 url = {https://doi.org/10.2307/1410360},
 publisher = {JSTOR},
 issn = {0263-323X},
}

@article{burr2016recent,
 author = {Burr, Geoffrey W. and BrightSky, Matthew J. and Sebastian, Abu and Cheng, Huai-Yu and Wu, Jau-Yi and Kim, Sangbum and Sosa, Norma E. and Papandreou, Nikolaos and Lung, Hsiang-Lan and Pozidis, Haralampos and Eleftheriou, Evangelos and Lam, Chung H.},
 title = {Recent Progress in Phase-{Change\ensuremath{<}?Pub} \_newline {?\ensuremath{>}Memory} Technology},
 year = {2016},
 journal = {IEEE Journal on Emerging and Selected Topics in Circuits and Systems},
 publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
 volume = {6},
 number = {2},
 pages = {146--162},
 doi = {10.1109/jetcas.2016.2547718},
 source = {Crossref},
 url = {https://doi.org/10.1109/jetcas.2016.2547718},
 issn = {2156-3357, 2156-3365},
 month = jun,
}

@misc{buyya2010energyefficient,
 author = {Buyya, Rajkumar and Beloglazov, Anton and Abawajy, Jemal},
 title = {Energy-Efficient Management of Data Center Resources for Cloud Computing: {A} Vision, Architectural Elements, and Open Challenges},
 year = {2010},
 eprint = {1006.0308},
 archiveprefix = {arXiv},
 primaryclass = {cs.DC},
}

@inproceedings{cai_online_2021,
 author = {Cai, Zhipeng and Sener, Ozan and Koltun, Vladlen},
 title = {Online Continual Learning with Natural Distribution Shifts: {An} Empirical Study with Visual Data},
 shorttitle = {Online Continual Learning with Natural Distribution Shifts},
 year = {2021},
 month = oct,
 booktitle = {2021 IEEE/CVF International Conference on Computer Vision (ICCV)},
 publisher = {IEEE},
 address = {Montreal, QC, Canada},
 pages = {8261--8270},
 doi = {10.1109/iccv48922.2021.00817},
 isbn = {978-1-66542-812-5},
 url = {https://doi.org/10.1109/iccv48922.2021.00817},
 urldate = {2023-10-26},
 file = {Cai et al. - 2021 - Online Continual Learning with Natural Distributio.pdf:/Users/alex/Zotero/storage/R7ZMIM4K/Cai et al. - 2021 - Online Continual Learning with Natural Distributio.pdf:application/pdf},
 language = {en},
 bdsk-url-1 = {https://ieeexplore.ieee.org/document/9710740/},
 bdsk-url-2 = {https://doi.org/10.1109/ICCV48922.2021.00817},
 source = {Crossref},
}

@article{cai_tinytl_nodate,
 author = {Cai, Han and Gan, Chuang and Zhu, Ligeng and Han, Song},
 title = {{TinyTL:} {Reduce} Memory, Not Parameters for Efficient On-Device Learning},
 year = {2020},
 journal = {Adv. Neur. In.},
 volume = {33},
 pages = {11285--11297},
 file = {Cai et al. - TinyTL Reduce Memory, Not Parameters for Efficient.pdf:/Users/alex/Zotero/storage/J9C8PTCX/Cai et al. - TinyTL Reduce Memory, Not Parameters for Efficient.pdf:application/pdf},
 language = {en},
}

@article{cai2018proxylessnas,
 author = {Cai, Han and Zhu, Ligeng and Han, Song},
 title = {Proxylessnas: {Direct} neural architecture search on target task and hardware},
 year = {2018},
 journal = {arXiv preprint arXiv:1812.00332},
}

@article{cai2020tinytl,
 author = {Cai, Han and Gan, Chuang and Zhu, Ligeng and Han, Song},
 title = {Tinytl: {Reduce} memory, not parameters for efficient on-device learning},
 year = {2020},
 journal = {Adv. Neur. In.},
 volume = {33},
 pages = {11285--11297},
}

@article{calvo2020supporting,
 author = {Calvo, Rafael A and Peters, Dorian and Vold, Karina and Ryan, Richard M},
 title = {Supporting human autonomy in {AI} systems: {A} framework for ethical enquiry},
 year = {2020},
 journal = {Ethics of digital well-being: A multidisciplinary approach},
 publisher = {Springer},
 pages = {31--54},
}

@article{Carbon_LNN,
 author = {Patterson, David and Gonzalez, Joseph and Holzle, Urs and Le, Quoc and Liang, Chen and Munguia, Lluis-Miquel and Rothchild, Daniel and So, David R. and Texier, Maud and Dean, Jeff},
 title = {The Carbon Footprint of Machine Learning Training Will Plateau, Then Shrink},
 year = {2022},
 journal = {Computer},
 doi = {10.1109/mc.2022.3148714},
 number = {7},
 source = {Crossref},
 url = {https://doi.org/10.1109/mc.2022.3148714},
 volume = {55},
 publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
 issn = {0018-9162, 1558-0814},
 pages = {18--28},
 month = jul,
}

@inproceedings{carlini2016hidden,
 author = {Carlini, Nicholas and Mishra, Pratyush and Vaidya, Tavish and Zhang, Yuankai and Sherr, Micah and Shields, Clay and Wagner, David and Zhou, Wenchao},
 title = {Hidden voice commands},
 year = {2016},
 booktitle = {25th USENIX security symposium (USENIX security 16)},
 pages = {513--530},
}

@inproceedings{carlini2017adversarial,
 author = {Carlini, Nicholas and Wagner, David},
 title = {Adversarial Examples Are Not Easily Detected},
 year = {2017},
 booktitle = {Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security},
 pages = {3--14},
 doi = {10.1145/3128572.3140444},
 source = {Crossref},
 url = {https://doi.org/10.1145/3128572.3140444},
 publisher = {ACM},
 subtitle = {Bypassing Ten Detection Methods},
 month = nov,
}

@inproceedings{carlini2023extracting,
 author = {Ippolito, Daphne and Tramer, Florian and Nasr, Milad and Zhang, Chiyuan and Jagielski, Matthew and Lee, Katherine and Choquette Choo, Christopher and Carlini, Nicholas},
 title = {Preventing Generation of Verbatim Memorization in Language Models Gives a False Sense of Privacy},
 year = {2023},
 booktitle = {Proceedings of the 16th International Natural Language Generation Conference},
 pages = {5253--5270},
 doi = {10.18653/v1/2023.inlg-main.3},
 source = {Crossref},
 url = {https://doi.org/10.18653/v1/2023.inlg-main.3},
 publisher = {Association for Computational Linguistics},
}

@article{cavoukian2009privacy,
 author = {Cavoukian, Ann},
 title = {Privacy by design},
 year = {2009},
 journal = {Office of the Information and Privacy Commissioner},
 date-added = {2023-11-22 17:55:45 -0500},
 date-modified = {2023-11-22 17:56:58 -0500},
}

@article{cenci_eco-friendly_2022,
 author = {Cenci, Marcelo Pilotto and Scarazzato, Tatiana and Munchen, Daniel Dotto and Dartora, Paula Cristina and Veit, Hugo Marcelo and Bernardes, Andrea Moura and Dias, Pablo R.},
 title = {{Eco-Friendly} {Electronics{\textemdash}A} Comprehensive Review},
 year = {2021},
 journal = {Adv. Mater. Technol.},
 volume = {7},
 number = {2},
 pages = {2001263},
 doi = {10.1002/admt.202001263},
 url = {https://doi.org/10.1002/admt.202001263},
 note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/admt.202001263},
 abstract = {Abstract Eco-friendliness is becoming an indispensable feature for electrical and electronic equipment to thrive in the competitive market. This comprehensive review is the first to define eco-friendly electronics in its multiple meanings: power saving devices, end-of-life impact attenuators, equipment whose manufacturing uses green processing, electronics that use materials that minimize environmental and health risks, designs that improve lifespan, reparability, etc. More specifically, this review discusses eco-friendly technologies and materials that are being introduced to replace the well-established ones. This is done for all material classes (metals, polymers, ceramics, and composites). Manufacturing, recycling, and final product characteristics are discussed in their various interconnected aspects. Additionally, the concept of consciously planned obsolescence is introduced to address the paradoxical relationship between durability and efficiency. The overall conclusions are that there is an important global trend to make electronics more eco-friendly. However, matching the performance and stability of well-established materials and technologies seems to be the main barrier to achieve it. These new implementations can have detrimental or beneficial net impacts on the environment. Assessing their net outcome is challenging because their impacts are frequently unknown and the current evaluation methods (and tools) are incapable of comprehensively quantifying these impacts and generating reliable verdicts.},
 keywords = {eco-friendly electronics, end-of-life, green electronics, green manufacturing, ICT sustainability, recycling, sustainable materials},
 source = {Crossref},
 publisher = {Wiley},
 issn = {2365-709X, 2365-709X},
 month = apr,
}

@article{Chapelle_Scholkopf_Zien,
 author = {Chapelle, O. and Scholkopf, B. and Zien, Eds., A.},
 title = {Semi-Supervised Learning {(Chapelle,} {O.} et al., Eds.; 2006) {[Book} reviews]},
 year = {2009},
 journal = {IEEE Trans. Neural Networks},
 volume = {20},
 number = {3},
 pages = {542--542},
 doi = {10.1109/tnn.2009.2015974},
 bdsk-url-1 = {https://doi.org/10.1109/tnn.2009.2015974},
 source = {Crossref},
 url = {https://doi.org/10.1109/tnn.2009.2015974},
 publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
 issn = {1045-9227},
 month = mar,
}

@misc{chen__inpainting_2022,
 author = {Xinyu, Chen},
 title = {Inpainting Fluid Dynamics with Tensor Decomposition {(NumPy)}},
 year = {2022},
 month = mar,
 journal = {Medium},
 url = {https://medium.com/@xinyu.chen/inpainting-fluid-dynamics-with-tensor-decomposition-numpy-d84065fead4d},
 urldate = {2023-10-20},
 abstract = {Some simple examples for showing how to use tensor decomposition to reconstruct fluid dynamics},
 language = {en},
 bdsk-url-1 = {https://medium.com/@xinyu.chen/inpainting-fluid-dynamics-with-tensor-decomposition-numpy-d84065fead4d},
}

@misc{chen_tvm_2018,
 author = {Chen, Tianqi and Moreau, Thierry and Jiang, Ziheng and Zheng, Lianmin and Yan, Eddie and Cowan, Meghan and Shen, Haichen and Wang, Leyuan and Hu, Yuwei and Ceze, Luis and Guestrin, Carlos and Krishnamurthy, Arvind},
 title = {{TVM:} {An} Automated End-to-End Optimizing Compiler for Deep Learning},
 shorttitle = {TVM},
 year = {2018},
 month = oct,
 booktitle = {13th USENIX Symposium on Operating Systems Design and Implementation (OSDI 18)},
 publisher = {arXiv},
 pages = {578--594},
 url = {http://arxiv.org/abs/1802.04799},
 urldate = {2023-10-26},
 note = {arXiv:1802.04799 [cs]},
 annote = {Comment: Significantly improved version, add automated optimization},
 file = {Chen et al. - 2018 - TVM An Automated End-to-End Optimizing Compiler f.pdf:/Users/alex/Zotero/storage/QR8MHJ38/Chen et al. - 2018 - TVM An Automated End-to-End Optimizing Compiler f.pdf:application/pdf},
 keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Programming Languages},
 language = {en},
 bdsk-url-1 = {http://arxiv.org/abs/1802.04799},
}

@article{chen2006gallium,
 author = {Chen, H.-W.},
 title = {Gallium, Indium, and Arsenic Pollution of Groundwater from a Semiconductor Manufacturing Area of {Taiwan}},
 year = {2006},
 journal = {B. Environ. Contam. Tox.},
 volume = {77},
 number = {2},
 doi = {10.1007/s00128-006-1062-3},
 source = {Crossref},
 url = {https://doi.org/10.1007/s00128-006-1062-3},
 publisher = {Springer Science and Business Media LLC},
 issn = {0007-4861, 1432-0800},
 pages = {289--296},
 month = aug,
}

@article{chen2016training,
 author = {Chen, Tianqi and Xu, Bing and Zhang, Chiyuan and Guestrin, Carlos},
 title = {Training deep nets with sublinear memory cost},
 year = {2016},
 journal = {arXiv preprint arXiv:1604.06174},
}

@inproceedings{chen2018tvm,
 author = {Chen, Tianqi and Moreau, Thierry and Jiang, Ziheng and Zheng, Lianmin and Yan, Eddie and Shen, Haichen and Cowan, Meghan and Wang, Leyuan and Hu, Yuwei and Ceze, Luis and others},
 title = {{TVM:} {An} automated End-to-End optimizing compiler for deep learning},
 year = {2018},
 booktitle = {13th USENIX Symposium on Operating Systems Design and Implementation (OSDI 18)},
 pages = {578--594},
}

@article{chen2019looks,
 author = {Chen, Chaofan and Li, Oscar and Tao, Daniel and Barnett, Alina and Rudin, Cynthia and Su, Jonathan K},
 title = {This looks like that: {Deep} learning for interpretable image recognition},
 year = {2019},
 journal = {Adv Neural Inf Process Syst},
 volume = {32},
}

@article{Chen2023,
 author = {Chen, Emma and Prakash, Shvetank and Janapa Reddi, Vijay and Kim, David and Rajpurkar, Pranav},
 title = {A framework for integrating artificial intelligence for clinical care with continuous therapeutic monitoring},
 year = {2023},
 month = nov,
 day = {06},
 journal = {Nat. Biomed. Eng.},
 doi = {10.1038/s41551-023-01115-0},
 issn = {2157-846X},
 url = {https://doi.org/10.1038/s41551-023-01115-0},
 bdsk-url-1 = {https://doi.org/10.1038/s41551-023-01115-0},
 source = {Crossref},
 publisher = {Springer Science and Business Media LLC},
}

@article{chen2023learning,
 author = {Chen, Zhiyong and Xu, Shugong},
 title = {Learning domain-heterogeneous speaker recognition systems with personalized continual federated learning},
 year = {2023},
 journal = {EURASIP Journal on Audio, Speech, and Music Processing},
 publisher = {Springer Science and Business Media LLC},
 volume = {2023},
 number = {1},
 pages = {33},
 doi = {10.1186/s13636-023-00299-2},
 source = {Crossref},
 url = {https://doi.org/10.1186/s13636-023-00299-2},
 issn = {1687-4722},
 month = sep,
}

@article{cheng2017survey,
 author = {Cheng, Yu and Wang, Duo and Zhou, Pan and Zhang, Tao},
 title = {Model Compression and Acceleration for Deep Neural Networks: {The} Principles, Progress, and Challenges},
 year = {2018},
 journal = {IEEE Signal Process Mag.},
 doi = {10.1109/msp.2017.2765695},
 number = {1},
 source = {Crossref},
 url = {https://doi.org/10.1109/msp.2017.2765695},
 volume = {35},
 publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
 issn = {1053-5888},
 pages = {126--136},
 month = jan,
}

@article{chenGalliumIndiumArsenic2006,
 author = {Chen, H.-W.},
 title = {Gallium, Indium, and Arsenic Pollution of Groundwater from a Semiconductor Manufacturing Area of {Taiwan}},
 year = {2006},
 journal = {B. Environ. Contam. Tox.},
 volume = {77},
 number = {2},
 doi = {10.1007/s00128-006-1062-3},
 source = {Crossref},
 url = {https://doi.org/10.1007/s00128-006-1062-3},
 publisher = {Springer Science and Business Media LLC},
 issn = {0007-4861, 1432-0800},
 pages = {289--296},
 month = aug,
}

@article{chi2016prime,
 author = {Chi, Ping and Li, Shuangchen and Xu, Cong and Zhang, Tao and Zhao, Jishen and Liu, Yongpan and Wang, Yu and Xie, Yuan},
 title = {Prime},
 year = {2016},
 journal = {ACM SIGARCH Computer Architecture News},
 publisher = {Association for Computing Machinery (ACM)},
 volume = {44},
 number = {3},
 pages = {27--39},
 doi = {10.1145/3007787.3001140},
 source = {Crossref},
 url = {https://doi.org/10.1145/3007787.3001140},
 subtitle = {a novel processing-in-memory architecture for neural network computation in ReRAM-based main memory},
 issn = {0163-5964},
 month = jun,
}

@misc{chollet2015,
 author = {Chollet, Fran\c{c}ois},
 title = {keras},
 year = {2015},
 journal = {GitHub repository},
 publisher = {GitHub},
 commit = {5bcac37},
 howpublished = {https://github.com/fchollet/keras},
}

@article{chollet2018keras,
 author = {Chollet, Fran\c{c}ois},
 title = {Introduction to keras},
 year = {2018},
 journal = {March 9th},
}

@article{christiano2017deep,
 author = {Christiano, Paul F and Leike, Jan and Brown, Tom and Martic, Miljan and Legg, Shane and Amodei, Dario},
 title = {Deep reinforcement learning from human preferences},
 year = {2017},
 journal = {Adv Neural Inf Process Syst},
 volume = {30},
}

@inproceedings{chu2021discovering,
 author = {Chu, Grace and Arikan, Okan and Bender, Gabriel and Wang, Weijun and Brighton, Achille and Kindermans, Pieter-Jan and Liu, Hanxiao and Akin, Berkin and Gupta, Suyog and Howard, Andrew},
 title = {Discovering Multi-Hardware Mobile Models via Architecture Search},
 year = {2021},
 booktitle = {2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)},
 pages = {3022--3031},
 archiveprefix = {arXiv},
 eprint = {2008.08178},
 primaryclass = {cs.CV},
 doi = {10.1109/cvprw53098.2021.00337},
 source = {Crossref},
 url = {https://doi.org/10.1109/cvprw53098.2021.00337},
 publisher = {IEEE},
 month = jun,
}

@article{chua1971memristor,
 author = {Chua, L.},
 title = {Memristor-The missing circuit element},
 year = {1971},
 journal = {\#IEEE\_J\_CT\#},
 publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
 volume = {18},
 number = {5},
 pages = {507--519},
 doi = {10.1109/tct.1971.1083337},
 source = {Crossref},
 url = {https://doi.org/10.1109/tct.1971.1083337},
 issn = {0018-9324},
}

@article{coleman2017dawnbench,
 author = {Coleman, Cody and Kang, Daniel and Narayanan, Deepak and Nardi, Luigi and Zhao, Tian and Zhang, Jian and Bailis, Peter and Olukotun, Kunle and R\'e, Chris and Zaharia, Matei},
 title = {Analysis of {DAWNBench,} a Time-to-Accuracy Machine Learning Performance Benchmark},
 year = {2019},
 journal = {ACM SIGOPS Operating Systems Review},
 volume = {53},
 number = {1},
 pages = {14--25},
 doi = {10.1145/3352020.3352024},
 source = {Crossref},
 url = {https://doi.org/10.1145/3352020.3352024},
 publisher = {Association for Computing Machinery (ACM)},
 issn = {0163-5980},
 month = jul,
}

@inproceedings{coleman2022similarity,
 author = {Coleman, Cody and Chou, Edward and Katz-Samuels, Julian and Culatana, Sean and Bailis, Peter and Berg, Alexander C and Nowak, Robert and Sumbaly, Roshan and Zaharia, Matei and Yalniz, I Zeki},
 title = {Similarity search for efficient active learning and search of rare concepts},
 year = {2022},
 booktitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
 volume = {36},
 number = {6},
 pages = {6402--6410},
}

@inproceedings{cooper2011semiconductor,
 author = {Cooper, Tom and Fallender, Suzanne and Pafumi, Joyann and Dettling, Jon and Humbert, Sebastien and Lessard, Lindsay},
 title = {A semiconductor company's examination of its water footprint approach},
 year = {2011},
 booktitle = {Proceedings of the 2011 IEEE International Symposium on Sustainable Systems and Technology},
 pages = {1--6},
 organization = {IEEE},
 doi = {10.1109/issst.2011.5936865},
 source = {Crossref},
 url = {https://doi.org/10.1109/issst.2011.5936865},
 publisher = {IEEE},
 month = may,
}

@article{cope2009pure,
 author = {Cope, Gord},
 title = {Pure water, semiconductors and the recession},
 year = {2009},
 journal = {Global Water Intelligence},
 volume = {10},
 number = {10},
}

@misc{cottier_trends_2023,
 author = {Cottier, Ben},
 title = {Trends in the Dollar Training Cost of Machine Learning Systems},
 year = {2023},
 month = jan,
 journal = {Epoch AI Report},
 url = {https://epochai.org/blog/trends-in-the-dollar-training-cost-of-machine-learning-systems},
 bdsk-url-1 = {https://epochai.org/blog/trends-in-the-dollar-training-cost-of-machine-learning-systems},
}

@book{d2023dataFeminism,
 author = {D'ignazio, Catherine and Klein, Lauren F},
 title = {Data feminism},
 year = {2023},
 publisher = {MIT press},
}

@article{dahl2023benchmarking,
 author = {Dahl, George E and Schneider, Frank and Nado, Zachary and Agarwal, Naman and Sastry, Chandramouli Shama and Hennig, Philipp and Medapati, Sourabh and Eschenhagen, Runa and Kasimbeg, Priya and Suo, Daniel and others},
 title = {{CSF} Findings in Acute {NMDAR} and {LGI1} {Antibody{\textendash}Associated} Autoimmune Encephalitis},
 year = {2021},
 journal = {Neurology Neuroimmunology \&amp; Neuroinflammation},
 doi = {10.1212/nxi.0000000000001086},
 number = {6},
 source = {Crossref},
 url = {https://doi.org/10.1212/nxi.0000000000001086},
 volume = {8},
 publisher = {Ovid Technologies (Wolters Kluwer Health)},
 issn = {2332-7812},
 month = nov,
}

@article{dally_evolution_2021,
 author = {Dally, William J. and Keckler, Stephen W. and Kirk, David B.},
 title = {Evolution of the Graphics Processing Unit {(GPU)}},
 year = {2021},
 month = nov,
 journal = {IEEE Micro},
 volume = {41},
 number = {6},
 pages = {42--51},
 doi = {10.1109/mm.2021.3113475},
 issn = {0272-1732, 1937-4143},
 url = {https://doi.org/10.1109/mm.2021.3113475},
 urldate = {2023-11-07},
 note = {Conference Name: IEEE Micro},
 abstract = {Graphics processing units (GPUs) power today's fastest supercomputers, are the dominant platform for deep learning, and provide the intelligence for devices ranging from self-driving cars to robots and smart cameras. They also generate compelling photorealistic images at real-time frame rates. GPUs have evolved by adding features to support new use cases. NVIDIA's GeForce 256, the first GPU, was a dedicated processor for real-time graphics, an application that demands large amounts of floating-point arithmetic for vertex and fragment shading computations and high memory bandwidth. As real-time graphics advanced, GPUs became programmable. The combination of programmability and floating-point performance made GPUs attractive for running scientific applications. Scientists found ways to use early programmable GPUs by casting their calculations as vertex and fragment shaders. GPUs evolved to meet the needs of scientific users by adding hardware for simpler programming, double-precision floating-point arithmetic, and resilience.},
 bdsk-url-1 = {https://ieeexplore.ieee.org/document/9623445},
 bdsk-url-2 = {https://doi.org/10.1109/MM.2021.3113475},
 source = {Crossref},
 publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
}

@article{data_centers_wheels,
 author = {Sudhakar, Soumya and Sze, Vivienne and Karaman, Sertac},
 title = {Data Centers on Wheels: {Emissions} From Computing Onboard Autonomous Vehicles},
 year = {2023},
 journal = {IEEE Micro},
 volume = {43},
 number = {1},
 pages = {29--39},
 doi = {10.1109/mm.2022.3219803},
 source = {Crossref},
 url = {https://doi.org/10.1109/mm.2022.3219803},
 publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
 issn = {0272-1732, 1937-4143},
 month = jan,
}

@misc{Datacent17:online,
 year = {},
 note = {(Accessed on 12/06/2023)},
 howpublished = {https://www.iea.org/energy-system/buildings/data-centres-and-data-transmission-networks},
}

@misc{david_tensorflow_2021,
 author = {David, Robert and Duke, Jared and Jain, Advait and Reddi, Vijay Janapa and Jeffries, Nat and Li, Jian and Kreeger, Nick and Nappier, Ian and Natraj, Meghna and Regev, Shlomi and Rhodes, Rocky and Wang, Tiezhen and Warden, Pete},
 title = {{TensorFlow} Lite Micro: {Embedded} Machine Learning on {TinyML} Systems},
 shorttitle = {TensorFlow Lite Micro},
 year = {2021},
 month = mar,
 publisher = {arXiv},
 url = {http://arxiv.org/abs/2010.08678},
 urldate = {2023-10-26},
 note = {arXiv:2010.08678 [cs]},
 file = {David et al. - 2021 - TensorFlow Lite Micro Embedded Machine Learning o.pdf:/Users/alex/Zotero/storage/YCFVNEVH/David et al. - 2021 - TensorFlow Lite Micro Embedded Machine Learning o.pdf:application/pdf},
 keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
 language = {en},
 bdsk-url-1 = {http://arxiv.org/abs/2010.08678},
}

@article{david2021tensorflow,
 author = {David, Robert and Duke, Jared and Jain, Advait and Janapa Reddi, Vijay and Jeffries, Nat and Li, Jian and Kreeger, Nick and Nappier, Ian and Natraj, Meghna and Wang, Tiezhen and others},
 title = {Tensorflow lite micro: {Embedded} machine learning for tinyml systems},
 year = {2021},
 journal = {Proceedings of Machine Learning and Systems},
 volume = {3},
 pages = {800--811},
}

@article{davies2018loihi,
 author = {Davies, Mike and Srinivasa, Narayan and Lin, Tsung-Han and Chinya, Gautham and Cao, Yongqiang and Choday, Sri Harsha and Dimou, Georgios and Joshi, Prasad and Imam, Nabil and Jain, Shweta and Liao, Yuyun and Lin, Chit-Kwan and Lines, Andrew and Liu, Ruokun and Mathaikutty, Deepak and McCoy, Steven and Paul, Arnab and Tse, Jonathan and Venkataramanan, Guruguhanathan and Weng, Yi-Hsin and Wild, Andreas and Yang, Yoonseok and Wang, Hong},
 title = {Loihi: {A} Neuromorphic Manycore Processor with On-Chip Learning},
 year = {2018},
 journal = {IEEE Micro},
 publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
 volume = {38},
 number = {1},
 pages = {82--99},
 doi = {10.1109/mm.2018.112130359},
 source = {Crossref},
 url = {https://doi.org/10.1109/mm.2018.112130359},
 issn = {0272-1732, 1937-4143},
 month = jan,
}

@article{davies2021advancing,
 author = {Davies, Mike and Wild, Andreas and Orchard, Garrick and Sandamirskaya, Yulia and Guerra, Gabriel A. Fonseca and Joshi, Prasad and Plank, Philipp and Risbud, Sumedh R.},
 title = {Advancing Neuromorphic Computing With Loihi: {A} Survey of Results and Outlook},
 year = {2021},
 journal = {Proc. IEEE},
 publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
 volume = {109},
 number = {5},
 pages = {911--934},
 doi = {10.1109/jproc.2021.3067593},
 source = {Crossref},
 url = {https://doi.org/10.1109/jproc.2021.3067593},
 issn = {0018-9219, 1558-2256},
 month = may,
}

@techreport{daviesEndangeredElements2011,
 author = {Davies, Emma},
 title = {Endangered elements: {Critical} thinking},
 year = {2011},
 month = jan,
 pages = {50--54},
 url = {https://www.rsc.org/images/Endangered%20Elements%20-%20Critical%20Thinking_tcm18-196054.pdf},
}

@article{dayarathna2015data,
 author = {Dayarathna, Miyuru and Wen, Yonggang and Fan, Rui},
 title = {Data Center Energy Consumption Modeling: {A} Survey},
 year = {2016},
 journal = {IEEE Communications Surveys \&amp; Tutorials},
 publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
 volume = {18},
 number = {1},
 pages = {732--794},
 doi = {10.1109/comst.2015.2481183},
 source = {Crossref},
 url = {https://doi.org/10.1109/comst.2015.2481183},
 issn = {1553-877X},
}

@misc{dean_jeff_numbers_nodate,
 author = {Jeff, Dean.},
 title = {Numbers Everyone Should Know},
 url = {https://brenocon.com/dean_perf.html},
 urldate = {2023-11-07},
 bdsk-url-1 = {https://brenocon.com/dean\_perf.html},
}

@article{dean2012large,
 author = {Dean, Jeffrey and Corrado, Greg and Monga, Rajat and Chen, Kai and Devin, Matthieu and Mao, Mark and Ranzato, Marc'aurelio and Senior, Andrew and Tucker, Paul and Yang, Ke and others},
 title = {Large scale distributed deep networks},
 year = {2012},
 journal = {Adv Neural Inf Process Syst},
 volume = {25},
}

@misc{deci,
 title = {The Ultimate Guide to Deep Learning Model Quantization and Quantization-Aware Training},
 url = {https://deci.ai/quantization-and-quantization-aware-training/},
 bdsk-url-1 = {https://deci.ai/quantization-and-quantization-aware-training/},
}

@misc{deepcompress,
 author = {Han and Mao and Dally},
 title = {Deep Compression: {Compressing} Deep Neural Networks with Pruning, Trained Quantization and {Huffman} Coding},
 year = {2016},
 journal = {arXiv preprint arXiv:1510.00149},
 doi = {10.48550/arXiv.1510.00149},
 url = {https://arxiv.org/abs/1510.00149},
 urldate = {2016-02-15},
 abstract = {Neural networks are both computationally intensive and memory intensive, making them difficult to deploy on embedded systems with limited hardware resources. To address this limitation, we introduce {\textquotedblright}deep compression{\textquotedblright}, a three stage pipeline: pruning, trained quantization and Huffman coding, that work together to reduce the storage requirement of neural networks by 35x to 49x without affecting their accuracy. Our method first prunes the network by learning only the important connections. Next, we quantize the weights to enforce weight sharing, finally, we apply Huffman coding. After the first two steps we retrain the network to fine tune the remaining connections and the quantized centroids. Pruning, reduces the number of connections by 9x to 13x; Quantization then reduces the number of bits that represent each connection from 32 to 5. On the ImageNet dataset, our method reduced the storage required by AlexNet by 35x, from 240MB to 6.9MB, without loss of accuracy. Our method reduced the size of VGG-16 by 49x from 552MB to 11.3MB, again with no loss of accuracy. This allows fitting the model into on-chip SRAM cache rather than off-chip DRAM memory. Our compression method also facilitates the use of complex neural networks in mobile applications where application size and download bandwidth are constrained. Benchmarked on CPU, GPU and mobile GPU, compressed network has 3x to 4x layerwise speedup and 3x to 7x better energy efficiency.},
 bdsk-url-1 = {https://arxiv.org/abs/1510.00149},
 bdsk-url-2 = {https://doi.org/10.48550/arXiv.1510.00149},
}

@article{demler_ceva_2020,
 author = {Demler, Mike},
 title = {Ceva Senspro Fuses Ai And Vector Dsp},
 year = {2020},
 language = {en},
}

@inproceedings{deng2009imagenet,
 author = {Deng, Jia and Socher, R. and Fei-Fei, Li and Dong, Wei and Li, Kai and Li, Li-Jia},
 title = {{ImageNet:} {A} large-scale hierarchical image database},
 year = {2009},
 month = jun,
 booktitle = {2009 IEEE Conference on Computer Vision and Pattern Recognition},
 volume = {00},
 pages = {248--255},
 doi = {10.1109/cvpr.2009.5206848},
 url = {https://doi.org/10.1109/cvpr.2009.5206848},
 added-at = {2018-09-20T15:22:39.000+0200},
 biburl = {https://www.bibsonomy.org/bibtex/252793859f5bcbbd3f7f9e5d083160acf/analyst},
 description = {ImageNet: A large-scale hierarchical image database},
 interhash = {fbfae3e4fe1a81c477ba00efd0d4d977},
 intrahash = {52793859f5bcbbd3f7f9e5d083160acf},
 keywords = {2009 computer-vision cvpr dataset ieee paper},
 timestamp = {2018-09-20T15:22:39.000+0200},
 bdsk-url-1 = {https://ieeexplore.ieee.org/abstract/document/5206848/},
 bdsk-url-2 = {https://doi.org/10.1109/CVPR.2009.5206848},
 source = {Crossref},
 publisher = {IEEE},
}

@article{desai2016five,
 author = {Desai, Tanvi and Ritchie, Felix and Welpton, Richard and others},
 title = {Five Safes: {Designing} data access for research},
 year = {2016},
 journal = {Economics Working Paper Series},
 volume = {1601},
 pages = {28},
}

@article{desai2020five,
 author = {Desai, Tanvi and Ritchie, Felix and Welpton, Richard},
 title = {Five Safes: {Designing} data access for research; 2016},
 year = {2020},
 journal = {URL https://www2. uwe. ac. uk/faculties/bbs/Documents/1601. pdf},
}

@article{devlin2018bert,
 author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
 title = {Bert: {Pre-training} of deep bidirectional transformers for language understanding},
 year = {2018},
 journal = {arXiv preprint arXiv:1810.04805},
}

@book{dhanjani2015abusing,
 author = {Greengard, Samuel},
 title = {The Internet of Things},
 year = {2015},
 publisher = {The MIT Press},
 date-added = {2023-11-22 17:09:41 -0500},
 date-modified = {2023-11-22 17:10:22 -0500},
 doi = {10.7551/mitpress/10277.001.0001},
 source = {Crossref},
 url = {https://doi.org/10.7551/mitpress/10277.001.0001},
 isbn = {9780262328937},
}

@article{dhar2021survey,
 author = {Dhar, Sauptik and Guo, Junyao and Liu, Jiayi (Jason) and Tripathi, Samarth and Kurup, Unmesh and Shah, Mohak},
 title = {A Survey of On-Device Machine Learning},
 year = {2021},
 journal = {ACM Transactions on Internet of Things},
 publisher = {Association for Computing Machinery (ACM)},
 volume = {2},
 number = {3},
 pages = {1--49},
 doi = {10.1145/3450494},
 source = {Crossref},
 url = {https://doi.org/10.1145/3450494},
 subtitle = {An Algorithms and Learning Theory Perspective},
 issn = {2691-1914, 2577-6207},
 month = jul,
}

@misc{dodge2022measuring,
 author = {Dodge, Jesse and Prewitt, Taylor and Combes, Remi Tachet Des and Odmark, Erika and Schwartz, Roy and Strubell, Emma and Luccioni, Alexandra Sasha and Smith, Noah A. and DeCario, Nicole and Buchanan, Will},
 title = {Measuring the Carbon Intensity of {AI} in Cloud Instances},
 year = {2022},
 eprint = {2206.05229},
 archiveprefix = {arXiv},
 primaryclass = {cs.LG},
}

@misc{dong2022splitnets,
 author = {Dong, Xin and Salvo, Barbara De and Li, Meng and Liu, Chiao and Qu, Zhongnan and Kung, H. T. and Li, Ziyun},
 title = {{SplitNets:} {Designing} Neural Architectures for Efficient Distributed Computing on Head-Mounted Systems},
 year = {2022},
 archiveprefix = {arXiv},
 eprint = {2204.04705},
 primaryclass = {cs.LG},
}

@article{Dongarra2009-na,
 author = {Dongarra, Jack J},
 title = {The evolution of high performance computing on system z},
 year = {2009},
 journal = {IBM J. Res. Dev.},
 volume = {53},
 pages = {3--4},
}

@article{dropout,
 author = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
 title = {Dropout: {A} Simple Way to Prevent Neural Networks from Overfitting},
 year = {2014},
 journal = {J. Mach. Learn. Res.},
 url = {http://jmlr.org/papers/v15/srivastava14a.html},
}

@article{duarte2022fastml,
 author = {Duarte, Javier and Tran, Nhan and Hawks, Ben and Herwig, Christian and Muhizi, Jules and Prakash, Shvetank and Reddi, Vijay Janapa},
 title = {{FastML} Science Benchmarks: {Accelerating} Real-Time Scientific Edge Machine Learning},
 year = {2022},
 journal = {arXiv preprint arXiv:2207.07958},
}

@article{duisterhof2019learning,
 author = {Duisterhof, Bardienus P and Krishnan, Srivatsan and Cruz, Jonathan J and Banbury, Colby R and Fu, William and Faust, Aleksandra and de Croon, Guido CHE and Reddi, Vijay Janapa},
 title = {Learning to seek: {Autonomous} source seeking with deep reinforcement learning onboard a nano drone microcontroller},
 year = {2019},
 journal = {arXiv preprint arXiv:1909.11236},
}

@inproceedings{duisterhof2021sniffy,
 author = {Duisterhof, Bardienus P. and Li, Shushuai and Burgues, Javier and Reddi, Vijay Janapa and de Croon, Guido C. H. E.},
 title = {Sniffy Bug: {A} Fully Autonomous Swarm of Gas-Seeking Nano Quadcopters in Cluttered Environments},
 year = {2021},
 booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
 pages = {9099--9106},
 organization = {IEEE},
 doi = {10.1109/iros51168.2021.9636217},
 source = {Crossref},
 url = {https://doi.org/10.1109/iros51168.2021.9636217},
 publisher = {IEEE},
 month = sep,
}

@inproceedings{Dwork2006Theory,
 author = {Dwork, Cynthia and McSherry, Frank and Nissim, Kobbi and Smith, Adam},
 editor = {Halevi, Shai and Rabin, Tal},
 title = {Calibrating Noise to Sensitivity in Private Data Analysis},
 year = {2006},
 booktitle = {Theory of Cryptography},
 publisher = {Springer Berlin Heidelberg},
 address = {Berlin, Heidelberg},
 pages = {265--284},
 date-added = {2023-11-22 18:04:12 -0500},
 date-modified = {2023-11-22 18:05:20 -0500},
}

@article{dwork2014algorithmic,
 author = {Dwork, Cynthia and Roth, Aaron},
 title = {The Algorithmic Foundations of Differential Privacy},
 year = {2013},
 journal = {Foundations and Trends{\textregistered} in Theoretical Computer Science},
 publisher = {Now Publishers},
 volume = {9},
 number = {3-4},
 pages = {211--407},
 doi = {10.1561/0400000042},
 source = {Crossref},
 url = {https://doi.org/10.1561/0400000042},
 issn = {1551-305X, 1551-3068},
}

@article{e_waste,
 author = {Singh, Narendra and Ogunseitan, Oladele A.},
 title = {Disentangling the worldwide web of e-waste and climate change co-benefits},
 year = {2022},
 month = dec,
 journal = {Circular Economy},
 publisher = {Elsevier BV},
 volume = {1},
 number = {2},
 pages = {100011},
 doi = {10.1016/j.cec.2022.100011},
 issn = {2773-1677},
 url = {https://doi.org/10.1016/j.cec.2022.100011},
 source = {Crossref},
}

@article{ebrahimi_review_2014,
 author = {Ebrahimi, Khosrow and Jones, Gerard F. and Fleischer, Amy S.},
 title = {A review of data center cooling technology, operating conditions and the corresponding low-grade waste heat recovery opportunities},
 year = {2014},
 journal = {Renewable Sustainable Energy Rev.},
 volume = {31},
 pages = {622--638},
 doi = {10.1016/j.rser.2013.12.007},
 issn = {1364-0321},
 url = {https://doi.org/10.1016/j.rser.2013.12.007},
 abstract = {The depletion of the world's limited reservoirs of fossil fuels, the worldwide impact of global warming and the high cost of energy are among the primary issues driving a renewed interest in the capture and reuse of waste energy. A major source of waste energy is being created by data centers through the increasing demand for cloud based connectivity and performance. In fact, recent figures show that data centers are responsible for more than 2\% of the US total electricity usage. Almost half of this power is used for cooling the electronics, creating a significant stream of waste heat. The difficulty associated with recovering and reusing this stream of waste heat is that the heat is of low quality. In this paper, the most promising methods and technologies for recovering data center low-grade waste heat in an effective and economically reasonable way are identified and discussed. A number of currently available and developmental low-grade waste heat recovery techniques including district/plant/water heating, absorption cooling, direct power generation (piezoelectric and thermoelectric), indirect power generation (steam and organic Rankine cycle), biomass co-location, and desalination/clean water are reviewed along with their operational requirements in order to assess the suitability and effectiveness of each technology for data center applications. Based on a comparison between data centers' operational thermodynamic conditions and the operational requirements of the discussed waste heat recovery techniques, absorption cooling and organic Rankine cycle are found to be among the most promising technologies for data center waste heat reuse.},
 keywords = {Absorption refrigeration, Data center, Organic Rankine cycle, Thermoelectric, Waste energy reuse, Waste heat recovery},
 source = {Crossref},
 publisher = {Elsevier BV},
 month = mar,
}

@article{el-rayis_reconfigurable_nodate,
 author = {El-Rayis, Ahmed Osman},
 title = {Reconfigurable Architectures for the Next Generation of Mobile Device Telecommunications Systems},
 language = {en},
}

@article{eldan2023whos,
 author = {Eldan, Ronen and Russinovich, Mark},
 title = {Who's Harry Potter? Approximate Unlearning in {LLMs}},
 year = {2023},
 journal = {arXiv preprint arXiv:2310.02238},
 date-added = {2023-11-22 19:24:35 -0500},
 date-modified = {2023-11-22 19:25:20 -0500},
}

@article{electronics12102287,
 author = {Moshawrab, Mohammad and Adda, Mehdi and Bouzouane, Abdenour and Ibrahim, Hussein and Raad, Ali},
 title = {Reviewing Federated Learning Aggregation Algorithms; Strategies, Contributions, Limitations and Future Perspectives},
 year = {2023},
 journal = {Electronics},
 publisher = {MDPI AG},
 volume = {12},
 number = {10},
 pages = {2287},
 doi = {10.3390/electronics12102287},
 issn = {2079-9292},
 url = {https://doi.org/10.3390/electronics12102287},
 article-number = {2287},
 bdsk-url-1 = {https://www.mdpi.com/2079-9292/12/10/2287},
 bdsk-url-2 = {https://doi.org/10.3390/electronics12102287},
 source = {Crossref},
 month = may,
}

@article{EnergyCons_Emission,
 author = {Liu, Yanan and Wei, Xiaoxia and Xiao, Jinyu and Liu, Zhijie and Xu, Yang and Tian, Yun},
 title = {Energy consumption and emission mitigation prediction based on data center traffic and {PUE} for global data centers},
 year = {2020},
 month = jun,
 journal = {Global Energy Interconnection},
 publisher = {Elsevier BV},
 volume = {3},
 number = {3},
 pages = {272--282},
 doi = {10.1016/j.gloei.2020.07.008},
 issn = {2096-5117},
 url = {https://doi.org/10.1016/j.gloei.2020.07.008},
 source = {Crossref},
}

@misc{energyproblem,
 author = {Isscc},
 title = {Computing's energy problem (and what we can do about it)},
 year = {2014},
 url = {https://ieeexplore.ieee.org/document/6757323},
 urldate = {2014-03-06},
 bdsk-url-1 = {https://ieeexplore.ieee.org/document/6757323},
}

@article{esteva2017dermatologist,
 author = {Esteva, Andre and Kuprel, Brett and Novoa, Roberto A. and Ko, Justin and Swetter, Susan M. and Blau, Helen M. and Thrun, Sebastian},
 title = {Dermatologist-level classification of skin cancer with deep neural networks},
 year = {2017},
 journal = {Nature},
 publisher = {Springer Science and Business Media LLC},
 volume = {542},
 number = {7639},
 pages = {115--118},
 doi = {10.1038/nature21056},
 source = {Crossref},
 url = {https://doi.org/10.1038/nature21056},
 issn = {0028-0836, 1476-4687},
 month = jan,
}

@article{eykholt2018robust,
 author = {Eykholt, Kevin and Evtimov, Ivan and Fernandes, Earlence and Li, Bo and Rahmati, Amir and Xiao, Chaowei and Prakash, Atul and Kohno, Tadayoshi and Song, Dawn},
 title = {Robust Physical-World Attacks on Deep Learning Models},
 year = {2018},
 journal = {arXiv preprint arXiv:1707.08945},
 date-added = {2023-11-22 16:30:51 -0500},
 date-modified = {2023-11-22 16:31:55 -0500},
}

@misc{fahim2021hls4ml,
 author = {Fahim, Farah and Hawks, Benjamin and Herwig, Christian and Hirschauer, James and Jindariani, Sergo and Tran, Nhan and Carloni, Luca P. and Guglielmo, Giuseppe Di and Harris, Philip and Krupa, Jeffrey and Rankin, Dylan and Valentin, Manuel Blanco and Hester, Josiah and Luo, Yingyi and Mamish, John and Orgrenci-Memik, Seda and Aarrestad, Thea and Javed, Hamza and Loncar, Vladimir and Pierini, Maurizio and Pol, Adrian Alan and Summers, Sioni and Duarte, Javier and Hauck, Scott and Hsu, Shih-Chieh and Ngadiuba, Jennifer and Liu, Mia and Hoang, Duc and Kreinar, Edward and Wu, Zhenbin},
 title = {hls4ml: {An} Open-Source Codesign Workflow to Empower Scientific Low-Power Machine Learning Devices},
 year = {2021},
 archiveprefix = {arXiv},
 eprint = {2103.05579},
 primaryclass = {cs.LG},
}

@article{farah2005neuroethics,
 author = {Farah, Martha J.},
 title = {Neuroethics: {The} practical and the philosophical},
 year = {2005},
 journal = {Trends Cogn. Sci.},
 publisher = {Elsevier BV},
 volume = {9},
 number = {1},
 pages = {34--40},
 doi = {10.1016/j.tics.2004.12.001},
 source = {Crossref},
 url = {https://doi.org/10.1016/j.tics.2004.12.001},
 issn = {1364-6613},
 month = jan,
}

@article{farwell2011stuxnet,
 author = {Farwell, James P. and Rohozinski, Rafal},
 title = {Stuxnet and the Future of Cyber War},
 year = {2011},
 journal = {Survival},
 volume = {53},
 number = {1},
 pages = {23--40},
 date-added = {2023-11-22 14:03:31 -0500},
 date-modified = {2023-11-22 14:05:19 -0500},
 doi = {10.1080/00396338.2011.555586},
 source = {Crossref},
 url = {https://doi.org/10.1080/00396338.2011.555586},
 publisher = {Informa UK Limited},
 issn = {0039-6338, 1468-2699},
 month = jan,
}

@inproceedings{fowers2018configurable,
 author = {Fowers, Jeremy and Ovtcharov, Kalin and Papamichael, Michael and Massengill, Todd and Liu, Ming and Lo, Daniel and Alkalay, Shlomi and Haselman, Michael and Adams, Logan and Ghandi, Mahdi and Heil, Stephen and Patel, Prerak and Sapek, Adam and Weisz, Gabriel and Woods, Lisa and Lanka, Sitaram and Reinhardt, Steven K. and Caulfield, Adrian M. and Chung, Eric S. and Burger, Doug},
 title = {A Configurable Cloud-Scale {DNN} Processor for Real-Time {AI}},
 year = {2018},
 booktitle = {2018 ACM/IEEE 45th Annual International Symposium on Computer Architecture (ISCA)},
 pages = {1--14},
 organization = {IEEE},
 doi = {10.1109/isca.2018.00012},
 source = {Crossref},
 url = {https://doi.org/10.1109/isca.2018.00012},
 publisher = {IEEE},
 month = jun,
}

@misc{frankle_lottery_2019,
 author = {Frankle, Jonathan and Carbin, Michael},
 title = {The Lottery Ticket Hypothesis: {Finding} Sparse, Trainable Neural Networks},
 shorttitle = {The Lottery Ticket Hypothesis},
 year = {2019},
 month = mar,
 publisher = {arXiv},
 doi = {10.48550/arXiv.1803.03635},
 url = {http://arxiv.org/abs/1803.03635},
 urldate = {2023-10-20},
 note = {arXiv:1803.03635 [cs]},
 abstract = {Neural network pruning techniques can reduce the parameter counts of trained networks by over 90\%, decreasing storage requirements and improving computational performance of inference without compromising accuracy. However, contemporary experience is that the sparse architectures produced by pruning are difficult to train from the start, which would similarly improve training performance. We find that a standard pruning technique naturally uncovers subnetworks whose initializations made them capable of training effectively. Based on these results, we articulate the {\textquotedblright}lottery ticket hypothesis:{\textquotedblright} dense, randomly-initialized, feed-forward networks contain subnetworks ({\textquotedblright}winning tickets{\textquotedblright}) that - when trained in isolation - reach test accuracy comparable to the original network in a similar number of iterations. The winning tickets we find have won the initialization lottery: their connections have initial weights that make training particularly effective. We present an algorithm to identify winning tickets and a series of experiments that support the lottery ticket hypothesis and the importance of these fortuitous initializations. We consistently find winning tickets that are less than 10-20\% of the size of several fully-connected and convolutional feed-forward architectures for MNIST and CIFAR10. Above this size, the winning tickets that we find learn faster than the original network and reach higher test accuracy.},
 file = {arXiv Fulltext PDF:/Users/jeffreyma/Zotero/storage/6STHYGW5/Frankle and Carbin - 2019 - The Lottery Ticket Hypothesis Finding Sparse, Tra.pdf:application/pdf;arXiv.org Snapshot:/Users/jeffreyma/Zotero/storage/QGNSCTQB/1803.html:text/html},
 keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
 bdsk-url-1 = {http://arxiv.org/abs/1803.03635},
 bdsk-url-2 = {https://doi.org/10.48550/arXiv.1803.03635},
}

@article{friedman1996value,
 author = {Friedman, Batya},
 title = {Value-sensitive design},
 year = {1996},
 journal = {Interactions},
 publisher = {Association for Computing Machinery (ACM)},
 volume = {3},
 number = {6},
 pages = {16--23},
 doi = {10.1145/242485.242493},
 source = {Crossref},
 url = {https://doi.org/10.1145/242485.242493},
 issn = {1072-5520, 1558-3449},
 month = dec,
}

@article{furber2016large,
 author = {Furber, Steve},
 title = {Large-scale neuromorphic computing systems},
 year = {2016},
 journal = {J. Neural Eng.},
 publisher = {IOP Publishing},
 volume = {13},
 number = {5},
 pages = {051001},
 doi = {10.1088/1741-2560/13/5/051001},
 source = {Crossref},
 url = {https://doi.org/10.1088/1741-2560/13/5/051001},
 issn = {1741-2560, 1741-2552},
 month = aug,
}

@article{gaitathome,
 author = {Liu, Yingcheng and Zhang, Guo and Tarolli, Christopher G. and Hristov, Rumen and Jensen-Roberts, Stella and Waddell, Emma M. and Myers, Taylor L. and Pawlik, Meghan E. and Soto, Julia M. and Wilson, Renee M. and Yang, Yuzhe and Nordahl, Timothy and Lizarraga, Karlo J. and Adams, Jamie L. and Schneider, Ruth B. and Kieburtz, Karl and Ellis, Terry and Dorsey, E. Ray and Katabi, Dina},
 title = {Monitoring gait at home with radio waves in Parkinson{\textquoteright}s disease: {A} marker of severity, progression, and medication response},
 year = {2022},
 journal = {Sci. Transl. Med.},
 volume = {14},
 number = {663},
 pages = {eadc9669},
 doi = {10.1126/scitranslmed.adc9669},
 url = {https://doi.org/10.1126/scitranslmed.adc9669},
 eprint = {https://www.science.org/doi/pdf/10.1126/scitranslmed.adc9669},
 bdsk-url-1 = {https://www.science.org/doi/abs/10.1126/scitranslmed.adc9669},
 bdsk-url-2 = {https://doi.org/10.1126/scitranslmed.adc9669},
 source = {Crossref},
 publisher = {American Association for the Advancement of Science (AAAS)},
 issn = {1946-6234, 1946-6242},
 month = sep,
}

@article{gale2019state,
 author = {Gale, Trevor and Elsen, Erich and Hooker, Sara},
 title = {The state of sparsity in deep neural networks},
 year = {2019},
 journal = {arXiv preprint arXiv:1902.09574},
}

@inproceedings{gandolfi2001electromagnetic,
 author = {Gandolfi, Karine and Mourtel, Christophe and Olivier, Francis},
 title = {Electromagnetic analysis: {Concrete} results},
 year = {2001},
 booktitle = {Cryptographic Hardware and Embedded Systems{\textemdash}CHES 2001: Third International Workshop Paris, France, May 14{\textendash}16, 2001 Proceedings 3},
 pages = {251--261},
 date-added = {2023-11-22 16:56:42 -0500},
 date-modified = {2023-11-22 16:57:40 -0500},
 organization = {Springer},
}

@inproceedings{gannot1994verilog,
 author = {Gannot, G. and Ligthart, M.},
 title = {Verilog {HDL} based {FPGA} design},
 year = {1994},
 booktitle = {International Verilog HDL Conference},
 volume = {},
 number = {},
 pages = {86--92},
 doi = {10.1109/ivc.1994.323743},
 bdsk-url-1 = {https://doi.org/10.1109/IVC.1994.323743},
 source = {Crossref},
 url = {https://doi.org/10.1109/ivc.1994.323743},
 publisher = {IEEE},
}

@article{Gao2020Physical,
 author = {Gao, Yansong and Al-Sarawi, Said F. and Abbott, Derek},
 title = {Physical unclonable functions},
 year = {2020},
 month = feb,
 journal = {Nature Electronics},
 volume = {3},
 number = {2},
 pages = {81--91},
 date-added = {2023-11-22 17:52:20 -0500},
 date-modified = {2023-11-22 17:54:56 -0500},
 doi = {10.1038/s41928-020-0372-5},
 source = {Crossref},
 url = {https://doi.org/10.1038/s41928-020-0372-5},
 publisher = {Springer Science and Business Media LLC},
 issn = {2520-1131},
}

@article{gates2009flexible,
 author = {Gates, Byron D.},
 title = {Flexible Electronics},
 year = {2009},
 journal = {Science},
 publisher = {American Association for the Advancement of Science (AAAS)},
 volume = {323},
 number = {5921},
 pages = {1566--1567},
 doi = {10.1126/science.1171230},
 source = {Crossref},
 url = {https://doi.org/10.1126/science.1171230},
 issn = {0036-8075, 1095-9203},
 month = mar,
}

@article{gaviria2022dollar,
 author = {Mattson, Peter and Reddi, Vijay Janapa and Cheng, Christine and Coleman, Cody and Diamos, Greg and Kanter, David and Micikevicius, Paulius and Patterson, David and Schmuelling, Guenther and Tang, Hanlin and Wei, Gu-Yeon and Wu, Carole-Jean},
 title = {{MLPerf:} {An} Industry Standard Benchmark Suite for Machine Learning Performance},
 year = {2020},
 journal = {IEEE Micro},
 volume = {40},
 pages = {8--16},
 doi = {10.1109/mm.2020.2974843},
 number = {2},
 source = {Crossref},
 url = {https://doi.org/10.1109/mm.2020.2974843},
 publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
 issn = {0272-1732, 1937-4143},
 month = mar,
}

@article{Gebru_Morgenstern_Vecchione_Vaughan_Wallach_III_Crawford_2021,
 author = {Gebru, Timnit and Morgenstern, Jamie and Vecchione, Briana and Vaughan, Jennifer Wortman and Wallach, Hanna and III, Hal Daum\'e and Crawford, Kate},
 title = {Datasheets for datasets},
 year = {2021},
 journal = {Commun. ACM},
 volume = {64},
 number = {12},
 pages = {86--92},
 doi = {10.1145/3458723},
 bdsk-url-1 = {https://doi.org/10.1145/3458723},
 source = {Crossref},
 url = {https://doi.org/10.1145/3458723},
 publisher = {Association for Computing Machinery (ACM)},
 issn = {0001-0782, 1557-7317},
 month = nov,
}

@article{geiger2021causal,
 author = {Geiger, Atticus and Lu, Hanson and Icard, Thomas and Potts, Christopher},
 title = {Causal abstractions of neural networks},
 year = {2021},
 journal = {Adv. Neur. In.},
 volume = {34},
 pages = {9574--9586},
}

@article{glucosemonitor,
 author = {Li, Jingzhen and Tobore, Igbe and Liu, Yuhang and Kandwal, Abhishek and Wang, Lei and Nie, Zedong},
 title = {Non-invasive Monitoring of Three Glucose Ranges Based On {ECG} By Using {DBSCAN}-{CNN}},
 year = {2021},
 journal = {\#IEEE\_J\_BHI\#},
 volume = {25},
 number = {9},
 pages = {3340--3350},
 doi = {10.1109/jbhi.2021.3072628},
 bdsk-url-1 = {https://doi.org/10.1109/JBHI.2021.3072628},
 source = {Crossref},
 url = {https://doi.org/10.1109/jbhi.2021.3072628},
 publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
 issn = {2168-2194, 2168-2208},
 month = sep,
}

@inproceedings{gnad2017voltage,
 author = {Gnad, Dennis R. E. and Oboril, Fabian and Tahoori, Mehdi B.},
 title = {Voltage drop-based fault attacks on {FPGAs} using valid bitstreams},
 year = {2017},
 booktitle = {2017 27th International Conference on Field Programmable Logic and Applications (FPL)},
 pages = {1--7},
 date-added = {2023-11-22 17:07:13 -0500},
 date-modified = {2023-11-22 17:07:59 -0500},
 organization = {IEEE},
 doi = {10.23919/fpl.2017.8056840},
 source = {Crossref},
 url = {https://doi.org/10.23919/fpl.2017.8056840},
 publisher = {IEEE},
 month = sep,
}

@article{goodfellow2020generative,
 author = {Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
 title = {Generative adversarial networks},
 year = {2020},
 journal = {Commun. ACM},
 publisher = {Association for Computing Machinery (ACM)},
 volume = {63},
 number = {11},
 pages = {139--144},
 doi = {10.1145/3422622},
 source = {Crossref},
 url = {https://doi.org/10.1145/3422622},
 issn = {0001-0782, 1557-7317},
 month = oct,
}

@article{goodyear2017social,
 author = {Goodyear, Victoria A.},
 title = {Social media, apps and wearable technologies: {Navigating} ethical dilemmas and procedures},
 year = {2017},
 journal = {Qualitative Research in Sport, Exercise and Health},
 publisher = {Informa UK Limited},
 volume = {9},
 number = {3},
 pages = {285--302},
 doi = {10.1080/2159676x.2017.1303790},
 source = {Crossref},
 url = {https://doi.org/10.1080/2159676x.2017.1303790},
 issn = {2159-676X, 2159-6778},
 month = mar,
}

@misc{Google,
 author = {Google},
 title = {Information quality content moderation},
 url = {https://blog.google/documents/83/},
 bdsk-url-1 = {https://blog.google/documents/83/},
}

@misc{gordon_morphnet_2018,
 author = {Gordon, Ariel and Eban, Elad and Nachum, Ofir and Chen, Bo and Wu, Hao and Yang, Tien-Ju and Choi, Edward},
 title = {{MorphNet:} {Fast} \& Simple Resource-Constrained Structure Learning of Deep Networks},
 shorttitle = {MorphNet},
 year = {2018},
 month = apr,
 booktitle = {Proceedings of the IEEE conference on computer vision and pattern recognition},
 publisher = {arXiv},
 pages = {1586--1595},
 doi = {10.48550/arXiv.1711.06798},
 url = {http://arxiv.org/abs/1711.06798},
 urldate = {2023-10-20},
 note = {arXiv:1711.06798 [cs, stat]},
 abstract = {We present MorphNet, an approach to automate the design of neural network structures. MorphNet iteratively shrinks and expands a network, shrinking via a resource-weighted sparsifying regularizer on activations and expanding via a uniform multiplicative factor on all layers. In contrast to previous approaches, our method is scalable to large networks, adaptable to specific resource constraints (e.g. the number of floating-point operations per inference), and capable of increasing the network's performance. When applied to standard network architectures on a wide variety of datasets, our approach discovers novel structures in each domain, obtaining higher performance while respecting the resource constraint.},
 file = {arXiv Fulltext PDF:/Users/jeffreyma/Zotero/storage/GV7N4CZC/Gordon et al. - 2018 - MorphNet Fast Simple Resource-Constrained Struc.pdf:application/pdf;arXiv.org Snapshot:/Users/jeffreyma/Zotero/storage/K6FUV82F/1711.html:text/html},
 keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
 bdsk-url-1 = {http://arxiv.org/abs/1711.06798},
 bdsk-url-2 = {https://doi.org/10.48550/arXiv.1711.06798},
}

@inproceedings{gordon2018morphnet,
 author = {Gordon, Ariel and Eban, Elad and Nachum, Ofir and Chen, Bo and Wu, Hao and Yang, Tien-Ju and Choi, Edward},
 title = {{MorphNet:} {Fast} \&amp; Simple Resource-Constrained Structure Learning of Deep Networks},
 year = {2018},
 booktitle = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},
 pages = {1586--1595},
 doi = {10.1109/cvpr.2018.00171},
 source = {Crossref},
 url = {https://doi.org/10.1109/cvpr.2018.00171},
 publisher = {IEEE},
 month = jun,
}

@inproceedings{govindavajhala2003using,
 author = {Govindavajhala, S. and Appel, A.W.},
 title = {Using memory errors to attack a virtual machine},
 year = {2003},
 booktitle = {Proceedings 19th International Conference on Data Engineering (Cat. No.03CH37405)},
 pages = {154--156},
 date-added = {2023-11-22 16:46:13 -0500},
 date-modified = {2023-11-22 16:47:03 -0500},
 organization = {IEEE},
 doi = {10.1109/secpri.2003.1199334},
 source = {Crossref},
 url = {https://doi.org/10.1109/secpri.2003.1199334},
 publisher = {IEEE Comput. Soc},
}

@article{green_AI,
 author = {Schwartz, Roy and Dodge, Jesse and Smith, Noah A. and Etzioni, Oren},
 title = {Green {AI}},
 year = {2020},
 journal = {Commun. ACM},
 publisher = {Association for Computing Machinery (ACM)},
 volume = {63},
 number = {12},
 pages = {54--63},
 doi = {10.1145/3381831},
 source = {Crossref},
 url = {https://doi.org/10.1145/3381831},
 issn = {0001-0782, 1557-7317},
 month = nov,
}

@book{grossmanHighTechTrash2007,
 author = {Grossman, Elizabeth},
 title = {High tech trash: {Digital} devices, hidden toxics, and human health},
 year = {2007},
 publisher = {Island press},
}

@article{gruslys2016memory,
 author = {Gruslys, Audrunas and Munos, R\'emi and Danihelka, Ivo and Lanctot, Marc and Graves, Alex},
 title = {Memory-efficient backpropagation through time},
 year = {2016},
 journal = {Adv Neural Inf Process Syst},
 volume = {29},
}

@article{gu2014towards,
 author = {Gu, Shixiang and Rigazio, Luca},
 title = {Towards deep neural network architectures robust to adversarial examples},
 year = {2014},
 journal = {arXiv preprint arXiv:1412.5068},
}

@article{gupta2016monotonic,
 author = {Gupta, Maya and Cotter, Andrew and Pfeifer, Jan and Voevodski, Konstantin and Canini, Kevin and Mangylov, Alexander and Moczydlowski, Wojciech and Van Esbroeck, Alexander},
 title = {Monotonic calibrated interpolated look-up tables},
 year = {2016},
 journal = {The Journal of Machine Learning Research},
 publisher = {JMLR. org},
 volume = {17},
 number = {1},
 pages = {3790--3836},
}

@inproceedings{gupta2022act,
 author = {Gupta, Udit and Elgamal, Mariam and Hills, Gage and Wei, Gu-Yeon and Lee, Hsien-Hsin S. and Brooks, David and Wu, Carole-Jean},
 title = {Act},
 shorttitle = {ACT: designing sustainable computer systems with an architectural carbon modeling tool},
 year = {2022},
 month = jun,
 booktitle = {Proceedings of the 49th Annual International Symposium on Computer Architecture},
 location = {New York, New York},
 publisher = {ACM},
 address = {New York, NY, USA},
 series = {ISCA '22},
 pages = {784--799},
 doi = {10.1145/3470496.3527408},
 isbn = {9781450386104},
 url = {https://doi.org/10.1145/3470496.3527408},
 urldate = {2023-12-06},
 abstract = {Given the performance and efficiency optimizations realized by the computer systems and architecture community over the last decades, the dominating source of computing's carbon footprint is shifting from operational emissions to embodied emissions. These embodied emissions owe to hardware manufacturing and infrastructure-related activities. Despite the rising embodied emissions, there is a distinct lack of architectural modeling tools to quantify and optimize the end-to-end carbon footprint of computing. This work proposes ACT, an architectural carbon footprint modeling framework, to enable carbon characterization and sustainability-driven early design space exploration. Using ACT we demonstrate optimizing hardware for carbon yields distinct solutions compared to optimizing for performance and efficiency. We construct use cases, based on the three tenets of sustainable design{\textemdash}Reduce, Reuse, Recycle{\textemdash}to highlight future methods that enable strong performance and efficiency scaling in an environmentally sustainable manner.},
 numpages = {16},
 keywords = {manufacturing, energy, sustainable computing, mobile, computer architecture},
 language = {en},
 source = {Crossref},
 subtitle = {designing sustainable computer systems with an architectural carbon modeling tool},
}

@article{Gupta2023ChatGPT,
 author = {Gupta, Maanak and Akiri, Charankumar and Aryal, Kshitiz and Parker, Eli and Praharaj, Lopamudra},
 title = {From {ChatGPT} to {ThreatGPT:} {Impact} of Generative {AI} in Cybersecurity and Privacy},
 year = {2023},
 journal = {\#IEEE\_O\_ACC\#},
 volume = {11},
 pages = {80218--80245},
 date-added = {2023-11-22 18:01:41 -0500},
 date-modified = {2023-11-22 18:02:55 -0500},
 doi = {10.1109/access.2023.3300381},
 source = {Crossref},
 url = {https://doi.org/10.1109/access.2023.3300381},
 publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
 issn = {2169-3536},
}

@inproceedings{guptaACTDesigningSustainable2022,
 author = {Gupta, Udit and Elgamal, Mariam and Hills, Gage and Wei, Gu-Yeon and Lee, Hsien-Hsin S. and Brooks, David and Wu, Carole-Jean},
 title = {Act},
 year = {2022},
 booktitle = {Proceedings of the 49th Annual International Symposium on Computer Architecture},
 pages = {784--799},
 doi = {10.1145/3470496.3527408},
 source = {Crossref},
 url = {https://doi.org/10.1145/3470496.3527408},
 publisher = {ACM},
 subtitle = {designing sustainable computer systems with an architectural carbon modeling tool},
 month = jun,
}

@article{gwennap_certus-nx_nodate,
 author = {Gwennap, Linley},
 title = {Certus-{NX} Innovates General-Purpose {FPGAs}},
 language = {en},
}

@article{haensch2018next,
 author = {Haensch, Wilfried and Gokmen, Tayfun and Puri, Ruchir},
 title = {The Next Generation of Deep Learning Hardware: {Analog} Computing},
 year = {2019},
 journal = {Proc. IEEE},
 publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
 volume = {107},
 number = {1},
 pages = {108--122},
 doi = {10.1109/jproc.2018.2871057},
 source = {Crossref},
 url = {https://doi.org/10.1109/jproc.2018.2871057},
 issn = {0018-9219, 1558-2256},
 month = jan,
}

@article{han2015deep,
 author = {Han, Song and Mao, Huizi and Dally, William J},
 title = {Deep compression: {Compressing} deep neural networks with pruning, trained quantization and huffman coding},
 year = {2015},
 journal = {arXiv preprint arXiv:1510.00149},
}

@misc{han2016deep,
 author = {Han, Song and Mao, Huizi and Dally, William J.},
 title = {Deep Compression: {Compressing} Deep Neural Networks with Pruning, Trained Quantization and {Huffman} Coding},
 year = {2016},
 archiveprefix = {arXiv},
 eprint = {1510.00149},
 primaryclass = {cs.CV},
}

@article{handlin1965science,
 author = {Handlin, Oscar},
 title = {Science and technology in popular culture},
 year = {1965},
 journal = {Daedalus-us.},
 publisher = {JSTOR},
 pages = {156--170},
}

@article{hardt2016equality,
 author = {Hardt, Moritz and Price, Eric and Srebro, Nati},
 title = {Equality of opportunity in supervised learning},
 year = {2016},
 journal = {Adv Neural Inf Process Syst},
 volume = {29},
}

@article{hazan2021neuromorphic,
 author = {Hazan, Avi and Ezra Tsur, Elishai},
 title = {Neuromorphic Analog Implementation of Neural Engineering Framework-Inspired Spiking Neuron for High-Dimensional Representation},
 year = {2021},
 journal = {Front. Neurosci.},
 publisher = {Frontiers Media SA},
 volume = {15},
 pages = {627221},
 doi = {10.3389/fnins.2021.627221},
 source = {Crossref},
 url = {https://doi.org/10.3389/fnins.2021.627221},
 issn = {1662-453X},
 month = feb,
}

@misc{he_structured_2023,
 author = {He, Yang and Xiao, Lingao},
 title = {Structured Pruning for Deep Convolutional Neural Networks: {A} survey},
 shorttitle = {Structured Pruning for Deep Convolutional Neural Networks},
 year = {2023},
 month = mar,
 publisher = {arXiv},
 doi = {10.48550/arXiv.2303.00566},
 url = {http://arxiv.org/abs/2303.00566},
 urldate = {2023-10-20},
 note = {arXiv:2303.00566 [cs]},
 abstract = {The remarkable performance of deep Convolutional neural networks (CNNs) is generally attributed to their deeper and wider architectures, which can come with significant computational costs. Pruning neural networks has thus gained interest since it effectively lowers storage and computational costs. In contrast to weight pruning, which results in unstructured models, structured pruning provides the benefit of realistic acceleration by producing models that are friendly to hardware implementation. The special requirements of structured pruning have led to the discovery of numerous new challenges and the development of innovative solutions. This article surveys the recent progress towards structured pruning of deep CNNs. We summarize and compare the state-of-the-art structured pruning techniques with respect to filter ranking methods, regularization methods, dynamic execution, neural architecture search, the lottery ticket hypothesis, and the applications of pruning. While discussing structured pruning algorithms, we briefly introduce the unstructured pruning counterpart to emphasize their differences. Furthermore, we provide insights into potential research opportunities in the field of structured pruning. A curated list of neural network pruning papers can be found at https://github.com/he-y/Awesome-Pruning},
 file = {arXiv Fulltext PDF:/Users/jeffreyma/Zotero/storage/K5RGQQA9/He and Xiao - 2023 - Structured Pruning for Deep Convolutional Neural N.pdf:application/pdf;arXiv.org Snapshot:/Users/jeffreyma/Zotero/storage/U7PVPU4C/2303.html:text/html},
 keywords = {Computer Science - Computer Vision and Pattern Recognition},
 bdsk-url-1 = {http://arxiv.org/abs/2303.00566},
 bdsk-url-2 = {https://doi.org/10.48550/arXiv.2303.00566},
}

@inproceedings{he2016deep,
 author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
 title = {Deep Residual Learning for Image Recognition},
 year = {2016},
 booktitle = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
 pages = {770--778},
 doi = {10.1109/cvpr.2016.90},
 source = {Crossref},
 url = {https://doi.org/10.1109/cvpr.2016.90},
 publisher = {IEEE},
 month = jun,
}

@inproceedings{hebert2018multicalibration,
 author = {H\'ebert-Johnson, Ursula and Kim, Michael and Reingold, Omer and Rothblum, Guy},
 title = {Multicalibration: {Calibration} for the (computationally-identifiable) masses},
 year = {2018},
 booktitle = {International Conference on Machine Learning},
 pages = {1939--1948},
 organization = {PMLR},
}

@article{henderson2020towards,
 author = {Henderson, Peter and Hu, Jieru and Romoff, Joshua and Brunskill, Emma and Jurafsky, Dan and Pineau, Joelle},
 title = {Towards the systematic reporting of the energy and carbon footprints of machine learning},
 year = {2020},
 journal = {The Journal of Machine Learning Research},
 publisher = {JMLRORG},
 volume = {21},
 number = {1},
 pages = {10039--10081},
}

@inproceedings{hendrycks2021natural,
 author = {Hendrycks, Dan and Zhao, Kevin and Basart, Steven and Steinhardt, Jacob and Song, Dawn},
 title = {Natural Adversarial Examples},
 year = {2021},
 booktitle = {2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
 pages = {15262--15271},
 doi = {10.1109/cvpr46437.2021.01501},
 source = {Crossref},
 url = {https://doi.org/10.1109/cvpr46437.2021.01501},
 publisher = {IEEE},
 month = jun,
}

@article{Hennessy2019-je,
 author = {Hennessy, John L. and Patterson, David A.},
 title = {A new golden age for computer architecture},
 year = {2019},
 month = jan,
 journal = {Commun. ACM},
 publisher = {Association for Computing Machinery (ACM)},
 volume = {62},
 number = {2},
 pages = {48--60},
 copyright = {http://www.acm.org/publications/policies/copyright\_policy\#Background},
 abstract = {Innovations like domain-specific hardware, enhanced security, open instruction sets, and agile chip development will lead the way.},
 language = {en},
 doi = {10.1145/3282307},
 source = {Crossref},
 url = {https://doi.org/10.1145/3282307},
 issn = {0001-0782, 1557-7317},
}

@article{himmelstein2022examination,
 author = {Himmelstein, Gracie and Bates, David and Zhou, Li},
 title = {Examination of Stigmatizing Language in the Electronic Health Record},
 year = {2022},
 journal = {JAMA Network Open},
 publisher = {American Medical Association (AMA)},
 volume = {5},
 number = {1},
 pages = {e2144967},
 doi = {10.1001/jamanetworkopen.2021.44967},
 source = {Crossref},
 url = {https://doi.org/10.1001/jamanetworkopen.2021.44967},
 issn = {2574-3805},
 month = jan,
}

@misc{hinton_distilling_2015,
 author = {Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
 title = {Distilling the Knowledge in a Neural Network},
 year = {2015},
 month = mar,
 publisher = {arXiv},
 doi = {10.48550/arXiv.1503.02531},
 url = {http://arxiv.org/abs/1503.02531},
 urldate = {2023-10-20},
 note = {arXiv:1503.02531 [cs, stat]},
 abstract = {A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel.},
 file = {arXiv Fulltext PDF:/Users/jeffreyma/Zotero/storage/VREDW45A/Hinton et al. - 2015 - Distilling the Knowledge in a Neural Network.pdf:application/pdf;arXiv.org Snapshot:/Users/jeffreyma/Zotero/storage/8MNJG4RP/1503.html:text/html},
 keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
 bdsk-url-1 = {http://arxiv.org/abs/1503.02531},
 bdsk-url-2 = {https://doi.org/10.48550/arXiv.1503.02531},
}

@misc{hinton2015distilling,
 author = {Hinton, Geoffrey},
 title = {Van {Nostrand's} Scientific Encyclopedia},
 year = {2005},
 archiveprefix = {arXiv},
 eprint = {1503.02531},
 primaryclass = {stat.ML},
 doi = {10.1002/0471743984.vse0673},
 source = {Crossref},
 url = {https://doi.org/10.1002/0471743984.vse0673},
 publisher = {Wiley},
 isbn = {9780471332305, 9780471743989},
 month = oct,
}

@incollection{Holland_Hosny_Newman_Joseph_Chmielinski_2020,
 author = {Holland, Sarah and Hosny, Ahmed and Newman, Sarah and Joseph, Joshua and Chmielinski, Kasia},
 title = {The Dataset Nutrition Label},
 year = {2020},
 journal = {Data Protection and Privacy},
 doi = {10.5040/9781509932771.ch-001},
 bdsk-url-1 = {https://doi.org/10.5040/9781509932771.ch-001},
 source = {Crossref},
 url = {https://doi.org/10.5040/9781509932771.ch-001},
 booktitle = {Data Protection and Privacy},
 publisher = {Hart Publishing},
 subtitle = {A Framework to Drive Higher Data Quality Standards},
 isbn = {9781509932740, 9781509932764, 9781509932757, 9781509932771},
}

@inproceedings{hong2023publishing,
 author = {Hong, Sanghyun and Carlini, Nicholas and Kurakin, Alexey},
 title = {Publishing Efficient On-device Models Increases Adversarial Vulnerability},
 year = {2023},
 booktitle = {2023 IEEE Conference on Secure and Trustworthy Machine Learning (SaTML)},
 pages = {271--290},
 organization = {IEEE},
 doi = {10.1109/satml54575.2023.00026},
 source = {Crossref},
 url = {https://doi.org/10.1109/satml54575.2023.00026},
 publisher = {IEEE},
 month = feb,
}

@article{hosseini2017deceiving,
 author = {Hosseini, Hossein and Kannan, Sreeram and Zhang, Baosen and Poovendran, Radha},
 title = {Deceiving google's perspective api built for detecting toxic comments},
 year = {2017},
 journal = {arXiv preprint arXiv:1702.08138},
 date-added = {2023-11-22 16:22:18 -0500},
 date-modified = {2023-11-22 16:23:43 -0500},
}

@misc{howard_mobilenets_2017,
 author = {Howard, Andrew G. and Zhu, Menglong and Chen, Bo and Kalenichenko, Dmitry and Wang, Weijun and Weyand, Tobias and Andreetto, Marco and Adam, Hartwig},
 title = {{MobileNets:} {Efficient} Convolutional Neural Networks for Mobile Vision Applications},
 shorttitle = {MobileNets},
 year = {2017},
 month = apr,
 publisher = {arXiv},
 doi = {10.48550/arXiv.1704.04861},
 url = {http://arxiv.org/abs/1704.04861},
 urldate = {2023-10-20},
 note = {arXiv:1704.04861 [cs]},
 abstract = {We present a class of efficient models called MobileNets for mobile and embedded vision applications. MobileNets are based on a streamlined architecture that uses depth-wise separable convolutions to build light weight deep neural networks. We introduce two simple global hyper-parameters that efficiently trade off between latency and accuracy. These hyper-parameters allow the model builder to choose the right sized model for their application based on the constraints of the problem. We present extensive experiments on resource and accuracy tradeoffs and show strong performance compared to other popular models on ImageNet classification. We then demonstrate the effectiveness of MobileNets across a wide range of applications and use cases including object detection, finegrain classification, face attributes and large scale geo-localization.},
 file = {arXiv Fulltext PDF:/Users/jeffreyma/Zotero/storage/IJ9P9ID9/Howard et al. - 2017 - MobileNets Efficient Convolutional Neural Network.pdf:application/pdf;arXiv.org Snapshot:/Users/jeffreyma/Zotero/storage/D9TS95GJ/1704.html:text/html},
 keywords = {Computer Science - Computer Vision and Pattern Recognition},
 bdsk-url-1 = {http://arxiv.org/abs/1704.04861},
 bdsk-url-2 = {https://doi.org/10.48550/arXiv.1704.04861},
}

@misc{howard2017mobilenets,
 author = {Howard, Andrew G. and Zhu, Menglong and Chen, Bo and Kalenichenko, Dmitry and Wang, Weijun and Weyand, Tobias and Andreetto, Marco and Adam, Hartwig},
 title = {{MobileNets:} {Efficient} Convolutional Neural Networks for Mobile Vision Applications},
 year = {2017},
 journal = {arXiv preprint arXiv:1704.04861},
 archiveprefix = {arXiv},
 eprint = {1704.04861},
 primaryclass = {cs.CV},
}

@inproceedings{hsiao2023mavfi,
 author = {Hsiao, Yu-Shun and Wan, Zishen and Jia, Tianyu and Ghosal, Radhika and Mahmoud, Abdulrahman and Raychowdhury, Arijit and Brooks, David and Wei, Gu-Yeon and Reddi, Vijay Janapa},
 title = {{MAVFI:} {An} End-to-End Fault Analysis Framework with Anomaly Detection and Recovery for Micro Aerial Vehicles},
 year = {2023},
 booktitle = {2023 Design, Automation \&amp; Test in Europe Conference \&amp; Exhibition (DATE)},
 pages = {1--6},
 date-added = {2023-11-22 16:54:11 -0500},
 date-modified = {2023-11-22 16:55:12 -0500},
 organization = {IEEE},
 doi = {10.23919/date56975.2023.10137246},
 source = {Crossref},
 url = {https://doi.org/10.23919/date56975.2023.10137246},
 publisher = {IEEE},
 month = apr,
}

@article{hsu2016accumulation,
 author = {Hsu, Liang-Ching and Huang, Ching-Yi and Chuang, Yen-Hsun and Chen, Ho-Wen and Chan, Ya-Ting and Teah, Heng Yi and Chen, Tsan-Yao and Chang, Chiung-Fen and Liu, Yu-Ting and Tzou, Yu-Min},
 title = {Accumulation of heavy metals and trace elements in fluvial sediments received effluents from traditional and semiconductor industries},
 year = {2016},
 journal = {Scientific Reports},
 publisher = {Springer Science and Business Media LLC},
 volume = {6},
 number = {1},
 pages = {34250},
 doi = {10.1038/srep34250},
 source = {Crossref},
 url = {https://doi.org/10.1038/srep34250},
 issn = {2045-2322},
 month = sep,
}

@article{huang2010pseudo,
 author = {Huang, Tsung-Ching and Fukuda, Kenjiro and Lo, Chun-Ming and Yeh, Yung-Hui and Sekitani, Tsuyoshi and Someya, Takao and Cheng, Kwang-Ting},
 title = {Pseudo-{CMOS:} {A} Design Style for Low-Cost and Robust Flexible Electronics},
 year = {2011},
 journal = {IEEE Trans. Electron Devices},
 publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
 volume = {58},
 number = {1},
 pages = {141--150},
 doi = {10.1109/ted.2010.2088127},
 source = {Crossref},
 url = {https://doi.org/10.1109/ted.2010.2088127},
 issn = {0018-9383, 1557-9646},
 month = jan,
}

@inproceedings{hutter2009contact,
 author = {Hutter, Michael and Schmidt, Jorn-Marc and Plos, Thomas},
 title = {Contact-based fault injections and power analysis on {RFID} tags},
 year = {2009},
 booktitle = {2009 European Conference on Circuit Theory and Design},
 pages = {409--412},
 date-added = {2023-11-22 16:43:29 -0500},
 date-modified = {2023-11-22 16:44:30 -0500},
 organization = {IEEE},
 doi = {10.1109/ecctd.2009.5275012},
 source = {Crossref},
 url = {https://doi.org/10.1109/ecctd.2009.5275012},
 publisher = {IEEE},
 month = aug,
}

@misc{iandola_squeezenet_2016,
 author = {Iandola, Forrest N. and Han, Song and Moskewicz, Matthew W. and Ashraf, Khalid and Dally, William J. and Keutzer, Kurt},
 title = {{SqueezeNet:} {Alexnet-level} accuracy with 50x fewer parameters and {0.5MB} model size},
 shorttitle = {SqueezeNet},
 year = {2016},
 month = nov,
 publisher = {arXiv},
 doi = {10.48550/arXiv.1602.07360},
 url = {http://arxiv.org/abs/1602.07360},
 urldate = {2023-10-20},
 note = {arXiv:1602.07360 [cs]},
 abstract = {Recent research on deep neural networks has focused primarily on improving accuracy. For a given accuracy level, it is typically possible to identify multiple DNN architectures that achieve that accuracy level. With equivalent accuracy, smaller DNN architectures offer at least three advantages: (1) Smaller DNNs require less communication across servers during distributed training. (2) Smaller DNNs require less bandwidth to export a new model from the cloud to an autonomous car. (3) Smaller DNNs are more feasible to deploy on FPGAs and other hardware with limited memory. To provide all of these advantages, we propose a small DNN architecture called SqueezeNet. SqueezeNet achieves AlexNet-level accuracy on ImageNet with 50x fewer parameters. Additionally, with model compression techniques we are able to compress SqueezeNet to less than 0.5MB (510x smaller than AlexNet). The SqueezeNet architecture is available for download here: https://github.com/DeepScale/SqueezeNet},
 file = {arXiv Fulltext PDF:/Users/jeffreyma/Zotero/storage/X3ZX9UTZ/Iandola et al. - 2016 - SqueezeNet AlexNet-level accuracy with 50x fewer .pdf:application/pdf;arXiv.org Snapshot:/Users/jeffreyma/Zotero/storage/DHI96QVT/1602.html:text/html},
 keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
 bdsk-url-1 = {http://arxiv.org/abs/1602.07360},
 bdsk-url-2 = {https://doi.org/10.48550/arXiv.1602.07360},
}

@article{iandola2016squeezenet,
 author = {Iandola, Forrest N and Han, Song and Moskewicz, Matthew W and Ashraf, Khalid and Dally, William J and Keutzer, Kurt},
 title = {{SqueezeNet:} {Alexnet-level} accuracy with 50x fewer parameters and 0.5 {MB} model size},
 year = {2016},
 journal = {arXiv preprint arXiv:1602.07360},
}

@article{Ignatov2018-kh,
 author = {Ignatov, Andrey and Timofte, Radu and Chou, William and Wang, Ke and Wu, Max and Hartley, Tim and Van Gool, Luc},
 title = {{AI} Benchmark: {Running} deep neural networks on Android smartphones},
 year = {2018},
 booktitle = {Proceedings of the European Conference on Computer Vision (ECCV) Workshops},
 publisher = {arXiv},
 pages = {0--0},
 abstract = {Over the last years, the computational power of mobile devices such as smartphones and tablets has grown dramatically, reaching the level of desktop computers available not long ago. While standard smartphone apps are no longer a problem for them, there is still a group of tasks that can easily challenge even high-end devices, namely running artificial intelligence algorithms. In this paper, we present a study of the current state of deep learning in the Android ecosystem and describe available frameworks, programming models and the limitations of running AI on smartphones. We give an overview of the hardware acceleration resources available on four main mobile chipset platforms: Qualcomm, HiSilicon, MediaTek and Samsung. Additionally, we present the real-world performance results of different mobile SoCs collected with AI Benchmark that are covering all main existing hardware configurations.},
}

@inproceedings{ignatov2018ai,
 author = {Ignatov, Andrey and Timofte, Radu and Kulik, Andrei and Yang, Seungsoo and Wang, Ke and Baum, Felix and Wu, Max and Xu, Lirong and Van Gool, Luc},
 title = {{AI} Benchmark: {All} About Deep Learning on Smartphones in 2019},
 year = {2019},
 booktitle = {2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW)},
 pages = {0--0},
 doi = {10.1109/iccvw.2019.00447},
 source = {Crossref},
 url = {https://doi.org/10.1109/iccvw.2019.00447},
 publisher = {IEEE},
 month = oct,
}

@inproceedings{ijcai2021p592,
 author = {Benmeziane, Hadjer and El Maghraoui, Kaoutar and Ouarnoughi, Hamza and Niar, Smail and Wistuba, Martin and Wang, Naigang},
 editor = {Zhou, Zhi-Hua},
 title = {Hardware-Aware Neural Architecture Search: {Survey} and Taxonomy},
 year = {2021},
 month = aug,
 booktitle = {Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence},
 publisher = {International Joint Conferences on Artificial Intelligence Organization},
 pages = {4322--4329},
 doi = {10.24963/ijcai.2021/592},
 url = {https://doi.org/10.24963/ijcai.2021/592},
 note = {Survey Track},
 bdsk-url-1 = {https://doi.org/10.24963/ijcai.2021/592},
 source = {Crossref},
}

@inproceedings{imani2016resistive,
 author = {Imani, Mohsen and Rahimi, Abbas and S. Rosing, Tajana},
 title = {Resistive Configurable Associative Memory for Approximate Computing},
 year = {2016},
 booktitle = {Proceedings of the 2016 Design, Automation \&amp; Test in Europe Conference \&amp; Exhibition (DATE)},
 pages = {1327--1332},
 organization = {IEEE},
 doi = {10.3850/9783981537079_0454},
 source = {Crossref},
 url = {https://doi.org/10.3850/9783981537079_0454},
 publisher = {Research Publishing Services},
}

@misc{intquantfordeepinf,
 author = {Wu and Judd, Zhang and Isaev, Micikevicius},
 title = {Integer Quantization for Deep Learning Inference: {Principles} and Empirical Evaluation)},
 year = {2020},
 doi = {10.48550/arXiv.2004.09602},
 url = {https://arxiv.org/abs/2004.09602},
 urldate = {2020-04-20},
 bdsk-url-1 = {https://arxiv.org/abs/2004.09602},
 bdsk-url-2 = {https://doi.org/10.48550/arXiv.2004.09602},
}

@article{irimia-vladu_green_2014,
 author = {Irimia-Vladu, Mihai},
 title = {{{\textquotedblleft}Green{\textquotedblright}} electronics: {Biodegradable} and biocompatible materials and devices for sustainable future},
 year = {2014},
 journal = {Chem. Soc. Rev.},
 volume = {43},
 number = {2},
 pages = {588--610},
 doi = {10.1039/c3cs60235d},
 url = {https://doi.org/10.1039/c3cs60235d},
 note = {Publisher: The Royal Society of Chemistry},
 abstract = {{\textquotedblleft}Green{\textquotedblright} electronics represents not only a novel scientific term but also an emerging area of research aimed at identifying compounds of natural origin and establishing economically efficient routes for the production of synthetic materials that have applicability in environmentally safe (biodegradable) and/or biocompatible devices. The ultimate goal of this research is to create paths for the production of human- and environmentally friendly electronics in general and the integration of such electronic circuits with living tissue in particular. Researching into the emerging class of {\textquotedblleft}green{\textquotedblright} electronics may help fulfill not only the original promise of organic electronics that is to deliver low-cost and energy efficient materials and devices but also achieve unimaginable functionalities for electronics, for example benign integration into life and environment. This Review will highlight recent research advancements in this emerging group of materials and their integration in unconventional organic electronic devices.},
 source = {Crossref},
 publisher = {Royal Society of Chemistry (RSC)},
 issn = {0306-0012, 1460-4744},
}

@inproceedings{jacob2018quantization,
 author = {Jacob, Benoit and Kligys, Skirmantas and Chen, Bo and Zhu, Menglong and Tang, Matthew and Howard, Andrew and Adam, Hartwig and Kalenichenko, Dmitry},
 title = {Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference},
 year = {2018},
 booktitle = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},
 pages = {2704--2713},
 doi = {10.1109/cvpr.2018.00286},
 source = {Crossref},
 url = {https://doi.org/10.1109/cvpr.2018.00286},
 publisher = {IEEE},
 month = jun,
}

@article{janapa2023edge,
 author = {Janapa Reddi, Vijay and Elium, Alexander and Hymel, Shawn and Tischler, David and Situnayake, Daniel and Ward, Carl and Moreau, Louis and Plunkett, Jenny and Kelcey, Matthew and Baaijens, Mathijs and others},
 title = {Edge Impulse: {An} {MLOps} Platform for Tiny Machine Learning},
 year = {2023},
 journal = {Proceedings of Machine Learning and Systems},
 volume = {5},
}

@book{jha2014rare,
 author = {Jha, A.R.},
 title = {Rare Earth Materials},
 year = {2014},
 publisher = {CRC Press},
 doi = {10.1201/b17045},
 source = {Crossref},
 url = {https://doi.org/10.1201/b17045},
 subtitle = {Properties and Applications},
 isbn = {9780429070808},
 month = jun,
}

@book{jhaRareEarthMaterials2014,
 author = {Jha, A.R.},
 title = {Rare Earth Materials},
 year = {2014},
 publisher = {CRC Press},
 doi = {10.1201/b17045},
 source = {Crossref},
 url = {https://doi.org/10.1201/b17045},
 subtitle = {Properties and Applications},
 isbn = {9780429070808},
 month = jun,
}

@misc{jia_dissecting_2018,
 author = {Jia, Zhe and Maggioni, Marco and Staiger, Benjamin and Scarpazza, Daniele P.},
 title = {Dissecting the {NVIDIA} {Volta} {GPU} Architecture via Microbenchmarking},
 year = {2018},
 month = apr,
 publisher = {arXiv},
 url = {http://arxiv.org/abs/1804.06826},
 urldate = {2023-11-07},
 note = {arXiv:1804.06826 [cs]},
 abstract = {Every year, novel NVIDIA GPU designs are introduced. This rapid architectural and technological progression, coupled with a reluctance by manufacturers to disclose low-level details, makes it difficult for even the most proficient GPU software designers to remain up-to-date with the technological advances at a microarchitectural level. To address this dearth of public, microarchitectural-level information on the novel NVIDIA GPUs, independent researchers have resorted to microbenchmarks-based dissection and discovery. This has led to a prolific line of publications that shed light on instruction encoding, and memory hierarchy's geometry and features at each level. Namely, research that describes the performance and behavior of the Kepler, Maxwell and Pascal architectures. In this technical report, we continue this line of research by presenting the microarchitectural details of the NVIDIA Volta architecture, discovered through microbenchmarks and instruction set disassembly. Additionally, we compare quantitatively our Volta findings against its predecessors, Kepler, Maxwell and Pascal.},
 keywords = {Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Performance},
 bdsk-url-1 = {http://arxiv.org/abs/1804.06826},
}

@inproceedings{jia2014caffe,
 author = {Jia, Yangqing and Shelhamer, Evan and Donahue, Jeff and Karayev, Sergey and Long, Jonathan and Girshick, Ross and Guadarrama, Sergio and Darrell, Trevor},
 title = {Caffe},
 year = {2014},
 booktitle = {Proceedings of the 22nd ACM international conference on Multimedia},
 pages = {675--678},
 doi = {10.1145/2647868.2654889},
 source = {Crossref},
 url = {https://doi.org/10.1145/2647868.2654889},
 publisher = {ACM},
 subtitle = {Convolutional Architecture for Fast Feature Embedding},
 month = nov,
}

@article{jia2019beyond,
 author = {Jia, Zhihao and Zaharia, Matei and Aiken, Alex},
 title = {Beyond Data and Model Parallelism for Deep Neural Networks.},
 year = {2019},
 journal = {Proceedings of Machine Learning and Systems},
 volume = {1},
 pages = {1--13},
}

@article{jia2023life,
 author = {Jia, Zhenge and Li, Dawei and Xu, Xiaowei and Li, Na and Hong, Feng and Ping, Lichuan and Shi, Yiyu},
 title = {Life-threatening ventricular arrhythmia detection challenge in implantable cardioverter{\textendash}defibrillators},
 year = {2023},
 journal = {Nature Machine Intelligence},
 publisher = {Springer Science and Business Media LLC},
 volume = {5},
 number = {5},
 pages = {554--555},
 doi = {10.1038/s42256-023-00659-9},
 source = {Crossref},
 url = {https://doi.org/10.1038/s42256-023-00659-9},
 issn = {2522-5839},
 month = may,
}

@misc{jiang2019accuracy,
 author = {Jiang, Weiwen and Zhang, Xinyi and Sha, Edwin H. -M. and Yang, Lei and Zhuge, Qingfeng and Shi, Yiyu and Hu, Jingtong},
 title = {Accuracy vs. Efficiency: {Achieving} Both through {FPGA}-Implementation Aware Neural Architecture Search},
 year = {2019},
 archiveprefix = {arXiv},
 eprint = {1901.11211},
 primaryclass = {cs.DC},
}

@inproceedings{Johnson-Roberson_Barto_Mehta_Sridhar_Rosaen_Vasudevan_2017,
 author = {Johnson-Roberson, Matthew and Barto, Charles and Mehta, Rounak and Sridhar, Sharath Nittur and Rosaen, Karl and Vasudevan, Ram},
 title = {Driving in the Matrix: {Can} virtual worlds replace human-generated annotations for real world tasks?},
 year = {2017},
 journal = {2017 IEEE International Conference on Robotics and Automation (ICRA)},
 doi = {10.1109/icra.2017.7989092},
 bdsk-url-1 = {https://doi.org/10.1109/icra.2017.7989092},
 source = {Crossref},
 url = {https://doi.org/10.1109/icra.2017.7989092},
 booktitle = {2017 IEEE International Conference on Robotics and Automation (ICRA)},
 publisher = {IEEE},
 month = may,
}

@article{jordan_machine_2015,
 author = {Jordan, M. I. and Mitchell, T. M.},
 title = {Machine learning: {Trends,} perspectives, and prospects},
 shorttitle = {Machine learning},
 year = {2015},
 month = jul,
 journal = {Science},
 volume = {349},
 number = {6245},
 pages = {255--260},
 doi = {10.1126/science.aaa8415},
 issn = {0036-8075, 1095-9203},
 url = {https://doi.org/10.1126/science.aaa8415},
 urldate = {2023-10-25},
 file = {Jordan and Mitchell - 2015 - Machine learning Trends, perspectives, and prospe.pdf:/Users/alex/Zotero/storage/RGU3CQ4Q/Jordan and Mitchell - 2015 - Machine learning Trends, perspectives, and prospe.pdf:application/pdf},
 language = {en},
 bdsk-url-1 = {https://www.science.org/doi/10.1126/science.aaa8415},
 bdsk-url-2 = {https://doi.org/10.1126/science.aaa8415},
 source = {Crossref},
 publisher = {American Association for the Advancement of Science (AAAS)},
}

@inproceedings{jouppi2017datacenter,
 author = {Jouppi, Norman P. and Young, Cliff and Patil, Nishant and Patterson, David and Agrawal, Gaurav and Bajwa, Raminder and Bates, Sarah and Bhatia, Suresh and Boden, Nan and Borchers, Al and Boyle, Rick and Cantin, Pierre-luc and Chao, Clifford and Clark, Chris and Coriell, Jeremy and Daley, Mike and Dau, Matt and Dean, Jeffrey and Gelb, Ben and Ghaemmaghami, Tara Vazir and Gottipati, Rajendra and Gulland, William and Hagmann, Robert and Ho, C. Richard and Hogberg, Doug and Hu, John and Hundt, Robert and Hurt, Dan and Ibarz, Julian and Jaffey, Aaron and Jaworski, Alek and Kaplan, Alexander and Khaitan, Harshit and Killebrew, Daniel and Koch, Andy and Kumar, Naveen and Lacy, Steve and Laudon, James and Law, James and Le, Diemthu and Leary, Chris and Liu, Zhuyuan and Lucke, Kyle and Lundin, Alan and MacKean, Gordon and Maggiore, Adriana and Mahony, Maire and Miller, Kieran and Nagarajan, Rahul and Narayanaswami, Ravi and Ni, Ray and Nix, Kathy and Norrie, Thomas and Omernick, Mark and Penukonda, Narayana and Phelps, Andy and Ross, Jonathan and Ross, Matt and Salek, Amir and Samadiani, Emad and Severn, Chris and Sizikov, Gregory and Snelham, Matthew and Souter, Jed and Steinberg, Dan and Swing, Andy and Tan, Mercedes and Thorson, Gregory and Tian, Bo and Toma, Horia and Tuttle, Erick and Vasudevan, Vijay and Walter, Richard and Wang, Walter and Wilcox, Eric and Yoon, Doe Hyun},
 title = {In-Datacenter Performance Analysis of a Tensor Processing Unit},
 year = {2017},
 booktitle = {Proceedings of the 44th Annual International Symposium on Computer Architecture},
 location = {Toronto, ON, Canada},
 publisher = {ACM},
 address = {New York, NY, USA},
 series = {ISCA '17},
 pages = {1--12},
 doi = {10.1145/3079856.3080246},
 isbn = {9781450348928},
 url = {https://doi.org/10.1145/3079856.3080246},
 abstract = {Many architects believe that major improvements in cost-energy-performance must now come from domain-specific hardware. This paper evaluates a custom ASIC{\textemdash}called a Tensor Processing Unit (TPU) {\textemdash} deployed in datacenters since 2015 that accelerates the inference phase of neural networks (NN). The heart of the TPU is a 65,536 8-bit MAC matrix multiply unit that offers a peak throughput of 92 TeraOps/second (TOPS) and a large (28 MiB) software-managed on-chip memory. The TPU's deterministic execution model is a better match to the 99th-percentile response-time requirement of our NN applications than are the time-varying optimizations of CPUs and GPUs that help average throughput more than guaranteed latency. The lack of such features helps explain why, despite having myriad MACs and a big memory, the TPU is relatively small and low power. We compare the TPU to a server-class Intel Haswell CPU and an Nvidia K80 GPU, which are contemporaries deployed in the same datacenters. Our workload, written in the high-level TensorFlow framework, uses production NN applications (MLPs, CNNs, and LSTMs) that represent 95\% of our datacenters' NN inference demand. Despite low utilization for some applications, the TPU is on average about 15X {\textendash} 30X faster than its contemporary GPU or CPU, with TOPS/Watt about 30X {\textendash} 80X higher. Moreover, using the CPU's GDDR5 memory in the TPU would triple achieved TOPS and raise TOPS/Watt to nearly 70X the GPU and 200X the CPU.},
 keywords = {accelerator, neural network, MLP, TPU, CNN, deep learning, domain-specific architecture, GPU, TensorFlow, DNN, RNN, LSTM},
 numpages = {12},
 bdsk-url-1 = {https://doi.org/10.1145/3079856.3080246},
 source = {Crossref},
 month = jun,
}

@inproceedings{Jouppi2023TPUv4,
 author = {Jouppi, Norm and Kurian, George and Li, Sheng and Ma, Peter and Nagarajan, Rahul and Nai, Lifeng and Patil, Nishant and Subramanian, Suvinay and Swing, Andy and Towles, Brian and Young, Clifford and Zhou, Xiang and Zhou, Zongwei and Patterson, David A},
 title = {{TPU} v4: {An} Optically Reconfigurable Supercomputer for Machine Learning with Hardware Support for Embeddings},
 year = {2023},
 booktitle = {Proceedings of the 50th Annual International Symposium on Computer Architecture},
 location = {Orlando, FL, USA},
 publisher = {ACM},
 address = {New York, NY, USA},
 series = {ISCA '23},
 doi = {10.1145/3579371.3589350},
 isbn = {9798400700958},
 url = {https://doi.org/10.1145/3579371.3589350},
 abstract = {In response to innovations in machine learning (ML) models, production workloads changed radically and rapidly. TPU v4 is the fifth Google domain specific architecture (DSA) and its third supercomputer for such ML models. Optical circuit switches (OCSes) dynamically reconfigure its interconnect topology to improve scale, availability, utilization, modularity, deployment, security, power, and performance; users can pick a twisted 3D torus topology if desired. Much cheaper, lower power, and faster than Infiniband, OCSes and underlying optical components are lt;5\% of system cost and lt;3\% of system power. Each TPU v4 includes SparseCores, dataflow processors that accelerate models that rely on embeddings by 5x{\textendash}7x yet use only 5\% of die area and power. Deployed since 2020, TPU v4 outperforms TPU v3 by 2.1x and improves performance/Watt by 2.7x. The TPU v4 supercomputer is 4x larger at 4096 chips and thus nearly 10x faster overall, which along with OCS flexibility and availability allows a large language model to train at an average of ~60\% of peak FLOPS/second. For similar sized systems, it is ~4.3x{\textendash}4.5x faster than the Graphcore IPU Bow and is 1.2x{\textendash}1.7x faster and uses 1.3x{\textendash}1.9x less power than the Nvidia A100. TPU v4s inside the energy-optimized warehouse scale computers of Google Cloud use ~2{\textendash}6x less energy and produce ~20x less CO2e than contemporary DSAs in typical on-premise data centers.},
 articleno = {82},
 keywords = {warehouse scale computer, embeddings, supercomputer, domain specific architecture, reconfigurable, TPU, large language model, power usage effectiveness, CO2 equivalent emissions, energy, optical interconnect, IPU, machine learning, GPU, carbon emissions},
 numpages = {14},
 bdsk-url-1 = {https://doi.org/10.1145/3579371.3589350},
 source = {Crossref},
 month = jun,
}

@book{joye2012fault,
 author = {Joye, Marc and Tunstall, Michael},
 title = {Fault Analysis in Cryptography},
 year = {2012},
 publisher = {Springer Berlin Heidelberg},
 date-added = {2023-11-22 16:35:24 -0500},
 date-modified = {2023-11-22 16:36:20 -0500},
 doi = {10.1007/978-3-642-29656-7},
 source = {Crossref},
 url = {https://doi.org/10.1007/978-3-642-29656-7},
 issn = {1619-7100},
 isbn = {9783642296550, 9783642296567},
}

@misc{kaiming,
 author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
 title = {Delving Deep into Rectifiers: {Surpassing} Human-Level Performance on {ImageNet} Classification},
 year = {2015},
 booktitle = {Proceedings of the IEEE international conference on computer vision},
 eprint = {1502.01852},
 archiveprefix = {arXiv},
}

@article{kairouz2015secure,
 author = {Kairouz, Peter and Oh, Sewoong and Viswanath, Pramod},
 title = {Secure multi-party differential privacy},
 year = {2015},
 journal = {Adv Neural Inf Process Syst},
 volume = {28},
}

@article{karargyris2023federated,
 author = {Karargyris, Alexandros and Umeton, Renato and Sheller, Micah J and Aristizabal, Alejandro and George, Johnu and Wuest, Anna and Pati, Sarthak and Kassem, Hasan and Zenk, Maximilian and Baid, Ujjwal and others},
 title = {Federated benchmarking of medical artificial intelligence with {MedPerf}},
 year = {2023},
 journal = {Nature Machine Intelligence},
 publisher = {Springer Science and Business Media LLC},
 volume = {5},
 number = {7},
 pages = {799--810},
 doi = {10.1038/s42256-023-00652-2},
 source = {Crossref},
 url = {https://doi.org/10.1038/s42256-023-00652-2},
 issn = {2522-5839},
 month = jul,
}

@inproceedings{kaur2020interpreting,
 author = {Kaur, Harmanpreet and Nori, Harsha and Jenkins, Samuel and Caruana, Rich and Wallach, Hanna and Wortman Vaughan, Jennifer},
 title = {Interpreting Interpretability: {Understanding} Data Scientists' Use of Interpretability Tools for Machine Learning},
 year = {2020},
 booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
 pages = {1--14},
 doi = {10.1145/3313831.3376219},
 source = {Crossref},
 url = {https://doi.org/10.1145/3313831.3376219},
 publisher = {ACM},
 month = apr,
}

@article{khan2021knowledgeadaptation,
 author = {Khan, Mohammad Emtiyaz and Swaroop, Siddharth},
 title = {Knowledge-Adaptation Priors},
 year = {2021},
 journal = {arXiv preprint arXiv:2106.08769},
 date-added = {2023-11-22 19:22:50 -0500},
 date-modified = {2023-11-22 19:23:40 -0500},
}

@article{kiela2021dynabench,
 author = {Kiela, Douwe and Bartolo, Max and Nie, Yixin and Kaushik, Divyansh and Geiger, Atticus and Wu, Zhengxuan and Vidgen, Bertie and Prasad, Grusha and Singh, Amanpreet and Ringshia, Pratik and others},
 title = {Dynabench: {Rethinking} benchmarking in {NLP}},
 year = {2021},
 journal = {arXiv preprint arXiv:2104.14337},
}

@article{kim2018chemical,
 author = {Kim, Sunju and Yoon, Chungsik and Ham, Seunghon and Park, Jihoon and Kwon, Ohun and Park, Donguk and Choi, Sangjun and Kim, Seungwon and Ha, Kwonchul and Kim, Won},
 title = {Chemical use in the semiconductor manufacturing industry},
 year = {2018},
 journal = {Int. J. Occup. Env. Heal.},
 publisher = {Informa UK Limited},
 volume = {24},
 number = {3-4},
 pages = {109--118},
 doi = {10.1080/10773525.2018.1519957},
 source = {Crossref},
 url = {https://doi.org/10.1080/10773525.2018.1519957},
 issn = {1077-3525, 2049-3967},
 month = oct,
}

@inproceedings{kim2018interpretability,
 author = {Kim, Been and Wattenberg, Martin and Gilmer, Justin and Cai, Carrie and Wexler, James and Viegas, Fernanda and others},
 title = {Interpretability beyond feature attribution: {Quantitative} testing with concept activation vectors (tcav)},
 year = {2018},
 booktitle = {International conference on machine learning},
 pages = {2668--2677},
 organization = {PMLR},
}

@article{kimChemicalUseSemiconductor2018,
 author = {Kim, Sunju and Yoon, Chungsik and Ham, Seunghon and Park, Jihoon and Kwon, Ohun and Park, Donguk and Choi, Sangjun and Kim, Seungwon and Ha, Kwonchul and Kim, Won},
 title = {Chemical use in the semiconductor manufacturing industry},
 year = {2018},
 journal = {Int. J. Occup. Env. Heal.},
 publisher = {Informa UK Limited},
 volume = {24},
 number = {3-4},
 pages = {109--118},
 doi = {10.1080/10773525.2018.1519957},
 source = {Crossref},
 url = {https://doi.org/10.1080/10773525.2018.1519957},
 issn = {1077-3525, 2049-3967},
 month = oct,
}

@inproceedings{kocher1996timing,
 author = {Kocher, Paul C},
 title = {Timing attacks on implementations of Diffie-Hellman, {RSA,} {DSS,} and other systems},
 year = {1996},
 booktitle = {Advances in Cryptology{\textemdash}CRYPTO{\textquoteright}96: 16th Annual International Cryptology Conference Santa Barbara, California, USA August 18{\textendash}22, 1996 Proceedings 16},
 pages = {104--113},
 organization = {Springer},
}

@inproceedings{kocher1999differential,
 author = {Kocher, Paul and Jaffe, Joshua and Jun, Benjamin},
 title = {Differential power analysis},
 year = {1999},
 booktitle = {Advances in Cryptology{\textemdash}CRYPTO'99: 19th Annual International Cryptology Conference Santa Barbara, California, USA, August 15{\textendash}19, 1999 Proceedings 19},
 pages = {388--397},
 date-added = {2023-11-22 16:55:28 -0500},
 date-modified = {2023-11-22 16:56:18 -0500},
 organization = {Springer},
}

@article{Kocher2011Intro,
 author = {Kocher, Paul and Jaffe, Joshua and Jun, Benjamin and Rohatgi, Pankaj},
 title = {Introduction to differential power analysis},
 year = {2011},
 month = mar,
 journal = {Journal of Cryptographic Engineering},
 volume = {1},
 number = {1},
 pages = {5--27},
 date-added = {2023-11-22 16:58:42 -0500},
 date-modified = {2023-11-22 17:00:36 -0500},
 doi = {10.1007/s13389-011-0006-y},
 source = {Crossref},
 url = {https://doi.org/10.1007/s13389-011-0006-y},
 publisher = {Springer Science and Business Media LLC},
 issn = {2190-8508, 2190-8516},
}

@inproceedings{Kocher2018spectre,
 author = {Kocher, Paul and Horn, Jann and Fogh, Anders and Genkin, Daniel and Gruss, Daniel and Haas, Werner and Hamburg, Mike and Lipp, Moritz and Mangard, Stefan and Prescher, Thomas and Schwarz, Michael and Yarom, Yuval},
 title = {Spectre Attacks: {Exploiting} Speculative Execution},
 year = {2019},
 booktitle = {2019 IEEE Symposium on Security and Privacy (SP)},
 date-added = {2023-11-22 16:33:35 -0500},
 date-modified = {2023-11-22 16:34:01 -0500},
 doi = {10.1109/sp.2019.00002},
 source = {Crossref},
 url = {https://doi.org/10.1109/sp.2019.00002},
 publisher = {IEEE},
 month = may,
}

@inproceedings{koh2020concept,
 author = {Koh, Pang Wei and Nguyen, Thao and Tang, Yew Siang and Mussmann, Stephen and Pierson, Emma and Kim, Been and Liang, Percy},
 title = {Concept bottleneck models},
 year = {2020},
 booktitle = {International conference on machine learning},
 pages = {5338--5348},
 organization = {PMLR},
}

@inproceedings{koh2021wilds,
 author = {Koh, Pang Wei and Sagawa, Shiori and Marklund, Henrik and Xie, Sang Michael and Zhang, Marvin and Balsubramani, Akshay and Hu, Weihua and Yasunaga, Michihiro and Phillips, Richard Lanas and Gao, Irena and others},
 title = {Wilds: {A} benchmark of in-the-wild distribution shifts},
 year = {2021},
 booktitle = {International Conference on Machine Learning},
 pages = {5637--5664},
 organization = {PMLR},
}

@article{kolda_tensor_2009,
 author = {Kolda, Tamara G. and Bader, Brett W.},
 title = {Tensor Decompositions and Applications},
 year = {2009},
 month = aug,
 journal = {SIAM Rev.},
 volume = {51},
 number = {3},
 pages = {455--500},
 doi = {10.1137/07070111x},
 issn = {0036-1445, 1095-7200},
 url = {https://doi.org/10.1137/07070111x},
 urldate = {2023-10-20},
 abstract = {This survey provides an overview of higher-order tensor decompositions, their applications, and available software. A tensor is a multidimensional or N -way array. Decompositions of higher-order tensors (i.e., N -way arrays with N 3) have applications in psychometrics, chemometrics, signal processing, numerical linear algebra, computer vision, numerical analysis, data mining, neuroscience, graph analysis, and elsewhere. Two particular tensor decompositions can be considered to be higher-order extensions of the matrix singular value decomposition: CANDECOMP/PARAFAC (CP) decomposes a tensor as a sum of rank-one tensors, and the Tucker decomposition is a higher-order form of principal component analysis. There are many other tensor decompositions, including INDSCAL, PARAFAC2, CANDELINC, DEDICOM, and PARATUCK2 as well as nonnegative variants of all of the above. The N-way Toolbox, Tensor Toolbox, and Multilinear Engine are examples of software packages for working with tensors.},
 file = {Kolda and Bader - 2009 - Tensor Decompositions and Applications.pdf:/Users/jeffreyma/Zotero/storage/Q7ZG2267/Kolda and Bader - 2009 - Tensor Decompositions and Applications.pdf:application/pdf},
 language = {en},
 bdsk-url-1 = {http://epubs.siam.org/doi/10.1137/07070111X},
 bdsk-url-2 = {https://doi.org/10.1137/07070111X},
 source = {Crossref},
 publisher = {Society for Industrial \& Applied Mathematics (SIAM)},
}

@article{koshti2011cumulative,
 author = {Koshti, VV},
 title = {Cumulative sum control chart},
 year = {2011},
 journal = {International journal of physics and mathematical sciences},
 volume = {1},
 number = {1},
 pages = {28--32},
}

@misc{krishna2023raman,
 author = {Krishna, Adithya and Nudurupati, Srikanth Rohit and G, Chandana D and Dwivedi, Pritesh and van Schaik, Andr\'e and Mehendale, Mahesh and Thakur, Chetan Singh},
 title = {{RAMAN:} {A} Re-configurable and Sparse {tinyML} Accelerator for Inference on Edge},
 year = {2023},
 archiveprefix = {arXiv},
 eprint = {2306.06493},
 primaryclass = {cs.NE},
}

@article{krishnamoorthi2018quantizing,
 author = {Krishnamoorthi, Raghuraman},
 title = {Quantizing deep convolutional networks for efficient inference: {A} whitepaper},
 year = {2018},
 month = jun,
 journal = {arXiv preprint arXiv:1806.08342},
 publisher = {arXiv},
 doi = {10.48550/arXiv.1806.08342},
 url = {https://arxiv.org/abs/1806.08342},
 urldate = {2018-06-21},
 bdsk-url-1 = {https://arxiv.org/abs/1806.08342},
 bdsk-url-2 = {https://doi.org/10.48550/arXiv.1806.08342},
}

@article{Krishnan_Rajpurkar_Topol_2022,
 author = {Krishnan, Rayan and Rajpurkar, Pranav and Topol, Eric J.},
 title = {Self-supervised learning in medicine and healthcare},
 year = {2022},
 journal = {Nat. Biomed. Eng.},
 volume = {6},
 number = {12},
 pages = {1346--1352},
 doi = {10.1038/s41551-022-00914-1},
 bdsk-url-1 = {https://doi.org/10.1038/s41551-022-00914-1},
 source = {Crossref},
 url = {https://doi.org/10.1038/s41551-022-00914-1},
 publisher = {Springer Science and Business Media LLC},
 issn = {2157-846X},
 month = aug,
}

@inproceedings{krishnan2023archgym,
 author = {Krishnan, Srivatsan and Yazdanbakhsh, Amir and Prakash, Shvetank and Jabbour, Jason and Uchendu, Ikechukwu and Ghosh, Susobhan and Boroujerdian, Behzad and Richins, Daniel and Tripathy, Devashree and Faust, Aleksandra and Janapa Reddi, Vijay},
 title = {{ArchGym:} {An} Open-Source Gymnasium for Machine Learning Assisted Architecture Design},
 year = {2023},
 booktitle = {Proceedings of the 50th Annual International Symposium on Computer Architecture},
 pages = {1--16},
 doi = {10.1145/3579371.3589049},
 source = {Crossref},
 url = {https://doi.org/10.1145/3579371.3589049},
 publisher = {ACM},
 month = jun,
}

@article{krizhevsky2012imagenet,
 author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
 title = {{ImageNet} classification with deep convolutional neural networks},
 year = {2017},
 journal = {Commun. ACM},
 volume = {60},
 doi = {10.1145/3065386},
 number = {6},
 source = {Crossref},
 url = {https://doi.org/10.1145/3065386},
 publisher = {Association for Computing Machinery (ACM)},
 issn = {0001-0782, 1557-7317},
 pages = {84--90},
 month = may,
}

@inproceedings{kung1979systolic,
 author = {Kung, Hsiang Tsung and Leiserson, Charles E},
 title = {Systolic arrays (for {VLSI)}},
 year = {1979},
 booktitle = {Sparse Matrix Proceedings 1978},
 volume = {1},
 pages = {256--282},
 organization = {Society for industrial and applied mathematics Philadelphia, PA, USA},
}

@misc{kung2018packing,
 author = {Kung, H. T. and McDanel, Bradley and Zhang, Sai Qian},
 title = {Packing Sparse Convolutional Neural Networks for Efficient Systolic Array Implementations: {Column} Combining Under Joint Optimization},
 year = {2018},
 archiveprefix = {arXiv},
 eprint = {1811.04770},
 primaryclass = {cs.LG},
}

@incollection{kurkova_survey_2018,
 author = {Tan, Chuanqi and Sun, Fuchun and Kong, Tao and Zhang, Wenchang and Yang, Chao and Liu, Chunfang},
 editor = {K\r{u}rkov\'a, V\v{e}ra and Manolopoulos, Yannis and Hammer, Barbara and Iliadis, Lazaros and Maglogiannis, Ilias},
 title = {A Survey on Deep Transfer Learning},
 year = {2018},
 booktitle = {Artificial Neural Networks and Machine Learning {\textendash} ICANN 2018},
 publisher = {Springer International Publishing},
 address = {Cham},
 volume = {11141},
 pages = {270--279},
 doi = {10.1007/978-3-030-01424-7_27},
 isbn = {9783030014230, 9783030014247},
 url = {https://doi.org/10.1007/978-3-030-01424-7_27},
 urldate = {2023-10-26},
 note = {Series Title: Lecture Notes in Computer Science},
 file = {Tan et al. - 2018 - A Survey on Deep Transfer Learning.pdf:/Users/alex/Zotero/storage/5NZ36SGB/Tan et al. - 2018 - A Survey on Deep Transfer Learning.pdf:application/pdf},
 language = {en},
 bdsk-url-1 = {http://link.springer.com/10.1007/978-3-030-01424-7\_27},
 bdsk-url-2 = {https://doi.org/10.1007/978-3-030-01424-7\_27},
 source = {Crossref},
 issn = {0302-9743, 1611-3349},
}

@inproceedings{kurth2023fourcastnet,
 author = {Kurth, Thorsten and Subramanian, Shashank and Harrington, Peter and Pathak, Jaideep and Mardani, Morteza and Hall, David and Miele, Andrea and Kashinath, Karthik and Anandkumar, Anima},
 title = {{FourCastNet:} {Accelerating} Global High-Resolution Weather Forecasting Using Adaptive {Fourier} Neural Operators},
 year = {2023},
 booktitle = {Proceedings of the Platform for Advanced Scientific Computing Conference},
 pages = {1--11},
 doi = {10.1145/3592979.3593412},
 source = {Crossref},
 url = {https://doi.org/10.1145/3592979.3593412},
 publisher = {ACM},
 month = jun,
}

@misc{kuzmin2022fp8,
 author = {Kuzmin, Andrey and Baalen, Mart Van and Ren, Yuwei and Nagel, Markus and Peters, Jorn and Blankevoort, Tijmen},
 title = {{FP8} Quantization: {The} Power of the Exponent},
 year = {2022},
 archiveprefix = {arXiv},
 eprint = {2208.09225},
 primaryclass = {cs.LG},
}

@misc{kwon_tinytrain_2023,
 author = {Kwon, Young D. and Li, Rui and Venieris, Stylianos I. and Chauhan, Jagmohan and Lane, Nicholas D. and Mascolo, Cecilia},
 title = {Machine and Deep Learning Using {MATLAB}},
 shorttitle = {TinyTrain},
 year = {2023},
 month = oct,
 journal = {arXiv preprint arXiv:2307.09988},
 publisher = {Wiley},
 url = {https://doi.org/10.1002/9781394209118.ch7},
 urldate = {2023-10-26},
 note = {arXiv:2307.09988 [cs]},
 file = {Kwon et al. - 2023 - TinyTrain Deep Neural Network Training at the Ext.pdf:/Users/alex/Zotero/storage/L2ST472U/Kwon et al. - 2023 - TinyTrain Deep Neural Network Training at the Ext.pdf:application/pdf},
 keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
 language = {en},
 bdsk-url-1 = {http://arxiv.org/abs/2307.09988},
 doi = {10.1002/9781394209118.ch7},
 source = {Crossref},
 isbn = {9781394209088, 9781394209118},
 pages = {290--360},
}

@article{kwon2022flexible,
 author = {Kwon, Sun Hwa and Dong, Lin},
 title = {Flexible sensors and machine learning for heart monitoring},
 year = {2022},
 journal = {Nano Energy},
 publisher = {Elsevier BV},
 pages = {107632},
 doi = {10.1016/j.nanoen.2022.107632},
 source = {Crossref},
 url = {https://doi.org/10.1016/j.nanoen.2022.107632},
 volume = {102},
 issn = {2211-2855},
 month = nov,
}

@article{kwon2023tinytrain,
 author = {Kwon, Young D and Li, Rui and Venieris, Stylianos I and Chauhan, Jagmohan and Lane, Nicholas D and Mascolo, Cecilia},
 title = {{TinyTrain:} {Deep} Neural Network Training at the Extreme Edge},
 year = {2023},
 journal = {arXiv preprint arXiv:2307.09988},
}

@misc{Labelbox,
 journal = {Labelbox},
 url = {https://labelbox.com/},
 bdsk-url-1 = {https://labelbox.com/},
}

@article{lai2018cmsis,
 author = {Lai, Liangzhen and Suda, Naveen and Chandra, Vikas},
 title = {Cmsis-nn: {Efficient} neural network kernels for arm cortex-m cpus},
 year = {2018},
 journal = {arXiv preprint arXiv:1801.06601},
}

@misc{lai2018cmsisnn,
 author = {Lai, Liangzhen and Suda, Naveen and Chandra, Vikas},
 title = {{CMSIS}-{NN:} {Efficient} Neural Network Kernels for Arm Cortex-M {CPUs}},
 year = {2018},
 archiveprefix = {arXiv},
 eprint = {1801.06601},
 primaryclass = {cs.NE},
}

@inproceedings{lakkaraju2020fool,
 author = {Lakkaraju, Himabindu and Bastani, Osbert},
 title = {{''How} do I fool you?''},
 year = {2020},
 booktitle = {Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
 pages = {79--85},
 doi = {10.1145/3375627.3375833},
 source = {Crossref},
 url = {https://doi.org/10.1145/3375627.3375833},
 publisher = {ACM},
 subtitle = {Manipulating User Trust via Misleading Black Box Explanations},
 month = feb,
}

@article{lam2023learning,
 author = {Lam, Remi and Sanchez-Gonzalez, Alvaro and Willson, Matthew and Wirnsberger, Peter and Fortunato, Meire and Alet, Ferran and Ravuri, Suman and Ewalds, Timo and Eaton-Rosen, Zach and Hu, Weihua and Merose, Alexander and Hoyer, Stephan and Holland, George and Vinyals, Oriol and Stott, Jacklynn and Pritzel, Alexander and Mohamed, Shakir and Battaglia, Peter},
 title = {Learning skillful medium-range global weather forecasting},
 year = {2023},
 journal = {Science},
 publisher = {American Association for the Advancement of Science (AAAS)},
 pages = {eadi2336},
 doi = {10.1126/science.adi2336},
 source = {Crossref},
 url = {https://doi.org/10.1126/science.adi2336},
 issn = {0036-8075, 1095-9203},
 month = nov,
}

@article{lannelongueGreenAlgorithmsQuantifying2020,
 author = {Lannelongue, Lo{\"\i}c and Grealey, Jason and Inouye, Michael},
 shorttitle = {Green Algorithms},
 year = {2020},
 doi = {10.48550/ARXIV.2007.07610},
 url = {https://arxiv.org/abs/2007.07610},
 urldate = {2023-12-06},
 copyright = {Creative Commons Attribution 4.0 International},
 note = {Publisher: arXiv Version Number: 5},
}

@article{lannelongueGreenAlgorithmsQuantifying2021,
 author = {Lannelongue, Lo{\"\i}c and Grealey, Jason and Inouye, Michael},
 shorttitle = {Green Algorithms},
 year = {2021},
 month = may,
 journal = {Adv. Sci.},
 volume = {8},
 number = {12},
 pages = {2100707},
 doi = {10.1002/advs.202100707},
 issn = {2198-3844, 2198-3844},
 url = {https://doi.org/10.1002/advs.202100707},
 urldate = {2023-12-01},
 language = {en},
 source = {Crossref},
 publisher = {Wiley},
 title = {Green Algorithms: {Quantifying} the Carbon Footprint of Computation},
}

@article{lecocq2022mitigation,
 author = {Winkler, Harald and Lecocq, Franck and Lofgren, Hans and Vilari\~no, Maria Virginia and Kartha, Sivan and Portugal-Pereira, Joana},
 title = {Examples of shifting development pathways: {Lessons} on how to enable broader, deeper, and faster climate action},
 year = {2022},
 publisher = {Springer Science and Business Media LLC},
 doi = {10.1007/s44168-022-00026-1},
 number = {1},
 source = {Crossref},
 url = {https://doi.org/10.1007/s44168-022-00026-1},
 volume = {1},
 journal = {Climate Action},
 issn = {2731-3263},
 month = dec,
}

@inproceedings{lecun_optimal_1989,
 author = {LeCun, Yann and Denker, John and Solla, Sara},
 title = {Optimal Brain Damage},
 year = {1989},
 journal = {Adv Neural Inf Process Syst},
 booktitle = {Advances in Neural Information Processing Systems},
 publisher = {Morgan-Kaufmann},
 volume = {2},
 url = {https://proceedings.neurips.cc/paper/1989/hash/6c9882bbac1c7093bd25041881277658-Abstract.html},
 urldate = {2023-10-20},
 abstract = {We have used information-theoretic ideas to derive a class of prac(cid:173) tical and nearly optimal schemes for adapting the size of a neural network. By removing unimportant weights from a network, sev(cid:173) eral improvements can be expected: better generalization, fewer training examples required, and improved speed of learning and/or classification. The basic idea is to use second-derivative informa(cid:173) tion to make a tradeoff between network complexity and training set error. Experiments confirm the usefulness of the methods on a real-world application.},
 file = {Full Text PDF:/Users/jeffreyma/Zotero/storage/BYHQQSST/LeCun et al. - 1989 - Optimal Brain Damage.pdf:application/pdf},
 bdsk-url-1 = {https://proceedings.neurips.cc/paper/1989/hash/6c9882bbac1c7093bd25041881277658-Abstract.html},
}

@article{lecun1989optimal,
 author = {LeCun, Yann and Denker, John and Solla, Sara},
 title = {Optimal brain damage},
 year = {1989},
 journal = {Adv Neural Inf Process Syst},
 volume = {2},
}

@article{li2014communication,
 author = {Li, Mu and Andersen, David G and Smola, Alexander J and Yu, Kai},
 title = {Communication efficient distributed machine learning with the parameter server},
 year = {2014},
 journal = {Adv. Neur. In.},
 volume = {27},
}

@article{li2016lightrnn,
 author = {Li, Xiang and Qin, Tao and Yang, Jian and Liu, Tie-Yan},
 title = {{LightRNN:} {Memory} and computation-efficient recurrent neural networks},
 year = {2016},
 journal = {Adv. Neur. In.},
 volume = {29},
}

@article{li2017deep,
 author = {Li, Yuxi},
 title = {Deep reinforcement learning: {An} overview},
 year = {2017},
 journal = {arXiv preprint arXiv:1701.07274},
}

@article{li2017learning,
 author = {Li, Zhizhong and Hoiem, Derek},
 title = {Learning without Forgetting},
 year = {2018},
 journal = {IEEE Trans. Pattern Anal. Mach. Intell.},
 publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
 volume = {40},
 number = {12},
 pages = {2935--2947},
 doi = {10.1109/tpami.2017.2773081},
 source = {Crossref},
 url = {https://doi.org/10.1109/tpami.2017.2773081},
 issn = {0162-8828, 2160-9292, 1939-3539},
 month = dec,
}

@article{li2019edge,
 author = {Li, En and Zeng, Liekang and Zhou, Zhi and Chen, Xu},
 title = {Edge {AI:} {On-demand} Accelerating Deep Neural Network Inference via Edge Computing},
 year = {2020},
 journal = {IEEE Trans. Wireless Commun.},
 publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
 volume = {19},
 number = {1},
 pages = {447--457},
 doi = {10.1109/twc.2019.2946140},
 source = {Crossref},
 url = {https://doi.org/10.1109/twc.2019.2946140},
 issn = {1536-1276, 1558-2248},
 month = jan,
}

@inproceedings{Li2020Additive,
 author = {Li, Yuhang and Dong, Xin and Wang, Wei},
 title = {Additive Powers-of-Two Quantization: {An} Efficient Non-uniform Discretization for Neural Networks},
 year = {2020},
 booktitle = {International Conference on Learning Representations},
 url = {https://openreview.net/forum?id=BkgXT24tDS},
 bdsk-url-1 = {https://openreview.net/forum?id=BkgXT24tDS},
}

@article{Li2020Federated,
 author = {Li, Tian and Sahu, Anit Kumar and Talwalkar, Ameet and Smith, Virginia},
 title = {Federated Learning: {Challenges,} Methods, and Future Directions},
 year = {2020},
 journal = {IEEE Signal Process Mag.},
 volume = {37},
 number = {3},
 pages = {50--60},
 date-added = {2023-11-22 19:15:13 -0500},
 date-modified = {2023-11-22 19:17:19 -0500},
 doi = {10.1109/msp.2020.2975749},
 source = {Crossref},
 url = {https://doi.org/10.1109/msp.2020.2975749},
 publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
 issn = {1053-5888, 1558-0792},
 month = may,
}

@misc{liao_can_2023,
 author = {Liao, Zhu and Qu\'etu, Victor and Nguyen, Van-Tam and Tartaglione, Enzo},
 title = {Can Unstructured Pruning Reduce the Depth in Deep Neural Networks?},
 year = {2023},
 month = aug,
 publisher = {arXiv},
 doi = {10.48550/arXiv.2308.06619},
 url = {http://arxiv.org/abs/2308.06619},
 urldate = {2023-10-20},
 note = {arXiv:2308.06619 [cs]},
 abstract = {Pruning is a widely used technique for reducing the size of deep neural networks while maintaining their performance. However, such a technique, despite being able to massively compress deep models, is hardly able to remove entire layers from a model (even when structured): is this an addressable task? In this study, we introduce EGP, an innovative Entropy Guided Pruning algorithm aimed at reducing the size of deep neural networks while preserving their performance. The key focus of EGP is to prioritize pruning connections in layers with low entropy, ultimately leading to their complete removal. Through extensive experiments conducted on popular models like ResNet-18 and Swin-T, our findings demonstrate that EGP effectively compresses deep neural networks while maintaining competitive performance levels. Our results not only shed light on the underlying mechanism behind the advantages of unstructured pruning, but also pave the way for further investigations into the intricate relationship between entropy, pruning techniques, and deep learning performance. The EGP algorithm and its insights hold great promise for advancing the field of network compression and optimization. The source code for EGP is released open-source.},
 file = {arXiv Fulltext PDF:/Users/jeffreyma/Zotero/storage/V6P3XB5H/Liao et al. - 2023 - Can Unstructured Pruning Reduce the Depth in Deep .pdf:application/pdf;arXiv.org Snapshot:/Users/jeffreyma/Zotero/storage/WSQ4ZUH4/2308.html:text/html},
 keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
 bdsk-url-1 = {http://arxiv.org/abs/2308.06619},
 bdsk-url-2 = {https://doi.org/10.48550/arXiv.2308.06619},
}

@misc{ligozat2022unraveling,
 author = {Ligozat, Anne-Laure and Lef\`evre, Julien and Bugeau, Aur\'elie and Combaz, Jacques},
 title = {Unraveling the Hidden Environmental Impacts of {AI} Solutions for Environment},
 year = {2022},
 eprint = {2110.11822},
 archiveprefix = {arXiv},
 primaryclass = {cs.AI},
}

@misc{lin_-device_2022,
 author = {Lin, Ji and Zhu, Ligeng and Chen, Wei-Ming and Wang, Wei-Chen and Gan, Chuang and Han, Song},
 title = {On-Device Training Under {256KB} Memory},
 year = {2022},
 month = nov,
 journal = {Adv. Neur. In.},
 booktitle = {ArXiv},
 publisher = {arXiv},
 volume = {35},
 pages = {22941--22954},
 url = {http://arxiv.org/abs/2206.15472},
 urldate = {2023-10-26},
 note = {arXiv:2206.15472 [cs]},
 annote = {Comment: NeurIPS 2022},
 file = {Lin et al. - 2022 - On-Device Training Under 256KB Memory.pdf:/Users/alex/Zotero/storage/GMF6SWGT/Lin et al. - 2022 - On-Device Training Under 256KB Memory.pdf:application/pdf},
 keywords = {Computer Science - Computer Vision and Pattern Recognition},
 language = {en},
 bdsk-url-1 = {http://arxiv.org/abs/2206.15472},
}

@misc{lin_mcunet_2020,
 author = {Lin, Ji and Chen, Wei-Ming and Lin, Yujun and Cohn, John and Gan, Chuang and Han, Song},
 title = {{MCUNet:} {Tiny} Deep Learning on {IoT} Devices},
 shorttitle = {MCUNet},
 year = {2020},
 month = nov,
 journal = {Adv. Neur. In.},
 publisher = {arXiv},
 volume = {33},
 pages = {11711--11722},
 doi = {10.48550/arXiv.2007.10319},
 url = {http://arxiv.org/abs/2007.10319},
 urldate = {2023-10-20},
 note = {arXiv:2007.10319 [cs]},
 abstract = {Machine learning on tiny IoT devices based on microcontroller units (MCU) is appealing but challenging: the memory of microcontrollers is 2-3 orders of magnitude smaller even than mobile phones. We propose MCUNet, a framework that jointly designs the efficient neural architecture (TinyNAS) and the lightweight inference engine (TinyEngine), enabling ImageNet-scale inference on microcontrollers. TinyNAS adopts a two-stage neural architecture search approach that first optimizes the search space to fit the resource constraints, then specializes the network architecture in the optimized search space. TinyNAS can automatically handle diverse constraints (i.e.device, latency, energy, memory) under low search costs.TinyNAS is co-designed with TinyEngine, a memory-efficient inference library to expand the search space and fit a larger model. TinyEngine adapts the memory scheduling according to the overall network topology rather than layer-wise optimization, reducing the memory usage by 4.8x, and accelerating the inference by 1.7-3.3x compared to TF-Lite Micro and CMSIS-NN. MCUNet is the first to achieves 70\% ImageNet top1 accuracy on an off-the-shelf commercial microcontroller, using 3.5x less SRAM and 5.7x less Flash compared to quantized MobileNetV2 and ResNet-18. On visual\&audio wake words tasks, MCUNet achieves state-of-the-art accuracy and runs 2.4-3.4x faster than MobileNetV2 and ProxylessNAS-based solutions with 3.7-4.1x smaller peak SRAM. Our study suggests that the era of always-on tiny machine learning on IoT devices has arrived. Code and models can be found here: https://tinyml.mit.edu.},
 annote = {Comment: NeurIPS 2020 (spotlight)},
 file = {arXiv Fulltext PDF:/Users/jeffreyma/Zotero/storage/IX2JN4P9/Lin et al. - 2020 - MCUNet Tiny Deep Learning on IoT Devices.pdf:application/pdf;arXiv.org Snapshot:/Users/jeffreyma/Zotero/storage/BAKHZ46Y/2007.html:text/html},
 keywords = {Computer Science - Computer Vision and Pattern Recognition},
 language = {en},
 bdsk-url-1 = {http://arxiv.org/abs/2007.10319},
 bdsk-url-2 = {https://doi.org/10.48550/arXiv.2007.10319},
 archiveprefix = {arXiv},
 eprint = {2007.10319},
 primaryclass = {cs.CV},
}

@inproceedings{lin2014microsoft,
 author = {Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Hays, James and Perona, Pietro and Ramanan, Deva and Doll\'ar, Piotr and Zitnick, C Lawrence},
 title = {Microsoft coco: {Common} objects in context},
 year = {2014},
 booktitle = {Computer Vision{\textendash}ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13},
 pages = {740--755},
 organization = {Springer},
}

@article{lin2020mcunet,
 author = {Lin, Ji and Chen, Wei-Ming and Lin, Yujun and Gan, Chuang and Han, Song and others},
 title = {Mcunet: {Tiny} deep learning on iot devices},
 year = {2020},
 journal = {Adv. Neur. In.},
 volume = {33},
 pages = {11711--11722},
 eprint = {2007.10319},
 archiveprefix = {arXiv},
 primaryclass = {cs.CV},
}

@article{lin2022device,
 author = {Lin, Ji and Zhu, Ligeng and Chen, Wei-Ming and Wang, Wei-Chen and Gan, Chuang and Han, Song},
 title = {On-device training under 256kb memory},
 year = {2022},
 journal = {Adv. Neur. In.},
 volume = {35},
 pages = {22941--22954},
}

@inproceedings{lin2022ondevice,
 author = {Zhu, Ligeng and Hu, Lanxiang and Lin, Ji and Chen, Wei-Ming and Wang, Wei-Chen and Gan, Chuang and Han, Song},
 title = {{PockEngine:} {Sparse} and Efficient Fine-tuning in a Pocket},
 year = {2023},
 booktitle = {56th Annual IEEE/ACM International Symposium on Microarchitecture},
 doi = {10.1145/3613424.3614307},
 source = {Crossref},
 url = {https://doi.org/10.1145/3613424.3614307},
 publisher = {ACM},
 month = oct,
}

@article{lin2023awq,
 author = {Lin, Ji and Tang, Jiaming and Tang, Haotian and Yang, Shang and Dang, Xingyu and Han, Song},
 title = {{AWQ:} {Activation-aware} Weight Quantization for {LLM} Compression and Acceleration},
 year = {2023},
 journal = {arXiv},
}

@book{lindgren2023handbook,
 author = {Lindgren, Simon},
 title = {Handbook of Critical Studies of Artificial Intelligence},
 year = {2023},
 publisher = {Edward Elgar Publishing},
}

@article{lindholm_nvidia_2008,
 author = {Lindholm, Erik and Nickolls, John and Oberman, Stuart and Montrym, John},
 title = {{NVIDIA} Tesla: {A} Unified Graphics and Computing Architecture},
 shorttitle = {NVIDIA Tesla},
 year = {2008},
 month = mar,
 journal = {IEEE Micro},
 volume = {28},
 number = {2},
 pages = {39--55},
 doi = {10.1109/mm.2008.31},
 issn = {0272-1732},
 url = {https://doi.org/10.1109/mm.2008.31},
 urldate = {2023-11-07},
 note = {Conference Name: IEEE Micro},
 abstract = {To enable flexible, programmable graphics and high-performance computing, NVIDIA has developed the Tesla scalable unified graphics and parallel computing architecture. Its scalable parallel array of processors is massively multithreaded and programmable in C or via graphics APIs.},
 bdsk-url-1 = {https://ieeexplore.ieee.org/document/4523358},
 bdsk-url-2 = {https://doi.org/10.1109/MM.2008.31},
 source = {Crossref},
 publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
}

@inproceedings{Lipp2018meltdown,
 author = {Kocher, Paul and Horn, Jann and Fogh, Anders and Genkin, Daniel and Gruss, Daniel and Haas, Werner and Hamburg, Mike and Lipp, Moritz and Mangard, Stefan and Prescher, Thomas and Schwarz, Michael and Yarom, Yuval},
 title = {Spectre Attacks: {Exploiting} Speculative Execution},
 year = {2019},
 booktitle = {2019 IEEE Symposium on Security and Privacy (SP)},
 date-added = {2023-11-22 16:32:26 -0500},
 date-modified = {2023-11-22 16:33:08 -0500},
 doi = {10.1109/sp.2019.00002},
 source = {Crossref},
 url = {https://doi.org/10.1109/sp.2019.00002},
 publisher = {IEEE},
 month = may,
}

@article{loh20083d,
 author = {Loh, Gabriel H.},
 title = {{3D}-Stacked Memory Architectures for Multi-core Processors},
 year = {2008},
 journal = {ACM SIGARCH Computer Architecture News},
 publisher = {Association for Computing Machinery (ACM)},
 volume = {36},
 number = {3},
 pages = {453--464},
 doi = {10.1145/1394608.1382159},
 source = {Crossref},
 url = {https://doi.org/10.1145/1394608.1382159},
 issn = {0163-5964},
 month = jun,
}

@inproceedings{long2020ai,
 author = {Long, Duri and Magerko, Brian},
 title = {What is {AI} Literacy? Competencies and Design Considerations},
 year = {2020},
 booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
 pages = {1--16},
 doi = {10.1145/3313831.3376727},
 source = {Crossref},
 url = {https://doi.org/10.1145/3313831.3376727},
 publisher = {ACM},
 month = apr,
}

@inproceedings{lou2013accurate,
 author = {Lou, Yin and Caruana, Rich and Gehrke, Johannes and Hooker, Giles},
 title = {Accurate intelligible models with pairwise interactions},
 year = {2013},
 booktitle = {Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining},
 pages = {623--631},
 doi = {10.1145/2487575.2487579},
 source = {Crossref},
 url = {https://doi.org/10.1145/2487575.2487579},
 publisher = {ACM},
 month = aug,
}

@article{lowy2021fermi,
 author = {Lowy, Andrew and Pavan, Rakesh and Baharlouei, Sina and Razaviyayn, Meisam and Beirami, Ahmad},
 title = {Fermi: {Fair} empirical risk minimization via exponential R\'enyi mutual information},
 year = {2021},
}

@misc{lu_notes_2016,
 author = {Lu, Yuan and Yang, Jie},
 title = {Notes on Low-rank Matrix Factorization},
 year = {2016},
 month = may,
 publisher = {arXiv},
 doi = {10.48550/arXiv.1507.00333},
 url = {http://arxiv.org/abs/1507.00333},
 urldate = {2023-10-20},
 note = {arXiv:1507.00333 [cs]},
 abstract = {Low-rank matrix factorization (MF) is an important technique in data science. The key idea of MF is that there exists latent structures in the data, by uncovering which we could obtain a compressed representation of the data. By factorizing an original matrix to low-rank matrices, MF provides a unified method for dimension reduction, clustering, and matrix completion. In this article we review several important variants of MF, including: Basic MF, Non-negative MF, Orthogonal non-negative MF. As can be told from their names, non-negative MF and orthogonal non-negative MF are variants of basic MF with non-negativity and/or orthogonality constraints. Such constraints are useful in specific senarios. In the first part of this article, we introduce, for each of these models, the application scenarios, the distinctive properties, and the optimizing method. By properly adapting MF, we can go beyond the problem of clustering and matrix completion. In the second part of this article, we will extend MF to sparse matrix compeletion, enhance matrix compeletion using various regularization methods, and make use of MF for (semi-)supervised learning by introducing latent space reinforcement and transformation. We will see that MF is not only a useful model but also as a flexible framework that is applicable for various prediction problems.},
 file = {arXiv Fulltext PDF:/Users/jeffreyma/Zotero/storage/4QED5ZU9/Lu and Yang - 2016 - Notes on Low-rank Matrix Factorization.pdf:application/pdf;arXiv.org Snapshot:/Users/jeffreyma/Zotero/storage/XIBZBDJQ/1507.html:text/html},
 keywords = {Computer Science - Information Retrieval, Computer Science - Machine Learning, Mathematics - Numerical Analysis},
 bdsk-url-1 = {http://arxiv.org/abs/1507.00333},
 bdsk-url-2 = {https://doi.org/10.48550/arXiv.1507.00333},
}

@inproceedings{luebke2008cuda,
 author = {Luebke, David},
 title = {{CUDA:} {Scalable} parallel programming for high-performance scientific computing},
 year = {2008},
 booktitle = {2008 5th IEEE International Symposium on Biomedical Imaging: From Nano to Macro},
 volume = {},
 number = {},
 pages = {836--838},
 doi = {10.1109/isbi.2008.4541126},
 bdsk-url-1 = {https://doi.org/10.1109/ISBI.2008.4541126},
 source = {Crossref},
 url = {https://doi.org/10.1109/isbi.2008.4541126},
 publisher = {IEEE},
 month = may,
}

@article{lundberg2017unified,
 author = {Lundberg, Scott M and Lee, Su-In},
 title = {A unified approach to interpreting model predictions},
 year = {2017},
 journal = {Adv Neural Inf Process Syst},
 volume = {30},
}

@article{maass1997networks,
 author = {Maass, Wolfgang},
 title = {Networks of spiking neurons: {The} third generation of neural network models},
 year = {1997},
 journal = {Neural Networks},
 publisher = {Elsevier BV},
 volume = {10},
 number = {9},
 pages = {1659--1671},
 doi = {10.1016/s0893-6080(97)00011-7},
 source = {Crossref},
 url = {https://doi.org/10.1016/s0893-6080(97)00011-7},
 issn = {0893-6080},
 month = dec,
}

@article{MAL-083,
 author = {Kairouz, Peter and McMahan, H. Brendan and Avent, Brendan and Bellet, Aur\'elien and Bennis, Mehdi and Nitin Bhagoji, Arjun and Bonawitz, Kallista and Charles, Zachary and Cormode, Graham and Cummings, Rachel and D{\textquoteright}Oliveira, Rafael G. L. and Eichner, Hubert and El Rouayheb, Salim and Evans, David and Gardner, Josh and Garrett, Zachary and Gasc\'on, Adri\`a and Ghazi, Badih and Gibbons, Phillip B. and Gruteser, Marco and Harchaoui, Zaid and He, Chaoyang and He, Lie and Huo, Zhouyuan and Hutchinson, Ben and Hsu, Justin and Jaggi, Martin and Javidi, Tara and Joshi, Gauri and Khodak, Mikhail and Konecn\'y, Jakub and Korolova, Aleksandra and Koushanfar, Farinaz and Koyejo, Sanmi and Lepoint, Tancr\`ede and Liu, Yang and Mittal, Prateek and Mohri, Mehryar and Nock, Richard and \"Ozg\"ur, Ayfer and Pagh, Rasmus and Qi, Hang and Ramage, Daniel and Raskar, Ramesh and Raykova, Mariana and Song, Dawn and Song, Weikang and Stich, Sebastian U. and Sun, Ziteng and Suresh, Ananda Theertha and Tram\`er, Florian and Vepakomma, Praneeth and Wang, Jianyu and Xiong, Li and Xu, Zheng and Yang, Qiang and Yu, Felix X. and Yu, Han and Zhao, Sen},
 title = {Advances and Open Problems in Federated Learning},
 year = {2021},
 journal = {Foundations and Trends{\textregistered} in Machine Learning},
 volume = {14},
 number = {1{\textendash}2},
 pages = {1--210},
 doi = {10.1561/2200000083},
 issn = {1935-8237, 1935-8245},
 url = {https://doi.org/10.1561/2200000083},
 date-added = {2023-11-22 19:14:08 -0500},
 date-modified = {2023-11-22 19:14:08 -0500},
 bdsk-url-1 = {http://dx.doi.org/10.1561/2200000083},
 source = {Crossref},
 publisher = {Now Publishers},
}

@article{markovic2020,
 author = {Markovi\'c, Danijela and Mizrahi, Alice and Querlioz, Damien and Grollier, Julie},
 title = {Physics for neuromorphic computing},
 year = {2020},
 journal = {Nature Reviews Physics},
 publisher = {Springer Science and Business Media LLC},
 volume = {2},
 number = {9},
 pages = {499--510},
 doi = {10.1038/s42254-020-0208-2},
 source = {Crossref},
 url = {https://doi.org/10.1038/s42254-020-0208-2},
 issn = {2522-5820},
 month = jul,
}

@article{martin1993myth,
 author = {Martin, C. Dianne},
 title = {The myth of the awesome thinking machine},
 year = {1993},
 journal = {Commun. ACM},
 publisher = {Association for Computing Machinery (ACM)},
 volume = {36},
 number = {4},
 pages = {120--133},
 doi = {10.1145/255950.153587},
 source = {Crossref},
 url = {https://doi.org/10.1145/255950.153587},
 issn = {0001-0782, 1557-7317},
 month = apr,
}

@article{maslej2023artificial,
 author = {Maslej, Nestor and Fattorini, Loredana and Brynjolfsson, Erik and Etchemendy, John and Ligett, Katrina and Lyons, Terah and Manyika, James and Ngo, Helen and Niebles, Juan Carlos and Parli, Vanessa and others},
 title = {Artificial intelligence index report 2023},
 year = {2023},
 journal = {arXiv preprint arXiv:2310.03715},
}

@article{mattson2020mlperf,
 author = {Mattson, Peter and Reddi, Vijay Janapa and Cheng, Christine and Coleman, Cody and Diamos, Greg and Kanter, David and Micikevicius, Paulius and Patterson, David and Schmuelling, Guenther and Tang, Hanlin and Wei, Gu-Yeon and Wu, Carole-Jean},
 title = {{MLPerf:} {An} Industry Standard Benchmark Suite for Machine Learning Performance},
 year = {2020},
 journal = {IEEE Micro},
 volume = {40},
 pages = {8--16},
 doi = {10.1109/mm.2020.2974843},
 number = {2},
 source = {Crossref},
 url = {https://doi.org/10.1109/mm.2020.2974843},
 publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
 issn = {0272-1732, 1937-4143},
 month = mar,
}

@article{maxime2016impact,
 author = {Cohen, Maxime C. and Lobel, Ruben and Perakis, Georgia},
 title = {The Impact of Demand Uncertainty on Consumer Subsidies for Green Technology Adoption},
 year = {2016},
 journal = {Manage. Sci.},
 publisher = {Institute for Operations Research and the Management Sciences (INFORMS)},
 volume = {62},
 number = {5},
 pages = {1235--1258},
 issn = {0025-1909, 1526-5501},
 url = {https://doi.org/10.1287/mnsc.2015.2173},
 urldate = {2023-12-01},
 abstract = {This paper studies government subsidies for green technology adoption while considering the manufacturing industry's response. Government subsidies offered directly to consumers impact the supplier's production and pricing decisions. Our analysis expands the current understanding of the price-setting newsvendor model, incorporating the external influence from the government, who is now an additional player in the system. We quantify how demand uncertainty impacts the various players (government, industry, and consumers) when designing policies. We further show that, for convex demand functions, an increase in demand uncertainty leads to higher production quantities and lower prices, resulting in lower profits for the supplier. With this in mind, one could expect consumer surplus to increase with uncertainty. In fact, we show that this is not always the case and that the uncertainty impact on consumer surplus depends on the trade-off between lower prices and the possibility of underserving customers with high valuations. We also show that when policy makers such as governments ignore demand uncertainty when designing consumer subsidies, they can significantly miss the desired adoption target level. From a coordination perspective, we demonstrate that the decentralized decisions are also optimal for a central planner managing jointly the supplier and the government. As a result, subsidies provide a coordination mechanism.},
 doi = {10.1287/mnsc.2015.2173},
 source = {Crossref},
 month = may,
}

@incollection{mccarthy1981epistemological,
 author = {McCarthy, John},
 title = {Epistemological Problems Of Artificial Intelligence},
 year = {1981},
 booktitle = {Readings in Artificial Intelligence},
 publisher = {Elsevier},
 pages = {459--465},
 doi = {10.1016/b978-0-934613-03-3.50035-0},
 source = {Crossref},
 url = {https://doi.org/10.1016/b978-0-934613-03-3.50035-0},
}

@inproceedings{mcmahan2017communication,
 author = {McMahan, Brendan and Moore, Eider and Ramage, Daniel and Hampson, Seth and y Arcas, Blaise Aguera},
 title = {Communication-efficient learning of deep networks from decentralized data},
 year = {2017},
 booktitle = {Artificial intelligence and statistics},
 pages = {1273--1282},
 organization = {PMLR},
}

@inproceedings{mcmahan2023communicationefficient,
 author = {McMahan, Brendan and Moore, Eider and Ramage, Daniel and Hampson, Seth and y Arcas, Blaise Aguera},
 title = {Communication-efficient learning of deep networks from decentralized data},
 year = {2017},
 booktitle = {Artificial intelligence and statistics},
 pages = {1273--1282},
 organization = {PMLR},
}

@article{miller2000optical,
 author = {Miller, D.A.B.},
 title = {Optical interconnects to silicon},
 year = {2000},
 journal = {\#IEEE\_J\_JSTQE\#},
 publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
 volume = {6},
 number = {6},
 pages = {1312--1317},
 doi = {10.1109/2944.902184},
 source = {Crossref},
 url = {https://doi.org/10.1109/2944.902184},
 issn = {1077-260X, 1558-4542},
 month = nov,
}

@article{miller2015remote,
 author = {Miller, Charlie and Valasek, Chris},
 title = {Remote exploitation of an unaltered passenger vehicle},
 year = {2015},
 journal = {Black Hat USA},
 volume = {2015},
 number = {S 91},
 pages = {1--91},
 date-added = {2023-11-22 17:11:27 -0500},
 date-modified = {2023-11-22 17:12:18 -0500},
}

@article{miller2019lessons,
 author = {Miller, Charlie},
 title = {Lessons learned from hacking a car},
 year = {2019},
 journal = {IEEE Design \&amp; Test},
 volume = {36},
 number = {6},
 pages = {7--9},
 date-added = {2023-11-22 16:12:04 -0500},
 date-modified = {2023-11-22 16:13:31 -0500},
 doi = {10.1109/mdat.2018.2863106},
 source = {Crossref},
 url = {https://doi.org/10.1109/mdat.2018.2863106},
 publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
 issn = {2168-2356, 2168-2364},
 month = dec,
}

@article{mills1997overview,
 author = {Mills, Andrew and Le Hunte, Stephen},
 title = {An overview of semiconductor photocatalysis},
 year = {1997},
 journal = {J. Photochem. Photobiol., A},
 publisher = {Elsevier BV},
 volume = {108},
 number = {1},
 pages = {1--35},
 doi = {10.1016/s1010-6030(97)00118-4},
 source = {Crossref},
 url = {https://doi.org/10.1016/s1010-6030(97)00118-4},
 issn = {1010-6030},
 month = jul,
}

@article{mittal2021survey,
 author = {Mittal, Sparsh and Verma, Gaurav and Kaushik, Brajesh and Khanday, Farooq A.},
 title = {A survey of {SRAM}-based in-memory computing techniques and applications},
 year = {2021},
 journal = {J. Syst. Architect.},
 publisher = {Elsevier BV},
 volume = {119},
 pages = {102276},
 doi = {10.1016/j.sysarc.2021.102276},
 source = {Crossref},
 url = {https://doi.org/10.1016/j.sysarc.2021.102276},
 issn = {1383-7621},
 month = oct,
}

@article{modha2023neural,
 author = {Modha, Dharmendra S. and Akopyan, Filipp and Andreopoulos, Alexander and Appuswamy, Rathinakumar and Arthur, John V. and Cassidy, Andrew S. and Datta, Pallab and DeBole, Michael V. and Esser, Steven K. and Otero, Carlos Ortega and Sawada, Jun and Taba, Brian and Amir, Arnon and Bablani, Deepika and Carlson, Peter J. and Flickner, Myron D. and Gandhasri, Rajamohan and Garreau, Guillaume J. and Ito, Megumi and Klamo, Jennifer L. and Kusnitz, Jeffrey A. and McClatchey, Nathaniel J. and McKinstry, Jeffrey L. and Nakamura, Yutaka and Nayak, Tapan K. and Risk, William P. and Schleupen, Kai and Shaw, Ben and Sivagnaname, Jay and Smith, Daniel F. and Terrizzano, Ignacio and Ueda, Takanori},
 title = {Neural inference at the frontier of energy, space, and time},
 year = {2023},
 journal = {Science},
 publisher = {American Association for the Advancement of Science (AAAS)},
 volume = {382},
 number = {6668},
 pages = {329--335},
 doi = {10.1126/science.adh1174},
 source = {Crossref},
 url = {https://doi.org/10.1126/science.adh1174},
 issn = {0036-8075, 1095-9203},
 month = oct,
}

@article{monyei2018electrons,
 author = {Monyei, Chukwuka G. and Jenkins, Kirsten E.H.},
 title = {Electrons have no identity: {Setting} right misrepresentations in Google and Apple{\textquoteright}s clean energy purchasing},
 year = {2018},
 journal = {Energy Research \&amp; Social Science},
 publisher = {Elsevier BV},
 volume = {46},
 pages = {48--51},
 doi = {10.1016/j.erss.2018.06.015},
 source = {Crossref},
 url = {https://doi.org/10.1016/j.erss.2018.06.015},
 issn = {2214-6296},
 month = dec,
}

@article{moshawrab2023reviewing,
 author = {Moshawrab, Mohammad and Adda, Mehdi and Bouzouane, Abdenour and Ibrahim, Hussein and Raad, Ali},
 title = {Reviewing Federated Learning Aggregation Algorithms; Strategies, Contributions, Limitations and Future Perspectives},
 year = {2023},
 journal = {Electronics},
 publisher = {MDPI AG},
 volume = {12},
 number = {10},
 pages = {2287},
 doi = {10.3390/electronics12102287},
 source = {Crossref},
 url = {https://doi.org/10.3390/electronics12102287},
 issn = {2079-9292},
 month = may,
}

@article{muhammad2022survey,
 author = {Muhammad, Awais and Bae, Sung-Ho},
 title = {A Survey on Efficient Methods for Adversarial Robustness},
 year = {2022},
 journal = {\#IEEE\_O\_ACC\#},
 publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
 volume = {10},
 pages = {118815--118830},
 doi = {10.1109/access.2022.3216291},
 source = {Crossref},
 url = {https://doi.org/10.1109/access.2022.3216291},
 issn = {2169-3536},
}

@inproceedings{munshi2009opencl,
 author = {Munshi, Aaftab},
 title = {The {OpenCL} specification},
 year = {2009},
 booktitle = {2009 IEEE Hot Chips 21 Symposium (HCS)},
 volume = {},
 number = {},
 pages = {1--314},
 doi = {10.1109/hotchips.2009.7478342},
 bdsk-url-1 = {https://doi.org/10.1109/HOTCHIPS.2009.7478342},
 source = {Crossref},
 url = {https://doi.org/10.1109/hotchips.2009.7478342},
 publisher = {IEEE},
 month = aug,
}

@article{musk2019integrated,
 author = {Musk, Elon and others},
 title = {An Integrated Brain-Machine Interface Platform With Thousands of Channels},
 year = {2019},
 journal = {J. Med. Internet Res.},
 publisher = {JMIR Publications Inc.},
 volume = {21},
 number = {10},
 pages = {e16194},
 doi = {10.2196/16194},
 source = {Crossref},
 url = {https://doi.org/10.2196/16194},
 issn = {1438-8871},
 month = oct,
}

@book{nakano2021geopolitics,
 author = {Nakano, Jane},
 title = {The geopolitics of critical minerals supply chains},
 year = {2021},
 publisher = {JSTOR},
}

@article{narayanan2006break,
 author = {Narayanan, Arvind and Shmatikov, Vitaly},
 title = {How to break anonymity of the netflix prize dataset},
 year = {2006},
 journal = {arXiv preprint cs/0610105},
 date-added = {2023-11-22 16:16:19 -0500},
 date-modified = {2023-11-22 16:16:59 -0500},
}

@misc{nas,
 author = {Zoph, Barret and Le, Quoc V.},
 title = {Cybernetical Intelligence},
 year = {2023},
 eprint = {1611.01578},
 archiveprefix = {arXiv},
 primaryclass = {cs.LG},
 doi = {10.1002/9781394217519.ch17},
 source = {Crossref},
 url = {https://doi.org/10.1002/9781394217519.ch17},
 publisher = {Wiley},
 isbn = {9781394217489, 9781394217519},
 pages = {367--392},
 month = oct,
}

@article{ng2021ai,
 author = {Ng, Davy Tsz Kit and Leung, Jac Ka Lok and Chu, Kai Wah Samuel and Qiao, Maggie Shen},
 title = {{AI} literacy: {Definition,} teaching, evaluation and ethical issues},
 year = {2021},
 journal = {Proceedings of the Association for Information Science and Technology},
 publisher = {Wiley Online Library},
 volume = {58},
 number = {1},
 pages = {504--509},
}

@article{ngo2022alignment,
 author = {Ngo, Richard and Chan, Lawrence and Mindermann, S\"oren},
 title = {The alignment problem from a deep learning perspective},
 year = {2022},
 journal = {arXiv preprint arXiv:2209.00626},
}

@inproceedings{nguyen2023re,
 author = {Nguyen, Ngoc-Bao and Chandrasegaran, Keshigeyan and Abdollahzadeh, Milad and Cheung, Ngai-Man},
 title = {Re-Thinking Model Inversion Attacks Against Deep Neural Networks},
 year = {2023},
 booktitle = {2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
 pages = {16384--16393},
 doi = {10.1109/cvpr52729.2023.01572},
 source = {Crossref},
 url = {https://doi.org/10.1109/cvpr52729.2023.01572},
 publisher = {IEEE},
 month = jun,
}

@misc{noauthor_amd_nodate,
 title = {{AMD} Radeon {RX} 7000 Series Desktop Graphics Cards},
 url = {https://www.amd.com/en/graphics/radeon-rx-graphics},
 urldate = {2023-11-07},
 bdsk-url-1 = {https://www.amd.com/en/graphics/radeon-rx-graphics},
}

@misc{noauthor_deep_nodate,
 author = {Gu, Ivy},
 title = {Deep Learning Model Compression (ii) by Ivy Gu Medium},
 year = {2023},
 url = {https://ivygdy.medium.com/deep-learning-model-compression-ii-546352ea9453},
 urldate = {2023-10-20},
 bdsk-url-1 = {https://ivygdy.medium.com/deep-learning-model-compression-ii-546352ea9453},
}

@misc{noauthor_evolution_2023,
 title = {The Evolution of Audio {DSPs}},
 year = {2023},
 month = oct,
 journal = {audioXpress},
 url = {https://audioxpress.com/article/the-evolution-of-audio-dsps},
 urldate = {2023-11-07},
 abstract = {To complement the extensive perspective of another Market Update feature article on DSP Products and Applications, published in the November 2020 edition, audioXpress was honored to have the valuable contribution from one of the main suppliers in the field. In this article, Youval Nachum, CEVA's Senior Product Marketing Manager, writes about \&quot;The Evolution of Audio DSPs,\&quot; discussing how DSP technology has evolved, its impact on the user experience, and what the future of DSP has in store for us.},
 language = {en},
 bdsk-url-1 = {https://audioxpress.com/article/the-evolution-of-audio-dsps},
}

@misc{noauthor_fpga_nodate,
 title = {{FPGA} Architecture Overview},
 url = {https://www.intel.com/content/www/us/en/docs/oneapi-fpga-add-on/optimization-guide/2023-1/fpga-architecture-overview.html},
 urldate = {2023-11-07},
 bdsk-url-1 = {https://www.intel.com/content/www/us/en/docs/oneapi-fpga-add-on/optimization-guide/2023-1/fpga-architecture-overview.html},
}

@misc{noauthor_google_2023,
 title = {Google Tensor G3: {The} new chip that gives your Pixel an {AI} upgrade},
 shorttitle = {Google Tensor G3},
 year = {2023},
 month = oct,
 journal = {Google},
 url = {https://blog.google/products/pixel/google-tensor-g3-pixel-8/},
 urldate = {2023-11-07},
 abstract = {Tensor G3 on Pixel 8 and Pixel 8 Pro is more helpful, more efficient and more powerful.},
 language = {en-us},
 bdsk-url-1 = {https://blog.google/products/pixel/google-tensor-g3-pixel-8/},
}

@misc{noauthor_hexagon_nodate,
 title = {Hexagon {DSP} {SDK} Processor},
 journal = {Qualcomm Developer Network},
 url = {https://developer.qualcomm.com/software/hexagon-dsp-sdk/dsp-processor},
 urldate = {2023-11-07},
 abstract = {The Hexagon DSP processor has both CPU and DSP functionality to support deeply embedded processing needs of the mobile platform for both multimedia and modem functions.},
 language = {en},
 bdsk-url-1 = {https://developer.qualcomm.com/software/hexagon-dsp-sdk/dsp-processor},
}

@misc{noauthor_integrated_2023,
 title = {Integrated circuit},
 year = {2023},
 month = nov,
 journal = {Wikipedia},
 url = {https://en.wikipedia.org/w/index.php?title=Integrated_circuit&oldid=1183537457},
 urldate = {2023-11-07},
 copyright = {Creative Commons Attribution-ShareAlike License},
 note = {Page Version ID: 1183537457},
 abstract = {An integrated circuit (also known as an IC, a chip, or a microchip) is a set of electronic circuits on one small flat piece of semiconductor material, usually silicon. Large numbers of miniaturized transistors and other electronic components are integrated together on the chip. This results in circuits that are orders of magnitude smaller, faster, and less expensive than those constructed of discrete components, allowing a large transistor count. The IC's mass production capability, reliability, and building-block approach to integrated circuit design have ensured the rapid adoption of standardized ICs in place of designs using discrete transistors. ICs are now used in virtually all electronic equipment and have revolutionized the world of electronics. Computers, mobile phones and other home appliances are now essential parts of the structure of modern societies, made possible by the small size and low cost of ICs such as modern computer processors and microcontrollers. Very-large-scale integration was made practical by technological advancements in semiconductor device fabrication. Since their origins in the 1960s, the size, speed, and capacity of chips have progressed enormously, driven by technical advances that fit more and more transistors on chips of the same size {\textendash} a modern chip may have many billions of transistors in an area the size of a human fingernail. These advances, roughly following Moore's law, make the computer chips of today possess millions of times the capacity and thousands of times the speed of the computer chips of the early 1970s. ICs have three main advantages over discrete circuits: size, cost and performance. The size and cost is low because the chips, with all their components, are printed as a unit by photolithography rather than being constructed one transistor at a time. Furthermore, packaged ICs use much less material than discrete circuits. Performance is high because the IC's components switch quickly and consume comparatively little power because of their small size and proximity. The main disadvantage of ICs is the high initial cost of designing them and the enormous capital cost of factory construction. This high initial cost means ICs are only commercially viable when high production volumes are anticipated.},
 language = {en},
 bdsk-url-1 = {https://en.wikipedia.org/w/index.php?title=Integrated\_circuit oldid=1183537457},
}

@misc{noauthor_intel_nodate,
 title = {Intel{\textregistered} Arc{\texttrademark} Graphics Overview},
 journal = {Intel},
 url = {https://www.intel.com/content/www/us/en/products/details/discrete-gpus/arc.html},
 urldate = {2023-11-07},
 abstract = {Find out how Intel{\textregistered} Arc Graphics unlock lifelike gaming and seamless content creation.},
 language = {en},
 bdsk-url-1 = {https://www.intel.com/content/www/us/en/products/details/discrete-gpus/arc.html},
}

@misc{noauthor_introduction_nodate,
 author = {Hegde, Sumant},
 title = {An Introduction to Separable Convolutions - Analytics Vidhya},
 year = {2023},
 url = {https://www.analyticsvidhya.com/blog/2021/11/an-introduction-to-separable-convolutions/},
 urldate = {2023-10-20},
 bdsk-url-1 = {https://www.analyticsvidhya.com/blog/2021/11/an-introduction-to-separable-convolutions/},
}

@misc{noauthor_knowledge_nodate,
 author = {IntelLabs},
 title = {Knowledge Distillation - Neural Network Distiller},
 year = {2023},
 url = {https://intellabs.github.io/distiller/knowledge_distillation.html},
 urldate = {2023-10-20},
 bdsk-url-1 = {https://intellabs.github.io/distiller/knowledge\_distillation.html},
}

@misc{noauthor_project_nodate,
 title = {Project Catapult - Microsoft Research},
 url = {https://www.microsoft.com/en-us/research/project/project-catapult/},
 urldate = {2023-11-07},
 bdsk-url-1 = {https://www.microsoft.com/en-us/research/project/project-catapult/},
}

@misc{noauthor_what_nodate,
 title = {What is an {FPGA?} Field Programmable Gate Array},
 shorttitle = {What is an FPGA?},
 journal = {AMD},
 url = {https://www.xilinx.com/products/silicon-devices/fpga/what-is-an-fpga.html},
 urldate = {2023-11-07},
 abstract = {What is an FPGA - Field Programmable Gate Arrays are semiconductor devices that are based around a matrix of configurable logic blocks (CLBs) connected via programmable interconnects. FPGAs can be reprogrammed to desired application or functionality requirements after manufacturing.},
 language = {en},
 bdsk-url-1 = {https://www.xilinx.com/products/silicon-devices/fpga/what-is-an-fpga.html},
}

@misc{noauthor_who_nodate,
 title = {Who Invented the Microprocessor? - {CHM}},
 url = {https://computerhistory.org/blog/who-invented-the-microprocessor/},
 urldate = {2023-11-07},
 bdsk-url-1 = {https://computerhistory.org/blog/who-invented-the-microprocessor/},
}

@inproceedings{Norman2017TPUv1,
 author = {Jouppi, Norman P. and Young, Cliff and Patil, Nishant and Patterson, David and Agrawal, Gaurav and Bajwa, Raminder and Bates, Sarah and Bhatia, Suresh and Boden, Nan and Borchers, Al and Boyle, Rick and Cantin, Pierre-luc and Chao, Clifford and Clark, Chris and Coriell, Jeremy and Daley, Mike and Dau, Matt and Dean, Jeffrey and Gelb, Ben and Ghaemmaghami, Tara Vazir and Gottipati, Rajendra and Gulland, William and Hagmann, Robert and Ho, C. Richard and Hogberg, Doug and Hu, John and Hundt, Robert and Hurt, Dan and Ibarz, Julian and Jaffey, Aaron and Jaworski, Alek and Kaplan, Alexander and Khaitan, Harshit and Killebrew, Daniel and Koch, Andy and Kumar, Naveen and Lacy, Steve and Laudon, James and Law, James and Le, Diemthu and Leary, Chris and Liu, Zhuyuan and Lucke, Kyle and Lundin, Alan and MacKean, Gordon and Maggiore, Adriana and Mahony, Maire and Miller, Kieran and Nagarajan, Rahul and Narayanaswami, Ravi and Ni, Ray and Nix, Kathy and Norrie, Thomas and Omernick, Mark and Penukonda, Narayana and Phelps, Andy and Ross, Jonathan and Ross, Matt and Salek, Amir and Samadiani, Emad and Severn, Chris and Sizikov, Gregory and Snelham, Matthew and Souter, Jed and Steinberg, Dan and Swing, Andy and Tan, Mercedes and Thorson, Gregory and Tian, Bo and Toma, Horia and Tuttle, Erick and Vasudevan, Vijay and Walter, Richard and Wang, Walter and Wilcox, Eric and Yoon, Doe Hyun},
 title = {In-Datacenter Performance Analysis of a Tensor Processing Unit},
 year = {2017},
 booktitle = {Proceedings of the 44th Annual International Symposium on Computer Architecture},
 location = {Toronto, ON, Canada},
 publisher = {ACM},
 address = {New York, NY, USA},
 series = {ISCA '17},
 pages = {1--12},
 doi = {10.1145/3079856.3080246},
 isbn = {9781450348928},
 url = {https://doi.org/10.1145/3079856.3080246},
 abstract = {Many architects believe that major improvements in cost-energy-performance must now come from domain-specific hardware. This paper evaluates a custom ASIC{\textemdash}called a Tensor Processing Unit (TPU) {\textemdash} deployed in datacenters since 2015 that accelerates the inference phase of neural networks (NN). The heart of the TPU is a 65,536 8-bit MAC matrix multiply unit that offers a peak throughput of 92 TeraOps/second (TOPS) and a large (28 MiB) software-managed on-chip memory. The TPU's deterministic execution model is a better match to the 99th-percentile response-time requirement of our NN applications than are the time-varying optimizations of CPUs and GPUs that help average throughput more than guaranteed latency. The lack of such features helps explain why, despite having myriad MACs and a big memory, the TPU is relatively small and low power. We compare the TPU to a server-class Intel Haswell CPU and an Nvidia K80 GPU, which are contemporaries deployed in the same datacenters. Our workload, written in the high-level TensorFlow framework, uses production NN applications (MLPs, CNNs, and LSTMs) that represent 95\% of our datacenters' NN inference demand. Despite low utilization for some applications, the TPU is on average about 15X {\textendash} 30X faster than its contemporary GPU or CPU, with TOPS/Watt about 30X {\textendash} 80X higher. Moreover, using the CPU's GDDR5 memory in the TPU would triple achieved TOPS and raise TOPS/Watt to nearly 70X the GPU and 200X the CPU.},
 numpages = {12},
 keywords = {accelerator, neural network, MLP, TPU, CNN, deep learning, domain-specific architecture, GPU, TensorFlow, DNN, RNN, LSTM},
 bdsk-url-1 = {https://doi.org/10.1145/3079856.3080246},
 source = {Crossref},
 month = jun,
}

@article{Norrie2021TPUv2_3,
 author = {Norrie, Thomas and Patil, Nishant and Yoon, Doe Hyun and Kurian, George and Li, Sheng and Laudon, James and Young, Cliff and Jouppi, Norman and Patterson, David},
 title = {The Design Process for Google's Training Chips: {Tpuv2} and {TPUv3}},
 year = {2021},
 journal = {IEEE Micro},
 volume = {41},
 number = {2},
 pages = {56--63},
 doi = {10.1109/mm.2021.3058217},
 bdsk-url-1 = {https://doi.org/10.1109/MM.2021.3058217},
 source = {Crossref},
 url = {https://doi.org/10.1109/mm.2021.3058217},
 publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
 issn = {0272-1732, 1937-4143},
 month = mar,
}

@article{Northcutt_Athalye_Mueller_2021,
 author = {Northcutt, Curtis G and Athalye, Anish and Mueller, Jonas},
 title = {Pervasive Label Errors in Test Sets Destabilize Machine Learning Benchmarks},
 year = {2021},
 month = mar,
 journal = {arXiv},
 doi = {nbsp; https://doi.org/10.48550/arXiv.2103.14749 arXiv-issued DOI via DataCite},
 bdsk-url-1 = {nbsp;},
}

@article{obarBiggestLieInternet2018,
 author = {Obar, Jonathan A.},
 title = {The Biggest Lie on the Internet: {Ignoring} the Privacy Policies and Terms of Service Policies of Social Networking Services},
 year = {2016},
 month = jun,
 address = {Rochester, NY},
 doi = {10.2139/ssrn.2757465},
 url = {https://doi.org/10.2139/ssrn.2757465},
 urldate = {2023-04-20},
 type = {SSRN Scholarly Paper},
 language = {en},
 source = {Crossref},
 journal = {SSRN Electronic Journal},
 publisher = {Elsevier BV},
 issn = {1556-5068},
}

@article{obermeyer2019dissecting,
 author = {Obermeyer, Ziad and Powers, Brian and Vogeli, Christine and Mullainathan, Sendhil},
 title = {Dissecting racial bias in an algorithm used to manage the health of populations},
 year = {2019},
 journal = {Science},
 publisher = {American Association for the Advancement of Science (AAAS)},
 volume = {366},
 number = {6464},
 pages = {447--453},
 doi = {10.1126/science.aax2342},
 source = {Crossref},
 url = {https://doi.org/10.1126/science.aax2342},
 issn = {0036-8075, 1095-9203},
 month = oct,
}

@techreport{oecd22,
 author = {Oecd},
 title = {Measuring the environmental impacts of artificial intelligence compute and applications},
 year = {2022},
 number = {341},
 doi = {10.1787/7babf571-en},
 url = {https://doi.org/10.1787/7babf571-en},
 source = {Crossref},
 institution = {Organisation for Economic Co-Operation and Development (OECD)},
 subtitle = {The AI footprint},
 issn = {2071-6826},
 month = nov,
}

@article{olah2020zoom,
 author = {Olah, Chris and Cammarata, Nick and Schubert, Ludwig and Goh, Gabriel and Petrov, Michael and Carter, Shan},
 title = {Zoom In: {An} Introduction to Circuits},
 year = {2020},
 journal = {Distill},
 volume = {5},
 number = {3},
 pages = {e00024--001},
 doi = {10.23915/distill.00024.001},
 source = {Crossref},
 url = {https://doi.org/10.23915/distill.00024.001},
 publisher = {Distill Working Group},
 issn = {2476-0757},
 month = mar,
}

@article{oliynyk2023know,
 author = {Oliynyk, Daryna and Mayer, Rudolf and Rauber, Andreas},
 title = {I Know What You Trained Last Summer: {A} Survey on Stealing Machine Learning Models and Defences},
 year = {2023},
 month = jul,
 journal = {ACM Comput. Surv.},
 volume = {55},
 number = {14s},
 date-added = {2023-11-22 16:18:21 -0500},
 date-modified = {2023-11-22 16:20:44 -0500},
 keywords = {model stealing, Machine learning, model extraction},
 doi = {10.1145/3595292},
 source = {Crossref},
 url = {https://doi.org/10.1145/3595292},
 publisher = {Association for Computing Machinery (ACM)},
 issn = {0360-0300, 1557-7341},
 pages = {1--41},
}

@inproceedings{ooko2021tinyml,
 author = {Ooko, Samson Otieno and Muyonga Ogore, Marvin and Nsenga, Jimmy and Zennaro, Marco},
 title = {{TinyML} in Africa: {Opportunities} and Challenges},
 year = {2021},
 booktitle = {2021 IEEE Globecom Workshops (GC Wkshps)},
 pages = {1--6},
 organization = {IEEE},
 doi = {10.1109/gcwkshps52748.2021.9682107},
 source = {Crossref},
 url = {https://doi.org/10.1109/gcwkshps52748.2021.9682107},
 publisher = {IEEE},
 month = dec,
}

@article{oprea2022poisoning,
 author = {Oprea, Alina and Singhal, Anoop and Vassilev, Apostol},
 title = {Poisoning Attacks Against Machine Learning: {Can} Machine Learning Be Trustworthy?},
 year = {2022},
 journal = {Computer},
 publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
 volume = {55},
 number = {11},
 pages = {94--99},
 doi = {10.1109/mc.2022.3190787},
 source = {Crossref},
 url = {https://doi.org/10.1109/mc.2022.3190787},
 issn = {0018-9162, 1558-0814},
 month = nov,
}

@misc{ou_low_2023,
 author = {Ou, Xinwei and Chen, Zhangxin and Zhu, Ce and Liu, Yipeng},
 title = {Low Rank Optimization for Efficient Deep Learning: {Making} A Balance between Compact Architecture and Fast Training},
 shorttitle = {Low Rank Optimization for Efficient Deep Learning},
 year = {2023},
 month = mar,
 publisher = {arXiv},
 url = {http://arxiv.org/abs/2303.13635},
 urldate = {2023-10-20},
 note = {arXiv:2303.13635 [cs]},
 abstract = {Deep neural networks have achieved great success in many data processing applications. However, the high computational complexity and storage cost makes deep learning hard to be used on resource-constrained devices, and it is not environmental-friendly with much power cost. In this paper, we focus on low-rank optimization for efficient deep learning techniques. In the space domain, deep neural networks are compressed by low rank approximation of the network parameters, which directly reduces the storage requirement with a smaller number of network parameters. In the time domain, the network parameters can be trained in a few subspaces, which enables efficient training for fast convergence. The model compression in the spatial domain is summarized into three categories as pre-train, pre-set, and compression-aware methods, respectively. With a series of integrable techniques discussed, such as sparse pruning, quantization, and entropy coding, we can ensemble them in an integration framework with lower computational complexity and storage. Besides of summary of recent technical advances, we have two findings for motivating future works: one is that the effective rank outperforms other sparse measures for network compression. The other is a spatial and temporal balance for tensorized neural networks.},
 file = {arXiv.org Snapshot:/Users/jeffreyma/Zotero/storage/SPSZ2HR9/2303.html:text/html;Full Text PDF:/Users/jeffreyma/Zotero/storage/6TUEBTEX/Ou et al. - 2023 - Low Rank Optimization for Efficient Deep Learning.pdf:application/pdf},
 keywords = {Computer Science - Machine Learning},
 bdsk-url-1 = {http://arxiv.org/abs/2303.13635},
}

@article{pan_survey_2010,
 author = {Pan, Sinno Jialin and Yang, Qiang},
 title = {A Survey on Transfer Learning},
 year = {2010},
 month = oct,
 journal = {IEEE Trans. Knowl. Data Eng.},
 publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
 volume = {22},
 number = {10},
 pages = {1345--1359},
 doi = {10.1109/tkde.2009.191},
 issn = {1041-4347},
 url = {https://doi.org/10.1109/tkde.2009.191},
 urldate = {2023-10-25},
 file = {Pan and Yang - 2010 - A Survey on Transfer Learning.pdf:/Users/alex/Zotero/storage/T3H8E5K8/Pan and Yang - 2010 - A Survey on Transfer Learning.pdf:application/pdf},
 language = {en},
 bdsk-url-1 = {http://ieeexplore.ieee.org/document/5288526/},
 bdsk-url-2 = {https://doi.org/10.1109/TKDE.2009.191},
 source = {Crossref},
}

@article{pan2009survey,
 author = {Pan, Sinno Jialin and Yang, Qiang},
 title = {A Survey on Transfer Learning},
 year = {2010},
 journal = {IEEE Trans. Knowl. Data Eng.},
 publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
 volume = {22},
 number = {10},
 pages = {1345--1359},
 doi = {10.1109/tkde.2009.191},
 source = {Crossref},
 url = {https://doi.org/10.1109/tkde.2009.191},
 issn = {1041-4347},
 month = oct,
}

@inproceedings{papernot2017practical,
 author = {Papernot, Nicolas and McDaniel, Patrick and Goodfellow, Ian and Jha, Somesh and Celik, Z. Berkay and Swami, Ananthram},
 title = {Practical Black-Box Attacks against Machine Learning},
 year = {2017},
 booktitle = {Proceedings of the 2017 ACM on Asia Conference on Computer and Communications Security},
 pages = {506--519},
 doi = {10.1145/3052973.3053009},
 source = {Crossref},
 url = {https://doi.org/10.1145/3052973.3053009},
 publisher = {ACM},
 month = apr,
}

@article{parisi_continual_2019,
 author = {Parisi, German I. and Kemker, Ronald and Part, Jose L. and Kanan, Christopher and Wermter, Stefan},
 title = {Continual lifelong learning with neural networks: {A} review},
 shorttitle = {Continual lifelong learning with neural networks},
 year = {2019},
 month = may,
 journal = {Neural Networks},
 volume = {113},
 pages = {54--71},
 doi = {10.1016/j.neunet.2019.01.012},
 issn = {0893-6080},
 url = {https://doi.org/10.1016/j.neunet.2019.01.012},
 urldate = {2023-10-26},
 file = {Parisi et al. - 2019 - Continual lifelong learning with neural networks .pdf:/Users/alex/Zotero/storage/TCGHD5TW/Parisi et al. - 2019 - Continual lifelong learning with neural networks .pdf:application/pdf},
 language = {en},
 bdsk-url-1 = {https://linkinghub.elsevier.com/retrieve/pii/S0893608019300231},
 bdsk-url-2 = {https://doi.org/10.1016/j.neunet.2019.01.012},
 source = {Crossref},
 publisher = {Elsevier BV},
}

@article{parrish2023adversarial,
 author = {Parrish, Alicia and Kirk, Hannah Rose and Quaye, Jessica and Rastogi, Charvi and Bartolo, Max and Inel, Oana and Ciro, Juan and Mosquera, Rafael and Howard, Addison and Cukierski, Will and Sculley, D. and Reddi, Vijay Janapa and Aroyo, Lora},
 title = {Adversarial Nibbler: {A} Data-Centric Challenge for Improving the Safety of Text-to-Image Models},
 year = {2023},
 journal = {arXiv preprint arXiv:2305.14384},
 date-added = {2023-11-22 16:24:50 -0500},
 date-modified = {2023-11-22 16:26:30 -0500},
}

@article{paszke2019pytorch,
 author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and others},
 title = {Pytorch: {An} imperative style, high-performance deep learning library},
 year = {2019},
 journal = {Adv Neural Inf Process Syst},
 volume = {32},
}

@book{patterson2016computer,
 author = {Patterson, David A and Hennessy, John L},
 title = {Computer organization and design {ARM} edition: {The} hardware software interface},
 year = {2016},
 publisher = {Morgan kaufmann},
}

@article{patterson2022carbon,
 author = {Patterson, David and Gonzalez, Joseph and Holzle, Urs and Le, Quoc and Liang, Chen and Munguia, Lluis-Miquel and Rothchild, Daniel and So, David R. and Texier, Maud and Dean, Jeff},
 title = {The Carbon Footprint of Machine Learning Training Will Plateau, Then Shrink},
 year = {2022},
 journal = {Computer},
 publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
 volume = {55},
 number = {7},
 pages = {18--28},
 doi = {10.1109/mc.2022.3148714},
 source = {Crossref},
 url = {https://doi.org/10.1109/mc.2022.3148714},
 issn = {0018-9162, 1558-0814},
 month = jul,
}

@misc{PauseGia82:online,
 year = {},
 note = {(Accessed on 12/06/2023)},
 howpublished = {https://futureoflife.org/open-letter/pause-giant-ai-experiments/},
}

@misc{Perrigo_2023,
 author = {Perrigo, Billy},
 title = {{OpenAI} used {Kenyan} workers on less than 2 per hour: {Exclusive}},
 year = {2023},
 month = jan,
 journal = {Time},
 publisher = {Time},
 url = {https://time.com/6247678/openai-chatgpt-kenya-workers/},
 bdsk-url-1 = {https://time.com/6247678/openai-chatgpt-kenya-workers/},
}

@article{peters2018designing,
 author = {Peters, Dorian and Calvo, Rafael A. and Ryan, Richard M.},
 title = {Designing for Motivation, Engagement and Wellbeing in Digital Experience},
 year = {2018},
 journal = {Front. Psychol.},
 publisher = {Frontiers Media SA},
 pages = {797},
 doi = {10.3389/fpsyg.2018.00797},
 source = {Crossref},
 url = {https://doi.org/10.3389/fpsyg.2018.00797},
 volume = {9},
 issn = {1664-1078},
 month = may,
}

@article{phillips2020four,
 author = {Phillips, P Jonathon and Hahn, Carina A and Fontana, Peter C and Broniatowski, David A and Przybocki, Mark A},
 title = {Four principles of explainable artificial intelligence},
 year = {2020},
 journal = {Gaithersburg, Maryland},
 volume = {18},
}

@article{plasma,
 author = {Attia, Zachi I. and Sugrue, Alan and Asirvatham, Samuel J. and Ackerman, Michael J. and Kapa, Suraj and Friedman, Paul A. and Noseworthy, Peter A.},
 title = {Noninvasive assessment of dofetilide plasma concentration using a deep learning (neural network) analysis of the surface electrocardiogram: {A} proof of concept study},
 year = {2018},
 month = aug,
 journal = {PLoS One},
 volume = {13},
 pages = {e0201059},
 doi = {10.1371/journal.pone.0201059},
 bdsk-url-1 = {https://doi.org/10.1371/journal.pone.0201059},
 number = {8},
 source = {Crossref},
 url = {https://doi.org/10.1371/journal.pone.0201059},
 publisher = {Public Library of Science (PLoS)},
 issn = {1932-6203},
}

@article{poff2002aquatic,
 author = {LeRoy Poff, N and Brinson, MM and Day, JW},
 title = {Aquatic ecosystems \& Global climate change},
 year = {2002},
 journal = {Pew Center on Global Climate Change},
}

@inproceedings{Prakash_2023,
 author = {Prakash, Shvetank and Callahan, Tim and Bushagour, Joseph and Banbury, Colby and Green, Alan V. and Warden, Pete and Ansell, Tim and Reddi, Vijay Janapa},
 title = {{CFU} Playground: {Full-stack} Open-Source Framework for Tiny Machine Learning {(TinyML)} Acceleration on {FPGAs}},
 shorttitle = {CFU Playground},
 year = {2023},
 month = apr,
 booktitle = {2023 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS)},
 publisher = {IEEE},
 pages = {157--167},
 doi = {10.1109/ispass57527.2023.00024},
 url = {https://doi.org/10.1109/ispass57527.2023.00024},
 urldate = {2023-10-25},
 note = {arXiv:2201.01863 [cs]},
 bdsk-url-1 = {https://doi.org/10.1109},
 bdsk-url-2 = {https://doi.org/10.1109/ispass57527.2023.00024},
 file = {Prakash et al. - 2023 - CFU Playground Full-Stack Open-Source Framework f.pdf:/Users/alex/Zotero/storage/BZNRIDTL/Prakash et al. - 2023 - CFU Playground Full-Stack Open-Source Framework f.pdf:application/pdf},
 keywords = {Computer Science - Machine Learning, Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Hardware Architecture},
 language = {en},
 source = {Crossref},
}

@inproceedings{prakash_cfu_2023,
 author = {Prakash, Shvetank and Callahan, Tim and Bushagour, Joseph and Banbury, Colby and Green, Alan V. and Warden, Pete and Ansell, Tim and Reddi, Vijay Janapa},
 title = {{CFU} Playground: {Full-stack} Open-Source Framework for Tiny Machine Learning {(TinyML)} Acceleration on {FPGAs}},
 shorttitle = {CFU Playground},
 year = {2023},
 month = apr,
 booktitle = {2023 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS)},
 pages = {157--167},
 doi = {10.1109/ispass57527.2023.00024},
 url = {https://doi.org/10.1109/ispass57527.2023.00024},
 urldate = {2023-10-25},
 note = {arXiv:2201.01863 [cs]},
 language = {en},
 keywords = {Computer Science - Machine Learning, Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Hardware Architecture},
 file = {Prakash et al. - 2023 - CFU Playground Full-Stack Open-Source Framework f.pdf:/Users/alex/Zotero/storage/BZNRIDTL/Prakash et al. - 2023 - CFU Playground Full-Stack Open-Source Framework f.pdf:application/pdf},
 bdsk-url-1 = {http://arxiv.org/abs/2201.01863},
 bdsk-url-2 = {https://doi.org/10.1109/ISPASS57527.2023.00024},
 source = {Crossref},
 publisher = {IEEE},
}

@misc{prakash2023tinyml,
 author = {Prakash, Shvetank and Stewart, Matthew and Banbury, Colby and Mazumder, Mark and Warden, Pete and Plancher, Brian and Reddi, Vijay Janapa},
 title = {Is {TinyML} Sustainable? Assessing the Environmental Impacts of Machine Learning on Microcontrollers},
 year = {2023},
 journal = {arXiv preprint arXiv:2301.11899},
 eprint = {2301.11899},
 archiveprefix = {arXiv},
 primaryclass = {cs.LG},
}

@misc{prakashTinyMLSustainableAssessing2023,
 author = {Prakash, Shvetank and Stewart, Matthew and Banbury, Colby and Mazumder, Mark and Warden, Pete and Plancher, Brian and Reddi, Vijay Janapa},
 shorttitle = {Is TinyML Sustainable?},
 year = {2023},
 month = may,
 publisher = {arXiv},
 url = {http://arxiv.org/abs/2301.11899},
 urldate = {2023-11-12},
 note = {arXiv:2301.11899 [cs]},
 annote = {Comment: To appear in Communications of the ACM (CACM) in 2023},
}

@article{preparednesspublic,
 author = {Preparedness, Emergency},
 title = {Public Health Law},
}

@inproceedings{Pushkarna_Zaldivar_Kjartansson_2022,
 author = {Pushkarna, Mahima and Zaldivar, Andrew and Kjartansson, Oddur},
 title = {Data Cards: {Purposeful} and Transparent Dataset Documentation for Responsible {AI}},
 year = {2022},
 journal = {2022 ACM Conference on Fairness, Accountability, and Transparency},
 doi = {10.1145/3531146.3533231},
 bdsk-url-1 = {https://doi.org/10.1145/3531146.3533231},
 source = {Crossref},
 url = {https://doi.org/10.1145/3531146.3533231},
 booktitle = {2022 ACM Conference on Fairness, Accountability, and Transparency},
 publisher = {ACM},
 month = jun,
}

@article{putnam_reconfigurable_2014,
 author = {Putnam, Andrew and Caulfield, Adrian M. and Chung, Eric S. and Chiou, Derek and Constantinides, Kypros and Demme, John and Esmaeilzadeh, Hadi and Fowers, Jeremy and Gopal, Gopi Prashanth and Gray, Jan and Haselman, Michael and Hauck, Scott and Heil, Stephen and Hormati, Amir and Kim, Joo-Young and Lanka, Sitaram and Larus, James and Peterson, Eric and Pope, Simon and Smith, Aaron and Thong, Jason and Xiao, Phillip Yi and Burger, Doug},
 title = {A reconfigurable fabric for accelerating large-scale datacenter services},
 year = {2014},
 month = jun,
 journal = {ACM SIGARCH Computer Architecture News},
 volume = {42},
 number = {3},
 pages = {13--24},
 doi = {10.1145/2678373.2665678},
 issn = {0163-5964},
 url = {https://doi.org/10.1145/2678373.2665678},
 urldate = {2023-11-07},
 abstract = {Datacenter workloads demand high computational capabilities, flexibility, power efficiency, and low cost. It is challenging to improve all of these factors simultaneously. To advance datacenter capabilities beyond what commodity server designs can provide, we have designed and built a composable, reconfigurablefabric to accelerate portions of large-scale software services. Each instantiation of the fabric consists of a 6x8 2-D torus of high-end Stratix V FPGAs embedded into a half-rack of 48 machines. One FPGA is placed into each server, accessible through PCIe, and wired directly to other FPGAs with pairs of 10 Gb SAS cables In this paper, we describe a medium-scale deployment of this fabric on a bed of 1,632 servers, and measure its efficacy in accelerating the Bing web search engine. We describe the requirements and architecture of the system, detail the critical engineering challenges and solutions needed to make the system robust in the presence of failures, and measure the performance, power, and resilience of the system when ranking candidate documents. Under high load, the largescale reconfigurable fabric improves the ranking throughput of each server by a factor of 95\% for a fixed latency distribution{\textemdash} or, while maintaining equivalent throughput, reduces the tail latency by 29\%},
 language = {en},
 bdsk-url-1 = {https://dl.acm.org/doi/10.1145/2678373.2665678},
 bdsk-url-2 = {https://doi.org/10.1145/2678373.2665678},
 source = {Crossref},
 publisher = {Association for Computing Machinery (ACM)},
}

@article{qi_efficient_2021,
 author = {Qi, Chen and Shen, Shibo and Li, Rongpeng and Zhao, Zhifeng and Liu, Qing and Liang, Jing and Zhang, Honggang},
 title = {An efficient pruning scheme of deep neural networks for Internet of Things applications},
 year = {2021},
 month = jun,
 journal = {EURASIP Journal on Advances in Signal Processing},
 volume = {2021},
 doi = {10.1186/s13634-021-00744-4},
 abstract = {Nowadays, deep neural networks (DNNs) have been rapidly deployed to realize a number of functionalities like sensing, imaging, classification, recognition, etc. However, the computational-intensive requirement of DNNs makes it difficult to be applicable for resource-limited Internet of Things (IoT) devices. In this paper, we propose a novel pruning-based paradigm that aims to reduce the computational cost of DNNs, by uncovering a more compact structure and learning the effective weights therein, on the basis of not compromising the expressive capability of DNNs. In particular, our algorithm can achieve efficient end-to-end training that transfers a redundant neural network to a compact one with a specifically targeted compression rate directly. We comprehensively evaluate our approach on various representative benchmark datasets and compared with typical advanced convolutional neural network (CNN) architectures. The experimental results verify the superior performance and robust effectiveness of our scheme. For example, when pruning VGG on CIFAR-10, our proposed scheme is able to significantly reduce its FLOPs (floating-point operations) and number of parameters with a proportion of 76.2\% and 94.1\%, respectively, while still maintaining a satisfactory accuracy. To sum up, our scheme could facilitate the integration of DNNs into the common machine-learning-based IoT framework and establish distributed training of neural networks in both cloud and edge.},
 file = {Full Text PDF:/Users/jeffreyma/Zotero/storage/AGWCC5VS/Qi et al. - 2021 - An efficient pruning scheme of deep neural network.pdf:application/pdf},
 bdsk-url-1 = {https://doi.org/10.1186/s13634-021-00744-4},
 number = {1},
 source = {Crossref},
 url = {https://doi.org/10.1186/s13634-021-00744-4},
 publisher = {Springer Science and Business Media LLC},
 issn = {1687-6180},
}

@misc{quantdeep,
 author = {Krishnamoorthi},
 title = {Quantizing deep convolutional networks for efficient inference: {A} whitepaper},
 year = {2018},
 month = jun,
 publisher = {arXiv},
 doi = {10.48550/arXiv.1806.08342},
 url = {https://arxiv.org/abs/1806.08342},
 urldate = {2018-06-21},
 bdsk-url-1 = {https://arxiv.org/abs/1806.08342},
 bdsk-url-2 = {https://doi.org/10.48550/arXiv.1806.08342},
}

@inproceedings{raina_large-scale_2009,
 author = {Raina, Rajat and Madhavan, Anand and Ng, Andrew Y.},
 title = {Large-scale deep unsupervised learning using graphics processors},
 year = {2009},
 month = jun,
 booktitle = {Proceedings of the 26th Annual International Conference on Machine Learning},
 publisher = {ACM},
 address = {Montreal Quebec Canada},
 pages = {873--880},
 doi = {10.1145/1553374.1553486},
 isbn = {978-1-60558-516-1},
 url = {https://doi.org/10.1145/1553374.1553486},
 urldate = {2023-11-07},
 language = {en},
 bdsk-url-1 = {https://dl.acm.org/doi/10.1145/1553374.1553486},
 bdsk-url-2 = {https://doi.org/10.1145/1553374.1553486},
 source = {Crossref},
}

@inproceedings{ramaswamy2023overlooked,
 author = {Ramaswamy, Vikram V. and Kim, Sunnie S. Y. and Fong, Ruth and Russakovsky, Olga},
 title = {Overlooked Factors in Concept-Based Explanations: {Dataset} Choice, Concept Learnability, and Human Capability},
 year = {2023},
 booktitle = {2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
 pages = {10932--10941},
 doi = {10.1109/cvpr52729.2023.01052},
 source = {Crossref},
 url = {https://doi.org/10.1109/cvpr52729.2023.01052},
 publisher = {IEEE},
 month = jun,
}

@article{ramaswamy2023ufo,
 author = {Ramaswamy, Vikram V and Kim, Sunnie SY and Fong, Ruth and Russakovsky, Olga},
 title = {{UFO:} {A} unified method for controlling Understandability and Faithfulness Objectives in concept-based explanations for {CNNs}},
 year = {2023},
 journal = {arXiv preprint arXiv:2303.15632},
}

@article{ramcharan2017deep,
 author = {Ramcharan, Amanda and Baranowski, Kelsee and McCloskey, Peter and Ahmed, Babuali and Legg, James and Hughes, David P.},
 title = {Deep Learning for Image-Based Cassava Disease Detection},
 year = {2017},
 journal = {Front. Plant Sci.},
 publisher = {Frontiers Media SA},
 volume = {8},
 pages = {1852},
 doi = {10.3389/fpls.2017.01852},
 source = {Crossref},
 url = {https://doi.org/10.3389/fpls.2017.01852},
 issn = {1664-462X},
 month = oct,
}

@inproceedings{ramesh2021zero,
 author = {Ramesh, Aditya and Pavlov, Mikhail and Goh, Gabriel and Gray, Scott and Voss, Chelsea and Radford, Alec and Chen, Mark and Sutskever, Ilya},
 title = {Zero-shot text-to-image generation},
 year = {2021},
 booktitle = {International Conference on Machine Learning},
 pages = {8821--8831},
 organization = {PMLR},
}

@article{Ranganathan2011-dc,
 author = {Ranganathan, Parthasarathy},
 title = {From Microprocessors to Nanostores: {Rethinking} Data-Centric Systems},
 year = {2011},
 month = jan,
 journal = {Computer},
 publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
 volume = {44},
 number = {1},
 pages = {39--48},
 doi = {10.1109/mc.2011.18},
 source = {Crossref},
 url = {https://doi.org/10.1109/mc.2011.18},
 issn = {0018-9162},
}

@misc{Rao_2021,
 author = {Rao, Ravi},
 year = {2021},
 month = dec,
 journal = {www.wevolver.com},
 url = {https://www.wevolver.com/article/tinyml-unlocks-new-possibilities-for-sustainable-development-technologies},
 bdsk-url-1 = {https://www.wevolver.com/article/tinyml-unlocks-new-possibilities-for-sustainable-development-technologies},
}

@inproceedings{Rashmi2018Secure,
 author = {R.V., Rashmi and A., Karthikeyan},
 title = {Secure boot of Embedded Applications - A Review},
 year = {2018},
 booktitle = {2018 Second International Conference on Electronics, Communication and Aerospace Technology (ICECA)},
 pages = {291--298},
 date-added = {2023-11-22 17:50:16 -0500},
 date-modified = {2023-11-22 17:51:39 -0500},
 doi = {10.1109/iceca.2018.8474730},
 source = {Crossref},
 url = {https://doi.org/10.1109/iceca.2018.8474730},
 publisher = {IEEE},
 month = mar,
}

@inproceedings{Ratner_Hancock_Dunnmon_Goldman_R_2018,
 author = {Ratner, Alex and Hancock, Braden and Dunnmon, Jared and Goldman, Roger and R\'e, Christopher},
 title = {Snorkel {MeTaL}},
 year = {2018},
 journal = {Proceedings of the Second Workshop on Data Management for End-To-End Machine Learning},
 doi = {10.1145/3209889.3209898},
 source = {Crossref},
 url = {https://doi.org/10.1145/3209889.3209898},
 booktitle = {Proceedings of the Second Workshop on Data Management for End-To-End Machine Learning},
 publisher = {ACM},
 subtitle = {Weak Supervision for Multi-Task Learning},
 month = jun,
}

@inproceedings{reddi2020mlperf,
 author = {Reddi, Vijay Janapa and Cheng, Christine and Kanter, David and Mattson, Peter and Schmuelling, Guenther and Wu, Carole-Jean and Anderson, Brian and Breughe, Maximilien and Charlebois, Mark and Chou, William and Chukka, Ramesh and Coleman, Cody and Davis, Sam and Deng, Pan and Diamos, Greg and Duke, Jared and Fick, Dave and Gardner, J. Scott and Hubara, Itay and Idgunji, Sachin and Jablin, Thomas B. and Jiao, Jeff and John, Tom St. and Kanwar, Pankaj and Lee, David and Liao, Jeffery and Lokhmotov, Anton and Massa, Francisco and Meng, Peng and Micikevicius, Paulius and Osborne, Colin and Pekhimenko, Gennady and Rajan, Arun Tejusve Raghunath and Sequeira, Dilip and Sirasao, Ashish and Sun, Fei and Tang, Hanlin and Thomson, Michael and Wei, Frank and Wu, Ephrem and Xu, Lingjie and Yamada, Koichi and Yu, Bing and Yuan, George and Zhong, Aaron and Zhang, Peizhao and Zhou, Yuchen},
 title = {{MLPerf} Inference Benchmark},
 year = {2020},
 booktitle = {2020 ACM/IEEE 47th Annual International Symposium on Computer Architecture (ISCA)},
 pages = {446--459},
 organization = {IEEE},
 doi = {10.1109/isca45697.2020.00045},
 source = {Crossref},
 url = {https://doi.org/10.1109/isca45697.2020.00045},
 publisher = {IEEE},
 month = may,
}

@inproceedings{ribeiro2016should,
 author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
 title = {{\textquotedblright} Why should i trust you?{\textquotedblright} Explaining the predictions of any classifier},
 year = {2016},
 booktitle = {Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining},
 pages = {1135--1144},
}

@misc{rmsprop,
 author = {Hinton, Geoffrey},
 title = {Overview of Minibatch Gradient Descent},
 year = {2017},
 institution = {University of Toronto},
 howpublished = {University Lecture},
}

@inproceedings{Rombach22cvpr,
 author = {Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bjorn},
 title = {High-Resolution Image Synthesis with Latent Diffusion Models},
 year = {2022},
 booktitle = {2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
 url = {https://doi.org/10.1109/cvpr52688.2022.01042},
 doi = {10.1109/cvpr52688.2022.01042},
 source = {Crossref},
 publisher = {IEEE},
 month = jun,
}

@book{rosenblatt1957perceptron,
 author = {Rosenblatt, Frank},
 title = {The perceptron, a perceiving and recognizing automaton Project Para},
 year = {1957},
 publisher = {Cornell Aeronautical Laboratory},
}

@article{roskies2002neuroethics,
 author = {Roskies, Adina},
 title = {Neuroethics for the New Millenium},
 year = {2002},
 journal = {Neuron},
 publisher = {Elsevier BV},
 volume = {35},
 number = {1},
 pages = {21--23},
 doi = {10.1016/s0896-6273(02)00763-8},
 source = {Crossref},
 url = {https://doi.org/10.1016/s0896-6273(02)00763-8},
 issn = {0896-6273},
 month = jul,
}

@inproceedings{ross2018improving,
 author = {Ross, Andrew and Doshi-Velez, Finale},
 title = {Improving the adversarial robustness and interpretability of deep neural networks by regularizing their input gradients},
 year = {2018},
 booktitle = {Proceedings of the AAAI conference on artificial intelligence},
 volume = {32},
 number = {1},
}

@inproceedings{rouhani2017tinydl,
 author = {Darvish Rouhani, Bita and Mirhoseini, Azalia and Koushanfar, Farinaz},
 title = {{TinyDL:} {Just-in-time} deep learning solution for constrained embedded systems},
 year = {2017},
 month = may,
 pages = {1--4},
 doi = {10.1109/iscas.2017.8050343},
 bdsk-url-1 = {https://doi.org/10.1109/ISCAS.2017.8050343},
 source = {Crossref},
 url = {https://doi.org/10.1109/iscas.2017.8050343},
 booktitle = {2017 IEEE International Symposium on Circuits and Systems (ISCAS)},
 publisher = {IEEE},
}

@article{ruder2016overview,
 author = {Ruder, Sebastian},
 title = {An overview of gradient descent optimization algorithms},
 year = {2016},
 journal = {arXiv preprint arXiv:1609.04747},
}

@article{rudin2019stop,
 author = {Rudin, Cynthia},
 title = {Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead},
 year = {2019},
 journal = {Nature Machine Intelligence},
 publisher = {Springer Science and Business Media LLC},
 volume = {1},
 number = {5},
 pages = {206--215},
 doi = {10.1038/s42256-019-0048-x},
 source = {Crossref},
 url = {https://doi.org/10.1038/s42256-019-0048-x},
 issn = {2522-5839},
 month = may,
}

@article{rumelhart1986learning,
 author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
 title = {Learning representations by back-propagating errors},
 year = {1986},
 journal = {Nature},
 publisher = {Springer Science and Business Media LLC},
 volume = {323},
 number = {6088},
 pages = {533--536},
 doi = {10.1038/323533a0},
 source = {Crossref},
 url = {https://doi.org/10.1038/323533a0},
 issn = {0028-0836, 1476-4687},
 month = oct,
}

@article{russell2021human,
 author = {Russell, Stuart},
 title = {Human-compatible artificial intelligence},
 year = {2021},
 journal = {Human-like machine intelligence},
 publisher = {Oxford University Press Oxford},
 pages = {3--23},
}

@article{ruvolo_ella_nodate,
 author = {Ruvolo, Paul and Eaton, Eric},
 title = {{ELLA:} {An} Efficient Lifelong Learning Algorithm},
 file = {Ruvolo and Eaton - ELLA An Efficient Lifelong Learning Algorithm.pdf:/Users/alex/Zotero/storage/QA5G29GL/Ruvolo and Eaton - ELLA An Efficient Lifelong Learning Algorithm.pdf:application/pdf},
 language = {en},
}

@article{ryan2000self,
 author = {Ryan, Richard M. and Deci, Edward L.},
 title = {Self-determination theory and the facilitation of intrinsic motivation, social development, and well-being.},
 year = {2000},
 journal = {Am. Psychol.},
 publisher = {American Psychological Association (APA)},
 volume = {55},
 number = {1},
 pages = {68--78},
 doi = {10.1037/0003-066x.55.1.68},
 source = {Crossref},
 url = {https://doi.org/10.1037/0003-066x.55.1.68},
 issn = {1935-990X, 0003-066X},
}

@article{samajdar2018scale,
 author = {Samajdar, Ananda and Zhu, Yuhao and Whatmough, Paul and Mattina, Matthew and Krishna, Tushar},
 title = {Scale-sim: {Systolic} cnn accelerator simulator},
 year = {2018},
 journal = {arXiv preprint arXiv:1811.02883},
}

@misc{ScaleAI,
 journal = {ScaleAI},
 url = {https://scale.com/data-engine},
 bdsk-url-1 = {https://scale.com/data-engine},
}

@article{scaling_laws_NLM,
 author = {Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B. and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
 title = {Scaling Laws for Neural Language Models},
 year = {2020},
 journal = {CoRR},
 volume = {abs/2001.08361},
 url = {https://arxiv.org/abs/2001.08361},
 eprinttype = {arXiv},
 eprint = {2001.08361},
 timestamp = {Wed, 03 Jun 2020 10:55:13 +0200},
 biburl = {https://dblp.org/rec/journals/corr/abs-2001-08361.bib},
 bibsource = {dblp computer science bibliography, https://dblp.org},
}

@article{schafer2023notorious,
 author = {Sch\"afer, Mike S.},
 title = {The Notorious {GPT:} {Science} communication in the age of artificial intelligence},
 year = {2023},
 journal = {Journal of Science Communication},
 publisher = {Sissa Medialab Srl},
 volume = {22},
 number = {02},
 pages = {Y02},
 doi = {10.22323/2.22020402},
 source = {Crossref},
 url = {https://doi.org/10.22323/2.22020402},
 issn = {1824-2049},
 month = may,
}

@article{schuman2022,
 author = {Schuman, Catherine D. and Kulkarni, Shruti R. and Parsa, Maryam and Mitchell, J. Parker and Date, Prasanna and Kay, Bill},
 title = {Opportunities for neuromorphic computing algorithms and applications},
 year = {2022},
 journal = {Nature Computational Science},
 publisher = {Springer Science and Business Media LLC},
 volume = {2},
 number = {1},
 pages = {10--19},
 doi = {10.1038/s43588-021-00184-y},
 source = {Crossref},
 url = {https://doi.org/10.1038/s43588-021-00184-y},
 issn = {2662-8457},
 month = jan,
}

@article{schwartz2020green,
 author = {Schwartz, Roy and Dodge, Jesse and Smith, Noah A. and Etzioni, Oren},
 title = {Green {AI}},
 year = {2020},
 journal = {Commun. ACM},
 publisher = {Association for Computing Machinery (ACM)},
 volume = {63},
 number = {12},
 pages = {54--63},
 doi = {10.1145/3381831},
 source = {Crossref},
 url = {https://doi.org/10.1145/3381831},
 issn = {0001-0782, 1557-7317},
 month = nov,
}

@inproceedings{schwartz2021deployment,
 author = {Schwartz, Daniel and Selman, Jonathan Michael Gomes and Wrege, Peter and Paepcke, Andreas},
 title = {Deployment of Embedded Edge-{AI} for Wildlife Monitoring in Remote Regions},
 year = {2021},
 booktitle = {2021 20th IEEE International Conference on Machine Learning and Applications (ICMLA)},
 pages = {1035--1042},
 organization = {IEEE},
 doi = {10.1109/icmla52953.2021.00170},
 source = {Crossref},
 url = {https://doi.org/10.1109/icmla52953.2021.00170},
 publisher = {IEEE},
 month = dec,
}

@inproceedings{schwartzDeploymentEmbeddedEdgeAI2021,
 author = {Schwartz, Daniel and Selman, Jonathan Michael Gomes and Wrege, Peter and Paepcke, Andreas},
 title = {Deployment of Embedded Edge-{AI} for Wildlife Monitoring in Remote Regions},
 year = {2021},
 booktitle = {2021 20th IEEE International Conference on Machine Learning and Applications (ICMLA)},
 pages = {1035--1042},
 organization = {IEEE},
 doi = {10.1109/icmla52953.2021.00170},
 source = {Crossref},
 url = {https://doi.org/10.1109/icmla52953.2021.00170},
 publisher = {IEEE},
 month = dec,
}

@inproceedings{schwarzschild2021just,
 author = {Schwarzschild, Avi and Goldblum, Micah and Gupta, Arjun and Dickerson, John P and Goldstein, Tom},
 title = {Just how toxic is data poisoning? a unified benchmark for backdoor and data poisoning attacks},
 year = {2021},
 booktitle = {International Conference on Machine Learning},
 pages = {9389--9398},
 organization = {PMLR},
}

@inproceedings{sculley2015hidden,
 author = {Sambasivan, Nithya and Kapania, Shivani and Highfill, Hannah and Akrong, Diana and Paritosh, Praveen and Aroyo, Lora M},
 title = {{{\textquotedblleft}Everyone} wants to do the model work, not the data work{\textquotedblright}: {Data} Cascades in High-Stakes {AI}},
 year = {2021},
 doi = {10.1145/3411764.3445518},
 source = {Crossref},
 url = {https://doi.org/10.1145/3411764.3445518},
 booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
 publisher = {ACM},
 month = may,
}

@misc{see_compression_2016,
 author = {See, Abigail and Luong, Minh-Thang and Manning, Christopher D.},
 title = {Compression of Neural Machine Translation Models via Pruning},
 year = {2016},
 month = jun,
 publisher = {arXiv},
 doi = {10.48550/arXiv.1606.09274},
 url = {http://arxiv.org/abs/1606.09274},
 urldate = {2023-10-20},
 note = {arXiv:1606.09274 [cs]},
 abstract = {Neural Machine Translation (NMT), like many other deep learning domains, typically suffers from over-parameterization, resulting in large storage sizes. This paper examines three simple magnitude-based pruning schemes to compress NMT models, namely class-blind, class-uniform, and class-distribution, which differ in terms of how pruning thresholds are computed for the different classes of weights in the NMT architecture. We demonstrate the efficacy of weight pruning as a compression technique for a state-of-the-art NMT system. We show that an NMT model with over 200 million parameters can be pruned by 40\% with very little performance loss as measured on the WMT'14 English-German translation task. This sheds light on the distribution of redundancy in the NMT architecture. Our main result is that with retraining, we can recover and even surpass the original performance with an 80\%-pruned model.},
 file = {arXiv Fulltext PDF:/Users/jeffreyma/Zotero/storage/2CJ4TSNR/See et al. - 2016 - Compression of Neural Machine Translation Models v.pdf:application/pdf},
 keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Neural and Evolutionary Computing},
 bdsk-url-1 = {http://arxiv.org/abs/1606.09274},
 bdsk-url-2 = {https://doi.org/10.48550/arXiv.1606.09274},
}

@misc{segal1999opengl,
 author = {Segal, Mark and Akeley, Kurt},
 title = {The {OpenGL} graphics system: {A} specification (version 1.1)},
 year = {1999},
}

@article{segura2018ethical,
 author = {Segura Anaya, L. H. and Alsadoon, Abeer and Costadopoulos, N. and Prasad, P. W. C.},
 title = {Ethical Implications of User Perceptions of Wearable Devices},
 year = {2017},
 journal = {Sci. Eng. Ethics},
 publisher = {Springer Science and Business Media LLC},
 volume = {24},
 pages = {1--28},
 doi = {10.1007/s11948-017-9872-8},
 number = {1},
 source = {Crossref},
 url = {https://doi.org/10.1007/s11948-017-9872-8},
 issn = {1353-3452, 1471-5546},
 month = feb,
}

@inproceedings{seide2016cntk,
 author = {Seide, Frank and Agarwal, Amit},
 title = {Cntk},
 year = {2016},
 booktitle = {Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
 pages = {2135--2135},
 doi = {10.1145/2939672.2945397},
 source = {Crossref},
 url = {https://doi.org/10.1145/2939672.2945397},
 publisher = {ACM},
 subtitle = {Microsoft's Open-Source Deep-Learning Toolkit},
 month = aug,
}

@inproceedings{selvaraju2017grad,
 author = {Selvaraju, Ramprasaath R. and Cogswell, Michael and Das, Abhishek and Vedantam, Ramakrishna and Parikh, Devi and Batra, Dhruv},
 title = {Grad-{CAM:} {Visual} Explanations from Deep Networks via Gradient-Based Localization},
 year = {2017},
 booktitle = {2017 IEEE International Conference on Computer Vision (ICCV)},
 pages = {618--626},
 doi = {10.1109/iccv.2017.74},
 source = {Crossref},
 url = {https://doi.org/10.1109/iccv.2017.74},
 publisher = {IEEE},
 month = oct,
}

@misc{sevilla_compute_2022,
 author = {Sevilla, Jaime and Heim, Lennart and Ho, Anson and Besiroglu, Tamay and Hobbhahn, Marius and Villalobos, Pablo},
 title = {Compute Trends Across Three Eras of Machine Learning},
 year = {2022},
 month = mar,
 publisher = {arXiv},
 url = {http://arxiv.org/abs/2202.05924},
 urldate = {2023-10-25},
 note = {arXiv:2202.05924 [cs]},
 file = {Sevilla et al. - 2022 - Compute Trends Across Three Eras of Machine Learni.pdf:/Users/alex/Zotero/storage/24N9RZ72/Sevilla et al. - 2022 - Compute Trends Across Three Eras of Machine Learni.pdf:application/pdf},
 keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computers and Society},
 language = {en},
 bdsk-url-1 = {http://arxiv.org/abs/2202.05924},
}

@article{seyedzadeh2018machine,
 author = {Seyedzadeh, Saleh and Rahimian, Farzad Pour and Glesk, Ivan and Roper, Marc},
 title = {Machine learning for estimation of building energy consumption and performance: {A} review},
 year = {2018},
 journal = {Visualization in Engineering},
 publisher = {Springer Science and Business Media LLC},
 volume = {6},
 pages = {1--20},
 doi = {10.1186/s40327-018-0064-7},
 number = {1},
 source = {Crossref},
 url = {https://doi.org/10.1186/s40327-018-0064-7},
 issn = {2213-7459},
 month = oct,
}

@article{sgd,
 author = {Robbins, Herbert and Monro, Sutton},
 title = {A Stochastic Approximation Method},
 year = {1951},
 journal = {The Annals of Mathematical Statistics},
 url = {https://doi.org/10.1214/aoms/1177729586},
 doi = {10.1214/aoms/1177729586},
 number = {3},
 source = {Crossref},
 volume = {22},
 publisher = {Institute of Mathematical Statistics},
 issn = {0003-4851},
 pages = {400--407},
 month = sep,
}

@article{shalev2017formal,
 author = {Shalev-Shwartz, Shai and Shammah, Shaked and Shashua, Amnon},
 title = {On a formal model of safe and scalable self-driving cars},
 year = {2017},
 journal = {arXiv preprint arXiv:1708.06374},
}

@article{shamir1979share,
 author = {Shamir, Adi},
 title = {How to share a secret},
 year = {1979},
 journal = {Commun. ACM},
 publisher = {Association for Computing Machinery (ACM)},
 volume = {22},
 number = {11},
 pages = {612--613},
 doi = {10.1145/359168.359176},
 source = {Crossref},
 url = {https://doi.org/10.1145/359168.359176},
 issn = {0001-0782, 1557-7317},
 month = nov,
}

@article{shan2023prompt,
 author = {Shan, Shawn and Ding, Wenxin and Passananti, Josephine and Zheng, Haitao and Zhao, Ben Y},
 title = {Prompt-Specific Poisoning Attacks on Text-to-Image Generative Models},
 year = {2023},
 journal = {arXiv preprint arXiv:2310.13828},
}

@article{shastri2021photonics,
 author = {Shastri, Bhavin J. and Tait, Alexander N. and Ferreira de Lima, T. and Pernice, Wolfram H. P. and Bhaskaran, Harish and Wright, C. D. and Prucnal, Paul R.},
 title = {Photonics for artificial intelligence and neuromorphic computing},
 year = {2021},
 journal = {Nat. Photonics},
 publisher = {Springer Science and Business Media LLC},
 volume = {15},
 number = {2},
 pages = {102--114},
 doi = {10.1038/s41566-020-00754-y},
 source = {Crossref},
 url = {https://doi.org/10.1038/s41566-020-00754-y},
 issn = {1749-4885, 1749-4893},
 month = jan,
}

@article{Sheng_Zhang_2019,
 author = {Sheng, Victor S. and Zhang, Jing},
 title = {Machine Learning with Crowdsourcing: {A} Brief Summary of the Past Research and Future Directions},
 year = {2019},
 journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
 volume = {33},
 number = {01},
 pages = {9837--9843},
 doi = {10.1609/aaai.v33i01.33019837},
 bdsk-url-1 = {https://doi.org/10.1609/aaai.v33i01.33019837},
 source = {Crossref},
 url = {https://doi.org/10.1609/aaai.v33i01.33019837},
 publisher = {Association for the Advancement of Artificial Intelligence (AAAI)},
 issn = {2374-3468, 2159-5399},
 month = jul,
}

@misc{Sheth_2022,
 author = {Sheth, Dhruv},
 title = {Eletect - {TinyML} and {IOT} based Smart Wildlife Tracker},
 year = {2022},
 month = mar,
 journal = {Hackster.io},
 url = {https://www.hackster.io/dhruvsheth_/eletect-tinyml-and-iot-based-smart-wildlife-tracker-c03e5a},
 bdsk-url-1 = {https://www.hackster.io/dhruvsheth\_/eletect-tinyml-and-iot-based-smart-wildlife-tracker-c03e5a},
}

@inproceedings{shi2022data,
 author = {Shi, Hongrui and Radu, Valentin},
 title = {Data selection for efficient model update in federated learning},
 year = {2022},
 booktitle = {Proceedings of the 2nd European Workshop on Machine Learning and Systems},
 pages = {72--78},
 doi = {10.1145/3517207.3526980},
 source = {Crossref},
 url = {https://doi.org/10.1145/3517207.3526980},
 publisher = {ACM},
 month = apr,
}

@book{shneiderman2022human,
 author = {Shneiderman, Ben},
 title = {Human-centered {AI}},
 year = {2022},
 publisher = {Oxford University Press},
}

@inproceedings{shokri2017membership,
 author = {Shokri, Reza and Stronati, Marco and Song, Congzheng and Shmatikov, Vitaly},
 title = {Membership Inference Attacks Against Machine Learning Models},
 year = {2017},
 booktitle = {2017 IEEE Symposium on Security and Privacy (SP)},
 pages = {3--18},
 organization = {IEEE},
 doi = {10.1109/sp.2017.41},
 source = {Crossref},
 url = {https://doi.org/10.1109/sp.2017.41},
 publisher = {IEEE},
 month = may,
}

@article{silvestro2022improving,
 author = {Silvestro, Daniele and Goria, Stefano and Sterner, Thomas and Antonelli, Alexandre},
 title = {Improving biodiversity protection through artificial intelligence},
 year = {2022},
 journal = {Nature Sustainability},
 publisher = {Springer Science and Business Media LLC},
 volume = {5},
 number = {5},
 pages = {415--424},
 doi = {10.1038/s41893-022-00851-6},
 source = {Crossref},
 url = {https://doi.org/10.1038/s41893-022-00851-6},
 issn = {2398-9629},
 month = mar,
}

@inproceedings{skorobogatov2003optical,
 author = {Skorobogatov, Sergei P and Anderson, Ross J},
 title = {Optical fault induction attacks},
 year = {2003},
 booktitle = {Cryptographic Hardware and Embedded Systems-CHES 2002: 4th International Workshop Redwood Shores, CA, USA, August 13{\textendash}15, 2002 Revised Papers 4},
 pages = {2--12},
 organization = {Springer},
}

@inproceedings{skorobogatov2009local,
 author = {Skorobogatov, Sergei},
 title = {Local heating attacks on Flash memory devices},
 year = {2009},
 booktitle = {2009 IEEE International Workshop on Hardware-Oriented Security and Trust},
 pages = {1--6},
 organization = {IEEE},
 doi = {10.1109/hst.2009.5225028},
 source = {Crossref},
 url = {https://doi.org/10.1109/hst.2009.5225028},
 publisher = {IEEE},
}

@article{smestad2023systematic,
 author = {Smestad, Carl and Li, Jingyue},
 title = {A Systematic Literature Review on Client Selection in Federated Learning},
 year = {2023},
 journal = {arXiv preprint arXiv:2306.04862},
}

@article{smilkov2017smoothgrad,
 author = {Smilkov, Daniel and Thorat, Nikhil and Kim, Been and Vi\'egas, Fernanda and Wattenberg, Martin},
 title = {Smoothgrad: {Removing} noise by adding noise},
 year = {2017},
 journal = {arXiv preprint arXiv:1706.03825},
}

@misc{smoothquant,
 author = {Xiao and Lin, Seznec and Wu, Demouth and Han},
 title = {{SmoothQuant:} {Accurate} and Efficient Post-Training Quantization for Large Language Models},
 year = {2023},
 doi = {10.48550/arXiv.2211.10438},
 url = {https://arxiv.org/abs/2211.10438},
 urldate = {2023-06-05},
 abstract = {Large language models (LLMs) show excellent performance but are compute- and memory-intensive. Quantization can reduce memory and accelerate inference. However, existing methods cannot maintain accuracy and hardware efficiency at the same time. We propose SmoothQuant, a training-free, accuracy-preserving, and general-purpose post-training quantization (PTQ) solution to enable 8-bit weight, 8-bit activation (W8A8) quantization for LLMs. Based on the fact that weights are easy to quantize while activations are not, SmoothQuant smooths the activation outliers by offline migrating the quantization difficulty from activations to weights with a mathematically equivalent transformation. SmoothQuant enables an INT8 quantization of both weights and activations for all the matrix multiplications in LLMs, including OPT, BLOOM, GLM, MT-NLG, and LLaMA family. We demonstrate up to 1.56x speedup and 2x memory reduction for LLMs with negligible loss in accuracy. SmoothQuant enables serving 530B LLM within a single node. Our work offers a turn-key solution that reduces hardware costs and democratizes LLMs.},
 bdsk-url-1 = {https://arxiv.org/abs/2211.10438},
 bdsk-url-2 = {https://doi.org/10.48550/arXiv.2211.10438},
}

@article{soufleri2023synthetic,
 author = {Soufleri, Efstathia and Saha, Gobinda and Roy, Kaushik},
 title = {Synthetic Dataset Generation for Privacy-Preserving Machine Learning},
 year = {2023},
 journal = {arXiv preprint arXiv:2210.03205},
 date-added = {2023-11-22 19:26:18 -0500},
 date-modified = {2023-11-22 19:26:57 -0500},
}

@misc{strubell2019energy,
 author = {Strubell, Emma and Ganesh, Ananya and McCallum, Andrew},
 title = {Energy and Policy Considerations for Deep Learning in {NLP}},
 year = {2019},
 eprint = {1906.02243},
 archiveprefix = {arXiv},
 primaryclass = {cs.CL},
}

@article{strubellEnergyPolicyConsiderations2019,
 author = {Strubell, Emma and Ganesh, Ananya and McCallum, Andrew},
 shorttitle = {Energy and Policy Considerations for Deep Learning in NLP},
 year = {2019},
 doi = {10.48550/ARXIV.1906.02243},
 url = {https://arxiv.org/abs/1906.02243},
 urldate = {2023-12-06},
 copyright = {arXiv.org perpetual, non-exclusive license},
 note = {Publisher: arXiv Version Number: 1},
 annote = {Other In the 57th Annual Meeting of the Association for Computational Linguistics (ACL). Florence, Italy. July 2019},
}

@inproceedings{suda2016throughput,
 author = {Suda, Naveen and Chandra, Vikas and Dasika, Ganesh and Mohanty, Abinash and Ma, Yufei and Vrudhula, Sarma and Seo, Jae-sun and Cao, Yu},
 title = {Throughput-Optimized {OpenCL}-based {FPGA} Accelerator for Large-Scale Convolutional Neural Networks},
 year = {2016},
 booktitle = {Proceedings of the 2016 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays},
 pages = {16--25},
 doi = {10.1145/2847263.2847276},
 source = {Crossref},
 url = {https://doi.org/10.1145/2847263.2847276},
 publisher = {ACM},
 month = feb,
}

@misc{surveyofquant,
 author = {Gholami and Kim, Dong and Yao, Mahoney and Keutzer},
 title = {A Survey of Quantization Methods for Efficient Neural Network Inference)},
 year = {2021},
 doi = {10.48550/arXiv.2103.13630},
 url = {https://arxiv.org/abs/2103.13630},
 urldate = {2021-06-21},
 abstract = {As soon as abstract mathematical computations were adapted to computation on digital computers, the problem of efficient representation, manipulation, and communication of the numerical values in those computations arose. Strongly related to the problem of numerical representation is the problem of quantization: in what manner should a set of continuous real-valued numbers be distributed over a fixed discrete set of numbers to minimize the number of bits required and also to maximize the accuracy of the attendant computations? This perennial problem of quantization is particularly relevant whenever memory and/or computational resources are severely restricted, and it has come to the forefront in recent years due to the remarkable performance of Neural Network models in computer vision, natural language processing, and related areas. Moving from floating-point representations to low-precision fixed integer values represented in four bits or less holds the potential to reduce the memory footprint and latency by a factor of 16x; and, in fact, reductions of 4x to 8x are often realized in practice in these applications. Thus, it is not surprising that quantization has emerged recently as an important and very active sub-area of research in the efficient implementation of computations associated with Neural Networks. In this article, we survey approaches to the problem of quantizing the numerical values in deep Neural Network computations, covering the advantages/disadvantages of current methods. With this survey and its organization, we hope to have presented a useful snapshot of the current research in quantization for Neural Networks and to have given an intelligent organization to ease the evaluation of future research in this area.},
 bdsk-url-1 = {https://arxiv.org/abs/2103.13630},
 bdsk-url-2 = {https://doi.org/10.48550/arXiv.2103.13630},
}

@article{Sze2017-ak,
 author = {Sze, Vivienne and Chen, Yu-Hsin and Yang, Tien-Ju and Emer, Joel S.},
 title = {Efficient Processing of Deep Neural Networks: {A} Tutorial and Survey},
 year = {2017},
 month = dec,
 copyright = {http://arxiv.org/licenses/nonexclusive-distrib/1.0/},
 abstract = {Deep neural networks (DNNs) are currently widely used for many artificial intelligence (AI) applications including computer vision, speech recognition, and robotics. While DNNs deliver state-of-the-art accuracy on many AI tasks, it comes at the cost of high computational complexity. Accordingly, techniques that enable efficient processing of DNNs to improve energy efficiency and throughput without sacrificing application accuracy or increasing hardware cost are critical to the wide deployment of DNNs in AI systems. This article aims to provide a comprehensive tutorial and survey about the recent advances towards the goal of enabling efficient processing of DNNs. Specifically, it will provide an overview of DNNs, discuss various hardware platforms and architectures that support DNNs, and highlight key trends in reducing the computation cost of DNNs either solely via hardware design changes or via joint hardware design and DNN algorithm changes. It will also summarize various development resources that enable researchers and practitioners to quickly get started in this field, and highlight important benchmarking metrics and design considerations that should be used for evaluating the rapidly growing number of DNN hardware designs, optionally including algorithmic co-designs, being proposed in academia and industry. The reader will take away the following concepts from this article: understand the key design considerations for DNNs; be able to evaluate different DNN hardware implementations with benchmarks and comparison metrics; understand the trade-offs between various hardware architectures and platforms; be able to evaluate the utility of various DNN design techniques for efficient processing; and understand recent implementation trends and opportunities.},
 archiveprefix = {arXiv},
 eprint = {1703.09039},
 primaryclass = {cs.CV},
 doi = {10.1109/jproc.2017.2761740},
 number = {12},
 source = {Crossref},
 url = {https://doi.org/10.1109/jproc.2017.2761740},
 volume = {105},
 journal = {Proc. IEEE},
 publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
 issn = {0018-9219, 1558-2256},
 pages = {2295--2329},
}

@article{sze2017efficient,
 author = {Sze, Vivienne and Chen, Yu-Hsin and Yang, Tien-Ju and Emer, Joel S.},
 title = {Efficient Processing of Deep Neural Networks: {A} Tutorial and Survey},
 year = {2017},
 journal = {Proc. IEEE},
 publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
 volume = {105},
 number = {12},
 pages = {2295--2329},
 doi = {10.1109/jproc.2017.2761740},
 source = {Crossref},
 url = {https://doi.org/10.1109/jproc.2017.2761740},
 issn = {0018-9219, 1558-2256},
 month = dec,
}

@article{szegedy2013intriguing,
 author = {Szegedy, Christian and Zaremba, Wojciech and Sutskever, Ilya and Bruna, Joan and Erhan, Dumitru and Goodfellow, Ian and Fergus, Rob},
 title = {Intriguing properties of neural networks},
 year = {2013},
 journal = {arXiv preprint arXiv:1312.6199},
}

@misc{tan_efficientnet_2020,
 author = {Tan, Mingxing and Le, Quoc V.},
 title = {{EfficientNet:} {Rethinking} Model Scaling for Convolutional Neural Networks},
 shorttitle = {EfficientNet},
 year = {2020},
 month = sep,
 publisher = {arXiv},
 doi = {10.48550/arXiv.1905.11946},
 url = {http://arxiv.org/abs/1905.11946},
 urldate = {2023-10-20},
 note = {arXiv:1905.11946 [cs, stat]},
 abstract = {Convolutional Neural Networks (ConvNets) are commonly developed at a fixed resource budget, and then scaled up for better accuracy if more resources are available. In this paper, we systematically study model scaling and identify that carefully balancing network depth, width, and resolution can lead to better performance. Based on this observation, we propose a new scaling method that uniformly scales all dimensions of depth/width/resolution using a simple yet highly effective compound coefficient. We demonstrate the effectiveness of this method on scaling up MobileNets and ResNet. To go even further, we use neural architecture search to design a new baseline network and scale it up to obtain a family of models, called EfficientNets, which achieve much better accuracy and efficiency than previous ConvNets. In particular, our EfficientNet-B7 achieves state-of-the-art 84.3\% top-1 accuracy on ImageNet, while being 8.4x smaller and 6.1x faster on inference than the best existing ConvNet. Our EfficientNets also transfer well and achieve state-of-the-art accuracy on CIFAR-100 (91.7\%), Flowers (98.8\%), and 3 other transfer learning datasets, with an order of magnitude fewer parameters. Source code is at https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet.},
 file = {arXiv Fulltext PDF:/Users/jeffreyma/Zotero/storage/KISBF35I/Tan and Le - 2020 - EfficientNet Rethinking Model Scaling for Convolu.pdf:application/pdf;arXiv.org Snapshot:/Users/jeffreyma/Zotero/storage/TUD4PH4M/1905.html:text/html},
 keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
 bdsk-url-1 = {http://arxiv.org/abs/1905.11946},
 bdsk-url-2 = {https://doi.org/10.48550/arXiv.1905.11946},
}

@inproceedings{tan2019mnasnet,
 author = {Tan, Mingxing and Chen, Bo and Pang, Ruoming and Vasudevan, Vijay and Sandler, Mark and Howard, Andrew and Le, Quoc V.},
 title = {{MnasNet:} {Platform-aware} Neural Architecture Search for Mobile},
 year = {2019},
 booktitle = {2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
 pages = {2820--2828},
 doi = {10.1109/cvpr.2019.00293},
 source = {Crossref},
 url = {https://doi.org/10.1109/cvpr.2019.00293},
 publisher = {IEEE},
 month = jun,
}

@misc{tan2020efficientnet,
 author = {Tan, Mingxing and Le, Quoc V.},
 title = {Demystifying Deep Learning},
 year = {2023},
 archiveprefix = {arXiv},
 eprint = {1905.11946},
 primaryclass = {cs.LG},
 doi = {10.1002/9781394205639.ch6},
 source = {Crossref},
 url = {https://doi.org/10.1002/9781394205639.ch6},
 publisher = {Wiley},
 isbn = {9781394205608, 9781394205639},
 pages = {111--131},
 month = dec,
}

@article{tang2022soft,
 author = {Tang, Xin and He, Yichun and Liu, Jia},
 title = {Soft bioelectronics for cardiac interfaces},
 year = {2022},
 journal = {Biophysics Reviews},
 publisher = {AIP Publishing},
 volume = {3},
 number = {1},
 doi = {10.1063/5.0069516},
 source = {Crossref},
 url = {https://doi.org/10.1063/5.0069516},
 issn = {2688-4089},
 month = jan,
}

@article{tang2023flexible,
 author = {Tang, Xin and Shen, Hao and Zhao, Siyuan and Li, Na and Liu, Jia},
 title = {Flexible brain{\textendash}computer interfaces},
 year = {2023},
 journal = {Nature Electronics},
 publisher = {Springer Science and Business Media LLC},
 volume = {6},
 number = {2},
 pages = {109--118},
 doi = {10.1038/s41928-022-00913-9},
 source = {Crossref},
 url = {https://doi.org/10.1038/s41928-022-00913-9},
 issn = {2520-1131},
 month = feb,
}

@article{tarun2023deep,
 author = {Tarun, Ayush K and Chundawat, Vikram S and Mandal, Murari and Kankanhalli, Mohan},
 title = {Deep Regression Unlearning},
 year = {2023},
 journal = {arXiv preprint arXiv:2210.08196},
 date-added = {2023-11-22 19:20:59 -0500},
 date-modified = {2023-11-22 19:21:59 -0500},
}

@misc{Team_2023,
 author = {Team, Snorkel},
 title = {Data-centric {AI} for the Enterprise},
 year = {2023},
 month = aug,
 journal = {Snorkel AI},
 url = {https://snorkel.ai/},
 bdsk-url-1 = {https://snorkel.ai/},
}

@misc{Thefutur92:online,
 author = {ARM.com},
 title = {The future is being built on Arm: {Market} diversification continues to drive strong royalty and licensing growth as ecosystem reaches quarter of a trillion chips milestone {\textendash} Arm{\textregistered}},
 note = {(Accessed on 09/16/2023)},
 howpublished = {https://www.arm.com/company/news/2023/02/arm-announces-q3-fy22-results},
}

@misc{threefloat,
 author = {Google},
 title = {Three Floating Point Formats},
 year = {2023},
 url = {https://storage.googleapis.com/gweb-cloudblog-publish/images/Three_floating-point_formats.max-624x261.png},
 urldate = {2023-10-20},
 bdsk-url-1 = {https://storage.googleapis.com/gweb-cloudblog-publish/images/Three\_floating-point\_formats.max-624x261.png},
}

@article{tillFishDieoffsAre2019,
 author = {Till, Aaron and Rypel, Andrew L. and Bray, Andrew and Fey, Samuel B.},
 title = {Fish die-offs are concurrent with thermal extremes in north temperate lakes},
 year = {2019},
 journal = {Nat. Clim. Change},
 publisher = {Springer Science and Business Media LLC},
 volume = {9},
 number = {8},
 pages = {637--641},
 doi = {10.1038/s41558-019-0520-y},
 source = {Crossref},
 url = {https://doi.org/10.1038/s41558-019-0520-y},
 issn = {1758-678X, 1758-6798},
 month = jul,
}

@article{tirtalistyani2022indonesia,
 author = {Tirtalistyani, Rose and Murtiningrum, Murtiningrum and Kanwar, Rameshwar S.},
 title = {{Indonesia} Rice Irrigation System: {Time} for Innovation},
 year = {2022},
 journal = {Sustainability},
 publisher = {MDPI AG},
 volume = {14},
 number = {19},
 pages = {12477},
 doi = {10.3390/su141912477},
 source = {Crossref},
 url = {https://doi.org/10.3390/su141912477},
 issn = {2071-1050},
 month = sep,
}

@inproceedings{tokui2015chainer,
 author = {Tokui, Seiya and Okuta, Ryosuke and Akiba, Takuya and Niitani, Yusuke and Ogawa, Toru and Saito, Shunta and Suzuki, Shuji and Uenishi, Kota and Vogel, Brian and Yamazaki Vincent, Hiroyuki},
 title = {Chainer},
 year = {2019},
 booktitle = {Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery \&amp; Data Mining},
 volume = {5},
 pages = {1--6},
 doi = {10.1145/3292500.3330756},
 source = {Crossref},
 url = {https://doi.org/10.1145/3292500.3330756},
 publisher = {ACM},
 subtitle = {A Deep Learning Framework for Accelerating the Research Cycle},
 month = jul,
}

@inproceedings{tramer2019adversarial,
 author = {Tram\`er, Florian and Dupr\'e, Pascal and Rusak, Gili and Pellegrino, Giancarlo and Boneh, Dan},
 title = {{AdVersarial}},
 year = {2019},
 booktitle = {Proceedings of the 2019 ACM SIGSAC Conference on Computer and Communications Security},
 pages = {2005--2021},
 doi = {10.1145/3319535.3354222},
 source = {Crossref},
 url = {https://doi.org/10.1145/3319535.3354222},
 publisher = {ACM},
 subtitle = {Perceptual Ad Blocking meets Adversarial Machine Learning},
 month = nov,
}

@article{uddin_energy_2012,
 author = {Uddin, Mueen and Rahman, Azizah Abdul},
 title = {Energy efficiency and low carbon enabler green {IT} framework for data centers considering green metrics},
 year = {2012},
 journal = {Renewable Sustainable Energy Rev.},
 volume = {16},
 number = {6},
 pages = {4078--4094},
 doi = {10.1016/j.rser.2012.03.014},
 issn = {1364-0321},
 url = {https://doi.org/10.1016/j.rser.2012.03.014},
 abstract = {The increasing demand for storage, networking and computation has driven intensification of large complex data centers that run many of today's Internet, financial, commercial and business applications. A data center comprises of many thousands of servers and can use as much energy as small city. Massive amount of computation power is required to drive and run these server farms resulting in many challenging like huge energy consumptions, emission of green house gases, backups and recovery; This paper proposes energy efficiency and low carbon enabler green IT framework for these large and complex server farms to save consumption of electricity and reduce the emission of green house gases to lower the effects of global warming. The framework uses latest energy saving techniques like virtualization, cloud computing and green metrics to achieve greener data centers. It comprises of five phase to properly implement green IT techniques to achieve green data centers. The proposed framework seamlessly divides data center components into different resource pools and then applies green metrics like Power Usage Effectiveness, Data Center Effectiveness and Carbon Emission Calculator to measure performance of individual components so that benchmarking values can be achieved and set as standard to be followed by data centers.},
 keywords = {Energy efficiency, Energy efficient data centers, Global warming, Green IT, Green IT framework, Green metrics, Virtualization},
 source = {Crossref},
 publisher = {Elsevier BV},
 month = aug,
}

@book{un_world_economic_forum_2019,
 author = {Un and Forum, World Economic},
 title = {A New Circular Vision for Electronics, Time for a Global Reboot},
 year = {2019},
 month = dec,
 publisher = {PACE - Platform for Accelerating the Circular Economy},
 url = {https://www3.weforum.org/docs/WEF_A_New_Circular_Vision_for_Electronics.pdf},
}

@techreport{uptime,
 author = {Davis, Jacqueline and Bizo, Daniel and Lawrence, Andy and Rogers, Owen and Smolaks, Max},
 title = {Uptime Institute Global Data Center Survey 2022},
 year = {2022},
 institution = {Uptime Institute},
}

@article{USA_energy,
 author = {Shehabi, Arman and Smith, Sarah and Sartor, Dale and Brown, Richard and Herrlin, Magnus and Koomey, Jonathan and Masanet, Eric and Horner, Nathaniel and Azevedo, In\^es and Lintner, William},
 title = {United states data center energy usage report},
 institution = {Berkeley Laboratory},
 year = {2016},
}

@article{USA_footprint,
 author = {Siddik, Md Abu Bakar and Shehabi, Arman and Marston, Landon},
 title = {The environmental footprint of data centers in the United States},
 year = {2021},
 month = may,
 journal = {Environ. Res. Lett.},
 publisher = {IOP Publishing},
 volume = {16},
 number = {6},
 pages = {064017},
 doi = {10.1088/1748-9326/abfba1},
 issn = {1748-9326},
 url = {https://doi.org/10.1088/1748-9326/abfba1},
 source = {Crossref},
}

@article{van_de_ven_three_2022,
 author = {van de Ven, Gido M. and Tuytelaars, Tinne and Tolias, Andreas S.},
 title = {Three types of incremental learning},
 year = {2022},
 month = dec,
 journal = {Nature Machine Intelligence},
 volume = {4},
 number = {12},
 pages = {1185--1197},
 doi = {10.1038/s42256-022-00568-3},
 issn = {2522-5839},
 url = {https://doi.org/10.1038/s42256-022-00568-3},
 urldate = {2023-10-26},
 file = {Van De Ven et al. - 2022 - Three types of incremental learning.pdf:/Users/alex/Zotero/storage/5ZAHXMQN/Van De Ven et al. - 2022 - Three types of incremental learning.pdf:application/pdf},
 language = {en},
 bdsk-url-1 = {https://www.nature.com/articles/s42256-022-00568-3},
 bdsk-url-2 = {https://doi.org/10.1038/s42256-022-00568-3},
 source = {Crossref},
 publisher = {Springer Science and Business Media LLC},
}

@article{vaswani2017attention,
 author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
 title = {Attention is all you need},
 year = {2017},
 journal = {Adv Neural Inf Process Syst},
 volume = {30},
}

@misc{Vectorbo78:online,
 title = {Vector-borne diseases},
 note = {(Accessed on 10/17/2023)},
 howpublished = {https://www.who.int/news-room/fact-sheets/detail/vector-borne-diseases},
}

@misc{Verma_2022,
 author = {Verma, Team Dual\_Boot: Swapnil},
 title = {Elephant {AI}},
 year = {2022},
 month = mar,
 journal = {Hackster.io},
 url = {https://www.hackster.io/dual_boot/elephant-ai-ba71e9},
 bdsk-url-1 = {https://www.hackster.io/dual\_boot/elephant-ai-ba71e9},
}

@article{verma2019memory,
 author = {Verma, Naveen and Jia, Hongyang and Valavi, Hossein and Tang, Yinqi and Ozatay, Murat and Chen, Lung-Yen and Zhang, Bonan and Deaville, Peter},
 title = {In-Memory Computing: {Advances} and Prospects},
 year = {2019},
 journal = {IEEE Solid-State Circuits Mag.},
 publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
 volume = {11},
 number = {3},
 pages = {43--55},
 doi = {10.1109/mssc.2019.2922889},
 source = {Crossref},
 url = {https://doi.org/10.1109/mssc.2019.2922889},
 issn = {1943-0582, 1943-0590},
}

@misc{villalobos_machine_2022,
 author = {Villalobos, Pablo and Sevilla, Jaime and Besiroglu, Tamay and Heim, Lennart and Ho, Anson and Hobbhahn, Marius},
 title = {Machine Learning Model Sizes and the Parameter Gap},
 year = {2022},
 month = jul,
 publisher = {arXiv},
 url = {http://arxiv.org/abs/2207.02852},
 urldate = {2023-10-25},
 note = {arXiv:2207.02852 [cs]},
 file = {Villalobos et al. - 2022 - Machine Learning Model Sizes and the Parameter Gap.pdf:/Users/alex/Zotero/storage/WW69A82B/Villalobos et al. - 2022 - Machine Learning Model Sizes and the Parameter Gap.pdf:application/pdf},
 keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computers and Society, Computer Science - Computation and Language},
 language = {en},
 bdsk-url-1 = {http://arxiv.org/abs/2207.02852},
}

@misc{villalobos_trends_2022,
 author = {Villalobos, Pablo and Ho, Anson},
 title = {Trends in Training Dataset Sizes},
 year = {2022},
 month = sep,
 journal = {Epoch AI},
 url = {https://epochai.org/blog/trends-in-training-dataset-sizes},
 bdsk-url-1 = {https://epochai.org/blog/trends-in-training-dataset-sizes},
}

@misc{VinBrain,
 journal = {VinBrain},
 url = {https://vinbrain.net/aiscaler},
 bdsk-url-1 = {https://vinbrain.net/aiscaler},
}

@article{vinuesa2020role,
 author = {Vinuesa, Ricardo and Azizpour, Hossein and Leite, Iolanda and Balaam, Madeline and Dignum, Virginia and Domisch, Sami and Fell\"ander, Anna and Langhans, Simone Daniela and Tegmark, Max and Fuso Nerini, Francesco},
 title = {The role of artificial intelligence in achieving the Sustainable Development Goals},
 year = {2020},
 journal = {Nat. Commun.},
 publisher = {Springer Science and Business Media LLC},
 volume = {11},
 number = {1},
 pages = {1--10},
 doi = {10.1038/s41467-019-14108-y},
 source = {Crossref},
 url = {https://doi.org/10.1038/s41467-019-14108-y},
 issn = {2041-1723},
 month = jan,
}

@article{Vivet2021,
 author = {Vivet, Pascal and Guthmuller, Eric and Thonnart, Yvain and Pillonnet, Gael and Fuguet, Cesar and Miro-Panades, Ivan and Moritz, Guillaume and Durupt, Jean and Bernard, Christian and Varreau, Didier and Pontes, Julian and Thuries, Sebastien and Coriat, David and Harrand, Michel and Dutoit, Denis and Lattard, Didier and Arnaud, Lucile and Charbonnier, Jean and Coudrain, Perceval and Garnier, Arnaud and Berger, Frederic and Gueugnot, Alain and Greiner, Alain and Meunier, Quentin L. and Farcy, Alexis and Arriordaz, Alexandre and Cheramy, Severine and Clermidy, Fabien},
 title = {{IntAct:} {A} 96-Core Processor With Six Chiplets {3D}-Stacked on an Active Interposer With Distributed Interconnects and Integrated Power Management},
 year = {2021},
 journal = {IEEE J. Solid-State Circuits},
 volume = {56},
 number = {1},
 pages = {79--97},
 doi = {10.1109/jssc.2020.3036341},
 bdsk-url-1 = {https://doi.org/10.1109/JSSC.2020.3036341},
 source = {Crossref},
 url = {https://doi.org/10.1109/jssc.2020.3036341},
 publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
 issn = {0018-9200, 1558-173X},
 month = jan,
}

@article{wachter2017counterfactual,
 author = {Wachter, Sandra and Mittelstadt, Brent and Russell, Chris},
 title = {Counterfactual Explanations Without Opening the Black Box: {Automated} Decisions and the {GDPR}},
 year = {2017},
 journal = {SSRN Electronic Journal},
 publisher = {Elsevier BV},
 volume = {31},
 pages = {841},
 doi = {10.2139/ssrn.3063289},
 source = {Crossref},
 url = {https://doi.org/10.2139/ssrn.3063289},
 issn = {1556-5068},
}

@article{wald1987semiconductor,
 author = {Wald, Peter H. and Jones, Jeffrey R.},
 title = {Semiconductor manufacturing: {An} introduction to processes and hazards},
 year = {1987},
 journal = {Am. J. Ind. Med.},
 publisher = {Wiley},
 volume = {11},
 number = {2},
 pages = {203--221},
 doi = {10.1002/ajim.4700110209},
 source = {Crossref},
 url = {https://doi.org/10.1002/ajim.4700110209},
 issn = {0271-3586, 1097-0274},
 month = jan,
}

@article{waldSemiconductorManufacturingIntroduction1987,
 author = {Wald, Peter H. and Jones, Jeffrey R.},
 title = {Semiconductor manufacturing: {An} introduction to processes and hazards},
 year = {1987},
 journal = {Am. J. Ind. Med.},
 publisher = {Wiley},
 volume = {11},
 number = {2},
 pages = {203--221},
 doi = {10.1002/ajim.4700110209},
 source = {Crossref},
 url = {https://doi.org/10.1002/ajim.4700110209},
 issn = {0271-3586, 1097-0274},
 month = jan,
}

@inproceedings{wang2020apq,
 author = {Wang, Tianzhe and Wang, Kuan and Cai, Han and Lin, Ji and Liu, Zhijian and Wang, Hanrui and Lin, Yujun and Han, Song},
 title = {{APQ:} {Joint} Search for Network Architecture, Pruning and Quantization Policy},
 year = {2020},
 booktitle = {2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
 volume = {},
 number = {},
 pages = {2075--2084},
 doi = {10.1109/cvpr42600.2020.00215},
 bdsk-url-1 = {https://doi.org/10.1109/CVPR42600.2020.00215},
 source = {Crossref},
 url = {https://doi.org/10.1109/cvpr42600.2020.00215},
 publisher = {IEEE},
 month = jun,
}

@article{wang2022interpretability,
 author = {Wang, LingFeng and Zhan, YaQing},
 title = {A conceptual peer review model for {arXiv} and other preprint databases},
 year = {2019},
 journal = {Learn. Publ.},
 doi = {10.1002/leap.1229},
 number = {3},
 source = {Crossref},
 url = {https://doi.org/10.1002/leap.1229},
 volume = {32},
 publisher = {Wiley},
 issn = {0953-1513, 1741-4857},
 pages = {213--219},
 month = feb,
}

@article{warden2018speech,
 author = {Warden, Pete},
 title = {Speech commands: {A} dataset for limited-vocabulary speech recognition},
 year = {2018},
 journal = {arXiv preprint arXiv:1804.03209},
}

@book{warden2019tinyml,
 author = {Warden, Pete and Situnayake, Daniel},
 title = {Tinyml: {Machine} learning with tensorflow lite on arduino and ultra-low-power microcontrollers},
 year = {2019},
 publisher = {O'Reilly Media},
}

@article{wearableinsulin,
 author = {Psoma, Sotiria D. and Kanthou, Chryso},
 title = {Wearable Insulin Biosensors for Diabetes Management: {Advances} and Challenges},
 year = {2023},
 journal = {Biosensors},
 volume = {13},
 number = {7},
 doi = {10.3390/bios13070719},
 issn = {2079-6374},
 url = {https://doi.org/10.3390/bios13070719},
 article-number = {719},
 pubmedid = {37504117},
 bdsk-url-1 = {https://www.mdpi.com/2079-6374/13/7/719},
 bdsk-url-2 = {https://doi.org/10.3390/bios13070719},
 source = {Crossref},
 publisher = {MDPI AG},
 pages = {719},
 month = jul,
}

@inproceedings{weforum,
 author = {Challenge, WEF Net-Zero},
 title = {The Supply Chain Opportunity},
 booktitle = {World Economic Forum: Geneva, Switzerland},
 year = {2021},
}

@book{weik_survey_1955,
 author = {Weik, Martin H.},
 title = {A Survey of Domestic Electronic Digital Computing Systems},
 year = {1955},
 publisher = {Ballistic Research Laboratories},
 language = {en},
}

@article{weiss_survey_2016,
 author = {Weiss, Karl and Khoshgoftaar, Taghi M. and Wang, DingDing},
 title = {A survey of transfer learning},
 year = {2016},
 month = may,
 journal = {Journal of Big Data},
 volume = {3},
 number = {1},
 pages = {9},
 doi = {10.1186/s40537-016-0043-6},
 issn = {2196-1115},
 url = {https://doi.org/10.1186/s40537-016-0043-6},
 urldate = {2023-10-25},
 file = {Weiss et al. - 2016 - A survey of transfer learning.pdf:/Users/alex/Zotero/storage/3FN2Y6EA/Weiss et al. - 2016 - A survey of transfer learning.pdf:application/pdf},
 language = {en},
 bdsk-url-1 = {http://journalofbigdata.springeropen.com/articles/10.1186/s40537-016-0043-6},
 bdsk-url-2 = {https://doi.org/10.1186/s40537-016-0043-6},
 source = {Crossref},
 publisher = {Springer Science and Business Media LLC},
}

@article{wiener1960some,
 author = {Wiener, Norbert},
 title = {Some Moral and Technical Consequences of Automation},
 year = {1960},
 journal = {Science},
 publisher = {American Association for the Advancement of Science (AAAS)},
 volume = {131},
 number = {3410},
 pages = {1355--1358},
 doi = {10.1126/science.131.3410.1355},
 source = {Crossref},
 url = {https://doi.org/10.1126/science.131.3410.1355},
 subtitle = {As machines learn they may develop unforeseen strategies at rates that baffle their programmers.},
 issn = {0036-8075, 1095-9203},
 month = may,
}

@article{wong2012metal,
 author = {Wong, H.-S. Philip and Lee, Heng-Yuan and Yu, Shimeng and Chen, Yu-Sheng and Wu, Yi and Chen, Pang-Shiu and Lee, Byoungil and Chen, Frederick T. and Tsai, Ming-Jinn},
 title = {{Metal{\textendash}Oxide} {RRAM}},
 year = {2012},
 journal = {Proc. IEEE},
 publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
 volume = {100},
 number = {6},
 pages = {1951--1970},
 doi = {10.1109/jproc.2012.2190369},
 source = {Crossref},
 url = {https://doi.org/10.1109/jproc.2012.2190369},
 issn = {0018-9219, 1558-2256},
 month = jun,
}

@inproceedings{wong2018provable,
 author = {Wong, Eric and Kolter, Zico},
 title = {Provable defenses against adversarial examples via the convex outer adversarial polytope},
 year = {2018},
 booktitle = {International conference on machine learning},
 pages = {5286--5295},
 organization = {PMLR},
}

@inproceedings{wu2019fbnet,
 author = {Wu, Bichen and Keutzer, Kurt and Dai, Xiaoliang and Zhang, Peizhao and Wang, Yanghan and Sun, Fei and Wu, Yiming and Tian, Yuandong and Vajda, Peter and Jia, Yangqing},
 title = {{FBNet:} {Hardware-aware} Efficient {ConvNet} Design via Differentiable Neural Architecture Search},
 year = {2019},
 booktitle = {2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
 pages = {10734--10742},
 doi = {10.1109/cvpr.2019.01099},
 source = {Crossref},
 url = {https://doi.org/10.1109/cvpr.2019.01099},
 publisher = {IEEE},
 month = jun,
}

@article{wu2022sustainable,
 author = {Wu, Carole-Jean and Raghavendra, Ramya and Gupta, Udit and Acun, Bilge and Ardalani, Newsha and Maeng, Kiwan and Chang, Gloria and Aga, Fiona and Huang, Jinshi and Bai, Charles and others},
 title = {Sustainable ai: {Environmental} implications, challenges and opportunities},
 year = {2022},
 journal = {Proceedings of Machine Learning and Systems},
 volume = {4},
 pages = {795--813},
}

@inproceedings{xavier,
 author = {Glorot, Xavier and Bengio, Yoshua},
 title = {Understanding the difficulty of training deep feedforward neural networks},
 year = {2010},
 booktitle = {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},
 url = {https://proceedings.mlr.press/v9/glorot10a.html},
}

@inproceedings{xie2020adversarial,
 author = {Xie, Cihang and Tan, Mingxing and Gong, Boqing and Wang, Jiang and Yuille, Alan L. and Le, Quoc V.},
 title = {Adversarial Examples Improve Image Recognition},
 year = {2020},
 booktitle = {2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
 pages = {819--828},
 doi = {10.1109/cvpr42600.2020.00090},
 source = {Crossref},
 url = {https://doi.org/10.1109/cvpr42600.2020.00090},
 publisher = {IEEE},
 month = jun,
}

@article{xiong_mri-based_2021,
 author = {Xiong, Siyu and Wu, Guoqing and Fan, Xitian and Feng, Xuan and Huang, Zhongcheng and Cao, Wei and Zhou, Xuegong and Ding, Shijin and Yu, Jinhua and Wang, Lingli and Shi, Zhifeng},
 title = {{MRI}-based brain tumor segmentation using {FPGA}-accelerated neural network},
 year = {2021},
 month = sep,
 journal = {BMC Bioinf.},
 volume = {22},
 number = {1},
 pages = {421},
 doi = {10.1186/s12859-021-04347-6},
 issn = {1471-2105},
 url = {https://doi.org/10.1186/s12859-021-04347-6},
 urldate = {2023-11-07},
 abstract = {Brain tumor segmentation is a challenging problem in medical image processing and analysis. It is a very time-consuming and error-prone task. In order to reduce the burden on physicians and improve the segmentation accuracy, the computer-aided detection (CAD) systems need to be developed. Due to the powerful feature learning ability of the deep learning technology, many deep learning-based methods have been applied to the brain tumor segmentation CAD systems and achieved satisfactory accuracy. However, deep learning neural networks have high computational complexity, and the brain tumor segmentation process consumes significant time. Therefore, in order to achieve the high segmentation accuracy of brain tumors and obtain the segmentation results efficiently, it is very demanding to speed up the segmentation process of brain tumors.},
 keywords = {Brain tumor segmatation, FPGA acceleration, Neural network},
 bdsk-url-1 = {https://doi.org/10.1186/s12859-021-04347-6},
 source = {Crossref},
 publisher = {Springer Science and Business Media LLC},
}

@article{xiu2019time,
 author = {Xiu, Liming},
 title = {Time Moore: {Exploiting} {Moore's} Law From The Perspective of Time},
 year = {2019},
 journal = {IEEE Solid-State Circuits Mag.},
 publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
 volume = {11},
 number = {1},
 pages = {39--55},
 doi = {10.1109/mssc.2018.2882285},
 source = {Crossref},
 url = {https://doi.org/10.1109/mssc.2018.2882285},
 issn = {1943-0582, 1943-0590},
}

@article{xu2018alternating,
 author = {Xu, Chen and Yao, Jianqiang and Lin, Zhouchen and Ou, Wenwu and Cao, Yuanbin and Wang, Zhirong and Zha, Hongbin},
 title = {Alternating multi-bit quantization for recurrent neural networks},
 year = {2018},
 journal = {arXiv preprint arXiv:1802.00150},
}

@article{xu2023demystifying,
 author = {Xu, Hu and Xie, Saining and Tan, Xiaoqing Ellen and Huang, Po-Yao and Howes, Russell and Sharma, Vasu and Li, Shang-Wen and Ghosh, Gargi and Zettlemoyer, Luke and Feichtenhofer, Christoph},
 title = {Demystifying {CLIP} Data},
 year = {2023},
 journal = {arXiv preprint arXiv:2309.16671},
}

@article{xu2023federated,
 author = {Xu, Zheng and Zhang, Yanxiang and Andrew, Galen and Choquette-Choo, Christopher A and Kairouz, Peter and McMahan, H Brendan and Rosenstock, Jesse and Zhang, Yuanbo},
 title = {Federated Learning of Gboard Language Models with Differential Privacy},
 year = {2023},
 journal = {arXiv preprint arXiv:2305.18465},
}

@article{yamashita2023coffee,
 author = {Bordin Yamashita, Jo\~ao Vitor Yukio and Leite, Jo\~ao Paulo R.R.},
 title = {Coffee disease classification at the edge using deep learning},
 year = {2023},
 journal = {Smart Agricultural Technology},
 publisher = {Elsevier BV},
 volume = {4},
 pages = {100183},
 doi = {10.1016/j.atech.2023.100183},
 source = {Crossref},
 url = {https://doi.org/10.1016/j.atech.2023.100183},
 issn = {2772-3755},
 month = aug,
}

@misc{yang2020coexploration,
 author = {Ho Yoon, Jung and Jung, Hyung-Suk and Hwan Lee, Min and Hwan Kim, Gun and Ji Song, Seul and Yeong Seok, Jun and Jean Yoon, Kyung and Seong Hwang, Cheol and Besland, M.-P. and Tranchant, J. and Souchier, E. and Moreau, P. and Salmon, S. and Corraze, B. and Janod, E. and Cario, L. and Zazpe, Ra\'ul and Ungureanu, Mariana and Llopis, Roger and Golmar, Federico and Stoliar, Pablo and Casanova, F\'elix and Eduardo Hueso, Luis and Hermes, C. and Wimmer, M. and Menzel, S. and Fleck, K. and Rana, V. and Salinga, M. and B\"ottger, U. and Bruchhaus, R. and Wuttig, M. and Waser, R. and Lentz, F. and Hermes, C. and R\"osgen, B. and Selle, T. and Bruchhaus, R. and Rana, V. and Waser, R. and Marchewka, Astrid and Menzel, Stephan and B\"ottger, Ulrich and Waser, Rainer and Hoskins, Brian and Alibart, Fabien and Strukov, Dmitri and Pellegrino, Luca and Manca, Nicola and Kanki, Teruo and Tanaka, Hidekazu and Biasotti, Michele and Bellingeri, Emilio and Sergio Siri, Antonio and Marr\'e, Daniele and M. Padilha, Antonio Claudio and Martini Dalpian, Gustavo and Reily Rocha, Alexandre and Prodromakis, Themistoklis and Salaoru, Iulia and Khiat, Ali and Toumazou, Christopher and Gale, Ella M. and Madhavan, A. and Adam, G. and Alibart, F. and Gao, L. and Strukov, D. B. and Wamwangi, D. and Welnic, W. and Wuttig, M. and Gholipour, Behrad and Huang, Chung-Che and Anastasopoulos, Alexandros and Al-Saab, Feras and Hayden, Brian E. and Hewak, Daniel W. and Lan, Rui and Endo, Rie and Kuwahara, Masashi and Kobayashi, Yoshinao and Susa, Masahiro and Baumeister, Paul and Wortmann, Daniel and Bl\"ugel, Stefan and Mazzarello, Riccardo and Li, Yan and Zhang, Wei and Ronneberger, Ider and Simon, Ronnie and Gallus, Jens and Bessas, Dimitrios and Sergueev, Ilya and Wille, Hans-Christian and Pierre Hermann, Rapha\"el and Luckas, Jennifer and Rausch, Pascal and Krebs, Daniel and Zalden, Peter and Boltz, Janika and Raty, Jean-Yves and Salinga, Martin and Longeaud, Christophe and Wuttig, Matthias and Kim, Haeri and Kim, Dong-Wook and Phark, Soo-Hyon and Hong, Seungbum and Park, C. and Herpers, A. and Bruchhaus, R. and Verbeeck, J. and Egoavil, R. and Borgatti, F. and Panaccione, G. and Offi, F. and Dittmann, R. and Clima, Sergiu and Sankaran, Kiroubanand and Mees, Maarten and Yin Chen, Yang and Goux, Ludovic and Govoreanu, Bogdan and Wouters, Dirk J. and Kittl, Jorge and Jurczak, Malgorzata and Pourtois, Geoffrey and Calka, P. and Martinez, E. and Delaye, V. and Lafond, D. and Audoit, G. and Mariolle, D. and Chevalier, N. and Grampeix, H. and Cagli, C. and Jousseaume, V. and Guedj, C. and Shrestha, Pragya and Ochia, Adaku and Cheung, Kin. P. and Campbell, Jason and Baumgart, Helmut and Harris, Gary and Scherff, Malte and Meyer, Bjoern and Scholz, Julius and Hoffmann, Joerg and Jooss, Christian and Xiao, Bo and Tada, Tomofumi and Gu, Tingkun and Tawara, Arihiro and Watanabe, Satoshi and Young, Tai-Fa and Yang, Ya-Liang and Chang, Ting-Chang and Hsu, Kuang-Ting and Chen, Chao-Yu and Burkert, A and Valov, I. and Staikov, G. and Waser, R. and van den Hurk, Jan and Valov, Ilia and Waser, Rainer and Valov, Ilia and Tappertzhofen, Stefan and van der Hurk, Jan and Waser, Rainer and Adam, G. and Alibart, F. and Gao, L. and Hoskins, B. and Strukov, D. B. and Jean Yoon, Kyung and Ji Song, Seul and Kim, Gun Hwan and Seok, Jun Yeong and Ho Yoon, Jeong and Seong Hwang, Cheol and Yoon, Jung Ho and Yoon, Kyung Jin and Shuai, Yao and Wu, Chuangui and Zhang, Wanli and Zhou, Shengqiang and B\"urger, Danilo and Slesazeck, Stefan and Mikolajick, Thomas and Helm, Manfred and Schmidt, Heidemarie and Gale, Ella and Pearson, David and Kitson, Stephen and Adamatzky, Andrew and Costello, Ben de Lacy and Lehtonen, Eero and Poikonen, Jussi and Laiho, Mika and Kanerva, Pentti and Lim, Hyungkwang and Jang, Ho-won and Jeong, Doo Seok and Cao, Xun and Jiang, Meng and Zhang, Feng and Liu, Xinjun and Jin, Ping and Zhang, Kai and Tangirala, Madhavi and Shrestha, Pragya and Baumgart, Helmut and Kittiwatanakul, Salinporn and Lu, Jiwei and Wolf, Stuart and Pallem, Venkateswara and Dussarrat, Christian and Pinto, S. and Krishna, R. and Dias, C. and Pimentel, G. and Oliveira, G. N. P. and Teixeira, J. M. and Aguiar, P. and Titus, E. and Gracio, J. and Ventura, J. and Araujo, J. P.},
 title = {Frontiers in Electronic Materials},
 year = {2012},
 archiveprefix = {arXiv},
 eprint = {2002.04116},
 primaryclass = {cs.LG},
 doi = {10.1002/9783527667703.ch67},
 source = {Crossref},
 url = {https://doi.org/10.1002/9783527667703.ch67},
 publisher = {Wiley},
 isbn = {9783527411917, 9783527667703},
 pages = {523--587},
 month = jun,
}

@inproceedings{yang2023online,
 author = {Yang, Tien-Ju and Xiao, Yonghui and Motta, Giovanni and Beaufays, Fran\c{c}oise and Mathews, Rajiv and Chen, Mingqing},
 title = {Online Model Compression for Federated Learning with Large Models},
 year = {2023},
 booktitle = {ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
 pages = {1--5},
 organization = {IEEE},
 doi = {10.1109/icassp49357.2023.10097124},
 source = {Crossref},
 url = {https://doi.org/10.1109/icassp49357.2023.10097124},
 publisher = {IEEE},
 month = jun,
}

@misc{yik2023neurobench,
 author = {Yik, Jason and Ahmed, Soikat Hasan and Ahmed, Zergham and Anderson, Brian and Andreou, Andreas G. and Bartolozzi, Chiara and Basu, Arindam and den Blanken, Douwe and Bogdan, Petrut and Bohte, Sander and Bouhadjar, Younes and Buckley, Sonia and Cauwenberghs, Gert and Corradi, Federico and de Croon, Guido and Danielescu, Andreea and Daram, Anurag and Davies, Mike and Demirag, Yigit and Eshraghian, Jason and Forest, Jeremy and Furber, Steve and Furlong, Michael and Gilra, Aditya and Indiveri, Giacomo and Joshi, Siddharth and Karia, Vedant and Khacef, Lyes and Knight, James C. and Kriener, Laura and Kubendran, Rajkumar and Kudithipudi, Dhireesha and Lenz, Gregor and Manohar, Rajit and Mayr, Christian and Michmizos, Konstantinos and Muir, Dylan and Neftci, Emre and Nowotny, Thomas and Ottati, Fabrizio and Ozcelikkale, Ayca and Pacik-Nelson, Noah and Panda, Priyadarshini and Pao-Sheng, Sun and Payvand, Melika and Pehle, Christian and Petrovici, Mihai A. and Posch, Christoph and Renner, Alpha and Sandamirskaya, Yulia and Schaefer, Clemens JS and van Schaik, Andr\'e and Schemmel, Johannes and Schuman, Catherine and Seo, Jae-sun and Sheik, Sadique and Shrestha, Sumit Bam and Sifalakis, Manolis and Sironi, Amos and Stewart, Kenneth and Stewart, Terrence C. and Stratmann, Philipp and Tang, Guangzhi and Timcheck, Jonathan and Verhelst, Marian and Vineyard, Craig M. and Vogginger, Bernhard and Yousefzadeh, Amirreza and Zhou, Biyan and Zohora, Fatima Tuz and Frenkel, Charlotte and Reddi, Vijay Janapa},
 title = {{NeuroBench:} {Advancing} Neuromorphic Computing through Collaborative, Fair and Representative Benchmarking},
 year = {2023},
 archiveprefix = {arXiv},
 eprint = {2304.04640},
 primaryclass = {cs.AI},
}

@article{young2018recent,
 author = {Young, Tom and Hazarika, Devamanyu and Poria, Soujanya and Cambria, Erik},
 title = {Recent Trends in Deep Learning Based Natural Language Processing {[Review} Article]},
 year = {2018},
 journal = {IEEE Comput. Intell. Mag.},
 publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
 volume = {13},
 number = {3},
 pages = {55--75},
 doi = {10.1109/mci.2018.2840738},
 source = {Crossref},
 url = {https://doi.org/10.1109/mci.2018.2840738},
 issn = {1556-603X, 1556-6048},
 month = aug,
}

@inproceedings{zafrir2019q8bert,
 author = {Zafrir, Ofir and Boudoukh, Guy and Izsak, Peter and Wasserblat, Moshe},
 title = {{Q8BERT:} {Quantized} {8Bit} {BERT}},
 year = {2019},
 booktitle = {2019 Fifth Workshop on Energy Efficient Machine Learning and Cognitive Computing - NeurIPS Edition (EMC2-NIPS)},
 pages = {36--39},
 organization = {IEEE},
 doi = {10.1109/emc2-nips53020.2019.00016},
 source = {Crossref},
 url = {https://doi.org/10.1109/emc2-nips53020.2019.00016},
 publisher = {IEEE},
 month = dec,
}

@inproceedings{zennaro2022tinyml,
 author = {Zennaro, Marco and Plancher, Brian and Reddi, V Janapa},
 title = {{TinyML:} {Applied} {AI} for development},
 year = {2022},
 booktitle = {The UN 7th Multi-stakeholder Forum on Science, Technology and Innovation for the Sustainable Development Goals},
 pages = {2022--05},
}

@article{zennarobridging,
 author = {Zennaro, Marco and Plancher, Brian and Reddi, Vijay Janapa},
 title = {Bridging the Digital Divide: {The} Promising Impact of {TinyML} for Developing Countries},
}

@inproceedings{Zhang_2020_CVPR_Workshops,
 author = {Zhang, Li Lyna and Yang, Yuqing and Jiang, Yuhang and Zhu, Wenwu and Liu, Yunxin},
 title = {Fast Hardware-Aware Neural Architecture Search},
 year = {2020},
 month = jun,
 booktitle = {2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)},
 doi = {10.1109/cvprw50498.2020.00354},
 source = {Crossref},
 url = {https://doi.org/10.1109/cvprw50498.2020.00354},
 publisher = {IEEE},
}

@inproceedings{zhang2015fpga,
 author = {Zhang, Chen and Li, Peng and Sun, Guangyu and Guan, Yijin and Xiao, Bingjun and Cong, Jason Optimizing},
 title = {{FPGA}-based Accelerator Design for Deep Convolutional Neural Networks Proceedings of the 2015 {ACM}},
 year = {2015},
 booktitle = {SIGDA International Symposium on Field-Programmable Gate Arrays-FPGA},
 volume = {15},
 pages = {161--170},
}

@article{Zhang2017,
 author = {Zhang, Qingxue and Zhou, Dian and Zeng, Xuan},
 title = {Highly wearable cuff-less blood pressure and heart rate monitoring with single-arm electrocardiogram and photoplethysmogram signals},
 year = {2017},
 month = feb,
 day = {06},
 journal = {BioMedical Engineering OnLine},
 volume = {16},
 number = {1},
 pages = {23},
 doi = {10.1186/s12938-017-0317-z},
 issn = {1475-925X},
 url = {https://doi.org/10.1186/s12938-017-0317-z},
 bdsk-url-1 = {https://doi.org/10.1186/s12938-017-0317-z},
 source = {Crossref},
 publisher = {Springer Science and Business Media LLC},
}

@article{zhang2018review,
 author = {Zhang, Dongxia and Han, Xiaoqing and Deng, Chunyu},
 title = {Review on the research and practice of deep learning and reinforcement learning in smart grids},
 year = {2018},
 journal = {CSEE Journal of Power and Energy Systems},
 publisher = {Power System Technology Press},
 volume = {4},
 number = {3},
 pages = {362--370},
 doi = {10.17775/cseejpes.2018.00520},
 source = {Crossref},
 url = {https://doi.org/10.17775/cseejpes.2018.00520},
 issn = {2096-0042},
 month = sep,
}

@misc{zhang2019autoshrink,
 author = {Zhang, Tunhou and Cheng, Hsin-Pai and Li, Zhenwen and Yan, Feng and Huang, Chengyu and Li, Hai and Chen, Yiran},
 title = {{AutoShrink:} {A} Topology-aware {NAS} for Discovering Efficient Neural Architecture},
 year = {2019},
 archiveprefix = {arXiv},
 eprint = {1911.09251},
 primaryclass = {cs.LG},
}

@article{zhao2018federated,
 author = {Zhao, Yue and Li, Meng and Lai, Liangzhen and Suda, Naveen and Civin, Damon and Chandra, Vikas},
 title = {Federated learning with non-iid data},
 year = {2018},
 journal = {arXiv preprint arXiv:1806.00582},
}

@inproceedings{zhao2018fpga,
 author = {Zhao, Mark and Suh, G. Edward},
 title = {{FPGA}-Based Remote Power Side-Channel Attacks},
 year = {2018},
 booktitle = {2018 IEEE Symposium on Security and Privacy (SP)},
 pages = {229--244},
 date-added = {2023-11-22 17:08:21 -0500},
 date-modified = {2023-11-22 17:09:07 -0500},
 organization = {IEEE},
 doi = {10.1109/sp.2018.00049},
 source = {Crossref},
 url = {https://doi.org/10.1109/sp.2018.00049},
 publisher = {IEEE},
 month = may,
}

@misc{zhou_deep_2023,
 author = {Zhou, Da-Wei and Wang, Qi-Wei and Qi, Zhi-Hong and Ye, Han-Jia and Zhan, De-Chuan and Liu, Ziwei},
 title = {Deep Class-Incremental Learning: {A} Survey},
 shorttitle = {Deep Class-Incremental Learning},
 year = {2023},
 month = feb,
 publisher = {arXiv},
 url = {http://arxiv.org/abs/2302.03648},
 urldate = {2023-10-26},
 note = {arXiv:2302.03648 [cs]},
 annote = {Comment: Code is available at https://github.com/zhoudw-zdw/CIL\_Survey/},
 language = {en},
 bdsk-url-1 = {http://arxiv.org/abs/2302.03648},
 file = {Zhou et al. - 2023 - Deep Class-Incremental Learning A Survey.pdf:/Users/alex/Zotero/storage/859VZG7W/Zhou et al. - 2023 - Deep Class-Incremental Learning A Survey.pdf:application/pdf},
 keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@inproceedings{zhou2018interpretable,
 author = {Zhou, Bolei and Sun, Yiyou and Bau, David and Torralba, Antonio},
 title = {Interpretable basis decomposition for visual explanation},
 year = {2018},
 booktitle = {Proceedings of the European Conference on Computer Vision (ECCV)},
 pages = {119--134},
}

@misc{zhou2021analognets,
 author = {Zhou, Chuteng and Redondo, Fernando Garcia and B\"uchel, Julian and Boybat, Irem and Comas, Xavier Timoneda and Nandakumar, S. R. and Das, Shidhartha and Sebastian, Abu and Gallo, Manuel Le and Whatmough, Paul N.},
 title = {{AnalogNets:} {Ml-hw} Co-Design of Noise-robust {TinyML} Models and Always-On Analog Compute-in-Memory Accelerator},
 year = {2021},
 archiveprefix = {arXiv},
 eprint = {2111.06503},
 primaryclass = {cs.AR},
}

@article{zhou2022photonic,
 author = {Zhou, Hailong and Dong, Jianji and Cheng, Junwei and Dong, Wenchan and Huang, Chaoran and Shen, Yichen and Zhang, Qiming and Gu, Min and Qian, Chao and Chen, Hongsheng and Ruan, Zhichao and Zhang, Xinliang},
 title = {Photonic matrix multiplication lights up photonic accelerator and beyond},
 year = {2022},
 journal = {Light: Science \&amp; Applications},
 publisher = {Springer Science and Business Media LLC},
 volume = {11},
 number = {1},
 pages = {30},
 doi = {10.1038/s41377-022-00717-8},
 source = {Crossref},
 url = {https://doi.org/10.1038/s41377-022-00717-8},
 issn = {2047-7538},
 month = feb,
}

@inproceedings{zhu2018benchmarking,
 author = {Zhu, Hongyu and Akrout, Mohamed and Zheng, Bojian and Pelegris, Andrew and Jayarajan, Anand and Phanishayee, Amar and Schroeder, Bianca and Pekhimenko, Gennady},
 title = {Benchmarking and Analyzing Deep Neural Network Training},
 year = {2018},
 booktitle = {2018 IEEE International Symposium on Workload Characterization (IISWC)},
 pages = {88--100},
 organization = {IEEE},
 doi = {10.1109/iiswc.2018.8573476},
 source = {Crossref},
 url = {https://doi.org/10.1109/iiswc.2018.8573476},
 publisher = {IEEE},
 month = sep,
}

@article{zhuang_comprehensive_2021,
 author = {Zhuang, Fuzhen and Qi, Zhiyuan and Duan, Keyu and Xi, Dongbo and Zhu, Yongchun and Zhu, Hengshu and Xiong, Hui and He, Qing},
 title = {A Comprehensive Survey on Transfer Learning},
 year = {2021},
 month = jan,
 journal = {Proc. IEEE},
 publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
 volume = {109},
 number = {1},
 pages = {43--76},
 doi = {10.1109/jproc.2020.3004555},
 issn = {0018-9219, 1558-2256},
 url = {https://doi.org/10.1109/jproc.2020.3004555},
 urldate = {2023-10-25},
 language = {en},
 source = {Crossref},
}

@article{zhuang2020comprehensive,
 author = {Zhuang, Fuzhen and Qi, Zhiyuan and Duan, Keyu and Xi, Dongbo and Zhu, Yongchun and Zhu, Hengshu and Xiong, Hui and He, Qing},
 title = {A Comprehensive Survey on Transfer Learning},
 year = {2021},
 journal = {Proc. IEEE},
 publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
 volume = {109},
 number = {1},
 pages = {43--76},
 doi = {10.1109/jproc.2020.3004555},
 source = {Crossref},
 url = {https://doi.org/10.1109/jproc.2020.3004555},
 issn = {0018-9219, 1558-2256},
 month = jan,
}
