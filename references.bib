@inproceedings{abadi2016deep,
	title        = {Deep learning with differential privacy},
	author       = {Abadi, Martin and Chu, Andy and Goodfellow, Ian and McMahan, H Brendan and Mironov, Ilya and Talwar, Kunal and Zhang, Li},
	year         = 2016,
	booktitle    = {Proceedings of the 2016 ACM SIGSAC conference on computer and communications security},
	pages        = {308--318}
}

@inproceedings{krishnan2023archgym,
  title={ArchGym: An Open-Source Gymnasium for Machine Learning Assisted Architecture Design},
  author={Krishnan, Srivatsan and Yazdanbakhsh, Amir and Prakash, Shvetank and Jabbour, Jason and Uchendu, Ikechukwu and Ghosh, Susobhan and Boroujerdian, Behzad and Richins, Daniel and Tripathy, Devashree and Faust, Aleksandra and Janapa Reddi, Vijay},
  booktitle={Proceedings of the 50th Annual International Symposium on Computer Architecture},
  pages={1--16},
  year={2023}
}

@misc{kuzmin2022fp8,
      title={FP8 Quantization: The Power of the Exponent}, 
      author={Andrey Kuzmin and Mart Van Baalen and Yuwei Ren and Markus Nagel and Jorn Peters and Tijmen Blankevoort},
      year={2022},
      eprint={2208.09225},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{abadi2016tensorflow,
	title        = {$\{$TensorFlow$\}$: a system for $\{$Large-Scale$\}$ machine learning},
	author       = {Abadi, Mart{\'\i}n and Barham, Paul and Chen, Jianmin and Chen, Zhifeng and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Irving, Geoffrey and Isard, Michael and others},
	year         = 2016,
	booktitle    = {12th USENIX symposium on operating systems design and implementation (OSDI 16)},
	pages        = {265--283}
}


@article{shastri2021photonics,
  title={Photonics for artificial intelligence and neuromorphic computing},
  author={Shastri, Bhavin J and Tait, Alexander N and Ferreira de Lima, Thomas and Pernice, Wolfram HP and Bhaskaran, Harish and Wright, C David and Prucnal, Paul R},
  journal={Nature Photonics},
  volume={15},
  number={2},
  pages={102--114},
  year={2021},
  publisher={Nature Publishing Group UK London}
}

@inproceedings{jouppi2017datacenter,
  title={In-datacenter performance analysis of a tensor processing unit},
  author={Jouppi, Norman P and Young, Cliff and Patil, Nishant and Patterson, David and Agrawal, Gaurav and Bajwa, Raminder and Bates, Sarah and Bhatia, Suresh and Boden, Nan and Borchers, Al and others},
  booktitle={Proceedings of the 44th annual international symposium on computer architecture},
  pages={1--12},
  year={2017}
}

@inproceedings{ignatov2018ai,
title={Ai benchmark: Running deep neural networks on android smartphones},
  author={Ignatov, Andrey and Timofte, Radu and Chou, William and Wang, Ke and Wu, Max and Hartley, Tim and Van Gool, Luc},
  booktitle={Proceedings of the European Conference on Computer Vision (ECCV) Workshops},
  pages={0--0},
  year={2018}
}


@inproceedings{adolf2016fathom,
	title        = {Fathom: Reference workloads for modern deep learning methods},
	author       = {Adolf, Robert and Rama, Saketh and Reagen, Brandon and Wei, Gu-Yeon and Brooks, David},
	year         = 2016,
	booktitle    = {2016 IEEE International Symposium on Workload Characterization (IISWC)},
	pages        = {1--10},
	organization = {IEEE}
}


@misc{al2016theano,
	title        = {Theano: A Python framework for fast computation of mathematical expressions},
	author       = {The Theano Development Team and Rami Al-Rfou and Guillaume Alain and Amjad Almahairi and Christof Angermueller and Dzmitry Bahdanau and Nicolas Ballas and Frédéric Bastien and Justin Bayer and Anatoly Belikov and Alexander Belopolsky and Yoshua Bengio and Arnaud Bergeron and James Bergstra and Valentin Bisson and Josh Bleecher Snyder and Nicolas Bouchard and Nicolas Boulanger-Lewandowski and Xavier Bouthillier and Alexandre de Brébisson and Olivier Breuleux and Pierre-Luc Carrier and Kyunghyun Cho and Jan Chorowski and Paul Christiano and Tim Cooijmans and Marc-Alexandre Côté and Myriam Côté and Aaron Courville and Yann N. Dauphin and Olivier Delalleau and Julien Demouth and Guillaume Desjardins and Sander Dieleman and Laurent Dinh and Mélanie Ducoffe and Vincent Dumoulin and Samira Ebrahimi Kahou and Dumitru Erhan and Ziye Fan and Orhan Firat and Mathieu Germain and Xavier Glorot and Ian Goodfellow and Matt Graham and Caglar Gulcehre and Philippe Hamel and Iban Harlouchet and Jean-Philippe Heng and Balázs Hidasi and Sina Honari and Arjun Jain and Sébastien Jean and Kai Jia and Mikhail Korobov and Vivek Kulkarni and Alex Lamb and Pascal Lamblin and Eric Larsen and César Laurent and Sean Lee and Simon Lefrancois and Simon Lemieux and Nicholas Léonard and Zhouhan Lin and Jesse A. Livezey and Cory Lorenz and Jeremiah Lowin and Qianli Ma and Pierre-Antoine Manzagol and Olivier Mastropietro and Robert T. McGibbon and Roland Memisevic and Bart van Merriënboer and Vincent Michalski and Mehdi Mirza and Alberto Orlandi and Christopher Pal and Razvan Pascanu and Mohammad Pezeshki and Colin Raffel and Daniel Renshaw and Matthew Rocklin and Adriana Romero and Markus Roth and Peter Sadowski and John Salvatier and François Savard and Jan Schlüter and John Schulman and Gabriel Schwartz and Iulian Vlad Serban and Dmitriy Serdyuk and Samira Shabanian and Étienne Simon and Sigurd Spieckermann and S. Ramana Subramanyam and Jakub Sygnowski and Jérémie Tanguay and Gijs van Tulder and Joseph Turian and Sebastian Urban and Pascal Vincent and Francesco Visin and Harm de Vries and David Warde-Farley and Dustin J. Webb and Matthew Willson and Kelvin Xu and Lijun Xue and Li Yao and Saizheng Zhang and Ying Zhang},
	year         = 2016,
	eprint       = {1605.02688},
	archiveprefix = {arXiv},
	primaryclass = {cs.SC}
}


@article{Aledhari_Razzak_Parizi_Saeed_2020,
	title        = {Federated learning: A survey on enabling technologies, Protocols, and applications},
	author       = {Aledhari, Mohammed and Razzak, Rehma and Parizi, Reza M. and Saeed, Fahad},
	year         = 2020,
	journal      = {IEEE Access},
	volume       = 8,
	pages        = {140699–140725},
	doi          = {10.1109/access.2020.3013541}
}


@article{aljundi_gradient_nodate,
	title        = {Gradient based sample selection for online continual learning},
	author       = {Aljundi, Rahaf and Lin, Min and Goujaud, Baptiste and Bengio, Yoshua},
	language     = {en},
	file         = {Aljundi et al. - Gradient based sample selection for online continu.pdf:/Users/alex/Zotero/storage/GPHM4KY7/Aljundi et al. - Gradient based sample selection for online continu.pdf:application/pdf}
}


@inproceedings{altayeb2022classifying,
	title        = {Classifying mosquito wingbeat sound using TinyML},
	author       = {Altayeb, Moez and Zennaro, Marco and Rovai, Marcelo},
	year         = 2022,
	booktitle    = {Proceedings of the 2022 ACM Conference on Information Technology for Social Good},
	pages        = {132--137}
}


@misc{amodei_ai_2018,
	title        = {{AI} and {Compute}},
	author       = {Amodei, Dario and Hernandez, Danny},
	year         = 2018,
	month        = may,
	journal      = {OpenAI Blog},
	url          = {https://openai.com/research/ai-and-compute}
}


@inproceedings{antol2015vqa,
	title        = {Vqa: Visual question answering},
	author       = {Antol, Stanislaw and Agrawal, Aishwarya and Lu, Jiasen and Mitchell, Margaret and Batra, Dhruv and Zitnick, C Lawrence and Parikh, Devi},
	year         = 2015,
	booktitle    = {Proceedings of the IEEE international conference on computer vision},
	pages        = {2425--2433}
}


@article{app112211073,
	title        = {Hardware/Software Co-Design for TinyML Voice-Recognition Application on Resource Frugal Edge Devices},
	author       = {Kwon, Jisu and Park, Daejin},
	year         = 2021,
	journal      = {Applied Sciences},
	volume       = 11,
	number       = 22,
	doi          = {10.3390/app112211073},
	issn         = {2076-3417},
	url          = {https://www.mdpi.com/2076-3417/11/22/11073},
	article-number = 11073
}


@article{Ardila_Branson_Davis_Henretty_Kohler_Meyer_Morais_Saunders_Tyers_Weber_2020,
	title        = {Common Voice: A Massively-Multilingual Speech Corpus},
	author       = {Ardila, Rosana and Branson, Megan and Davis, Kelly and Henretty, Michael and Kohler, Michael and Meyer, Josh and Morais, Reuben and Saunders, Lindsay and Tyers, Francis M. and Weber, Gregor},
	year         = 2020,
	month        = {May},
	journal      = {Proceedings of the 12th Conference on Language Resources and Evaluation},
	pages        = {4218-4222}
}


@misc{awq,
	title        = {AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration},
	author       = {Lin and Tang, Tang and Yang, Dang and Gan, Han},
	year         = 2023,
	doi          = {10.48550/arXiv.2306.00978},
	url          = {https://arxiv.org/abs/2306.00978},
	urldate      = {2023-10-03}
}


@inproceedings{bamoumen2022tinyml,
	title        = {How TinyML Can be Leveraged to Solve Environmental Problems: A Survey},
	author       = {Bamoumen, Hatim and Temouden, Anas and Benamar, Nabil and Chtouki, Yousra},
	year         = 2022,
	booktitle    = {2022 International Conference on Innovation and Intelligence for Informatics, Computing, and Technologies (3ICT)},
	pages        = {338--343},
	organization = {IEEE}
}


@article{banbury2020benchmarking,
	title        = {Benchmarking tinyml systems: Challenges and direction},
	author       = {Banbury, Colby R and Reddi, Vijay Janapa and Lam, Max and Fu, William and Fazel, Amin and Holleman, Jeremy and Huang, Xinyuan and Hurtado, Robert and Kanter, David and Lokhmotov, Anton and others},
	year         = 2020,
	journal      = {arXiv preprint arXiv:2003.04821}
}


@article{bank2023autoencoders,
	title        = {Autoencoders},
	author       = {Bank, Dor and Koenigstein, Noam and Giryes, Raja},
	year         = 2023,
	journal      = {Machine Learning for Data Science Handbook: Data Mining and Knowledge Discovery Handbook},
	publisher    = {Springer},
	pages        = {353--374}
}


@book{barroso2019datacenter,
	title        = {The datacenter as a computer: Designing warehouse-scale machines},
	author       = {Barroso, Luiz Andr{\'e} and H{\"o}lzle, Urs and Ranganathan, Parthasarathy},
	year         = 2019,
	publisher    = {Springer Nature}
}


@article{Bender_Friedman_2018,
	title        = {Data statements for natural language processing: Toward mitigating system bias and enabling better science},
	author       = {Bender, Emily M. and Friedman, Batya},
	year         = 2018,
	journal      = {Transactions of the Association for Computational Linguistics},
	volume       = 6,
	pages        = {587-604},
	doi          = {10.1162/tacl_a_00041}
}


@article{beyer2020we,
	title        = {Are we done with imagenet?},
	author       = {Beyer, Lucas and H{\'e}naff, Olivier J and Kolesnikov, Alexander and Zhai, Xiaohua and Oord, A{\"a}ron van den},
	year         = 2020,
	journal      = {arXiv preprint arXiv:2006.07159}
}


@article{biggio2014pattern,
	title        = {Pattern recognition systems under attack: Design issues and research challenges},
	author       = {Biggio, Battista and Fumera, Giorgio and Roli, Fabio},
	year         = 2014,
	journal      = {International Journal of Pattern Recognition and Artificial Intelligence},
	publisher    = {World Scientific},
	volume       = 28,
	number       = {07},
	pages        = 1460002
}


@misc{blalock_what_2020,
	title        = {What is the {State} of {Neural} {Network} {Pruning}?},
	author       = {Blalock, Davis and Ortiz, Jose Javier Gonzalez and Frankle, Jonathan and Guttag, John},
	year         = 2020,
	month        = mar,
	publisher    = {arXiv},
	doi          = {10.48550/arXiv.2003.03033},
	url          = {http://arxiv.org/abs/2003.03033},
	urldate      = {2023-10-20},
	note         = {arXiv:2003.03033 [cs, stat]},
	abstract     = {Neural network pruning---the task of reducing the size of a network by removing parameters---has been the subject of a great deal of work in recent years. We provide a meta-analysis of the literature, including an overview of approaches to pruning and consistent findings in the literature. After aggregating results across 81 papers and pruning hundreds of models in controlled conditions, our clearest finding is that the community suffers from a lack of standardized benchmarks and metrics. This deficiency is substantial enough that it is hard to compare pruning techniques to one another or determine how much progress the field has made over the past three decades. To address this situation, we identify issues with current practices, suggest concrete remedies, and introduce ShrinkBench, an open-source framework to facilitate standardized evaluations of pruning methods. We use ShrinkBench to compare various pruning techniques and show that its comprehensive evaluation can prevent common pitfalls when comparing pruning methods.},
	keywords     = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file         = {arXiv Fulltext PDF:/Users/jeffreyma/Zotero/storage/MA4QGZ6E/Blalock et al. - 2020 - What is the State of Neural Network Pruning.pdf:application/pdf;arXiv.org Snapshot:/Users/jeffreyma/Zotero/storage/8DFKG4GL/2003.html:text/html}
}


@article{brown2020language,
	title        = {Language models are few-shot learners},
	author       = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
	year         = 2020,
	journal      = {Advances in neural information processing systems},
	volume       = 33,
	pages        = {1877--1901}
}


@inproceedings{cai_online_2021,
	title        = {Online {Continual} {Learning} with {Natural} {Distribution} {Shifts}: {An} {Empirical} {Study} with {Visual} {Data}},
	shorttitle   = {Online {Continual} {Learning} with {Natural} {Distribution} {Shifts}},
	author       = {Cai, Zhipeng and Sener, Ozan and Koltun, Vladlen},
	year         = 2021,
	month        = oct,
	booktitle    = {2021 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	publisher    = {IEEE},
	address      = {Montreal, QC, Canada},
	pages        = {8261--8270},
	doi          = {10.1109/ICCV48922.2021.00817},
	isbn         = {978-1-66542-812-5},
	url          = {https://ieeexplore.ieee.org/document/9710740/},
	urldate      = {2023-10-26},
	language     = {en},
	file         = {Cai et al. - 2021 - Online Continual Learning with Natural Distributio.pdf:/Users/alex/Zotero/storage/R7ZMIM4K/Cai et al. - 2021 - Online Continual Learning with Natural Distributio.pdf:application/pdf}
}


@article{cai_tinytl_nodate,
	title        = {{TinyTL}: {Reduce} {Memory}, {Not} {Parameters} for {Efﬁcient} {On}-{Device} {Learning}},
	author       = {Cai, Han and Gan, Chuang and Zhu, Ligeng and Han, Song},
	language     = {en},
	file         = {Cai et al. - TinyTL Reduce Memory, Not Parameters for Efﬁcient.pdf:/Users/alex/Zotero/storage/J9C8PTCX/Cai et al. - TinyTL Reduce Memory, Not Parameters for Efﬁcient.pdf:application/pdf}
}


@article{cai2018proxylessnas,
	title        = {Proxylessnas: Direct neural architecture search on target task and hardware},
	author       = {Cai, Han and Zhu, Ligeng and Han, Song},
	year         = 2018,
	journal      = {arXiv preprint arXiv:1812.00332}
}


@article{cai2020tinytl,
	title        = {Tinytl: Reduce memory, not parameters for efficient on-device learning},
	author       = {Cai, Han and Gan, Chuang and Zhu, Ligeng and Han, Song},
	year         = 2020,
	journal      = {Advances in Neural Information Processing Systems},
	volume       = 33,
	pages        = {11285--11297}
}


@article{Chapelle_Scholkopf_Zien,
	title        = {Semi-supervised learning (Chapelle, O. et al., eds.; 2006) [book reviews]},
	author       = {Chapelle, O. and Scholkopf, B. and Zien, Eds., A.},
	year         = 2009,
	journal      = {IEEE Transactions on Neural Networks},
	volume       = 20,
	number       = 3,
	pages        = {542–542},
	doi          = {10.1109/tnn.2009.2015974}
}


@misc{chen__inpainting_2022,
	title        = {Inpainting {Fluid} {Dynamics} with {Tensor} {Decomposition} ({NumPy})},
	author       = {Chen (陈新宇), Xinyu},
	year         = 2022,
	month        = mar,
	journal      = {Medium},
	url          = {https://medium.com/@xinyu.chen/inpainting-fluid-dynamics-with-tensor-decomposition-numpy-d84065fead4d},
	urldate      = {2023-10-20},
	abstract     = {Some simple examples for showing how to use tensor decomposition to reconstruct fluid dynamics},
	language     = {en}
}


@misc{chen_tvm_2018,
	title        = {{TVM}: {An} {Automated} {End}-to-{End} {Optimizing} {Compiler} for {Deep} {Learning}},
	shorttitle   = {{TVM}},
	author       = {Chen, Tianqi and Moreau, Thierry and Jiang, Ziheng and Zheng, Lianmin and Yan, Eddie and Cowan, Meghan and Shen, Haichen and Wang, Leyuan and Hu, Yuwei and Ceze, Luis and Guestrin, Carlos and Krishnamurthy, Arvind},
	year         = 2018,
	month        = oct,
	publisher    = {arXiv},
	url          = {http://arxiv.org/abs/1802.04799},
	urldate      = {2023-10-26},
	note         = {arXiv:1802.04799 [cs]},
	language     = {en},
	keywords     = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Programming Languages},
	annote       = {Comment: Significantly improved version, add automated optimization},
	file         = {Chen et al. - 2018 - TVM An Automated End-to-End Optimizing Compiler f.pdf:/Users/alex/Zotero/storage/QR8MHJ38/Chen et al. - 2018 - TVM An Automated End-to-End Optimizing Compiler f.pdf:application/pdf}
}


@article{chen2016training,
	title        = {Training deep nets with sublinear memory cost},
	author       = {Chen, Tianqi and Xu, Bing and Zhang, Chiyuan and Guestrin, Carlos},
	year         = 2016,
	journal      = {arXiv preprint arXiv:1604.06174}
}


@inproceedings{chen2018tvm,
	title        = {$\{$TVM$\}$: An automated $\{$End-to-End$\}$ optimizing compiler for deep learning},
	author       = {Chen, Tianqi and Moreau, Thierry and Jiang, Ziheng and Zheng, Lianmin and Yan, Eddie and Shen, Haichen and Cowan, Meghan and Wang, Leyuan and Hu, Yuwei and Ceze, Luis and others},
	year         = 2018,
	booktitle    = {13th USENIX Symposium on Operating Systems Design and Implementation (OSDI 18)},
	pages        = {578--594}
}


@article{chen2023learning,
	title        = {Learning domain-heterogeneous speaker recognition systems with personalized continual federated learning},
	author       = {Chen, Zhiyong and Xu, Shugong},
	year         = 2023,
	journal      = {EURASIP Journal on Audio, Speech, and Music Processing},
	publisher    = {Springer},
	volume       = 2023,
	number       = 1,
	pages        = 33
}


@misc{chollet2015,
	title        = {keras},
	author       = {François Chollet},
	year         = 2015,
	journal      = {GitHub repository},
	publisher    = {GitHub},
	howpublished = {\url{https://github.com/fchollet/keras}},
	commit       = {5bcac37}
}


@article{chollet2018keras,
	title        = {Introduction to keras},
	author       = {Chollet, Fran{\c{c}}ois},
	year         = 2018,
	journal      = {March 9th}
}


@inproceedings{chu2021discovering,
	title        = {Discovering multi-hardware mobile models via architecture search},
	author       = {Chu, Grace and Arikan, Okan and Bender, Gabriel and Wang, Weijun and Brighton, Achille and Kindermans, Pieter-Jan and Liu, Hanxiao and Akin, Berkin and Gupta, Suyog and Howard, Andrew},
	year         = 2021,
	booktitle    = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
	pages        = {3022--3031},
	eprint       = {2008.08178},
	archiveprefix = {arXiv},
	primaryclass = {cs.CV}
}


@article{coleman2017dawnbench,
	title        = {Dawnbench: An end-to-end deep learning benchmark and competition},
	author       = {Coleman, Cody and Narayanan, Deepak and Kang, Daniel and Zhao, Tian and Zhang, Jian and Nardi, Luigi and Bailis, Peter and Olukotun, Kunle and R{\'e}, Chris and Zaharia, Matei},
	year         = 2017,
	journal      = {Training},
	volume       = 100,
	number       = 101,
	pages        = 102
}


@inproceedings{coleman2022similarity,
	title        = {Similarity search for efficient active learning and search of rare concepts},
	author       = {Coleman, Cody and Chou, Edward and Katz-Samuels, Julian and Culatana, Sean and Bailis, Peter and Berg, Alexander C and Nowak, Robert and Sumbaly, Roshan and Zaharia, Matei and Yalniz, I Zeki},
	year         = 2022,
	booktitle    = {Proceedings of the AAAI Conference on Artificial Intelligence},
	volume       = 36,
	number       = 6,
	pages        = {6402--6410}
}


@misc{cottier_trends_2023,
	title        = {Trends in the {Dollar} {Training} {Cost} of {Machine} {Learning} {Systems}},
	author       = {Cottier, Ben},
	year         = 2023,
	month        = jan,
	journal      = {Epoch AI Report},
	url          = {https://epochai.org/blog/trends-in-the-dollar-training-cost-of-machine-learning-systems}
}


@misc{david_tensorflow_2021,
	title        = {{TensorFlow} {Lite} {Micro}: {Embedded} {Machine} {Learning} on {TinyML} {Systems}},
	shorttitle   = {{TensorFlow} {Lite} {Micro}},
	author       = {David, Robert and Duke, Jared and Jain, Advait and Reddi, Vijay Janapa and Jeffries, Nat and Li, Jian and Kreeger, Nick and Nappier, Ian and Natraj, Meghna and Regev, Shlomi and Rhodes, Rocky and Wang, Tiezhen and Warden, Pete},
	year         = 2021,
	month        = mar,
	publisher    = {arXiv},
	url          = {http://arxiv.org/abs/2010.08678},
	urldate      = {2023-10-26},
	note         = {arXiv:2010.08678 [cs]},
	language     = {en},
	keywords     = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file         = {David et al. - 2021 - TensorFlow Lite Micro Embedded Machine Learning o.pdf:/Users/alex/Zotero/storage/YCFVNEVH/David et al. - 2021 - TensorFlow Lite Micro Embedded Machine Learning o.pdf:application/pdf}
}


@article{david2021tensorflow,
	title        = {Tensorflow lite micro: Embedded machine learning for tinyml systems},
	author       = {David, Robert and Duke, Jared and Jain, Advait and Janapa Reddi, Vijay and Jeffries, Nat and Li, Jian and Kreeger, Nick and Nappier, Ian and Natraj, Meghna and Wang, Tiezhen and others},
	year         = 2021,
	journal      = {Proceedings of Machine Learning and Systems},
	volume       = 3,
	pages        = {800--811}
}


@article{dean2012large,
	title        = {Large scale distributed deep networks},
	author       = {Dean, Jeffrey and Corrado, Greg and Monga, Rajat and Chen, Kai and Devin, Matthieu and Mao, Mark and Ranzato, Marc'aurelio and Senior, Andrew and Tucker, Paul and Yang, Ke and others},
	year         = 2012,
	journal      = {Advances in neural information processing systems},
	volume       = 25
}


@misc{deci,
	title        = {The Ultimate Guide to Deep Learning Model Quantization and Quantization-Aware Training},
	url          = {https://deci.ai/quantization-and-quantization-aware-training/}
}


@misc{deepcompress,
	title        = {Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding},
	author       = {Han and Mao and Dally},
	year         = 2016,
	doi          = {10.48550/arXiv.1510.00149},
	url          = {https://arxiv.org/abs/1510.00149},
	urldate      = {2016-02-15},
	abstract     = {Neural networks are both computationally intensive and memory intensive, making them difficult to deploy on embedded systems with limited hardware resources. To address this limitation, we introduce "deep compression", a three stage pipeline: pruning, trained quantization and Huffman coding, that work together to reduce the storage requirement of neural networks by 35x to 49x without affecting their accuracy. Our method first prunes the network by learning only the important connections. Next, we quantize the weights to enforce weight sharing, finally, we apply Huffman coding. After the first two steps we retrain the network to fine tune the remaining connections and the quantized centroids. Pruning, reduces the number of connections by 9x to 13x; Quantization then reduces the number of bits that represent each connection from 32 to 5. On the ImageNet dataset, our method reduced the storage required by AlexNet by 35x, from 240MB to 6.9MB, without loss of accuracy. Our method reduced the size of VGG-16 by 49x from 552MB to 11.3MB, again with no loss of accuracy. This allows fitting the model into on-chip SRAM cache rather than off-chip DRAM memory. Our compression method also facilitates the use of complex neural networks in mobile applications where application size and download bandwidth are constrained. Benchmarked on CPU, GPU and mobile GPU, compressed network has 3x to 4x layerwise speedup and 3x to 7x better energy efficiency.}
}


@inproceedings{deng2009imagenet,
	title        = {ImageNet: A large-scale hierarchical image database},
	author       = {Deng, Jia and Socher, R. and Fei-Fei, Li and Dong, Wei and Li, Kai and Li, Li-Jia},
	year         = 2009,
	month        = {06},
	booktitle    = {2009 IEEE Conference on Computer Vision and Pattern Recognition(CVPR)},
	volume       = 00,
	pages        = {248--255},
	doi          = {10.1109/CVPR.2009.5206848},
	url          = {https://ieeexplore.ieee.org/abstract/document/5206848/},
	added-at     = {2018-09-20T15:22:39.000+0200},
	biburl       = {https://www.bibsonomy.org/bibtex/252793859f5bcbbd3f7f9e5d083160acf/analyst},
	description  = {ImageNet: A large-scale hierarchical image database},
	interhash    = {fbfae3e4fe1a81c477ba00efd0d4d977},
	intrahash    = {52793859f5bcbbd3f7f9e5d083160acf},
	keywords     = {2009 computer-vision cvpr dataset ieee paper},
	timestamp    = {2018-09-20T15:22:39.000+0200}
}


@article{desai2016five,
	title        = {Five Safes: designing data access for research},
	author       = {Desai, Tanvi and Ritchie, Felix and Welpton, Richard and others},
	year         = 2016,
	journal      = {Economics Working Paper Series},
	volume       = 1601,
	pages        = 28
}


@article{desai2020five,
	title        = {Five Safes: designing data access for research; 2016},
	author       = {Desai, Tanvi and Ritchie, Felix and Welpton, Richard},
	year         = 2020,
	journal      = {URL https://www2. uwe. ac. uk/faculties/bbs/Documents/1601. pdf}
}


@article{devlin2018bert,
	title        = {Bert: Pre-training of deep bidirectional transformers for language understanding},
	author       = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	year         = 2018,
	journal      = {arXiv preprint arXiv:1810.04805}
}


@article{dhar2021survey,
	title        = {A survey of on-device machine learning: An algorithms and learning theory perspective},
	author       = {Dhar, Sauptik and Guo, Junyao and Liu, Jiayi and Tripathi, Samarth and Kurup, Unmesh and Shah, Mohak},
	year         = 2021,
	journal      = {ACM Transactions on Internet of Things},
	publisher    = {ACM New York, NY, USA},
	volume       = 2,
	number       = 3,
	pages        = {1--49}
}


@misc{dong2022splitnets,
	title        = {SplitNets: Designing Neural Architectures for Efficient Distributed Computing on Head-Mounted Systems},
	author       = {Xin Dong and Barbara De Salvo and Meng Li and Chiao Liu and Zhongnan Qu and H. T. Kung and Ziyun Li},
	year         = 2022,
	eprint       = {2204.04705},
	archiveprefix = {arXiv},
	primaryclass = {cs.LG}
}


@article{duisterhof2019learning,
	title        = {Learning to seek: Autonomous source seeking with deep reinforcement learning onboard a nano drone microcontroller},
	author       = {Duisterhof, Bardienus P and Krishnan, Srivatsan and Cruz, Jonathan J and Banbury, Colby R and Fu, William and Faust, Aleksandra and de Croon, Guido CHE and Reddi, Vijay Janapa},
	year         = 2019,
	journal      = {arXiv preprint arXiv:1909.11236}
}


@inproceedings{duisterhof2021sniffy,
	title        = {Sniffy bug: A fully autonomous swarm of gas-seeking nano quadcopters in cluttered environments},
	author       = {Duisterhof, Bardienus P and Li, Shushuai and Burgu{\'e}s, Javier and Reddi, Vijay Janapa and de Croon, Guido CHE},
	year         = 2021,
	booktitle    = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
	pages        = {9099--9106},
	organization = {IEEE}
}


@article{dwork2014algorithmic,
	title        = {The algorithmic foundations of differential privacy},
	author       = {Dwork, Cynthia and Roth, Aaron and others},
	year         = 2014,
	journal      = {Foundations and Trends{\textregistered} in Theoretical Computer Science},
	publisher    = {Now Publishers, Inc.},
	volume       = 9,
	number       = {3--4},
	pages        = {211--407}
}


@article{electronics12102287,
	title        = {Reviewing Federated Learning Aggregation Algorithms; Strategies, Contributions, Limitations and Future Perspectives},
	author       = {Moshawrab, Mohammad and Adda, Mehdi and Bouzouane, Abdenour and Ibrahim, Hussein and Raad, Ali},
	year         = 2023,
	journal      = {Electronics},
	volume       = 12,
	number       = 10,
	doi          = {10.3390/electronics12102287},
	issn         = {2079-9292},
	url          = {https://www.mdpi.com/2079-9292/12/10/2287},
	article-number = 2287
}


@misc{energyproblem,
	title        = {Computing's energy problem (and what we can do about it)},
	author       = {ISSCC},
	year         = 2014,
	url          = {https://ieeexplore.ieee.org/document/6757323},
	urldate      = {2014-03-06}
}


@article{esteva2017dermatologist,
	title        = {Dermatologist-level classification of skin cancer with deep neural networks},
	author       = {Esteva, Andre and Kuprel, Brett and Novoa, Roberto A and Ko, Justin and Swetter, Susan M and Blau, Helen M and Thrun, Sebastian},
	year         = 2017,
	journal      = {nature},
	publisher    = {Nature Publishing Group},
	volume       = 542,
	number       = 7639,
	pages        = {115--118}
}


@misc{fahim2021hls4ml,
	title        = {hls4ml: An Open-Source Codesign Workflow to Empower Scientific Low-Power Machine Learning Devices},
	author       = {Farah Fahim and Benjamin Hawks and Christian Herwig and James Hirschauer and Sergo Jindariani and Nhan Tran and Luca P. Carloni and Giuseppe Di Guglielmo and Philip Harris and Jeffrey Krupa and Dylan Rankin and Manuel Blanco Valentin and Josiah Hester and Yingyi Luo and John Mamish and Seda Orgrenci-Memik and Thea Aarrestad and Hamza Javed and Vladimir Loncar and Maurizio Pierini and Adrian Alan Pol and Sioni Summers and Javier Duarte and Scott Hauck and Shih-Chieh Hsu and Jennifer Ngadiuba and Mia Liu and Duc Hoang and Edward Kreinar and Zhenbin Wu},
	year         = 2021,
	eprint       = {2103.05579},
	archiveprefix = {arXiv},
	primaryclass = {cs.LG}
}


@misc{frankle_lottery_2019,
	title        = {The {Lottery} {Ticket} {Hypothesis}: {Finding} {Sparse}, {Trainable} {Neural} {Networks}},
	shorttitle   = {The {Lottery} {Ticket} {Hypothesis}},
	author       = {Frankle, Jonathan and Carbin, Michael},
	year         = 2019,
	month        = mar,
	publisher    = {arXiv},
	doi          = {10.48550/arXiv.1803.03635},
	url          = {http://arxiv.org/abs/1803.03635},
	urldate      = {2023-10-20},
	note         = {arXiv:1803.03635 [cs]},
	abstract     = {Neural network pruning techniques can reduce the parameter counts of trained networks by over 90\%, decreasing storage requirements and improving computational performance of inference without compromising accuracy. However, contemporary experience is that the sparse architectures produced by pruning are difficult to train from the start, which would similarly improve training performance. We find that a standard pruning technique naturally uncovers subnetworks whose initializations made them capable of training effectively. Based on these results, we articulate the "lottery ticket hypothesis:" dense, randomly-initialized, feed-forward networks contain subnetworks ("winning tickets") that - when trained in isolation - reach test accuracy comparable to the original network in a similar number of iterations. The winning tickets we find have won the initialization lottery: their connections have initial weights that make training particularly effective. We present an algorithm to identify winning tickets and a series of experiments that support the lottery ticket hypothesis and the importance of these fortuitous initializations. We consistently find winning tickets that are less than 10-20\% of the size of several fully-connected and convolutional feed-forward architectures for MNIST and CIFAR10. Above this size, the winning tickets that we find learn faster than the original network and reach higher test accuracy.},
	keywords     = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	file         = {arXiv Fulltext PDF:/Users/jeffreyma/Zotero/storage/6STHYGW5/Frankle and Carbin - 2019 - The Lottery Ticket Hypothesis Finding Sparse, Tra.pdf:application/pdf;arXiv.org Snapshot:/Users/jeffreyma/Zotero/storage/QGNSCTQB/1803.html:text/html}
}


@article{gaviria2022dollar,
	title        = {The Dollar Street Dataset: Images Representing the Geographic and Socioeconomic Diversity of the World},
	author       = {Gaviria Rojas, William and Diamos, Sudnya and Kini, Keertan and Kanter, David and Janapa Reddi, Vijay and Coleman, Cody},
	year         = 2022,
	journal      = {Advances in Neural Information Processing Systems},
	volume       = 35,
	pages        = {12979--12990}
}


@article{Gebru_Morgenstern_Vecchione_Vaughan_Wallach_III_Crawford_2021,
	title        = {Datasheets for datasets},
	author       = {Gebru, Timnit and Morgenstern, Jamie and Vecchione, Briana and Vaughan, Jennifer Wortman and Wallach, Hanna and III, Hal Daumé and Crawford, Kate},
	year         = 2021,
	journal      = {Communications of the ACM},
	volume       = 64,
	number       = 12,
	pages        = {86–92},
	doi          = {10.1145/3458723}
}


@article{goodfellow2020generative,
	title        = {Generative adversarial networks},
	author       = {Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
	year         = 2020,
	journal      = {Communications of the ACM},
	publisher    = {ACM New York, NY, USA},
	volume       = 63,
	number       = 11,
	pages        = {139--144}
}


@misc{Google,
	title        = {Information quality & content moderation},
	author       = {Google},
	url          = {https://blog.google/documents/83/}
}


@misc{gordon_morphnet_2018,
	title        = {{MorphNet}: {Fast} \& {Simple} {Resource}-{Constrained} {Structure} {Learning} of {Deep} {Networks}},
	shorttitle   = {{MorphNet}},
	author       = {Gordon, Ariel and Eban, Elad and Nachum, Ofir and Chen, Bo and Wu, Hao and Yang, Tien-Ju and Choi, Edward},
	year         = 2018,
	month        = apr,
	publisher    = {arXiv},
	doi          = {10.48550/arXiv.1711.06798},
	url          = {http://arxiv.org/abs/1711.06798},
	urldate      = {2023-10-20},
	note         = {arXiv:1711.06798 [cs, stat]},
	abstract     = {We present MorphNet, an approach to automate the design of neural network structures. MorphNet iteratively shrinks and expands a network, shrinking via a resource-weighted sparsifying regularizer on activations and expanding via a uniform multiplicative factor on all layers. In contrast to previous approaches, our method is scalable to large networks, adaptable to specific resource constraints (e.g. the number of floating-point operations per inference), and capable of increasing the network's performance. When applied to standard network architectures on a wide variety of datasets, our approach discovers novel structures in each domain, obtaining higher performance while respecting the resource constraint.},
	keywords     = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file         = {arXiv Fulltext PDF:/Users/jeffreyma/Zotero/storage/GV7N4CZC/Gordon et al. - 2018 - MorphNet Fast & Simple Resource-Constrained Struc.pdf:application/pdf;arXiv.org Snapshot:/Users/jeffreyma/Zotero/storage/K6FUV82F/1711.html:text/html}
}


@inproceedings{gordon2018morphnet,
	title        = {Morphnet: Fast \& simple resource-constrained structure learning of deep networks},
	author       = {Gordon, Ariel and Eban, Elad and Nachum, Ofir and Chen, Bo and Wu, Hao and Yang, Tien-Ju and Choi, Edward},
	year         = 2018,
	booktitle    = {Proceedings of the IEEE conference on computer vision and pattern recognition},
	pages        = {1586--1595}
}


@article{gruslys2016memory,
	title        = {Memory-efficient backpropagation through time},
	author       = {Gruslys, Audrunas and Munos, R{\'e}mi and Danihelka, Ivo and Lanctot, Marc and Graves, Alex},
	year         = 2016,
	journal      = {Advances in neural information processing systems},
	volume       = 29
}


@article{han2015deep,
	title        = {Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding},
	author       = {Han, Song and Mao, Huizi and Dally, William J},
	year         = 2015,
	journal      = {arXiv preprint arXiv:1510.00149}
}


@misc{han2016deep,
	title        = {Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding},
	author       = {Song Han and Huizi Mao and William J. Dally},
	year         = 2016,
	eprint       = {1510.00149},
	archiveprefix = {arXiv},
	primaryclass = {cs.CV}
}


@misc{he_structured_2023,
	title        = {Structured {Pruning} for {Deep} {Convolutional} {Neural} {Networks}: {A} survey},
	shorttitle   = {Structured {Pruning} for {Deep} {Convolutional} {Neural} {Networks}},
	author       = {He, Yang and Xiao, Lingao},
	year         = 2023,
	month        = mar,
	publisher    = {arXiv},
	doi          = {10.48550/arXiv.2303.00566},
	url          = {http://arxiv.org/abs/2303.00566},
	urldate      = {2023-10-20},
	note         = {arXiv:2303.00566 [cs]},
	abstract     = {The remarkable performance of deep Convolutional neural networks (CNNs) is generally attributed to their deeper and wider architectures, which can come with significant computational costs. Pruning neural networks has thus gained interest since it effectively lowers storage and computational costs. In contrast to weight pruning, which results in unstructured models, structured pruning provides the benefit of realistic acceleration by producing models that are friendly to hardware implementation. The special requirements of structured pruning have led to the discovery of numerous new challenges and the development of innovative solutions. This article surveys the recent progress towards structured pruning of deep CNNs. We summarize and compare the state-of-the-art structured pruning techniques with respect to filter ranking methods, regularization methods, dynamic execution, neural architecture search, the lottery ticket hypothesis, and the applications of pruning. While discussing structured pruning algorithms, we briefly introduce the unstructured pruning counterpart to emphasize their differences. Furthermore, we provide insights into potential research opportunities in the field of structured pruning. A curated list of neural network pruning papers can be found at https://github.com/he-y/Awesome-Pruning},
	keywords     = {Computer Science - Computer Vision and Pattern Recognition},
	file         = {arXiv Fulltext PDF:/Users/jeffreyma/Zotero/storage/K5RGQQA9/He and Xiao - 2023 - Structured Pruning for Deep Convolutional Neural N.pdf:application/pdf;arXiv.org Snapshot:/Users/jeffreyma/Zotero/storage/U7PVPU4C/2303.html:text/html}
}


@inproceedings{he2016deep,
	title        = {Deep residual learning for image recognition},
	author       = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	year         = 2016,
	booktitle    = {Proceedings of the IEEE conference on computer vision and pattern recognition},
	pages        = {770--778}
}


@inproceedings{hendrycks2021natural,
	title        = {Natural adversarial examples},
	author       = {Hendrycks, Dan and Zhao, Kevin and Basart, Steven and Steinhardt, Jacob and Song, Dawn},
	year         = 2021,
	booktitle    = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
	pages        = {15262--15271}
}


@misc{hinton_distilling_2015,
	title        = {Distilling the {Knowledge} in a {Neural} {Network}},
	author       = {Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
	year         = 2015,
	month        = mar,
	publisher    = {arXiv},
	doi          = {10.48550/arXiv.1503.02531},
	url          = {http://arxiv.org/abs/1503.02531},
	urldate      = {2023-10-20},
	note         = {arXiv:1503.02531 [cs, stat]},
	abstract     = {A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel.},
	keywords     = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
	file         = {arXiv Fulltext PDF:/Users/jeffreyma/Zotero/storage/VREDW45A/Hinton et al. - 2015 - Distilling the Knowledge in a Neural Network.pdf:application/pdf;arXiv.org Snapshot:/Users/jeffreyma/Zotero/storage/8MNJG4RP/1503.html:text/html}
}


@misc{hinton2015distilling,
	title        = {Distilling the Knowledge in a Neural Network},
	author       = {Geoffrey Hinton and Oriol Vinyals and Jeff Dean},
	year         = 2015,
	eprint       = {1503.02531},
	archiveprefix = {arXiv},
	primaryclass = {stat.ML}
}


@article{Holland_Hosny_Newman_Joseph_Chmielinski_2020,
	title        = {The Dataset Nutrition label},
	author       = {Holland, Sarah and Hosny, Ahmed and Newman, Sarah and Joseph, Joshua and Chmielinski, Kasia},
	year         = 2020,
	journal      = {Data Protection and Privacy},
	doi          = {10.5040/9781509932771.ch-001}
}


@inproceedings{hong2023publishing,
	title        = {Publishing Efficient On-device Models Increases Adversarial Vulnerability},
	author       = {Hong, Sanghyun and Carlini, Nicholas and Kurakin, Alexey},
	year         = 2023,
	booktitle    = {2023 IEEE Conference on Secure and Trustworthy Machine Learning (SaTML)},
	pages        = {271--290},
	organization = {IEEE}
}


@misc{howard_mobilenets_2017,
	title        = {{MobileNets}: {Efficient} {Convolutional} {Neural} {Networks} for {Mobile} {Vision} {Applications}},
	shorttitle   = {{MobileNets}},
	author       = {Howard, Andrew G. and Zhu, Menglong and Chen, Bo and Kalenichenko, Dmitry and Wang, Weijun and Weyand, Tobias and Andreetto, Marco and Adam, Hartwig},
	year         = 2017,
	month        = apr,
	publisher    = {arXiv},
	doi          = {10.48550/arXiv.1704.04861},
	url          = {http://arxiv.org/abs/1704.04861},
	urldate      = {2023-10-20},
	note         = {arXiv:1704.04861 [cs]},
	abstract     = {We present a class of efficient models called MobileNets for mobile and embedded vision applications. MobileNets are based on a streamlined architecture that uses depth-wise separable convolutions to build light weight deep neural networks. We introduce two simple global hyper-parameters that efficiently trade off between latency and accuracy. These hyper-parameters allow the model builder to choose the right sized model for their application based on the constraints of the problem. We present extensive experiments on resource and accuracy tradeoffs and show strong performance compared to other popular models on ImageNet classification. We then demonstrate the effectiveness of MobileNets across a wide range of applications and use cases including object detection, finegrain classification, face attributes and large scale geo-localization.},
	keywords     = {Computer Science - Computer Vision and Pattern Recognition},
	file         = {arXiv Fulltext PDF:/Users/jeffreyma/Zotero/storage/IJ9P9ID9/Howard et al. - 2017 - MobileNets Efficient Convolutional Neural Network.pdf:application/pdf;arXiv.org Snapshot:/Users/jeffreyma/Zotero/storage/D9TS95GJ/1704.html:text/html}
}


@misc{howard2017mobilenets,
	title        = {MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications},
	author       = {Andrew G. Howard and Menglong Zhu and Bo Chen and Dmitry Kalenichenko and Weijun Wang and Tobias Weyand and Marco Andreetto and Hartwig Adam},
	year         = 2017,
	journal      = {arXiv preprint arXiv:1704.04861},
	eprint       = {1704.04861},
	archiveprefix = {arXiv},
	primaryclass = {cs.CV}
}


@misc{iandola_squeezenet_2016,
	title        = {{SqueezeNet}: {AlexNet}-level accuracy with 50x fewer parameters and {\textless}0.{5MB} model size},
	shorttitle   = {{SqueezeNet}},
	author       = {Iandola, Forrest N. and Han, Song and Moskewicz, Matthew W. and Ashraf, Khalid and Dally, William J. and Keutzer, Kurt},
	year         = 2016,
	month        = nov,
	publisher    = {arXiv},
	doi          = {10.48550/arXiv.1602.07360},
	url          = {http://arxiv.org/abs/1602.07360},
	urldate      = {2023-10-20},
	note         = {arXiv:1602.07360 [cs]},
	abstract     = {Recent research on deep neural networks has focused primarily on improving accuracy. For a given accuracy level, it is typically possible to identify multiple DNN architectures that achieve that accuracy level. With equivalent accuracy, smaller DNN architectures offer at least three advantages: (1) Smaller DNNs require less communication across servers during distributed training. (2) Smaller DNNs require less bandwidth to export a new model from the cloud to an autonomous car. (3) Smaller DNNs are more feasible to deploy on FPGAs and other hardware with limited memory. To provide all of these advantages, we propose a small DNN architecture called SqueezeNet. SqueezeNet achieves AlexNet-level accuracy on ImageNet with 50x fewer parameters. Additionally, with model compression techniques we are able to compress SqueezeNet to less than 0.5MB (510x smaller than AlexNet). The SqueezeNet architecture is available for download here: https://github.com/DeepScale/SqueezeNet},
	keywords     = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
	file         = {arXiv Fulltext PDF:/Users/jeffreyma/Zotero/storage/X3ZX9UTZ/Iandola et al. - 2016 - SqueezeNet AlexNet-level accuracy with 50x fewer .pdf:application/pdf;arXiv.org Snapshot:/Users/jeffreyma/Zotero/storage/DHI96QVT/1602.html:text/html}
}


@article{iandola2016squeezenet,
	title        = {SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and< 0.5 MB model size},
	author       = {Iandola, Forrest N and Han, Song and Moskewicz, Matthew W and Ashraf, Khalid and Dally, William J and Keutzer, Kurt},
	year         = 2016,
	journal      = {arXiv preprint arXiv:1602.07360}
}


@inproceedings{ignatov2018ai,
	title        = {Ai benchmark: Running deep neural networks on android smartphones},
	author       = {Ignatov, Andrey and Timofte, Radu and Chou, William and Wang, Ke and Wu, Max and Hartley, Tim and Van Gool, Luc},
	year         = 2018,
	booktitle    = {Proceedings of the European Conference on Computer Vision (ECCV) Workshops},
	pages        = {0--0}
}


@inproceedings{ijcai2021p592,
	title        = {Hardware-Aware Neural Architecture Search: Survey and Taxonomy},
	author       = {Benmeziane, Hadjer and El Maghraoui, Kaoutar and Ouarnoughi, Hamza and Niar, Smail and Wistuba, Martin and Wang, Naigang},
	year         = 2021,
	month        = 8,
	booktitle    = {Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, {IJCAI-21}},
	publisher    = {International Joint Conferences on Artificial Intelligence Organization},
	pages        = {4322--4329},
	doi          = {10.24963/ijcai.2021/592},
	url          = {https://doi.org/10.24963/ijcai.2021/592},
	note         = {Survey Track},
	editor       = {Zhi-Hua Zhou}
}


@misc{intquantfordeepinf,
	title        = {Integer Quantization for Deep Learning Inference: Principles and Empirical Evaluation)},
	author       = {Wu and Judd, Zhang and Isaev, Micikevicius},
	year         = 2020,
	doi          = {10.48550/arXiv.2004.09602},
	url          = {https://arxiv.org/abs/2004.09602},
	urldate      = {2020-04-20}
}


@inproceedings{jia2014caffe,
	title        = {Caffe: Convolutional architecture for fast feature embedding},
	author       = {Jia, Yangqing and Shelhamer, Evan and Donahue, Jeff and Karayev, Sergey and Long, Jonathan and Girshick, Ross and Guadarrama, Sergio and Darrell, Trevor},
	year         = 2014,
	booktitle    = {Proceedings of the 22nd ACM international conference on Multimedia},
	pages        = {675--678}
}


@article{jia2023life,
	title        = {Life-threatening ventricular arrhythmia detection challenge in implantable cardioverter--defibrillators},
	author       = {Jia, Zhenge and Li, Dawei and Xu, Xiaowei and Li, Na and Hong, Feng and Ping, Lichuan and Shi, Yiyu},
	year         = 2023,
	journal      = {Nature Machine Intelligence},
	publisher    = {Nature Publishing Group UK London},
	volume       = 5,
	number       = 5,
	pages        = {554--555}
}


@misc{jiang2019accuracy,
	title        = {Accuracy vs. Efficiency: Achieving Both through FPGA-Implementation Aware Neural Architecture Search},
	author       = {Weiwen Jiang and Xinyi Zhang and Edwin H. -M. Sha and Lei Yang and Qingfeng Zhuge and Yiyu Shi and Jingtong Hu},
	year         = 2019,
	eprint       = {1901.11211},
	archiveprefix = {arXiv},
	primaryclass = {cs.DC}
}


@article{Johnson-Roberson_Barto_Mehta_Sridhar_Rosaen_Vasudevan_2017,
	title        = {Driving in the matrix: Can virtual worlds replace human-generated annotations for real world tasks?},
	author       = {Johnson-Roberson, Matthew and Barto, Charles and Mehta, Rounak and Sridhar, Sharath Nittur and Rosaen, Karl and Vasudevan, Ram},
	year         = 2017,
	journal      = {2017 IEEE International Conference on Robotics and Automation (ICRA)},
	doi          = {10.1109/icra.2017.7989092}
}


@article{jordan_machine_2015,
	title        = {Machine learning: {Trends}, perspectives, and prospects},
	shorttitle   = {Machine learning},
	author       = {Jordan, M. I. and Mitchell, T. M.},
	year         = 2015,
	month        = jul,
	journal      = {Science},
	volume       = 349,
	number       = 6245,
	pages        = {255--260},
	doi          = {10.1126/science.aaa8415},
	issn         = {0036-8075, 1095-9203},
	url          = {https://www.science.org/doi/10.1126/science.aaa8415},
	urldate      = {2023-10-25},
	language     = {en},
	file         = {Jordan and Mitchell - 2015 - Machine learning Trends, perspectives, and prospe.pdf:/Users/alex/Zotero/storage/RGU3CQ4Q/Jordan and Mitchell - 2015 - Machine learning Trends, perspectives, and prospe.pdf:application/pdf}
}


@inproceedings{jouppi2017datacenter,
	title        = {In-datacenter performance analysis of a tensor processing unit},
	author       = {Jouppi, Norman P and Young, Cliff and Patil, Nishant and Patterson, David and Agrawal, Gaurav and Bajwa, Raminder and Bates, Sarah and Bhatia, Suresh and Boden, Nan and Borchers, Al and others},
	year         = 2017,
	booktitle    = {Proceedings of the 44th annual international symposium on computer architecture},
	pages        = {1--12}
}


@article{kairouz2015secure,
	title        = {Secure multi-party differential privacy},
	author       = {Kairouz, Peter and Oh, Sewoong and Viswanath, Pramod},
	year         = 2015,
	journal      = {Advances in neural information processing systems},
	volume       = 28
}


@article{karargyris2023federated,
	title        = {Federated benchmarking of medical artificial intelligence with MedPerf},
	author       = {Karargyris, Alexandros and Umeton, Renato and Sheller, Micah J and Aristizabal, Alejandro and George, Johnu and Wuest, Anna and Pati, Sarthak and Kassem, Hasan and Zenk, Maximilian and Baid, Ujjwal and others},
	year         = 2023,
	journal      = {Nature Machine Intelligence},
	publisher    = {Nature Publishing Group UK London},
	volume       = 5,
	number       = 7,
	pages        = {799--810}
}


@article{kiela2021dynabench,
	title        = {Dynabench: Rethinking benchmarking in NLP},
	author       = {Kiela, Douwe and Bartolo, Max and Nie, Yixin and Kaushik, Divyansh and Geiger, Atticus and Wu, Zhengxuan and Vidgen, Bertie and Prasad, Grusha and Singh, Amanpreet and Ringshia, Pratik and others},
	year         = 2021,
	journal      = {arXiv preprint arXiv:2104.14337}
}


@inproceedings{koh2021wilds,
	title        = {Wilds: A benchmark of in-the-wild distribution shifts},
	author       = {Koh, Pang Wei and Sagawa, Shiori and Marklund, Henrik and Xie, Sang Michael and Zhang, Marvin and Balsubramani, Akshay and Hu, Weihua and Yasunaga, Michihiro and Phillips, Richard Lanas and Gao, Irena and others},
	year         = 2021,
	booktitle    = {International Conference on Machine Learning},
	pages        = {5637--5664},
	organization = {PMLR}
}


@article{kolda_tensor_2009,
	title        = {Tensor {Decompositions} and {Applications}},
	author       = {Kolda, Tamara G. and Bader, Brett W.},
	year         = 2009,
	month        = aug,
	journal      = {SIAM Review},
	volume       = 51,
	number       = 3,
	pages        = {455--500},
	doi          = {10.1137/07070111X},
	issn         = {0036-1445, 1095-7200},
	url          = {http://epubs.siam.org/doi/10.1137/07070111X},
	urldate      = {2023-10-20},
	abstract     = {This survey provides an overview of higher-order tensor decompositions, their applications, and available software. A tensor is a multidimensional or N -way array. Decompositions of higher-order tensors (i.e., N -way arrays with N ≥ 3) have applications in psychometrics, chemometrics, signal processing, numerical linear algebra, computer vision, numerical analysis, data mining, neuroscience, graph analysis, and elsewhere. Two particular tensor decompositions can be considered to be higher-order extensions of the matrix singular value decomposition: CANDECOMP/PARAFAC (CP) decomposes a tensor as a sum of rank-one tensors, and the Tucker decomposition is a higher-order form of principal component analysis. There are many other tensor decompositions, including INDSCAL, PARAFAC2, CANDELINC, DEDICOM, and PARATUCK2 as well as nonnegative variants of all of the above. The N-way Toolbox, Tensor Toolbox, and Multilinear Engine are examples of software packages for working with tensors.},
	language     = {en},
	file         = {Kolda and Bader - 2009 - Tensor Decompositions and Applications.pdf:/Users/jeffreyma/Zotero/storage/Q7ZG2267/Kolda and Bader - 2009 - Tensor Decompositions and Applications.pdf:application/pdf}
}


@article{koshti2011cumulative,
	title        = {Cumulative sum control chart},
	author       = {Koshti, VV},
	year         = 2011,
	journal      = {International journal of physics and mathematical sciences},
	volume       = 1,
	number       = 1,
	pages        = {28--32}
}


@misc{krishna2023raman,
	title        = {RAMAN: A Re-configurable and Sparse tinyML Accelerator for Inference on Edge},
	author       = {Adithya Krishna and Srikanth Rohit Nudurupati and Chandana D G and Pritesh Dwivedi and André van Schaik and Mahesh Mehendale and Chetan Singh Thakur},
	year         = 2023,
	eprint       = {2306.06493},
	archiveprefix = {arXiv},
	primaryclass = {cs.NE}
}


@article{krishnamoorthi2018quantizing,
	title        = {Quantizing deep convolutional networks for efficient inference: A whitepaper},
	author       = {Krishnamoorthi, Raghuraman},
	year         = 2018,
	journal      = {arXiv preprint arXiv:1806.08342}
}


@article{Krishnan_Rajpurkar_Topol_2022,
	title        = {Self-supervised learning in medicine and Healthcare},
	author       = {Krishnan, Rayan and Rajpurkar, Pranav and Topol, Eric J.},
	year         = 2022,
	journal      = {Nature Biomedical Engineering},
	volume       = 6,
	number       = 12,
	pages        = {1346–1352},
	doi          = {10.1038/s41551-022-00914-1}
}


@article{krizhevsky2012imagenet,
	title        = {Imagenet classification with deep convolutional neural networks},
	author       = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
	year         = 2012,
	journal      = {Advances in neural information processing systems},
	volume       = 25
}


@inproceedings{kung1979systolic,
	title        = {Systolic arrays (for VLSI)},
	author       = {Kung, Hsiang Tsung and Leiserson, Charles E},
	year         = 1979,
	booktitle    = {Sparse Matrix Proceedings 1978},
	volume       = 1,
	pages        = {256--282},
	organization = {Society for industrial and applied mathematics Philadelphia, PA, USA}
}


@misc{kung2018packing,
	title        = {Packing Sparse Convolutional Neural Networks for Efficient Systolic Array Implementations: Column Combining Under Joint Optimization},
	author       = {H. T. Kung and Bradley McDanel and Sai Qian Zhang},
	year         = 2018,
	eprint       = {1811.04770},
	archiveprefix = {arXiv},
	primaryclass = {cs.LG}
}


@incollection{kurkova_survey_2018,
	title        = {A {Survey} on {Deep} {Transfer} {Learning}},
	author       = {Tan, Chuanqi and Sun, Fuchun and Kong, Tao and Zhang, Wenchang and Yang, Chao and Liu, Chunfang},
	year         = 2018,
	booktitle    = {Artificial {Neural} {Networks} and {Machine} {Learning} – {ICANN} 2018},
	publisher    = {Springer International Publishing},
	address      = {Cham},
	volume       = 11141,
	pages        = {270--279},
	doi          = {10.1007/978-3-030-01424-7_27},
	isbn         = {978-3-030-01423-0 978-3-030-01424-7},
	url          = {http://link.springer.com/10.1007/978-3-030-01424-7_27},
	urldate      = {2023-10-26},
	note         = {Series Title: Lecture Notes in Computer Science},
	language     = {en},
	editor       = {Kůrková, Věra and Manolopoulos, Yannis and Hammer, Barbara and Iliadis, Lazaros and Maglogiannis, Ilias},
	file         = {Tan et al. - 2018 - A Survey on Deep Transfer Learning.pdf:/Users/alex/Zotero/storage/5NZ36SGB/Tan et al. - 2018 - A Survey on Deep Transfer Learning.pdf:application/pdf}
}


@misc{kuzmin2022fp8,
	title        = {FP8 Quantization: The Power of the Exponent},
	author       = {Andrey Kuzmin and Mart Van Baalen and Yuwei Ren and Markus Nagel and Jorn Peters and Tijmen Blankevoort},
	year         = 2022,
	eprint       = {2208.09225},
	archiveprefix = {arXiv},
	primaryclass = {cs.LG}
}


@misc{kwon_tinytrain_2023,
	title        = {{TinyTrain}: {Deep} {Neural} {Network} {Training} at the {Extreme} {Edge}},
	shorttitle   = {{TinyTrain}},
	author       = {Kwon, Young D. and Li, Rui and Venieris, Stylianos I. and Chauhan, Jagmohan and Lane, Nicholas D. and Mascolo, Cecilia},
	year         = 2023,
	month        = jul,
	publisher    = {arXiv},
	url          = {http://arxiv.org/abs/2307.09988},
	urldate      = {2023-10-26},
	note         = {arXiv:2307.09988 [cs]},
	language     = {en},
	keywords     = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file         = {Kwon et al. - 2023 - TinyTrain Deep Neural Network Training at the Ext.pdf:/Users/alex/Zotero/storage/L2ST472U/Kwon et al. - 2023 - TinyTrain Deep Neural Network Training at the Ext.pdf:application/pdf}
}


@article{kwon2023tinytrain,
	title        = {TinyTrain: Deep Neural Network Training at the Extreme Edge},
	author       = {Kwon, Young D and Li, Rui and Venieris, Stylianos I and Chauhan, Jagmohan and Lane, Nicholas D and Mascolo, Cecilia},
	year         = 2023,
	journal      = {arXiv preprint arXiv:2307.09988}
}


@misc{Labelbox,
	journal      = {Labelbox},
	url          = {https://labelbox.com/}
}


@article{lai2018cmsis,
	title        = {Cmsis-nn: Efficient neural network kernels for arm cortex-m cpus},
	author       = {Lai, Liangzhen and Suda, Naveen and Chandra, Vikas},
	year         = 2018,
	journal      = {arXiv preprint arXiv:1801.06601}
}


@misc{lai2018cmsisnn,
	title        = {CMSIS-NN: Efficient Neural Network Kernels for Arm Cortex-M CPUs},
	author       = {Liangzhen Lai and Naveen Suda and Vikas Chandra},
	year         = 2018,
	eprint       = {1801.06601},
	archiveprefix = {arXiv},
	primaryclass = {cs.NE}
}


@inproceedings{lecun_optimal_1989,
	title        = {Optimal {Brain} {Damage}},
	author       = {LeCun, Yann and Denker, John and Solla, Sara},
	year         = 1989,
	booktitle    = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher    = {Morgan-Kaufmann},
	volume       = 2,
	url          = {https://proceedings.neurips.cc/paper/1989/hash/6c9882bbac1c7093bd25041881277658-Abstract.html},
	urldate      = {2023-10-20},
	abstract     = {We  have used  information-theoretic ideas  to derive  a class of prac(cid:173) tical  and  nearly  optimal schemes  for  adapting the size  of a  neural  network.  By  removing  unimportant  weights  from  a  network, sev(cid:173) eral  improvements  can  be  expected:  better  generalization, fewer  training examples required,  and improved speed  of learning and/or classification.  The  basic  idea  is  to  use  second-derivative informa(cid:173) tion to make a  tradeoff between  network  complexity  and training  set error.  Experiments confirm  the usefulness  of the methods on a  real-world  application.},
	file         = {Full Text PDF:/Users/jeffreyma/Zotero/storage/BYHQQSST/LeCun et al. - 1989 - Optimal Brain Damage.pdf:application/pdf}
}


@article{lecun1989optimal,
	title        = {Optimal brain damage},
	author       = {LeCun, Yann and Denker, John and Solla, Sara},
	year         = 1989,
	journal      = {Advances in neural information processing systems},
	volume       = 2
}


@article{li2014communication,
	title        = {Communication efficient distributed machine learning with the parameter server},
	author       = {Li, Mu and Andersen, David G and Smola, Alexander J and Yu, Kai},
	year         = 2014,
	journal      = {Advances in Neural Information Processing Systems},
	volume       = 27
}


@article{li2016lightrnn,
	title        = {LightRNN: Memory and computation-efficient recurrent neural networks},
	author       = {Li, Xiang and Qin, Tao and Yang, Jian and Liu, Tie-Yan},
	year         = 2016,
	journal      = {Advances in Neural Information Processing Systems},
	volume       = 29
}


@article{li2017deep,
	title        = {Deep reinforcement learning: An overview},
	author       = {Li, Yuxi},
	year         = 2017,
	journal      = {arXiv preprint arXiv:1701.07274}
}


@article{li2017learning,
	title        = {Learning without forgetting},
	author       = {Li, Zhizhong and Hoiem, Derek},
	year         = 2017,
	journal      = {IEEE transactions on pattern analysis and machine intelligence},
	publisher    = {IEEE},
	volume       = 40,
	number       = 12,
	pages        = {2935--2947}
}


@article{li2019edge,
	title        = {Edge AI: On-demand accelerating deep neural network inference via edge computing},
	author       = {Li, En and Zeng, Liekang and Zhou, Zhi and Chen, Xu},
	year         = 2019,
	journal      = {IEEE Transactions on Wireless Communications},
	publisher    = {IEEE},
	volume       = 19,
	number       = 1,
	pages        = {447--457}
}


@misc{liao_can_2023,
	title        = {Can {Unstructured} {Pruning} {Reduce} the {Depth} in {Deep} {Neural} {Networks}?},
	author       = {Liao, Zhu and Quétu, Victor and Nguyen, Van-Tam and Tartaglione, Enzo},
	year         = 2023,
	month        = aug,
	publisher    = {arXiv},
	doi          = {10.48550/arXiv.2308.06619},
	url          = {http://arxiv.org/abs/2308.06619},
	urldate      = {2023-10-20},
	note         = {arXiv:2308.06619 [cs]},
	abstract     = {Pruning is a widely used technique for reducing the size of deep neural networks while maintaining their performance. However, such a technique, despite being able to massively compress deep models, is hardly able to remove entire layers from a model (even when structured): is this an addressable task? In this study, we introduce EGP, an innovative Entropy Guided Pruning algorithm aimed at reducing the size of deep neural networks while preserving their performance. The key focus of EGP is to prioritize pruning connections in layers with low entropy, ultimately leading to their complete removal. Through extensive experiments conducted on popular models like ResNet-18 and Swin-T, our findings demonstrate that EGP effectively compresses deep neural networks while maintaining competitive performance levels. Our results not only shed light on the underlying mechanism behind the advantages of unstructured pruning, but also pave the way for further investigations into the intricate relationship between entropy, pruning techniques, and deep learning performance. The EGP algorithm and its insights hold great promise for advancing the field of network compression and optimization. The source code for EGP is released open-source.},
	keywords     = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file         = {arXiv Fulltext PDF:/Users/jeffreyma/Zotero/storage/V6P3XB5H/Liao et al. - 2023 - Can Unstructured Pruning Reduce the Depth in Deep .pdf:application/pdf;arXiv.org Snapshot:/Users/jeffreyma/Zotero/storage/WSQ4ZUH4/2308.html:text/html}
}


@misc{lin_-device_2022,
	title        = {On-{Device} {Training} {Under} {256KB} {Memory}},
	author       = {Lin, Ji and Zhu, Ligeng and Chen, Wei-Ming and Wang, Wei-Chen and Gan, Chuang and Han, Song},
	year         = 2022,
	month        = nov,
	publisher    = {arXiv},
	url          = {http://arxiv.org/abs/2206.15472},
	urldate      = {2023-10-26},
	note         = {arXiv:2206.15472 [cs]},
	language     = {en},
	keywords     = {Computer Science - Computer Vision and Pattern Recognition},
	annote       = {Comment: NeurIPS 2022},
	file         = {Lin et al. - 2022 - On-Device Training Under 256KB Memory.pdf:/Users/alex/Zotero/storage/GMF6SWGT/Lin et al. - 2022 - On-Device Training Under 256KB Memory.pdf:application/pdf}
}


@misc{lin_-device_2022-1,
	title        = {On-{Device} {Training} {Under} {256KB} {Memory}},
	author       = {Lin, Ji and Zhu, Ligeng and Chen, Wei-Ming and Wang, Wei-Chen and Gan, Chuang and Han, Song},
	year         = 2022,
	month        = nov,
	publisher    = {arXiv},
	url          = {http://arxiv.org/abs/2206.15472},
	urldate      = {2023-10-25},
	note         = {arXiv:2206.15472 [cs]},
	language     = {en},
	keywords     = {Computer Science - Computer Vision and Pattern Recognition},
	annote       = {Comment: NeurIPS 2022},
	file         = {Lin et al. - 2022 - On-Device Training Under 256KB Memory.pdf:/Users/alex/Zotero/storage/DNIY32R2/Lin et al. - 2022 - On-Device Training Under 256KB Memory.pdf:application/pdf}
}


@misc{lin_mcunet_2020,
	title        = {{MCUNet}: {Tiny} {Deep} {Learning} on {IoT} {Devices}},
	shorttitle   = {{MCUNet}},
	author       = {Lin, Ji and Chen, Wei-Ming and Lin, Yujun and Cohn, John and Gan, Chuang and Han, Song},
	year         = 2020,
	month        = nov,
	publisher    = {arXiv},
	doi          = {10.48550/arXiv.2007.10319},
	url          = {http://arxiv.org/abs/2007.10319},
	urldate      = {2023-10-20},
	note         = {arXiv:2007.10319 [cs]},
	abstract     = {Machine learning on tiny IoT devices based on microcontroller units (MCU) is appealing but challenging: the memory of microcontrollers is 2-3 orders of magnitude smaller even than mobile phones. We propose MCUNet, a framework that jointly designs the efficient neural architecture (TinyNAS) and the lightweight inference engine (TinyEngine), enabling ImageNet-scale inference on microcontrollers. TinyNAS adopts a two-stage neural architecture search approach that first optimizes the search space to fit the resource constraints, then specializes the network architecture in the optimized search space. TinyNAS can automatically handle diverse constraints (i.e.device, latency, energy, memory) under low search costs.TinyNAS is co-designed with TinyEngine, a memory-efficient inference library to expand the search space and fit a larger model. TinyEngine adapts the memory scheduling according to the overall network topology rather than layer-wise optimization, reducing the memory usage by 4.8x, and accelerating the inference by 1.7-3.3x compared to TF-Lite Micro and CMSIS-NN. MCUNet is the first to achieves {\textgreater}70\% ImageNet top1 accuracy on an off-the-shelf commercial microcontroller, using 3.5x less SRAM and 5.7x less Flash compared to quantized MobileNetV2 and ResNet-18. On visual\&audio wake words tasks, MCUNet achieves state-of-the-art accuracy and runs 2.4-3.4x faster than MobileNetV2 and ProxylessNAS-based solutions with 3.7-4.1x smaller peak SRAM. Our study suggests that the era of always-on tiny machine learning on IoT devices has arrived. Code and models can be found here: https://tinyml.mit.edu.},
	keywords     = {Computer Science - Computer Vision and Pattern Recognition},
	file         = {arXiv Fulltext PDF:/Users/jeffreyma/Zotero/storage/IX2JN4P9/Lin et al. - 2020 - MCUNet Tiny Deep Learning on IoT Devices.pdf:application/pdf;arXiv.org Snapshot:/Users/jeffreyma/Zotero/storage/BAKHZ46Y/2007.html:text/html},
	language     = {en},
	annote       = {Comment: NeurIPS 2020 (spotlight)}
}


@inproceedings{lin2014microsoft,
	title        = {Microsoft coco: Common objects in context},
	author       = {Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Hays, James and Perona, Pietro and Ramanan, Deva and Doll{\'a}r, Piotr and Zitnick, C Lawrence},
	year         = 2014,
	booktitle    = {Computer Vision--ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13},
	pages        = {740--755},
	organization = {Springer}
}


@article{lin2020mcunet,
	title        = {Mcunet: Tiny deep learning on iot devices},
	author       = {Lin, Ji and Chen, Wei-Ming and Lin, Yujun and Gan, Chuang and Han, Song and others},
	year         = 2020,
	journal      = {Advances in Neural Information Processing Systems},
	volume       = 33,
	pages        = {11711--11722},
	eprint       = {2007.10319},
	archiveprefix = {arXiv},
	primaryclass = {cs.CV}
}


@article{lin2022device,
	title        = {On-device training under 256kb memory},
	author       = {Lin, Ji and Zhu, Ligeng and Chen, Wei-Ming and Wang, Wei-Chen and Gan, Chuang and Han, Song},
	year         = 2022,
	journal      = {Advances in Neural Information Processing Systems},
	volume       = 35,
	pages        = {22941--22954}
}


@misc{lu_notes_2016,
	title        = {Notes on {Low}-rank {Matrix} {Factorization}},
	author       = {Lu, Yuan and Yang, Jie},
	year         = 2016,
	month        = may,
	publisher    = {arXiv},
	doi          = {10.48550/arXiv.1507.00333},
	url          = {http://arxiv.org/abs/1507.00333},
	urldate      = {2023-10-20},
	note         = {arXiv:1507.00333 [cs]},
	abstract     = {Low-rank matrix factorization (MF) is an important technique in data science. The key idea of MF is that there exists latent structures in the data, by uncovering which we could obtain a compressed representation of the data. By factorizing an original matrix to low-rank matrices, MF provides a unified method for dimension reduction, clustering, and matrix completion. In this article we review several important variants of MF, including: Basic MF, Non-negative MF, Orthogonal non-negative MF. As can be told from their names, non-negative MF and orthogonal non-negative MF are variants of basic MF with non-negativity and/or orthogonality constraints. Such constraints are useful in specific senarios. In the first part of this article, we introduce, for each of these models, the application scenarios, the distinctive properties, and the optimizing method. By properly adapting MF, we can go beyond the problem of clustering and matrix completion. In the second part of this article, we will extend MF to sparse matrix compeletion, enhance matrix compeletion using various regularization methods, and make use of MF for (semi-)supervised learning by introducing latent space reinforcement and transformation. We will see that MF is not only a useful model but also as a flexible framework that is applicable for various prediction problems.},
	keywords     = {Computer Science - Information Retrieval, Computer Science - Machine Learning, Mathematics - Numerical Analysis},
	file         = {arXiv Fulltext PDF:/Users/jeffreyma/Zotero/storage/4QED5ZU9/Lu and Yang - 2016 - Notes on Low-rank Matrix Factorization.pdf:application/pdf;arXiv.org Snapshot:/Users/jeffreyma/Zotero/storage/XIBZBDJQ/1507.html:text/html}
}


@article{lundberg2017unified,
	title        = {A unified approach to interpreting model predictions},
	author       = {Lundberg, Scott M and Lee, Su-In},
	year         = 2017,
	journal      = {Advances in neural information processing systems},
	volume       = 30
}


@article{mattson2020mlperf,
	title        = {Mlperf training benchmark},
	author       = {Mattson, Peter and Cheng, Christine and Diamos, Gregory and Coleman, Cody and Micikevicius, Paulius and Patterson, David and Tang, Hanlin and Wei, Gu-Yeon and Bailis, Peter and Bittorf, Victor and others},
	year         = 2020,
	journal      = {Proceedings of Machine Learning and Systems},
	volume       = 2,
	pages        = {336--349}
}


@inproceedings{mcmahan2017communication,
	title        = {Communication-efficient learning of deep networks from decentralized data},
	author       = {McMahan, Brendan and Moore, Eider and Ramage, Daniel and Hampson, Seth and y Arcas, Blaise Aguera},
	year         = 2017,
	booktitle    = {Artificial intelligence and statistics},
	pages        = {1273--1282},
	organization = {PMLR}
}


@inproceedings{mcmahan2023communicationefficient,
	title        = {Communication-efficient learning of deep networks from decentralized data},
	author       = {McMahan, Brendan and Moore, Eider and Ramage, Daniel and Hampson, Seth and y Arcas, Blaise Aguera},
	year         = 2017,
	booktitle    = {Artificial intelligence and statistics},
	pages        = {1273--1282},
	organization = {PMLR}
}


@article{moshawrab2023reviewing,
	title        = {Reviewing Federated Learning Aggregation Algorithms; Strategies, Contributions, Limitations and Future Perspectives},
	author       = {Moshawrab, Mohammad and Adda, Mehdi and Bouzouane, Abdenour and Ibrahim, Hussein and Raad, Ali},
	year         = 2023,
	journal      = {Electronics},
	publisher    = {MDPI},
	volume       = 12,
	number       = 10,
	pages        = 2287
}


@inproceedings{nguyen2023re,
	title        = {Re-thinking Model Inversion Attacks Against Deep Neural Networks},
	author       = {Nguyen, Ngoc-Bao and Chandrasegaran, Keshigeyan and Abdollahzadeh, Milad and Cheung, Ngai-Man},
	year         = 2023,
	booktitle    = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
	pages        = {16384--16393}
}


@misc{noauthor_deep_nodate,
	title        = {Deep {Learning} {Model} {Compression} (ii) {\textbar} by {Ivy} {Gu} {\textbar} {Medium}},
	author       = {Ivy Gu},
	year         = {2023},
	url          = {https://ivygdy.medium.com/deep-learning-model-compression-ii-546352ea9453},
	urldate      = {2023-10-20}
}


@misc{noauthor_introduction_nodate,
	title        = {An {Introduction} to {Separable} {Convolutions} - {Analytics} {Vidhya}},
	author       = {Hegde, Sumant},
	year         = {2023},
	url          = {https://www.analyticsvidhya.com/blog/2021/11/an-introduction-to-separable-convolutions/},
	urldate      = {2023-10-20}
}


@misc{noauthor_knowledge_nodate,
	title        = {Knowledge {Distillation} - {Neural} {Network} {Distiller}},
	author       = {IntelLabs},
	year         = {2023},
	url          = {https://intellabs.github.io/distiller/knowledge_distillation.html},
	urldate      = {2023-10-20}
}


@article{Northcutt_Athalye_Mueller_2021,
	title        = {Pervasive Label Errors in Test Sets Destabilize Machine Learning Benchmarks},
	author       = {Northcutt, Curtis G and Athalye, Anish and Mueller, Jonas},
	year         = {2021},
	month        = {Mar},
	journal      = {arXiv},
	doi          = {&nbsp;  https://doi.org/10.48550/arXiv.2103.14749 arXiv-issued DOI via DataCite}
}


@inproceedings{ooko2021tinyml,
	title        = {TinyML in Africa: Opportunities and challenges},
	author       = {Ooko, Samson Otieno and Ogore, Marvin Muyonga and Nsenga, Jimmy and Zennaro, Marco},
	year         = {2021},
	booktitle    = {2021 IEEE Globecom Workshops (GC Wkshps)},
	pages        = {1--6},
	organization = {IEEE}
}


@misc{ou_low_2023,
	title        = {Low {Rank} {Optimization} for {Efficient} {Deep} {Learning}: {Making} {A} {Balance} between {Compact} {Architecture} and {Fast} {Training}},
	shorttitle   = {Low {Rank} {Optimization} for {Efficient} {Deep} {Learning}},
	author       = {Ou, Xinwei and Chen, Zhangxin and Zhu, Ce and Liu, Yipeng},
	year         = {2023},
	month        = {Mar},
	publisher    = {arXiv},
	url          = {http://arxiv.org/abs/2303.13635},
	urldate      = {2023-10-20},
	note         = {arXiv:2303.13635 [cs]},
	abstract     = {Deep neural networks have achieved great success in many data processing applications. However, the high computational complexity and storage cost makes deep learning hard to be used on resource-constrained devices, and it is not environmental-friendly with much power cost. In this paper, we focus on low-rank optimization for efficient deep learning techniques. In the space domain, deep neural networks are compressed by low rank approximation of the network parameters, which directly reduces the storage requirement with a smaller number of network parameters. In the time domain, the network parameters can be trained in a few subspaces, which enables efficient training for fast convergence. The model compression in the spatial domain is summarized into three categories as pre-train, pre-set, and compression-aware methods, respectively. With a series of integrable techniques discussed, such as sparse pruning, quantization, and entropy coding, we can ensemble them in an integration framework with lower computational complexity and storage. Besides of summary of recent technical advances, we have two findings for motivating future works: one is that the effective rank outperforms other sparse measures for network compression. The other is a spatial and temporal balance for tensorized neural networks.},
	keywords     = {Computer Science - Machine Learning},
	file         = {arXiv.org Snapshot:/Users/jeffreyma/Zotero/storage/SPSZ2HR9/2303.html:text/html;Full Text PDF:/Users/jeffreyma/Zotero/storage/6TUEBTEX/Ou et al. - 2023 - Low Rank Optimization for Efficient Deep Learning.pdf:application/pdf}
}


@article{pan_survey_2010,
	title        = {A {Survey} on {Transfer} {Learning}},
	author       = {Pan, Sinno Jialin and Yang, Qiang},
	year         = {2010},
	month        = {Oct},
	journal      = {IEEE Transactions on Knowledge and Data Engineering},
	volume       = {22},
	number       = {10},
	pages        = {1345--1359},
	doi          = {10.1109/TKDE.2009.191},
	issn         = {1041-4347},
	url          = {http://ieeexplore.ieee.org/document/5288526/},
	urldate      = {2023-10-25},
	language     = {en},
	file         = {Pan and Yang - 2010 - A Survey on Transfer Learning.pdf:/Users/alex/Zotero/storage/T3H8E5K8/Pan and Yang - 2010 - A Survey on Transfer Learning.pdf:application/pdf}
}


@article{pan2009survey,
	title        = {A survey on transfer learning},
	author       = {Pan, Sinno Jialin and Yang, Qiang},
	year         = {2009},
	journal      = {IEEE Transactions on knowledge and data engineering},
	publisher    = {IEEE},
	volume       = {22},
	number       = {10},
	pages        = {1345--1359}
}


@article{parisi_continual_2019,
	title        = {Continual lifelong learning with neural networks: {A} review},
	shorttitle   = {Continual lifelong learning with neural networks},
	author       = {Parisi, German I. and Kemker, Ronald and Part, Jose L. and Kanan, Christopher and Wermter, Stefan},
	year         = {2019},
	month        = {May},
	journal      = {Neural Networks},
	volume       = {113},
	pages        = {54--71},
	doi          = {10.1016/j.neunet.2019.01.012},
	issn         = {08936080},
	url          = {https://linkinghub.elsevier.com/retrieve/pii/S0893608019300231},
	urldate      = {2023-10-26},
	language     = {en},
	file         = {Parisi et al. - 2019 - Continual lifelong learning with neural networks .pdf:/Users/alex/Zotero/storage/TCGHD5TW/Parisi et al. - 2019 - Continual lifelong learning with neural networks .pdf:application/pdf}
}


@article{paszke2019pytorch,
	title        = {Pytorch: An imperative style, high-performance deep learning library},
	author       = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and others},
	year         = {2019},
	journal      = {Advances in neural information processing systems},
	volume       = {32}
}


@misc{Perrigo_2023,
	title        = {OpenAI used Kenyan workers on less than $2 per hour: Exclusive},
	author       = {Perrigo, Billy},
	year         = {2023},
	month        = {Jan},
	journal      = {Time},
	publisher    = {Time},
	url          = {https://time.com/6247678/openai-chatgpt-kenya-workers/}
}


@inproceedings{Prakash_2023,
	title        = {{CFU} Playground: Full-Stack Open-Source Framework for Tiny Machine Learning ({TinyML}) Acceleration on {FPGAs}},
	author       = {Shvetank Prakash and Tim Callahan and Joseph Bushagour and Colby Banbury and Alan V. Green and Pete Warden and Tim Ansell and Vijay Janapa Reddi},
	year         = {2023},
	month        = {apr},
	booktitle    = {2023 {IEEE} International Symposium on Performance Analysis of Systems and Software ({ISPASS})},
	publisher    = {{IEEE}},
	doi          = {10.1109/ispass57527.2023.00024},
	url          = {https://doi.org/10.1109%2Fispass57527.2023.00024}
}


@inproceedings{prakash_cfu_2023,
	title        = {{CFU} {Playground}: {Full}-{Stack} {Open}-{Source} {Framework} for {Tiny} {Machine} {Learning} ({tinyML}) {Acceleration} on {FPGAs}},
	shorttitle   = {{CFU} {Playground}},
	author       = {Prakash, Shvetank and Callahan, Tim and Bushagour, Joseph and Banbury, Colby and Green, Alan V. and Warden, Pete and Ansell, Tim and Reddi, Vijay Janapa},
	year         = {2023},
	month        = {Apr},
	booktitle    = {2023 {IEEE} {International} {Symposium} on {Performance} {Analysis} of {Systems} and {Software} ({ISPASS})},
	pages        = {157--167},
	doi          = {10.1109/ISPASS57527.2023.00024},
	url          = {http://arxiv.org/abs/2201.01863},
	urldate      = {2023-10-25},
	note         = {arXiv:2201.01863 [cs]},
	language     = {en},
	keywords     = {Computer Science - Machine Learning, Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Hardware Architecture},
	file         = {Prakash et al. - 2023 - CFU Playground Full-Stack Open-Source Framework f.pdf:/Users/alex/Zotero/storage/BZNRIDTL/Prakash et al. - 2023 - CFU Playground Full-Stack Open-Source Framework f.pdf:application/pdf}
}


@article{preparednesspublic,
	title        = {Public Health Law},
	author       = {Preparedness, Emergency}
}


@article{Pushkarna_Zaldivar_Kjartansson_2022,
	title        = {Data cards: Purposeful and transparent dataset documentation for responsible ai},
	author       = {Pushkarna, Mahima and Zaldivar, Andrew and Kjartansson, Oddur},
	year         = {2022},
	journal      = {2022 ACM Conference on Fairness, Accountability, and Transparency},
	doi          = {10.1145/3531146.3533231}
}


@article{qi_efficient_2021,
	title        = {An efficient pruning scheme of deep neural networks for {Internet} of {Things} applications},
	author       = {Qi, Chen and Shen, Shibo and Li, Rongpeng and Zhifeng, Zhao and Liu, Qing and Liang, Jing and Zhang, Honggang},
	year         = {2021},
	month        = {Jun},
	journal      = {EURASIP Journal on Advances in Signal Processing},
	volume       = 2021,
	doi          = {10.1186/s13634-021-00744-4},
	abstract     = {Nowadays, deep neural networks (DNNs) have been rapidly deployed to realize a number of functionalities like sensing, imaging, classification, recognition, etc. However, the computational-intensive requirement of DNNs makes it difficult to be applicable for resource-limited Internet of Things (IoT) devices. In this paper, we propose a novel pruning-based paradigm that aims to reduce the computational cost of DNNs, by uncovering a more compact structure and learning the effective weights therein, on the basis of not compromising the expressive capability of DNNs. In particular, our algorithm can achieve efficient end-to-end training that transfers a redundant neural network to a compact one with a specifically targeted compression rate directly. We comprehensively evaluate our approach on various representative benchmark datasets and compared with typical advanced convolutional neural network (CNN) architectures. The experimental results verify the superior performance and robust effectiveness of our scheme. For example, when pruning VGG on CIFAR-10, our proposed scheme is able to significantly reduce its FLOPs (floating-point operations) and number of parameters with a proportion of 76.2\% and 94.1\%, respectively, while still maintaining a satisfactory accuracy. To sum up, our scheme could facilitate the integration of DNNs into the common machine-learning-based IoT framework and establish distributed training of neural networks in both cloud and edge.},
	file         = {Full Text PDF:/Users/jeffreyma/Zotero/storage/AGWCC5VS/Qi et al. - 2021 - An efficient pruning scheme of deep neural network.pdf:application/pdf}
}


@misc{quantdeep,
	title        = {Quantizing deep convolutional networks for efficient inference: A whitepaper},
	author       = {Krishnamoorthi},
	year         = 2018,
	month        = jun,
	publisher    = {arXiv},
	doi          = {10.48550/arXiv.1806.08342},
	url          = {https://arxiv.org/abs/1806.08342},
	urldate      = {2018-06-21}
}


@article{ramcharan2017deep,
	title        = {Deep learning for image-based cassava disease detection},
	author       = {Ramcharan, Amanda and Baranowski, Kelsee and McCloskey, Peter and Ahmed, Babuali and Legg, James and Hughes, David P},
	year         = 2017,
	journal      = {Frontiers in plant science},
	publisher    = {Frontiers Media SA},
	volume       = 8,
	pages        = 1852
}


@misc{Rao_2021,
	author       = {Rao, Ravi},
	year         = 2021,
	month        = {Dec},
	journal      = {www.wevolver.com},
	url          = {https://www.wevolver.com/article/tinyml-unlocks-new-possibilities-for-sustainable-development-technologies}
}


@article{Ratner_Hancock_Dunnmon_Goldman_Ré_2018,
	title        = {Snorkel metal: Weak supervision for multi-task learning.},
	author       = {Ratner, Alex and Hancock, Braden and Dunnmon, Jared and Goldman, Roger and Ré, Christopher},
	year         = 2018,
	journal      = {Proceedings of the Second Workshop on Data Management for End-To-End Machine Learning},
	doi          = {10.1145/3209889.3209898}
}


@inproceedings{reddi2020mlperf,
	title        = {Mlperf inference benchmark},
	author       = {Reddi, Vijay Janapa and Cheng, Christine and Kanter, David and Mattson, Peter and Schmuelling, Guenther and Wu, Carole-Jean and Anderson, Brian and Breughe, Maximilien and Charlebois, Mark and Chou, William and others},
	year         = 2020,
	booktitle    = {2020 ACM/IEEE 47th Annual International Symposium on Computer Architecture (ISCA)},
	pages        = {446--459},
	organization = {IEEE}
}


@inproceedings{ribeiro2016should,
	title        = {" Why should i trust you?" Explaining the predictions of any classifier},
	author       = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
	year         = 2016,
	booktitle    = {Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining},
	pages        = {1135--1144}
}


@book{rosenblatt1957perceptron,
	title        = {The perceptron, a perceiving and recognizing automaton Project Para},
	author       = {Rosenblatt, Frank},
	year         = 1957,
	publisher    = {Cornell Aeronautical Laboratory}
}


@inproceedings{rouhani2017tinydl,
	title        = {TinyDL: Just-in-time deep learning solution for constrained embedded systems},
	author       = {Rouhani, Bita and Mirhoseini, Azalia and Koushanfar, Farinaz},
	year         = 2017,
	month        = {05},
	pages        = {1--4},
	doi          = {10.1109/ISCAS.2017.8050343}
}


@article{rumelhart1986learning,
	title        = {Learning representations by back-propagating errors},
	author       = {Rumelhart, David E and Hinton, Geoffrey E and Williams, Ronald J},
	year         = 1986,
	journal      = {nature},
	publisher    = {Nature Publishing Group UK London},
	volume       = 323,
	number       = 6088,
	pages        = {533--536}
}


@article{ruvolo_ella_nodate,
	title        = {{ELLA}: {An} {Efficient} {Lifelong} {Learning} {Algorithm}},
	author       = {Ruvolo, Paul and Eaton, Eric},
	language     = {en},
	file         = {Ruvolo and Eaton - ELLA An Efficient Lifelong Learning Algorithm.pdf:/Users/alex/Zotero/storage/QA5G29GL/Ruvolo and Eaton - ELLA An Efficient Lifelong Learning Algorithm.pdf:application/pdf}
}


@misc{ScaleAI,
	journal      = {ScaleAI},
	url          = {https://scale.com/data-engine}
}


@inproceedings{schwarzschild2021just,
	title        = {Just how toxic is data poisoning? a unified benchmark for backdoor and data poisoning attacks},
	author       = {Schwarzschild, Avi and Goldblum, Micah and Gupta, Arjun and Dickerson, John P and Goldstein, Tom},
	year         = 2021,
	booktitle    = {International Conference on Machine Learning},
	pages        = {9389--9398},
	organization = {PMLR}
}


@misc{see_compression_2016,
	title        = {Compression of {Neural} {Machine} {Translation} {Models} via {Pruning}},
	author       = {See, Abigail and Luong, Minh-Thang and Manning, Christopher D.},
	year         = 2016,
	month        = jun,
	publisher    = {arXiv},
	doi          = {10.48550/arXiv.1606.09274},
	url          = {http://arxiv.org/abs/1606.09274},
	urldate      = {2023-10-20},
	note         = {arXiv:1606.09274 [cs]},
	abstract     = {Neural Machine Translation (NMT), like many other deep learning domains, typically suffers from over-parameterization, resulting in large storage sizes. This paper examines three simple magnitude-based pruning schemes to compress NMT models, namely class-blind, class-uniform, and class-distribution, which differ in terms of how pruning thresholds are computed for the different classes of weights in the NMT architecture. We demonstrate the efficacy of weight pruning as a compression technique for a state-of-the-art NMT system. We show that an NMT model with over 200 million parameters can be pruned by 40\% with very little performance loss as measured on the WMT'14 English-German translation task. This sheds light on the distribution of redundancy in the NMT architecture. Our main result is that with retraining, we can recover and even surpass the original performance with an 80\%-pruned model.},
	keywords     = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Neural and Evolutionary Computing},
	file         = {arXiv Fulltext PDF:/Users/jeffreyma/Zotero/storage/2CJ4TSNR/See et al. - 2016 - Compression of Neural Machine Translation Models v.pdf:application/pdf}
}


@inproceedings{seide2016cntk,
	title        = {CNTK: Microsoft's open-source deep-learning toolkit},
	author       = {Seide, Frank and Agarwal, Amit},
	year         = 2016,
	booktitle    = {Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining},
	pages        = {2135--2135}
}


@misc{sevilla_compute_2022,
	title        = {Compute {Trends} {Across} {Three} {Eras} of {Machine} {Learning}},
	author       = {Sevilla, Jaime and Heim, Lennart and Ho, Anson and Besiroglu, Tamay and Hobbhahn, Marius and Villalobos, Pablo},
	year         = 2022,
	month        = mar,
	publisher    = {arXiv},
	url          = {http://arxiv.org/abs/2202.05924},
	urldate      = {2023-10-25},
	note         = {arXiv:2202.05924 [cs]},
	language     = {en},
	keywords     = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computers and Society},
	file         = {Sevilla et al. - 2022 - Compute Trends Across Three Eras of Machine Learni.pdf:/Users/alex/Zotero/storage/24N9RZ72/Sevilla et al. - 2022 - Compute Trends Across Three Eras of Machine Learni.pdf:application/pdf}
}


@article{seyedzadeh2018machine,
	title        = {Machine learning for estimation of building energy consumption and performance: a review},
	author       = {Seyedzadeh, Saleh and Rahimian, Farzad Pour and Glesk, Ivan and Roper, Marc},
	year         = 2018,
	journal      = {Visualization in Engineering},
	publisher    = {Springer},
	volume       = 6,
	pages        = {1--20}
}


@article{shamir1979share,
	title        = {How to share a secret},
	author       = {Shamir, Adi},
	year         = 1979,
	journal      = {Communications of the ACM},
	publisher    = {ACm New York, NY, USA},
	volume       = 22,
	number       = 11,
	pages        = {612--613}
}


@article{Sheng_Zhang_2019,
	title        = {Machine learning with crowdsourcing: A brief summary of the past research and Future Directions},
	author       = {Sheng, Victor S. and Zhang, Jing},
	year         = 2019,
	journal      = {Proceedings of the AAAI Conference on Artificial Intelligence},
	volume       = 33,
	number       = {01},
	pages        = {9837–9843},
	doi          = {10.1609/aaai.v33i01.33019837}
}


@misc{Sheth_2022,
	title        = {Eletect - TinyML and IOT based Smart Wildlife Tracker},
	author       = {Sheth, Dhruv},
	year         = 2022,
	month        = {Mar},
	journal      = {Hackster.io},
	url          = {https://www.hackster.io/dhruvsheth_/eletect-tinyml-and-iot-based-smart-wildlife-tracker-c03e5a}
}


@inproceedings{shi2022data,
	title        = {Data selection for efficient model update in federated learning},
	author       = {Shi, Hongrui and Radu, Valentin},
	year         = 2022,
	booktitle    = {Proceedings of the 2nd European Workshop on Machine Learning and Systems},
	pages        = {72--78}
}


@article{smestad2023systematic,
	title        = {A Systematic Literature Review on Client Selection in Federated Learning},
	author       = {Smestad, Carl and Li, Jingyue},
	year         = 2023,
	journal      = {arXiv preprint arXiv:2306.04862}
}


@misc{smoothquant,
	title        = {SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models},
	author       = {Xiao and Lin, Seznec and Wu, Demouth and Han},
	year         = 2023,
	doi          = {10.48550/arXiv.2211.10438},
	url          = {https://arxiv.org/abs/2211.10438},
	urldate      = {2023-06-05},
	abstract     = {Large language models (LLMs) show excellent performance but are compute- and memory-intensive. Quantization can reduce memory and accelerate inference. However, existing methods cannot maintain accuracy and hardware efficiency at the same time. We propose SmoothQuant, a training-free, accuracy-preserving, and general-purpose post-training quantization (PTQ) solution to enable 8-bit weight, 8-bit activation (W8A8) quantization for LLMs. Based on the fact that weights are easy to quantize while activations are not, SmoothQuant smooths the activation outliers by offline migrating the quantization difficulty from activations to weights with a mathematically equivalent transformation. SmoothQuant enables an INT8 quantization of both weights and activations for all the matrix multiplications in LLMs, including OPT, BLOOM, GLM, MT-NLG, and LLaMA family. We demonstrate up to 1.56x speedup and 2x memory reduction for LLMs with negligible loss in accuracy. SmoothQuant enables serving 530B LLM within a single node. Our work offers a turn-key solution that reduces hardware costs and democratizes LLMs.}
}


@misc{surveyofquant,
	title        = {A Survey of Quantization Methods for Efficient Neural Network Inference)},
	author       = {Gholami and Kim, Dong and Yao, Mahoney and Keutzer},
	year         = 2021,
	doi          = {10.48550/arXiv.2103.13630},
	url          = {https://arxiv.org/abs/2103.13630},
	urldate      = {2021-06-21},
	abstract     = {As soon as abstract mathematical computations were adapted to computation on digital computers, the problem of efficient representation, manipulation, and communication of the numerical values in those computations arose. Strongly related to the problem of numerical representation is the problem of quantization: in what manner should a set of continuous real-valued numbers be distributed over a fixed discrete set of numbers to minimize the number of bits required and also to maximize the accuracy of the attendant computations? This perennial problem of quantization is particularly relevant whenever memory and/or computational resources are severely restricted, and it has come to the forefront in recent years due to the remarkable performance of Neural Network models in computer vision, natural language processing, and related areas. Moving from floating-point representations to low-precision fixed integer values represented in four bits or less holds the potential to reduce the memory footprint and latency by a factor of 16x; and, in fact, reductions of 4x to 8x are often realized in practice in these applications. Thus, it is not surprising that quantization has emerged recently as an important and very active sub-area of research in the efficient implementation of computations associated with Neural Networks. In this article, we survey approaches to the problem of quantizing the numerical values in deep Neural Network computations, covering the advantages/disadvantages of current methods. With this survey and its organization, we hope to have presented a useful snapshot of the current research in quantization for Neural Networks and to have given an intelligent organization to ease the evaluation of future research in this area.}
}


@misc{tan_efficientnet_2020,
	title        = {{EfficientNet}: {Rethinking} {Model} {Scaling} for {Convolutional} {Neural} {Networks}},
	shorttitle   = {{EfficientNet}},
	author       = {Tan, Mingxing and Le, Quoc V.},
	year         = 2020,
	month        = sep,
	publisher    = {arXiv},
	doi          = {10.48550/arXiv.1905.11946},
	url          = {http://arxiv.org/abs/1905.11946},
	urldate      = {2023-10-20},
	note         = {arXiv:1905.11946 [cs, stat]},
	abstract     = {Convolutional Neural Networks (ConvNets) are commonly developed at a fixed resource budget, and then scaled up for better accuracy if more resources are available. In this paper, we systematically study model scaling and identify that carefully balancing network depth, width, and resolution can lead to better performance. Based on this observation, we propose a new scaling method that uniformly scales all dimensions of depth/width/resolution using a simple yet highly effective compound coefficient. We demonstrate the effectiveness of this method on scaling up MobileNets and ResNet. To go even further, we use neural architecture search to design a new baseline network and scale it up to obtain a family of models, called EfficientNets, which achieve much better accuracy and efficiency than previous ConvNets. In particular, our EfficientNet-B7 achieves state-of-the-art 84.3\% top-1 accuracy on ImageNet, while being 8.4x smaller and 6.1x faster on inference than the best existing ConvNet. Our EfficientNets also transfer well and achieve state-of-the-art accuracy on CIFAR-100 (91.7\%), Flowers (98.8\%), and 3 other transfer learning datasets, with an order of magnitude fewer parameters. Source code is at https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet.},
	keywords     = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
	file         = {arXiv Fulltext PDF:/Users/jeffreyma/Zotero/storage/KISBF35I/Tan and Le - 2020 - EfficientNet Rethinking Model Scaling for Convolu.pdf:application/pdf;arXiv.org Snapshot:/Users/jeffreyma/Zotero/storage/TUD4PH4M/1905.html:text/html}
}


@inproceedings{tan2019mnasnet,
	title        = {Mnasnet: Platform-aware neural architecture search for mobile},
	author       = {Tan, Mingxing and Chen, Bo and Pang, Ruoming and Vasudevan, Vijay and Sandler, Mark and Howard, Andrew and Le, Quoc V},
	year         = 2019,
	booktitle    = {Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
	pages        = {2820--2828}
}


@misc{tan2020efficientnet,
	title        = {EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks},
	author       = {Mingxing Tan and Quoc V. Le},
	year         = 2020,
	eprint       = {1905.11946},
	archiveprefix = {arXiv},
	primaryclass = {cs.LG}
}


@misc{Team_2023,
	title        = {Data-centric AI for the Enterprise},
	author       = {Team, Snorkel},
	year         = 2023,
	month        = {Aug},
	journal      = {Snorkel AI},
	url          = {https://snorkel.ai/}
}


@misc{Thefutur92:online,
	title        = {The future is being built on Arm: Market diversification continues to drive strong royalty and licensing growth as ecosystem reaches quarter of a trillion chips milestone – Arm®},
	author       = {ARM.com},
	note         = {(Accessed on 09/16/2023)},
	howpublished = {\url{https://www.arm.com/company/news/2023/02/arm-announces-q3-fy22-results}}
}


@misc{threefloat,
	title        = {Three Floating Point Formats},
	author       = {Google},
	year         = 2023,
	url          = {https://storage.googleapis.com/gweb-cloudblog-publish/images/Three_floating-point_formats.max-624x261.png},
	urldate      = {2023-10-20}
}


@article{tirtalistyani2022indonesia,
	title        = {Indonesia rice irrigation system: Time for innovation},
	author       = {Tirtalistyani, Rose and Murtiningrum, Murtiningrum and Kanwar, Rameshwar S},
	year         = 2022,
	journal      = {Sustainability},
	publisher    = {MDPI},
	volume       = 14,
	number       = 19,
	pages        = 12477
}


@inproceedings{tokui2015chainer,
	title        = {Chainer: a next-generation open source framework for deep learning},
	author       = {Tokui, Seiya and Oono, Kenta and Hido, Shohei and Clayton, Justin},
	year         = 2015,
	booktitle    = {Proceedings of workshop on machine learning systems (LearningSys) in the twenty-ninth annual conference on neural information processing systems (NIPS)},
	volume       = 5,
	pages        = {1--6}
}


@article{van_de_ven_three_2022,
	title        = {Three types of incremental learning},
	author       = {Van De Ven, Gido M. and Tuytelaars, Tinne and Tolias, Andreas S.},
	year         = 2022,
	month        = dec,
	journal      = {Nature Machine Intelligence},
	volume       = 4,
	number       = 12,
	pages        = {1185--1197},
	doi          = {10.1038/s42256-022-00568-3},
	issn         = {2522-5839},
	url          = {https://www.nature.com/articles/s42256-022-00568-3},
	urldate      = {2023-10-26},
	language     = {en},
	file         = {Van De Ven et al. - 2022 - Three types of incremental learning.pdf:/Users/alex/Zotero/storage/5ZAHXMQN/Van De Ven et al. - 2022 - Three types of incremental learning.pdf:application/pdf}
}


@article{vaswani2017attention,
	title        = {Attention is all you need},
	author       = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
	year         = 2017,
	journal      = {Advances in neural information processing systems},
	volume       = 30
}


@misc{Vectorbo78:online,
	title        = {Vector-borne diseases},
	note         = {(Accessed on 10/17/2023)},
	howpublished = {\url{https://www.who.int/news-room/fact-sheets/detail/vector-borne-diseases}}
}


@misc{Verma_2022,
	title        = {Elephant AI},
	author       = {Verma, Team Dual_Boot: Swapnil},
	year         = 2022,
	month        = {Mar},
	journal      = {Hackster.io},
	url          = {https://www.hackster.io/dual_boot/elephant-ai-ba71e9}
}


@misc{villalobos_machine_2022,
	title        = {Machine {Learning} {Model} {Sizes} and the {Parameter} {Gap}},
	author       = {Villalobos, Pablo and Sevilla, Jaime and Besiroglu, Tamay and Heim, Lennart and Ho, Anson and Hobbhahn, Marius},
	year         = 2022,
	month        = jul,
	publisher    = {arXiv},
	url          = {http://arxiv.org/abs/2207.02852},
	urldate      = {2023-10-25},
	note         = {arXiv:2207.02852 [cs]},
	language     = {en},
	keywords     = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computers and Society, Computer Science - Computation and Language},
	file         = {Villalobos et al. - 2022 - Machine Learning Model Sizes and the Parameter Gap.pdf:/Users/alex/Zotero/storage/WW69A82B/Villalobos et al. - 2022 - Machine Learning Model Sizes and the Parameter Gap.pdf:application/pdf}
}


@misc{villalobos_trends_2022,
	title        = {Trends in {Training} {Dataset} {Sizes}},
	author       = {Villalobos, Pablo and Ho, Anson},
	year         = 2022,
	month        = sep,
	journal      = {Epoch AI},
	url          = {https://epochai.org/blog/trends-in-training-dataset-sizes}
}


@misc{VinBrain,
	journal      = {VinBrain},
	url          = {https://vinbrain.net/aiscaler}
}


@article{vinuesa2020role,
	title        = {The role of artificial intelligence in achieving the Sustainable Development Goals},
	author       = {Vinuesa, Ricardo and Azizpour, Hossein and Leite, Iolanda and Balaam, Madeline and Dignum, Virginia and Domisch, Sami and Fell{\"a}nder, Anna and Langhans, Simone Daniela and Tegmark, Max and Fuso Nerini, Francesco},
	year         = 2020,
	journal      = {Nature communications},
	publisher    = {Nature Publishing Group},
	volume       = 11,
	number       = 1,
	pages        = {1--10}
}


@article{warden2018speech,
	title        = {Speech commands: A dataset for limited-vocabulary speech recognition},
	author       = {Warden, Pete},
	year         = 2018,
	journal      = {arXiv preprint arXiv:1804.03209}
}


@book{warden2019tinyml,
	title        = {Tinyml: Machine learning with tensorflow lite on arduino and ultra-low-power microcontrollers},
	author       = {Warden, Pete and Situnayake, Daniel},
	year         = 2019,
	publisher    = {O'Reilly Media}
}


@article{weiss_survey_2016,
	title        = {A survey of transfer learning},
	author       = {Weiss, Karl and Khoshgoftaar, Taghi M. and Wang, DingDing},
	year         = 2016,
	month        = dec,
	journal      = {Journal of Big Data},
	volume       = 3,
	number       = 1,
	pages        = 9,
	doi          = {10.1186/s40537-016-0043-6},
	issn         = {2196-1115},
	url          = {http://journalofbigdata.springeropen.com/articles/10.1186/s40537-016-0043-6},
	urldate      = {2023-10-25},
	language     = {en},
	file         = {Weiss et al. - 2016 - A survey of transfer learning.pdf:/Users/alex/Zotero/storage/3FN2Y6EA/Weiss et al. - 2016 - A survey of transfer learning.pdf:application/pdf}
}


@inproceedings{wu2019fbnet,
	title        = {Fbnet: Hardware-aware efficient convnet design via differentiable neural architecture search},
	author       = {Wu, Bichen and Dai, Xiaoliang and Zhang, Peizhao and Wang, Yanghan and Sun, Fei and Wu, Yiming and Tian, Yuandong and Vajda, Peter and Jia, Yangqing and Keutzer, Kurt},
	year         = 2019,
	booktitle    = {Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
	pages        = {10734--10742}
}


@article{wu2022sustainable,
	title        = {Sustainable ai: Environmental implications, challenges and opportunities},
	author       = {Wu, Carole-Jean and Raghavendra, Ramya and Gupta, Udit and Acun, Bilge and Ardalani, Newsha and Maeng, Kiwan and Chang, Gloria and Aga, Fiona and Huang, Jinshi and Bai, Charles and others},
	year         = 2022,
	journal      = {Proceedings of Machine Learning and Systems},
	volume       = 4,
	pages        = {795--813}
}


@inproceedings{xie2020adversarial,
	title        = {Adversarial examples improve image recognition},
	author       = {Xie, Cihang and Tan, Mingxing and Gong, Boqing and Wang, Jiang and Yuille, Alan L and Le, Quoc V},
	year         = 2020,
	booktitle    = {Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
	pages        = {819--828}
}


@article{xu2018alternating,
	title        = {Alternating multi-bit quantization for recurrent neural networks},
	author       = {Xu, Chen and Yao, Jianqiang and Lin, Zhouchen and Ou, Wenwu and Cao, Yuanbin and Wang, Zhirong and Zha, Hongbin},
	year         = 2018,
	journal      = {arXiv preprint arXiv:1802.00150}
}


@article{xu2023demystifying,
	title        = {Demystifying CLIP Data},
	author       = {Xu, Hu and Xie, Saining and Tan, Xiaoqing Ellen and Huang, Po-Yao and Howes, Russell and Sharma, Vasu and Li, Shang-Wen and Ghosh, Gargi and Zettlemoyer, Luke and Feichtenhofer, Christoph},
	year         = 2023,
	journal      = {arXiv preprint arXiv:2309.16671}
}


@article{xu2023federated,
	title        = {Federated Learning of Gboard Language Models with Differential Privacy},
	author       = {Xu, Zheng and Zhang, Yanxiang and Andrew, Galen and Choquette-Choo, Christopher A and Kairouz, Peter and McMahan, H Brendan and Rosenstock, Jesse and Zhang, Yuanbo},
	year         = 2023,
	journal      = {arXiv preprint arXiv:2305.18465}
}


@article{yamashita2023coffee,
	title        = {Coffee disease classification at the edge using deep learning},
	author       = {Yamashita, Jo{\~a}o Vitor Yukio Bordin and Leite, Jo{\~a}o Paulo RR},
	year         = 2023,
	journal      = {Smart Agricultural Technology},
	publisher    = {Elsevier},
	volume       = 4,
	pages        = 100183
}


@misc{yang2020coexploration,
	title        = {Co-Exploration of Neural Architectures and Heterogeneous ASIC Accelerator Designs Targeting Multiple Tasks},
	author       = {Lei Yang and Zheyu Yan and Meng Li and Hyoukjun Kwon and Liangzhen Lai and Tushar Krishna and Vikas Chandra and Weiwen Jiang and Yiyu Shi},
	year         = 2020,
	eprint       = {2002.04116},
	archiveprefix = {arXiv},
	primaryclass = {cs.LG}
}


@inproceedings{yang2023online,
	title        = {Online Model Compression for Federated Learning with Large Models},
	author       = {Yang, Tien-Ju and Xiao, Yonghui and Motta, Giovanni and Beaufays, Fran{\c{c}}oise and Mathews, Rajiv and Chen, Mingqing},
	year         = 2023,
	booktitle    = {ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
	pages        = {1--5},
	organization = {IEEE}
}


@inproceedings{zennaro2022tinyml,
	title        = {TinyML: applied AI for development},
	author       = {Zennaro, Marco and Plancher, Brian and Reddi, V Janapa},
	year         = 2022,
	booktitle    = {The UN 7th Multi-stakeholder Forum on Science, Technology and Innovation for the Sustainable Development Goals},
	pages        = {2022--05}
}


@article{zennarobridging,
	title        = {Bridging the Digital Divide: the Promising Impact of TinyML for Developing Countries},
	author       = {Zennaro, Marco and Plancher, Brian and Reddi, Vijay Janapa}
}


@inproceedings{Zhang_2020_CVPR_Workshops,
	title        = {Fast Hardware-Aware Neural Architecture Search},
	author       = {Zhang, Li Lyna and Yang, Yuqing and Jiang, Yuhang and Zhu, Wenwu and Liu, Yunxin},
	year         = 2020,
	month        = {June},
	booktitle    = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops}
}


@misc{zhang2019autoshrink,
	title        = {AutoShrink: A Topology-aware NAS for Discovering Efficient Neural Architecture},
	author       = {Tunhou Zhang and Hsin-Pai Cheng and Zhenwen Li and Feng Yan and Chengyu Huang and Hai Li and Yiran Chen},
	year         = 2019,
	eprint       = {1911.09251},
	archiveprefix = {arXiv},
	primaryclass = {cs.LG}
}


@article{zhao2018federated,
	title        = {Federated learning with non-iid data},
	author       = {Zhao, Yue and Li, Meng and Lai, Liangzhen and Suda, Naveen and Civin, Damon and Chandra, Vikas},
	year         = 2018,
	journal      = {arXiv preprint arXiv:1806.00582}
}


@misc{zhou_deep_2023,
	title        = {Deep {Class}-{Incremental} {Learning}: {A} {Survey}},
	shorttitle   = {Deep {Class}-{Incremental} {Learning}},
	author       = {Zhou, Da-Wei and Wang, Qi-Wei and Qi, Zhi-Hong and Ye, Han-Jia and Zhan, De-Chuan and Liu, Ziwei},
	year         = 2023,
	month        = feb,
	publisher    = {arXiv},
	url          = {http://arxiv.org/abs/2302.03648},
	urldate      = {2023-10-26},
	note         = {arXiv:2302.03648 [cs]},
	language     = {en},
	keywords     = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	annote       = {Comment: Code is available at https://github.com/zhoudw-zdw/CIL\_Survey/},
	file         = {Zhou et al. - 2023 - Deep Class-Incremental Learning A Survey.pdf:/Users/alex/Zotero/storage/859VZG7W/Zhou et al. - 2023 - Deep Class-Incremental Learning A Survey.pdf:application/pdf}
}


@misc{noauthor_who_nodate,
	title = {Who {Invented} the {Microprocessor}? - {CHM}},
	url = {https://computerhistory.org/blog/who-invented-the-microprocessor/},
	urldate = {2023-11-07},
}


@book{weik_survey_1955,
	title = {A {Survey} of {Domestic} {Electronic} {Digital} {Computing} {Systems}},
	language = {en},
	publisher = {Ballistic Research Laboratories},
	author = {Weik, Martin H.},
	year = {1955},
}


@inproceedings{brown_language_2020,
	title = {Language {Models} are {Few}-{Shot} {Learners}},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper_files/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html},
	abstract = {We demonstrate that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even becoming competitive with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting.  For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model.  GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks. We also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora.},
	urldate = {2023-11-07},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
	year = {2020},
	pages = {1877--1901},
}


@misc{jia_dissecting_2018,
	title = {Dissecting the {NVIDIA} {Volta} {GPU} {Architecture} via {Microbenchmarking}},
	url = {http://arxiv.org/abs/1804.06826},
	abstract = {Every year, novel NVIDIA GPU designs are introduced. This rapid architectural and technological progression, coupled with a reluctance by manufacturers to disclose low-level details, makes it difficult for even the most proficient GPU software designers to remain up-to-date with the technological advances at a microarchitectural level. To address this dearth of public, microarchitectural-level information on the novel NVIDIA GPUs, independent researchers have resorted to microbenchmarks-based dissection and discovery. This has led to a prolific line of publications that shed light on instruction encoding, and memory hierarchy's geometry and features at each level. Namely, research that describes the performance and behavior of the Kepler, Maxwell and Pascal architectures. In this technical report, we continue this line of research by presenting the microarchitectural details of the NVIDIA Volta architecture, discovered through microbenchmarks and instruction set disassembly. Additionally, we compare quantitatively our Volta findings against its predecessors, Kepler, Maxwell and Pascal.},
	urldate = {2023-11-07},
	publisher = {arXiv},
	author = {Jia, Zhe and Maggioni, Marco and Staiger, Benjamin and Scarpazza, Daniele P.},
	month = apr,
	year = {2018},
	note = {arXiv:1804.06826 [cs]},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Performance},
}


@article{jia2019beyond,
  title={Beyond Data and Model Parallelism for Deep Neural Networks.},
  author={Jia, Zhihao and Zaharia, Matei and Aiken, Alex},
  journal={Proceedings of Machine Learning and Systems},
  volume={1},
  pages={1--13},
  year={2019}
}


@inproceedings{raina_large-scale_2009,
	address = {Montreal Quebec Canada},
	title = {Large-scale deep unsupervised learning using graphics processors},
	isbn = {978-1-60558-516-1},
	url = {https://dl.acm.org/doi/10.1145/1553374.1553486},
	doi = {10.1145/1553374.1553486},
	language = {en},
	urldate = {2023-11-07},
	booktitle = {Proceedings of the 26th {Annual} {International} {Conference} on {Machine} {Learning}},
	publisher = {ACM},
	author = {Raina, Rajat and Madhavan, Anand and Ng, Andrew Y.},
	month = jun,
	year = {2009},
	pages = {873--880},
}


@misc{noauthor_amd_nodate,
	title = {{AMD} {Radeon} {RX} 7000 {Series} {Desktop} {Graphics} {Cards}},
	url = {https://www.amd.com/en/graphics/radeon-rx-graphics},
	urldate = {2023-11-07},
}


@misc{noauthor_intel_nodate,
	title = {Intel® {Arc}™ {Graphics} {Overview}},
	url = {https://www.intel.com/content/www/us/en/products/details/discrete-gpus/arc.html},
	abstract = {Find out how Intel® Arc Graphics unlock lifelike gaming and seamless content creation.},
	language = {en},
	urldate = {2023-11-07},
	journal = {Intel},
}


@article{lindholm_nvidia_2008,
	title = {{NVIDIA} {Tesla}: {A} {Unified} {Graphics} and {Computing} {Architecture}},
	volume = {28},
	issn = {1937-4143},
	shorttitle = {{NVIDIA} {Tesla}},
	url = {https://ieeexplore.ieee.org/document/4523358},
	doi = {10.1109/MM.2008.31},
	abstract = {To enable flexible, programmable graphics and high-performance computing, NVIDIA has developed the Tesla scalable unified graphics and parallel computing architecture. Its scalable parallel array of processors is massively multithreaded and programmable in C or via graphics APIs.},
	number = {2},
	urldate = {2023-11-07},
	journal = {IEEE Micro},
	author = {Lindholm, Erik and Nickolls, John and Oberman, Stuart and Montrym, John},
	month = mar,
	year = {2008},
	note = {Conference Name: IEEE Micro},
	pages = {39--55},
}


@article{dally_evolution_2021,
	title = {Evolution of the {Graphics} {Processing} {Unit} ({GPU})},
	volume = {41},
	issn = {1937-4143},
	url = {https://ieeexplore.ieee.org/document/9623445},
	doi = {10.1109/MM.2021.3113475},
	abstract = {Graphics processing units (GPUs) power today’s fastest supercomputers, are the dominant platform for deep learning, and provide the intelligence for devices ranging from self-driving cars to robots and smart cameras. They also generate compelling photorealistic images at real-time frame rates. GPUs have evolved by adding features to support new use cases. NVIDIA’s GeForce 256, the first GPU, was a dedicated processor for real-time graphics, an application that demands large amounts of floating-point arithmetic for vertex and fragment shading computations and high memory bandwidth. As real-time graphics advanced, GPUs became programmable. The combination of programmability and floating-point performance made GPUs attractive for running scientific applications. Scientists found ways to use early programmable GPUs by casting their calculations as vertex and fragment shaders. GPUs evolved to meet the needs of scientific users by adding hardware for simpler programming, double-precision floating-point arithmetic, and resilience.},
	number = {6},
	urldate = {2023-11-07},
	journal = {IEEE Micro},
	author = {Dally, William J. and Keckler, Stephen W. and Kirk, David B.},
	month = nov,
	year = {2021},
	note = {Conference Name: IEEE Micro},
	pages = {42--51},
}


@article{demler_ceva_2020,
	title = {{CEVA} {SENSPRO} {FUSES} {AI} {AND} {VECTOR} {DSP}},
	language = {en},
	author = {Demler, Mike},
	year = {2020},
}


@misc{noauthor_google_2023,
	title = {Google {Tensor} {G3}: {The} new chip that gives your {Pixel} an {AI} upgrade},
	shorttitle = {Google {Tensor} {G3}},
	url = {https://blog.google/products/pixel/google-tensor-g3-pixel-8/},
	abstract = {Tensor G3 on Pixel 8 and Pixel 8 Pro is more helpful, more efficient and more powerful.},
	language = {en-us},
	urldate = {2023-11-07},
	journal = {Google},
	month = oct,
	year = {2023},
}


@misc{noauthor_hexagon_nodate,
	title = {Hexagon {DSP} {SDK} {Processor}},
	url = {https://developer.qualcomm.com/software/hexagon-dsp-sdk/dsp-processor},
	abstract = {The Hexagon DSP processor has both CPU and DSP functionality to support deeply embedded processing needs of the mobile platform for both multimedia and modem functions.},
	language = {en},
	urldate = {2023-11-07},
	journal = {Qualcomm Developer Network},
}


@misc{noauthor_evolution_2023,
	title = {The {Evolution} of {Audio} {DSPs}},
	url = {https://audioxpress.com/article/the-evolution-of-audio-dsps},
	abstract = {To complement the extensive perspective of another Market Update feature article on DSP Products and Applications, published in the November 2020 edition, audioXpress was honored to have the valuable contribution from one of the main suppliers in the field. In this article, Youval Nachum, CEVA’s Senior Product Marketing Manager, writes about \&quot;The Evolution of Audio DSPs,\&quot; discussing how DSP technology has evolved, its impact on the user experience, and what the future of DSP has in store for us.},
	language = {en},
	urldate = {2023-11-07},
	journal = {audioXpress},
	month = oct,
	year = {2023},
}


@article{xiong_mri-based_2021,
	title = {{MRI}-based brain tumor segmentation using {FPGA}-accelerated neural network},
	volume = {22},
	issn = {1471-2105},
	url = {https://doi.org/10.1186/s12859-021-04347-6},
	doi = {10.1186/s12859-021-04347-6},
	abstract = {Brain tumor segmentation is a challenging problem in medical image processing and analysis. It is a very time-consuming and error-prone task. In order to reduce the burden on physicians and improve the segmentation accuracy, the computer-aided detection (CAD) systems need to be developed. Due to the powerful feature learning ability of the deep learning technology, many deep learning-based methods have been applied to the brain tumor segmentation CAD systems and achieved satisfactory accuracy. However, deep learning neural networks have high computational complexity, and the brain tumor segmentation process consumes significant time. Therefore, in order to achieve the high segmentation accuracy of brain tumors and obtain the segmentation results efficiently, it is very demanding to speed up the segmentation process of brain tumors.},
	number = {1},
	urldate = {2023-11-07},
	journal = {BMC Bioinformatics},
	author = {Xiong, Siyu and Wu, Guoqing and Fan, Xitian and Feng, Xuan and Huang, Zhongcheng and Cao, Wei and Zhou, Xuegong and Ding, Shijin and Yu, Jinhua and Wang, Lingli and Shi, Zhifeng},
	month = sep,
	year = {2021},
	keywords = {Brain tumor segmatation, FPGA acceleration, Neural network},
	pages = {421},
}


@article{gwennap_certus-nx_nodate,
	title = {Certus-{NX} {Innovates} {General}-{Purpose} {FPGAs}},
	language = {en},
	author = {Gwennap, Linley},
}


@misc{noauthor_fpga_nodate,
	title = {{FPGA} {Architecture} {Overview}},
	url = {https://www.intel.com/content/www/us/en/docs/oneapi-fpga-add-on/optimization-guide/2023-1/fpga-architecture-overview.html},
	urldate = {2023-11-07},
}


@misc{noauthor_what_nodate,
	title = {What is an {FPGA}? {Field} {Programmable} {Gate} {Array}},
	shorttitle = {What is an {FPGA}?},
	url = {https://www.xilinx.com/products/silicon-devices/fpga/what-is-an-fpga.html},
	abstract = {What is an FPGA - Field Programmable Gate Arrays are semiconductor devices that are based around a matrix of configurable logic blocks (CLBs) connected via programmable interconnects. FPGAs can be reprogrammed to desired application or functionality requirements after manufacturing.},
	language = {en},
	urldate = {2023-11-07},
	journal = {AMD},
}


@article{putnam_reconfigurable_2014,
	title = {A reconfigurable fabric for accelerating large-scale datacenter services},
	volume = {42},
	issn = {0163-5964},
	url = {https://dl.acm.org/doi/10.1145/2678373.2665678},
	doi = {10.1145/2678373.2665678},
	abstract = {Datacenter workloads demand high computational capabilities, flexibility, power efficiency, and low cost. It is challenging to improve all of these factors simultaneously. To advance datacenter capabilities beyond what commodity server designs can provide, we have designed and built a composable, reconfigurablefabric to accelerate portions of large-scale software services. Each instantiation of the fabric consists of a 6x8 2-D torus of high-end Stratix V FPGAs embedded into a half-rack of 48 machines. One FPGA is placed into each server, accessible through PCIe, and wired directly to other FPGAs with pairs of 10 Gb SAS cables
            In this paper, we describe a medium-scale deployment of this fabric on a bed of 1,632 servers, and measure its efficacy in accelerating the Bing web search engine. We describe the requirements and architecture of the system, detail the critical engineering challenges and solutions needed to make the system robust in the presence of failures, and measure the performance, power, and resilience of the system when ranking candidate documents. Under high load, the largescale reconfigurable fabric improves the ranking throughput of each server by a factor of 95\% for a fixed latency distribution--- or, while maintaining equivalent throughput, reduces the tail latency by 29\%},
	language = {en},
	number = {3},
	urldate = {2023-11-07},
	journal = {ACM SIGARCH Computer Architecture News},
	author = {Putnam, Andrew and Caulfield, Adrian M. and Chung, Eric S. and Chiou, Derek and Constantinides, Kypros and Demme, John and Esmaeilzadeh, Hadi and Fowers, Jeremy and Gopal, Gopi Prashanth and Gray, Jan and Haselman, Michael and Hauck, Scott and Heil, Stephen and Hormati, Amir and Kim, Joo-Young and Lanka, Sitaram and Larus, James and Peterson, Eric and Pope, Simon and Smith, Aaron and Thong, Jason and Xiao, Phillip Yi and Burger, Doug},
	month = oct,
	year = {2014},
	pages = {13--24},
}


@misc{noauthor_project_nodate,
	title = {Project {Catapult} - {Microsoft} {Research}},
	url = {https://www.microsoft.com/en-us/research/project/project-catapult/},
	urldate = {2023-11-07},
}


@misc{dean_jeff_numbers_nodate,
	title = {Numbers {Everyone} {Should} {Know}},
	url = {https://brenocon.com/dean_perf.html},
	urldate = {2023-11-07},
	author = {Dean. Jeff},
}


@misc{bailey_enabling_2018,
	title = {Enabling {Cheaper} {Design}},
	url = {https://semiengineering.com/enabling-cheaper-design/},
	abstract = {Enabling Cheaper Design, At what point does cheaper design enable a significant growth in custom semiconductor content? Not everyone is onboard with the idea.},
	language = {en-US},
	urldate = {2023-11-07},
	journal = {Semiconductor Engineering},
	author = {Bailey, Brian},
	month = sep,
	year = {2018},
}


@misc{noauthor_integrated_2023,
	title = {Integrated circuit},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=Integrated_circuit&oldid=1183537457},
	abstract = {An integrated circuit (also known as an IC, a chip, or a microchip) is a set of electronic circuits on one small flat piece of semiconductor material, usually silicon. Large numbers of miniaturized transistors and other electronic components are integrated together on the chip. This results in circuits that are orders of magnitude smaller, faster, and less expensive than those constructed of discrete components, allowing a large transistor count.
The IC's mass production capability, reliability, and building-block approach to integrated circuit design have ensured the rapid adoption of standardized ICs in place of designs using discrete transistors. ICs are now used in virtually all electronic equipment and have revolutionized the world of electronics. Computers, mobile phones and other home appliances are now essential parts of the structure of modern societies, made possible by the small size and low cost of ICs such as modern computer processors and microcontrollers.
Very-large-scale integration was made practical by technological advancements in semiconductor device fabrication. Since their origins in the 1960s, the size, speed, and capacity of chips have progressed enormously, driven by technical advances that fit more and more transistors on chips of the same size – a modern chip may have many billions of transistors in an area the size of a human fingernail. These advances, roughly following Moore's law, make the computer chips of today possess millions of times the capacity and thousands of times the speed of the computer chips of the early 1970s.
ICs have three main advantages over discrete circuits: size, cost and performance. The size and cost is low because the chips, with all their components, are printed as a unit by photolithography rather than being constructed one transistor at a time. Furthermore, packaged ICs use much less material than discrete circuits. Performance is high because the IC's components switch quickly and consume comparatively little power because of their small size and proximity. The main disadvantage of ICs is the high initial cost of designing them and the enormous capital cost of factory construction. This high initial cost means ICs are only commercially viable when high production volumes are anticipated.},
	language = {en},
	urldate = {2023-11-07},
	journal = {Wikipedia},
	month = nov,
	year = {2023},
	note = {Page Version ID: 1183537457},
}


@article{el-rayis_reconfigurable_nodate,
	title = {Reconfigurable {Architectures} for the {Next} {Generation} of {Mobile} {Device} {Telecommunications} {Systems}},
	language = {en},
	author = {El-Rayis, Ahmed Osman},
}


@misc{noauthor_intel_nodate,
	title = {Intel® {Stratix}® 10 {NX} {FPGA} {Overview} - {High} {Performance} {Stratix}® {FPGA}},
	url = {https://www.intel.com/content/www/us/en/products/details/fpga/stratix/10/nx.html},
	abstract = {View Intel® Stratix® 10 NX FPGAs and find product specifications, features, applications and more.},
	language = {en},
	urldate = {2023-11-07},
	journal = {Intel},
}


@book{patterson2016computer,
  title={Computer organization and design ARM edition: the hardware software interface},
  author={Patterson, David A and Hennessy, John L},
  year={2016},
  publisher={Morgan kaufmann}
}


@article{xiu2019time,
 title={Time Moore: Exploiting Moore's Law from the perspective of time}, 
 author={Xiu, Liming}, 
 journal={IEEE Solid-State Circuits Magazine}, 
 volume={11}, 
 number={1}, 
 pages={39--55}, 
 year={2019}, 
 publisher={IEEE} 
}


@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}


@article{cheng2017survey,
  title={A survey of model compression and acceleration for deep neural networks},
  author={Cheng, Yu and Wang, Duo and Zhou, Pan and Zhang, Tao},
  journal={arXiv preprint arXiv:1710.09282},
  year={2017}
}


@article{sze2017efficient,
  title={Efficient processing of deep neural networks: A tutorial and survey},
  author={Sze, Vivienne and Chen, Yu-Hsin and Yang, Tien-Ju and Emer, Joel S},
  journal={Proceedings of the IEEE},
  volume={105},
  number={12},
  pages={2295--2329},
  year={2017},
  publisher={Ieee}
}


@article{young2018recent,
  title={Recent trends in deep learning based natural language processing},
  author={Young, Tom and Hazarika, Devamanyu and Poria, Soujanya and Cambria, Erik},
  journal={ieee Computational intelligenCe magazine},
  volume={13},
  number={3},
  pages={55--75},
  year={2018},
  publisher={IEEE}
}


@inproceedings{jacob2018quantization,
  title={Quantization and training of neural networks for efficient integer-arithmetic-only inference},
  author={Jacob, Benoit and Kligys, Skirmantas and Chen, Bo and Zhu, Menglong and Tang, Matthew and Howard, Andrew and Adam, Hartwig and Kalenichenko, Dmitry},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={2704--2713},
  year={2018}
}


@article{gale2019state,
  title={The state of sparsity in deep neural networks},
  author={Gale, Trevor and Elsen, Erich and Hooker, Sara},
  journal={arXiv preprint arXiv:1902.09574},
  year={2019}
}


@inproceedings{zhang2015fpga,
  title={FPGA-based Accelerator Design for Deep Convolutional Neural Networks Proceedings of the 2015 ACM},
  author={Zhang, Chen and Li, Peng and Sun, Guangyu and Guan, Yijin and Xiao, Bingjun and Cong, Jason Optimizing},
  booktitle={SIGDA International Symposium on Field-Programmable Gate Arrays-FPGA},
  volume={15},
  pages={161--170},
  year={2015}
}


@inproceedings{suda2016throughput,
  title={Throughput-optimized OpenCL-based FPGA accelerator for large-scale convolutional neural networks},
  author={Suda, Naveen and Chandra, Vikas and Dasika, Ganesh and Mohanty, Abinash and Ma, Yufei and Vrudhula, Sarma and Seo, Jae-sun and Cao, Yu},
  booktitle={Proceedings of the 2016 ACM/SIGDA international symposium on field-programmable gate arrays},
  pages={16--25},
  year={2016}
}


@inproceedings{fowers2018configurable,
  title={A configurable cloud-scale DNN processor for real-time AI},
  author={Fowers, Jeremy and Ovtcharov, Kalin and Papamichael, Michael and Massengill, Todd and Liu, Ming and Lo, Daniel and Alkalay, Shlomi and Haselman, Michael and Adams, Logan and Ghandi, Mahdi and others},
  booktitle={2018 ACM/IEEE 45th Annual International Symposium on Computer Architecture (ISCA)},
  pages={1--14},
  year={2018},
  organization={IEEE}
}


@article{jia2019beyond,
  title={Beyond Data and Model Parallelism for Deep Neural Networks.},
  author={Jia, Zhihao and Zaharia, Matei and Aiken, Alex},
  journal={Proceedings of Machine Learning and Systems},
  volume={1},
  pages={1--13},
  year={2019}
}


@inproceedings{zhu2018benchmarking,
  title={Benchmarking and analyzing deep neural network training},
  author={Zhu, Hongyu and Akrout, Mohamed and Zheng, Bojian and Pelegris, Andrew and Jayarajan, Anand and Phanishayee, Amar and Schroeder, Bianca and Pekhimenko, Gennady},
  booktitle={2018 IEEE International Symposium on Workload Characterization (IISWC)},
  pages={88--100},
  year={2018},
  organization={IEEE}
}


@article{samajdar2018scale,
  title={Scale-sim: Systolic cnn accelerator simulator},
  author={Samajdar, Ananda and Zhu, Yuhao and Whatmough, Paul and Mattina, Matthew and Krishna, Tushar},
  journal={arXiv preprint arXiv:1811.02883},
  year={2018}
}


@INPROCEEDINGS{munshi2009opencl,
  author={Munshi, Aaftab},
  booktitle={2009 IEEE Hot Chips 21 Symposium (HCS)}, 
  title={The OpenCL specification}, 
  year={2009},
  volume={},
  number={},
  pages={1-314},
  doi={10.1109/HOTCHIPS.2009.7478342}
}


@INPROCEEDINGS{luebke2008cuda,
  author={Luebke, David},
  booktitle={2008 5th IEEE International Symposium on Biomedical Imaging: From Nano to Macro}, 
  title={CUDA: Scalable parallel programming for high-performance scientific computing}, 
  year={2008},
  volume={},
  number={},
  pages={836-838},
  doi={10.1109/ISBI.2008.4541126}
}


@misc{segal1999opengl,
  title={The OpenGL graphics system: A specification (version 1.1)},
  author={Segal, Mark and Akeley, Kurt},
  year={1999}
}


@INPROCEEDINGS{gannot1994verilog,
  author={Gannot, G. and Ligthart, M.},
  booktitle={International Verilog HDL Conference}, 
  title={Verilog HDL based FPGA design}, 
  year={1994},
  volume={},
  number={},
  pages={86-92},
  doi={10.1109/IVC.1994.323743}
}


@article{binkert2011gem5,
  title={The gem5 simulator},
  author={Binkert, Nathan and Beckmann, Bradford and Black, Gabriel and Reinhardt, Steven K and Saidi, Ali and Basu, Arkaprava and Hestness, Joel and Hower, Derek R and Krishna, Tushar and Sardashti, Somayeh and others},
  journal={ACM SIGARCH computer architecture news},
  volume={39},
  number={2},
  pages={1--7},
  year={2011},
  publisher={ACM New York, NY, USA}
}


@ARTICLE{Vivet2021, author={Vivet, Pascal and Guthmuller, Eric and Thonnart, Yvain and Pillonnet, Gael and Fuguet, César and Miro-Panades, Ivan and Moritz, Guillaume and Durupt, Jean and Bernard, Christian and Varreau, Didier and Pontes, Julian and Thuries, Sébastien and Coriat, David and Harrand, Michel and Dutoit, Denis and Lattard, Didier and Arnaud, Lucile and Charbonnier, Jean and Coudrain, Perceval and Garnier, Arnaud and Berger, Frédéric and Gueugnot, Alain and Greiner, Alain and Meunier, Quentin L. and Farcy, Alexis and Arriordaz, Alexandre and Chéramy, Séverine and Clermidy, Fabien},
journal={IEEE Journal of Solid-State Circuits},
title={IntAct: A 96-Core Processor With Six Chiplets 3D-Stacked on an Active Interposer With Distributed Interconnects and Integrated Power Management},
year={2021},
volume={56},
number={1},
pages={79-97},
doi={10.1109/JSSC.2020.3036341}}


@article{schuman2022,
  title={Opportunities for neuromorphic computing algorithms and applications},
  author={Schuman, Catherine D and Kulkarni, Shruti R and Parsa, Maryam and Mitchell, J Parker and Date, Prasanna and Kay, Bill},
  journal={Nature Computational Science},
  volume={2},
  number={1},
  pages={10--19},
  year={2022},
  publisher={Nature Publishing Group US New York}
}


@article{markovic2020,
  title={Physics for neuromorphic computing},
  author={Markovi{\'c}, Danijela and Mizrahi, Alice and Querlioz, Damien and Grollier, Julie},
  journal={Nature Reviews Physics},
  volume={2},
  number={9},
  pages={499--510},
  year={2020},
  publisher={Nature Publishing Group UK London}
}


@article{furber2016large,
  title={Large-scale neuromorphic computing systems},
  author={Furber, Steve},
  journal={Journal of neural engineering},
  volume={13},
  number={5},
  pages={051001},
  year={2016},
  publisher={IOP Publishing}
}


@article{davies2018loihi,
  title={Loihi: A neuromorphic manycore processor with on-chip learning},
  author={Davies, Mike and Srinivasa, Narayan and Lin, Tsung-Han and Chinya, Gautham and Cao, Yongqiang and Choday, Sri Harsha and Dimou, Georgios and Joshi, Prasad and Imam, Nabil and Jain, Shweta and others},
  journal={Ieee Micro},
  volume={38},
  number={1},
  pages={82--99},
  year={2018},
  publisher={IEEE}
}


@article{davies2021advancing,
  title={Advancing neuromorphic computing with loihi: A survey of results and outlook},
  author={Davies, Mike and Wild, Andreas and Orchard, Garrick and Sandamirskaya, Yulia and Guerra, Gabriel A Fonseca and Joshi, Prasad and Plank, Philipp and Risbud, Sumedh R},
  journal={Proceedings of the IEEE},
  volume={109},
  number={5},
  pages={911--934},
  year={2021},
  publisher={IEEE}
}


@article{modha2023neural,
  title={Neural inference at the frontier of energy, space, and time},
  author={Modha, Dharmendra S and Akopyan, Filipp and Andreopoulos, Alexander and Appuswamy, Rathinakumar and Arthur, John V and Cassidy, Andrew S and Datta, Pallab and DeBole, Michael V and Esser, Steven K and Otero, Carlos Ortega and others},
  journal={Science},
  volume={382},
  number={6668},
  pages={329--335},
  year={2023},
  publisher={American Association for the Advancement of Science}
}


@article{maass1997networks,
  title={Networks of spiking neurons: the third generation of neural network models},
  author={Maass, Wolfgang},
  journal={Neural networks},
  volume={10},
  number={9},
  pages={1659--1671},
  year={1997},
  publisher={Elsevier}
}


@article{10242251,
author={Eshraghian, Jason K. and Ward, Max and Neftci, Emre O. and Wang, Xinxin and Lenz, Gregor and Dwivedi, Girish and Bennamoun, Mohammed and Jeong, Doo Seok and Lu, Wei D.},
journal={Proceedings of the IEEE},
title={Training Spiking Neural Networks Using Lessons From Deep Learning},
year={2023},
volume={111},
number={9},
pages={1016-1054},
doi={10.1109/JPROC.2023.3308088}
}


@article{chua1971memristor,
  title={Memristor-the missing circuit element},
  author={Chua, Leon},
  journal={IEEE Transactions on circuit theory},
  volume={18},
  number={5},
  pages={507--519},
  year={1971},
  publisher={IEEE}
}


@article{shastri2021photonics,
  title={Photonics for artificial intelligence and neuromorphic computing},
  author={Shastri, Bhavin J and Tait, Alexander N and Ferreira de Lima, Thomas and Pernice, Wolfram HP and Bhaskaran, Harish and Wright, C David and Prucnal, Paul R},
  journal={Nature Photonics},
  volume={15},
  number={2},
  pages={102--114},
  year={2021},
  publisher={Nature Publishing Group UK London}
}


@article{haensch2018next,
title={The next generation of deep learning hardware: Analog computing},
author={Haensch, Wilfried and Gokmen, Tayfun and Puri, Ruchir},
journal={Proceedings of the IEEE},
volume={107},
number={1},
pages={108--122},
year={2018},
publisher={IEEE}
}


@article{hazan2021neuromorphic,
  title={Neuromorphic analog implementation of neural engineering framework-inspired spiking neuron for high-dimensional representation},
  author={Hazan, Avi and Ezra Tsur, Elishai},
  journal={Frontiers in Neuroscience},
  volume={15},
  pages={627221},
  year={2021},
  publisher={Frontiers Media SA}
}


@article{gates2009flexible,
  title={Flexible electronics},
  author={Gates, Byron D},
  journal={Science},
  volume={323},
  number={5921},
  pages={1566--1567},
  year={2009},
  publisher={American Association for the Advancement of Science}
}


@article{musk2019integrated,
  title={An integrated brain-machine interface platform with thousands of channels},
  author={Musk, Elon and others},
  journal={Journal of medical Internet research},
  volume={21},
  number={10},
  pages={e16194},
  year={2019},
  publisher={JMIR Publications Inc., Toronto, Canada}
}


@article{tang2023flexible,
  title={Flexible brain--computer interfaces},
  author={Tang, Xin and Shen, Hao and Zhao, Siyuan and Li, Na and Liu, Jia},
  journal={Nature Electronics},
  volume={6},
  number={2},
  pages={109--118},
  year={2023},
  publisher={Nature Publishing Group UK London}
}


@article{tang2022soft,
  title={Soft bioelectronics for cardiac interfaces},
  author={Tang, Xin and He, Yichun and Liu, Jia},
  journal={Biophysics Reviews},
  volume={3},
  number={1},
  year={2022},
  publisher={AIP Publishing}
}


@article{kwon2022flexible,
  title={Flexible sensors and machine learning for heart monitoring},
  author={Kwon, Sun Hwa and Dong, Lin},
  journal={Nano Energy},
  pages={107632},
  year={2022},
  publisher={Elsevier}
}


@article{huang2010pseudo,
  title={Pseudo-CMOS: A design style for low-cost and robust flexible electronics},
  author={Huang, Tsung-Ching and Fukuda, Kenjiro and Lo, Chun-Ming and Yeh, Yung-Hui and Sekitani, Tsuyoshi and Someya, Takao and Cheng, Kwang-Ting},
  journal={IEEE Transactions on Electron Devices},
  volume={58},
  number={1},
  pages={141--150},
  year={2010},
  publisher={IEEE}
}


@article{biggs2021natively,
  title={A natively flexible 32-bit Arm microprocessor},
  author={Biggs, John and Myers, James and Kufel, Jedrzej and Ozer, Emre and Craske, Simon and Sou, Antony and Ramsdale, Catherine and Williamson, Ken and Price, Richard and White, Scott},
  journal={Nature},
  volume={595},
  number={7868},
  pages={532--536},
  year={2021},
  publisher={Nature Publishing Group UK London}
}


@article{farah2005neuroethics,
  title={Neuroethics: the practical and the philosophical},
  author={Farah, Martha J},
  journal={Trends in cognitive sciences},
  volume={9},
  number={1},
  pages={34--40},
  year={2005},
  publisher={Elsevier}
}


@article{segura2018ethical,
  title={Ethical implications of user perceptions of wearable devices},
  author={Segura Anaya, LH and Alsadoon, Abeer and Costadopoulos, Nectar and Prasad, PWC},
  journal={Science and engineering ethics},
  volume={24},
  pages={1--28},
  year={2018},
  publisher={Springer}
}


@article{goodyear2017social,
  title={Social media, apps and wearable technologies: navigating ethical dilemmas and procedures},
  author={Goodyear, Victoria A},
  journal={Qualitative research in sport, exercise and health},
  volume={9},
  number={3},
  pages={285--302},
  year={2017},
  publisher={Taylor \& Francis}
}


@article{roskies2002neuroethics,
  title={Neuroethics for the new millenium},
  author={Roskies, Adina},
  journal={Neuron},
  volume={35},
  number={1},
  pages={21--23},
  year={2002},
  publisher={Elsevier}
}


@article{duarte2022fastml,
  title={FastML Science Benchmarks: Accelerating Real-Time Scientific Edge Machine Learning},
  author={Duarte, Javier and Tran, Nhan and Hawks, Ben and Herwig, Christian and Muhizi, Jules and Prakash, Shvetank and Reddi, Vijay Janapa},
  journal={arXiv preprint arXiv:2207.07958},
  year={2022}
}


@article{verma2019memory,
  title={In-memory computing: Advances and prospects},
  author={Verma, Naveen and Jia, Hongyang and Valavi, Hossein and Tang, Yinqi and Ozatay, Murat and Chen, Lung-Yen and Zhang, Bonan and Deaville, Peter},
  journal={IEEE Solid-State Circuits Magazine},
  volume={11},
  number={3},
  pages={43--55},
  year={2019},
  publisher={IEEE}
}


@article{chi2016prime,
  title={Prime: A novel processing-in-memory architecture for neural network computation in reram-based main memory},
  author={Chi, Ping and Li, Shuangchen and Xu, Cong and Zhang, Tao and Zhao, Jishen and Liu, Yongpan and Wang, Yu and Xie, Yuan},
  journal={ACM SIGARCH Computer Architecture News},
  volume={44},
  number={3},
  pages={27--39},
  year={2016},
  publisher={ACM New York, NY, USA}
}


@article{burr2016recent,
  title={Recent progress in phase-change memory technology},
  author={Burr, Geoffrey W and Brightsky, Matthew J and Sebastian, Abu and Cheng, Huai-Yu and Wu, Jau-Yi and Kim, Sangbum and Sosa, Norma E and Papandreou, Nikolaos and Lung, Hsiang-Lan and Pozidis, Haralampos and others},
  journal={IEEE Journal on Emerging and Selected Topics in Circuits and Systems},
  volume={6},
  number={2},
  pages={146--162},
  year={2016},
  publisher={IEEE}
}


@article{loh20083d,
  title={3D-stacked memory architectures for multi-core processors},
  author={Loh, Gabriel H},
  journal={ACM SIGARCH computer architecture news},
  volume={36},
  number={3},
  pages={453--464},
  year={2008},
  publisher={ACM New York, NY, USA}
}


@article{mittal2021survey,
  title={A survey of SRAM-based in-memory computing techniques and applications},
  author={Mittal, Sparsh and Verma, Gaurav and Kaushik, Brajesh and Khanday, Farooq A},
  journal={Journal of Systems Architecture},
  volume={119},
  pages={102276},
  year={2021},
  publisher={Elsevier}
}


@article{wong2012metal,
  title={Metal--oxide RRAM},
  author={Wong, H-S Philip and Lee, Heng-Yuan and Yu, Shimeng and Chen, Yu-Sheng and Wu, Yi and Chen, Pang-Shiu and Lee, Byoungil and Chen, Frederick T and Tsai, Ming-Jinn},
  journal={Proceedings of the IEEE},
  volume={100},
  number={6},
  pages={1951--1970},
  year={2012},
  publisher={IEEE}
}


@inproceedings{imani2016resistive,
  title={Resistive configurable associative memory for approximate computing},
  author={Imani, Mohsen and Rahimi, Abbas and Rosing, Tajana S},
  booktitle={2016 Design, Automation \& Test in Europe Conference \& Exhibition (DATE)},
  pages={1327--1332},
  year={2016},
  organization={IEEE}
}


@article{miller2000optical,
  title={Optical interconnects to silicon},
  author={Miller, David AB},
  journal={IEEE Journal of Selected Topics in Quantum Electronics},
  volume={6},
  number={6},
  pages={1312--1317},
  year={2000},
  publisher={IEEE}
}


@article{zhou2022photonic,
title={Photonic matrix multiplication lights up photonic accelerator and beyond},
author={Zhou, Hailong and Dong, Jianji and Cheng, Junwei and Dong, Wenchan and Huang, Chaoran and Shen, Yichen and Zhang, Qiming and Gu, Min and Qian, Chao and Chen, Hongsheng and others},
journal={Light: Science \& Applications},
volume={11},
number={1},
pages={30},
year={2022},
publisher={Nature Publishing Group UK London}
}


@article{bains2020business,
  title={The business of building brains},
  author={Bains, Sunny},
  journal={Nat. Electron},
  volume={3},
  number={7},
  pages={348--351},
  year={2020}
}


@ARTICLE{Hennessy2019-je,
  title     = "A new golden age for computer architecture",
  author    = "Hennessy, John L and Patterson, David A",
  abstract  = "Innovations like domain-specific hardware, enhanced security,
               open instruction sets, and agile chip development will lead the
               way.",
  journal   = "Commun. ACM",
  publisher = "Association for Computing Machinery (ACM)",
  volume    =  62,
  number    =  2,
  pages     = "48--60",
  month     =  jan,
  year      =  2019,
  copyright = "http://www.acm.org/publications/policies/copyright\_policy\#Background",
  language  = "en"
}


@ARTICLE{Dongarra2009-na,
  title   = "The evolution of high performance computing on system z",
  author  = "Dongarra, Jack J",
  journal = "IBM Journal of Research and Development",
  volume  =  53,
  pages   = "3--4",
  year    =  2009
}


@ARTICLE{Ranganathan2011-dc,
  title     = "From microprocessors to nanostores: Rethinking data-centric
               systems",
  author    = "Ranganathan, Parthasarathy",
  journal   = "Computer (Long Beach Calif.)",
  publisher = "Institute of Electrical and Electronics Engineers (IEEE)",
  volume    =  44,
  number    =  1,
  pages     = "39--48",
  month     =  jan,
  year      =  2011
}


@ARTICLE{Ignatov2018-kh,
  title     = "{AI} Benchmark: Running deep neural networks on Android
               smartphones",
  author    = "Ignatov, Andrey and Timofte, Radu and Chou, William and Wang, Ke
               and Wu, Max and Hartley, Tim and Van Gool, Luc",
  abstract  = "Over the last years, the computational power of mobile devices
               such as smartphones and tablets has grown dramatically, reaching
               the level of desktop computers available not long ago. While
               standard smartphone apps are no longer a problem for them, there
               is still a group of tasks that can easily challenge even
               high-end devices, namely running artificial intelligence
               algorithms. In this paper, we present a study of the current
               state of deep learning in the Android ecosystem and describe
               available frameworks, programming models and the limitations of
               running AI on smartphones. We give an overview of the hardware
               acceleration resources available on four main mobile chipset
               platforms: Qualcomm, HiSilicon, MediaTek and Samsung.
               Additionally, we present the real-world performance results of
               different mobile SoCs collected with AI Benchmark that are
               covering all main existing hardware configurations.",
  publisher = "arXiv",
  year      =  2018
}


@ARTICLE{Sze2017-ak,
  title         = "Efficient processing of deep neural networks: A tutorial and
                   survey",
  author        = "Sze, Vivienne and Chen, Yu-Hsin and Yang, Tien-Ju and Emer,
                   Joel",
  abstract      = "Deep neural networks (DNNs) are currently widely used for
                   many artificial intelligence (AI) applications including
                   computer vision, speech recognition, and robotics. While
                   DNNs deliver state-of-the-art accuracy on many AI tasks, it
                   comes at the cost of high computational complexity.
                   Accordingly, techniques that enable efficient processing of
                   DNNs to improve energy efficiency and throughput without
                   sacrificing application accuracy or increasing hardware cost
                   are critical to the wide deployment of DNNs in AI systems.
                   This article aims to provide a comprehensive tutorial and
                   survey about the recent advances towards the goal of
                   enabling efficient processing of DNNs. Specifically, it will
                   provide an overview of DNNs, discuss various hardware
                   platforms and architectures that support DNNs, and highlight
                   key trends in reducing the computation cost of DNNs either
                   solely via hardware design changes or via joint hardware
                   design and DNN algorithm changes. It will also summarize
                   various development resources that enable researchers and
                   practitioners to quickly get started in this field, and
                   highlight important benchmarking metrics and design
                   considerations that should be used for evaluating the
                   rapidly growing number of DNN hardware designs, optionally
                   including algorithmic co-designs, being proposed in academia
                   and industry. The reader will take away the following
                   concepts from this article: understand the key design
                   considerations for DNNs; be able to evaluate different DNN
                   hardware implementations with benchmarks and comparison
                   metrics; understand the trade-offs between various hardware
                   architectures and platforms; be able to evaluate the utility
                   of various DNN design techniques for efficient processing;
                   and understand recent implementation trends and
                   opportunities.",
  month         =  mar,
  year          =  2017,
  copyright     = "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "1703.09039"
}


@inproceedings{lin2022ondevice,
   title     = {On-Device Training Under 256KB Memory},
   author    = {Lin, Ji and Zhu, Ligeng and Chen, Wei-Ming and Wang, Wei-Chen and Gan, Chuang and Han, Song},
   booktitle = {ArXiv},
   year      = {2022}
}


@article{lin2023awq,
  title={AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration},
  author={Lin, Ji and Tang, Jiaming and Tang, Haotian and Yang, Shang and Dang, Xingyu and Han, Song},
  journal={arXiv},
  year={2023}
}


@inproceedings{wang2020apq,
  author={Wang, Tianzhe and Wang, Kuan and Cai, Han and Lin, Ji and Liu, Zhijian and Wang, Hanrui and Lin, Yujun and Han, Song},
  booktitle={2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={APQ: Joint Search for Network Architecture, Pruning and Quantization Policy}, 
  year={2020},
  volume={},
  number={},
  pages={2075-2084},
  doi={10.1109/CVPR42600.2020.00215}
}


@inproceedings{Li2020Additive,
title={Additive Powers-of-Two Quantization: An Efficient Non-uniform Discretization for Neural Networks},
author={Yuhang Li and Xin Dong and Wei Wang},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=BkgXT24tDS}
}


@article{janapa2023edge,
  title={Edge Impulse: An MLOps Platform for Tiny Machine Learning},
  author={Janapa Reddi, Vijay and Elium, Alexander and Hymel, Shawn and Tischler, David and Situnayake, Daniel and Ward, Carl and Moreau, Louis and Plunkett, Jenny and Kelcey, Matthew and Baaijens, Mathijs and others},
  journal={Proceedings of Machine Learning and Systems},
  volume={5},
  year={2023}
}


@article{zhuang2020comprehensive,
  title={A comprehensive survey on transfer learning},
  author={Zhuang, Fuzhen and Qi, Zhiyuan and Duan, Keyu and Xi, Dongbo and Zhu, Yongchun and Zhu, Hengshu and Xiong, Hui and He, Qing},
  journal={Proceedings of the IEEE},
  volume={109},
  number={1},
  pages={43--76},
  year={2020},
  publisher={IEEE}
}


@article{zhuang_comprehensive_2021,
	title = {A {Comprehensive} {Survey} on {Transfer} {Learning}},
	volume = {109},
	issn = {0018-9219, 1558-2256},
	url = {https://ieeexplore.ieee.org/document/9134370/},
	doi = {10.1109/JPROC.2020.3004555},
	language = {en},
	number = {1},
	urldate = {2023-10-25},
	journal = {Proceedings of the IEEE},
	author = {Zhuang, Fuzhen and Qi, Zhiyuan and Duan, Keyu and Xi, Dongbo and Zhu, Yongchun and Zhu, Hengshu and Xiong, Hui and He, Qing},
	month = jan,
	year = {2021},
	pages = {43--76},
	file = {Zhuang et al. - 2021 - A Comprehensive Survey on Transfer Learning.pdf:/Users/alex/Zotero/storage/CHJB2WE4/Zhuang et al. - 2021 - A Comprehensive Survey on Transfer Learning.pdf:application/pdf},
}


@inproceedings{Norman2017TPUv1,
author = {Jouppi, Norman P. and Young, Cliff and Patil, Nishant and Patterson, David and Agrawal, Gaurav and Bajwa, Raminder and Bates, Sarah and Bhatia, Suresh and Boden, Nan and Borchers, Al and Boyle, Rick and Cantin, Pierre-luc and Chao, Clifford and Clark, Chris and Coriell, Jeremy and Daley, Mike and Dau, Matt and Dean, Jeffrey and Gelb, Ben and Ghaemmaghami, Tara Vazir and Gottipati, Rajendra and Gulland, William and Hagmann, Robert and Ho, C. Richard and Hogberg, Doug and Hu, John and Hundt, Robert and Hurt, Dan and Ibarz, Julian and Jaffey, Aaron and Jaworski, Alek and Kaplan, Alexander and Khaitan, Harshit and Killebrew, Daniel and Koch, Andy and Kumar, Naveen and Lacy, Steve and Laudon, James and Law, James and Le, Diemthu and Leary, Chris and Liu, Zhuyuan and Lucke, Kyle and Lundin, Alan and MacKean, Gordon and Maggiore, Adriana and Mahony, Maire and Miller, Kieran and Nagarajan, Rahul and Narayanaswami, Ravi and Ni, Ray and Nix, Kathy and Norrie, Thomas and Omernick, Mark and Penukonda, Narayana and Phelps, Andy and Ross, Jonathan and Ross, Matt and Salek, Amir and Samadiani, Emad and Severn, Chris and Sizikov, Gregory and Snelham, Matthew and Souter, Jed and Steinberg, Dan and Swing, Andy and Tan, Mercedes and Thorson, Gregory and Tian, Bo and Toma, Horia and Tuttle, Erick and Vasudevan, Vijay and Walter, Richard and Wang, Walter and Wilcox, Eric and Yoon, Doe Hyun},
title = {In-Datacenter Performance Analysis of a Tensor Processing Unit},
year = {2017},
isbn = {9781450348928},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3079856.3080246},
doi = {10.1145/3079856.3080246},
abstract = {Many architects believe that major improvements in cost-energy-performance must now come from domain-specific hardware. This paper evaluates a custom ASIC---called a Tensor Processing Unit (TPU) --- deployed in datacenters since 2015 that accelerates the inference phase of neural networks (NN). The heart of the TPU is a 65,536 8-bit MAC matrix multiply unit that offers a peak throughput of 92 TeraOps/second (TOPS) and a large (28 MiB) software-managed on-chip memory. The TPU's deterministic execution model is a better match to the 99th-percentile response-time requirement of our NN applications than are the time-varying optimizations of CPUs and GPUs that help average throughput more than guaranteed latency. The lack of such features helps explain why, despite having myriad MACs and a big memory, the TPU is relatively small and low power. We compare the TPU to a server-class Intel Haswell CPU and an Nvidia K80 GPU, which are contemporaries deployed in the same datacenters. Our workload, written in the high-level TensorFlow framework, uses production NN applications (MLPs, CNNs, and LSTMs) that represent 95\% of our datacenters' NN inference demand. Despite low utilization for some applications, the TPU is on average about 15X -- 30X faster than its contemporary GPU or CPU, with TOPS/Watt about 30X -- 80X higher. Moreover, using the CPU's GDDR5 memory in the TPU would triple achieved TOPS and raise TOPS/Watt to nearly 70X the GPU and 200X the CPU.},
booktitle = {Proceedings of the 44th Annual International Symposium on Computer Architecture},
pages = {1-12},
numpages = {12},
keywords = {accelerator, neural network, MLP, TPU, CNN, deep learning, domain-specific architecture, GPU, TensorFlow, DNN, RNN, LSTM},
location = {Toronto, ON, Canada},
series = {ISCA '17}
}


@ARTICLE{Norrie2021TPUv2_3,
  author={Norrie, Thomas and Patil, Nishant and Yoon, Doe Hyun and Kurian, George and Li, Sheng and Laudon, James and Young, Cliff and Jouppi, Norman and Patterson, David},
  journal={IEEE Micro}, 
  title={The Design Process for Google's Training Chips: TPUv2 and TPUv3}, 
  year={2021},
  volume={41},
  number={2},
  pages={56-63},
  doi={10.1109/MM.2021.3058217}
}


@inproceedings{Jouppi2023TPUv4,
author = {Jouppi, Norm and Kurian, George and Li, Sheng and Ma, Peter and Nagarajan, Rahul and Nai, Lifeng and Patil, Nishant and Subramanian, Suvinay and Swing, Andy and Towles, Brian and Young, Clifford and Zhou, Xiang and Zhou, Zongwei and Patterson, David A},
title = {TPU v4: An Optically Reconfigurable Supercomputer for Machine Learning with Hardware Support for Embeddings},
year = {2023},
isbn = {9798400700958},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579371.3589350},
doi = {10.1145/3579371.3589350},
abstract = {In response to innovations in machine learning (ML) models, production workloads changed radically and rapidly. TPU v4 is the fifth Google domain specific architecture (DSA) and its third supercomputer for such ML models. Optical circuit switches (OCSes) dynamically reconfigure its interconnect topology to improve scale, availability, utilization, modularity, deployment, security, power, and performance; users can pick a twisted 3D torus topology if desired. Much cheaper, lower power, and faster than Infiniband, OCSes and underlying optical components are &lt;5\% of system cost and &lt;3\% of system power. Each TPU v4 includes SparseCores, dataflow processors that accelerate models that rely on embeddings by 5x--7x yet use only 5\% of die area and power. Deployed since 2020, TPU v4 outperforms TPU v3 by 2.1x and improves performance/Watt by 2.7x. The TPU v4 supercomputer is 4x larger at 4096 chips and thus nearly 10x faster overall, which along with OCS flexibility and availability allows a large language model to train at an average of ~60\% of peak FLOPS/second. For similar sized systems, it is ~4.3x--4.5x faster than the Graphcore IPU Bow and is 1.2x--1.7x faster and uses 1.3x--1.9x less power than the Nvidia A100. TPU v4s inside the energy-optimized warehouse scale computers of Google Cloud use ~2--6x less energy and produce ~20x less CO2e than contemporary DSAs in typical on-premise data centers.},
booktitle = {Proceedings of the 50th Annual International Symposium on Computer Architecture},
articleno = {82},
numpages = {14},
keywords = {warehouse scale computer, embeddings, supercomputer, domain specific architecture, reconfigurable, TPU, large language model, power usage effectiveness, CO2 equivalent emissions, energy, optical interconnect, IPU, machine learning, GPU, carbon emissions},
location = {Orlando, FL, USA},
series = {ISCA '23}
}


@misc{zhou2021analognets,
	title        = {AnalogNets: ML-HW Co-Design of Noise-robust TinyML Models and Always-On Analog Compute-in-Memory Accelerator},
	author       = {Chuteng Zhou and Fernando Garcia Redondo and Julian Büchel and Irem Boybat and Xavier Timoneda Comas and S. R. Nandakumar and Shidhartha Das and Abu Sebastian and Manuel Le Gallo and Paul N. Whatmough},
	year         = 2021,
	eprint       = {2111.06503},
	archiveprefix = {arXiv},
	primaryclass = {cs.AR}
}


@article{wearableinsulin,
  author = {Psoma, Sotiria D. and Kanthou, Chryso},
  title = {Wearable Insulin Biosensors for Diabetes Management: Advances and Challenges},
  journal = {Biosensors},
  volume = {13},
  year = {2023},
  number = {7},
  article-number = {719},
  url = {https://www.mdpi.com/2079-6374/13/7/719},
  pubmedid = {37504117},
  issn = {2079-6374},
  doi = {10.3390/bios13070719}
}


@article{glucosemonitor,
  author={Li, Jingzhen and Tobore, Igbe and Liu, Yuhang and Kandwal, Abhishek and Wang, Lei and Nie, Zedong},
  journal={IEEE Journal of Biomedical and Health Informatics}, 
  title={Non-invasive Monitoring of Three Glucose Ranges Based On ECG By Using DBSCAN-CNN}, 
  year={2021},
  volume={25},
  number={9},
  pages={3340-3350},
  doi={10.1109/JBHI.2021.3072628}
}


@article{plasma,
  author = {Attia, Zachi and Sugrue, Alan and Asirvatham, Samuel and Ackerman, Michael and Kapa, Suraj and Friedman, Paul and Noseworthy, Peter},
  year = {2018},
  month = {08},
  pages = {e0201059},
  title = {Noninvasive assessment of dofetilide plasma concentration using a deep learning (neural network) analysis of the surface electrocardiogram: A proof of concept study},
  volume = {13},
  journal = {PLOS ONE},
  doi = {10.1371/journal.pone.0201059}
}


@article{afib,
  author = {Yutao Guo  and Hao Wang  and Hui Zhang  and Tong Liu  and Zhaoguang Liang  and Yunlong Xia  and Li Yan  and Yunli Xing  and Haili Shi  and Shuyan Li  and Yanxia Liu  and Fan Liu  and Mei Feng  and Yundai Chen  and Gregory Y.H. Lip  and null null },
  title = {Mobile Photoplethysmographic Technology to Detect Atrial Fibrillation},
  journal = {Journal of the American College of Cardiology},
  volume = {74},
  number = {19},
  pages = {2365-2375},
  year = {2019},
  doi = {10.1016/j.jacc.2019.08.019}
}


@article{gaitathome,
  author = {Yingcheng Liu  and Guo Zhang  and Christopher G. Tarolli  and Rumen Hristov  and Stella Jensen-Roberts  and Emma M. Waddell  and Taylor L. Myers  and Meghan E. Pawlik  and Julia M. Soto  and Renee M. Wilson  and Yuzhe Yang  and Timothy Nordahl  and Karlo J. Lizarraga  and Jamie L. Adams  and Ruth B. Schneider  and Karl Kieburtz  and Terry Ellis  and E. Ray Dorsey  and Dina Katabi },
  title = {Monitoring gait at home with radio waves in Parkinson's disease: A marker of severity, progression, and medication response},
  journal = {Science Translational Medicine},
  volume = {14},
  number = {663},
  pages = {eadc9669},
  year = {2022},
  doi = {10.1126/scitranslmed.adc9669},
  URL = {https://www.science.org/doi/abs/10.1126/scitranslmed.adc9669},
  eprint = {https://www.science.org/doi/pdf/10.1126/scitranslmed.adc9669}
}


@article{Chen2023,
  author={Chen, Emma and Prakash, Shvetank and Janapa Reddi, Vijay and Kim, David and Rajpurkar, Pranav},
  title={A framework for integrating artificial intelligence for clinical care with continuous therapeutic monitoring},
  journal={Nature Biomedical Engineering},
  year={2023},
  month={Nov},
  day={06},
  issn={2157-846X},
  doi={10.1038/s41551-023-01115-0},
  url={https://doi.org/10.1038/s41551-023-01115-0}
}


@article{Zhang2017,
  author={Zhang, Qingxue and Zhou, Dian and Zeng, Xuan},
  title={Highly wearable cuff-less blood pressure and heart rate monitoring with single-arm electrocardiogram and photoplethysmogram signals},
  journal={BioMedical Engineering OnLine},
  year={2017},
  month={Feb},
  day={06},
  volume={16},
  number={1},
  pages={23},
  issn={1475-925X},
  doi={10.1186/s12938-017-0317-z},
  url={https://doi.org/10.1186/s12938-017-0317-z}
}
