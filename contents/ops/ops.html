<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.549">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Machine Learning Systems - 14&nbsp; Embedded AIOps</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../contents/privacy_security/privacy_security.html" rel="next">
<link href="../../contents/ondevice_learning/ondevice_learning.html" rel="prev">
<link href="../../favicon.png" rel="icon" type="image/png">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-M21L0CBCVN"></script>


</head>

<body class="nav-sidebar floating nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Machine Learning Systems</span>
    </a>
  </div>
        <div class="quarto-navbar-tools tools-wide">
    <a href="https://github.com/harvard-edge/cs249r_book" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
    <div class="dropdown">
      <a href="" title="Download" id="quarto-navigation-tool-dropdown-0" class="quarto-navigation-tool dropdown-toggle px-1" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download"><i class="bi bi-download"></i></a>
      <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="quarto-navigation-tool-dropdown-0">
          <li>
            <a class="dropdown-item quarto-navbar-tools-item" href="../../Machine-Learning-Systems.pdf">
              <i class="bi bi-bi-file-pdf pe-1"></i>
            Download PDF
            </a>
          </li>
          <li>
            <a class="dropdown-item quarto-navbar-tools-item" href="../../Machine-Learning-Systems.epub">
              <i class="bi bi-bi-journal pe-1"></i>
            Download ePub
            </a>
          </li>
      </ul>
    </div>
    <div class="dropdown">
      <a href="" title="Share" id="quarto-navigation-tool-dropdown-1" class="quarto-navigation-tool dropdown-toggle px-1" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Share"><i class="bi bi-share"></i></a>
      <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="quarto-navigation-tool-dropdown-1">
          <li>
            <a class="dropdown-item quarto-navbar-tools-item" href="https://twitter.com/intent/tweet?url=|url|">
              <i class="bi bi-bi-twitter pe-1"></i>
            Twitter
            </a>
          </li>
          <li>
            <a class="dropdown-item quarto-navbar-tools-item" href="https://www.facebook.com/sharer/sharer.php?u=|url|">
              <i class="bi bi-bi-facebook pe-1"></i>
            Facebook
            </a>
          </li>
      </ul>
    </div>
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
  <a href="" class="quarto-reader-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleReader(); return false;" title="Toggle reader mode">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
          <div id="quarto-search" class="" title="Search"></div>
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../contents/introduction.html">MAIN</a></li><li class="breadcrumb-item"><a href="../../contents/ops/ops.html"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Embedded AIOps</span></a></li></ol></nav>
        <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
 <span class="menu-text">FRONT MATTER</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/dedication.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Dedication</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/acknowledgements.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Acknowledgements</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/contributors.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Contributors</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/copyright.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Copyright</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/about.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">About the Book</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">
 <span class="menu-text">MAIN</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/introduction.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/embedded_sys/embedded_sys.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Embedded Systems</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/dl_primer/dl_primer.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Deep Learning Primer</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/embedded_ml/embedded_ml.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Embedded AI</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/workflow/workflow.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">AI Workflow</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/data_engineering/data_engineering.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Data Engineering</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/frameworks/frameworks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">AI Frameworks</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/training/training.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">AI Training</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/efficient_ai/efficient_ai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Efficient AI</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/optimizations/optimizations.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Model Optimizations</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/hw_acceleration/hw_acceleration.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">AI Acceleration</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/benchmarking/benchmarking.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Benchmarking AI</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/ondevice_learning/ondevice_learning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">On-Device Learning</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/ops/ops.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Embedded AIOps</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/privacy_security/privacy_security.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Security &amp; Privacy</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/responsible_ai/responsible_ai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Responsible AI</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/sustainable_ai/sustainable_ai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Sustainable AI</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/ai_for_good/ai_for_good.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">AI for Good</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/robust_ai/robust_ai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Robust AI</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/generative_ai/generative_ai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Generative AI</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true">
 <span class="menu-text">REFERENCES</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="true">
 <span class="menu-text">EXERCISES</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/niclav_sys/niclav_sys.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Setup Nicla Vision</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/image_classification/image_classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">CV on Nicla Vision</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/object_detection_fomo/object_detection_fomo.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Object Detection</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/kws_feature_eng/kws_feature_eng.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Audio Feature Engineering</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/kws_nicla/kws_nicla.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Keyword Spotting (KWS)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/dsp_spectral_features_block/dsp_spectral_features_block.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">DSP - Spectral Features</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/motion_classify_ad/motion_classify_ad.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Motion Classification and Anomaly Detection</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="true">
 <span class="menu-text">Appendices</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/tools.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">Tools</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/zoo_datasets.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">B</span>&nbsp; <span class="chapter-title">Datasets</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/zoo_models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">C</span>&nbsp; <span class="chapter-title">Model Zoo</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/learning_resources.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">D</span>&nbsp; <span class="chapter-title">Resources</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/community.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">E</span>&nbsp; <span class="chapter-title">Communities</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/case_studies.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">F</span>&nbsp; <span class="chapter-title">Case Studies</span></span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction"><span class="header-section-number">14.1</span> Introduction</a></li>
  <li><a href="#historical-context" id="toc-historical-context" class="nav-link" data-scroll-target="#historical-context"><span class="header-section-number">14.2</span> Historical Context</a>
  <ul>
  <li><a href="#devops" id="toc-devops" class="nav-link" data-scroll-target="#devops"><span class="header-section-number">14.2.1</span> DevOps</a></li>
  <li><a href="#mlops" id="toc-mlops" class="nav-link" data-scroll-target="#mlops"><span class="header-section-number">14.2.2</span> MLOps</a></li>
  </ul></li>
  <li><a href="#key-components-of-mlops" id="toc-key-components-of-mlops" class="nav-link" data-scroll-target="#key-components-of-mlops"><span class="header-section-number">14.3</span> Key Components of MLOps</a>
  <ul>
  <li><a href="#data-management" id="toc-data-management" class="nav-link" data-scroll-target="#data-management"><span class="header-section-number">14.3.1</span> Data Management</a></li>
  <li><a href="#cicd-pipelines" id="toc-cicd-pipelines" class="nav-link" data-scroll-target="#cicd-pipelines"><span class="header-section-number">14.3.2</span> CI/CD Pipelines</a></li>
  <li><a href="#model-training" id="toc-model-training" class="nav-link" data-scroll-target="#model-training"><span class="header-section-number">14.3.3</span> Model Training</a></li>
  <li><a href="#model-evaluation" id="toc-model-evaluation" class="nav-link" data-scroll-target="#model-evaluation"><span class="header-section-number">14.3.4</span> Model Evaluation</a></li>
  <li><a href="#model-deployment" id="toc-model-deployment" class="nav-link" data-scroll-target="#model-deployment"><span class="header-section-number">14.3.5</span> Model Deployment</a></li>
  <li><a href="#infrastructure-management" id="toc-infrastructure-management" class="nav-link" data-scroll-target="#infrastructure-management"><span class="header-section-number">14.3.6</span> Infrastructure Management</a></li>
  <li><a href="#monitoring" id="toc-monitoring" class="nav-link" data-scroll-target="#monitoring"><span class="header-section-number">14.3.7</span> Monitoring</a></li>
  <li><a href="#governance" id="toc-governance" class="nav-link" data-scroll-target="#governance"><span class="header-section-number">14.3.8</span> Governance</a></li>
  <li><a href="#communication-collaboration" id="toc-communication-collaboration" class="nav-link" data-scroll-target="#communication-collaboration"><span class="header-section-number">14.3.9</span> Communication &amp; Collaboration</a></li>
  </ul></li>
  <li><a href="#hidden-technical-debt-in-ml-systems" id="toc-hidden-technical-debt-in-ml-systems" class="nav-link" data-scroll-target="#hidden-technical-debt-in-ml-systems"><span class="header-section-number">14.4</span> Hidden Technical Debt in ML Systems</a>
  <ul>
  <li><a href="#model-boundary-erosion" id="toc-model-boundary-erosion" class="nav-link" data-scroll-target="#model-boundary-erosion"><span class="header-section-number">14.4.1</span> Model Boundary Erosion</a></li>
  <li><a href="#entanglement" id="toc-entanglement" class="nav-link" data-scroll-target="#entanglement"><span class="header-section-number">14.4.2</span> Entanglement</a></li>
  <li><a href="#correction-cascades" id="toc-correction-cascades" class="nav-link" data-scroll-target="#correction-cascades"><span class="header-section-number">14.4.3</span> Correction Cascades</a></li>
  <li><a href="#undeclared-consumers" id="toc-undeclared-consumers" class="nav-link" data-scroll-target="#undeclared-consumers"><span class="header-section-number">14.4.4</span> Undeclared Consumers</a></li>
  <li><a href="#data-dependency-debt" id="toc-data-dependency-debt" class="nav-link" data-scroll-target="#data-dependency-debt"><span class="header-section-number">14.4.5</span> Data Dependency Debt</a></li>
  <li><a href="#analysis-debt-from-feedback-loops" id="toc-analysis-debt-from-feedback-loops" class="nav-link" data-scroll-target="#analysis-debt-from-feedback-loops"><span class="header-section-number">14.4.6</span> Analysis Debt from Feedback Loops</a></li>
  <li><a href="#pipeline-jungles" id="toc-pipeline-jungles" class="nav-link" data-scroll-target="#pipeline-jungles"><span class="header-section-number">14.4.7</span> Pipeline Jungles</a></li>
  <li><a href="#configuration-debt" id="toc-configuration-debt" class="nav-link" data-scroll-target="#configuration-debt"><span class="header-section-number">14.4.8</span> Configuration Debt</a></li>
  <li><a href="#the-changing-world" id="toc-the-changing-world" class="nav-link" data-scroll-target="#the-changing-world"><span class="header-section-number">14.4.9</span> The Changing World</a></li>
  <li><a href="#navigating-technical-debt-in-early-stages" id="toc-navigating-technical-debt-in-early-stages" class="nav-link" data-scroll-target="#navigating-technical-debt-in-early-stages"><span class="header-section-number">14.4.10</span> Navigating Technical Debt in Early Stages</a></li>
  <li><a href="#summary" id="toc-summary" class="nav-link" data-scroll-target="#summary"><span class="header-section-number">14.4.11</span> Summary</a></li>
  </ul></li>
  <li><a href="#roles-and-responsibilities" id="toc-roles-and-responsibilities" class="nav-link" data-scroll-target="#roles-and-responsibilities"><span class="header-section-number">14.5</span> Roles and Responsibilities</a>
  <ul>
  <li><a href="#data-engineers" id="toc-data-engineers" class="nav-link" data-scroll-target="#data-engineers"><span class="header-section-number">14.5.1</span> Data Engineers</a></li>
  <li><a href="#data-scientists" id="toc-data-scientists" class="nav-link" data-scroll-target="#data-scientists"><span class="header-section-number">14.5.2</span> Data Scientists</a></li>
  <li><a href="#ml-engineers" id="toc-ml-engineers" class="nav-link" data-scroll-target="#ml-engineers"><span class="header-section-number">14.5.3</span> ML Engineers</a></li>
  <li><a href="#devops-engineers" id="toc-devops-engineers" class="nav-link" data-scroll-target="#devops-engineers"><span class="header-section-number">14.5.4</span> DevOps Engineers</a></li>
  <li><a href="#project-managers" id="toc-project-managers" class="nav-link" data-scroll-target="#project-managers"><span class="header-section-number">14.5.5</span> Project Managers</a></li>
  </ul></li>
  <li><a href="#embedded-system-challenges" id="toc-embedded-system-challenges" class="nav-link" data-scroll-target="#embedded-system-challenges"><span class="header-section-number">14.6</span> Embedded System Challenges</a>
  <ul>
  <li><a href="#limited-compute-resources" id="toc-limited-compute-resources" class="nav-link" data-scroll-target="#limited-compute-resources"><span class="header-section-number">14.6.1</span> Limited Compute Resources</a></li>
  <li><a href="#constrained-memory" id="toc-constrained-memory" class="nav-link" data-scroll-target="#constrained-memory"><span class="header-section-number">14.6.2</span> Constrained Memory</a></li>
  <li><a href="#intermittent-connectivity" id="toc-intermittent-connectivity" class="nav-link" data-scroll-target="#intermittent-connectivity"><span class="header-section-number">14.6.3</span> Intermittent Connectivity</a></li>
  <li><a href="#power-limitations" id="toc-power-limitations" class="nav-link" data-scroll-target="#power-limitations"><span class="header-section-number">14.6.4</span> Power Limitations</a></li>
  <li><a href="#fleet-management" id="toc-fleet-management" class="nav-link" data-scroll-target="#fleet-management"><span class="header-section-number">14.6.5</span> Fleet Management</a></li>
  <li><a href="#on-device-data-collection" id="toc-on-device-data-collection" class="nav-link" data-scroll-target="#on-device-data-collection"><span class="header-section-number">14.6.6</span> On-Device Data Collection</a></li>
  <li><a href="#device-specific-personalization" id="toc-device-specific-personalization" class="nav-link" data-scroll-target="#device-specific-personalization"><span class="header-section-number">14.6.7</span> Device-Specific Personalization</a></li>
  <li><a href="#safety-considerations" id="toc-safety-considerations" class="nav-link" data-scroll-target="#safety-considerations"><span class="header-section-number">14.6.8</span> Safety Considerations</a></li>
  <li><a href="#diverse-hardware-targets" id="toc-diverse-hardware-targets" class="nav-link" data-scroll-target="#diverse-hardware-targets"><span class="header-section-number">14.6.9</span> Diverse Hardware Targets</a></li>
  <li><a href="#testing-coverage" id="toc-testing-coverage" class="nav-link" data-scroll-target="#testing-coverage"><span class="header-section-number">14.6.10</span> Testing Coverage</a></li>
  <li><a href="#concept-drift-detection" id="toc-concept-drift-detection" class="nav-link" data-scroll-target="#concept-drift-detection"><span class="header-section-number">14.6.11</span> Concept Drift Detection</a></li>
  </ul></li>
  <li><a href="#traditional-mlops-vs.-embedded-mlops" id="toc-traditional-mlops-vs.-embedded-mlops" class="nav-link" data-scroll-target="#traditional-mlops-vs.-embedded-mlops"><span class="header-section-number">14.7</span> Traditional MLOps vs.&nbsp;Embedded MLOps</a>
  <ul>
  <li><a href="#model-lifecycle-management" id="toc-model-lifecycle-management" class="nav-link" data-scroll-target="#model-lifecycle-management"><span class="header-section-number">14.7.1</span> Model Lifecycle Management</a>
  <ul class="collapse">
  <li><a href="#data-management-1" id="toc-data-management-1" class="nav-link" data-scroll-target="#data-management-1">Data Management</a></li>
  <li><a href="#model-training-1" id="toc-model-training-1" class="nav-link" data-scroll-target="#model-training-1">Model Training</a></li>
  <li><a href="#model-evaluation-1" id="toc-model-evaluation-1" class="nav-link" data-scroll-target="#model-evaluation-1">Model Evaluation</a></li>
  <li><a href="#model-deployment-1" id="toc-model-deployment-1" class="nav-link" data-scroll-target="#model-deployment-1">Model Deployment</a></li>
  </ul></li>
  <li><a href="#development-and-operations-integration" id="toc-development-and-operations-integration" class="nav-link" data-scroll-target="#development-and-operations-integration"><span class="header-section-number">14.7.2</span> Development and Operations Integration</a>
  <ul class="collapse">
  <li><a href="#cicd-pipelines-1" id="toc-cicd-pipelines-1" class="nav-link" data-scroll-target="#cicd-pipelines-1">CI/CD Pipelines</a></li>
  <li><a href="#infrastructure-management-1" id="toc-infrastructure-management-1" class="nav-link" data-scroll-target="#infrastructure-management-1">Infrastructure Management</a></li>
  <li><a href="#communication-collaboration-1" id="toc-communication-collaboration-1" class="nav-link" data-scroll-target="#communication-collaboration-1">Communication &amp; Collaboration</a></li>
  </ul></li>
  <li><a href="#operational-excellence" id="toc-operational-excellence" class="nav-link" data-scroll-target="#operational-excellence"><span class="header-section-number">14.7.3</span> Operational Excellence</a>
  <ul class="collapse">
  <li><a href="#monitoring-1" id="toc-monitoring-1" class="nav-link" data-scroll-target="#monitoring-1">Monitoring</a></li>
  <li><a href="#governance-1" id="toc-governance-1" class="nav-link" data-scroll-target="#governance-1">Governance</a></li>
  </ul></li>
  <li><a href="#comparison" id="toc-comparison" class="nav-link" data-scroll-target="#comparison"><span class="header-section-number">14.7.4</span> Comparison</a></li>
  </ul></li>
  <li><a href="#commercial-offerings" id="toc-commercial-offerings" class="nav-link" data-scroll-target="#commercial-offerings"><span class="header-section-number">14.8</span> Commercial Offerings</a>
  <ul>
  <li><a href="#traditional-mlops" id="toc-traditional-mlops" class="nav-link" data-scroll-target="#traditional-mlops"><span class="header-section-number">14.8.1</span> Traditional MLOps</a>
  <ul class="collapse">
  <li><a href="#data-management-2" id="toc-data-management-2" class="nav-link" data-scroll-target="#data-management-2">Data Management</a></li>
  <li><a href="#model-training-2" id="toc-model-training-2" class="nav-link" data-scroll-target="#model-training-2">Model Training</a></li>
  <li><a href="#model-evaluation-2" id="toc-model-evaluation-2" class="nav-link" data-scroll-target="#model-evaluation-2">Model Evaluation</a></li>
  <li><a href="#model-deployment-2" id="toc-model-deployment-2" class="nav-link" data-scroll-target="#model-deployment-2">Model Deployment</a></li>
  </ul></li>
  <li><a href="#embedded-mlops" id="toc-embedded-mlops" class="nav-link" data-scroll-target="#embedded-mlops"><span class="header-section-number">14.8.2</span> Embedded MLOps</a>
  <ul class="collapse">
  <li><a href="#edge-impulse" id="toc-edge-impulse" class="nav-link" data-scroll-target="#edge-impulse">Edge Impulse</a></li>
  <li><a href="#limitations" id="toc-limitations" class="nav-link" data-scroll-target="#limitations">Limitations</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#case-studies" id="toc-case-studies" class="nav-link" data-scroll-target="#case-studies"><span class="header-section-number">14.9</span> Case Studies</a>
  <ul>
  <li><a href="#oura-ring" id="toc-oura-ring" class="nav-link" data-scroll-target="#oura-ring"><span class="header-section-number">14.9.1</span> Oura Ring</a></li>
  <li><a href="#clinaiops" id="toc-clinaiops" class="nav-link" data-scroll-target="#clinaiops"><span class="header-section-number">14.9.2</span> ClinAIOps</a>
  <ul class="collapse">
  <li><a href="#feedback-loops" id="toc-feedback-loops" class="nav-link" data-scroll-target="#feedback-loops">Feedback Loops</a></li>
  <li><a href="#hypertension-example" id="toc-hypertension-example" class="nav-link" data-scroll-target="#hypertension-example">Hypertension Example</a></li>
  <li><a href="#mlops-vs.-clinaiops" id="toc-mlops-vs.-clinaiops" class="nav-link" data-scroll-target="#mlops-vs.-clinaiops">MLOps vs.&nbsp;ClinAIOps</a></li>
  <li><a href="#summary-1" id="toc-summary-1" class="nav-link" data-scroll-target="#summary-1">Summary</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion"><span class="header-section-number">14.10</span> Conclusion</a></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/harvard-edge/cs249r_book/edit/main/contents/ops/ops.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li><li><a href="https://github.com/harvard-edge/cs249r_book/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li><li><a href="https://github.com/harvard-edge/cs249r_book/blob/main/contents/ops/ops.qmd" class="toc-action"><i class="bi empty"></i>View source</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../contents/introduction.html">MAIN</a></li><li class="breadcrumb-item"><a href="../../contents/ops/ops.html"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Embedded AIOps</span></a></li></ol></nav>
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Embedded AIOps</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./images/png/cover_ml_ops.png" class="img-fluid figure-img"></p>
<figcaption><em>DALL·E 3 Prompt: Create a detailed, wide rectangular illustration of an AI workflow. The image should showcase the process across six stages, with a flow from left to right: 1. Data collection, with diverse individuals of different genders and descents using a variety of devices like laptops, smartphones, and sensors to gather data. 2. Data processing, displaying a data center with active servers and databases with glowing lights. 3. Model training, represented by a computer screen with code, neural network diagrams, and progress indicators. 4. Model evaluation, featuring people examining data analytics on large monitors. 5. Deployment, where the AI is integrated into robotics, mobile apps, and industrial equipment. 6. Monitoring, showing professionals tracking AI performance metrics on dashboards to check for accuracy and concept drift over time. Each stage should be distinctly marked and the style should be clean, sleek, and modern with a dynamic and informative color scheme.</em></figcaption>
</figure>
</div>
<p>This chapter explores the practices and architectures needed to effectively develop, deploy, and manage ML models across their entire lifecycle. We examine the various phases of the ML process including data collection, model training, evaluation, deployment, and monitoring. The importance of automation, collaboration, and continuous improvement is also discussed. We contrast different environments for ML model deployment, from cloud servers to embedded edge devices, and analyze their distinct constraints. Through concrete examples, we demonstrate how to tailor ML system design and operations for reliable and optimized model performance in any target environment. The goal is to provide readers with a comprehensive understanding of ML model management so they can successfully build and run ML applications that sustainably deliver value.</p>
<div class="callout callout-style-simple callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Learning Objectives
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><p>Understand what is MLOps and why it is needed</p></li>
<li><p>Learn the architectural patterns for traditional MLOps</p></li>
<li><p>Contrast traditional vs.&nbsp;embedded MLOps across the ML lifecycle</p></li>
<li><p>Identify key constraints of embedded environments</p></li>
<li><p>Learn strategies to mitigate embedded ML challenges</p></li>
<li><p>Examine real-world case studies demonstrating embedded MLOps principles</p></li>
<li><p>Appreciate the need for holistic technical and human approaches</p></li>
</ul>
</div>
</div>
<section id="introduction" class="level2" data-number="14.1">
<h2 data-number="14.1" class="anchored" data-anchor-id="introduction"><span class="header-section-number">14.1</span> Introduction</h2>
<p>Machine Learning Operations (MLOps), is a systematic approach that combines machine learning (ML), data science, and software engineering to automate the end-to-end ML lifecycle. This includes everything from data preparation and model training to deployment and maintenance. MLOps ensures that ML models are developed, deployed, and maintained efficiently and effectively.</p>
<p>Let’s start by taking a general example (i.e., non-edge ML) case. Consider a ridesharing company that wants to deploy a machine-learning model to predict rider demand in real time. The data science team spends months developing a model, but when it’s time to deploy, they realize it needs to be compatible with the engineering team’s production environment. Deploying the model requires rebuilding it from scratch - costing weeks of additional work. This is where MLOps comes in.</p>
<p>With MLOps, there are protocols and tools in place to ensure that the model developed by the data science team can be seamlessly deployed and integrated into the production environment. In essence, MLOps removes friction during the development, deployment, and maintenance of ML systems. It improves collaboration between teams through defined workflows and interfaces. MLOps also accelerates iteration speed by enabling continuous delivery for ML models.</p>
<p>For the ridesharing company, implementing MLOps means their demand prediction model can be frequently retrained and deployed based on new incoming data. This keeps the model accurate despite changing rider behavior. MLOps also allows the company to experiment with new modeling techniques since models can be quickly tested and updated.</p>
<p>Other MLOps benefits include enhanced model lineage tracking, reproducibility, and auditing. Cataloging ML workflows and standardizing artifacts - such as logging model versions, tracking data lineage, and packaging models and parameters - enables deeper insight into model provenance. Standardizing these artifacts facilitates tracing a model back to its origins, replicating the model development process, and examining how a model version has changed over time. This also facilitates regulation compliance, which is especially critical in regulated industries like healthcare and finance where being able to audit and explain models is important.</p>
<p>Major organizations adopt MLOps to boost productivity, increase collaboration, and accelerate ML outcomes. It provides the frameworks, tools, and best practices to manage ML systems throughout their lifecycle effectively. This results in better-performing models, faster time-to-value, and sustained competitive advantage. As we explore MLOps further, consider how implementing these practices can help address embedded ML challenges today and in the future.</p>
</section>
<section id="historical-context" class="level2" data-number="14.2">
<h2 data-number="14.2" class="anchored" data-anchor-id="historical-context"><span class="header-section-number">14.2</span> Historical Context</h2>
<p>MLOps has its roots in DevOps, which is a set of practices that combines software development (Dev) and IT operations (Ops) to shorten the development lifecycle and provide continuous delivery of high-quality software. The parallels between MLOps and DevOps are evident in their focus on automation, collaboration, and continuous improvement. In both cases, the goal is to break down silos between different teams (developers, operations, and, in the case of MLOps, data scientists and ML engineers) and to create a more streamlined and efficient process. It is useful to understand the history of this evolution to better understand MLOps in the context of traditional systems.</p>
<section id="devops" class="level3" data-number="14.2.1">
<h3 data-number="14.2.1" class="anchored" data-anchor-id="devops"><span class="header-section-number">14.2.1</span> DevOps</h3>
<p>The term “DevOps” was first coined in 2009 by <a href="https://www.jedi.be/">Patrick Debois</a>, a consultant and Agile practitioner. Debois organized the first <a href="https://www.devopsdays.org/">DevOpsDays</a> conference in Ghent, Belgium, in 2009, which brought together development and operations professionals to discuss ways to improve collaboration and automate processes.</p>
<p>DevOps has its roots in the <a href="https://agilemanifesto.org/">Agile</a> movement, which began in the early 2000s. Agile provided the foundation for a more collaborative approach to software development and emphasized small, iterative releases. However, Agile primarily focused on collaboration between development teams. As Agile methodologies became more popular, organizations realized the need to extend this collaboration to operations teams as well.</p>
<p>The siloed nature of development and operations teams often led to inefficiencies, conflicts, and delays in software delivery. This need for better collaboration and integration between these teams led to the <a href="https://www.atlassian.com/devops">DevOps</a> movement. In a sense, DevOps can be seen as an extension of the Agile principles to include operations teams.</p>
<p>The key principles of DevOps include collaboration, automation, continuous integration and delivery, and feedback. DevOps focuses on automating the entire software delivery pipeline, from development to deployment. It aims to improve the collaboration between development and operations teams, utilizing tools like <a href="https://www.jenkins.io/">Jenkins</a>, <a href="https://www.docker.com/">Docker</a>, and <a href="https://kubernetes.io/">Kubernetes</a> to streamline the development lifecycle.</p>
<p>While Agile and DevOps share common principles around collaboration and feedback, DevOps specifically targets the integration of development and IT operations - expanding Agile beyond just development teams. It introduces practices and tools to automate software delivery and enhance the speed and quality of software releases.</p>
</section>
<section id="mlops" class="level3" data-number="14.2.2">
<h3 data-number="14.2.2" class="anchored" data-anchor-id="mlops"><span class="header-section-number">14.2.2</span> MLOps</h3>
<p><a href="https://cloud.google.com/solutions/machine-learning/mlops-continuous-delivery-and-automation-pipelines-in-machine-learning">MLOps</a>, on the other hand, stands for MLOps, and it extends the principles of DevOps to the ML lifecycle. MLOps aims to automate and streamline the end-to-end ML lifecycle, from data preparation and model development to deployment and monitoring. The main focus of MLOps is to facilitate collaboration between data scientists, data engineers, and IT operations, and to automate the deployment, monitoring, and management of ML models. Some key factors led to the rise of MLOps.</p>
<ul>
<li><strong>Data drift:</strong> Data drift degrades model performance over time, motivating the need for rigorous monitoring and automated retraining procedures provided by MLOps.</li>
<li><strong>Reproducibility:</strong> The lack of reproducibility in machine learning experiments motivated the need for MLOps systems to track code, data, and environment variables to enable reproducible ML workflows.</li>
<li><strong>Explainability:</strong> The black box nature and lack of explainability of complex models motivated the need for MLOps capabilities to increase model transparency and explainability.</li>
<li><strong>Monitoring:</strong> The inability to reliably monitor model performance post-deployment highlighted the need for MLOps solutions with robust model performance instrumentation and alerting.</li>
<li><strong>Friction:</strong> The friction in manually retraining and deploying models motivated the need for MLOps systems that automate machine learning deployment pipelines.</li>
<li><strong>Optimization:</strong> The complexity of configuring infrastructure for machine learning motivated the need for MLOps platforms with optimized, ready-made ML infrastructure.</li>
</ul>
<p>While both DevOps and MLOps share the common goal of automating and streamlining processes, they differ in their focus and challenges. DevOps primarily deals with the challenges of software development and IT operations. In contrast, MLOps deals with the additional complexities of managing ML models, such as <a href="https://dvc.org/">data versioning</a>, <a href="https://dvc.org/">model versioning</a>, and <a href="https://www.fiddler.ai/">model monitoring</a>. MLOps also requires collaboration between various stakeholders, including data scientists, data engineers, and IT operations.</p>
<p>While DevOps and MLOps share similarities in their goals and principles, they differ in their focus and challenges. DevOps focuses on improving the collaboration between development and operations teams and automating software delivery. In contrast, MLOps focuses on streamlining and automating the ML lifecycle and facilitating collaboration between data scientists, data engineers, and IT operations.</p>
<p>Here is a table that summarizes them side by side.</p>
<table class="table">
<colgroup>
<col style="width: 23%">
<col style="width: 36%">
<col style="width: 40%">
</colgroup>
<thead>
<tr class="header">
<th>Aspect</th>
<th>DevOps</th>
<th>MLOps</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Objective</strong></td>
<td>Streamlining software development and operations processes</td>
<td>Optimizing the lifecycle of machine learning models</td>
</tr>
<tr class="even">
<td><strong>Methodology</strong></td>
<td>Continuous Integration and Continuous Delivery (CI/CD) for software development</td>
<td>Similar to CI/CD but focuses on machine learning workflows</td>
</tr>
<tr class="odd">
<td><strong>Primary Tools</strong></td>
<td>Version control (Git), CI/CD tools (Jenkins, Travis CI), Configuration management (Ansible, Puppet)</td>
<td>Data versioning tools, Model training and deployment tools, CI/CD pipelines tailored for ML</td>
</tr>
<tr class="even">
<td><strong>Primary Concerns</strong></td>
<td>Code integration, Testing, Release management, Automation, Infrastructure as code</td>
<td>Data management, Model versioning, Experiment tracking, Model deployment, Scalability of ML workflows</td>
</tr>
<tr class="odd">
<td><strong>Typical Outcomes</strong></td>
<td>Faster and more reliable software releases, Improved collaboration between development and operations teams</td>
<td>Efficient management and deployment of machine learning models, Enhanced collaboration between data scientists and engineers</td>
</tr>
</tbody>
</table>
</section>
</section>
<section id="key-components-of-mlops" class="level2" data-number="14.3">
<h2 data-number="14.3" class="anchored" data-anchor-id="key-components-of-mlops"><span class="header-section-number">14.3</span> Key Components of MLOps</h2>
<p>In this chapter, we will provide an overview of the core components of MLOps, an emerging set of practices that enables robust delivery and lifecycle management of ML models in production. While some MLOps elements like automation and monitoring were covered in previous chapters, we will integrate them into an integrated framework and expand on additional capabilities like governance. Additionally, we will describe and link to popular tools used within each component, such as <a href="https://labelstud.io/">LabelStudio</a> for data labeling. By the end, we hope that you will understand the end-to-end MLOps methodology that takes models from ideation to sustainable value creation within organizations.</p>
<section id="data-management" class="level3" data-number="14.3.1">
<h3 data-number="14.3.1" class="anchored" data-anchor-id="data-management"><span class="header-section-number">14.3.1</span> Data Management</h3>
<p>Robust data management and data engineering actively empower successful <a href="https://cloud.google.com/solutions/machine-learning/mlops-continuous-delivery-and-automation-pipelines-in-machine-learning">MLOps</a> implementations. Teams properly ingest, store, and prepare raw data from sensors, databases, apps, and other systems for model training and deployment.</p>
<p>Teams actively track changes to datasets over time using version control with <a href="https://git-scm.com/">Git</a> and tools like <a href="https://github.com/">GitHub</a> or <a href="https://about.gitlab.com/">GitLab</a>. Data scientists collaborate on curating datasets by merging changes from multiple contributors. Teams can review or roll back each iteration of a dataset if needed.</p>
<p>Teams meticulously label and annotate data using labeling software like <a href="https://labelstud.io/">LabelStudio</a>, which enables distributed teams to work on tagging datasets together. As the target variables and labeling conventions evolve, teams maintain accessibility to earlier versions.</p>
<p>Teams store the raw dataset and all derived assets on cloud storage services like <a href="https://aws.amazon.com/s3/">Amazon S3</a> or <a href="https://cloud.google.com/storage">Google Cloud Storage</a> which provide scalable, resilient storage with versioning capabilities. Teams can set granular access permissions.</p>
<p>Robust data pipelines created by teams automate the extraction, joining, cleansing and transformation of raw data into analysis-ready datasets. <a href="https://www.prefect.io/">Prefect</a>, <a href="https://airflow.apache.org/">Apache Airflow</a>, <a href="https://www.getdbt.com/">dbt</a> are workflow orchestrators that allow engineers to develop flexible, reusable data processing pipelines.</p>
<p>For instance, a pipeline may ingest data from <a href="https://www.postgresql.org/">PostgreSQL</a> databases, REST APIs, and CSVs stored on S3. It can filter, deduplicate, and aggregate the data, handle errors, and save the output to S3. The pipeline can also push the transformed data into a feature store like <a href="https://www.tecton.ai/">Tecton</a> or <a href="https://feast.dev/">Feast</a> for low-latency access.</p>
<p>In an industrial predictive maintenance use case, sensor data is ingested from devices into S3. A Prefect pipeline processes the sensor data, joining it with maintenance records. The enriched dataset is stored in Feast so models can easily retrieve the latest data for training and predictions.</p>
</section>
<section id="cicd-pipelines" class="level3" data-number="14.3.2">
<h3 data-number="14.3.2" class="anchored" data-anchor-id="cicd-pipelines"><span class="header-section-number">14.3.2</span> CI/CD Pipelines</h3>
<p>Continuous integration and continuous delivery (CI/CD) pipelines actively automate the progression of ML models from initial development into production deployment. Adapted for ML systems, CI/CD principles empower teams to rapidly and robustly deliver new models with minimized manual errors.</p>
<p>CI/CD pipelines orchestrate key steps, including checking out new code changes, transforming data, training and registering new models, validation testing, containerization, deploying to environments like staging clusters, and promoting to production. Teams leverage popular CI/CD solutions like <a href="https://www.jenkins.io/">Jenkins</a>, <a href="https://circleci.com/">CircleCI</a> and <a href="https://github.com/features/actions">GitHub Actions</a> to execute these MLOps pipelines, while <a href="https://www.prefect.io/">Prefect</a>, <a href="https://metaflow.org/">Metaflow</a> and <a href="https://www.kubeflow.org/">Kubeflow</a> offer ML-focused options.</p>
<p><a href="#fig-ci-cd" class="quarto-xref">Figure&nbsp;<span>14.1</span></a> illustrates a CI/CD pipeline specifically tailored for MLOps. The process starts with a dataset and feature repository (on the left), which feeds into a dataset ingestion stage. Post-ingestion, the data undergoes validation to ensure its quality before being transformed for training. Parallel to this, a retraining trigger can initiate the pipeline based on specified criteria. The data then passes through a model training/tuning phase within a data processing engine, followed by model evaluation and validation. Once validated, the model is registered and stored in a machine learning metadata and artifact repository. The final stage involves deploying the trained model back into the dataset and feature repository, thereby creating a cyclical process for continuous improvement and deployment of machine learning models</p>
<div id="fig-ci-cd" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-ci-cd-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/cicd_pipelines.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-ci-cd-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;14.1: MLOps CI/CD diagram. Credit: HarvardX.
</figcaption>
</figure>
</div>
<p>For example, when a data scientist checks improvements to an image classification model into a <a href="https://github.com/">GitHub</a> repository, this actively triggers a Jenkins CI/CD pipeline. The pipeline reruns data transformations and model training on the latest data, tracking experiments with <a href="https://mlflow.org/">MLflow</a>. After automated validation testing, teams deploy the model container to a <a href="https://kubernetes.io/">Kubernetes</a> staging cluster for further QA. Once approved, Jenkins facilitates a phased rollout of the model to production with <a href="https://kubernetes.io/docs/concepts/cluster-administration/manage-deployment/#canary-deployments">canary deployments</a> to catch any issues. If anomalies are detected, the pipeline enables teams to roll back to the previous model version gracefully.</p>
<p>By connecting the disparate steps from development to deployment under continuous automation, CI/CD pipelines empower teams to iterate and deliver ML models rapidly. Integrating MLOps tools like MLflow enhances model packaging, versioning, and pipeline traceability. CI/CD is integral for progressing models beyond prototypes into sustainable business systems.</p>
</section>
<section id="model-training" class="level3" data-number="14.3.3">
<h3 data-number="14.3.3" class="anchored" data-anchor-id="model-training"><span class="header-section-number">14.3.3</span> Model Training</h3>
<p>In the model training phase, data scientists actively experiment with different ML architectures and algorithms to create optimized models that effectively extract insights and patterns from data. MLOps introduces best practices and automation to make this iterative process more efficient and reproducible.</p>
<p>Modern ML frameworks like <a href="https://www.tensorflow.org/">TensorFlow</a>, <a href="https://pytorch.org/">PyTorch</a> and <a href="https://keras.io/">Keras</a> provide pre-built components that simplify designing neural networks and other model architectures. Data scientists leverage built-in modules for layers, activations, losses, etc. and high-level APIs like Keras to focus more on model architecture.</p>
<p>MLOps enables teams to package model training code into reusable, tracked scripts and notebooks. As models are developed, capabilities like <a href="https://cloud.google.com/ai-platform/training/docs/hyperparameter-tuning-overview">hyperparameter tuning</a>, <a href="https://arxiv.org/abs/1808.05377">neural architecture search</a> and <a href="https://scikit-learn.org/stable/modules/feature_selection.html">automatic feature selection</a> rapidly iterate to find the best-performing configurations.</p>
<p>Teams put training code under version control using Git and host it in repositories like GitHub to track changes over time. This allows seamless collaboration between data scientists.</p>
<p>Notebooks like <a href="https://jupyter.org/">Jupyter</a> make an excellent environment for interactive model development. The notebooks contain data ingestion, preprocessing, model declaration, training loop, evaluation, and export code in one reproducible document.</p>
<p>Finally, teams orchestrate model training as part of a CI/CD pipeline for automation. For instance, a Jenkins pipeline can trigger a Python script to load new training data, retrain a TensorFlow classifier, evaluate model metrics, and automatically register the model if performance thresholds are met.</p>
<p>An example workflow has a data scientist using a PyTorch notebook to develop a CNN model for image classification. The <a href="https://www.fast.ai/">fastai</a> library provides high-level APIs to simplify training CNNs on image datasets. The notebook trains the model on sample data, evaluates accuracy metrics, and tunes hyperparameters like learning rate and layers to optimize performance. This reproducible notebook is version-controlled and integrated into a retraining pipeline.</p>
<p>Automating and standardizing model training empowers teams to accelerate experimentation and achieve the rigor needed for production of ML systems.</p>
</section>
<section id="model-evaluation" class="level3" data-number="14.3.4">
<h3 data-number="14.3.4" class="anchored" data-anchor-id="model-evaluation"><span class="header-section-number">14.3.4</span> Model Evaluation</h3>
<p>Before deploying models, teams perform rigorous evaluation and testing to validate meeting performance benchmarks and readiness for release. MLOps introduces best practices around model validation, auditing and <a href="https://martinfowler.com/bliki/CanaryRelease.html">canary testing</a>.</p>
<p>Teams typically evaluate models against holdout <a href="https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets">test datasets</a> not used during training. The test data originates from the same distribution as production data. Teams calculate metrics like <a href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a>, <a href="https://en.wikipedia.org/wiki/Receiver_operating_characteristic#Area_under_the_curve">AUC</a>, <a href="https://en.wikipedia.org/wiki/Precision_and_recall">precision</a>, <a href="https://en.wikipedia.org/wiki/Precision_and_recall">recall</a>, and <a href="https://en.wikipedia.org/wiki/F1_score">F1 score</a>.</p>
<p>Teams also track the same metrics over time against test data samples. If evaluation data comes from live production streams, this catches <a href="https://www.ibm.com/cloud/learn/data-drift">data drifts</a> over time that degrade model performance.</p>
<p>Human oversight for model release remains important. Data scientists review performance across key segments and slices. Error analysis helps identify model weaknesses to guide enhancement. Teams apply <a href="https://developers.google.com/machine-learning/fairness-overview">fairness</a> and <a href="https://developers.google.com/machine-learning/fairness-overview">bias detection</a> techniques.</p>
<p>Canary testing releases a model to a small subset of users to evaluate real-world performance before wide deployment. Teams incrementally route traffic to the canary release while monitoring for issues.</p>
<p>For example, a retailer evaluates a personalized product recommendation model against historical test data, reviewing accuracy and diversity metrics. Teams also calculate metrics on live customer data over time, detecting decreased accuracy over the last 2 weeks. Before full rollout, the new model is released to 5% of web traffic to ensure no degradation.</p>
<p>Automating evaluation and canary releases reduces deployment risks. But human review remains critical to assess less quantifiable dynamics of model behavior. Rigorous pre-deployment validation provides confidence in putting models into production.</p>
</section>
<section id="model-deployment" class="level3" data-number="14.3.5">
<h3 data-number="14.3.5" class="anchored" data-anchor-id="model-deployment"><span class="header-section-number">14.3.5</span> Model Deployment</h3>
<p>To reliably deploy ML models to production, teams need to properly package, test and track them. MLOps introduces frameworks and procedures to actively version, deploy, monitor and update models in sustainable ways.</p>
<p>Teams containerize models using <a href="https://www.docker.com/">Docker</a> which bundles code, libraries and dependencies into a standardized unit. Containers enable smooth portability across environments.</p>
<p>Frameworks like <a href="https://www.tensorflow.org/tfx/guide/serving">TensorFlow Serving</a> and <a href="https://bentoml.org/">BentoML</a> help serve predictions from deployed models via performance-optimized APIs. These frameworks handle versioning, scaling and monitoring.</p>
<p>Teams first deploy updated models to staging or QA environments for testing before full production rollout. Shadow or canary deployments route a sample of traffic to test model variants. Teams incrementally increase access to new models.</p>
<p>Teams build robust rollback procedures in case issues emerge. Rollbacks revert to the last known good model version. Integration with CI/CD pipelines simplifies redeployment if needed.</p>
<p>Teams carefully track model artifacts like scripts, weights, logs and metrics for each version with ML metadata tools like <a href="https://mlflow.org/">MLflow</a>. This maintains lineage and auditability.</p>
<p>For example, a retailer containerizes a product recommendation model in TensorFlow Serving and deploys it to a <a href="https://kubernetes.io/">Kubernetes</a> staging cluster. After monitoring and approving performance on sample traffic, Kubernetes shifts 10% of production traffic to the new model. If no issues are detected after a few days, the new model takes over 100% of traffic. But teams keep the previous version accessible for rollback if needed.</p>
<p>Model deployment processes enable teams to make ML systems resilient in production by accounting for all transition states.</p>
</section>
<section id="infrastructure-management" class="level3" data-number="14.3.6">
<h3 data-number="14.3.6" class="anchored" data-anchor-id="infrastructure-management"><span class="header-section-number">14.3.6</span> Infrastructure Management</h3>
<p>MLOps teams heavily leverage <a href="https://www.infoworld.com/article/3271126/what-is-iac-infrastructure-as-code-explained.html">infrastructure as code (IaC)</a> tools and robust cloud architectures to actively manage the resources needed for development, training and deployment of ML systems.</p>
<p>Teams use IaC tools like <a href="https://www.terraform.io/">Terraform</a>, <a href="https://aws.amazon.com/cloudformation/">CloudFormation</a> and <a href="https://www.ansible.com/">Ansible</a> to programmatically define, provision and update infrastructure in a version controlled manner. For MLOps, teams widely use Terraform to spin up resources on <a href="https://aws.amazon.com/">AWS</a>, <a href="https://cloud.google.com/">GCP</a> and <a href="https://azure.microsoft.com/">Azure</a>.</p>
<p>For model building and training, teams dynamically provision compute resources like GPU servers, container clusters, storage and databases through Terraform as needed by data scientists. Code encapsulates and preserves infrastructure definitions.</p>
<p>Containers and orchestrators like Docker and Kubernetes provide means for teams to package models and reliably deploy them across different environments. Containers can be predictably spun up or down automatically based on demand.</p>
<p>By leveraging cloud elasticity, teams scale resources up and down to meet spikes in workloads like hyperparameter tuning jobs or spikes in prediction requests. <a href="https://aws.amazon.com/autoscaling/">Auto-scaling</a> enables optimized cost efficiency.</p>
<p>Infrastructure spans on-prem, cloud and edge devices. A robust technology stack provides flexibility and resilience. Monitoring tools give teams observability into resource utilization.</p>
<p>For example, a Terraform config may deploy a GCP Kubernetes cluster to host trained TensorFlow models exposed as prediction microservices. The cluster scales up pods to handle increased traffic. CI/CD integration seamlessly rolls out new model containers.</p>
<p>Carefully managing infrastructure through IaC and monitoring enables teams to prevent bottlenecks in operationalizing ML systems at scale.</p>
</section>
<section id="monitoring" class="level3" data-number="14.3.7">
<h3 data-number="14.3.7" class="anchored" data-anchor-id="monitoring"><span class="header-section-number">14.3.7</span> Monitoring</h3>
<p>MLOps teams actively maintain robust monitoring to sustain visibility into ML models deployed in production. Monitoring continuously provides insights into model and system performance so teams can rapidly detect and address issues to minimize disruption.</p>
<p>Teams actively monitor key model aspects including analyzing samples of live predictions to track metrics like accuracy and <a href="https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html">confusion matrix</a> over time.</p>
<p>When monitoring performance, it is important for teams to profile incoming data to check for model drift - a steady decline in model accuracy over time after production deployment. Model drift can occur in one of two ways: <a href="https://en.wikipedia.org/wiki/Concept_drift">concept drift</a> and data drift. Concept drift refers to a fundamental change observed in the relationship between the input data and the target outcomes. For instance, as the COVID-19 pandemic progressed e-commerce and retail sites had to correct their model recommendations, since purchase data was overwhelmingly skewed towards items like hand sanitizer. Data drift describes changes in the distribution of data over time. For example, image recognition algorithms used in self-driving cars will need to account for seasonality in observing their surroundings. Teams also track application performance metrics like latency and errors for model integrations.</p>
<p>From an infrastructure perspective, teams monitor for capacity issues like high CPU, memory and disk utilization as well as system outages. Tools like <a href="https://prometheus.io/">Prometheus</a>, <a href="https://grafana.com/">Grafana</a> and <a href="https://www.elastic.co/">Elastic</a> enable teams to actively collect, analyze, query and visualize diverse monitoring metrics. Dashboards make dynamics highly visible.</p>
<p>Teams configure alerting for key monitoring metrics like accuracy declines and system faults to enable proactively responding to events that threaten reliability. For example, drops in model accuracy trigger alerts for teams to investigate potential data drift and retrain models using updated, representative data samples.</p>
<p>Comprehensive monitoring enables teams to maintain confidence in model and system health after deployment. It empowers teams to catch and resolve deviations through data-driven alerts and dashboards preemptively. Active monitoring is essential for maintaining highly available, trustworthy ML systems.</p>
</section>
<section id="governance" class="level3" data-number="14.3.8">
<h3 data-number="14.3.8" class="anchored" data-anchor-id="governance"><span class="header-section-number">14.3.8</span> Governance</h3>
<p>MLOps teams actively establish proper governance practices as a critical component. Governance provides oversight into ML models to ensure they are trustworthy, ethical, and compliant. Without governance, significant risks exist of models behaving in dangerous or prohibited ways when deployed in applications and business processes.</p>
<p>MLOps governance employs techniques to provide transparency into model predictions, performance, and behavior throughout the ML lifecycle. Explainability methods like <a href="https://github.com/slundberg/shap">SHAP</a> and <a href="https://github.com/marcotcr/lime">LIME</a> help auditors understand why models make certain predictions by highlighting influential input features behind decisions. <a href="https://developers.google.com/machine-learning/fairness-overview">Bias detection</a> analyzes model performance across different demographic groups defined by attributes like age, gender and ethnicity to detect any systematic skews. Teams perform rigorous testing procedures on representative datasets to validate model performance before deployment.</p>
<p>Once in production, teams monitor <a href="https://en.wikipedia.org/wiki/Concept_drift">concept drift</a> to track if predictive relationships change over time in ways that degrade model accuracy. Teams analyze production logs to uncover patterns in the types of errors models generate. Documentation about data provenance, development procedures, and evaluation metrics provides additional visibility.</p>
<p>Platforms like <a href="https://www.ibm.com/cloud/watson-openscale">Watson OpenScale</a> incorporate governance capabilities like bias monitoring and explainability directly into model building, testing and production monitoring. The key focus areas of governance are transparency, fairness, and compliance. This minimizes risks of models behaving incorrectly or dangerously when integrated into business processes. Embedding governance practices into MLOps workflows enables teams to ensure trustworthy AI.</p>
</section>
<section id="communication-collaboration" class="level3" data-number="14.3.9">
<h3 data-number="14.3.9" class="anchored" data-anchor-id="communication-collaboration"><span class="header-section-number">14.3.9</span> Communication &amp; Collaboration</h3>
<p>MLOps actively breaks down silos and enables free flow of information and insights between teams through all ML lifecycle stages. Tools like <a href="https://mlflow.org/">MLflow</a>, <a href="https://wandb.ai/">Weights &amp; Biases</a>, and data contexts provide traceability and visibility to improve collaboration.</p>
<p>Teams use MLflow to systematize tracking of model experiments, versions, and artifacts. Experiments can be programmatically logged from data science notebooks and training jobs. The model registry provides a central hub for teams to store production-ready models before deployment, with metadata like descriptions, metrics, tags and lineage. Integrations with <a href="https://github.com/">Github</a>, <a href="https://about.gitlab.com/">GitLab</a> facilitate code change triggers.</p>
<p>Weights &amp; Biases provides collaborative tools tailored to ML teams. Data scientists log experiments, visualize metrics like loss curves, and share experimentation insights with colleagues. Comparison dashboards highlight model differences. Teams discuss progress and next steps.</p>
<p>Establishing shared data contexts - glossaries, <a href="https://en.wikipedia.org/wiki/Data_dictionary">data dictionaries</a>, schema references - ensures alignment on data meaning and usage across roles. Documentation aids understanding for those without direct data access.</p>
<p>For example, a data scientist may use Weights &amp; Biases to analyze an anomaly detection model experiment and share the evaluation results with other team members to discuss improvements. The final model can then be registered with MLflow before handing off for deployment.</p>
<p>Enabling transparency, traceability and communication via MLOps empowers teams to remove bottlenecks and accelerate delivery of impactful ML systems.</p>
</section>
</section>
<section id="hidden-technical-debt-in-ml-systems" class="level2" data-number="14.4">
<h2 data-number="14.4" class="anchored" data-anchor-id="hidden-technical-debt-in-ml-systems"><span class="header-section-number">14.4</span> Hidden Technical Debt in ML Systems</h2>
<p>Technical debt is an increasingly pressing issue for ML systems (see Figure 14.2). This metaphor, originally proposed in the 1990s, likens the long-term costs of quick software development to financial debt. Just as some financial debt powers beneficial growth, carefully managed technical debt enables rapid iteration. However, left unchecked, accumulating technical debt can outweigh any gains.</p>
<p><a href="#fig-technical-debt" class="quarto-xref">Figure&nbsp;<span>14.2</span></a> illustrates the various components that contribute to hidden technical debt in ML systems. It shows the interconnected nature of configuration, data collection, and feature extraction, which are foundational to the ML codebase. The box sizes indicate the proportion of the entire system represented by each component. In industry ML systems, the code for the model algorithm makes up only a very tiny fraction (see the small black box in the middle as compared to all the other large boxes). The complexity of ML systems and the fast-paced nature of the industry make it very easy to accumulate technical debt.</p>
<div id="fig-technical-debt" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-technical-debt-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/hidden_debt.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-technical-debt-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;14.2: ML system components. Credit: <span class="citation" data-cites="sculley2015hidden">Sambasivan et al. (<a href="../../references.html#ref-sculley2015hidden" role="doc-biblioref">2021a</a>)</span>
</figcaption>
</figure>
</div>
<section id="model-boundary-erosion" class="level3" data-number="14.4.1">
<h3 data-number="14.4.1" class="anchored" data-anchor-id="model-boundary-erosion"><span class="header-section-number">14.4.1</span> Model Boundary Erosion</h3>
<p>Unlike traditional software, ML lacks clear boundaries between components as seen in the diagram above. This erosion of abstraction creates entanglements that exacerbate technical debt in several ways:</p>
</section>
<section id="entanglement" class="level3" data-number="14.4.2">
<h3 data-number="14.4.2" class="anchored" data-anchor-id="entanglement"><span class="header-section-number">14.4.2</span> Entanglement</h3>
<p>Tight coupling between ML model components makes isolating changes difficult. Modifying one part causes unpredictable ripple effects throughout the system. Changing anything changes everything (also known as CACE) is a phenomenon that applies to any tweak you make to your system. Potential mitigations include decomposing the problem when possible or closely monitoring for changes in behavior to contain their impact.</p>
</section>
<section id="correction-cascades" class="level3" data-number="14.4.3">
<h3 data-number="14.4.3" class="anchored" data-anchor-id="correction-cascades"><span class="header-section-number">14.4.3</span> Correction Cascades</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/png/data_cascades.png" class="img-fluid figure-img"></p>
<figcaption>Figure 14.3: The flowchart depicts the concept of correction cascades in the ML workflow, from problem statement to model deployment. The arcs represent the potential iterative corrections needed at each stage of the workflow, with different colors corresponding to distinct issues such as interacting with physical world brittleness, inadequate application-domain expertise, conflicting reward systems, and poor cross-organizational documentation. The red arrows indicate the impact of cascades, which can lead to significant revisions in the model development process, while the dotted red line represents the drastic measure of abandoning the process to restart. This visual emphasizes the complex, interconnected nature of ML system development and the importance of addressing these issues early in the development cycle to mitigate their amplifying effects downstream. <span class="citation" data-cites="sculley2015hidden">(<a href="../../references.html#ref-sculley2015hidden" role="doc-biblioref">Sambasivan et al. 2021a</a>)</span></figcaption>
</figure>
</div>
<p>Building models sequentially creates risky dependencies where later models rely on earlier ones. For example, taking an existing model and fine-tuning it for a new use case seems efficient. However, this bakes in assumptions from the original model that may eventually need correction.</p>
<p>There are several factors that inform the decision to build models sequentially or not:</p>
<ul>
<li><strong>Dataset size and rate of growth:</strong> With small, static datasets, it often makes sense to fine-tune existing models. For large, growing datasets, training custom models from scratch allows more flexibility to account for new data.</li>
<li><strong>Available computing resources:</strong> Fine-tuning requires less resources than training large models from scratch. With limited resources, leveraging existing models may be the only feasible approach.</li>
</ul>
<p>While fine-tuning can be efficient, modifying foundational components later becomes extremely costly due to the cascading effects on subsequent models. Careful thought should be given to identifying points where introducing fresh model architectures, even with large resource requirements, can avoid correction cascades down the line (see Figure 14.3). There are still scenarios where sequential model building makes sense, so it entails weighing these tradeoffs around efficiency, flexibility, and technical debt.</p>
<p><a href="#fig-data-cascades-debt" class="quarto-xref">Figure&nbsp;<span>14.3</span></a> depicts the concept of correction cascades in the ML workflow, from problem statement to model deployment. The arcs represent the potential iterative corrections needed at each stage of the workflow, with different colors corresponding to distinct issues such as interacting with physical world brittleness, inadequate application-domain expertise, conflicting reward systems, and poor cross-organizational documentation. The red arrows indicate the impact of cascades, which can lead to significant revisions in the model development process, while the dotted red line represents the drastic measure of abandoning the process to restart. This visual emphasizes the complex, interconnected nature of ML system development and the importance of addressing these issues early in the development cycle to mitigate their amplifying effects downstream.</p>
<div id="fig-data-cascades-debt" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-data-cascades-debt-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/data_cascades.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-data-cascades-debt-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;14.3: Data cascades. Credit: <span class="citation" data-cites="sambasivan2021">Sambasivan et al. (<a href="../../references.html#ref-sambasivan2021" role="doc-biblioref">2021b</a>)</span>.
</figcaption>
</figure>
</div>
</section>
<section id="undeclared-consumers" class="level3" data-number="14.4.4">
<h3 data-number="14.4.4" class="anchored" data-anchor-id="undeclared-consumers"><span class="header-section-number">14.4.4</span> Undeclared Consumers</h3>
<p>Once ML model predictions are made available, many downstream systems may silently consume them as inputs for further processing. However, the original model was not designed to accommodate this broad reuse. Due to the inherent opacity of ML systems, it becomes impossible to fully analyze the impact of the model’s outputs as inputs elsewhere. Changes to the model can then have expensive and dangerous consequences by breaking undiscovered dependencies.</p>
<p>Undeclared consumers can also enable hidden feedback loops if their outputs indirectly influence the original model’s training data. Mitigations include restricting access to predictions, defining strict service contracts, and monitoring for signs of un-modelled influences. Architecting ML systems to encapsulate and isolate their effects limits the risks from unanticipated propagation.</p>
</section>
<section id="data-dependency-debt" class="level3" data-number="14.4.5">
<h3 data-number="14.4.5" class="anchored" data-anchor-id="data-dependency-debt"><span class="header-section-number">14.4.5</span> Data Dependency Debt</h3>
<p>Data dependency debt refers to unstable and underutilized data dependencies which can have detrimental and hard to detect repercussions. While this is a key contributor to tech debt for traditional software, those systems can benefit from the use of widely available tools for static analysis by compilers and linkers to identify dependencies of these types. ML systems lack similar tooling.</p>
<p>One mitigation for unstable data dependencies is to use versioning which ensures the stability of inputs but comes with the cost of managing multiple sets of data and the potential for staleness of the data. A mitigation for underutilized data dependencies is to conduct exhaustive leave-one-feature-out evaluation.</p>
</section>
<section id="analysis-debt-from-feedback-loops" class="level3" data-number="14.4.6">
<h3 data-number="14.4.6" class="anchored" data-anchor-id="analysis-debt-from-feedback-loops"><span class="header-section-number">14.4.6</span> Analysis Debt from Feedback Loops</h3>
<p>Unlike traditional software, ML systems can change their own behavior over time, making it difficult to analyze pre-deployment. This debt manifests in feedback loops, both direct and hidden.</p>
<p>Direct feedback loops occur when a model influences its own future inputs, such as by recommending products to users that in turn shape future training data. Hidden loops arise indirectly between models, such as two systems that interact via real-world environments. Gradual feedback loops are especially hard to detect. These loops lead to analysis debt—the inability to fully predict how a model will act after release. They undermine pre-deployment validation by enabling unmodeled self-influence.</p>
<p>Careful monitoring and canary deployments help detect feedback. But fundamental challenges remain in understanding complex model interactions. Architectural choices that reduce entanglement and coupling mitigate analysis debt’s compounding effect.</p>
</section>
<section id="pipeline-jungles" class="level3" data-number="14.4.7">
<h3 data-number="14.4.7" class="anchored" data-anchor-id="pipeline-jungles"><span class="header-section-number">14.4.7</span> Pipeline Jungles</h3>
<p>ML workflows often lack standardized interfaces between components. This leads teams to incrementally “glue” together pipelines with custom code. What emerges are “pipeline jungles”—tangled preprocessing steps that are brittle and resist change. Avoiding modifications to these messy pipelines causes teams to experiment through alternate prototypes. Soon, multiple ways of doing everything proliferate. The lack of abstractions and interfaces then impedes sharing, reuse, and efficiency.</p>
<p>Technical debt accumulates as one-off pipelines solidify into legacy constraints. Teams sink time into managing idiosyncratic code rather than maximizing model performance. Architectural principles like modularity and encapsulation are needed to establish clean interfaces. Shared abstractions enable interchangeable components, prevent lock-in, and promote best practice diffusion across teams. Breaking free of pipeline jungles ultimately requires enforcing standards that prevent accretion of abstraction debt. The benefits of interfaces and APIs that tame complexity outweigh the transitional costs.</p>
</section>
<section id="configuration-debt" class="level3" data-number="14.4.8">
<h3 data-number="14.4.8" class="anchored" data-anchor-id="configuration-debt"><span class="header-section-number">14.4.8</span> Configuration Debt</h3>
<p>ML systems involve extensive configuration of hyperparameters, architectures, and other tuning parameters. However, configuration is often an afterthought, lacking rigor and testing. Ad hoc configurations proliferate, amplified by the many knobs available for tuning complex ML models.</p>
<p>This accumulation of technical debt has several consequences. Fragile and outdated configurations lead to hidden dependencies and bugs that cause production failures. Knowledge about optimal configurations is isolated rather than shared, leading to redundant work. Reproducing and comparing results becomes difficult when configuration lacks documentation. Legacy constraints accrete as teams fear changing poorly understood configurations.</p>
<p>Addressing configuration debt requires establishing standards to document, test, validate, and centrally store configurations. Investing in more automated approaches such as hyperparameter optimization and architecture search reduces dependence on manual tuning. Better configuration hygiene makes iterative improvement more tractable by preventing complexity from compounding endlessly. The key is recognizing configuration as an integral part of the ML system lifecycle rather than an ad hoc afterthought.</p>
</section>
<section id="the-changing-world" class="level3" data-number="14.4.9">
<h3 data-number="14.4.9" class="anchored" data-anchor-id="the-changing-world"><span class="header-section-number">14.4.9</span> The Changing World</h3>
<p>ML systems operate in dynamic real-world environments. Thresholds and decisions that are initially effective become outdated as the world evolves. But legacy constraints make it difficult to adapt systems to reflect changing populations, usage patterns, and other shifting contextual factors.</p>
<p>This debt manifests in two main ways. First, preset thresholds and heuristics require constant re-evaluation and tuning as their optimal values drift. Second, validating systems through static unit and integration tests fails when inputs and behaviors are moving targets.</p>
<p>Responding to a changing world in real-time with legacy ML systems is challenging. Technical debt accumulates as assumptions decay. The lack of modular architecture and ability to dynamically update components without side effects exacerbates these issues.</p>
<p>Mitigating this requires building in configurability, monitoring, and modular updatability. Online learning where models continuously adapt, as well as robust feedback loops to training pipelines, help automatically tune to the world. But anticipating and architecting for change is essential to prevent erosion of real-world performance over time.</p>
</section>
<section id="navigating-technical-debt-in-early-stages" class="level3" data-number="14.4.10">
<h3 data-number="14.4.10" class="anchored" data-anchor-id="navigating-technical-debt-in-early-stages"><span class="header-section-number">14.4.10</span> Navigating Technical Debt in Early Stages</h3>
<p>It is understandable that technical debt accumulates naturally in early stages of model development. When aiming to build MVP models quickly, teams often lack complete information on what components will reach scale or require modification. Some deferred work is expected.</p>
<p>However, even scrappy initial systems should follow principles like “Flexible Foundations” to avoid painting themselves into corners:</p>
<ul>
<li>Modular code and reusable libraries allow components to be swapped later</li>
<li>Loose coupling between models, data stores, and business logic facilitates change</li>
<li>Abstraction layers hide implementation details that may shift over time</li>
<li>Containerized model serving keeps options open on deployment requirements</li>
</ul>
<p>Decisions that seem expedient in the moment can seriously limit future flexibility. For example, baking key business logic into model code rather than keeping it separate makes subsequent model changes extremely difficult.</p>
<p>With thoughtful design, though, it is possible to build quickly at first while retaining degrees of freedom to improve. As the system matures, prudent break points emerge where introducing fresh architectures proactively avoids massive rework down the line. This balances urgent timelines with reducing future correction cascades.</p>
</section>
<section id="summary" class="level3" data-number="14.4.11">
<h3 data-number="14.4.11" class="anchored" data-anchor-id="summary"><span class="header-section-number">14.4.11</span> Summary</h3>
<p>Although financial debt is a good metaphor to understand the tradeoffs, it differs from technical debt in its measurability. Technical debt lacks the ability to be fully tracked and quantified. This makes it hard for teams to navigate the tradeoffs between moving quickly and inherently introducing more debt versus taking the time to pay down that debt.</p>
<p>The <a href="https://papers.nips.cc/paper_files/paper/2015/file/86df7dcfd896fcaf2674f757a2463eba-Paper.pdf">Hidden Technical Debt of Machine Learning Systems</a> paper spreads awareness of the nuances of ML system specific tech debt and encourages additional development in the broad area of maintainable ML.</p>
</section>
</section>
<section id="roles-and-responsibilities" class="level2" data-number="14.5">
<h2 data-number="14.5" class="anchored" data-anchor-id="roles-and-responsibilities"><span class="header-section-number">14.5</span> Roles and Responsibilities</h2>
<p>Given the vastness of MLOps, successfully implementing ML systems requires diverse skills and close collaboration between people with different areas of expertise. While data scientists build the core ML models, it takes cross-functional teamwork to successfully deploy these models into production environments and enable them to deliver business value in a sustainable way.</p>
<p>MLOps provides the framework and practices for coordinating the efforts of various roles involved in developing, deploying and running MLg systems. Bridging traditional silos between data, engineering and operations teams is key to MLOps success. Enabling seamless collaboration through the machine learning lifecycle accelerates benefit realization while ensuring long-term reliability and performance of ML models.</p>
<p>We will look at some of the key roles involved in MLOps and their primary responsibilities. Understanding the breadth of skills needed to operationalize ML models provides guidance on assembling MLOps teams. It also clarifies how the workflows between different roles fit together under the overarching MLOps methodology.</p>
<section id="data-engineers" class="level3" data-number="14.5.1">
<h3 data-number="14.5.1" class="anchored" data-anchor-id="data-engineers"><span class="header-section-number">14.5.1</span> Data Engineers</h3>
<p>Data engineers are responsible for building and maintaining the data infrastructure and pipelines that feed data to ML models. They ensure data is smoothly moved from source systems into the storage, processing, and feature engineering environments needed for ML model development and deployment. Their main responsibilities include:</p>
<ul>
<li>Migrating raw data from on-prem databases, sensors, apps into cloud-based data lakes like Amazon S3 or Google Cloud Storage. This provides cost-efficient, scalable storage.</li>
<li>Building data pipelines with workflow schedulers like Apache Airflow, Prefect, dbt. These extract data from sources, transform and validate data, and load it into destinations like data warehouses, feature stores or directly for model training.</li>
<li>Transforming messy raw data into structured, analysis-ready datasets. This includes handling null or malformed values, deduplicating, joining disparate data sources, aggregating data and engineering new features.</li>
<li>Maintaining data infrastructure components like cloud data warehouses (<a href="https://www.snowflake.com/en/data-cloud/workloads/data-warehouse/">Snowflake</a>, <a href="https://aws.amazon.com/redshift/">Redshift</a>, <a href="https://cloud.google.com/bigquery?hl=en">BigQuery</a>), data lakes, and metadata management systems. Provisioning and optimizing data processing systems.</li>
<li>Establishing data versioning, backup and archival processes for ML datasets and features. Enforcing data governance policies.</li>
</ul>
<p>For example, a manufacturing firm may use Apache Airflow pipelines to extract sensor data from PLCs on the factory floor into an Amazon S3 data lake. The data engineers would then process this raw data to filter, clean, and join it with product metadata. These pipeline outputs would then load into a Snowflake data warehouse from which features can be read for model training and prediction.</p>
<p>The data engineering team builds and sustains the data foundation for reliable model development and operations. Their work enables data scientists and ML engineers to focus on building, training and deploying ML models at scale.</p>
</section>
<section id="data-scientists" class="level3" data-number="14.5.2">
<h3 data-number="14.5.2" class="anchored" data-anchor-id="data-scientists"><span class="header-section-number">14.5.2</span> Data Scientists</h3>
<p>The job of the data scientists is to focus on the research, experimentation, development and continuous improvement of ML models. They leverage their expertise in statistics, modeling and algorithms to create high-performing models. Their main responsibilities include:</p>
<ul>
<li>Working with business and data teams to identify opportunities where ML can add value. Framing the problem and defining success metrics.</li>
<li>Performing exploratory data analysis to understand relationships in data and derive insights. Identifying relevant features for modeling.</li>
<li>Researching and experimenting with different ML algorithms and model architectures based on the problem and data characteristics. Leveraging libraries like TensorFlow, PyTorch, Keras.</li>
<li>Training and fine-tuning models by tuning hyperparameters, adjusting neural network architectures, feature engineering, etc. to maximize performance.</li>
<li>Evaluating model performance through metrics like accuracy, AUC, F1 scores. Performing error analysis to identify areas for improvement.</li>
<li>Developing new model versions by incorporating new data, testing different approaches, and optimizing model behavior. Maintaining documentation and lineage for models.</li>
</ul>
<p>For example, a data scientist may leverage TensorFlow and <a href="https://www.tensorflow.org/probability">TensorFlow Probability</a> to develop a demand forecasting model for retail inventory planning. They would iterate on different sequence models like LSTMs and experiment with features derived from product, sales and seasonal data. The model would be evaluated based on error metrics versus actual demand before deployment. The data scientist monitors performance and retrains/enhances the model as new data comes in.</p>
<p>Data scientists drive model creation, improvement and innovation through their expertise in ML techniques. They collaborate closely with other roles to ensure models create maximum business impact.</p>
</section>
<section id="ml-engineers" class="level3" data-number="14.5.3">
<h3 data-number="14.5.3" class="anchored" data-anchor-id="ml-engineers"><span class="header-section-number">14.5.3</span> ML Engineers</h3>
<p>ML engineers enable models data scientists develop to be productized and deployed at scale. Their expertise makes models reliably serve predictions in applications and business processes. Their main responsibilities include:</p>
<ul>
<li>Taking prototype models from data scientists and hardening them for production environments through coding best practices.</li>
<li>Building APIs and microservices for model deployment using tools like <a href="https://flask.palletsprojects.com/en/3.0.x/">Flask</a>, <a href="https://fastapi.tiangolo.com/">FastAPI</a>. Containerizing models with Docker.</li>
<li>Managing model versions and sinaging new models into production using CI/CD pipelines. Implementing canary releases, A/B tests, and rollback procedures.</li>
<li>Optimizing model performance for high scalability, low latency and cost-efficiency. Leveraging compression, quantization, multi-model serving.</li>
<li>Monitoring models once in production and ensuring continued reliability and accuracy. Retraining models periodically.</li>
</ul>
<p>For example, a ML engineer may take a TensorFlow fraud detection model developed by data scientists and containerize it using TensorFlow Serving for scalable deployment. The model would be integrated into the company’s transaction processing pipeline via APIs. The ML engineer implements a model registry and CI/CD pipeline using MLFlow and Jenkins to reliably deploy model updates. The ML engineers would then monitor the running model for continued performance using tools like Prometheus and Grafana. If model accuracy drops, they initiate retraining and deployment of a new model version.</p>
<p>The ML engineering team enables data science models to progress smoothly into sustainable and robust production systems. Their expertise in building modular, monitored systems delivers continuous business value.</p>
</section>
<section id="devops-engineers" class="level3" data-number="14.5.4">
<h3 data-number="14.5.4" class="anchored" data-anchor-id="devops-engineers"><span class="header-section-number">14.5.4</span> DevOps Engineers</h3>
<p>DevOps engineers enable MLOps by building and managing the underlying infrastructure for developing, deploying, and monitoring ML models. They provide the cloud architecture and automation pipelines. Their main responsibilities include:</p>
<ul>
<li>Provisioning and managing cloud infrastructure for ML workflows using IaC tools like Terraform, Docker, Kubernetes.</li>
<li>Developing CI/CD pipelines for model retraining, validation, and deployment. Integrating ML tools into the pipeline like MLflow, Kubeflow.</li>
<li>Monitoring model and infrastructure performance using tools like <a href="https://prometheus.io/">Prometheus</a>, <a href="https://grafana.com/">Grafana</a>, <a href="https://aws.amazon.com/what-is/elk-stack/">ELK stack</a>. Building alerts and dashboards.</li>
<li>Implementing governance practices around model development, testing, and promotion. Enabling reproducibility and traceability.</li>
<li>Embedding ML models within applications. Exposing models via APIs and microservices for integration.</li>
<li>Optimizing infrastructure performance and costs. Leveraging autoscaling, spot instances, and availability across regions.</li>
</ul>
<p>For example, a DevOps engineer provisions a Kubernetes cluster on AWS using Terraform to run ML training jobs and online deployment. They build a CI/CD pipeline in Jenkins which triggers model retraining if new data is available. After automated testing, the model is registered with MLflow and deployed in the Kubernetes cluster. The engineer then monitors cluster health, container resource usage, and API latency using Prometheus and Grafana.</p>
<p>The DevOps team enables rapid experimentation and reliable deployments for ML through expertise in cloud, automation, and monitoring. Their work maximizes model impact while minimizing technical debt.</p>
</section>
<section id="project-managers" class="level3" data-number="14.5.5">
<h3 data-number="14.5.5" class="anchored" data-anchor-id="project-managers"><span class="header-section-number">14.5.5</span> Project Managers</h3>
<p>Project managers play a vital role in MLOps by coordinating the activities between the different teams involved in delivering ML projects. They help drive alignment, accountability, and accelerated results. Their main responsibilities include:</p>
<ul>
<li>Working with stakeholders to define project goals, success metrics, timelines and budgets. Outlining specifications and scope.</li>
<li>Creating a project plan spanning activities like data acquisition, model development, infrastructure setup, deployment, and monitoring.</li>
<li>Coordinating design, development and testing efforts between data engineers, data scientists, ML engineers and DevOps roles.</li>
<li>Tracking progress and milestones. Identifying roadblocks and resolving through corrective actions. Managing risks and issues.</li>
<li>Facilitating communication through status reports, meetings, workshops, documentation. Enabling seamless collaboration.</li>
<li>Driving adherence to timelines and budget. Escalating anticipated overruns or shortfalls for mitigation.</li>
</ul>
<p>For example, a project manager would create a project plan for the development and ongoing enhancement of a customer churn prediction model. They coordinate between data engineers building data pipelines, data scientists experimenting with models, ML engineers productionalizing models, and DevOps setting up deployment infrastructure. The project manager tracks progress via milestones like dataset preparation, model prototyping, deployment, and monitoring. They surface any risks, delays or budget issues to enact preventive solutions.</p>
<p>Skilled project managers enable MLOps teams to work synergistically to deliver maximum business value from ML investments rapidly. Their leadership and organization align with diverse teams.</p>
</section>
</section>
<section id="embedded-system-challenges" class="level2" data-number="14.6">
<h2 data-number="14.6" class="anchored" data-anchor-id="embedded-system-challenges"><span class="header-section-number">14.6</span> Embedded System Challenges</h2>
<p>We will briefly review the challenges with embedded systems so taht it sets the context for the specific challenges that emerge with embedded MLOps that we will discuss in the following section.</p>
<section id="limited-compute-resources" class="level3" data-number="14.6.1">
<h3 data-number="14.6.1" class="anchored" data-anchor-id="limited-compute-resources"><span class="header-section-number">14.6.1</span> Limited Compute Resources</h3>
<p>Embedded devices like microcontrollers and mobile phones have much more constrained compute power compared to data center machines or GPUs. A typical microcontroller may have only KB of RAM, MHz of CPU speed, and no GPU. For example, a microcontroller in a smartwatch may only have a 32-bit processor running at 120MHz with 320KB of RAM <span class="citation" data-cites="stm2021l4">(<a href="../../references.html#ref-stm2021l4" role="doc-biblioref"><em>STM32L4Q5AG</em> 2021</a>)</span>. This allows relatively simple ML models like small linear regressions or random forests, but more complex deep neural networks would be infeasible. Strategies to mitigate this include quantization, pruning, efficient model architectures, and offloading certain computations to the cloud when connectivity allows.</p>
</section>
<section id="constrained-memory" class="level3" data-number="14.6.2">
<h3 data-number="14.6.2" class="anchored" data-anchor-id="constrained-memory"><span class="header-section-number">14.6.2</span> Constrained Memory</h3>
<p>With limited memory, storing large ML models and datasets directly on embedded devices is often infeasible. For example, a deep neural network model can easily take hundreds of MB, which exceeds the storage capacity of many embedded systems. Consider this example. A wildlife camera that captures images to detect animals may have only a 2GB memory card. This is insufficient to store a deep learning model for image classification that is often hundreds of MB in size. Consequently, this requires optimization of memory usage through methods like weights compression, lower-precision numerics, and streaming inference pipelines.</p>
</section>
<section id="intermittent-connectivity" class="level3" data-number="14.6.3">
<h3 data-number="14.6.3" class="anchored" data-anchor-id="intermittent-connectivity"><span class="header-section-number">14.6.3</span> Intermittent Connectivity</h3>
<p>Many embedded devices operate in remote environments without reliable internet connectivity. This means we cannot rely on constant cloud access for convenient retraining, monitoring, and deployment. Instead, we need smart scheduling and caching strategies to optimize for intermittent connections. For example, a model predicting crop yield on a remote farm may need to make predictions daily, but only have connectivity to the cloud once a week when the farmer drives into town. The model needs to operate independently in between connections.</p>
</section>
<section id="power-limitations" class="level3" data-number="14.6.4">
<h3 data-number="14.6.4" class="anchored" data-anchor-id="power-limitations"><span class="header-section-number">14.6.4</span> Power Limitations</h3>
<p>Embedded devices like phones, wearables, and remote sensors are battery-powered. Continual inference and communication can quickly drain those batteries, limiting functionality. For example, a smart collar tagging endangered animals runs on a small battery. Continuously running a GPS tracking model would drain the battery within days. The collar has to carefully schedule when to activate the model. Thus, embedded ML has to carefully manage tasks to conserve power. Techniques include optimized hardware accelerators, prediction caching, and adaptive model execution.</p>
</section>
<section id="fleet-management" class="level3" data-number="14.6.5">
<h3 data-number="14.6.5" class="anchored" data-anchor-id="fleet-management"><span class="header-section-number">14.6.5</span> Fleet Management</h3>
<p>For mass-produced embedded devices, there can be millions of units deployed in the field to orchestrate updates for. Hypothetically, updating a fraud detection model on 100 million (future smart) credit cards requires securely pushing updates to each distributed device rather than a centralized data center. Such distributed scale makes fleet-wide management much harder than a centralized server cluster. It requires intelligent protocols for over-the-air updates, handling connectivity issues, and monitoring resource constraints across devices.</p>
</section>
<section id="on-device-data-collection" class="level3" data-number="14.6.6">
<h3 data-number="14.6.6" class="anchored" data-anchor-id="on-device-data-collection"><span class="header-section-number">14.6.6</span> On-Device Data Collection</h3>
<p>Collecting useful training data requires engineering both the sensors on device as well as the software pipelines. This is unlike servers where we can pull data from external sources. Challenges include handling sensor noise. Sensors on an industrial machine detect vibrations and temperature to predict maintenance needs. This requires tuning the sensors and sampling rates to capture useful data.</p>
</section>
<section id="device-specific-personalization" class="level3" data-number="14.6.7">
<h3 data-number="14.6.7" class="anchored" data-anchor-id="device-specific-personalization"><span class="header-section-number">14.6.7</span> Device-Specific Personalization</h3>
<p>A smart speaker learns an individual user’s voice patterns and speech cadence to improve recognition accuracy, all while protecting privacy. Adapting ML models to specific devices and users is important but this poses privacy challenges. On-device learning allows personalization without transmitting as much private data. But balancing model improvement, privacy preservation, and constraints requires novel techniques.</p>
</section>
<section id="safety-considerations" class="level3" data-number="14.6.8">
<h3 data-number="14.6.8" class="anchored" data-anchor-id="safety-considerations"><span class="header-section-number">14.6.8</span> Safety Considerations</h3>
<p>For extremely large embedded ML in systems like self-driving vehicles, there are serious safety risks if not engineered carefully. Self-driving cars must undergo extensive track testing in simulated rain, snow, and obstacle scenarios to ensure safe operation before deployment. This requires extensive validation, fail-safes, simulators, and standards compliance before deployment.</p>
</section>
<section id="diverse-hardware-targets" class="level3" data-number="14.6.9">
<h3 data-number="14.6.9" class="anchored" data-anchor-id="diverse-hardware-targets"><span class="header-section-number">14.6.9</span> Diverse Hardware Targets</h3>
<p>There are a diverse range of embedded processors including ARM, x86, specialized AI accelerators, FPGAs etc. Supporting this heterogeneity makes deployment challenging. We need strategies like standardized frameworks, extensive testing, and allowing model tuning for each platform. For example, an object detection model needs efficient implementations across embedded devices like a Raspberry Pi, Nvidia Jetson, and Google Edge TPU.</p>
</section>
<section id="testing-coverage" class="level3" data-number="14.6.10">
<h3 data-number="14.6.10" class="anchored" data-anchor-id="testing-coverage"><span class="header-section-number">14.6.10</span> Testing Coverage</h3>
<p>Rigorously testing edge cases is difficult with constrained embedded resources for simulation. But exhaustive testing is critical in systems like self-driving cars. Exhaustively testing an autopilot model requires millions of simulated kilometers exposing it to extremely rare events like sensor failures. Therefore, strategies like synthetic data generation, distributed simulation, and chaos engineering help improve coverage.</p>
</section>
<section id="concept-drift-detection" class="level3" data-number="14.6.11">
<h3 data-number="14.6.11" class="anchored" data-anchor-id="concept-drift-detection"><span class="header-section-number">14.6.11</span> Concept Drift Detection</h3>
<p>With limited monitoring data from each remote device, detecting changes in the input data over time is much harder. Drift can lead to degraded model performance. Lightweight methods are needed to identify when retraining is necessary. A model predicting power grid loads shows declining performance as usage patterns change over time. With only local device data, this trend is difficult to spot.</p>
</section>
</section>
<section id="traditional-mlops-vs.-embedded-mlops" class="level2" data-number="14.7">
<h2 data-number="14.7" class="anchored" data-anchor-id="traditional-mlops-vs.-embedded-mlops"><span class="header-section-number">14.7</span> Traditional MLOps vs.&nbsp;Embedded MLOps</h2>
<p>In traditional MLOps, ML models are typically deployed in cloud-based or server environments, where resources like computing power and memory are abundant. These environments facilitate the smooth operation of complex models that require significant computational resources. For instance, a cloud-based image recognition model might be used by a social media platform to tag photos with relevant labels automatically. In this case, the model can leverage the extensive resources available in the cloud to process vast data efficiently.</p>
<p>On the other hand, embedded MLOps involves deploying ML models on embedded systems, specialized computing systems designed to perform specific functions within larger systems. Embedded systems are typically characterized by their limited computational resources and power. For example, a ML model might be embedded in a smart thermostat to optimize heating and cooling based on the user’s preferences and habits. In this case, the model must be optimized to run efficiently on the thermostat’s limited hardware, without compromising its performance or accuracy.</p>
<p>The key difference between traditional and embedded MLOps lies in the resource constraints of embedded systems. While traditional MLOps can leverage abundant cloud or server resources, embedded MLOps must contend with the hardware limitations on which the model is deployed. This requires careful optimization and fine-tuning of the model to ensure it can deliver accurate and valuable insights within the constraints of the embedded system.</p>
<p>Furthermore, embedded MLOps must consider the unique challenges posed by integrating ML models with other components of the embedded system. For example, the model must be compatible with the system’s software and hardware and must be able to interface seamlessly with other components, such as sensors or actuators. This requires a deep understanding of both ML and embedded systems, as well as close collaboration between data scientists, engineers, and other stakeholders.</p>
<p>So, while traditional MLOps and embedded MLOps share the common goal of deploying and maintaining ML models in production environments, the unique challenges posed by embedded systems require a specialized approach. Embedded MLOps must carefully balance the need for model accuracy and performance with the constraints of the hardware on which the model is deployed. This requires a deep understanding of both ML and embedded systems, as well as close collaboration between various stakeholders to ensure the successful integration of ML models into embedded systems.</p>
<p>This time we will group the subtopics under broader categories to streamline the structure of our thought process on MLOps. This structure will help you understand how different aspects of MLOps are interconnected and why each is important for the efficient operation of ML systems as we discuss the challenges in the context of embedded systems.</p>
<ul>
<li>Model Lifecycle Management
<ul>
<li>Data Management: Handling data ingestion, validation, and version control.</li>
<li>Model Training: Techniques and practices for effective and scalable model training.</li>
<li>Model Evaluation: Strategies for testing and validating model performance.</li>
<li>Model Deployment: Approaches for deploying models into production environments.</li>
</ul></li>
<li>Development and Operations Integration
<ul>
<li>CI/CD Pipelines: Integrating ML models into continuous integration and continuous deployment pipelines.</li>
<li>Infrastructure Management: Setting up and maintaining the infrastructure required for training and deploying models.</li>
<li>Communication &amp; Collaboration: Ensuring smooth communication and collaboration practices between data scientists, ML engineers, and operations teams.</li>
</ul></li>
<li>Operational Excellence
<ul>
<li>Monitoring: Techniques for monitoring model performance, data drift, and operational health.</li>
<li>Governance: Implementing policies for model auditability, compliance, and ethical considerations.</li>
</ul></li>
</ul>
<section id="model-lifecycle-management" class="level3" data-number="14.7.1">
<h3 data-number="14.7.1" class="anchored" data-anchor-id="model-lifecycle-management"><span class="header-section-number">14.7.1</span> Model Lifecycle Management</h3>
<section id="data-management-1" class="level4">
<h4 class="anchored" data-anchor-id="data-management-1">Data Management</h4>
<p>In traditional centralized MLOps, data is aggregated into large datasets and data lakes, then processed on cloud or on-prem servers. However, embedded MLOps relies on decentralized data from local on-device sensors. Devices collect smaller batches of incremental data, often noisy and unstructured. With connectivity constraints, this data cannot always be instantly transmitted to the cloud and needs to be intelligently cached and processed at the edge.</p>
<p>Embedded devices can only preprocess and clean data minimally before transmission due to limited on-device compute. Early filtering and processing occurs at edge gateways to reduce transmission loads. While leveraging cloud storage, more processing and storage happens at the edge to account for intermittent connectivity. Devices identify and transmit only the most critical subsets of data to the cloud.</p>
<p>Labeling also faces challenges without centralized data access, requiring more automated techniques like federated learning where devices collaboratively label peers’ data. With personal edge devices, data privacy and regulations are critical concerns. Data collection, transmission and storage must be secure and compliant.</p>
<p>For instance, a smartwatch may collect step count, heart rate, GPS coordinates throughout the day. This data is cached locally and transmitted to an edge gateway when WiFi is available. The gateway processes and filters data before syncing relevant subsets with the cloud platform to retrain models.</p>
</section>
<section id="model-training-1" class="level4">
<h4 class="anchored" data-anchor-id="model-training-1">Model Training</h4>
<p>In traditional centralized MLOps, models are trained using abundant data via deep learning on high-powered cloud GPU servers. However, embedded MLOps faces severe constraints on model complexity, data availability and compute resources for training.</p>
<p>The volume of aggregated data is much lower, often requiring techniques like federated learning across devices to create training sets. The specialized nature of edge data also limits public datasets for pre-training. With privacy concerns, data samples need to be tightly controlled and anonymized where possible.</p>
<p>Furthermore, the models themselves need to use simplified architectures optimized for low-power edge hardware. There is no access to high-end GPUs for intensive deep learning given the compute limitations. Training leverages lower-powered edge servers and clusters with distributed approaches to spread load.</p>
<p>To mitigate data scarcity and irregularity, strategies like transfer learning become essential (see Figure 14.5). Models can pre-train on large public datasets, then fine-tune the training on limited domain-specific edge data. Even incremental on-device learning to customize models helps overcome the decentralized nature of embedded data. The lack of broad labeled data also motivates semi-supervised techniques.</p>
<p><a href="#fig-transfer-learning-mlops" class="quarto-xref">Figure&nbsp;<span>14.4</span></a> illustrates the concept of transfer learning in model training within an MLOps framework. It showcases a neural network where the initial layers (W_{A1} to W_{A4}), which are responsible for general feature extraction, are frozen (indicated by the green dashed line), meaning their weights are not updated during training. This reuse of pre-trained layers accelerates learning by utilizing knowledge gained from previous tasks. The latter layers (W_{A5} to W_{A7}), depicted beyond the blue dashed line, are fine-tuned for the specific task at hand, focusing on task-specific feature learning. This approach allows the model to adapt to the new task using fewer resources and potentially achieve higher performance on specialized tasks by reusing the general features learned from a broader dataset.</p>
<div id="fig-transfer-learning-mlops" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-transfer-learning-mlops-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/transfer_learning.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-transfer-learning-mlops-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;14.4: Transfer learning in MLOps. Credit: HarvardX.
</figcaption>
</figure>
</div>
<p>For example, a smart home assistant may pre-train an audio recognition model on public YouTube clips which helps bootstrap with general knowledge. It then transfer learns on a small sample of home data to classify customized appliances and events, specializing the model. The model distills down into a lightweight neural network optimized for microphone-enabled devices across the home.</p>
<p>So embedded MLOps faces acute challenges in constructing training datasets, designing efficient models, and distributing compute for model development compared to traditional settings. Careful adaptation such as transfer learning and distributed training is required to train models given the embedded constraints.</p>
</section>
<section id="model-evaluation-1" class="level4">
<h4 class="anchored" data-anchor-id="model-evaluation-1">Model Evaluation</h4>
<p>In traditional centralized MLOps, models are evaluated primarily on accuracy metrics using holdout test datasets. However, embedded MLOps requires more holistic evaluation accounting for system constraints beyond just accuracy.</p>
<p>Models need to be tested early and often on real deployed edge hardware covering diverse configurations. In addition to accuracy, factors like latency, CPU usage, memory footprint and power consumption are critical evaluation criteria. Models are selected based on tradeoffs between these metrics to meet edge device constraints.</p>
<p>Data drift must also be monitored - where models trained on cloud data degrade in accuracy over time on local edge data. Embedded data often has more variability than centralized training sets. Evaluating models across diverse operational edge data samples is key. But sometimes getting the data for monitoring the drift can be challenging if these devices are in the wild and communication is a barrier.</p>
<p>Ongoing monitoring provides visibility into real-world performance post-deployment, revealing bottlenecks not caught during testing. For instance, a smart camera model update may be canary tested on 100 cameras first and rolled back if degraded accuracy is observed before expanding to all 5000 cameras.</p>
</section>
<section id="model-deployment-1" class="level4">
<h4 class="anchored" data-anchor-id="model-deployment-1">Model Deployment</h4>
<p>In traditional MLOps, new model versions are directly deployed onto servers via API endpoints. However, embedded devices require optimized delivery mechanisms to receive updated models. Over-the-air (OTA) updates provide a standardized approach to wirelessly distribute new software or firmware releases to embedded devices. Rather than direct API access, OTA packages allow remotely deploying models and dependencies as pre-built bundles. As an alternative, <a href="@sec-fl">federated learning</a> allows model updates without direct access to raw training data. This decentralized approach has potential for continuous model improvement, but currently lacks robust MLOps platforms.</p>
<p>For deeply embedded devices lacking connectivity, model delivery relies on physical interfaces like USB or UART serial connections. The model packaging still follows similar principles to OTA updates, but the deployment mechanism is tailored for the capabilities of the edge hardware. Moreover, specialized OTA protocols optimized for IoT networks are often used rather than standard WiFi or Bluetooth protocols. Key factors include efficiency, reliability, security, and telemetry like progress tracking. Solutions like <a href="https://mender.io/">Mender.io</a> provide embedded-focused OTA services handling differential updates across device fleets.</p>
<p><a href="#fig-model-lifecycle" class="quarto-xref">Figure&nbsp;<span>14.5</span></a> presents an overview of Model Lifecycle Management in an MLOps context, illustrating the flow from development (top left) to deployment and monitoring (bottom right). The process begins with ML Development, where code and configurations are version-controlled. Data and model management are central to the process, involving datasets and feature repositories. Continuous training, model conversion, and model registry are key stages in the operationalization of training. Model deployment includes serving the model and managing serving logs. Alerting mechanisms are in place to flag issues, which feed into continuous monitoring to ensure model performance and reliability over time. This integrated approach ensures that models are not only developed but also maintained effectively throughout their lifecycle.</p>
<div id="fig-model-lifecycle" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-model-lifecycle-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/mlops_flow.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-model-lifecycle-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;14.5: Model lifecycle management. Credit: HarvardX.
</figcaption>
</figure>
</div>
</section>
</section>
<section id="development-and-operations-integration" class="level3" data-number="14.7.2">
<h3 data-number="14.7.2" class="anchored" data-anchor-id="development-and-operations-integration"><span class="header-section-number">14.7.2</span> Development and Operations Integration</h3>
<section id="cicd-pipelines-1" class="level4">
<h4 class="anchored" data-anchor-id="cicd-pipelines-1">CI/CD Pipelines</h4>
<p>In traditional MLOps, robust CI/CD infrastructure like Jenkins and Kubernetes enables automating pipelines for large-scale model deployment. However, embedded MLOps lacks this centralized infrastructure and needs more tailored CI/CD workflows for edge devices.</p>
<p>Building CI/CD pipelines has to account for a fragmented landscape of diverse hardware, firmware versions and connectivity constraints. There is no standard platform on which to orchestrate pipelines and tooling support is more limited.</p>
<p>Testing needs to cover this wide spectrum of target embedded devices early, which is difficult without centralized access. Companies must invest significant effort into acquiring and managing test infrastructure across the heterogeneous embedded ecosystem.</p>
<p>Over-the-air updates require setting up specialized servers to securely distribute model bundles to devices in the field. Rollout and rollback procedures must be carefully tailored for particular device families.</p>
<p>With traditional CI/CD tools less applicable, embedded MLOps relies more on custom scripts and integration. Companies take varied approaches from open source frameworks to fully in-house solutions. Tight integration between developers, edge engineers and end customers establishes trusted release processes.</p>
<p>Therefore, embedded MLOps can’t leverage centralized cloud infrastructure for CI/CD. Companies cobble together custom pipelines, testing infrastructure and OTA delivery to deploy models across fragmented and disconnected edge systems.</p>
</section>
<section id="infrastructure-management-1" class="level4">
<h4 class="anchored" data-anchor-id="infrastructure-management-1">Infrastructure Management</h4>
<p>In traditional centralized MLOps, infrastructure entails provisioning cloud servers, GPUs and high-bandwidth networks for intensive workloads like model training and serving predictions at scale. However, embedded MLOps requires more heterogeneous infrastructure spanning edge devices, gateways, and cloud.</p>
<p>Edge devices like sensors capture and preprocess data locally before intermittent transmission to avoid overloading networks. Gateways aggregate and process data from devices before sending select subsets to the cloud for training and analysis. The cloud provides centralized management and supplemental compute.</p>
<p>This infrastructure needs tight integration, balancing processing and communication loads. Network bandwidth is limited, requiring careful data filtering and compression. Edge compute capabilities are modest compared to the cloud, imposing optimization constraints.</p>
<p>Managing secure OTA updates across large device fleets presents challenges at the edge. Rollouts must be incremental and rollback-ready for quick mitigation. Updating edge infrastructure requires coordination given decentralized environments.</p>
<p>For example, an industrial plant may perform basic signal processing on sensors before sending data to an on-prem gateway. The gateway handles data aggregation, infrastructure monitoring, and OTA updates. Only curated data is transmitted to the cloud for advanced analytics and model retraining.</p>
<p>In summary, embedded MLOps requires holistic management of distributed infrastructure spanning constrained edge, gateways, and centralized cloud. Workloads are balanced across tiers while accounting for connectivity, compute and security challenges.</p>
</section>
<section id="communication-collaboration-1" class="level4">
<h4 class="anchored" data-anchor-id="communication-collaboration-1">Communication &amp; Collaboration</h4>
<p>In traditional MLOps, collaboration tends to be centered around data scientists, ML engineers and DevOps teams. But embedded MLOps requires tighter cross-functional coordination between additional roles to address system constraints.</p>
<p>Edge engineers optimize model architectures for target hardware environments. They provide feedback to data scientists during development so models fit device capabilities early on. Similarly, product teams define operational requirements informed by end-user contexts.</p>
<p>With more stakeholders across the embedded ecosystem, communication channels must facilitate information sharing between centralized and remote teams. Issue tracking and project management ensures alignment.</p>
<p>Collaborative tools optimize models for particular devices. Data scientists can log issues replicated from field devices so models specialize on niche data. Remote device access aids debugging and data collection.</p>
<p>For example, data scientists may collaborate with field teams managing fleets of wind turbines to retrieve operational data samples. This data is used to specialize models detecting anomalies specific to that turbine class. Model updates are first tested in simulations then reviewed by engineers before field deployment.</p>
<p>In essence, embedded MLOps mandates continuous coordination between data scientists, engineers, end customers and other stakeholders throughout the ML lifecycle. Only through close collaboration can models be tailored and optimized for targeted edge devices.</p>
</section>
</section>
<section id="operational-excellence" class="level3" data-number="14.7.3">
<h3 data-number="14.7.3" class="anchored" data-anchor-id="operational-excellence"><span class="header-section-number">14.7.3</span> Operational Excellence</h3>
<section id="monitoring-1" class="level4">
<h4 class="anchored" data-anchor-id="monitoring-1">Monitoring</h4>
<p>In traditional MLOps, monitoring focuses on tracking model accuracy, performance metrics and data drift centrally. But embedded MLOps must account for decentralized monitoring across diverse edge devices and environments.</p>
<p>Edge devices require optimized data collection to transmit key monitoring metrics without overloading networks. Metrics help assess model performance, data patterns, resource usage and other behaviors on remote devices.</p>
<p>With limited connectivity, more analysis occurs at the edge before aggregating insights centrally. Gateways play a key role in monitoring fleet health and coordinating software updates. Confirmed indicators are eventually propagated to the cloud.</p>
<p>Broad device coverage is challenging but critical. Issues specific to certain device types may arise so monitoring needs to cover the full spectrum. Canary deployments help trial monitoring processes before scaling.</p>
<p>Anomaly detection identifies incidents requiring rolling back models or retraining on new data. But interpreting alerts requires understanding unique device contexts based on input from engineers and customers.</p>
<p>For example, an automaker may monitor autonomous vehicles for indicators of model degradation using caching, aggregation and real-time streams. Engineers assess when identified anomalies warrant OTA updates to improve models based on factors like location and vehicle age.</p>
<p>Embedded MLOps monitoring provides observability into model and system performance across decentralized edge environments. Careful data collection, analysis and collaboration delivers meaningful insights to maintain reliability.</p>
</section>
<section id="governance-1" class="level4">
<h4 class="anchored" data-anchor-id="governance-1">Governance</h4>
<p>In traditional MLOps, governance focuses on model explainability, fairness and compliance for centralized systems. But embedded MLOps must also address device-level governance challenges around data privacy, security and safety.</p>
<p>With sensors collecting personal and sensitive data, local data governance on devices is critical. Data access controls, anonymization, and encrypted caching help address privacy risks and compliance like HIPAA and GDPR. Updates must maintain security patches and settings.</p>
<p>Safety governance considers the physical impacts of flawed device behavior. Failures could cause unsafe conditions in vehicles, factories and critical systems. Redundancy, fail-safes and warning systems help mitigate risks.</p>
<p>Traditional governance like bias monitoring and model explainability remains imperative but is harder to implement for embedded AI. Peeking into black-box models on low-power devices poses challenges.</p>
<p>For example, a medical device may scrub personal data on-device before transmission. Strict data governance protocols approve model updates. Model explainability is limited but the focus is detecting anomalous behavior. Backup systems prevent failures.</p>
<p>In essence, embedded MLOps governance must span the dimensions of privacy, security, safety, transparency, and ethics. Specialized techniques and team collaboration are needed to help establish trust and accountability within decentralized environments.</p>
</section>
</section>
<section id="comparison" class="level3" data-number="14.7.4">
<h3 data-number="14.7.4" class="anchored" data-anchor-id="comparison"><span class="header-section-number">14.7.4</span> Comparison</h3>
<p>Here is a comparison table highlighting similarities and differences between Traditional MLOps and Embedded MLOps based on all the things we have learned thus far:</p>
<table class="table">
<colgroup>
<col style="width: 33%">
<col style="width: 33%">
<col style="width: 33%">
</colgroup>
<thead>
<tr class="header">
<th>Area</th>
<th>Traditional MLOps</th>
<th>Embedded MLOps</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Data Management</td>
<td>Large datasets, data lakes, feature stores</td>
<td>On-device data capture, edge caching and processing</td>
</tr>
<tr class="even">
<td>Model Development</td>
<td>Leverage deep learning, complex neural nets, GPU training</td>
<td>Constraints on model complexity, need for optimization</td>
</tr>
<tr class="odd">
<td>Deployment</td>
<td>Server clusters, cloud deployment, low latency at scale</td>
<td>OTA deployment to devices, intermittent connectivity</td>
</tr>
<tr class="even">
<td>Monitoring</td>
<td>Dashboards, logs, alerts for cloud model performance</td>
<td>On-device monitoring of predictions, resource usage</td>
</tr>
<tr class="odd">
<td>Retraining</td>
<td>Retrain models on new data</td>
<td>Federated learning from devices, edge retraining</td>
</tr>
<tr class="even">
<td>Infrastructure</td>
<td>Dynamic cloud infrastructure</td>
<td>Heterogeneous edge/cloud infrastructure</td>
</tr>
<tr class="odd">
<td>Collaboration</td>
<td>Shared experiment tracking and model registry</td>
<td>Collaboration for device-specific optimization</td>
</tr>
</tbody>
</table>
<p>So while Embedded MLOps shares foundational MLOps principles, it faces unique constraints to tailor workflows and infrastructure specifically for resource-constrained edge devices.</p>
</section>
</section>
<section id="commercial-offerings" class="level2" data-number="14.8">
<h2 data-number="14.8" class="anchored" data-anchor-id="commercial-offerings"><span class="header-section-number">14.8</span> Commercial Offerings</h2>
<p>While no replacement for understanding the principles, there are an increasing number of commercial offerings that help ease the burden of building ML pipelines and integrating tools together to build, test, deploy, and monitor ML models in production.</p>
<section id="traditional-mlops" class="level3" data-number="14.8.1">
<h3 data-number="14.8.1" class="anchored" data-anchor-id="traditional-mlops"><span class="header-section-number">14.8.1</span> Traditional MLOps</h3>
<p>Google, Microsoft, and Amazon all offer their own version of managed ML services. These include services that manage model training and experimentation, model hosting and scaling, and monitoring. These offerings are available via an API and client SDKs, as well as through web UIs. While it is possible to build your own end-to-end MLOps solutions using pieces from each, the greatest ease of use benefits come by staying within a single provider ecosystem to take advantage of interservice integrations.</p>
<p>I will provide a quick overview of the services offered that fit into each part of the MLOps life cycle described above, providing examples of offerings from different providers. The space is moving very quickly; new companies and products are entering the scene very rapidly, and these are not meant to serve as an endorsement of a particular company’s offering.</p>
<section id="data-management-2" class="level4">
<h4 class="anchored" data-anchor-id="data-management-2">Data Management</h4>
<p>Data storage and versioning are table stakes for any commercial offering and most take advantage of existing general purpose storage solutions such as S3. Others use more specialized options such as a git-based storage (Example: <a href="https://huggingface.co/datasets">Hugging Face’s Dataset Hub</a> This is an area where providers make it easy to support their competitors’ data storage options, as they don’t want this to be a barrier for adoptions of the rest of their MLOps services. For example, Vertex AI’s training pipeline seamlessly supports datasets stored in S3, Google Cloud Buckets, or Hugging Face’s Dataset Hub.</p>
</section>
<section id="model-training-2" class="level4">
<h4 class="anchored" data-anchor-id="model-training-2">Model Training</h4>
<p>Managed training services are where cloud providers really shine, as they provide on demand access to hardware that is out of reach for most smaller companies. They bill only for hardware during training time, and this puts GPU accelerated training within reach of even the smallest developer teams. The level of control that developers have over their training workflow can vary widely depending on their needs. Some providers have services that provide little more than access to the resources and rely on the developer to manage the training loop, logging, and model storage themselves. Other services are as simple as pointing to a base model and a labeled data set to kick off a fully managed fine tuning job (example: <a href="https://cloud.google.com/vertex-ai/docs/generative-ai/models/tune-models">Vertex AI Fine Tuning</a>).</p>
<p>A word of warning: As of 2023, GPU hardware demand well exceeds the supply and as a result cloud providers are rationing access to their GPUs, and in some data center regions may be unavailable or require long term contracts.</p>
</section>
<section id="model-evaluation-2" class="level4">
<h4 class="anchored" data-anchor-id="model-evaluation-2">Model Evaluation</h4>
<p>Model evaluation tasks typically involve monitoring the accuracy, latency, and resource usage of models in both the testing and production phases. Unlike in embedded systems, ML models deployed to the cloud benefit from constant internet connectivity and virtually unlimited logging capacities. As a result it is often feasible to capture and log every request and response. This makes replaying or generating synthetic requests to enable comparison across different models and versions tractable.</p>
<p>Some providers also offer services that automate the experiment tracking of modifying model hyperparameters. They track the runs, performance, and generated artifacts from these model training runs. Example: <a href="https://wandb.ai/">WeightsAndBiases</a></p>
</section>
<section id="model-deployment-2" class="level4">
<h4 class="anchored" data-anchor-id="model-deployment-2">Model Deployment</h4>
<p>Each provider typically has a service referred to as a “model registry” where training models are stored and accessed. Often these registries may also provide access to base models that are either open source or provided by larger technology companies (or in some cases like <a href="https://ai.meta.com/llama/">LLAMA</a>, both!). These model registries are a common place to compare all of the models and their versions together to allow easy decision making on which to pick for a given use case. Example: <a href="https://cloud.google.com/vertex-ai/docs/model-registry/introduction">Vertex AI’s model registry</a></p>
<p>From the model registry it is quick and simple to deploy a model to an inference endpoint, which handles the resource provisioning, model weight downloading, and hosting of a given model. These services typically give access to the model via a REST API where inference requests can be sent. Depending on the model type, the specific required resources can be configured, such as which type of GPU accelerator may be needed to hit the desired performance. Some providers may also offer serverless inference, or batch inference options that do not need a persistent endpoint for accessing the model. Example: <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/deploy-model.html">AWS SageMaker Inference</a></p>
</section>
</section>
<section id="embedded-mlops" class="level3" data-number="14.8.2">
<h3 data-number="14.8.2" class="anchored" data-anchor-id="embedded-mlops"><span class="header-section-number">14.8.2</span> Embedded MLOps</h3>
<p>Despite the proliferation of new ML Ops tools in response to the increase in demand, the challenges described earlier have constrained the availability of such tools in embedded systems environments. More recently, new tools such as Edge Impulse <span class="citation" data-cites="janapa2023edge">(<a href="../../references.html#ref-janapa2023edge" role="doc-biblioref">Janapa Reddi et al. 2023</a>)</span> have made the development process somewhat easier, as we’ll describe below.</p>
<section id="edge-impulse" class="level4">
<h4 class="anchored" data-anchor-id="edge-impulse">Edge Impulse</h4>
<p><a href="https://edgeimpulse.com/">Edge Impulse</a> is an end-to-end development platform for creating and deploying machine learning models onto edge devices such as microcontrollers and small processors. It aims to make embedded machine learning more accessible to software developers through its easy-to-use web interface and integrated tools for data collection, model development, optimization and deployment. It’s key capabilities include:</p>
<ul>
<li>Intuitive drag and drop workflow for building ML models without coding required</li>
<li>Tools for acquiring, labeling, visualizing and preprocessing data from sensors</li>
<li>Choice of model architectures including neural networks and unsupervised learning</li>
<li>Model optimization techniques to balance performance metrics and hardware constraints<br>
</li>
<li>Seamless deployment onto edge devices through compilation, SDKs and benchmarks</li>
<li>Collaboration features for teams and integration with other platforms</li>
</ul>
<p>With Edge Impulse, developers with limited data science expertise can develop specialized ML models that run efficiently within small computing environments. It provides a comprehensive solution for creating embedded intelligence and taking machine learning to the edge.</p>
<section id="user-interface" class="level5">
<h5 class="anchored" data-anchor-id="user-interface">User Interface</h5>
<p>Edge Impulse was designed with seven key principles in mind: accessibility, end-to-end capabilities, a data-centric approach, iterativeness, extensibility, team orientation, and community support. The intuitive user interface, shown in <a href="#fig-edge-impulse-ui" class="quarto-xref">Figure&nbsp;<span>14.6</span></a>, guides developers at all experience levels through uploading data, selecting a model architecture, training the model, and deploying it across relevant hardware platforms. It should be noted that, like any tool, Edge Impulse is intended to assist with, not replace, foundational considerations such as determining if ML is an appropriate solution or acquiring the requisite domain expertise for a given application.</p>
<div id="fig-edge-impulse-ui" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-edge-impulse-ui-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/edge_impulse_dashboard.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-edge-impulse-ui-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;14.6: Screenshot of Edge Impulse user interface for building workflows from input data to output features.
</figcaption>
</figure>
</div>
<p>What makes Edge Impulse notable is its comprehensive yet intuitive end-to-end workflow. Developers start by uploading their data, either through file upload or command line interface (CLI) tools, after which they can examine raw samples and visualize the distribution of data in the training and test splits. Next, users can pick from a variety of preprocessing “blocks” to facilitate digital signal processing (DSP). While default parameter values are provided, users have the option to customize the parameters as needed, with considerations around memory and latency displayed. Users can easily choose their neural network architecture - without any code needed.</p>
<p>Thanks to the platform’s visual editor, users can customize the components of the architecture and the specific parameters, all while ensuring that the model is still trainable. Users can also leverage unsupervised learning algorithms, such as K-means clustering and Gaussian mixture models (GMM).</p>
</section>
<section id="optimizations" class="level5">
<h5 class="anchored" data-anchor-id="optimizations">Optimizations</h5>
<p>To accommodate the resource constraints of TinyML applications, Edge Impulse provides a confusion matrix summarizing key performance metrics including per-class accuracy and F1 scores. The platform elucidates the tradeoffs between model performance, size, and latency using simulations in <a href="https://renode.io/">Renode</a> and device-specific benchmarking. For streaming data use cases, a performance calibration tool leverages a genetic algorithm to find ideal post-processing configurations balancing false acceptance and false rejection rates. To optimize models, techniques like quantization, code optimization, and device-specific optimization are available. For deployment, models can be compiled in appropriate formats for target edge devices. Native firmware SDKs also enable direct data collection on devices.</p>
<p>In addition to streamlining development, Edge Impulse scales the modeling process itself. A key capability is the <a href="https://docs.edgeimpulse.com/docs/edge-impulse-studio/eon-tuner">EON Tuner</a>, an automated machine learning (AutoML) tool that assists users in hyperparameter tuning based on system constraints. It runs a random search to quickly generate configurations for digital signal processing and training steps. The resulting models are displayed for the user to select based on relevant performance, memory, and latency metrics. For data, active learning facilitates training on a small labeled subset then manually or automatically labeling new samples based on proximity to existing classes. This expands data efficiency.</p>
</section>
<section id="use-cases" class="level5">
<h5 class="anchored" data-anchor-id="use-cases">Use Cases</h5>
<p>Beyond the accessibility of the platform itself, the Edge Impulse team has expanded the knowledge base of the embedded ML ecosystem. The platform lends itself to academic environments, having been used in online courses and on-site workshops globally. Numerous case studies featuring industry and research use cases have been published, most notably <a href="https://ouraring.com/">Oura Ring</a>, which uses ML to identify sleep patterns. The team has made repositories open source on GitHub, facilitating community growth. Users can also make projects public to share techniques and download libraries to share via Apache. Organization-level access enables collaboration on workflows.</p>
<p>Overall, Edge Impulse is uniquely comprehensive and integrateable for developer workflows. Larger platforms like Google and Microsoft focus more on cloud versus embedded systems. TinyMLOps frameworks such as Neuton AI and Latent AI offer some functionality but lack Edge Impulse’s end-to-end capabilities. TensorFlow Lite Micro is the standard inference engine due to flexibility, open source status, and TensorFlow integration but uses more memory and storage than Edge Impulse’s EON Compiler. Other platforms are outdated, academic-focused, or less versatile. In summary, Edge Impulse aims to streamline and scale embedded ML through an accessible, automated platform.</p>
</section>
</section>
<section id="limitations" class="level4">
<h4 class="anchored" data-anchor-id="limitations">Limitations</h4>
<p>While Edge Impulse provides an accessible pipeline for embedded ML, there are still important limitations and risks to consider. A key challenge is data quality and availability - the models are only as good as the data used to train them. Users must have sufficient labeled samples that capture the breadth of expected operating conditions and failure modes. Labeled anomalies and outliers are critical yet time-consuming to collect and identify. Insufficient or biased data leads to poor model performance regardless of the tool’s capabilities.</p>
<p>There are also inherent challenges in deploying to low-powered devices. Optimized models may still be too resource intensive for ultra-low power MCUs. Striking the right balance of compression versus accuracy takes some experimentation. The tool simplifies but doesn’t eliminate the need for foundational ML and signal processing expertise. Embedded environments also constrain debugging and interpretability compared to the cloud.</p>
<p>While impressive results are achievable, users shouldn’t view Edge Impulse as a “Push Button ML” solution. Careful project scoping, data collection, model evaluation and testing is still essential. As with any development tool, reasonable expectations and diligence in application are advised. But for developers willing to invest the requisite data science and engineering effort, Edge Impulse can accelerate embedded ML prototyping and deployment.</p>
</section>
</section>
</section>
<section id="case-studies" class="level2" data-number="14.9">
<h2 data-number="14.9" class="anchored" data-anchor-id="case-studies"><span class="header-section-number">14.9</span> Case Studies</h2>
<section id="oura-ring" class="level3" data-number="14.9.1">
<h3 data-number="14.9.1" class="anchored" data-anchor-id="oura-ring"><span class="header-section-number">14.9.1</span> Oura Ring</h3>
<p>The <a href="https://ouraring.com/">Oura Ring</a> is a wearable that, when placed on the user’s finger, can measure activity, sleep, and recovery. Using sensors to track physiological metrics, the device uses embedded ML to predict the stages of sleep. To establish a baseline of legitimacy in the industry, Oura conducted a correlation experiment to evaluate the success of the device in predicting sleep stages against a baseline study, resulting in a solid 62% correlation compared to the baseline of 82-83%. Thus, the team set out to determine how they could improve their performance even further.</p>
<p>The first challenge was to obtain better data, in terms of both quantity and quality. They could host a larger study to get a more comprehensive data set, but the data would be noisy and at such a large scale that it would be difficult to aggregate, scrub, and analyze. This is where Edge Impulse comes in.</p>
<p>Oura was able to host a massive sleep study of 100 men and women between the ages of 15 and 73 across three continents (Asia, Europe, North America). In addition to wearing the Oura Ring, participants were responsible for undergoing the industry standard PSG testing, which provided a “label” for this data set. With 440 nights of sleep from 106 participants, the data set totaled 3,444 hours in length across Ring and PSG data. With Edge Impulse, Oura was able to easily upload and consolidate the data from different sources into a private S3 bucket. They were also able to set up a Data Pipeline to merge data samples into individual files, as well as preprocess the data without having to conduct manual scrubbing.</p>
<p>Because of the time saved on data processing thanks to Edge Impulse, the Oura team was able to focus on the key drivers of their prediction. In fact, they ended up only extracting three types of sensor data: heart rate, motion, and body temperature. After partitioning the data using five-fold cross validation and classifying sleep stage, the team was able to achieve a correlation of 79% - just a few percentage points off the standard. They were able to readily deploy two types of models for sleep detection: one simplified using just the ring’s accelerometer and one more comprehensive leveraging Autonomic Nervous System (ANS)-mediated peripheral signals and circadian features. With Edge Impulse, they plan to conduct further analyses of different activity types and leverage the scalability of the platform to continue to experiment with different sources of data and subsets of features extracted.</p>
<p>While most ML research focuses on the model-dominant steps such as training and finetuning, this case study underscores the importance of a holistic approach to ML Ops, where even the initial steps of data aggregation and preprocessing have a fundamental impact on successful outcomes.</p>
</section>
<section id="clinaiops" class="level3" data-number="14.9.2">
<h3 data-number="14.9.2" class="anchored" data-anchor-id="clinaiops"><span class="header-section-number">14.9.2</span> ClinAIOps</h3>
<p>Let’s take a look at MLOps in the context of medical health monitoring to better understand how MLOps “matures” in the context of a real world deployment. Specifically, let’s consider continuous therapeutic monitoring (CTM) enabled by wearable devices and sensors , providing the opportunity for more frequent and personalized adjustments to treatments by capturing detailed physiological data from patients.</p>
<p>Wearable ML enabled sensors enable continuous physiological and activity monitoring outside of clinics, opening up possibilities for timely, data-driven adjustments of therapies. For example, wearable insulin biosensors <span class="citation" data-cites="psoma2023wearable">(<a href="../../references.html#ref-psoma2023wearable" role="doc-biblioref">Psoma and Kanthou 2023</a>)</span> and wrist-worn ECG sensors for glucose monitoring <span class="citation" data-cites="li2021noninvasive">(<a href="../../references.html#ref-li2021noninvasive" role="doc-biblioref">Li et al. 2021</a>)</span> can automate insulin dosing for diabetes, wrist-worn ECG and PPG sensors can adjust blood thinners based on atrial fibrillation patterns <span class="citation" data-cites="attia2018noninvasive guo2019mobile">(<a href="../../references.html#ref-attia2018noninvasive" role="doc-biblioref">Attia et al. 2018</a>; <a href="../../references.html#ref-guo2019mobile" role="doc-biblioref">Guo et al. 2019</a>)</span>, and accelerometers tracking gait can trigger preventative care for declining mobility in the elderly <span class="citation" data-cites="liu2022monitoring">(<a href="../../references.html#ref-liu2022monitoring" role="doc-biblioref">Liu et al. 2022</a>)</span>. The variety of signals that can now be captured passively and continuously allows therapy titration and optimization tailored to each patient’s changing needs. By closing the loop between physiological sensing and therapeutic response with TinyML and ondevice learning, wearables are poised to transform many areas of personalized medicine.</p>
<p>ML holds great promise in analyzing CTM data to provide data-driven recommendations for therapy adjustments. But simply deploying AI models in silos, without integrating them properly into clinical workflows and decision making, can lead to poor adoption or suboptimal outcomes. In other words, thinking about MLOps alone is simply insufficient to make them useful in practice. What is needed are frameworks to seamlessly incorporate AI and CTM into real-world clinical practice as this study shows.</p>
<p>This case study analyzes “ClinAIOps” as a model for embedded ML operations in complex clinical environments <span class="citation" data-cites="chen2023framework">(<a href="../../references.html#ref-chen2023framework" role="doc-biblioref">Chen et al. 2023</a>)</span>. We provide an overview of the framework and why it’s needed, walk through an application example, and discuss key implementation challenges related to model monitoring, workflow integration, and stakeholder incentives. Analyzing real-world examples like ClinAIOps illuminates crucial principles and best practices needed for reliable and effective AI Ops across many domains.</p>
<p>Traditional MLOps frameworks are insufficient for integrating continuous therapeutic monitoring (CTM) and AI in clinical settings for a few key reasons:</p>
<ul>
<li><p>MLOps focuses on the ML model lifecycle - training, deployment, monitoring. But healthcare involves coordinating multiple human stakeholders - patients, clinicians - not just models.</p></li>
<li><p>MLOps aims to automate IT system monitoring and management. But optimizing patient health requires personalized care and human oversight, not just automation.</p></li>
<li><p>CTM and healthcare delivery are complex sociotechnical systems with many moving parts. MLOps doesn’t provide a framework for coordinating human and AI decision-making.</p></li>
<li><p>There are ethical considerations regarding healthcare AI that require human judgment, oversight and accountability. MLOps frameworks lack processes for ethical oversight.</p></li>
<li><p>Patient health data is highly sensitive and regulated. MLOps alone doesn’t ensure handling of protected health information to privacy and regulatory standards.</p></li>
<li><p>Clinical validation of AI-guided treatment plans is essential for provider adoption. MLOps doesn’t incorporate domain-specific evaluation of model recommendations.</p></li>
<li><p>Optimizing healthcare metrics like patient outcomes requires aligning stakeholder incentives and workflows, which pure tech-focused MLOps overlooks.</p></li>
</ul>
<p>Thus, effectively integrating AI/ML and CTM in clinical practice requires more than just model and data pipelines, but coordinating complex human-AI collaborative decision making, which ClinAIOps aims to address via its multi-stakeholder feedback loops.</p>
<section id="feedback-loops" class="level4">
<h4 class="anchored" data-anchor-id="feedback-loops">Feedback Loops</h4>
<p>The ClinAIOps framework, shown in <a href="#fig-clinaiops" class="quarto-xref">Figure&nbsp;<span>14.7</span></a>, provides these mechanisms through three feedback loops. The loops are useful for coordinating the insights from continuous physiological monitoring, clinician expertise, and AI guidance via feedback loops, enabling data-driven precision medicine while maintaining human accountability. ClinAIOps provides a model for effective human-AI symbiosis in healthcare: the patient is at the center, providing health challenges and goals which inform the therapy regimen; the clinician oversees this regimen, giving inputs for adjustments based on continuous monitoring data and health reports from the patient; whereas AI developers play a crucial role by creating systems that generate alerts for therapy updates, which are then vetted by the clinician.</p>
<p>These feedback loops which we will discuss below help maintain clinician responsibility and control over treatment plans, by reviewing AI suggestions before they impact patients. They help dynamically customize AI model behavior and outputs to each patient’s changing health status. They help improve model accuracy and clinical utility over time by learning from clinician and patient responses. They facilitate shared decision-making and personalized care during patient-clinician interactions. They enable rapid optimization of therapies based on frequent patient data that clinicians cannot manually analyze.</p>
<div id="fig-clinaiops" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-clinaiops-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/clinaiops.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-clinaiops-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;14.7: ClinAIOps cycle. Credit: <span class="citation" data-cites="chen2023framework">Chen et al. (<a href="../../references.html#ref-chen2023framework" role="doc-biblioref">2023</a>)</span>.
</figcaption>
</figure>
</div>
<section id="patient-ai-loop" class="level5">
<h5 class="anchored" data-anchor-id="patient-ai-loop">Patient-AI Loop</h5>
<p>The patient-AI loop enables frequent therapy optimization driven by continuous physiological monitoring. Patients are prescribed wearables like smartwatches or skin patches to passively collect relevant health signals. For example, a diabetic patient could have a continuous glucose monitor, or a heart disease patient may wear an ECG patch. The patient’s longitudinal health data streams are analyzed by an AI model in context of their electronic medical records - their diagnoses, lab tests, medications, and demographics. The AI model suggests adjustments to the treatment regimen tailored to that individual, like changing a medication dose or administration schedule. Minor adjustments within a pre-approved safe range can be made by the patient independently, while major changes are reviewed by the clinician first. This tight feedback between the patient’s physiology and AI-guided therapy allows data-driven, timely optimizations like automated insulin dosing recommendations based on real-time glucose levels for diabetes patients.</p>
</section>
<section id="clinician-ai-loop" class="level5">
<h5 class="anchored" data-anchor-id="clinician-ai-loop">Clinician-AI Loop</h5>
<p>The clinician-AI loop allows clinical oversight over AI-generated recommendations to ensure safety and accountability. The AI model provides the clinician with treatment recommendations, along with easily reviewed summaries of the relevant patient data the suggestions are based on. For instance, an AI may suggest lowering a hypertension patient’s blood pressure medication dose based on continuously low readings. The clinician can choose to accept, reject, or modify the AI’s proposed prescription changes. This clinician feedback further trains and improves the model. Additionally, the clinician sets the bounds for the types and extents of treatment changes the AI can autonomously recommend to patients. By reviewing AI suggestions, the clinician maintains ultimate treatment authority based on their clinical judgment and accountability. This loop allows them to efficiently oversee patient cases with AI assistance.</p>
</section>
<section id="patient-clinician-loop" class="level5">
<h5 class="anchored" data-anchor-id="patient-clinician-loop">Patient-Clinician Loop</h5>
<p>Instead of routine data collection, the clinician can focus on interpreting high-level data patterns and collaborating with the patient to set health goals and priorities. The AI assistance will also free up clinician time, allowing them to focus more deeply on listening to patients’ stories and concerns. For instance, the clinician may discuss diet and exercise changes with a diabetes patient to improve their glucose control based on their continuous monitoring data. Appointment frequency can also be dynamically adjusted based on patient progress rather than following a fixed calendar. Freed from basic data gathering, the clinician can provide coaching and care customized to each patient informed by their continuous health data. The patient-clinician relationship is made more productive and personalized.</p>
</section>
</section>
<section id="hypertension-example" class="level4">
<h4 class="anchored" data-anchor-id="hypertension-example">Hypertension Example</h4>
<p>Let’s consider an example. According to the Centers for Disease Control and Prevention, nearly half of adults have hypertension (48.1%, 119.9 million). Hypertension can be managed through ClinAIOps with the help of wearable sensors using the following approach:</p>
<section id="data-collection" class="level5">
<h5 class="anchored" data-anchor-id="data-collection">Data Collection</h5>
<p>The data collected would include continuous blood pressure monitoring using a wrist-worn device equipped with photoplethysmography (PPG) and electrocardiography (ECG) sensors to estimate blood pressure <span class="citation" data-cites="zhang2017highly">(<a href="../../references.html#ref-zhang2017highly" role="doc-biblioref">Zhang, Zhou, and Zeng 2017</a>)</span>. The wearable would also track the patient’s physical activity via embedded accelerometers. The patient would log any antihypertensive medications they take, along with the time and dose. Additionally, the patient’s demographic details and medical history from their electronic health record (EHR) would be incorporated. This multimodal real-world data provides valuable context for the AI model to analyze the patient’s blood pressure patterns, activity levels, medication adherence, and responses to therapy.</p>
</section>
<section id="ai-model" class="level5">
<h5 class="anchored" data-anchor-id="ai-model">AI Model</h5>
<p>The on-device AI model would analyze the patient’s continuous blood pressure trends, circadian patterns, physical activity levels, medication adherence behaviors, and other context. It would use ML to predict optimal antihypertensive medication doses and timing to control the individual’s blood pressure. The model would send dosage change recommendations directly to the patient for minor adjustments, or to the reviewing clinician for approval for more significant modifications. By observing clinician feedback on its recommendations, as well as evaluating the resulting blood pressure outcomes in patients, the AI model could be continually retrained and improved to enhance performance. The goal is fully personalized blood pressure management optimized for each patient’s needs and responses.</p>
</section>
<section id="patient-ai-loop-1" class="level5">
<h5 class="anchored" data-anchor-id="patient-ai-loop-1">Patient-AI Loop</h5>
<p>In the Patient-AI loop, the hypertensive patient would receive notifications on their wearable device or tethered smartphone app recommending adjustments to their antihypertensive medications. For minor dose changes within a pre-defined safe range, the patient could independently implement the AI model’s suggested adjustment to their regimen. However, for more significant modifications, the patient would need to obtain clinician approval before changing their dosage. By providing personalized and timely medication recommendations, this automates an element of hypertension self-management for the patient. It can improve their adherence to the regimen as well as treatment outcomes. The patient is empowered to leverage AI insights to better control their blood pressure.</p>
</section>
<section id="clinician-ai-loop-1" class="level5">
<h5 class="anchored" data-anchor-id="clinician-ai-loop-1">Clinician-AI Loop</h5>
<p>In the Clinician-AI loop, the provider would receive summaries of the patient’s continuous blood pressure trends and visualizations of their medication taking patterns and adherence. They review the AI model’s suggested antihypertensive dosage changes and decide whether to approve, reject, or modify the recommendations before they reach the patient. The clinician also specifies the boundaries for how much the AI can independently recommend changing dosages without clinician oversight. If the patient’s blood pressure is trending at dangerous levels, the system alerts the clinician so they can promptly intervene and adjust medications or request an emergency room visit. By keeping the clinician in charge of approving major treatment changes, this loop maintains accountability and safety while allowing the clinician to harness AI insights.</p>
</section>
<section id="patient-clinician-loop-1" class="level5">
<h5 class="anchored" data-anchor-id="patient-clinician-loop-1">Patient-Clinician Loop</h5>
<p>In the Patient-Clinician loop, shown in <a href="#fig-interactive-loop" class="quarto-xref">Figure&nbsp;<span>14.8</span></a>, the in-person visits would focus less on collecting data or basic medication adjustments. Instead, the clinician could interpret high-level trends and patterns in the patient’s continuous monitoring data and have focused discussions about diet, exercise, stress management, and other lifestyle changes to holistically improve their blood pressure control. The frequency of appointments could be dynamically optimized based on the patient’s stability rather than following a fixed calendar. Since the clinician would not need to review all the granular data, they could concentrate on delivering personalized care and recommendations during visits. With continuous monitoring and AI-assisted optimization of medications between visits, the clinician-patient relationship focuses on overall wellness goals and becomes more impactful. This proactive and tailored data-driven approach can help avoid hypertension complications like stroke, heart failure, and other threats to patient health and wellbeing.</p>
<div id="fig-interactive-loop" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-interactive-loop-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/clinaiops_loops.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-interactive-loop-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;14.8: ClinAIOps interactive loop. Credit: <span class="citation" data-cites="chen2023framework">Chen et al. (<a href="../../references.html#ref-chen2023framework" role="doc-biblioref">2023</a>)</span>.
</figcaption>
</figure>
</div>
</section>
</section>
<section id="mlops-vs.-clinaiops" class="level4">
<h4 class="anchored" data-anchor-id="mlops-vs.-clinaiops">MLOps vs.&nbsp;ClinAIOps</h4>
<p>The hypertension example illustrates well why traditional MLOps is insufficient for many real-world AI applications, and why frameworks like ClinAIOps are needed instead.</p>
<p>With hypertension, simply developing and deploying an ML model for adjusting medications would fail without considering the broader clinical context. The patient, clinician, and health system each have concerns that shape adoption. And the AI model cannot optimize blood pressure outcomes alone - it requires integrating with workflows, behaviors, and incentives.</p>
<ul>
<li>Some key gaps the example highlights in a pure MLOps approach:</li>
<li>The model itself would lack the real-world patient data at scale to reliably recommend treatments. ClinAIOps enables this through collecting feedback from clinicians and patients via continuous monitoring.</li>
<li>Clinicians would not trust model recommendations without transparency, explainability, and accountability. ClinAIOps keeps the clinician in the loop to build confidence.</li>
<li>Patients need personalized coaching and motivation - not just AI notifications. The ClinAIOps patient-clinician loop facilitates this.</li>
<li>Sensor reliability and data accuracy would be insufficient without clinical oversight. ClinAIOps validates recommendations.</li>
<li>Liability for treatment outcomes is unclear with just an ML model. ClinAIOps maintains human accountability.</li>
<li>Health systems would lack incentive to change workflows without demonstrating value. ClinAIOps aligns stakeholders.</li>
</ul>
<p>The hypertension case clearly shows the need to look beyond just training and deploying a performant ML model to considering the entire human-AI socio-technical system. This is the key gap ClinAIOps aims to address over traditional MLOps. Put another way, traditional MLOps is overly tech-focused on automating ML model development and deployment, while ClinAIOps incorporates clinical context and human-AI coordination through multi-stakeholder feedback loops.</p>
<p>Here is a table comparing them. The point of this table is to highlight how when MLOps is put into practice, we need to think about more than just ML models.</p>
<table class="table">
<colgroup>
<col style="width: 2%">
<col style="width: 50%">
<col style="width: 47%">
</colgroup>
<thead>
<tr class="header">
<th></th>
<th>Traditional MLOps</th>
<th>ClinAIOps</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Focus</td>
<td>ML model development and deployment</td>
<td>Coordinating human and AI decision-making</td>
</tr>
<tr class="even">
<td>Stakeholders</td>
<td>Data scientists, IT engineers</td>
<td>Patients, clinicians, AI developers</td>
</tr>
<tr class="odd">
<td>Feedback loops</td>
<td>Model retraining, monitoring</td>
<td>Patient-AI, clinician-AI, patient-clinician</td>
</tr>
<tr class="even">
<td>Objective</td>
<td>Operationalize ML deployments</td>
<td>Optimize patient health outcomes</td>
</tr>
<tr class="odd">
<td>Processes</td>
<td>Automated pipelines and infrastructure</td>
<td>Integrates clinical workflows and oversight</td>
</tr>
<tr class="even">
<td>Data considerations</td>
<td>Building training datasets</td>
<td>Privacy, ethics, protected health information</td>
</tr>
<tr class="odd">
<td>Model validation</td>
<td>Testing model performance metrics</td>
<td>Clinical evaluation of recommendations</td>
</tr>
<tr class="even">
<td>Implementation</td>
<td>Focuses on technical integration</td>
<td>Aligns incentives of human stakeholders</td>
</tr>
</tbody>
</table>
</section>
<section id="summary-1" class="level4">
<h4 class="anchored" data-anchor-id="summary-1">Summary</h4>
<p>In complex domains like healthcare, successfully deploying AI requires moving beyond a narrow focus on just training and deploying performant ML models. As illustrated through the hypertension example, real-world integration of AI necessitates coordinating diverse stakeholders, aligning incentives, validating recommendations, and maintaining accountability. Frameworks like ClinAIOps, which facilitate collaborative human-AI decision making through integrated feedback loops, are needed to address these multifaceted challenges. Rather than just automating tasks, AI must augment human capabilities and clinical workflows. This allows AI to deliver a positive impact on patient outcomes, population health, and healthcare efficiency.</p>
</section>
</section>
</section>
<section id="conclusion" class="level2" data-number="14.10">
<h2 data-number="14.10" class="anchored" data-anchor-id="conclusion"><span class="header-section-number">14.10</span> Conclusion</h2>
<p>Embedded ML is poised to transform many industries by enabling AI capabilities directly on edge devices like smartphones, sensors, and IoT hardware. However, developing and deploying TinyML models on resource-constrained embedded systems poses unique challenges compared to traditional cloud-based MLOps.</p>
<p>This chapter provided an in-depth analysis of key differences between traditional and embedded MLOps across the model lifecycle, development workflows, infrastructure management, and operational practices. We discussed how factors like intermittent connectivity, decentralized data, and limited on-device compute necessitate innovative techniques like federated learning, on-device inference, and model optimization. Architectural patterns like cross-device learning and hierarchical edge-cloud infrastructure help mitigate constraints.</p>
<p>Through concrete examples like Oura Ring and ClinAIOps, we demonstrated applied principles for embedded MLOps. The case studies highlighted critical considerations beyond just core ML engineering, like aligning stakeholder incentives, maintaining accountability, and coordinating human-AI decision making. This underscores the need for a holistic approach spanning both technical and human elements.</p>
<p>While embedded MLOps faces impediments, emerging tools like Edge Impulse and lessons from pioneers help accelerate TinyML innovation. A solid understanding of foundational MLOps principles tailored to embedded environments will empower more organizations to overcome constraints and deliver distributed AI capabilities. As frameworks and best practices mature, seamlessly integrating ML into edge devices and processes will transform industries through localized intelligence.</p>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list" style="display: none">
<div id="ref-attia2018noninvasive" class="csl-entry" role="listitem">
Attia, Zachi I., Alan Sugrue, Samuel J. Asirvatham, Michael J. Ackerman, Suraj Kapa, Paul A. Friedman, and Peter A. Noseworthy. 2018. <span>“Noninvasive Assessment of Dofetilide Plasma Concentration Using a Deep Learning (Neural Network) Analysis of the Surface Electrocardiogram: <span>A</span> Proof of Concept Study.”</span> <em>PLoS One</em> 13 (8): e0201059. <a href="https://doi.org/10.1371/journal.pone.0201059">https://doi.org/10.1371/journal.pone.0201059</a>.
</div>
<div id="ref-chen2023framework" class="csl-entry" role="listitem">
Chen, Emma, Shvetank Prakash, Vijay Janapa Reddi, David Kim, and Pranav Rajpurkar. 2023. <span>“A Framework for Integrating Artificial Intelligence for Clinical Care with Continuous Therapeutic Monitoring.”</span> <em>Nat. Biomed. Eng.</em> <a href="https://doi.org/10.1038/s41551-023-01115-0">https://doi.org/10.1038/s41551-023-01115-0</a>.
</div>
<div id="ref-guo2019mobile" class="csl-entry" role="listitem">
Guo, Yutao, Hao Wang, Hui Zhang, Tong Liu, Zhaoguang Liang, Yunlong Xia, Li Yan, et al. 2019. <span>“Mobile Photoplethysmographic Technology to Detect Atrial Fibrillation.”</span> <em>J. Am. Coll. Cardiol.</em> 74 (19): 2365–75. <a href="https://doi.org/10.1016/j.jacc.2019.08.019">https://doi.org/10.1016/j.jacc.2019.08.019</a>.
</div>
<div id="ref-janapa2023edge" class="csl-entry" role="listitem">
Janapa Reddi, Vijay, Alexander Elium, Shawn Hymel, David Tischler, Daniel Situnayake, Carl Ward, Louis Moreau, et al. 2023. <span>“Edge Impulse: <span>An</span> <span>MLOps</span> Platform for Tiny Machine Learning.”</span> <em>Proceedings of Machine Learning and Systems</em> 5.
</div>
<div id="ref-li2021noninvasive" class="csl-entry" role="listitem">
Li, Jingzhen, Igbe Tobore, Yuhang Liu, Abhishek Kandwal, Lei Wang, and Zedong Nie. 2021. <span>“Non-Invasive Monitoring of Three Glucose Ranges Based on <span>ECG</span> by Using <span>DBSCAN</span>-<span>CNN</span>.”</span> <em>#IEEE_J_BHI#</em> 25 (9): 3340–50. <a href="https://doi.org/10.1109/jbhi.2021.3072628">https://doi.org/10.1109/jbhi.2021.3072628</a>.
</div>
<div id="ref-liu2022monitoring" class="csl-entry" role="listitem">
Liu, Yingcheng, Guo Zhang, Christopher G. Tarolli, Rumen Hristov, Stella Jensen-Roberts, Emma M. Waddell, Taylor L. Myers, et al. 2022. <span>“Monitoring Gait at Home with Radio Waves in Parkinson<span>’</span>s Disease: <span>A</span> Marker of Severity, Progression, and Medication Response.”</span> <em>Sci. Transl. Med.</em> 14 (663): eadc9669. <a href="https://doi.org/10.1126/scitranslmed.adc9669">https://doi.org/10.1126/scitranslmed.adc9669</a>.
</div>
<div id="ref-psoma2023wearable" class="csl-entry" role="listitem">
Psoma, Sotiria D., and Chryso Kanthou. 2023. <span>“Wearable Insulin Biosensors for Diabetes Management: <span>Advances</span> and Challenges.”</span> <em>Biosensors</em> 13 (7): 719. <a href="https://doi.org/10.3390/bios13070719">https://doi.org/10.3390/bios13070719</a>.
</div>
<div id="ref-sculley2015hidden" class="csl-entry" role="listitem">
Sambasivan, Nithya, Shivani Kapania, Hannah Highfill, Diana Akrong, Praveen Paritosh, and Lora M Aroyo. 2021a. <span>“<span><span>“</span>Everyone</span> Wants to Do the Model Work, Not the Data Work<span>”</span>: <span>Data</span> Cascades in High-Stakes <span>AI</span>.”</span> In <em>Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems</em>. ACM. <a href="https://doi.org/10.1145/3411764.3445518">https://doi.org/10.1145/3411764.3445518</a>.
</div>
<div id="ref-sambasivan2021" class="csl-entry" role="listitem">
———. 2021b. <span>“Everyone Wants to Do the Model Work, Not the Data Work: Data Cascades in High-Stakes AI.”</span> In <em>Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems</em>. CHI ’21. New York, NY, USA: Association for Computing Machinery. <a href="https://doi.org/10.1145/3411764.3445518">https://doi.org/10.1145/3411764.3445518</a>.
</div>
<div id="ref-stm2021l4" class="csl-entry" role="listitem">
<em>STM32L4Q5AG</em>. 2021. STMicroelectronics.
</div>
<div id="ref-zhang2017highly" class="csl-entry" role="listitem">
Zhang, Qingxue, Dian Zhou, and Xuan Zeng. 2017. <span>“Highly Wearable Cuff-Less Blood Pressure and Heart Rate Monitoring with Single-Arm Electrocardiogram and Photoplethysmogram Signals.”</span> <em>BioMedical Engineering OnLine</em> 16 (1): 23. <a href="https://doi.org/10.1186/s12938-017-0317-z">https://doi.org/10.1186/s12938-017-0317-z</a>.
</div>
</div>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
          // target, if specified
          link.setAttribute("target", "_blank");
          if (link.getAttribute("rel") === null) {
            link.setAttribute("rel", "noopener");
          }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../../contents/ondevice_learning/ondevice_learning.html" class="pagination-link  aria-label=" &lt;span="" learning&lt;="" span&gt;"="">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">On-Device Learning</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../../contents/privacy_security/privacy_security.html" class="pagination-link" aria-label="<span class='chapter-number'>15</span>&nbsp; <span class='chapter-title'>Security &amp; Privacy</span>">
        <span class="nav-page-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Security &amp; Privacy</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>Edited by Prof.&nbsp;Vijay Janapa Reddi (Harvard University)</p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    <div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/harvard-edge/cs249r_book/edit/main/contents/ops/ops.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li><li><a href="https://github.com/harvard-edge/cs249r_book/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li><li><a href="https://github.com/harvard-edge/cs249r_book/blob/main/contents/ops/ops.qmd" class="toc-action"><i class="bi empty"></i>View source</a></li></ul></div></div>
    <div class="nav-footer-right">
<p>This book was built with <a href="https://quarto.org/">Quarto</a>.</p>
</div>
  </div>
</footer>




</body></html>