<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.32">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Object Detection ‚Äì Machine Learning Systems</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../../">
<link href="../../../../contents/labs/raspi/llm/llm.html" rel="next">
<link href="../../../../contents/labs/raspi/image_classification/image_classification.html" rel="prev">
<link href="../../../../favicon.png" rel="icon" type="image/png">
<script src="../../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../../site_libs/quarto-html/quarto-syntax-highlighting-fc6d358c97f25a8ea829b86655043430.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../../../site_libs/quarto-html/quarto-syntax-highlighting-dark-5c6c0ad7bdfb89369003da8042cd4f02.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../../site_libs/bootstrap/bootstrap-71dacee0c6392ff771ab8565bd7b5b7b.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../../../site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script src="../../../../site_libs/quarto-contrib/glightbox/glightbox.min.js"></script>
<link href="../../../../site_libs/quarto-contrib/glightbox/glightbox.min.css" rel="stylesheet">
<link href="../../../../site_libs/quarto-contrib/glightbox/lightbox.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "~",
    "/"
  ],
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-M21L0CBCVN"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-M21L0CBCVN', { 'anonymize_ip': true});
</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-M21L0CBCVN"></script>
<script type="module" src="../../../../scripts/ai_menu/dist/bundle.js" defer=""></script>


</head>

<body class="nav-sidebar floating nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../../../index.html">
    <span class="navbar-title">Machine Learning Systems</span>
    </a>
  </div>
        <div class="quarto-navbar-tools tools-wide tools-end">
    <a href="https://github.com/harvard-edge/cs249r_book" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
    <a href="../../../../Machine-Learning-Systems.pdf" title="Download PDF" class="quarto-navigation-tool px-1" aria-label="Download PDF"><i class="bi bi-file-pdf"></i></a>
    <div class="dropdown">
      <a href="" title="Share" id="quarto-navigation-tool-dropdown-0" class="quarto-navigation-tool dropdown-toggle px-1" data-bs-toggle="dropdown" aria-expanded="false" role="link" aria-label="Share"><i class="bi bi-share"></i></a>
      <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="quarto-navigation-tool-dropdown-0">
          <li>
            <a class="dropdown-item quarto-navbar-tools-item" href="https://twitter.com/intent/tweet?url=|url|">
              <i class="bi bi-twitter pe-1"></i>
            Twitter
            </a>
          </li>
          <li>
            <a class="dropdown-item quarto-navbar-tools-item" href="https://www.facebook.com/sharer/sharer.php?u=|url|">
              <i class="bi bi-facebook pe-1"></i>
            Facebook
            </a>
          </li>
      </ul>
    </div>
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
  <a href="" class="quarto-reader-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleReader(); return false;" title="Toggle reader mode">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../../../contents/labs/raspi/raspi.html">Raspberry Pi</a></li><li class="breadcrumb-item"><a href="../../../../contents/labs/raspi/object_detection/object_detection.html">Object Detection</a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
  <div id="quarto-announcement" data-announcement-id="468ec4500d92e615b1615ac01f7d119e" class="alert alert-primary hidden"><i class="bi bi-star-half quarto-announcement-icon"></i><div class="quarto-announcement-content">
<p>üìò <b>What‚Äôs new?</b> üéâ Happy New Year 2025! Chapters 1‚Äì5 expanded and improved, with a brand new Chapter 4!<br> üöÄ <b>Every learner deserves to be a ‚≠ê</b> 1 GitHub ‚≠ê = 1 üë©‚Äçüéì Learner. Your support fuels free, impactful educational resources on AI.<br> üôè <b>Thank you for your support!</b> Every GitHub star shows a learner engaging or a supporter driving our mission. You can click <a href="https://github.com/harvard-edge/cs249r_book">here</a> to star us.</p>
</div><i class="bi bi-x-lg quarto-announcement-action"></i></div>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/frontmatter/foreword.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Author‚Äôs Note</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/frontmatter/about/about.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">About the Book</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/frontmatter/acknowledgements/acknowledgements.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Acknowledgements</span></a>
  </div>
</li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/frontmatter/ai/socratiq.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">ü§ñ SocratiQ AI</span></a>
  </div>
</li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item sidebar-item-section">
      <span class="sidebar-item-text sidebar-link text-start">
 <span class="menu-text"><div class="part">The Essentials</div></span></span>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/core/introduction/introduction.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/core/ml_systems/ml_systems.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">ML Systems</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/core/dl_primer/dl_primer.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">DL Primer</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/core/dnn_architectures/dnn_architectures.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">DNN Architectures</span></span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <span class="sidebar-item-text sidebar-link text-start">
 <span class="menu-text"><div class="part">Engineering Principles</div></span></span>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/core/workflow/workflow.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">AI Workflow</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/core/data_engineering/data_engineering.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Data Engineering</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/core/frameworks/frameworks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">AI Frameworks</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/core/training/training.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">AI Training</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/core/efficient_ai/efficient_ai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Efficient AI</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/core/optimizations/optimizations.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Model Optimizations</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/core/hw_acceleration/hw_acceleration.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">AI Acceleration</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/core/benchmarking/benchmarking.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Benchmarking AI</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/core/ops/ops.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">ML Operations</span></span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <span class="sidebar-item-text sidebar-link text-start">
 <span class="menu-text"><div class="part">Best Practices in AI</div></span></span>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/core/ondevice_learning/ondevice_learning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">On-Device Learning</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/core/privacy_security/privacy_security.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Security &amp; Privacy</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/core/responsible_ai/responsible_ai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Responsible AI</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/core/sustainable_ai/sustainable_ai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Sustainable AI</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/core/robust_ai/robust_ai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Robust AI</span></span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <span class="sidebar-item-text sidebar-link text-start">
 <span class="menu-text"><div class="part">Closing Perspectives</div></span></span>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/core/ai_for_good/ai_for_good.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">AI for Good</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/core/conclusion/conclusion.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Conclusion</span></span></a>
  </div>
</li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../../../contents/labs/labs.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">LABS</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/labs/overview.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Overview</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/labs/getting_started.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Getting Started</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../../../contents/labs/arduino/nicla_vision/nicla_vision.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Nicla Vision</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/labs/arduino/nicla_vision/setup/setup.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Setup</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/labs/arduino/nicla_vision/image_classification/image_classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Image Classification</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/labs/arduino/nicla_vision/object_detection/object_detection.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Object Detection</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/labs/arduino/nicla_vision/kws/kws.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Keyword Spotting (KWS)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/labs/arduino/nicla_vision/motion_classification/motion_classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Motion Classification and Anomaly Detection</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../../../contents/labs/seeed/xiao_esp32s3/xiao_esp32s3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">XIAO ESP32S3</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-7" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/labs/seeed/xiao_esp32s3/setup/setup.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Setup</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/labs/seeed/xiao_esp32s3/image_classification/image_classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Image Classification</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/labs/seeed/xiao_esp32s3/object_detection/object_detection.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Object Detection</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/labs/seeed/xiao_esp32s3/kws/kws.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Keyword Spotting (KWS)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/labs/seeed/xiao_esp32s3/motion_classification/motion_classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Motion Classification and Anomaly Detection</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../../../contents/labs/raspi/raspi.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Raspberry Pi</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-8" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/labs/raspi/setup/setup.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Setup</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/labs/raspi/image_classification/image_classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Image Classification</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/labs/raspi/object_detection/object_detection.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">Object Detection</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/labs/raspi/llm/llm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Small Language Models (SLM)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/labs/raspi/vlm/vlm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Vision-Language Models (VLM)</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../../../contents/labs/shared/shared.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Shared Labs</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-9" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-9" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/labs/shared/kws_feature_eng/kws_feature_eng.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">KWS Feature Engineering</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/labs/shared/dsp_spectral_features_block/dsp_spectral_features_block.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">DSP Spectral Features</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-10" role="navigation" aria-expanded="true">
 <span class="menu-text">REFERENCES</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-10" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-10" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/core/references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#overview" id="toc-overview" class="nav-link active" data-scroll-target="#overview">Overview</a>
  <ul>
  <li><a href="#object-detection-fundamentals" id="toc-object-detection-fundamentals" class="nav-link" data-scroll-target="#object-detection-fundamentals">Object Detection Fundamentals</a>
  <ul class="collapse">
  <li><a href="#image-classification-vs.-object-detection" id="toc-image-classification-vs.-object-detection" class="nav-link" data-scroll-target="#image-classification-vs.-object-detection">Image Classification vs.&nbsp;Object Detection</a></li>
  <li><a href="#key-components-of-object-detection" id="toc-key-components-of-object-detection" class="nav-link" data-scroll-target="#key-components-of-object-detection">Key Components of Object Detection</a></li>
  <li><a href="#challenges-in-object-detection" id="toc-challenges-in-object-detection" class="nav-link" data-scroll-target="#challenges-in-object-detection">Challenges in Object Detection</a></li>
  <li><a href="#approaches-to-object-detection" id="toc-approaches-to-object-detection" class="nav-link" data-scroll-target="#approaches-to-object-detection">Approaches to Object Detection</a></li>
  <li><a href="#evaluation-metrics" id="toc-evaluation-metrics" class="nav-link" data-scroll-target="#evaluation-metrics">Evaluation Metrics</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#pre-trained-object-detection-models-overview" id="toc-pre-trained-object-detection-models-overview" class="nav-link" data-scroll-target="#pre-trained-object-detection-models-overview">Pre-Trained Object Detection Models Overview</a>
  <ul>
  <li><a href="#setting-up-the-tflite-environment" id="toc-setting-up-the-tflite-environment" class="nav-link" data-scroll-target="#setting-up-the-tflite-environment">Setting Up the TFLite Environment</a></li>
  <li><a href="#creating-a-working-directory" id="toc-creating-a-working-directory" class="nav-link" data-scroll-target="#creating-a-working-directory">Creating a Working Directory:</a></li>
  <li><a href="#inference-and-post-processing" id="toc-inference-and-post-processing" class="nav-link" data-scroll-target="#inference-and-post-processing">Inference and Post-Processing</a></li>
  <li><a href="#efficientdet" id="toc-efficientdet" class="nav-link" data-scroll-target="#efficientdet">EfficientDet</a></li>
  </ul></li>
  <li><a href="#object-detection-project" id="toc-object-detection-project" class="nav-link" data-scroll-target="#object-detection-project">Object Detection Project</a>
  <ul>
  <li><a href="#the-goal" id="toc-the-goal" class="nav-link" data-scroll-target="#the-goal">The Goal</a></li>
  <li><a href="#raw-data-collection" id="toc-raw-data-collection" class="nav-link" data-scroll-target="#raw-data-collection">Raw Data Collection</a></li>
  <li><a href="#labeling-data" id="toc-labeling-data" class="nav-link" data-scroll-target="#labeling-data">Labeling Data</a>
  <ul class="collapse">
  <li><a href="#annotate" id="toc-annotate" class="nav-link" data-scroll-target="#annotate">Annotate</a></li>
  <li><a href="#data-pre-processing" id="toc-data-pre-processing" class="nav-link" data-scroll-target="#data-pre-processing">Data Pre-Processing</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#training-an-ssd-mobilenet-model-on-edge-impulse-studio" id="toc-training-an-ssd-mobilenet-model-on-edge-impulse-studio" class="nav-link" data-scroll-target="#training-an-ssd-mobilenet-model-on-edge-impulse-studio">Training an SSD MobileNet Model on Edge Impulse Studio</a>
  <ul>
  <li><a href="#uploading-the-annotated-data" id="toc-uploading-the-annotated-data" class="nav-link" data-scroll-target="#uploading-the-annotated-data">Uploading the annotated data</a></li>
  <li><a href="#the-impulse-design" id="toc-the-impulse-design" class="nav-link" data-scroll-target="#the-impulse-design">The Impulse Design</a></li>
  <li><a href="#preprocessing-all-dataset" id="toc-preprocessing-all-dataset" class="nav-link" data-scroll-target="#preprocessing-all-dataset">Preprocessing all dataset</a></li>
  <li><a href="#model-design-training-and-test" id="toc-model-design-training-and-test" class="nav-link" data-scroll-target="#model-design-training-and-test">Model Design, Training, and Test</a></li>
  <li><a href="#deploying-the-model" id="toc-deploying-the-model" class="nav-link" data-scroll-target="#deploying-the-model">Deploying the model</a></li>
  <li><a href="#inference-and-post-processing-1" id="toc-inference-and-post-processing-1" class="nav-link" data-scroll-target="#inference-and-post-processing-1">Inference and Post-Processing</a></li>
  </ul></li>
  <li><a href="#training-a-fomo-model-at-edge-impulse-studio" id="toc-training-a-fomo-model-at-edge-impulse-studio" class="nav-link" data-scroll-target="#training-a-fomo-model-at-edge-impulse-studio">Training a FOMO Model at Edge Impulse Studio</a>
  <ul>
  <li><a href="#how-fomo-works" id="toc-how-fomo-works" class="nav-link" data-scroll-target="#how-fomo-works">How FOMO works?</a></li>
  <li><a href="#impulse-design-new-training-and-testing" id="toc-impulse-design-new-training-and-testing" class="nav-link" data-scroll-target="#impulse-design-new-training-and-testing">Impulse Design, new Training and Testing</a></li>
  <li><a href="#deploying-the-model-1" id="toc-deploying-the-model-1" class="nav-link" data-scroll-target="#deploying-the-model-1">Deploying the model</a></li>
  <li><a href="#inference-and-post-processing-2" id="toc-inference-and-post-processing-2" class="nav-link" data-scroll-target="#inference-and-post-processing-2">Inference and Post-Processing</a></li>
  </ul></li>
  <li><a href="#exploring-a-yolo-model-using-ultralitics" id="toc-exploring-a-yolo-model-using-ultralitics" class="nav-link" data-scroll-target="#exploring-a-yolo-model-using-ultralitics">Exploring a YOLO Model using Ultralitics</a>
  <ul>
  <li><a href="#talking-about-the-yolo-model" id="toc-talking-about-the-yolo-model" class="nav-link" data-scroll-target="#talking-about-the-yolo-model">Talking about the YOLO Model</a>
  <ul class="collapse">
  <li><a href="#key-features" id="toc-key-features" class="nav-link" data-scroll-target="#key-features">Key Features:</a></li>
  </ul></li>
  <li><a href="#installation" id="toc-installation" class="nav-link" data-scroll-target="#installation">Installation</a></li>
  <li><a href="#testing-the-yolo" id="toc-testing-the-yolo" class="nav-link" data-scroll-target="#testing-the-yolo">Testing the YOLO</a></li>
  <li><a href="#export-model-to-ncnn-format" id="toc-export-model-to-ncnn-format" class="nav-link" data-scroll-target="#export-model-to-ncnn-format">Export Model to NCNN format</a></li>
  <li><a href="#exploring-yolo-with-python" id="toc-exploring-yolo-with-python" class="nav-link" data-scroll-target="#exploring-yolo-with-python">Exploring YOLO with Python</a></li>
  <li><a href="#training-yolov8-on-a-customized-dataset" id="toc-training-yolov8-on-a-customized-dataset" class="nav-link" data-scroll-target="#training-yolov8-on-a-customized-dataset">Training YOLOv8 on a Customized Dataset</a>
  <ul class="collapse">
  <li><a href="#critical-points-on-the-notebook" id="toc-critical-points-on-the-notebook" class="nav-link" data-scroll-target="#critical-points-on-the-notebook">Critical points on the Notebook:</a></li>
  </ul></li>
  <li><a href="#inference-with-the-trained-model-using-the-raspi" id="toc-inference-with-the-trained-model-using-the-raspi" class="nav-link" data-scroll-target="#inference-with-the-trained-model-using-the-raspi">Inference with the trained model, using the Raspi</a></li>
  </ul></li>
  <li><a href="#object-detection-on-a-live-stream" id="toc-object-detection-on-a-live-stream" class="nav-link" data-scroll-target="#object-detection-on-a-live-stream">Object Detection on a live stream</a></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion">Conclusion</a></li>
  <li><a href="#resources" id="toc-resources" class="nav-link" data-scroll-target="#resources">Resources</a></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/harvard-edge/cs249r_book/edit/dev/contents/labs/raspi/object_detection/object_detection.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li><li><a href="https://github.com/harvard-edge/cs249r_book/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li><li><a href="https://github.com/harvard-edge/cs249r_book/blob/dev/contents/labs/raspi/object_detection/object_detection.qmd" class="toc-action"><i class="bi empty"></i>View source</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../../../contents/labs/raspi/raspi.html">Raspberry Pi</a></li><li class="breadcrumb-item"><a href="../../../../contents/labs/raspi/object_detection/object_detection.html">Object Detection</a></li></ol></nav>
<div class="quarto-title">
<h1 class="title">Object Detection</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/jpeg/cover.jpg" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="DALL¬∑E prompt - A cover image for an ‚ÄòObject Detection‚Äô chapter in a Raspberry Pi tutorial, designed in the same vintage 1950s electronics lab style as previous covers. The scene should prominently feature wheels and cubes, similar to those provided by the user, placed on a workbench in the foreground. A Raspberry Pi with a connected camera module should be capturing an image of these objects. Surround the scene with classic lab tools like soldering irons, resistors, and wires. The lab background should include vintage equipment like oscilloscopes and tube radios, maintaining the detailed and nostalgic feel of the era. No text or logos should be included."><img src="images/jpeg/cover.jpg" class="img-fluid figure-img" alt="DALL¬∑E prompt - A cover image for an ‚ÄòObject Detection‚Äô chapter in a Raspberry Pi tutorial, designed in the same vintage 1950s electronics lab style as previous covers. The scene should prominently feature wheels and cubes, similar to those provided by the user, placed on a workbench in the foreground. A Raspberry Pi with a connected camera module should be capturing an image of these objects. Surround the scene with classic lab tools like soldering irons, resistors, and wires. The lab background should include vintage equipment like oscilloscopes and tube radios, maintaining the detailed and nostalgic feel of the era. No text or logos should be included."></a></p>
<figcaption><em>DALL¬∑E prompt - A cover image for an ‚ÄòObject Detection‚Äô chapter in a Raspberry Pi tutorial, designed in the same vintage 1950s electronics lab style as previous covers. The scene should prominently feature wheels and cubes, similar to those provided by the user, placed on a workbench in the foreground. A Raspberry Pi with a connected camera module should be capturing an image of these objects. Surround the scene with classic lab tools like soldering irons, resistors, and wires. The lab background should include vintage equipment like oscilloscopes and tube radios, maintaining the detailed and nostalgic feel of the era. No text or logos should be included.</em></figcaption>
</figure>
</div>
<section id="overview" class="level2">
<h2 class="anchored" data-anchor-id="overview">Overview</h2>
<p>Building upon our exploration of image classification, we now turn our attention to a more advanced computer vision task: object detection. While image classification assigns a single label to an entire image, object detection goes further by identifying and locating multiple objects within a single image. This capability opens up many new applications and challenges, particularly in edge computing and IoT devices like the Raspberry Pi.</p>
<p>Object detection combines the tasks of classification and localization. It not only determines what objects are present in an image but also pinpoints their locations by, for example, drawing bounding boxes around them. This added complexity makes object detection a more powerful tool for understanding visual scenes, but it also requires more sophisticated models and training techniques.</p>
<p>In edge AI, where we work with constrained computational resources, implementing efficient object detection models becomes crucial. The challenges we faced with image classification‚Äîbalancing model size, inference speed, and accuracy‚Äîare amplified in object detection. However, the rewards are also more significant, as object detection enables more nuanced and detailed visual data analysis.</p>
<p>Some applications of object detection on edge devices include:</p>
<ol type="1">
<li>Surveillance and security systems</li>
<li>Autonomous vehicles and drones</li>
<li>Industrial quality control</li>
<li>Wildlife monitoring</li>
<li>Augmented reality applications</li>
</ol>
<p>As we put our hands into object detection, we‚Äôll build upon the concepts and techniques we explored in image classification. We‚Äôll examine popular object detection architectures designed for efficiency, such as:</p>
<ul>
<li>Single Stage Detectors, such as MobileNet and EfficientDet,</li>
<li>FOMO (Faster Objects, More Objects), and</li>
<li>YOLO (You Only Look Once).</li>
</ul>
<blockquote class="blockquote">
<p>To learn more about object detection models, follow the tutorial <a href="https://machinelearningmastery.com/object-recognition-with-deep-learning/">A Gentle Introduction to Object Recognition With Deep Learning</a>.</p>
</blockquote>
<p>We will explore those object detection models using</p>
<ul>
<li>TensorFlow Lite Runtime (now changed to <a href="https://ai.google.dev/edge/litert">LiteRT</a>),</li>
<li>Edge Impulse Linux Python SDK and</li>
<li>Ultralitics</li>
</ul>
<p><a href="images/png/block.png" class="lightbox" data-gallery="quarto-lightbox-gallery-2"><img src="images/png/block.png" class="img-fluid"></a></p>
<p>Throughout this lab, we‚Äôll cover the fundamentals of object detection and how it differs from image classification. We‚Äôll also learn how to train, fine-tune, test, optimize, and deploy popular object detection architectures using a dataset created from scratch.</p>
<section id="object-detection-fundamentals" class="level3">
<h3 class="anchored" data-anchor-id="object-detection-fundamentals">Object Detection Fundamentals</h3>
<p>Object detection builds upon the foundations of image classification but extends its capabilities significantly. To understand object detection, it‚Äôs crucial first to recognize its key differences from image classification:</p>
<section id="image-classification-vs.-object-detection" class="level4">
<h4 class="anchored" data-anchor-id="image-classification-vs.-object-detection">Image Classification vs.&nbsp;Object Detection</h4>
<p><strong>Image Classification:</strong></p>
<ul>
<li>Assigns a single label to an entire image</li>
<li>Answers the question: ‚ÄúWhat is this image‚Äôs primary object or scene?‚Äù</li>
<li>Outputs a single class prediction for the whole image</li>
</ul>
<p><strong>Object Detection:</strong></p>
<ul>
<li>Identifies and locates multiple objects within an image</li>
<li>Answers the questions: ‚ÄúWhat objects are in this image, and where are they located?‚Äù</li>
<li>Outputs multiple predictions, each consisting of a class label and a bounding box</li>
</ul>
<p>To visualize this difference, let‚Äôs consider an example: <img src="images/jpeg/objxclas.jpg" class="img-fluid"></p>
<p>This diagram illustrates the critical difference: image classification provides a single label for the entire image, while object detection identifies multiple objects, their classes, and their locations within the image.</p>
</section>
<section id="key-components-of-object-detection" class="level4">
<h4 class="anchored" data-anchor-id="key-components-of-object-detection">Key Components of Object Detection</h4>
<p>Object detection systems typically consist of two main components:</p>
<ol type="1">
<li><p>Object Localization: This component identifies where objects are located in the image. It typically outputs bounding boxes, rectangular regions encompassing each detected object.</p></li>
<li><p>Object Classification: This component determines the class or category of each detected object, similar to image classification but applied to each localized region.</p></li>
</ol>
</section>
<section id="challenges-in-object-detection" class="level4">
<h4 class="anchored" data-anchor-id="challenges-in-object-detection">Challenges in Object Detection</h4>
<p>Object detection presents several challenges beyond those of image classification:</p>
<ul>
<li>Multiple objects: An image may contain multiple objects of various classes, sizes, and positions.</li>
<li>Varying scales: Objects can appear at different sizes within the image.</li>
<li>Occlusion: Objects may be partially hidden or overlapping.</li>
<li>Background clutter: Distinguishing objects from complex backgrounds can be challenging.</li>
<li>Real-time performance: Many applications require fast inference times, especially on edge devices.</li>
</ul>
</section>
<section id="approaches-to-object-detection" class="level4">
<h4 class="anchored" data-anchor-id="approaches-to-object-detection">Approaches to Object Detection</h4>
<p>There are two main approaches to object detection:</p>
<ol type="1">
<li><p>Two-stage detectors: These first propose regions of interest and then classify each region. Examples include R-CNN and its variants (Fast R-CNN, Faster R-CNN).</p></li>
<li><p>Single-stage detectors: These predict bounding boxes (or centroids) and class probabilities in one forward pass of the network. Examples include YOLO (You Only Look Once), EfficientDet, SSD (Single Shot Detector), and FOMO (Faster Objects, More Objects). These are often faster and more suitable for edge devices like Raspberry Pi.</p></li>
</ol>
</section>
<section id="evaluation-metrics" class="level4">
<h4 class="anchored" data-anchor-id="evaluation-metrics">Evaluation Metrics</h4>
<p>Object detection uses different metrics compared to image classification:</p>
<ul>
<li><strong>Intersection over Union (IoU)</strong>: Measures the overlap between predicted and ground truth bounding boxes.</li>
<li><strong>Mean Average Precision (mAP)</strong>: Combines precision and recall across all classes and IoU thresholds.</li>
<li><strong>Frames Per Second (FPS)</strong>: Measures detection speed, crucial for real-time applications on edge devices.</li>
</ul>
</section>
</section>
</section>
<section id="pre-trained-object-detection-models-overview" class="level2">
<h2 class="anchored" data-anchor-id="pre-trained-object-detection-models-overview">Pre-Trained Object Detection Models Overview</h2>
<p>As we saw in the introduction, given an image or a video stream, an object detection model can identify which of a known set of objects might be present and provide information about their positions within the image.</p>
<blockquote class="blockquote">
<p>You can test some common models online by visiting <a href="https://mediapipe-studio.webapps.google.com/studio/demo/object_detector">Object Detection - MediaPipe Studio</a></p>
</blockquote>
<p>On <a href="https://www.kaggle.com/models?id=298,130,299">Kaggle</a>, we can find the most common pre-trained tflite models to use with the Raspi, <a href="https://www.kaggle.com/models/tensorflow/ssd-mobilenet-v1/tfLite">ssd_mobilenet_v1,</a> and <a href="https://www.kaggle.com/models/tensorflow/efficientdet/tfLite">EfficientDet</a>. Those models were trained on the COCO (Common Objects in Context) dataset, with over 200,000 labeled images in 91 categories. Go, download the models, and upload them to the <code>./models</code> folder in the Raspi.</p>
<blockquote class="blockquote">
<p>Alternatively<a href="https://github.com/Mjrovai/EdgeML-with-Raspberry-Pi/tree/main/OBJ_DETEC/models">,</a> you can find the models and the COCO labels on <a href="https://github.com/Mjrovai/EdgeML-with-Raspberry-Pi/tree/main/OBJ_DETEC/models">GitHub</a>.</p>
</blockquote>
<p>For the first part of this lab, we will focus on a pre-trained 300x300 SSD-Mobilenet V1 model and compare it with the 320x320 EfficientDet-lite0, also trained using the COCO 2017 dataset. Both models were converted to a TensorFlow Lite format (4.2MB for the SSD Mobilenet and 4.6MB for the EfficientDet).</p>
<blockquote class="blockquote">
<p>SSD-Mobilenet V2 or V3 is recommended for transfer learning projects, but once the V1 TFLite model is publicly available, we will use it for this overview.</p>
</blockquote>
<p><a href="images/png/model-deploy.png" class="lightbox" data-gallery="quarto-lightbox-gallery-3"><img src="images/png/model-deploy.png" class="img-fluid"></a></p>
<section id="setting-up-the-tflite-environment" class="level3">
<h3 class="anchored" data-anchor-id="setting-up-the-tflite-environment">Setting Up the TFLite Environment</h3>
<p>We should confirm the steps done on the last Hands-On Lab, Image Classification, as follows:</p>
<ul>
<li><p>Updating the Raspberry Pi</p></li>
<li><p>Installing Required Libraries</p></li>
<li><p>Setting up a Virtual Environment (Optional but Recommended)</p></li>
</ul>
<div class="sourceCode" id="cb1"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="bu">source</span> ~/tflite/bin/activate</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<ul>
<li><p>Installing TensorFlow Lite Runtime</p></li>
<li><p>Installing Additional Python Libraries (inside the environment)</p></li>
</ul>
</section>
<section id="creating-a-working-directory" class="level3">
<h3 class="anchored" data-anchor-id="creating-a-working-directory">Creating a Working Directory:</h3>
<p>Considering that we have created the <code>Documents/TFLITE</code> folder in the last Lab, let‚Äôs now create the specific folders for this object detection lab:</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="bu">cd</span> Documents/TFLITE/</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="fu">mkdir</span> OBJ_DETECT</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="bu">cd</span> OBJ_DETECT</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="fu">mkdir</span> images</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="fu">mkdir</span> models</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a><span class="bu">cd</span> models</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="inference-and-post-processing" class="level3">
<h3 class="anchored" data-anchor-id="inference-and-post-processing">Inference and Post-Processing</h3>
<p>Let‚Äôs start a new <a href="https://github.com/Mjrovai/EdgeML-with-Raspberry-Pi/blob/main/OBJ_DETEC/notebooks/SSD_MobileNetV1.ipynb">notebook</a> to follow all the steps to detect objects on an image:</p>
<p>Import the needed libraries:</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> time</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> PIL <span class="im">import</span> Image</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> tflite_runtime.interpreter <span class="im">as</span> tflite</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Load the TFLite model and allocate tensors:</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>model_path <span class="op">=</span> <span class="st">"./models/ssd-mobilenet-v1-tflite-default-v1.tflite"</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>interpreter <span class="op">=</span> tflite.Interpreter(model_path<span class="op">=</span>model_path)</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>interpreter.allocate_tensors()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Get input and output tensors.</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>input_details <span class="op">=</span> interpreter.get_input_details()</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>output_details <span class="op">=</span> interpreter.get_output_details()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><strong>Input details</strong> will inform us how the model should be fed with an image. The shape of <code>(1, 300, 300, 3)</code> with a dtype of <code>uint8</code> tells us that a non-normalized (pixel value range from 0 to 255) image with dimensions (300x300x3) should be input one by one (Batch Dimension: 1).</p>
<p>The <strong>output details</strong> include not only the labels (‚Äúclasses‚Äù) and probabilities (‚Äúscores‚Äù) but also the relative window position of the bounding boxes (‚Äúboxes‚Äù) about where the object is located on the image and the number of detected objects (‚Äúnum_detections‚Äù). The output details also tell us that the model can detect a <strong>maximum of 10 objects</strong> in the image.</p>
<p><a href="images/png/inference result.png" class="lightbox" data-gallery="quarto-lightbox-gallery-4"><img src="images/png/inference result.png" class="img-fluid"></a></p>
<p>So, for the above example, using the same cat image used with the <em>Image Classification Lab</em> looking for the output, we have a <strong>76% probability</strong> of having found an object with a <strong>class ID of 16</strong> on an area delimited by a <strong>bounding box of [0.028011084, 0.020121813, 0.9886069, 0.802299]</strong>. Those four numbers are related to <code>ymin</code>, <code>xmin</code>, <code>ymax</code> and <code>xmax</code>, the box coordinates.</p>
<p>Taking into consideration that <strong>y</strong> goes from the top <code>(ymin</code>) to the bottom (<code>ymax</code>) and <strong>x</strong> goes from left (<code>xmin</code>) to the right (<code>xmax</code>), we have, in fact, the coordinates of the top/left corner and the bottom/right one. With both edges and knowing the shape of the picture, it is possible to draw a rectangle around the object, as shown in the figure below:</p>
<p><a href="images/png/boulding-boxes.png" class="lightbox" data-gallery="quarto-lightbox-gallery-5"><img src="images/png/boulding-boxes.png" class="img-fluid"></a></p>
<p>Next, we should find what class ID equal to 16 means. Opening the file <code>coco_labels.txt</code>, as a list, each element has an associated index, and inspecting index 16, we get, as expected, <code>cat</code>. The probability is the value returning from the score.</p>
<p>Let‚Äôs now upload some images with multiple objects on it for testing.</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>img_path <span class="op">=</span> <span class="st">"./images/cat_dog.jpeg"</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>orig_img <span class="op">=</span> Image.<span class="bu">open</span>(img_path)</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Display the image</span></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">8</span>))</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>plt.imshow(orig_img)</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Original Image"</span>)</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><a href="images/png/cat-dog.png" class="lightbox" data-gallery="quarto-lightbox-gallery-6"><img src="images/png/cat-dog.png" class="img-fluid"></a></p>
<p>Based on the input details, let‚Äôs pre-process the image, changing its shape and expanding its dimension:</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>img <span class="op">=</span> orig_img.resize((input_details[<span class="dv">0</span>][<span class="st">'shape'</span>][<span class="dv">1</span>], </span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>                  input_details[<span class="dv">0</span>][<span class="st">'shape'</span>][<span class="dv">2</span>]))</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>input_data <span class="op">=</span> np.expand_dims(img, axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>input_data.shape, input_data.dtype </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>The new input_data shape is<code>(1, 300, 300, 3)</code> with a dtype of <code>uint8</code>, which is compatible with what the model expects.</p>
<p>Using the input_data, let‚Äôs run the interpreter, measure the latency, and get the output:</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>start_time <span class="op">=</span> time.time()</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>interpreter.set_tensor(input_details[<span class="dv">0</span>][<span class="st">'index'</span>], input_data)</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>interpreter.invoke()</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>end_time <span class="op">=</span> time.time()</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>inference_time <span class="op">=</span> (end_time <span class="op">-</span> start_time) <span class="op">*</span> <span class="dv">1000</span>  <span class="co"># Convert to milliseconds</span></span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span> (<span class="st">"Inference time: </span><span class="sc">{:.1f}</span><span class="st">ms"</span>.<span class="bu">format</span>(inference_time))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>With a latency of around 800ms, we can get 4 distinct outputs:</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>boxes <span class="op">=</span> interpreter.get_tensor(output_details[<span class="dv">0</span>][<span class="st">'index'</span>])[<span class="dv">0</span>] </span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>classes <span class="op">=</span> interpreter.get_tensor(output_details[<span class="dv">1</span>][<span class="st">'index'</span>])[<span class="dv">0</span>]  </span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>scores <span class="op">=</span> interpreter.get_tensor(output_details[<span class="dv">2</span>][<span class="st">'index'</span>])[<span class="dv">0</span>]   </span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>num_detections <span class="op">=</span> <span class="bu">int</span>(interpreter.get_tensor(output_details[<span class="dv">3</span>][<span class="st">'index'</span>])[<span class="dv">0</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>On a quick inspection, we can see that the model detected 2 objects with a score over 0.5:</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(num_detections):</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> scores[i] <span class="op">&gt;</span> <span class="fl">0.5</span>:  <span class="co"># Confidence threshold</span></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"Object </span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">:"</span>)</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"  Bounding Box: </span><span class="sc">{</span>boxes[i]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"  Confidence: </span><span class="sc">{</span>scores[i]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"  Class: </span><span class="sc">{</span>classes[i]<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><a href="images/png/infer-mobv1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-7"><img src="images/png/infer-mobv1.png" class="img-fluid"></a></p>
<p>And we can also visualize the results:</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">8</span>))</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>plt.imshow(orig_img)</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(num_detections):</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> scores[i] <span class="op">&gt;</span> <span class="fl">0.5</span>:  <span class="co"># Adjust threshold as needed</span></span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>        ymin, xmin, ymax, xmax <span class="op">=</span> boxes[i]</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>        (left, right, top, bottom) <span class="op">=</span> (xmin <span class="op">*</span> orig_img.width, </span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>                                      xmax <span class="op">*</span> orig_img.width, </span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a>                                      ymin <span class="op">*</span> orig_img.height, </span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a>                                      ymax <span class="op">*</span> orig_img.height)</span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a>        rect <span class="op">=</span> plt.Rectangle((left, top), right<span class="op">-</span>left, bottom<span class="op">-</span>top, </span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a>                             fill<span class="op">=</span><span class="va">False</span>, color<span class="op">=</span><span class="st">'red'</span>, linewidth<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a>        plt.gca().add_patch(rect)</span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a>        class_id <span class="op">=</span> <span class="bu">int</span>(classes[i])</span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a>        class_name <span class="op">=</span> labels[class_id]</span>
<span id="cb11-15"><a href="#cb11-15" aria-hidden="true" tabindex="-1"></a>        plt.text(left, top<span class="op">-</span><span class="dv">10</span>, <span class="ss">f'</span><span class="sc">{</span>class_name<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>scores[i]<span class="sc">:.2f}</span><span class="ss">'</span>, </span>
<span id="cb11-16"><a href="#cb11-16" aria-hidden="true" tabindex="-1"></a>                 color<span class="op">=</span><span class="st">'red'</span>, fontsize<span class="op">=</span><span class="dv">12</span>, backgroundcolor<span class="op">=</span><span class="st">'white'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><a href="images/png/visual_inf.png" class="lightbox" data-gallery="quarto-lightbox-gallery-8"><img src="images/png/visual_inf.png" class="img-fluid"></a></p>
</section>
<section id="efficientdet" class="level3">
<h3 class="anchored" data-anchor-id="efficientdet">EfficientDet</h3>
<p>EfficientDet is not technically an SSD (Single Shot Detector) model, but it shares some similarities and builds upon ideas from SSD and other object detection architectures:</p>
<ol type="1">
<li>EfficientDet:
<ul>
<li>Developed by Google researchers in 2019</li>
<li>Uses EfficientNet as the backbone network</li>
<li>Employs a novel bi-directional feature pyramid network (BiFPN)</li>
<li>It uses compound scaling to scale the backbone network and the object detection components efficiently.</li>
</ul></li>
<li>Similarities to SSD:
<ul>
<li>Both are single-stage detectors, meaning they perform object localization and classification in a single forward pass.</li>
<li>Both use multi-scale feature maps to detect objects at different scales.</li>
</ul></li>
<li>Key differences:
<ul>
<li>Backbone: SSD typically uses VGG or MobileNet, while EfficientDet uses EfficientNet.</li>
<li>Feature fusion: SSD uses a simple feature pyramid, while EfficientDet uses the more advanced BiFPN.</li>
<li>Scaling method: EfficientDet introduces compound scaling for all components of the network</li>
</ul></li>
<li>Advantages of EfficientDet:
<ul>
<li>Generally achieves better accuracy-efficiency trade-offs than SSD and many other object detection models.</li>
<li>More flexible scaling allows for a family of models with different size-performance trade-offs.</li>
</ul></li>
</ol>
<p>While EfficientDet is not an SSD model, it can be seen as an evolution of single-stage detection architectures, incorporating more advanced techniques to improve efficiency and accuracy. When using EfficientDet, we can expect similar output structures to SSD (e.g., bounding boxes and class scores).</p>
<blockquote class="blockquote">
<p>On GitHub, you can find another <a href="https://github.com/Mjrovai/EdgeML-with-Raspberry-Pi/blob/main/OBJ_DETEC/notebooks/SSD_EfficientDet.ipynb">notebook</a> exploring the EfficientDet model that we did with SSD MobileNet.</p>
</blockquote>
</section>
</section>
<section id="object-detection-project" class="level2">
<h2 class="anchored" data-anchor-id="object-detection-project">Object Detection Project</h2>
<p>Now, we will develop a complete Image Classification project from data collection to training and deployment. As we did with the Image Classification project, the trained and converted model will be used for inference.</p>
<p>We will use the same dataset to train 3 models: SSD-MobileNet V2, FOMO, and YOLO.</p>
<section id="the-goal" class="level3">
<h3 class="anchored" data-anchor-id="the-goal">The Goal</h3>
<p>All Machine Learning projects need to start with a goal. Let‚Äôs assume we are in an industrial facility and must sort and count <strong>wheels</strong> and special <strong>boxes</strong>.</p>
<p><a href="images/jpeg/proj_goal.jpg" class="lightbox" data-gallery="quarto-lightbox-gallery-9"><img src="images/jpeg/proj_goal.jpg" class="img-fluid"></a></p>
<p>In other words, we should perform a multi-label classification, where each image can have three classes:</p>
<ul>
<li><p>Background (no objects)</p></li>
<li><p>Box</p></li>
<li><p>Wheel</p></li>
</ul>
</section>
<section id="raw-data-collection" class="level3">
<h3 class="anchored" data-anchor-id="raw-data-collection">Raw Data Collection</h3>
<p>Once we have defined our Machine Learning project goal, the next and most crucial step is collecting the dataset. We can use a phone, the Raspi, or a mix to create the raw dataset (with no labels). Let‚Äôs use the simple web app on our Raspberry Pi to view the <code>QVGA (320 x 240)</code> captured images in a browser.</p>
<p>From GitHub, get the Python script <a href="https://github.com/Mjrovai/EdgeML-with-Raspberry-Pi/blob/main/IMG_CLASS/python_scripts/get_img_data.py">get_img_data.py</a> and open it in the terminal:</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="ex">python3</span> get_img_data.py </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Access the web interface:</p>
<ul>
<li>On the Raspberry Pi itself (if you have a GUI): Open a web browser and go to <code>http://localhost:5000</code></li>
<li>From another device on the same network: Open a web browser and go to <code>http://&lt;raspberry_pi_ip&gt;:5000</code> (Replace <code>&lt;raspberry_pi_ip&gt;</code> with your Raspberry Pi‚Äôs IP address). For example: <code>http://192.168.4.210:5000/</code></li>
</ul>
<p><img src="images/png/app.png" class="img-fluid">The Python script creates a web-based interface for capturing and organizing image datasets using a Raspberry Pi and its camera. It‚Äôs handy for machine learning projects that require labeled image data or not, as in our case here.</p>
<p>Access the web interface from a browser, enter a generic label for the images you want to capture, and press <code>Start Capture</code>.</p>
<p><a href="images/png/cap-img.png" class="lightbox" data-gallery="quarto-lightbox-gallery-10"><img src="images/png/cap-img.png" class="img-fluid"></a></p>
<blockquote class="blockquote">
<p>Note that the images to be captured will have multiple labels that should be defined later.</p>
</blockquote>
<p>Use the live preview to position the camera and click <code>Capture Image</code> to save images under the current label (in this case, <code>box-wheel</code>.</p>
<p><a href="images/png/img_cap-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-11"><img src="images/png/img_cap-1.png" class="img-fluid"></a></p>
<p>When we have enough images, we can press <code>Stop Capture</code>. The captured images are saved on the folder dataset/box-wheel:</p>
<p><a href="images/png/dataset.png" class="lightbox" data-gallery="quarto-lightbox-gallery-12"><img src="images/png/dataset.png" class="img-fluid"></a></p>
<blockquote class="blockquote">
<p>Get around 60 images. Try to capture different angles, backgrounds, and light conditions. Filezilla can transfer the created raw dataset to your main computer.</p>
</blockquote>
</section>
<section id="labeling-data" class="level3">
<h3 class="anchored" data-anchor-id="labeling-data">Labeling Data</h3>
<p>The next step in an Object Detect project is to create a labeled dataset. We should label the raw dataset images, creating bounding boxes around each picture‚Äôs objects (box and wheel). We can use labeling tools like <a href="https://pypi.org/project/labelImg/">LabelImg,</a> <a href="https://www.cvat.ai/">CVAT,</a> <a href="https://roboflow.com/annotate">Roboflow,</a> or even the <a href="https://edgeimpulse.com/">Edge Impulse Studio.</a> Once we have explored the Edge Impulse tool in other labs, let‚Äôs use Roboflow here.</p>
<blockquote class="blockquote">
<p>We are using Roboflow (free version) here for two main reasons. 1) We can have auto-labeler, and 2) The annotated dataset is available in several formats and can be used both on Edge Impulse Studio (we will use it for MobileNet V2 and FOMO train) and on CoLab (YOLOv8 train), for example. Having the annotated dataset on Edge Impulse (Free account), it is not possible to use it for training on other platforms.</p>
</blockquote>
<p>We should upload the raw dataset to <a href="https://roboflow.com/">Roboflow.</a> Create a free account there and start a new project, for example, (‚Äúbox-versus-wheel‚Äù).</p>
<p><a href="images/png/create-project-rf.png" class="lightbox" data-gallery="quarto-lightbox-gallery-13"><img src="images/png/create-project-rf.png" class="img-fluid"></a></p>
<blockquote class="blockquote">
<p>We will not enter in deep details about the Roboflow process once many tutorials are available.</p>
</blockquote>
<section id="annotate" class="level4">
<h4 class="anchored" data-anchor-id="annotate">Annotate</h4>
<p>Once the project is created and the dataset is uploaded, you should make the annotations using the ‚ÄúAuto-Label‚Äù Tool. Note that you can also upload images with only a background, which should be saved w/o any annotations.</p>
<p><a href="images/png/annotation.png" class="lightbox" data-gallery="quarto-lightbox-gallery-14"><img src="images/png/annotation.png" class="img-fluid"></a></p>
<p>Once all images are annotated, you should split them into training, validation, and testing.</p>
<p><a href="images/png/dataset_rf.png" class="lightbox" data-gallery="quarto-lightbox-gallery-15"><img src="images/png/dataset_rf.png" class="img-fluid"></a></p>
</section>
<section id="data-pre-processing" class="level4">
<h4 class="anchored" data-anchor-id="data-pre-processing">Data Pre-Processing</h4>
<p>The last step with the dataset is preprocessing to generate a final version for training. Let‚Äôs resize all images to 320x320 and generate augmented versions of each image (augmentation) to create new training examples from which our model can learn.</p>
<p>For augmentation, we will rotate the images (+/-15<sup>o</sup>), crop, and vary the brightness and exposure.</p>
<p><a href="images/png/pre-proc.png" class="lightbox" data-gallery="quarto-lightbox-gallery-16"><img src="images/png/pre-proc.png" class="img-fluid"></a></p>
<p>At the end of the process, we will have 153 images.</p>
<p><a href="images/png/final-dataset.png" class="lightbox" data-gallery="quarto-lightbox-gallery-17"><img src="images/png/final-dataset.png" class="img-fluid"></a></p>
<p>Now, you should export the annotated dataset in a format that Edge Impulse, Ultralitics, and other frameworks/tools understand, for example, <code>YOLOv8</code>. Let‚Äôs download a zipped version of the dataset to our desktop.</p>
<p><a href="images/png/download-dataset.png" class="lightbox" data-gallery="quarto-lightbox-gallery-18"><img src="images/png/download-dataset.png" class="img-fluid"></a></p>
<p>Here, it is possible to review how the dataset was structured</p>
<p><a href="images/png/dataset-struct.png" class="lightbox" data-gallery="quarto-lightbox-gallery-19"><img src="images/png/dataset-struct.png" class="img-fluid"></a></p>
<p>There are 3 separate folders, one for each split (<code>train</code>/<code>test</code>/<code>valid</code>). For each of them, there are 2 subfolders, <code>images</code>, and <code>labels</code>. The pictures are stored as <strong>image_id.jpg</strong> and <strong>images_id.txt</strong>, where ‚Äúimage_id‚Äù is unique for every picture.</p>
<p>The labels file format will be <code>class_id</code> <code>bounding box coordinates</code>, where in our case, class_id will be <code>0</code> for <code>box</code> and <code>1</code> for <code>wheel</code>. The numerical id (o, 1, 2‚Ä¶) will follow the alphabetical order of the class name.</p>
<p>The <code>data.yaml</code> file has info about the dataset as the classes‚Äô names (<code>names: ['box', 'wheel']</code>) following the YOLO format.</p>
<p>And that‚Äôs it! We are ready to start training using the Edge Impulse Studio (as we will do in the following step), Ultralytics (as we will when discussing YOLO), or even training from scratch on CoLab (as we did with the Cifar-10 dataset on the Image Classification lab).</p>
<blockquote class="blockquote">
<p>The pre-processed dataset can be found at the <a href="https://universe.roboflow.com/marcelo-rovai-riila/box-versus-wheel-auto-dataset">Roboflow site</a>, or here: <a href="https://universe.roboflow.com/marcelo-rovai-riila/box-versus-wheel-auto-dataset"> <img src="https://app.roboflow.com/images/download-dataset-badge.svg"> </a></p>
</blockquote>
</section>
</section>
</section>
<section id="training-an-ssd-mobilenet-model-on-edge-impulse-studio" class="level2">
<h2 class="anchored" data-anchor-id="training-an-ssd-mobilenet-model-on-edge-impulse-studio">Training an SSD MobileNet Model on Edge Impulse Studio</h2>
<p>Go to <a href="https://www.edgeimpulse.com/">Edge Impulse Studio,</a> enter your credentials at <strong>Login</strong> (or create an account), and start a new project.</p>
<blockquote class="blockquote">
<p>Here, you can clone the project developed for this hands-on lab: <a href="https://studio.edgeimpulse.com/public/515477/live">Raspi - Object Detection</a>.</p>
</blockquote>
<p>On the Project <code>Dashboard</code> tab, go down and on <strong>Project info,</strong> and for Labeling method select <code>Bounding boxes (object detection)</code></p>
<section id="uploading-the-annotated-data" class="level3">
<h3 class="anchored" data-anchor-id="uploading-the-annotated-data">Uploading the annotated data</h3>
<p>On Studio, go to the <code>Data acquisition</code> tab, and on the <code>UPLOAD DATA</code> section, upload from your computer the raw dataset.</p>
<p>We can use the option <code>Select a folder</code>, choosing, for example, the folder <code>train</code> in your computer, which contains two sub-folders, <code>images</code>, and <code>labels</code>. Select the <code>Image label format</code>, ‚ÄúYOLO TXT‚Äù, upload into the caegory <code>Training</code>, and press <code>Upload data</code>.</p>
<p><a href="images/png/upload-data.png" class="lightbox" data-gallery="quarto-lightbox-gallery-20"><img src="images/png/upload-data.png" class="img-fluid"></a></p>
<p>Repeat the process for the test data (upload both folders, test, and validation). At the end of the upload process, you should end with the annotated dataset of 153 images split in the train/test (84%/16%).</p>
<blockquote class="blockquote">
<p>Note that labels will be stored at the labels files <code>0</code> and <code>1</code> , which are equivalent to <code>box</code> and <code>wheel</code>.</p>
</blockquote>
<p><a href="images/png/ei-dataset.png" class="lightbox" data-gallery="quarto-lightbox-gallery-21"><img src="images/png/ei-dataset.png" class="img-fluid"></a></p>
</section>
<section id="the-impulse-design" class="level3">
<h3 class="anchored" data-anchor-id="the-impulse-design">The Impulse Design</h3>
<p>The first thing to define when we enter the <code>Create impulse</code> step is to describe the target device for deployment. A pop-up window will appear. We will select Raspberry 4, an intermediary device between the Raspi-Zero and the Raspi-5.</p>
<blockquote class="blockquote">
<p>This choice will not interfere with the training; it will only give us an idea about the latency of the model on that specific target.</p>
</blockquote>
<p><a href="images/png/target-device.png" class="lightbox" data-gallery="quarto-lightbox-gallery-22"><img src="images/png/target-device.png" class="img-fluid"></a></p>
<p>In this phase, you should define how to:</p>
<ul>
<li><p><strong>Pre-processing</strong> consists of resizing the individual images. In our case, the images were pre-processed on Roboflow, to <code>320x320</code> , so let‚Äôs keep it. The resize will not matter here because the images are already squared. If you upload a rectangular image, squash it (squared form, without cropping). Afterward, you could define if the images are converted from RGB to Grayscale or not.</p></li>
<li><p><strong>Design a Model,</strong> in this case, ‚ÄúObject Detection.‚Äù</p></li>
</ul>
<p><a href="images/png/impulse-design.png" class="lightbox" data-gallery="quarto-lightbox-gallery-23"><img src="images/png/impulse-design.png" class="img-fluid"></a></p>
</section>
<section id="preprocessing-all-dataset" class="level3">
<h3 class="anchored" data-anchor-id="preprocessing-all-dataset">Preprocessing all dataset</h3>
<p>In the section <code>Image</code>, select <strong>Color depth</strong> as <code>RGB</code>, and press <code>Save parameters</code>.</p>
<p><a href="images/png/ei-image-pre.png" class="lightbox" data-gallery="quarto-lightbox-gallery-24"><img src="images/png/ei-image-pre.png" class="img-fluid"></a></p>
<p>The Studio moves automatically to the next section, <code>Generate features</code>, where all samples will be pre-processed, resulting in 480 objects: 207 boxes and 273 wheels.</p>
<p><a href="images/png/ei-features.png" class="lightbox" data-gallery="quarto-lightbox-gallery-25"><img src="images/png/ei-features.png" class="img-fluid"></a></p>
<p>The feature explorer shows that all samples evidence a good separation after the feature generation.</p>
</section>
<section id="model-design-training-and-test" class="level3">
<h3 class="anchored" data-anchor-id="model-design-training-and-test">Model Design, Training, and Test</h3>
<p>For training, we should select a pre-trained model. Let‚Äôs use the <strong>MobileNetV2 SSD FPN-Lite (320x320 only)</strong> . It is a pre-trained object detection model designed to locate up to 10 objects within an image, outputting a bounding box for each object detected. The model is around 3.7MB in size. It supports an RGB input at 320x320px.</p>
<p>Regarding the training hyper-parameters, the model will be trained with:</p>
<ul>
<li>Epochs: 25</li>
<li>Batch size: 32</li>
<li>Learning Rate: 0.15.</li>
</ul>
<p>For validation during training, 20% of the dataset (<em>validation_dataset</em>) will be spared.</p>
<p><a href="images/png/ei-train-result.png" class="lightbox" data-gallery="quarto-lightbox-gallery-26"><img src="images/png/ei-train-result.png" class="img-fluid"></a></p>
<p>As a result, the model ends with an overall precision score (based on COCO mAP) of 88.8%, higher than the result when using the test data (83.3%).</p>
</section>
<section id="deploying-the-model" class="level3">
<h3 class="anchored" data-anchor-id="deploying-the-model">Deploying the model</h3>
<p>We have two ways to deploy our model:</p>
<ul>
<li><strong>TFLite model</strong>, which lets deploy the trained model as <code>.tflite</code> for the Raspi to run it using Python.</li>
<li><strong>Linux (AARCH64)</strong>, a binary for Linux (AARCH64), implements the Edge Impulse Linux protocol, which lets us run our models on any Linux-based development board, with SDKs for Python, for example. See the documentation for more information and <a href="https://docs.edgeimpulse.com/docs/edge-impulse-for-linux">setup instructions</a>.</li>
</ul>
<p>Let‚Äôs deploy the <strong>TFLite model</strong>. On the <code>Dashboard</code> tab, go to Transfer learning model (int8 quantized) and click on the download icon:</p>
<p><a href="images/png/ei-deploy-int8.png" class="lightbox" data-gallery="quarto-lightbox-gallery-27"><img src="images/png/ei-deploy-int8.png" class="img-fluid"></a></p>
<p>Transfer the model from your computer to the Raspi folder<code>./models</code> and capture or get some images for inference and save them in the folder <code>./images</code>.</p>
</section>
<section id="inference-and-post-processing-1" class="level3">
<h3 class="anchored" data-anchor-id="inference-and-post-processing-1">Inference and Post-Processing</h3>
<p>The inference can be made as discussed in the <em>Pre-Trained Object Detection Models Overview</em>. Let‚Äôs start a new <a href="https://github.com/Mjrovai/EdgeML-with-Raspberry-Pi/blob/main/OBJ_DETEC/notebooks/EI-SSD-MobileNetV2.ipynb">notebook</a> to follow all the steps to detect cubes and wheels on an image.</p>
<p>Import the needed libraries:</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> time</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.patches <span class="im">as</span> patches</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> PIL <span class="im">import</span> Image</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> tflite_runtime.interpreter <span class="im">as</span> tflite</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Define the model path and labels:</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>model_path <span class="op">=</span> <span class="st">"./models/ei-raspi-object-detection-SSD-MobileNetv2-320x0320-</span><span class="ch">\</span></span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a><span class="st">int8.lite"</span></span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>labels <span class="op">=</span> [<span class="st">'box'</span>, <span class="st">'wheel'</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<blockquote class="blockquote">
<p>Remember that the model will output the class ID as values (0 and 1), following an alphabetic order regarding the class names.</p>
</blockquote>
<p>Load the model, allocate the tensors, and get the input and output tensor details:</p>
<div class="sourceCode" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Load the TFLite model</span></span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>interpreter <span class="op">=</span> tflite.Interpreter(model_path<span class="op">=</span>model_path)</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>interpreter.allocate_tensors()</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Get input and output tensors</span></span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a>input_details <span class="op">=</span> interpreter.get_input_details()</span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a>output_details <span class="op">=</span> interpreter.get_output_details()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>One crucial difference to note is that the <code>dtype</code> of the input details of the model is now <code>int8</code>, which means that the input values go from -128 to +127, while each pixel of our raw image goes from 0 to 256. This means that we should pre-process the image to match it. We can check here:</p>
<div class="sourceCode" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>input_dtype <span class="op">=</span> input_details[<span class="dv">0</span>][<span class="st">'dtype'</span>]</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>input_dtype</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre><code>numpy.int8</code></pre>
<p>So, let‚Äôs open the image and show it:</p>
<div class="sourceCode" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Load the image</span></span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>img_path <span class="op">=</span> <span class="st">"./images/box_2_wheel_2.jpg"</span></span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>orig_img <span class="op">=</span> Image.<span class="bu">open</span>(img_path)</span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Display the image</span></span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">6</span>, <span class="dv">6</span>))</span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a>plt.imshow(orig_img)</span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Original Image"</span>)</span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><a href="images/png/orig-img.png" class="lightbox" data-gallery="quarto-lightbox-gallery-28"><img src="images/png/orig-img.png" class="img-fluid"></a></p>
<p>And perform the pre-processing:</p>
<div class="sourceCode" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>scale, zero_point <span class="op">=</span> input_details[<span class="dv">0</span>][<span class="st">'quantization'</span>]</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>img <span class="op">=</span> orig_img.resize((input_details[<span class="dv">0</span>][<span class="st">'shape'</span>][<span class="dv">1</span>], </span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>                  input_details[<span class="dv">0</span>][<span class="st">'shape'</span>][<span class="dv">2</span>]))</span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a>img_array <span class="op">=</span> np.array(img, dtype<span class="op">=</span>np.float32) <span class="op">/</span> <span class="fl">255.0</span></span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a>img_array <span class="op">=</span> (img_array <span class="op">/</span> scale <span class="op">+</span> zero_point).clip(<span class="op">-</span><span class="dv">128</span>, <span class="dv">127</span>).astype(np.int8)</span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a>input_data <span class="op">=</span> np.expand_dims(img_array, axis<span class="op">=</span><span class="dv">0</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Checking the input data, we can verify that the input tensor is compatible with what is expected by the model:</p>
<div class="sourceCode" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a>input_data.shape, input_data.dtype</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre><code>((1, 320, 320, 3), dtype('int8'))</code></pre>
<p>Now, it is time to perform the inference. Let‚Äôs also calculate the latency of the model:</p>
<div class="sourceCode" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Inference on Raspi-Zero</span></span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>start_time <span class="op">=</span> time.time()</span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a>interpreter.set_tensor(input_details[<span class="dv">0</span>][<span class="st">'index'</span>], input_data)</span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a>interpreter.invoke()</span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a>end_time <span class="op">=</span> time.time()</span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a>inference_time <span class="op">=</span> (end_time <span class="op">-</span> start_time) <span class="op">*</span> <span class="dv">1000</span>  <span class="co"># Convert to milliseconds</span></span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span> (<span class="st">"Inference time: </span><span class="sc">{:.1f}</span><span class="st">ms"</span>.<span class="bu">format</span>(inference_time))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>The model will take around 600ms to perform the inference in the Raspi-Zero, which is around 5 times longer than a Raspi-5.</p>
<p>Now, we can get the output classes of objects detected, its bounding boxes coordinates, and probabilities.</p>
<div class="sourceCode" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a>boxes <span class="op">=</span> interpreter.get_tensor(output_details[<span class="dv">1</span>][<span class="st">'index'</span>])[<span class="dv">0</span>]  </span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>classes <span class="op">=</span> interpreter.get_tensor(output_details[<span class="dv">3</span>][<span class="st">'index'</span>])[<span class="dv">0</span>]  </span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a>scores <span class="op">=</span> interpreter.get_tensor(output_details[<span class="dv">0</span>][<span class="st">'index'</span>])[<span class="dv">0</span>]        </span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a>num_detections <span class="op">=</span> <span class="bu">int</span>(interpreter.get_tensor(output_details[<span class="dv">2</span>][<span class="st">'index'</span>])[<span class="dv">0</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(num_detections):</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> scores[i] <span class="op">&gt;</span> <span class="fl">0.5</span>:  <span class="co"># Confidence threshold</span></span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"Object </span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">:"</span>)</span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"  Bounding Box: </span><span class="sc">{</span>boxes[i]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"  Confidence: </span><span class="sc">{</span>scores[i]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"  Class: </span><span class="sc">{</span>classes[i]<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><a href="images/png/infer-text.png" class="lightbox" data-gallery="quarto-lightbox-gallery-29"><img src="images/png/infer-text.png" class="img-fluid"></a></p>
<p>From the results, we can see that 4 objects were detected: two with class ID 0 (<code>box</code>)and two with class ID 1 (<code>wheel</code>), what is correct!</p>
<p>Let‚Äôs visualize the result for a <code>threshold</code> of 0.5</p>
<div class="sourceCode" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a>threshold <span class="op">=</span> <span class="fl">0.5</span></span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">6</span>,<span class="dv">6</span>))</span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a>plt.imshow(orig_img)</span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(num_detections):</span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> scores[i] <span class="op">&gt;</span> threshold:  </span>
<span id="cb25-6"><a href="#cb25-6" aria-hidden="true" tabindex="-1"></a>        ymin, xmin, ymax, xmax <span class="op">=</span> boxes[i]</span>
<span id="cb25-7"><a href="#cb25-7" aria-hidden="true" tabindex="-1"></a>        (left, right, top, bottom) <span class="op">=</span> (xmin <span class="op">*</span> orig_img.width, </span>
<span id="cb25-8"><a href="#cb25-8" aria-hidden="true" tabindex="-1"></a>                                      xmax <span class="op">*</span> orig_img.width, </span>
<span id="cb25-9"><a href="#cb25-9" aria-hidden="true" tabindex="-1"></a>                                      ymin <span class="op">*</span> orig_img.height, </span>
<span id="cb25-10"><a href="#cb25-10" aria-hidden="true" tabindex="-1"></a>                                      ymax <span class="op">*</span> orig_img.height)</span>
<span id="cb25-11"><a href="#cb25-11" aria-hidden="true" tabindex="-1"></a>        rect <span class="op">=</span> plt.Rectangle((left, top), right<span class="op">-</span>left, bottom<span class="op">-</span>top, </span>
<span id="cb25-12"><a href="#cb25-12" aria-hidden="true" tabindex="-1"></a>                             fill<span class="op">=</span><span class="va">False</span>, color<span class="op">=</span><span class="st">'red'</span>, linewidth<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb25-13"><a href="#cb25-13" aria-hidden="true" tabindex="-1"></a>        plt.gca().add_patch(rect)</span>
<span id="cb25-14"><a href="#cb25-14" aria-hidden="true" tabindex="-1"></a>        class_id <span class="op">=</span> <span class="bu">int</span>(classes[i])</span>
<span id="cb25-15"><a href="#cb25-15" aria-hidden="true" tabindex="-1"></a>        class_name <span class="op">=</span> labels[class_id]</span>
<span id="cb25-16"><a href="#cb25-16" aria-hidden="true" tabindex="-1"></a>        plt.text(left, top<span class="op">-</span><span class="dv">10</span>, <span class="ss">f'</span><span class="sc">{</span>class_name<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>scores[i]<span class="sc">:.2f}</span><span class="ss">'</span>, </span>
<span id="cb25-17"><a href="#cb25-17" aria-hidden="true" tabindex="-1"></a>                 color<span class="op">=</span><span class="st">'red'</span>, fontsize<span class="op">=</span><span class="dv">12</span>, backgroundcolor<span class="op">=</span><span class="st">'white'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><a href="images/png/infer-visual.png" class="lightbox" data-gallery="quarto-lightbox-gallery-30"><img src="images/png/infer-visual.png" class="img-fluid"></a></p>
<p>But what happens if we reduce the threshold to 0.3, for example?</p>
<p><a href="images/png/infer-mult.png" class="lightbox" data-gallery="quarto-lightbox-gallery-31"><img src="images/png/infer-mult.png" class="img-fluid"></a></p>
<p>We start to see false positives and <strong>multiple detections</strong>, where the model detects the same object multiple times with different confidence levels and slightly different bounding boxes.</p>
<p>Commonly, sometimes, we need to adjust the threshold to smaller values to capture all objects, avoiding false negatives, which would lead to multiple detections.</p>
<p>To improve the detection results, we should implement <strong>Non-Maximum Suppression (NMS</strong>), which helps eliminate overlapping bounding boxes and keeps only the most confident detection.</p>
<p>For that, let‚Äôs create a general function named <code>non_max_suppression()</code>, with the role of refining object detection results by eliminating redundant and overlapping bounding boxes. It achieves this by iteratively selecting the detection with the highest confidence score and removing other significantly overlapping detections based on an Intersection over Union (IoU) threshold.</p>
<div class="sourceCode" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> non_max_suppression(boxes, scores, threshold):</span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Convert to corner coordinates</span></span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a>    x1 <span class="op">=</span> boxes[:, <span class="dv">0</span>]</span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a>    y1 <span class="op">=</span> boxes[:, <span class="dv">1</span>]</span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a>    x2 <span class="op">=</span> boxes[:, <span class="dv">2</span>]</span>
<span id="cb26-6"><a href="#cb26-6" aria-hidden="true" tabindex="-1"></a>    y2 <span class="op">=</span> boxes[:, <span class="dv">3</span>]</span>
<span id="cb26-7"><a href="#cb26-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-8"><a href="#cb26-8" aria-hidden="true" tabindex="-1"></a>    areas <span class="op">=</span> (x2 <span class="op">-</span> x1 <span class="op">+</span> <span class="dv">1</span>) <span class="op">*</span> (y2 <span class="op">-</span> y1 <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb26-9"><a href="#cb26-9" aria-hidden="true" tabindex="-1"></a>    order <span class="op">=</span> scores.argsort()[::<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb26-10"><a href="#cb26-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-11"><a href="#cb26-11" aria-hidden="true" tabindex="-1"></a>    keep <span class="op">=</span> []</span>
<span id="cb26-12"><a href="#cb26-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">while</span> order.size <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb26-13"><a href="#cb26-13" aria-hidden="true" tabindex="-1"></a>        i <span class="op">=</span> order[<span class="dv">0</span>]</span>
<span id="cb26-14"><a href="#cb26-14" aria-hidden="true" tabindex="-1"></a>        keep.append(i)</span>
<span id="cb26-15"><a href="#cb26-15" aria-hidden="true" tabindex="-1"></a>        xx1 <span class="op">=</span> np.maximum(x1[i], x1[order[<span class="dv">1</span>:]])</span>
<span id="cb26-16"><a href="#cb26-16" aria-hidden="true" tabindex="-1"></a>        yy1 <span class="op">=</span> np.maximum(y1[i], y1[order[<span class="dv">1</span>:]])</span>
<span id="cb26-17"><a href="#cb26-17" aria-hidden="true" tabindex="-1"></a>        xx2 <span class="op">=</span> np.minimum(x2[i], x2[order[<span class="dv">1</span>:]])</span>
<span id="cb26-18"><a href="#cb26-18" aria-hidden="true" tabindex="-1"></a>        yy2 <span class="op">=</span> np.minimum(y2[i], y2[order[<span class="dv">1</span>:]])</span>
<span id="cb26-19"><a href="#cb26-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-20"><a href="#cb26-20" aria-hidden="true" tabindex="-1"></a>        w <span class="op">=</span> np.maximum(<span class="fl">0.0</span>, xx2 <span class="op">-</span> xx1 <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb26-21"><a href="#cb26-21" aria-hidden="true" tabindex="-1"></a>        h <span class="op">=</span> np.maximum(<span class="fl">0.0</span>, yy2 <span class="op">-</span> yy1 <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb26-22"><a href="#cb26-22" aria-hidden="true" tabindex="-1"></a>        inter <span class="op">=</span> w <span class="op">*</span> h</span>
<span id="cb26-23"><a href="#cb26-23" aria-hidden="true" tabindex="-1"></a>        ovr <span class="op">=</span> inter <span class="op">/</span> (areas[i] <span class="op">+</span> areas[order[<span class="dv">1</span>:]] <span class="op">-</span> inter)</span>
<span id="cb26-24"><a href="#cb26-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-25"><a href="#cb26-25" aria-hidden="true" tabindex="-1"></a>        inds <span class="op">=</span> np.where(ovr <span class="op">&lt;=</span> threshold)[<span class="dv">0</span>]</span>
<span id="cb26-26"><a href="#cb26-26" aria-hidden="true" tabindex="-1"></a>        order <span class="op">=</span> order[inds <span class="op">+</span> <span class="dv">1</span>]</span>
<span id="cb26-27"><a href="#cb26-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-28"><a href="#cb26-28" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> keep</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>How it works:</p>
<ol type="1">
<li><p>Sorting: It starts by sorting all detections by their confidence scores, highest to lowest.</p></li>
<li><p>Selection: It selects the highest-scoring box and adds it to the final list of detections.</p></li>
<li><p>Comparison: This selected box is compared with all remaining lower-scoring boxes.</p></li>
<li><p>Elimination: Any box that overlaps significantly (above the IoU threshold) with the selected box is eliminated.</p></li>
<li><p>Iteration: This process repeats with the next highest-scoring box until all boxes are processed.</p></li>
</ol>
<p>Now, we can define a more precise visualization function that will take into consideration an IoU threshold, detecting only the objects that were selected by the <code>non_max_suppression</code> function:</p>
<div class="sourceCode" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> visualize_detections(image, boxes, classes, scores, </span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a>                         labels, threshold, iou_threshold):</span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="bu">isinstance</span>(image, Image.Image):</span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a>        image_np <span class="op">=</span> np.array(image)</span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb27-6"><a href="#cb27-6" aria-hidden="true" tabindex="-1"></a>        image_np <span class="op">=</span> image</span>
<span id="cb27-7"><a href="#cb27-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-8"><a href="#cb27-8" aria-hidden="true" tabindex="-1"></a>    height, width <span class="op">=</span> image_np.shape[:<span class="dv">2</span>]</span>
<span id="cb27-9"><a href="#cb27-9" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb27-10"><a href="#cb27-10" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Convert normalized coordinates to pixel coordinates</span></span>
<span id="cb27-11"><a href="#cb27-11" aria-hidden="true" tabindex="-1"></a>    boxes_pixel <span class="op">=</span> boxes <span class="op">*</span> np.array([height, width, height, width])</span>
<span id="cb27-12"><a href="#cb27-12" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb27-13"><a href="#cb27-13" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Apply NMS</span></span>
<span id="cb27-14"><a href="#cb27-14" aria-hidden="true" tabindex="-1"></a>    keep <span class="op">=</span> non_max_suppression(boxes_pixel, scores, iou_threshold)</span>
<span id="cb27-15"><a href="#cb27-15" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb27-16"><a href="#cb27-16" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Set the figure size to 12x8 inches</span></span>
<span id="cb27-17"><a href="#cb27-17" aria-hidden="true" tabindex="-1"></a>    fig, ax <span class="op">=</span> plt.subplots(<span class="dv">1</span>, figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">8</span>))</span>
<span id="cb27-18"><a href="#cb27-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-19"><a href="#cb27-19" aria-hidden="true" tabindex="-1"></a>    ax.imshow(image_np)</span>
<span id="cb27-20"><a href="#cb27-20" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb27-21"><a href="#cb27-21" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> keep:</span>
<span id="cb27-22"><a href="#cb27-22" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> scores[i] <span class="op">&gt;</span> threshold:</span>
<span id="cb27-23"><a href="#cb27-23" aria-hidden="true" tabindex="-1"></a>            ymin, xmin, ymax, xmax <span class="op">=</span> boxes[i]</span>
<span id="cb27-24"><a href="#cb27-24" aria-hidden="true" tabindex="-1"></a>            rect <span class="op">=</span> patches.Rectangle((xmin <span class="op">*</span> width, ymin <span class="op">*</span> height),</span>
<span id="cb27-25"><a href="#cb27-25" aria-hidden="true" tabindex="-1"></a>                                     (xmax <span class="op">-</span> xmin) <span class="op">*</span> width,</span>
<span id="cb27-26"><a href="#cb27-26" aria-hidden="true" tabindex="-1"></a>                                     (ymax <span class="op">-</span> ymin) <span class="op">*</span> height,</span>
<span id="cb27-27"><a href="#cb27-27" aria-hidden="true" tabindex="-1"></a>                                     linewidth<span class="op">=</span><span class="dv">2</span>, edgecolor<span class="op">=</span><span class="st">'r'</span>, facecolor<span class="op">=</span><span class="st">'none'</span>)</span>
<span id="cb27-28"><a href="#cb27-28" aria-hidden="true" tabindex="-1"></a>            ax.add_patch(rect)</span>
<span id="cb27-29"><a href="#cb27-29" aria-hidden="true" tabindex="-1"></a>            class_name <span class="op">=</span> labels[<span class="bu">int</span>(classes[i])]</span>
<span id="cb27-30"><a href="#cb27-30" aria-hidden="true" tabindex="-1"></a>            ax.text(xmin <span class="op">*</span> width, ymin <span class="op">*</span> height <span class="op">-</span> <span class="dv">10</span>,</span>
<span id="cb27-31"><a href="#cb27-31" aria-hidden="true" tabindex="-1"></a>                    <span class="ss">f'</span><span class="sc">{</span>class_name<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>scores[i]<span class="sc">:.2f}</span><span class="ss">'</span>, color<span class="op">=</span><span class="st">'red'</span>,</span>
<span id="cb27-32"><a href="#cb27-32" aria-hidden="true" tabindex="-1"></a>                    fontsize<span class="op">=</span><span class="dv">12</span>, backgroundcolor<span class="op">=</span><span class="st">'white'</span>)</span>
<span id="cb27-33"><a href="#cb27-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-34"><a href="#cb27-34" aria-hidden="true" tabindex="-1"></a>    plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Now we can create a function that will call the others, performing inference on any image:</p>
<div class="sourceCode" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> detect_objects(img_path, conf<span class="op">=</span><span class="fl">0.5</span>, iou<span class="op">=</span><span class="fl">0.5</span>):</span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a>    orig_img <span class="op">=</span> Image.<span class="bu">open</span>(img_path)</span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a>    scale, zero_point <span class="op">=</span> input_details[<span class="dv">0</span>][<span class="st">'quantization'</span>]</span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a>    img <span class="op">=</span> orig_img.resize((input_details[<span class="dv">0</span>][<span class="st">'shape'</span>][<span class="dv">1</span>], </span>
<span id="cb28-5"><a href="#cb28-5" aria-hidden="true" tabindex="-1"></a>                      input_details[<span class="dv">0</span>][<span class="st">'shape'</span>][<span class="dv">2</span>]))</span>
<span id="cb28-6"><a href="#cb28-6" aria-hidden="true" tabindex="-1"></a>    img_array <span class="op">=</span> np.array(img, dtype<span class="op">=</span>np.float32) <span class="op">/</span> <span class="fl">255.0</span></span>
<span id="cb28-7"><a href="#cb28-7" aria-hidden="true" tabindex="-1"></a>    img_array <span class="op">=</span> (img_array <span class="op">/</span> scale <span class="op">+</span> zero_point).clip(<span class="op">-</span><span class="dv">128</span>, <span class="dv">127</span>).<span class="op">\</span></span>
<span id="cb28-8"><a href="#cb28-8" aria-hidden="true" tabindex="-1"></a>    astype(np.int8)</span>
<span id="cb28-9"><a href="#cb28-9" aria-hidden="true" tabindex="-1"></a>    input_data <span class="op">=</span> np.expand_dims(img_array, axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb28-10"><a href="#cb28-10" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb28-11"><a href="#cb28-11" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Inference on Raspi-Zero</span></span>
<span id="cb28-12"><a href="#cb28-12" aria-hidden="true" tabindex="-1"></a>    start_time <span class="op">=</span> time.time()</span>
<span id="cb28-13"><a href="#cb28-13" aria-hidden="true" tabindex="-1"></a>    interpreter.set_tensor(input_details[<span class="dv">0</span>][<span class="st">'index'</span>], input_data)</span>
<span id="cb28-14"><a href="#cb28-14" aria-hidden="true" tabindex="-1"></a>    interpreter.invoke()</span>
<span id="cb28-15"><a href="#cb28-15" aria-hidden="true" tabindex="-1"></a>    end_time <span class="op">=</span> time.time()</span>
<span id="cb28-16"><a href="#cb28-16" aria-hidden="true" tabindex="-1"></a>    inference_time <span class="op">=</span> (end_time <span class="op">-</span> start_time) <span class="op">*</span> <span class="dv">1000</span>  <span class="co"># Convert to ms</span></span>
<span id="cb28-17"><a href="#cb28-17" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span> (<span class="st">"Inference time: </span><span class="sc">{:.1f}</span><span class="st">ms"</span>.<span class="bu">format</span>(inference_time))</span>
<span id="cb28-18"><a href="#cb28-18" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb28-19"><a href="#cb28-19" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Extract the outputs</span></span>
<span id="cb28-20"><a href="#cb28-20" aria-hidden="true" tabindex="-1"></a>    boxes <span class="op">=</span> interpreter.get_tensor(output_details[<span class="dv">1</span>][<span class="st">'index'</span>])[<span class="dv">0</span>]  </span>
<span id="cb28-21"><a href="#cb28-21" aria-hidden="true" tabindex="-1"></a>    classes <span class="op">=</span> interpreter.get_tensor(output_details[<span class="dv">3</span>][<span class="st">'index'</span>])[<span class="dv">0</span>]  </span>
<span id="cb28-22"><a href="#cb28-22" aria-hidden="true" tabindex="-1"></a>    scores <span class="op">=</span> interpreter.get_tensor(output_details[<span class="dv">0</span>][<span class="st">'index'</span>])[<span class="dv">0</span>]        </span>
<span id="cb28-23"><a href="#cb28-23" aria-hidden="true" tabindex="-1"></a>    num_detections <span class="op">=</span> <span class="bu">int</span>(interpreter.get_tensor(output_details[<span class="dv">2</span>][<span class="st">'index'</span>])[<span class="dv">0</span>])</span>
<span id="cb28-24"><a href="#cb28-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-25"><a href="#cb28-25" aria-hidden="true" tabindex="-1"></a>    visualize_detections(orig_img, boxes, classes, scores, labels, </span>
<span id="cb28-26"><a href="#cb28-26" aria-hidden="true" tabindex="-1"></a>                         threshold<span class="op">=</span>conf, </span>
<span id="cb28-27"><a href="#cb28-27" aria-hidden="true" tabindex="-1"></a>                         iou_threshold<span class="op">=</span>iou)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Now, running the code, having the same image again with a confidence threshold of 0.3, but with a small IoU:</p>
<div class="sourceCode" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a>img_path <span class="op">=</span> <span class="st">"./images/box_2_wheel_2.jpg"</span></span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a>detect_objects(img_path, conf<span class="op">=</span><span class="fl">0.3</span>,iou<span class="op">=</span><span class="fl">0.05</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><a href="images/png/infer-iou.png" class="lightbox" data-gallery="quarto-lightbox-gallery-32"><img src="images/png/infer-iou.png" class="img-fluid"></a></p>
</section>
</section>
<section id="training-a-fomo-model-at-edge-impulse-studio" class="level2">
<h2 class="anchored" data-anchor-id="training-a-fomo-model-at-edge-impulse-studio">Training a FOMO Model at Edge Impulse Studio</h2>
<p>The inference with the SSD MobileNet model worked well, but the latency was significantly high. The inference varied from 0.5 to 1.3 seconds on a Raspi-Zero, which means around or less than 1 FPS (1 frame per second). One alternative to speed up the process is to use FOMO (Faster Objects, More Objects).</p>
<p>This novel machine learning algorithm lets us count multiple objects and find their location in an image in real-time using up to 30x less processing power and memory than MobileNet SSD or YOLO. The main reason this is possible is that while other models calculate the object‚Äôs size by drawing a square around it (bounding box), FOMO ignores the size of the image, providing only the information about where the object is located in the image through its centroid coordinates.</p>
<section id="how-fomo-works" class="level3">
<h3 class="anchored" data-anchor-id="how-fomo-works">How FOMO works?</h3>
<p>In a typical object detection pipeline, the first stage is extracting features from the input image. <strong>FOMO leverages MobileNetV2 to perform this task</strong>. MobileNetV2 processes the input image to produce a feature map that captures essential characteristics, such as textures, shapes, and object edges, in a computationally efficient way.</p>
<p><a href="images/png/fomo-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-33"><img src="images/png/fomo-1.png" class="img-fluid"></a></p>
<p>Once these features are extracted, FOMO‚Äôs simpler architecture, focused on center-point detection, interprets the feature map to determine where objects are located in the image. The output is a grid of cells, where each cell represents whether or not an object center is detected. The model outputs one or more confidence scores for each cell, indicating the likelihood of an object being present.</p>
<p>Let‚Äôs see how it works on an image.</p>
<p>FOMO divides the image into blocks of pixels using a factor of 8. For the input of 96x96, the grid would be 12x12 (96/8=12). For a 160x160, the grid will be 20x20, and so on. Next, FOMO will run a classifier through each pixel block to calculate the probability that there is a box or a wheel in each of them and, subsequently, determine the regions that have the highest probability of containing the object (If a pixel block has no objects, it will be classified as <em>background</em>). From the overlap of the final region, the FOMO provides the coordinates (related to the image dimensions) of the centroid of this region.</p>
<p><a href="images/png/fomo-works.png" class="lightbox" data-gallery="quarto-lightbox-gallery-34"><img src="images/png/fomo-works.png" class="img-fluid"></a></p>
<p><strong>Trade-off Between Speed and Precision</strong>:</p>
<ul>
<li><strong>Grid Resolution</strong>: FOMO uses a grid of fixed resolution, meaning each cell can detect if an object is present in that part of the image. While it doesn‚Äôt provide high localization accuracy, it makes a trade-off by being fast and computationally light, which is crucial for edge devices.</li>
<li><strong>Multi-Object Detection</strong>: Since each cell is independent, FOMO can detect multiple objects simultaneously in an image by identifying multiple centers.</li>
</ul>
</section>
<section id="impulse-design-new-training-and-testing" class="level3">
<h3 class="anchored" data-anchor-id="impulse-design-new-training-and-testing">Impulse Design, new Training and Testing</h3>
<p>Return to Edge Impulse Studio, and in the <code>Experiments</code> tab, create another impulse. Now, the input images should be 160x160 (this is the expected input size for MobilenetV2).</p>
<p><a href="images/png/impulse-2.png" class="lightbox" data-gallery="quarto-lightbox-gallery-35"><img src="images/png/impulse-2.png" class="img-fluid"></a></p>
<p>On the <code>Image</code> tab, generate the features and go to the <code>Object detection</code> tab.</p>
<p>We should select a pre-trained model for training. Let‚Äôs use the <strong>FOMO (Faster Objects, More Objects) MobileNetV2 0.35.</strong></p>
<p><a href="images/png/model-choice.png" class="lightbox" data-gallery="quarto-lightbox-gallery-36"><img src="images/png/model-choice.png" class="img-fluid"></a></p>
<p>Regarding the training hyper-parameters, the model will be trained with:</p>
<ul>
<li>Epochs: 30</li>
<li>Batch size: 32</li>
<li>Learning Rate: 0.001.</li>
</ul>
<p>For validation during training, 20% of the dataset (<em>validation_dataset</em>) will be spared. We will not apply Data Augmentation for the remaining 80% (<em>train_dataset</em>) because our dataset was already augmented during the labeling phase at Roboflow.</p>
<p>As a result, the model ends with an overall F1 score of 93.3% with an impressive latency of 8ms (Raspi-4), around 60X less than we got with the SSD MovileNetV2.</p>
<p><a href="images/png/fomo-train-result.png" class="lightbox" data-gallery="quarto-lightbox-gallery-37"><img src="images/png/fomo-train-result.png" class="img-fluid"></a></p>
<blockquote class="blockquote">
<p>Note that FOMO automatically added a third label background to the two previously defined <em>boxes</em> (0) and <em>wheels</em> (1).</p>
</blockquote>
<p>On the <code>Model testing</code> tab, we can see that the accuracy was 94%. Here is one of the test sample results:</p>
<p><a href="images/png/fomo-test.png" class="lightbox" data-gallery="quarto-lightbox-gallery-38"><img src="images/png/fomo-test.png" class="img-fluid"></a></p>
<blockquote class="blockquote">
<p>In object detection tasks, accuracy is generally not the primary <a href="https://learnopencv.com/mean-average-precision-map-object-detection-model-evaluation-metric/">evaluation metric.</a> Object detection involves classifying objects and providing bounding boxes around them, making it a more complex problem than simple classification. The issue is that we do not have the bounding box, only the centroids. In short, using accuracy as a metric could be misleading and may not provide a complete understanding of how well the model is performing.</p>
</blockquote>
</section>
<section id="deploying-the-model-1" class="level3">
<h3 class="anchored" data-anchor-id="deploying-the-model-1">Deploying the model</h3>
<p>As we did in the previous section, we can deploy the trained model as TFLite or Linux (AARCH64). Let‚Äôs do it now as <strong>Linux (AARCH64)</strong>, a binary that implements the <a href="https://docs.edgeimpulse.com/docs/tools/edge-impulse-for-linux">Edge Impulse Linux</a> protocol.</p>
<p>Edge Impulse for Linux models is delivered in <code>.eim</code> format. This <a href="https://docs.edgeimpulse.com/docs/run-inference/linux-eim-executable">executable</a> contains our ‚Äúfull impulse‚Äù created in Edge Impulse Studio. The impulse consists of the signal processing block(s) and any learning and anomaly block(s) we added and trained. It is compiled with optimizations for our processor or GPU (e.g., NEON instructions on ARM cores), plus a straightforward IPC layer (over a Unix socket).</p>
<p>At the <code>Deploy</code> tab, select the option <code>Linux (AARCH64)</code>, the <code>int8</code>model and press <code>Build</code>.</p>
<p><a href="images/png/deploy-linux.png" class="lightbox" data-gallery="quarto-lightbox-gallery-39"><img src="images/png/deploy-linux.png" class="img-fluid"></a></p>
<p>The model will be automatically downloaded to your computer.</p>
<p>On our Raspi, let‚Äôs create a new working area:</p>
<div class="sourceCode" id="cb30"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="bu">cd</span> ~</span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a><span class="bu">cd</span> Documents</span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a><span class="fu">mkdir</span> EI_Linux</span>
<span id="cb30-4"><a href="#cb30-4" aria-hidden="true" tabindex="-1"></a><span class="bu">cd</span> EI_Linux</span>
<span id="cb30-5"><a href="#cb30-5" aria-hidden="true" tabindex="-1"></a><span class="fu">mkdir</span> models</span>
<span id="cb30-6"><a href="#cb30-6" aria-hidden="true" tabindex="-1"></a><span class="fu">mkdir</span> images</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Rename the model for easy identification:</p>
<p>For example, <code>raspi-object-detection-linux-aarch64-FOMO-int8.eim</code> and transfer it to the new Raspi folder<code>./models</code> and capture or get some images for inference and save them in the folder <code>./images</code>.</p>
</section>
<section id="inference-and-post-processing-2" class="level3">
<h3 class="anchored" data-anchor-id="inference-and-post-processing-2">Inference and Post-Processing</h3>
<p>The inference will be made using the <a href="https://docs.edgeimpulse.com/docs/tools/edge-impulse-for-linux/linux-python-sdk">Linux Python SDK</a>. This library lets us run machine learning models and collect sensor data on <a href="https://docs.edgeimpulse.com/docs/tools/edge-impulse-for-linux">Linux</a> machines using Python. The SDK is open source and hosted on GitHub: <a href="https://github.com/edgeimpulse/linux-sdk-python">edgeimpulse/linux-sdk-python</a>.</p>
<p>Let‚Äôs set up a Virtual Environment for working with the Linux Python SDK</p>
<div class="sourceCode" id="cb31"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a><span class="ex">python3</span> <span class="at">-m</span> venv ~/eilinux</span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a><span class="bu">source</span> ~/eilinux/bin/activate</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>And Install the all the libraries needed:</p>
<div class="sourceCode" id="cb32"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="fu">sudo</span> apt-get update</span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a><span class="fu">sudo</span> apt-get install libatlas-base-dev libportaudio0 libportaudio2</span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a><span class="fu">sudo</span> apt-get installlibportaudiocpp0 portaudio19-dev</span>
<span id="cb32-4"><a href="#cb32-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-5"><a href="#cb32-5" aria-hidden="true" tabindex="-1"></a><span class="ex">pip3</span> install edge_impulse_linux <span class="at">-i</span> https://pypi.python.org/simple</span>
<span id="cb32-6"><a href="#cb32-6" aria-hidden="true" tabindex="-1"></a><span class="ex">pip3</span> install Pillow matplotlib pyaudio opencv-contrib-python</span>
<span id="cb32-7"><a href="#cb32-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-8"><a href="#cb32-8" aria-hidden="true" tabindex="-1"></a><span class="fu">sudo</span> apt-get install portaudio19-dev</span>
<span id="cb32-9"><a href="#cb32-9" aria-hidden="true" tabindex="-1"></a><span class="ex">pip3</span> install pyaudio </span>
<span id="cb32-10"><a href="#cb32-10" aria-hidden="true" tabindex="-1"></a><span class="ex">pip3</span> install opencv-contrib-python</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Permit our model to be executable.</p>
<div class="sourceCode" id="cb33"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a><span class="fu">chmod</span> +x raspi-object-detection-linux-aarch64-FOMO-int8.eim</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Install the Jupiter Notebook on the new environment</p>
<div class="sourceCode" id="cb34"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a><span class="ex">pip3</span> install jupyter</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Run a notebook locally (on the Raspi-4 or 5 with desktop)</p>
<div class="sourceCode" id="cb35"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a><span class="ex">jupyter</span> notebook</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>or on the browser on your computer:</p>
<div class="sourceCode" id="cb36"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a><span class="ex">jupyter</span> notebook <span class="at">--ip</span><span class="op">=</span>192.168.4.210 <span class="at">--no-browser</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Let‚Äôs start a new <a href="https://github.com/Mjrovai/EdgeML-with-Raspberry-Pi/blob/main/OBJ_DETEC/notebooks/EI-Linux-FOMO.ipynb">notebook</a> by following all the steps to detect cubes and wheels on an image using the FOMO model and the Edge Impulse Linux Python SDK.</p>
<p>Import the needed libraries:</p>
<div class="sourceCode" id="cb37"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> sys, time</span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb37-3"><a href="#cb37-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb37-4"><a href="#cb37-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.patches <span class="im">as</span> patches</span>
<span id="cb37-5"><a href="#cb37-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> PIL <span class="im">import</span> Image</span>
<span id="cb37-6"><a href="#cb37-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> cv2</span>
<span id="cb37-7"><a href="#cb37-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> edge_impulse_linux.image <span class="im">import</span> ImageImpulseRunner</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Define the model path and labels:</p>
<div class="sourceCode" id="cb38"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a>model_file <span class="op">=</span> <span class="st">"raspi-object-detection-linux-aarch64-int8.eim"</span></span>
<span id="cb38-2"><a href="#cb38-2" aria-hidden="true" tabindex="-1"></a>model_path <span class="op">=</span> <span class="st">"models/"</span><span class="op">+</span> model_file <span class="co"># Trained ML model from Edge Impulse</span></span>
<span id="cb38-3"><a href="#cb38-3" aria-hidden="true" tabindex="-1"></a>labels <span class="op">=</span> [<span class="st">'box'</span>, <span class="st">'wheel'</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<blockquote class="blockquote">
<p>Remember that the model will output the class ID as values (0 and 1), following an alphabetic order regarding the class names.</p>
</blockquote>
<p>Load and initialize the model:</p>
<div class="sourceCode" id="cb39"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Load the model file</span></span>
<span id="cb39-2"><a href="#cb39-2" aria-hidden="true" tabindex="-1"></a>runner <span class="op">=</span> ImageImpulseRunner(model_path)</span>
<span id="cb39-3"><a href="#cb39-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-4"><a href="#cb39-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize model</span></span>
<span id="cb39-5"><a href="#cb39-5" aria-hidden="true" tabindex="-1"></a>model_info <span class="op">=</span> runner.init()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>The <code>model_info</code> will contain critical information about our model. However, unlike the TFLite interpreter, the EI Linux Python SDK library will now prepare the model for inference.</p>
<p>So, let‚Äôs open the image and show it (Now, for compatibility, we will use OpenCV, the CV Library used internally by EI. OpenCV reads the image as BGR, so we will need to convert it to RGB :</p>
<div class="sourceCode" id="cb40"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Load the image</span></span>
<span id="cb40-2"><a href="#cb40-2" aria-hidden="true" tabindex="-1"></a>img_path <span class="op">=</span> <span class="st">"./images/1_box_1_wheel.jpg"</span></span>
<span id="cb40-3"><a href="#cb40-3" aria-hidden="true" tabindex="-1"></a>orig_img <span class="op">=</span> cv2.imread(img_path)</span>
<span id="cb40-4"><a href="#cb40-4" aria-hidden="true" tabindex="-1"></a>img_rgb <span class="op">=</span> cv2.cvtColor(orig_img, cv2.COLOR_BGR2RGB)</span>
<span id="cb40-5"><a href="#cb40-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-6"><a href="#cb40-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Display the image</span></span>
<span id="cb40-7"><a href="#cb40-7" aria-hidden="true" tabindex="-1"></a>plt.imshow(img_rgb)</span>
<span id="cb40-8"><a href="#cb40-8" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Original Image"</span>)</span>
<span id="cb40-9"><a href="#cb40-9" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><a href="images/png/orig-fomo-img.png" class="lightbox" data-gallery="quarto-lightbox-gallery-40"><img src="images/png/orig-fomo-img.png" class="img-fluid"></a></p>
<p>Now we will get the features and the preprocessed image (<code>cropped</code>) using the <code>runner</code>:</p>
<div class="sourceCode" id="cb41"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a>features, cropped <span class="op">=</span> runner.get_features_from_image_auto_studio_setings(img_rgb)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>And perform the inference. Let‚Äôs also calculate the latency of the model:</p>
<div class="sourceCode" id="cb42"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a>res <span class="op">=</span> runner.classify(features)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Let‚Äôs get the output classes of objects detected, their bounding boxes centroids, and probabilities.</p>
<div class="sourceCode" id="cb43"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Found </span><span class="sc">%d</span><span class="st"> bounding boxes (</span><span class="sc">%d</span><span class="st"> ms.)'</span> <span class="op">%</span> (</span>
<span id="cb43-2"><a href="#cb43-2" aria-hidden="true" tabindex="-1"></a>  <span class="bu">len</span>(res[<span class="st">"result"</span>][<span class="st">"bounding_boxes"</span>]), </span>
<span id="cb43-3"><a href="#cb43-3" aria-hidden="true" tabindex="-1"></a>  res[<span class="st">'timing'</span>][<span class="st">'dsp'</span>] <span class="op">+</span> res[<span class="st">'timing'</span>][<span class="st">'classification'</span>]))</span>
<span id="cb43-4"><a href="#cb43-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> bb <span class="kw">in</span> res[<span class="st">"result"</span>][<span class="st">"bounding_boxes"</span>]:</span>
<span id="cb43-5"><a href="#cb43-5" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">'</span><span class="ch">\t</span><span class="sc">%s</span><span class="st"> (</span><span class="sc">%.2f</span><span class="st">): x=</span><span class="sc">%d</span><span class="st"> y=</span><span class="sc">%d</span><span class="st"> w=</span><span class="sc">%d</span><span class="st"> h=</span><span class="sc">%d</span><span class="st">'</span> <span class="op">%</span> (</span>
<span id="cb43-6"><a href="#cb43-6" aria-hidden="true" tabindex="-1"></a>      bb[<span class="st">'label'</span>], bb[<span class="st">'value'</span>], bb[<span class="st">'x'</span>], </span>
<span id="cb43-7"><a href="#cb43-7" aria-hidden="true" tabindex="-1"></a>      bb[<span class="st">'y'</span>], bb[<span class="st">'width'</span>], bb[<span class="st">'height'</span>]))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre><code>Found 2 bounding boxes (29 ms.)
    1 (0.91): x=112 y=40 w=16 h=16
    0 (0.75): x=48 y=56 w=8 h=8</code></pre>
<p>The results show that two objects were detected: one with class ID 0 (<code>box</code>) and one with class ID 1 (<code>wheel</code>), which is correct!</p>
<p>Let‚Äôs visualize the result (The <code>threshold</code> is 0.5, the default value set during the model testing on the Edge Impulse Studio).</p>
<div class="sourceCode" id="cb45"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'</span><span class="ch">\t</span><span class="st">Found </span><span class="sc">%d</span><span class="st"> bounding boxes (latency: </span><span class="sc">%d</span><span class="st"> ms)'</span> <span class="op">%</span> (</span>
<span id="cb45-2"><a href="#cb45-2" aria-hidden="true" tabindex="-1"></a>  <span class="bu">len</span>(res[<span class="st">"result"</span>][<span class="st">"bounding_boxes"</span>]), </span>
<span id="cb45-3"><a href="#cb45-3" aria-hidden="true" tabindex="-1"></a>  res[<span class="st">'timing'</span>][<span class="st">'dsp'</span>] <span class="op">+</span> res[<span class="st">'timing'</span>][<span class="st">'classification'</span>]))</span>
<span id="cb45-4"><a href="#cb45-4" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">5</span>,<span class="dv">5</span>))</span>
<span id="cb45-5"><a href="#cb45-5" aria-hidden="true" tabindex="-1"></a>plt.imshow(cropped)</span>
<span id="cb45-6"><a href="#cb45-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-7"><a href="#cb45-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Go through each of the returned bounding boxes</span></span>
<span id="cb45-8"><a href="#cb45-8" aria-hidden="true" tabindex="-1"></a>bboxes <span class="op">=</span> res[<span class="st">'result'</span>][<span class="st">'bounding_boxes'</span>]</span>
<span id="cb45-9"><a href="#cb45-9" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> bbox <span class="kw">in</span> bboxes:</span>
<span id="cb45-10"><a href="#cb45-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-11"><a href="#cb45-11" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Get the corners of the bounding box</span></span>
<span id="cb45-12"><a href="#cb45-12" aria-hidden="true" tabindex="-1"></a>    left <span class="op">=</span> bbox[<span class="st">'x'</span>]</span>
<span id="cb45-13"><a href="#cb45-13" aria-hidden="true" tabindex="-1"></a>    top <span class="op">=</span> bbox[<span class="st">'y'</span>]</span>
<span id="cb45-14"><a href="#cb45-14" aria-hidden="true" tabindex="-1"></a>    width <span class="op">=</span> bbox[<span class="st">'width'</span>]</span>
<span id="cb45-15"><a href="#cb45-15" aria-hidden="true" tabindex="-1"></a>    height <span class="op">=</span> bbox[<span class="st">'height'</span>]</span>
<span id="cb45-16"><a href="#cb45-16" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb45-17"><a href="#cb45-17" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Draw a circle centered on the detection</span></span>
<span id="cb45-18"><a href="#cb45-18" aria-hidden="true" tabindex="-1"></a>    circ <span class="op">=</span> plt.Circle((left<span class="op">+</span>width<span class="op">//</span><span class="dv">2</span>, top<span class="op">+</span>height<span class="op">//</span><span class="dv">2</span>), <span class="dv">5</span>, </span>
<span id="cb45-19"><a href="#cb45-19" aria-hidden="true" tabindex="-1"></a>                     fill<span class="op">=</span><span class="va">False</span>, color<span class="op">=</span><span class="st">'red'</span>, linewidth<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb45-20"><a href="#cb45-20" aria-hidden="true" tabindex="-1"></a>    plt.gca().add_patch(circ)</span>
<span id="cb45-21"><a href="#cb45-21" aria-hidden="true" tabindex="-1"></a>    class_id <span class="op">=</span> <span class="bu">int</span>(bbox[<span class="st">'label'</span>])</span>
<span id="cb45-22"><a href="#cb45-22" aria-hidden="true" tabindex="-1"></a>    class_name <span class="op">=</span> labels[class_id]</span>
<span id="cb45-23"><a href="#cb45-23" aria-hidden="true" tabindex="-1"></a>    plt.text(left, top<span class="op">-</span><span class="dv">10</span>, <span class="ss">f'</span><span class="sc">{</span>class_name<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>bbox[<span class="st">"value"</span>]<span class="sc">:.2f}</span><span class="ss">'</span>, </span>
<span id="cb45-24"><a href="#cb45-24" aria-hidden="true" tabindex="-1"></a>              color<span class="op">=</span><span class="st">'red'</span>, fontsize<span class="op">=</span><span class="dv">12</span>, backgroundcolor<span class="op">=</span><span class="st">'white'</span>)</span>
<span id="cb45-25"><a href="#cb45-25" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><a href="images/png/infer-fomo-result.png" class="lightbox" data-gallery="quarto-lightbox-gallery-41"><img src="images/png/infer-fomo-result.png" class="img-fluid"></a></p>
</section>
</section>
<section id="exploring-a-yolo-model-using-ultralitics" class="level2">
<h2 class="anchored" data-anchor-id="exploring-a-yolo-model-using-ultralitics">Exploring a YOLO Model using Ultralitics</h2>
<p>For this lab, we will explore YOLOv8. <a href="https://ultralytics.com/">Ultralytics</a> <a href="https://github.com/ultralytics/ultralytics">YOLOv8</a> is a version of the acclaimed real-time object detection and image segmentation model, YOLO. YOLOv8 is built on cutting-edge advancements in deep learning and computer vision, offering unparalleled performance in terms of speed and accuracy. Its streamlined design makes it suitable for various applications and easily adaptable to different hardware platforms, from edge devices to cloud APIs.</p>
<section id="talking-about-the-yolo-model" class="level3">
<h3 class="anchored" data-anchor-id="talking-about-the-yolo-model">Talking about the YOLO Model</h3>
<p>The YOLO (You Only Look Once) model is a highly efficient and widely used object detection algorithm known for its real-time processing capabilities. Unlike traditional object detection systems that repurpose classifiers or localizers to perform detection, YOLO frames the detection problem as a single regression task. This innovative approach enables YOLO to simultaneously predict multiple bounding boxes and their class probabilities from full images in one evaluation, significantly boosting its speed.</p>
<section id="key-features" class="level4">
<h4 class="anchored" data-anchor-id="key-features">Key Features:</h4>
<ol type="1">
<li><p><strong>Single Network Architecture</strong>:</p>
<ul>
<li>YOLO employs a single neural network to process the entire image. This network divides the image into a grid and, for each grid cell, directly predicts bounding boxes and associated class probabilities. This end-to-end training improves speed and simplifies the model architecture.</li>
</ul></li>
<li><p><strong>Real-Time Processing</strong>:</p>
<ul>
<li>One of YOLO‚Äôs standout features is its ability to perform object detection in real-time. Depending on the version and hardware, YOLO can process images at high frames per second (FPS). This makes it ideal for applications requiring quick and accurate object detection, such as video surveillance, autonomous driving, and live sports analysis.</li>
</ul></li>
<li><p><strong>Evolution of Versions</strong>:</p>
<ul>
<li>Over the years, YOLO has undergone significant improvements, from YOLOv1 to the latest YOLOv10. Each iteration has introduced enhancements in accuracy, speed, and efficiency. YOLOv8, for instance, incorporates advancements in network architecture, improved training methodologies, and better support for various hardware, ensuring a more robust performance.</li>
<li>Although YOLOv10 is the family‚Äôs newest member with an encouraging performance based on its paper, it was just released (May 2024) and is not fully integrated with the Ultralitycs library. Conversely, the precision-recall curve analysis suggests that YOLOv8 generally outperforms YOLOv9, capturing a higher proportion of true positives while minimizing false positives more effectively (for more details, see this <a href="https://encord.com/blog/performanceyolov9-vs-yolov8-custom-dataset/">article</a>). So, this lab is based on the YOLOv8n.</li>
</ul>
<p><a href="images/jpeg/versions.jpg" class="lightbox" data-gallery="quarto-lightbox-gallery-42"><img src="images/jpeg/versions.jpg" class="img-fluid"></a></p></li>
<li><p><strong>Accuracy and Efficiency</strong>:</p>
<ul>
<li>While early versions of YOLO traded off some accuracy for speed, recent versions have made substantial strides in balancing both. The newer models are faster and more accurate, detecting small objects (such as bees) and performing well on complex datasets.</li>
</ul></li>
<li><p><strong>Wide Range of Applications</strong>:</p>
<ul>
<li>YOLO‚Äôs versatility has led to its adoption in numerous fields. It is used in traffic monitoring systems to detect and count vehicles, security applications to identify potential threats and agricultural technology to monitor crops and livestock. Its application extends to any domain requiring efficient and accurate object detection.</li>
</ul></li>
<li><p><strong>Community and Development</strong>:</p>
<ul>
<li>YOLO continues to evolve and is supported by a strong community of developers and researchers (being the YOLOv8 very strong). Open-source implementations and extensive documentation have made it accessible for customization and integration into various projects. Popular deep learning frameworks like Darknet, TensorFlow, and PyTorch support YOLO, further broadening its applicability.</li>
<li><a href="https://github.com/ultralytics/ultralytics?tab=readme-ov-file">Ultralitics YOLOv8</a> can not only <a href="https://docs.ultralytics.com/tasks/detect">Detect</a> (our case here) but also <a href="https://docs.ultralytics.com/tasks/segment">Segment</a> and <a href="https://docs.ultralytics.com/tasks/pose">Pose</a> models pre-trained on the <a href="https://docs.ultralytics.com/datasets/detect/coco">COCO</a> dataset and YOLOv8 <a href="https://docs.ultralytics.com/tasks/classify">Classify</a> models pre-trained on the <a href="https://docs.ultralytics.com/datasets/classify/imagenet">ImageNet</a> dataset. <a href="https://docs.ultralytics.com/modes/track">Track</a> mode is available for all Detect, Segment, and Pose models.</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="https://raw.githubusercontent.com/ultralytics/assets/main/im/banner-tasks.png" class="lightbox" data-gallery="quarto-lightbox-gallery-43" title="Ultralytics YOLO supported tasks"><img src="https://raw.githubusercontent.com/ultralytics/assets/main/im/banner-tasks.png" class="img-fluid figure-img" alt="Ultralytics YOLO supported tasks"></a></p>
<figcaption>Ultralytics YOLO supported tasks</figcaption>
</figure>
</div></li>
</ol>
</section>
</section>
<section id="installation" class="level3">
<h3 class="anchored" data-anchor-id="installation">Installation</h3>
<p>On our Raspi, let‚Äôs deactivate the current environment to create a new working area:</p>
<div class="sourceCode" id="cb46"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a><span class="ex">deactivate</span></span>
<span id="cb46-2"><a href="#cb46-2" aria-hidden="true" tabindex="-1"></a><span class="bu">cd</span> ~</span>
<span id="cb46-3"><a href="#cb46-3" aria-hidden="true" tabindex="-1"></a><span class="bu">cd</span> Documents/</span>
<span id="cb46-4"><a href="#cb46-4" aria-hidden="true" tabindex="-1"></a><span class="fu">mkdir</span> YOLO</span>
<span id="cb46-5"><a href="#cb46-5" aria-hidden="true" tabindex="-1"></a><span class="bu">cd</span> YOLO</span>
<span id="cb46-6"><a href="#cb46-6" aria-hidden="true" tabindex="-1"></a><span class="fu">mkdir</span> models</span>
<span id="cb46-7"><a href="#cb46-7" aria-hidden="true" tabindex="-1"></a><span class="fu">mkdir</span> images</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Let‚Äôs set up a Virtual Environment for working with the Ultralytics YOLOv8</p>
<div class="sourceCode" id="cb47"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb47-1"><a href="#cb47-1" aria-hidden="true" tabindex="-1"></a><span class="ex">python3</span> <span class="at">-m</span> venv ~/yolo</span>
<span id="cb47-2"><a href="#cb47-2" aria-hidden="true" tabindex="-1"></a><span class="bu">source</span> ~/yolo/bin/activate</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>And install the Ultralytics packages for local inference on the Raspi</p>
<ol type="1">
<li>Update the packages list, install pip, and upgrade to the latest:</li>
</ol>
<div class="sourceCode" id="cb48"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a><span class="fu">sudo</span> apt update</span>
<span id="cb48-2"><a href="#cb48-2" aria-hidden="true" tabindex="-1"></a><span class="fu">sudo</span> apt install python3-pip <span class="at">-y</span></span>
<span id="cb48-3"><a href="#cb48-3" aria-hidden="true" tabindex="-1"></a><span class="ex">pip</span> install <span class="at">-U</span> pip</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<ol start="2" type="1">
<li>Install the <code>ultralytics</code> pip package with optional dependencies:</li>
</ol>
<div class="sourceCode" id="cb49"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb49-1"><a href="#cb49-1" aria-hidden="true" tabindex="-1"></a><span class="ex">pip</span> install ultralytics<span class="pp">[</span><span class="ss">export</span><span class="pp">]</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<ol start="3" type="1">
<li>Reboot the device:</li>
</ol>
<div class="sourceCode" id="cb50"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb50-1"><a href="#cb50-1" aria-hidden="true" tabindex="-1"></a><span class="fu">sudo</span> reboot</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="testing-the-yolo" class="level3">
<h3 class="anchored" data-anchor-id="testing-the-yolo">Testing the YOLO</h3>
<p>After the Raspi-Zero booting, let‚Äôs activate the <code>yolo</code> env, go to the working directory,</p>
<div class="sourceCode" id="cb51"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb51-1"><a href="#cb51-1" aria-hidden="true" tabindex="-1"></a><span class="bu">source</span> ~/yolo/bin/activate</span>
<span id="cb51-2"><a href="#cb51-2" aria-hidden="true" tabindex="-1"></a><span class="bu">cd</span> /Documents/YOLO</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>and run inference on an image that will be downloaded from the Ultralytics website, using the YOLOV8n model (the smallest in the family) at the Terminal (CLI):</p>
<div class="sourceCode" id="cb52"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb52-1"><a href="#cb52-1" aria-hidden="true" tabindex="-1"></a><span class="ex">yolo</span> predict model=<span class="st">'yolov8n'</span> source=<span class="st">'https://ultralytics.com/images/bus.jpg'</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<blockquote class="blockquote">
<p>The YOLO model family is pre-trained with the COCO dataset.</p>
</blockquote>
<p>The inference result will appear in the terminal. In the image (bus.jpg), 4 <code>persons</code>, 1 <code>bus,</code> and 1 <code>stop signal</code> were detected:</p>
<p><a href="images/png/yolo-infer-bus.png" class="lightbox" data-gallery="quarto-lightbox-gallery-44"><img src="images/png/yolo-infer-bus.png" class="img-fluid"></a></p>
<p>Also, we got a message that <code>Results saved to runs/detect/predict</code>. Inspecting that directory, we can see a new image saved (bus.jpg). Let‚Äôs download it from the Raspi-Zero to our desktop for inspection:</p>
<p><a href="images/png/yolo-bus.png" class="lightbox" data-gallery="quarto-lightbox-gallery-45"><img src="images/png/yolo-bus.png" class="img-fluid"></a></p>
<p>So, the Ultrayitics YOLO is correctly installed on our Raspi. But, on the Raspi-Zero, an issue is the high latency for this inference, around 18 seconds, even with the most miniature model of the family (YOLOv8n).</p>
</section>
<section id="export-model-to-ncnn-format" class="level3">
<h3 class="anchored" data-anchor-id="export-model-to-ncnn-format">Export Model to NCNN format</h3>
<p>Deploying computer vision models on edge devices with limited computational power, such as the Raspi-Zero, can cause latency issues. One alternative is to use a format optimized for optimal performance. This ensures that even devices with limited processing power can handle advanced computer vision tasks well.</p>
<p>Of all the model export formats supported by Ultralytics, the <a href="https://docs.ultralytics.com/integrations/ncnn">NCNN</a> is a high-performance neural network inference computing framework optimized for mobile platforms. From the beginning of the design, NCNN was deeply considerate about deployment and use on mobile phones and did not have third-party dependencies. It is cross-platform and runs faster than all known open-source frameworks (such as TFLite).</p>
<p>NCNN delivers the best inference performance when working with Raspberry Pi devices. NCNN is highly optimized for mobile embedded platforms (such as ARM architecture).</p>
<p>So, let‚Äôs convert our model and rerun the inference:</p>
<ol type="1">
<li>Export a YOLOv8n PyTorch model to NCNN format, creating: ‚Äò/yolov8n_ncnn_model‚Äô</li>
</ol>
<div class="sourceCode" id="cb53"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb53-1"><a href="#cb53-1" aria-hidden="true" tabindex="-1"></a><span class="ex">yolo</span> export model=yolov8n.pt format=ncnn </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<ol start="2" type="1">
<li>Run inference with the exported model (now the source could be the bus.jpg image that was downloaded from the website to the current directory on the last inference):</li>
</ol>
<div class="sourceCode" id="cb54"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb54-1"><a href="#cb54-1" aria-hidden="true" tabindex="-1"></a><span class="ex">yolo</span> predict model=<span class="st">'./yolov8n_ncnn_model'</span> source=<span class="st">'bus.jpg'</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<blockquote class="blockquote">
<p>The first inference, when the model is loaded, usually has a high latency (around 17s), but from the 2nd, it is possible to note that the inference goes down to around 2s.</p>
</blockquote>
</section>
<section id="exploring-yolo-with-python" class="level3">
<h3 class="anchored" data-anchor-id="exploring-yolo-with-python">Exploring YOLO with Python</h3>
<p>To start, let‚Äôs call the Python Interpreter so we can explore how the YOLO model works, line by line:</p>
<div class="sourceCode" id="cb55"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb55-1"><a href="#cb55-1" aria-hidden="true" tabindex="-1"></a><span class="ex">python3</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Now, we should call the YOLO library from Ultralitics and load the model:</p>
<div class="sourceCode" id="cb56"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb56-1"><a href="#cb56-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> ultralytics <span class="im">import</span> YOLO</span>
<span id="cb56-2"><a href="#cb56-2" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> YOLO(<span class="st">'yolov8n_ncnn_model'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Next, run inference over an image (let‚Äôs use again <code>bus.jpg</code>):</p>
<div class="sourceCode" id="cb57"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb57-1"><a href="#cb57-1" aria-hidden="true" tabindex="-1"></a>img <span class="op">=</span> <span class="st">'bus.jpg'</span></span>
<span id="cb57-2"><a href="#cb57-2" aria-hidden="true" tabindex="-1"></a>result <span class="op">=</span> model.predict(img, save<span class="op">=</span><span class="va">True</span>, imgsz<span class="op">=</span><span class="dv">640</span>, conf<span class="op">=</span><span class="fl">0.5</span>, iou<span class="op">=</span><span class="fl">0.3</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><a href="images/png/python-infer-bus.png" class="lightbox" data-gallery="quarto-lightbox-gallery-46"><img src="images/png/python-infer-bus.png" class="img-fluid"></a></p>
<p>We can verify that the result is almost identical to the one we get running the inference at the terminal level (CLI), except that the bus stop was not detected with the reduced NCNN model. Note that the latency was reduced.</p>
<p>Let‚Äôs analyze the ‚Äúresult‚Äù content.</p>
<p>For example, we can see <code>result[0].boxes.data</code>, showing us the main inference result, which is a tensor shape (4, 6). Each line is one of the objects detected, being the 4 first columns, the bounding boxes coordinates, the 5th, the confidence, and the 6th, the class (in this case, <code>0: person</code> and <code>5: bus</code>):</p>
<p><a href="images/png/result-bus.png" class="lightbox" data-gallery="quarto-lightbox-gallery-47"><img src="images/png/result-bus.png" class="img-fluid"></a></p>
<p>We can access several inference results separately, as the inference time, and have it printed in a better format:</p>
<div class="sourceCode" id="cb58"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb58-1"><a href="#cb58-1" aria-hidden="true" tabindex="-1"></a>inference_time <span class="op">=</span> <span class="bu">int</span>(result[<span class="dv">0</span>].speed[<span class="st">'inference'</span>])</span>
<span id="cb58-2"><a href="#cb58-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Inference Time: </span><span class="sc">{</span>inference_time<span class="sc">}</span><span class="ss"> ms"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Or we can have the total number of objects detected:</p>
<div class="sourceCode" id="cb59"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb59-1"><a href="#cb59-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'Number of objects: </span><span class="sc">{</span><span class="bu">len</span> (result[<span class="dv">0</span>].boxes.cls)<span class="sc">}</span><span class="ss">'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><a href="images/png/data-bus.png" class="lightbox" data-gallery="quarto-lightbox-gallery-48"><img src="images/png/data-bus.png" class="img-fluid"></a></p>
<p>With Python, we can create a detailed output that meets our needs (See <a href="https://docs.ultralytics.com/modes/predict/">Model Prediction with Ultralytics YOLO</a> for more details). Let‚Äôs run a Python script instead of manually entering it line by line in the interpreter, as shown below. Let‚Äôs use <code>nano</code> as our text editor. First, we should create an empty Python script named, for example, <code>yolov8_tests.py</code>:</p>
<div class="sourceCode" id="cb60"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb60-1"><a href="#cb60-1" aria-hidden="true" tabindex="-1"></a>nano yolov8_tests.py</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Enter with the code lines:</p>
<div class="sourceCode" id="cb61"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb61-1"><a href="#cb61-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> ultralytics <span class="im">import</span> YOLO</span>
<span id="cb61-2"><a href="#cb61-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-3"><a href="#cb61-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Load the YOLOv8 model</span></span>
<span id="cb61-4"><a href="#cb61-4" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> YOLO(<span class="st">'yolov8n_ncnn_model'</span>)</span>
<span id="cb61-5"><a href="#cb61-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-6"><a href="#cb61-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Run inference</span></span>
<span id="cb61-7"><a href="#cb61-7" aria-hidden="true" tabindex="-1"></a>img <span class="op">=</span> <span class="st">'bus.jpg'</span></span>
<span id="cb61-8"><a href="#cb61-8" aria-hidden="true" tabindex="-1"></a>result <span class="op">=</span> model.predict(img, save<span class="op">=</span><span class="va">False</span>, imgsz<span class="op">=</span><span class="dv">640</span>, conf<span class="op">=</span><span class="fl">0.5</span>, iou<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb61-9"><a href="#cb61-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb61-10"><a href="#cb61-10" aria-hidden="true" tabindex="-1"></a><span class="co"># print the results</span></span>
<span id="cb61-11"><a href="#cb61-11" aria-hidden="true" tabindex="-1"></a>inference_time <span class="op">=</span> <span class="bu">int</span>(result[<span class="dv">0</span>].speed[<span class="st">'inference'</span>])</span>
<span id="cb61-12"><a href="#cb61-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Inference Time: </span><span class="sc">{</span>inference_time<span class="sc">}</span><span class="ss"> ms"</span>)</span>
<span id="cb61-13"><a href="#cb61-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'Number of objects: </span><span class="sc">{</span><span class="bu">len</span> (result[<span class="dv">0</span>].boxes.cls)<span class="sc">}</span><span class="ss">'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><a href="images/png/yolo-py-script.png" class="lightbox" data-gallery="quarto-lightbox-gallery-49"><img src="images/png/yolo-py-script.png" class="img-fluid"></a></p>
<p>And enter with the commands: <code>[CTRL+O]</code> + <code>[ENTER]</code> +<code>[CTRL+X]</code> to save the Python script.</p>
<p>Run the script:</p>
<div class="sourceCode" id="cb62"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb62-1"><a href="#cb62-1" aria-hidden="true" tabindex="-1"></a><span class="ex">python</span> yolov8_tests.py</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>The result is the same as running the inference at the terminal level (CLI) and with the built-in Python interpreter.</p>
<blockquote class="blockquote">
<p>Calling the YOLO library and loading the model for inference for the first time takes a long time, but the inferences after that will be much faster. For example, the first single inference can take several seconds, but after that, the inference time should be reduced to less than 1 second.</p>
</blockquote>
</section>
<section id="training-yolov8-on-a-customized-dataset" class="level3">
<h3 class="anchored" data-anchor-id="training-yolov8-on-a-customized-dataset">Training YOLOv8 on a Customized Dataset</h3>
<p>Return to our ‚ÄúBoxe versus Wheel‚Äù dataset, labeled on <a href="https://universe.roboflow.com/marcelo-rovai-riila/box-versus-wheel-auto-dataset">Roboflow</a>. On the <code>Download Dataset</code>, instead of <code>Download a zip to computer</code> option done for training on Edge Impulse Studio, we will opt for <code>Show download code</code>. This option will open a pop-up window with a code snippet that should be pasted into our training notebook.</p>
<p><a href="images/png/dataset_code.png" class="lightbox" data-gallery="quarto-lightbox-gallery-50"><img src="images/png/dataset_code.png" class="img-fluid"></a></p>
<p>For training, let‚Äôs adapt one of the public examples available from Ultralitytics and run it on Google Colab. Below, you can find mine to be adapted in your project:</p>
<ul>
<li>YOLOv8 Box versus Wheel Dataset Training <a href="https://colab.research.google.com/github.com/Mjrovai/EdgeML-with-Raspberry-Pi/blob/main/OBJ_DETEC/notebooks/yolov8_box_vs_wheel.ipynb">[Open In Colab]</a></li>
</ul>
<section id="critical-points-on-the-notebook" class="level4">
<h4 class="anchored" data-anchor-id="critical-points-on-the-notebook">Critical points on the Notebook:</h4>
<ol type="1">
<li><p>Run it with GPU (the NVidia T4 is free)</p></li>
<li><p>Install Ultralytics using PIP.</p>
<p><a href="images/png/yolo-train-lib.png" class="lightbox" data-gallery="quarto-lightbox-gallery-51"><img src="images/png/yolo-train-lib.png" class="img-fluid"></a></p></li>
<li><p>Now, you can import the YOLO and upload your dataset to the CoLab, pasting the Download code that we get from Roboflow. Note that our dataset will be mounted under <code>/content/datasets/</code>:</p></li>
</ol>
<p><a href="images/png/yolo-dataset-upload.png" class="lightbox" data-gallery="quarto-lightbox-gallery-52"><img src="images/png/yolo-dataset-upload.png" class="img-fluid"></a></p>
<ol start="4" type="1">
<li>It is essential to verify and change the file <code>data.yaml</code> with the correct path for the images (copy the path on each <code>images</code> folder).</li>
</ol>
<div class="sourceCode" id="cb63"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb63-1"><a href="#cb63-1" aria-hidden="true" tabindex="-1"></a><span class="ex">names:</span></span>
<span id="cb63-2"><a href="#cb63-2" aria-hidden="true" tabindex="-1"></a><span class="ex">-</span> box</span>
<span id="cb63-3"><a href="#cb63-3" aria-hidden="true" tabindex="-1"></a><span class="ex">-</span> wheel</span>
<span id="cb63-4"><a href="#cb63-4" aria-hidden="true" tabindex="-1"></a><span class="ex">nc:</span> 2</span>
<span id="cb63-5"><a href="#cb63-5" aria-hidden="true" tabindex="-1"></a><span class="ex">roboflow:</span></span>
<span id="cb63-6"><a href="#cb63-6" aria-hidden="true" tabindex="-1"></a>  <span class="ex">license:</span> CC BY 4.0</span>
<span id="cb63-7"><a href="#cb63-7" aria-hidden="true" tabindex="-1"></a>  <span class="ex">project:</span> box-versus-wheel-auto-dataset</span>
<span id="cb63-8"><a href="#cb63-8" aria-hidden="true" tabindex="-1"></a>  <span class="ex">url:</span> https://universe.roboflow.com/marcelo-rovai-riila/box-versus-wheel-auto-dataset/dataset/5</span>
<span id="cb63-9"><a href="#cb63-9" aria-hidden="true" tabindex="-1"></a>  <span class="ex">version:</span> 5</span>
<span id="cb63-10"><a href="#cb63-10" aria-hidden="true" tabindex="-1"></a>  <span class="ex">workspace:</span> marcelo-rovai-riila</span>
<span id="cb63-11"><a href="#cb63-11" aria-hidden="true" tabindex="-1"></a><span class="ex">test:</span> /content/datasets/Box-versus-Wheel-auto-dataset-5/test/images</span>
<span id="cb63-12"><a href="#cb63-12" aria-hidden="true" tabindex="-1"></a><span class="ex">train:</span> /content/datasets/Box-versus-Wheel-auto-dataset-5/train/images</span>
<span id="cb63-13"><a href="#cb63-13" aria-hidden="true" tabindex="-1"></a><span class="ex">val:</span> /content/datasets/Box-versus-Wheel-auto-dataset-5/valid/images</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<ol start="5" type="1">
<li><p>Define the main hyperparameters that you want to change from default, for example:</p>
<div class="sourceCode" id="cb64"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb64-1"><a href="#cb64-1" aria-hidden="true" tabindex="-1"></a><span class="ex">MODEL</span> = <span class="st">'yolov8n.pt'</span></span>
<span id="cb64-2"><a href="#cb64-2" aria-hidden="true" tabindex="-1"></a><span class="ex">IMG_SIZE</span> = 640</span>
<span id="cb64-3"><a href="#cb64-3" aria-hidden="true" tabindex="-1"></a><span class="ex">EPOCHS</span> = 25 <span class="co"># For a final project, you should consider at least 100 epochs </span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></li>
<li><p>Run the training (using CLI):</p>
<div class="sourceCode" id="cb65"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb65-1"><a href="#cb65-1" aria-hidden="true" tabindex="-1"></a><span class="ex">!yolo</span> task=detect mode=train model={MODEL} data={dataset.location}/data.yaml epochs={EPOCHS} imgsz={IMG_SIZE} plots=True </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/png/train-result.png" class="lightbox" data-gallery="quarto-lightbox-gallery-53" title="image-20240910111319804"><img src="images/png/train-result.png" class="img-fluid figure-img" alt="image-20240910111319804"></a></p>
<figcaption>image-20240910111319804</figcaption>
</figure>
</div></li>
</ol>
<p>‚Äã The model took a few minutes to be trained and has an excellent result (mAP50 of 0.995). At the end of the training, all results are saved in the folder listed, for example: <code>/runs/detect/train/</code>. There, you can find, for example, the confusion matrix.</p>
<p><a href="images/png/matrix.png" class="lightbox" data-gallery="quarto-lightbox-gallery-54"><img src="images/png/matrix.png" class="img-fluid"></a></p>
<ol start="7" type="1">
<li>Note that the trained model (<code>best.pt</code>) is saved in the folder <code>/runs/detect/train/weights/</code>. Now, you should validate the trained model with the <code>valid/images</code>.</li>
</ol>
<div class="sourceCode" id="cb66"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb66-1"><a href="#cb66-1" aria-hidden="true" tabindex="-1"></a><span class="ex">!yolo</span> task=detect mode=val model={HOME}/runs/detect/train/weights/best.pt data={dataset.location}/data.yaml</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>‚Äã The results were similar to training.</p>
<ol start="8" type="1">
<li>Now, we should perform inference on the images left aside for testing</li>
</ol>
<div class="sourceCode" id="cb67"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb67-1"><a href="#cb67-1" aria-hidden="true" tabindex="-1"></a><span class="ex">!yolo</span> task=detect mode=predict model={HOME}/runs/detect/train/weights/best.pt conf=0.25 source={dataset.location}/test/images save=True</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>The inference results are saved in the folder <code>runs/detect/predict</code>. Let‚Äôs see some of them:</p>
<p><a href="images/png/test-infer-yolo.png" class="lightbox" data-gallery="quarto-lightbox-gallery-55"><img src="images/png/test-infer-yolo.png" class="img-fluid"></a></p>
<ol start="9" type="1">
<li><p>It is advised to export the train, validation, and test results for a Drive at Google. To do so, we should mount the drive.</p>
<div class="sourceCode" id="cb68"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb68-1"><a href="#cb68-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> google.colab <span class="im">import</span> drive</span>
<span id="cb68-2"><a href="#cb68-2" aria-hidden="true" tabindex="-1"></a>drive.mount(<span class="st">'/content/gdrive'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>and copy the content of <code>/runs</code> folder to a folder that you should create in your Drive, for example:</p>
<div class="sourceCode" id="cb69"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb69-1"><a href="#cb69-1" aria-hidden="true" tabindex="-1"></a><span class="ex">!scp</span> <span class="at">-r</span> /content/runs <span class="st">'/content/gdrive/MyDrive/10_UNIFEI/Box_vs_Wheel_Project'</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></li>
</ol>
</section>
</section>
<section id="inference-with-the-trained-model-using-the-raspi" class="level3">
<h3 class="anchored" data-anchor-id="inference-with-the-trained-model-using-the-raspi">Inference with the trained model, using the Raspi</h3>
<p>Download the trained model <code>/runs/detect/train/weights/best.pt</code> to your computer. Using the FileZilla FTP, let‚Äôs transfer the <code>best.pt</code> to the Raspi models folder (before the transfer, you may change the model name, for example, <code>box_wheel_320_yolo.pt</code>).</p>
<p>Using the FileZilla FTP, let‚Äôs transfer a few images from the test dataset to <code>.\YOLO\images</code>:</p>
<p>Let‚Äôs return to the YOLO folder and use the Python Interpreter:</p>
<div class="sourceCode" id="cb70"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb70-1"><a href="#cb70-1" aria-hidden="true" tabindex="-1"></a><span class="bu">cd</span> ..</span>
<span id="cb70-2"><a href="#cb70-2" aria-hidden="true" tabindex="-1"></a><span class="ex">python</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>As before, we will import the YOLO library and define our converted model to detect bees:</p>
<div class="sourceCode" id="cb71"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb71-1"><a href="#cb71-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> ultralytics <span class="im">import</span> YOLO</span>
<span id="cb71-2"><a href="#cb71-2" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> YOLO(<span class="st">'./models/box_wheel_320_yolo.pt'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Now, let‚Äôs define an image and call the inference (we will save the image result this time to external verification):</p>
<div class="sourceCode" id="cb72"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb72-1"><a href="#cb72-1" aria-hidden="true" tabindex="-1"></a>img <span class="op">=</span> <span class="st">'./images/1_box_1_wheel.jpg'</span></span>
<span id="cb72-2"><a href="#cb72-2" aria-hidden="true" tabindex="-1"></a>result <span class="op">=</span> model.predict(img, save<span class="op">=</span><span class="va">True</span>, imgsz<span class="op">=</span><span class="dv">320</span>, conf<span class="op">=</span><span class="fl">0.5</span>, iou<span class="op">=</span><span class="fl">0.3</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Let‚Äôs repeat for several images. The inference result is saved on the variable <code>result,</code> and the processed image on <code>runs/detect/predict8</code></p>
<p><a href="images/png/infer-yolo.png" class="lightbox" data-gallery="quarto-lightbox-gallery-56"><img src="images/png/infer-yolo.png" class="img-fluid"></a></p>
<p>Using FileZilla FTP, we can send the inference result to our Desktop for verification:</p>
<p><a href="images/png/yolo-infer-raspi.png" class="lightbox" data-gallery="quarto-lightbox-gallery-57"><img src="images/png/yolo-infer-raspi.png" class="img-fluid"></a></p>
<p>We can see that the inference result is excellent! The model was trained based on the smaller base model of the YOLOv8 family (YOLOv8n). The issue is the latency, around 1 second (or 1 FPS on the Raspi-Zero). Of course, we can reduce this latency and convert the model to TFLite or NCNN.</p>
</section>
</section>
<section id="object-detection-on-a-live-stream" class="level2">
<h2 class="anchored" data-anchor-id="object-detection-on-a-live-stream">Object Detection on a live stream</h2>
<p>All the models explored in this lab can detect objects in real-time using a camera. The captured image should be the input for the trained and converted model. For the Raspi-4 or 5 with a desktop, OpenCV can capture the frames and display the inference result.</p>
<p>However, creating a live stream with a webcam to detect objects in real-time is also possible. For example, let‚Äôs start with the script developed for the Image Classification app and adapt it for a <em>Real-Time Object Detection Web Application Using TensorFlow Lite and Flask</em>.</p>
<p>This app version will work for all TFLite models. Verify if the model is in its correct folder, for example:</p>
<div class="sourceCode" id="cb73"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb73-1"><a href="#cb73-1" aria-hidden="true" tabindex="-1"></a>model_path <span class="op">=</span> <span class="st">"./models/ssd-mobilenet-v1-tflite-default-v1.tflite"</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Download the Python script <code>object_detection_app.py</code> from <a href="https://github.com/Mjrovai/EdgeML-with-Raspberry-Pi/blob/main/OBJ_DETEC/python_scripts/object_detection_app.py">GitHub</a>.</p>
<p>And on the terminal, run:</p>
<div class="sourceCode" id="cb74"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb74-1"><a href="#cb74-1" aria-hidden="true" tabindex="-1"></a><span class="ex">python3</span> object_detection_app.py</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>And access the web interface:</p>
<ul>
<li>On the Raspberry Pi itself (if you have a GUI): Open a web browser and go to <code>http://localhost:5000</code></li>
<li>From another device on the same network: Open a web browser and go to <code>http://&lt;raspberry_pi_ip&gt;:5000</code> (Replace <code>&lt;raspberry_pi_ip&gt;</code> with your Raspberry Pi‚Äôs IP address). For example: <code>http://192.168.4.210:5000/</code></li>
</ul>
<p>Here are some screenshots of the app running on an external desktop</p>
<p><a href="images/png/app-running.png" class="lightbox" data-gallery="quarto-lightbox-gallery-58"><img src="images/png/app-running.png" class="img-fluid"></a></p>
<p>Let‚Äôs see a technical description of the key modules used in the object detection application:</p>
<ol type="1">
<li><strong>TensorFlow Lite (tflite_runtime)</strong>:
<ul>
<li>Purpose: Efficient inference of machine learning models on edge devices.</li>
<li>Why: TFLite offers reduced model size and optimized performance compared to full TensorFlow, which is crucial for resource-constrained devices like Raspberry Pi. It supports hardware acceleration and quantization, further improving efficiency.</li>
<li>Key functions: <code>Interpreter</code> for loading and running the model,<code>get_input_details(),</code> and <code>get_output_details()</code> for interfacing with the model.</li>
</ul></li>
<li><strong>Flask:</strong>
<ul>
<li>Purpose: Lightweight web framework for creating the backend server.</li>
<li>Why: Flask‚Äôs simplicity and flexibility make it ideal for rapidly developing and deploying web applications. It‚Äôs less resource-intensive than larger frameworks suitable for edge devices.</li>
<li>Key components: route decorators for defining API endpoints, <code>Response</code> objects for streaming video, <code>render_template_string</code> for serving dynamic HTML.</li>
</ul></li>
<li><strong>Picamera2:</strong>
<ul>
<li>Purpose: Interface with the Raspberry Pi camera module.</li>
<li>Why: Picamera2 is the latest library for controlling Raspberry Pi cameras, offering improved performance and features over the original Picamera library.</li>
<li>Key functions: <code>create_preview_configuration()</code> for setting up the camera, <code>capture_file()</code> for capturing frames.</li>
</ul></li>
<li><strong>PIL (Python Imaging Library):</strong>
<ul>
<li>Purpose: Image processing and manipulation.</li>
<li>Why: PIL provides a wide range of image processing capabilities. It‚Äôs used here to resize images, draw bounding boxes, and convert between image formats.</li>
<li>Key classes: <code>Image</code> for loading and manipulating images, <code>ImageDraw</code> for drawing shapes and text on images.</li>
</ul></li>
<li><strong>NumPy:</strong>
<ul>
<li>Purpose: Efficient array operations and numerical computing.</li>
<li>Why: NumPy‚Äôs array operations are much faster than pure Python lists, which is crucial for efficiently processing image data and model inputs/outputs.</li>
<li>Key functions: <code>array()</code> for creating arrays, <code>expand_dims()</code> for adding dimensions to arrays.</li>
</ul></li>
<li><strong>Threading:</strong>
<ul>
<li>Purpose: Concurrent execution of tasks.</li>
<li>Why: Threading allows simultaneous frame capture, object detection, and web server operation, crucial for maintaining real-time performance.</li>
<li>Key components: <code>Thread</code> class creates separate execution threads, and Lock is used for thread synchronization.</li>
</ul></li>
<li><strong>io.BytesIO:</strong>
<ul>
<li>Purpose: In-memory binary streams.</li>
<li>Why: Allows efficient handling of image data in memory without needing temporary files, improving speed and reducing I/O operations.</li>
</ul></li>
<li><strong>time:</strong>
<ul>
<li>Purpose: Time-related functions.</li>
<li>Why: Used for adding delays (<code>time.sleep()</code>) to control frame rate and for performance measurements.</li>
</ul></li>
<li><strong>jQuery (client-side)</strong>:
<ul>
<li>Purpose: Simplified DOM manipulation and AJAX requests.</li>
<li>Why: It makes it easy to update the web interface dynamically and communicate with the server without page reloads.</li>
<li>Key functions: <code>.get()</code> and <code>.post()</code> for AJAX requests, DOM manipulation methods for updating the UI.</li>
</ul></li>
</ol>
<p>Regarding the main app system architecture:</p>
<ol type="1">
<li><strong>Main Thread</strong>: Runs the Flask server, handling HTTP requests and serving the web interface.</li>
<li><strong>Camera Thread</strong>: Continuously captures frames from the camera.</li>
<li><strong>Detection Thread</strong>: Processes frames through the TFLite model for object detection.</li>
<li><strong>Frame Buffer</strong>: Shared memory space (protected by locks) storing the latest frame and detection results.</li>
</ol>
<p>And the app data flow, we can describe in short:</p>
<ol type="1">
<li>Camera captures frame ‚Üí Frame Buffer</li>
<li>Detection thread reads from Frame Buffer ‚Üí Processes through TFLite model ‚Üí Updates detection results in Frame Buffer</li>
<li>Flask routes access Frame Buffer to serve the latest frame and detection results</li>
<li>Web client receives updates via AJAX and updates UI</li>
</ol>
<p>This architecture allows for efficient, real-time object detection while maintaining a responsive web interface running on a resource-constrained edge device like a Raspberry Pi. Threading and efficient libraries like TFLite and PIL enable the system to process video frames in real-time, while Flask and jQuery provide a user-friendly way to interact with them.</p>
<p>You can test the app with another pre-processed model, such as the EfficientDet, changing the app line:</p>
<div class="sourceCode" id="cb75"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb75-1"><a href="#cb75-1" aria-hidden="true" tabindex="-1"></a>model_path <span class="op">=</span> <span class="st">"./models/lite-model_efficientdet_lite0_detection_metadata_1.tflite"</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<blockquote class="blockquote">
<p>If we want to use the app for the SSD-MobileNetV2 model, trained on Edge Impulse Studio with the ‚ÄúBox versus Wheel‚Äù dataset, the code should also be adapted depending on the input details, as we have explored on its <a href="https://github.com/Mjrovai/EdgeML-with-Raspberry-Pi/blob/main/OBJ_DETEC/notebooks/EI-SSD-MobileNetV2.ipynb">notebook</a>.</p>
</blockquote>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>This lab has explored the implementation of object detection on edge devices like the Raspberry Pi, demonstrating the power and potential of running advanced computer vision tasks on resource-constrained hardware. We‚Äôve covered several vital aspects:</p>
<ol type="1">
<li><p><strong>Model Comparison</strong>: We examined different object detection models, including SSD-MobileNet, EfficientDet, FOMO, and YOLO, comparing their performance and trade-offs on edge devices.</p></li>
<li><p><strong>Training and Deployment</strong>: Using a custom dataset of boxes and wheels (labeled on Roboflow), we walked through the process of training models using Edge Impulse Studio and Ultralytics and deploying them on Raspberry Pi.</p></li>
<li><p><strong>Optimization Techniques</strong>: To improve inference speed on edge devices, we explored various optimization methods, such as model quantization (TFLite int8) and format conversion (e.g., to NCNN).</p></li>
<li><p><strong>Real-time Applications</strong>: The lab exemplified a real-time object detection web application, demonstrating how these models can be integrated into practical, interactive systems.</p></li>
<li><p><strong>Performance Considerations</strong>: Throughout the lab, we discussed the balance between model accuracy and inference speed, a critical consideration for edge AI applications.</p></li>
</ol>
<p>The ability to perform object detection on edge devices opens up numerous possibilities across various domains, from precision agriculture, industrial automation, and quality control to smart home applications and environmental monitoring. By processing data locally, these systems can offer reduced latency, improved privacy, and operation in environments with limited connectivity.</p>
<p>Looking ahead, potential areas for further exploration include: - Implementing multi-model pipelines for more complex tasks - Exploring hardware acceleration options for Raspberry Pi - Integrating object detection with other sensors for more comprehensive edge AI systems - Developing edge-to-cloud solutions that leverage both local processing and cloud resources</p>
<p>Object detection on edge devices can create intelligent, responsive systems that bring the power of AI directly into the physical world, opening up new frontiers in how we interact with and understand our environment.</p>
</section>
<section id="resources" class="level2">
<h2 class="anchored" data-anchor-id="resources">Resources</h2>
<ul>
<li><p><a href="https://universe.roboflow.com/marcelo-rovai-riila/box-versus-wheel-auto-dataset">Dataset (‚ÄúBox versus Wheel‚Äù)</a></p></li>
<li><p><a href="https://github.com/Mjrovai/EdgeML-with-Raspberry-Pi/blob/main/OBJ_DETEC/notebooks/SSD_MobileNetV1.ipynb">SSD-MobileNet Notebook on a Raspi</a></p></li>
<li><p><a href="https://github.com/Mjrovai/EdgeML-with-Raspberry-Pi/blob/main/OBJ_DETEC/notebooks/SSD_EfficientDet.ipynb">EfficientDet Notebook on a Raspi</a></p></li>
<li><p><a href="https://github.com/Mjrovai/EdgeML-with-Raspberry-Pi/blob/main/OBJ_DETEC/notebooks/EI-Linux-FOMO.ipynb">FOMO - EI Linux Notebook on a Raspi</a></p></li>
<li><p><a href="https://github.com/Mjrovai/EdgeML-with-Raspberry-Pi/blob/main/OBJ_DETEC/notebooks/yolov8_box_vs_wheel.ipynb">YOLOv8 Box versus Wheel Dataset Training on CoLab</a></p></li>
<li><p><a href="https://studio.edgeimpulse.com/public/515477/live">Edge Impulse Project - SSD MobileNet and FOMO</a></p></li>
<li><p><a href="https://github.com/Mjrovai/EdgeML-with-Raspberry-Pi/tree/main/OBJ_DETEC/python_scripts">Python Scripts</a></p></li>
<li><p><a href="https://github.com/Mjrovai/EdgeML-with-Raspberry-Pi/tree/main/OBJ_DETEC/models">Models</a></p></li>
</ul>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "Óßã";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
          // target, if specified
          link.setAttribute("target", "_blank");
          if (link.getAttribute("rel") === null) {
            link.setAttribute("rel", "noopener");
          }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
});
</script>
<script src="https://giscus.app/client.js" data-repo="harvard-edge/cs249r_book" data-repo-id="R_kgDOKQSOaw" data-category="General" data-category-id="DIC_kwDOKQSOa84CZ8Ry" data-mapping="title" data-reactions-enabled="1" data-emit-metadata="0" data-input-position="top" data-theme="light" data-lang="en" crossorigin="anonymous" async="">
</script>
<input type="hidden" id="giscus-base-theme" value="light">
<input type="hidden" id="giscus-alt-theme" value="dark">
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../../../../contents/labs/raspi/image_classification/image_classification.html" class="pagination-link" aria-label="Image Classification">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">Image Classification</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../../../../contents/labs/raspi/llm/llm.html" class="pagination-link" aria-label="Small Language Models (SLM)">
        <span class="nav-page-text">Small Language Models (SLM)</span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>Written, edited and curated by Prof.&nbsp;Vijay Janapa Reddi (Harvard University)</p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    <div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/harvard-edge/cs249r_book/edit/dev/contents/labs/raspi/object_detection/object_detection.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li><li><a href="https://github.com/harvard-edge/cs249r_book/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li><li><a href="https://github.com/harvard-edge/cs249r_book/blob/dev/contents/labs/raspi/object_detection/object_detection.qmd" class="toc-action"><i class="bi empty"></i>View source</a></li></ul></div></div>
    <div class="nav-footer-right">
<p>This book was built with <a href="https://quarto.org/">Quarto</a>.</p>
</div>
  </div>
</footer>
<script>var lightboxQuarto = GLightbox({"closeEffect":"zoom","descPosition":"bottom","loop":false,"openEffect":"zoom","selector":".lightbox"});
(function() {
  let previousOnload = window.onload;
  window.onload = () => {
    if (previousOnload) {
      previousOnload();
    }
    lightboxQuarto.on('slide_before_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      const href = trigger.getAttribute('href');
      if (href !== null) {
        const imgEl = window.document.querySelector(`a[href="${href}"] img`);
        if (imgEl !== null) {
          const srcAttr = imgEl.getAttribute("src");
          if (srcAttr && srcAttr.startsWith("data:")) {
            slideConfig.href = srcAttr;
          }
        }
      } 
    });
  
    lightboxQuarto.on('slide_after_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(slideNode);
      }
    });
  
  };
  
})();
          </script>




</body></html>