<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.32">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Small Language Models (SLM) ‚Äì Machine Learning Systems</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../../">
<link href="../../../../contents/labs/raspi/vlm/vlm.html" rel="next">
<link href="../../../../contents/labs/raspi/object_detection/object_detection.html" rel="prev">
<link href="../../../../favicon.png" rel="icon" type="image/png">
<script src="../../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../../site_libs/quarto-html/quarto-syntax-highlighting-fc6d358c97f25a8ea829b86655043430.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../../../site_libs/quarto-html/quarto-syntax-highlighting-dark-5c6c0ad7bdfb89369003da8042cd4f02.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../../site_libs/bootstrap/bootstrap-cee66da4b661a56ad5a2bc1f39f34084.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../../../site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script src="../../../../site_libs/quarto-contrib/glightbox/glightbox.min.js"></script>
<link href="../../../../site_libs/quarto-contrib/glightbox/glightbox.min.css" rel="stylesheet">
<link href="../../../../site_libs/quarto-contrib/glightbox/lightbox.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "~",
    "/"
  ],
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-M21L0CBCVN"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-M21L0CBCVN', { 'anonymize_ip': true});
</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-M21L0CBCVN"></script>
<script type="module" src="../../../../scripts/ai_menu/dist/bundle.js" defer=""></script>


</head>

<body class="nav-sidebar floating nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../../../index.html">
    <span class="navbar-title">Machine Learning Systems</span>
    </a>
  </div>
        <div class="quarto-navbar-tools tools-wide tools-end">
    <a href="https://github.com/harvard-edge/cs249r_book" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
    <a href="../../../../Machine-Learning-Systems.pdf" title="Download PDF" class="quarto-navigation-tool px-1" aria-label="Download PDF"><i class="bi bi-file-pdf"></i></a>
    <div class="dropdown">
      <a href="" title="Share" id="quarto-navigation-tool-dropdown-0" class="quarto-navigation-tool dropdown-toggle px-1" data-bs-toggle="dropdown" aria-expanded="false" role="link" aria-label="Share"><i class="bi bi-share"></i></a>
      <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="quarto-navigation-tool-dropdown-0">
          <li>
            <a class="dropdown-item quarto-navbar-tools-item" href="https://twitter.com/intent/tweet?url=|url|">
              <i class="bi bi-twitter pe-1"></i>
            Twitter
            </a>
          </li>
          <li>
            <a class="dropdown-item quarto-navbar-tools-item" href="https://www.facebook.com/sharer/sharer.php?u=|url|">
              <i class="bi bi-facebook pe-1"></i>
            Facebook
            </a>
          </li>
      </ul>
    </div>
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
  <a href="" class="quarto-reader-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleReader(); return false;" title="Toggle reader mode">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../../../contents/labs/raspi/raspi.html">Raspberry Pi</a></li><li class="breadcrumb-item"><a href="../../../../contents/labs/raspi/llm/llm.html">Small Language Models (SLM)</a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
  <div id="quarto-announcement" data-announcement-id="7e1ccd421e575f022f45ab9719f2cc9d" class="alert alert-primary hidden"><i class="bi bi-star-half quarto-announcement-icon"></i><div class="quarto-announcement-content">
<p>üìò <b>What‚Äôs New?</b> Happy New Year 2025! üéâ Chapters 1-5 have been expanded and improved, in addition to a brand-new Chapter 4!<br> üöÄ <b>Our Mission:</b> 1 ‚≠ê = 1 üë©‚Äçüéì | Every GitHub ‚≠ê supports learners, reflects their engagement, and affirms the field‚Äôs significance.<br> üôè <b>Thank You for Your Support!</b> Help us empower more learners by reaching 10,000 GitHub ‚≠ê <a href="https://github.com/harvard-edge/cs249r_book">Star us on GitHub</a>.</p>
</div><i class="bi bi-x-lg quarto-announcement-action"></i></div>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/frontmatter/foreword.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Author‚Äôs Note</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/frontmatter/about/about.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">About the Book</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/frontmatter/acknowledgements/acknowledgements.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Acknowledgements</span></a>
  </div>
</li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/frontmatter/ai/socratiq.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">ü§ñ SocratiQ AI</span></a>
  </div>
</li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item sidebar-item-section">
      <span class="sidebar-item-text sidebar-link text-start">
 <span class="menu-text"><div class="part">The Essentials</div></span></span>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/core/introduction/introduction.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/core/ml_systems/ml_systems.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">ML Systems</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/core/dl_primer/dl_primer.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">DL Primer</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/core/dnn_architectures/dnn_architectures.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">DNN Architectures</span></span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <span class="sidebar-item-text sidebar-link text-start">
 <span class="menu-text"><div class="part">Engineering Principles</div></span></span>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/core/workflow/workflow.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">AI Workflow</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/core/data_engineering/data_engineering.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Data Engineering</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/core/frameworks/frameworks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">AI Frameworks</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/core/training/training.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">AI Training</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/core/efficient_ai/efficient_ai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Efficient AI</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/core/optimizations/optimizations.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Model Optimizations</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/core/hw_acceleration/hw_acceleration.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">AI Acceleration</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/core/benchmarking/benchmarking.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Benchmarking AI</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/core/ops/ops.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">ML Operations</span></span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <span class="sidebar-item-text sidebar-link text-start">
 <span class="menu-text"><div class="part">Best Practices in AI</div></span></span>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/core/ondevice_learning/ondevice_learning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">On-Device Learning</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/core/privacy_security/privacy_security.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Security &amp; Privacy</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/core/responsible_ai/responsible_ai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Responsible AI</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/core/sustainable_ai/sustainable_ai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Sustainable AI</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/core/robust_ai/robust_ai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Robust AI</span></span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <span class="sidebar-item-text sidebar-link text-start">
 <span class="menu-text"><div class="part">Closing Perspectives</div></span></span>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/core/ai_for_good/ai_for_good.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">AI for Good</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/core/conclusion/conclusion.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Conclusion</span></span></a>
  </div>
</li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item sidebar-item-section">
      <span class="sidebar-item-text sidebar-link text-start">
 <span class="menu-text">LABS</span></span>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/labs/overview.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Overview</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/labs/getting_started.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Getting Started</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../../../contents/labs/arduino/nicla_vision/nicla_vision.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Nicla Vision</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/labs/arduino/nicla_vision/setup/setup.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Setup</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/labs/arduino/nicla_vision/image_classification/image_classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Image Classification</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/labs/arduino/nicla_vision/object_detection/object_detection.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Object Detection</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/labs/arduino/nicla_vision/kws/kws.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Keyword Spotting (KWS)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/labs/arduino/nicla_vision/motion_classification/motion_classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Motion Classification and Anomaly Detection</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../../../contents/labs/seeed/xiao_esp32s3/xiao_esp32s3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">XIAO ESP32S3</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-7" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/labs/seeed/xiao_esp32s3/setup/setup.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Setup</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/labs/seeed/xiao_esp32s3/image_classification/image_classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Image Classification</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/labs/seeed/xiao_esp32s3/object_detection/object_detection.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Object Detection</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/labs/seeed/xiao_esp32s3/kws/kws.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Keyword Spotting (KWS)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/labs/seeed/xiao_esp32s3/motion_classification/motion_classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Motion Classification and Anomaly Detection</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../../../contents/labs/raspi/raspi.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Raspberry Pi</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-8" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/labs/raspi/setup/setup.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Setup</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/labs/raspi/image_classification/image_classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Image Classification</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/labs/raspi/object_detection/object_detection.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Object Detection</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/labs/raspi/llm/llm.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">Small Language Models (SLM)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/labs/raspi/vlm/vlm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Vision-Language Models (VLM)</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../../../contents/labs/shared/shared.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Shared Labs</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-9" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-9" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/labs/shared/kws_feature_eng/kws_feature_eng.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">KWS Feature Engineering</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/labs/shared/dsp_spectral_features_block/dsp_spectral_features_block.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">DSP Spectral Features</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item sidebar-item-section">
      <span class="sidebar-item-text sidebar-link text-start">
 <span class="menu-text">REFERENCES</span></span>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/core/references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#overview" id="toc-overview" class="nav-link active" data-scroll-target="#overview">Overview</a></li>
  <li><a href="#setup" id="toc-setup" class="nav-link" data-scroll-target="#setup">Setup</a>
  <ul>
  <li><a href="#raspberry-pi-active-cooler" id="toc-raspberry-pi-active-cooler" class="nav-link" data-scroll-target="#raspberry-pi-active-cooler">Raspberry Pi Active Cooler</a></li>
  </ul></li>
  <li><a href="#generative-ai-genai" id="toc-generative-ai-genai" class="nav-link" data-scroll-target="#generative-ai-genai">Generative AI (GenAI)</a>
  <ul>
  <li><a href="#large-language-models-llms" id="toc-large-language-models-llms" class="nav-link" data-scroll-target="#large-language-models-llms">Large Language Models (LLMs)</a></li>
  <li><a href="#closed-vs-open-models" id="toc-closed-vs-open-models" class="nav-link" data-scroll-target="#closed-vs-open-models">Closed vs Open Models:</a></li>
  <li><a href="#small-language-models-slms" id="toc-small-language-models-slms" class="nav-link" data-scroll-target="#small-language-models-slms">Small Language Models (SLMs)</a></li>
  </ul></li>
  <li><a href="#ollama" id="toc-ollama" class="nav-link" data-scroll-target="#ollama">Ollama</a>
  <ul>
  <li><a href="#installing-ollama" id="toc-installing-ollama" class="nav-link" data-scroll-target="#installing-ollama">Installing Ollama</a></li>
  <li><a href="#meta-llama-3.2-1b3b" id="toc-meta-llama-3.2-1b3b" class="nav-link" data-scroll-target="#meta-llama-3.2-1b3b">Meta Llama 3.2 1B/3B</a></li>
  <li><a href="#google-gemma-2-2b" id="toc-google-gemma-2-2b" class="nav-link" data-scroll-target="#google-gemma-2-2b">Google Gemma 2 2B</a></li>
  <li><a href="#microsoft-phi3.5-3.8b" id="toc-microsoft-phi3.5-3.8b" class="nav-link" data-scroll-target="#microsoft-phi3.5-3.8b">Microsoft Phi3.5 3.8B</a></li>
  <li><a href="#multimodal-models" id="toc-multimodal-models" class="nav-link" data-scroll-target="#multimodal-models">Multimodal Models</a></li>
  <li><a href="#inspecting-local-resources" id="toc-inspecting-local-resources" class="nav-link" data-scroll-target="#inspecting-local-resources">Inspecting local resources</a></li>
  </ul></li>
  <li><a href="#ollama-python-library" id="toc-ollama-python-library" class="nav-link" data-scroll-target="#ollama-python-library">Ollama Python Library</a>
  <ul>
  <li><a href="#function-calling" id="toc-function-calling" class="nav-link" data-scroll-target="#function-calling">Function Calling</a>
  <ul class="collapse">
  <li><a href="#but-what-exactly-is-function-calling" id="toc-but-what-exactly-is-function-calling" class="nav-link" data-scroll-target="#but-what-exactly-is-function-calling">But what exactly is ‚Äúfunction calling‚Äù?</a></li>
  <li><a href="#lets-create-a-project." id="toc-lets-create-a-project." class="nav-link" data-scroll-target="#lets-create-a-project.">Let‚Äôs create a project.</a></li>
  </ul></li>
  <li><a href="#importing-libraries" id="toc-importing-libraries" class="nav-link" data-scroll-target="#importing-libraries">1. Importing Libraries</a></li>
  <li><a href="#defining-input-and-model" id="toc-defining-input-and-model" class="nav-link" data-scroll-target="#defining-input-and-model">2. Defining Input and Model</a></li>
  <li><a href="#defining-the-response-data-structure" id="toc-defining-the-response-data-structure" class="nav-link" data-scroll-target="#defining-the-response-data-structure">3. Defining the Response Data Structure</a></li>
  <li><a href="#setting-up-the-openai-client" id="toc-setting-up-the-openai-client" class="nav-link" data-scroll-target="#setting-up-the-openai-client">4. Setting Up the OpenAI Client</a></li>
  <li><a href="#generating-the-response" id="toc-generating-the-response" class="nav-link" data-scroll-target="#generating-the-response">5. Generating the Response</a></li>
  <li><a href="#calculating-the-distance" id="toc-calculating-the-distance" class="nav-link" data-scroll-target="#calculating-the-distance">6. Calculating the Distance</a></li>
  <li><a href="#adding-images" id="toc-adding-images" class="nav-link" data-scroll-target="#adding-images">Adding images</a></li>
  </ul></li>
  <li><a href="#slms-optimization-techniques" id="toc-slms-optimization-techniques" class="nav-link" data-scroll-target="#slms-optimization-techniques">SLMs: Optimization Techniques</a></li>
  <li><a href="#rag-implementation" id="toc-rag-implementation" class="nav-link" data-scroll-target="#rag-implementation">RAG Implementation</a>
  <ul>
  <li><a href="#a-simple-rag-project" id="toc-a-simple-rag-project" class="nav-link" data-scroll-target="#a-simple-rag-project">A simple RAG project</a></li>
  <li><a href="#going-further" id="toc-going-further" class="nav-link" data-scroll-target="#going-further">Going Further</a></li>
  </ul></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion">Conclusion</a></li>
  <li><a href="#resources" id="toc-resources" class="nav-link" data-scroll-target="#resources">Resources</a></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/harvard-edge/cs249r_book/edit/dev/contents/labs/raspi/llm/llm.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li><li><a href="https://github.com/harvard-edge/cs249r_book/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li><li><a href="https://github.com/harvard-edge/cs249r_book/blob/dev/contents/labs/raspi/llm/llm.qmd" class="toc-action"><i class="bi empty"></i>View source</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../../../contents/labs/raspi/raspi.html">Raspberry Pi</a></li><li class="breadcrumb-item"><a href="../../../../contents/labs/raspi/llm/llm.html">Small Language Models (SLM)</a></li></ol></nav>
<div class="quarto-title">
<h1 class="title">Small Language Models (SLM)</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/jpeg/cover.jpg" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="DALL¬∑E prompt - A 1950s-style cartoon illustration showing a Raspberry Pi running a small language model at the edge. The Raspberry Pi is stylized in a retro-futuristic way with rounded edges and chrome accents, connected to playful cartoonish sensors and devices. Speech bubbles are floating around, representing language processing, and the background has a whimsical landscape of interconnected devices with wires and small gadgets, all drawn in a vintage cartoon style. The color palette uses soft pastel colors and bold outlines typical of 1950s cartoons, giving a fun and nostalgic vibe to the scene."><img src="images/jpeg/cover.jpg" class="img-fluid figure-img" alt="DALL¬∑E prompt - A 1950s-style cartoon illustration showing a Raspberry Pi running a small language model at the edge. The Raspberry Pi is stylized in a retro-futuristic way with rounded edges and chrome accents, connected to playful cartoonish sensors and devices. Speech bubbles are floating around, representing language processing, and the background has a whimsical landscape of interconnected devices with wires and small gadgets, all drawn in a vintage cartoon style. The color palette uses soft pastel colors and bold outlines typical of 1950s cartoons, giving a fun and nostalgic vibe to the scene."></a></p>
<figcaption><em>DALL¬∑E prompt - A 1950s-style cartoon illustration showing a Raspberry Pi running a small language model at the edge. The Raspberry Pi is stylized in a retro-futuristic way with rounded edges and chrome accents, connected to playful cartoonish sensors and devices. Speech bubbles are floating around, representing language processing, and the background has a whimsical landscape of interconnected devices with wires and small gadgets, all drawn in a vintage cartoon style. The color palette uses soft pastel colors and bold outlines typical of 1950s cartoons, giving a fun and nostalgic vibe to the scene.</em></figcaption>
</figure>
</div>
<section id="overview" class="level2">
<h2 class="anchored" data-anchor-id="overview">Overview</h2>
<p>In the fast-growing area of artificial intelligence, edge computing presents an opportunity to decentralize capabilities traditionally reserved for powerful, centralized servers. This lab explores the practical integration of small versions of traditional large language models (LLMs) into a Raspberry Pi 5, transforming this edge device into an AI hub capable of real-time, on-site data processing.</p>
<p>As large language models grow in size and complexity, Small Language Models (SLMs) offer a compelling alternative for edge devices, striking a balance between performance and resource efficiency. By running these models directly on Raspberry Pi, we can create responsive, privacy-preserving applications that operate even in environments with limited or no internet connectivity.</p>
<p>This lab will guide you through setting up, optimizing, and leveraging SLMs on Raspberry Pi. We will explore the installation and utilization of <a href="https://ollama.com/">Ollama</a>. This open-source framework allows us to run LLMs locally on our machines (our desktops or edge devices such as the Raspberry Pis or NVidia Jetsons). Ollama is designed to be efficient, scalable, and easy to use, making it a good option for deploying AI models such as Microsoft Phi, Google Gemma, Meta Llama, and LLaVa (Multimodal). We will integrate some of those models into projects using Python‚Äôs ecosystem, exploring their potential in real-world scenarios (or at least point in this direction).</p>
<p><a href="images/jpeg/slm-example.jpg" class="lightbox" data-gallery="quarto-lightbox-gallery-2"><img src="images/jpeg/slm-example.jpg" class="img-fluid"></a></p>
</section>
<section id="setup" class="level2">
<h2 class="anchored" data-anchor-id="setup">Setup</h2>
<p>We could use any Raspi model in the previous labs, but here, the choice must be the Raspberry Pi 5 (Raspi-5). It is a robust platform that substantially upgrades the last version 4, equipped with the Broadcom BCM2712, a 2.4GHz quad-core 64-bit Arm Cortex-A76 CPU featuring Cryptographic Extension and enhanced caching capabilities. It boasts a VideoCore VII GPU, dual 4Kp60 HDMI¬Æ outputs with HDR, and a 4Kp60 HEVC decoder. Memory options include 4GB and 8GB of high-speed LPDDR4X SDRAM, with 8GB being our choice to run SLMs. It also features expandable storage via a microSD card slot and a PCIe 2.0 interface for fast peripherals such as M.2 SSDs (Solid State Drives).</p>
<blockquote class="blockquote">
<p>For real SSL applications, SSDs are a better option than SD cards.</p>
</blockquote>
<p>By the way, as <a href="https://www.hackster.io/aallan">Alasdair Allan</a> discussed, inferencing directly on the Raspberry Pi 5 CPU‚Äîwith no GPU acceleration‚Äîis now on par with the performance of the Coral TPU.</p>
<p><a href="images/png/bench.png" class="lightbox" data-gallery="quarto-lightbox-gallery-3"><img src="images/png/bench.png" class="img-fluid"></a></p>
<p>For more info, please see the complete article: <a href="https://www.hackster.io/news/benchmarking-tensorflow-and-tensorflow-lite-on-raspberry-pi-5-b9156d58a6a2?mc_cid=0cab3d08f4&amp;mc_eid=e96256ccba">Benchmarking TensorFlow and TensorFlow Lite on Raspberry Pi 5</a>.</p>
<section id="raspberry-pi-active-cooler" class="level3">
<h3 class="anchored" data-anchor-id="raspberry-pi-active-cooler">Raspberry Pi Active Cooler</h3>
<p>We suggest installing an Active Cooler, a dedicated clip-on cooling solution for Raspberry Pi 5 (Raspi-5), for this lab. It combines an aluminum heatsink with a temperature-controlled blower fan to keep the Raspi-5 operating comfortably under heavy loads, such as running SLMs.</p>
<p><a href="images/png/cooler.png" class="lightbox" data-gallery="quarto-lightbox-gallery-4"><img src="images/png/cooler.png" class="img-fluid"></a></p>
<p>The Active Cooler has pre-applied thermal pads for heat transfer and is mounted directly to the Raspberry Pi 5 board using spring-loaded push pins. The Raspberry Pi firmware actively manages it: at 60¬∞C, the blower‚Äôs fan will be turned on; at 67.5¬∞C, the fan speed will be increased; and finally, at 75¬∞C, the fan increases to full speed. The blower‚Äôs fan will spin down automatically when the temperature drops below these limits.</p>
<p><a href="images/png/temp_comp.png" class="lightbox" data-gallery="quarto-lightbox-gallery-5"><img src="images/png/temp_comp.png" class="img-fluid"></a></p>
<blockquote class="blockquote">
<p>To prevent overheating, all Raspberry Pi boards begin to throttle the processor when the temperature reaches 80¬∞Cand throttle even further when it reaches the maximum temperature of 85¬∞C (more detail <a href="https://www.raspberrypi.com/news/heating-and-cooling-raspberry-pi-5/">here</a>).</p>
</blockquote>
</section>
</section>
<section id="generative-ai-genai" class="level2">
<h2 class="anchored" data-anchor-id="generative-ai-genai">Generative AI (GenAI)</h2>
<p>Generative AI is an artificial intelligence system capable of creating new, original content across various mediums such as <strong>text, images, audio, and video</strong>. These systems learn patterns from existing data and use that knowledge to generate novel outputs that didn‚Äôt previously exist. <strong>Large Language Models (LLMs)</strong>, <strong>Small Language Models (SLMs)</strong>, and <strong>multimodal models</strong> can all be considered types of GenAI when used for generative tasks.</p>
<p>GenAI provides the conceptual framework for AI-driven content creation, with LLMs serving as powerful general-purpose text generators. SLMs adapt this technology for edge computing, while multimodal models extend GenAI capabilities across different data types. Together, they represent a spectrum of generative AI technologies, each with its strengths and applications, collectively driving AI-powered content creation and understanding.</p>
<section id="large-language-models-llms" class="level3">
<h3 class="anchored" data-anchor-id="large-language-models-llms">Large Language Models (LLMs)</h3>
<p>Large Language Models (LLMs) are advanced artificial intelligence systems that understand, process, and generate human-like text. These models are characterized by their massive scale in terms of the amount of data they are trained on and the number of parameters they contain. Critical aspects of LLMs include:</p>
<ol type="1">
<li><p><strong>Size</strong>: LLMs typically contain billions of parameters. For example, GPT-3 has 175 billion parameters, while some newer models exceed a trillion parameters.</p></li>
<li><p><strong>Training Data</strong>: They are trained on vast amounts of text data, often including books, websites, and other diverse sources, amounting to hundreds of gigabytes or even terabytes of text.</p></li>
<li><p><strong>Architecture</strong>: Most LLMs use <a href="https://en.wikipedia.org/wiki/Transformer_(deep_learning_architecture)">transformer-based architectures</a>, which allow them to process and generate text by paying attention to different parts of the input simultaneously.</p></li>
<li><p><strong>Capabilities</strong>: LLMs can perform a wide range of language tasks without specific fine-tuning, including:</p>
<ul>
<li>Text generation</li>
<li>Translation</li>
<li>Summarization</li>
<li>Question answering</li>
<li>Code generation</li>
<li>Logical reasoning</li>
</ul></li>
<li><p><strong>Few-shot Learning</strong>: They can often understand and perform new tasks with minimal examples or instructions.</p></li>
<li><p><strong>Resource-Intensive</strong>: Due to their size, LLMs typically require significant computational resources to run, often needing powerful GPUs or TPUs.</p></li>
<li><p><strong>Continual Development</strong>: The field of LLMs is rapidly evolving, with new models and techniques constantly emerging.</p></li>
<li><p><strong>Ethical Considerations</strong>: The use of LLMs raises important questions about bias, misinformation, and the environmental impact of training such large models.</p></li>
<li><p><strong>Applications</strong>: LLMs are used in various fields, including content creation, customer service, research assistance, and software development.</p></li>
<li><p><strong>Limitations</strong>: Despite their power, LLMs can produce incorrect or biased information and lack true understanding or reasoning capabilities.</p></li>
</ol>
<p>We must note that we use large models beyond text, calling them <em>multi-modal models</em>. These models integrate and process information from multiple types of input simultaneously. They are designed to understand and generate content across various forms of data, such as text, images, audio, and video.</p>
<p>Certainly. Let‚Äôs define open and closed models in the context of AI and language models:</p>
</section>
<section id="closed-vs-open-models" class="level3">
<h3 class="anchored" data-anchor-id="closed-vs-open-models">Closed vs Open Models:</h3>
<p><strong>Closed models</strong>, also called proprietary models, are AI models whose internal workings, code, and training data are not publicly disclosed. Examples: GPT-4 (by OpenAI), Claude (by Anthropic), Gemini (by Google).</p>
<p><strong>Open models</strong>, also known as open-source models, are AI models whose underlying code, architecture, and often training data are publicly available and accessible. Examples: Gemma (by Google), LLaMA (by Meta) and Phi (by Microsoft).</p>
<p>Open models are particularly relevant for running models on edge devices like Raspberry Pi as they can be more easily adapted, optimized, and deployed in resource-constrained environments. Still, it is crucial to verify their Licenses. Open models come with various open-source licenses that may affect their use in commercial applications, while closed models have clear, albeit restrictive, terms of service.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/png/llms-slm.png" class="lightbox" data-gallery="quarto-lightbox-gallery-6" title="Adapted from https://arxiv.org/pdf/2304.13712"><img src="images/png/llms-slm.png" class="img-fluid figure-img" alt="Adapted from https://arxiv.org/pdf/2304.13712"></a></p>
<figcaption>Adapted from https://arxiv.org/pdf/2304.13712</figcaption>
</figure>
</div>
</section>
<section id="small-language-models-slms" class="level3">
<h3 class="anchored" data-anchor-id="small-language-models-slms">Small Language Models (SLMs)</h3>
<p>In the context of edge computing on devices like Raspberry Pi, full-scale LLMs are typically too large and resource-intensive to run directly. This limitation has driven the development of smaller, more efficient models, such as the Small Language Models (SLMs).</p>
<p>SLMs are compact versions of LLMs designed to run efficiently on resource-constrained devices such as smartphones, IoT devices, and single-board computers like the Raspberry Pi. These models are significantly smaller in size and computational requirements than their larger counterparts while still retaining impressive language understanding and generation capabilities.</p>
<p>Key characteristics of SLMs include:</p>
<ol type="1">
<li><p><strong>Reduced parameter count</strong>: Typically ranging from a few hundred million to a few billion parameters, compared to two-digit billions in larger models.</p></li>
<li><p><strong>Lower memory footprint</strong>: Requiring, at most, a few gigabytes of memory rather than tens or hundreds of gigabytes.</p></li>
<li><p><strong>Faster inference time</strong>: Can generate responses in milliseconds to seconds on edge devices.</p></li>
<li><p><strong>Energy efficiency</strong>: Consuming less power, making them suitable for battery-powered devices.</p></li>
<li><p><strong>Privacy-preserving</strong>: Enabling on-device processing without sending data to cloud servers.</p></li>
<li><p><strong>Offline functionality</strong>: Operating without an internet connection.</p></li>
</ol>
<p>SLMs achieve their compact size through various techniques such as knowledge distillation, model pruning, and quantization. While they may not match the broad capabilities of larger models, SLMs excel in specific tasks and domains, making them ideal for targeted applications on edge devices.</p>
<blockquote class="blockquote">
<p>We will generally consider SLMs, language models with less than 5 billion parameters quantized to 4 bits.</p>
</blockquote>
<p>Examples of SLMs include compressed versions of models like Meta Llama, Microsoft PHI, and Google Gemma. These models enable a wide range of natural language processing tasks directly on edge devices, from text classification and sentiment analysis to question answering and limited text generation.</p>
<p>For more information on SLMs, the paper, <a href="https://arxiv.org/pdf/2408.11796">LLM Pruning and Distillation in Practice: The Minitron Approach</a>, provides an approach applying pruning and distillation to obtain SLMs from LLMs. And, <a href="https://arxiv.org/pdf/2409.15790">SMALL LANGUAGE MODELS: SURVEY, MEASUREMENTS, AND INSIGHTS</a>, presents a comprehensive survey and analysis of Small Language Models (SLMs), which are language models with 100 million to 5 billion parameters designed for resource-constrained devices.</p>
</section>
</section>
<section id="ollama" class="level2">
<h2 class="anchored" data-anchor-id="ollama">Ollama</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="https://ollama.com/public/ollama.png" class="lightbox" data-gallery="quarto-lightbox-gallery-7" title="ollama logo"><img src="https://ollama.com/public/ollama.png" class="img-fluid figure-img" alt="ollama logo"></a></p>
<figcaption>ollama logo</figcaption>
</figure>
</div>
<p><a href="https://ollama.com/">Ollama</a> is an open-source framework that allows us to run language models (LMs), large or small, locally on our machines. Here are some critical points about Ollama:</p>
<ol type="1">
<li><p><strong>Local Model Execution</strong>: Ollama enables running LMs on personal computers or edge devices such as the Raspi-5, eliminating the need for cloud-based API calls.</p></li>
<li><p><strong>Ease of Use</strong>: It provides a simple command-line interface for downloading, running, and managing different language models.</p></li>
<li><p><strong>Model Variety</strong>: Ollama supports various LLMs, including Phi, Gemma, Llama, Mistral, and other open-source models.</p></li>
<li><p><strong>Customization</strong>: Users can create and share custom models tailored to specific needs or domains.</p></li>
<li><p><strong>Lightweight</strong>: Designed to be efficient and run on consumer-grade hardware.</p></li>
<li><p><strong>API Integration</strong>: Offers an API that allows integration with other applications and services.</p></li>
<li><p><strong>Privacy-Focused</strong>: By running models locally, it addresses privacy concerns associated with sending data to external servers.</p></li>
<li><p><strong>Cross-Platform</strong>: Available for macOS, Windows, and Linux systems (our case, here).</p></li>
<li><p><strong>Active Development</strong>: Regularly updated with new features and model support.</p></li>
<li><p><strong>Community-Driven</strong>: Benefits from community contributions and model sharing.</p></li>
</ol>
<p>To learn more about what Ollama is and how it works under the hood, you should see this short video from <a href="https://www.youtube.com/@technovangelist">Matt Williams</a>, one of the founders of Ollama:</p>
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/90ozfdsQOKo" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
<blockquote class="blockquote">
<p>Matt has an entirely free course about Ollama that we recommend: </p><div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/9KEUFe4KQAI?si=D_-q3CMbHiT-twuy" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div><p></p>
</blockquote>
<section id="installing-ollama" class="level3">
<h3 class="anchored" data-anchor-id="installing-ollama">Installing Ollama</h3>
<p>Let‚Äôs set up and activate a Virtual Environment for working with Ollama:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="ex">python3</span> <span class="at">-m</span> venv ~/ollama</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="bu">source</span> ~/ollama/bin/activate</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>And run the command to install Ollama:</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="ex">curl</span> <span class="at">-fsSL</span> https://ollama.com/install.sh <span class="kw">|</span> <span class="fu">sh</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>As a result, an API will run in the background on <code>127.0.0.1:11434</code>. From now on, we can run Ollama via the terminal. For starting, let‚Äôs verify the Ollama version, which will also tell us that it is correctly installed:</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="ex">ollama</span> <span class="at">-v</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><a href="images/png/install-ollama-rpi5.png" class="lightbox" data-gallery="quarto-lightbox-gallery-8"><img src="images/png/install-ollama-rpi5.png" class="img-fluid"></a></p>
<p>On the <a href="https://ollama.com/library">Ollama Library page</a>, we can find the models Ollama supports. For example, by filtering by <code>Most popular</code>, we can see Meta Llama, Google Gemma, Microsoft Phi, LLaVa, etc.</p>
</section>
<section id="meta-llama-3.2-1b3b" class="level3">
<h3 class="anchored" data-anchor-id="meta-llama-3.2-1b3b">Meta Llama 3.2 1B/3B</h3>
<p><a href="images/png/small_and_multimodal.png" class="lightbox" data-gallery="quarto-lightbox-gallery-9"><img src="images/png/small_and_multimodal.png" class="img-fluid"></a></p>
<p>Let‚Äôs install and run our first small language model, <a href="https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/">Llama 3.2</a> 1B (and 3B). The Meta Llama 3.2 series comprises a set of multilingual generative language models available in 1 billion and 3 billion parameter sizes. These models are designed to process text input and generate text output. The instruction-tuned variants within this collection are specifically optimized for multilingual conversational applications, including tasks involving information retrieval and summarization with an agentic approach. When compared to many existing open-source and proprietary chat models, the Llama 3.2 instruction-tuned models demonstrate superior performance on widely-used industry benchmarks.</p>
<p>The 1B and 3B models were pruned from the Llama 8B, and then logits from the 8B and 70B models were used as token-level targets (token-level distillation). Knowledge distillation was used to recover performance (they were trained with 9 trillion tokens). The 1B model has 1,24B, quantized to integer (Q8_0), and the 3B, 3.12B parameters, with a Q4_0 quantization, which ends with a size of 1.3 GB and 2GB, respectively. Its context window is 131,072 tokens.</p>
<p><a href="images/jpeg/llama3_2.jpg" class="lightbox" data-gallery="quarto-lightbox-gallery-10"><img src="images/jpeg/llama3_2.jpg" class="img-fluid"></a></p>
<p><strong>Install and run the</strong> <strong>Model</strong></p>
<div class="sourceCode" id="cb4"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="ex">ollama</span> run llama3.2:1b</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Running the model with the command before, we should have the Ollama prompt available for us to input a question and start chatting with the LLM model; for example,</p>
<p><code>&gt;&gt;&gt; What is the capital of France?</code></p>
<p>Almost immediately, we get the correct answer:</p>
<p><code>The capital of France is Paris.</code></p>
<p>Using the option <code>--verbose</code> when calling the model will generate several statistics about its performance (The model will be polling only the first time we run the command).</p>
<p><a href="images/png/llama3_2_1b_performance.png" class="lightbox" data-gallery="quarto-lightbox-gallery-11"><img src="images/png/llama3_2_1b_performance.png" class="img-fluid"></a></p>
<p>Each metric gives insights into how the model processes inputs and generates outputs. Here‚Äôs a breakdown of what each metric means:</p>
<ul>
<li><strong>Total Duration (2.620170326s)</strong>: This is the complete time taken from the start of the command to the completion of the response. It encompasses loading the model, processing the input prompt, and generating the response.</li>
<li><strong>Load Duration (39.947908ms)</strong>: This duration indicates the time to load the model or necessary components into memory. If this value is minimal, it can suggest that the model was preloaded or that only a minimal setup was required.</li>
<li><strong>Prompt Eval Count (32 tokens)</strong>: The number of tokens in the input prompt. In NLP, tokens are typically words or subwords, so this count includes all the tokens that the model evaluated to understand and respond to the query.</li>
<li><strong>Prompt Eval Duration (1.644773s)</strong>: This measures the model‚Äôs time to evaluate or process the input prompt. It accounts for the bulk of the total duration, implying that understanding the query and preparing a response is the most time-consuming part of the process.</li>
<li><strong>Prompt Eval Rate (19.46 tokens/s)</strong>: This rate indicates how quickly the model processes tokens from the input prompt. It reflects the model‚Äôs speed in terms of natural language comprehension.</li>
<li><strong>Eval Count (8 token(s))</strong>: This is the number of tokens in the model‚Äôs response, which in this case was, ‚ÄúThe capital of France is Paris.‚Äù</li>
<li><strong>Eval Duration (889.941ms)</strong>: This is the time taken to generate the output based on the evaluated input. It‚Äôs much shorter than the prompt evaluation, suggesting that generating the response is less complex or computationally intensive than understanding the prompt.</li>
<li><strong>Eval Rate (8.99 tokens/s)</strong>: Similar to the prompt eval rate, this indicates the speed at which the model generates output tokens. It‚Äôs a crucial metric for understanding the model‚Äôs efficiency in output generation.</li>
</ul>
<p>This detailed breakdown can help understand the computational demands and performance characteristics of running SLMs like Llama on edge devices like the Raspberry Pi 5. It shows that while prompt evaluation is more time-consuming, the actual generation of responses is relatively quicker. This analysis is crucial for optimizing performance and diagnosing potential bottlenecks in real-time applications.</p>
<p>Loading and running the 3B model, we can see the difference in performance for the same prompt;</p>
<p><a href="images/png/llama3_2_3b_performance.png" class="lightbox" data-gallery="quarto-lightbox-gallery-12"><img src="images/png/llama3_2_3b_performance.png" class="img-fluid"></a></p>
<p>The eval rate is lower, 5.3 tokens/s versus 9 tokens/s with the smaller model.</p>
<p>When question about</p>
<p><code>&gt;&gt;&gt; What is the distance between Paris and Santiago, Chile?</code></p>
<p>The 1B model answered <code>9,841 kilometers (6,093 miles)</code>, which is inaccurate, and the 3B model answered <code>7,300 miles (11,700 km)</code>, which is close to the correct (11,642 km).</p>
<p>Let‚Äôs ask for the Paris‚Äôs coordinates:</p>
<p><code>&gt;&gt;&gt; what is the latitude and longitude of Paris?</code></p>
<div class="sourceCode" id="cb5"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="ex">The</span> latitude and longitude of Paris are 48.8567¬∞ N <span class="er">(</span><span class="ex">48¬∞55</span><span class="st">' </span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="st">42" N) and 2.3510¬∞ E (2¬∞22'</span> 8<span class="st">" E), respectively.</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><a href="images/png/paris-lat-lon.png" class="lightbox" data-gallery="quarto-lightbox-gallery-13"><img src="images/png/paris-lat-lon.png" class="img-fluid"></a></p>
<p>Both 1B and 3B models gave correct answers.</p>
</section>
<section id="google-gemma-2-2b" class="level3">
<h3 class="anchored" data-anchor-id="google-gemma-2-2b">Google Gemma 2 2B</h3>
<p>Let‚Äôs install <a href="https://ollama.com/library/gemma2:2b">Gemma 2</a>, a high-performing and efficient model available in three sizes: 2B, 9B, and 27B. We will install <a href="https://huggingface.co/collections/google/gemma-2-2b-release-66a20f3796a2ff2a7c76f98f"><strong>Gemma 2 2B</strong></a>, a lightweight model trained with 2 trillion tokens that produces outsized results by learning from larger models through distillation. The model has 2.6 billion parameters and a Q4_0 quantization, which ends with a size of 1.6 GB. Its context window is 8,192 tokens.</p>
<p><a href="images/png/gemma_2.png" class="lightbox" data-gallery="quarto-lightbox-gallery-14"><img src="images/png/gemma_2.png" class="img-fluid"></a></p>
<p><strong>Install and run the</strong> <strong>Model</strong></p>
<div class="sourceCode" id="cb6"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="ex">ollama</span> run gemma2:2b <span class="at">--verbose</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Running the model with the command before, we should have the Ollama prompt available for us to input a question and start chatting with the LLM model; for example,</p>
<p><code>&gt;&gt;&gt; What is the capital of France?</code></p>
<p>Almost immediately, we get the correct answer:</p>
<p><code>The capital of France is **Paris**. üóº</code></p>
<p>And it‚Äô statistics.</p>
<p><a href="images/png/gemma.png" class="lightbox" data-gallery="quarto-lightbox-gallery-15"><img src="images/png/gemma.png" class="img-fluid"></a></p>
<p>We can see that Gemma 2:2B has around the same performance as Lama 3.2:3B, but having less parameters.</p>
<p><strong>Other examples:</strong></p>
<div class="sourceCode" id="cb7"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> What <span class="ex">is</span> the distance between Paris and Santiago, Chile<span class="pp">?</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="ex">The</span> distance between Paris, France and Santiago, Chile is </span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a><span class="ex">approximately</span> <span class="pp">**</span>7,000 miles <span class="er">(</span><span class="ex">11,267</span> kilometers<span class="kw">)</span><span class="ex">**.</span> </span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a><span class="ex">Keep</span> in mind that this is a straight-line distance, and actual </span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a><span class="ex">travel</span> distance can vary depending on the chosen routes and any </span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a><span class="ex">stops</span> along the way. ‚úàÔ∏è<span class="kw">`</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Also, a good response but less accurate than Llama3.2:3B.</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> what <span class="ex">is</span> the latitude and longitude of Paris<span class="pp">?</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a><span class="ex">You</span> got it! Here are the latitudes and longitudes of Paris, </span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a><span class="ex">France:</span></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a><span class="ex">*</span> <span class="pp">**</span>Latitude:<span class="pp">**</span> 48.8566¬∞ N <span class="er">(</span><span class="ex">north</span><span class="kw">)</span></span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a><span class="ex">*</span> <span class="pp">**</span>Longitude:<span class="pp">**</span> 2.3522¬∞ E <span class="er">(</span><span class="ex">east</span><span class="kw">)</span> </span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a><span class="ex">Let</span> me know if you<span class="st">'d like to explore more about Paris or its </span></span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a><span class="st">location! üóºüá´üá∑ </span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>A good and accurate answer (a little more verbose than the Llama answers).</p>
</section>
<section id="microsoft-phi3.5-3.8b" class="level3">
<h3 class="anchored" data-anchor-id="microsoft-phi3.5-3.8b">Microsoft Phi3.5 3.8B</h3>
<p>Let‚Äôs pull a bigger (but still tiny) model, the <a href="https://ollama.com/library/phi3.5">PHI3.5,</a> a 3.8B lightweight state-of-the-art open model by Microsoft. The model belongs to the Phi-3 model family and supports <code>128K token</code> context length and the languages: Arabic, Chinese, Czech, Danish, Dutch, English, Finnish, French, German, Hebrew, Hungarian, Italian, Japanese, Korean, Norwegian, Polish, Portuguese, Russian, Spanish, Swedish, Thai, Turkish and Ukrainian.</p>
<p>The model size, in terms of bytes, will depend on the specific quantization format used. The size can go from 2-bit quantization (<code>q2_k</code>) of 1.4GB (higher performance/lower quality) to 16-bit quantization (fp-16) of 7.6GB (lower performance/higher quality).</p>
<p>Let‚Äôs run the 4-bit quantization (<code>Q4_0</code>), which will need 2.2GB of RAM, with an intermediary trade-off regarding output quality and performance.</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="ex">ollama</span> run phi3.5:3.8b <span class="at">--verbose</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<blockquote class="blockquote">
<p>You can use <code>run</code> or <code>pull</code> to download the model. What happens is that Ollama keeps note of the pulled models, and once the PHI3 does not exist, before running it, Ollama pulls it.</p>
</blockquote>
<p>Let‚Äôs enter with the same prompt used before:</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> What <span class="ex">is</span> the capital of France<span class="pp">?</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a><span class="ex">The</span> capital of France is Paris. It<span class="st">' extradites significant </span></span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a><span class="st">historical, cultural, and political importance to the country as </span></span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a><span class="st">well as being a major European city known for its art, fashion, </span></span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a><span class="st">gastronomy, and culture. Its influence extends beyond national </span></span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a><span class="st">borders, with millions of tourists visiting each year from around </span></span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a><span class="st">the globe. The Seine River flows through Paris before it reaches </span></span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a><span class="st">the broader English Channel at Le Havre. Moreover, France is one </span></span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a><span class="st">of Europe'</span>s leading economies with its capital playing a key role </span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a><span class="ex">...</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>The answer was very ‚Äúverbose‚Äù, let‚Äôs specify a better prompt:</p>
<p><a href="images/png/paris-2.png" class="lightbox" data-gallery="quarto-lightbox-gallery-16"><img src="images/png/paris-2.png" class="img-fluid"></a></p>
<p>In this case, the answer was still longer than we expected, with an eval rate of 2.25 tokens/s, more than double that of Gemma and Llama.</p>
<blockquote class="blockquote">
<p>Choosing the most appropriate prompt is one of the most important skills to be used with LLMs, no matter its size.</p>
</blockquote>
<p>When we asked the same questions about distance and Latitude/Longitude, we did not get a good answer for a distance of <code>13,507 kilometers (8,429 miles)</code>, but it was OK for coordinates. Again, it could have been less verbose (more than 200 tokens for each answer).</p>
<p>We can use any model as an assistant since their speed is relatively decent, but on September 24 (2023), the Llama2:3B is a better choice. You should try other models, depending on your needs. <a href="https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard">ü§ó Open LLM Leaderboard</a> can give you an idea about the best models in size, benchmark, license, etc.</p>
<blockquote class="blockquote">
<p>The best model to use is the one fit for your specific necessity. Also, take into consideration that this field evolves with new models everyday.</p>
</blockquote>
</section>
<section id="multimodal-models" class="level3">
<h3 class="anchored" data-anchor-id="multimodal-models">Multimodal Models</h3>
<p>Multimodal models are artificial intelligence (AI) systems that can process and understand information from multiple sources, such as images, text, audio, and video. In our context, multimodal LLMs can process various inputs, including text, images, and audio, as prompts and convert those prompts into various outputs, not just the source type.</p>
<p>We will work here with <a href="https://ollama.com/library/llava-phi3:3.8b">LLaVA-Phi-3</a>, a fine-tuned LLaVA model from Phi 3 Mini 4k. It has strong performance benchmarks that are on par with the original <a href="https://llava-vl.github.io/">LLaVA</a> (Large Language and Vision Assistant) model.</p>
<p>The LLaVA-Phi-3 is an end-to-end trained large multimodal model designed to understand and generate content based on visual inputs (images) and textual instructions. It combines the capabilities of a visual encoder and a language model to process and respond to multimodal inputs.</p>
<p>Let‚Äôs install the model:</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="ex">ollama</span> run llava-phi3:3.8b <span class="at">--verbose</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Let‚Äôs start with a text input:</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> You <span class="ex">are</span> a helpful AI assistant. What is the capital of France<span class="pp">?</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a><span class="ex">As</span> an AI language model, I can tell you that the capital of France </span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a><span class="ex">is</span> Paris. It<span class="st">'s not only the largest city in the country but also </span></span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a><span class="st">serves as its political and administrative center. Paris is known </span></span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a><span class="st">for its iconic landmarks such as the Eiffel Tower, Notre-Dame </span></span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a><span class="st">Cathedral, and the Louvre Museum. The city has a rich history, </span></span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a><span class="st">beautiful architecture, and is widely considered to be one of the </span></span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a><span class="st">most romantic cities in the world.</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>The response took around 30s, with an eval rate of 3.93 tokens/s! Not bad!</p>
<p>But let us know to enter with an image as input. For that, let‚Äôs create a directory for working:</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="bu">cd</span> Documents/</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a><span class="fu">mkdir</span> OLLAMA</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a><span class="bu">cd</span> OLLAMA</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Let‚Äôs download a 640x320 image from the internet, for example (Wikipedia: <a href="https://upload.wikimedia.org/wikipedia/commons/thumb/4/4b/La_Tour_Eiffel_vue_de_la_Tour_Saint-Jacques%2C_Paris_ao%C3%BBt_2014_%282%29.jpg/640px-La_Tour_Eiffel_vue_de_la_Tour_Saint-Jacques%2C_Paris_ao%C3%BBt_2014_%282%29.jpg">Paris, France)</a>:</p>
<p><a href="images/jpeg/paris.jpg" class="lightbox" data-gallery="quarto-lightbox-gallery-17"><img src="images/jpeg/paris.jpg" class="img-fluid"></a></p>
<p>Using FileZilla, for example, let‚Äôs upload the image to the OLLAMA folder at the Raspi-5 and name it <code>image_test_1.jpg</code>. We should have the whole image path (we can use <code>pwd</code> to get it).</p>
<p><code>/home/mjrovai/Documents/OLLAMA/image_test_1.jpg</code></p>
<p>If you use a desktop, you can copy the image path by clicking the image with the mouse‚Äôs right button.</p>
<p><a href="images/png/image_test-path.png" class="lightbox" data-gallery="quarto-lightbox-gallery-18"><img src="images/png/image_test-path.png" class="img-fluid"></a></p>
<p>Let‚Äôs enter with this prompt:</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> Describe <span class="ex">the</span> image /home/mjrovai/Documents/OLLAMA/image_test_1.jpg</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>The result was great, but the overall latency was significant; almost 4 minutes to perform the inference.</p>
<p><a href="images/png/paris-infer-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-19"><img src="images/png/paris-infer-1.png" class="img-fluid"></a></p>
</section>
<section id="inspecting-local-resources" class="level3">
<h3 class="anchored" data-anchor-id="inspecting-local-resources">Inspecting local resources</h3>
<p>Using htop, we can monitor the resources running on our device.</p>
<div class="sourceCode" id="cb15"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="ex">htop</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>During the time that the model is running, we can inspect the resources:</p>
<p><a href="images/png/htop.png" class="lightbox" data-gallery="quarto-lightbox-gallery-20"><img src="images/png/htop.png" class="img-fluid"></a></p>
<p>All four CPUs run at almost 100% of their capacity, and the memory used with the model loaded is <code>3.24GB</code>. Exiting Ollama, the memory goes down to around <code>377MB</code> (with no desktop).</p>
<p>It is also essential to monitor the temperature. When running the Raspberry with a desktop, you can have the temperature shown on the taskbar:</p>
<p><a href="images/png/resourses-temp.png" class="lightbox" data-gallery="quarto-lightbox-gallery-21"><img src="images/png/resourses-temp.png" class="img-fluid"></a></p>
<p>If you are ‚Äúheadless‚Äù, the temperature can be monitored with the command:</p>
<div class="sourceCode" id="cb16"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="ex">vcgencmd</span> measure_temp</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>If you are doing nothing, the temperature is around <code>50¬∞C</code> for CPUs running at 1%. During inference, with the CPUs at 100%, the temperature can rise to almost <code>70¬∞C</code>. This is OK and means the active cooler is working, keeping the temperature below 80¬∞C / 85¬∞C (its limit).</p>
</section>
</section>
<section id="ollama-python-library" class="level2">
<h2 class="anchored" data-anchor-id="ollama-python-library">Ollama Python Library</h2>
<p>So far, we have explored SLMs‚Äô chat capability using the command line on a terminal. However, we want to integrate those models into our projects, so Python seems to be the right path. The good news is that Ollama has such a library.</p>
<p>The <a href="https://github.com/ollama/ollama-python">Ollama Python library</a> simplifies interaction with advanced LLM models, enabling more sophisticated responses and capabilities, besides providing the easiest way to integrate Python 3.8+ projects with <a href="https://github.com/ollama/ollama">Ollama.</a></p>
<p>For a better understanding of how to create apps using Ollama with Python, we can follow <a href="https://www.youtube.com/@technovangelist">Matt Williams‚Äôs videos</a>, as the one below:</p>
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/_4K20tOsXK8" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
<p><strong>Installation:</strong></p>
<p>In the terminal, run the command:</p>
<div class="sourceCode" id="cb17"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="ex">pip</span> install ollama</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>We will need a text editor or an IDE to create a Python script. If you run the Raspberry OS on a desktop, several options, such as Thonny and Geany, have already been installed by default (accessed by <code>[Menu][Programming]</code>). You can download other IDEs, such as Visual Studio Code, from <code>[Menu][Recommended Software]</code>. When the window pops up, go to <code>[Programming]</code>, select the option of your choice, and press <code>[Apply]</code>.</p>
<p><a href="images/png/menu.png" class="lightbox" data-gallery="quarto-lightbox-gallery-22"><img src="images/png/menu.png" class="img-fluid"></a></p>
<p>If you prefer using Jupyter Notebook for development:</p>
<div class="sourceCode" id="cb18"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="ex">pip</span> install jupyter</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a><span class="ex">jupyter</span> notebook <span class="at">--generate-config</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>To run Jupyter Notebook, run the command (change the IP address for yours):</p>
<div class="sourceCode" id="cb19"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="ex">jupyter</span> notebook <span class="at">--ip</span><span class="op">=</span>192.168.4.209 <span class="at">--no-browser</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>On the terminal, you can see the local URL address to open the notebook:</p>
<p><a href="images/png/jupyter.png" class="lightbox" data-gallery="quarto-lightbox-gallery-23"><img src="images/png/jupyter.png" class="img-fluid"></a></p>
<p>We can access it from another computer by entering the Raspberry Pi‚Äôs IP address and the provided token in a web browser (we should copy it from the terminal).</p>
<p>In our working directory in the Raspi, we will create a new Python 3 notebook.</p>
<p>Let‚Äôs enter with a very simple script to verify the installed models:</p>
<div class="sourceCode" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> ollama</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>ollama.<span class="bu">list</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>All the models will be printed as a dictionary, for example:</p>
<div class="sourceCode" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a>  {<span class="st">'name'</span>: <span class="st">'gemma2:2b'</span>,</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>   <span class="st">'model'</span>: <span class="st">'gemma2:2b'</span>,</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a>   <span class="st">'modified_at'</span>: <span class="st">'2024-09-24T19:30:40.053898094+01:00'</span>,</span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a>   <span class="st">'size'</span>: <span class="dv">1629518495</span>,</span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a>   <span class="st">'digest'</span>: <span class="st">'8ccf136fdd5298f3ffe2d69862750ea7fb56555fa4d5b18c04e3fa4d82ee09d7'</span>,</span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a>   <span class="st">'details'</span>: {<span class="st">'parent_model'</span>: <span class="st">''</span>,</span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a>    <span class="st">'format'</span>: <span class="st">'gguf'</span>,</span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a>    <span class="st">'family'</span>: <span class="st">'gemma2'</span>,</span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"></a>    <span class="st">'families'</span>: [<span class="st">'gemma2'</span>],</span>
<span id="cb21-10"><a href="#cb21-10" aria-hidden="true" tabindex="-1"></a>    <span class="st">'parameter_size'</span>: <span class="st">'2.6B'</span>,</span>
<span id="cb21-11"><a href="#cb21-11" aria-hidden="true" tabindex="-1"></a>    <span class="st">'quantization_level'</span>: <span class="st">'Q4_0'</span>}}]}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Let‚Äôs repeat one of the questions that we did before, but now using <code>ollama.generate()</code> from Ollama python library. This API will generate a response for the given prompt with the provided model. This is a streaming endpoint, so there will be a series of responses. The final response object will include statistics and additional data from the request.</p>
<div class="sourceCode" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a>MODEL <span class="op">=</span> <span class="st">'gemma2:2b'</span></span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>PROMPT <span class="op">=</span> <span class="st">'What is the capital of France?'</span></span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a>res <span class="op">=</span> ollama.generate(model<span class="op">=</span>MODEL, prompt<span class="op">=</span>PROMPT)</span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span> (res)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>In case you are running the code as a Python script, you should save it, for example, test_ollama.py. You can use the IDE to run it or do it directly on the terminal. Also, remember that you should always call the model and define it when running a stand-alone script.</p>
<div class="sourceCode" id="cb23"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="ex">python</span> test_ollama.py</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>As a result, we will have the model response in a JSON format:</p>
<pre><code>{'model': 'gemma2:2b', 'created_at': '2024-09-25T14:43:31.869633807Z', 
'response': 'The capital of France is **Paris**. üá´üá∑ \n', 'done': True, 
'done_reason': 'stop', 'context': [106, 1645, 108, 1841, 603, 573, 6037, 576,
6081, 235336, 107, 108, 106, 2516, 108, 651, 6037, 576, 6081, 603, 5231, 29437, 
168428, 235248, 244304, 241035, 235248, 108], 'total_duration': 24259469458, 
'load_duration': 19830013859, 'prompt_eval_count': 16, 'prompt_eval_duration': 
1908757000, 'eval_count': 14, 'eval_duration': 2475410000}</code></pre>
<p>As we can see, several pieces of information are generated, such as:</p>
<ul>
<li><strong>response</strong>: the main output text generated by the model in response to our prompt.
<ul>
<li><code>The capital of France is **Paris**. üá´üá∑</code></li>
</ul></li>
<li><strong>context</strong>: the token IDs representing the input and context used by the model. Tokens are numerical representations of text used for processing by the language model.
<ul>
<li><code>[106, 1645, 108, 1841, 603, 573, 6037, 576, 6081, 235336, 107, 108,</code> <code>106, 2516, 108, 651, 6037, 576, 6081, 603, 5231, 29437, 168428,</code> <code>235248, 244304, 241035, 235248, 108]</code></li>
</ul></li>
</ul>
<p>The Performance Metrics:</p>
<ul>
<li><strong>total_duration</strong>: The total time taken for the operation in nanoseconds. In this case, approximately 24.26 seconds.</li>
<li><strong>load_duration</strong>: The time taken to load the model or components in nanoseconds. About 19.83 seconds.</li>
<li><strong>prompt_eval_duration</strong>: The time taken to evaluate the prompt in nanoseconds. Around 16 nanoseconds.</li>
<li><strong>eval_count</strong>: The number of tokens evaluated during the generation. Here, 14 tokens.</li>
<li><strong>eval_duration</strong>: The time taken for the model to generate the response in nanoseconds. Approximately 2.5 seconds.</li>
</ul>
<p>But, what we want is the plain ‚Äòresponse‚Äô and, perhaps for analysis, the total duration of the inference, so let‚Äôs change the code to extract it from the dictionary:</p>
<div class="sourceCode" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="sc">{</span>res[<span class="st">'response'</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss"> [INFO] Total Duration: </span><span class="sc">{</span>(res[<span class="st">'total_duration'</span>]<span class="op">/</span><span class="fl">1e9</span>)<span class="sc">:.2f}</span><span class="ss"> seconds"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Now, we got:</p>
<div class="sourceCode" id="cb26"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="ex">The</span> capital of France is <span class="pp">**</span>Paris<span class="pp">**</span>. üá´üá∑ </span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a> <span class="ex">[INFO]</span> Total Duration: 24.26 seconds</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><strong>Using Ollama.chat()</strong></p>
<p>Another way to get our response is to use <code>ollama.chat()</code>, which generates the next message in a chat with a provided model. This is a streaming endpoint, so a series of responses will occur. Streaming can be disabled using <code>"stream": false</code>. The final response object will also include statistics and additional data from the request.</p>
<div class="sourceCode" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a>PROMPT_1 <span class="op">=</span> <span class="st">'What is the capital of France?'</span></span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a>response <span class="op">=</span> ollama.chat(model<span class="op">=</span>MODEL, messages<span class="op">=</span>[</span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a>{<span class="st">'role'</span>: <span class="st">'user'</span>,<span class="st">'content'</span>: PROMPT_1,},])</span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a>resp_1 <span class="op">=</span> response[<span class="st">'message'</span>][<span class="st">'content'</span>]</span>
<span id="cb27-6"><a href="#cb27-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="sc">{</span>resp_1<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb27-7"><a href="#cb27-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss"> [INFO] Total Duration: </span><span class="sc">{</span>(res[<span class="st">'total_duration'</span>]<span class="op">/</span><span class="fl">1e9</span>)<span class="sc">:.2f}</span><span class="ss"> seconds"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>The answer is the same as before.</p>
<p>An important consideration is that by using <code>ollama.generate()</code>, the response is ‚Äúclear‚Äù from the model‚Äôs ‚Äúmemory‚Äù after the end of inference (only used once), but If we want to keep a conversation, we must use <code>ollama.chat()</code>. Let‚Äôs see it in action:</p>
<div class="sourceCode" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a>PROMPT_1 <span class="op">=</span> <span class="st">'What is the capital of France?'</span></span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a>response <span class="op">=</span> ollama.chat(model<span class="op">=</span>MODEL, messages<span class="op">=</span>[</span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a>{<span class="st">'role'</span>: <span class="st">'user'</span>,<span class="st">'content'</span>: PROMPT_1,},])</span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a>resp_1 <span class="op">=</span> response[<span class="st">'message'</span>][<span class="st">'content'</span>]</span>
<span id="cb28-5"><a href="#cb28-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="sc">{</span>resp_1<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb28-6"><a href="#cb28-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss"> [INFO] Total Duration: </span><span class="sc">{</span>(response[<span class="st">'total_duration'</span>]<span class="op">/</span><span class="fl">1e9</span>)<span class="sc">:.2f}</span><span class="ss"> seconds"</span>)</span>
<span id="cb28-7"><a href="#cb28-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-8"><a href="#cb28-8" aria-hidden="true" tabindex="-1"></a>PROMPT_2 <span class="op">=</span> <span class="st">'and of Italy?'</span></span>
<span id="cb28-9"><a href="#cb28-9" aria-hidden="true" tabindex="-1"></a>response <span class="op">=</span> ollama.chat(model<span class="op">=</span>MODEL, messages<span class="op">=</span>[</span>
<span id="cb28-10"><a href="#cb28-10" aria-hidden="true" tabindex="-1"></a>{<span class="st">'role'</span>: <span class="st">'user'</span>,<span class="st">'content'</span>: PROMPT_1,},</span>
<span id="cb28-11"><a href="#cb28-11" aria-hidden="true" tabindex="-1"></a>{<span class="st">'role'</span>: <span class="st">'assistant'</span>,<span class="st">'content'</span>: resp_1,},</span>
<span id="cb28-12"><a href="#cb28-12" aria-hidden="true" tabindex="-1"></a>{<span class="st">'role'</span>: <span class="st">'user'</span>,<span class="st">'content'</span>: PROMPT_2,},])</span>
<span id="cb28-13"><a href="#cb28-13" aria-hidden="true" tabindex="-1"></a>resp_2 <span class="op">=</span> response[<span class="st">'message'</span>][<span class="st">'content'</span>]</span>
<span id="cb28-14"><a href="#cb28-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="sc">{</span>resp_2<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb28-15"><a href="#cb28-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss"> [INFO] Total Duration: </span><span class="sc">{</span>(response[<span class="st">'total_duration'</span>]<span class="op">/</span><span class="fl">1e9</span>)<span class="sc">:.2f}</span><span class="ss"> seconds"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>In the above code, we are running two queries, and the second prompt considers the result of the first one.</p>
<p>Here is how the model responded:</p>
<div class="sourceCode" id="cb29"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="ex">The</span> capital of France is <span class="pp">**</span>Paris<span class="pp">**</span>. üá´üá∑ </span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a> <span class="ex">[INFO]</span> Total Duration: 2.82 seconds</span>
<span id="cb29-4"><a href="#cb29-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-5"><a href="#cb29-5" aria-hidden="true" tabindex="-1"></a><span class="ex">The</span> capital of Italy is <span class="pp">**</span>Rome<span class="pp">**</span>. üáÆüáπ </span>
<span id="cb29-6"><a href="#cb29-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-7"><a href="#cb29-7" aria-hidden="true" tabindex="-1"></a> <span class="ex">[INFO]</span> Total Duration: 4.46 seconds</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><strong>Getting an image description</strong>:</p>
<p>In the same way that we have used the <code>LlaVa-PHI-3</code> model with the command line to analyze an image, the same can be done here with Python. Let‚Äôs use the same image of Paris, but now with the <code>ollama.generate()</code>:</p>
<div class="sourceCode" id="cb30"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a>MODEL <span class="op">=</span> <span class="st">'llava-phi3:3.8b'</span></span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a>PROMPT <span class="op">=</span> <span class="st">"Describe this picture"</span></span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-4"><a href="#cb30-4" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> <span class="bu">open</span>(<span class="st">'image_test_1.jpg'</span>, <span class="st">'rb'</span>) <span class="im">as</span> image_file:</span>
<span id="cb30-5"><a href="#cb30-5" aria-hidden="true" tabindex="-1"></a>    img <span class="op">=</span> image_file.read()</span>
<span id="cb30-6"><a href="#cb30-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-7"><a href="#cb30-7" aria-hidden="true" tabindex="-1"></a>response <span class="op">=</span> ollama.generate(</span>
<span id="cb30-8"><a href="#cb30-8" aria-hidden="true" tabindex="-1"></a>    model<span class="op">=</span>MODEL,</span>
<span id="cb30-9"><a href="#cb30-9" aria-hidden="true" tabindex="-1"></a>    prompt<span class="op">=</span>PROMPT,</span>
<span id="cb30-10"><a href="#cb30-10" aria-hidden="true" tabindex="-1"></a>    images<span class="op">=</span> [img]</span>
<span id="cb30-11"><a href="#cb30-11" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb30-12"><a href="#cb30-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="sc">{</span>response[<span class="st">'response'</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb30-13"><a href="#cb30-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss"> [INFO] Total Duration: </span><span class="sc">{</span>(res[<span class="st">'total_duration'</span>]<span class="op">/</span><span class="fl">1e9</span>)<span class="sc">:.2f}</span><span class="ss"> seconds"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Here is the result:</p>
<div class="sourceCode" id="cb31"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a><span class="ex">This</span> image captures the iconic cityscape of Paris, France. The vantage point </span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a><span class="ex">is</span> high, providing a panoramic view of the Seine River that meanders through </span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a><span class="ex">the</span> heart of the city. Several bridges arch gracefully over the river, </span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a><span class="ex">connecting</span> different parts of the city. The Eiffel Tower, an iron lattice </span>
<span id="cb31-5"><a href="#cb31-5" aria-hidden="true" tabindex="-1"></a><span class="ex">structure</span> with a pointed top and two antennas on its summit, stands tall in the </span>
<span id="cb31-6"><a href="#cb31-6" aria-hidden="true" tabindex="-1"></a><span class="ex">background,</span> piercing the sky. It is painted in a light gray color, contrasting </span>
<span id="cb31-7"><a href="#cb31-7" aria-hidden="true" tabindex="-1"></a><span class="ex">against</span> the blue sky speckled with white clouds.</span>
<span id="cb31-8"><a href="#cb31-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-9"><a href="#cb31-9" aria-hidden="true" tabindex="-1"></a><span class="ex">The</span> buildings that line the river are predominantly white or beige, their uniform</span>
<span id="cb31-10"><a href="#cb31-10" aria-hidden="true" tabindex="-1"></a><span class="ex">color</span> palette broken occasionally by red roofs peeking through. The Seine River </span>
<span id="cb31-11"><a href="#cb31-11" aria-hidden="true" tabindex="-1"></a><span class="ex">itself</span> appears calm and wide, reflecting the city<span class="st">'s architectural beauty in its </span></span>
<span id="cb31-12"><a href="#cb31-12" aria-hidden="true" tabindex="-1"></a><span class="st">surface. On either side of the river, trees add a touch of green to the urban </span></span>
<span id="cb31-13"><a href="#cb31-13" aria-hidden="true" tabindex="-1"></a><span class="st">landscape.</span></span>
<span id="cb31-14"><a href="#cb31-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-15"><a href="#cb31-15" aria-hidden="true" tabindex="-1"></a><span class="st">The image is taken from an elevated perspective, looking down on the city. This </span></span>
<span id="cb31-16"><a href="#cb31-16" aria-hidden="true" tabindex="-1"></a><span class="st">viewpoint allows for a comprehensive view of Paris'</span>s beautiful architecture and </span>
<span id="cb31-17"><a href="#cb31-17" aria-hidden="true" tabindex="-1"></a><span class="ex">layout.</span> The relative positions of the buildings, bridges, and other structures </span>
<span id="cb31-18"><a href="#cb31-18" aria-hidden="true" tabindex="-1"></a><span class="ex">create</span> a harmonious composition that showcases the city<span class="st">'s charm.</span></span>
<span id="cb31-19"><a href="#cb31-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-20"><a href="#cb31-20" aria-hidden="true" tabindex="-1"></a><span class="st">In summary, this image presents a serene day in Paris, with its architectural </span></span>
<span id="cb31-21"><a href="#cb31-21" aria-hidden="true" tabindex="-1"></a><span class="st">marvels - from the Eiffel Tower to the river-side buildings - all bathed in soft </span></span>
<span id="cb31-22"><a href="#cb31-22" aria-hidden="true" tabindex="-1"></a><span class="st">colors under a clear sky.</span></span>
<span id="cb31-23"><a href="#cb31-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-24"><a href="#cb31-24" aria-hidden="true" tabindex="-1"></a><span class="st"> [INFO] Total Duration: 256.45 seconds</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>The model took about 4 minutes (256.45 s) to return with a detailed image description.</p>
<blockquote class="blockquote">
<p>In the <a href="https://github.com/Mjrovai/EdgeML-with-Raspberry-Pi/blob/main/OLLAMA_SLMs/10-Ollama_Python_Library.ipynb">10-Ollama_Python_Library</a> notebook, it is possible to find the experiments with the Ollama Python library.</p>
</blockquote>
<section id="function-calling" class="level3">
<h3 class="anchored" data-anchor-id="function-calling">Function Calling</h3>
<p>So far, we can observe that by using the model‚Äôs response into a variable, we can effectively incorporate it into real-world projects. However, a major issue arises when the model provides varying responses to the same input. For instance, let‚Äôs assume that we only need the name of a country‚Äôs capital and its coordinates as the model‚Äôs response in the previous examples, without any additional information, even when utilizing verbose models like Microsoft Phi. To ensure consistent responses, we can employ the ‚ÄòOllama function call,‚Äô which is fully compatible with the OpenAI API.</p>
<section id="but-what-exactly-is-function-calling" class="level4">
<h4 class="anchored" data-anchor-id="but-what-exactly-is-function-calling">But what exactly is ‚Äúfunction calling‚Äù?</h4>
<p>In modern artificial intelligence, function calling with Large Language Models (LLMs) allows these models to perform actions beyond generating text. By integrating with external functions or APIs, LLMs can access real-time data, automate tasks, and interact with various systems.</p>
<p>For instance, instead of merely responding to a query about the weather, an LLM can call a weather API to fetch the current conditions and provide accurate, up-to-date information. This capability enhances the relevance and accuracy of the model‚Äôs responses and makes it a powerful tool for driving workflows and automating processes, transforming it into an active participant in real-world applications.</p>
<p>For more details about Function Calling, please see this video made by <a href="https://www.youtube.com/@MervinPraison">Marvin Prison</a>:</p>
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/eHfMCtlsb1o" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</section>
<section id="lets-create-a-project." class="level4">
<h4 class="anchored" data-anchor-id="lets-create-a-project.">Let‚Äôs create a project.</h4>
<p>We want to create an <em>app</em> where the user enters a country‚Äôs name and gets, as an output, the distance in km from the capital city of such a country and the app‚Äôs location (for simplicity, We will use Santiago, Chile, as the app location).</p>
<p><a href="images/png/block-fc-proj.png" class="lightbox" data-gallery="quarto-lightbox-gallery-24"><img src="images/png/block-fc-proj.png" class="img-fluid"></a></p>
<p>Once the user enters a country name, the model will return the name of its capital city (as a string) and the latitude and longitude of such city (in float). Using those coordinates, we can use a simple Python library (<a href="https://pypi.org/project/haversine/">haversine</a>) to calculate the distance between those 2 points.</p>
<p>The idea of this project is to demonstrate a combination of language model interaction, structured data handling with Pydantic, and geospatial calculations using the Haversine formula (traditional computing).</p>
<p>First, let us install some libraries. Besides <em>Haversine</em>, the main one is the <a href="https://github.com/openai/openai-python">OpenAI Python library</a>, which provides convenient access to the OpenAI REST API from any Python 3.7+ application. The other one is <a href="https://docs.pydantic.dev/latest/">Pydantic</a> (and instructor), a robust data validation and settings management library engineered by Python to enhance the robustness and reliability of our codebase. In short, <em>Pydantic</em> will help ensure that our model‚Äôs response will always be consistent.</p>
<div class="sourceCode" id="cb32"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="ex">pip</span> install haversine</span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a><span class="ex">pip</span> install openai </span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a><span class="ex">pip</span> install pydantic </span>
<span id="cb32-4"><a href="#cb32-4" aria-hidden="true" tabindex="-1"></a><span class="ex">pip</span> install instructor</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Now, we should create a Python script designed to interact with our model (LLM) to determine the coordinates of a country‚Äôs capital city and calculate the distance from Santiago de Chile to that capital.</p>
<p>Let‚Äôs go over the code:</p>
</section>
</section>
<section id="importing-libraries" class="level3">
<h3 class="anchored" data-anchor-id="importing-libraries">1. Importing Libraries</h3>
<div class="sourceCode" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> sys</span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> haversine <span class="im">import</span> haversine</span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> openai <span class="im">import</span> OpenAI</span>
<span id="cb33-4"><a href="#cb33-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pydantic <span class="im">import</span> BaseModel, Field</span>
<span id="cb33-5"><a href="#cb33-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> instructor</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<ul>
<li><strong>sys</strong>: Provides access to system-specific parameters and functions. It‚Äôs used to get command-line arguments.</li>
<li><strong>haversine</strong>: A function from the haversine library that calculates the distance between two geographic points using the Haversine formula.</li>
<li><strong>openAI</strong>: A module for interacting with the OpenAI API (although it‚Äôs used in conjunction with a local setup, Ollama). Everything is off-line here.</li>
<li><strong>pydantic</strong>: Provides data validation and settings management using Python-type annotations. It‚Äôs used to define the structure of expected response data.</li>
<li><strong>instructor</strong>: A module is used to patch the OpenAI client to work in a specific mode (likely related to structured data handling).</li>
</ul>
</section>
<section id="defining-input-and-model" class="level3">
<h3 class="anchored" data-anchor-id="defining-input-and-model">2. Defining Input and Model</h3>
<div class="sourceCode" id="cb34"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a>country <span class="op">=</span> sys.argv[<span class="dv">1</span>]       <span class="co"># Get the country from command-line arguments</span></span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a>MODEL <span class="op">=</span> <span class="st">'phi3.5:3.8b'</span>     <span class="co"># The name of the model to be used</span></span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a>mylat <span class="op">=</span> <span class="op">-</span><span class="fl">33.33</span>              <span class="co"># Latitude of Santiago de Chile</span></span>
<span id="cb34-4"><a href="#cb34-4" aria-hidden="true" tabindex="-1"></a>mylon <span class="op">=</span> <span class="op">-</span><span class="fl">70.51</span>              <span class="co"># Longitude of Santiago de Chile</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<ul>
<li><strong>country</strong>: On a Python script, getting the country name from command-line arguments is possible. On a Jupyter notebook, we can enter its name, for example,
<ul>
<li><code>country = "France"</code></li>
</ul></li>
<li><strong>MODEL</strong>: Specifies the model being used, which is, in this example, the phi3.5.</li>
<li><strong>mylat</strong> <strong>and</strong> <strong>mylon</strong>: Coordinates of Santiago de Chile, used as the starting point for the distance calculation.</li>
</ul>
</section>
<section id="defining-the-response-data-structure" class="level3">
<h3 class="anchored" data-anchor-id="defining-the-response-data-structure">3. Defining the Response Data Structure</h3>
<div class="sourceCode" id="cb35"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> CityCoord(BaseModel):</span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a>    city: <span class="bu">str</span> <span class="op">=</span> Field(..., description<span class="op">=</span><span class="st">"Name of the city"</span>)</span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a>    lat: <span class="bu">float</span> <span class="op">=</span> Field(..., description<span class="op">=</span><span class="st">"Decimal Latitude of the city"</span>)</span>
<span id="cb35-4"><a href="#cb35-4" aria-hidden="true" tabindex="-1"></a>    lon: <span class="bu">float</span> <span class="op">=</span> Field(..., description<span class="op">=</span><span class="st">"Decimal Longitude of the city"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<ul>
<li><strong>CityCoord</strong>: A Pydantic model that defines the expected structure of the response from the LLM. It expects three fields: city (name of the city), lat (latitude), and lon (longitude).</li>
</ul>
</section>
<section id="setting-up-the-openai-client" class="level3">
<h3 class="anchored" data-anchor-id="setting-up-the-openai-client">4. Setting Up the OpenAI Client</h3>
<div class="sourceCode" id="cb36"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a>client <span class="op">=</span> instructor.patch(</span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a>    OpenAI(</span>
<span id="cb36-3"><a href="#cb36-3" aria-hidden="true" tabindex="-1"></a>        base_url<span class="op">=</span><span class="st">"http://localhost:11434/v1"</span>,  <span class="co"># Local API base URL (Ollama)</span></span>
<span id="cb36-4"><a href="#cb36-4" aria-hidden="true" tabindex="-1"></a>        api_key<span class="op">=</span><span class="st">"ollama"</span>,                      <span class="co"># API key (not used)</span></span>
<span id="cb36-5"><a href="#cb36-5" aria-hidden="true" tabindex="-1"></a>    ),</span>
<span id="cb36-6"><a href="#cb36-6" aria-hidden="true" tabindex="-1"></a>    mode<span class="op">=</span>instructor.Mode.JSON,                 <span class="co"># Mode for structured JSON output</span></span>
<span id="cb36-7"><a href="#cb36-7" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<ul>
<li><strong>OpenAI</strong>: This setup initializes an OpenAI client with a local base URL and an API key (ollama). It uses a local server.</li>
<li><strong>instructor.patch</strong>: Patches the OpenAI client to work in JSON mode, enabling structured output that matches the Pydantic model.</li>
</ul>
</section>
<section id="generating-the-response" class="level3">
<h3 class="anchored" data-anchor-id="generating-the-response">5. Generating the Response</h3>
<div class="sourceCode" id="cb37"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a>resp <span class="op">=</span> client.chat.completions.create(</span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a>    model<span class="op">=</span>MODEL,</span>
<span id="cb37-3"><a href="#cb37-3" aria-hidden="true" tabindex="-1"></a>    messages<span class="op">=</span>[</span>
<span id="cb37-4"><a href="#cb37-4" aria-hidden="true" tabindex="-1"></a>        {</span>
<span id="cb37-5"><a href="#cb37-5" aria-hidden="true" tabindex="-1"></a>            <span class="st">"role"</span>: <span class="st">"user"</span>,</span>
<span id="cb37-6"><a href="#cb37-6" aria-hidden="true" tabindex="-1"></a>            <span class="st">"content"</span>: <span class="ss">f"return the decimal latitude and decimal longitude </span><span class="ch">\</span></span>
<span id="cb37-7"><a href="#cb37-7" aria-hidden="true" tabindex="-1"></a><span class="ss">            of the capital of the </span><span class="sc">{</span>country<span class="sc">}</span><span class="ss">."</span></span>
<span id="cb37-8"><a href="#cb37-8" aria-hidden="true" tabindex="-1"></a>        }</span>
<span id="cb37-9"><a href="#cb37-9" aria-hidden="true" tabindex="-1"></a>    ],</span>
<span id="cb37-10"><a href="#cb37-10" aria-hidden="true" tabindex="-1"></a>    response_model<span class="op">=</span>CityCoord,</span>
<span id="cb37-11"><a href="#cb37-11" aria-hidden="true" tabindex="-1"></a>    max_retries<span class="op">=</span><span class="dv">10</span></span>
<span id="cb37-12"><a href="#cb37-12" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<ul>
<li><strong>client.chat.completions.create</strong>: Calls the LLM to generate a response.</li>
<li><strong>model</strong>: Specifies the model to use (llava-phi3).</li>
<li><strong>messages</strong>: Contains the prompt for the LLM, asking for the latitude and longitude of the capital city of the specified country.</li>
<li><strong>response_model</strong>: Indicates that the response should conform to the CityCoord model.</li>
<li><strong>max_retries</strong>: The maximum number of retry attempts if the request fails.</li>
</ul>
</section>
<section id="calculating-the-distance" class="level3">
<h3 class="anchored" data-anchor-id="calculating-the-distance">6. Calculating the Distance</h3>
<div class="sourceCode" id="cb38"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a>distance <span class="op">=</span> haversine((mylat, mylon), (resp.lat, resp.lon), unit<span class="op">=</span><span class="st">'km'</span>)</span>
<span id="cb38-2"><a href="#cb38-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Santiago de Chile is about </span><span class="sc">{</span><span class="bu">int</span>(<span class="bu">round</span>(distance, <span class="op">-</span><span class="dv">1</span>))<span class="sc">:,}</span><span class="ss"> </span><span class="ch">\</span></span>
<span id="cb38-3"><a href="#cb38-3" aria-hidden="true" tabindex="-1"></a><span class="ss">        kilometers away from </span><span class="sc">{</span>resp<span class="sc">.</span>city<span class="sc">}</span><span class="ss">."</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<ul>
<li><strong>haversine</strong>: Calculates the distance between Santiago de Chile and the capital city returned by the LLM using their respective coordinates.</li>
<li><strong>(mylat, mylon)</strong>: Coordinates of Santiago de Chile.</li>
<li><strong>resp.city</strong>: Name of the country‚Äôs capital</li>
<li><strong>(resp.lat, resp.lon)</strong>: Coordinates of the capital city are provided by the LLM response.</li>
<li><strong>unit=‚Äòkm‚Äô</strong>: Specifies that the distance should be calculated in kilometers.</li>
<li><strong>print</strong>: Outputs the distance, rounded to the nearest 10 kilometers, with thousands of separators for readability.</li>
</ul>
<p><strong>Running the code</strong></p>
<p>If we enter different countries, for example, France, Colombia, and the United States, We can note that we always receive the same structured information:</p>
<div class="sourceCode" id="cb39"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a><span class="ex">Santiago</span> de Chile is about 8,060 kilometers away from Washington, D.C..</span>
<span id="cb39-2"><a href="#cb39-2" aria-hidden="true" tabindex="-1"></a><span class="ex">Santiago</span> de Chile is about 4,250 kilometers away from Bogot√°.</span>
<span id="cb39-3"><a href="#cb39-3" aria-hidden="true" tabindex="-1"></a><span class="ex">Santiago</span> de Chile is about 11,630 kilometers away from Paris.</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>If you run the code as a script, the result will be printed on the terminal:</p>
<p><a href="images/png/script-fc.png" class="lightbox" data-gallery="quarto-lightbox-gallery-25"><img src="images/png/script-fc.png" class="img-fluid"></a></p>
<p>And the calculations are pretty good!</p>
<p><a href="images/png/calc_real.png" class="lightbox" data-gallery="quarto-lightbox-gallery-26"><img src="images/png/calc_real.png" class="img-fluid"></a></p>
<blockquote class="blockquote">
<p>In the <a href="https://github.com/Mjrovai/EdgeML-with-Raspberry-Pi/blob/main/OLLAMA_SLMs/20-Ollama_Function_Calling.ipynb">20-Ollama_Function_Calling</a> notebook, it is possible to find experiments with all models installed.</p>
</blockquote>
</section>
<section id="adding-images" class="level3">
<h3 class="anchored" data-anchor-id="adding-images">Adding images</h3>
<p>Now it is time to wrap up everything so far! Let‚Äôs modify the script so that instead of entering the country name (as a text), the user enters an image, and the application (based on SLM) returns the city in the image and its geographic location. With those data, we can calculate the distance as before.</p>
<p><a href="images/png/block-2.png" class="lightbox" data-gallery="quarto-lightbox-gallery-27"><img src="images/png/block-2.png" class="img-fluid"></a></p>
<p>For simplicity, we will implement this new code in two steps. First, the LLM will analyze the image and create a description (text). This text will be passed on to another instance, where the model will extract the information needed to pass along.</p>
<p>We will start importing the libraries</p>
<div class="sourceCode" id="cb40"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> sys</span>
<span id="cb40-2"><a href="#cb40-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> time</span>
<span id="cb40-3"><a href="#cb40-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> haversine <span class="im">import</span> haversine</span>
<span id="cb40-4"><a href="#cb40-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> ollama</span>
<span id="cb40-5"><a href="#cb40-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> openai <span class="im">import</span> OpenAI</span>
<span id="cb40-6"><a href="#cb40-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pydantic <span class="im">import</span> BaseModel, Field</span>
<span id="cb40-7"><a href="#cb40-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> instructor</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>We can see the image if you run the code on the Jupyter Notebook. For that we need also import:</p>
<div class="sourceCode" id="cb41"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb41-2"><a href="#cb41-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> PIL <span class="im">import</span> Image</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<blockquote class="blockquote">
<p>Those libraries are unnecessary if we run the code as a script.</p>
</blockquote>
<p>Now, we define the model and the local coordinates:</p>
<div class="sourceCode" id="cb42"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a>MODEL <span class="op">=</span> <span class="st">'llava-phi3:3.8b'</span></span>
<span id="cb42-2"><a href="#cb42-2" aria-hidden="true" tabindex="-1"></a>mylat <span class="op">=</span> <span class="op">-</span><span class="fl">33.33</span></span>
<span id="cb42-3"><a href="#cb42-3" aria-hidden="true" tabindex="-1"></a>mylon <span class="op">=</span> <span class="op">-</span><span class="fl">70.51</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>We can download a new image, for example, Machu Picchu from Wikipedia. On the Notebook we can see it:</p>
<div class="sourceCode" id="cb43"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Load the image</span></span>
<span id="cb43-2"><a href="#cb43-2" aria-hidden="true" tabindex="-1"></a>img_path <span class="op">=</span> <span class="st">"image_test_3.jpg"</span></span>
<span id="cb43-3"><a href="#cb43-3" aria-hidden="true" tabindex="-1"></a>img <span class="op">=</span> Image.<span class="bu">open</span>(img_path)</span>
<span id="cb43-4"><a href="#cb43-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-5"><a href="#cb43-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Display the image</span></span>
<span id="cb43-6"><a href="#cb43-6" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">8</span>))</span>
<span id="cb43-7"><a href="#cb43-7" aria-hidden="true" tabindex="-1"></a>plt.imshow(img)</span>
<span id="cb43-8"><a href="#cb43-8" aria-hidden="true" tabindex="-1"></a>plt.axis(<span class="st">'off'</span>)</span>
<span id="cb43-9"><a href="#cb43-9" aria-hidden="true" tabindex="-1"></a><span class="co">#plt.title("Image")</span></span>
<span id="cb43-10"><a href="#cb43-10" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><a href="images/jpeg/image_test_3.jpg" class="lightbox" data-gallery="quarto-lightbox-gallery-28"><img src="images/jpeg/image_test_3.jpg" class="img-fluid"></a></p>
<p>Now, let‚Äôs define a function that will receive the image and will <code>return the decimal latitude and decimal longitude of the city in the image, its name, and what country it is located</code></p>
<div class="sourceCode" id="cb44"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> image_description(img_path):</span>
<span id="cb44-2"><a href="#cb44-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> <span class="bu">open</span>(img_path, <span class="st">'rb'</span>) <span class="im">as</span> <span class="bu">file</span>:</span>
<span id="cb44-3"><a href="#cb44-3" aria-hidden="true" tabindex="-1"></a>        response <span class="op">=</span> ollama.chat(</span>
<span id="cb44-4"><a href="#cb44-4" aria-hidden="true" tabindex="-1"></a>            model<span class="op">=</span>MODEL,</span>
<span id="cb44-5"><a href="#cb44-5" aria-hidden="true" tabindex="-1"></a>            messages<span class="op">=</span>[</span>
<span id="cb44-6"><a href="#cb44-6" aria-hidden="true" tabindex="-1"></a>              {</span>
<span id="cb44-7"><a href="#cb44-7" aria-hidden="true" tabindex="-1"></a>                <span class="st">'role'</span>: <span class="st">'user'</span>,</span>
<span id="cb44-8"><a href="#cb44-8" aria-hidden="true" tabindex="-1"></a>                <span class="st">'content'</span>: <span class="st">'''return the decimal latitude and decimal longitude </span></span>
<span id="cb44-9"><a href="#cb44-9" aria-hidden="true" tabindex="-1"></a><span class="st">                              of the city in the image, its name, and </span></span>
<span id="cb44-10"><a href="#cb44-10" aria-hidden="true" tabindex="-1"></a><span class="st">                              what country it is located'''</span>,</span>
<span id="cb44-11"><a href="#cb44-11" aria-hidden="true" tabindex="-1"></a>                <span class="st">'images'</span>: [<span class="bu">file</span>.read()],</span>
<span id="cb44-12"><a href="#cb44-12" aria-hidden="true" tabindex="-1"></a>              },</span>
<span id="cb44-13"><a href="#cb44-13" aria-hidden="true" tabindex="-1"></a>            ],</span>
<span id="cb44-14"><a href="#cb44-14" aria-hidden="true" tabindex="-1"></a>            options <span class="op">=</span> {</span>
<span id="cb44-15"><a href="#cb44-15" aria-hidden="true" tabindex="-1"></a>              <span class="st">'temperature'</span>: <span class="dv">0</span>,</span>
<span id="cb44-16"><a href="#cb44-16" aria-hidden="true" tabindex="-1"></a>              }</span>
<span id="cb44-17"><a href="#cb44-17" aria-hidden="true" tabindex="-1"></a>      )</span>
<span id="cb44-18"><a href="#cb44-18" aria-hidden="true" tabindex="-1"></a>    <span class="co">#print(response['message']['content'])</span></span>
<span id="cb44-19"><a href="#cb44-19" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> response[<span class="st">'message'</span>][<span class="st">'content'</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<blockquote class="blockquote">
<p>We can print the entire response for debug purposes.</p>
</blockquote>
<p>The image description generated for the function will be passed as a prompt for the model again.</p>
<div class="sourceCode" id="cb45"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a>start_time <span class="op">=</span> time.perf_counter()  <span class="co"># Start timing</span></span>
<span id="cb45-2"><a href="#cb45-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-3"><a href="#cb45-3" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> CityCoord(BaseModel):</span>
<span id="cb45-4"><a href="#cb45-4" aria-hidden="true" tabindex="-1"></a>    city: <span class="bu">str</span> <span class="op">=</span> Field(..., description<span class="op">=</span><span class="st">"Name of the city in the image"</span>)</span>
<span id="cb45-5"><a href="#cb45-5" aria-hidden="true" tabindex="-1"></a>    country: <span class="bu">str</span> <span class="op">=</span> Field(..., description<span class="op">=</span><span class="st">"""Name of the country where"</span></span>
<span id="cb45-6"><a href="#cb45-6" aria-hidden="true" tabindex="-1"></a><span class="st">                                             the city in the image is located</span></span>
<span id="cb45-7"><a href="#cb45-7" aria-hidden="true" tabindex="-1"></a><span class="st">                                             """</span>)</span>
<span id="cb45-8"><a href="#cb45-8" aria-hidden="true" tabindex="-1"></a>    lat: <span class="bu">float</span> <span class="op">=</span> Field(..., description<span class="op">=</span><span class="st">"""Decimal Latitude of the city in"</span></span>
<span id="cb45-9"><a href="#cb45-9" aria-hidden="true" tabindex="-1"></a><span class="st">                                            the image"""</span>)</span>
<span id="cb45-10"><a href="#cb45-10" aria-hidden="true" tabindex="-1"></a>    lon: <span class="bu">float</span> <span class="op">=</span> Field(..., description<span class="op">=</span><span class="st">"""Decimal Longitude of the city in"</span></span>
<span id="cb45-11"><a href="#cb45-11" aria-hidden="true" tabindex="-1"></a><span class="st">                                           the image"""</span>)</span>
<span id="cb45-12"><a href="#cb45-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-13"><a href="#cb45-13" aria-hidden="true" tabindex="-1"></a><span class="co"># enables `response_model` in create call</span></span>
<span id="cb45-14"><a href="#cb45-14" aria-hidden="true" tabindex="-1"></a>client <span class="op">=</span> instructor.patch(</span>
<span id="cb45-15"><a href="#cb45-15" aria-hidden="true" tabindex="-1"></a>    OpenAI(</span>
<span id="cb45-16"><a href="#cb45-16" aria-hidden="true" tabindex="-1"></a>        base_url<span class="op">=</span><span class="st">"http://localhost:11434/v1"</span>,</span>
<span id="cb45-17"><a href="#cb45-17" aria-hidden="true" tabindex="-1"></a>        api_key<span class="op">=</span><span class="st">"ollama"</span></span>
<span id="cb45-18"><a href="#cb45-18" aria-hidden="true" tabindex="-1"></a>    ),</span>
<span id="cb45-19"><a href="#cb45-19" aria-hidden="true" tabindex="-1"></a>    mode<span class="op">=</span>instructor.Mode.JSON,</span>
<span id="cb45-20"><a href="#cb45-20" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb45-21"><a href="#cb45-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-22"><a href="#cb45-22" aria-hidden="true" tabindex="-1"></a>image_description <span class="op">=</span> image_description(img_path)</span>
<span id="cb45-23"><a href="#cb45-23" aria-hidden="true" tabindex="-1"></a><span class="co"># Send this description to the model</span></span>
<span id="cb45-24"><a href="#cb45-24" aria-hidden="true" tabindex="-1"></a>resp <span class="op">=</span> client.chat.completions.create(</span>
<span id="cb45-25"><a href="#cb45-25" aria-hidden="true" tabindex="-1"></a>    model<span class="op">=</span>MODEL,</span>
<span id="cb45-26"><a href="#cb45-26" aria-hidden="true" tabindex="-1"></a>    messages<span class="op">=</span>[</span>
<span id="cb45-27"><a href="#cb45-27" aria-hidden="true" tabindex="-1"></a>        {</span>
<span id="cb45-28"><a href="#cb45-28" aria-hidden="true" tabindex="-1"></a>            <span class="st">"role"</span>: <span class="st">"user"</span>,</span>
<span id="cb45-29"><a href="#cb45-29" aria-hidden="true" tabindex="-1"></a>            <span class="st">"content"</span>: image_description,</span>
<span id="cb45-30"><a href="#cb45-30" aria-hidden="true" tabindex="-1"></a>        }</span>
<span id="cb45-31"><a href="#cb45-31" aria-hidden="true" tabindex="-1"></a>    ],</span>
<span id="cb45-32"><a href="#cb45-32" aria-hidden="true" tabindex="-1"></a>    response_model<span class="op">=</span>CityCoord,</span>
<span id="cb45-33"><a href="#cb45-33" aria-hidden="true" tabindex="-1"></a>    max_retries<span class="op">=</span><span class="dv">10</span>,</span>
<span id="cb45-34"><a href="#cb45-34" aria-hidden="true" tabindex="-1"></a>    temperature<span class="op">=</span><span class="dv">0</span>,</span>
<span id="cb45-35"><a href="#cb45-35" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>If we print the image description , we will get:</p>
<div class="sourceCode" id="cb46"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a><span class="ex">The</span> image shows the ancient city of Machu Picchu, located in Peru. The city is</span>
<span id="cb46-2"><a href="#cb46-2" aria-hidden="true" tabindex="-1"></a><span class="ex">perched</span> on a steep hillside and consists of various structures made of stone. It </span>
<span id="cb46-3"><a href="#cb46-3" aria-hidden="true" tabindex="-1"></a><span class="ex">is</span> surrounded by lush greenery and towering mountains. The sky above is blue with</span>
<span id="cb46-4"><a href="#cb46-4" aria-hidden="true" tabindex="-1"></a><span class="ex">scattered</span> clouds. </span>
<span id="cb46-5"><a href="#cb46-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-6"><a href="#cb46-6" aria-hidden="true" tabindex="-1"></a><span class="ex">Machu</span> Picchu<span class="st">'s latitude is approximately 13.5086¬∞ S, and its longitude is around</span></span>
<span id="cb46-7"><a href="#cb46-7" aria-hidden="true" tabindex="-1"></a><span class="st">72.5494¬∞ W.</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>And the second response from the model (<code>resp</code>) will be:</p>
<div class="sourceCode" id="cb47"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb47-1"><a href="#cb47-1" aria-hidden="true" tabindex="-1"></a><span class="ex">CityCoord</span><span class="er">(</span><span class="va">city</span><span class="op">=</span><span class="st">'Machu Picchu'</span>, <span class="va">country</span><span class="op">=</span><span class="st">'Peru'</span>, <span class="va">lat</span><span class="op">=</span>-13.5086, <span class="va">lon</span><span class="op">=</span>-72.5494<span class="kw">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Now, we can do a ‚ÄúPost-Processing‚Äù, calculating the distance and preparing the final answer:</p>
<div class="sourceCode" id="cb48"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a>distance <span class="op">=</span> haversine((mylat, mylon), (resp.lat, resp.lon), unit<span class="op">=</span><span class="st">'km'</span>)</span>
<span id="cb48-2"><a href="#cb48-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-3"><a href="#cb48-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss"> The image shows </span><span class="sc">{</span>resp<span class="sc">.</span>city<span class="sc">}</span><span class="ss">, with lat:</span><span class="sc">{</span><span class="bu">round</span>(resp.lat, <span class="dv">2</span>)<span class="sc">}</span><span class="ss"> and </span><span class="ch">\</span></span>
<span id="cb48-4"><a href="#cb48-4" aria-hidden="true" tabindex="-1"></a><span class="ss">      long: </span><span class="sc">{</span><span class="bu">round</span>(resp.lon, <span class="dv">2</span>)<span class="sc">}</span><span class="ss">, located in </span><span class="sc">{</span>resp<span class="sc">.</span>country<span class="sc">}</span><span class="ss"> and about </span><span class="ch">\</span></span>
<span id="cb48-5"><a href="#cb48-5" aria-hidden="true" tabindex="-1"></a><span class="ss">            </span><span class="sc">{</span><span class="bu">int</span>(<span class="bu">round</span>(distance, <span class="op">-</span><span class="dv">1</span>))<span class="sc">:,}</span><span class="ss"> kilometers away from </span><span class="ch">\</span></span>
<span id="cb48-6"><a href="#cb48-6" aria-hidden="true" tabindex="-1"></a><span class="ss">            Santiago, Chile.</span><span class="ch">\n</span><span class="ss">"</span>)</span>
<span id="cb48-7"><a href="#cb48-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-8"><a href="#cb48-8" aria-hidden="true" tabindex="-1"></a>end_time <span class="op">=</span> time.perf_counter()  <span class="co"># End timing</span></span>
<span id="cb48-9"><a href="#cb48-9" aria-hidden="true" tabindex="-1"></a>elapsed_time <span class="op">=</span> end_time <span class="op">-</span> start_time  <span class="co"># Calculate elapsed time</span></span>
<span id="cb48-10"><a href="#cb48-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f" [INFO] ==&gt; The code (running </span><span class="sc">{</span>MODEL<span class="sc">}</span><span class="ss">), took </span><span class="sc">{</span>elapsed_time<span class="sc">:.1f}</span><span class="ss"> </span><span class="ch">\</span></span>
<span id="cb48-11"><a href="#cb48-11" aria-hidden="true" tabindex="-1"></a><span class="ss">      seconds to execute.</span><span class="ch">\n</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>And we will get:</p>
<div class="sourceCode" id="cb49"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb49-1"><a href="#cb49-1" aria-hidden="true" tabindex="-1"></a> <span class="ex">The</span> image shows Machu Picchu, with lat:-13.16 and long: <span class="at">-72.54,</span> located in Peru</span>
<span id="cb49-2"><a href="#cb49-2" aria-hidden="true" tabindex="-1"></a> <span class="ex">and</span> about 2,250 kilometers away from Santiago, Chile.</span>
<span id="cb49-3"><a href="#cb49-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-4"><a href="#cb49-4" aria-hidden="true" tabindex="-1"></a> <span class="ex">[INFO]</span> ==<span class="op">&gt;</span> The code <span class="er">(</span><span class="ex">running</span> llava-phi3:3.8b<span class="kw">)</span><span class="ex">,</span> took 491.3 seconds to execute.</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>In the <a href="https://github.com/Mjrovai/EdgeML-with-Raspberry-Pi/blob/main/OLLAMA_SLMs/30-Function_Calling_with_images.ipynb">30-Function_Calling_with_images</a> notebook, it is possible to find the experiments with multiple images.</p>
<p>Let‚Äôs now download the script <code>calc_distance_image.py</code> from the <a href="">GitHub</a> and run it on the terminal with the command:</p>
<div class="sourceCode" id="cb50"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb50-1"><a href="#cb50-1" aria-hidden="true" tabindex="-1"></a><span class="ex">python</span> calc_distance_image.py /home/mjrovai/Documents/OLLAMA/image_test_3.jpg</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Enter with the Machu Picchu image full patch as an argument. We will get the same previous result.</p>
<p><a href="images/png/app-machu-picchu.png" class="lightbox" data-gallery="quarto-lightbox-gallery-29"><img src="images/png/app-machu-picchu.png" class="img-fluid"></a></p>
<p><em>How</em> about Paris?</p>
<p><a href="images/png/paris-app.png" class="lightbox" data-gallery="quarto-lightbox-gallery-30"><img src="images/png/paris-app.png" class="img-fluid"></a></p>
<p>Of course, there are many ways to optimize the code used here. Still, the idea is to explore the considerable potential of <em>function calling</em> with SLMs at the edge, allowing those models to integrate with external functions or APIs. Going beyond text generation, SLMs can access real-time data, automate tasks, and interact with various systems.</p>
</section>
</section>
<section id="slms-optimization-techniques" class="level2">
<h2 class="anchored" data-anchor-id="slms-optimization-techniques">SLMs: Optimization Techniques</h2>
<p>Large Language Models (LLMs) have revolutionized natural language processing, but their deployment and optimization come with unique challenges. One significant issue is the tendency for LLMs (and more, the SLMs) to generate plausible-sounding but factually incorrect information, a phenomenon known as <strong>hallucination</strong>. This occurs when models produce content that seems coherent but is not grounded in truth or real-world facts.</p>
<p>Other challenges include the immense computational resources required for training and running these models, the difficulty in maintaining up-to-date knowledge within the model, and the need for domain-specific adaptations. Privacy concerns also arise when handling sensitive data during training or inference. Additionally, ensuring consistent performance across diverse tasks and maintaining ethical use of these powerful tools present ongoing challenges. Addressing these issues is crucial for the effective and responsible deployment of LLMs in real-world applications.</p>
<p>The fundamental techniques for enhancing LLM (and SLM) performance and efficiency are Fine-tuning, Prompt engineering, and Retrieval-Augmented Generation (RAG).</p>
<ul>
<li><p><strong>Fine-tuning</strong>, while more resource-intensive, offers a way to specialize LLMs for particular domains or tasks. This process involves further training the model on carefully curated datasets, allowing it to adapt its vast general knowledge to specific applications. Fine-tuning can lead to substantial improvements in performance, especially in specialized fields or for unique use cases.</p></li>
<li><p><strong>Prompt engineering</strong> is at the forefront of LLM optimization. By carefully crafting input prompts, we can guide models to produce more accurate and relevant outputs. This technique involves structuring queries that leverage the model‚Äôs pre-trained knowledge and capabilities, often incorporating examples or specific instructions to shape the desired response.</p></li>
<li><p><strong>Retrieval-Augmented Generation (RAG)</strong> represents another powerful approach to improving LLM performance. This method combines the vast knowledge embedded in pre-trained models with the ability to access and incorporate external, up-to-date information. By retrieving relevant data to supplement the model‚Äôs decision-making process, RAG can significantly enhance accuracy and reduce the likelihood of generating outdated or false information.</p></li>
</ul>
<p>For edge applications, it is more beneficial to focus on techniques like RAG that can enhance model performance without needing on-device fine-tuning. Let‚Äôs explore it.</p>
</section>
<section id="rag-implementation" class="level2">
<h2 class="anchored" data-anchor-id="rag-implementation">RAG Implementation</h2>
<p>In a basic interaction between a user and a language model, the user asks a question, which is sent as a prompt to the model. The model generates a response based solely on its pre-trained knowledge. In a RAG process, there‚Äôs an additional step between the user‚Äôs question and the model‚Äôs response. The user‚Äôs question triggers a retrieval process from a knowledge base.</p>
<p><a href="images/png/rag-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-31"><img src="images/png/rag-1.png" class="img-fluid"></a></p>
<section id="a-simple-rag-project" class="level3">
<h3 class="anchored" data-anchor-id="a-simple-rag-project">A simple RAG project</h3>
<p>Here are the steps to implement a basic Retrieval Augmented Generation (RAG):</p>
<ul>
<li><p><strong>Determine the type of documents you‚Äôll be using:</strong> The best types are documents from which we can get clean and unobscured text. PDFs can be problematic because they are designed for printing, not for extracting sensible text. To work with PDFs, we should get the source document or use tools to handle it.</p></li>
<li><p><strong>Chunk the text:</strong> We can‚Äôt store the text as one long stream because of context size limitations and the potential for confusion. Chunking involves splitting the text into smaller pieces. Chunk text has many ways, such as character count, tokens, words, paragraphs, or sections. It is also possible to overlap chunks.</p></li>
<li><p><strong>Create embeddings:</strong> Embeddings are numerical representations of text that capture semantic meaning. We create embeddings by passing each chunk of text through a particular embedding model. The model outputs a vector, the length of which depends on the embedding model used. We should pull one (or more) <a href="https://ollama.com/blog/embedding-models">embedding models</a> from Ollama, to perform this task. Here are some examples of embedding models available at Ollama.</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Model</th>
<th>Parameter Size</th>
<th>Embedding Size</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>mxbai-embed-large</td>
<td>334M</td>
<td>1024</td>
</tr>
<tr class="even">
<td>nomic-embed-text</td>
<td>137M</td>
<td>768</td>
</tr>
<tr class="odd">
<td>all-minilm</td>
<td>23M</td>
<td>384</td>
</tr>
</tbody>
</table>
<blockquote class="blockquote">
<p>Generally, larger embedding sizes capture more nuanced information about the input. Still, they also require more computational resources to process, and a higher number of parameters should increase the latency (but also the quality of the response).</p>
</blockquote></li>
<li><p><strong>Store the chunks and embeddings in a vector database:</strong> We will need a way to efficiently find the most relevant chunks of text for a given prompt, which is where a vector database comes in. We will use <a href="https://www.trychroma.com/">Chromadb</a>, an AI-native open-source vector database, which simplifies building RAGs by creating knowledge, facts, and skills pluggable for LLMs. Both the embedding and the source text for each chunk are stored.</p></li>
<li><p><strong>Build the prompt:</strong> When we have a question, we create an embedding and query the vector database for the most similar chunks. Then, we select the top few results and include their text in the prompt.</p></li>
</ul>
<p>The goal of RAG is to provide the model with the most relevant information from our documents, allowing it to generate more accurate and informative responses. So, let‚Äôs implement a simple example of an SLM incorporating a particular set of facts about bees (‚ÄúBee Facts‚Äù).</p>
<p>Inside the <code>ollama</code> env, enter the command in the terminal for Chromadb instalation:</p>
<div class="sourceCode" id="cb51"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb51-1"><a href="#cb51-1" aria-hidden="true" tabindex="-1"></a><span class="ex">pip</span> install ollama chromadb</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Let‚Äôs pull an intermediary embedding model, <code>nomic-embed-text</code></p>
<div class="sourceCode" id="cb52"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb52-1"><a href="#cb52-1" aria-hidden="true" tabindex="-1"></a><span class="ex">ollama</span> pull nomic-embed-text</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>And create a working directory:</p>
<div class="sourceCode" id="cb53"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb53-1"><a href="#cb53-1" aria-hidden="true" tabindex="-1"></a>cd Documents<span class="op">/</span>OLLAMA<span class="op">/</span></span>
<span id="cb53-2"><a href="#cb53-2" aria-hidden="true" tabindex="-1"></a>mkdir RAG<span class="op">-</span>simple<span class="op">-</span>bee</span>
<span id="cb53-3"><a href="#cb53-3" aria-hidden="true" tabindex="-1"></a>cd RAG<span class="op">-</span>simple<span class="op">-</span>bee<span class="op">/</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Let‚Äôs create a new Jupyter notebook, <a href="https://github.com/Mjrovai/EdgeML-with-Raspberry-Pi/blob/main/OLLAMA_SLMs/40-RAG-simple-bee.ipynb">40-RAG-simple-bee</a> for some exploration:</p>
<p>Import the needed libraries:</p>
<div class="sourceCode" id="cb54"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb54-1"><a href="#cb54-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> ollama</span>
<span id="cb54-2"><a href="#cb54-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> chromadb</span>
<span id="cb54-3"><a href="#cb54-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> time</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>And define aor models:</p>
<div class="sourceCode" id="cb55"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb55-1"><a href="#cb55-1" aria-hidden="true" tabindex="-1"></a>EMB_MODEL <span class="op">=</span> <span class="st">"nomic-embed-text"</span></span>
<span id="cb55-2"><a href="#cb55-2" aria-hidden="true" tabindex="-1"></a>MODEL <span class="op">=</span> <span class="st">'llama3.2:3B'</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Initially, a knowledge base about bee facts should be created. This involves collecting relevant documents and converting them into vector embeddings. These embeddings are then stored in a vector database, allowing for efficient similarity searches later. Enter with the ‚Äúdocument,‚Äù a base of ‚Äúbee facts‚Äù as a list:</p>
<div class="sourceCode" id="cb56"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb56-1"><a href="#cb56-1" aria-hidden="true" tabindex="-1"></a>documents <span class="op">=</span> [</span>
<span id="cb56-2"><a href="#cb56-2" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Bee-keeping, also known as apiculture, involves the maintenance of bee </span><span class="ch">\</span></span>
<span id="cb56-3"><a href="#cb56-3" aria-hidden="true" tabindex="-1"></a><span class="st">    colonies, typically in hives, by humans."</span>,</span>
<span id="cb56-4"><a href="#cb56-4" aria-hidden="true" tabindex="-1"></a>    <span class="st">"The most commonly kept species of bees is the European honey bee (Apis </span><span class="ch">\</span></span>
<span id="cb56-5"><a href="#cb56-5" aria-hidden="true" tabindex="-1"></a><span class="st">    mellifera)."</span>,</span>
<span id="cb56-6"><a href="#cb56-6" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb56-7"><a href="#cb56-7" aria-hidden="true" tabindex="-1"></a>    ...</span>
<span id="cb56-8"><a href="#cb56-8" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb56-9"><a href="#cb56-9" aria-hidden="true" tabindex="-1"></a>    <span class="st">"There are another 20,000 different bee species in the world."</span>,  </span>
<span id="cb56-10"><a href="#cb56-10" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Brazil alone has more than 300 different bee species, and the </span><span class="ch">\</span></span>
<span id="cb56-11"><a href="#cb56-11" aria-hidden="true" tabindex="-1"></a><span class="st">    vast majority, unlike western honey bees, don‚Äôt sting."</span>, </span>
<span id="cb56-12"><a href="#cb56-12" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Reports written in 1577 by Hans Staden, mention three native bees </span><span class="ch">\</span></span>
<span id="cb56-13"><a href="#cb56-13" aria-hidden="true" tabindex="-1"></a><span class="st">    used by indigenous people in Brazil."</span>,</span>
<span id="cb56-14"><a href="#cb56-14" aria-hidden="true" tabindex="-1"></a>    <span class="st">"The indigenous people in Brazil used bees for medicine and food purposes"</span>,</span>
<span id="cb56-15"><a href="#cb56-15" aria-hidden="true" tabindex="-1"></a>    <span class="st">"From Hans Staden report: probable species: manda√ßaia (Melipona </span><span class="ch">\</span></span>
<span id="cb56-16"><a href="#cb56-16" aria-hidden="true" tabindex="-1"></a><span class="st">    quadrifasciata), mandaguari (Scaptotrigona postica) and jata√≠-amarela </span><span class="ch">\</span></span>
<span id="cb56-17"><a href="#cb56-17" aria-hidden="true" tabindex="-1"></a><span class="st">    (Tetragonisca angustula)."</span></span>
<span id="cb56-18"><a href="#cb56-18" aria-hidden="true" tabindex="-1"></a>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<blockquote class="blockquote">
<p>We do not need to ‚Äúchunk‚Äù the document here because we will use each element of the list and a chunk.</p>
</blockquote>
<p>Now, we will create our vector embedding database <code>bee_facts</code> and store the document in it:</p>
<div class="sourceCode" id="cb57"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb57-1"><a href="#cb57-1" aria-hidden="true" tabindex="-1"></a>client <span class="op">=</span> chromadb.Client()</span>
<span id="cb57-2"><a href="#cb57-2" aria-hidden="true" tabindex="-1"></a>collection <span class="op">=</span> client.create_collection(name<span class="op">=</span><span class="st">"bee_facts"</span>)</span>
<span id="cb57-3"><a href="#cb57-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-4"><a href="#cb57-4" aria-hidden="true" tabindex="-1"></a><span class="co"># store each document in a vector embedding database</span></span>
<span id="cb57-5"><a href="#cb57-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, d <span class="kw">in</span> <span class="bu">enumerate</span>(documents):</span>
<span id="cb57-6"><a href="#cb57-6" aria-hidden="true" tabindex="-1"></a>  response <span class="op">=</span> ollama.embeddings(model<span class="op">=</span>EMB_MODEL, prompt<span class="op">=</span>d)</span>
<span id="cb57-7"><a href="#cb57-7" aria-hidden="true" tabindex="-1"></a>  embedding <span class="op">=</span> response[<span class="st">"embedding"</span>]</span>
<span id="cb57-8"><a href="#cb57-8" aria-hidden="true" tabindex="-1"></a>  collection.add(</span>
<span id="cb57-9"><a href="#cb57-9" aria-hidden="true" tabindex="-1"></a>    ids<span class="op">=</span>[<span class="bu">str</span>(i)],</span>
<span id="cb57-10"><a href="#cb57-10" aria-hidden="true" tabindex="-1"></a>    embeddings<span class="op">=</span>[embedding],</span>
<span id="cb57-11"><a href="#cb57-11" aria-hidden="true" tabindex="-1"></a>    documents<span class="op">=</span>[d]</span>
<span id="cb57-12"><a href="#cb57-12" aria-hidden="true" tabindex="-1"></a>  )</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Now that we have our ‚ÄúKnowledge Base‚Äù created, we can start making queries, retrieving data from it:</p>
<p><a href="images/png/rag-2-2.png" class="lightbox" data-gallery="quarto-lightbox-gallery-32"><img src="images/png/rag-2-2.png" class="img-fluid"></a></p>
<p><strong>User Query:</strong> The process begins when a user asks a question, such as ‚ÄúHow many bees are in a colony? Who lays eggs, and how much? How about common pests and diseases?‚Äù</p>
<div class="sourceCode" id="cb58"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb58-1"><a href="#cb58-1" aria-hidden="true" tabindex="-1"></a>prompt <span class="op">=</span> <span class="st">"How many bees are in a colony? Who lays eggs and how much? How about</span><span class="ch">\</span></span>
<span id="cb58-2"><a href="#cb58-2" aria-hidden="true" tabindex="-1"></a><span class="st">          common pests and diseases?"</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><strong>Query Embedding:</strong> The user‚Äôs question is converted into a vector embedding using <strong>the same embedding model</strong> used for the knowledge base.</p>
<div class="sourceCode" id="cb59"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb59-1"><a href="#cb59-1" aria-hidden="true" tabindex="-1"></a>response <span class="op">=</span> ollama.embeddings(</span>
<span id="cb59-2"><a href="#cb59-2" aria-hidden="true" tabindex="-1"></a>  prompt<span class="op">=</span>prompt,</span>
<span id="cb59-3"><a href="#cb59-3" aria-hidden="true" tabindex="-1"></a>  model<span class="op">=</span>EMB_MODEL</span>
<span id="cb59-4"><a href="#cb59-4" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><strong>Relevant Document Retrieval:</strong> The system searches the knowledge base using the query embedding to find the most relevant documents (in this case, the 5 more probable). This is done using a similarity search, which compares the query embedding to the document embeddings in the database.</p>
<div class="sourceCode" id="cb60"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb60-1"><a href="#cb60-1" aria-hidden="true" tabindex="-1"></a>results <span class="op">=</span> collection.query(</span>
<span id="cb60-2"><a href="#cb60-2" aria-hidden="true" tabindex="-1"></a>  query_embeddings<span class="op">=</span>[response[<span class="st">"embedding"</span>]],</span>
<span id="cb60-3"><a href="#cb60-3" aria-hidden="true" tabindex="-1"></a>  n_results<span class="op">=</span><span class="dv">5</span></span>
<span id="cb60-4"><a href="#cb60-4" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb60-5"><a href="#cb60-5" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> results[<span class="st">'documents'</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><strong>Prompt Augmentation:</strong> The retrieved relevant information is combined with the original user query to create an augmented prompt. This prompt now contains the user‚Äôs question and pertinent facts from the knowledge base.</p>
<div class="sourceCode" id="cb61"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb61-1"><a href="#cb61-1" aria-hidden="true" tabindex="-1"></a>prompt<span class="op">=</span><span class="ss">f"Using this data: </span><span class="sc">{</span>data<span class="sc">}</span><span class="ss">. Respond to this prompt: </span><span class="sc">{</span>prompt<span class="sc">}</span><span class="ss">"</span>,</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><strong>Answer Generation:</strong> The augmented prompt is then fed into a language model, in this case, the <code>llama3.2:3b</code> model. The model uses this enriched context to generate a comprehensive answer. Parameters like temperature, top_k, and top_p are set to control the randomness and quality of the generated response.</p>
<div class="sourceCode" id="cb62"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb62-1"><a href="#cb62-1" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> ollama.generate(</span>
<span id="cb62-2"><a href="#cb62-2" aria-hidden="true" tabindex="-1"></a>  model<span class="op">=</span>MODEL,</span>
<span id="cb62-3"><a href="#cb62-3" aria-hidden="true" tabindex="-1"></a>  prompt<span class="op">=</span><span class="ss">f"Using this data: </span><span class="sc">{</span>data<span class="sc">}</span><span class="ss">. Respond to this prompt: </span><span class="sc">{</span>prompt<span class="sc">}</span><span class="ss">"</span>,</span>
<span id="cb62-4"><a href="#cb62-4" aria-hidden="true" tabindex="-1"></a>  options<span class="op">=</span>{</span>
<span id="cb62-5"><a href="#cb62-5" aria-hidden="true" tabindex="-1"></a>    <span class="st">"temperature"</span>: <span class="fl">0.0</span>,</span>
<span id="cb62-6"><a href="#cb62-6" aria-hidden="true" tabindex="-1"></a>    <span class="st">"top_k"</span>:<span class="dv">10</span>,</span>
<span id="cb62-7"><a href="#cb62-7" aria-hidden="true" tabindex="-1"></a>    <span class="st">"top_p"</span>:<span class="fl">0.5</span>                          }</span>
<span id="cb62-8"><a href="#cb62-8" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><strong>Response Delivery:</strong> Finally, the system returns the generated answer to the user.</p>
<div class="sourceCode" id="cb63"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb63-1"><a href="#cb63-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(output[<span class="st">'response'</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb64"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb64-1"><a href="#cb64-1" aria-hidden="true" tabindex="-1"></a><span class="ex">Based</span> on the provided data, here are the answers to your questions:</span>
<span id="cb64-2"><a href="#cb64-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-3"><a href="#cb64-3" aria-hidden="true" tabindex="-1"></a><span class="ex">1.</span> How many bees are in a colony<span class="pp">?</span></span>
<span id="cb64-4"><a href="#cb64-4" aria-hidden="true" tabindex="-1"></a><span class="ex">A</span> typical bee colony can contain between 20,000 and 80,000 bees.</span>
<span id="cb64-5"><a href="#cb64-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-6"><a href="#cb64-6" aria-hidden="true" tabindex="-1"></a><span class="ex">2.</span> Who lays eggs and how much<span class="pp">?</span></span>
<span id="cb64-7"><a href="#cb64-7" aria-hidden="true" tabindex="-1"></a><span class="ex">The</span> queen bee lays up to 2,000 eggs per day during peak seasons.</span>
<span id="cb64-8"><a href="#cb64-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-9"><a href="#cb64-9" aria-hidden="true" tabindex="-1"></a><span class="ex">3.</span> What about common pests and diseases<span class="pp">?</span></span>
<span id="cb64-10"><a href="#cb64-10" aria-hidden="true" tabindex="-1"></a><span class="ex">Common</span> pests and diseases that affect bees include varroa mites, hive beetles,</span>
<span id="cb64-11"><a href="#cb64-11" aria-hidden="true" tabindex="-1"></a><span class="ex">and</span> foulbrood.</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Let‚Äôs create a function to help answer new questions:</p>
<div class="sourceCode" id="cb65"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb65-1"><a href="#cb65-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> rag_bees(prompt, n_results<span class="op">=</span><span class="dv">5</span>, temp<span class="op">=</span><span class="fl">0.0</span>, top_k<span class="op">=</span><span class="dv">10</span>, top_p<span class="op">=</span><span class="fl">0.5</span>):</span>
<span id="cb65-2"><a href="#cb65-2" aria-hidden="true" tabindex="-1"></a>    start_time <span class="op">=</span> time.perf_counter()  <span class="co"># Start timing</span></span>
<span id="cb65-3"><a href="#cb65-3" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb65-4"><a href="#cb65-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># generate an embedding for the prompt and retrieve the data </span></span>
<span id="cb65-5"><a href="#cb65-5" aria-hidden="true" tabindex="-1"></a>    response <span class="op">=</span> ollama.embeddings(</span>
<span id="cb65-6"><a href="#cb65-6" aria-hidden="true" tabindex="-1"></a>      prompt<span class="op">=</span>prompt,</span>
<span id="cb65-7"><a href="#cb65-7" aria-hidden="true" tabindex="-1"></a>      model<span class="op">=</span>EMB_MODEL</span>
<span id="cb65-8"><a href="#cb65-8" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb65-9"><a href="#cb65-9" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb65-10"><a href="#cb65-10" aria-hidden="true" tabindex="-1"></a>    results <span class="op">=</span> collection.query(</span>
<span id="cb65-11"><a href="#cb65-11" aria-hidden="true" tabindex="-1"></a>      query_embeddings<span class="op">=</span>[response[<span class="st">"embedding"</span>]],</span>
<span id="cb65-12"><a href="#cb65-12" aria-hidden="true" tabindex="-1"></a>      n_results<span class="op">=</span>n_results</span>
<span id="cb65-13"><a href="#cb65-13" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb65-14"><a href="#cb65-14" aria-hidden="true" tabindex="-1"></a>    data <span class="op">=</span> results[<span class="st">'documents'</span>]</span>
<span id="cb65-15"><a href="#cb65-15" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb65-16"><a href="#cb65-16" aria-hidden="true" tabindex="-1"></a>    <span class="co"># generate a response combining the prompt and data retrieved</span></span>
<span id="cb65-17"><a href="#cb65-17" aria-hidden="true" tabindex="-1"></a>    output <span class="op">=</span> ollama.generate(</span>
<span id="cb65-18"><a href="#cb65-18" aria-hidden="true" tabindex="-1"></a>      model<span class="op">=</span>MODEL,</span>
<span id="cb65-19"><a href="#cb65-19" aria-hidden="true" tabindex="-1"></a>      prompt<span class="op">=</span><span class="ss">f"Using this data: </span><span class="sc">{</span>data<span class="sc">}</span><span class="ss">. Respond to this prompt: </span><span class="sc">{</span>prompt<span class="sc">}</span><span class="ss">"</span>,</span>
<span id="cb65-20"><a href="#cb65-20" aria-hidden="true" tabindex="-1"></a>      options<span class="op">=</span>{</span>
<span id="cb65-21"><a href="#cb65-21" aria-hidden="true" tabindex="-1"></a>        <span class="st">"temperature"</span>: temp,</span>
<span id="cb65-22"><a href="#cb65-22" aria-hidden="true" tabindex="-1"></a>        <span class="st">"top_k"</span>: top_k,</span>
<span id="cb65-23"><a href="#cb65-23" aria-hidden="true" tabindex="-1"></a>        <span class="st">"top_p"</span>: top_p                          }</span>
<span id="cb65-24"><a href="#cb65-24" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb65-25"><a href="#cb65-25" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb65-26"><a href="#cb65-26" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(output[<span class="st">'response'</span>])</span>
<span id="cb65-27"><a href="#cb65-27" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb65-28"><a href="#cb65-28" aria-hidden="true" tabindex="-1"></a>    end_time <span class="op">=</span> time.perf_counter()  <span class="co"># End timing</span></span>
<span id="cb65-29"><a href="#cb65-29" aria-hidden="true" tabindex="-1"></a>    elapsed_time <span class="op">=</span> <span class="bu">round</span>((end_time <span class="op">-</span> start_time), <span class="dv">1</span>)  <span class="co"># Calculate elapsed time</span></span>
<span id="cb65-30"><a href="#cb65-30" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb65-31"><a href="#cb65-31" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss"> [INFO] ==&gt; The code for model: </span><span class="sc">{</span>MODEL<span class="sc">}</span><span class="ss">, took </span><span class="sc">{</span>elapsed_time<span class="sc">}</span><span class="ss">s </span><span class="ch">\</span></span>
<span id="cb65-32"><a href="#cb65-32" aria-hidden="true" tabindex="-1"></a><span class="ss">          to generate the answer.</span><span class="ch">\n</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>We can now create queries and call the function:</p>
<div class="sourceCode" id="cb66"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb66-1"><a href="#cb66-1" aria-hidden="true" tabindex="-1"></a>prompt <span class="op">=</span> <span class="st">"Are bees in Brazil?"</span></span>
<span id="cb66-2"><a href="#cb66-2" aria-hidden="true" tabindex="-1"></a>rag_bees(prompt)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb67"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb67-1"><a href="#cb67-1" aria-hidden="true" tabindex="-1"></a><span class="ex">Yes,</span> bees are found in Brazil. According to the data, Brazil has more than 300</span>
<span id="cb67-2"><a href="#cb67-2" aria-hidden="true" tabindex="-1"></a><span class="ex">different</span> bee species, and indigenous people in Brazil used bees for medicine and</span>
<span id="cb67-3"><a href="#cb67-3" aria-hidden="true" tabindex="-1"></a><span class="ex">food</span> purposes. Additionally, reports from 1577 mention three native bees used by</span>
<span id="cb67-4"><a href="#cb67-4" aria-hidden="true" tabindex="-1"></a><span class="ex">indigenous</span> people in Brazil.</span>
<span id="cb67-5"><a href="#cb67-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-6"><a href="#cb67-6" aria-hidden="true" tabindex="-1"></a> <span class="ex">[INFO]</span> ==<span class="op">&gt;</span> The code for model: llama3.2:3b, took 22.7s to generate the answer.</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>By the way, if the model used supports multiple languages, we can use it (for example, Portuguese), even if the dataset was created in English:</p>
<div class="sourceCode" id="cb68"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb68-1"><a href="#cb68-1" aria-hidden="true" tabindex="-1"></a>prompt <span class="op">=</span> <span class="st">"Existem abelhas no Brazil?"</span></span>
<span id="cb68-2"><a href="#cb68-2" aria-hidden="true" tabindex="-1"></a>rag_bees(prompt)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb69"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb69-1"><a href="#cb69-1" aria-hidden="true" tabindex="-1"></a><span class="ex">Sim,</span> existem abelhas no Brasil! De acordo com o relato de Hans Staden, h√° tr√™s </span>
<span id="cb69-2"><a href="#cb69-2" aria-hidden="true" tabindex="-1"></a><span class="ex">esp√©cies</span> de abelhas nativas do Brasil que foram mencionadas: manda√ßaia <span class="er">(</span><span class="ex">Melipona</span></span>
<span id="cb69-3"><a href="#cb69-3" aria-hidden="true" tabindex="-1"></a><span class="ex">quadrifasciata</span><span class="kw">)</span><span class="ex">,</span> mandaguari <span class="er">(</span><span class="ex">Scaptotrigona</span> postica<span class="kw">)</span> <span class="ex">e</span> jata√≠-amarela <span class="er">(</span><span class="ex">Tetragonisca</span></span>
<span id="cb69-4"><a href="#cb69-4" aria-hidden="true" tabindex="-1"></a><span class="ex">angustula</span><span class="kw">)</span><span class="bu">.</span> Al√©m disso, o Brasil √© conhecido por ter mais de 300 esp√©cies diferentes de abelhas, a maioria das quais n√£o √© agressiva e n√£o p√µe veneno.</span>
<span id="cb69-5"><a href="#cb69-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb69-6"><a href="#cb69-6" aria-hidden="true" tabindex="-1"></a> <span class="ex">[INFO]</span> ==<span class="op">&gt;</span> The code for model: llama3.2:3b, took 54.6s to generate the answer.</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="going-further" class="level3">
<h3 class="anchored" data-anchor-id="going-further">Going Further</h3>
<p>The small LLM models tested worked well at the edge, both in text and with images, but of course, they had high latency regarding the last one. A combination of specific and dedicated models can lead to better results; for example, in real cases, an Object Detection model (such as YOLO) can get a general description and count of objects on an image that, once passed to an LLM, can help extract essential insights and actions.</p>
<p>According to Avi Baum, CTO at Hailo,</p>
<blockquote class="blockquote">
<p>In the vast landscape of artificial intelligence (AI), one of the most intriguing journeys has been the evolution of AI on the edge. This journey has taken us from classic machine vision to the realms of discriminative AI, enhancive AI, and now, the groundbreaking frontier of generative AI. Each step has brought us closer to a future where intelligent systems seamlessly integrate with our daily lives, offering an immersive experience of not just perception but also creation at the palm of our hand.</p>
</blockquote>
<p><a href="images/jpeg/halo.jpg" class="lightbox" data-gallery="quarto-lightbox-gallery-33"><img src="images/jpeg/halo.jpg" class="img-fluid"></a></p>
</section>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>This lab has demonstrated how a Raspberry Pi 5 can be transformed into a potent AI hub capable of running large language models (LLMs) for real-time, on-site data analysis and insights using Ollama and Python. The Raspberry Pi‚Äôs versatility and power, coupled with the capabilities of lightweight LLMs like Llama 3.2 and LLaVa-Phi-3-mini, make it an excellent platform for edge computing applications.</p>
<p>The potential of running LLMs on the edge extends far beyond simple data processing, as in this lab‚Äôs examples. Here are some innovative suggestions for using this project:</p>
<p><strong>1. Smart Home Automation:</strong></p>
<ul>
<li>Integrate SLMs to interpret voice commands or analyze sensor data for intelligent home automation. This could include real-time monitoring and control of home devices, security systems, and energy management, all processed locally without relying on cloud services.</li>
</ul>
<p><strong>2. Field Data Collection and Analysis:</strong></p>
<ul>
<li>Deploy SLMs on Raspberry Pi in remote or mobile setups for real-time data collection and analysis. This can be used in agriculture to monitor crop health, in environmental studies for wildlife tracking, or in disaster response for situational awareness and resource management.</li>
</ul>
<p><strong>3. Educational Tools:</strong></p>
<ul>
<li>Create interactive educational tools that leverage SLMs to provide instant feedback, language translation, and tutoring. This can be particularly useful in developing regions with limited access to advanced technology and internet connectivity.</li>
</ul>
<p><strong>4. Healthcare Applications:</strong></p>
<ul>
<li>Use SLMs for medical diagnostics and patient monitoring. They can provide real-time analysis of symptoms and suggest potential treatments. This can be integrated into telemedicine platforms or portable health devices.</li>
</ul>
<p><strong>5. Local Business Intelligence:</strong></p>
<ul>
<li>Implement SLMs in retail or small business environments to analyze customer behavior, manage inventory, and optimize operations. The ability to process data locally ensures privacy and reduces dependency on external services.</li>
</ul>
<p><strong>6. Industrial IoT:</strong></p>
<ul>
<li>Integrate SLMs into industrial IoT systems for predictive maintenance, quality control, and process optimization. The Raspberry Pi can serve as a localized data processing unit, reducing latency and improving the reliability of automated systems.</li>
</ul>
<p><strong>7. Autonomous Vehicles:</strong></p>
<ul>
<li>Use SLMs to process sensory data from autonomous vehicles, enabling real-time decision-making and navigation. This can be applied to drones, robots, and self-driving cars for enhanced autonomy and safety.</li>
</ul>
<p><strong>8. Cultural Heritage and Tourism:</strong></p>
<ul>
<li>Implement SLMs to provide interactive and informative cultural heritage sites and museum guides. Visitors can use these systems to get real-time information and insights, enhancing their experience without internet connectivity.</li>
</ul>
<p><strong>9. Artistic and Creative Projects:</strong></p>
<ul>
<li>Use SLMs to analyze and generate creative content, such as music, art, and literature. This can foster innovative projects in the creative industries and allow for unique interactive experiences in exhibitions and performances.</li>
</ul>
<p><strong>10. Customized Assistive Technologies:</strong></p>
<ul>
<li>Develop assistive technologies for individuals with disabilities, providing personalized and adaptive support through real-time text-to-speech, language translation, and other accessible tools.</li>
</ul>
</section>
<section id="resources" class="level2">
<h2 class="anchored" data-anchor-id="resources">Resources</h2>
<ul>
<li><a href="https://github.com/Mjrovai/EdgeML-with-Raspberry-Pi/blob/main/OLLAMA_SLMs/10-Ollama_Python_Library.ipynb">10-Ollama_Python_Library notebook</a></li>
<li><a href="https://github.com/Mjrovai/EdgeML-with-Raspberry-Pi/blob/main/OLLAMA_SLMs/20-Ollama_Function_Calling.ipynb">20-Ollama_Function_Calling notebook</a></li>
<li><a href="https://github.com/Mjrovai/EdgeML-with-Raspberry-Pi/blob/main/OLLAMA_SLMs/30-Function_Calling_with_images.ipynb">30-Function_Calling_with_images notebook</a></li>
<li><a href="https://github.com/Mjrovai/EdgeML-with-Raspberry-Pi/blob/main/OLLAMA_SLMs/40-RAG-simple-bee.ipynb">40-RAG-simple-bee notebook</a></li>
<li><a href="https://github.com/Mjrovai/EdgeML-with-Raspberry-Pi/blob/main/OLLAMA_SLMs/calc_distance_image.py">calc_distance_image python script</a></li>
</ul>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "Óßã";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
          // target, if specified
          link.setAttribute("target", "_blank");
          if (link.getAttribute("rel") === null) {
            link.setAttribute("rel", "noopener");
          }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
});
</script>
<script src="https://giscus.app/client.js" data-repo="harvard-edge/cs249r_book" data-repo-id="R_kgDOKQSOaw" data-category="General" data-category-id="DIC_kwDOKQSOa84CZ8Ry" data-mapping="title" data-reactions-enabled="1" data-emit-metadata="0" data-input-position="top" data-theme="light" data-lang="en" crossorigin="anonymous" async="">
</script>
<input type="hidden" id="giscus-base-theme" value="light">
<input type="hidden" id="giscus-alt-theme" value="dark">
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../../../../contents/labs/raspi/object_detection/object_detection.html" class="pagination-link" aria-label="Object Detection">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">Object Detection</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../../../../contents/labs/raspi/vlm/vlm.html" class="pagination-link" aria-label="Vision-Language Models (VLM)">
        <span class="nav-page-text">Vision-Language Models (VLM)</span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>Written, edited and curated by Prof.&nbsp;Vijay Janapa Reddi (Harvard University)</p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    <div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/harvard-edge/cs249r_book/edit/dev/contents/labs/raspi/llm/llm.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li><li><a href="https://github.com/harvard-edge/cs249r_book/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li><li><a href="https://github.com/harvard-edge/cs249r_book/blob/dev/contents/labs/raspi/llm/llm.qmd" class="toc-action"><i class="bi empty"></i>View source</a></li></ul></div></div>
    <div class="nav-footer-right">
<p>This book was built with <a href="https://quarto.org/">Quarto</a>.</p>
</div>
  </div>
</footer>
<script>var lightboxQuarto = GLightbox({"closeEffect":"zoom","descPosition":"bottom","loop":false,"openEffect":"zoom","selector":".lightbox"});
(function() {
  let previousOnload = window.onload;
  window.onload = () => {
    if (previousOnload) {
      previousOnload();
    }
    lightboxQuarto.on('slide_before_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      const href = trigger.getAttribute('href');
      if (href !== null) {
        const imgEl = window.document.querySelector(`a[href="${href}"] img`);
        if (imgEl !== null) {
          const srcAttr = imgEl.getAttribute("src");
          if (srcAttr && srcAttr.startsWith("data:")) {
            slideConfig.href = srcAttr;
          }
        }
      } 
    });
  
    lightboxQuarto.on('slide_after_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(slideNode);
      }
    });
  
  };
  
})();
          </script>




</body></html>