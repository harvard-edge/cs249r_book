<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.31">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>ML Systems Textbook</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../../">
<link href="../../../../contents/labs/shared/shared.html" rel="next">
<link href="../../../../contents/labs/raspi/llm/llm.html" rel="prev">
<link href="../../../../assets/images/icons/favicon.png" rel="icon" type="image/png">
<script src="../../../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../../site_libs/quarto-html/quarto-syntax-highlighting-4a990d8dcb58f517c7c86712b8f2ac7c.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../../site_libs/bootstrap/bootstrap-208a45d5e3541500f9f69911177a25d6.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<link href="../../../../site_libs/quarto-contrib/foldbox/foldbox.css" rel="stylesheet">
<script src="../../../../site_libs/quarto-contrib/glightbox/glightbox.min.js"></script>
<link href="../../../../site_libs/quarto-contrib/glightbox/glightbox.min.css" rel="stylesheet">
<link href="../../../../site_libs/quarto-contrib/glightbox/lightbox.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-M21L0CBCVN"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-M21L0CBCVN', { 'anonymize_ip': true});
</script>
<script type="application/json" class="js-hypothesis-config">
{
  "theme": "clean",
  "openSidebar": false
}
</script>
<script async="" src="https://hypothes.is/embed.js"></script>
<script>
  window.document.addEventListener("DOMContentLoaded", function (_event) {
    document.body.classList.add('hypothesis-enabled');
  });
</script>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">

<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-M21L0CBCVN"></script>
<script type="module" src="../../../../tools/scripts/ai_menu/dist/bundle.js" defer=""></script>
<script type="module" src="../../../../tools/scripts/ai_menu/dist/sqlite3-opfs-async-proxy-B_ImRJXp.js"></script>
<script type="module" src="../../../../tools/scripts/ai_menu/dist/sqlite3-worker1-bundler-friendly-CbDNa4by.js"></script>
<script type="module" src="../../../../tools/scripts/ai_menu/dist/worker-voUF5YDa.js"></script>
<script src="../../../../assets/scripts/sidebar-auto-collapse.js" defer=""></script>
<style>
.callout-chapter-connection {
  --color1: #FDF2F7;
  --color2: #A51C30;
}
.callout-resource-exercises {
  --color1: #E0F2F1;
  --color2: #20B2AA;
}
.callout-code {
  --color1: #F2F4F8;
  --color2: #D1D7E0;
}
.callout-quiz-answer {
  --color1: #E8F2EA;
  --color2: #4a7c59;
}
.callout-quiz-question {
  --color1: #F0F0F8;
  --color2: #5B4B8A;
}
.callout-resource-slides {
  --color1: #E0F2F1;
  --color2: #20B2AA;
}
.callout-resource-videos {
  --color1: #E0F2F1;
  --color2: #20B2AA;
}
</style>
<style>
details.callout-quiz-question > summary::before {
  background-image: url("../../../../assets/images/icons/callouts/icon_callout-quiz-question.png");
}
details.callout-quiz-answer > summary::before {
  background-image: url("../../../../assets/images/icons/callouts/icon_callout-quiz-answer.png");
}
details.callout-chapter-connection > summary::before {
  background-image: url("../../../../assets/images/icons/callouts/icon_callout-chapter-connection.png");
}
details.callout-resource-slides > summary::before {
  background-image: url("../../../../assets/images/icons/callouts/icon_callout-resource-slides.png");
}
details.callout-resource-videos > summary::before {
  background-image: url("../../../../assets/images/icons/callouts/icon_callout-resource-videos.png");
}
details.callout-resource-exercises > summary::before {
  background-image: url("../../../../assets/images/icons/callouts/icon_callout-resource-exercises.png");
}
</style>


</head>

<body class="nav-sidebar floating nav-fixed quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-md " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="../../../../index.html" class="navbar-brand navbar-brand-logo">
    <img src="../../../../assets/images/icons/favicon.png" alt="" class="navbar-logo">
    </a>
    <a class="navbar-brand" href="../../../../index.html">
    <span class="navbar-title">Machine Learning Systems</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link active" href="../../../../contents/labs/labs.html" aria-current="page"> <i class="bi bi-code" role="img">
</i> 
<span class="menu-text">Labs</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../../contents/labs/kits.html"> <i class="bi bi-box" role="img">
</i> 
<span class="menu-text">Kits</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://mlsysbook.ai/pdf" target="_blank"> <i class="bi bi-download" role="img">
</i> 
<span class="menu-text">PDF</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://github.com/harvard-edge/cs249r_book" target="_blank"> <i class="bi bi-star" role="img">
</i> 
<span class="menu-text">Star</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://opencollective.com/mlsysbook" target="_blank"> <i class="bi bi-heart" role="img">
</i> 
<span class="menu-text">Support</span></a>
  </li>  
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-github" role="link" data-bs-toggle="dropdown" aria-expanded="false">
      <i class="bi bi-github" role="img">
</i> 
 <span class="menu-text">GitHub</span>
    </a>
    <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="nav-menu-github">    
        <li>
    <a class="dropdown-item" href="https://github.com/harvard-edge/cs249r_book" target="_blank"><i class="bi bi-pencil" role="img">
</i> 
 <span class="dropdown-text">Edit this page</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://github.com/harvard-edge/cs249r_book/issues/new" target="_blank"><i class="bi bi-bug" role="img">
</i> 
 <span class="dropdown-text">Report an issue</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://github.com/harvard-edge/cs249r_book/discussions" target="_blank"><i class="bi bi-chat" role="img">
</i> 
 <span class="dropdown-text">Discussions</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://github.com/harvard-edge/cs249r_book" target="_blank"><i class="bi bi-code" role="img">
</i> 
 <span class="dropdown-text">View source</span></a>
  </li>  
    </ul>
  </li>
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../../../contents/labs/raspi/raspi.html">Raspberry Pi</a></li><li class="breadcrumb-item"><a href="../../../../contents/labs/raspi/vlm/vlm.html">Vision-Language Models (VLM)</a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
    </div>
  </nav>
  <div id="quarto-announcement" data-announcement-id="80cf830e7ea2136b91547bb117b654b4" class="alert alert-primary hidden"><i class="bi bi-megaphone quarto-announcement-icon"></i><div class="quarto-announcement-content">
<p>🎉 <strong>Just Announced:</strong> <em>Machine Learning Systems</em> will be published by <strong>MIT Press</strong>. <a href="https://www.linkedin.com/posts/vijay-janapa-reddi-63a6a173_tinyml-tikz-ai-activity-7338324711145136128-6WU-?utm_source=share&amp;utm_medium=member_desktop&amp;rcm=ACoAAA-V7E4BcYtyZgLSeGhXC2t9jRSlmazfp-I">See the news →</a><br></p>
<p>🚀 <strong>Sneak Peek:</strong> <a href="https://mlsysbook.github.io/TinyTorch/intro.html">Tiny🔥Torch</a>. Build your own machine learning framework from scratch!<br></p>
<p>🧠 <strong>Self-checks:</strong> Added lightweight <a href="../../../../contents/core/introduction/introduction.html#quiz-question-sec-introduction-ai-ml-basics-041a">quizzes</a> to each chapter for self-assessment.<br></p>
<p>📦 <strong>New Hardware:</strong> <a href="../../../../contents/labs/kits.html">Seeed TinyML Kit</a>. Latest hands-on learning platform.</p>
</div><i class="bi bi-x-lg quarto-announcement-action"></i></div>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Homepage</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/frontmatter/foreword.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Foreword</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/frontmatter/about/about.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">About the Book</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/frontmatter/changelog/changelog.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Book Changelog</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/frontmatter/acknowledgements/acknowledgements.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Acknowledgements</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/frontmatter/socratiq/socratiq.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">SocratiQ AI</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Systems Foundations</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/core/introduction/introduction.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduction</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/core/ml_systems/ml_systems.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">ML Systems</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/core/dl_primer/dl_primer.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">DL Primer</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/core/dnn_architectures/dnn_architectures.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">DNN Architectures</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">Design Principles</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/core/workflow/workflow.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">AI Workflow</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/core/data_engineering/data_engineering.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Data Engineering</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/core/frameworks/frameworks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">AI Frameworks</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/core/training/training.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">AI Training</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true">
 <span class="menu-text">Performance Engineering</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/core/efficient_ai/efficient_ai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Efficient AI</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/core/optimizations/optimizations.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Model Optimizations</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/core/hw_acceleration/hw_acceleration.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">AI Acceleration</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/core/benchmarking/benchmarking.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Benchmarking AI</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true">
 <span class="menu-text">Robust Deployment</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/core/ops/ops.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">ML Operations</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/core/ondevice_learning/ondevice_learning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">On-Device Learning</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/core/robust_ai/robust_ai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Robust AI</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/core/privacy_security/privacy_security.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Security &amp; Privacy</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true">
 <span class="menu-text">Trustworthy Systems</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/core/responsible_ai/responsible_ai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Responsible AI</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/core/sustainable_ai/sustainable_ai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Sustainable AI</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/core/ai_for_good/ai_for_good.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">AI for Good</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true">
 <span class="menu-text">Frontiers of ML Systems</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/core/conclusion/conclusion.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Conclusion</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="true">
 <span class="menu-text">Hands-on Labs</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-7" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/labs/labs.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Getting Started</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/labs/kits.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Hardware Kits</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/labs/ide_setup.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">IDE Setup</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" role="navigation" aria-expanded="true">
 <span class="menu-text">Arduino</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-8" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/labs/arduino/nicla_vision/nicla_vision.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Overview</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/labs/arduino/nicla_vision/setup/setup.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Setup</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/labs/arduino/nicla_vision/image_classification/image_classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Image Classification</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/labs/arduino/nicla_vision/object_detection/object_detection.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Object Detection</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/labs/arduino/nicla_vision/kws/kws.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Keyword Spotting (KWS)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/labs/arduino/nicla_vision/motion_classification/motion_classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Motion Classification and Anomaly Detection</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-9" role="navigation" aria-expanded="true">
 <span class="menu-text">Seeed XIAO ESP32S3</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-9" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-9" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/labs/seeed/xiao_esp32s3/xiao_esp32s3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Overview</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/labs/seeed/xiao_esp32s3/setup/setup.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Setup</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/labs/seeed/xiao_esp32s3/image_classification/image_classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Image Classification</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/labs/seeed/xiao_esp32s3/object_detection/object_detection.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Object Detection</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/labs/seeed/xiao_esp32s3/kws/kws.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Keyword Spotting (KWS)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/labs/seeed/xiao_esp32s3/motion_classification/motion_classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Motion Classification and Anomaly Detection</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-10" role="navigation" aria-expanded="true">
 <span class="menu-text">Grove Vision</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-10" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-10" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/labs/seeed/grove_vision_ai_v2/grove_vision_ai_v2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Overview</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/labs/seeed/grove_vision_ai_v2/setup_and_no_code_apps/setup_and_no_code_apps.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Setup and No-Code Applications</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/labs/seeed/grove_vision_ai_v2/image_classification/image_classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Image Classification</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/labs/seeed/grove_vision_ai_v2/object_detection/object_detection.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Object Detection</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-11" role="navigation" aria-expanded="true">
 <span class="menu-text">Raspberry Pi</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-11" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-11" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/labs/raspi/raspi.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Overview</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/labs/raspi/setup/setup.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Setup</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/labs/raspi/image_classification/image_classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Image Classification</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/labs/raspi/object_detection/object_detection.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Object Detection</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/labs/raspi/llm/llm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Small Language Models (SLM)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/labs/raspi/vlm/vlm.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">Vision-Language Models (VLM)</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-12" role="navigation" aria-expanded="true">
 <span class="menu-text">Shared</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-12" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-12" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/labs/shared/shared.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Overview</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/labs/shared/kws_feature_eng/kws_feature_eng.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">KWS Feature Engineering</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/labs/shared/dsp_spectral_features_block/dsp_spectral_features_block.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">DSP Spectral Features</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-13" role="navigation" aria-expanded="true">
 <span class="menu-text">Resources</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-13" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-13" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../contents/backmatter/resources/phd_survival_guide.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">PhD Survival Guide</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active" data-toc-expanded="99">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#vision-language-models-vlm" id="toc-vision-language-models-vlm" class="nav-link active" data-scroll-target="#vision-language-models-vlm">Vision-Language Models (VLM)</a>
  <ul>
  <li><a href="#sec-visionlanguage-models-vlm-introduction-a2b5" id="toc-sec-visionlanguage-models-vlm-introduction-a2b5" class="nav-link" data-scroll-target="#sec-visionlanguage-models-vlm-introduction-a2b5">Introduction</a>
  <ul class="collapse">
  <li><a href="#sec-visionlanguage-models-vlm-florence2-edge-dee7" id="toc-sec-visionlanguage-models-vlm-florence2-edge-dee7" class="nav-link" data-scroll-target="#sec-visionlanguage-models-vlm-florence2-edge-dee7">Why Florence-2 at the Edge?</a></li>
  <li><a href="#sec-visionlanguage-models-vlm-florence2-model-architecture-38b0" id="toc-sec-visionlanguage-models-vlm-florence2-model-architecture-38b0" class="nav-link" data-scroll-target="#sec-visionlanguage-models-vlm-florence2-model-architecture-38b0">Florence-2 Model Architecture</a></li>
  </ul></li>
  <li><a href="#sec-visionlanguage-models-vlm-technical-overview-e7c5" id="toc-sec-visionlanguage-models-vlm-technical-overview-e7c5" class="nav-link" data-scroll-target="#sec-visionlanguage-models-vlm-technical-overview-e7c5">Technical Overview</a>
  <ul class="collapse">
  <li><a href="#sec-visionlanguage-models-vlm-architecture-151a" id="toc-sec-visionlanguage-models-vlm-architecture-151a" class="nav-link" data-scroll-target="#sec-visionlanguage-models-vlm-architecture-151a">Architecture</a></li>
  <li><a href="#sec-visionlanguage-models-vlm-training-dataset-fld5b-66dd" id="toc-sec-visionlanguage-models-vlm-training-dataset-fld5b-66dd" class="nav-link" data-scroll-target="#sec-visionlanguage-models-vlm-training-dataset-fld5b-66dd">Training Dataset (FLD-5B)</a></li>
  <li><a href="#sec-visionlanguage-models-vlm-key-capabilities-284b" id="toc-sec-visionlanguage-models-vlm-key-capabilities-284b" class="nav-link" data-scroll-target="#sec-visionlanguage-models-vlm-key-capabilities-284b">Key Capabilities</a>
  <ul class="collapse">
  <li><a href="#sec-visionlanguage-models-vlm-zeroshot-performance-257d" id="toc-sec-visionlanguage-models-vlm-zeroshot-performance-257d" class="nav-link" data-scroll-target="#sec-visionlanguage-models-vlm-zeroshot-performance-257d">Zero-shot Performance</a></li>
  <li><a href="#sec-visionlanguage-models-vlm-finetuned-performance-a223" id="toc-sec-visionlanguage-models-vlm-finetuned-performance-a223" class="nav-link" data-scroll-target="#sec-visionlanguage-models-vlm-finetuned-performance-a223">Fine-tuned Performance</a></li>
  </ul></li>
  <li><a href="#sec-visionlanguage-models-vlm-practical-applications-3c36" id="toc-sec-visionlanguage-models-vlm-practical-applications-3c36" class="nav-link" data-scroll-target="#sec-visionlanguage-models-vlm-practical-applications-3c36">Practical Applications</a></li>
  <li><a href="#sec-visionlanguage-models-vlm-comparing-florence2-vlms-eb11" id="toc-sec-visionlanguage-models-vlm-comparing-florence2-vlms-eb11" class="nav-link" data-scroll-target="#sec-visionlanguage-models-vlm-comparing-florence2-vlms-eb11">Comparing Florence-2 with other VLMs</a></li>
  </ul></li>
  <li><a href="#sec-visionlanguage-models-vlm-setup-installation-577b" id="toc-sec-visionlanguage-models-vlm-setup-installation-577b" class="nav-link" data-scroll-target="#sec-visionlanguage-models-vlm-setup-installation-577b">Setup and Installation</a>
  <ul class="collapse">
  <li><a href="#sec-visionlanguage-models-vlm-environment-configuration-8709" id="toc-sec-visionlanguage-models-vlm-environment-configuration-8709" class="nav-link" data-scroll-target="#sec-visionlanguage-models-vlm-environment-configuration-8709">Environment configuration</a></li>
  <li><a href="#sec-visionlanguage-models-vlm-testing-installation-2041" id="toc-sec-visionlanguage-models-vlm-testing-installation-2041" class="nav-link" data-scroll-target="#sec-visionlanguage-models-vlm-testing-installation-2041">Testing the installation</a>
  <ul class="collapse">
  <li><a href="#sec-visionlanguage-models-vlm-importing-required-libraries-9822" id="toc-sec-visionlanguage-models-vlm-importing-required-libraries-9822" class="nav-link" data-scroll-target="#sec-visionlanguage-models-vlm-importing-required-libraries-9822">Importing Required Libraries</a></li>
  <li><a href="#sec-visionlanguage-models-vlm-determining-device-data-type-2130" id="toc-sec-visionlanguage-models-vlm-determining-device-data-type-2130" class="nav-link" data-scroll-target="#sec-visionlanguage-models-vlm-determining-device-data-type-2130">Determining the Device and Data Type</a></li>
  <li><a href="#sec-visionlanguage-models-vlm-loading-model-processor-224b" id="toc-sec-visionlanguage-models-vlm-loading-model-processor-224b" class="nav-link" data-scroll-target="#sec-visionlanguage-models-vlm-loading-model-processor-224b">Loading the Model and Processor</a></li>
  </ul></li>
  <li><a href="#sec-visionlanguage-models-vlm-defining-prompt-1064" id="toc-sec-visionlanguage-models-vlm-defining-prompt-1064" class="nav-link" data-scroll-target="#sec-visionlanguage-models-vlm-defining-prompt-1064">Defining the Prompt</a>
  <ul class="collapse">
  <li><a href="#sec-visionlanguage-models-vlm-downloading-loading-image-287b" id="toc-sec-visionlanguage-models-vlm-downloading-loading-image-287b" class="nav-link" data-scroll-target="#sec-visionlanguage-models-vlm-downloading-loading-image-287b">Downloading and Loading the Image</a></li>
  <li><a href="#sec-visionlanguage-models-vlm-processing-inputs-6fec" id="toc-sec-visionlanguage-models-vlm-processing-inputs-6fec" class="nav-link" data-scroll-target="#sec-visionlanguage-models-vlm-processing-inputs-6fec">Processing Inputs</a></li>
  </ul></li>
  <li><a href="#sec-visionlanguage-models-vlm-generating-output-4c56" id="toc-sec-visionlanguage-models-vlm-generating-output-4c56" class="nav-link" data-scroll-target="#sec-visionlanguage-models-vlm-generating-output-4c56">Generating the Output</a>
  <ul class="collapse">
  <li><a href="#sec-visionlanguage-models-vlm-decoding-generated-text-22bd" id="toc-sec-visionlanguage-models-vlm-decoding-generated-text-22bd" class="nav-link" data-scroll-target="#sec-visionlanguage-models-vlm-decoding-generated-text-22bd">Decoding the Generated Text</a></li>
  <li><a href="#sec-visionlanguage-models-vlm-postprocessing-generation-0f5d" id="toc-sec-visionlanguage-models-vlm-postprocessing-generation-0f5d" class="nav-link" data-scroll-target="#sec-visionlanguage-models-vlm-postprocessing-generation-0f5d">Post-processing the Generation</a></li>
  <li><a href="#sec-visionlanguage-models-vlm-printing-output-ca98" id="toc-sec-visionlanguage-models-vlm-printing-output-ca98" class="nav-link" data-scroll-target="#sec-visionlanguage-models-vlm-printing-output-ca98">Printing the Output</a></li>
  <li><a href="#sec-visionlanguage-models-vlm-result-2b3b" id="toc-sec-visionlanguage-models-vlm-result-2b3b" class="nav-link" data-scroll-target="#sec-visionlanguage-models-vlm-result-2b3b">Result</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#sec-visionlanguage-models-vlm-florence2-tasks-bf28" id="toc-sec-visionlanguage-models-vlm-florence2-tasks-bf28" class="nav-link" data-scroll-target="#sec-visionlanguage-models-vlm-florence2-tasks-bf28">Florence-2 Tasks</a>
  <ul class="collapse">
  <li><a href="#sec-visionlanguage-models-vlm-object-detection-od-0969" id="toc-sec-visionlanguage-models-vlm-object-detection-od-0969" class="nav-link" data-scroll-target="#sec-visionlanguage-models-vlm-object-detection-od-0969">Object Detection (OD)</a></li>
  <li><a href="#sec-visionlanguage-models-vlm-image-captioning-13d5" id="toc-sec-visionlanguage-models-vlm-image-captioning-13d5" class="nav-link" data-scroll-target="#sec-visionlanguage-models-vlm-image-captioning-13d5">Image Captioning</a></li>
  <li><a href="#sec-visionlanguage-models-vlm-detailed-captioning-32fa" id="toc-sec-visionlanguage-models-vlm-detailed-captioning-32fa" class="nav-link" data-scroll-target="#sec-visionlanguage-models-vlm-detailed-captioning-32fa">Detailed Captioning</a></li>
  <li><a href="#sec-visionlanguage-models-vlm-visual-grounding-7d6d" id="toc-sec-visionlanguage-models-vlm-visual-grounding-7d6d" class="nav-link" data-scroll-target="#sec-visionlanguage-models-vlm-visual-grounding-7d6d">Visual Grounding</a></li>
  <li><a href="#sec-visionlanguage-models-vlm-segmentation-a9ee" id="toc-sec-visionlanguage-models-vlm-segmentation-a9ee" class="nav-link" data-scroll-target="#sec-visionlanguage-models-vlm-segmentation-a9ee">Segmentation</a></li>
  <li><a href="#sec-visionlanguage-models-vlm-dense-region-captioning-939a" id="toc-sec-visionlanguage-models-vlm-dense-region-captioning-939a" class="nav-link" data-scroll-target="#sec-visionlanguage-models-vlm-dense-region-captioning-939a">Dense Region Captioning</a></li>
  <li><a href="#sec-visionlanguage-models-vlm-ocr-region-c4a6" id="toc-sec-visionlanguage-models-vlm-ocr-region-c4a6" class="nav-link" data-scroll-target="#sec-visionlanguage-models-vlm-ocr-region-c4a6">OCR with Region</a></li>
  <li><a href="#sec-visionlanguage-models-vlm-phrase-grounding-specific-expressions-0749" id="toc-sec-visionlanguage-models-vlm-phrase-grounding-specific-expressions-0749" class="nav-link" data-scroll-target="#sec-visionlanguage-models-vlm-phrase-grounding-specific-expressions-0749">Phrase Grounding for Specific Expressions</a></li>
  <li><a href="#sec-visionlanguage-models-vlm-open-vocabulary-object-detection-2ebd" id="toc-sec-visionlanguage-models-vlm-open-vocabulary-object-detection-2ebd" class="nav-link" data-scroll-target="#sec-visionlanguage-models-vlm-open-vocabulary-object-detection-2ebd">Open Vocabulary Object Detection</a></li>
  </ul></li>
  <li><a href="#sec-visionlanguage-models-vlm-exploring-computer-vision-visionlanguage-tasks-02e8" id="toc-sec-visionlanguage-models-vlm-exploring-computer-vision-visionlanguage-tasks-02e8" class="nav-link" data-scroll-target="#sec-visionlanguage-models-vlm-exploring-computer-vision-visionlanguage-tasks-02e8">Exploring computer vision and vision-language tasks</a>
  <ul class="collapse">
  <li><a href="#sec-visionlanguage-models-vlm-caption-17b2" id="toc-sec-visionlanguage-models-vlm-caption-17b2" class="nav-link" data-scroll-target="#sec-visionlanguage-models-vlm-caption-17b2">Caption</a></li>
  <li><a href="#sec-visionlanguage-models-vlm-detailed_caption-d790" id="toc-sec-visionlanguage-models-vlm-detailed_caption-d790" class="nav-link" data-scroll-target="#sec-visionlanguage-models-vlm-detailed_caption-d790">Detailed Caption</a></li>
  <li><a href="#sec-visionlanguage-models-vlm-more_detailed_caption-efa1" id="toc-sec-visionlanguage-models-vlm-more_detailed_caption-efa1" class="nav-link" data-scroll-target="#sec-visionlanguage-models-vlm-more_detailed_caption-efa1">More Detailed Caption</a></li>
  <li><a href="#sec-visionlanguage-models-vlm-od-object-detection-94d3" id="toc-sec-visionlanguage-models-vlm-od-object-detection-94d3" class="nav-link" data-scroll-target="#sec-visionlanguage-models-vlm-od-object-detection-94d3">Object Detection</a></li>
  <li><a href="#sec-visionlanguage-models-vlm-dense_region_caption-fe1d" id="toc-sec-visionlanguage-models-vlm-dense_region_caption-fe1d" class="nav-link" data-scroll-target="#sec-visionlanguage-models-vlm-dense_region_caption-fe1d">Dense Region Caption</a></li>
  <li><a href="#sec-visionlanguage-models-vlm-caption_to_phrase_grounding-703e" id="toc-sec-visionlanguage-models-vlm-caption_to_phrase_grounding-703e" class="nav-link" data-scroll-target="#sec-visionlanguage-models-vlm-caption_to_phrase_grounding-703e">Caption to Phrase Grounding</a></li>
  <li><a href="#sec-visionlanguage-models-vlm-cascade-tasks-ea4f" id="toc-sec-visionlanguage-models-vlm-cascade-tasks-ea4f" class="nav-link" data-scroll-target="#sec-visionlanguage-models-vlm-cascade-tasks-ea4f">Cascade Tasks</a></li>
  <li><a href="#sec-visionlanguage-models-vlm-open_vocabulary_detection-9621" id="toc-sec-visionlanguage-models-vlm-open_vocabulary_detection-9621" class="nav-link" data-scroll-target="#sec-visionlanguage-models-vlm-open_vocabulary_detection-9621">Open Vocabulary Detection</a></li>
  <li><a href="#sec-visionlanguage-models-vlm-referring-expression-segmentation-451b" id="toc-sec-visionlanguage-models-vlm-referring-expression-segmentation-451b" class="nav-link" data-scroll-target="#sec-visionlanguage-models-vlm-referring-expression-segmentation-451b">Referring expression segmentation</a></li>
  <li><a href="#sec-visionlanguage-models-vlm-region-segmentation-a34f" id="toc-sec-visionlanguage-models-vlm-region-segmentation-a34f" class="nav-link" data-scroll-target="#sec-visionlanguage-models-vlm-region-segmentation-a34f">Region to Segmentation</a></li>
  <li><a href="#sec-visionlanguage-models-vlm-region-texts-57dc" id="toc-sec-visionlanguage-models-vlm-region-texts-57dc" class="nav-link" data-scroll-target="#sec-visionlanguage-models-vlm-region-texts-57dc">Region to Texts</a></li>
  <li><a href="#sec-visionlanguage-models-vlm-ocr-5f56" id="toc-sec-visionlanguage-models-vlm-ocr-5f56" class="nav-link" data-scroll-target="#sec-visionlanguage-models-vlm-ocr-5f56">OCR</a></li>
  </ul></li>
  <li><a href="#sec-visionlanguage-models-vlm-latency-summary-aa9d" id="toc-sec-visionlanguage-models-vlm-latency-summary-aa9d" class="nav-link" data-scroll-target="#sec-visionlanguage-models-vlm-latency-summary-aa9d">Latency Summary</a></li>
  <li><a href="#sec-visionlanguage-models-vlm-finetunning-d35f" id="toc-sec-visionlanguage-models-vlm-finetunning-d35f" class="nav-link" data-scroll-target="#sec-visionlanguage-models-vlm-finetunning-d35f">Fine-Tunning</a></li>
  <li><a href="#sec-visionlanguage-models-vlm-summary-25e4" id="toc-sec-visionlanguage-models-vlm-summary-25e4" class="nav-link" data-scroll-target="#sec-visionlanguage-models-vlm-summary-25e4">Summary</a>
  <ul class="collapse">
  <li><a href="#sec-visionlanguage-models-vlm-key-advantages-florence2-5113" id="toc-sec-visionlanguage-models-vlm-key-advantages-florence2-5113" class="nav-link" data-scroll-target="#sec-visionlanguage-models-vlm-key-advantages-florence2-5113">Key Advantages of Florence-2</a></li>
  <li><a href="#sec-visionlanguage-models-vlm-tradeoffs-dfeb" id="toc-sec-visionlanguage-models-vlm-tradeoffs-dfeb" class="nav-link" data-scroll-target="#sec-visionlanguage-models-vlm-tradeoffs-dfeb">Trade-offs</a></li>
  <li><a href="#sec-visionlanguage-models-vlm-best-use-cases-651b" id="toc-sec-visionlanguage-models-vlm-best-use-cases-651b" class="nav-link" data-scroll-target="#sec-visionlanguage-models-vlm-best-use-cases-651b">Best Use Cases</a></li>
  </ul></li>
  <li><a href="#sec-visionlanguage-models-vlm-future-implications-4fad" id="toc-sec-visionlanguage-models-vlm-future-implications-4fad" class="nav-link" data-scroll-target="#sec-visionlanguage-models-vlm-future-implications-4fad">Future Implications</a></li>
  <li><a href="#sec-visionlanguage-models-vlm-resources-99e6" id="toc-sec-visionlanguage-models-vlm-resources-99e6" class="nav-link" data-scroll-target="#sec-visionlanguage-models-vlm-resources-99e6">Resources</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../../../contents/labs/raspi/raspi.html">Raspberry Pi</a></li><li class="breadcrumb-item"><a href="../../../../contents/labs/raspi/vlm/vlm.html">Vision-Language Models (VLM)</a></li></ol></nav></header>




<section id="vision-language-models-vlm" class="level1 unnumbered">
<h1 class="unnumbered">Vision-Language Models (VLM)</h1>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/jpeg/cover.jpg" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="DALL·E prompt - A Raspberry Pi setup featuring vision tasks. The image shows a Raspberry Pi connected to a camera, with various computer vision tasks displayed visually around it, including object detection, image captioning, segmentation, and visual grounding. The Raspberry Pi is placed on a desk, with a display showing bounding boxes and annotations related to these tasks. The background should be a home workspace, with tools and devices typically used by developers and hobbyists."><img src="images/jpeg/cover.jpg" class="img-fluid figure-img" alt="DALL·E prompt - A Raspberry Pi setup featuring vision tasks. The image shows a Raspberry Pi connected to a camera, with various computer vision tasks displayed visually around it, including object detection, image captioning, segmentation, and visual grounding. The Raspberry Pi is placed on a desk, with a display showing bounding boxes and annotations related to these tasks. The background should be a home workspace, with tools and devices typically used by developers and hobbyists."></a></p>
<figcaption><em>DALL·E prompt - A Raspberry Pi setup featuring vision tasks. The image shows a Raspberry Pi connected to a camera, with various computer vision tasks displayed visually around it, including object detection, image captioning, segmentation, and visual grounding. The Raspberry Pi is placed on a desk, with a display showing bounding boxes and annotations related to these tasks. The background should be a home workspace, with tools and devices typically used by developers and hobbyists.</em></figcaption>
</figure>
</div>
<section id="sec-visionlanguage-models-vlm-introduction-a2b5" class="level2">
<h2 class="anchored" data-anchor-id="sec-visionlanguage-models-vlm-introduction-a2b5">Introduction</h2>
<p>In this hands-on lab, we will continuously explore AI applications at the Edge, from the basic setup of the Florence-2, Microsoft’s state-of-the-art vision foundation model, to advanced implementations on devices like the Raspberry Pi. We will learn to use Vision-Languageor Models (VLMs) for tasks such as captioning, object detection, grounding, segmentation, and OCR on a Raspberry Pi.</p>
<section id="sec-visionlanguage-models-vlm-florence2-edge-dee7" class="level3">
<h3 class="anchored" data-anchor-id="sec-visionlanguage-models-vlm-florence2-edge-dee7">Why Florence-2 at the Edge?</h3>
<p><a href="https://arxiv.org/abs/2311.06242">Florence-2</a> is a vision-language model open-sourced by Microsoft under the MIT license, which significantly advances vision-language models by combining a lightweight architecture with robust capabilities. Thanks to its training on the massive FLD-5B dataset, which contains 126 million images and 5.4 billion visual annotations, it achieves performance comparable to larger models. This makes Florence-2 ideal for deployment at the edge, where power and computational resources are limited.</p>
<p>In this tutorial, we will explore how to use Florence-2 for real-time computer vision applications, such as:</p>
<ul>
<li>Image captioning</li>
<li>Object detection</li>
<li>Segmentation</li>
<li>Visual grounding</li>
</ul>
<blockquote class="blockquote">
<p><strong>Visual grounding</strong> involves linking textual descriptions to specific regions within an image. This enables the model to understand where particular objects or entities described in a prompt are in the image. For example, if the prompt is “a red car,” the model will identify and highlight the region where the red car is found in the image. Visual grounding is helpful for applications where precise alignment between text and visual content is needed, such as human-computer interaction, image annotation, and interactive AI systems.</p>
</blockquote>
<p>In the tutorial, we will walk through:</p>
<ul>
<li>Setting up Florence-2 on the Raspberry Pi</li>
<li>Running inference tasks such as object detection and captioning</li>
<li>Optimizing the model to get the best performance from the edge device</li>
<li>Exploring practical, real-world applications with fine-tuning.</li>
</ul>
</section>
<section id="sec-visionlanguage-models-vlm-florence2-model-architecture-38b0" class="level3">
<h3 class="anchored" data-anchor-id="sec-visionlanguage-models-vlm-florence2-model-architecture-38b0">Florence-2 Model Architecture</h3>
<p>Florence-2 utilizes a unified, prompt-based representation to handle various vision-language tasks. The model architecture consists of two main components: an <strong>image encoder</strong> and a <strong>multi-modal transformer encoder-decoder</strong>.</p>
<img src="vlm_files/mediabag/be4aa185e71e7f8a962ec5818dc973a8cc9cca34.svg" class="img-fluid">
<ul>
<li><p><strong>Image Encoder</strong>: The image encoder is based on the <a href="https://arxiv.org/abs/2204.03645">DaViT (Dual Attention Vision Transformers) architecture</a>. It converts input images into a series of visual token embeddings. These embeddings serve as the foundational representations of the visual content, capturing both spatial and contextual information about the image.</p></li>
<li><p><strong>Multi-Modal Transformer Encoder-Decoder</strong>: Florence-2’s core is the multi-modal transformer encoder-decoder, which combines visual token embeddings from the image encoder with textual embeddings generated by a BERT-like model. This combination allows the model to simultaneously process visual and textual inputs, enabling a unified approach to tasks such as image captioning, object detection, and segmentation.</p></li>
</ul>
<p>The model’s training on the extensive FLD-5B dataset ensures it can effectively handle diverse vision tasks without requiring task-specific modifications. Florence-2 uses textual prompts to activate specific tasks, making it highly flexible and capable of zero-shot generalization. For tasks like object detection or visual grounding, the model incorporates additional location tokens to represent regions within the image, ensuring a precise understanding of spatial relationships.</p>
<blockquote class="blockquote">
<p>Florence-2’s compact architecture and innovative training approach allow it to perform computer vision tasks accurately, even on resource-constrained devices like the Raspberry Pi.</p>
</blockquote>
</section>
</section>
<section id="sec-visionlanguage-models-vlm-technical-overview-e7c5" class="level2">
<h2 class="anchored" data-anchor-id="sec-visionlanguage-models-vlm-technical-overview-e7c5">Technical Overview</h2>
<p>Florence-2 introduces several innovative features that set it apart:</p>
<section id="sec-visionlanguage-models-vlm-architecture-151a" class="level3">
<h3 class="anchored" data-anchor-id="sec-visionlanguage-models-vlm-architecture-151a">Architecture</h3>
<p> <img src="images/png/arch.png" class="img-fluid quarto-figure quarto-figure-center" style="width:85.0%"></p>
<ul>
<li><strong>Lightweight Design</strong>: Two variants available
<ul>
<li>Florence-2-Base: 232 million parameters</li>
<li>Florence-2-Large: 771 million parameters</li>
</ul></li>
<li><strong>Unified Representation</strong>: Handles multiple vision tasks through a single architecture</li>
<li><strong>DaViT Vision Encoder</strong>: Converts images into visual token embeddings</li>
<li><strong>Transformer-based Multi-modal Encoder-Decoder</strong>: Processes combined visual and text embeddings</li>
</ul>
</section>
<section id="sec-visionlanguage-models-vlm-training-dataset-fld5b-66dd" class="level3">
<h3 class="anchored" data-anchor-id="sec-visionlanguage-models-vlm-training-dataset-fld5b-66dd">Training Dataset (FLD-5B)</h3>
<p> <img src="images/png/Dataset_FLD-5B.png" class="img-fluid quarto-figure quarto-figure-center" style="width:85.0%"></p>
<ul>
<li>126 million unique images</li>
<li>5.4 billion comprehensive annotations, including:
<ul>
<li>500M text annotations</li>
<li>1.3B region-text annotations</li>
<li>3.6B text-phrase-region annotations</li>
</ul></li>
<li>Automated annotation pipeline using specialist models</li>
<li>Iterative refinement process for high-quality labels</li>
</ul>
</section>
<section id="sec-visionlanguage-models-vlm-key-capabilities-284b" class="level3">
<h3 class="anchored" data-anchor-id="sec-visionlanguage-models-vlm-key-capabilities-284b">Key Capabilities</h3>
<p>Florence-2 excels in multiple vision tasks:</p>
<section id="sec-visionlanguage-models-vlm-zeroshot-performance-257d" class="level4">
<h4 class="anchored" data-anchor-id="sec-visionlanguage-models-vlm-zeroshot-performance-257d">Zero-shot Performance</h4>
<ul>
<li>Image Captioning: Achieves 135.6 CIDEr score on COCO</li>
<li>Visual Grounding: 84.4% recall@1 on Flickr30k</li>
<li>Object Detection: 37.5 mAP on COCO val2017</li>
<li>Referring Expression: 67.0% accuracy on RefCOCO</li>
</ul>
</section>
<section id="sec-visionlanguage-models-vlm-finetuned-performance-a223" class="level4">
<h4 class="anchored" data-anchor-id="sec-visionlanguage-models-vlm-finetuned-performance-a223">Fine-tuned Performance</h4>
<ul>
<li>Competitive with specialist models despite the smaller size</li>
<li>Outperforms larger models in specific benchmarks</li>
<li>Efficient adaptation to new tasks</li>
</ul>
</section>
</section>
<section id="sec-visionlanguage-models-vlm-practical-applications-3c36" class="level3">
<h3 class="anchored" data-anchor-id="sec-visionlanguage-models-vlm-practical-applications-3c36">Practical Applications</h3>
<p>Florence-2 can be applied across various domains:</p>
<ol type="1">
<li><strong>Content Understanding</strong>
<ul>
<li>Automated image captioning for accessibility</li>
<li>Visual content moderation</li>
<li>Media asset management</li>
</ul></li>
<li><strong>E-commerce</strong>
<ul>
<li>Product image analysis</li>
<li>Visual search</li>
<li>Automated product tagging</li>
</ul></li>
<li><strong>Healthcare</strong>
<ul>
<li>Medical image analysis</li>
<li>Diagnostic assistance</li>
<li>Research data processing</li>
</ul></li>
<li><strong>Security &amp; Surveillance</strong>
<ul>
<li>Object detection and tracking</li>
<li>Anomaly detection</li>
<li>Scene understanding</li>
</ul></li>
</ol>
</section>
<section id="sec-visionlanguage-models-vlm-comparing-florence2-vlms-eb11" class="level3">
<h3 class="anchored" data-anchor-id="sec-visionlanguage-models-vlm-comparing-florence2-vlms-eb11">Comparing Florence-2 with other VLMs</h3>
<p>Florence-2 stands out from other visual language models due to its impressive zero-shot capabilities. Unlike models like <a href="https://huggingface.co/blog/paligemma">Google PaliGemma</a>, which rely on extensive fine-tuning to adapt to various tasks, Florence-2 works right out of the box, as we will see in this lab. It can also compete with larger models like GPT-4V and Flamingo, which often have many more parameters but only sometimes match Florence-2’s performance. For example, Florence-2 achieves better zero-shot results than Kosmos-2 despite having over twice the parameters.</p>
<p>In benchmark tests, Florence-2 has shown remarkable performance in tasks like COCO captioning and referring expression comprehension. It outperformed models like PolyFormer and UNINEXT in object detection and segmentation tasks on the <a href="https://docs.ultralytics.com/datasets/detect/coco/">COCO dataset</a>. It is a highly competitive choice for real-world applications where both performance and resource efficiency are crucial.</p>
</section>
</section>
<section id="sec-visionlanguage-models-vlm-setup-installation-577b" class="level2">
<h2 class="anchored" data-anchor-id="sec-visionlanguage-models-vlm-setup-installation-577b">Setup and Installation</h2>
<p>Our choice of edge device is the Raspberry Pi 5 (Raspi-5). Its robust platform is equipped with the Broadcom BCM2712, a 2.4 GHz quad-core 64-bit Arm Cortex-A76 CPU featuring Cryptographic Extension and enhanced caching capabilities. It boasts a VideoCore VII GPU, dual 4Kp60 HDMI® outputs with HDR, and a 4Kp60 HEVC decoder. Memory options include 4 GB and 8 GB of high-speed LPDDR4X SDRAM, with 8 GB being our choice to run Florence-2. It also features expandable storage via a microSD card slot and a PCIe 2.0 interface for fast peripherals such as M.2 SSDs (Solid State Drives).</p>
<blockquote class="blockquote">
<p>For real applications, SSDs are a better option than SD cards.</p>
</blockquote>
<p>We suggest installing an Active Cooler, a dedicated clip-on cooling solution for Raspberry Pi 5 (Raspi-5), for this lab. It combines an aluminum heatsink with a temperature-controlled blower fan to keep the Raspi-5 operating comfortably under heavy loads, such as running Florense-2.</p>
<p> <img src="images/jpeg/raspi5-active-cooler.jpg" class="img-fluid quarto-figure quarto-figure-center" style="width:80.0%"></p>
<section id="sec-visionlanguage-models-vlm-environment-configuration-8709" class="level3">
<h3 class="anchored" data-anchor-id="sec-visionlanguage-models-vlm-environment-configuration-8709">Environment configuration</h3>
<p>To run <a href="https://huggingface.co/microsoft/Florence-2-base">Microsoft Florense-2</a> on the Raspberry Pi 5, we’ll need a few libraries:</p>
<ol type="1">
<li><strong><a href="https://huggingface.co/docs/transformers/en/index">Transformers</a></strong>:
<ul>
<li>Florence-2 uses the <code>transformers</code> library from Hugging Face for model loading and inference. This library provides the architecture for working with pre-trained vision-language models, making it easy to perform tasks like image captioning, object detection, and more. Essentially, <code>transformers</code> helps in interacting with the model, processing input prompts, and obtaining outputs.</li>
</ul></li>
<li><strong>PyTorch</strong>:
<ul>
<li>PyTorch is a deep learning framework that provides the infrastructure needed to run the Florence-2 model, which includes tensor operations, GPU acceleration (if a GPU is available), and model training/inference functionalities. The Florence-2 model is trained in PyTorch, and we need it to leverage its functions, layers, and computation capabilities to perform inferences on the Raspberry Pi.</li>
</ul></li>
<li><strong>Timm</strong> (PyTorch Image Models):
<ul>
<li>Florence-2 uses <code>timm</code> to access efficient implementations of vision models and pre-trained weights. Specifically, the <code>timm</code> library is utilized for the <strong>image encoder</strong> part of Florence-2, particularly for managing the DaViT architecture. It provides model definitions and optimized code for common vision tasks and allows the easy integration of different backbones that are lightweight and suitable for edge devices.</li>
</ul></li>
<li><strong>Einops</strong>:
<ul>
<li><code>Einops</code> is a library for flexible and powerful tensor operations. It makes it easy to reshape and manipulate tensor dimensions, which is especially important for the multi-modal processing done in Florence-2. Vision-language models like Florence-2 often need to rearrange image data, text embeddings, and visual embeddings to align correctly for the transformer blocks, and <code>einops</code> simplifies these complex operations, making the code more readable and concise.</li>
</ul></li>
</ol>
<p>In short, these libraries enable different essential components of Florence-2:</p>
<ul>
<li><strong>Transformers</strong> and <strong>PyTorch</strong> are needed to load the model and run the inference.</li>
<li><strong>Timm</strong> is used to access and efficiently implement the vision encoder.</li>
<li><strong>Einops</strong> helps reshape data, facilitating the integration of visual and text features.</li>
</ul>
<p>All these components work together to help Florence-2 run seamlessly on our Raspberry Pi, allowing it to perform complex vision-language tasks relatively quickly.</p>
<p>Considering that the Raspberry Pi already has its OS installed, let’s use <code>SSH</code> to reach it from another computer:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ssh</span> mjrovai@raspi-5.local</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>And check the IP allocated to it:</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="fu">hostname</span> <span class="at">-I</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><code>192.168.4.209</code></p>
<p> <img src="images/png/ssh.png" class="img-fluid"></p>
<p><strong>Updating the Raspberry Pi</strong></p>
<p>First, ensure your Raspberry Pi is up to date:</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="fu">sudo</span> apt update</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="fu">sudo</span> apt upgrade <span class="at">-y</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><strong>Initial setup for using PIP</strong>:</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="fu">sudo</span> apt install python3-pip</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="fu">sudo</span> rm /usr/lib/python3.11/EXTERNALLY-MANAGED</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="ex">pip3</span> install <span class="at">--upgrade</span> pip</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><strong>Install Dependencies</strong></p>
<div class="sourceCode" id="cb5"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="fu">sudo</span> apt-get install libjpeg-dev libopenblas-dev libopenmpi-dev <span class="dt">\</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>    libomp-dev</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Let’s set up and activate a <strong>Virtual Environment</strong> for working with Florence-2:</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="ex">python3</span> <span class="at">-m</span> venv ~/florence</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="bu">source</span> ~/florence/bin/activate</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><strong>Install PyTorch</strong></p>
<div class="sourceCode" id="cb7"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="ex">pip3</span> install setuptools numpy Cython</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="ex">pip3</span> install requests</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="ex">pip3</span> install torch torchvision <span class="dt">\</span></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>    <span class="at">--index-url</span> https://download.pytorch.org/whl/cpu</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a><span class="ex">pip3</span> install torchaudio <span class="dt">\</span></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>    <span class="at">--index-url</span> https://download.pytorch.org/whl/cpu</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Let’s verify that PyTorch is correctly installed:</p>
<p> <img src="images/png/test-pytorch.png" class="img-fluid"></p>
<p><strong>Install Transformers, Timm and Einops</strong>:</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="ex">pip3</span> install transformers</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="ex">pip3</span> install timm einops</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><strong>Install the model</strong>:</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="ex">pip3</span> install autodistill-florence-2</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><strong>Jupyter Notebook and Python libraries</strong></p>
<p>Installing a Jupyter Notebook to run and test our Python scripts is possible.</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="ex">pip3</span> install jupyter</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a><span class="ex">pip3</span> install numpy Pillow matplotlib</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a><span class="ex">jupyter</span> notebook <span class="at">--generate-config</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="sec-visionlanguage-models-vlm-testing-installation-2041" class="level3">
<h3 class="anchored" data-anchor-id="sec-visionlanguage-models-vlm-testing-installation-2041">Testing the installation</h3>
<p>Running the Jupyter Notebook on the remote computer</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="ex">jupyter</span> notebook <span class="at">--ip</span><span class="op">=</span>192.168.4.209 <span class="at">--no-browser</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Running the above command on the SSH terminal, we can see the local URL address to open the notebook:</p>
<p> <img src="images/png/jupyter.png" class="img-fluid"></p>
<p>The notebook with the code used on this initial test can be found on the Lab GitHub:</p>
<ul>
<li><a href="https://github.com/Mjrovai/EdgeML-with-Raspberry-Pi/blob/main/FLORENCE-2/notebooks/10-florence2_test.ipynb">10-florence2_test.ipynb</a></li>
</ul>
<p>We can access it on the remote computer by entering the Raspberry Pi’s IP address and the provided token in a web browser (copy the entire URL from the terminal).</p>
<p>From the Home page, create a new notebook [<code>Python 3 (ipykernel)</code> ] and copy and paste the <a href="https://huggingface.co/microsoft/Florence-2-base#how-to-get-started-with-the-model">example code</a> from Hugging Face Hub.</p>
<p>The code is designed to run Florence-2 on a given image to perform <strong>object detection</strong>. It loads the model, processes an image and a prompt, and then generates a response to identify and describe the objects in the image.</p>
<ul>
<li>The <strong>processor</strong> helps prepare text and image inputs.</li>
<li>The <strong>model</strong> takes the processed inputs to generate a meaningful response.</li>
<li>The <strong>post-processing</strong> step refines the generated output into a more interpretable form, like bounding boxes for detected objects.</li>
</ul>
<blockquote class="blockquote">
<p>This workflow leverages the versatility of Florence-2 to handle <strong>vision-language tasks</strong> and is implemented efficiently using PyTorch, Transformers, and related image-processing tools.</p>
</blockquote>
<div class="sourceCode" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> requests</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> PIL <span class="im">import</span> Image</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> AutoProcessor, AutoModelForCausalLM</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>device <span class="op">=</span> <span class="st">"cuda:0"</span> <span class="cf">if</span> torch.cuda.is_available() <span class="cf">else</span> <span class="st">"cpu"</span></span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a>torch_dtype <span class="op">=</span> (</span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a>   torch.float16 <span class="cf">if</span> torch.cuda.is_available() <span class="cf">else</span> torch.float32</span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> AutoModelForCausalLM.from_pretrained(</span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a>    <span class="st">"microsoft/Florence-2-base"</span>,</span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"></a>    torch_dtype<span class="op">=</span>torch_dtype,</span>
<span id="cb12-14"><a href="#cb12-14" aria-hidden="true" tabindex="-1"></a>    trust_remote_code<span class="op">=</span><span class="va">True</span></span>
<span id="cb12-15"><a href="#cb12-15" aria-hidden="true" tabindex="-1"></a>).to(device)</span>
<span id="cb12-16"><a href="#cb12-16" aria-hidden="true" tabindex="-1"></a>processor <span class="op">=</span> AutoProcessor.from_pretrained(</span>
<span id="cb12-17"><a href="#cb12-17" aria-hidden="true" tabindex="-1"></a>    <span class="st">"microsoft/Florence-2-base"</span>,</span>
<span id="cb12-18"><a href="#cb12-18" aria-hidden="true" tabindex="-1"></a>    trust_remote_code<span class="op">=</span><span class="va">True</span></span>
<span id="cb12-19"><a href="#cb12-19" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb12-20"><a href="#cb12-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-21"><a href="#cb12-21" aria-hidden="true" tabindex="-1"></a>prompt <span class="op">=</span> <span class="st">"&lt;OD&gt;"</span></span>
<span id="cb12-22"><a href="#cb12-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-23"><a href="#cb12-23" aria-hidden="true" tabindex="-1"></a>url <span class="op">=</span> (</span>
<span id="cb12-24"><a href="#cb12-24" aria-hidden="true" tabindex="-1"></a>    <span class="st">"https://huggingface.co/datasets/huggingface/"</span></span>
<span id="cb12-25"><a href="#cb12-25" aria-hidden="true" tabindex="-1"></a>    <span class="st">"documentation-images/resolve/main/transformers/"</span></span>
<span id="cb12-26"><a href="#cb12-26" aria-hidden="true" tabindex="-1"></a>    <span class="st">"tasks/car.jpg?download=true"</span></span>
<span id="cb12-27"><a href="#cb12-27" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb12-28"><a href="#cb12-28" aria-hidden="true" tabindex="-1"></a>image <span class="op">=</span> Image.<span class="bu">open</span>(requests.get(url, stream<span class="op">=</span><span class="va">True</span>).raw)</span>
<span id="cb12-29"><a href="#cb12-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-30"><a href="#cb12-30" aria-hidden="true" tabindex="-1"></a>inputs <span class="op">=</span> processor(</span>
<span id="cb12-31"><a href="#cb12-31" aria-hidden="true" tabindex="-1"></a>    text<span class="op">=</span>prompt,</span>
<span id="cb12-32"><a href="#cb12-32" aria-hidden="true" tabindex="-1"></a>    images<span class="op">=</span>image,</span>
<span id="cb12-33"><a href="#cb12-33" aria-hidden="true" tabindex="-1"></a>    return_tensors<span class="op">=</span><span class="st">"pt"</span></span>
<span id="cb12-34"><a href="#cb12-34" aria-hidden="true" tabindex="-1"></a>).to(device, torch_dtype)</span>
<span id="cb12-35"><a href="#cb12-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-36"><a href="#cb12-36" aria-hidden="true" tabindex="-1"></a>generated_ids <span class="op">=</span> model.generate(</span>
<span id="cb12-37"><a href="#cb12-37" aria-hidden="true" tabindex="-1"></a>    input_ids<span class="op">=</span>inputs[<span class="st">"input_ids"</span>],</span>
<span id="cb12-38"><a href="#cb12-38" aria-hidden="true" tabindex="-1"></a>    pixel_values<span class="op">=</span>inputs[<span class="st">"pixel_values"</span>],</span>
<span id="cb12-39"><a href="#cb12-39" aria-hidden="true" tabindex="-1"></a>    max_new_tokens<span class="op">=</span><span class="dv">1024</span>,</span>
<span id="cb12-40"><a href="#cb12-40" aria-hidden="true" tabindex="-1"></a>    do_sample<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb12-41"><a href="#cb12-41" aria-hidden="true" tabindex="-1"></a>    num_beams<span class="op">=</span><span class="dv">3</span>,</span>
<span id="cb12-42"><a href="#cb12-42" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb12-43"><a href="#cb12-43" aria-hidden="true" tabindex="-1"></a>generated_text <span class="op">=</span> processor.batch_decode(</span>
<span id="cb12-44"><a href="#cb12-44" aria-hidden="true" tabindex="-1"></a>   generated_ids, skip_special_tokens<span class="op">=</span><span class="va">False</span>)[<span class="dv">0</span>]</span>
<span id="cb12-45"><a href="#cb12-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-46"><a href="#cb12-46" aria-hidden="true" tabindex="-1"></a>parsed_answer <span class="op">=</span> processor.post_process_generation(</span>
<span id="cb12-47"><a href="#cb12-47" aria-hidden="true" tabindex="-1"></a>    generated_text,</span>
<span id="cb12-48"><a href="#cb12-48" aria-hidden="true" tabindex="-1"></a>    task<span class="op">=</span><span class="st">"&lt;OD&gt;"</span>,</span>
<span id="cb12-49"><a href="#cb12-49" aria-hidden="true" tabindex="-1"></a>    image_size<span class="op">=</span>(image.width, image.height)</span>
<span id="cb12-50"><a href="#cb12-50" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb12-51"><a href="#cb12-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-52"><a href="#cb12-52" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(parsed_answer)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Let’s break down the provided code step by step:</p>
<section id="sec-visionlanguage-models-vlm-importing-required-libraries-9822" class="level4">
<h4 class="anchored" data-anchor-id="sec-visionlanguage-models-vlm-importing-required-libraries-9822">Importing Required Libraries</h4>
<div class="sourceCode" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> requests</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> PIL <span class="im">import</span> Image</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> AutoProcessor, AutoModelForCausalLM</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<ul>
<li><strong>requests</strong>: Used to make HTTP requests. In this case, it downloads an image from a URL.</li>
<li><strong>PIL (Pillow)</strong>: Provides tools for manipulating images. Here, it’s used to open the downloaded image.</li>
<li><strong>torch</strong>: PyTorch is imported to handle tensor operations and determine the hardware availability (CPU or GPU).</li>
<li><strong>transformers</strong>: This module provides easy access to Florence-2 by using <code>AutoProcessor</code> and <code>AutoModelForCausalLM</code> to load pre-trained models and process inputs.</li>
</ul>
</section>
<section id="sec-visionlanguage-models-vlm-determining-device-data-type-2130" class="level4">
<h4 class="anchored" data-anchor-id="sec-visionlanguage-models-vlm-determining-device-data-type-2130">Determining the Device and Data Type</h4>
<div class="sourceCode" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>device <span class="op">=</span> (</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>    <span class="st">"cuda:0"</span></span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> torch.cuda.is_available()</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span> <span class="st">"cpu"</span></span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>torch_dtype <span class="op">=</span> (</span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>    torch.float16</span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> torch.cuda.is_available()</span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span> torch.float32</span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<ul>
<li><strong>Device Setup</strong>: The code checks if a CUDA-enabled GPU is available (<code>torch.cuda.is_available()</code>). The device is set to “cuda:0” if a GPU is available. Otherwise, it defaults to <code>"cpu"</code> (our case here).</li>
<li><strong>Data Type Setup</strong>: If a GPU is available, <code>torch.float16</code> is chosen, which uses half-precision floats to speed up processing and reduce memory usage. On the CPU, it defaults to <code>torch.float32</code> to maintain compatibility.</li>
</ul>
</section>
<section id="sec-visionlanguage-models-vlm-loading-model-processor-224b" class="level4">
<h4 class="anchored" data-anchor-id="sec-visionlanguage-models-vlm-loading-model-processor-224b">Loading the Model and Processor</h4>
<div class="sourceCode" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> AutoModelForCausalLM.from_pretrained(</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>    <span class="st">"microsoft/Florence-2-base"</span>,</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>    torch_dtype<span class="op">=</span>torch_dtype,</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>    trust_remote_code<span class="op">=</span><span class="va">True</span></span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>).to(device)</span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a>processor <span class="op">=</span> AutoProcessor.from_pretrained(</span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a>    <span class="st">"microsoft/Florence-2-base"</span>,</span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a>    trust_remote_code<span class="op">=</span><span class="va">True</span></span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<ul>
<li><strong>Model Initialization</strong>:
<ul>
<li><strong><code>AutoModelForCausalLM.from_pretrained()</code></strong> loads the pre-trained Florence-2 model from Microsoft’s repository on Hugging Face. The <code>torch_dtype</code> is set according to the available hardware (GPU/CPU), and <code>trust_remote_code=True</code> allows the use of any custom code that might be provided with the model.</li>
<li><strong><code>.to(device)</code></strong> moves the model to the appropriate device (either CPU or GPU). In our case, it will be set to <code>CPU</code>.</li>
</ul></li>
<li><strong>Processor Initialization</strong>:
<ul>
<li><strong><code>AutoProcessor.from_pretrained()</code></strong> loads the processor for Florence-2. The processor is responsible for transforming text and image inputs into a format the model can work with (e.g., encoding text, normalizing images, etc.).</li>
</ul></li>
</ul>
</section>
</section>
<section id="sec-visionlanguage-models-vlm-defining-prompt-1064" class="level3">
<h3 class="anchored" data-anchor-id="sec-visionlanguage-models-vlm-defining-prompt-1064">Defining the Prompt</h3>
<div class="sourceCode" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>prompt <span class="op">=</span> <span class="st">"&lt;OD&gt;"</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<ul>
<li><strong>Prompt Definition</strong>: The string <code>"&lt;OD&gt;"</code> is used as a prompt. This refers to “Object Detection”, instructing the model to detect objects on the image.</li>
</ul>
<section id="sec-visionlanguage-models-vlm-downloading-loading-image-287b" class="level4">
<h4 class="anchored" data-anchor-id="sec-visionlanguage-models-vlm-downloading-loading-image-287b">Downloading and Loading the Image</h4>
<div class="sourceCode" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>url <span class="op">=</span> <span class="st">"https://huggingface.co/datasets/huggingface/"</span></span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>      <span class="co">"documentation-images/resolve/main/transformers/"</span></span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>      <span class="co">"tasks/car.jpg?download=true"</span></span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a>image <span class="op">=</span> Image.<span class="bu">open</span>(requests.get(url, stream<span class="op">=</span><span class="va">True</span>).raw)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<ul>
<li><strong>Downloading the Image</strong>: The <strong><code>requests.get()</code></strong> function fetches the image from the specified URL. The <code>stream=True</code> parameter ensures the image is streamed rather than downloaded completely at once.</li>
<li><strong>Opening the Image</strong>: <strong><code>Image.open()</code></strong> opens the image so the model can process it.</li>
</ul>
</section>
<section id="sec-visionlanguage-models-vlm-processing-inputs-6fec" class="level4">
<h4 class="anchored" data-anchor-id="sec-visionlanguage-models-vlm-processing-inputs-6fec">Processing Inputs</h4>
<div class="sourceCode" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>inputs <span class="op">=</span> processor(</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>    text<span class="op">=</span>prompt,</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>    images<span class="op">=</span>image,</span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a>    return_tensors<span class="op">=</span><span class="st">"pt"</span></span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a>).to(</span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a>    device,</span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a>    torch_dtype</span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<ul>
<li><strong>Processing Input Data</strong>: The <strong><code>processor()</code></strong> function processes the text (<code>prompt</code>) and the image (<code>image</code>). The <code>return_tensors="pt"</code> argument converts the processed data into PyTorch tensors, which are necessary for inputting data into the model.</li>
<li><strong>Moving Inputs to Device</strong>: <strong><code>.to(device, torch_dtype)</code></strong> moves the inputs to the correct device (CPU or GPU) and assigns the appropriate data type.</li>
</ul>
</section>
</section>
<section id="sec-visionlanguage-models-vlm-generating-output-4c56" class="level3">
<h3 class="anchored" data-anchor-id="sec-visionlanguage-models-vlm-generating-output-4c56">Generating the Output</h3>
<div class="sourceCode" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>generated_ids <span class="op">=</span> model.generate(</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>    input_ids<span class="op">=</span>inputs[<span class="st">"input_ids"</span>],</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>    pixel_values<span class="op">=</span>inputs[<span class="st">"pixel_values"</span>],</span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a>    max_new_tokens<span class="op">=</span><span class="dv">1024</span>,</span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a>    do_sample<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a>    num_beams<span class="op">=</span><span class="dv">3</span>,</span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<ul>
<li><strong>Model Generation</strong>: <strong><code>model.generate()</code></strong> is used to generate the output based on the input data.
<ul>
<li><strong><code>input_ids</code></strong>: Represents the tokenized form of the prompt.</li>
<li><strong><code>pixel_values</code></strong>: Contains the processed image data.</li>
<li><strong><code>max_new_tokens=1024</code></strong>: Specifies the maximum number of new tokens to be generated in the response. This limits the response length.</li>
<li><strong><code>do_sample=False</code></strong>: Disables sampling; instead, the generation uses deterministic methods (beam search).</li>
<li><strong><code>num_beams=3</code></strong>: Enables beam search with three beams, which improves output quality by considering multiple possibilities during generation.</li>
</ul></li>
</ul>
<section id="sec-visionlanguage-models-vlm-decoding-generated-text-22bd" class="level4">
<h4 class="anchored" data-anchor-id="sec-visionlanguage-models-vlm-decoding-generated-text-22bd">Decoding the Generated Text</h4>
<div class="sourceCode" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a>generated_text <span class="op">=</span> processor.batch_decode(</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>    generated_ids,</span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>    skip_special_tokens<span class="op">=</span><span class="va">False</span></span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a>)[<span class="dv">0</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<ul>
<li><strong>Batch Decode</strong>: <strong><code>processor.batch_decode()</code></strong> decodes the generated IDs (tokens) into readable text. The <code>skip_special_tokens=False</code> parameter means that the output will include any special tokens that may be part of the response.</li>
</ul>
</section>
<section id="sec-visionlanguage-models-vlm-postprocessing-generation-0f5d" class="level4">
<h4 class="anchored" data-anchor-id="sec-visionlanguage-models-vlm-postprocessing-generation-0f5d">Post-processing the Generation</h4>
<div class="sourceCode" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a>parsed_answer <span class="op">=</span> processor.post_process_generation(</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>    generated_text,</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a>    task<span class="op">=</span><span class="st">"&lt;OD&gt;"</span>,</span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a>    image_size<span class="op">=</span>(image.width, image.height)</span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<ul>
<li><strong>Post-Processing</strong>: <strong><code>processor.post_process_generation()</code></strong> is called to process the generated text further, interpreting it based on the task (<code>"&lt;OD&gt;"</code> for object detection) and the size of the image.</li>
<li>This function extracts specific information from the generated text, such as bounding boxes for detected objects, making the output more useful for visual tasks.</li>
</ul>
</section>
<section id="sec-visionlanguage-models-vlm-printing-output-ca98" class="level4">
<h4 class="anchored" data-anchor-id="sec-visionlanguage-models-vlm-printing-output-ca98">Printing the Output</h4>
<div class="sourceCode" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(parsed_answer)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<ul>
<li>Finally, <strong><code>print(parsed_answer)</code></strong> displays the output, which could include object detection results, such as bounding box coordinates and labels for the detected objects in the image.</li>
</ul>
</section>
<section id="sec-visionlanguage-models-vlm-result-2b3b" class="level4">
<h4 class="anchored" data-anchor-id="sec-visionlanguage-models-vlm-result-2b3b">Result</h4>
<p>Running the code, we get as the Parsed Answer:</p>
<div class="sourceCode" id="cb23"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="ex">[{</span><span class="st">'&lt;OD&gt;'</span><span class="ex">:</span> {</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>   <span class="st">'bboxes'</span><span class="ex">:</span> [</span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a>     <span class="ex">[34.23999786376953,</span> 160.0800018310547, 597.4400024414062],</span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a>     <span class="ex">[371.7599792480469,</span> 272.32000732421875, 241.67999267578125],</span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a>     <span class="ex">[303.67999267578125,</span> 247.4399871826172, 454.0799865722656],</span>
<span id="cb23-6"><a href="#cb23-6" aria-hidden="true" tabindex="-1"></a>     <span class="ex">[276.7200012207031,</span> 553.9199829101562, 370.79998779296875],</span>
<span id="cb23-7"><a href="#cb23-7" aria-hidden="true" tabindex="-1"></a>     <span class="ex">[96.31999969482422,</span> 280.55999755859375, 198.0800018310547],</span>
<span id="cb23-8"><a href="#cb23-8" aria-hidden="true" tabindex="-1"></a>     <span class="ex">[371.2799987792969]</span></span>
<span id="cb23-9"><a href="#cb23-9" aria-hidden="true" tabindex="-1"></a>    <span class="ex">],</span></span>
<span id="cb23-10"><a href="#cb23-10" aria-hidden="true" tabindex="-1"></a>    <span class="st">'labels'</span><span class="ex">:</span> [<span class="st">'car'</span>, <span class="st">'door handle'</span>, <span class="st">'wheel'</span>, <span class="st">'wheel'</span>]</span>
<span id="cb23-11"><a href="#cb23-11" aria-hidden="true" tabindex="-1"></a><span class="er">}}</span><span class="ex">]</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>First, let’s inspect the image:</p>
<div class="sourceCode" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">8</span>))</span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a>plt.imshow(image)</span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a>plt.axis(<span class="st">'off'</span>)</span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p> <img src="images/png/fusca.png" class="img-fluid quarto-figure quarto-figure-center" style="width:80.0%"></p>
<p>By the Object Detection result, we can see that:</p>
<div class="sourceCode" id="cb25"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="st">'labels'</span><span class="ex">:</span> [<span class="st">'car'</span>, <span class="st">'door handle'</span>, <span class="st">'wheel'</span>, <span class="st">'wheel'</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>It seems that at least a few objects were detected. We can also implement a code to draw the bounding boxes in the find objects:</p>
<div class="sourceCode" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_bbox(image, data):</span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>   <span class="co"># Create a figure and axes</span></span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a>    fig, ax <span class="op">=</span> plt.subplots()</span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Display the image</span></span>
<span id="cb26-6"><a href="#cb26-6" aria-hidden="true" tabindex="-1"></a>    ax.imshow(image)</span>
<span id="cb26-7"><a href="#cb26-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-8"><a href="#cb26-8" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Plot each bounding box</span></span>
<span id="cb26-9"><a href="#cb26-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> bbox, label <span class="kw">in</span> <span class="bu">zip</span>(data[<span class="st">'bboxes'</span>], data[<span class="st">'labels'</span>]):</span>
<span id="cb26-10"><a href="#cb26-10" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Unpack the bounding box coordinates</span></span>
<span id="cb26-11"><a href="#cb26-11" aria-hidden="true" tabindex="-1"></a>        x1, y1, x2, y2 <span class="op">=</span> bbox</span>
<span id="cb26-12"><a href="#cb26-12" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Create a Rectangle patch</span></span>
<span id="cb26-13"><a href="#cb26-13" aria-hidden="true" tabindex="-1"></a>        rect <span class="op">=</span> patches.Rectangle(</span>
<span id="cb26-14"><a href="#cb26-14" aria-hidden="true" tabindex="-1"></a>           (x1, y1), x2 <span class="op">-</span> x1, y2 <span class="op">-</span> y1,</span>
<span id="cb26-15"><a href="#cb26-15" aria-hidden="true" tabindex="-1"></a>           linewidth<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb26-16"><a href="#cb26-16" aria-hidden="true" tabindex="-1"></a>           edgecolor<span class="op">=</span><span class="st">'r'</span>,</span>
<span id="cb26-17"><a href="#cb26-17" aria-hidden="true" tabindex="-1"></a>           facecolor<span class="op">=</span><span class="st">'none'</span></span>
<span id="cb26-18"><a href="#cb26-18" aria-hidden="true" tabindex="-1"></a>         )</span>
<span id="cb26-19"><a href="#cb26-19" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Add the rectangle to the Axes</span></span>
<span id="cb26-20"><a href="#cb26-20" aria-hidden="true" tabindex="-1"></a>        ax.add_patch(rect)</span>
<span id="cb26-21"><a href="#cb26-21" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Annotate the label</span></span>
<span id="cb26-22"><a href="#cb26-22" aria-hidden="true" tabindex="-1"></a>        plt.text(x1, y1, label, color<span class="op">=</span><span class="st">'white'</span>, fontsize<span class="op">=</span><span class="dv">8</span>,</span>
<span id="cb26-23"><a href="#cb26-23" aria-hidden="true" tabindex="-1"></a>                 bbox<span class="op">=</span><span class="bu">dict</span>(facecolor<span class="op">=</span><span class="st">'red'</span>, alpha<span class="op">=</span><span class="fl">0.5</span>))</span>
<span id="cb26-24"><a href="#cb26-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-25"><a href="#cb26-25" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Remove the axis ticks and labels</span></span>
<span id="cb26-26"><a href="#cb26-26" aria-hidden="true" tabindex="-1"></a>    ax.axis(<span class="st">'off'</span>)</span>
<span id="cb26-27"><a href="#cb26-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-28"><a href="#cb26-28" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Show the plot</span></span>
<span id="cb26-29"><a href="#cb26-29" aria-hidden="true" tabindex="-1"></a>    plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<blockquote class="blockquote">
<p><strong>Box (x0, y0, x1, y1)</strong>: Location tokens correspond to the top-left and bottom-right corners of a box.</p>
</blockquote>
<p>And running</p>
<pre><code>plot_bbox(image, parsed_answer['&lt;OD&gt;'])</code></pre>
<p>We get:</p>
<p> <img src="images/png/result_test.png" class="img-fluid quarto-figure quarto-figure-center" style="width:80.0%"></p>
</section>
</section>
</section>
<section id="sec-visionlanguage-models-vlm-florence2-tasks-bf28" class="level2">
<h2 class="anchored" data-anchor-id="sec-visionlanguage-models-vlm-florence2-tasks-bf28">Florence-2 Tasks</h2>
<p>Florence-2 is designed to perform a variety of computer vision and vision-language tasks through <code>prompts</code>. These tasks can be activated by providing a specific textual prompt to the model, as we saw with <code>&lt;OD&gt;</code> (Object Detection).</p>
<p>Florence-2’s versatility comes from combining these prompts, allowing us to guide the model’s behavior to perform specific vision tasks. Changing the prompt allows us to adapt Florence-2 to different tasks without needing task-specific modifications in the architecture. This capability directly results from Florence-2’s unified model architecture and large-scale multi-task training on the FLD-5B dataset.</p>
<p>Here are some of the key tasks that Florence-2 can perform, along with example prompts:</p>
<section id="sec-visionlanguage-models-vlm-object-detection-od-0969" class="level3">
<h3 class="anchored" data-anchor-id="sec-visionlanguage-models-vlm-object-detection-od-0969">Object Detection (OD)</h3>
<ul>
<li><strong>Prompt</strong>: <code>"&lt;OD&gt;"</code></li>
<li><strong>Description</strong>: Identifies objects in an image and provides bounding boxes for each detected object. This task is helpful for applications like visual inspection, surveillance, and general object recognition.</li>
</ul>
</section>
<section id="sec-visionlanguage-models-vlm-image-captioning-13d5" class="level3">
<h3 class="anchored" data-anchor-id="sec-visionlanguage-models-vlm-image-captioning-13d5">Image Captioning</h3>
<ul>
<li><strong>Prompt</strong>: <code>"&lt;CAPTION&gt;"</code></li>
<li><strong>Description</strong>: Generates a textual description for an input image. This task helps the model describe what is happening in the image, providing a human-readable caption for content understanding.</li>
</ul>
</section>
<section id="sec-visionlanguage-models-vlm-detailed-captioning-32fa" class="level3">
<h3 class="anchored" data-anchor-id="sec-visionlanguage-models-vlm-detailed-captioning-32fa">Detailed Captioning</h3>
<ul>
<li><strong>Prompt</strong>: <code>"&lt;DETAILED_CAPTION&gt;"</code></li>
<li><strong>Description</strong>: Generates a more detailed caption with more nuanced information about the scene, such as the objects present and their relationships.</li>
</ul>
</section>
<section id="sec-visionlanguage-models-vlm-visual-grounding-7d6d" class="level3">
<h3 class="anchored" data-anchor-id="sec-visionlanguage-models-vlm-visual-grounding-7d6d">Visual Grounding</h3>
<ul>
<li><strong>Prompt</strong>: <code>"&lt;CAPTION_TO_PHRASE_GROUNDING&gt;"</code></li>
<li><strong>Description</strong>: Links a textual description to specific regions in an image. For example, given a prompt like “a green car,” the model highlights where the green car is in the image. This is useful for human-computer interaction, where you must find specific objects based on text.</li>
</ul>
</section>
<section id="sec-visionlanguage-models-vlm-segmentation-a9ee" class="level3">
<h3 class="anchored" data-anchor-id="sec-visionlanguage-models-vlm-segmentation-a9ee">Segmentation</h3>
<ul>
<li><strong>Prompt</strong>: <code>"&lt;REFERRING_EXPRESSION_SEGMENTATION&gt;"</code></li>
<li><strong>Description</strong>: Performs segmentation based on a referring expression, such as “the blue cup.” The model identifies and segments the specific region containing the object mentioned in the prompt (all related pixels).</li>
</ul>
</section>
<section id="sec-visionlanguage-models-vlm-dense-region-captioning-939a" class="level3">
<h3 class="anchored" data-anchor-id="sec-visionlanguage-models-vlm-dense-region-captioning-939a">Dense Region Captioning</h3>
<ul>
<li><strong>Prompt</strong>: <code>"&lt;DENSE_REGION_CAPTION&gt;"</code></li>
<li><strong>Description</strong>: Provides captions for multiple regions within an image, offering a detailed breakdown of all visible areas, including different objects and their relationships.</li>
</ul>
</section>
<section id="sec-visionlanguage-models-vlm-ocr-region-c4a6" class="level3">
<h3 class="anchored" data-anchor-id="sec-visionlanguage-models-vlm-ocr-region-c4a6">OCR with Region</h3>
<ul>
<li><strong>Prompt</strong>: <code>"&lt;OCR_WITH_REGION&gt;"</code></li>
<li><strong>Description</strong>: Performs Optical Character Recognition (OCR) on an image and provides bounding boxes for the detected text. This is useful for extracting and locating textual information in images, such as reading signs, labels, or other forms of text in images.</li>
</ul>
</section>
<section id="sec-visionlanguage-models-vlm-phrase-grounding-specific-expressions-0749" class="level3">
<h3 class="anchored" data-anchor-id="sec-visionlanguage-models-vlm-phrase-grounding-specific-expressions-0749">Phrase Grounding for Specific Expressions</h3>
<ul>
<li><strong>Prompt</strong>: <code>"&lt;CAPTION_TO_PHRASE_GROUNDING&gt;"</code> along with a specific expression, such as <code>"a wine glass"</code>.</li>
<li><strong>Description</strong>: Locates the area in the image that corresponds to a specific textual phrase. This task allows for identifying particular objects or elements when prompted with a word or keyword.</li>
</ul>
</section>
<section id="sec-visionlanguage-models-vlm-open-vocabulary-object-detection-2ebd" class="level3">
<h3 class="anchored" data-anchor-id="sec-visionlanguage-models-vlm-open-vocabulary-object-detection-2ebd">Open Vocabulary Object Detection</h3>
<ul>
<li><strong>Prompt</strong>: <code>"&lt;OPEN_VOCABULARY_OD&gt;"</code></li>
<li><strong>Description</strong>: The model can detect objects without being restricted to a predefined list of classes, making it helpful in recognizing a broader range of items based on general visual understanding.</li>
</ul>
</section>
</section>
<section id="sec-visionlanguage-models-vlm-exploring-computer-vision-visionlanguage-tasks-02e8" class="level2">
<h2 class="anchored" data-anchor-id="sec-visionlanguage-models-vlm-exploring-computer-vision-visionlanguage-tasks-02e8">Exploring computer vision and vision-language tasks</h2>
<p>For exploration, all codes can be found on the GitHub:</p>
<ul>
<li><a href="https://github.com/Mjrovai/EdgeML-with-Raspberry-Pi/blob/main/FLORENCE-2/notebooks/20-florence_2.ipynb">20-florence_2.ipynb</a></li>
</ul>
<p>Let’s use a couple of images created by Dall-E and upload them to the Rasp-5 (FileZilla can be used for that). The images will be saved on a sub-folder named <code>images</code> :</p>
<div class="sourceCode" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a>dogs_cats <span class="op">=</span> Image.<span class="bu">open</span>(<span class="st">'./images/dogs-cats.jpg'</span>)</span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a>table <span class="op">=</span> Image.<span class="bu">open</span>(<span class="st">'./images/table.jpg'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p> <img src="images/png/img_test.png" class="img-fluid"></p>
<p>Let’s create a function to facilitate our exploration and to keep track of the latency of the model for different tasks:</p>
<div class="sourceCode" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> run_example(task_prompt, text_input<span class="op">=</span><span class="va">None</span>, image<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a>    start_time <span class="op">=</span> time.perf_counter()  <span class="co"># Start timing</span></span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> text_input <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb29-4"><a href="#cb29-4" aria-hidden="true" tabindex="-1"></a>        prompt <span class="op">=</span> task_prompt</span>
<span id="cb29-5"><a href="#cb29-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb29-6"><a href="#cb29-6" aria-hidden="true" tabindex="-1"></a>        prompt <span class="op">=</span> task_prompt <span class="op">+</span> text_input</span>
<span id="cb29-7"><a href="#cb29-7" aria-hidden="true" tabindex="-1"></a>    inputs <span class="op">=</span> processor(text<span class="op">=</span>prompt, images<span class="op">=</span>image,</span>
<span id="cb29-8"><a href="#cb29-8" aria-hidden="true" tabindex="-1"></a>                       return_tensors<span class="op">=</span><span class="st">"pt"</span>).to(device)</span>
<span id="cb29-9"><a href="#cb29-9" aria-hidden="true" tabindex="-1"></a>    generated_ids <span class="op">=</span> model.generate(</span>
<span id="cb29-10"><a href="#cb29-10" aria-hidden="true" tabindex="-1"></a>      input_ids<span class="op">=</span>inputs[<span class="st">"input_ids"</span>],</span>
<span id="cb29-11"><a href="#cb29-11" aria-hidden="true" tabindex="-1"></a>      pixel_values<span class="op">=</span>inputs[<span class="st">"pixel_values"</span>],</span>
<span id="cb29-12"><a href="#cb29-12" aria-hidden="true" tabindex="-1"></a>      max_new_tokens<span class="op">=</span><span class="dv">1024</span>,</span>
<span id="cb29-13"><a href="#cb29-13" aria-hidden="true" tabindex="-1"></a>      early_stopping<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb29-14"><a href="#cb29-14" aria-hidden="true" tabindex="-1"></a>      do_sample<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb29-15"><a href="#cb29-15" aria-hidden="true" tabindex="-1"></a>      num_beams<span class="op">=</span><span class="dv">3</span>,</span>
<span id="cb29-16"><a href="#cb29-16" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb29-17"><a href="#cb29-17" aria-hidden="true" tabindex="-1"></a>    generated_text <span class="op">=</span> processor.batch_decode(</span>
<span id="cb29-18"><a href="#cb29-18" aria-hidden="true" tabindex="-1"></a>       generated_ids,</span>
<span id="cb29-19"><a href="#cb29-19" aria-hidden="true" tabindex="-1"></a>       skip_special_tokens<span class="op">=</span><span class="va">False</span></span>
<span id="cb29-20"><a href="#cb29-20" aria-hidden="true" tabindex="-1"></a>    )[<span class="dv">0</span>]</span>
<span id="cb29-21"><a href="#cb29-21" aria-hidden="true" tabindex="-1"></a>    parsed_answer <span class="op">=</span> processor.post_process_generation(</span>
<span id="cb29-22"><a href="#cb29-22" aria-hidden="true" tabindex="-1"></a>        generated_text,</span>
<span id="cb29-23"><a href="#cb29-23" aria-hidden="true" tabindex="-1"></a>        task<span class="op">=</span>task_prompt,</span>
<span id="cb29-24"><a href="#cb29-24" aria-hidden="true" tabindex="-1"></a>        image_size<span class="op">=</span>(image.width, image.height)</span>
<span id="cb29-25"><a href="#cb29-25" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb29-26"><a href="#cb29-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-27"><a href="#cb29-27" aria-hidden="true" tabindex="-1"></a>    end_time <span class="op">=</span> time.perf_counter() <span class="co"># End timing</span></span>
<span id="cb29-28"><a href="#cb29-28" aria-hidden="true" tabindex="-1"></a>    elapsed_time <span class="op">=</span> end_time <span class="op">-</span> start_time <span class="co"># Calculate elapsed time</span></span>
<span id="cb29-29"><a href="#cb29-29" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f" </span><span class="ch">\n</span><span class="ss">[INFO] ==&gt; Florence-2-base (</span><span class="sc">{</span>task_prompt<span class="sc">}</span><span class="ss">), </span><span class="op">\</span></span>
<span id="cb29-30"><a href="#cb29-30" aria-hidden="true" tabindex="-1"></a><span class="ss">          took </span><span class="sc">{</span>elapsed_time<span class="sc">:.1f}</span><span class="ss"> seconds to execute.</span><span class="ch">\n</span><span class="ss">"</span>)</span>
<span id="cb29-31"><a href="#cb29-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-32"><a href="#cb29-32" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> parsed_answer</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<section id="sec-visionlanguage-models-vlm-caption-17b2" class="level3">
<h3 class="anchored" data-anchor-id="sec-visionlanguage-models-vlm-caption-17b2">Caption</h3>
<p><strong>1. Dogs and Cats</strong></p>
<div class="sourceCode" id="cb30"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a>run_example(task_prompt<span class="op">=</span><span class="st">'&lt;CAPTION&gt;'</span>,image<span class="op">=</span>dogs_cats)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb31"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a><span class="ex">[INFO]</span> ==<span class="op">&gt;</span> Florence-2-base <span class="er">(</span><span class="op">&lt;</span>CAPTION<span class="op">&gt;</span><span class="kw">)</span><span class="ex">,</span> <span class="dt">\</span></span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a>took 16.1 seconds to execute.</span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a><span class="ex">{</span><span class="st">'&lt;CAPTION&gt;'</span><span class="ex">:</span> <span class="st">'A group of dogs and cats sitting in a garden.'</span>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><strong>2. Table</strong></p>
<div class="sourceCode" id="cb32"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a>run_example(task_prompt<span class="op">=</span><span class="st">'&lt;CAPTION&gt;'</span>,image<span class="op">=</span>table)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb33"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a><span class="ex">[INFO]</span> ==<span class="op">&gt;</span> Florence-2-base <span class="er">(</span><span class="op">&lt;</span>CAPTION<span class="op">&gt;</span><span class="kw">)</span><span class="ex">,</span> <span class="dt">\</span></span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a>took 16.5 seconds to execute.</span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-4"><a href="#cb33-4" aria-hidden="true" tabindex="-1"></a><span class="ex">{</span><span class="st">'&lt;CAPTION&gt;'</span><span class="ex">:</span> <span class="st">'A wooden table topped with a plate of fruit \</span></span>
<span id="cb33-5"><a href="#cb33-5" aria-hidden="true" tabindex="-1"></a><span class="st">and a glass of wine.'</span>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="sec-visionlanguage-models-vlm-detailed_caption-d790" class="level3">
<h3 class="anchored" data-anchor-id="sec-visionlanguage-models-vlm-detailed_caption-d790">Detailed Caption</h3>
<p><strong>1. Dogs and Cats</strong></p>
<div class="sourceCode" id="cb34"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a>run_example(task_prompt<span class="op">=</span><span class="st">'&lt;DETAILED_CAPTION&gt;'</span>,image<span class="op">=</span>dogs_cats)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb35"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a><span class="ex">[INFO]</span> ==<span class="op">&gt;</span> Florence-2-base <span class="er">(</span><span class="op">&lt;</span>DETAILED_CAPTION<span class="op">&gt;</span><span class="kw">)</span><span class="ex">,</span> <span class="dt">\</span></span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a>took 25.5 seconds to execute.</span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-4"><a href="#cb35-4" aria-hidden="true" tabindex="-1"></a><span class="ex">{</span><span class="st">'&lt;DETAILED_CAPTION&gt;'</span><span class="ex">:</span> <span class="st">'The image shows a group of cats and \</span></span>
<span id="cb35-5"><a href="#cb35-5" aria-hidden="true" tabindex="-1"></a><span class="st">dogs sitting on top of a lush green field, surrounded by plants \</span></span>
<span id="cb35-6"><a href="#cb35-6" aria-hidden="true" tabindex="-1"></a><span class="st">with flowers, trees, and a house in the background. The sky is \</span></span>
<span id="cb35-7"><a href="#cb35-7" aria-hidden="true" tabindex="-1"></a><span class="st">visible above them, creating a peaceful atmosphere.'</span>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><strong>2. Table</strong></p>
<div class="sourceCode" id="cb36"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a>run_example(task_prompt<span class="op">=</span><span class="st">'&lt;DETAILED_CAPTION&gt;'</span>,image<span class="op">=</span>table)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb37"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a><span class="ex">[INFO]</span> ==<span class="op">&gt;</span> Florence-2-base <span class="er">(</span><span class="op">&lt;</span>DETAILED_CAPTION<span class="op">&gt;</span><span class="kw">)</span><span class="ex">,</span> <span class="dt">\</span></span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a>took 26.8 seconds to execute.</span>
<span id="cb37-3"><a href="#cb37-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-4"><a href="#cb37-4" aria-hidden="true" tabindex="-1"></a><span class="ex">{</span><span class="st">'&lt;DETAILED_CAPTION&gt;'</span><span class="ex">:</span> <span class="st">'The image shows a wooden table with \</span></span>
<span id="cb37-5"><a href="#cb37-5" aria-hidden="true" tabindex="-1"></a><span class="st">a bottle of wine and a glass of wine on it, surrounded by \</span></span>
<span id="cb37-6"><a href="#cb37-6" aria-hidden="true" tabindex="-1"></a><span class="st">a variety of fruits such as apples, oranges, and grapes. \</span></span>
<span id="cb37-7"><a href="#cb37-7" aria-hidden="true" tabindex="-1"></a><span class="st">In the background, there are chairs, plants, trees, and \</span></span>
<span id="cb37-8"><a href="#cb37-8" aria-hidden="true" tabindex="-1"></a><span class="st">a house, all slightly blurred.'</span>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="sec-visionlanguage-models-vlm-more_detailed_caption-efa1" class="level3">
<h3 class="anchored" data-anchor-id="sec-visionlanguage-models-vlm-more_detailed_caption-efa1">More Detailed Caption</h3>
<p><strong>1. Dogs and Cats</strong></p>
<div class="sourceCode" id="cb38"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a>run_example(task_prompt<span class="op">=</span><span class="st">'&lt;MORE_DETAILED_CAPTION&gt;'</span>,image<span class="op">=</span>dogs_cats)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb39"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a><span class="ex">[INFO]</span> ==<span class="op">&gt;</span> Florence-2-base <span class="er">(</span><span class="op">&lt;</span>MORE_DETAILED_CAPTION<span class="op">&gt;</span><span class="kw">)</span><span class="ex">,</span> <span class="dt">\</span></span>
<span id="cb39-2"><a href="#cb39-2" aria-hidden="true" tabindex="-1"></a>took 49.8 seconds to execute.</span>
<span id="cb39-3"><a href="#cb39-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-4"><a href="#cb39-4" aria-hidden="true" tabindex="-1"></a><span class="ex">{</span><span class="st">'&lt;MORE_DETAILED_CAPTION&gt;'</span><span class="ex">:</span> <span class="st">'The image shows a group of four \</span></span>
<span id="cb39-5"><a href="#cb39-5" aria-hidden="true" tabindex="-1"></a><span class="st">cats and a dog in a garden. The garden is filled with colorful \</span></span>
<span id="cb39-6"><a href="#cb39-6" aria-hidden="true" tabindex="-1"></a><span class="st">flowers and plants, and there is a pathway leading up to \</span></span>
<span id="cb39-7"><a href="#cb39-7" aria-hidden="true" tabindex="-1"></a><span class="st">a house in the background. The main focus of the image is \</span></span>
<span id="cb39-8"><a href="#cb39-8" aria-hidden="true" tabindex="-1"></a><span class="st">a large German Shepherd dog standing on the left side of \</span></span>
<span id="cb39-9"><a href="#cb39-9" aria-hidden="true" tabindex="-1"></a><span class="st">the garden, with its tongue hanging out and its mouth open, \</span></span>
<span id="cb39-10"><a href="#cb39-10" aria-hidden="true" tabindex="-1"></a><span class="st">as if it is panting or panting. On the right side, there are \</span></span>
<span id="cb39-11"><a href="#cb39-11" aria-hidden="true" tabindex="-1"></a><span class="st">two smaller cats, one orange and one gray, sitting on the \</span></span>
<span id="cb39-12"><a href="#cb39-12" aria-hidden="true" tabindex="-1"></a><span class="st">grass. In the background, there is another golden retriever \</span></span>
<span id="cb39-13"><a href="#cb39-13" aria-hidden="true" tabindex="-1"></a><span class="st">dog sitting and looking at the camera. The sky is blue and \</span></span>
<span id="cb39-14"><a href="#cb39-14" aria-hidden="true" tabindex="-1"></a><span class="st">the sun is shining, creating a warm and inviting atmosphere.'</span>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><strong>2. Table</strong></p>
<div class="sourceCode" id="cb40"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a>run_example(task_prompt<span class="op">=</span><span class="st">'&lt; MORE_DETAILED_CAPTION&gt;'</span>,image<span class="op">=</span>table)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb41"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a><span class="ex">INFO]</span> ==<span class="op">&gt;</span> Florence-2-base <span class="er">(</span><span class="op">&lt;</span>MORE_DETAILED_CAPTION<span class="op">&gt;</span><span class="kw">)</span><span class="ex">,</span> <span class="dt">\</span></span>
<span id="cb41-2"><a href="#cb41-2" aria-hidden="true" tabindex="-1"></a>took 32.4 seconds to execute.</span>
<span id="cb41-3"><a href="#cb41-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-4"><a href="#cb41-4" aria-hidden="true" tabindex="-1"></a><span class="ex">{</span><span class="st">'&lt;MORE_DETAILED_CAPTION&gt;'</span><span class="ex">:</span> <span class="st">'The image shows a wooden table \</span></span>
<span id="cb41-5"><a href="#cb41-5" aria-hidden="true" tabindex="-1"></a><span class="st">with a wooden tray on it. On the tray, there are various \</span></span>
<span id="cb41-6"><a href="#cb41-6" aria-hidden="true" tabindex="-1"></a><span class="st">fruits such as grapes, oranges, apples, and grapes. There \</span></span>
<span id="cb41-7"><a href="#cb41-7" aria-hidden="true" tabindex="-1"></a><span class="st">is also a bottle of red wine on the table. The background \</span></span>
<span id="cb41-8"><a href="#cb41-8" aria-hidden="true" tabindex="-1"></a><span class="st">shows a garden with trees and a house. The overall mood \</span></span>
<span id="cb41-9"><a href="#cb41-9" aria-hidden="true" tabindex="-1"></a><span class="st">of the image is peaceful and serene.'</span>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<blockquote class="blockquote">
<p>We can note that the more detailed the caption task, the longer the latency and the possibility of mistakes (like “The image shows a group of four cats and a dog in a garden”, instead of two dogs and three cats).</p>
</blockquote>
</section>
<section id="sec-visionlanguage-models-vlm-od-object-detection-94d3" class="level3">
<h3 class="anchored" data-anchor-id="sec-visionlanguage-models-vlm-od-object-detection-94d3">Object Detection</h3>
<p>We can run the same previous function for object detection using the prompt <code>&lt;OD&gt;</code>.</p>
<div class="sourceCode" id="cb42"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a>task_prompt <span class="op">=</span> <span class="st">'&lt;OD&gt;'</span></span>
<span id="cb42-2"><a href="#cb42-2" aria-hidden="true" tabindex="-1"></a>results <span class="op">=</span> run_example(task_prompt,image<span class="op">=</span>dogs_cats)</span>
<span id="cb42-3"><a href="#cb42-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(results)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Let’s see the result:</p>
<div class="sourceCode" id="cb43"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a><span class="ex">[INFO]</span> ==<span class="op">&gt;</span> Florence-2-base <span class="er">(</span><span class="op">&lt;</span>OD<span class="op">&gt;</span><span class="kw">)</span><span class="ex">,</span> took 20.9 seconds to execute.</span>
<span id="cb43-2"><a href="#cb43-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-3"><a href="#cb43-3" aria-hidden="true" tabindex="-1"></a><span class="ex">{</span><span class="st">'&lt;OD&gt;'</span><span class="ex">:</span> {<span class="st">'bboxes'</span>: [</span>
<span id="cb43-4"><a href="#cb43-4" aria-hidden="true" tabindex="-1"></a>  <span class="ex">[737.79,</span> 571.90, 1022.46, 980.48],</span>
<span id="cb43-5"><a href="#cb43-5" aria-hidden="true" tabindex="-1"></a>  <span class="ex">[0.51,</span> 593.40, 211.45, 991.74],</span>
<span id="cb43-6"><a href="#cb43-6" aria-hidden="true" tabindex="-1"></a>  <span class="ex">[445.95,</span> 721.40, 680.44, 850.43],</span>
<span id="cb43-7"><a href="#cb43-7" aria-hidden="true" tabindex="-1"></a>  <span class="ex">[39.42,</span> 91.64, 491.00, 933.37],</span>
<span id="cb43-8"><a href="#cb43-8" aria-hidden="true" tabindex="-1"></a>  <span class="ex">[570.88,</span> 184.83, 974.33, 782.84]</span>
<span id="cb43-9"><a href="#cb43-9" aria-hidden="true" tabindex="-1"></a>  <span class="ex">],</span></span>
<span id="cb43-10"><a href="#cb43-10" aria-hidden="true" tabindex="-1"></a>  <span class="st">'labels'</span><span class="ex">:</span> [<span class="st">'cat'</span>, <span class="st">'cat'</span>, <span class="st">'cat'</span>, <span class="st">'dog'</span>, <span class="st">'dog'</span>]</span>
<span id="cb43-11"><a href="#cb43-11" aria-hidden="true" tabindex="-1"></a><span class="er">}}</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Only by the labels <code>['cat,' 'cat,' 'cat,' 'dog,' 'dog']</code> is it possible to see that the main objects in the image were captured. Let’s apply the function used before to draw the bounding boxes:</p>
<div class="sourceCode" id="cb44"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a>plot_bbox(dogs_cats, results[<span class="st">'&lt;OD&gt;'</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p> <img src="images/png/od_cat_dogs.png" class="img-fluid quarto-figure quarto-figure-center" style="width:65.0%"></p>
<p>Let’s also do it with the Table image:</p>
<div class="sourceCode" id="cb45"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a>task_prompt <span class="op">=</span> <span class="st">'&lt;OD&gt;'</span></span>
<span id="cb45-2"><a href="#cb45-2" aria-hidden="true" tabindex="-1"></a>results <span class="op">=</span> run_example(task_prompt,image<span class="op">=</span>table)</span>
<span id="cb45-3"><a href="#cb45-3" aria-hidden="true" tabindex="-1"></a>plot_bbox(table, results[<span class="st">'&lt;OD&gt;'</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="ba"><code>[INFO] ==&gt; Florence-2-base (&lt;OD&gt;), took 40.8 seconds to execute.</code></pre>
<p> <img src="images/png/od_table.png" class="img-fluid quarto-figure quarto-figure-center" style="width:75.0%"></p>
</section>
<section id="sec-visionlanguage-models-vlm-dense_region_caption-fe1d" class="level3">
<h3 class="anchored" data-anchor-id="sec-visionlanguage-models-vlm-dense_region_caption-fe1d">Dense Region Caption</h3>
<p>It is possible to mix the classic Object Detection with the Caption task in specific sub-regions of the image:</p>
<div class="sourceCode" id="cb47"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb47-1"><a href="#cb47-1" aria-hidden="true" tabindex="-1"></a>task_prompt <span class="op">=</span> <span class="st">'&lt;DENSE_REGION_CAPTION&gt;'</span></span>
<span id="cb47-2"><a href="#cb47-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-3"><a href="#cb47-3" aria-hidden="true" tabindex="-1"></a>results <span class="op">=</span> run_example(task_prompt,image<span class="op">=</span>dogs_cats)</span>
<span id="cb47-4"><a href="#cb47-4" aria-hidden="true" tabindex="-1"></a>plot_bbox(dogs_cats, results[<span class="st">'&lt;DENSE_REGION_CAPTION&gt;'</span>])</span>
<span id="cb47-5"><a href="#cb47-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-6"><a href="#cb47-6" aria-hidden="true" tabindex="-1"></a>results <span class="op">=</span> run_example(task_prompt,image<span class="op">=</span>table)</span>
<span id="cb47-7"><a href="#cb47-7" aria-hidden="true" tabindex="-1"></a>plot_bbox(table, results[<span class="st">'&lt;DENSE_REGION_CAPTION&gt;'</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p> <img src="images/png/od_caption_cat_dog_table.png" class="img-fluid"></p>
</section>
<section id="sec-visionlanguage-models-vlm-caption_to_phrase_grounding-703e" class="level3">
<h3 class="anchored" data-anchor-id="sec-visionlanguage-models-vlm-caption_to_phrase_grounding-703e">Caption to Phrase Grounding</h3>
<p>With this task, we can enter with a caption, such as “a wine glass”, “a wine bottle,” or “a half orange,” and Florence-2 will localize the object in the image:</p>
<div class="sourceCode" id="cb48"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a>task_prompt <span class="op">=</span> <span class="st">'&lt;CAPTION_TO_PHRASE_GROUNDING&gt;'</span></span>
<span id="cb48-2"><a href="#cb48-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-3"><a href="#cb48-3" aria-hidden="true" tabindex="-1"></a>results <span class="op">=</span> run_example(</span>
<span id="cb48-4"><a href="#cb48-4" aria-hidden="true" tabindex="-1"></a>    task_prompt,</span>
<span id="cb48-5"><a href="#cb48-5" aria-hidden="true" tabindex="-1"></a>    text_input<span class="op">=</span><span class="st">"a wine bottle"</span>,</span>
<span id="cb48-6"><a href="#cb48-6" aria-hidden="true" tabindex="-1"></a>    image<span class="op">=</span>table</span>
<span id="cb48-7"><a href="#cb48-7" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb48-8"><a href="#cb48-8" aria-hidden="true" tabindex="-1"></a>plot_bbox(table, results[<span class="st">'&lt;CAPTION_TO_PHRASE_GROUNDING&gt;'</span>])</span>
<span id="cb48-9"><a href="#cb48-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-10"><a href="#cb48-10" aria-hidden="true" tabindex="-1"></a>results <span class="op">=</span> run_example(</span>
<span id="cb48-11"><a href="#cb48-11" aria-hidden="true" tabindex="-1"></a>    task_prompt,</span>
<span id="cb48-12"><a href="#cb48-12" aria-hidden="true" tabindex="-1"></a>    text_input<span class="op">=</span><span class="st">"a wine glass"</span>,</span>
<span id="cb48-13"><a href="#cb48-13" aria-hidden="true" tabindex="-1"></a>    image<span class="op">=</span>table</span>
<span id="cb48-14"><a href="#cb48-14" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb48-15"><a href="#cb48-15" aria-hidden="true" tabindex="-1"></a>plot_bbox(table, results[<span class="st">'&lt;CAPTION_TO_PHRASE_GROUNDING&gt;'</span>])</span>
<span id="cb48-16"><a href="#cb48-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-17"><a href="#cb48-17" aria-hidden="true" tabindex="-1"></a>results <span class="op">=</span> run_example(</span>
<span id="cb48-18"><a href="#cb48-18" aria-hidden="true" tabindex="-1"></a>    task_prompt,</span>
<span id="cb48-19"><a href="#cb48-19" aria-hidden="true" tabindex="-1"></a>    text_input<span class="op">=</span><span class="st">"a half orange"</span>,</span>
<span id="cb48-20"><a href="#cb48-20" aria-hidden="true" tabindex="-1"></a>    image<span class="op">=</span>table</span>
<span id="cb48-21"><a href="#cb48-21" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb48-22"><a href="#cb48-22" aria-hidden="true" tabindex="-1"></a>plot_bbox(table, results[<span class="st">'&lt;CAPTION_TO_PHRASE_GROUNDING&gt;'</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p> <img src="images/png/caption_ground.png" class="img-fluid"></p>
<div class="sourceCode" id="cb49"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb49-1"><a href="#cb49-1" aria-hidden="true" tabindex="-1"></a><span class="ex">[INFO]</span> ==<span class="op">&gt;</span> Florence-2-base <span class="er">(</span><span class="op">&lt;</span>CAPTION_TO_PHRASE_GROUNDING<span class="op">&gt;</span><span class="kw">)</span><span class="ex">,</span> <span class="dt">\</span></span>
<span id="cb49-2"><a href="#cb49-2" aria-hidden="true" tabindex="-1"></a>took 15.7 seconds to execute</span>
<span id="cb49-3"><a href="#cb49-3" aria-hidden="true" tabindex="-1"></a><span class="ex">each</span> task.</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="sec-visionlanguage-models-vlm-cascade-tasks-ea4f" class="level3">
<h3 class="anchored" data-anchor-id="sec-visionlanguage-models-vlm-cascade-tasks-ea4f">Cascade Tasks</h3>
<p>We can also enter the image caption as the input text to push Florence-2 to find more objects:</p>
<div class="sourceCode" id="cb50"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb50-1"><a href="#cb50-1" aria-hidden="true" tabindex="-1"></a>task_prompt <span class="op">=</span> <span class="st">'&lt;CAPTION&gt;'</span></span>
<span id="cb50-2"><a href="#cb50-2" aria-hidden="true" tabindex="-1"></a>results <span class="op">=</span> run_example(task_prompt,image<span class="op">=</span>dogs_cats)</span>
<span id="cb50-3"><a href="#cb50-3" aria-hidden="true" tabindex="-1"></a>text_input <span class="op">=</span> results[task_prompt]</span>
<span id="cb50-4"><a href="#cb50-4" aria-hidden="true" tabindex="-1"></a>task_prompt <span class="op">=</span> <span class="st">'&lt;CAPTION_TO_PHRASE_GROUNDING&gt;'</span></span>
<span id="cb50-5"><a href="#cb50-5" aria-hidden="true" tabindex="-1"></a>results <span class="op">=</span> run_example(task_prompt, text_input,image<span class="op">=</span>dogs_cats)</span>
<span id="cb50-6"><a href="#cb50-6" aria-hidden="true" tabindex="-1"></a>plot_bbox(dogs_cats, results[<span class="st">'&lt;CAPTION_TO_PHRASE_GROUNDING&gt;'</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Changing the task_prompt among <code>&lt;CAPTION,&gt;</code> <code>&lt;DETAILED_CAPTION&gt;</code> and <code>&lt;MORE_DETAILED_CAPTION&gt;</code>, we will get more objects in the image.</p>
<p> <img src="images/png/cascade.png" class="img-fluid"></p>
</section>
<section id="sec-visionlanguage-models-vlm-open_vocabulary_detection-9621" class="level3">
<h3 class="anchored" data-anchor-id="sec-visionlanguage-models-vlm-open_vocabulary_detection-9621">Open Vocabulary Detection</h3>
<p><code>&lt;OPEN_VOCABULARY_DETECTION&gt;</code> allows Florence-2 to detect recognizable objects in an image without relying on a predefined list of categories, making it a versatile tool for identifying various items that may not have been explicitly labeled during training. Unlike <code>&lt;CAPTION_TO_PHRASE_GROUNDING&gt;</code>, which requires a specific text phrase to locate and highlight a particular object in an image, <code>&lt;OPEN_VOCABULARY_DETECTION&gt;</code> performs a broad scan to find and classify all objects present.</p>
<p>This makes <code>&lt;OPEN_VOCABULARY_DETECTION&gt;</code> particularly useful for applications where you need a comprehensive overview of everything in an image without prior knowledge of what to expect. Enter with a text describing specific objects not previously detected, resulting in their detection. For example:</p>
<div class="sourceCode" id="cb51"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb51-1"><a href="#cb51-1" aria-hidden="true" tabindex="-1"></a>task_prompt <span class="op">=</span> <span class="st">'&lt;OPEN_VOCABULARY_DETECTION&gt;'</span></span>
<span id="cb51-2"><a href="#cb51-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-3"><a href="#cb51-3" aria-hidden="true" tabindex="-1"></a>text <span class="op">=</span> [</span>
<span id="cb51-4"><a href="#cb51-4" aria-hidden="true" tabindex="-1"></a>    <span class="st">"a house"</span>,</span>
<span id="cb51-5"><a href="#cb51-5" aria-hidden="true" tabindex="-1"></a>    <span class="st">"a tree"</span>,</span>
<span id="cb51-6"><a href="#cb51-6" aria-hidden="true" tabindex="-1"></a>    <span class="st">"a standing cat at the left"</span>,</span>
<span id="cb51-7"><a href="#cb51-7" aria-hidden="true" tabindex="-1"></a>    <span class="st">"a sleeping cat on the ground"</span>,</span>
<span id="cb51-8"><a href="#cb51-8" aria-hidden="true" tabindex="-1"></a>    <span class="st">"a standing cat at the right"</span>,</span>
<span id="cb51-9"><a href="#cb51-9" aria-hidden="true" tabindex="-1"></a>    <span class="st">"a yellow cat"</span></span>
<span id="cb51-10"><a href="#cb51-10" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb51-11"><a href="#cb51-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-12"><a href="#cb51-12" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> txt <span class="kw">in</span> text:</span>
<span id="cb51-13"><a href="#cb51-13" aria-hidden="true" tabindex="-1"></a>    results <span class="op">=</span> run_example(</span>
<span id="cb51-14"><a href="#cb51-14" aria-hidden="true" tabindex="-1"></a>        task_prompt,</span>
<span id="cb51-15"><a href="#cb51-15" aria-hidden="true" tabindex="-1"></a>        text_input<span class="op">=</span>txt,</span>
<span id="cb51-16"><a href="#cb51-16" aria-hidden="true" tabindex="-1"></a>        image<span class="op">=</span>dogs_cats</span>
<span id="cb51-17"><a href="#cb51-17" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb51-18"><a href="#cb51-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-19"><a href="#cb51-19" aria-hidden="true" tabindex="-1"></a>    bbox_results <span class="op">=</span> convert_to_od_format(</span>
<span id="cb51-20"><a href="#cb51-20" aria-hidden="true" tabindex="-1"></a>        results[<span class="st">'&lt;OPEN_VOCABULARY_DETECTION&gt;'</span>]</span>
<span id="cb51-21"><a href="#cb51-21" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb51-22"><a href="#cb51-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-23"><a href="#cb51-23" aria-hidden="true" tabindex="-1"></a>    plot_bbox(dogs_cats, bbox_results)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p> <img src="images/png/open_vacab_exemples.png" class="img-fluid quarto-figure quarto-figure-center" style="width:85.0%"></p>
<div class="sourceCode" id="cb52"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb52-1"><a href="#cb52-1" aria-hidden="true" tabindex="-1"></a><span class="ex">[INFO]</span> ==<span class="op">&gt;</span> Florence-2-base <span class="er">(</span><span class="op">&lt;</span>OPEN_VOCABULARY_DETECTION<span class="op">&gt;</span><span class="kw">)</span><span class="ex">,</span> <span class="dt">\</span></span>
<span id="cb52-2"><a href="#cb52-2" aria-hidden="true" tabindex="-1"></a>took 15.1 seconds to execute</span>
<span id="cb52-3"><a href="#cb52-3" aria-hidden="true" tabindex="-1"></a><span class="ex">each</span> task.</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<blockquote class="blockquote">
<p>Note: Trying to use Florence-2 to find objects that were not found can leads to mistakes (see exaamples on the Notebook).</p>
</blockquote>
</section>
<section id="sec-visionlanguage-models-vlm-referring-expression-segmentation-451b" class="level3">
<h3 class="anchored" data-anchor-id="sec-visionlanguage-models-vlm-referring-expression-segmentation-451b">Referring expression segmentation</h3>
<p>We can also segment a specific object in the image and give its description (caption), such as “a wine bottle” on the table image or “a German Sheppard” on the dogs_cats.</p>
<p>Referring expression segmentation results format: <code>{'&lt;REFERRING_EXPRESSION_SEGMENTATION&gt;': {'Polygons': [[[polygon]], ...], 'labels': ['', '', ...]}}</code>, one object is represented by a list of polygons. each polygon is <code>[x1, y1, x2, y2, ..., xn, yn]</code>.</p>
<blockquote class="blockquote">
<p><strong>Polygon (x1, y1, …, xn, yn)</strong>: Location tokens represent the vertices of a polygon in clockwise order.</p>
</blockquote>
<p>So, let’s first create a function to plot the segmentation:</p>
<div class="sourceCode" id="cb53"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb53-1"><a href="#cb53-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> PIL <span class="im">import</span> Image, ImageDraw, ImageFont</span>
<span id="cb53-2"><a href="#cb53-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> copy</span>
<span id="cb53-3"><a href="#cb53-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> random</span>
<span id="cb53-4"><a href="#cb53-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb53-5"><a href="#cb53-5" aria-hidden="true" tabindex="-1"></a>colormap <span class="op">=</span> [</span>
<span id="cb53-6"><a href="#cb53-6" aria-hidden="true" tabindex="-1"></a>    <span class="st">'blue'</span>, <span class="st">'orange'</span>, <span class="st">'green'</span>, <span class="st">'purple'</span>, <span class="st">'brown'</span>, <span class="st">'pink'</span>, <span class="st">'gray'</span>,</span>
<span id="cb53-7"><a href="#cb53-7" aria-hidden="true" tabindex="-1"></a>    <span class="st">'olive'</span>, <span class="st">'cyan'</span>, <span class="st">'red'</span>, <span class="st">'lime'</span>, <span class="st">'indigo'</span>, <span class="st">'violet'</span>, <span class="st">'aqua'</span>,</span>
<span id="cb53-8"><a href="#cb53-8" aria-hidden="true" tabindex="-1"></a>    <span class="st">'magenta'</span>, <span class="st">'coral'</span>, <span class="st">'gold'</span>, <span class="st">'tan'</span>, <span class="st">'skyblue'</span></span>
<span id="cb53-9"><a href="#cb53-9" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb53-10"><a href="#cb53-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-11"><a href="#cb53-11" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> draw_polygons(image, prediction, fill_mask<span class="op">=</span><span class="va">False</span>):</span>
<span id="cb53-12"><a href="#cb53-12" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb53-13"><a href="#cb53-13" aria-hidden="true" tabindex="-1"></a><span class="co">    Draws segmentation masks with polygons on an image.</span></span>
<span id="cb53-14"><a href="#cb53-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-15"><a href="#cb53-15" aria-hidden="true" tabindex="-1"></a><span class="co">    Parameters:</span></span>
<span id="cb53-16"><a href="#cb53-16" aria-hidden="true" tabindex="-1"></a><span class="co">      - image_path: Path to the image file.</span></span>
<span id="cb53-17"><a href="#cb53-17" aria-hidden="true" tabindex="-1"></a><span class="co">      - prediction: Dictionary containing 'polygons' and 'labels'</span></span>
<span id="cb53-18"><a href="#cb53-18" aria-hidden="true" tabindex="-1"></a><span class="co">              keys. 'polygons' is a list of lists, each</span></span>
<span id="cb53-19"><a href="#cb53-19" aria-hidden="true" tabindex="-1"></a><span class="co">              containing vertices of a polygon. 'labels' is</span></span>
<span id="cb53-20"><a href="#cb53-20" aria-hidden="true" tabindex="-1"></a><span class="co">              a list of labels corresponding to each polygon.</span></span>
<span id="cb53-21"><a href="#cb53-21" aria-hidden="true" tabindex="-1"></a><span class="co">- fill_mask: Boolean indicating whether to fill the polygons</span></span>
<span id="cb53-22"><a href="#cb53-22" aria-hidden="true" tabindex="-1"></a><span class="co">             with color.</span></span>
<span id="cb53-23"><a href="#cb53-23" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb53-24"><a href="#cb53-24" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Load the image</span></span>
<span id="cb53-25"><a href="#cb53-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-26"><a href="#cb53-26" aria-hidden="true" tabindex="-1"></a>    draw <span class="op">=</span> ImageDraw.Draw(image)</span>
<span id="cb53-27"><a href="#cb53-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-28"><a href="#cb53-28" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Set up scale factor if needed (use 1 if not scaling)</span></span>
<span id="cb53-29"><a href="#cb53-29" aria-hidden="true" tabindex="-1"></a>    scale <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb53-30"><a href="#cb53-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-31"><a href="#cb53-31" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Iterate over polygons and labels</span></span>
<span id="cb53-32"><a href="#cb53-32" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> polygons, label <span class="kw">in</span> <span class="bu">zip</span>(</span>
<span id="cb53-33"><a href="#cb53-33" aria-hidden="true" tabindex="-1"></a>        prediction[<span class="st">'polygons'</span>],</span>
<span id="cb53-34"><a href="#cb53-34" aria-hidden="true" tabindex="-1"></a>        prediction[<span class="st">'labels'</span>]</span>
<span id="cb53-35"><a href="#cb53-35" aria-hidden="true" tabindex="-1"></a>    ):</span>
<span id="cb53-36"><a href="#cb53-36" aria-hidden="true" tabindex="-1"></a>        color <span class="op">=</span> random.choice(colormap)</span>
<span id="cb53-37"><a href="#cb53-37" aria-hidden="true" tabindex="-1"></a>        fill_color <span class="op">=</span> (</span>
<span id="cb53-38"><a href="#cb53-38" aria-hidden="true" tabindex="-1"></a>            random.choice(colormap)</span>
<span id="cb53-39"><a href="#cb53-39" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> fill_mask <span class="cf">else</span> <span class="va">None</span></span>
<span id="cb53-40"><a href="#cb53-40" aria-hidden="true" tabindex="-1"></a>         )</span>
<span id="cb53-41"><a href="#cb53-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-42"><a href="#cb53-42" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> _polygon <span class="kw">in</span> polygons:</span>
<span id="cb53-43"><a href="#cb53-43" aria-hidden="true" tabindex="-1"></a>            _polygon <span class="op">=</span> np.array(_polygon).reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">2</span>)</span>
<span id="cb53-44"><a href="#cb53-44" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> <span class="bu">len</span>(_polygon) <span class="op">&lt;</span> <span class="dv">3</span>:</span>
<span id="cb53-45"><a href="#cb53-45" aria-hidden="true" tabindex="-1"></a>                <span class="bu">print</span>(<span class="st">'Invalid polygon:'</span>, _polygon)</span>
<span id="cb53-46"><a href="#cb53-46" aria-hidden="true" tabindex="-1"></a>                <span class="cf">continue</span></span>
<span id="cb53-47"><a href="#cb53-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-48"><a href="#cb53-48" aria-hidden="true" tabindex="-1"></a>            _polygon <span class="op">=</span> (_polygon <span class="op">*</span> scale).reshape(<span class="op">-</span><span class="dv">1</span>).tolist()</span>
<span id="cb53-49"><a href="#cb53-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-50"><a href="#cb53-50" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Draw the polygon</span></span>
<span id="cb53-51"><a href="#cb53-51" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> fill_mask:</span>
<span id="cb53-52"><a href="#cb53-52" aria-hidden="true" tabindex="-1"></a>               draw.polygon(</span>
<span id="cb53-53"><a href="#cb53-53" aria-hidden="true" tabindex="-1"></a>                   _polygon,</span>
<span id="cb53-54"><a href="#cb53-54" aria-hidden="true" tabindex="-1"></a>                  outline<span class="op">=</span>color,</span>
<span id="cb53-55"><a href="#cb53-55" aria-hidden="true" tabindex="-1"></a>                  fill<span class="op">=</span>fill_color</span>
<span id="cb53-56"><a href="#cb53-56" aria-hidden="true" tabindex="-1"></a>               )</span>
<span id="cb53-57"><a href="#cb53-57" aria-hidden="true" tabindex="-1"></a>            <span class="cf">else</span>:</span>
<span id="cb53-58"><a href="#cb53-58" aria-hidden="true" tabindex="-1"></a>               draw.polygon(</span>
<span id="cb53-59"><a href="#cb53-59" aria-hidden="true" tabindex="-1"></a>               _polygon,</span>
<span id="cb53-60"><a href="#cb53-60" aria-hidden="true" tabindex="-1"></a>               outline<span class="op">=</span>color</span>
<span id="cb53-61"><a href="#cb53-61" aria-hidden="true" tabindex="-1"></a>               )</span>
<span id="cb53-62"><a href="#cb53-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-63"><a href="#cb53-63" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Draw the label text</span></span>
<span id="cb53-64"><a href="#cb53-64" aria-hidden="true" tabindex="-1"></a>            draw.text(</span>
<span id="cb53-65"><a href="#cb53-65" aria-hidden="true" tabindex="-1"></a>               (_polygon[<span class="dv">0</span>] <span class="op">+</span> <span class="dv">8</span>, _polygon[<span class="dv">1</span>] <span class="op">+</span> <span class="dv">2</span>),</span>
<span id="cb53-66"><a href="#cb53-66" aria-hidden="true" tabindex="-1"></a>               label,</span>
<span id="cb53-67"><a href="#cb53-67" aria-hidden="true" tabindex="-1"></a>               fill<span class="op">=</span>color</span>
<span id="cb53-68"><a href="#cb53-68" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb53-69"><a href="#cb53-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-70"><a href="#cb53-70" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Save or display the image</span></span>
<span id="cb53-71"><a href="#cb53-71" aria-hidden="true" tabindex="-1"></a>    <span class="co">#image.show()  # Display the image</span></span>
<span id="cb53-72"><a href="#cb53-72" aria-hidden="true" tabindex="-1"></a>    display(image)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Now we can run the functions:</p>
<div class="sourceCode" id="cb54"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb54-1"><a href="#cb54-1" aria-hidden="true" tabindex="-1"></a>task_prompt <span class="op">=</span> <span class="st">'&lt;REFERRING_EXPRESSION_SEGMENTATION&gt;'</span></span>
<span id="cb54-2"><a href="#cb54-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-3"><a href="#cb54-3" aria-hidden="true" tabindex="-1"></a>results <span class="op">=</span> run_example(</span>
<span id="cb54-4"><a href="#cb54-4" aria-hidden="true" tabindex="-1"></a>    task_prompt,</span>
<span id="cb54-5"><a href="#cb54-5" aria-hidden="true" tabindex="-1"></a>    text_input<span class="op">=</span><span class="st">"a wine bottle"</span>,</span>
<span id="cb54-6"><a href="#cb54-6" aria-hidden="true" tabindex="-1"></a>    image<span class="op">=</span>table</span>
<span id="cb54-7"><a href="#cb54-7" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb54-8"><a href="#cb54-8" aria-hidden="true" tabindex="-1"></a>output_image <span class="op">=</span> copy.deepcopy(table)</span>
<span id="cb54-9"><a href="#cb54-9" aria-hidden="true" tabindex="-1"></a>draw_polygons(output_image,</span>
<span id="cb54-10"><a href="#cb54-10" aria-hidden="true" tabindex="-1"></a>              results[<span class="st">'&lt;REFERRING_EXPRESSION_SEGMENTATION&gt;'</span>],</span>
<span id="cb54-11"><a href="#cb54-11" aria-hidden="true" tabindex="-1"></a>              fill_mask<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb54-12"><a href="#cb54-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-13"><a href="#cb54-13" aria-hidden="true" tabindex="-1"></a>results <span class="op">=</span> run_example(</span>
<span id="cb54-14"><a href="#cb54-14" aria-hidden="true" tabindex="-1"></a>    task_prompt,</span>
<span id="cb54-15"><a href="#cb54-15" aria-hidden="true" tabindex="-1"></a>    text_input<span class="op">=</span><span class="st">"a german sheppard"</span>,</span>
<span id="cb54-16"><a href="#cb54-16" aria-hidden="true" tabindex="-1"></a>    image<span class="op">=</span>dogs_cats</span>
<span id="cb54-17"><a href="#cb54-17" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb54-18"><a href="#cb54-18" aria-hidden="true" tabindex="-1"></a>output_image <span class="op">=</span> copy.deepcopy(dogs_cats)</span>
<span id="cb54-19"><a href="#cb54-19" aria-hidden="true" tabindex="-1"></a>draw_polygons(output_image,</span>
<span id="cb54-20"><a href="#cb54-20" aria-hidden="true" tabindex="-1"></a>              results[<span class="st">'&lt;REFERRING_EXPRESSION_SEGMENTATION&gt;'</span>],</span>
<span id="cb54-21"><a href="#cb54-21" aria-hidden="true" tabindex="-1"></a>              fill_mask<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p> <img src="images/png/dog_bottle_seg.png" class="img-fluid quarto-figure quarto-figure-center" style="width:75.0%"></p>
<div class="sourceCode" id="cb55"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb55-1"><a href="#cb55-1" aria-hidden="true" tabindex="-1"></a><span class="ex">[INFO]</span> ==<span class="op">&gt;</span> Florence-2-base</span>
<span id="cb55-2"><a href="#cb55-2" aria-hidden="true" tabindex="-1"></a><span class="kw">(</span><span class="op">&lt;</span>REFERRING_EXPRESSION_SEGMENTATION<span class="op">&gt;</span><span class="kw">)</span><span class="ex">,</span> took 207.0 seconds</span>
<span id="cb55-3"><a href="#cb55-3" aria-hidden="true" tabindex="-1"></a><span class="ex">to</span> execute each task.</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="sec-visionlanguage-models-vlm-region-segmentation-a34f" class="level3">
<h3 class="anchored" data-anchor-id="sec-visionlanguage-models-vlm-region-segmentation-a34f">Region to Segmentation</h3>
<p>With this task, it is also possible to give the object coordinates in the image to segment it. The input format is <code>'&lt;loc_x1&gt;&lt;loc_y1&gt;&lt;loc_x2&gt;&lt;loc_y2&gt;', [x1, y1, x2, y2]</code> , which is the quantized coordinates in [0, 999].</p>
<p>For example, when running the code:</p>
<div class="sourceCode" id="cb56"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb56-1"><a href="#cb56-1" aria-hidden="true" tabindex="-1"></a>task_prompt <span class="op">=</span> <span class="st">'&lt;CAPTION_TO_PHRASE_GROUNDING&gt;'</span></span>
<span id="cb56-2"><a href="#cb56-2" aria-hidden="true" tabindex="-1"></a>results <span class="op">=</span> run_example(</span>
<span id="cb56-3"><a href="#cb56-3" aria-hidden="true" tabindex="-1"></a>    task_prompt,</span>
<span id="cb56-4"><a href="#cb56-4" aria-hidden="true" tabindex="-1"></a>    text_input<span class="op">=</span><span class="st">"a half orange"</span>,</span>
<span id="cb56-5"><a href="#cb56-5" aria-hidden="true" tabindex="-1"></a>    image<span class="op">=</span>table</span>
<span id="cb56-6"><a href="#cb56-6" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb56-7"><a href="#cb56-7" aria-hidden="true" tabindex="-1"></a>results</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>The results were:</p>
<pre><code>{'&lt;CAPTION_TO_PHRASE_GROUNDING&gt;': {'bboxes': [[343.552001953125,
    689.6640625,
    530.9440307617188,
    873.9840698242188]],
  'labels': ['a half']}}</code></pre>
<p>Using the bboxes rounded coordinates:</p>
<div class="sourceCode" id="cb58"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb58-1"><a href="#cb58-1" aria-hidden="true" tabindex="-1"></a>task_prompt <span class="op">=</span> <span class="st">'&lt;REGION_TO_SEGMENTATION&gt;'</span></span>
<span id="cb58-2"><a href="#cb58-2" aria-hidden="true" tabindex="-1"></a>results <span class="op">=</span> run_example(</span>
<span id="cb58-3"><a href="#cb58-3" aria-hidden="true" tabindex="-1"></a>    task_prompt,</span>
<span id="cb58-4"><a href="#cb58-4" aria-hidden="true" tabindex="-1"></a>    text_input<span class="op">=</span>(</span>
<span id="cb58-5"><a href="#cb58-5" aria-hidden="true" tabindex="-1"></a>        <span class="st">"&lt;loc_343&gt;&lt;loc_690&gt;"</span></span>
<span id="cb58-6"><a href="#cb58-6" aria-hidden="true" tabindex="-1"></a>        <span class="st">"&lt;loc_531&gt;&lt;loc_874&gt;"</span></span>
<span id="cb58-7"><a href="#cb58-7" aria-hidden="true" tabindex="-1"></a>    ),</span>
<span id="cb58-8"><a href="#cb58-8" aria-hidden="true" tabindex="-1"></a>    image<span class="op">=</span>table</span>
<span id="cb58-9"><a href="#cb58-9" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb58-10"><a href="#cb58-10" aria-hidden="true" tabindex="-1"></a>output_image <span class="op">=</span> copy.deepcopy(table)</span>
<span id="cb58-11"><a href="#cb58-11" aria-hidden="true" tabindex="-1"></a>draw_polygons(</span>
<span id="cb58-12"><a href="#cb58-12" aria-hidden="true" tabindex="-1"></a>    output_image,</span>
<span id="cb58-13"><a href="#cb58-13" aria-hidden="true" tabindex="-1"></a>    results[<span class="st">'&lt;REGION_TO_SEGMENTATION&gt;'</span>],</span>
<span id="cb58-14"><a href="#cb58-14" aria-hidden="true" tabindex="-1"></a>    fill_mask<span class="op">=</span><span class="va">True</span></span>
<span id="cb58-15"><a href="#cb58-15" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>We got the segmentation of the object on those coordinates (Latency: 83 seconds):</p>
<p> <img src="images/png/orange_seg.png" class="img-fluid quarto-figure quarto-figure-center" style="width:65.0%"></p>
</section>
<section id="sec-visionlanguage-models-vlm-region-texts-57dc" class="level3">
<h3 class="anchored" data-anchor-id="sec-visionlanguage-models-vlm-region-texts-57dc">Region to Texts</h3>
<p>We can also give the region (coordinates and ask for a caption):</p>
<div class="sourceCode" id="cb59"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb59-1"><a href="#cb59-1" aria-hidden="true" tabindex="-1"></a>task_prompt <span class="op">=</span> <span class="st">'&lt;REGION_TO_CATEGORY&gt;'</span></span>
<span id="cb59-2"><a href="#cb59-2" aria-hidden="true" tabindex="-1"></a>results <span class="op">=</span> run_example(</span>
<span id="cb59-3"><a href="#cb59-3" aria-hidden="true" tabindex="-1"></a>    task_prompt,</span>
<span id="cb59-4"><a href="#cb59-4" aria-hidden="true" tabindex="-1"></a>    text_input<span class="op">=</span>(</span>
<span id="cb59-5"><a href="#cb59-5" aria-hidden="true" tabindex="-1"></a>        <span class="st">"&lt;loc_343&gt;&lt;loc_690&gt;"</span></span>
<span id="cb59-6"><a href="#cb59-6" aria-hidden="true" tabindex="-1"></a>        <span class="st">"&lt;loc_531&gt;&lt;loc_874&gt;"</span></span>
<span id="cb59-7"><a href="#cb59-7" aria-hidden="true" tabindex="-1"></a>    ),</span>
<span id="cb59-8"><a href="#cb59-8" aria-hidden="true" tabindex="-1"></a>    image<span class="op">=</span>table</span>
<span id="cb59-9"><a href="#cb59-9" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb59-10"><a href="#cb59-10" aria-hidden="true" tabindex="-1"></a>results</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb60"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb60-1"><a href="#cb60-1" aria-hidden="true" tabindex="-1"></a>[INFO] <span class="op">==&gt;</span> Florence<span class="op">-</span><span class="dv">2</span><span class="op">-</span>base (<span class="op">&lt;</span>REGION_TO_CATEGORY<span class="op">&gt;</span>), <span class="op">\</span></span>
<span id="cb60-2"><a href="#cb60-2" aria-hidden="true" tabindex="-1"></a>took <span class="fl">14.3</span> seconds to execute.</span>
<span id="cb60-3"><a href="#cb60-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-4"><a href="#cb60-4" aria-hidden="true" tabindex="-1"></a>{{</span>
<span id="cb60-5"><a href="#cb60-5" aria-hidden="true" tabindex="-1"></a>  <span class="st">'&lt;REGION_TO_CATEGORY&gt;'</span>:</span>
<span id="cb60-6"><a href="#cb60-6" aria-hidden="true" tabindex="-1"></a>    <span class="st">'orange&lt;loc_343&gt;&lt;loc_690&gt;'</span></span>
<span id="cb60-7"><a href="#cb60-7" aria-hidden="true" tabindex="-1"></a>    <span class="st">'&lt;loc_531&gt;&lt;loc_874&gt;'</span></span>
<span id="cb60-8"><a href="#cb60-8" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>The model identified an orange in that region. Let’s ask for a description:</p>
<div class="sourceCode" id="cb61"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb61-1"><a href="#cb61-1" aria-hidden="true" tabindex="-1"></a>task_prompt <span class="op">=</span> <span class="st">'&lt;REGION_TO_DESCRIPTION&gt;'</span></span>
<span id="cb61-2"><a href="#cb61-2" aria-hidden="true" tabindex="-1"></a>results <span class="op">=</span> run_example(</span>
<span id="cb61-3"><a href="#cb61-3" aria-hidden="true" tabindex="-1"></a>    task_prompt,</span>
<span id="cb61-4"><a href="#cb61-4" aria-hidden="true" tabindex="-1"></a>    text_input<span class="op">=</span>(</span>
<span id="cb61-5"><a href="#cb61-5" aria-hidden="true" tabindex="-1"></a>        <span class="st">"&lt;loc_343&gt;&lt;loc_690&gt;"</span></span>
<span id="cb61-6"><a href="#cb61-6" aria-hidden="true" tabindex="-1"></a>        <span class="st">"&lt;loc_531&gt;&lt;loc_874&gt;"</span></span>
<span id="cb61-7"><a href="#cb61-7" aria-hidden="true" tabindex="-1"></a>    ),</span>
<span id="cb61-8"><a href="#cb61-8" aria-hidden="true" tabindex="-1"></a>    image<span class="op">=</span>table</span>
<span id="cb61-9"><a href="#cb61-9" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb61-10"><a href="#cb61-10" aria-hidden="true" tabindex="-1"></a>results</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb62"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb62-1"><a href="#cb62-1" aria-hidden="true" tabindex="-1"></a>[INFO] <span class="op">==&gt;</span> Florence<span class="op">-</span><span class="dv">2</span><span class="op">-</span>base (<span class="op">&lt;</span>REGION_TO_CATEGORY<span class="op">&gt;</span>), <span class="op">\</span></span>
<span id="cb62-2"><a href="#cb62-2" aria-hidden="true" tabindex="-1"></a>took <span class="fl">14.6</span> seconds to execute.</span>
<span id="cb62-3"><a href="#cb62-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb62-4"><a href="#cb62-4" aria-hidden="true" tabindex="-1"></a>{</span>
<span id="cb62-5"><a href="#cb62-5" aria-hidden="true" tabindex="-1"></a>  <span class="st">'&lt;REGION_TO_CATEGORY&gt;'</span>:</span>
<span id="cb62-6"><a href="#cb62-6" aria-hidden="true" tabindex="-1"></a>    <span class="st">'orange&lt;loc_343&gt;&lt;loc_690&gt;'</span></span>
<span id="cb62-7"><a href="#cb62-7" aria-hidden="true" tabindex="-1"></a>    <span class="st">'&lt;loc_531&gt;&lt;loc_874&gt;'</span></span>
<span id="cb62-8"><a href="#cb62-8" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>In this case, the description did not provide more details, but it could. Try another example.</p>
</section>
<section id="sec-visionlanguage-models-vlm-ocr-5f56" class="level3">
<h3 class="anchored" data-anchor-id="sec-visionlanguage-models-vlm-ocr-5f56">OCR</h3>
<p>With Florence-2, we can perform Optical Character Recognition (OCR) on an image, getting what is written on it (<code>task_prompt = '&lt;OCR&gt;'</code> and also get the bounding boxes (location) for the detected text (<code>ask_prompt = '&lt;OCR_WITH_REGION&gt;'</code>). Those tasks can help extract and locate textual information in images, such as reading signs, labels, or other forms of text in images.</p>
<p>Let’s upload a flyer from a talk in Brazil to Raspi. Let’s test works in another language, here Portuguese):</p>
<div class="sourceCode" id="cb63"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb63-1"><a href="#cb63-1" aria-hidden="true" tabindex="-1"></a>flayer <span class="op">=</span> Image.<span class="bu">open</span>(<span class="st">'./images/embarcados.jpg'</span>)</span>
<span id="cb63-2"><a href="#cb63-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Display the image</span></span>
<span id="cb63-3"><a href="#cb63-3" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">8</span>))</span>
<span id="cb63-4"><a href="#cb63-4" aria-hidden="true" tabindex="-1"></a>plt.imshow(flayer)</span>
<span id="cb63-5"><a href="#cb63-5" aria-hidden="true" tabindex="-1"></a>plt.axis(<span class="st">'off'</span>)</span>
<span id="cb63-6"><a href="#cb63-6" aria-hidden="true" tabindex="-1"></a><span class="co">#plt.title("Image")</span></span>
<span id="cb63-7"><a href="#cb63-7" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p> <img src="images/png/flyer.png" class="img-fluid quarto-figure quarto-figure-center" style="width:80.0%"></p>
<p>Let’s examine the image with <code>'&lt;MORE_DETAILED_CAPTION&gt;'</code> :</p>
<div class="sourceCode" id="cb64"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb64-1"><a href="#cb64-1" aria-hidden="true" tabindex="-1"></a><span class="ex">[INFO]</span> ==<span class="op">&gt;</span> Florence-2-base <span class="er">(</span><span class="op">&lt;</span>MORE_DETAILED_CAPTION<span class="op">&gt;</span><span class="kw">)</span><span class="ex">,</span> <span class="dt">\</span></span>
<span id="cb64-2"><a href="#cb64-2" aria-hidden="true" tabindex="-1"></a>took 85.2 seconds to execute.</span>
<span id="cb64-3"><a href="#cb64-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-4"><a href="#cb64-4" aria-hidden="true" tabindex="-1"></a><span class="ex">{</span><span class="st">'&lt;MORE_DETAILED_CAPTION&gt;'</span><span class="ex">:</span> <span class="st">'The image is a promotional poster \</span></span>
<span id="cb64-5"><a href="#cb64-5" aria-hidden="true" tabindex="-1"></a><span class="st">for an event called "Machine Learning Embarcados" hosted by \</span></span>
<span id="cb64-6"><a href="#cb64-6" aria-hidden="true" tabindex="-1"></a><span class="st">Marcelo Roval. The poster has a black background with white \</span></span>
<span id="cb64-7"><a href="#cb64-7" aria-hidden="true" tabindex="-1"></a><span class="st">text. On the left side of the poster, there is a logo of a \</span></span>
<span id="cb64-8"><a href="#cb64-8" aria-hidden="true" tabindex="-1"></a><span class="st">coffee cup with the text "Café Com Embarcados" above it. \</span></span>
<span id="cb64-9"><a href="#cb64-9" aria-hidden="true" tabindex="-1"></a><span class="st">Below the logo, it says "25 de Setembro as 17th" which \</span></span>
<span id="cb64-10"><a href="#cb64-10" aria-hidden="true" tabindex="-1"></a><span class="st">translates to "25th of September as 17" in English. \n\nOn \</span></span>
<span id="cb64-11"><a href="#cb64-11" aria-hidden="true" tabindex="-1"></a><span class="st">the right side, there aretwo smaller text boxes with the names \</span></span>
<span id="cb64-12"><a href="#cb64-12" aria-hidden="true" tabindex="-1"></a><span class="st">of the participants and their names. The first text box reads \</span></span>
<span id="cb64-13"><a href="#cb64-13" aria-hidden="true" tabindex="-1"></a><span class="st">"Democratizando a Inteligência Artificial para Paises em \</span></span>
<span id="cb64-14"><a href="#cb64-14" aria-hidden="true" tabindex="-1"></a><span class="st">Desenvolvimento" and the second text box says "Toda \</span></span>
<span id="cb64-15"><a href="#cb64-15" aria-hidden="true" tabindex="-1"></a><span class="st">quarta-feira" which is Portuguese for "Transmissão via in \</span></span>
<span id="cb64-16"><a href="#cb64-16" aria-hidden="true" tabindex="-1"></a><span class="st">Portuguese".\n\nIn the center of the image, there has a photo \</span></span>
<span id="cb64-17"><a href="#cb64-17" aria-hidden="true" tabindex="-1"></a><span class="st">of Marcelo, a man with a beard and glasses, smiling at the \</span></span>
<span id="cb64-18"><a href="#cb64-18" aria-hidden="true" tabindex="-1"></a><span class="st">camera. He is wearing a white hard hat and a white shirt. \</span></span>
<span id="cb64-19"><a href="#cb64-19" aria-hidden="true" tabindex="-1"></a><span class="st">The text boxes are in orange and yellow colors.'</span>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>The description is very accurate. Let’s get to the more important words with the task OCR:</p>
<div class="sourceCode" id="cb65"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb65-1"><a href="#cb65-1" aria-hidden="true" tabindex="-1"></a>task_prompt <span class="op">=</span> <span class="st">'&lt;OCR&gt;'</span></span>
<span id="cb65-2"><a href="#cb65-2" aria-hidden="true" tabindex="-1"></a>run_example(task_prompt,image<span class="op">=</span>flayer)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb66"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb66-1"><a href="#cb66-1" aria-hidden="true" tabindex="-1"></a><span class="ex">[INFO]</span> ==<span class="op">&gt;</span> Florence-2-base <span class="er">(</span><span class="op">&lt;</span>OCR<span class="op">&gt;</span><span class="kw">)</span><span class="ex">,</span> took 37.7 seconds to execute.</span>
<span id="cb66-2"><a href="#cb66-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-3"><a href="#cb66-3" aria-hidden="true" tabindex="-1"></a><span class="ex">{</span><span class="st">'&lt;OCR&gt;'</span><span class="ex">:</span></span>
<span id="cb66-4"><a href="#cb66-4" aria-hidden="true" tabindex="-1"></a> <span class="st">'Machine Learning Café com Embarcado Embarcados '</span></span>
<span id="cb66-5"><a href="#cb66-5" aria-hidden="true" tabindex="-1"></a> <span class="st">'Democratizando a Inteligência Artificial para Paises em '</span></span>
<span id="cb66-6"><a href="#cb66-6" aria-hidden="true" tabindex="-1"></a> <span class="st">'25 de Setembro às 17h Desenvolvimento Toda quarta-feira '</span></span>
<span id="cb66-7"><a href="#cb66-7" aria-hidden="true" tabindex="-1"></a> <span class="st">'Marcelo Roval Professor na UNIFIEI e Transmissão via in '</span></span>
<span id="cb66-8"><a href="#cb66-8" aria-hidden="true" tabindex="-1"></a> <span class="st">'Co-Director do TinyML4D'</span><span class="ex">}</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Let’s locate the words in the flyer:</p>
<div class="sourceCode" id="cb67"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb67-1"><a href="#cb67-1" aria-hidden="true" tabindex="-1"></a>task_prompt <span class="op">=</span> <span class="st">'&lt;OCR_WITH_REGION&gt;'</span></span>
<span id="cb67-2"><a href="#cb67-2" aria-hidden="true" tabindex="-1"></a>results <span class="op">=</span> run_example(task_prompt,image<span class="op">=</span>flayer)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Let’s also create a function to draw bounding boxes around the detected words:</p>
<div class="sourceCode" id="cb68"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb68-1"><a href="#cb68-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> draw_ocr_bboxes(image, prediction):</span>
<span id="cb68-2"><a href="#cb68-2" aria-hidden="true" tabindex="-1"></a>    scale <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb68-3"><a href="#cb68-3" aria-hidden="true" tabindex="-1"></a>    draw <span class="op">=</span> ImageDraw.Draw(image)</span>
<span id="cb68-4"><a href="#cb68-4" aria-hidden="true" tabindex="-1"></a>    bboxes <span class="op">=</span> prediction[<span class="st">'quad_boxes'</span>]</span>
<span id="cb68-5"><a href="#cb68-5" aria-hidden="true" tabindex="-1"></a>    labels <span class="op">=</span> prediction[<span class="st">'labels'</span>]</span>
<span id="cb68-6"><a href="#cb68-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> box, label <span class="kw">in</span> <span class="bu">zip</span>(bboxes, labels):</span>
<span id="cb68-7"><a href="#cb68-7" aria-hidden="true" tabindex="-1"></a>        color <span class="op">=</span> random.choice(colormap)</span>
<span id="cb68-8"><a href="#cb68-8" aria-hidden="true" tabindex="-1"></a>        new_box <span class="op">=</span> (np.array(box) <span class="op">*</span> scale).tolist()</span>
<span id="cb68-9"><a href="#cb68-9" aria-hidden="true" tabindex="-1"></a>        draw.polygon(new_box, width<span class="op">=</span><span class="dv">3</span>, outline<span class="op">=</span>color)</span>
<span id="cb68-10"><a href="#cb68-10" aria-hidden="true" tabindex="-1"></a>        draw.text((new_box[<span class="dv">0</span>]<span class="op">+</span><span class="dv">8</span>, new_box[<span class="dv">1</span>]<span class="op">+</span><span class="dv">2</span>),</span>
<span id="cb68-11"><a href="#cb68-11" aria-hidden="true" tabindex="-1"></a>                    <span class="st">"</span><span class="sc">{}</span><span class="st">"</span>.<span class="bu">format</span>(label),</span>
<span id="cb68-12"><a href="#cb68-12" aria-hidden="true" tabindex="-1"></a>                    align<span class="op">=</span><span class="st">"right"</span>,</span>
<span id="cb68-13"><a href="#cb68-13" aria-hidden="true" tabindex="-1"></a>                    fill<span class="op">=</span>color)</span>
<span id="cb68-14"><a href="#cb68-14" aria-hidden="true" tabindex="-1"></a>    display(image)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb69"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb69-1"><a href="#cb69-1" aria-hidden="true" tabindex="-1"></a>output_image <span class="op">=</span> copy.deepcopy(flayer)</span>
<span id="cb69-2"><a href="#cb69-2" aria-hidden="true" tabindex="-1"></a>draw_ocr_bboxes(output_image, results[<span class="st">'&lt;OCR_WITH_REGION&gt;'</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p> <img src="images/png/output_ocr.png" class="img-fluid quarto-figure quarto-figure-center" style="width:80.0%"></p>
<p>We can inspect the detected words:</p>
<div class="sourceCode" id="cb70"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb70-1"><a href="#cb70-1" aria-hidden="true" tabindex="-1"></a>results[<span class="st">'&lt;OCR_WITH_REGION&gt;'</span>][<span class="st">'labels'</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb71"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb71-1"><a href="#cb71-1" aria-hidden="true" tabindex="-1"></a><span class="st">'&lt;/s&gt;Machine Learning'</span><span class="ex">,</span></span>
<span id="cb71-2"><a href="#cb71-2" aria-hidden="true" tabindex="-1"></a> <span class="st">'Café'</span><span class="ex">,</span></span>
<span id="cb71-3"><a href="#cb71-3" aria-hidden="true" tabindex="-1"></a> <span class="st">'com'</span><span class="ex">,</span></span>
<span id="cb71-4"><a href="#cb71-4" aria-hidden="true" tabindex="-1"></a> <span class="st">'Embarcado'</span><span class="ex">,</span></span>
<span id="cb71-5"><a href="#cb71-5" aria-hidden="true" tabindex="-1"></a> <span class="st">'Embarcados'</span><span class="ex">,</span></span>
<span id="cb71-6"><a href="#cb71-6" aria-hidden="true" tabindex="-1"></a> <span class="st">'Democratizando a Inteligência'</span><span class="ex">,</span></span>
<span id="cb71-7"><a href="#cb71-7" aria-hidden="true" tabindex="-1"></a> <span class="st">'Artificial para Paises em'</span><span class="ex">,</span></span>
<span id="cb71-8"><a href="#cb71-8" aria-hidden="true" tabindex="-1"></a> <span class="st">'25 de Setembro ás 17h'</span><span class="ex">,</span></span>
<span id="cb71-9"><a href="#cb71-9" aria-hidden="true" tabindex="-1"></a> <span class="st">'Desenvolvimento'</span><span class="ex">,</span></span>
<span id="cb71-10"><a href="#cb71-10" aria-hidden="true" tabindex="-1"></a> <span class="st">'Toda quarta-feira'</span><span class="ex">,</span></span>
<span id="cb71-11"><a href="#cb71-11" aria-hidden="true" tabindex="-1"></a> <span class="st">'Marcelo Roval'</span><span class="ex">,</span></span>
<span id="cb71-12"><a href="#cb71-12" aria-hidden="true" tabindex="-1"></a> <span class="st">'Professor na UNIFIEI e'</span><span class="ex">,</span></span>
<span id="cb71-13"><a href="#cb71-13" aria-hidden="true" tabindex="-1"></a> <span class="st">'Transmissão via'</span><span class="ex">,</span></span>
<span id="cb71-14"><a href="#cb71-14" aria-hidden="true" tabindex="-1"></a> <span class="st">'in'</span><span class="ex">,</span></span>
<span id="cb71-15"><a href="#cb71-15" aria-hidden="true" tabindex="-1"></a> <span class="st">'Co-Director do TinyML4D'</span><span class="ex">]</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
</section>
<section id="sec-visionlanguage-models-vlm-latency-summary-aa9d" class="level2">
<h2 class="anchored" data-anchor-id="sec-visionlanguage-models-vlm-latency-summary-aa9d">Latency Summary</h2>
<p>The latency observed for different tasks using Florence-2 on the Raspberry Pi (Raspi-5) varied depending on the complexity of the task:</p>
<ul>
<li><strong>Image Captioning</strong>: It took approximately 16-17 seconds to generate a caption for an image.</li>
<li><strong>Detailed Captioning</strong>: Increased latency to around 25-27 seconds, requiring generating more nuanced scene descriptions.</li>
<li><strong>More Detailed Captioning</strong>: It took about 32-50 seconds, and the latency increased as the description grew more complex.</li>
<li><strong>Object Detection</strong>: It took approximately 20-41 seconds, depending on the image’s complexity and the number of detected objects.</li>
<li><strong>Visual Grounding</strong>: Approximately 15-16 seconds to localize specific objects based on textual prompts.</li>
<li><strong>OCR (Optical Character Recognition)</strong>: Extracting text from an image took around 37-38 seconds.</li>
<li><strong>Segmentation and Region to Segmentation</strong>: Segmentation tasks took considerably longer, with a latency of around 83-207 seconds, depending on the complexity and the number of regions to be segmented.</li>
</ul>
<p>These latency times highlight the resource constraints of edge devices like the Raspberry Pi and emphasize the need to optimize the model and the environment to achieve real-time performance.</p>
<p> <img src="images/png/htop.png" class="img-fluid"></p>
<blockquote class="blockquote">
<p>Running complex tasks can use all 8 GB of the Raspi-5’s memory. For example, the above screenshot during the Florence OD task shows 4 CPUs at full speed and over 5 GB of memory in use. Consider increasing the SWAP memory to 2 GB.</p>
</blockquote>
<p>Checking the CPU temperature with <code>vcgencmd measure_temp</code> , showed that temperature can go up to +80oC.</p>
</section>
<section id="sec-visionlanguage-models-vlm-finetunning-d35f" class="level2">
<h2 class="anchored" data-anchor-id="sec-visionlanguage-models-vlm-finetunning-d35f">Fine-Tunning</h2>
<p>As explored in this lab, Florence supports many tasks out of the box, including captioning, object detection, OCR, and more. However, like other pre-trained foundational models, Florence-2 may need domain-specific knowledge. For example, it may need to improve with medical or satellite imagery. In such cases, <strong>fine-tuning</strong> with a custom dataset is necessary. The Roboflow tutorial, <a href="https://blog.roboflow.com/fine-tune-florence-2-object-detection/">How to Fine-tune Florence-2 for Object Detection Tasks</a>, shows how to fine-tune Florence-2 on object detection datasets to improve model performance for our specific use case.</p>
<p>Based on the above tutorial, it is possible to fine-tune the Florence-2 model to detect boxes and wheels used in previous labs:</p>
<p> <img src="images/png/fine-tuning.png" class="img-fluid"></p>
<p>It is important to note that after fine-tuning, the model can still detect classes that don’t belong to our custom dataset, like cats, dogs, grapes, etc, as seen before).</p>
<p>The complete fine-tunning project using a previously annotated dataset in Roboflow and executed on CoLab can be found in the notebook:</p>
<ul>
<li><a href="https://github.com/Mjrovai/EdgeML-with-Raspberry-Pi/blob/main/FLORENCE-2/notebooks/30-Finetune_florence_2_on_detection_dataset_box_vs_wheel.ipynb">30-Finetune_florence_2_on_detection_dataset_box_vs_wheel.ipynb</a></li>
</ul>
<p>In another example, in the post, <a href="https://huggingface.co/blog/finetune-florence2">Fine-tuning Florence-2 - Microsoft’s Cutting-edge Vision Language Models</a>, the authors show an example of fine-tuning Florence on <code>DocVQA</code>. The authors report that Florence 2 can perform visual question answering (VQA), but the released models don’t include VQA capability.</p>
</section>
<section id="sec-visionlanguage-models-vlm-summary-25e4" class="level2">
<h2 class="anchored" data-anchor-id="sec-visionlanguage-models-vlm-summary-25e4">Summary</h2>
<p>Florence-2 offers a versatile and powerful approach to vision-language tasks at the edge, providing performance that rivals larger, task-specific models, such as YOLO for object detection, BERT/RoBERTa for text analysis, and specialized OCR models.</p>
<p>Thanks to its multi-modal transformer architecture, Florence-2 is more flexible than YOLO in terms of the tasks it can handle. These include object detection, image captioning, and visual grounding.</p>
<p>Unlike <strong>BERT</strong>, which focuses purely on language, Florence-2 integrates vision and language, allowing it to excel in applications that require both modalities, such as image captioning and visual grounding.</p>
<p>Moreover, while traditional <strong>OCR models</strong> such as Tesseract and EasyOCR are designed solely for recognizing and extracting text from images, Florence-2’s OCR capabilities are part of a broader framework that includes contextual understanding and visual-text alignment. This makes it particularly useful for scenarios that require both reading text and interpreting its context within images.</p>
<p>Overall, Florence-2 stands out for its ability to seamlessly integrate various vision-language tasks into a unified model that is efficient enough to run on edge devices like the Raspberry Pi. This makes it a compelling choice for developers and researchers exploring AI applications at the edge.</p>
<section id="sec-visionlanguage-models-vlm-key-advantages-florence2-5113" class="level3">
<h3 class="anchored" data-anchor-id="sec-visionlanguage-models-vlm-key-advantages-florence2-5113">Key Advantages of Florence-2</h3>
<ol type="1">
<li><strong>Unified Architecture</strong>
<ul>
<li>Single model handles multiple vision tasks vs.&nbsp;specialized models (YOLO, BERT, Tesseract)</li>
<li>Eliminates the need for multiple model deployments and integrations</li>
<li>Consistent API and interface across tasks</li>
</ul></li>
<li><strong>Performance Comparison</strong>
<ul>
<li>Object Detection: Comparable to YOLOv8 (~37.5 mAP on COCO vs.&nbsp;YOLOv8’s ~39.7 mAP) despite being general-purpose</li>
<li>Text Recognition: Handles multiple languages effectively like specialized OCR models (Tesseract, EasyOCR)</li>
<li>Language Understanding: Integrates BERT-like capabilities for text processing while adding visual context</li>
</ul></li>
<li><strong>Resource Efficiency</strong>
<ul>
<li>The Base model (232M parameters) achieves strong results despite smaller size</li>
<li>Runs effectively on edge devices (Raspberry Pi)</li>
<li>Single model deployment vs.&nbsp;multiple specialized models</li>
</ul></li>
</ol>
</section>
<section id="sec-visionlanguage-models-vlm-tradeoffs-dfeb" class="level3">
<h3 class="anchored" data-anchor-id="sec-visionlanguage-models-vlm-tradeoffs-dfeb">Trade-offs</h3>
<ol type="1">
<li><strong>Performance vs.&nbsp;Specialized Models</strong>
<ul>
<li>YOLO series may offer faster inference for pure object detection</li>
<li>Specialized OCR models might handle complex document layouts better</li>
<li>BERT/RoBERTa provide deeper language understanding for text-only tasks</li>
</ul></li>
<li><strong>Resource Requirements</strong>
<ul>
<li>Higher latency on edge devices (15-200s depending on task)</li>
<li>Requires careful memory management on Raspberry Pi</li>
<li>It may need optimization for real-time applications</li>
</ul></li>
<li><strong>Deployment Considerations</strong>
<ul>
<li>Initial setup is more complex than single-purpose models</li>
<li>Requires understanding of multiple task types and prompts</li>
<li>The learning curve for optimal prompt engineering</li>
</ul></li>
</ol>
</section>
<section id="sec-visionlanguage-models-vlm-best-use-cases-651b" class="level3">
<h3 class="anchored" data-anchor-id="sec-visionlanguage-models-vlm-best-use-cases-651b">Best Use Cases</h3>
<ol type="1">
<li><strong>Resource-Constrained Environments</strong>
<ul>
<li>Edge devices requiring multiple vision capabilities</li>
<li>Systems with limited storage/deployment capacity</li>
<li>Applications needing flexible vision processing</li>
</ul></li>
<li><strong>Multi-modal Applications</strong>
<ul>
<li>Content moderation systems</li>
<li>Accessibility tools</li>
<li>Document analysis workflows</li>
</ul></li>
<li><strong>Rapid Prototyping</strong>
<ul>
<li>Quick deployment of vision capabilities</li>
<li>Testing multiple vision tasks without separate models</li>
<li>Proof-of-concept development</li>
</ul></li>
</ol>
</section>
</section>
<section id="sec-visionlanguage-models-vlm-future-implications-4fad" class="level2">
<h2 class="anchored" data-anchor-id="sec-visionlanguage-models-vlm-future-implications-4fad">Future Implications</h2>
<p>Florence-2 represents a shift toward unified vision models that could eventually replace task-specific architectures in many applications. While specialized models maintain advantages in specific scenarios, the convenience and efficiency of unified models like Florence-2 make them increasingly attractive for real-world deployments.</p>
<p>The lab demonstrates Florence-2’s viability on edge devices, suggesting future IoT, mobile computing, and embedded systems applications where deploying multiple specialized models would be impractical.</p>
</section>
<section id="sec-visionlanguage-models-vlm-resources-99e6" class="level2">
<h2 class="anchored" data-anchor-id="sec-visionlanguage-models-vlm-resources-99e6">Resources</h2>
<ul>
<li><p><a href="https://github.com/Mjrovai/EdgeML-with-Raspberry-Pi/blob/main/FLORENCE-2/notebooks/10-florence2_test.ipynb">10-florence2_test.ipynb</a></p></li>
<li><p><a href="https://github.com/Mjrovai/EdgeML-with-Raspberry-Pi/blob/main/FLORENCE-2/notebooks/20-florence_2.ipynb">20-florence_2.ipynb</a></p></li>
<li><p><a href="https://github.com/Mjrovai/EdgeML-with-Raspberry-Pi/blob/main/FLORENCE-2/notebooks/30-Finetune_florence_2_on_detection_dataset_box_vs_wheel.ipynb">30-Finetune_florence_2_on_detection_dataset_box_vs_wheel.ipynb</a></p></li>
</ul>


</section>
</section>

<a onclick="window.scrollTo(0, 0); return false;" role="button" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> Back to top</a></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
            // target, if specified
            link.setAttribute("target", "_blank");
            if (link.getAttribute("rel") === null) {
              link.setAttribute("rel", "noopener");
            }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../../../../contents/labs/raspi/llm/llm.html" class="pagination-link" aria-label="Small Language Models (SLM)">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">Small Language Models (SLM)</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../../../../contents/labs/shared/shared.html" class="pagination-link" aria-label="Overview">
        <span class="nav-page-text">Overview</span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>© 2024 Harvard University. Licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC-BY-NC-SA 4.0</a></p>
</div>   
    <div class="nav-footer-center">
<p>Written, edited and curated by Prof.&nbsp;Vijay Janapa Reddi (Harvard University)</p>
</div>
    <div class="nav-footer-right">
      <ul class="footer-items list-unstyled">
    <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/harvard-edge/cs249r_book">
      <i class="bi bi-github" role="img" aria-label="View source on GitHub">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/harvard-edge/cs249r_book">
      <i class="bi bi-star" role="img" aria-label="Star this repository">
</i> 
    </a>
  </li>  
</ul>
    </div>
  </div>
</footer>
<script>var lightboxQuarto = GLightbox({"closeEffect":"zoom","descPosition":"bottom","loop":false,"openEffect":"zoom","selector":".lightbox"});
(function() {
  let previousOnload = window.onload;
  window.onload = () => {
    if (previousOnload) {
      previousOnload();
    }
    lightboxQuarto.on('slide_before_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      const href = trigger.getAttribute('href');
      if (href !== null) {
        const imgEl = window.document.querySelector(`a[href="${href}"] img`);
        if (imgEl !== null) {
          const srcAttr = imgEl.getAttribute("src");
          if (srcAttr && srcAttr.startsWith("data:")) {
            slideConfig.href = srcAttr;
          }
        }
      } 
    });
  
    lightboxQuarto.on('slide_after_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(slideNode);
      }
    });
  
  };
  
})();
          </script>




</body></html>