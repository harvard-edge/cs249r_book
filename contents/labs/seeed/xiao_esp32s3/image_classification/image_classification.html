<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.31">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>ML Systems Textbook</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../../../">
<link href="../../../../../contents/labs/seeed/xiao_esp32s3/object_detection/object_detection.html" rel="next">
<link href="../../../../../contents/labs/seeed/xiao_esp32s3/setup/setup.html" rel="prev">
<link href="../../../../../assets/images/icons/favicon.png" rel="icon" type="image/png">
<script src="../../../../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../../../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../../../site_libs/quarto-html/quarto-syntax-highlighting-4a990d8dcb58f517c7c86712b8f2ac7c.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../../../site_libs/bootstrap/bootstrap-cbc843cc95873402613d6df7a37f2654.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<link href="../../../../../site_libs/quarto-contrib/foldbox/foldbox.css" rel="stylesheet">
<script src="../../../../../site_libs/quarto-contrib/glightbox/glightbox.min.js"></script>
<link href="../../../../../site_libs/quarto-contrib/glightbox/glightbox.min.css" rel="stylesheet">
<link href="../../../../../site_libs/quarto-contrib/glightbox/lightbox.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-M21L0CBCVN"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-M21L0CBCVN', { 'anonymize_ip': true});
</script>
<script type="application/json" class="js-hypothesis-config">
{
  "theme": "clean",
  "openSidebar": false
}
</script>
<script async="" src="https://hypothes.is/embed.js"></script>
<script>
  window.document.addEventListener("DOMContentLoaded", function (_event) {
    document.body.classList.add('hypothesis-enabled');
  });
</script>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">

<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-M21L0CBCVN"></script>
<script type="module" src="../../../../../tools/scripts/ai_menu/dist/bundle.js" defer=""></script>
<script type="module" src="../../../../../tools/scripts/ai_menu/dist/sqlite3-opfs-async-proxy-B_ImRJXp.js"></script>
<script type="module" src="../../../../../tools/scripts/ai_menu/dist/sqlite3-worker1-bundler-friendly-CbDNa4by.js"></script>
<script type="module" src="../../../../../tools/scripts/ai_menu/dist/worker-voUF5YDa.js"></script>
<script src="../../../../../assets/scripts/sidebar-auto-collapse.js" defer=""></script>
<style>
.callout-code {
  --color1: #F2F4F8;
  --color2: #D1D7E0;
}
.callout-resource-slides {
  --color1: #E0F2F1;
  --color2: #20B2AA;
}
.callout-resource-videos {
  --color1: #E0F2F1;
  --color2: #20B2AA;
}
.callout-chapter-connection {
  --color1: #FDF2F7;
  --color2: #A51C30;
}
.callout-quiz-answer {
  --color1: #E8F2EA;
  --color2: #4a7c59;
}
.callout-resource-exercises {
  --color1: #E0F2F1;
  --color2: #20B2AA;
}
.callout-quiz-question {
  --color1: #F0F0F8;
  --color2: #5B4B8A;
}
</style>
<style>
details.callout-quiz-question > summary::before {
  background-image: url("../../../../../assets/images/icons/callouts/icon_callout-quiz-question.png");
}
details.callout-quiz-answer > summary::before {
  background-image: url("../../../../../assets/images/icons/callouts/icon_callout-quiz-answer.png");
}
details.callout-chapter-connection > summary::before {
  background-image: url("../../../../../assets/images/icons/callouts/icon_callout-chapter-connection.png");
}
details.callout-resource-slides > summary::before {
  background-image: url("../../../../../assets/images/icons/callouts/icon_callout-resource-slides.png");
}
details.callout-resource-videos > summary::before {
  background-image: url("../../../../../assets/images/icons/callouts/icon_callout-resource-videos.png");
}
details.callout-resource-exercises > summary::before {
  background-image: url("../../../../../assets/images/icons/callouts/icon_callout-resource-exercises.png");
}
</style>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating nav-fixed quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-md " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="../../../../../index.html" class="navbar-brand navbar-brand-logo">
    <img src="../../../../../assets/images/icons/favicon.png" alt="" class="navbar-logo">
    </a>
    <a class="navbar-brand" href="../../../../../index.html">
    <span class="navbar-title">Machine Learning Systems</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link active" href="../../../../../contents/labs/labs.html" aria-current="page"> <i class="bi bi-code" role="img">
</i> 
<span class="menu-text">Labs</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../../../contents/labs/kits.html"> <i class="bi bi-box" role="img">
</i> 
<span class="menu-text">Kits</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://mlsysbook.ai/pdf" target="_blank"> <i class="bi bi-file-pdf" role="img">
</i> 
<span class="menu-text">PDF</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://mlsysbook.ai/epub" target="_blank"> <i class="bi bi-book" role="img">
</i> 
<span class="menu-text">EPUB</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://github.com/harvard-edge/cs249r_book" target="_blank"> <i class="bi bi-star" role="img">
</i> 
<span class="menu-text">Star</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://opencollective.com/mlsysbook" target="_blank"> <i class="bi bi-heart" role="img">
</i> 
<span class="menu-text">Support</span></a>
  </li>  
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-github" role="link" data-bs-toggle="dropdown" aria-expanded="false">
      <i class="bi bi-github" role="img">
</i> 
 <span class="menu-text">GitHub</span>
    </a>
    <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="nav-menu-github">    
        <li>
    <a class="dropdown-item" href="https://github.com/harvard-edge/cs249r_book" target="_blank"><i class="bi bi-pencil" role="img">
</i> 
 <span class="dropdown-text">Edit this page</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://github.com/harvard-edge/cs249r_book/issues/new" target="_blank"><i class="bi bi-bug" role="img">
</i> 
 <span class="dropdown-text">Report an issue</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://github.com/harvard-edge/cs249r_book/discussions" target="_blank"><i class="bi bi-chat" role="img">
</i> 
 <span class="dropdown-text">Discussions</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://github.com/harvard-edge/cs249r_book" target="_blank"><i class="bi bi-code" role="img">
</i> 
 <span class="dropdown-text">View source</span></a>
  </li>  
    </ul>
  </li>
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../../../../contents/labs/seeed/xiao_esp32s3/xiao_esp32s3.html">Seeed XIAO ESP32S3</a></li><li class="breadcrumb-item"><a href="../../../../../contents/labs/seeed/xiao_esp32s3/image_classification/image_classification.html">Image Classification</a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
    </div>
  </nav>
  <div id="quarto-announcement" data-announcement-id="80cf830e7ea2136b91547bb117b654b4" class="alert alert-primary hidden"><i class="bi bi-megaphone quarto-announcement-icon"></i><div class="quarto-announcement-content">
<p>🎉 <strong>Just Announced:</strong> <em>Machine Learning Systems</em> will be published by <strong>MIT Press</strong>. <a href="https://www.linkedin.com/posts/vijay-janapa-reddi-63a6a173_tinyml-tikz-ai-activity-7338324711145136128-6WU-?utm_source=share&amp;utm_medium=member_desktop&amp;rcm=ACoAAA-V7E4BcYtyZgLSeGhXC2t9jRSlmazfp-I">See the news →</a><br></p>
<p>🚀 <strong>Sneak Peek:</strong> <a href="https://mlsysbook.github.io/TinyTorch/intro.html">Tiny🔥Torch</a>. Build your own machine learning framework from scratch!<br></p>
<p>🧠 <strong>Self-checks:</strong> Added lightweight <a href="../../../../../contents/core/introduction/introduction.html#quiz-question-sec-introduction-ai-ml-basics-041a">quizzes</a> to each chapter for self-assessment.<br></p>
<p>📦 <strong>New Hardware:</strong> <a href="../../../../../contents/labs/kits.html">Seeed TinyML Kit</a>. Latest hands-on learning platform.</p>
</div><i class="bi bi-x-lg quarto-announcement-action"></i></div>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Homepage</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../contents/frontmatter/foreword.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Foreword</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../contents/frontmatter/about/about.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">About the Book</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../contents/frontmatter/changelog/changelog.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Book Changelog</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../contents/frontmatter/acknowledgements/acknowledgements.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Acknowledgements</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../contents/frontmatter/socratiq/socratiq.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">SocratiQ AI</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Systems Foundations</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../contents/core/introduction/introduction.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduction</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../contents/core/ml_systems/ml_systems.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">ML Systems</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../contents/core/dl_primer/dl_primer.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">DL Primer</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../contents/core/dnn_architectures/dnn_architectures.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">DNN Architectures</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">Design Principles</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../contents/core/workflow/workflow.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">AI Workflow</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../contents/core/data_engineering/data_engineering.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Data Engineering</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../contents/core/frameworks/frameworks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">AI Frameworks</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../contents/core/training/training.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">AI Training</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true">
 <span class="menu-text">Performance Engineering</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../contents/core/efficient_ai/efficient_ai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Efficient AI</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../contents/core/optimizations/optimizations.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Model Optimizations</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../contents/core/hw_acceleration/hw_acceleration.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">AI Acceleration</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../contents/core/benchmarking/benchmarking.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Benchmarking AI</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true">
 <span class="menu-text">Robust Deployment</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../contents/core/ops/ops.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">ML Operations</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../contents/core/ondevice_learning/ondevice_learning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">On-Device Learning</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../contents/core/robust_ai/robust_ai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Robust AI</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../contents/core/privacy_security/privacy_security.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Security &amp; Privacy</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true">
 <span class="menu-text">Trustworthy Systems</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../contents/core/responsible_ai/responsible_ai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Responsible AI</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../contents/core/sustainable_ai/sustainable_ai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Sustainable AI</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../contents/core/ai_for_good/ai_for_good.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">AI for Good</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true">
 <span class="menu-text">Frontiers of ML Systems</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../contents/core/conclusion/conclusion.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Conclusion</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="true">
 <span class="menu-text">Hands-on Labs</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-7" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../contents/labs/labs.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Getting Started</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../contents/labs/kits.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Hardware Kits</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../contents/labs/ide_setup.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">IDE Setup</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" role="navigation" aria-expanded="true">
 <span class="menu-text">Arduino</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-8" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../contents/labs/arduino/nicla_vision/nicla_vision.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Overview</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../contents/labs/arduino/nicla_vision/setup/setup.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Setup</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../contents/labs/arduino/nicla_vision/image_classification/image_classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Image Classification</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../contents/labs/arduino/nicla_vision/object_detection/object_detection.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Object Detection</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../contents/labs/arduino/nicla_vision/kws/kws.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Keyword Spotting (KWS)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../contents/labs/arduino/nicla_vision/motion_classification/motion_classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Motion Classification and Anomaly Detection</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-9" role="navigation" aria-expanded="true">
 <span class="menu-text">Seeed XIAO ESP32S3</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-9" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-9" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../contents/labs/seeed/xiao_esp32s3/xiao_esp32s3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Overview</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../contents/labs/seeed/xiao_esp32s3/setup/setup.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Setup</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../contents/labs/seeed/xiao_esp32s3/image_classification/image_classification.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">Image Classification</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../contents/labs/seeed/xiao_esp32s3/object_detection/object_detection.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Object Detection</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../contents/labs/seeed/xiao_esp32s3/kws/kws.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Keyword Spotting (KWS)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../contents/labs/seeed/xiao_esp32s3/motion_classification/motion_classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Motion Classification and Anomaly Detection</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-10" role="navigation" aria-expanded="true">
 <span class="menu-text">Grove Vision</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-10" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-10" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../contents/labs/seeed/grove_vision_ai_v2/grove_vision_ai_v2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Overview</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../contents/labs/seeed/grove_vision_ai_v2/setup_and_no_code_apps/setup_and_no_code_apps.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Setup and No-Code Applications</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../contents/labs/seeed/grove_vision_ai_v2/image_classification/image_classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Image Classification</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../contents/labs/seeed/grove_vision_ai_v2/object_detection/object_detection.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Object Detection</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-11" role="navigation" aria-expanded="true">
 <span class="menu-text">Raspberry Pi</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-11" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-11" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../contents/labs/raspi/raspi.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Overview</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../contents/labs/raspi/setup/setup.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Setup</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../contents/labs/raspi/image_classification/image_classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Image Classification</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../contents/labs/raspi/object_detection/object_detection.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Object Detection</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../contents/labs/raspi/llm/llm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Small Language Models (SLM)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../contents/labs/raspi/vlm/vlm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Vision-Language Models (VLM)</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-12" role="navigation" aria-expanded="true">
 <span class="menu-text">Shared</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-12" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-12" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../contents/labs/shared/shared.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Overview</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../contents/labs/shared/kws_feature_eng/kws_feature_eng.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">KWS Feature Engineering</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../contents/labs/shared/dsp_spectral_features_block/dsp_spectral_features_block.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">DSP Spectral Features</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-13" role="navigation" aria-expanded="true">
 <span class="menu-text">Resources</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-13" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-13" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../../contents/backmatter/resources/phd_survival_guide.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">PhD Survival Guide</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active" data-toc-expanded="99">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#image-classification" id="toc-image-classification" class="nav-link active" data-scroll-target="#image-classification">Image Classification</a>
  <ul>
  <li><a href="#overview" id="toc-overview" class="nav-link" data-scroll-target="#overview">Overview</a></li>
  <li><a href="#image-classification-1" id="toc-image-classification-1" class="nav-link" data-scroll-target="#image-classification-1">Image Classification</a>
  <ul class="collapse">
  <li><a href="#image-classification-on-the-sensecraft-ai-workspace" id="toc-image-classification-on-the-sensecraft-ai-workspace" class="nav-link" data-scroll-target="#image-classification-on-the-sensecraft-ai-workspace">Image Classification on the SenseCraft AI Workspace</a></li>
  <li><a href="#post-processing" id="toc-post-processing" class="nav-link" data-scroll-target="#post-processing">Post-Processing</a></li>
  </ul></li>
  <li><a href="#an-image-classification-project" id="toc-an-image-classification-project" class="nav-link" data-scroll-target="#an-image-classification-project">An Image Classification Project</a>
  <ul class="collapse">
  <li><a href="#the-goal" id="toc-the-goal" class="nav-link" data-scroll-target="#the-goal">The Goal</a></li>
  <li><a href="#data-collection" id="toc-data-collection" class="nav-link" data-scroll-target="#data-collection">Data Collection</a></li>
  <li><a href="#training" id="toc-training" class="nav-link" data-scroll-target="#training">Training</a></li>
  <li><a href="#test" id="toc-test" class="nav-link" data-scroll-target="#test">Test</a></li>
  <li><a href="#deployment" id="toc-deployment" class="nav-link" data-scroll-target="#deployment">Deployment</a></li>
  <li><a href="#saving-the-model" id="toc-saving-the-model" class="nav-link" data-scroll-target="#saving-the-model">Saving the Model</a></li>
  </ul></li>
  <li><a href="#image-classification-project-from-a-dataset" id="toc-image-classification-project-from-a-dataset" class="nav-link" data-scroll-target="#image-classification-project-from-a-dataset">Image Classification Project from a Dataset</a></li>
  <li><a href="#training-the-model-with-edge-impulse-studio" id="toc-training-the-model-with-edge-impulse-studio" class="nav-link" data-scroll-target="#training-the-model-with-edge-impulse-studio">Training the model with Edge Impulse Studio</a>
  <ul class="collapse">
  <li><a href="#data-acquisition" id="toc-data-acquisition" class="nav-link" data-scroll-target="#data-acquisition">Data Acquisition</a></li>
  <li><a href="#impulse-design" id="toc-impulse-design" class="nav-link" data-scroll-target="#impulse-design">Impulse Design</a></li>
  <li><a href="#pre-processing-feature-generation" id="toc-pre-processing-feature-generation" class="nav-link" data-scroll-target="#pre-processing-feature-generation">Pre-processing (Feature Generation)</a></li>
  <li><a href="#model-design-training-and-test" id="toc-model-design-training-and-test" class="nav-link" data-scroll-target="#model-design-training-and-test">Model Design, Training, and Test</a></li>
  </ul></li>
  <li><a href="#model-deployment" id="toc-model-deployment" class="nav-link" data-scroll-target="#model-deployment">Model Deployment</a>
  <ul class="collapse">
  <li><a href="#model-deployment-on-the-sensecraft-ai" id="toc-model-deployment-on-the-sensecraft-ai" class="nav-link" data-scroll-target="#model-deployment-on-the-sensecraft-ai">Model Deployment on the SenseCraft AI</a>
  <ul class="collapse">
  <li><a href="#post-processing-1" id="toc-post-processing-1" class="nav-link" data-scroll-target="#post-processing-1">Post-Processing</a></li>
  </ul></li>
  <li><a href="#model-deployment-as-an-arduino-library-at-ei-studio" id="toc-model-deployment-as-an-arduino-library-at-ei-studio" class="nav-link" data-scroll-target="#model-deployment-as-an-arduino-library-at-ei-studio">Model Deployment as an Arduino Library at EI Studio</a>
  <ul class="collapse">
  <li><a href="#xiao-esp32s3-image-classification-code-explained" id="toc-xiao-esp32s3-image-classification-code-explained" class="nav-link" data-scroll-target="#xiao-esp32s3-image-classification-code-explained">XIAO ESP32S3 Image Classification Code Explained</a></li>
  </ul></li>
  <li><a href="#inference" id="toc-inference" class="nav-link" data-scroll-target="#inference">Inference</a></li>
  <li><a href="#post-processing-2" id="toc-post-processing-2" class="nav-link" data-scroll-target="#post-processing-2">Post-Processing</a></li>
  </ul></li>
  <li><a href="#summary" id="toc-summary" class="nav-link" data-scroll-target="#summary">Summary</a></li>
  <li><a href="#resources" id="toc-resources" class="nav-link" data-scroll-target="#resources">Resources</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../../../../contents/labs/seeed/xiao_esp32s3/xiao_esp32s3.html">Seeed XIAO ESP32S3</a></li><li class="breadcrumb-item"><a href="../../../../../contents/labs/seeed/xiao_esp32s3/image_classification/image_classification.html">Image Classification</a></li></ol></nav></header>




<section id="image-classification" class="level1 unnumbered">
<h1 class="unnumbered">Image Classification</h1>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="./images/png/ini-dalle.png" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="DALL·E prompt - 1950s style cartoon illustration based on a real image by Marcelo Rovai"><img src="./images/png/ini-dalle.png" class="img-fluid figure-img" alt="DALL·E prompt - 1950s style cartoon illustration based on a real image by Marcelo Rovai"></a></p>
<figcaption><em>DALL·E prompt - 1950s style cartoon illustration based on a real image by Marcelo Rovai</em></figcaption>
</figure>
</div>
<section id="overview" class="level2">
<h2 class="anchored" data-anchor-id="overview">Overview</h2>
<p>We are increasingly facing an artificial intelligence (AI) revolution, where, as <a href="https://www.researchgate.net/figure/Gartner-2023-Artificial-intelligence-emerging-technologies-impact-radar-T-Nguyen-2023_fig1_372048156">Gartner</a> states, <strong>Edge AI and Computer Vision</strong> have a very high impact potential, and <strong>it is for now</strong>!</p>
<p>When we look into Machine Learning (ML) applied to vision, the first concept that greets us is <strong>Image Classification</strong>, a kind of ML’s <em>Hello World</em> that is both simple and profound!</p>
<p>The Seeed Studio XIAOML Kit provides a comprehensive hardware solution centered around the<a href="https://www.seeedstudio.com/xiao-series-page">XIAO ESP32-S3 Sense</a>, featuring an integrated <strong>OV3660</strong> camera and SD card support. Those features make the XIAO ESP32S3 Sense an excellent starting point for exploring TinyML vision AI.</p>
<p>In this Lab, we will explore Image Classification using the non-code tool <strong>SenseCraft AI</strong> and explore a more detailed development with <strong>Edge Impulse Studio</strong> and <strong>Arduino IDE</strong>.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Learning Objectives
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><p><strong>Deploy Pre-trained Models</strong> using SenseCraft AI Studio for immediate computer vision applications</p></li>
<li><p><strong>Collect and Manage Image Datasets</strong> for custom classification tasks with proper data organization</p></li>
<li><p><strong>Train Custom Image Classification Models</strong> using transfer learning with MobileNet V2 architecture</p></li>
<li><p><strong>Optimize Models for Edge Deployment</strong> through quantization and memory-efficient preprocessing</p></li>
<li><p><strong>Implement Post-processing Pipelines,</strong> including GPIO control and real-time inference integration</p></li>
<li><p><strong>Compare Development Approaches</strong> between no-code and advanced ML platforms for embedded applications</p></li>
</ul>
</div>
</div>
</section>
<section id="image-classification-1" class="level2">
<h2 class="anchored" data-anchor-id="image-classification-1">Image Classification</h2>
<p>Image classification is a fundamental task in computer vision that involves categorizing entire images into one of several predefined classes. This process entails analyzing the visual content of an image and assigning it a label from a fixed set of categories based on the dominant object or scene it depicts.</p>
<p>Image classification is crucial in various applications, ranging from organizing and searching through large databases of images in digital libraries and social media platforms to enabling autonomous systems to comprehend their surroundings. Common architectures that have significantly advanced the field of image classification include Convolutional Neural Networks (CNNs), such as AlexNet, VGGNet, and ResNet. These models have demonstrated remarkable accuracy on challenging datasets, such as <a href="https://www.image-net.org/index.php">ImageNet</a>, by learning hierarchical representations of visual data.</p>
<p>As the cornerstone of many computer vision systems, image classification drives innovation, laying the groundwork for more complex tasks like object detection and image segmentation, and facilitating a deeper understanding of visual data across various industries. So, let’s start exploring the <a href="https://sensecraft.seeed.cc/ai/view-model/60768-person-classification?tab=public">Person Classification</a> model (“Person - No Person”), a ready-to-use computer vision application on the <strong><a href="https://sensecraft.seeed.cc/ai/device/local/32">SenseCraft AI</a></strong>.</p>
<p> <img src="./images/png/person-class.png" class="img-fluid quarto-figure quarto-figure-center" style="width:85.0%"></p>
<section id="image-classification-on-the-sensecraft-ai-workspace" class="level3">
<h3 class="anchored" data-anchor-id="image-classification-on-the-sensecraft-ai-workspace">Image Classification on the SenseCraft AI Workspace</h3>
<p>Start by connecting the XIAOML Kit (or just the XIAO ESP32S3 Sense, disconnected from the Expansion Board) to the computer via USB-C, and then open the <a href="https://sensecraft.seeed.cc/ai/device/local/32">SenseCraft AI Workspace</a> to connect it.</p>
<p> <img src="./images/png/connection.png" class="img-fluid quarto-figure quarto-figure-center" style="width:85.0%"></p>
<p>Once connected, select the option <code>[Select Model...]</code> and enter in the search window: “<em>Person Classification</em>”. From the options available, select the one trained over the MobileNet V2 (passing the mouse over the models will open a pop-up window with its main characteristics).</p>
<p> <img src="./images/png/model_selection.png" class="img-fluid quarto-figure quarto-figure-center" style="width:85.0%"></p>
<p>Click on the chosen model and confirm the deployment. A new firmware for the model should start uploading to our device.</p>
<blockquote class="blockquote">
<p>Note that the percentage of models downloaded and firmware uploaded will be displayed. If not, try disconnecting the device, then reconnect it and press the boot button.</p>
</blockquote>
<p>After the model is uploaded successfully, we can view the live feed from the XIAO camera and the classification result (<code>Person</code> or <code>Not a Person</code>) in the <strong>Preview</strong> area, along with the inference details displayed in the <strong>Device Logger</strong>.</p>
<blockquote class="blockquote">
<p>Note that we can also select our <strong>Inference Frame Interval</strong>, from “Real-Time” (Default) to 10 seconds, and the <strong>Mode</strong> (UART, I2C, etc) as the data is shared by the device (the default is UART via USB).</p>
</blockquote>
<p> <img src="./images/png/person-detect-inference.png" class="img-fluid quarto-figure quarto-figure-center" style="width:85.0%"></p>
<p>At the Device Logger, we can see that the latency of the model is from 52 to 78 ms for pre-processing and around 532ms for inference, which will give us a total time of a little less than 600ms, or about <strong>1.7 Frames per second (FPS)</strong>.</p>
<blockquote class="blockquote">
<p>To run the Mobilenet V2 0.35, the XIAO had a peak current of 160mA at 5.23V, resulting in a <strong>power consumption of 830mW</strong>.</p>
</blockquote>
</section>
<section id="post-processing" class="level3">
<h3 class="anchored" data-anchor-id="post-processing">Post-Processing</h3>
<p>An essential step in an Image Classification project pipeline is to define what we want to do with the inference result. So, imagine that we will use the XIAO to automatically turn on the room lights if a person is detected.</p>
<p> <img src="./images/png/pipeline.png" class="img-fluid quarto-figure quarto-figure-center" style="width:75.0%"></p>
<p>With the SebseCraft AI, we can do it on the <code>Output -&gt; GPIO</code> section. Click on the Add icon to trigger the action when event conditions are met. A pop-up window will open, where you can define the action to be taken. For example, if a person is detected with a confidence of more than 60% the internal <code>LED</code> should be ON. In a real scenario, a GPIO, for example, <code>D0</code>, <code>D1</code>, <code>D2</code>, <code>D11</code>, or <code>D12</code>, would be used to trigger a relay to turn on a light.</p>
<p> <img src="./images/png/action.png" class="img-fluid quarto-figure quarto-figure-center" style="width:65.0%"></p>
<p>Once confirmed, the created <strong>Trigger Action</strong> will be shown. Press <code>Send</code> to upload the command to the XIAO.</p>
<p> <img src="./images/png/trigger.png" class="img-fluid quarto-figure quarto-figure-center" style="width:85.0%"></p>
<p>Now, pointing the XIAO at a person will make the internal LED go ON.</p>
<p> <img src="./images/png/person-trigger.png" class="img-fluid quarto-figure quarto-figure-center" style="width:65.0%"></p>
<blockquote class="blockquote">
<p>We will explore more trigger actions and post-processing techniques further in this lab.</p>
</blockquote>
</section>
</section>
<section id="an-image-classification-project" class="level2">
<h2 class="anchored" data-anchor-id="an-image-classification-project">An Image Classification Project</h2>
<p>Let’s create a simple Image Classification project using SenseCraft AI Studio. Below, we can see a typical machine learning pipeline that will be used in our project.</p>
<p> <img src="./images/png/pipeline-sensecraft.png" class="img-fluid quarto-figure quarto-figure-center" style="width:85.0%"></p>
<p>On SenseCraft AI Studio: Let’s open the tab <a href="https://sensecraft.seeed.cc/ai/training">Training</a>:</p>
<p> <img src="./images/png/img-class-project.png" class="img-fluid quarto-figure quarto-figure-center" style="width:85.0%"></p>
<p>The default is to train a <code>Classification</code> model with a WebCam if it is available. Let’s select the <code>XIAOESP32S3 Sense</code> instead. Pressing the green button <code>[Connect]</code> will cause a Pop-Up window to appear. Select the corresponding Port and press the blue button <code>[Connect]</code>.</p>
<p> <img src="./images/png/train-img-class.png" class="img-fluid quarto-figure quarto-figure-center" style="width:85.0%"></p>
<p>The image streamed from the Grove Vision AI V2 will be displayed.</p>
<section id="the-goal" class="level3">
<h3 class="anchored" data-anchor-id="the-goal">The Goal</h3>
<p>The first step, as we can see in the ML pipeline, is to define a goal. Let’s imagine that we have an industrial installation that should automatically sort wheels and boxes.</p>
<p> <img src="./images/png/distr-line.png" class="img-fluid quarto-figure quarto-figure-center" style="width:75.0%"></p>
<p>So, let’s simulate it, classifying, for example, a toy <code>box</code> and a toy <code>wheel</code>. We should also include a 3rd class of images, <code>background</code>, where there are no objects in the scene.</p>
<p> <img src="./images/png/classes_img_class.png" class="img-fluid quarto-figure quarto-figure-center" style="width:75.0%"></p>
</section>
<section id="data-collection" class="level3">
<h3 class="anchored" data-anchor-id="data-collection">Data Collection</h3>
<p>Let’s create the classes, following, for example, an alphabetical order:</p>
<ul>
<li>Class1: background</li>
<li>Class 2: box</li>
<li>Class 3: wheel</li>
</ul>
<p> <img src="./images/png/classes.png" class="img-fluid quarto-figure quarto-figure-center" style="width:75.0%"></p>
<p>Select one of the classes and keep pressing the green button (<code>Hold to Record</code>) under the preview area. The collected images (and their counting) will appear on the Image Samples Screen. Carefully and slowly, move the camera to capture different angles of the object. To modify the position or interfere with the image, release the green button, rearrange the object, and then hold it again to resume the capture.</p>
<p> <img src="./images/png/collect-imaages.png" class="img-fluid quarto-figure quarto-figure-center" style="width:85.0%"></p>
<p>After collecting the images, review them and delete any incorrect ones.</p>
<p> <img src="./images/png/clean_dataset.png" class="img-fluid quarto-figure quarto-figure-center" style="width:75.0%"></p>
<p>Collect around <strong>50 images</strong> from each class and go to Training Step.</p>
<blockquote class="blockquote">
<p>Note that it is possible to download the collected images to be used in another application, for example, with the Edge Impulse Studio.</p>
</blockquote>
</section>
<section id="training" class="level3">
<h3 class="anchored" data-anchor-id="training">Training</h3>
<p>Confirm if the correct device is selected (<code>XIAO ESP32S3 Sense</code>) and press <code>[Start Training]</code></p>
<p> <img src="./images/png/train-setup.png" class="img-fluid quarto-figure quarto-figure-center" style="width:85.0%"></p>
</section>
<section id="test" class="level3">
<h3 class="anchored" data-anchor-id="test">Test</h3>
<p>After training, the inference result can be previewed.</p>
<blockquote class="blockquote">
<p>Note that the model is not running on the device. We are, in fact, only capturing the images with the device and performing a <strong>live preview</strong> using the training model, which is running in the Studio.</p>
</blockquote>
<p> <img src="./images/png/img-class-infer.png" class="img-fluid quarto-figure quarto-figure-center" style="width:85.0%"></p>
<p>Now is the time to really deploy the model in the device.</p>
</section>
<section id="deployment" class="level3">
<h3 class="anchored" data-anchor-id="deployment">Deployment</h3>
<p>Select the trained model and <code>XIAO ESP32S3 Sense</code> at the <code>Supported Devices</code> window. And press <code>[Deploy to device]</code>.</p>
<p> <img src="./images/png/select-to-deploy.png" class="img-fluid quarto-figure quarto-figure-center" style="width:85.0%"></p>
<p>The SeneCrafit AI will redirect us to the <strong>Vision Workplace</strong> tab. <code>Confirm</code> the deployment, select the Port, and <code>Connect</code> it.</p>
<p> <img src="./images/png/upload-model.png" class="img-fluid quarto-figure quarto-figure-center" style="width:85.0%"></p>
<p>The model will be flashed into the device. After an automatic reset, the model will start running on the device. On the Device Logger, we can see that the inference has a <strong>latency of approximately 426 ms</strong>, plus a <strong>pre-processing of around 110ms</strong>, corresponding to a <strong>frame rate of 1.8 frames per second (FPS)</strong>.</p>
<p>Also, note that in <strong>Settings</strong>, it is possible to adjust the model’s confidence.</p>
<p> <img src="./images/png/infer.png" class="img-fluid quarto-figure quarto-figure-center" style="width:85.0%"></p>
<blockquote class="blockquote">
<p>To run the Image Classification Model, the XIAO ESP32S3 had a peak current of 14mA at 5.23V, resulting in a <strong>power consumption of 730mW</strong>.</p>
</blockquote>
<p>As before, in the <strong>Output –&gt; GPIO</strong>, we can turn the GPIOs or the Internal LED ON based on the detected class. For example, the LED will be turned ON when the wheel is detected.</p>
<p> <img src="./images/png/led-wheel.png" class="img-fluid quarto-figure quarto-figure-center" style="width:65.0%"></p>
</section>
<section id="saving-the-model" class="level3">
<h3 class="anchored" data-anchor-id="saving-the-model">Saving the Model</h3>
<p>It is possible to save the model in the SenseCraft AI Studio. The Studio will retain all our models for later deployment. For that, return to the <code>Training</code> tab and select the button <code>[Save to SenseCraft</code>]:</p>
<p> <img src="./images/png/save-model.png" class="img-fluid quarto-figure quarto-figure-center" style="width:75.0%"></p>
<p>Follow the instructions to enter the model’s name, description, image, and other details.</p>
<p> <img src="./images/png/model-saved.png" class="img-fluid quarto-figure quarto-figure-center" style="width:85.0%"></p>
<p>Note that the trained model (an Int8 MobileNet V2 with a size of 320KB) can be downloaded for further use or even analysis, for example, using <a href="https://github.com/lutzroeder/netron">Netron</a>. Note that the model uses images of size 224x224x3 as its Input Tensor. In the next step, we will use different hyperparameters on the Edge Impulse Studio.</p>
<p> <img src="./images/png/netron.png" class="img-fluid quarto-figure quarto-figure-center" style="width:85.0%"></p>
<p>Also, the model can be deployed again to the device at any time. Automatically, the <strong>Workspace</strong> will be open on the SenseCraft AI.</p>
</section>
</section>
<section id="image-classification-project-from-a-dataset" class="level2">
<h2 class="anchored" data-anchor-id="image-classification-project-from-a-dataset">Image Classification Project from a Dataset</h2>
<p>The primary objective of our project is to train a model and perform inference on the XIAO ESP32S3 Sense. For training, we should find some data <strong>(in fact, tons of data!)</strong>.</p>
<p><em>But as we alheady know, first of all, we need a goal! What do we want to classify?</em></p>
<p>With TinyML, a set of techniques associated with machine learning inference on embedded devices, we should limit the classification to three or four categories due to limitations (mainly memory). We can, for example, train the images captured for the Box versus Wheel, which can be downloaded from the SenseCraft AI Studio.</p>
<blockquote class="blockquote">
<p>Alternatively, we can use a completely new dataset, such as one that differentiates apples from bananas and potatoes, or other categories. If possible, try finding a specific dataset that includes images from those categories. <a href="https://www.kaggle.com/kritikseth/fruit-and-vegetable-image-recognition">Kaggle fruit-and-vegetable-image-recognition</a> is a good start.</p>
</blockquote>
<p>Let’s download the dataset captured in the previous section. Open the menu (3 dots) on each of the captured classes and select <code>Export Data</code>.</p>
<p> <img src="./images/png/export-dataset.png" class="img-fluid quarto-figure quarto-figure-center" style="width:75.0%"></p>
<p>The dataset will be downloaded to the computer as a .ZIP file, with one file for each class. Save them in your working folder and unzip them. You should have three folders, one for each class.</p>
<p> <img src="./images/png/dataset-pc.png" class="img-fluid quarto-figure quarto-figure-center" style="width:85.0%"></p>
<blockquote class="blockquote">
<p>Optionally, you can add some fresh images, using, for example, the code discussed in the setup lab.</p>
</blockquote>
</section>
<section id="training-the-model-with-edge-impulse-studio" class="level2">
<h2 class="anchored" data-anchor-id="training-the-model-with-edge-impulse-studio">Training the model with Edge Impulse Studio</h2>
<p>We will use the Edge Impulse Studio to train our model. <a href="https://www.edgeimpulse.com/">Edge Impulse</a> is a leading development platform for machine learning on edge devices.</p>
<p>Enter your account credentials (or create a free account) at Edge Impulse. Next, create a new project:</p>
<section id="data-acquisition" class="level3">
<h3 class="anchored" data-anchor-id="data-acquisition">Data Acquisition</h3>
<p>Next, go to the <strong>Data acquisition</strong> section and there, select <code>+ Add data</code>. A pop-up window will appear. Select <code>UPLOAD DATA</code>.</p>
<p> <img src="./images/png/get-dataset.png" class="img-fluid quarto-figure quarto-figure-center" style="width:85.0%"></p>
<p>After selection, a new Pop-Up window will appear, asking to update the data.</p>
<ul>
<li>In Upload mode: <code>select a folder</code> and press <code>[Choose Files]</code>.</li>
<li>Go to the folder that contains one of the classes and press <code>[Upload]</code></li>
</ul>
<p> <img src="./images/png/select-folder.png" class="img-fluid quarto-figure quarto-figure-center" style="width:85.0%"></p>
<ul>
<li>You will return automatically to the Upload data window.</li>
<li>Select <code>Automatically split between training and testing</code></li>
<li>And enter the label of the images that are in the folder.</li>
<li>Select <code>[Upload data]</code></li>
<li>At this point, the files will start to be uploaded, and after that, another Pop-Up window will appear asking if you are building an object detection project. Select <code>[no]</code></li>
</ul>
<p> <img src="./images/png/up-data-2.png" class="img-fluid quarto-figure quarto-figure-center" style="width:85.0%"></p>
<p>Repeat the procedure for all classes. <strong>Do not forget to change the label’s name</strong>. If you forget and the images are uploaded, please note that they will be mixed in the Studio. Do not worry, you can manually move the data between classes further.</p>
<p>Close the Upload Data window and return to the <strong>Data acquisition</strong> page. We can see that all dataset was uploaded. Note that on the upper panel, we can see that we have 158 items, all of which are balanced. Also, 19% of the images were left for testing.</p>
<p> <img src="./images/png/dataset.png" class="img-fluid quarto-figure quarto-figure-center" style="width:85.0%"></p>
</section>
<section id="impulse-design" class="level3">
<h3 class="anchored" data-anchor-id="impulse-design">Impulse Design</h3>
<blockquote class="blockquote">
<p>An impulse takes raw data (in this case, images), extracts features (resizes pictures), and then uses a learning block to classify new data.</p>
</blockquote>
<p>Classifying images is the most common application of deep learning, but a substantial amount of data is required to accomplish this task. We have around 50 images for each category. Is this number enough? Not at all! We will need thousands of images to “teach” or “model” each class, allowing us to differentiate them. However, we can resolve this issue by retraining a previously trained model using thousands of images. We refer to this technique as <strong>“Transfer Learning” (TL)</strong>. With TL, we can fine-tune a pre-trained image classification model on our data, achieving good performance even with relatively small image datasets, as in our case.</p>
<p> <img src="./images/png/tl.png" class="img-fluid quarto-figure quarto-figure-center" style="width:85.0%"></p>
<p>With TL, we can fine-tune a pre-trained image classification model on our data, performing well even with relatively small image datasets (our case).</p>
<p>So, starting from the raw images, we will resize them <span class="math inline">\((96\times 96)\)</span> Pixels are fed to our Transfer Learning block. Let’s create an Inpulse.</p>
<blockquote class="blockquote">
<p>At this point, we can also define our target device to monitor our “budget” (memory and latency). The XIAO ESP32S3 is not officially supported by Edge Impulse, so let’s consider the Espressif ESP-EYE, which is similar but slower.</p>
</blockquote>
<p> <img src="./images/png/impulse.png" class="img-fluid quarto-figure quarto-figure-center" style="width:85.0%"></p>
<p>Save the Impulse, as shown above, and go to the <strong>Image</strong> section.</p>
</section>
<section id="pre-processing-feature-generation" class="level3">
<h3 class="anchored" data-anchor-id="pre-processing-feature-generation">Pre-processing (Feature Generation)</h3>
<p>Besides resizing the images, we can convert them to grayscale or retain their original RGB color depth. Let’s select <code>[RGB]</code> in the <code>Image</code> section. Doing that, each data sample will have a dimension of 27,648 features (96x96x3). Pressing <code>[Save Parameters]</code> will open a new tab, <code>Generate Features</code>. Press the button <code>[Generate Features]</code>to generate the features.</p>
</section>
<section id="model-design-training-and-test" class="level3">
<h3 class="anchored" data-anchor-id="model-design-training-and-test">Model Design, Training, and Test</h3>
<p>In 2007, Google introduced <a href="https://research.googleblog.com/2017/06/mobilenets-open-source-models-for.html">MobileNetV1</a>. In 2018, <a href="https://arxiv.org/abs/1801.04381">MobileNetV2: Inverted Residuals and Linear Bottlenecks</a>, was launched, and, in 2019, the V3. The Mobilinet is a family of general-purpose computer vision neural networks explicitly designed for mobile devices to support classification, detection, and other applications. MobileNets are small, low-latency, low-power models parameterized to meet the resource constraints of various use cases.</p>
<p>Although the base MobileNet architecture is already compact and has low latency, a specific use case or application may often require the model to be even smaller and faster. MobileNets introduce a straightforward parameter, <strong>α</strong> (alpha), called the width multiplier to construct these smaller, less computationally expensive models. The role of the width multiplier α is to thin a network uniformly at each layer.</p>
<p>Edge Impulse Studio has available MobileNet V1 (96x96 images) and V2 (96x96 and 160x160 images), with several different <strong>α</strong> values (from 0.05 to 1.0). For example, you will get the highest accuracy with V2, 160x160 images, and α=1.0. Of course, there is a trade-off. The higher the accuracy, the more memory (around 1.3M RAM and 2.6M ROM) will be needed to run the model, implying more latency. The smaller footprint will be obtained at another extreme with MobileNet V1 and α=0.10 (around 53.2K RAM and 101K ROM).</p>
<blockquote class="blockquote">
<p>We will use the <strong>MobileNet V2 0.35</strong> as our base model (but a model with a greater alpha can be used here). The final layer of our model, preceding the output layer, will have 16 neurons with a 10% dropout rate for preventing overfitting.</p>
</blockquote>
<p>Another necessary technique to use with deep learning is <strong>data augmentation</strong>. Data augmentation is a method that can help improve the accuracy of machine learning models by creating additional artificial data. A data augmentation system makes small, random changes to your training data during the training process (such as flipping, cropping, or rotating the images).</p>
<p>Under the hood, here you can see how Edge Impulse implements a data Augmentation policy on your data:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode cpp code-with-copy"><code class="sourceCode cpp"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="pp"># </span><span class="er">Implements the data augmentation policy</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>def augment_image<span class="op">(</span>image<span class="op">,</span> label<span class="op">):</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>    <span class="pp"># </span><span class="er">Flips the image randomly</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>    image <span class="op">=</span> tf<span class="op">.</span>image<span class="op">.</span>random_flip_left_right<span class="op">(</span>image<span class="op">)</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>    <span class="pp"># </span><span class="er">Increase the image size, then randomly crop it down to</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>    <span class="pp"># </span><span class="er">the original dimensions</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>    resize_factor <span class="op">=</span> random<span class="op">.</span>uniform<span class="op">(</span><span class="dv">1</span><span class="op">,</span> <span class="fl">1.2</span><span class="op">)</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>    new_height <span class="op">=</span> math<span class="op">.</span>floor<span class="op">(</span>resize_factor <span class="op">*</span> INPUT_SHAPE<span class="op">[</span><span class="dv">0</span><span class="op">])</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>    new_width <span class="op">=</span> math<span class="op">.</span>floor<span class="op">(</span>resize_factor <span class="op">*</span> INPUT_SHAPE<span class="op">[</span><span class="dv">1</span><span class="op">])</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>    image <span class="op">=</span> tf<span class="op">.</span>image<span class="op">.</span>resize_with_crop_or_pad<span class="op">(</span>image<span class="op">,</span> new_height<span class="op">,</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>                                             new_width<span class="op">)</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>    image <span class="op">=</span> tf<span class="op">.</span>image<span class="op">.</span>random_crop<span class="op">(</span>image<span class="op">,</span> size<span class="op">=</span>INPUT_SHAPE<span class="op">)</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>    <span class="pp"># </span><span class="er">Vary the brightness of the image</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>    image <span class="op">=</span> tf<span class="op">.</span>image<span class="op">.</span>random_brightness<span class="op">(</span>image<span class="op">,</span> max_delta<span class="op">=</span><span class="fl">0.2</span><span class="op">)</span></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> image<span class="op">,</span> label</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Now, let’s us define the hyperparameters:</p>
<ul>
<li>Epochs: 20,</li>
<li>Bach Size: 32</li>
<li>Learning Rate: 0.0005</li>
<li>Validation size: 20%</li>
</ul>
<p>And, so, we have as a training result:</p>
<p> <img src="./images/png/train-result.png" class="img-fluid quarto-figure quarto-figure-center" style="width:85.0%"></p>
<p>The model profile predicts <strong>233 KB of RAM and 546 KB of Flash</strong>, indicating no problem with the Xiao ESP32S3, which has 8 MB of PSRAM. Additionally, the Studio indicates a <strong>latency of around 1160 ms</strong>, which is very high. However, this is to be expected, given that we are using the ESP-EYE, whose CPU is an Extensa LX6, and the ESP32S3 uses a newer and more powerful Xtensa LX7.</p>
<blockquote class="blockquote">
<p>With the test data, we also achieved 100% accuracy, even with a quantized INT8 model. This result is not typical in real projects, but our project here is relatively simple, with two objects that are very distinctive from each other.</p>
</blockquote>
</section>
</section>
<section id="model-deployment" class="level2">
<h2 class="anchored" data-anchor-id="model-deployment">Model Deployment</h2>
<p>We can deploy the trained model:</p>
<ul>
<li>As <code>.TFLITE</code> to be used on the <strong>SenseCraft AI </strong></li>
<li>As an <code>Arduino Library</code> in the <strong>Edge Impulse Studio</strong>.</li>
</ul>
<p>Let’s start with the SenseCraft, which is more straightforward and more intuitive.</p>
<section id="model-deployment-on-the-sensecraft-ai" class="level3">
<h3 class="anchored" data-anchor-id="model-deployment-on-the-sensecraft-ai">Model Deployment on the SenseCraft AI</h3>
<p>On the <strong>Dashboard</strong>, it is possible to download the trained model in several different formats. Let’s download <code>TensorFlow Lite (int8 quantized)</code>, which has a size of 623KB.</p>
<p> <img src="./images/png/deploy-1.png" class="img-fluid quarto-figure quarto-figure-center" style="width:85.0%"></p>
<p>On <strong>SenseCraft AI Studio</strong>, go to the <code>Workspace</code> tab, select <code>XIAO ESP32S3</code>, the corresponding Port, and connect the device.</p>
<p>You should see the last model that was uploaded to the device. Select the green button <code>[Upload Model]</code>. A pop-up window will prompt you to enter the model name, the model file, and the class names (<strong>objects</strong>). We should use labels in alphabetical order: <code>0: background</code>, <code>1: box</code>, and <code>2: wheel</code>, and then press <code>[Send]</code>.</p>
<p> <img src="./images/png/model-tflite-infer.png" class="img-fluid quarto-figure quarto-figure-center" style="width:85.0%"></p>
<p>After a few seconds, the model will be uploaded (“flashed”) to our device, and the camera image will appear in real-time on the <strong>Preview</strong> Sector. The Classification result will be displayed under the image preview. It is also possible to select the <code>Confidence Threshold</code> of your inference using the cursor on <strong>Settings</strong>.</p>
<p>On the <strong>Device Logger</strong>, we can view the Serial Monitor, where we can observe the latency, which is approximately 81 ms for pre-processing and 205 ms for inference, <strong>corresponding to a frame rate of 3.4 frames per second (FPS)</strong>, what is double of we got, training the model on SenseCraft, because we are working with smaller images (96x96 versus 224x224).</p>
<blockquote class="blockquote">
<p>The total latency is around <strong>4 times faster</strong> than the estimation made in Edge Impulse Studio on an Xtensa LX6 CPU; now we are performing the inference on an Xtensa LX7 CPU.</p>
</blockquote>
<p> <img src="./images/png/tfmodel-infer.png" class="img-fluid quarto-figure quarto-figure-center" style="width:85.0%"></p>
<section id="post-processing-1" class="level4">
<h4 class="anchored" data-anchor-id="post-processing-1">Post-Processing</h4>
<p>It is possible to obtain the output of a model inference, including Latency, Class ID, and Confidence, as shown on the Device Logger in SenseCraft AI. This allows us to utilize the <strong>XIAO ESP32S3 Sense as an AI sensor</strong>. In other words, we can retrieve the model data using different communication protocols such as MQTT, UART, I2C, or SPI, depending on our project requirements.</p>
<blockquote class="blockquote">
<p>The idea is similar to what we have done on the <a href="https://www.mlsysbook.ai/contents/labs/seeed/grove_vision_ai_v2/image_classification/image_classification#sec-image-classification-postprocessing-553f">Seeed Grove Vision AI V2 Image Classification Post-Processing Lab</a>.</p>
</blockquote>
<p>Below is an example of a connection using the I2C bus.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="https://files.seeedstudio.com/wiki/SenseCraft_AI/img2/iic_connection.png" class="lightbox" data-gallery="quarto-lightbox-gallery-2" title="As a Sensor | Seeed Studio Wiki"><img src="https://files.seeedstudio.com/wiki/SenseCraft_AI/img2/iic_connection.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:75.0%" alt="As a Sensor | Seeed Studio Wiki"></a></p>
</figure>
</div>
<figcaption>As a Sensor | Seeed Studio Wiki</figcaption>
</figure>
</div>
<p>Please refer to the <a href="https://wiki.seeedstudio.com/sensecraft-ai/tutorials/sensecraft-ai-output-libraries-xiao/">Seeed Studio Wiki</a> for more information.</p>
</section>
</section>
<section id="model-deployment-as-an-arduino-library-at-ei-studio" class="level3">
<h3 class="anchored" data-anchor-id="model-deployment-as-an-arduino-library-at-ei-studio">Model Deployment as an Arduino Library at EI Studio</h3>
<p>On the <strong>Deploy</strong> section at Edge Impulse Studio, Select <code>Arduino library</code>, <code>TensorFlow Lite</code>, <code>Quantized(int8)</code>, and press <code>[Build]</code>. The trained model will be downloaded as a .zip Arduino library:</p>
<p> <img src="./images/png/ard-deploy.png" class="img-fluid quarto-figure quarto-figure-center" style="width:80.0%"></p>
<p>Open your Arduino IDE, and under <strong>Sketch,</strong> go to <strong>Include Library</strong> and <strong>add .ZIP Library.</strong> Next, select the file downloaded from Edge Impulse Studio and press <code>[Open]</code>.</p>
<p> <img src="./images/png/ard-install_lib.png" class="img-fluid quarto-figure quarto-figure-center" style="width:80.0%"></p>
<p>Go to the Arduino IDE <code>Examples</code> and look for the project by its name (in this case: “Box_versus_Whell_…Interfering”. Open <code>esp32</code> -&gt; <code>esp32_camera</code>. The sketch <code>esp32_camera.ino</code> will be downloaded to the IDE.</p>
<p>This sketch was developed for the standard ESP32 and will not work with the XIAO ESP32S3 Sense. It should be modified. Let’s download the modified one from the project GitHub: <a href="https://github.com/Mjrovai/XIAO-ESP32S3-Sense/blob/main/XIAOML_Kit_code/image_class_XIAOML-Kit/image_class_XIAOML-Kit.ino">Image_class_XIAOML-Kit.ino</a>.</p>
<section id="xiao-esp32s3-image-classification-code-explained" class="level4">
<h4 class="anchored" data-anchor-id="xiao-esp32s3-image-classification-code-explained">XIAO ESP32S3 Image Classification Code Explained</h4>
<p>The code captures images from the onboard camera, processes them, and classifies them (in this case, “Box”, “Wheel”, or “Background”) using the trained model on EI Studio. It runs continuously, performing real-time inference on the edge device.</p>
<p>In short,:</p>
<p>Camera → JPEG Image → RGB888 Conversion → Resize to 96x96 → Neural Network → Classification Results → Serial Output</p>
<section id="key-components" class="level5">
<h5 class="anchored" data-anchor-id="key-components">Key Components</h5>
<ol type="1">
<li><strong>Library Includes and Dependencies</strong></li>
</ol>
<div class="sourceCode" id="cb2"><pre class="sourceCode cpp code-with-copy"><code class="sourceCode cpp"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="pp">#include </span><span class="im">&lt;Box_versus_Wheel_-_XIAO_ESP32S3_inferencing.h&gt;</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="pp">#include </span><span class="im">"edge-impulse-sdk/dsp/image/image.hpp"</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="pp">#include </span><span class="im">"esp_camera.h"</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<ul>
<li><strong>Edge Impulse Inference Library</strong>: Contains our trained model and inference engine</li>
<li><strong>Image Processing</strong>: Provides functions for image manipulation</li>
<li><strong>ESP Camera</strong>: Hardware interface for the camera module</li>
</ul>
<ol start="2" type="1">
<li><strong>Camera Pin Configurations</strong></li>
</ol>
<p>The XIAO ESP32S3 Sense can work with different camera sensors (OV2640 or OV3660), which may have different pin configurations. The code defines three possible configurations:</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode cpp code-with-copy"><code class="sourceCode cpp"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co">// Configuration 1: Most common OV2640 configuration</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="pp">#define CONFIG_1_XCLK_GPIO_NUM    </span><span class="dv">10</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="pp">#define CONFIG_1_SIOD_GPIO_NUM    </span><span class="dv">40</span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="pp">#define CONFIG_1_SIOC_GPIO_NUM    </span><span class="dv">39</span></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="co">// ... more pins</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>This flexibility allows the code to automatically try different pin mappings if the first one doesn’t work, making it more robust across different hardware revisions.</p>
<ol start="3" type="1">
<li><strong>Memory Management Settings</strong></li>
</ol>
<div class="sourceCode" id="cb4"><pre class="sourceCode cpp code-with-copy"><code class="sourceCode cpp"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="pp">#define EI_CAMERA_RAW_FRAME_BUFFER_COLS   </span><span class="dv">320</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="pp">#define EI_CAMERA_RAW_FRAME_BUFFER_ROWS   </span><span class="dv">240</span></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="pp">#define EI_CLASSIFIER_ALLOCATION_HEAP      </span><span class="dv">1</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<ul>
<li><strong>Frame Buffer Size</strong>: Defines the raw image size (320x240 pixels)</li>
<li><strong>Heap Allocation</strong>: Uses dynamic memory allocation for flexibility</li>
<li><strong>PSRAM Support</strong>: The ESP32S3 has 8MB of PSRAM for storing large data like images</li>
</ul>
</section>
<section id="setup---initialization" class="level5">
<h5 class="anchored" data-anchor-id="setup---initialization"><code>setup()</code> - Initialization</h5>
<div class="sourceCode" id="cb5"><pre class="sourceCode cpp code-with-copy"><code class="sourceCode cpp"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="dt">void</span> setup<span class="op">()</span> <span class="op">{</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>    Serial<span class="op">.</span>begin<span class="op">(</span><span class="dv">115200</span><span class="op">);</span></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">while</span> <span class="op">(!</span>Serial<span class="op">);</span></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="op">(</span>ei_camera_init<span class="op">()</span> <span class="op">==</span> <span class="kw">false</span><span class="op">)</span> <span class="op">{</span></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>        ei_printf<span class="op">(</span><span class="st">"Failed to initialize Camera!</span><span class="sc">\r\n</span><span class="st">"</span><span class="op">);</span></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>    <span class="op">}</span> <span class="cf">else</span> <span class="op">{</span></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>        ei_printf<span class="op">(</span><span class="st">"Camera initialized</span><span class="sc">\r\n</span><span class="st">"</span><span class="op">);</span></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>    <span class="op">}</span></span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>    ei_sleep<span class="op">(</span><span class="dv">2000</span><span class="op">);</span>  <span class="co">// Wait 2 seconds before starting</span></span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>This function:</p>
<ol type="1">
<li>Initializes serial communication for debugging output</li>
<li>Initializes the camera with automatic configuration detection</li>
<li>Waits 2 seconds before starting continuous inference</li>
</ol>
</section>
<section id="loop---main-processing-loop" class="level5">
<h5 class="anchored" data-anchor-id="loop---main-processing-loop"><code>loop()</code> - Main Processing Loop</h5>
<p>The loop performs these steps continuously:</p>
<p><strong>Step 1: Memory Allocation</strong></p>
<div class="sourceCode" id="cb6"><pre class="sourceCode cpp code-with-copy"><code class="sourceCode cpp"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>snapshot_buf <span class="op">=</span> <span class="op">(</span><span class="dt">uint8_t</span><span class="op">*)</span>ps_malloc<span class="op">(</span>EI_CAMERA_RAW_FRAME_BUFFER_COLS <span class="op">*</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>                                   EI_CAMERA_RAW_FRAME_BUFFER_ROWS <span class="op">*</span></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>                                   EI_CAMERA_FRAME_BYTE_SIZE<span class="op">);</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Allocates memory for the image buffer, preferring PSRAM (faster external RAM) but falling back to regular heap if needed.</p>
<p><strong>Step 2: Image Capture</strong></p>
<div class="sourceCode" id="cb7"><pre class="sourceCode cpp code-with-copy"><code class="sourceCode cpp"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> <span class="op">(</span>ei_camera_capture<span class="op">((</span><span class="dt">size_t</span><span class="op">)</span>EI_CLASSIFIER_INPUT_WIDTH<span class="op">,</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>                     <span class="op">(</span><span class="dt">size_t</span><span class="op">)</span>EI_CLASSIFIER_INPUT_HEIGHT<span class="op">,</span></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>                     snapshot_buf<span class="op">)</span> <span class="op">==</span> <span class="kw">false</span><span class="op">)</span> <span class="op">{</span></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>    ei_printf<span class="op">(</span><span class="st">"Failed to capture image</span><span class="sc">\r\n</span><span class="st">"</span><span class="op">);</span></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>    free<span class="op">(</span>snapshot_buf<span class="op">);</span></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span><span class="op">;</span></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Captures an image from the camera and stores it in the buffer.</p>
<p><strong>Step 3: Run Inference</strong></p>
<div class="sourceCode" id="cb8"><pre class="sourceCode cpp code-with-copy"><code class="sourceCode cpp"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="dt">ei_impulse_result_t</span> result <span class="op">=</span> <span class="op">{</span> <span class="dv">0</span> <span class="op">};</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>EI_IMPULSE_ERROR err <span class="op">=</span> run_classifier<span class="op">(&amp;</span>signal<span class="op">,</span> <span class="op">&amp;</span>result<span class="op">,</span> <span class="kw">false</span><span class="op">);</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Runs the machine learning model on the captured image.</p>
<p><strong>Step 4: Output Results</strong></p>
<div class="sourceCode" id="cb9"><pre class="sourceCode cpp code-with-copy"><code class="sourceCode cpp"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> <span class="op">(</span><span class="dt">uint16_t</span> i <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> i <span class="op">&lt;</span> EI_CLASSIFIER_LABEL_COUNT<span class="op">;</span> i<span class="op">++)</span> <span class="op">{</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>    ei_printf<span class="op">(</span><span class="st">"  </span><span class="sc">%s</span><span class="st">: </span><span class="sc">%.5f\r\n</span><span class="st">"</span><span class="op">,</span></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>              ei_classifier_inferencing_categories<span class="op">[</span>i<span class="op">],</span></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>              result<span class="op">.</span>classification<span class="op">[</span>i<span class="op">].</span>value<span class="op">);</span></span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Prints the classification results showing confidence scores for each category.</p>
</section>
<section id="ei_camera_init---smart-camera-initialization" class="level5">
<h5 class="anchored" data-anchor-id="ei_camera_init---smart-camera-initialization"><code>ei_camera_init()</code> - Smart Camera Initialization</h5>
<p>This function implements an intelligent initialization sequence:</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode cpp code-with-copy"><code class="sourceCode cpp"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="dt">bool</span> ei_camera_init<span class="op">(</span><span class="dt">void</span><span class="op">)</span> <span class="op">{</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">// Try Configuration 1 (OV2640 common)</span></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>    update_camera_config<span class="op">(</span><span class="dv">1</span><span class="op">);</span></span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>    <span class="dt">esp_err_t</span> err <span class="op">=</span> esp_camera_init<span class="op">(&amp;</span>camera_config<span class="op">);</span></span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="op">(</span>err <span class="op">==</span> ESP_OK<span class="op">)</span> <span class="cf">goto</span> camera_init_success<span class="op">;</span></span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>    <span class="co">// Try Configuration 2 (OV3660)</span></span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a>    esp_camera_deinit<span class="op">();</span></span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a>    update_camera_config<span class="op">(</span><span class="dv">2</span><span class="op">);</span></span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a>    err <span class="op">=</span> esp_camera_init<span class="op">(&amp;</span>camera_config<span class="op">);</span></span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="op">(</span>err <span class="op">==</span> ESP_OK<span class="op">)</span> <span class="cf">goto</span> camera_init_success<span class="op">;</span></span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a>    <span class="co">// Continue trying other configurations...</span></span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>The function:</p>
<ol type="1">
<li>Tries multiple pin configurations</li>
<li>Tests different clock frequencies (10MHz or 16MHz)</li>
<li>Attempts PSRAM first, then falls back to DRAM</li>
<li>Applies sensor-specific settings based on detected hardware</li>
</ol>
</section>
<section id="ei_camera_capture---image-processing-pipeline" class="level5">
<h5 class="anchored" data-anchor-id="ei_camera_capture---image-processing-pipeline"><code>ei_camera_capture()</code> - Image Processing Pipeline</h5>
<div class="sourceCode" id="cb11"><pre class="sourceCode cpp code-with-copy"><code class="sourceCode cpp"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="dt">bool</span> ei_camera_capture<span class="op">(</span><span class="dt">uint32_t</span> img_width<span class="op">,</span> <span class="dt">uint32_t</span> img_height<span class="op">,</span> <span class="dt">uint8_t</span> <span class="op">*</span>out_buf<span class="op">)</span> <span class="op">{</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">// 1. Get frame from camera</span></span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>    <span class="dt">camera_fb_t</span> <span class="op">*</span>fb <span class="op">=</span> esp_camera_fb_get<span class="op">();</span></span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>    <span class="co">// 2. Convert JPEG to RGB888 format</span></span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>    <span class="dt">bool</span> converted <span class="op">=</span> fmt2rgb888<span class="op">(</span>fb<span class="op">-&gt;</span>buf<span class="op">,</span> fb<span class="op">-&gt;</span>len<span class="op">,</span> PIXFORMAT_JPEG<span class="op">,</span> snapshot_buf<span class="op">);</span></span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a>    <span class="co">// 3. Return frame buffer to camera driver</span></span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a>    esp_camera_fb_return<span class="op">(</span>fb<span class="op">);</span></span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a>    <span class="co">// 4. Resize if needed</span></span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="op">(</span>do_resize<span class="op">)</span> <span class="op">{</span></span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a>        ei<span class="op">::</span>image<span class="op">::</span>processing<span class="op">::</span>crop_and_interpolate_rgb888<span class="op">(...);</span></span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a>    <span class="op">}</span></span>
<span id="cb11-15"><a href="#cb11-15" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>This function:</p>
<ol type="1">
<li>Captures a JPEG image from the camera</li>
<li>Converts it to RGB888 format (required by the ML model)</li>
<li>Resizes the image to match the model’s input size (96x96 pixels)</li>
</ol>
</section>
</section>
</section>
<section id="inference" class="level3">
<h3 class="anchored" data-anchor-id="inference">Inference</h3>
<ul>
<li>Upload the code to the XIAO ESP32S3 Sense.</li>
</ul>
<blockquote class="blockquote">
<p>⚠️ <strong>Attention</strong></p>
<ul>
<li>The Xiao ESP32S3 <strong>MUST</strong> have the PSRAM enabled. You can check it on the Arduino IDE upper menu: <code>Tools</code>–&gt; <code>PSRAM:OPI PSRAM</code></li>
<li>The Arduino Library (<code>esp32 by Espressif Systems</code> should be <strong>version 2.017</strong>. Do not update it)</li>
</ul>
</blockquote>
<p> <img src="./images/png/ard-ide.png" class="img-fluid quarto-figure quarto-figure-center" style="width:90.0%"></p>
<ul>
<li>Open the Serial Monitor</li>
<li>Point the camera at the objects, and check the result on the Serial Monitor.</li>
</ul>
<p> <img src="./images/png/ard-inf.png" class="img-fluid quarto-figure quarto-figure-center" style="width:90.0%"></p>
</section>
<section id="post-processing-2" class="level3">
<h3 class="anchored" data-anchor-id="post-processing-2">Post-Processing</h3>
<p>In edge AI applications, the inference result is only as valuable as our ability to act upon it. While serial output provides detailed information for debugging and development, real-world deployments require immediate, human-readable feedback that doesn’t depend on external monitors or connections.</p>
<p>The XIAOML Kit tiny 0.42” OLED display (72×40 pixels) serves as a crucial post-processing component that transforms raw ML inference results into immediate, human-readable feedback—displaying detected class names and confidence levels directly on the device, eliminating the need for external monitors and enabling truly standalone edge AI deployment in industrial, agricultural, or retail environments where instant visual confirmation of AI predictions is essential.</p>
<p>So, let’s modify the sketch to automatically adapt to the model trained on Edge Impulse by reading the class names and count directly from the model. The display will show abbreviated class names (3 letters) with larger fonts for better visibility on the tiny 72x40 pixel display. Download the code from the GitHub: <a href="https://github.com/Mjrovai/XIAO-ESP32S3-Sense/tree/main/XIAOML_Kit_code/XIAOML-Kit-Img_Class_OLED_Gen">XIAOML-Kit-Img_Class_OLED_Gen</a>.</p>
<p>Running the code, we can see the result:</p>
<p> <img src="./images/png/ard-inher-gen-oled.png" class="img-fluid quarto-figure quarto-figure-center" style="width:90.0%"></p>
</section>
</section>
<section id="summary" class="level2">
<h2 class="anchored" data-anchor-id="summary">Summary</h2>
<p>The XIAO ESP32S3 Sense is a remarkably capable and flexible platform for image classification applications. Through this lab, we’ve explored two distinct development approaches that cater to different skill levels and project requirements.</p>
<ul>
<li><p>The <strong>SenseCraft AI Studio</strong> provides an accessible entry point with its <strong>no-code interface</strong>, enabling rapid prototyping and deployment of pre-trained models like person detection. With real-time inference and integrated post-processing capabilities, it demonstrates how AI can be deployed without extensive programming or ML knowledge.</p></li>
<li><p>For more advanced applications, <strong>Edge Impulse Studio</strong> offers comprehensive machine learning pipeline tools, including custom dataset management, transfer learning with several pre-trained models, such as MobileNet, and model optimization.</p></li>
</ul>
<p>Key insights from this lab include the importance of image resolution trade-offs, the effectiveness of transfer learning for small datasets, and the practical considerations of edge AI deployment, such as power consumption and memory constraints.</p>
<p>The Lab demonstrates fundamental TinyML principles that extend beyond this specific hardware: resource-constrained inference, real-time processing requirements, and the complete pipeline from data collection through model deployment to practical applications. With built-in post-processing capabilities including GPIO control and communication protocols, the XIAO serves as more than just an inference engine—it becomes a complete AI sensor platform.</p>
<p>This foundation in image classification prepares you for more complex computer vision tasks while showcasing how modern edge AI makes sophisticated computer vision accessible, cost-effective, and deployable in real-world embedded applications ranging from industrial automation to smart home systems.</p>
</section>
<section id="resources" class="level2">
<h2 class="anchored" data-anchor-id="resources">Resources</h2>
<ul>
<li><a href="https://wiki.seeedstudio.com/xiao_esp32s3_getting_started/">Getting Started with the XIAO ESP32S3</a></li>
<li><a href="https://sensecraft.seeed.cc/ai/home">SenseCraft AI Studio Home</a></li>
<li><a href="https://sensecraft.seeed.cc/ai/device/local/32">SenseCraft Vision Workspace</a></li>
<li><a href="https://www.kaggle.com/kritikseth/fruit-and-vegetable-image-recognition">Dataset example</a></li>
<li><a href="https://studio.edgeimpulse.com/public/757065/live">Edge Impulse Project</a></li>
<li><a href="https://wiki.seeedstudio.com/sensecraft-ai/tutorials/sensecraft-ai-output-libraries-xiao/">XIAO as an AI Sensor</a></li>
<li><a href="https://github.com/Seeed-Studio/Seeed_Arduino_SSCMA">Seeed Arduino SSCMA Library</a></li>
<li><a href="https://github.com/Mjrovai/XIAO-ESP32S3-Sense/tree/main/XIAOML_Kit_code">XIAOML Kit Code</a></li>
</ul>


</section>
</section>

<a onclick="window.scrollTo(0, 0); return false;" role="button" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> Back to top</a></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
            // target, if specified
            link.setAttribute("target", "_blank");
            if (link.getAttribute("rel") === null) {
              link.setAttribute("rel", "noopener");
            }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../../../../../contents/labs/seeed/xiao_esp32s3/setup/setup.html" class="pagination-link" aria-label="Setup">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">Setup</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../../../../../contents/labs/seeed/xiao_esp32s3/object_detection/object_detection.html" class="pagination-link" aria-label="Object Detection">
        <span class="nav-page-text">Object Detection</span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>© 2024 Harvard University. Licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC-BY-NC-SA 4.0</a></p>
</div>   
    <div class="nav-footer-center">
<p>Written, edited and curated by Prof.&nbsp;Vijay Janapa Reddi (Harvard University)</p>
</div>
    <div class="nav-footer-right">
      <ul class="footer-items list-unstyled">
    <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/harvard-edge/cs249r_book">
      <i class="bi bi-github" role="img" aria-label="View source on GitHub">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/harvard-edge/cs249r_book">
      <i class="bi bi-star" role="img" aria-label="Star this repository">
</i> 
    </a>
  </li>  
</ul>
    </div>
  </div>
</footer>
<script>var lightboxQuarto = GLightbox({"closeEffect":"zoom","descPosition":"bottom","loop":false,"openEffect":"zoom","selector":".lightbox"});
(function() {
  let previousOnload = window.onload;
  window.onload = () => {
    if (previousOnload) {
      previousOnload();
    }
    lightboxQuarto.on('slide_before_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      const href = trigger.getAttribute('href');
      if (href !== null) {
        const imgEl = window.document.querySelector(`a[href="${href}"] img`);
        if (imgEl !== null) {
          const srcAttr = imgEl.getAttribute("src");
          if (srcAttr && srcAttr.startsWith("data:")) {
            slideConfig.href = srcAttr;
          }
        }
      } 
    });
  
    lightboxQuarto.on('slide_after_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(slideNode);
      }
    });
  
  };
  
})();
          </script>




</body></html>