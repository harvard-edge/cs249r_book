--- 
bibliography: data_engineering.bib
--- 

# Data Engineering {#sec-data_engineering}

::: {.content-visible when-format="html"}
Resources: [Slides](#sec-data-engineering-resource), [Labs](#sec-data-engineering-resource), [Exercises](#sec-data-engineering-resource)
:::

![_DALL·E 3 Prompt: Create a rectangular illustration visualizing the concept of data engineering. Include raw data sources, data processing pipelines, storage systems, and refined datasets. Show how raw data is transformed through cleaning, processing, and storage to become valuable information that can be analyzed and used for decision-making._](images/png/cover_data_engineering.png)

Data is the lifeblood of AI systems. Without good data, even the most advanced machine-learning algorithms will not succeed. However, TinyML models operate on devices with limited processing power and memory. This section explores the intricacies of building high-quality datasets to fuel our AI models. Data engineering involves collecting, storing, processing, and managing data to train machine learning models.


::: {.callout-tip}

## Learning Objectives

* Understand the importance of clearly defining the problem statement and objectives when embarking on an ML project.

* Recognize various data sourcing techniques, such as web scraping, crowdsourcing, and synthetic data generation, along with their advantages and limitations.

* Appreciate the need for thoughtful data labeling, using manual or AI-assisted approaches, to create high-quality training datasets.

* Briefly learn different methods for storing and managing data, such as databases, data warehouses, and data lakes.

* Comprehend the role of transparency through metadata and dataset documentation and tracking data provenance to facilitate ethics, auditing, and reproducibility.

* Understand how licensing protocols govern legal data access and usage, necessitating careful compliance.

* Recognize key challenges in data engineering, including privacy risks, representation gaps, legal restrictions around data access, and balancing competing priorities.

:::


## Introduction
Imagine a world where AI can diagnose diseases with unprecedented accuracy, but only if the data used to train it is unbiased and reliable. This is where data engineering comes in. While over 90% of the world's data has been created in the past two decades, this vast amount of information is only helpful for building effective AI models with proper processing and preparation. Data engineering bridges this gap by transforming raw data into a high-quality format that fuels AI innovation.
In today's data-driven world, protecting user privacy is paramount. Whether mandated by law or driven by user concerns, anonymization techniques like differential privacy and aggregation are vital in mitigating privacy risks. However, careful implementation is crucial to ensure these methods don't compromise data utility. Dataset creators face complex privacy and representation challenges when building high-quality training data, especially for sensitive domains like healthcare. Legally, creators may need to remove direct identifiers like names and ages. Even without legal obligations, removing such information can help build user trust. However, excessive anonymization can compromise dataset utility. Techniques like differential privacy$^{1}$, aggregation, and reducing detail provide alternatives to balance privacy and utility but have downsides. Creators must strike a thoughtful balance based on the use case.

While privacy is paramount, ensuring fair and robust AI models requires addressing representation gaps in the data. It is crucial yet insufficient to ensure diversity across individual variables like gender, race, and accent. These combinations, sometimes called higher-order gaps, can significantly impact model performance. For example, a medical dataset could have balanced gender, age, and diagnosis data individually, but it lacks enough cases to capture older women with a specific condition. Such [higher-order gaps](https://blog.google/technology/health/healthcare-ai-systems-put-people-center/) are not immediately obvious but can critically impact model performance.

Creating useful, ethical training data requires holistic consideration of privacy risks and representation gaps. Elusive perfect solutions necessitate conscientious data engineering practices like anonymization, aggregation, under-sampling of overrepresented groups, and synthesized data generation to balance competing needs. This facilitates models that are both accurate and socially responsible. Cross-functional collaboration and external audits can also strengthen training data. The challenges are multifaceted but surmountable with thoughtful effort.
We begin by discussing data collection: Where do we source data, and how do we gather it? Options range from scraping the web, accessing APIs, and utilizing sensors and IoT devices to conducting surveys and gathering user input. These methods reflect real-world practices. Next, we delve into data labeling, including considerations for human involvement. We'll discuss the trade-offs and limitations of human labeling and explore emerging methods for automated labeling. Following that, we'll address data cleaning and preprocessing, a crucial yet frequently undervalued step in preparing raw data for AI model training. Data augmentation comes next, a strategy for enhancing limited datasets by generating synthetic samples. This is particularly pertinent for embedded systems, as many use cases need extensive data repositories readily available for curation. Synthetic data generation emerges as a viable alternative with advantages and disadvantages. We'll also touch upon dataset versioning, emphasizing the importance of tracking data modifications over time. Data is ever-evolving; hence, it's imperative to devise strategies for managing and storing expansive datasets. By the end of this section, you'll possess a comprehensive understanding of the entire data pipeline, from collection to storage, essential for operationalizing AI systems. Let's embark on this journey!

## Problem Definition

In many machine learning domains, sophisticated algorithms take center stage, while the fundamental importance of data quality is often overlooked. This neglect gives rise to ["Data Cascades"](https://research.google/pubs/pub49953/) by @Data_Cascades_2021 (see @fig-cascades)—events where lapses in data quality compound, leading to negative downstream consequences such as flawed predictions, project terminations, and even potential harm to communities. In @fig-cascades, we illustrate potential data pitfalls at every stage and how they influence the entire process down the line. The influence of data collection errors is especially pronounced. Any lapses in this stage will become apparent at later stages (in model evaluation and deployment) and might lead to costly consequences, such as abandoning the entire model and restarting anew. Therefore, investing in data engineering techniques from the onset will help us detect errors early.

![Data cascades: compounded costs. Credit: @Data_Cascades_2021.](images/png/data_engineering_cascades.png){#fig-cascades}

Despite many ML professionals recognizing the importance of data, numerous practitioners report facing these cascades. This highlights a systemic issue: while the allure of developing advanced models remains, data often needs to be more appreciated.

Take, for example, Keyword Spotting (KWS) (see @fig-keywords). KWS is a prime example of TinyML in action and is a critical technology behind voice-enabled interfaces on endpoint devices such as smartphones. Typically functioning as lightweight wake-word engines, these systems are consistently active, listening for a specific phrase to trigger further actions. When we say "OK, Google" or "Alexa," this initiates a process on a microcontroller embedded within the device. Despite their limited resources, these microcontrollers play an important role in enabling seamless voice interactions with devices, often operating in environments with high ambient noise. The uniqueness of the wake word helps minimize false positives, ensuring that the system is not triggered inadvertently.

It is important to appreciate that these keyword-spotting technologies are not isolated; they integrate seamlessly into larger systems, processing signals continuously while managing low power consumption. These systems extend beyond simple keyword recognition, evolving to facilitate diverse sound detections, such as glass breaking. This evolution is geared towards creating intelligent devices capable of understanding and responding to vocal commands, heralding a future where even household appliances can be controlled through voice interactions.

![Keyword Spotting example: interacting with Alexa. Credit: Amazon.](images/png/data_engineering_kws.png){#fig-keywords}

Building a reliable KWS model is a complex task. It demands a deep understanding of the deployment scenario, encompassing where and how these devices will operate. For instance, a KWS model's effectiveness is not just about recognizing a word; it's about discerning it among various accents and background noises, whether in a bustling cafe or amid the blaring sound of a television in a living room or a kitchen where these devices are commonly found. It's about ensuring that a whispered "Alexa" in the dead of night or a shouted "OK Google" in a noisy marketplace are recognized with equal precision.

Moreover, many current KWS voice assistants support a limited number of languages, leaving a substantial portion of the world's linguistic diversity unrepresented. This limitation is partly due to the difficulty in gathering and monetizing data for languages spoken by smaller populations. The long-tail distribution of languages implies that many languages have limited data, making the development of supportive technologies challenging.

This level of accuracy and robustness hinges on the availability and quality of data, the ability to label the data correctly, and the transparency of the data for the end user before it is used to train the model. However, it all begins with clearly understanding the problem statement or definition.

Generally, in ML, problem definition has a few key steps:

1. Identifying the problem definition clearly

2. Setting clear objectives

3. Establishing success benchmark

4. Understanding end-user engagement/use

5. Understanding the constraints and limitations of deployment

6. Followed by finally doing the data collection.

A solid project foundation is essential for its trajectory and eventual success. Central to this foundation is first identifying a clear problem, such as ensuring that voice commands in voice assistance systems are recognized consistently across varying environments. Establishing clear objectives, such as creating representative datasets for diverse scenarios, provides a unified direction. Benchmarks, like system accuracy in keyword detection, offer measurable outcomes to gauge progress. Engaging with stakeholders, from end-users to investors, provides invaluable insights and ensures alignment with market needs.

Understanding platform constraints is also pivotal when delving into areas like voice assistance. Embedded systems, such as microcontrollers, have inherent processing power, memory, and energy efficiency limitations. Recognizing these limitations is especially crucial for data engineers, as it impacts data collection, pre-processing, and model training on these devices. Functionalities like keyword detection must be tailored to operate optimally, balancing performance with resource conservation while ensuring high data quality.
In this context, using KWS as an example, we can break each of the steps out as follows:

1. **Identifying the Problem:**
In the field of TinyML, Keyword Spotting (KWS) plays a crucial role in enabling intelligent voice interactions on devices with limited computational resources. The core challenge involves designing a KWS system capable of accurately detecting specific keywords, such as wake words or commands, amid background noise and other spoken words. Achieving this necessitates setting clear objectives that consider the constraints of TinyML devices.
2. **Setting Clear Objectives:**
Designing a KWS system for TinyML involves navigating trade-offs between various critical factors. Key objectives include:
* ** High Accuracy: ** Ensuring the system accurately identifies keywords, often aiming for a specific accuracy rate (e.g., 98%).
* ** Seamless User Experience: ** Minimizing detection time to maintain fluid interactions, with a common target being keyword detection and response within 200 milliseconds.
* ** Power Efficiency: ** Reducing power consumption to extend battery life on embedded devices is crucial for many TinyML applications.
* ** Optimized Model Size: ** Due to TinyML devices' inherent memory limitations, it is essential to adjust the model's size to fit within the device's memory constraints.
These objectives are theoretical concepts crucial for successfully deploying KWS systems in real-world TinyML applications. They ensure the systems are both practical and efficient within the constraints of the devices they operate on. The following sections will delve deeper into the trade-offs between these objectives, providing a comprehensive understanding of the design considerations involved.

3. **Benchmarks for Success:**
   Establishing clear metrics to evaluate the performance of the KWS system is essential for ensuring its effectiveness and efficiency. These metrics should directly relate to the objectives outlined earlier. Common metrics include:
* **True Positive Rate (TPR): ** The percentage of correctly identified keywords. This metric reflects the system's ability to detect target keywords accurately.
* **False Positive Rate (FPR):  ** The percentage of non-keywords incorrectly identified as keywords. A low FPR is crucial to avoid unnecessary system activations.
* **Response Time: ** The time between keyword utterance and system response elapsed. This metric is critical for a seamless user experience.
* **Power Consumption: ** This is the average power consumption during keyword detection. Minimizing power consumption is essential for battery-powered embedded devices.
Systematically measuring these metrics ensures the KWS system meets real-world application performance standards.

4. **Stakeholder Engagement and Understanding:**
   Engaging actively with stakeholders throughout the KWS development process is crucial. Stakeholders typically include device manufacturers, hardware and software developers, and end-users. Understanding their needs, capabilities, and constraints is vital for designing a successful system.
* Device manufacturers might prioritize low power consumption to extend battery life.
*Software developers might emphasize ease of integration with existing software frameworks.
*End-users would prioritize high accuracy and responsiveness for a positive user experience.
*Effective stakeholder engagement ensures the KWS system meets all parties' diverse requirements.

5. **Understanding the Constraints and Limitations of Embedded Systems:**
Embedded devices present unique challenges for KWS system design. Key constraints include:

**Memory Limitations: ** KWS models must be lightweight to fit within the limited memory of embedded devices. Typically, models need to be as small as 16KB to operate within the always-on island of the System-on-Chip (SoC). This includes the model size and any additional application code for preprocessing.
* **Processing Power: ** The computational capabilities of embedded devices are often limited (a few hundred MHz of clock speed). Therefore, the KWS model must be optimized for efficient execution on these resource-constrained devices.
* **Power Consumption: ** Since many embedded devices are battery-powered, the KWS system must be power-efficient to maximize battery life.
* **Environmental Challenges: ** Devices might be deployed in various environments, from quiet bedrooms to noisy industrial settings. The KWS system must be robust enough to function effectively across these diverse scenarios.
Addressing these constraints is essential for developing a functional and reliable KWS system for embedded devices.

6. **Data Collection and Analysis:**
The quality and diversity of data are paramount for training a successful KWS system. Key considerations include:
*Variety of Accents: Collect data from speakers with various accents to ensure the system can recognize keywords spoken in different dialects.
*Background Noises: Include data samples with different ambient noises (e.g., traffic, music) to train the model for real-world scenarios.
*Keyword Variations: People might pronounce keywords differently or have slight variations in the wake word itself. Ensure the dataset captures these nuances to improve recognition accuracy.
Comprehensive data collection and analysis are foundational to the robustness of the KWS system.

7. **Iterative Feedback and Refinement:**
    Developing a KWS system is an iterative process. Once a prototype system is created, it's crucial to test it in real-world scenarios, gather feedback from users and stakeholders, and iteratively refine the model. This ensures that the system remains significantly aligned with the defined problem and objectives as deployment scenarios and user needs evolve.

:::{#exr-kws .callout-exercise collapse="true"}

### Keyword Spotting with TensorFlow Lite Micro

Explore a hands-on guide for building and deploying Keyword Spotting (KWS) systems using TensorFlow Lite Micro. Follow steps from data collection to model training and deployment to microcontrollers. Learn to create efficient KWS models that recognize specific keywords amidst background noise. Perfect for those interested in machine learning on embedded systems. Unlock the potential of voice-enabled devices with TensorFlow Lite Micro!

[![](https://colab.research.google.com/assets/colab-badge.png)](https://colab.research.google.com/drive/17I7GL8WTieGzXYKRtQM2FrFi3eLQIrOM)
:::

The current chapter underscores the essential role of data quality in ML, using Keyword Spotting (KWS) systems as an example. It outlines key steps, from problem definition to stakeholder engagement, emphasizing iterative feedback. The forthcoming chapter will delve deeper into data quality management, discussing its consequences and future trends, focusing on the importance of high-quality, diverse data in AI system development, addressing ethical considerations and data sourcing methods.

## Data Sourcing

The quality and diversity of data gathered are essential for developing accurate and robust AI systems, particularly for resource-constrained TinyML applications. Sourcing high-quality training data requires careful consideration of objectives, resource limitations, and ethical implications. Data can be obtained from various sources depending on the needs of the project:

### Pre-existing datasets

Platforms like [Kaggle](https://www.kaggle.com/) and [UCI Machine Learning Repository](https://archive.ics.uci.edu/) provide a convenient starting point. Pre-existing datasets offer several advantages:
* **Cost Efficiency: **Creating a dataset from scratch can be time-consuming and expensive. Pre-existing datasets can save significant resources.
* **Standard Benchmarks: ** Many datasets, like [ImageNet](https://www.image-net.org/), have become standard benchmarks, allowing consistent performance comparisons across different models and algorithms. This data availability means experiments can be started immediately without data collection and preprocessing delays. In a fast-moving field like ML, this practicality is essential.
* **Quality Assurance: **The widespread use of popular datasets often leads to their quality assurance, as the community frequently identifies and rectifies errors and biases. For instance, [the ImageNet dataset was found to have over 6.4% errors](https://arxiv.org/abs/2103.14749). This benefits students and newcomers, as they can focus on learning and experimentation without worrying about data integrity. Supporting documentation often accompanying existing datasets is invaluable, though this generally applies only to widely used datasets. Good documentation provides insights into the data collection process and variable definitions and sometimes even offers baseline model performances. This information aids understanding and promotes reproducibility in research, a cornerstone of scientific integrity; currently, there is a crisis around [improving reproducibility in machine learning systems](https://arxiv.org/abs/2003.12206).  When other researchers have access to the same data, they can validate findings, test new hypotheses, or apply different methodologies, thus allowing us to build on each other's work more rapidly.
However, it's crucial to understand the context in which pre-existing data was collected. Researchers should consider:
* **Overfitting: ** Popular datasets may lead to overfitting if multiple models are trained on them, inflating performance metrics.
***Real-World Representation: **Datasets may not always reflect real-world data distributions. Sometimes, these [datasets do not reflect the real-world data](https://venturebeat.com/uncategorized/3-big-problems-with-datasets-in-ai-and-machine-learning/).
* **Bias and Reproducibility: **Biases, validity, and reproducibility issues may exist in these datasets, and there has been a growing awareness of these issues in recent years. Furthermore, using the same dataset to train different models, as shown in @fig-misalignment, can sometimes create misalignment: training multiple models using the same dataset results in a 'misalignment' between the models and the world, in which an entire ecosystem of models reflects only a narrow subset of the real-world data.

![Training different models on the same dataset. Credit: (icons from left to right: Becris; Freepik; Freepik; Paul J; SBTS2018).](images/png/dataset_myopia.png){#fig-misalignment}


### Web Scraping

Web scraping refers to automated techniques for extracting data from websites. It typically involves sending HTTP requests to web servers, retrieving HTML content, and parsing that content to extract relevant information. Popular tools and frameworks for web scraping include Beautiful Soup, Scrapy, and Selenium. These tools offer different functionalities, from parsing HTML content to automating web browser interactions, especially for websites that load content dynamically using JavaScript.
Web scraping can effectively gather large datasets, particularly when human-labeled data is scarce. Here are some use cases:
* **Computer Vision: **Web scraping has enabled the collection of massive images and videos for tasks like object recognition. Examples include datasets like ImageNet](https://www.image-net.org/) and [OpenImages](https://storage.googleapis.com/openimages/web/index.html). For example, one could scrape e-commerce sites to amass product photos for object recognition or social media platforms to collect user uploads for facial analysis. Even before ImageNet, Stanford’s[LabelMe](https://people.csail.mit.edu/torralba/publications/labelmeApplications.pdf) project scraped Flickr for over 63,000 annotated images covering hundreds of object categories.
* **Natural Language Processing: **Researchers can scrape news sites, forums, or social media for tasks like sentiment analysis, dialogue systems research, or topic modeling. For example, the training data for chatbot ChatGPT was obtained by scraping much of the public Internet. GitHub repositories were scraped to train GitHub's Copilot AI coding assistant.
* **Structured Data:  **Web scraping can collect structured data like stock prices, weather data, or product information for analytical applications. Once data is scraped, storing it structured is essential, often using databases or data warehouses. Proper data management ensures the usability of the scraped data for future analysis and applications.
However, web scraping has limitations and ethical considerations:
* **Legal Restrictions:  **Not all websites permit scraping, and violating these restrictions can lead to legal repercussions. Scraping copyrighted material or private communications is also unethical and potentially illegal. Ethical web scraping mandates adherence to a website's robots.txt file and respecting rate limits.
* **Dynamic Content:  **The dynamic nature of web content can challenge consistency, especially for longitudinal studies. However, emerging trends like [Web Navigation](https://arxiv.org/abs/1812.09195) leverage machine learning for navigating dynamic content.
* **Data Quality: **The volume of pertinent data available for scraping might be limited for niche subjects. For example, while scraping for common topics like images of cats and dogs might yield abundant data, searching for rare medical conditions might be less fruitful. Moreover, the data obtained through scraping is often unstructured and noisy, necessitating thorough preprocessing and cleaning. It is crucial to understand that not all scraped data will be of high quality or accuracy. Employing verification methods, such as cross-referencing with alternate data sources, can enhance data reliability.
* **Data Quality Considerations: **
* ***Scarcity for Niche Domains: ** The volume of relevant data obtainable through scraping can be limited for specialized areas. This might restrict the applicability of web scraping for specific TinyML projects requiring data on less common subjects.
* ***Unstructured and Noisy Data: **Scraped data is often unstructured and requires significant preprocessing and cleaning before effectively training TinyML models. This cleaning process can be resource-intensive, especially for TinyML environments with limited computational power.
* ***Inconsistency and Accuracy: **The accuracy and consistency of scraped data can vary, potentially impacting the performance of TinyML models. Verification methods, such as cross-referencing with established datasets, become crucial for enhancing data reliability.

* **Privacy Concerns: **Anonymization is crucial when scraping personal data. It's paramount to adhere to the website's Terms of Service, confine data collection to public domains, and ensure the anonymity of any personal data acquired.
* ** Data Applicability:  **Web scraping might not be suitable for collecting sensor data required by TinyML applications. Additionally, scraped data can be inconsistent or inaccurate.
* **Scalability: **Web scraping can be a scalable method to amass large training datasets for AI systems, but its applicability is confined to specific data types. For example, web scraping makes sourcing data for Inertial Measurement Units (IMU) for gesture recognition more complex. At most, one can scrape an existing dataset.
* **Inconsistency: **Web scraping can yield inconsistent or inaccurate data. For example, the photo in @fig-traffic-light shows up when you search for 'traffic light' on Google Images. It is an image from 1914 that shows outdated traffic lights, which are also barely discernible because of the image's poor quality. This can be problematic for web-scraped datasets, as it pollutes the dataset with inapplicable (old) data samples.

This concept is explored in greater detail in a hands-on exercise available online, where you can discover the power of web scraping with Python using libraries like Beautiful Soup and Pandas.
![A picture of old traffic lights (1914). Credit: [Vox.](https://www.vox.com/2015/8/5/9097713/when-was-the-first-traffic-light-installed)](images/jpg/1914_traffic.jpeg){#fig-traffic-light}

:::{#exr-ws .callout-exercise collapse="true"}

### Web Scraping

Discover the power of web scraping with Python using libraries like Beautiful Soup and Pandas. This exercise will scrape Python documentation for function names and descriptions and explore NBA player stats. By the end, you'll have the skills to extract and analyze data from real-world websites. Ready to dive in? Access the Google Colab notebook below and start practicing!  

[![](https://colab.research.google.com/assets/colab-badge.png)](https://colab.research.google.com/github/Andy-Pham-72/Web-Scraping-with-BeautifulSoup-and-Pandas/blob/master/Web_scraping_with_beautiful_soup_and_pandas_complete.ipynb)
::: 

### Crowdsourcing

Crowdsourcing for Data Collection in TinyML
Crowdsourcing for datasets involves obtaining data through the collective efforts of a vast, distributed group of participants, typically via the Internet. This method leverages the services of many people, either from specific communities or the general public, rather than relying on a small team or a specific organization to collect or label data. Platforms like Amazon Mechanical Turk facilitate the distribution of annotation tasks to a large, diverse workforce, enabling the collection of labels for complex tasks such as sentiment analysis or image recognition that require human judgment.
Crowdsourcing has emerged as an effective approach for data collection and problem-solving. One major advantage is scalability—by distributing tasks to a global pool of contributors on digital platforms, projects can quickly process huge volumes of data. This makes crowdsourcing ideal for large-scale data labeling, collection, and analysis.
In addition, crowdsourcing taps into a diverse group of participants, bringing a wide range of perspectives, cultural insights, and language abilities that can enrich data and enhance creative problem-solving in ways that a more homogenous group may not. Because crowdsourcing draws from a large audience beyond traditional channels, it is often more cost-effective than conventional methods, especially for simpler microtasks.
Crowdsourcing platforms provide significant flexibility, as task parameters can be adjusted in real-time based on initial results. This creates a feedback loop for iterative improvements to the data collection process. Complex jobs can be broken down into microtasks and distributed to multiple people, with results cross-validated by assigning redundant versions of the same task. When thoughtfully managed, crowdsourcing enables community engagement around a collaborative project, where participants find reward in contributing.
* **Advantages for Specific Scenarios: ** While TinyML applications often require specialized sensor data, crowdsourcing can be advantageous for tasks where human perception and subjective labeling are crucial. For example, crowdsourcing could be suitable for collecting data to train TinyML models for tasks like audio anomaly detection, where human judgment is valuable in identifying unusual sounds.
* **Considerations for Effective Crowdsourcing: ** While crowdsourcing offers numerous advantages, it's essential to approach it with a clear strategy. Access to a diverse set of annotators introduces variability in the quality of annotations. Platforms like Mechanical Turk might not always capture a complete demographic spectrum; often, tech-savvy individuals are overrepresented, while children and older people may be underrepresented. Providing clear instructions and training for annotators is crucial. Periodic checks and validations of the labeled data help maintain quality. This ties back to the topic of clear problem definition that we discussed earlier. Crowdsourcing for datasets also requires careful attention to ethical considerations. It's crucial to ensure that participants are informed about how their data will be used and that their privacy is protected. Quality control through detailed protocols, transparency in sourcing, and auditing is essential to ensure reliable outcomes.
* **Challenges Specific to TinyML: ** For TinyML, crowdsourcing presents unique challenges due to the specialized nature of TinyML devices, which are designed for particular tasks within tight constraints:
* Specialized Data Requirements: TinyML applications often rely on data collected from specific sensors or hardware. Crowdsourcing such specialized data from a general audience may be challenging. For example, participants need access to specific devices, such as microphones with consistent sampling rates, to collect relevant audio data for keyword spotting.
* High Granularity and Quality: Given TinyML's limitations, the data must be highly granular and high-quality. Ensuring this level of detail from crowdsourcing participants unfamiliar with the application's context and requirements can be difficult.
* Privacy, Standardization, and Technical Expertise: Additional issues include maintaining privacy, real-time data collection, standardization, and the need for technical expertise to provide accurate data labeling.
* Narrow Task Focus: Many TinyML tasks are narrowly defined, making accurate data labeling easier with proper understanding. Participants may need full context to provide reliable annotations.
* **Careful Planning for Success: ** Thus, while crowdsourcing can work well in many cases, the specialized needs of TinyML introduce unique data challenges. Careful planning is required to set guidelines, target appropriate participants, and implement rigorous quality control. In some applications, crowdsourcing may be feasible, but others may require more focused data collection efforts to obtain relevant, high-quality training data. By understanding the advantages and limitations of crowdsourcing, researchers and developers can make informed decisions about its suitability for their specific TinyML data acquisition needs.

### Synthetic Data

Synthetic data generation offers a valuable approach for addressing data collection limitations, particularly when real-world data is scarce, expensive, or ethically challenging to acquire, as is often the case in TinyML applications. This technique involves creating data that wasn't originally captured or observed but is generated using algorithms, simulations, or other techniques to resemble real-world data closely. As illustrated in @fig-synthetic-data, synthetic data is merged with historical data and then used as input for model training. It has become a valuable tool in various fields, particularly when real-world data is scarce, expensive, or ethically challenging (e.g., TinyML). Various techniques, such as Generative Adversarial Networks (GANs), can produce high-quality synthetic data almost indistinguishable from real data. These techniques have advanced significantly, making synthetic data generation increasingly realistic and reliable.

Advantages of Synthetic Data for TinyML:

Addressing Data Scarcity: Many TinyML applications require particular datasets that may be difficult or expensive to collect in the real world. Synthetic data generation can overcome this challenge by producing large volumes of data that mimic real-world scenarios relevant to the TinyML task. For instance, consider a TinyML device designed for security applications that must identify breaking glass sounds. Gathering real-world data for such a task would require breaking numerous windows, which is impractical and costly. Synthetic data generation offers a viable alternative by creating realistic audio samples of breaking glass.
Enhancing Model Robustness: Diversity in datasets is crucial for effective machine learning, especially in deep learning models. Synthetic data can augment existing datasets by introducing variations in data points, thereby enhancing the robustness of models. For example, SpecAugment is a powerful data augmentation technique to improve automatic speech recognition (ASR) systems.
Privacy Preservation: Privacy and confidentiality are major concerns when dealing with sensitive or personal data datasets. Synthetic data, being artificially generated, doesn't have these direct ties to real individuals. This allows for safer use of data while preserving essential statistical properties relevant for model training.
Cost-Effectiveness: Once the generation mechanisms are established, synthetic data generation can be a more cost-effective alternative to traditional data collection methods. In the security application scenario mentioned earlier, synthetic data eliminates the need for expensive and impractical data collection involving breaking windows.
Control over Data Generation: Many embedded TinyML use cases deal with unique situations, such as those encountered in manufacturing plants, which can be difficult to simulate in real-world environments. Synthetic data allows researchers complete control over the data generation process, enabling the creation of specific scenarios or conditions that are challenging to capture in real life.
Considerations for Using Synthetic Data:

While synthetic data offers numerous advantages, it is essential to use it judiciously. Care must be taken to ensure that the generated data accurately represents the underlying real-world distributions and does not introduce unintended biases. Validation techniques are crucial to verify the quality and representativeness of the synthetic data before using it for model training.

Synthetic data generation is a powerful tool that complements traditional data collection methods for TinyML applications. By understanding the advantages and considerations associated with synthetic data, researchers and developers can leverage this technique to overcome data scarcity challenges and develop robust TinyML models.
![Increasing training data size with synthetic data generation. Credit: [AnyLogic](https://www.anylogic.com/features/artificial-intelligence/synthetic-data/).](images/jpg/synthetic_data.jpg){#fig-synthetic-data}

:::{#exr-sd .callout-exercise collapse="true"}

### Synthetic Data
Let us learn about synthetic data generation using Generative Adversarial Networks (GANs) on tabular data. We'll take a hands-on approach, diving into the workings of the CTGAN model and applying it to the Synthea dataset from the healthcare domain. From data preprocessing to model training and evaluation, we'll go step-by-step, learning how to create synthetic data, assess its quality, and unlock the potential of GANs for data augmentation and real-world applications.  

[![](https://colab.research.google.com/assets/colab-badge.png)](https://colab.research.google.com/drive/1nwbvkg32sOUC69zATCfXOygFUBeo0dsx?usp=sharing#scrollTo=TkwYknr44eFn)
:::

## Data Storage

Data sourcing and data storage go hand in hand, and data must be stored in a format that facilitates easy access and processing. Depending on the use case, various kinds of data storage systems can be used to store your datasets. Some examples are shown in @tbl-databases.

|                 **Database**        |    **Data Warehouse**    |    **Data Lake**       |
|------------------------------------|--------------------------|------------------------|
| **Purpose**                        | Operational and transactional | Analytical         |
| **Data type**                      | Structured              | Structured, semi-structured, and/or unstructured |
| **Scale**                          | Small to large volumes of data | Large volumes of integrated data | Large volumes of diverse data |
| **Examples**                       | MySQL | Google BigQuery, Amazon Redshift, Microsoft Azure Synapse, Google Cloud Storage, AWS S3, Azure Data Lake Storage |

  : Comparative overview of the database, data warehouse, and data lake. {#tbl-databases}

The stored data is often accompanied by metadata, defined as 'data about data .'It provides detailed contextual information about the data, such as means of data creation, time of creation, attached data use license, etc. For example, [[Hugging Face]{.underline}](https://huggingface.co/) has [[Dataset Cards]{.underline}](https://huggingface.co/docs/hub/datasets-cards). To promote responsible data use, dataset creators should disclose potential biases through the dataset cards. These cards can educate users about a dataset's contents and limitations. The cards also give vital context on appropriate dataset usage by highlighting biases and other important details. Having this type of metadata can also allow fast retrieval if structured properly. Once the model is developed and deployed to edge devices, the storage systems can continue to store incoming data, model updates, or analytical results.

**Data Governance:** With a large amount of data storage, it is also imperative to have policies and practices (i.e., data governance) that help manage data during its life cycle, from acquisition to disposal. Data governance frames how data is managed and includes making pivotal decisions about data access and control. @fig-governance illustrates the different domains involved in data governance. It involves exercising authority and making decisions concerning data to uphold its quality, ensure compliance, maintain security, and derive value. Data governance is operationalized by developing policies, incentives, and penalties, cultivating a culture that perceives data as a valuable asset. Specific procedures and assigned authorities are implemented to safeguard data quality and monitor its utilization and related risks.

Data governance utilizes three integrative approaches: planning and control, organizational, and risk-based.

* **The planning and control approach**, common in IT, aligns business and technology through annual cycles and continuous adjustments, focusing on policy-driven, auditable governance.

* **The organizational approach** emphasizes structure, establishing authoritative roles like Chief Data Officers and ensuring responsibility and accountability in governance.

* **The risk-based approach**, intensified by AI advancements, focuses on identifying and managing inherent risks in data and algorithms. It especially addresses AI-specific issues through regular assessments and proactive risk management strategies, allowing for incidental and preventive actions to mitigate undesired algorithm impacts.

![An overview of the data governance framework. Credit: [StarCIO.](https://www.groundwatergovernance.org/the-importance-of-governance-for-all-stakeholders/).](images/jpg/data_governance.jpg){#fig-governance}

Some examples of data governance across different sectors include:

* **Medicine:** [[Health Information Exchanges(HIEs)]{.underline}](https://www.healthit.gov/topic/health-it-and-health-information-exchange-basics/what-hie) enable the sharing of health information across different healthcare providers to improve patient care. They implement strict data governance practices to maintain data accuracy, integrity, privacy, and security, complying with regulations such as the [[Health Insurance Portability and Accountability Act (HIPAA)]{.underline}](https://www.cdc.gov/phlp/publications/topic/hipaa.html). Governance policies ensure that patient data is only shared with authorized entities and that patients can control access to their information.

* ** Finance: ** [[Basel III Framework] .underline] (https://www.bis.org/bcbs/basel3.htm) is an international regulatory framework for banks. It ensures that banks establish clear policies, practices, and responsibilities for data management, ensuring data accuracy, completeness, and timeliness. Not only does it enable banks to meet regulatory compliance, but it also prevents financial crises by more effectively managing risks.

* **Government:** Government agencies managing citizen data, public records, and administrative information implement data governance to manage data transparently and securely. The Social Security System in the US and the Aadhar system in India are good examples of such governance systems.

**Special data storage considerations for TinyML**

***Efficient Audio Storage Formats:*** Keyword spotting systems need specialized audio storage formats to enable quick keyword searching in audio data. Traditional formats like WAV and MP3 store full audio waveforms, which require extensive processing to search through. Keyword spotting uses compressed storage optimized for snippet-based search. One effective approach is storing compact acoustic features extracted from the raw audio, such as Mel-frequency cepstral coefficients (MFCCs), representing important audio characteristics.
Here's a breakdown of the workflow for efficient audio storage in TinyML:

* ** Feature Extraction:** Acoustic features like MFCCs are extracted from the raw audio data. These features capture essential characteristics of the sound, making them ideal for keyword spotting.

* ** Embedding Creation:** The extracted acoustic features are then transformed into low-dimensional vector embeddings. This process reduces the data size significantly while preserving the information crucial for keyword detection. Vector embeddings allow for more efficient storage and computation on resource-constrained devices.

* ** Vector Quantization:** High-dimensional data, like embeddings, are represented with lower-dimensional vectors through vector quantization, reducing storage needs. Initially, a codebook is generated from the training data to define a set of code vectors representing the original data vectors. Each data vector is subsequently matched to the nearest codeword according to the codebook, ensuring minimal information loss.

* ** Sequential Storage:** The audio is fragmented into short frames, and each frame's quantized features (or embeddings) are stored sequentially. This approach maintains the temporal order of the audio data, ensuring context and coherence are preserved for keyword matching.
 

This format enables decoding the features frame-by-frame for keyword matching. Searching the features is faster than decompressing the full audio.

***Selective Network Output Storage:*** During training, only the final network activations (outputs) are retained, discarding intermediate audio features for inference. This approach significantly reduces storage requirements, especially for complex models. The network processes the full audio data during training to extract the necessary features. However, only the final learned representations, captured in the network's outputs, are stored for deployment. This reduces the storage footprint by eliminating redundant intermediate feature layers not crucial for performing keyword spotting during inference.

## Data Processing

Data processing refers to the critical steps in transforming raw data into a format suitable for feeding into machine learning algorithms. It is the foundation for successful machine learning projects, enabling models to achieve optimal performance. The time-intensive nature of data cleaning and organization underscores its importance in building robust and reliable ML models.@fig-data-engineering shows a breakdown of a data scientist's time allocation, highlighting the significant portion spent on data cleaning and organizing (%60).

![Data scientists' tasks breakdown by time spent. Credit: [Forbes.](https://www.forbes.com/sites/gilpress/2016/03/23/data-preparation-most-time-consuming-least-enjoyable-data-science-task-survey-says/?sh=20c55a266f63)](images/jpg/data_engineering_features.jpg){#fig-data-engineering}

Proper data cleaning is a crucial step that directly impacts model performance. Real-world data can be imperfect, containing errors, missing values, noise, anomalies, and inconsistencies. Data cleaning involves identifying and addressing these inconsistencies to prepare high-quality data for modeling. By carefully selecting appropriate techniques, data scientists can improve model accuracy, reduce overfitting, and enable algorithms to learn more robust patterns from the data.
Data exploration is also vital in data processing, involving data analysis to understand its characteristics and potential issues. This step helps identify underlying patterns, trends, and anomalies affecting model performance.
Data often comes from diverse sources and can be unstructured or semi-structured. Thus, processing and standardizing it is essential, ensuring it adheres to a uniform format. Such transformations may include:

* Normalizing numerical variables
* Encoding categorical variables
* Using techniques like dimensionality reduction

Data validation serves a broader role than ensuring adherence to certain standards. It encompasses techniques to identify and address errors within the data. These errors can be missing values, outliers, or inconsistencies that may take time to be apparent. Rigorous validation processes, including verifying the initial annotation practices, detecting outliers, and handling missing values through techniques like mean imputation, contribute directly to the quality of datasets. This, in turn, impacts the performance, fairness, and safety of the models trained on them.

In TinyML applications, sensor malfunctions or environmental factors can introduce transient errors into the data. Validation becomes even more critical to catch such errors early and prevent them from propagating through the data pipeline.
Data processing pipelines are systematic and automated workflows for data transformation, storage, and processing. These pipelines will be explored in more detail throughout the book. A practical example of a data processing pipeline in the context of TinyML speech applications can be found in  @fig-data-engineering-kws2.

The Multilingual Spoken Words Corpus (MSWC) is a vast collection of audio recordings in 50 languages used by over 5 billion people. The raw audio recordings undergo several processing stages within this data processing pipeline, such as audio-word alignment and keyword extraction. By streamlining the data flow from raw recordings to usable datasets, data pipelines enhance productivity and expedite the development of machine learning models.

The MSWC is openly licensed under Creative Commons Attribution 4.0 for broad usage. It is a valuable resource for academic study and business applications in areas like keyword identification and speech-based search.

![An overview of the Multilingual Spoken Words Corpus (MSWC) data processing pipeline. Credit: @mazumder2021multilingual.](images/png/data_engineering_kws2.png){#fig-data-engineering-kws2}


The MSWC used a [forced alignment](https://montreal-forced-aligner.readthedocs.io/en/latest/) method to automatically extract individual word recordings to train keyword-spotting models from the [Common Voice](https://commonvoice.mozilla.org/) project, which features crowdsourced sentence-level recordings. Forced alignment refers to long-standing methods in speech processing that predict when speech phenomena like syllables, words, or sentences start and end within an audio recording. In the MSWC data, crowdsourced recordings often feature background noises, such as static and wind. Depending on the model's requirements, these noises can be removed or intentionally retained.

Maintaining the integrity of the data infrastructure is a continuous endeavor in TinyML applications. This encompasses data storage, security, error handling, and stringent version control. Regular updates are crucial, especially for dynamic applications like keyword spotting, to adapt to evolving trends and device integrations.
There is a boom in data processing pipelines, commonly found in ML operations toolchains, which we will discuss in the MLOps chapter. Briefly, these include frameworks like MLOps by Google Cloud. It provides methods for automation and monitoring at all steps of ML system construction, including integration, testing, releasing, deployment, and infrastructure management. Several mechanisms focus on data processing, an integral part of these systems.

:::{#exr-dp .callout-exercise collapse="true"}

### Data Processing

Let us explore two significant projects in speech data processing and machine learning. The MSWC is a vast audio dataset with over 340,000 keywords and 23.4 million 1-second spoken examples. It's used in various applications like voice-enabled devices and call center automation. The Few-Shot Keyword Spotting project introduces a new approach for keyword spotting across different languages, achieving impressive results with minimal training data. We'll delve into the MSWC dataset, learn how to structure it effectively, and then train a few-shot keyword-spotting model. Let's get started!  

[![](https://colab.research.google.com/assets/colab-badge.png)](https://colab.research.google.com/github/harvard-edge/multilingual_kws/blob/main/multilingual_kws_intro_tutorial.ipynb#scrollTo=ApnPyIlYNFYD)
:::

## Data Labeling

High-quality training datasets are essential for effective machine-learning models. Data labeling is critical in achieving this by providing ground truth information, allowing models to learn relationships between inputs and desired outputs. This section covers key considerations for selecting label types, formats, and content to capture the necessary task information. It discusses common annotation approaches, from manual labeling to crowdsourcing to AI-assisted methods, and best practices for ensuring label quality through training, guidelines, and quality checks. The ethical treatment of human annotators is also emphasized. Additionally, the integration of AI to accelerate and augment human annotation is explored. Understanding labeling needs, challenges, and strategies are essential for constructing reliable, useful datasets to train performant, trustworthy machine learning systems.

### Label Types

Labels capture information about key tasks or concepts. @fig-labels includes some common label types: a "classification label" is used for categorizing images with labels (labeling an image with "dog" if it features a dog); a "bounding box" identifies object location (drawing a box around the dog); a "segmentation map" classifies objects at the pixel level (highlighting the dog in a distinct color); a "caption" provides descriptive annotations (describing the dog's actions, position, color, etc.); and a "transcript" denotes audio content. The choice of label format depends on the use case and resource constraints, as more detailed labels require greater effort to collect (@10.1109/ICRA.2017.7989092). 

![An overview of common label types.](images/png/CS249r_Labels.png){#fig-labels}

Unless focused on self-supervised learning, a dataset will likely provide labels addressing one or more tasks of interest. Given their unique resource constraints, dataset creators must consider what information labels should capture and how they can practically obtain the necessary labels. Creators must first decide what type(s) of content labels should capture. For example, a creator interested in car detection would want to label cars in their dataset. Still, they might also consider whether to simultaneously collect labels for other tasks that the dataset could potentially be used for, such as pedestrian detection.

Additionally, annotators can provide metadata that provides insight into how the dataset represents different characteristics of interest (see @sec-data-transparency). The Common Voice dataset, for example, includes various types of metadata that provide information about the speakers, recordings, and dataset quality for each language represented (@ardila2020common). They include demographic splits showing the number of recordings by speaker age range and gender. This allows us to see who contributed recordings for each language. They also include statistics like average recording duration and total hours of validated recordings. These give insights into the nature and size of the datasets for each language.
Furthermore, quality control metrics like the percentage of recordings that have been validated are useful to know how complete and clean the datasets are. The metadata also includes normalized demographic splits scaled to 100% for comparison across languages. This highlights representation differences between higher and lower resource languages.
Having decided on the information to capture in labels, creators must determine the format next. For example, a creator interested in car detection might choose between binary classification labels that indicate whether a car is present, bounding boxes that show the general locations of any cars, or pixel-wise segmentation labels that show the exact location of each car. Their choice of label format will depend on the use case and resource constraints, as finer-grained labels are typically more expensive and time-consuming to acquire. Once the desired content and label format are determined, creators can begin the annotation process.

### Annotation Methods

Common annotation approaches include manual labeling, crowdsourcing, and semi-automated techniques. Manual labeling by experts yields high quality but needs more scalability. Crowdsourcing enables non-experts to distribute annotation, often through dedicated platforms (@victor2019machine). Weakly supervised and programmatic methods can reduce manual effort by heuristically or automatically generating labels (@ratner2018snorkel).

After deciding on their labels' desired content and format, creators begin the annotation process. To collect large numbers of labels from human annotators, creators frequently rely on dedicated annotation platforms, which can connect them to teams of human annotators. When using these platforms, creators may need more insight into annotators' backgrounds and experience levels with topics of interest. However, some platforms offer access to annotators with specific expertise (e.g., doctors).

### Ensuring Label Quality

There is no guarantee that the data labels are correct. @fig-hard-labels shows some examples of hard labeling cases: some errors arise from blurred pictures that make them hard to identify (the frog image), and others stem from a lack of domain knowledge (the black stork case). It is possible that despite the best instructions given to labelers, they still mislabel some images (@northcutt2021pervasive). Strategies like quality checks, training annotators, and collecting multiple labels per datapoint can help ensure label quality. Multiple annotators can help identify controversial datapoints and quantify disagreement levels for ambiguous tasks.

![Some examples of hard labeling cases. Credit: @northcutt2021pervasive.](https://raw.githubusercontent.com/cleanlab/assets/master/cleanlab/label-errors-examples.png){#fig-hard-labels}

When working with human annotators, offering fair compensation and otherwise prioritizing ethical treatment is important, as annotators can be exploited or otherwise harmed during the labeling process (Perrigo, 2023). For example, if a dataset is likely to contain disturbing content, annotators may benefit from having the option to view images in grayscale (@googleinformation).

### AI-Assisted Annotation

ML has an insatiable demand for data. Therefore, more data is needed. This raises the question of how we can get more labeled data. Rather than always generating and curating data manually, we can rely on existing AI models to help label datasets more quickly and cheaply, though often with lower quality than human annotation. This can be done in various ways, as shown in @fig-weak-supervision, including the following:

* **Pre-annotation:** AI models can generate preliminary labels for a dataset using methods such as semi-supervised learning (@chapelle2009semisupervised), which humans can then review and correct. This can save a significant amount of time, especially for large datasets.
* **Active learning:** AI models can identify the most informative data points in a dataset, which can then be prioritized for human annotation. This can help improve the labeled dataset's quality while reducing the overall annotation time.
* **Quality control:** AI models can identify and flag potential errors in human annotations, helping to ensure the accuracy and consistency of the labeled dataset.

Here are some examples of how AI-assisted annotation has been proposed to be useful:

* **Medical imaging:** AI-assisted annotation labels medical images, such as MRI scans and X-rays (@krishnan2022selfsupervised). Carefully annotating medical datasets is extremely challenging, especially at scale, since domain experts are scarce and become costly. This can help to train AI models to diagnose diseases and other medical conditions more accurately and efficiently.  
* **Self-driving cars:** AI-assisted annotation is being used to label images and videos from self-driving cars. This can help to train AI models to identify objects on the road, such as other vehicles, pedestrians, and traffic signs.
* **Social media:** AI-assisted annotation labels social media posts like images and videos. This can help to train AI models to identify and classify different types of content, such as news, advertising, and personal posts.

![Strategies for acquiring additional labeled training data. Credit: [Standford AI Lab.](https://ai.stanford.edu/blog/weak-supervision/)](https://dawn.cs.stanford.edu/assets/img/2017-07-16-weak-supervision/WS_mapping.png){#fig-weak-supervision}

## Data Version Control

Production systems are perpetually inundated with fluctuating and escalating volumes of data, prompting the rapid emergence of numerous data replicas. This increasing data serves as the foundation for training machine learning models. For instance, a global sales company engaged in sales forecasting continuously receives consumer behavior data. Similarly, healthcare systems formulating predictive models for disease diagnosis are consistently acquiring new patient data. TinyML applications, such as keyword spotting, are highly data-hungry regarding the amount of data generated. Consequently, meticulous tracking of data versions and the corresponding model performance is imperative.

Data Version Control offers a structured methodology to handle alterations and versions of datasets efficiently. It facilitates monitoring modifications, preserves multiple versions, and guarantees reproducibility and traceability in data-centric projects. Furthermore, data version control provides the versatility to review and utilize specific versions as needed, ensuring that each stage of the data processing and model development can be revisited and audited precisely and efficiently. It has a variety of practical uses -

**Risk Management:** Data version control allows transparency and accountability by tracking dataset versions.

**Collaboration and Efficiency:** Easy access to different dataset versions in one place can improve data sharing of specific checkpoints and enable efficient collaboration.

**Reproducibility:** Data version control allows for tracking the performance of models concerning different versions of the data,
and therefore enabling reproducibility.

**Key Concepts**

* **Commits:** It is an immutable snapshot of the data at a specific point in time, representing a unique version. Every commit is associated with a unique identifier to allow

* **Branches:** Branching allows developers and data scientists to diverge from the main development line and continue to work independently without affecting other branches. This is especially useful when experimenting with new features or models, enabling parallel development and experimentation without the risk of corrupting the stable main branch.

* **Merges:** Merges help to integrate changes from different branches while maintaining the integrity of the data.

With data version control in place, we can track the changes shown in @fig-data-version-ctrl, reproduce previous results by reverting to older versions, and collaborate safely by branching off and isolating the changes.

![Data versioning.](images/png/data_version_ctrl.png){#fig-data-version-ctrl}

**Popular Data Version Control Systems**

[**[DVC]{.underline}**](https://dvc.org/doc): It stands for Data Version Control in short and is an open-source, lightweight tool that works on top of Git Hub and supports all kinds of data formats. It can seamlessly integrate into the workflow if Git is used to manage code. It captures the versions of data and models in the Git commits while storing them on-premises or on the cloud (e.g., AWS, Google Cloud, Azure). These data and models (e.g., ML artifacts) are defined in the metadata files, which get updated in every commit. It allows metrics tracking of models on different versions of the data.

**[[lakeFS]{.underline}](https://docs.lakefs.io/):** It is an open-source tool that supports data version control on data lakes. It supports many git-like operations, such as branching and merging data and reverting to previous versions of the data. It also has a unique UI feature, making exploring and managing data much easier.

**[[Git LFS]{.underline}](https://git-lfs.com/):** It is useful for data version control on smaller-sized datasets. It uses Git's inbuilt branching and merging features but is limited in tracking metrics, reverting to previous versions, or integrating with data lakes.

## Optimizing Data for Embedded AI

Creators working on embedded systems may have unusual priorities when cleaning their datasets. On the one hand, models may be developed for particular use cases, requiring heavy filtering of datasets. While other large language models may be capable of turning any speech into text, a model for an embedded system may be focused on a single limited task, such as detecting a keyword. As a result, creators may aggressively filter out large amounts of data to address the task of interest. An embedded AI system may also be tied to specific hardware devices or environments. For example, a video model may need to process images from a single type of camera, which will only be mounted on doorbells in residential neighborhoods. In this scenario, creators may discard images if they came from a different kind of camera, show the wrong type of scenery, or were taken from the wrong height or angle.

On the other hand, embedded AI systems are often expected to provide especially accurate performance in unpredictable real-world settings. This may lead creators to design datasets to represent variations in potential inputs and promote model robustness. As a result, they may define a narrow scope for their project but then aim for deep coverage within those bounds. For example, creators of the doorbell model mentioned above might try to cover variations in data arising from:

* Geographically, socially, and architecturally diverse neighborhoods
* Different types of artificial and natural lighting
* Different seasons and weather conditions
* Obstructions (e.g., raindrops or delivery boxes obscuring the camera's view)

As described above, creators may consider crowdsourcing or synthetically generating data to include these variations.

## Data Transparency {#sec-data-transparency}

By providing clear, detailed documentation, creators can help developers understand how best to use their datasets. Several groups have suggested standardized documentation formats for datasets, such as Data Cards (@pushkarna2022data), datasheets (@gebru2021datasheets), data statements (@bender2018data), or Data Nutrition Labels (@holland2020dataset). When releasing a dataset, creators may describe what kinds of data they collected, how they collected and labeled it, and what kinds of use cases may be a good or poor fit for the dataset. Quantitatively, it may be appropriate to show how well the dataset represents different groups (e.g., different gender groups, different cameras).

@fig-data-card shows an example of a data card for a computer vision (CV) dataset. It includes some basic information about the dataset and instructions on how to use it, including known biases.

![Data card describing a CV dataset. Credit: @pushkarna2022data.](images/png/data_card.png){#fig-data-card}

**The Importance of Data Provenance in Machine Learning**

Data provenance, the ability to track the origin and journey of each data point through the machine learning pipeline, is no longer a nicety but a fundamental requirement for ensuring data quality. Transparent machine learning systems, enabled by robust data provenance, facilitate scrutinizing individual data points. This scrutiny empowers practitioners to identify and rectify errors, biases, and inconsistencies within the data.

For example, consider a medical ML model exhibiting performance deficiencies in specific areas. By tracing the data provenance, one can pinpoint the root cause: issues with data collection methods, underrepresentation of certain demographic groups, or other factors. This level of transparency goes beyond debugging; it fosters improving data quality. Reliable and trustworthy datasets, bolstered by verifiable data provenance, ultimately enhance model performance and user acceptance.

**Data Access and Maintenance Considerations**

When creating documentation, data creators should explicitly outline user access procedures and long-term maintenance plans for the dataset. For instance, accessing sensitive datasets, such as those containing medical information, may necessitate user training or special permissions from the creators. In some cases, users' direct data access might be restricted. Federated learning setups (@aledhari2020federated) offer an alternative approach, where users submit their models for training on the creators' hardware. Additionally, data creators should specify the dataset's accessibility timeframe, user error reporting mechanisms, and plans for future updates.

**Legal and Regulatory Landscape**

Data transparency is increasingly emphasized by legal and regulatory frameworks. The European Union's General Data Protection Regulation (GDPR) mandates stringent data processing and protection protocols for EU citizens' personal data. Organizations must provide clear, plain-language privacy policies that detail data collection purposes, storage duration, sharing practices, and legal justifications for processing. Additionally, GDPR mandates privacy notices encompassing data transfer procedures, retention periods, access and deletion rights, and contact information for data controllers.

California's Consumer Privacy Act (CCPA) echoes similar concerns, mandating clear privacy policies and user opt-out rights to sell personal data. Notably, CCPA empowers consumers to request access to their specific data, including details on its usage, categories collected, and recipients. This legislation represents a significant step towards consumer empowerment in managing personal data.

**Challenges and Considerations**

While data transparency offers undeniable benefits, it also presents challenges. Establishing and maintaining robust data provenance requires significant time and financial resources. The inherent complexity of data systems can also make achieving full transparency a time-consuming process. Furthermore, overly detailed information might overwhelm users. Finally, it is also important to balance the trade-off between transparency and privacy.

## Licensing

Many high-quality datasets either come from proprietary sources or contain copyrighted information. This introduces licensing as a challenging legal domain. Companies eager to train ML systems must engage in negotiations to obtain licenses that grant legal access to these datasets. Furthermore, licensing terms can impose restrictions on data applications and sharing methods. Failure to comply with these licenses can have severe consequences.

For instance, ImageNet, one of the most extensively utilized datasets for computer vision research, is a case in point. Most of its images were procured from public online sources without explicit permission, sparking ethical concerns (Prabhu and Birhane, 2020). Accessing the ImageNet dataset for corporations requires registration and adherence to its terms of use, which restricts commercial usage ([[ImageNet]{.underline}](https://www.image-net.org/#), 2021). Major players like Google and Microsoft invest significantly in licensing datasets to enhance their ML vision systems. However, the cost factor restricts accessibility for researchers from smaller companies with constrained budgets.

The legal domain of data licensing has seen major cases that help define fair use parameters. A prominent example is _Authors Guild, Inc. v. Google, Inc._ This 2005 lawsuit alleged that Google's book scanning project infringed copyrights by displaying snippets without permission. However, the courts ultimately ruled in Google's favor, upholding fair use based on the transformative nature of creating a searchable index and showing limited text excerpts. This precedent provides some legal grounds for arguing fair use protections apply to indexing datasets and generating representative samples for machine learning. However, license restrictions remain binding, so a comprehensive analysis of licensing terms is critical. The case demonstrates why negotiations with data providers are important to enable legal usage within acceptable bounds.

**New Data Regulations and Their Implications**

New data regulations also impact licensing practices. The legislative landscape is evolving with regulations like the EU's [[Artificial Intelligence Act]{.underline}](https://digital-strategy.ec.europa.eu/en/policies/european-approach-artificial-intelligence), which is poised to regulate AI system development and use within the European Union (EU). This legislation:

1. Classifies AI systems by risk.

2. Mandates development and usage prerequisites.

3. Emphasizes data quality, transparency, human oversight, and accountability.

Additionally, the EU Act addresses the ethical dimensions and operational challenges in sectors such as healthcare and finance. Key elements include the prohibition of AI systems posing \"unacceptable\" risks, stringent conditions for high-risk systems, and minimal obligations for \"limited risk\" AI systems. The proposed European AI Board will oversee and ensure the implementation of efficient regulation.

**Challenges in Constructing ML Training Datasets**

***Complexities in Data Access and Usage**
Assembling machine learning (ML) training datasets is a multifaceted, challenging endeavor. Intricate legal issues surrounding proprietary data, copyright law, and privacy regulations constrain the options for building robust datasets. Expanding accessibility through adopting more open licensing practices or fostering public-private data collaborations could significantly accelerate industry progress and elevate ethical standards.
***Data Anonymization, Filtering, and Regulatory Landscape**
Certain portions of a dataset may need to be removed or obfuscated to comply with data usage agreements or safeguard sensitive information. This process often occurs well after active data sourcing and model training. For instance, a user information dataset might require the removal of names, contact details, and other identifying data to ensure anonymity. Similarly, datasets containing copyrighted content or trade secrets may need to be filtered before distribution.
Regulations such as the General Data Protection Regulation (GDPR), the California Consumer Privacy Act (CCPA), and the Amended Act on the Protection of Personal Information([[APPI]{.underline}](https://www.ppc.go.jp/files/pdf/280222_amendedlaw.pdf)) have been established to guarantee the right to be forgotten. These regulations legally mandate model providers to erase user data upon request. Data collectors and providers must be equipped to take appropriate measures to de-identify or filter out any proprietary, licensed, confidential, or regulated information as required. Explicit user requests for data removal may also necessitate action.
***Balancing Data Usage with Privacy**
Data collectors and providers must be equipped to take appropriate measures to de-identify or filter out any proprietary, licensed, confidential, or regulated information as required. Sometimes, the users may explicitly request that their data be removed.

***Challenges of Data Removal in Trained Models**
The ability to update datasets by removing data allows creators to fulfill legal and ethical obligations. However, this approach has limitations. Models already trained on the dataset may not have a clear mechanism to eliminate the influence of specific data samples. This raises the question of whether models must be retrained from scratch every time data is removed, a costly and impractical solution. Even after data removal from the original dataset, its impact on the model's behavior might persist. Further research is needed to understand the effects of data removal on trained models and determine if retraining is necessary to avoid retaining artifacts of deleted data. This presents a critical consideration for balancing data licensing obligations with efficiency and practicality in dynamically evolving ML systems.

Dataset licensing is a multifaceted domain that intersects technology, ethics, and law. As the world evolves, understanding these intricacies becomes paramount for anyone building datasets during data engineering.

## Conclusion

Data is the fundamental building block of AI systems. Without quality data, even the most advanced machine learning algorithms will fail. Data engineering encompasses the end-to-end process of collecting, storing, processing, and managing data to fuel the development of machine learning models. It begins with clearly defining the core problem and objectives, which guides effective data collection. Data can be sourced from diverse means, including existing datasets, web scraping, crowdsourcing, and synthetic data generation. Each approach involves tradeoffs between cost, speed, privacy, and specificity.
 Once data is collected, thoughtful labeling through manual or AI-assisted annotation enables the creation of high-quality training datasets. Proper storage in databases, warehouses, or lakes facilitates easy access and analysis. Metadata provides contextual details about the data. Data processing transforms raw data into a clean, consistent format for machine learning model development. 
Throughout this pipeline, transparency through documentation and provenance tracking is crucial for ethics, auditability, and reproducibility. Data licensing protocols also govern legal data access and use. Key challenges in data engineering include privacy risks, representation gaps, legal restrictions around proprietary data, and the need to balance competing constraints like speed versus quality.
 By thoughtfully engineering high-quality training data, machine learning practitioners can develop accurate, robust, and responsible AI systems. This includes applications in embedded systems and TinyML, where resource constraints demand particularly efficient and effective data-handling practices. In the context of TinyML, data engineering practices take on a unique character. Resource-constrained devices often necessitate smaller datasets with high signal-to-noise ratios. Data collection may be limited to on-device sensors or specific environmental conditions. Crowdsourcing and synthetic data generation have become precious tools for generating specialized datasets with limited memory and processing power. Careful optimization techniques for data cleansing, feature selection, and model compression are essential for TinyML applications. By understanding these nuances, data engineers can empower the development of efficient and effective AI solutions at the edge.
## Resources {#sec-data-engineering-resource .unnumbered}

Here is a curated list of resources to support students and instructors in their learning and teaching journeys. We are continuously working on expanding this collection and will add new exercises soon.

:::{.callout-slide collapse="false"}
# Slides 

These slides are a valuable tool for instructors to deliver lectures and for students to review the material at their own pace. We encourage students and instructors to leverage these slides to enhance their understanding and facilitate effective knowledge transfer.

* [Data Engineering: Overview.](https://docs.google.com/presentation/d/1jlIfD6RtQWG8314jCAu1qdnG7YyESy60Yt5-zXhEsVA/edit#slide=id.g202a7c05d1a_0_0)

* [Feature engineering.](https://docs.google.com/presentation/d/1AIM1H-GfvjNPHQw9urxJz3vtMgb_9kizfthbymISPR4/edit#slide=id.g202a83498d1_0_0)

* [Data Standards: Speech Commands.](https://docs.google.com/presentation/d/1qDoHc7yzZ2lEha9NTMZ07Ls4tkIz-1f7kUYRlvjzsI4/edit?usp=drive_link&resourcekey=0-ol4Oqk_y706P_zIB5mbu7Q)

* [Crowdsourcing Data for the Long Tail.](https://docs.google.com/presentation/d/1d3KUit64L-4dXecCNBpikCxx7VO0xIJ13r9v1Ad22S4/edit#slide=id.ga4ca29c69e_0_179)

* [Reusing and Adapting Existing Datasets.](https://docs.google.com/presentation/d/1mHecDoCYHQD9nWSRYCrXXG0IOp9wYQk-fbxhoNIsGMY/edit#slide=id.ga4ca29c69e_0_206)

* [Responsible Data Collection.](https://docs.google.com/presentation/d/1vcmuhLVNFT2asKSCSGh_Ix9ht0mJZxMii8MufEMQhFA/edit?resourcekey=0-_pYLcW5aF3p3Bvud0PPQNg#slide=id.ga4ca29c69e_0_195)

* Data Anomaly Detection:
  * [Anamoly Detection: Overview.](https://docs.google.com/presentation/d/1R8A_5zKDZDZOdAb1XF9ovIOUTLWSIuFWDs20-avtxbM/edit?resourcekey=0-pklEaPv8PmLQ3ZzRYgRNxw#slide=id.g94db9f9f78_0_2)
 
  * [Anamoly Detection: Challenges.](https://docs.google.com/presentation/d/1JZxx2kLaO1a8O6z6rRVFpK0DN-8VMkaSrNnmk_VGbI4/edit#slide=id.g53eb988857_0_91)

  * [Anamoly Detection: Datasets.](https://docs.google.com/presentation/d/1wPDhp4RxVrOonp6pU0Capk0LWXZOGZ3x9BzW_VjpTQw/edit?resourcekey=0-y6wKAnuxrLWqhleq9ruLOA#slide=id.g53eb988857_0_91)

  * [Anamoly Detection: using Autoencoders.](https://docs.google.com/presentation/d/1Q4h7XrayNRIP0r52Hlk5VjxRcli-GY2xmyZ53nCd6CI/edit#slide=id.g53eb988857_0_91)

:::

:::{.callout-exercise collapse="false"}
# Exercises 

To reinforce the concepts covered in this chapter, we have curated a set of exercises that challenge students to apply their knowledge and deepen their understanding. 

* @exr-kws

* @exr-ws

* @exr-sd

* @exr-dp

:::

:::{.callout-lab collapse="false"}
# Labs 

In addition to exercises, we offer a series of hands-on labs allowing students to gain practical experience with embedded AI technologies. These labs provide step-by-step guidance, enabling students to develop their skills in a structured and supportive environment. We are excited to announce that new labs will be available soon, further enriching the learning experience.

*Coming soon.*
:::
