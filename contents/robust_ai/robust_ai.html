<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.549">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Machine Learning Systems - 17&nbsp; Robust AI</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../contents/generative_ai/generative_ai.html" rel="next">
<link href="../../contents/sustainable_ai/sustainable_ai.html" rel="prev">
<link href="../../favicon.png" rel="icon" type="image/png">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "?",
    "/"
  ],
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-M21L0CBCVN"></script>
<script src="../../scripts/ai_menu/dist/bundle.js" defer=""></script>
<script src="../../scripts/ai_menu/dist/142.bundle.js" defer=""></script>
<script src="../../scripts/ai_menu/dist/384.bundle.js" defer=""></script>
<script src="../../scripts/ai_menu/dist/761.bundle.js" defer=""></script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating nav-fixed slimcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Machine Learning Systems</span>
    </a>
  </div>
        <div class="quarto-navbar-tools tools-wide">
    <a href="https://github.com/harvard-edge/cs249r_book" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
    <a href="../../Machine-Learning-Systems.pdf" title="Download PDF" class="quarto-navigation-tool px-1" aria-label="Download PDF"><i class="bi bi-file-pdf"></i></a>
    <div class="dropdown">
      <a href="" title="Share" id="quarto-navigation-tool-dropdown-0" class="quarto-navigation-tool dropdown-toggle px-1" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Share"><i class="bi bi-share"></i></a>
      <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="quarto-navigation-tool-dropdown-0">
          <li>
            <a class="dropdown-item quarto-navbar-tools-item" href="https://twitter.com/intent/tweet?url=|url|">
              <i class="bi bi-bi-twitter pe-1"></i>
            Twitter
            </a>
          </li>
          <li>
            <a class="dropdown-item quarto-navbar-tools-item" href="https://www.facebook.com/sharer/sharer.php?u=|url|">
              <i class="bi bi-bi-facebook pe-1"></i>
            Facebook
            </a>
          </li>
      </ul>
    </div>
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
  <a href="" class="quarto-reader-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleReader(); return false;" title="Toggle reader mode">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../contents/foreword.html">MAIN</a></li><li class="breadcrumb-item"><a href="../../contents/robust_ai/robust_ai.html"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Robust AI</span></a></li></ol></nav>
        <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
 <span class="menu-text">FRONT MATTER</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/dedication.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Dedication</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/acknowledgements/acknowledgements.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Acknowledgements</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/contributors.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Contributors</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/copyright.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Copyright</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/about.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">About the Book</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../contents/foreword.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">MAIN</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/introduction/introduction.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/ml_systems/ml_systems.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">ML Systems</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/dl_primer/dl_primer.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">DL Primer</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/workflow/workflow.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">AI Workflow</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/data_engineering/data_engineering.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Data Engineering</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/frameworks/frameworks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">AI Frameworks</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/training/training.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">AI Training</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/efficient_ai/efficient_ai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Efficient AI</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/optimizations/optimizations.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Model Optimizations</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/hw_acceleration/hw_acceleration.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">AI Acceleration</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/benchmarking/benchmarking.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Benchmarking AI</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/ondevice_learning/ondevice_learning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">On-Device Learning</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/ops/ops.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">ML Operations</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/privacy_security/privacy_security.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Security &amp; Privacy</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/responsible_ai/responsible_ai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Responsible AI</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/sustainable_ai/sustainable_ai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Sustainable AI</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/robust_ai/robust_ai.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Robust AI</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/generative_ai/generative_ai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Generative AI</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/ai_for_good/ai_for_good.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">AI for Good</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/conclusion/conclusion.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Conclusion</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true">
 <span class="menu-text">REFERENCES</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../contents/labs.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">HANDS-ON LABS</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/niclav_sys/niclav_sys.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Setup Nicla Vision</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/image_classification/image_classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">CV on Nicla Vision</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/object_detection_fomo/object_detection_fomo.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Object Detection</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/kws_feature_eng/kws_feature_eng.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Audio Feature Engineering</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/kws_nicla/kws_nicla.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Keyword Spotting (KWS)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/dsp_spectral_features_block/dsp_spectral_features_block.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">DSP - Spectral Features</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/motion_classify_ad/motion_classify_ad.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Motion Classification and Anomaly Detection</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="true">
 <span class="menu-text">Appendices</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/tools.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">Tools</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/zoo_datasets.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">B</span>&nbsp; <span class="chapter-title">Datasets</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/zoo_models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">C</span>&nbsp; <span class="chapter-title">Model Zoo</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/learning_resources.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">D</span>&nbsp; <span class="chapter-title">Resources</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/community.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">E</span>&nbsp; <span class="chapter-title">Communities</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contents/case_studies.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">F</span>&nbsp; <span class="chapter-title">Case Studies</span></span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction"><span class="header-section-number">17.1</span> Introduction</a></li>
  <li><a href="#real-world-examples" id="toc-real-world-examples" class="nav-link" data-scroll-target="#real-world-examples"><span class="header-section-number">17.2</span> Real-World Examples</a>
  <ul>
  <li><a href="#cloud" id="toc-cloud" class="nav-link" data-scroll-target="#cloud"><span class="header-section-number">17.2.1</span> Cloud</a></li>
  <li><a href="#edge" id="toc-edge" class="nav-link" data-scroll-target="#edge"><span class="header-section-number">17.2.2</span> Edge</a></li>
  <li><a href="#embedded" id="toc-embedded" class="nav-link" data-scroll-target="#embedded"><span class="header-section-number">17.2.3</span> Embedded</a></li>
  </ul></li>
  <li><a href="#hardware-faults" id="toc-hardware-faults" class="nav-link" data-scroll-target="#hardware-faults"><span class="header-section-number">17.3</span> Hardware Faults</a>
  <ul>
  <li><a href="#transient-faults" id="toc-transient-faults" class="nav-link" data-scroll-target="#transient-faults"><span class="header-section-number">17.3.1</span> Transient Faults</a>
  <ul class="collapse">
  <li><a href="#definition-and-characteristics" id="toc-definition-and-characteristics" class="nav-link" data-scroll-target="#definition-and-characteristics">Definition and Characteristics</a></li>
  <li><a href="#causes-of-transient-faults" id="toc-causes-of-transient-faults" class="nav-link" data-scroll-target="#causes-of-transient-faults">Causes of Transient Faults</a></li>
  <li><a href="#mechanisms-of-transient-faults" id="toc-mechanisms-of-transient-faults" class="nav-link" data-scroll-target="#mechanisms-of-transient-faults">Mechanisms of Transient Faults</a></li>
  <li><a href="#impact-on-ml-systems" id="toc-impact-on-ml-systems" class="nav-link" data-scroll-target="#impact-on-ml-systems">Impact on ML Systems</a></li>
  </ul></li>
  <li><a href="#permanent-faults" id="toc-permanent-faults" class="nav-link" data-scroll-target="#permanent-faults"><span class="header-section-number">17.3.2</span> Permanent Faults</a>
  <ul class="collapse">
  <li><a href="#definition-and-characteristics-1" id="toc-definition-and-characteristics-1" class="nav-link" data-scroll-target="#definition-and-characteristics-1">Definition and Characteristics</a></li>
  <li><a href="#causes-of-permanent-faults" id="toc-causes-of-permanent-faults" class="nav-link" data-scroll-target="#causes-of-permanent-faults">Causes of Permanent Faults</a></li>
  <li><a href="#mechanisms-of-permanent-faults" id="toc-mechanisms-of-permanent-faults" class="nav-link" data-scroll-target="#mechanisms-of-permanent-faults">Mechanisms of Permanent Faults</a></li>
  <li><a href="#impact-on-ml-systems-1" id="toc-impact-on-ml-systems-1" class="nav-link" data-scroll-target="#impact-on-ml-systems-1">Impact on ML Systems</a></li>
  </ul></li>
  <li><a href="#intermittent-faults" id="toc-intermittent-faults" class="nav-link" data-scroll-target="#intermittent-faults"><span class="header-section-number">17.3.3</span> Intermittent Faults</a>
  <ul class="collapse">
  <li><a href="#definition-and-characteristics-2" id="toc-definition-and-characteristics-2" class="nav-link" data-scroll-target="#definition-and-characteristics-2">Definition and Characteristics</a></li>
  <li><a href="#causes-of-intermittent-faults" id="toc-causes-of-intermittent-faults" class="nav-link" data-scroll-target="#causes-of-intermittent-faults">Causes of Intermittent Faults</a></li>
  <li><a href="#mechanisms-of-intermittent-faults" id="toc-mechanisms-of-intermittent-faults" class="nav-link" data-scroll-target="#mechanisms-of-intermittent-faults">Mechanisms of Intermittent Faults</a></li>
  <li><a href="#impact-on-ml-systems-2" id="toc-impact-on-ml-systems-2" class="nav-link" data-scroll-target="#impact-on-ml-systems-2">Impact on ML Systems</a></li>
  </ul></li>
  <li><a href="#detection-and-mitigation" id="toc-detection-and-mitigation" class="nav-link" data-scroll-target="#detection-and-mitigation"><span class="header-section-number">17.3.4</span> Detection and Mitigation</a>
  <ul class="collapse">
  <li><a href="#fault-detection-techniques" id="toc-fault-detection-techniques" class="nav-link" data-scroll-target="#fault-detection-techniques">Fault Detection Techniques</a></li>
  </ul></li>
  <li><a href="#summary" id="toc-summary" class="nav-link" data-scroll-target="#summary"><span class="header-section-number">17.3.5</span> Summary</a></li>
  </ul></li>
  <li><a href="#ml-model-robustness" id="toc-ml-model-robustness" class="nav-link" data-scroll-target="#ml-model-robustness"><span class="header-section-number">17.4</span> ML Model Robustness</a>
  <ul>
  <li><a href="#adversarial-attacks" id="toc-adversarial-attacks" class="nav-link" data-scroll-target="#adversarial-attacks"><span class="header-section-number">17.4.1</span> Adversarial Attacks</a>
  <ul class="collapse">
  <li><a href="#definition-and-characteristics-3" id="toc-definition-and-characteristics-3" class="nav-link" data-scroll-target="#definition-and-characteristics-3">Definition and Characteristics</a></li>
  <li><a href="#mechanisms-of-adversarial-attacks" id="toc-mechanisms-of-adversarial-attacks" class="nav-link" data-scroll-target="#mechanisms-of-adversarial-attacks">Mechanisms of Adversarial Attacks</a></li>
  <li><a href="#impact-on-ml-systems-3" id="toc-impact-on-ml-systems-3" class="nav-link" data-scroll-target="#impact-on-ml-systems-3">Impact on ML Systems</a></li>
  </ul></li>
  <li><a href="#data-poisoning" id="toc-data-poisoning" class="nav-link" data-scroll-target="#data-poisoning"><span class="header-section-number">17.4.2</span> Data Poisoning</a>
  <ul class="collapse">
  <li><a href="#definition-and-characteristics-4" id="toc-definition-and-characteristics-4" class="nav-link" data-scroll-target="#definition-and-characteristics-4">Definition and Characteristics</a></li>
  <li><a href="#mechanisms-of-data-poisoning" id="toc-mechanisms-of-data-poisoning" class="nav-link" data-scroll-target="#mechanisms-of-data-poisoning">Mechanisms of Data Poisoning</a></li>
  <li><a href="#impact-on-ml-systems-4" id="toc-impact-on-ml-systems-4" class="nav-link" data-scroll-target="#impact-on-ml-systems-4">Impact on ML Systems</a></li>
  </ul></li>
  <li><a href="#distribution-shifts" id="toc-distribution-shifts" class="nav-link" data-scroll-target="#distribution-shifts"><span class="header-section-number">17.4.3</span> Distribution Shifts</a>
  <ul class="collapse">
  <li><a href="#definition-and-characteristics-5" id="toc-definition-and-characteristics-5" class="nav-link" data-scroll-target="#definition-and-characteristics-5">Definition and Characteristics</a></li>
  <li><a href="#mechanisms-of-distribution-shifts" id="toc-mechanisms-of-distribution-shifts" class="nav-link" data-scroll-target="#mechanisms-of-distribution-shifts">Mechanisms of Distribution Shifts</a></li>
  <li><a href="#impact-on-ml-systems-5" id="toc-impact-on-ml-systems-5" class="nav-link" data-scroll-target="#impact-on-ml-systems-5">Impact on ML Systems</a></li>
  </ul></li>
  <li><a href="#detection-and-mitigation-1" id="toc-detection-and-mitigation-1" class="nav-link" data-scroll-target="#detection-and-mitigation-1"><span class="header-section-number">17.4.4</span> Detection and Mitigation</a>
  <ul class="collapse">
  <li><a href="#adversarial-attacks-2" id="toc-adversarial-attacks-2" class="nav-link" data-scroll-target="#adversarial-attacks-2">Adversarial Attacks</a></li>
  <li><a href="#data-poisoning-1" id="toc-data-poisoning-1" class="nav-link" data-scroll-target="#data-poisoning-1">Data Poisoning</a></li>
  <li><a href="#distribution-shifts-1" id="toc-distribution-shifts-1" class="nav-link" data-scroll-target="#distribution-shifts-1">Distribution Shifts</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#software-faults" id="toc-software-faults" class="nav-link" data-scroll-target="#software-faults"><span class="header-section-number">17.5</span> Software Faults</a>
  <ul>
  <li><a href="#definition-and-characteristics-6" id="toc-definition-and-characteristics-6" class="nav-link" data-scroll-target="#definition-and-characteristics-6">Definition and Characteristics</a></li>
  <li><a href="#mechanisms-of-software-faults-in-ml-frameworks" id="toc-mechanisms-of-software-faults-in-ml-frameworks" class="nav-link" data-scroll-target="#mechanisms-of-software-faults-in-ml-frameworks">Mechanisms of Software Faults in ML Frameworks</a></li>
  <li><a href="#impact-on-ml-systems-6" id="toc-impact-on-ml-systems-6" class="nav-link" data-scroll-target="#impact-on-ml-systems-6">Impact on ML Systems</a></li>
  <li><a href="#detection-and-mitigation-2" id="toc-detection-and-mitigation-2" class="nav-link" data-scroll-target="#detection-and-mitigation-2">Detection and Mitigation</a></li>
  </ul></li>
  <li><a href="#tools-and-frameworks" id="toc-tools-and-frameworks" class="nav-link" data-scroll-target="#tools-and-frameworks"><span class="header-section-number">17.6</span> Tools and Frameworks</a>
  <ul>
  <li><a href="#fault-models-and-error-models" id="toc-fault-models-and-error-models" class="nav-link" data-scroll-target="#fault-models-and-error-models"><span class="header-section-number">17.6.1</span> Fault Models and Error Models</a></li>
  <li><a href="#hardware-based-fault-injection" id="toc-hardware-based-fault-injection" class="nav-link" data-scroll-target="#hardware-based-fault-injection"><span class="header-section-number">17.6.2</span> Hardware-based Fault Injection</a>
  <ul class="collapse">
  <li><a href="#methods" id="toc-methods" class="nav-link" data-scroll-target="#methods">Methods</a></li>
  <li><a href="#limitations" id="toc-limitations" class="nav-link" data-scroll-target="#limitations">Limitations</a></li>
  </ul></li>
  <li><a href="#software-based-fault-injection-tools" id="toc-software-based-fault-injection-tools" class="nav-link" data-scroll-target="#software-based-fault-injection-tools"><span class="header-section-number">17.6.3</span> Software-based Fault Injection Tools</a></li>
  <li><a href="#bridging-the-gap-between-hardware-and-software-error-models" id="toc-bridging-the-gap-between-hardware-and-software-error-models" class="nav-link" data-scroll-target="#bridging-the-gap-between-hardware-and-software-error-models"><span class="header-section-number">17.6.4</span> Bridging the Gap between Hardware and Software Error Models</a>
  <ul class="collapse">
  <li><a href="#fidelity-bridging-the-gap" id="toc-fidelity-bridging-the-gap" class="nav-link" data-scroll-target="#fidelity-bridging-the-gap">Fidelity: Bridging the Gap</a></li>
  <li><a href="#importance-of-capturing-true-hardware-behavior" id="toc-importance-of-capturing-true-hardware-behavior" class="nav-link" data-scroll-target="#importance-of-capturing-true-hardware-behavior">Importance of Capturing True Hardware Behavior</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion"><span class="header-section-number">17.7</span> Conclusion</a></li>
  <li><a href="#sec-robust-ai-resource" id="toc-sec-robust-ai-resource" class="nav-link" data-scroll-target="#sec-robust-ai-resource"><span class="header-section-number">17.8</span> Resources</a></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/harvard-edge/cs249r_book/edit/main/contents/robust_ai/robust_ai.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li><li><a href="https://github.com/harvard-edge/cs249r_book/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li><li><a href="https://github.com/harvard-edge/cs249r_book/blob/main/contents/robust_ai/robust_ai.qmd" class="toc-action"><i class="bi empty"></i>View source</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content page-columns page-full" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../contents/foreword.html">MAIN</a></li><li class="breadcrumb-item"><a href="../../contents/robust_ai/robust_ai.html"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Robust AI</span></a></li></ol></nav>
<div class="quarto-title">
<h1 class="title"><span id="sec-robust_ai" class="quarto-section-identifier"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Robust AI</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>Resources: <a href="#sec-robust-ai-resource">Slides</a>, <a href="#sec-robust-ai-resource">Exercises</a>, <a href="#sec-robust-ai-resource">Labs</a></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./images/png/cover_robust_ai.png" class="img-fluid figure-img"></p>
<figcaption><em>DALL·E 3 Prompt: Create an image featuring an advanced AI system symbolized by an intricate, glowing neural network, deeply nested within a series of progressively larger and more fortified shields. Each shield layer represents a layer of defense, showcasing the system’s robustness against external threats and internal errors. The neural network, at the heart of this fortress of shields, radiates with connections that signify the AI’s capacity for learning and adaptation. This visual metaphor emphasizes not only the technological sophistication of the AI but also its resilience and security, set against the backdrop of a state-of-the-art, secure server room filled with the latest in technological advancements. The image aims to convey the concept of ultimate protection and resilience in the field of artificial intelligence.</em></figcaption>
</figure>
</div>
<p>The development of robust machine learning systems has become increasingly crucial. As these systems are deployed in various critical applications, from autonomous vehicles to healthcare diagnostics, ensuring their resilience to faults and errors is paramount.</p>
<p>Robust AI, in the context of hardware faults, software faults, and errors, plays an important role in maintaining the reliability, safety, and performance of machine learning systems. By addressing the challenges posed by transient, permanent, and intermittent hardware faults <span class="citation" data-cites="ahmadilivani2024systematic">(<a href="../../references.html#ref-ahmadilivani2024systematic" role="doc-biblioref">Ahmadilivani et al. 2024</a>)</span>, as well as bugs, design flaws, and implementation errors in software <span class="citation" data-cites="zhang2008distribution">(<a href="../../references.html#ref-zhang2008distribution" role="doc-biblioref">H. Zhang 2008</a>)</span>, robust AI techniques enable machine learning systems to operate effectively even in adverse conditions.</p>
<div class="no-row-height column-margin column-container"></div><p>This chapter explores the fundamental concepts, techniques, and tools for building fault-tolerant and error-resilient machine learning systems. It empowers researchers and practitioners to develop AI solutions that can withstand the complexities and uncertainties of real-world environments.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Learning Objectives
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><p>Understand the importance of robust and resilient AI systems in real-world applications.</p></li>
<li><p>Identify and characterize hardware faults, software faults, and their impact on ML systems.</p></li>
<li><p>Recognize and develop defensive strategies against threats posed by adversarial attacks, data poisoning, and distribution shifts.</p></li>
<li><p>Learn techniques for detecting, mitigating, and designing fault-tolerant ML systems.</p></li>
<li><p>Become familiar with tools and frameworks for studying and enhancing ML system resilience throughout the AI development lifecycle.</p></li>
</ul>
</div>
</div>
<section id="introduction" class="level2" data-number="17.1">
<h2 data-number="17.1" class="anchored" data-anchor-id="introduction"><span class="header-section-number">17.1</span> Introduction</h2>
<p>Robust AI refers to a system’s ability to maintain its performance and reliability in the presence of hardware, software, and errors. A robust machine learning system is designed to be fault-tolerant and error-resilient, capable of operating effectively even under adverse conditions.</p>
<p>As ML systems become increasingly integrated into various aspects of our lives, from cloud-based services to edge devices and embedded systems, the impact of hardware and software faults on their performance and reliability becomes more significant. In the future, as ML systems become more complex and are deployed in even more critical applications, the need for robust and fault-tolerant designs will be paramount.</p>
<p>ML systems are expected to play crucial roles in autonomous vehicles, smart cities, healthcare, and industrial automation domains. In these domains, the consequences of hardware or software faults can be severe, potentially leading to loss of life, economic damage, or environmental harm.</p>
<p>Researchers and engineers must focus on developing advanced techniques for fault detection, isolation, and recovery to mitigate these risks and ensure the reliable operation of future ML systems.</p>
<p>This chapter will focus specifically on three main categories of faults and errors that can impact the robustness of ML systems: hardware faults, software faults, and human errors.</p>
<ul>
<li><p><strong>Hardware Faults:</strong> Transient, permanent, and intermittent faults can affect the hardware components of an ML system, corrupting computations and degrading performance.</p></li>
<li><p><strong>Model Robustness:</strong> ML models can be vulnerable to adversarial attacks, data poisoning, and distribution shifts, which can induce targeted misclassifications, skew the model’s learned behavior, or compromise the system’s integrity and reliability.</p></li>
<li><p><strong>Software Faults:</strong> Bugs, design flaws, and implementation errors in the software components, such as algorithms, libraries, and frameworks, can propagate errors and introduce vulnerabilities.</p></li>
</ul>
<p>The specific challenges and approaches to achieving robustness may vary depending on the scale and constraints of the ML system. Large-scale cloud computing or data center systems may focus on fault tolerance and resilience through redundancy, distributed processing, and advanced error detection and correction techniques. In contrast, resource-constrained edge devices or embedded systems face unique challenges due to limited computational power, memory, and energy resources.</p>
<p>Regardless of the scale and constraints, the key characteristics of a robust ML system include fault tolerance, error resilience, and performance maintenance. By understanding and addressing the multifaceted challenges to robustness, we can develop trustworthy and reliable ML systems that can navigate the complexities of real-world environments.</p>
<p>This chapter is not just about exploring ML systems’ tools, frameworks, and techniques for detecting and mitigating faults, attacks, and distributional shifts. It’s about emphasizing the crucial role of each one of you in prioritizing resilience throughout the AI development lifecycle, from data collection and model training to deployment and monitoring. By proactively addressing the challenges to robustness, we can unlock the full potential of ML technologies while ensuring their safe, reliable, and responsible deployment in real-world applications.</p>
<p>As AI continues to shape our future, the potential of ML technologies is immense. But it’s only when we build resilient systems that can withstand the challenges of the real world that we can truly harness this potential. This is a defining factor in the success and societal impact of this transformative technology, and it’s within our reach.</p>
</section>
<section id="real-world-examples" class="level2 page-columns page-full" data-number="17.2">
<h2 data-number="17.2" class="anchored" data-anchor-id="real-world-examples"><span class="header-section-number">17.2</span> Real-World Examples</h2>
<p>Here are some real-world examples of cases where faults in hardware or software have caused major issues in ML systems across cloud, edge, and embedded environments:</p>
<section id="cloud" class="level3 page-columns page-full" data-number="17.2.1">
<h3 data-number="17.2.1" class="anchored" data-anchor-id="cloud"><span class="header-section-number">17.2.1</span> Cloud</h3>
<p>In February 2017, Amazon Web Services (AWS) experienced <a href="https://aws.amazon.com/message/41926/">a significant outage</a> due to human error during maintenance. An engineer inadvertently entered an incorrect command, causing many servers to be taken offline. This outage disrupted many AWS services, including Amazon’s AI-powered assistant, Alexa. As a result, Alexa-powered devices, such as Amazon Echo and third-party products using Alexa Voice Service, could not respond to user requests for several hours. This incident highlights the potential impact of human errors on cloud-based ML systems and the need for robust maintenance procedures and failsafe mechanisms.</p>
<p>In another example <span class="citation" data-cites="dixit2021silent">(<a href="../../references.html#ref-dixit2021silent" role="doc-biblioref">Vangal et al. 2021</a>)</span>, Facebook encountered a silent data corruption (SDC) issue within its distributed querying infrastructure, as shown in <a href="#8owvod923jax"><span class="quarto-xref">Figure&nbsp;<span>17.1</span></span></a>. Facebook’s infrastructure includes a querying system that fetches and executes SQL and SQL-like queries across multiple datasets using frameworks like Presto, Hive, and Spark. One of the applications that utilized this querying infrastructure was a compression application to reduce the footprint of data stores. In this compression application, files were compressed when not being read and decompressed when a read request was made. Before decompression, the file size was checked to ensure it was greater than zero, indicating a valid compressed file with contents.</p>
<div class="no-row-height column-margin column-container"><div id="ref-dixit2021silent" class="csl-entry" role="listitem">
Vangal, Sriram, Somnath Paul, Steven Hsu, Amit Agarwal, Saurabh Kumar, Ram Krishnamurthy, Harish Krishnamurthy, James Tschanz, Vivek De, and Chris H. Kim. 2021. <span>“Wide-Range Many-Core <span>SoC</span> Design in Scaled <span>CMOS:</span> <span>Challenges</span> and Opportunities.”</span> <em>IEEE Trans. Very Large Scale Integr. VLSI Syst.</em> 29 (5): 843–56. <a href="https://doi.org/10.1109/tvlsi.2021.3061649">https://doi.org/10.1109/tvlsi.2021.3061649</a>.
</div></div><div id="fig-sdc-example" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-sdc-example-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./images/png/sdc_example.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-sdc-example-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;17.1: Silent data corruption in database applications (Credit: <a href="https://arxiv.org/pdf/2102.11245">Facebook</a>)
</figcaption>
</figure>
</div>
<p>However, in one instance, when the file size was being computed for a valid non-zero-sized file, the decompression algorithm invoked a power function from the Scala library. Unexpectedly, the Scala function returned a zero size value for the file despite having a known non-zero decompressed size. As a result, the decompression was not performed, and the file was not written to the output database. This issue manifested sporadically, with some occurrences of the same file size computation returning the correct non-zero value.</p>
<p>The impact of this silent data corruption was significant, leading to missing files and incorrect data in the output database. The application relying on the decompressed files failed due to the data inconsistencies. In the case study presented in the paper, Facebook’s infrastructure, which consists of hundreds of thousands of servers handling billions of requests per day from their massive user base, encountered a silent data corruption issue. The affected system processed user queries, image uploads, and media content, which required fast, reliable, and secure execution.</p>
<p>This case study illustrates how silent data corruption can propagate through multiple layers of an application stack, leading to data loss and application failures in a large-scale distributed system. The intermittent nature of the issue and the lack of explicit error messages made it particularly challenging to diagnose and resolve. But this is not restricted to just Meta, even other companies such as Google that operate AI hypercomputers face this challenge. <a href="#fig-sdc-jeffdean" class="quarto-xref">Figure&nbsp;<span>17.2</span></a> <a href="https://en.wikipedia.org/wiki/Jeff_Dean">Jeff Dean</a>, Chief Scientist at Google DeepMind and Google Research, discusses SDCS and their impact on ML systems.</p>
<div id="fig-sdc-jeffdean" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-sdc-jeffdean-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./images/jpg/sdc-google-jeff-dean.jpeg" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-sdc-jeffdean-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;17.2: Silent data corruption (SDC) errors are a major issue for AI hypercomputers. (Credit: <a href="https://en.wikipedia.org/wiki/Jeff_Dean">Jeff Dean</a> at <a href="https://mlsys.org/">MLSys 2024</a>, Keynote (Google))
</figcaption>
</figure>
</div>
</section>
<section id="edge" class="level3" data-number="17.2.2">
<h3 data-number="17.2.2" class="anchored" data-anchor-id="edge"><span class="header-section-number">17.2.2</span> Edge</h3>
<p>Regarding examples of faults and errors in edge ML systems, one area that has gathered significant attention is the domain of self-driving cars. Self-driving vehicles rely heavily on machine learning algorithms for perception, decision-making, and control, making them particularly susceptible to the impact of hardware and software faults. In recent years, several high-profile incidents involving autonomous vehicles have highlighted the challenges and risks associated with deploying these systems in real-world environments.</p>
<p>In May 2016, a fatal accident occurred when a Tesla Model S operating on Autopilot crashed into a white semi-trailer truck crossing the highway. The Autopilot system, which relied on computer vision and machine learning algorithms, failed to recognize the white trailer against a bright sky background. The driver, who was reportedly watching a movie when the crash, did not intervene in time, and the vehicle collided with the trailer at full speed. This incident raised concerns about the limitations of AI-based perception systems and the need for robust failsafe mechanisms in autonomous vehicles. It also highlighted the importance of driver awareness and the need for clear guidelines on using semi-autonomous driving features, as shown in <a href="#tckwqf2ctxw"><span class="quarto-xref">Figure&nbsp;<span>17.3</span></span></a>.</p>
<div id="fig-tesla-example" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-tesla-example-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./images/jpg/tesla_example.jpg" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-tesla-example-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;17.3: Tesla in the fatal California crash was on Autopilot (Credit: <a href="https://www.bbc.com/news/world-us-canada-43604440">BBC News</a>)
</figcaption>
</figure>
</div>
<p>In March 2018, an Uber self-driving test vehicle struck and killed a pedestrian crossing the street in Tempe, Arizona. The incident was caused by a software flaw in the vehicle’s object recognition system, which failed to identify the pedestrians appropriately to avoid them as obstacles. The safety driver, who was supposed to monitor the vehicle’s operation and intervene if necessary, was found distracted during the crash. <a href="https://money.cnn.com/2018/03/19/technology/uber-autonomous-car-fatal-crash/index.html?iid=EL">This incident</a> led to widespread scrutiny of Uber’s self-driving program and raised questions about the readiness of autonomous vehicle technology for public roads. It also emphasized the need for rigorous testing, validation, and safety measures in developing and deploying AI-based self-driving systems.</p>
<p>In 2021, Tesla faced increased scrutiny following several accidents involving vehicles operating on Autopilot mode. Some of these accidents were attributed to issues with the Autopilot system’s ability to detect and respond to certain road situations, such as stationary emergency vehicles or obstacles in the road. For example, in April 2021, a Tesla Model S crashed into a tree in Texas, killing two passengers. <a href="https://www.cnbc.com/2021/04/18/no-one-was-driving-in-tesla-crash-that-killed-two-men-in-spring-texas-report.html">Initial reports</a> suggested that no one was in the driver’s seat at the time of the crash, raising questions about the use and potential misuse of Autopilot features. These incidents highlight the ongoing challenges in developing robust and reliable autonomous driving systems and the need for clear regulations and consumer education regarding the capabilities and limitations of these technologies.</p>
</section>
<section id="embedded" class="level3" data-number="17.2.3">
<h3 data-number="17.2.3" class="anchored" data-anchor-id="embedded"><span class="header-section-number">17.2.3</span> Embedded</h3>
<p>Embedded systems, which often operate in resource-constrained environments and safety-critical applications, have long faced challenges related to hardware and software faults. As AI and machine learning technologies are increasingly integrated into these systems, the potential for faults and errors takes on new dimensions, with the added complexity of AI algorithms and the critical nature of the applications in which they are deployed.</p>
<p>Let’s consider a few examples, starting with outer space exploration. NASA’s Mars Polar Lander mission in 1999 suffered <a href="https://spaceref.com/uncategorized/nasa-reveals-probable-cause-of-mars-polar-lander-and-deep-space-2-mission-failures/">a catastrophic failure</a> due to a software error in the touchdown detection system (<a href="#e3z8hq3qpwn4"><span class="quarto-xref">Figure&nbsp;<span>17.4</span></span></a>). The spacecraft’s onboard software mistakenly interpreted the noise from the deployment of its landing legs as a sign that it had touched down on the Martian surface. As a result, the spacecraft prematurely shut down its engines, causing it to crash into the surface. This incident highlights the critical importance of robust software design and extensive testing in embedded systems, especially those operating in remote and unforgiving environments. As AI capabilities are integrated into future space missions, ensuring these systems’ reliability and fault tolerance will be paramount to mission success.</p>
<div id="fig-nasa-example" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-nasa-example-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./images/png/nasa_example.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-nasa-example-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;17.4: NASA’s Failed Mars Polar Lander mission in 1999 cost over $200M (Credit: <a href="https://www.slashgear.com/1094840/nasas-failed-mars-missions-that-cost-over-200-million/">SlashGear</a>)
</figcaption>
</figure>
</div>
<p>Back on earth, in 2015, a Boeing 787 Dreamliner experienced a complete electrical shutdown during a flight due to a software bug in its generator control units. The bug caused the generator control units to enter a failsafe mode, cutting power to the aircraft’s electrical systems and forcing an emergency landing. <a href="https://www.engineering.com/story/vzrxw">This incident</a> underscores the potential for software faults to have severe consequences in complex embedded systems like aircraft. As AI technologies are increasingly applied in aviation, such as in autonomous flight systems and predictive maintenance, ensuring the robustness and reliability of these systems will be critical to passenger safety.</p>
<p>As AI capabilities increasingly integrate into embedded systems, the potential for faults and errors becomes more complex and severe. Imagine a smart <a href="https://www.bbc.com/future/article/20221011-how-space-weather-causes-computer-errors">pacemaker</a> that has a sudden glitch. A patient could die from that effect. Therefore, AI algorithms, such as those used for perception, decision-making, and control, introduce new sources of potential faults, such as data-related issues, model uncertainties, and unexpected behaviors in edge cases. Moreover, the opaque nature of some AI models can make it challenging to identify and diagnose faults when they occur.</p>
</section>
</section>
<section id="hardware-faults" class="level2 page-columns page-full" data-number="17.3">
<h2 data-number="17.3" class="anchored" data-anchor-id="hardware-faults"><span class="header-section-number">17.3</span> Hardware Faults</h2>
<p>Hardware faults are a significant challenge in computing systems, including traditional and ML systems. These faults occur when physical components, such as processors, memory modules, storage devices, or interconnects, malfunction or behave abnormally. Hardware faults can cause incorrect computations, data corruption, system crashes, or complete system failure, compromising the integrity and trustworthiness of the computations performed by the system <span class="citation" data-cites="jha2019ml">(<a href="../../references.html#ref-jha2019ml" role="doc-biblioref">Jha et al. 2019</a>)</span>. A complete system failure refers to a situation where the entire computing system becomes unresponsive or inoperable due to a critical hardware malfunction. This type of failure is the most severe, as it renders the system unusable and may lead to data loss or corruption, requiring manual intervention to repair or replace the faulty components.</p>
<div class="no-row-height column-margin column-container"></div><p>Understanding the taxonomy of hardware faults is essential for anyone working with computing systems, especially in the context of ML systems. ML systems rely on complex hardware architectures and large-scale computations to train and deploy models that learn from data and make intelligent predictions or decisions. However, hardware faults can introduce errors and inconsistencies in the <a href="../../contents/ops/ops.html">MLOps pipeline</a>, affecting the trained models’ accuracy, robustness, and reliability <span class="citation" data-cites="li2017understanding">(<a href="../../references.html#ref-li2017understanding" role="doc-biblioref">G. Li et al. 2017</a>)</span>.</p>
<p>Knowing the different types of hardware faults, their mechanisms, and their potential impact on system behavior is crucial for developing effective strategies to detect, mitigate, and recover them. This knowledge is necessary for designing fault-tolerant computing systems, implementing robust ML algorithms, and ensuring the overall dependability of ML-based applications.</p>
<p>The following sections will explore the three main categories of hardware faults: transient, permanent, and intermittent. We will discuss their definitions, characteristics, causes, mechanisms, and examples of how they manifest in computing systems. We will also cover detection and mitigation techniques specific to each fault type.</p>
<ul>
<li><p><strong>Transient Faults:</strong> Transient faults are temporary and non-recurring. They are often caused by external factors such as cosmic rays, electromagnetic interference, or power fluctuations. A common example of a transient fault is a bit flip, where a single bit in a memory location or register changes its value unexpectedly. Transient faults can lead to incorrect computations or data corruption, but they do not cause permanent damage to the hardware.</p></li>
<li><p><strong>Permanent Faults:</strong> Permanent faults, also called hard errors, are irreversible and persist over time. They are typically caused by physical defects or wear-out of hardware components. Examples of permanent faults include stuck-at faults, where a bit or signal is permanently set to a specific value (e.g., always 0 or always 1), and device failures, such as a malfunctioning processor or a damaged memory module. Permanent faults can result in complete system failure or significant performance degradation.</p></li>
<li><p><strong>Intermittent Faults:</strong> Intermittent faults are recurring faults that appear and disappear intermittently. Unstable hardware conditions, such as loose connections, aging components, or manufacturing defects, often cause them. Intermittent faults can be challenging to diagnose and reproduce because they may occur sporadically and under specific conditions. Examples include intermittent short circuits or contact resistance issues. Intermittent faults can lead to unpredictable system behavior and intermittent errors.</p></li>
</ul>
<p>By the end of this discussion, readers will have a solid understanding of fault taxonomy and its relevance to traditional computing and ML systems. This foundation will help them make informed decisions when designing, implementing, and deploying fault-tolerant solutions, improving the reliability and trustworthiness of their computing systems and ML applications.</p>
<section id="transient-faults" class="level3 page-columns page-full" data-number="17.3.1">
<h3 data-number="17.3.1" class="anchored" data-anchor-id="transient-faults"><span class="header-section-number">17.3.1</span> Transient Faults</h3>
<p>Transient faults in hardware can manifest in various forms, each with its own unique characteristics and causes. These faults are temporary in nature and do not result in permanent damage to the hardware components.</p>
<section id="definition-and-characteristics" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="definition-and-characteristics">Definition and Characteristics</h4>
<p>Some of the common types of transient faults include Single Event Upsets (SEUs) caused by ionizing radiation, voltage fluctuations <span class="citation" data-cites="reddi2013resilient">(<a href="../../references.html#ref-reddi2013resilient" role="doc-biblioref">Reddi and Gupta 2013</a>)</span> due to power supply noise or electromagnetic interference, Electromagnetic Interference (EMI) induced by external electromagnetic fields, Electrostatic Discharge (ESD) resulting from sudden static electricity flow, crosstalk caused by unintended signal coupling, ground bounce triggered by simultaneous switching of multiple outputs, timing violations due to signal timing constraint breaches, and soft errors in combinational logic affecting the output of logic circuits <span class="citation" data-cites="mukherjee2005soft">(<a href="../../references.html#ref-mukherjee2005soft" role="doc-biblioref">Mukherjee, Emer, and Reinhardt 2005</a>)</span>. Understanding these different types of transient faults is crucial for designing robust and resilient hardware systems that can mitigate their impact and ensure reliable operation.</p>
<div class="no-row-height column-margin column-container"><div id="ref-reddi2013resilient" class="csl-entry" role="listitem">
Reddi, Vijay Janapa, and Meeta Sharma Gupta. 2013. <em>Resilient Architecture Design for Voltage Variation</em>. Springer International Publishing. <a href="https://doi.org/10.1007/978-3-031-01739-1">https://doi.org/10.1007/978-3-031-01739-1</a>.
</div><div id="ref-mukherjee2005soft" class="csl-entry" role="listitem">
Mukherjee, S. S., J. Emer, and S. K. Reinhardt. 2005. <span>“The Soft Error Problem: <span>An</span> Architectural Perspective.”</span> In <em>11th International Symposium on High-Performance Computer Architecture</em>, 243–47. IEEE; IEEE. <a href="https://doi.org/10.1109/hpca.2005.37">https://doi.org/10.1109/hpca.2005.37</a>.
</div></div><p>All of these transient faults are characterized by their short duration and non-permanent nature. They do not persist or leave any lasting impact on the hardware. However, they can still lead to incorrect computations, data corruption, or system misbehavior if not properly handled.</p>
<p><img src="./images/png/image22.png" class="img-fluid"></p>
</section>
<section id="causes-of-transient-faults" class="level4">
<h4 class="anchored" data-anchor-id="causes-of-transient-faults">Causes of Transient Faults</h4>
<p>Transient faults can be attributed to various external factors. One common cause is cosmic rays, high-energy particles originating from outer space. When these particles strike sensitive areas of the hardware, such as memory cells or transistors, they can induce charge disturbances that alter the stored or transmitted data. This is illustrated in <a href="#9jd0z5evi3fa"><span class="quarto-xref">Figure&nbsp;<span>17.5</span></span></a>. Another cause of transient faults is <a href="https://www.trentonsystems.com/en-us/resource-hub/blog/what-is-electromagnetic-interference">electromagnetic interference (EMI)</a> from nearby devices or power fluctuations. EMI can couple with the circuits and cause voltage spikes or glitches that temporarily disrupt the normal operation of the hardware.</p>
<div id="fig-transient-fault" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-transient-fault-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./images/png/transient_fault.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-transient-fault-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;17.5: Mechanism of Hardware Transient Fault Occurrence (Credit: <a href="https://group.ntt/en/newsrelease/2018/11/22/181122a.html">NTT</a>)
</figcaption>
</figure>
</div>
</section>
<section id="mechanisms-of-transient-faults" class="level4">
<h4 class="anchored" data-anchor-id="mechanisms-of-transient-faults">Mechanisms of Transient Faults</h4>
<p>Transient faults can manifest through different mechanisms depending on the affected hardware component. In memory devices like DRAM or SRAM, transient faults often lead to bit flips, where a single bit changes its value from 0 to 1 or vice versa. This can corrupt the stored data or instructions. In logic circuits, transient faults can cause glitches or voltage spikes propagating through the combinational logic, resulting in incorrect outputs or control signals. Transient faults can also affect communication channels, causing bit errors or packet losses during data transmission.</p>
</section>
<section id="impact-on-ml-systems" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="impact-on-ml-systems">Impact on ML Systems</h4>
<p>A common example of a transient fault is a bit flip in the main memory. If an important data structure or critical instruction is stored in the affected memory location, it can lead to incorrect computations or program misbehavior. If a transient fault occurs in the memory storing the model weights or gradients. For instance, a bit flip in the memory storing a loop counter can cause the loop to execute indefinitely or terminate prematurely. Transient faults in control registers or flag bits can alter the flow of program execution, leading to unexpected jumps or incorrect branch decisions. In communication systems, transient faults can corrupt transmitted data packets, resulting in retransmissions or data loss.</p>
<p>In ML systems, transient faults can have significant implications during the training phase <span class="citation" data-cites="he2023understanding">(<a href="../../references.html#ref-he2023understanding" role="doc-biblioref">He et al. 2023</a>)</span>. ML training involves iterative computations and updates to model parameters based on large datasets. If a transient fault occurs in the memory storing the model weights or gradients, it can lead to incorrect updates and compromise the convergence and accuracy of the training process. <a href="#fig-sdc-training-fault" class="quarto-xref">Figure&nbsp;<span>17.6</span></a> Show a real-world example from Google’s production fleet where an SDC anomaly caused a significant difference in the gradient norm.</p>
<div class="no-row-height column-margin column-container"></div><div id="fig-sdc-training-fault" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-sdc-training-fault-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./images/jpg/google_sdc_jeff_dean_anomaly.jpg" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-sdc-training-fault-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;17.6: SDC in ML training phase results in anomalies in the gradient norm. (Credit: Jeff Dean, MLSys 2024 Keynote (Google))
</figcaption>
</figure>
</div>
<p>For example, a bit flip in the weight matrix of a neural network can cause the model to learn incorrect patterns or associations, leading to degraded performance <span class="citation" data-cites="wan2021analyzing">(<a href="../../references.html#ref-wan2021analyzing" role="doc-biblioref">Wan et al. 2021</a>)</span>. Transient faults in the data pipeline, such as corruption of training samples or labels, can also introduce noise and affect the quality of the learned model.</p>
<div class="no-row-height column-margin column-container"><div id="ref-wan2021analyzing" class="csl-entry" role="listitem">
Wan, Zishen, Aqeel Anwar, Yu-Shun Hsiao, Tianyu Jia, Vijay Janapa Reddi, and Arijit Raychowdhury. 2021. <span>“Analyzing and Improving Fault Tolerance of Learning-Based Navigation Systems.”</span> In <em>2021 58th ACM/IEEE Design Automation Conference (DAC)</em>, 841–46. IEEE; IEEE. <a href="https://doi.org/10.1109/dac18074.2021.9586116">https://doi.org/10.1109/dac18074.2021.9586116</a>.
</div></div><p>During the inference phase, transient faults can impact the reliability and trustworthiness of ML predictions. If a transient fault occurs in the memory storing the trained model parameters or in the computation of the inference results, it can lead to incorrect or inconsistent predictions. For instance, a bit flip in the activation values of a neural network can alter the final classification or regression output <span class="citation" data-cites="mahmoud2020pytorchfi">(<a href="../../references.html#ref-mahmoud2020pytorchfi" role="doc-biblioref">Mahmoud et al. 2020</a>)</span>.</p>
<p>In safety-critical applications, such as autonomous vehicles or medical diagnosis, transient faults during inference can have severe consequences, leading to incorrect decisions or actions <span class="citation" data-cites="li2017understanding jha2019ml">(<a href="../../references.html#ref-li2017understanding" role="doc-biblioref">G. Li et al. 2017</a>; <a href="../../references.html#ref-jha2019ml" role="doc-biblioref">Jha et al. 2019</a>)</span>. Ensuring the resilience of ML systems against transient faults is crucial to maintaining the integrity and reliability of the predictions.</p>
<div class="no-row-height column-margin column-container"><div id="ref-li2017understanding" class="csl-entry" role="listitem">
Li, Guanpeng, Siva Kumar Sastry Hari, Michael Sullivan, Timothy Tsai, Karthik Pattabiraman, Joel Emer, and Stephen W. Keckler. 2017. <span>“Understanding Error Propagation in Deep Learning Neural Network <span>(DNN)</span> Accelerators and Applications.”</span> In <em>Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis</em>, 1–12. ACM. <a href="https://doi.org/10.1145/3126908.3126964">https://doi.org/10.1145/3126908.3126964</a>.
</div></div></section>
</section>
<section id="permanent-faults" class="level3 page-columns page-full" data-number="17.3.2">
<h3 data-number="17.3.2" class="anchored" data-anchor-id="permanent-faults"><span class="header-section-number">17.3.2</span> Permanent Faults</h3>
<p>Permanent faults are hardware defects that persist and cause irreversible damage to the affected components. These faults are characterized by their persistent nature and require repair or replacement of the faulty hardware to restore normal system functionality.</p>
<section id="definition-and-characteristics-1" class="level4">
<h4 class="anchored" data-anchor-id="definition-and-characteristics-1">Definition and Characteristics</h4>
<p>Permanent faults are hardware defects that cause persistent and irreversible malfunctions in the affected components. The faulty component remains non-operational until a permanent fault is repaired or replaced. These faults are characterized by their consistent and reproducible nature, meaning that the faulty behavior is observed every time the affected component is used. Permanent faults can impact various hardware components, such as processors, memory modules, storage devices, or interconnects, leading to system crashes, data corruption, or complete system failure.</p>
<p>One notable example of a permanent fault is the <a href="https://en.wikipedia.org/wiki/Pentium_FDIV_bug">Intel FDIV bug</a>, which was discovered in 1994. The FDIV bug was a flaw in certain Intel Pentium processors’ floating-point division (FDIV) units. The bug caused incorrect results for specific division operations, leading to inaccurate calculations.</p>
<p>The FDIV bug occurred due to an error in the lookup table used by the division unit. In rare cases, the processor would fetch an incorrect value from the lookup table, resulting in a slightly less precise result than expected. For instance, <a href="#djy9mhqllriw"><span class="quarto-xref">Figure&nbsp;<span>17.7</span></span></a> shows a fraction 4195835/3145727 plotted on a Pentium processor with the FDIV permanent fault. The triangular regions are where erroneous calculations occurred. Ideally, all correct values would round to 1.3338, but the erroneous results show 1.3337, indicating a mistake in the 5th digit.</p>
<p>Although the error was small, it could compound over many division operations, leading to significant inaccuracies in mathematical calculations. The impact of the FDIV bug was significant, especially for applications that relied heavily on precise floating-point division, such as scientific simulations, financial calculations, and computer-aided design. The bug led to incorrect results, which could have severe consequences in fields like finance or engineering.</p>
<div id="fig-permanent-fault" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-permanent-fault-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./images/png/permanent_fault.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-permanent-fault-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;17.7: Intel Pentium processor with the FDIV permanent fault. The triangular regions are where erroneous calculations occurred. (Credit: <a href="https://www.halfhill.com/byte/1995-3_truth.html">Byte Magazine</a>)
</figcaption>
</figure>
</div>
<p>The Intel FDIV bug is a cautionary tale for the potential impact of permanent faults on ML systems. In the context of ML, permanent faults in hardware components can lead to incorrect computations, affecting the accuracy and reliability of the models. For example, if an ML system relies on a processor with a faulty floating-point unit, similar to the Intel FDIV bug, it could introduce errors in the calculations performed during training or inference.</p>
<p>These errors can propagate through the model, leading to inaccurate predictions or skewed learning. In applications where ML is used for critical tasks, such as autonomous driving, medical diagnosis, or financial forecasting, the consequences of incorrect computations due to permanent faults can be severe.</p>
<p>It is crucial for ML practitioners to be aware of the potential impact of permanent faults and to incorporate fault-tolerant techniques, such as hardware redundancy, error detection and correction mechanisms, and robust algorithm design, to mitigate the risks associated with these faults. Additionally, thorough testing and validation of ML hardware components can help identify and address permanent faults before they impact the system’s performance and reliability.</p>
</section>
<section id="causes-of-permanent-faults" class="level4">
<h4 class="anchored" data-anchor-id="causes-of-permanent-faults">Causes of Permanent Faults</h4>
<p>Permanent faults can arise from several causes, including manufacturing defects and wear-out mechanisms. <a href="https://www.sciencedirect.com/science/article/pii/B9780128181058000206">Manufacturing defects</a> are inherent flaws introduced during the fabrication process of hardware components. These defects include improper etching, incorrect doping, or contamination, leading to non-functional or partially functional components.</p>
<p>On the other hand, <a href="https://semiengineering.com/what-causes-semiconductor-aging/">wear-out mechanisms</a> occur over time as the hardware components are subjected to prolonged use and stress. Factors such as electromigration, oxide breakdown, or thermal stress can cause gradual degradation of the components, eventually leading to permanent failures.</p>
</section>
<section id="mechanisms-of-permanent-faults" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="mechanisms-of-permanent-faults">Mechanisms of Permanent Faults</h4>
<p>Permanent faults can manifest through various mechanisms, depending on the nature and location of the fault. Stuck-at faults <span class="citation" data-cites="seong2010safer">(<a href="../../references.html#ref-seong2010safer" role="doc-biblioref">Seong et al. 2010</a>)</span> are common permanent faults where a signal or memory cell remains fixed at a particular value (either 0 or 1) regardless of the inputs, as illustrated in <a href="#ahtmh1s1mxgf"><span class="quarto-xref">Figure&nbsp;<span>17.8</span></span></a>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-seong2010safer" class="csl-entry" role="listitem">
Seong, Nak Hee, Dong Hyuk Woo, Vijayalakshmi Srinivasan, Jude A. Rivers, and Hsien-Hsin S. Lee. 2010. <span>“<span>SAFER:</span> <span class="nocase">Stuck-at-fault</span> Error Recovery for Memories.”</span> In <em>2010 43rd Annual IEEE/ACM International Symposium on Microarchitecture</em>, 115–24. IEEE; IEEE. <a href="https://doi.org/10.1109/micro.2010.46">https://doi.org/10.1109/micro.2010.46</a>.
</div></div><div id="fig-stuck-fault" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-stuck-fault-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./images/png/stuck_fault.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-stuck-fault-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;17.8: Stuck-at Fault Model in Digital Circuits (Credit: <a href="https://accendoreliability.com/digital-circuits-stuck-fault-model/">Accendo Reliability</a>)
</figcaption>
</figure>
</div>
<p>Stuck-at faults can occur in logic gates, memory cells, or interconnects, causing incorrect computations or data corruption. Another mechanism is device failures, where a component, such as a transistor or a memory cell, completely ceases to function. This can be due to manufacturing defects or severe wear-out. Bridging faults occur when two or more signal lines are unintentionally connected, causing short circuits or incorrect logic behavior.</p>
<p>In addition to stuck-at faults, there are several other types of permanent faults that can affect digital circuits that can impact an ML system. Delay faults can cause the propagation delay of a signal to exceed the specified limit, leading to timing violations. Interconnect faults, such as open faults (broken wires), resistive faults (increased resistance), or capacitive faults (increased capacitance), can cause signal integrity issues or timing violations. Memory cells can also suffer from various faults, including transition faults (inability to change state), coupling faults (interference between adjacent cells), and neighborhood pattern sensitive faults (faults that depend on the values of neighboring cells). Other permanent faults can occur in the power supply network or the clock distribution network, affecting the functionality and timing of the circuit.</p>
</section>
<section id="impact-on-ml-systems-1" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="impact-on-ml-systems-1">Impact on ML Systems</h4>
<p>Permanent faults can severely affect the behavior and reliability of computing systems. For example, a stuck-at-fault in a processor’s arithmetic logic unit (ALU) can cause incorrect computations, leading to erroneous results or system crashes. A permanent fault in a memory module, such as a stuck-at fault in a specific memory cell, can corrupt the stored data, causing data loss or program misbehavior. In storage devices, permanent faults like bad sectors or device failures can result in data inaccessibility or complete loss of stored information. Permanent interconnect faults can disrupt communication channels, causing data corruption or system hangs.</p>
<p>Permanent faults can significantly affect ML systems during the training and inference phases. During training, permanent faults in processing units or memory can lead to incorrect computations, resulting in corrupted or suboptimal models <span class="citation" data-cites="he2023understanding">(<a href="../../references.html#ref-he2023understanding" role="doc-biblioref">He et al. 2023</a>)</span>. Furthermore, faults in storage devices can corrupt the training data or the stored model parameters, leading to data loss or model inconsistencies <span class="citation" data-cites="he2023understanding">(<a href="../../references.html#ref-he2023understanding" role="doc-biblioref">He et al. 2023</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-zhang2018analyzing" class="csl-entry" role="listitem">
Zhang, Jeff Jun, Tianyu Gu, Kanad Basu, and Siddharth Garg. 2018. <span>“Analyzing and Mitigating the Impact of Permanent Faults on a Systolic Array Based Neural Network Accelerator.”</span> In <em>2018 IEEE 36th VLSI Test Symposium (VTS)</em>, 1–6. IEEE; IEEE. <a href="https://doi.org/10.1109/vts.2018.8368656">https://doi.org/10.1109/vts.2018.8368656</a>.
</div></div><p>During inference, permanent faults can impact the reliability and correctness of ML predictions. Faults in the processing units can produce incorrect results or cause system failures, while faults in memory storing the model parameters can lead to corrupted or outdated models being used for inference <span class="citation" data-cites="zhang2018analyzing">(<a href="../../references.html#ref-zhang2018analyzing" role="doc-biblioref">J. J. Zhang et al. 2018</a>)</span>.</p>
<p>To mitigate the impact of permanent faults in ML systems, fault-tolerant techniques must be employed at both the hardware and software levels. Hardware redundancy, such as duplicating critical components or using error-correcting codes <span class="citation" data-cites="kim2015bamboo">(<a href="../../references.html#ref-kim2015bamboo" role="doc-biblioref">Kim, Sullivan, and Erez 2015</a>)</span>, can help detect and recover from permanent faults. Software techniques, such as checkpoint and restart mechanisms <span class="citation" data-cites="egwutuoha2013survey">(<a href="../../references.html#ref-egwutuoha2013survey" role="doc-biblioref">Egwutuoha et al. 2013</a>)</span>, can enable the system to recover from permanent faults by returning to a previously saved state. Regular monitoring, testing, and maintenance of ML systems can help identify and replace faulty components before they cause significant disruptions.</p>
<div class="no-row-height column-margin column-container"><div id="ref-kim2015bamboo" class="csl-entry" role="listitem">
Kim, Jungrae, Michael Sullivan, and Mattan Erez. 2015. <span>“Bamboo <span>ECC:</span> <span>Strong,</span> Safe, and Flexible Codes for Reliable Computer Memory.”</span> In <em>2015 IEEE 21st International Symposium on High Performance Computer Architecture (HPCA)</em>, 101–12. IEEE; IEEE. <a href="https://doi.org/10.1109/hpca.2015.7056025">https://doi.org/10.1109/hpca.2015.7056025</a>.
</div><div id="ref-egwutuoha2013survey" class="csl-entry" role="listitem">
Egwutuoha, Ifeanyi P., David Levy, Bran Selic, and Shiping Chen. 2013. <span>“A Survey of Fault Tolerance Mechanisms and Checkpoint/Restart Implementations for High Performance Computing Systems.”</span> <em>The Journal of Supercomputing</em> 65 (3): 1302–26. <a href="https://doi.org/10.1007/s11227-013-0884-0">https://doi.org/10.1007/s11227-013-0884-0</a>.
</div></div><p>Designing ML systems with fault tolerance in mind is crucial to ensure their reliability and robustness in the presence of permanent faults. This may involve incorporating redundancy, error detection and correction mechanisms, and fail-safe strategies into the system architecture. By proactively addressing the challenges posed by permanent faults, ML systems can maintain their integrity, accuracy, and trustworthiness, even in the face of hardware failures.</p>
</section>
</section>
<section id="intermittent-faults" class="level3 page-columns page-full" data-number="17.3.3">
<h3 data-number="17.3.3" class="anchored" data-anchor-id="intermittent-faults"><span class="header-section-number">17.3.3</span> Intermittent Faults</h3>
<p>Intermittent faults are hardware faults that occur sporadically and unpredictably in a system. An example is illustrated in <a href="#kix.1c0l0udn3cp7"><span class="quarto-xref">Figure&nbsp;<span>17.9</span></span></a>, where cracks in the material can introduce increased resistance in circuitry. These faults are particularly challenging to detect and diagnose because they appear and disappear intermittently, making it difficult to reproduce and isolate the root cause. Intermittent faults can lead to system instability, data corruption, and performance degradation.</p>
<div id="fig-intermittent-fault" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-intermittent-fault-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./images/png/intermittent_fault.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-intermittent-fault-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;17.9: Increased resistance due to an intermittent fault – crack between copper bump and package solder (Credit: <a href="https://ieeexplore.ieee.org/document/4925824">Constantinescu</a>)
</figcaption>
</figure>
</div>
<section id="definition-and-characteristics-2" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="definition-and-characteristics-2">Definition and Characteristics</h4>
<p>Intermittent faults are characterized by their sporadic and non-deterministic nature. They occur irregularly and may appear and disappear spontaneously, with varying durations and frequencies. These faults do not consistently manifest every time the affected component is used, making them harder to detect than permanent faults. Intermittent faults can affect various hardware components, including processors, memory modules, storage devices, or interconnects. They can cause transient errors, data corruption, or unexpected system behavior.</p>
<p>Intermittent faults can significantly impact the behavior and reliability of computing systems <span class="citation" data-cites="rashid2014characterizing">(<a href="../../references.html#ref-rashid2014characterizing" role="doc-biblioref">Rashid, Pattabiraman, and Gopalakrishnan 2015</a>)</span>. For example, an intermittent fault in a processor’s control logic can cause irregular program flow, leading to incorrect computations or system hangs. Intermittent faults in memory modules can corrupt data values, resulting in erroneous program execution or data inconsistencies. In storage devices, intermittent faults can cause read/write errors or data loss. Intermittent faults in communication channels can lead to data corruption, packet loss, or intermittent connectivity issues. These faults can cause system crashes, data integrity problems, or performance degradation, depending on the severity and frequency of the intermittent failures.</p>
<div class="no-row-height column-margin column-container"><div id="ref-rashid2014characterizing" class="csl-entry" role="listitem">
———. 2015. <span>“Characterizing the Impact of Intermittent Hardware Faults on Programs.”</span> <em>IEEE Trans. Reliab.</em> 64 (1): 297–310. <a href="https://doi.org/10.1109/tr.2014.2363152">https://doi.org/10.1109/tr.2014.2363152</a>.
</div></div></section>
<section id="causes-of-intermittent-faults" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="causes-of-intermittent-faults">Causes of Intermittent Faults</h4>
<p>Intermittent faults can arise from several causes, both internal and external, to the hardware components <span class="citation" data-cites="constantinescu2008intermittent">(<a href="../../references.html#ref-constantinescu2008intermittent" role="doc-biblioref">Constantinescu 2008</a>)</span>. One common cause is aging and wear-out of the components. As electronic devices age, they become more susceptible to intermittent failures due to degradation mechanisms such as electromigration, oxide breakdown, or solder joint fatigue.</p>
<div class="no-row-height column-margin column-container"><div id="ref-constantinescu2008intermittent" class="csl-entry" role="listitem">
Constantinescu, Cristian. 2008. <span>“Intermittent Faults and Effects on Reliability of Integrated Circuits.”</span> In <em>2008 Annual Reliability and Maintainability Symposium</em>, 370–74. IEEE; IEEE. <a href="https://doi.org/10.1109/rams.2008.4925824">https://doi.org/10.1109/rams.2008.4925824</a>.
</div></div><p>Manufacturing defects or process variations can also introduce intermittent faults, where marginal or borderline components may exhibit sporadic failures under specific conditions, as shown in <a href="#kix.7lswkjecl7ra"><span class="quarto-xref">Figure&nbsp;<span>17.10</span></span></a>.</p>
<p>Environmental factors, such as temperature fluctuations, humidity, or vibrations, can trigger intermittent faults by altering the electrical characteristics of the components. Loose or degraded connections, such as those in connectors or printed circuit boards, can cause intermittent faults.</p>
<div id="fig-intermittent-fault-dram" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-intermittent-fault-dram-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./images/png/intermittent_fault_dram.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-intermittent-fault-dram-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;17.10: Residue induced intermittent fault in a DRAM chip (Credit: <a href="https://ieeexplore.ieee.org/document/4925824">Hynix Semiconductor</a>)
</figcaption>
</figure>
</div>
</section>
<section id="mechanisms-of-intermittent-faults" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="mechanisms-of-intermittent-faults">Mechanisms of Intermittent Faults</h4>
<p>Intermittent faults can manifest through various mechanisms, depending on the underlying cause and the affected component. One mechanism is the intermittent open or short circuit, where a signal path or connection becomes temporarily disrupted or shorted, causing erratic behavior. Another mechanism is the intermittent delay fault <span class="citation" data-cites="zhang2018thundervolt">(<a href="../../references.html#ref-zhang2018thundervolt" role="doc-biblioref">J. Zhang et al. 2018</a>)</span>, where the timing of signals or propagation delays becomes inconsistent, leading to synchronization issues or incorrect computations. Intermittent faults can manifest as transient bit flips or soft errors in memory cells or registers, causing data corruption or incorrect program execution.</p>
<div class="no-row-height column-margin column-container"><div id="ref-zhang2018thundervolt" class="csl-entry" role="listitem">
Zhang, Jeff, Kartheek Rangineni, Zahra Ghodsi, and Siddharth Garg. 2018. <span>“<span>ThUnderVolt:</span> <span>Enabling</span> Aggressive Voltage Underscaling and Timing Error Resilience for Energy Efficient Deep Learning Accelerators.”</span> In <em>2018 55th ACM/ESDA/IEEE Design Automation Conference (DAC)</em>, 1–6. IEEE. <a href="https://doi.org/10.1109/dac.2018.8465918">https://doi.org/10.1109/dac.2018.8465918</a>.
</div></div></section>
<section id="impact-on-ml-systems-2" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="impact-on-ml-systems-2">Impact on ML Systems</h4>
<p>In the context of ML systems, intermittent faults can introduce significant challenges and impact the system’s reliability and performance. During the training phase, intermittent faults in processing units or memory can lead to inconsistencies in computations, resulting in incorrect or noisy gradients and weight updates. This can affect the convergence and accuracy of the training process, leading to suboptimal or unstable models. Intermittent data storage or retrieval faults can corrupt the training data, introducing noise or errors that degrade the quality of the learned models <span class="citation" data-cites="he2023understanding">(<a href="../../references.html#ref-he2023understanding" role="doc-biblioref">He et al. 2023</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-he2023understanding" class="csl-entry" role="listitem">
He, Yi, Mike Hutton, Steven Chan, Robert De Gruijl, Rama Govindaraju, Nishant Patil, and Yanjing Li. 2023. <span>“Understanding and Mitigating Hardware Failures in Deep Learning Training Systems.”</span> In <em>Proceedings of the 50th Annual International Symposium on Computer Architecture</em>, 1–16. IEEE; ACM. <a href="https://doi.org/10.1145/3579371.3589105">https://doi.org/10.1145/3579371.3589105</a>.
</div></div><p>During the inference phase, intermittent faults can impact the reliability and consistency of ML predictions. Faults in the processing units or memory can cause incorrect computations or data corruption, leading to erroneous or inconsistent predictions. Intermittent faults in the data pipeline can introduce noise or errors in the input data, affecting the accuracy and robustness of the predictions. In safety-critical applications, such as autonomous vehicles or medical diagnosis systems, intermittent faults can have severe consequences, leading to incorrect decisions or actions that compromise safety and reliability.</p>
<p>Mitigating the impact of intermittent faults in ML systems requires a multifaceted approach <span class="citation" data-cites="rashid2012intermittent">(<a href="../../references.html#ref-rashid2012intermittent" role="doc-biblioref">Rashid, Pattabiraman, and Gopalakrishnan 2012</a>)</span>. At the hardware level, techniques such as robust design practices, component selection, and environmental control can help reduce the occurrence of intermittent faults. Redundancy and error correction mechanisms can be employed to detect and recover from intermittent failures. At the software level, runtime monitoring, anomaly detection, and fault-tolerant techniques can be incorporated into the ML pipeline. This may include techniques such as data validation, outlier detection, model ensembling, or runtime model adaptation to handle intermittent faults gracefully.</p>
<div class="no-row-height column-margin column-container"><div id="ref-rashid2012intermittent" class="csl-entry" role="listitem">
Rashid, Layali, Karthik Pattabiraman, and Sathish Gopalakrishnan. 2012. <span>“Intermittent Hardware Errors Recovery: <span>Modeling</span> and Evaluation.”</span> In <em>2012 Ninth International Conference on Quantitative Evaluation of Systems</em>, 220–29. IEEE; IEEE. <a href="https://doi.org/10.1109/qest.2012.37">https://doi.org/10.1109/qest.2012.37</a>.
</div></div><p>Designing ML systems resilient to intermittent faults is crucial to ensuring their reliability and robustness. This involves incorporating fault-tolerant techniques, runtime monitoring, and adaptive mechanisms into the system architecture. By proactively addressing the challenges of intermittent faults, ML systems can maintain their accuracy, consistency, and trustworthiness, even in sporadic hardware failures. Regular testing, monitoring, and maintenance of ML systems can help identify and mitigate intermittent faults before they cause significant disruptions or performance degradation.</p>
</section>
</section>
<section id="detection-and-mitigation" class="level3 page-columns page-full" data-number="17.3.4">
<h3 data-number="17.3.4" class="anchored" data-anchor-id="detection-and-mitigation"><span class="header-section-number">17.3.4</span> Detection and Mitigation</h3>
<p>This section explores various fault detection techniques, including hardware-level and software-level approaches, and discusses effective mitigation strategies to enhance the resilience of ML systems. Additionally, we will look into resilient ML system design considerations, present case studies and examples, and highlight future research directions in fault-tolerant ML systems.</p>
<section id="fault-detection-techniques" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="fault-detection-techniques">Fault Detection Techniques</h4>
<p>Fault detection techniques are important for identifying and localizing hardware faults in ML systems. These techniques can be broadly categorized into hardware-level and software-level approaches, each offering unique capabilities and advantages.</p>
<section id="hardware-level-fault-detection" class="level5 page-columns page-full">
<h5 class="anchored" data-anchor-id="hardware-level-fault-detection">Hardware-level fault detection</h5>
<p>Hardware-level fault detection techniques are implemented at the physical level of the system and aim to identify faults in the underlying hardware components. There are several hardware techniques, but broadly, we can bucket these different mechanisms into the following categories.</p>
<p><strong>Built-in self-test (BIST) mechanisms:</strong> BIST is a powerful technique for detecting faults in hardware components <span class="citation" data-cites="bushnell2002built">(<a href="../../references.html#ref-bushnell2002built" role="doc-biblioref">Bushnell and Agrawal 2002</a>)</span>. It involves incorporating additional hardware circuitry into the system for self-testing and fault detection. BIST can be applied to various components, such as processors, memory modules, or application-specific integrated circuits (ASICs). For example, BIST can be implemented in a processor using scan chains, which are dedicated paths that allow access to internal registers and logic for testing purposes.</p>
<div class="no-row-height column-margin column-container"><div id="ref-bushnell2002built" class="csl-entry" role="listitem">
Bushnell, Michael L, and Vishwani D Agrawal. 2002. <span>“Built-in Self-Test.”</span> <em>Essentials of Electronic Testing for Digital, Memory and Mixed-Signal VLSI Circuits</em>, 489–548.
</div></div><p>During the BIST process, predefined test patterns are applied to the processor’s internal circuitry, and the responses are compared against expected values. Any discrepancies indicate the presence of faults. Intel’s Xeon processors, for instance, include BIST mechanisms to test the CPU cores, cache memory, and other critical components during system startup.</p>
<p><strong>Error detection codes:</strong> Error detection codes are widely used to detect data storage and transmission errors <span class="citation" data-cites="hamming1950error">(<a href="../../references.html#ref-hamming1950error" role="doc-biblioref">Hamming 1950</a>)</span>. These codes add redundant bits to the original data, allowing the detection of bit errors. Example: Parity checks are a simple form of error detection code shown in <a href="#kix.2vxlbeehnemj"><span class="quarto-xref">Figure&nbsp;<span>17.11</span></span></a>. In a single-bit parity scheme, an extra bit is appended to each data word, making the number of 1s in the word even (even parity) or odd (odd parity).</p>
<div class="no-row-height column-margin column-container"><div id="ref-hamming1950error" class="csl-entry" role="listitem">
Hamming, R. W. 1950. <span>“Error Detecting and Error Correcting Codes.”</span> <em>Bell Syst. Tech. J.</em> 29 (2): 147–60. <a href="https://doi.org/10.1002/j.1538-7305.1950.tb00463.x">https://doi.org/10.1002/j.1538-7305.1950.tb00463.x</a>.
</div></div><div id="fig-parity" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-parity-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./images/png/parity.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-parity-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;17.11: Parity bit example (Credit: <a href="https://www.computerhope.com/jargon/p/paritybi.htm">Computer Hope</a>)
</figcaption>
</figure>
</div>
<p>When reading the data, the parity is checked, and if it doesn’t match the expected value, an error is detected. More advanced error detection codes, such as cyclic redundancy checks (CRC), calculate a checksum based on the data and append it to the message. The checksum is recalculated at the receiving end and compared with the transmitted checksum to detect errors. Error-correcting code (ECC) memory modules, commonly used in servers and critical systems, employ advanced error detection and correction codes to detect and correct single-bit or multi-bit errors in memory.</p>
<p><strong>Hardware redundancy and voting mechanisms:</strong> Hardware redundancy involves duplicating critical components and comparing their outputs to detect and mask faults <span class="citation" data-cites="sheaffer2007hardware">(<a href="../../references.html#ref-sheaffer2007hardware" role="doc-biblioref">Sheaffer, Luebke, and Skadron 2007</a>)</span>. Voting mechanisms, such as triple modular redundancy (TMR), employ multiple instances of a component and compare their outputs to identify and mask faulty behavior <span class="citation" data-cites="arifeen2020approximate">(<a href="../../references.html#ref-arifeen2020approximate" role="doc-biblioref">Arifeen, Hassan, and Lee 2020</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-sheaffer2007hardware" class="csl-entry" role="listitem">
Sheaffer, Jeremy W, David P Luebke, and Kevin Skadron. 2007. <span>“A Hardware Redundancy and Recovery Mechanism for Reliable Scientific Computation on Graphics Processors.”</span> In <em>Graphics Hardware</em>, 2007:55–64. Citeseer.
</div><div id="ref-arifeen2020approximate" class="csl-entry" role="listitem">
Arifeen, Tooba, Abdus Sami Hassan, and Jeong-A Lee. 2020. <span>“Approximate Triple Modular Redundancy: <span>A</span> Survey.”</span> <em>#IEEE_O_ACC#</em> 8: 139851–67. <a href="https://doi.org/10.1109/access.2020.3012673">https://doi.org/10.1109/access.2020.3012673</a>.
</div><div id="ref-yeh1996triple" class="csl-entry" role="listitem">
Yeh, Y. C. 1996. <span>“Triple-Triple Redundant 777 Primary Flight Computer.”</span> In <em>1996 IEEE Aerospace Applications Conference. Proceedings</em>, 1:293–307. IEEE; IEEE. <a href="https://doi.org/10.1109/aero.1996.495891">https://doi.org/10.1109/aero.1996.495891</a>.
</div></div><p>In a TMR system, three identical instances of a hardware component, such as a processor or a sensor, perform the same computation in parallel. The outputs of these instances are fed into a voting circuit, which compares the results and selects the majority value as the final output. If one of the instances produces an incorrect result due to a fault, the voting mechanism masks the error and maintains the correct output. TMR is commonly used in aerospace and aviation systems, where high reliability is critical. For instance, the Boeing 777 aircraft employs TMR in its primary flight computer system to ensure the availability and correctness of flight control functions <span class="citation" data-cites="yeh1996triple">(<a href="../../references.html#ref-yeh1996triple" role="doc-biblioref">Yeh 1996</a>)</span>.</p>
<p>Tesla’s self-driving computers employ a redundant hardware architecture to ensure the safety and reliability of critical functions, such as perception, decision-making, and vehicle control, as shown in <a href="#kix.nsc1yczcug9r"><span class="quarto-xref">Figure&nbsp;<span>17.12</span></span></a>. One key component of this architecture is using dual modular redundancy (DMR) in the car’s onboard computer systems.</p>
<div id="fig-tesla-dmr" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-tesla-dmr-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./images/png/tesla_dmr.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-tesla-dmr-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;17.12: Tesla full self-driving computer with dual redundant SoCs (Credit: <a href="https://old.hotchips.org/hc31/HC31_2.3_Tesla_Hotchips_ppt_Final_0817.pdf">Tesla</a>)
</figcaption>
</figure>
</div>
<p>In Tesla’s DMR implementation, two identical hardware units, often called “redundant computers” or “redundant control units,” perform the same computations in parallel <span class="citation" data-cites="bannon2019computer">(<a href="../../references.html#ref-bannon2019computer" role="doc-biblioref">Bannon et al. 2019</a>)</span>. Each unit independently processes sensor data, executes perception and decision-making algorithms, and generates control commands for the vehicle’s actuators (e.g., steering, acceleration, and braking).</p>
<div class="no-row-height column-margin column-container"><div id="ref-bannon2019computer" class="csl-entry" role="listitem">
Bannon, Pete, Ganesh Venkataramanan, Debjit Das Sarma, and Emil Talpes. 2019. <span>“Computer and Redundancy Solution for the Full Self-Driving Computer.”</span> In <em>2019 IEEE Hot Chips 31 Symposium (HCS)</em>, 1–22. IEEE Computer Society; IEEE. <a href="https://doi.org/10.1109/hotchips.2019.8875645">https://doi.org/10.1109/hotchips.2019.8875645</a>.
</div></div><p>The outputs of these two redundant units are continuously compared to detect any discrepancies or faults. If the outputs match, the system assumes that both units function correctly, and the control commands are sent to the vehicle’s actuators. However, if there is a mismatch between the outputs, the system identifies a potential fault in one of the units and takes appropriate action to ensure safe operation.</p>
<p>The system may employ additional mechanisms to determine which unit is faulty in a mismatch. This can involve using diagnostic algorithms, comparing the outputs with data from other sensors or subsystems, or analyzing the consistency of the outputs over time. Once the faulty unit is identified, the system can isolate it and continue operating using the output from the non-faulty unit.</p>
<p>DMR in Tesla’s self-driving computer provides an extra safety and fault tolerance layer. By having two independent units performing the same computations, the system can detect and mitigate faults that may occur in one of the units. This redundancy helps prevent single points of failure and ensures that critical functions remain operational despite hardware faults.</p>
<p>Furthermore, Tesla also incorporates additional redundancy mechanisms beyond DMR. For example, they utilize redundant power supplies, steering and braking systems, and diverse sensor suites (e.g., cameras, radar, and ultrasonic sensors) to provide multiple layers of fault tolerance. These redundancies collectively contribute to the overall safety and reliability of the self-driving system.</p>
<p>It’s important to note that while DMR provides fault detection and some level of fault tolerance, TMR may provide a different level of fault masking. In DMR, if both units experience simultaneous faults or the fault affects the comparison mechanism, the system may be unable to identify the fault. Therefore, Tesla’s SDCs rely on a combination of DMR and other redundancy mechanisms to achieve a high level of fault tolerance.</p>
<p>The use of DMR in Tesla’s self-driving computer highlights the importance of hardware redundancy in safety-critical applications. By employing redundant computing units and comparing their outputs, the system can detect and mitigate faults, enhancing the overall safety and reliability of the self-driving functionality.</p>
<p>Google employs redundant hot spares to deal with SDC issues within its data centers, thereby enhancing the reliability of critical functions. As illustrated in <a href="#fig-sdc-controller" class="quarto-xref">Figure&nbsp;<span>17.13</span></a>, during the normal training phase, multiple synchronous training workers function flawlessly. However, if a worker becomes defective and causes SDC, an SDC checker automatically identifies the issues. Upon detecting the SDC, the SDC checker moves the training to a hot spare and sends the defective machine for repair. This redundancy safeguards the continuity and reliability of ML training, effectively minimizing downtime and preserving data integrity.</p>
<div id="fig-sdc-controller" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-sdc-controller-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./images/jpg/sdc_controller_google.jpg" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-sdc-controller-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;17.13: Google employs hot spare cores to transparently handle SDCs in the data center. (Credit: Jeff Dean, MLSys 2024 Keynote (Google))
</figcaption>
</figure>
</div>
<p><strong>Watchdog timers:</strong> Watchdog timers are hardware components that monitor the execution of critical tasks or processes <span class="citation" data-cites="pont2002using">(<a href="../../references.html#ref-pont2002using" role="doc-biblioref">Pont and Ong 2002</a>)</span>. They are commonly used to detect and recover from software or hardware faults that cause a system to become unresponsive or stuck in an infinite loop. In an embedded system, a watchdog timer can be configured to monitor the execution of the main control loop, as illustrated in <a href="#3l259jcz0lli"><span class="quarto-xref">Figure&nbsp;<span>17.14</span></span></a>. The software periodically resets the watchdog timer to indicate that it functions correctly. Suppose the software fails to reset the timer within a specified time limit (timeout period). In that case, the watchdog timer assumes that the system has encountered a fault and triggers a predefined recovery action, such as resetting the system or switching to a backup component. Watchdog timers are widely used in automotive electronics, industrial control systems, and other safety-critical applications to ensure the timely detection and recovery from faults.</p>
<div class="no-row-height column-margin column-container"><div id="ref-pont2002using" class="csl-entry" role="listitem">
Pont, Michael J, and Royan HL Ong. 2002. <span>“Using Watchdog Timers to Improve the Reliability of Single-Processor Embedded Systems: <span>Seven</span> New Patterns and a Case Study.”</span> In <em>Proceedings of the First Nordic Conference on Pattern Languages of Programs</em>, 159–200. Citeseer.
</div></div><div id="fig-watchdog" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-watchdog-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./images/png/watchdog.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-watchdog-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;17.14: Watchdog timer example in detecting MCU faults (Credit: <a href="https://www.ablic.com/en/semicon/products/automotive/automotive-watchdog-timer/intro/">Ablic</a>)
</figcaption>
</figure>
</div>
</section>
<section id="software-level-fault-detection" class="level5 page-columns page-full">
<h5 class="anchored" data-anchor-id="software-level-fault-detection">Software-level fault detection</h5>
<p>Software-level fault detection techniques rely on software algorithms and monitoring mechanisms to identify system faults. These techniques can be implemented at various levels of the software stack, including the operating system, middleware, or application level.</p>
<p><strong>Runtime monitoring and anomaly detection:</strong> Runtime monitoring involves continuously observing the behavior of the system and its components during execution <span class="citation" data-cites="francalanza2017foundation">(<a href="../../references.html#ref-francalanza2017foundation" role="doc-biblioref">Francalanza et al. 2017</a>)</span>. It helps detect anomalies, errors, or unexpected behavior that may indicate the presence of faults. For example, consider an ML-based image classification system deployed in a self-driving car. Runtime monitoring can be implemented to track the classification model’s performance and behavior <span class="citation" data-cites="mahmoud2021issre">(<a href="../../references.html#ref-mahmoud2021issre" role="doc-biblioref">Mahmoud et al. 2021</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-francalanza2017foundation" class="csl-entry" role="listitem">
Francalanza, Adrian, Luca Aceto, Antonis Achilleos, Duncan Paul Attard, Ian Cassar, Dario Della Monica, and Anna Ingólfsdóttir. 2017. <span>“A Foundation for Runtime Monitoring.”</span> In <em>International Conference on Runtime Verification</em>, 8–29. Springer.
</div><div id="ref-mahmoud2021issre" class="csl-entry" role="listitem">
Mahmoud, Abdulrahman, Siva Kumar Sastry Hari, Christopher W. Fletcher, Sarita V. Adve, Charbel Sakr, Naresh Shanbhag, Pavlo Molchanov, Michael B. Sullivan, Timothy Tsai, and Stephen W. Keckler. 2021. <span>“Optimizing Selective Protection for <span>CNN</span> Resilience.”</span> In <em>2021 IEEE 32nd International Symposium on Software Reliability Engineering (ISSRE)</em>, 127–38. IEEE. <a href="https://doi.org/10.1109/issre52982.2021.00025">https://doi.org/10.1109/issre52982.2021.00025</a>.
</div><div id="ref-chandola2009anomaly" class="csl-entry" role="listitem">
Chandola, Varun, Arindam Banerjee, and Vipin Kumar. 2009. <span>“Anomaly Detection: A Survey.”</span> <em>ACM Comput. Surv.</em> 41 (3): 1–58. <a href="https://doi.org/10.1145/1541880.1541882">https://doi.org/10.1145/1541880.1541882</a>.
</div></div><p>Anomaly detection algorithms can be applied to the model’s predictions or intermediate layer activations, such as statistical outlier detection or machine learning-based approaches (e.g., One-Class SVM or Autoencoders) <span class="citation" data-cites="chandola2009anomaly">(<a href="../../references.html#ref-chandola2009anomaly" role="doc-biblioref">Chandola, Banerjee, and Kumar 2009</a>)</span>. <a href="#a0u8fu59ui0r"><span class="quarto-xref">Figure&nbsp;<span>17.15</span></span></a> shows example of anomaly detection. Suppose the monitoring system detects a significant deviation from the expected patterns, such as a sudden drop in classification accuracy or out-of-distribution samples. In that case, it can raise an alert indicating a potential fault in the model or the input data pipeline. This early detection allows for timely intervention and fault mitigation strategies to be applied.</p>
<div id="fig-ad" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-ad-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./images/png/ad.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-ad-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;17.15: Examples of anomaly detection. (a) Fully supervised anomaly detection, (b) normal-only anomaly detection, (c, d, e) semi-supervised anomaly detection, (f) unsupervised anomaly detection (Credit: <a href="https://www.google.com/url?sa=i&amp;url=http%3A%2F%2Fresearch.google%2Fblog%2Funsupervised-and-semi-supervised-anomaly-detection-with-data-centric-ml%2F&amp;psig=AOvVaw1p9owe13lxfZogUHTZnxrj&amp;ust=1714877457779000&amp;source=images&amp;cd=vfe&amp;opi=89978449&amp;ved=0CBIQjRxqFwoTCIjMmMP-8oUDFQAAAAAdAAAAABAE">Google</a>)
</figcaption>
</figure>
</div>
<p><strong>Consistency checks and data validation:</strong> Consistency checks and data validation techniques ensure data integrity and correctness at different processing stages in an ML system <span class="citation" data-cites="lindholm2019data">(<a href="../../references.html#ref-lindholm2019data" role="doc-biblioref">Lindholm et al. 2019</a>)</span>. These checks help detect data corruption, inconsistencies, or errors that may propagate and affect the system’s behavior. Example: In a distributed ML system where multiple nodes collaborate to train a model, consistency checks can be implemented to validate the integrity of the shared model parameters. Each node can compute a checksum or hash of the model parameters before and after the training iteration, as shown in <a href="#fig-ad" class="quarto-xref">Figure&nbsp;<span>17.15</span></a>. Any inconsistencies or data corruption can be detected by comparing the checksums across nodes. Additionally, range checks can be applied to the input data and model outputs to ensure they fall within expected bounds. For instance, if an autonomous vehicle’s perception system detects an object with unrealistic dimensions or velocities, it can indicate a fault in the sensor data or the perception algorithms <span class="citation" data-cites="wan2023vpp">(<a href="../../references.html#ref-wan2023vpp" role="doc-biblioref">Wan et al. 2023</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-lindholm2019data" class="csl-entry" role="listitem">
Lindholm, Andreas, Dave Zachariah, Petre Stoica, and Thomas B. Schon. 2019. <span>“Data Consistency Approach to Model Validation.”</span> <em>#IEEE_O_ACC#</em> 7: 59788–96. <a href="https://doi.org/10.1109/access.2019.2915109">https://doi.org/10.1109/access.2019.2915109</a>.
</div><div id="ref-wan2023vpp" class="csl-entry" role="listitem">
Wan, Zishen, Yiming Gan, Bo Yu, S Liu, A Raychowdhury, and Y Zhu. 2023. <span>“Vpp: <span>The</span> Vulnerability-Proportional Protection Paradigm Towards Reliable Autonomous Machines.”</span> In <em>Proceedings of the 5th International Workshop on Domain Specific System Architecture (DOSSA)</em>, 1–6.
</div><div id="ref-kawazoe1997heartbeat" class="csl-entry" role="listitem">
Kawazoe Aguilera, Marcos, Wei Chen, and Sam Toueg. 1997. <span>“Heartbeat: <span>A</span> Timeout-Free Failure Detector for Quiescent Reliable Communication.”</span> In <em>Distributed Algorithms: 11th International Workshop, WDAG’97 Saarbrücken, Germany, September 24<span></span>26, 1997 Proceedings 11</em>, 126–40. Springer.
</div></div><p><strong>Heartbeat and timeout mechanisms:</strong> Heartbeat mechanisms and timeouts are commonly used to detect faults in distributed systems and ensure the liveness and responsiveness of components <span class="citation" data-cites="kawazoe1997heartbeat">(<a href="../../references.html#ref-kawazoe1997heartbeat" role="doc-biblioref">Kawazoe Aguilera, Chen, and Toueg 1997</a>)</span>. These are quite similar to the watchdog timers found in hardware. For example, in a distributed ML system, where multiple nodes collaborate to perform tasks such as data preprocessing, model training, or inference, heartbeat mechanisms can be implemented to monitor the health and availability of each node. Each node periodically sends a heartbeat message to a central coordinator or its peer nodes, indicating its status and availability. Suppose a node fails to send a heartbeat within a specified timeout period, as shown in <a href="#ojufkz2g56e"><span class="quarto-xref">Figure&nbsp;<span>17.16</span></span></a>. In that case, it is considered faulty, and appropriate actions can be taken, such as redistributing the workload or initiating a failover mechanism. Timeouts can also be used to detect and handle hanging or unresponsive components. For example, if a data loading process exceeds a predefined timeout threshold, it may indicate a fault in the data pipeline, and the system can take corrective measures.</p>
<div id="fig-heartbeat" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-heartbeat-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./images/png/heartbeat.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-heartbeat-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;17.16: Heartbeat messages in distributed systems (Credit: <a href="https://www.geeksforgeeks.org/what-are-heartbeat-messages/">GeeksforGeeks</a>)
</figcaption>
</figure>
</div>
<!-- @fig-Reed-Solomon Heartbeat messages in distributed systems. Credit: [GeeksforGeeks](https://www.geeksforgeeks.org/what-are-heartbeat-messages/) -->
<p><strong>Software-implemented fault tolerance (SIFT) techniques:</strong> SIFT techniques introduce redundancy and fault detection mechanisms at the software level to enhance the reliability and fault tolerance of the system <span class="citation" data-cites="reis2005swift">(<a href="../../references.html#ref-reis2005swift" role="doc-biblioref">Reis et al. 2005</a>)</span>. Example: N-version programming is a SIFT technique where multiple functionally equivalent software component versions are developed independently by different teams. This can be applied to critical components such as the model inference engine in an ML system. Multiple versions of the inference engine can be executed in parallel, and their outputs can be compared for consistency. It is considered the correct result if most versions produce the same output. If there is a discrepancy, it indicates a potential fault in one or more versions, and appropriate error-handling mechanisms can be triggered. Another example is using software-based error correction codes, such as Reed-Solomon codes <span class="citation" data-cites="plank1997tutorial">(<a href="../../references.html#ref-plank1997tutorial" role="doc-biblioref">Plank 1997</a>)</span>, to detect and correct errors in data storage or transmission, as shown in <a href="#kjmtegsny44z"><span class="quarto-xref">Figure&nbsp;<span>17.17</span></span></a>. These codes add redundancy to the data, enabling detecting and correcting certain errors and enhancing the system’s fault tolerance.</p>
<div class="no-row-height column-margin column-container"><div id="ref-reis2005swift" class="csl-entry" role="listitem">
Reis, G. A., J. Chang, N. Vachharajani, R. Rangan, and D. I. August. 2005. <span>“<span>SWIFT:</span> <span>Software</span> Implemented Fault Tolerance.”</span> In <em>International Symposium on Code Generation and Optimization</em>, 243–54. IEEE; IEEE. <a href="https://doi.org/10.1109/cgo.2005.34">https://doi.org/10.1109/cgo.2005.34</a>.
</div><div id="ref-plank1997tutorial" class="csl-entry" role="listitem">
Plank, James S. 1997. <span>“A Tutorial on <span>Reed<span></span>Solomon</span> Coding for Fault-Tolerance in <span>RAID</span>-Like Systems.”</span> <em>Software: Practice and Experience</em> 27 (9): 995–1012.
</div></div><div id="fig-Reed-Solomon" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-Reed-Solomon-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./images/png/Reed-Solomon.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-Reed-Solomon-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;17.17: n-bits representation of the Reed-Solomon codes (Credit: <a href="https://www.geeksforgeeks.org/what-is-reed-solomon-code/">GeeksforGeeks</a>)
</figcaption>
</figure>
</div>
<div id="exr-ad" class="callout callout-style-default callout-caution callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Exercise&nbsp;17.1: Anomaly Detection
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<p>In this Colab, play the role of an AI fault detective! You’ll build an autoencoder-based anomaly detector to pinpoint errors in heart health data. Learn how to identify malfunctions in ML systems, a vital skill for creating dependable AI. We’ll use Keras Tuner to fine-tune your autoencoder for top-notch fault detection. This experience directly links to the Robust AI chapter, demonstrating the importance of fault detection in real-world applications like healthcare and autonomous systems. Get ready to strengthen the reliability of your AI creations!</p>
<p><a href="https://colab.research.google.com/drive/1TXaQzsSj2q0E3Ni1uxFDXGpY1SCnu46v?usp=sharing"><img src="https://colab.research.google.com/assets/colab-badge.png" class="img-fluid"></a></p>
</div>
</div>
</div>
</section>
</section>
</section>
<section id="summary" class="level3" data-number="17.3.5">
<h3 data-number="17.3.5" class="anchored" data-anchor-id="summary"><span class="header-section-number">17.3.5</span> Summary</h3>
<p><a href="#tbl-fault_types" class="quarto-xref">Table&nbsp;<span>17.1</span></a> provides an extensive comparative analysis of transient, permanent, and intermittent faults. It outlines the primary characteristics or dimensions that distinguish these fault types. Here, we summarize the relevant dimensions we examined and explore the nuances that differentiate transient, permanent, and intermittent faults in greater detail.</p>
<div id="tbl-fault_types" class="quarto-float anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-fault_types-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;17.1: Comparison of transient, permanent, and intermittent faults.
</figcaption>
<div aria-describedby="tbl-fault_types-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="table">
<colgroup>
<col style="width: 16%">
<col style="width: 26%">
<col style="width: 26%">
<col style="width: 30%">
</colgroup>
<thead>
<tr class="header">
<th>Dimension</th>
<th>Transient Faults</th>
<th>Permanent Faults</th>
<th>Intermittent Faults</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Duration</td>
<td>Short-lived, temporary</td>
<td>Persistent, remains until repair or replacement</td>
<td>Sporadic, appears and disappears intermittently</td>
</tr>
<tr class="even">
<td>Persistence</td>
<td>Disappears after the fault condition passes</td>
<td>Consistently present until addressed</td>
<td>Recurs irregularly, not always present</td>
</tr>
<tr class="odd">
<td>Causes</td>
<td>External factors (e.g., electromagnetic interference, cosmic rays)</td>
<td>Hardware defects, physical damage, wear-out</td>
<td>Unstable hardware conditions, loose connections, aging components</td>
</tr>
<tr class="even">
<td>Manifestation</td>
<td>Bit flips, glitches, temporary data corruption</td>
<td>Stuck-at faults, broken components, complete device failures</td>
<td>Occasional bit flips, intermittent signal issues, sporadic malfunctions</td>
</tr>
<tr class="odd">
<td>Impact on ML Systems</td>
<td>Introduces temporary errors or noise in computations</td>
<td>Causes consistent errors or failures, affecting reliability</td>
<td>Leads to sporadic and unpredictable errors, challenging to diagnose and mitigate</td>
</tr>
<tr class="even">
<td>Detection</td>
<td>Error detection codes, comparison with expected values</td>
<td>Built-in self-tests, error detection codes, consistency checks</td>
<td>Monitoring for anomalies, analyzing error patterns and correlations</td>
</tr>
<tr class="odd">
<td>Mitigation</td>
<td>Error correction codes, redundancy, checkpoint and restart</td>
<td>Hardware repair or replacement, component redundancy, failover mechanisms</td>
<td>Robust design, environmental control, runtime monitoring, fault-tolerant techniques</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
</section>
</section>
<section id="ml-model-robustness" class="level2 page-columns page-full" data-number="17.4">
<h2 data-number="17.4" class="anchored" data-anchor-id="ml-model-robustness"><span class="header-section-number">17.4</span> ML Model Robustness</h2>
<section id="adversarial-attacks" class="level3 page-columns page-full" data-number="17.4.1">
<h3 data-number="17.4.1" class="anchored" data-anchor-id="adversarial-attacks"><span class="header-section-number">17.4.1</span> Adversarial Attacks</h3>
<section id="definition-and-characteristics-3" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="definition-and-characteristics-3">Definition and Characteristics</h4>
<p>Adversarial attacks aim to trick models into making incorrect predictions by providing them with specially crafted, deceptive inputs (called adversarial examples) <span class="citation" data-cites="parrish2023adversarial">(<a href="../../references.html#ref-parrish2023adversarial" role="doc-biblioref">Parrish et al. 2023</a>)</span>. By adding slight perturbations to input data, adversaries can "hack" a model’s pattern recognition and deceive it. These are sophisticated techniques where slight, often imperceptible alterations to input data can trick an ML model into making a wrong prediction, as shown in <a href="#fig-adversarial-attack-noise-example" class="quarto-xref">Figure&nbsp;<span>17.18</span></a>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-parrish2023adversarial" class="csl-entry" role="listitem">
Parrish, Alicia, Hannah Rose Kirk, Jessica Quaye, Charvi Rastogi, Max Bartolo, Oana Inel, Juan Ciro, et al. 2023. <span>“Adversarial Nibbler: <span>A</span> Data-Centric Challenge for Improving the Safety of Text-to-Image Models.”</span> <em>ArXiv Preprint</em> abs/2305.14384. <a href="https://arxiv.org/abs/2305.14384">https://arxiv.org/abs/2305.14384</a>.
</div></div><div id="fig-adversarial-attack-noise-example" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-adversarial-attack-noise-example-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./images/png/adversarial_attack_detection.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-adversarial-attack-noise-example-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;17.18: A small adversarial noise added to the original image can make the neural network classify the image as a Guacamole instead of an Egyptian cat (Credit: <a href="https://www.mdpi.com/2079-9292/10/1/52">Sutanto</a>)
</figcaption>
</figure>
</div>
<p>One can generate prompts that lead to unsafe images in text-to-image models like DALLE <span class="citation" data-cites="ramesh2021zero">(<a href="../../references.html#ref-ramesh2021zero" role="doc-biblioref">Ramesh et al. 2021</a>)</span> or Stable Diffusion <span class="citation" data-cites="rombach2022highresolution">(<a href="../../references.html#ref-rombach2022highresolution" role="doc-biblioref">Rombach et al. 2022</a>)</span>. For example, by altering the pixel values of an image, attackers can deceive a facial recognition system into identifying a face as a different person.</p>
<div class="no-row-height column-margin column-container"><div id="ref-ramesh2021zero" class="csl-entry" role="listitem">
Ramesh, Aditya, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. 2021. <span>“Zero-Shot Text-to-Image Generation.”</span> In <em>Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event</em>, edited by Marina Meila and Tong Zhang, 139:8821–31. Proceedings of Machine Learning Research. PMLR. <a href="http://proceedings.mlr.press/v139/ramesh21a.html">http://proceedings.mlr.press/v139/ramesh21a.html</a>.
</div><div id="ref-rombach2022highresolution" class="csl-entry" role="listitem">
Rombach, Robin, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. 2022. <span>“High-Resolution Image Synthesis with Latent Diffusion Models.”</span> In <em>2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>. IEEE. <a href="https://doi.org/10.1109/cvpr52688.2022.01042">https://doi.org/10.1109/cvpr52688.2022.01042</a>.
</div></div><p>Adversarial attacks exploit the way ML models learn and make decisions during inference. These models work on the principle of recognizing patterns in data. An adversary crafts special inputs with perturbations to mislead the model’s pattern recognition---essentially ‘hacking’ the model’s perceptions.</p>
<p>Adversarial attacks fall under different scenarios:</p>
<ul>
<li><p><strong>Whitebox Attacks:</strong> The attacker fully knows the target model’s internal workings, including the training data, parameters, and architecture <span class="citation" data-cites="ye2021thundernna">(<a href="../../references.html#ref-ye2021thundernna" role="doc-biblioref">Ye and Hamidi 2021</a>)</span>. This comprehensive access creates favorable conditions for attackers to exploit the model’s vulnerabilities. The attacker can use specific and subtle weaknesses to craft effective adversarial examples.</p></li>
<li><p><strong>Blackbox Attacks:</strong> In contrast to white-box attacks, black-box attacks involve the attacker having little to no knowledge of the target model <span class="citation" data-cites="guo2019simple">(<a href="../../references.html#ref-guo2019simple" role="doc-biblioref">Guo et al. 2019</a>)</span>. To carry out the attack, the adversarial actor must carefully observe the model’s output behavior.</p></li>
<li><p><strong>Greybox Attacks:</strong> These fall between blackbox and whitebox attacks. The attacker has only partial knowledge about the target model’s internal design <span class="citation" data-cites="xu2021grey">(<a href="../../references.html#ref-xu2021grey" role="doc-biblioref">Xu et al. 2021</a>)</span>. For example, the attacker could have knowledge about training data but not the architecture or parameters. In the real world, practical attacks fall under black black-box box grey-boxes.</p></li>
</ul>
<div class="no-row-height column-margin column-container"><div id="ref-ye2021thundernna" class="csl-entry" role="listitem">
Ye, Linfeng, and Shayan Mohajer Hamidi. 2021. <span>“Thundernna: <span>A</span> White Box Adversarial Attack.”</span> <em>arXiv Preprint arXiv:2111.12305</em>.
</div><div id="ref-guo2019simple" class="csl-entry" role="listitem">
Guo, Chuan, Jacob Gardner, Yurong You, Andrew Gordon Wilson, and Kilian Weinberger. 2019. <span>“Simple Black-Box Adversarial Attacks.”</span> In <em>International Conference on Machine Learning</em>, 2484–93. PMLR.
</div><div id="ref-xu2021grey" class="csl-entry" role="listitem">
Xu, Ying, Xu Zhong, Antonio Jimeno Yepes, and Jey Han Lau. 2021. <span>“<span>Grey</span>-Box Adversarial Attack and Defence for Sentiment Classification.”</span> <em>arXiv Preprint arXiv:2103.11576</em>.
</div></div><p>The landscape of machine learning models is complex and broad, especially given their relatively recent integration into commercial applications. This rapid adoption, while transformative, has brought to light numerous vulnerabilities within these models. Consequently, various adversarial attack methods have emerged, each strategically exploiting different aspects of different models. Below, we highlight a subset of these methods, showcasing the multifaceted nature of adversarial attacks on machine learning models:</p>
<ul>
<li><p><strong>Generative Adversarial Networks (GANs)</strong> are deep learning models that consist of two networks competing against each other: a generator and a discriminator <span class="citation" data-cites="goodfellow2020generative">(<a href="../../references.html#ref-goodfellow2020generative" role="doc-biblioref">Goodfellow et al. 2020</a>)</span>. The generator tries to synthesize realistic data while the discriminator evaluates whether they are real or fake. GANs can be used to craft adversarial examples. The generator network is trained to produce inputs that the target model misclassifies. These GAN-generated images can then attack a target classifier or detection model. The generator and the target model are engaged in a competitive process, with the generator continually improving its ability to create deceptive examples and the target model enhancing its resistance to such examples. GANs provide a powerful framework for crafting complex and diverse adversarial inputs, illustrating the adaptability of generative models in the adversarial landscape.</p></li>
<li><p><strong>Transfer Learning Adversarial Attacks</strong> exploit the knowledge transferred from a pre-trained model to a target model, creating adversarial examples that can deceive both models. These attacks pose a growing concern, particularly when adversaries have knowledge of the feature extractor but lack access to the classification head (the part or layer responsible for making the final classifications). Referred to as "headless attacks," these transferable adversarial strategies leverage the expressive capabilities of feature extractors to craft perturbations while being oblivious to the label space or training data. The existence of such attacks underscores the importance of developing robust defenses for transfer learning applications, especially since pre-trained models are commonly used <span class="citation" data-cites="ahmed2020headless">(<a href="../../references.html#ref-ahmed2020headless" role="doc-biblioref">Abdelkader et al. 2020</a>)</span>.</p></li>
</ul>
<div class="no-row-height column-margin column-container"><div id="ref-goodfellow2020generative" class="csl-entry" role="listitem">
Goodfellow, Ian, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2020. <span>“Generative Adversarial Networks.”</span> <em>Commun. ACM</em> 63 (11): 139–44. <a href="https://doi.org/10.1145/3422622">https://doi.org/10.1145/3422622</a>.
</div><div id="ref-ahmed2020headless" class="csl-entry" role="listitem">
Abdelkader, Ahmed, Michael J. Curry, Liam Fowl, Tom Goldstein, Avi Schwarzschild, Manli Shu, Christoph Studer, and Chen Zhu. 2020. <span>“Headless Horseman: <span>Adversarial</span> Attacks on Transfer Learning Models.”</span> In <em>ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>, 3087–91. IEEE. <a href="https://doi.org/10.1109/icassp40776.2020.9053181">https://doi.org/10.1109/icassp40776.2020.9053181</a>.
</div></div></section>
<section id="mechanisms-of-adversarial-attacks" class="level4">
<h4 class="anchored" data-anchor-id="mechanisms-of-adversarial-attacks">Mechanisms of Adversarial Attacks</h4>
<div id="fig-gradient-attack" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-gradient-attack-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./images/png/gradient_attack.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-gradient-attack-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;17.19: Gradient-Based Attacks (Credit: <a href="https://defence.ai/ai-security/gradient-based-attacks/">Ivezic</a>)
</figcaption>
</figure>
</div>
<p><strong>Gradient-based Attacks</strong></p>
<p>One prominent category of adversarial attacks is gradient-based attacks. These attacks leverage the gradients of the ML model’s loss function to craft adversarial examples. The <a href="https://www.tensorflow.org/tutorials/generative/adversarial_fgsm">Fast Gradient Sign Method</a> (FGSM) is a well-known technique in this category. FGSM perturbs the input data by adding small noise in the gradient direction, aiming to maximize the model’s prediction error. FGSM can quickly generate adversarial examples, as shown in <a href="#fig-gradient-attack" class="quarto-xref">Figure&nbsp;<span>17.19</span></a>, by taking a single step in the gradient direction.</p>
<p>Another variant, the Projected Gradient Descent (PGD) attack, extends FGSM by iteratively applying the gradient update step, allowing for more refined and powerful adversarial examples. The Jacobian-based Saliency Map Attack (JSMA) is another gradient-based approach that identifies the most influential input features and perturbs them to create adversarial examples.</p>
<p><strong>Optimization-based Attacks</strong></p>
<p>These attacks formulate the generation of adversarial examples as an optimization problem. The Carlini and Wagner (C&amp;W) attack is a prominent example in this category. It aims to find the smallest perturbation that can cause misclassification while maintaining the perceptual similarity to the original input. The C&amp;W attack employs an iterative optimization process to minimize the perturbation while maximizing the model’s prediction error.</p>
<p>Another optimization-based approach is the Elastic Net Attack to DNNs (EAD), which incorporates elastic net regularization to generate adversarial examples with sparse perturbations.</p>
<p><strong>Transfer-based Attacks</strong></p>
<p>Transfer-based attacks exploit the transferability property of adversarial examples. Transferability refers to the phenomenon where adversarial examples crafted for one ML model can often fool other models, even if they have different architectures or were trained on different datasets. This enables attackers to generate adversarial examples using a surrogate model and then transfer them to the target model without requiring direct access to its parameters or gradients. Transfer-based attacks highlight the generalization of adversarial vulnerabilities across different models and the potential for black-box attacks.</p>
<p><strong>Physical-world Attacks</strong></p>
<p>Physical-world attacks bring adversarial examples into the realm of real-world scenarios. These attacks involve creating physical objects or manipulations that can deceive ML models when captured by sensors or cameras. Adversarial patches, for example, are small, carefully designed patches that can be placed on objects to fool object detection or classification models. When attached to real-world objects, these patches can cause models to misclassify or fail to detect the objects accurately. Adversarial objects, such as 3D-printed sculptures or modified road signs, can also be crafted to deceive ML systems in physical environments.</p>
<p><strong>Summary</strong></p>
<p><a href="#tbl-attack_types" class="quarto-xref">Table&nbsp;<span>17.2</span></a> a concise overview of the different categories of adversarial attacks, including gradient-based attacks (FGSM, PGD, JSMA), optimization-based attacks (C&amp;W, EAD), transfer-based attacks, and physical-world attacks (adversarial patches and objects). Each attack is briefly described, highlighting its key characteristics and mechanisms.</p>
<div id="tbl-attack_types" class="quarto-float anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-attack_types-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;17.2: Different attack types on ML models.
</figcaption>
<div aria-describedby="tbl-attack_types-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="table">
<colgroup>
<col style="width: 13%">
<col style="width: 21%">
<col style="width: 65%">
</colgroup>
<thead>
<tr class="header">
<th>Attack Category</th>
<th>Attack Name</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Gradient-based</td>
<td>Fast Gradient Sign Method (FGSM)</td>
<td>Perturbs input data by adding small noise in the gradient direction to maximize prediction error.</td>
</tr>
<tr class="even">
<td></td>
<td>Projected Gradient Descent (PGD)</td>
<td>Extends FGSM by iteratively applying the gradient update step for more refined adversarial examples.</td>
</tr>
<tr class="odd">
<td></td>
<td>Jacobian-based Saliency Map Attack (JSMA)</td>
<td>Identifies influential input features and perturbs them to create adversarial examples.</td>
</tr>
<tr class="even">
<td>Optimization-based</td>
<td>Carlini and Wagner (C&amp;W) Attack</td>
<td>Finds the smallest perturbation that causes misclassification while maintaining perceptual similarity.</td>
</tr>
<tr class="odd">
<td></td>
<td>Elastic Net Attack to DNNs (EAD)</td>
<td>Incorporates elastic net regularization to generate adversarial examples with sparse perturbations.</td>
</tr>
<tr class="even">
<td>Transfer-based</td>
<td>Transferability-based Attacks</td>
<td>Exploits the transferability of adversarial examples across different models, enabling black-box attacks.</td>
</tr>
<tr class="odd">
<td>Physical-world</td>
<td>Adversarial Patches</td>
<td>Small, carefully designed patches placed on objects to fool object detection or classification models.</td>
</tr>
<tr class="even">
<td></td>
<td>Adversarial Objects</td>
<td>Physical objects (e.g., 3D-printed sculptures, modified road signs) crafted to deceive ML systems in real-world scenarios.</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p>The mechanisms of adversarial attacks reveal the intricate interplay between the ML model’s decision boundaries, the input data, and the attacker’s objectives. By carefully manipulating the input data, attackers can exploit the model’s sensitivities and blind spots, leading to incorrect predictions. The success of adversarial attacks highlights the need for a deeper understanding of ML models’ robustness and generalization properties.</p>
<p>Defending against adversarial attacks requires a multifaceted approach. Adversarial training is one common defense strategy in which models are trained on adversarial examples to improve robustness. Exposing the model to adversarial examples during training teaches it to classify them correctly and become more resilient to attacks. Defensive distillation, input preprocessing, and ensemble methods are other techniques that can help mitigate the impact of adversarial attacks.</p>
<p>As adversarial machine learning evolves, researchers explore new attack mechanisms and develop more sophisticated defenses. The arms race between attackers and defenders drives the need for constant innovation and vigilance in securing ML systems against adversarial threats. Understanding the mechanisms of adversarial attacks is crucial for developing robust and reliable ML models that can withstand the ever-evolving landscape of adversarial examples.</p>
</section>
<section id="impact-on-ml-systems-3" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="impact-on-ml-systems-3">Impact on ML Systems</h4>
<p>Adversarial attacks on machine learning systems have emerged as a significant concern in recent years, highlighting the potential vulnerabilities and risks associated with the widespread adoption of ML technologies. These attacks involve carefully crafted perturbations to input data that can deceive or mislead ML models, leading to incorrect predictions or misclassifications, as shown in <a href="#fig-adversarial-googlenet" class="quarto-xref">Figure&nbsp;<span>17.20</span></a>. The impact of adversarial attacks on ML systems is far-reaching and can have serious consequences in various domains.</p>
<p>One striking example of the impact of adversarial attacks was demonstrated by researchers in 2017. They experimented with small black and white stickers on stop signs <span class="citation" data-cites="eykholt2018robust">(<a href="../../references.html#ref-eykholt2018robust" role="doc-biblioref">Eykholt et al. 2017</a>)</span>. To the human eye, these stickers did not obscure the sign or prevent its interpretability. However, when images of the sticker-modified stop signs were fed into standard traffic sign classification ML models, a shocking result emerged. The models misclassified the stop signs as speed limit signs over 85% of the time.</p>
<div class="no-row-height column-margin column-container"><div id="ref-eykholt2018robust" class="csl-entry" role="listitem">
Eykholt, Kevin, Ivan Evtimov, Earlence Fernandes, Bo Li, Amir Rahmati, Chaowei Xiao, Atul Prakash, Tadayoshi Kohno, and Dawn Song. 2017. <span>“Robust Physical-World Attacks on Deep Learning Models.”</span> <em>ArXiv Preprint</em> abs/1707.08945. <a href="https://arxiv.org/abs/1707.08945">https://arxiv.org/abs/1707.08945</a>.
</div></div><p>This demonstration shed light on the alarming potential of simple adversarial stickers to trick ML systems into misreading critical road signs. The implications of such attacks in the real world are significant, particularly in the context of autonomous vehicles. If deployed on actual roads, these adversarial stickers could cause self-driving cars to misinterpret stop signs as speed limits, leading to dangerous situations, as shown in <a href="#fig-graffiti" class="quarto-xref">Figure&nbsp;<span>17.21</span></a>. Researchers warned that this could result in rolling stops or unintended acceleration into intersections, endangering public safety.</p>
<div id="fig-adversarial-googlenet" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-adversarial-googlenet-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./images/png/adversarial_googlenet.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-adversarial-googlenet-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;17.20: Adversarial example generation applied to GoogLeNet (Szegedy et al., 2014a) on ImageNet (Credit: <a href="https://arxiv.org/abs/1412.6572">Goodfellow</a>)
</figcaption>
</figure>
</div>
<div id="fig-graffiti" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-graffiti-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./images/png/graffiti.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-graffiti-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;17.21: Graffiti on a stop sign tricked a self-driving car into thinking it was a 45 mph speed limit sign (Credit: <a href="https://arxiv.org/abs/1707.08945">Eykholt</a>)
</figcaption>
</figure>
</div>
<p>The case study of the adversarial stickers on stop signs provides a concrete illustration of how adversarial examples exploit how ML models recognize patterns. By subtly manipulating the input data in ways that are invisible to humans, attackers can induce incorrect predictions and create serious risks, especially in safety-critical applications like autonomous vehicles. The attack’s simplicity highlights the vulnerability of ML models to even minor changes in the input, emphasizing the need for robust defenses against such threats.</p>
<p>The impact of adversarial attacks extends beyond the degradation of model performance. These attacks raise significant security and safety concerns, particularly in domains where ML models are relied upon for critical decision-making. In healthcare applications, adversarial attacks on medical imaging models could lead to misdiagnosis or incorrect treatment recommendations, jeopardizing patient well-being <span class="citation" data-cites="tsai2023adversarial">(<a href="../../references.html#ref-tsai2023adversarial" role="doc-biblioref">M.-J. Tsai, Lin, and Lee 2023</a>)</span>. In financial systems, adversarial attacks could enable fraud or manipulation of trading algorithms, resulting in substantial economic losses.</p>
<div class="no-row-height column-margin column-container"><div id="ref-tsai2023adversarial" class="csl-entry" role="listitem">
Tsai, Min-Jen, Ping-Yi Lin, and Ming-En Lee. 2023. <span>“Adversarial Attacks on Medical Image Classification.”</span> <em>Cancers</em> 15 (17): 4228. <a href="https://doi.org/10.3390/cancers15174228">https://doi.org/10.3390/cancers15174228</a>.
</div><div id="ref-fursov2021adversarial" class="csl-entry" role="listitem">
Fursov, Ivan, Matvey Morozov, Nina Kaploukhaya, Elizaveta Kovtun, Rodrigo Rivera-Castro, Gleb Gusev, Dmitry Babaev, Ivan Kireev, Alexey Zaytsev, and Evgeny Burnaev. 2021. <span>“Adversarial Attacks on Deep Models for Financial Transaction Records.”</span> In <em>Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp;Amp; Data Mining</em>, 2868–78. ACM. <a href="https://doi.org/10.1145/3447548.3467145">https://doi.org/10.1145/3447548.3467145</a>.
</div></div><p>Moreover, adversarial vulnerabilities undermine the trustworthiness and interpretability of ML models. If carefully crafted perturbations can easily fool models, confidence in their predictions and decisions erodes. Adversarial examples expose the models’ reliance on superficial patterns and the inability to capture the true underlying concepts, challenging the reliability of ML systems <span class="citation" data-cites="fursov2021adversarial">(<a href="../../references.html#ref-fursov2021adversarial" role="doc-biblioref">Fursov et al. 2021</a>)</span>.</p>
<p>Defending against adversarial attacks often requires additional computational resources and can impact the overall system performance. Techniques like adversarial training, where models are trained on adversarial examples to improve robustness, can significantly increase training time and computational requirements <span class="citation" data-cites="bai2021recent">(<a href="../../references.html#ref-bai2021recent" role="doc-biblioref">Bai et al. 2021</a>)</span>. Runtime detection and mitigation mechanisms, such as input preprocessing <span class="citation" data-cites="addepalli2020towards">(<a href="../../references.html#ref-addepalli2020towards" role="doc-biblioref">Addepalli et al. 2020</a>)</span> or prediction consistency checks, introduce latency and affect the real-time performance of ML systems.</p>
<div class="no-row-height column-margin column-container"><div id="ref-bai2021recent" class="csl-entry" role="listitem">
Bai, Tao, Jinqi Luo, Jun Zhao, Bihan Wen, and Qian Wang. 2021. <span>“Recent Advances in Adversarial Training for Adversarial Robustness.”</span> <em>arXiv Preprint arXiv:2102.01356</em>.
</div><div id="ref-addepalli2020towards" class="csl-entry" role="listitem">
Addepalli, Sravanti, B. S. Vivek, Arya Baburaj, Gaurang Sriramanan, and R. Venkatesh Babu. 2020. <span>“Towards Achieving Adversarial Robustness by Enforcing Feature Consistency Across Bit Planes.”</span> In <em>2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 1020–29. IEEE. <a href="https://doi.org/10.1109/cvpr42600.2020.00110">https://doi.org/10.1109/cvpr42600.2020.00110</a>.
</div></div><p>The presence of adversarial vulnerabilities also complicates the deployment and maintenance of ML systems. System designers and operators must consider the potential for adversarial attacks and incorporate appropriate defenses and monitoring mechanisms. Regular updates and retraining of models become necessary to adapt to new adversarial techniques and maintain system security and performance over time.</p>
<p>The impact of adversarial attacks on ML systems is significant and multifaceted. These attacks expose ML models’ vulnerabilities, from degrading model performance and raising security and safety concerns to challenging model trustworthiness and interpretability. Developers and researchers must prioritize the development of robust defenses and countermeasures to mitigate the risks posed by adversarial attacks. By addressing these challenges, we can build more secure, reliable, and trustworthy ML systems that can withstand the ever-evolving landscape of adversarial threats.</p>
<div id="exr-aa" class="callout callout-style-default callout-caution callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-3-contents" aria-controls="callout-3" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Exercise&nbsp;17.2: Adversarial Attacks
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-3" class="callout-3-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<p>Get ready to become an AI adversary! In this Colab, you’ll become a white-box hacker, learning to craft attacks that deceive image classification models. We’ll focus on the Fast Gradient Sign Method (FGSM), where you’ll weaponize a model’s gradients against it! You’ll deliberately distort images with tiny perturbations, observing how they increasingly fool the AI more intensely. This hands-on exercise highlights the importance of building secure AI – a critical skill as AI integrates into cars and healthcare. The Colab directly ties into the Robust AI chapter of your book, moving adversarial attacks from theory into your own hands-on experience.</p>
<p><a href="https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/generative/adversarial_fgsm.ipynb#scrollTo=W1L3zJP6pPGD"><img src="https://colab.research.google.com/assets/colab-badge.png" class="img-fluid"></a></p>
<p>Think you can outsmart an AI? In this Colab, learn how to trick image classification models with adversarial attacks. We’ll use methods like FGSM to change images and subtly fool the AI. Discover how to design deceptive image patches and witness the surprising vulnerability of these powerful models. This is crucial knowledge for building truly robust AI systems!</p>
<p><a href="https://colab.research.google.com/github/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/tutorial10/Adversarial_Attacks.ipynb#scrollTo=C5HNmh1-Ka9J"><img src="https://colab.research.google.com/assets/colab-badge.png" class="img-fluid"></a></p>
</div>
</div>
</div>
</section>
</section>
<section id="data-poisoning" class="level3 page-columns page-full" data-number="17.4.2">
<h3 data-number="17.4.2" class="anchored" data-anchor-id="data-poisoning"><span class="header-section-number">17.4.2</span> Data Poisoning</h3>
<section id="definition-and-characteristics-4" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="definition-and-characteristics-4">Definition and Characteristics</h4>
<p>Data poisoning is an attack where the training data is tampered with, leading to a compromised model <span class="citation" data-cites="biggio2012poisoning">(<a href="../../references.html#ref-biggio2012poisoning" role="doc-biblioref">Biggio, Nelson, and Laskov 2012</a>)</span>, as shown in <a href="#fig-poisoning-example" class="quarto-xref">Figure&nbsp;<span>17.22</span></a>. Attackers can modify existing training examples, insert new malicious data points, or influence the data collection process. The poisoned data is labeled in such a way as to skew the model’s learned behavior. This can be particularly damaging in applications where ML models make automated decisions based on learned patterns. Beyond training sets, poisoning tests, and validation data can allow adversaries to boost reported model performance artificially.</p>
<div class="no-row-height column-margin column-container"><div id="ref-biggio2012poisoning" class="csl-entry" role="listitem">
Biggio, Battista, Blaine Nelson, and Pavel Laskov. 2012. <span>“Poisoning Attacks Against Support Vector Machines.”</span> In <em>Proceedings of the 29th International Conference on Machine Learning, ICML 2012, Edinburgh, Scotland, UK, June 26 - July 1, 2012</em>. icml.cc / Omnipress. <a href="http://icml.cc/2012/papers/880.pdf">http://icml.cc/2012/papers/880.pdf</a>.
</div></div><div id="fig-poisoning-example" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-poisoning-example-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./images/png/poisoning_example.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-poisoning-example-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;17.22: NightShade’s poisoning effects on Stable Diffusion (Credit: <a href="https://telefonicatech.com/en/blog/attacks-on-artificial-intelligence-iii-data-poisoning">TOMÉ</a>)
</figcaption>
</figure>
</div>
<p>The process usually involves the following steps:</p>
<ul>
<li><p><strong>Injection:</strong> The attacker adds incorrect or misleading examples into the training set. These examples are often designed to look normal to cursory inspection but have been carefully crafted to disrupt the learning process.</p></li>
<li><p><strong>Training:</strong> The ML model trains on this manipulated dataset and develops skewed understandings of the data patterns.</p></li>
<li><p><strong>Deployment:</strong> Once the model is deployed, the corrupted training leads to flawed decision-making or predictable vulnerabilities the attacker can exploit.</p></li>
</ul>
<p>The impact of data poisoning extends beyond classification errors or accuracy drops. In critical applications like healthcare, such alterations can lead to significant trust and safety issues <span class="citation" data-cites="marulli2022sensitivity">(<a href="../../references.html#ref-marulli2022sensitivity" role="doc-biblioref">Marulli, Marrone, and Verde 2022</a>)</span>. Later, we will discuss a few case studies of these issues.</p>
<div class="no-row-height column-margin column-container"><div id="ref-marulli2022sensitivity" class="csl-entry" role="listitem">
Marulli, Fiammetta, Stefano Marrone, and Laura Verde. 2022. <span>“Sensitivity of Machine Learning Approaches to Fake and Untrusted Data in Healthcare Domain.”</span> <em>Journal of Sensor and Actuator Networks</em> 11 (2): 21. <a href="https://doi.org/10.3390/jsan11020021">https://doi.org/10.3390/jsan11020021</a>.
</div><div id="ref-oprea2022poisoning" class="csl-entry" role="listitem">
Oprea, Alina, Anoop Singhal, and Apostol Vassilev. 2022. <span>“Poisoning Attacks Against Machine Learning: <span>Can</span> Machine Learning Be Trustworthy?”</span> <em>Computer</em> 55 (11): 94–99. <a href="https://doi.org/10.1109/mc.2022.3190787">https://doi.org/10.1109/mc.2022.3190787</a>.
</div></div><p>There are six main categories of data poisoning <span class="citation" data-cites="oprea2022poisoning">(<a href="../../references.html#ref-oprea2022poisoning" role="doc-biblioref">Oprea, Singhal, and Vassilev 2022</a>)</span>:</p>
<ul>
<li><p><strong>Availability Attacks:</strong> These attacks aim to compromise the overall functionality of a model. They cause it to misclassify most testing samples, rendering the model unusable for practical applications. An example is label flipping, where labels of a specific, targeted class are replaced with labels from a different one.</p></li>
<li><p><strong>Targeted Attacks:</strong> In contrast to availability attacks, targeted attacks aim to compromise a small number of the testing samples. So, the effect is localized to a limited number of classes, while the model maintains the same original level of accuracy for the majority of the classes. The targeted nature of the attack requires the attacker to possess knowledge of the model’s classes, making detecting these attacks more challenging.</p></li>
<li><p><strong>Backdoor Attacks:</strong> In these attacks, an adversary targets specific patterns in the data. The attacker introduces a backdoor (a malicious, hidden trigger or pattern) into the training data, such as manipulating certain features in structured data or manipulating a pattern of pixels at a fixed position. This causes the model to associate the malicious pattern with specific labels. As a result, when the model encounters test samples that contain a malicious pattern, it makes false predictions.</p></li>
<li><p><strong>Subpopulation Attacks:</strong> Attackers selectively choose to compromise a subset of the testing samples while maintaining accuracy on the rest of the samples. You can think of these attacks as a combination of availability and targeted attacks: performing availability attacks (performance degradation) within the scope of a targeted subset. Although subpopulation attacks may seem very similar to targeted attacks, the two have clear differences:</p></li>
<li><p><strong>Scope:</strong> While targeted attacks target a selected set of samples, subpopulation attacks target a general subpopulation with similar feature representations. For example, in a targeted attack, an actor inserts manipulated images of a ‘speed bump’ warning sign (with carefully crafted perturbations or patterns), which causes an autonomous car to fail to recognize such a sign and slow down. On the other hand, manipulating all samples of people with a British accent so that a speech recognition model would misclassify a British person’s speech is an example of a subpopulation attack.</p></li>
<li><p><strong>Knowledge:</strong> While targeted attacks require a high degree of familiarity with the data, subpopulation attacks require less intimate knowledge to be effective.</p></li>
</ul>
<p>The characteristics of data poisoning include:</p>
<p><strong>Subtle and hard-to-detect manipulations of training data:</strong> Data poisoning often involves subtle manipulations of the training data that are carefully crafted to be difficult to detect through casual inspection. Attackers employ sophisticated techniques to ensure that the poisoned samples blend seamlessly with the legitimate data, making them easier to identify with thorough analysis. These manipulations can target specific features or attributes of the data, such as altering numerical values, modifying categorical labels, or introducing carefully designed patterns. The goal is to influence the model’s learning process while evading detection, allowing the poisoned data to subtly corrupt the model’s behavior.</p>
<p><strong>Can be performed by insiders or external attackers:</strong> Data poisoning attacks can be carried out by various actors, including malicious insiders with access to the training data and external attackers who find ways to influence the data collection or preprocessing pipeline. Insiders pose a significant threat because they often have privileged access and knowledge of the system, enabling them to introduce poisoned data without raising suspicions. On the other hand, external attackers may exploit vulnerabilities in data sourcing, crowdsourcing platforms, or data aggregation processes to inject poisoned samples into the training dataset. This highlights the importance of implementing strong access controls, data governance policies, and monitoring mechanisms to mitigate the risk of insider threats and external attacks.</p>
<p><strong>Exploits vulnerabilities in data collection and preprocessing:</strong> Data poisoning attacks often exploit vulnerabilities in the machine learning pipeline’s data collection and preprocessing stages. Attackers carefully design poisoned samples to evade common data validation techniques, ensuring that the manipulated data still falls within acceptable ranges, follows expected distributions, or maintains consistency with other features. This allows the poisoned data to pass through data preprocessing steps without detection. Furthermore, poisoning attacks can take advantage of weaknesses in data preprocessing, such as inadequate data cleaning, insufficient outlier detection, or lack of integrity checks. Attackers may also exploit the lack of robust data provenance and lineage tracking mechanisms to introduce poisoned data without leaving a traceable trail. Addressing these vulnerabilities requires rigorous data validation, anomaly detection, and data provenance tracking techniques to ensure the integrity and trustworthiness of the training data.</p>
<p><strong>Disrupts the learning process and skews model behavior:</strong> Data poisoning attacks are designed to disrupt the learning process of machine learning models and skew their behavior towards the attacker’s objectives. The poisoned data is typically manipulated with specific goals, such as skewing the model’s behavior towards certain classes, introducing backdoors, or degrading overall performance. These manipulations are not random but targeted to achieve the attacker’s desired outcomes. By introducing label inconsistencies, where the manipulated samples have labels that do not align with their true nature, poisoning attacks can confuse the model during training and lead to biased or incorrect predictions. The disruption caused by poisoned data can have far-reaching consequences, as the compromised model may make flawed decisions or exhibit unintended behavior when deployed in real-world applications.</p>
<p><strong>Impacts model performance, fairness, and trustworthiness:</strong> Poisoned data in the training dataset can have severe implications for machine learning models’ performance, fairness, and trustworthiness. Poisoned data can degrade the accuracy and performance of the trained model, leading to increased misclassifications or errors in predictions. This can have significant consequences, especially in critical applications where the model’s outputs inform important decisions. Moreover, poisoning attacks can introduce biases and fairness issues, causing the model to make discriminatory or unfair decisions for certain subgroups or classes. This undermines machine learning systems’ ethical and social responsibilities and can perpetuate or amplify existing biases. Furthermore, poisoned data erodes the trustworthiness and reliability of the entire ML system. The model’s outputs become questionable and potentially harmful, leading to a loss of confidence in the system’s integrity. The impact of poisoned data can propagate throughout the entire ML pipeline, affecting downstream components and decisions that rely on the compromised model. Addressing these concerns requires robust data governance, regular model auditing, and ongoing monitoring to detect and mitigate the effects of data poisoning attacks.</p>
</section>
<section id="mechanisms-of-data-poisoning" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="mechanisms-of-data-poisoning">Mechanisms of Data Poisoning</h4>
<p>Data poisoning attacks can be carried out through various mechanisms, exploiting different ML pipeline vulnerabilities. These mechanisms allow attackers to manipulate the training data and introduce malicious samples that can compromise the model’s performance, fairness, or integrity. Understanding these mechanisms is crucial for developing effective defenses against data poisoning and ensuring the robustness of ML systems. Data poisoning mechanisms can be broadly categorized based on the attacker’s approach and the stage of the ML pipeline they target. Some common mechanisms include modifying training data labels, altering feature values, injecting carefully crafted malicious samples, exploiting data collection and preprocessing vulnerabilities, manipulating data at the source, poisoning data in online learning scenarios, and collaborating with insiders to manipulate data.</p>
<p>Each of these mechanisms presents unique challenges and requires different mitigation strategies. For example, detecting label manipulation may involve analyzing the distribution of labels and identifying anomalies <span class="citation" data-cites="zhou2018learning">(<a href="../../references.html#ref-zhou2018learning" role="doc-biblioref">Zhou et al. 2018</a>)</span>, while preventing feature manipulation may require secure data preprocessing and anomaly detection techniques <span class="citation" data-cites="carta2020local">(<a href="../../references.html#ref-carta2020local" role="doc-biblioref">Carta et al. 2020</a>)</span>. Defending against insider threats may involve strict access control policies and monitoring of data access patterns. Moreover, the effectiveness of data poisoning attacks often depends on the attacker’s knowledge of the ML system, including the model architecture, training algorithms, and data distribution. Attackers may use adversarial machine learning or data synthesis techniques to craft samples that are more likely to bypass detection and achieve their malicious objectives.</p>
<div class="no-row-height column-margin column-container"><div id="ref-zhou2018learning" class="csl-entry" role="listitem">
Zhou, Peng, Xintong Han, Vlad I. Morariu, and Larry S. Davis. 2018. <span>“Learning Rich Features for Image Manipulation Detection.”</span> In <em>2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, 1053–61. IEEE. <a href="https://doi.org/10.1109/cvpr.2018.00116">https://doi.org/10.1109/cvpr.2018.00116</a>.
</div><div id="ref-carta2020local" class="csl-entry" role="listitem">
Carta, Salvatore, Alessandro Sebastian Podda, Diego Reforgiato Recupero, and Roberto Saia. 2020. <span>“A Local Feature Engineering Strategy to Improve Network Anomaly Detection.”</span> <em>Future Internet</em> 12 (10): 177. <a href="https://doi.org/10.3390/fi12100177">https://doi.org/10.3390/fi12100177</a>.
</div></div><div id="fig-distribution-shift-example" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-distribution-shift-example-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./images/png/distribution_shift_example.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-distribution-shift-example-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;17.23: Garbage In – Garbage Out (Credit: <a href="https://informationmatters.net/data-poisoning-ai/">Information Matters</a>)
</figcaption>
</figure>
</div>
<p><strong>Modifying training data labels:</strong> One of the most straightforward mechanisms of data poisoning is modifying the training data labels. In this approach, the attacker selectively changes the labels of a subset of the training samples to mislead the model’s learning process as shown in <a href="#fig-distribution-shift-example" class="quarto-xref">Figure&nbsp;<span>17.23</span></a>. For example, in a binary classification task, the attacker might flip the labels of some positive samples to negative, or vice versa. By introducing such label noise, the attacker aims to degrade the model’s performance or cause it to make incorrect predictions for specific target instances.</p>
<p><strong>Altering feature values in training data:</strong> Another mechanism of data poisoning involves altering the feature values of the training samples without modifying the labels. The attacker carefully crafts the feature values to introduce specific biases or vulnerabilities into the model. For instance, in an image classification task, the attacker might add imperceptible perturbations to a subset of images, causing the model to learn a particular pattern or association. This type of poisoning can create backdoors or trojans in the trained model, which specific input patterns can trigger.</p>
<p><strong>Injecting carefully crafted malicious samples:</strong> In this mechanism, the attacker creates malicious samples designed to poison the model. These samples are crafted to have a specific impact on the model’s behavior while blending in with the legitimate training data. The attacker might use techniques such as adversarial perturbations or data synthesis to generate poisoned samples that are difficult to detect. The attacker aims to manipulate the model’s decision boundaries by injecting these malicious samples into the training data or introducing targeted misclassifications.</p>
<p><strong>Exploiting data collection and preprocessing vulnerabilities:</strong> Data poisoning attacks can also exploit the data collection and preprocessing pipeline vulnerabilities. If the data collection process is not secure or there are weaknesses in the data preprocessing steps, an attacker can manipulate the data before it reaches the training phase. For example, if data is collected from untrusted sources or issues in data cleaning or aggregation, an attacker can introduce poisoned samples or manipulate the data to their advantage.</p>
<p><strong>Manipulating data at the source (e.g., sensor data):</strong> In some cases, attackers can manipulate the data at its source, such as sensor data or input devices. By tampering with the sensors or manipulating the environment in which data is collected, attackers can introduce poisoned samples or bias the data distribution. For instance, in a self-driving car scenario, an attacker might manipulate the sensors or the environment to feed misleading information into the training data, compromising the model’s ability to make safe and reliable decisions.</p>
<div id="fig-poisoning-attack-example" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-poisoning-attack-example-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./images/png/poisoning_attack_example.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-poisoning-attack-example-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;17.24: Data Poisoning Attack (Credit: <a href="https://www.researchgate.net/publication/366883200_A_Detailed_Survey_on_Federated_Learning_Attacks_and_Defenses">Sikandar</a>)
</figcaption>
</figure>
</div>
<p><strong>Poisoning data in online learning scenarios:</strong> Data poisoning attacks can also target ML systems that employ online learning, where the model is continuously updated with new data in real time. In such scenarios, an attacker can gradually inject poisoned samples over time, slowly manipulating the model’s behavior. Online learning systems are particularly vulnerable to data poisoning because they adapt to new data without extensive validation, making it easier for attackers to introduce malicious samples, as shown in <a href="#fig-poisoning-attack-example" class="quarto-xref">Figure&nbsp;<span>17.24</span></a>.</p>
<p><strong>Collaborating with insiders to manipulate data:</strong> Sometimes, data poisoning attacks can involve collaboration with insiders with access to the training data. Malicious insiders, such as employees or data providers, can manipulate the data before it is used to train the model. Insider threats are particularly challenging to detect and prevent, as the attackers have legitimate access to the data and can carefully craft the poisoning strategy to evade detection.</p>
<p>These are the key mechanisms of data poisoning in ML systems. Attackers often employ these mechanisms to make their attacks more effective and harder to detect. The risk of data poisoning attacks grows as ML systems become increasingly complex and rely on larger datasets from diverse sources. Defending against data poisoning requires a multifaceted approach. ML practitioners and system designers must be aware of the various mechanisms of data poisoning and adopt a comprehensive approach to data security and model resilience. This includes secure data collection, robust data validation, and continuous model performance monitoring. Implementing secure data collection and preprocessing practices is crucial to prevent data poisoning at the source. Data validation and anomaly detection techniques can also help identify and mitigate potential poisoning attempts. Monitoring model performance for signs of data poisoning is also essential to detect and respond to attacks promptly.</p>
</section>
<section id="impact-on-ml-systems-4" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="impact-on-ml-systems-4">Impact on ML Systems</h4>
<p>Data poisoning attacks can severely affect ML systems, compromising their performance, reliability, and trustworthiness. The impact of data poisoning can manifest in various ways, depending on the attacker’s objectives and the specific mechanism used. Let’s explore each of the potential impacts in detail.</p>
<p><strong>Degradation of model performance:</strong> One of the primary impacts of data poisoning is the degradation of the model’s overall performance. By manipulating the training data, attackers can introduce noise, biases, or inconsistencies that hinder the model’s ability to learn accurate patterns and make reliable predictions. This can reduce accuracy, precision, recall, or other performance metrics. The degradation of model performance can have significant consequences, especially in critical applications such as healthcare, finance, or security, where the reliability of predictions is crucial.</p>
<p><strong>Misclassification of specific targets:</strong> Data poisoning attacks can also be designed to cause the model to misclassify specific target instances. Attackers may introduce carefully crafted poisoned samples similar to the target instances, leading the model to learn incorrect associations. This can result in the model consistently misclassifying the targeted instances, even if it performs well on other inputs. Such targeted misclassification can have severe consequences, such as causing a malware detection system to overlook specific malicious files or leading to the wrong diagnosis in a medical imaging application.</p>
<p><strong>Backdoors and trojans in trained models:</strong> Data poisoning can introduce backdoors or trojans into the trained model. Backdoors are hidden functionalities that allow attackers to trigger specific behaviors or bypass normal authentication mechanisms. On the other hand, Trojans are malicious components embedded within the model that can activate specific input patterns. By poisoning the training data, attackers can create models that appear to perform normally but contain hidden vulnerabilities that can be exploited later. Backdoors and trojans can compromise the integrity and security of the ML system, allowing attackers to gain unauthorized access, manipulate predictions, or exfiltrate sensitive information.</p>
<p><strong>Biased or unfair model outcomes:</strong> Data poisoning attacks can introduce biases or unfairness into the model’s predictions. By manipulating the training data distribution or injecting samples with specific biases, attackers can cause the model to learn and perpetuate discriminatory patterns. This can lead to unfair treatment of certain groups or individuals based on sensitive attributes such as race, gender, or age. Biased models can have severe societal implications, reinforcing existing inequalities and discriminatory practices. Ensuring fairness and mitigating biases is crucial for building trustworthy and ethical ML systems.</p>
<p><strong>Increased false positives or false negatives:</strong> Data poisoning can also impact the model’s ability to correctly identify positive or negative instances, leading to increased false positives or false negatives. False positives occur when the model incorrectly identifies a negative instance as positive, while false negatives happen when a positive instance is misclassified as negative. The consequences of increased false positives or false negatives can be significant depending on the application. For example, in a fraud detection system, high false positives can lead to unnecessary investigations and customer frustration, while high false negatives can allow fraudulent activities to go undetected.</p>
<p><strong>Compromised system reliability and trustworthiness:</strong> Data poisoning attacks can undermine ML systems’ overall reliability and trustworthiness. When models are trained on poisoned data, their predictions become reliable and trustworthy. This can erode user confidence in the system and lead to a loss of trust in the decisions made by the model. In critical applications where ML systems are relied upon for decision-making, such as autonomous vehicles or medical diagnosis, compromised reliability can have severe consequences, putting lives and property at risk.</p>
<p>Addressing the impact of data poisoning requires a proactive approach to data security, model testing, and monitoring. Organizations must implement robust measures to ensure the integrity and quality of training data, employ techniques to detect and mitigate poisoning attempts, and continuously monitor the performance and behavior of deployed models. Collaboration between ML practitioners, security experts, and domain specialists is essential to develop comprehensive strategies for preventing and responding to data poisoning attacks.</p>
<section id="case-study-1" class="level5 page-columns page-full">
<h5 class="anchored" data-anchor-id="case-study-1">Case Study 1</h5>
<p>In 2017, researchers demonstrated a data poisoning attack against a popular toxicity classification model called Perspective <span class="citation" data-cites="hosseini2017deceiving">(<a href="../../references.html#ref-hosseini2017deceiving" role="doc-biblioref">Hosseini et al. 2017</a>)</span>. This ML model detects toxic comments online.</p>
<div class="no-row-height column-margin column-container"><div id="ref-hosseini2017deceiving" class="csl-entry" role="listitem">
Hosseini, Hossein, Sreeram Kannan, Baosen Zhang, and Radha Poovendran. 2017. <span>“Deceiving Google’s Perspective Api Built for Detecting Toxic Comments.”</span> <em>ArXiv Preprint</em> abs/1702.08138. <a href="https://arxiv.org/abs/1702.08138">https://arxiv.org/abs/1702.08138</a>.
</div></div><p>The researchers added synthetically generated toxic comments with slight misspellings and grammatical errors to the model’s training data. This slowly corrupted the model, causing it to misclassify increasing numbers of severely toxic inputs as non-toxic over time.</p>
<p>After retraining on the poisoned data, the model’s false negative rate increased from 1.4% to 27% - allowing extremely toxic comments to bypass detection. The researchers warned this stealthy data poisoning could enable the spread of hate speech, harassment, and abuse if deployed against real moderation systems.</p>
<p>This case highlights how data poisoning can degrade model accuracy and reliability. For social media platforms, a poisoning attack that impairs toxicity detection could lead to the proliferation of harmful content and distrust of ML moderation systems. The example demonstrates why securing training data integrity and monitoring for poisoning is critical across application domains.</p>
</section>
<section id="case-study-2" class="level5 page-columns page-full">
<h5 class="anchored" data-anchor-id="case-study-2">Case Study 2</h5>
<div id="fig-dirty-label-example" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-dirty-label-example-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./images/png/dirty_label_example.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-dirty-label-example-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;17.25: Samples of dirty-label poison data regarding mismatched text/image pairs (Credit: <a href="https://arxiv.org/pdf/2310.13828">Shan</a>)
</figcaption>
</figure>
</div>
<p>Interestingly enough, data poisoning attacks are not always malicious <span class="citation" data-cites="shan2023prompt">(<a href="../../references.html#ref-shan2023prompt" role="doc-biblioref">Shan et al. 2023</a>)</span>. Nightshade, a tool developed by a team led by Professor Ben Zhao at the University of Chicago, utilizes data poisoning to help artists protect their art against scraping and copyright violations by generative AI models. Artists can use the tool to make subtle modifications to their images before uploading them online, as shown in <a href="#fig-dirty-label-example" class="quarto-xref">Figure&nbsp;<span>17.25</span></a>.</p>
<div class="no-row-height column-margin column-container"></div><p>While these changes are indiscernible to the human eye, they can significantly disrupt the performance of generative AI models when incorporated into the training data. Generative models can be manipulated to generate hallucinations and weird images. For example, with only 300 poisoned images, the University of Chicago researchers could trick the latest Stable Diffusion model into generating images of dogs that look like cats or images of cows when prompted for cars.</p>
<p>As the number of poisoned images on the internet increases, the performance of the models that use scraped data will deteriorate exponentially. First, the poisoned data is hard to detect and requires manual elimination. Second, the "poison" spreads quickly to other labels because generative models rely on connections between words and concepts as they generate images. So a poisoned image of a "car" could spread into generated images associated with words like "truck,” "train,” " bus,” etc.</p>
<p>On the other hand, this tool can be used maliciously and can affect legitimate applications of the generative models. This shows the very challenging and novel nature of machine learning attacks.</p>
<p><a href="#fig-poisoning" class="quarto-xref">Figure&nbsp;<span>17.26</span></a> demonstrates the effects of different levels of data poisoning (50 samples, 100 samples, and 300 samples of poisoned images) on generating images in different categories. Notice how the images start deforming and deviating from the desired category. For example, after 300 poison samples, a car prompt generates a cow.</p>
<div id="fig-poisoning" class="quarto-figure quarto-figure-center quarto-float anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-poisoning-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/image14.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-poisoning-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;17.26: Data poisoning (Credit: <span class="citation" data-cites="shan2023prompt">Shan et al. (<a href="../../references.html#ref-shan2023prompt" role="doc-biblioref">2023</a>)</span>)
</figcaption>
<div class="no-row-height column-margin column-container"><div id="ref-shan2023prompt" class="csl-entry" role="listitem">
Shan, Shawn, Wenxin Ding, Josephine Passananti, Haitao Zheng, and Ben Y Zhao. 2023. <span>“Prompt-Specific Poisoning Attacks on Text-to-Image Generative Models.”</span> <em>ArXiv Preprint</em> abs/2310.13828. <a href="https://arxiv.org/abs/2310.13828">https://arxiv.org/abs/2310.13828</a>.
</div></div></figure>
</div>
<div id="exr-pa" class="callout callout-style-default callout-caution callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-4-contents" aria-controls="callout-4" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Exercise&nbsp;17.3: Poisoning Attacks
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-4" class="callout-4-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<p>Get ready to explore the dark side of AI security! In this Colab, you’ll learn about data poisoning – how bad data can trick AI models into making wrong decisions. We’ll focus on a real-world attack against a Support Vector Machine (SVM), observing how the AI’s behavior changes under attack. This hands-on exercise will highlight why protecting AI systems is crucial, especially as they become more integrated into our lives. Think like a hacker, understand the vulnerability, and brainstorm how to defend our AI systems!</p>
<p><a href="https://colab.research.google.com/github/pralab/secml/blob/HEAD/tutorials/05-Poisoning.ipynb#scrollTo=-8onNPNTOLk2"><img src="https://colab.research.google.com/assets/colab-badge.png" class="img-fluid"></a></p>
</div>
</div>
</div>
</section>
</section>
</section>
<section id="distribution-shifts" class="level3" data-number="17.4.3">
<h3 data-number="17.4.3" class="anchored" data-anchor-id="distribution-shifts"><span class="header-section-number">17.4.3</span> Distribution Shifts</h3>
<section id="definition-and-characteristics-5" class="level4">
<h4 class="anchored" data-anchor-id="definition-and-characteristics-5">Definition and Characteristics</h4>
<p>Distribution shift refers to the phenomenon where the data distribution encountered by an ML model during deployment (inference) differs from the distribution it was trained on, as shown in <a href="#fig-distribution-shift" class="quarto-xref">Figure&nbsp;<span>17.27</span></a>. This is not so much an attack as it is that the model’s robustness will vary over time. In other words, the data’s statistical properties, patterns, or underlying assumptions can change between the training and test phases.</p>
<div id="fig-distribution-shift" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-distribution-shift-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./images/png/distribution_shift.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-distribution-shift-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;17.27: The curly brackets enclose the distribution shift between the environments. Here, z stands for the spurious feature, and y stands for label class (Credit: <a href="https://www.researchgate.net/publication/366423741_On_the_Connection_between_Invariant_Learning_and_Adversarial_Training_for_Out-of-Distribution_Generalization">Xin</a>)
</figcaption>
</figure>
</div>
<p>The key characteristics of distribution shift include:</p>
<p><strong>Domain mismatch:</strong> The input data during inference comes from a different domain or distribution than the training data. When the input data during inference comes from a domain or distribution different from the training data, it can significantly affect the model’s performance. This is because the model has learned patterns and relationships specific to the training domain, and when applied to a different domain, those learned patterns may not hold. For example, consider a sentiment analysis model trained on movie reviews. Suppose this model is applied to analyze sentiment in tweets. In that case, it may need help to accurately classify the sentiment because the language, grammar, and context of tweets can differ from movie reviews. This domain mismatch can result in poor performance and unreliable predictions, limiting the model’s practical utility.</p>
<p><strong>Temporal drift:</strong> The data distribution evolves, leading to a gradual or sudden shift in the input characteristics. Temporal drift is important because ML models are often deployed in dynamic environments where the data distribution can change over time. If the model is not updated or adapted to these changes, its performance can gradually degrade. For instance, the patterns and behaviors associated with fraudulent activities may evolve in a fraud detection system as fraudsters adapt their techniques. If the model is not retrained or updated to capture these new patterns, it may fail to detect new types of fraud effectively. Temporal drift can lead to a decline in the model’s accuracy and reliability over time, making monitoring and addressing this type of distribution shift crucial.</p>
<p><strong>Contextual changes:</strong> The ML model’s context can vary, resulting in different data distributions based on factors such as location, user behavior, or environmental conditions. Contextual changes matter because ML models are often deployed in various contexts or environments that can have different data distributions. If the model cannot generalize well to these different contexts, its performance may improve. For example, consider a computer vision model trained to recognize objects in a controlled lab environment. When deployed in a real-world setting, factors such as lighting conditions, camera angles, or background clutter can vary significantly, leading to a distribution shift. If the model is robust to these contextual changes, it may be able to accurately recognize objects in the new environment, limiting its practical utility.</p>
<p><strong>Unrepresentative training data:</strong> The training data may only partially capture the variability and diversity of the real-world data encountered during deployment. Unrepresentative training data can lead to biased or skewed models that perform poorly on real-world data. Suppose the training data needs to capture the variability and diversity of the real-world data adequately. In that case, the model may learn patterns specific to the training set but needs to generalize better to new, unseen data. This can result in poor performance, biased predictions, and limited model applicability. For instance, if a facial recognition model is trained primarily on images of individuals from a specific demographic group, it may struggle to accurately recognize faces from other demographic groups when deployed in a real-world setting. Ensuring that the training data is representative and diverse is crucial for building models that can generalize well to real-world scenarios.</p>
<div id="fig-drift-over-time" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-drift-over-time-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./images/png/drift_over_time.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-drift-over-time-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;17.28: Concept drift refers to a change in data patterns and relationships over time (Credit: <a href="https://www.evidentlyai.com/ml-in-production/concept-drift">Evidently AI</a>)
</figcaption>
</figure>
</div>
<p>Distribution shift can manifest in various forms, such as:</p>
<p><strong>Covariate shift:</strong> The distribution of the input features (covariates) changes while the conditional distribution of the target variable given the input remains the same. Covariate shift matters because it can impact the model’s ability to make accurate predictions when the input features (covariates) differ between the training and test data. Even if the relationship between the input features and the target variable remains the same, a change in the distribution of the input features can affect the model’s performance. For example, consider a model trained to predict housing prices based on features like square footage, number of bedrooms, and location. Suppose the distribution of these features in the test data significantly differs from the training data (e.g., the test data contains houses with much larger square footage). In that case, the model’s predictions may become less accurate. Addressing covariate shifts is important to ensure the model’s robustness and reliability when applied to new data.</p>
<p><strong>Concept drift:</strong> The relationship between the input features and the target variable changes over time, altering the underlying concept the model is trying to learn, as shown in <a href="#fig-drift-over-time" class="quarto-xref">Figure&nbsp;<span>17.28</span></a>. Concept drift is important because it indicates changes in the fundamental relationship between the input features and the target variable over time. When the underlying concept that the model is trying to learn shifts, its performance can deteriorate if not adapted to the new concept. For instance, in a customer churn prediction model, the factors influencing customer churn may evolve due to market conditions, competitor offerings, or customer preferences. If the model is not updated to capture these changes, its predictions may become less accurate and irrelevant. Detecting and adapting to concept drift is crucial to maintaining the model’s effectiveness and alignment with evolving real-world concepts.</p>
<p><strong>Domain generalization:</strong> The model must generalize to unseen domains or distributions not present during training. Domain generalization is important because it enables ML models to be applied to new, unseen domains without requiring extensive retraining or adaptation. In real-world scenarios, training data that covers all possible domains or distributions that the model may encounter is often infeasible. Domain generalization techniques aim to learn domain-invariant features or models that can generalize well to new domains. For example, consider a model trained to classify images of animals. If the model can learn features invariant to different backgrounds, lighting conditions, or poses, it can generalize well to classify animals in new, unseen environments. Domain generalization is crucial for building models that can be deployed in diverse and evolving real-world settings.</p>
<p>The presence of a distribution shift can significantly impact the performance and reliability of ML models, as the models may need help generalizing well to the new data distribution. Detecting and adapting to distribution shifts is crucial to ensure ML systems’ robustness and practical utility in real-world scenarios.</p>
</section>
<section id="mechanisms-of-distribution-shifts" class="level4">
<h4 class="anchored" data-anchor-id="mechanisms-of-distribution-shifts">Mechanisms of Distribution Shifts</h4>
<p>The mechanisms of distribution shift, such as changes in data sources, temporal evolution, domain-specific variations, selection bias, feedback loops, and adversarial manipulations, are important to understand because they help identify the underlying causes of distribution shift. By understanding these mechanisms, practitioners can develop targeted strategies to mitigate their impact and improve the model’s robustness. Here are some common mechanisms:</p>
<div id="fig-temporal-evoltion" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-temporal-evoltion-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./images/png/temporal_evoltion.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-temporal-evoltion-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;17.29: Temporal evolution (Credit: <a href="https://www.nannyml.com/blog/types-of-data-shift">Białek</a>)
</figcaption>
</figure>
</div>
<p><strong>Changes in data sources:</strong> Distribution shifts can occur when the data sources used for training and inference differ. For example, if a model is trained on data from one sensor but deployed on data from another sensor with different characteristics, it can lead to a distribution shift.</p>
<p><strong>Temporal evolution:</strong> Over time, the underlying data distribution can evolve due to changes in user behavior, market dynamics, or other temporal factors. For instance, in a recommendation system, user preferences may shift over time, leading to a distribution shift in the input data, as shown in <a href="#fig-temporal-evoltion" class="quarto-xref">Figure&nbsp;<span>17.29</span></a>.</p>
<p><strong>Domain-specific variations:</strong> Different domains or contexts can have distinct data distributions. A model trained on data from one domain may only generalize well to another domain with appropriate adaptation techniques. For example, an image classification model trained on indoor scenes may struggle when applied to outdoor scenes.</p>
<p><strong>Selection bias:</strong> A Distribution shift can arise from selection bias during data collection or sampling. If the training data does not represent the true population or certain subgroups are over- or underrepresented, this can lead to a mismatch between the training and test distributions.</p>
<p><strong>Feedback loops:</strong> In some cases, the predictions or actions taken by an ML model can influence future data distribution. For example, in a dynamic pricing system, the prices set by the model can impact customer behavior, leading to a shift in the data distribution over time.</p>
<p><strong>Adversarial manipulations:</strong> Adversaries can intentionally manipulate the input data to create a distribution shift and deceive the ML model. By introducing carefully crafted perturbations or generating out-of-distribution samples, attackers can exploit the model’s vulnerabilities and cause it to make incorrect predictions.</p>
<p>Understanding the mechanisms of distribution shift is important for developing effective strategies to detect and mitigate its impact on ML systems. By identifying the sources and characteristics of the shift, practitioners can design appropriate techniques, such as domain adaptation, transfer learning, or continual learning, to improve the model’s robustness and performance under distributional changes.</p>
</section>
<section id="impact-on-ml-systems-5" class="level4">
<h4 class="anchored" data-anchor-id="impact-on-ml-systems-5">Impact on ML Systems</h4>
<p>Distribution shifts can significantly negatively impact the performance and reliability of ML systems. Here are some key ways in which distribution shift can affect ML models:</p>
<p><strong>Degraded predictive performance:</strong> When the data distribution encountered during inference differs from the training distribution, the model’s predictive accuracy can deteriorate. The model may need help generalizing the new data well, leading to increased errors and suboptimal performance.</p>
<p><strong>Reduced reliability and trustworthiness:</strong> Distribution shift can undermine the reliability and trustworthiness of ML models. If the model’s predictions become unreliable or inconsistent due to the shift, users may lose confidence in the system’s outputs, leading to potential misuse or disuse of the model.</p>
<p><strong>Biased predictions:</strong> Distribution shift can introduce biases in the model’s predictions. If the training data does not represent the real-world distribution or certain subgroups are underrepresented, the model may make biased predictions that discriminate against certain groups or perpetuate societal biases.</p>
<p><strong>Increased uncertainty and risk:</strong> Distribution shift introduces additional uncertainty and risk into the ML system. The model’s behavior and performance may become less predictable, making it challenging to assess its reliability and suitability for critical applications. This uncertainty can lead to increased operational risks and potential failures.</p>
<p><strong>Adaptability challenges:</strong> ML models trained on a specific data distribution may need help to adapt to changing environments or new domains. The lack of adaptability can limit the model’s usefulness and applicability in dynamic real-world scenarios where the data distribution evolves.</p>
<p><strong>Maintenance and update difficulties:</strong> Distribution shift can complicate the maintenance and updating of ML models. As the data distribution changes, the model may require frequent retraining or fine-tuning to maintain its performance. This can be time-consuming and resource-intensive, especially if the shift occurs rapidly or continuously.</p>
<p><strong>Vulnerability to adversarial attacks:</strong> Distribution shift can make ML models more vulnerable to adversarial attacks. Adversaries can exploit the model’s sensitivity to distributional changes by crafting adversarial examples outside the training distribution, causing the model to make incorrect predictions or behave unexpectedly.</p>
<p>To mitigate the impact of distribution shifts, it is crucial to develop robust ML systems that detect and adapt to distributional changes. Techniques such as domain adaptation, transfer learning, and continual learning can help improve the model’s generalization ability across different distributions. ML model monitoring, testing, and updating are also necessary to ensure their performance and reliability during distribution shifts.</p>
</section>
</section>
<section id="detection-and-mitigation-1" class="level3 page-columns page-full" data-number="17.4.4">
<h3 data-number="17.4.4" class="anchored" data-anchor-id="detection-and-mitigation-1"><span class="header-section-number">17.4.4</span> Detection and Mitigation</h3>
<section id="adversarial-attacks-2" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="adversarial-attacks-2">Adversarial Attacks</h4>
<p>As you may recall from above, adversarial attacks pose a significant threat to the robustness and reliability of ML systems. These attacks involve crafting carefully designed inputs, known as adversarial examples, to deceive ML models and cause them to make incorrect predictions. To safeguard ML systems against adversarial attacks, developing effective techniques for detecting and mitigating these threats is crucial.</p>
<section id="adversarial-example-detection-techniques" class="level5 page-columns page-full">
<h5 class="anchored" data-anchor-id="adversarial-example-detection-techniques">Adversarial Example Detection Techniques</h5>
<p>Detecting adversarial examples is the first line of defense against adversarial attacks. Several techniques have been proposed to identify and flag suspicious inputs that may be adversarial.</p>
<p>Statistical methods aim to detect adversarial examples by analyzing the statistical properties of the input data. These methods often compare the input data distribution to a reference distribution, such as the training data distribution or a known benign distribution. Techniques like the <a href="https://www.itl.nist.gov/div898/handbook/eda/section3/eda35g.htm">Kolmogorov-Smirnov</a> <span class="citation" data-cites="berger2014kolmogorov">(<a href="../../references.html#ref-berger2014kolmogorov" role="doc-biblioref">Berger and Zhou 2014</a>)</span> test or the <a href="https://www.itl.nist.gov/div898/handbook/eda/section3/eda35e.htm">Anderson-Darling</a> test can be used to measure the discrepancy between the distributions and flag inputs that deviate significantly from the expected distribution.</p>
<div class="no-row-height column-margin column-container"><div id="ref-berger2014kolmogorov" class="csl-entry" role="listitem">
Berger, Vance W, and YanYan Zhou. 2014. <span>“Kolmogorov<span></span>smirnov Test: <span>Overview</span>.”</span> <em>Wiley Statsref: Statistics Reference Online</em>.
</div></div><p><a href="https://mathisonian.github.io/kde/">Kernel density estimation (KDE)</a> is a non-parametric technique used to estimate the probability density function of a dataset. In the context of adversarial example detection, KDE can be used to estimate the density of benign examples in the input space. Adversarial examples often lie in low-density regions and can be detected by comparing their estimated density to a threshold. Inputs with an estimated density below the threshold are flagged as potential adversarial examples.</p>
<p>Another technique is feature squeezing <span class="citation" data-cites="panda2019discretization">(<a href="../../references.html#ref-panda2019discretization" role="doc-biblioref">Panda, Chakraborty, and Roy 2019</a>)</span>, which reduces the complexity of the input space by applying dimensionality reduction or discretization. The idea behind feature squeezing is that adversarial examples often rely on small, imperceptible perturbations that can be eliminated or reduced through these transformations. Inconsistencies can be detected by comparing the model’s predictions on the original input and the squeezed input, indicating the presence of adversarial examples.</p>
<div class="no-row-height column-margin column-container"><div id="ref-panda2019discretization" class="csl-entry" role="listitem">
Panda, Priyadarshini, Indranil Chakraborty, and Kaushik Roy. 2019. <span>“Discretization Based Solutions for Secure Machine Learning Against Adversarial Attacks.”</span> <em>#IEEE_O_ACC#</em> 7: 70157–68. <a href="https://doi.org/10.1109/access.2019.2919463">https://doi.org/10.1109/access.2019.2919463</a>.
</div></div><p>Model uncertainty estimation techniques aim to quantify the confidence or uncertainty associated with a model’s predictions. Adversarial examples often exploit regions of high uncertainty in the model’s decision boundary. By estimating the uncertainty using techniques like Bayesian neural networks, dropout-based uncertainty estimation, or ensemble methods, inputs with high uncertainty can be flagged as potential adversarial examples.</p>
</section>
<section id="adversarial-defense-strategies" class="level5 page-columns page-full">
<h5 class="anchored" data-anchor-id="adversarial-defense-strategies">Adversarial Defense Strategies</h5>
<p>Once adversarial examples are detected, various defense strategies can be employed to mitigate their impact and improve the robustness of ML models.</p>
<p>Adversarial training is a technique that involves augmenting the training data with adversarial examples and retraining the model on this augmented dataset. Exposing the model to adversarial examples during training teaches it to classify them correctly and becomes more robust to adversarial attacks. Adversarial training can be performed using various attack methods, such as the <a href="https://www.tensorflow.org/tutorials/generative/adversarial_fgsm">Fast Gradient Sign Method (FGSM)</a> or Projected Gradient Descent (PGD) <span class="citation" data-cites="madry2017towards">(<a href="../../references.html#ref-madry2017towards" role="doc-biblioref">Madry et al. 2017</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-madry2017towards" class="csl-entry" role="listitem">
Madry, Aleksander, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. 2017. <span>“Towards Deep Learning Models Resistant to Adversarial Attacks.”</span> <em>arXiv Preprint arXiv:1706.06083</em>.
</div><div id="ref-papernot2016distillation" class="csl-entry" role="listitem">
Papernot, Nicolas, Patrick McDaniel, Xi Wu, Somesh Jha, and Ananthram Swami. 2016. <span>“Distillation as a Defense to Adversarial Perturbations Against Deep Neural Networks.”</span> In <em>2016 IEEE Symposium on Security and Privacy (SP)</em>, 582–97. IEEE; IEEE. <a href="https://doi.org/10.1109/sp.2016.41">https://doi.org/10.1109/sp.2016.41</a>.
</div></div><p>Defensive distillation <span class="citation" data-cites="papernot2016distillation">(<a href="../../references.html#ref-papernot2016distillation" role="doc-biblioref">Papernot et al. 2016</a>)</span> is a technique that trains a second model (the student model) to mimic the behavior of the original model (the teacher model). The student model is trained on the soft labels produced by the teacher model, which are less sensitive to small perturbations. Using the student model for inference can reduce the impact of adversarial perturbations, as the student model learns to generalize better and is less sensitive to adversarial noise.</p>
<p>Input preprocessing and transformation techniques aim to remove or mitigate the effect of adversarial perturbations before feeding the input to the ML model. These techniques include image denoising, JPEG compression, random resizing, padding, or applying random transformations to the input data. By reducing the impact of adversarial perturbations, these preprocessing steps can help improve the model’s robustness to adversarial attacks.</p>
<p>Ensemble methods combine multiple models to make more robust predictions. The ensemble can reduce the impact of adversarial attacks by using a diverse set of models with different architectures, training data, or hyperparameters. Adversarial examples that fool one model may not fool others in the ensemble, leading to more reliable and robust predictions. Model diversification techniques, such as using different preprocessing techniques or feature representations for each model in the ensemble, can further enhance the robustness.</p>
</section>
<section id="robustness-evaluation-and-testing" class="level5 page-columns page-full">
<h5 class="anchored" data-anchor-id="robustness-evaluation-and-testing">Robustness Evaluation and Testing</h5>
<p>Conduct thorough evaluation and testing to assess the effectiveness of adversarial defense techniques and measure the robustness of ML models.</p>
<p>Adversarial robustness metrics quantify the model’s resilience to adversarial attacks. These metrics can include the model’s accuracy on adversarial examples, the average distortion required to fool the model, or the model’s performance under different attack strengths. By comparing these metrics across different models or defense techniques, practitioners can assess and compare their robustness levels.</p>
<p>Standardized adversarial attack benchmarks and datasets provide a common ground for evaluating and comparing the robustness of ML models. These benchmarks include datasets with pre-generated adversarial examples and tools and frameworks for generating adversarial attacks. Examples of popular adversarial attack benchmarks include the <a href="https://github.com/google-research/mnist-c">MNIST-C</a>, <a href="https://paperswithcode.com/dataset/cifar-10c">CIFAR-10-C</a>, and ImageNet-C <span class="citation" data-cites="hendrycks2019benchmarking">(<a href="../../references.html#ref-hendrycks2019benchmarking" role="doc-biblioref">Hendrycks and Dietterich 2019</a>)</span> datasets, which contain corrupted or perturbed versions of the original datasets.</p>
<div class="no-row-height column-margin column-container"><div id="ref-hendrycks2019benchmarking" class="csl-entry" role="listitem">
Hendrycks, Dan, and Thomas Dietterich. 2019. <span>“Benchmarking Neural Network Robustness to Common Corruptions and Perturbations.”</span> <em>arXiv Preprint arXiv:1903.12261</em>.
</div></div><p>Practitioners can develop more robust and resilient ML systems by leveraging these adversarial example detection techniques, defense strategies, and robustness evaluation methods. However, it is important to note that adversarial robustness is an ongoing research area, and no single technique provides complete protection against all types of adversarial attacks. A comprehensive approach that combines multiple defense mechanisms and regular testing is essential to maintain the security and reliability of ML systems in the face of evolving adversarial threats.</p>
</section>
</section>
<section id="data-poisoning-1" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="data-poisoning-1">Data Poisoning</h4>
<p>Recall that data poisoning is an attack that targets the integrity of the training data used to build ML models. By manipulating or corrupting the training data, attackers can influence the model’s behavior and cause it to make incorrect predictions or perform unintended actions. Detecting and mitigating data poisoning attacks is crucial to ensure the trustworthiness and reliability of ML systems, as shown in <a href="#fig-adversarial-attack-injection" class="quarto-xref">Figure&nbsp;<span>17.30</span></a>.</p>
<section id="anomaly-detection-techniques-for-identifying-poisoned-data" class="level5">
<h5 class="anchored" data-anchor-id="anomaly-detection-techniques-for-identifying-poisoned-data">Anomaly Detection Techniques for Identifying Poisoned Data</h5>
<div id="fig-adversarial-attack-injection" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-adversarial-attack-injection-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./images/png/adversarial_attack_injection.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-adversarial-attack-injection-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;17.30: Malicious data injection (Credit: <a href="https://www.mdpi.com/2227-7390/12/2/247">Li</a>)
</figcaption>
</figure>
</div>
<p>Statistical outlier detection methods identify data points that deviate significantly from most data. These methods assume that poisoned data instances are likely to be statistical outliers. Techniques such as the <a href="https://ubalt.pressbooks.pub/mathstatsguides/chapter/z-score-basics/">Z-score method</a>, <a href="https://www.itl.nist.gov/div898/handbook/prc/section4/prc471.htm">Tukey’s method</a>, or the [Mahalanobis] <a href="https://www.statisticshowto.com/mahalanobis-distance/">distance</a> can be used to measure the deviation of each data point from the central tendency of the dataset. Data points that exceed a predefined threshold are flagged as potential outliers and considered suspicious for data poisoning.</p>
<p>Clustering-based methods group similar data points together based on their features or attributes. The assumption is that poisoned data instances may form distinct clusters or lie far away from the normal data clusters. By applying clustering algorithms like <a href="https://www.oreilly.com/library/view/data-algorithms/9781491906170/ch12.html">K-means</a>, <a href="https://www.oreilly.com/library/view/machine-learning-algorithms/9781789347999/50efb27d-abbe-4855-ad81-a5357050161f.xhtml">DBSCAN</a>, or <a href="https://www.oreilly.com/library/view/cluster-analysis-5th/9780470978443/chapter04.html">hierarchical clustering</a>, anomalous clusters or data points that do not belong to any cluster can be identified. These anomalous instances are then treated as potentially poisoned data.</p>
<div id="fig-autoencoder" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-autoencoder-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./images/png/autoencoder.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-autoencoder-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;17.31: Autoencoder (Credit: <a href="https://towardsdatascience.com/applied-deep-learning-part-3-autoencoders-1c083af4d798">Dertat</a>)
</figcaption>
</figure>
</div>
<p>Autoencoders are neural networks trained to reconstruct the input data from a compressed representation, as shown in <a href="#fig-autoencoder" class="quarto-xref">Figure&nbsp;<span>17.31</span></a>. They can be used for anomaly detection by learning the normal patterns in the data and identifying instances that deviate from them. During training, the autoencoder is trained on clean, unpoisoned data. At inference time, the reconstruction error for each data point is computed. Data points with high reconstruction errors are considered abnormal and potentially poisoned, as they do not conform to the learned normal patterns.</p>
</section>
<section id="data-sanitization-and-preprocessing-techniques" class="level5">
<h5 class="anchored" data-anchor-id="data-sanitization-and-preprocessing-techniques">Data Sanitization and Preprocessing Techniques</h5>
<p>Data poisoning can be avoided by cleaning data, which involves identifying and removing or correcting noisy, incomplete, or inconsistent data points. Techniques such as data deduplication, missing value imputation, and outlier removal can be applied to improve the quality of the training data. By eliminating or filtering out suspicious or anomalous data points, the impact of poisoned instances can be reduced.</p>
<p>Data validation involves verifying the integrity and consistency of the training data. This can include checking for data type consistency, range validation, and cross-field dependencies. By defining and enforcing data validation rules, anomalous or inconsistent data points indicative of data poisoning can be identified and flagged for further investigation.</p>
<p>Data provenance and lineage tracking involve maintaining a record of data’s origin, transformations, and movements throughout the ML pipeline. By documenting the data sources, preprocessing steps, and any modifications made to the data, practitioners can trace anomalies or suspicious patterns back to their origin. This helps identify potential points of data poisoning and facilitates the investigation and mitigation process.</p>
</section>
<section id="robust-training-techniques" class="level5 page-columns page-full">
<h5 class="anchored" data-anchor-id="robust-training-techniques">Robust Training Techniques</h5>
<p>Robust optimization techniques can be used to modify the training objective to minimize the impact of outliers or poisoned instances. This can be achieved by using robust loss functions less sensitive to extreme values, such as the Huber loss or the modified Huber loss. Regularization techniques, such as <a href="https://towardsdatascience.com/l1-and-l2-regularization-methods-ce25e7fc831c">L1 or L2 regularization</a>, can also help in reducing the model’s sensitivity to poisoned data by constraining the model’s complexity and preventing overfitting.</p>
<p>Robust loss functions are designed to be less sensitive to outliers or noisy data points. Examples include the modified <a href="https://pytorch.org/docs/stable/generated/torch.nn.HuberLoss.html">Huber loss</a>, the Tukey loss <span class="citation" data-cites="beaton1974fitting">(<a href="../../references.html#ref-beaton1974fitting" role="doc-biblioref">Beaton and Tukey 1974</a>)</span>, and the trimmed mean loss. These loss functions down-weight or ignore the contribution of abnormal instances during training, reducing their impact on the model’s learning process. Robust objective functions, such as the minimax or distributionally robust objective, aim to optimize the model’s performance under worst-case scenarios or in the presence of adversarial perturbations.</p>
<div class="no-row-height column-margin column-container"><div id="ref-beaton1974fitting" class="csl-entry" role="listitem">
Beaton, Albert E., and John W. Tukey. 1974. <span>“The Fitting of Power Series, Meaning Polynomials, Illustrated on Band-Spectroscopic Data.”</span> <em>Technometrics</em> 16 (2): 147. <a href="https://doi.org/10.2307/1267936">https://doi.org/10.2307/1267936</a>.
</div></div><p>Data augmentation techniques involve generating additional training examples by applying random transformations or perturbations to the existing data <a href="#fig-data-augmentation" class="quarto-xref">Figure&nbsp;<span>17.32</span></a>. This helps in increasing the diversity and robustness of the training dataset. By introducing controlled variations in the data, the model becomes less sensitive to specific patterns or artifacts that may be present in poisoned instances. Randomization techniques, such as random subsampling or bootstrap aggregating, can also help reduce the impact of poisoned data by training multiple models on different subsets of the data and combining their predictions.</p>
<div id="fig-data-augmentation" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-data-augmentation-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./images/png/data_augmentation.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-data-augmentation-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;17.32: An image of the number “3” in original form and with basic augmentations applied.
</figcaption>
</figure>
</div>
</section>
<section id="secure-and-trusted-data-sourcing" class="level5">
<h5 class="anchored" data-anchor-id="secure-and-trusted-data-sourcing">Secure and Trusted Data Sourcing</h5>
<p>Implementing the best data collection and curation practices can help mitigate the risk of data poisoning. This includes establishing clear data collection protocols, verifying the authenticity and reliability of data sources, and conducting regular data quality assessments. Sourcing data from trusted and reputable providers and following secure data handling practices can reduce the likelihood of introducing poisoned data into the training pipeline.</p>
<p>Strong data governance and access control mechanisms are essential to prevent unauthorized modifications or tampering with the training data. This involves defining clear roles and responsibilities for data access, implementing access control policies based on the principle of least privilege, and monitoring and logging data access activities. By restricting access to the training data and maintaining an audit trail, potential data poisoning attempts can be detected and investigated.</p>
<p>Detecting and mitigating data poisoning attacks requires a multifaceted approach that combines anomaly detection, data sanitization, robust training techniques, and secure data sourcing practices. By implementing these measures, ML practitioners can enhance the resilience of their models against data poisoning and ensure the integrity and trustworthiness of the training data. However, it is important to note that data poisoning is an active area of research, and new attack vectors and defense mechanisms continue to emerge. Staying informed about the latest developments and adopting a proactive and adaptive approach to data security is crucial for maintaining the robustness of ML systems.</p>
</section>
</section>
<section id="distribution-shifts-1" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="distribution-shifts-1">Distribution Shifts</h4>
<section id="detecting-and-mitigating-distribution-shifts" class="level5">
<h5 class="anchored" data-anchor-id="detecting-and-mitigating-distribution-shifts">Detecting and Mitigating Distribution Shifts</h5>
<p>Recall that distribution shifts occur when the data distribution encountered by a machine learning (ML) model during deployment differs from the distribution it was trained on. These shifts can significantly impact the model’s performance and generalization ability, leading to suboptimal or incorrect predictions. Detecting and mitigating distribution shifts is crucial to ensure the robustness and reliability of ML systems in real-world scenarios.</p>
</section>
<section id="detection-techniques-for-distribution-shifts" class="level5">
<h5 class="anchored" data-anchor-id="detection-techniques-for-distribution-shifts">Detection Techniques for Distribution Shifts</h5>
<p>Statistical tests can be used to compare the distributions of the training and test data to identify significant differences. Techniques such as the Kolmogorov-Smirnov test or the Anderson-Darling test measure the discrepancy between two distributions and provide a quantitative assessment of the presence of distribution shift. By applying these tests to the input features or the model’s predictions, practitioners can detect if there is a statistically significant difference between the training and test distributions.</p>
<p>Divergence metrics quantify the dissimilarity between two probability distributions. Commonly used divergence metrics include the <a href="https://towardsdatascience.com/understanding-kl-divergence-f3ddc8dff254">Kullback-Leibler (KL) divergence</a> and the [Jensen-Shannon (JS)] <a href="https://towardsdatascience.com/how-to-understand-and-use-jensen-shannon-divergence-b10e11b03fd6">divergence</a>. By calculating the divergence between the training and test data distributions, practitioners can assess the extent of the distribution shift. High divergence values indicate a significant difference between the distributions, suggesting the presence of a distribution shift.</p>
<p>Uncertainty quantification techniques, such as Bayesian neural networks or ensemble methods, can estimate the uncertainty associated with the model’s predictions. When a model is applied to data from a different distribution, its predictions may have higher uncertainty. By monitoring the uncertainty levels, practitioners can detect distribution shifts. If the uncertainty consistently exceeds a predetermined threshold for test samples, it suggests that the model is operating outside its trained distribution.</p>
<p>In addition, domain classifiers are trained to distinguish between different domains or distributions. Practitioners can detect distribution shifts by training a classifier to differentiate between the training and test domains. If the domain classifier achieves high accuracy in distinguishing between the two domains, it indicates a significant difference in the underlying distributions. The performance of the domain classifier serves as a measure of the distribution shift.</p>
</section>
<section id="mitigation-techniques-for-distribution-shifts" class="level5 page-columns page-full">
<h5 class="anchored" data-anchor-id="mitigation-techniques-for-distribution-shifts">Mitigation Techniques for Distribution Shifts</h5>
<div id="fig-transfer-learning" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-transfer-learning-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./images/png/transfer_learning.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-transfer-learning-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;17.33: Transfer learning (Credit: <a href="https://medium.com/modern-nlp/transfer-learning-in-nlp-f5035cc3f62f">Bhavsar</a>)
</figcaption>
</figure>
</div>
<p>Transfer learning leverages knowledge gained from one domain to improve performance in another, as shown in <a href="#fig-transfer-learning" class="quarto-xref">Figure&nbsp;<span>17.33</span></a>. By using pre-trained models or transferring learned features from a source domain to a target domain, transfer learning can help mitigate the impact of distribution shifts. The pre-trained model can be fine-tuned on a small amount of labeled data from the target domain, allowing it to adapt to the new distribution. Transfer learning is particularly effective when the source and target domains share similar characteristics or when labeled data in the target domain is scarce.</p>
<p>Continual learning, also known as lifelong learning, enables ML models to learn continuously from new data distributions while retaining knowledge from previous distributions. Techniques such as elastic weight consolidation (EWC) <span class="citation" data-cites="kirkpatrick2017overcoming">(<a href="../../references.html#ref-kirkpatrick2017overcoming" role="doc-biblioref">Kirkpatrick et al. 2017</a>)</span> or gradient episodic memory (GEM) <span class="citation" data-cites="lopez2017gradient">(<a href="../../references.html#ref-lopez2017gradient" role="doc-biblioref">Lopez-Paz and Ranzato 2017</a>)</span> allow models to adapt to evolving data distributions over time. These techniques aim to balance the plasticity of the model (ability to learn from new data) with the stability of the model (retaining previously learned knowledge). By incrementally updating the model with new data and mitigating catastrophic forgetting, continual learning helps models stay robust to distribution shifts.</p>
<div class="no-row-height column-margin column-container"><div id="ref-kirkpatrick2017overcoming" class="csl-entry" role="listitem">
Kirkpatrick, James, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A. Rusu, Kieran Milan, et al. 2017. <span>“Overcoming Catastrophic Forgetting in Neural Networks.”</span> <em>Proc. Natl. Acad. Sci.</em> 114 (13): 3521–26. <a href="https://doi.org/10.1073/pnas.1611835114">https://doi.org/10.1073/pnas.1611835114</a>.
</div><div id="ref-lopez2017gradient" class="csl-entry" role="listitem">
Lopez-Paz, David, and Marc’Aurelio Ranzato. 2017. <span>“Gradient Episodic Memory for Continual Learning.”</span> <em>Adv Neural Inf Process Syst</em> 30.
</div></div><p>Data augmentation techniques, such as those we have seen previously, involve applying transformations or perturbations to the existing training data to increase its diversity and improve the model’s robustness to distribution shifts. By introducing variations in the data, such as rotations, translations, scaling, or adding noise, data augmentation helps the model learn invariant features and generalize better to unseen distributions. Data augmentation can be performed during training and inference to enhance the model’s ability to handle distribution shifts.</p>
<p>Ensemble methods combine multiple models to make predictions more robust to distribution shifts. By training models on different subsets of the data, using different algorithms, or with different hyperparameters, ensemble methods can capture diverse aspects of the data distribution. When presented with a shifted distribution, the ensemble can leverage the strengths of individual models to make more accurate and stable predictions. Techniques like bagging, boosting, or stacking can create effective ensembles.</p>
<p>Regularly updating models with new data from the target distribution is crucial to mitigate the impact of distribution shifts. As the data distribution evolves, models should be retrained or fine-tuned on the latest available data to adapt to the changing patterns. Monitoring model performance and data characteristics can help detect when an update is necessary. By keeping the models up to date, practitioners can ensure they remain relevant and accurate in the face of distribution shifts.</p>
<p>Evaluating models using robust metrics less sensitive to distribution shifts can provide a more reliable assessment of model performance. Metrics such as the area under the precision-recall curve (AUPRC) or the F1 score are more robust to class imbalance and can better capture the model’s performance across different distributions. Additionally, using domain-specific evaluation metrics that align with the desired outcomes in the target domain can provide a more meaningful measure of the model’s effectiveness.</p>
<p>Detecting and mitigating distribution shifts is an ongoing process that requires continuous monitoring, adaptation, and improvement. By employing a combination of detection techniques and mitigation strategies, ML practitioners can proactively identify and address distribution shifts, ensuring the robustness and reliability of their models in real-world deployments. It is important to note that distribution shifts can take various forms and may require domain-specific approaches depending on the nature of the data and the application. Staying informed about the latest research and best practices in handling distribution shifts is essential for building resilient ML systems.</p>
</section>
</section>
</section>
</section>
<section id="software-faults" class="level2 page-columns page-full" data-number="17.5">
<h2 data-number="17.5" class="anchored" data-anchor-id="software-faults"><span class="header-section-number">17.5</span> Software Faults</h2>
<section id="definition-and-characteristics-6" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="definition-and-characteristics-6">Definition and Characteristics</h4>
<p>Software faults refer to defects, errors, or bugs in the runtime software frameworks and components that support the execution and deployment of ML models <span class="citation" data-cites="myllyaho2022misbehaviour">(<a href="../../references.html#ref-myllyaho2022misbehaviour" role="doc-biblioref">Myllyaho et al. 2022</a>)</span>. These faults can arise from various sources, such as programming mistakes, design flaws, or compatibility issues <span class="citation" data-cites="zhang2008distribution">(<a href="../../references.html#ref-zhang2008distribution" role="doc-biblioref">H. Zhang 2008</a>)</span>, and can have significant implications for ML systems’ performance, reliability, and security. Software faults in ML frameworks exhibit several key characteristics:</p>
<div class="no-row-height column-margin column-container"><div id="ref-myllyaho2022misbehaviour" class="csl-entry" role="listitem">
Myllyaho, Lalli, Mikko Raatikainen, Tomi Männistö, Jukka K. Nurminen, and Tommi Mikkonen. 2022. <span>“On Misbehaviour and Fault Tolerance in Machine Learning Systems.”</span> <em>J. Syst. Software</em> 183 (January): 111096. <a href="https://doi.org/10.1016/j.jss.2021.111096">https://doi.org/10.1016/j.jss.2021.111096</a>.
</div><div id="ref-zhang2008distribution" class="csl-entry" role="listitem">
Zhang, Hongyu. 2008. <span>“On the Distribution of Software Faults.”</span> <em>IEEE Trans. Software Eng.</em> 34 (2): 301–2. <a href="https://doi.org/10.1109/tse.2007.70771">https://doi.org/10.1109/tse.2007.70771</a>.
</div></div><ul>
<li><p><strong>Diversity:</strong> Software faults can manifest in different forms, ranging from simple logic and syntax mistakes to more complex issues like memory leaks, race conditions, and integration problems. The variety of fault types adds to the challenge of detecting and mitigating them effectively.</p></li>
<li><p><strong>Propagation:</strong> In ML systems, software faults can propagate through the various layers and components of the framework. A fault in one module can trigger a cascade of errors or unexpected behavior in other parts of the system, making it difficult to pinpoint the root cause and assess the full impact of the fault.</p></li>
<li><p><strong>Intermittency:</strong> Some software faults may exhibit intermittent behavior, occurring sporadically or under specific conditions. These faults can be particularly challenging to reproduce and debug, as they may manifest inconsistently during testing or normal operation.</p></li>
<li><p><strong>Interaction with ML models:</strong> Software faults in ML frameworks can interact with the trained models in subtle ways. For example, a fault in the data preprocessing pipeline may introduce noise or bias into the model’s inputs, leading to degraded performance or incorrect predictions. Similarly, faults in the model serving component may cause inconsistencies between the training and inference environments.</p></li>
<li><p><strong>Impact on system properties:</strong> Software faults can compromise various desirable properties of ML systems, such as performance, scalability, reliability, and security. Faults may lead to slowdowns, crashes, incorrect outputs, or vulnerabilities that attackers can exploit.</p></li>
<li><p><strong>Dependency on external factors:</strong> The occurrence and impact of software faults in ML frameworks often depend on external factors, such as the choice of hardware, operating system, libraries, and configurations. Compatibility issues and version mismatches can introduce faults that are difficult to anticipate and mitigate.</p></li>
</ul>
<p>Understanding the characteristics of software faults in ML frameworks is crucial for developing effective fault prevention, detection, and mitigation strategies. By recognizing the diversity, propagation, intermittency, and impact of software faults, ML practitioners can design more robust and reliable systems resilient to these issues.</p>
</section>
<section id="mechanisms-of-software-faults-in-ml-frameworks" class="level4">
<h4 class="anchored" data-anchor-id="mechanisms-of-software-faults-in-ml-frameworks">Mechanisms of Software Faults in ML Frameworks</h4>
<p>Machine learning frameworks, such as TensorFlow, PyTorch, and sci-kit-learn, provide powerful tools and abstractions for building and deploying ML models. However, these frameworks are not immune to software faults that can impact ML systems’ performance, reliability, and correctness. Let’s explore some of the common software faults that can occur in ML frameworks:</p>
<p><strong>Memory Leaks and Resource Management Issues:</strong> Improper memory management, such as failing to release memory or close file handles, can lead to memory leaks and resource exhaustion over time. This issue is compounded by inefficient memory usage, where creating unnecessary copies of large tensors or not leveraging memory-efficient data structures can cause excessive memory consumption and degrade system performance. Additionally, failing to manage GPU memory properly can result in out-of-memory errors or suboptimal utilization of GPU resources, further exacerbating the problem as shown in <a href="#nt13lz9kgr7t"><span class="quarto-xref">Figure&nbsp;<span>17.34</span></span></a>.</p>
<div id="fig-gpu-out-of-memory" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-gpu-out-of-memory-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./images/png/gpu_out_of_memory.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-gpu-out-of-memory-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;17.34: Example of GPU out-of-the-memory and suboptimal utilization issues
</figcaption>
</figure>
</div>
<p><strong>Synchronization and Concurrency Problems:</strong> Incorrect synchronization between threads or processes can lead to race conditions, deadlocks, or inconsistent behavior in multi-threaded or distributed ML systems. This issue is often tied to improper handling of <a href="https://odsc.medium.com/optimizing-ml-serving-with-asynchronous-architectures-1071fc1be8e2">asynchronous operations</a>, such as non-blocking I/O or parallel data loading, which can cause synchronization issues and impact the correctness of the ML pipeline. Moreover, proper coordination and communication between distributed nodes in a cluster can result in consistency or stale data during training or inference, compromising the reliability of the ML system.</p>
<p><strong>Compatibility Issues:</strong> Mismatches between the versions of ML frameworks, libraries, or dependencies can introduce compatibility problems and runtime errors. Upgrading or changing the versions of underlying libraries without thoroughly testing the impact on the ML system can lead to unexpected behavior or breakages. Furthermore, inconsistencies between the training and deployment environments, such as differences in hardware, operating systems, or package versions, can cause compatibility issues and affect the reproducibility of ML models, making it challenging to ensure consistent performance across different platforms.</p>
<p><strong>Numerical Instability and Precision Errors:</strong> Inadequate handling of <a href="https://pythonnumericalmethods.studentorg.berkeley.edu/notebooks/chapter22.04-Numerical-Error-and-Instability.html">numerical instabilities</a>, such as division by zero, underflow, or overflow, can lead to incorrect calculations or convergence issues during training. This problem is compounded by insufficient precision or rounding errors, which can accumulate over time and impact the accuracy of the ML models, especially in deep learning architectures with many layers. Moreover, improper scaling or normalization of input data can cause numerical instabilities and affect the convergence and performance of optimization algorithms, resulting in suboptimal or unreliable model performance.</p>
<p><strong>Inadequate Error Handling and Exception Management:</strong> Proper error handling and exception management can prevent ML systems from crashing or behaving unexpectedly when encountering exceptional conditions or invalid inputs. Failing to catch and handle specific exceptions or relying on generic exception handling can make it difficult to diagnose and recover from errors gracefully, leading to system instability and reduced reliability. Furthermore, incomplete or misleading error messages can hinder the ability to effectively debug and resolve software faults in ML frameworks, prolonging the time required to identify and fix issues.</p>
</section>
<section id="impact-on-ml-systems-6" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="impact-on-ml-systems-6">Impact on ML Systems</h4>
<p>Software faults in machine learning frameworks can have significant and far-reaching impacts on ML systems’ performance, reliability, and security. Let’s explore the various ways in which software faults can affect ML systems:</p>
<p><strong>Performance Degradation and System Slowdowns:</strong> Memory leaks and inefficient resource management can lead to gradual performance degradation over time as the system becomes increasingly memory-constrained and spends more time on garbage collection or memory swapping <span class="citation" data-cites="maas2008combining">(<a href="../../references.html#ref-maas2008combining" role="doc-biblioref">Maas et al. 2024</a>)</span>. This issue is compounded by synchronization issues and concurrency bugs, which can cause delays, reduced throughput, and suboptimal utilization of computational resources, especially in multi-threaded or distributed ML systems. Furthermore, compatibility problems or inefficient code paths can introduce additional overhead and slowdowns, affecting the overall performance of the ML system.</p>
<div class="no-row-height column-margin column-container"><div id="ref-maas2008combining" class="csl-entry" role="listitem">
Maas, Martin, David G. Andersen, Michael Isard, Mohammad Mahdi Javanmard, Kathryn S. McKinley, and Colin Raffel. 2024. <span>“Combining Machine Learning and Lifetime-Based Resource Management for Memory Allocation and Beyond.”</span> <em>Commun. ACM</em> 67 (4): 87–96. <a href="https://doi.org/10.1145/3611018">https://doi.org/10.1145/3611018</a>.
</div></div><p><strong>Incorrect Predictions or Outputs:</strong> Software faults in data preprocessing, feature engineering, or model evaluation can introduce biases, noise, or errors propagating through the ML pipeline and resulting in incorrect predictions or outputs. Over time, numerical instabilities, precision errors, or <a href="https://www.cs.drexel.edu/~popyack/Courses/CSP/Fa17/extras/Rounding/index.html">rounding issues</a> can accumulate and lead to degraded accuracy or convergence problems in the trained models. Moreover, faults in the model serving or inference components can cause inconsistencies between the expected and actual outputs, leading to incorrect or unreliable predictions in production.</p>
<p><strong>Reliability and Stability Issues:</strong> Software faults can cause Unparalleled exceptions, crashes, or sudden terminations that can compromise the reliability and stability of ML systems, especially in production environments. Intermittent or sporadic faults can be difficult to reproduce and diagnose, leading to unpredictable behavior and reduced confidence in the ML system’s outputs. Additionally, faults in checkpointing, model serialization, or state management can cause data loss or inconsistencies, affecting the reliability and recoverability of the ML system.</p>
<p><strong>Security Vulnerabilities:</strong> Software faults, such as buffer overflows, injection vulnerabilities, or improper access control, can introduce security risks and expose the ML system to potential attacks or unauthorized access. Adversaries may exploit faults in the preprocessing or feature extraction stages to manipulate the input data and deceive the ML models, leading to incorrect or malicious behavior. Furthermore, inadequate protection of sensitive data, such as user information or confidential model parameters, can lead to data breaches or privacy violations <span class="citation" data-cites="li2021survey">(<a href="../../references.html#ref-li2021survey" role="doc-biblioref">Q. Li et al. 2023</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-li2021survey" class="csl-entry" role="listitem">
Li, Qinbin, Zeyi Wen, Zhaomin Wu, Sixu Hu, Naibo Wang, Yuan Li, Xu Liu, and Bingsheng He. 2023. <span>“A Survey on Federated Learning Systems: <span>Vision,</span> Hype and Reality for Data Privacy and Protection.”</span> <em>IEEE Trans. Knowl. Data Eng.</em> 35 (4): 3347–66. <a href="https://doi.org/10.1109/tkde.2021.3124599">https://doi.org/10.1109/tkde.2021.3124599</a>.
</div></div><p><strong>Difficulty in Reproducing and Debugging:</strong> Software faults can make it challenging to reproduce and debug issues in ML systems, especially when the faults are intermittent or dependent on specific runtime conditions. Incomplete or ambiguous error messages, coupled with the complexity of ML frameworks and models, can prolong the debugging process and hinder the ability to identify and fix the underlying faults. Moreover, inconsistencies between development, testing, and production environments can make reproducing and diagnosing faults in specific contexts difficult.</p>
<p><strong>Increased Development and Maintenance Costs</strong> Software faults can lead to increased development and maintenance costs, as teams spend more time and resources debugging, fixing, and validating the ML system. The need for extensive testing, monitoring, and fault-tolerant mechanisms to mitigate the impact of software faults can add complexity and overhead to the ML development process. Frequent patches, updates, and bug fixes to address software faults can disrupt the development workflow and require additional effort to ensure the stability and compatibility of the ML system.</p>
<p>Understanding the potential impact of software faults on ML systems is crucial for prioritizing testing efforts, implementing fault-tolerant designs, and establishing effective monitoring and debugging practices. By proactively addressing software faults and their consequences, ML practitioners can build more robust, reliable, and secure ML systems that deliver accurate and trustworthy results.</p>
</section>
<section id="detection-and-mitigation-2" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="detection-and-mitigation-2">Detection and Mitigation</h4>
<p>Detecting and mitigating software faults in machine learning frameworks is essential to ensure ML systems’ reliability, performance, and security. Let’s explore various techniques and approaches that can be employed to identify and address software faults effectively:</p>
<p><strong>Thorough Testing and Validation:</strong> Comprehensive unit testing of individual components and modules can verify their correctness and identify potential faults early in development. Integration testing validates the interaction and compatibility between different components of the ML framework, ensuring seamless integration. Systematic testing of edge cases, boundary conditions, and exceptional scenarios helps uncover hidden faults and vulnerabilities. <a href="https://u-tor.com/topic/regression-vs-integration">Continuous testing and regression testing</a> as shown in <a href="#gaprh7zcofc9"><span class="quarto-xref">Figure&nbsp;<span>17.35</span></span></a> detect faults introduced by code changes or updates to the ML framework.</p>
<div id="fig-regression-testing" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-regression-testing-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./images/png/regression_testing.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-regression-testing-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;17.35: Automated regression testing (Credit: <a href="https://u-tor.com/topic/regression-vs-integration">UTOR</a>)
</figcaption>
</figure>
</div>
<p><strong>Static Code Analysis and Linting:</strong> Utilizing static code analysis tools automatically identifies potential coding issues, such as syntax errors, undefined variables, or security vulnerabilities. Enforcing coding standards and best practices through linting tools maintains code quality and reduces the likelihood of common programming mistakes. Conducting regular code reviews allows manual inspection of the codebase, identification of potential faults, and ensures adherence to coding guidelines and design principles.</p>
<p><strong>Runtime Monitoring and Logging:</strong> Implementing comprehensive logging mechanisms captures relevant information during runtime, such as input data, model parameters, and system events. Monitoring key performance metrics, resource utilization, and error rates helps detect anomalies, performance bottlenecks, or unexpected behavior. Employing runtime assertion checks and invariants validates assumptions and detects violations of expected conditions during program execution. Utilizing <a href="https://microsoft.github.io/code-with-engineering-playbook/machine-learning/ml-profiling/">profiling tools</a> identifies performance bottlenecks, memory leaks, or inefficient code paths that may indicate the presence of software faults.</p>
<p><strong>Fault-Tolerant Design Patterns:</strong> Implementing error handling and exception management mechanisms enables graceful handling and recovery from exceptional conditions or runtime errors. Employing redundancy and failover mechanisms, such as backup systems or redundant computations, ensures the availability and reliability of the ML system in the presence of faults. Designing modular and loosely coupled architectures minimizes the propagation and impact of faults across different components of the ML system. Utilizing checkpointing and recovery mechanisms <span class="citation" data-cites="eisenman2022check">(<a href="../../references.html#ref-eisenman2022check" role="doc-biblioref">Eisenman et al. 2022</a>)</span> allows the system to resume from a known stable state in case of failures or interruptions.</p>
<div class="no-row-height column-margin column-container"><div id="ref-eisenman2022check" class="csl-entry" role="listitem">
Eisenman, Assaf, Kiran Kumar Matam, Steven Ingram, Dheevatsa Mudigere, Raghuraman Krishnamoorthi, Krishnakumar Nair, Misha Smelyanskiy, and Murali Annavaram. 2022. <span>“Check-n-Run: <span>A</span> Checkpointing System for Training Deep Learning Recommendation Models.”</span> In <em>19th USENIX Symposium on Networked Systems Design and Implementation (NSDI 22)</em>, 929–43.
</div></div><p><strong>Regular Updates and Patches:</strong> Staying up to date with the latest versions and patches of the ML frameworks, libraries, and dependencies provides benefits from bug fixes, security updates, and performance improvements. Monitoring release notes, security advisories, and community forums inform practitioners about known issues, vulnerabilities, or compatibility problems in the ML framework. Establishing a systematic process for testing and validating updates and patches before applying them to production systems ensures stability and compatibility.</p>
<p><strong>Containerization and Isolation:</strong> Leveraging containerization technologies, such as <a href="https://www.docker.com">Docker</a> or <a href="https://kubernetes.io">Kubernetes</a>, encapsulates ML components and their dependencies in isolated environments. Utilizing containerization ensures consistent and reproducible runtime environments across development, testing, and production stages, reducing the likelihood of compatibility issues or environment-specific faults. Employing isolation techniques, such as virtual environments or sandboxing, prevents faults or vulnerabilities in one component from affecting other parts of the ML system.</p>
<p><strong>Automated Testing and Continuous Integration/Continuous Deployment (CI/CD):</strong> Implement automated testing frameworks and scripts, execute comprehensive test suites, and catch faults early in development. Integrating automated testing into the CI/CD pipeline, as shown in <a href="#f14k3aj3u8av"><span class="quarto-xref">Figure&nbsp;<span>17.36</span></span></a>, ensures that code changes are thoroughly tested before being merged or deployed to production. Utilizing continuous monitoring and automated alerting systems detects and notifies developers and operators about potential faults or anomalies in real-time.</p>
<div id="fig-CI-CD-procedure" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-CI-CD-procedure-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./images/png/CI_CD_procedure.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-CI-CD-procedure-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;17.36: Continuous Integration/Continuous Deployment (CI/CD) procedure (Credit: <a href="https://www.geeksforgeeks.org/ci-cd-continuous-integration-and-continuous-delivery/">geeksforgeeks</a>)
</figcaption>
</figure>
</div>
<p>Adopting a proactive and systematic approach to fault detection and mitigation can significantly improve ML systems’ robustness, reliability, and maintainability. By investing in comprehensive testing, monitoring, and fault-tolerant design practices, organizations can minimize the impact of software faults and ensure their ML systems’ smooth operation in production environments.</p>
<div id="exr-ft" class="callout callout-style-default callout-caution callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-5-contents" aria-controls="callout-5" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Exercise&nbsp;17.4: Fault Tolerance
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-5" class="callout-5-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<p>Get ready to become an AI fault-fighting superhero! Software glitches can derail machine learning systems, but in this Colab, you’ll learn how to make them resilient. We’ll simulate software faults to see how AI can break, then explore techniques to save your ML model’s progress, like checkpoints in a game. You’ll see how to train your AI to bounce back after a crash, ensuring it stays on track. This is crucial for building reliable, trustworthy AI, especially in critical applications. So gear up because this Colab directly connects with the Robust AI chapter – you’ll move from theory to hands-on troubleshooting and build AI systems that can handle the unexpected!</p>
<p><a href="https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/guide/migrate/fault_tolerance.ipynb#scrollTo=77z2OchJTk0l"><img src="https://colab.research.google.com/assets/colab-badge.png" class="img-fluid"></a></p>
</div>
</div>
</div>
</section>
</section>
<section id="tools-and-frameworks" class="level2 page-columns page-full" data-number="17.6">
<h2 data-number="17.6" class="anchored" data-anchor-id="tools-and-frameworks"><span class="header-section-number">17.6</span> Tools and Frameworks</h2>
<p>Given the significance or importance of developing robust AI systems, in recent years, researchers and practitioners have developed a wide range of tools and frameworks to understand how hardware faults manifest and propagate to impact ML systems. These tools and frameworks play a crucial role in evaluating the resilience of ML systems to hardware faults by simulating various fault scenarios and analyzing their impact on the system’s performance. This enables designers to identify potential vulnerabilities and develop effective mitigation strategies, ultimately creating more robust and reliable ML systems that can operate safely despite hardware faults. This section provides an overview of widely used fault models in the literature and the tools and frameworks developed to evaluate the impact of such faults on ML systems.</p>
<section id="fault-models-and-error-models" class="level3 page-columns page-full" data-number="17.6.1">
<h3 data-number="17.6.1" class="anchored" data-anchor-id="fault-models-and-error-models"><span class="header-section-number">17.6.1</span> Fault Models and Error Models</h3>
<p>As discussed previously, hardware faults can manifest in various ways, including transient, permanent, and intermittent faults. In addition to the type of fault under study, <em>how</em> the fault manifests is also important. For example, does the fault happen in a memory cell or during the computation of a functional unit? Is the impact on a single bit, or does it impact multiple bits? Does the fault propagate all the way and impact the application (causing an error), or does it get masked quickly and is considered benign? All these details impact what is known as the <em>fault model</em>, which plays a major role in simulating and measuring what happens to a system when a fault occurs.</p>
<p>To effectively study and understand the impact of hardware faults on ML systems, it is essential to understand the concepts of fault models and error models. A fault model describes how a hardware fault manifests itself in the system, while an error model represents how the fault propagates and affects the system’s behavior.</p>
<p>Fault models can be categorized based on various characteristics:</p>
<ul>
<li><p><strong>Duration:</strong> Transient faults occur briefly and then disappear, while permanent faults persist indefinitely. Intermittent faults occur sporadically and may be difficult to diagnose.</p></li>
<li><p><strong>Location:</strong> Faults can occur in hardware parts, such as memory cells, functional units, or interconnects.</p></li>
<li><p><strong>Granularity:</strong> Faults can affect a single bit (e.g., bitflip) or multiple bits (e.g., burst errors) within a hardware component.</p></li>
</ul>
<p>On the other hand, error models describe how a fault propagates through the system and manifests as an error. An error may cause the system to deviate from its expected behavior, leading to incorrect results or even system failures. Error models can be defined at different levels of abstraction, from the hardware level (e.g., register-level bitflips) to the software level (e.g., corrupted weights or activations in an ML model).</p>
<p>The fault model (or error model, typically the more applicable terminology in understanding the robustness of an ML system) plays a major role in simulating and measuring what happens to a system when a fault occurs. The chosen model informs the assumptions made about the system being studied. For example, a system focusing on single-bit transient errors <span class="citation" data-cites="sangchoolie2017one">(<a href="../../references.html#ref-sangchoolie2017one" role="doc-biblioref">Sangchoolie, Pattabiraman, and Karlsson 2017</a>)</span> would not be well-suited to understand the impact of permanent, multi-bit flip errors <span class="citation" data-cites="wilkening2014calculating">(<a href="../../references.html#ref-wilkening2014calculating" role="doc-biblioref">Wilkening et al. 2014</a>)</span>, as it is designed assuming a different model altogether.</p>
<div class="no-row-height column-margin column-container"><div id="ref-wilkening2014calculating" class="csl-entry" role="listitem">
Wilkening, Mark, Vilas Sridharan, Si Li, Fritz Previlon, Sudhanva Gurumurthi, and David R. Kaeli. 2014. <span>“Calculating Architectural Vulnerability Factors for Spatial Multi-Bit Transient Faults.”</span> In <em>2014 47th Annual IEEE/ACM International Symposium on Microarchitecture</em>, 293–305. IEEE; IEEE. <a href="https://doi.org/10.1109/micro.2014.15">https://doi.org/10.1109/micro.2014.15</a>.
</div></div><p>Furthermore, implementing an error model is also an important consideration, particularly regarding where an error is said to occur in the compute stack. For instance, a single-bit flip model at the architectural register level differs from a single-bit flip in the weight of a model at the PyTorch level. Although both target a similar error model, the former would usually be modeled in an architecturally accurate simulator (like gem5 [binkert2011gem5]), which captures error propagation compared to the latter, focusing on value propagation through a model.</p>
<p>Recent research has shown that certain characteristics of error models may exhibit similar behaviors across different levels of abstraction <span class="citation" data-cites="sangchoolie2017one">(<a href="../../references.html#ref-sangchoolie2017one" role="doc-biblioref">Sangchoolie, Pattabiraman, and Karlsson 2017</a>)</span> <span class="citation" data-cites="papadimitriou2021demystifying">(<a href="../../references.html#ref-papadimitriou2021demystifying" role="doc-biblioref">Papadimitriou and Gizopoulos 2021</a>)</span>. For example, single-bit errors are generally more problematic than multi-bit errors, regardless of whether they are modeled at the hardware or software level. However, other characteristics, such as error masking <span class="citation" data-cites="mohanram2003partial">(<a href="../../references.html#ref-mohanram2003partial" role="doc-biblioref">Mohanram and Touba 2003</a>)</span> as shown in <a href="#kncu0umx706t"><span class="quarto-xref">Figure&nbsp;<span>17.37</span></span></a>, may not always be accurately captured by software-level models, as they can hide underlying system effects.</p>
<div class="no-row-height column-margin column-container"><div id="ref-sangchoolie2017one" class="csl-entry" role="listitem">
Sangchoolie, Behrooz, Karthik Pattabiraman, and Johan Karlsson. 2017. <span>“One Bit Is <span>(Not)</span> Enough: <span>An</span> Empirical Study of the Impact of Single and Multiple Bit-Flip Errors.”</span> In <em>2017 47th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN)</em>, 97–108. IEEE; IEEE. <a href="https://doi.org/10.1109/dsn.2017.30">https://doi.org/10.1109/dsn.2017.30</a>.
</div><div id="ref-papadimitriou2021demystifying" class="csl-entry" role="listitem">
Papadimitriou, George, and Dimitris Gizopoulos. 2021. <span>“Demystifying the System Vulnerability Stack: <span>Transient</span> Fault Effects Across the Layers.”</span> In <em>2021 ACM/IEEE 48th Annual International Symposium on Computer Architecture (ISCA)</em>, 902–15. IEEE; IEEE. <a href="https://doi.org/10.1109/isca52012.2021.00075">https://doi.org/10.1109/isca52012.2021.00075</a>.
</div><div id="ref-mohanram2003partial" class="csl-entry" role="listitem">
Mohanram, K., and N. A. Touba. 2003. <span>“Partial Error Masking to Reduce Soft Error Failure Rate in Logic Circuits.”</span> In <em>Proceedings. 16th IEEE Symposium on Computer Arithmetic</em>, 433–40. IEEE; IEEE Comput. Soc. <a href="https://doi.org/10.1109/dftvs.2003.1250141">https://doi.org/10.1109/dftvs.2003.1250141</a>.
</div></div><div id="fig-error-masking" class="quarto-figure quarto-figure-center quarto-float anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-error-masking-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./images/png/error_masking.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-error-masking-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;17.37: Example of error masking in microarchitectural components <span class="citation" data-cites="ko2021characterizing">(<a href="../../references.html#ref-ko2021characterizing" role="doc-biblioref">Ko 2021</a>)</span>
</figcaption>
<div class="no-row-height column-margin column-container"><div id="ref-ko2021characterizing" class="csl-entry" role="listitem">
Ko, Yohan. 2021. <span>“Characterizing System-Level Masking Effects Against Soft Errors.”</span> <em>Electronics</em> 10 (18): 2286. <a href="https://doi.org/10.3390/electronics10182286">https://doi.org/10.3390/electronics10182286</a>.
</div></div></figure>
</div>
<p>Some tools, such as Fidelity <span class="citation" data-cites="he2020fidelity">(<a href="../../references.html#ref-he2020fidelity" role="doc-biblioref">He, Balaprakash, and Li 2020</a>)</span>, aim to bridge the gap between hardware-level and software-level error models by mapping patterns between the two levels of abstraction <span class="citation" data-cites="cheng2016clear">(<a href="../../references.html#ref-cheng2016clear" role="doc-biblioref">Cheng et al. 2016</a>)</span>. This allows for more accurate modeling of hardware faults in software-based tools, essential for developing robust and reliable ML systems. Lower-level tools typically represent more accurate error propagation characteristics but must be faster in simulating many errors due to the complex nature of hardware system designs. On the other hand, higher-level tools, such as those implemented in ML frameworks like PyTorch or TensorFlow, which we will discuss soon in the later sections, are often faster and more efficient for evaluating the robustness of ML systems.</p>
<div class="no-row-height column-margin column-container"><div id="ref-cheng2016clear" class="csl-entry" role="listitem">
Cheng, Eric, Shahrzad Mirkhani, Lukasz G. Szafaryn, Chen-Yong Cher, Hyungmin Cho, Kevin Skadron, Mircea R. Stan, et al. 2016. <span>“Clear: <span class="math inline">\(&lt;\)</span>U<span class="math inline">\(&gt;\)</span>c<span class="math inline">\(&lt;\)</span>/u<span class="math inline">\(&gt;\)</span> Ross <span class="math inline">\(&lt;\)</span>u<span class="math inline">\(&gt;\)</span>-l<span class="math inline">\(&lt;\)</span>/u<span class="math inline">\(&gt;\)</span> Ayer <span class="math inline">\(&lt;\)</span>u<span class="math inline">\(&gt;\)</span>e<span class="math inline">\(&lt;\)</span>/u<span class="math inline">\(&gt;\)</span> Xploration for <span class="math inline">\(&lt;\)</span>u<span class="math inline">\(&gt;\)</span>a<span class="math inline">\(&lt;\)</span>/u<span class="math inline">\(&gt;\)</span> Rchitecting <span class="math inline">\(&lt;\)</span>u<span class="math inline">\(&gt;\)</span>r<span class="math inline">\(&lt;\)</span>/u<span class="math inline">\(&gt;\)</span> Esilience - Combining Hardware and Software Techniques to Tolerate Soft Errors in Processor Cores.”</span> In <em>Proceedings of the 53rd Annual Design Automation Conference</em>, 1–6. ACM. <a href="https://doi.org/10.1145/2897937.2897996">https://doi.org/10.1145/2897937.2897996</a>.
</div></div><p>In the following subsections, we will discuss various hardware-based and software-based fault injection methods and tools, highlighting their capabilities, limitations, and the fault and error models they support.</p>
</section>
<section id="hardware-based-fault-injection" class="level3 page-columns page-full" data-number="17.6.2">
<h3 data-number="17.6.2" class="anchored" data-anchor-id="hardware-based-fault-injection"><span class="header-section-number">17.6.2</span> Hardware-based Fault Injection</h3>
<p>An error injection tool is a tool that allows the user to implement a particular error model, such as a transient single-bit flip during inference <a href="#fig-hardware-errors" class="quarto-xref">Figure&nbsp;<span>17.38</span></a>. Most error injection tools are software-based, as software-level tools are faster for ML robustness studies. However, hardware-based fault injection methods are still important for grounding the higher-level error models, as they are considered the most accurate way to study the impact of faults on ML systems by directly manipulating the hardware to introduce faults. These methods allow researchers to observe the system’s behavior under real-world fault conditions. Both software-based and hardware-based error injection tools are described in this section in more detail.</p>
<div id="fig-hardware-errors" class="quarto-figure quarto-figure-center quarto-float anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-hardware-errors-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./images/png/hardware_errors.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-hardware-errors-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;17.38: Hardware errors can occur due to a variety of reasons and at different times and/or locations in a system, which can be explored when studying the impact of hardware-based errors on systems <span class="citation" data-cites="ahmadilivani2024systematic">(<a href="../../references.html#ref-ahmadilivani2024systematic" role="doc-biblioref">Ahmadilivani et al. 2024</a>)</span>
</figcaption>
<div class="no-row-height column-margin column-container"><div id="ref-ahmadilivani2024systematic" class="csl-entry" role="listitem">
Ahmadilivani, Mohammad Hasan, Mahdi Taheri, Jaan Raik, Masoud Daneshtalab, and Maksim Jenihhin. 2024. <span>“A Systematic Literature Review on Hardware Reliability Assessment Methods for Deep Neural Networks.”</span> <em>ACM Comput. Surv.</em> 56 (6): 1–39. <a href="https://doi.org/10.1145/3638242">https://doi.org/10.1145/3638242</a>.
</div></div></figure>
</div>
<section id="methods" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="methods">Methods</h4>
<p>Two of the most common hardware-based fault injection methods are FPGA-based fault injection and radiation or beam testing.</p>
<p><strong>FPGA-based Fault Injection:</strong> Field-Programmable Gate Arrays (FPGAs) are reconfigurable integrated circuits that can be programmed to implement various hardware designs. In the context of fault injection, FPGAs offer high precision and accuracy, as researchers can target specific bits or sets of bits within the hardware. By modifying the FPGA configuration, faults can be introduced at specific locations and times during the execution of an ML model. FPGA-based fault injection allows for fine-grained control over the fault model, enabling researchers to study the impact of different types of faults, such as single-bit flips or multi-bit errors. This level of control makes FPGA-based fault injection a valuable tool for understanding the resilience of ML systems to hardware faults.</p>
<p><strong>Radiation or Beam Testing:</strong> Radiation or beam testing <span class="citation" data-cites="velazco2010combining">(<a href="../../references.html#ref-velazco2010combining" role="doc-biblioref">Velazco, Foucard, and Peronnard 2010</a>)</span> involves exposing the hardware running an ML model to high-energy particles, such as protons or neutrons as illustrated in <a href="#5a77jp776dxi"><span class="quarto-xref">Figure&nbsp;<span>17.39</span></span></a>. These particles can cause bitflips or other types of faults in the hardware, mimicking the effects of real-world radiation-induced faults. Beam testing is widely regarded as a highly accurate method for measuring the error rate induced by particle strikes on a running application. It provides a realistic representation of the faults in real-world environments, particularly in applications exposed to high radiation levels, such as space systems or particle physics experiments. However, unlike FPGA-based fault injection, beam testing could be more precise in targeting specific bits or components within the hardware, as it might be difficult to aim the beam of particles to a particular bit in the hardware. Despite being quite expensive from a research standpoint, beam testing is a well-regarded industry practice for reliability.</p>
<div class="no-row-height column-margin column-container"><div id="ref-velazco2010combining" class="csl-entry" role="listitem">
Velazco, Raoul, Gilles Foucard, and Paul Peronnard. 2010. <span>“Combining Results of Accelerated Radiation Tests and Fault Injections to Predict the Error Rate of an Application Implemented in <span>SRAM</span>-Based <span>FPGAs</span>.”</span> <em>IEEE Trans. Nucl. Sci.</em> 57 (6): 3500–3505. <a href="https://doi.org/10.1109/tns.2010.2087355">https://doi.org/10.1109/tns.2010.2087355</a>.
</div></div><p><img src="./images/png/image15.png" class="img-fluid"></p>
<div id="fig-beam-testing" class="quarto-figure quarto-figure-center quarto-float anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-beam-testing-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./images/png/image14.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-beam-testing-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;17.39: Radiation test setup for semiconductor components <span class="citation" data-cites="lee2022design">(<a href="../../references.html#ref-lee2022design" role="doc-biblioref">Lee et al. 2022</a>)</span> (Credit: <a href="https://jdinstruments.net/tester-capabilities-radiation-test/">JD Instrument</a>)
</figcaption>
<div class="no-row-height column-margin column-container"><div id="ref-lee2022design" class="csl-entry" role="listitem">
Lee, Minwoong, Namho Lee, Huijeong Gwon, Jongyeol Kim, Younggwan Hwang, and Seongik Cho. 2022. <span>“Design of Radiation-Tolerant High-Speed Signal Processing Circuit for Detecting Prompt Gamma Rays by Nuclear Explosion.”</span> <em>Electronics</em> 11 (18): 2970. <a href="https://doi.org/10.3390/electronics11182970">https://doi.org/10.3390/electronics11182970</a>.
</div></div></figure>
</div>
</section>
<section id="limitations" class="level4">
<h4 class="anchored" data-anchor-id="limitations">Limitations</h4>
<p>Despite their high accuracy, hardware-based fault injection methods have several limitations that can hinder their widespread adoption:</p>
<p><strong>Cost:</strong> FPGA-based fault injection and beam testing require specialized hardware and facilities, which can be expensive to set up and maintain. The cost of these methods can be a significant barrier for researchers and organizations with limited resources.</p>
<p><strong>Scalability:</strong> Hardware-based methods are generally slower and less scalable than software-based methods. Injecting faults and collecting data on hardware can take time, limiting the number of experiments performed within a given timeframe. This can be particularly challenging when studying the resilience of large-scale ML systems or conducting statistical analyses that require many fault injection experiments.</p>
<p><strong>Flexibility:</strong> Hardware-based methods may not be as flexible as software-based methods in terms of the range of fault models and error models they can support. Modifying the hardware configuration or the experimental setup to accommodate different fault models can be more challenging and time-consuming than software-based methods.</p>
<p>Despite these limitations, hardware-based fault injection methods remain essential tools for validating the accuracy of software-based methods and for studying the impact of faults on ML systems in realistic settings. By combining hardware-based and software-based methods, researchers can gain a more comprehensive understanding of ML systems’ resilience to hardware faults and develop effective mitigation strategies.</p>
</section>
</section>
<section id="software-based-fault-injection-tools" class="level3 page-columns page-full" data-number="17.6.3">
<h3 data-number="17.6.3" class="anchored" data-anchor-id="software-based-fault-injection-tools"><span class="header-section-number">17.6.3</span> Software-based Fault Injection Tools</h3>
<p>With the rapid development of ML frameworks in recent years, software-based fault injection tools have gained popularity in studying the resilience of ML systems to hardware faults. These tools simulate the effects of hardware faults by modifying the software representation of the ML model or the underlying computational graph. The rise of ML frameworks such as TensorFlow, PyTorch, and Keras has facilitated the development of fault injection tools that are tightly integrated with these frameworks, making it easier for researchers to conduct fault injection experiments and analyze the results.</p>
<section id="advantages-and-trade-offs" class="level5">
<h5 class="anchored" data-anchor-id="advantages-and-trade-offs">Advantages and Trade-offs</h5>
<p>Software-based fault injection tools offer several advantages over hardware-based methods:</p>
<p><strong>Speed:</strong> Software-based tools are generally faster than hardware-based methods, as they do not require the modification of physical hardware or the setup of specialized equipment. This allows researchers to conduct more fault injection experiments in a shorter time, enabling more comprehensive analyses of the resilience of ML systems.</p>
<p><strong>Flexibility:</strong> Software-based tools are more flexible than hardware-based methods in terms of the range of fault and error models they can support. Researchers can easily modify the fault injection tool’s software implementation to accommodate different fault models or to target specific components of the ML system.</p>
<p><strong>Accessibility:</strong> Software-based tools are more accessible than hardware-based methods, as they do not require specialized hardware or facilities. This makes it easier for researchers and practitioners to conduct fault injection experiments and study the resilience of ML systems, even with limited resources.</p>
</section>
<section id="limitations-1" class="level5">
<h5 class="anchored" data-anchor-id="limitations-1">Limitations</h5>
<p>Software-based fault injection tools also have some limitations compared to hardware-based methods:</p>
<p><strong>Accuracy:</strong> Software-based tools may not always capture the full range of effects that hardware faults can have on the system. As these tools operate at a higher level of abstraction, they may need to catch up on some of the low-level hardware interactions and error propagation mechanisms that can impact the behavior of the ML system.</p>
<p><strong>Fidelity:</strong> Software-based tools may provide a different level of Fidelity than hardware-based methods in terms of representing real-world fault conditions. The accuracy of the results obtained from software-based fault injection experiments may depend on how closely the software model approximates the actual hardware behavior.</p>
<div id="fig-mavfi" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-mavfi-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./images/jpg/mavfi.jpg" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-mavfi-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;17.40: Comparison of techniques at layers of abstraction (Credit: <a href="https://ieeexplore.ieee.org/abstract/document/10315202">MAVFI</a>)
</figcaption>
</figure>
</div>
</section>
<section id="types-of-fault-injection-tools" class="level5 page-columns page-full">
<h5 class="anchored" data-anchor-id="types-of-fault-injection-tools">Types of Fault Injection Tools</h5>
<p>Software-based fault injection tools can be categorized based on their target frameworks or use cases. Here, we will discuss some of the most popular tools in each category:</p>
<p>Ares <span class="citation" data-cites="reagen2018ares">(<a href="../../references.html#ref-reagen2018ares" role="doc-biblioref">Reagen et al. 2018</a>)</span>, a fault injection tool initially developed for the Keras framework in 2018, emerged as one of the first tools to study the impact of hardware faults on deep neural networks (DNNs) in the context of the rising popularity of ML frameworks in the mid-to-late 2010s. The tool was validated against a DNN accelerator implemented in silicon, demonstrating its effectiveness in modeling hardware faults. Ares provides a comprehensive study on the impact of hardware faults in both weights and activation values, characterizing the effects of single-bit flips and bit-error rates (BER) on hardware structures. Later, the Ares framework was extended to support the PyTorch ecosystem, enabling researchers to investigate hardware faults in a more modern setting and further extending its utility in the field.</p>
<div class="no-row-height column-margin column-container"><div id="ref-reagen2018ares" class="csl-entry" role="listitem">
Reagen, Brandon, Udit Gupta, Lillian Pentecost, Paul Whatmough, Sae Kyu Lee, Niamh Mulholland, David Brooks, and Gu-Yeon Wei. 2018. <span>“Ares: <span>A</span> Framework for Quantifying the Resilience of Deep Neural Networks.”</span> In <em>2018 55th ACM/ESDA/IEEE Design Automation Conference (DAC)</em>, 1–6. IEEE. <a href="https://doi.org/10.1109/dac.2018.8465834">https://doi.org/10.1109/dac.2018.8465834</a>.
</div></div><div id="fig-phantom-objects" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-phantom-objects-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./images/png/phantom_objects.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-phantom-objects-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;17.41: Hardware bitflips in ML workloads can cause phantom objects and misclassifications, which can erroneously be used downstream by larger systems, such as in autonomous driving. Shown above is a correct and faulty version of the same image using the PyTorchFI injection framework.
</figcaption>
</figure>
</div>
<p>PyTorchFI <span class="citation" data-cites="mahmoud2020pytorchfi">(<a href="../../references.html#ref-mahmoud2020pytorchfi" role="doc-biblioref">Mahmoud et al. 2020</a>)</span>, a fault injection tool specifically designed for the PyTorch framework, was developed in 2020 in collaboration with Nvidia Research. It enables the injection of faults into the weights, activations, and gradients of PyTorch models, supporting a wide range of fault models. By leveraging the GPU acceleration capabilities of PyTorch, PyTorchFI provides a fast and efficient implementation for conducting fault injection experiments on large-scale ML systems, as shown in <a href="#txkz61sj1mj4"><span class="quarto-xref">Figure&nbsp;<span>17.41</span></span></a>. The tool’s speed and ease of use have led to widespread adoption in the community, resulting in multiple developer-led projects, such as PyTorchALFI by Intel xColabs, which focuses on safety in automotive environments. Follow-up PyTorch-centric tools for fault injection include Dr.&nbsp;DNA by Meta <span class="citation" data-cites="ma2024dr">(<a href="../../references.html#ref-ma2024dr" role="doc-biblioref">Ma et al. 2024</a>)</span> (which further facilitates the Pythonic programming model for ease of use), and the GoldenEye framework <span class="citation" data-cites="mahmoud2022dsn">(<a href="../../references.html#ref-mahmoud2022dsn" role="doc-biblioref">Mahmoud et al. 2022</a>)</span>, which incorporates novel numerical datatypes (such as AdaptivFloat <span class="citation" data-cites="tambe2020algorithm">(<a href="../../references.html#ref-tambe2020algorithm" role="doc-biblioref">Tambe et al. 2020</a>)</span> and <a href="https://en.wikipedia.org/wiki/Bfloat16_floating-point_format">BlockFloat</a> in the context of hardware bit flips.</p>
<div class="no-row-height column-margin column-container"><div id="ref-mahmoud2020pytorchfi" class="csl-entry" role="listitem">
Mahmoud, Abdulrahman, Neeraj Aggarwal, Alex Nobbe, Jose Rodrigo Sanchez Vicarte, Sarita V. Adve, Christopher W. Fletcher, Iuri Frosio, and Siva Kumar Sastry Hari. 2020. <span>“<span>PyTorchFI:</span> <span>A</span> Runtime Perturbation Tool for <span>DNNs</span>.”</span> In <em>2020 50th Annual IEEE/IFIP International Conference on Dependable Systems and Networks Workshops (DSN-w)</em>, 25–31. IEEE; IEEE. <a href="https://doi.org/10.1109/dsn-w50199.2020.00014">https://doi.org/10.1109/dsn-w50199.2020.00014</a>.
</div><div id="ref-ma2024dr" class="csl-entry" role="listitem">
Ma, Dongning, Fred Lin, Alban Desmaison, Joel Coburn, Daniel Moore, Sriram Sankar, and Xun Jiao. 2024. <span>“<span>Dr.</span> <span>DNA:</span> <span>Combating</span> Silent Data Corruptions in Deep Learning Using Distribution of Neuron Activations.”</span> In <em>Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 3</em>, 239–52. ACM. <a href="https://doi.org/10.1145/3620666.3651349">https://doi.org/10.1145/3620666.3651349</a>.
</div><div id="ref-mahmoud2022dsn" class="csl-entry" role="listitem">
Mahmoud, Abdulrahman, Thierry Tambe, Tarek Aloui, David Brooks, and Gu-Yeon Wei. 2022. <span>“<span>GoldenEye:</span> <span>A</span> Platform for Evaluating Emerging Numerical Data Formats in <span>DNN</span> Accelerators.”</span> In <em>2022 52nd Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN)</em>, 206–14. IEEE. <a href="https://doi.org/10.1109/dsn53405.2022.00031">https://doi.org/10.1109/dsn53405.2022.00031</a>.
</div><div id="ref-tambe2020algorithm" class="csl-entry" role="listitem">
Tambe, Thierry, En-Yu Yang, Zishen Wan, Yuntian Deng, Vijay Janapa Reddi, Alexander Rush, David Brooks, and Gu-Yeon Wei. 2020. <span>“Algorithm-Hardware Co-Design of Adaptive Floating-Point Encodings for Resilient Deep Learning Inference.”</span> In <em>2020 57th ACM/IEEE Design Automation Conference (DAC)</em>, 1–6. IEEE; IEEE. <a href="https://doi.org/10.1109/dac18072.2020.9218516">https://doi.org/10.1109/dac18072.2020.9218516</a>.
</div><div id="ref-chen2020tensorfi" class="csl-entry" role="listitem">
Chen, Zitao, Niranjhana Narayanan, Bo Fang, Guanpeng Li, Karthik Pattabiraman, and Nathan DeBardeleben. 2020. <span>“<span>TensorFI:</span> <span>A</span> Flexible Fault Injection Framework for <span>TensorFlow</span> Applications.”</span> In <em>2020 IEEE 31st International Symposium on Software Reliability Engineering (ISSRE)</em>, 426–35. IEEE; IEEE. <a href="https://doi.org/10.1109/issre5003.2020.00047">https://doi.org/10.1109/issre5003.2020.00047</a>.
</div><div id="ref-chen2019sc" class="csl-entry" role="listitem">
Chen, Zitao, Guanpeng Li, Karthik Pattabiraman, and Nathan DeBardeleben. 2019. <span>“<span class="nocase"><span class="math inline">\(&lt;\)</span>i<span class="math inline">\(&gt;\)</span>BinFI<span class="math inline">\(&lt;\)</span>/i<span class="math inline">\(&gt;\)</span></span>: An Efficient Fault Injector for Safety-Critical Machine Learning Systems.”</span> In <em>Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis</em>. SC ’19. New York, NY, USA: ACM. <a href="https://doi.org/10.1145/3295500.3356177">https://doi.org/10.1145/3295500.3356177</a>.
</div></div><p>TensorFI <span class="citation" data-cites="chen2020tensorfi">(<a href="../../references.html#ref-chen2020tensorfi" role="doc-biblioref">Chen et al. 2020</a>)</span>, or the TensorFlow Fault Injector, is a fault injection tool developed specifically for the TensorFlow framework. Analogous to Ares and PyTorchFI, TensorFI is considered the state-of-the-art tool for ML robustness studies in the TensorFlow ecosystem. It allows researchers to inject faults into the computational graph of TensorFlow models and study their impact on the model’s performance, supporting a wide range of fault models. One of the key benefits of TensorFI is its ability to evaluate the resilience of various ML models, not just DNNs. Further advancements, such as BinFi <span class="citation" data-cites="chen2019sc">(<a href="../../references.html#ref-chen2019sc" role="doc-biblioref">Chen et al. 2019</a>)</span>, provide a mechanism to speed up error injection experiments by focusing on the "important" bits in the system, accelerating the process of ML robustness analysis and prioritizing the critical components of a model.</p>
<p>NVBitFI <span class="citation" data-cites="tsai2021nvbitfi">(<a href="../../references.html#ref-tsai2021nvbitfi" role="doc-biblioref">T. Tsai et al. 2021</a>)</span>, a general-purpose fault injection tool developed by Nvidia for their GPU platforms, operates at a lower level compared to framework-specific tools like Ares, PyTorchFI, and TensorFlow. While these tools focus on various deep learning platforms to implement and perform robustness analysis, NVBitFI targets the underlying hardware assembly code for fault injection. This allows researchers to inject faults into any application running on Nvidia GPUs, making it a versatile tool for studying the resilience of ML systems and other GPU-accelerated applications. By enabling users to inject errors at the architectural level, NVBitFI provides a more general-purpose fault model that is not restricted to just ML models. As Nvidia’s GPU systems are commonly used in many ML-based systems, NVBitFI is a valuable tool for comprehensive fault injection analysis across various applications.</p>
<div class="no-row-height column-margin column-container"><div id="ref-tsai2021nvbitfi" class="csl-entry" role="listitem">
Tsai, Timothy, Siva Kumar Sastry Hari, Michael Sullivan, Oreste Villa, and Stephen W. Keckler. 2021. <span>“<span>NVBitFI:</span> <span>Dynamic</span> Fault Injection for <span>GPUs</span>.”</span> In <em>2021 51st Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN)</em>, 284–91. IEEE; IEEE. <a href="https://doi.org/10.1109/dsn48987.2021.00041">https://doi.org/10.1109/dsn48987.2021.00041</a>.
</div></div><section id="domain-specific-examples" class="level6 page-columns page-full">
<h6 class="anchored" data-anchor-id="domain-specific-examples">Domain-specific Examples</h6>
<p>Domain-specific fault injection tools have been developed to address various ML application domains’ unique challenges and requirements, such as autonomous vehicles and robotics. This section highlights three domain-specific fault injection tools: DriveFI and PyTorchALFI for autonomous vehicles and MAVFI for uncrewed aerial vehicles (UAVs). These tools enable researchers to inject hardware faults into these complex systems’ perception, control, and other subsystems, allowing them to study the impact of faults on system performance and safety. The development of these software-based fault injection tools has greatly expanded the capabilities of the ML community to develop more robust and reliable systems that can operate safely and effectively in the presence of hardware faults.</p>
<p>DriveFI <span class="citation" data-cites="jha2019ml">(<a href="../../references.html#ref-jha2019ml" role="doc-biblioref">Jha et al. 2019</a>)</span> is a fault injection tool designed for autonomous vehicles. It enables the injection of hardware faults into the perception and control pipelines of autonomous vehicle systems, allowing researchers to study the impact of these faults on the system’s performance and safety. DriveFI has been integrated with industry-standard autonomous driving platforms, such as Nvidia DriveAV and Baidu Apollo, making it a valuable tool for evaluating the resilience of autonomous vehicle systems.</p>
<div class="no-row-height column-margin column-container"><div id="ref-jha2019ml" class="csl-entry" role="listitem">
Jha, Saurabh, Subho Banerjee, Timothy Tsai, Siva K. S. Hari, Michael B. Sullivan, Zbigniew T. Kalbarczyk, Stephen W. Keckler, and Ravishankar K. Iyer. 2019. <span>“<span>ML</span>-Based Fault Injection for Autonomous Vehicles: <span>A</span> Case for <span>Bayesian</span> Fault Injection.”</span> In <em>2019 49th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN)</em>, 112–24. IEEE; IEEE. <a href="https://doi.org/10.1109/dsn.2019.00025">https://doi.org/10.1109/dsn.2019.00025</a>.
</div><div id="ref-grafe2023large" class="csl-entry" role="listitem">
Gräfe, Ralf, Qutub Syed Sha, Florian Geissler, and Michael Paulitsch. 2023. <span>“Large-Scale Application of Fault Injection into <span>PyTorch</span> Models -an Extension to <span>PyTorchFI</span> for Validation Efficiency.”</span> In <em>2023 53rd Annual IEEE/IFIP International Conference on Dependable Systems and Networks - Supplemental Volume (DSN-s)</em>, 56–62. IEEE; IEEE. <a href="https://doi.org/10.1109/dsn-s58398.2023.00025">https://doi.org/10.1109/dsn-s58398.2023.00025</a>.
</div></div><p>PyTorchALFI <span class="citation" data-cites="grafe2023large">(<a href="../../references.html#ref-grafe2023large" role="doc-biblioref">Gräfe et al. 2023</a>)</span> is an extension of PyTorchFI developed by Intel xColabs for the autonomous vehicle domain. It builds upon PyTorchFI’s fault injection capabilities. It adds features specifically tailored for evaluating the resilience of autonomous vehicle systems, such as the ability to inject faults into the camera and LiDAR sensor data.</p>
<p>MAVFI <span class="citation" data-cites="hsiao2023mavfi">(<a href="../../references.html#ref-hsiao2023mavfi" role="doc-biblioref">Hsiao et al. 2023</a>)</span> is a fault injection tool designed for the robotics domain, specifically for uncrewed aerial vehicles (UAVs). MAVFI is built on top of the Robot Operating System (ROS) framework and allows researchers to inject faults into the various components of a UAV system, such as sensors, actuators, and control algorithms. By evaluating the impact of these faults on the UAV’s performance and stability, researchers can develop more resilient and fault-tolerant UAV systems.</p>
<div class="no-row-height column-margin column-container"><div id="ref-hsiao2023mavfi" class="csl-entry" role="listitem">
Hsiao, Yu-Shun, Zishen Wan, Tianyu Jia, Radhika Ghosal, Abdulrahman Mahmoud, Arijit Raychowdhury, David Brooks, Gu-Yeon Wei, and Vijay Janapa Reddi. 2023. <span>“<span>MAVFI:</span> <span>An</span> End-to-End Fault Analysis Framework with Anomaly Detection and Recovery for Micro Aerial Vehicles.”</span> In <em>2023 Design, Automation &amp;Amp; Test in Europe Conference &amp;Amp; Exhibition (DATE)</em>, 1–6. IEEE; IEEE. <a href="https://doi.org/10.23919/date56975.2023.10137246">https://doi.org/10.23919/date56975.2023.10137246</a>.
</div></div><p>The development of software-based fault injection tools has greatly expanded the capabilities of researchers and practitioners to study the resilience of ML systems to hardware faults. By leveraging the speed, flexibility, and accessibility of these tools, the ML community can develop more robust and reliable systems that can operate safely and effectively in the presence of hardware faults.</p>
</section>
</section>
</section>
<section id="bridging-the-gap-between-hardware-and-software-error-models" class="level3 page-columns page-full" data-number="17.6.4">
<h3 data-number="17.6.4" class="anchored" data-anchor-id="bridging-the-gap-between-hardware-and-software-error-models"><span class="header-section-number">17.6.4</span> Bridging the Gap between Hardware and Software Error Models</h3>
<p>While software-based fault injection tools offer many advantages in speed, flexibility, and accessibility, they may not always accurately capture the full range of effects that hardware faults can have on the system. This is because software-based tools operate at a higher level of abstraction than hardware-based methods and may miss some of the low-level hardware interactions and error propagation mechanisms that can impact the behavior of the ML system.</p>
<p>As <span class="citation" data-cites="bolchini2022fast">Bolchini et al. (<a href="../../references.html#ref-bolchini2022fast" role="doc-biblioref">2023</a>)</span> illustrates in their work, hardware errors can manifest in complex spatial distribution patterns that are challenging to fully replicate with software-based fault injection alone. They identify four distinct patterns: (a) single point, where the fault corrupts a single value in a feature map; (b) same row, where the fault corrupts a partial or entire row in a single feature map; (c) bullet wake, where the fault corrupts the same location across multiple feature maps; and (d) shatter glass, which combines the effects of same row and bullet wake patterns, as shown in <a href="#fig-hardware-errors-bolchini" class="quarto-xref">Figure&nbsp;<span>17.42</span></a>. These intricate error propagation mechanisms highlight the need for hardware-aware fault injection techniques to accurately assess the resilience of ML systems.</p>
<div class="no-row-height column-margin column-container"></div><div id="fig-hardware-errors-bolchini" class="quarto-figure quarto-figure-center quarto-float anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-hardware-errors-bolchini-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./images/png/hardware_errors_Bolchini.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-hardware-errors-bolchini-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;17.42: Hardware errors may manifest themselves in different ways at the software level, as classified by Bolchini et al. <span class="citation" data-cites="bolchini2022fast">(<a href="../../references.html#ref-bolchini2022fast" role="doc-biblioref">Bolchini et al. 2023</a>)</span>
</figcaption>
<div class="no-row-height column-margin column-container"><div id="ref-bolchini2022fast" class="csl-entry" role="listitem">
Bolchini, Cristiana, Luca Cassano, Antonio Miele, and Alessandro Toschi. 2023. <span>“Fast and Accurate Error Simulation for <span>CNNs</span> Against Soft Errors.”</span> <em>IEEE Trans. Comput.</em> 72 (4): 984–97. <a href="https://doi.org/10.1109/tc.2022.3184274">https://doi.org/10.1109/tc.2022.3184274</a>.
</div></div></figure>
</div>
<p>Researchers have developed tools to address this issue by bridging the gap between low-level hardware error models and higher-level software error models. One such tool is Fidelity, designed to map patterns between hardware-level faults and their software-level manifestations.</p>
<section id="fidelity-bridging-the-gap" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="fidelity-bridging-the-gap">Fidelity: Bridging the Gap</h4>
<p>Fidelity <span class="citation" data-cites="he2020fidelity">(<a href="../../references.html#ref-he2020fidelity" role="doc-biblioref">He, Balaprakash, and Li 2020</a>)</span> is a tool for accurately modeling hardware faults in software-based fault injection experiments. It achieves this by carefully studying the relationship between hardware-level faults and their impact on the software representation of the ML system.</p>
<div class="no-row-height column-margin column-container"><div id="ref-he2020fidelity" class="csl-entry" role="listitem">
He, Yi, Prasanna Balaprakash, and Yanjing Li. 2020. <span>“<span>FIdelity:</span> <span>Efficient</span> Resilience Analysis Framework for Deep Learning Accelerators.”</span> In <em>2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)</em>, 270–81. IEEE; IEEE. <a href="https://doi.org/10.1109/micro50266.2020.00033">https://doi.org/10.1109/micro50266.2020.00033</a>.
</div></div><p>The key insights behind Fidelity are:</p>
<ul>
<li><p><strong>Fault Propagation:</strong> Fidelity models how faults propagate through the hardware and manifest as errors in the software-visible state of the system. By understanding these propagation patterns, Fidelity can more accurately simulate the effects of hardware faults in software-based experiments.</p></li>
<li><p><strong>Fault Equivalence:</strong> Fidelity identifies equivalent classes of hardware faults that produce similar software-level errors. This allows researchers to design software-based fault models that are representative of the underlying hardware faults without the need to model every possible hardware fault individually.</p></li>
<li><p><strong>Layered Approach:</strong> Fidelity employs a layered approach to fault modeling, where the effects of hardware faults are propagated through multiple levels of abstraction, from the hardware to the software level. This approach ensures that the software-based fault models are grounded in the actual behavior of the hardware.</p></li>
</ul>
<p>By incorporating these insights, Fidelity enables software-based fault injection tools to capture the effects of hardware faults on ML systems accurately. This is particularly important for safety-critical applications, where the system’s resilience to hardware faults is paramount.</p>
</section>
<section id="importance-of-capturing-true-hardware-behavior" class="level4">
<h4 class="anchored" data-anchor-id="importance-of-capturing-true-hardware-behavior">Importance of Capturing True Hardware Behavior</h4>
<p>Capturing true hardware behavior in software-based fault injection tools is crucial for several reasons:</p>
<ul>
<li><p><strong>Accuracy:</strong> By accurately modeling the effects of hardware faults, software-based tools can provide more reliable insights into the resilience of ML systems. This is essential for designing and validating fault-tolerant systems that can operate safely and effectively in the presence of hardware faults.</p></li>
<li><p><strong>Reproducibility:</strong> When software-based tools accurately capture hardware behavior, fault injection experiments become more reproducible across different platforms and environments. This is important for the scientific study of ML system resilience, as it allows researchers to compare and validate results across different studies and implementations.</p></li>
<li><p><strong>Efficiency:</strong> Software-based tools that capture true hardware behavior can be more efficient in their fault injection experiments by focusing on the most representative and impactful fault models. This allows researchers to cover a wider range of fault scenarios and system configurations with limited computational resources.</p></li>
<li><p><strong>Mitigation Strategies:</strong> Understanding how hardware faults manifest at the software level is crucial for developing effective mitigation strategies. By accurately capturing hardware behavior, software-based fault injection tools can help researchers identify the most vulnerable components of the ML system and design targeted hardening techniques to improve resilience.</p></li>
</ul>
<p>Tools like Fidelity are vital in advancing the state-of-the-art in ML system resilience research. These tools enable researchers to conduct more accurate, reproducible, and efficient fault injection experiments by bridging the gap between hardware and software error models. As the complexity and criticality of ML systems continue to grow, the importance of capturing true hardware behavior in software-based fault injection tools will only become more apparent.</p>
<p>Ongoing research in this area aims to refine the mapping between hardware and software error models and develop new techniques for efficiently simulating hardware faults in software-based experiments. As these tools mature, they will provide the ML community with increasingly powerful and accessible means to study and improve the resilience of ML systems to hardware faults.</p>
</section>
</section>
</section>
<section id="conclusion" class="level2" data-number="17.7">
<h2 data-number="17.7" class="anchored" data-anchor-id="conclusion"><span class="header-section-number">17.7</span> Conclusion</h2>
<p>Developing robust and resilient AI is paramount as machine learning systems become increasingly integrated into safety-critical applications and real-world environments. This chapter has explored the key challenges to AI robustness arising from hardware faults, malicious attacks, distribution shifts, and software bugs.</p>
<p>Some of the key takeaways include the following:</p>
<ul>
<li><p><strong>Hardware Faults:</strong> Transient, permanent, and intermittent faults in hardware components can corrupt computations and degrade the performance of machine learning models if not properly detected and mitigated. Techniques such as redundancy, error correction, and fault-tolerant designs play a crucial role in building resilient ML systems that can withstand hardware faults.</p></li>
<li><p><strong>Model Robustness:</strong> Malicious actors can exploit vulnerabilities in ML models through adversarial attacks and data poisoning, aiming to induce targeted misclassifications, skew the model’s learned behavior, or compromise the system’s integrity and reliability. Also, distribution shifts can occur when the data distribution encountered during deployment differs from those seen during training, leading to performance degradation. Implementing defensive measures, including adversarial training, anomaly detection, robust model architectures, and techniques such as domain adaptation, transfer learning, and continual learning, is essential to safeguard against these challenges and ensure the model’s reliability and generalization in dynamic environments.</p></li>
<li><p><strong>Software Faults:</strong> Faults in ML frameworks, libraries, and software stacks can propagate errors, degrade performance, and introduce security vulnerabilities. Rigorous testing, runtime monitoring, and adopting fault-tolerant design patterns are essential for building robust software infrastructure supporting reliable ML systems.</p></li>
</ul>
<p>As ML systems take on increasingly complex tasks with real-world consequences, prioritizing resilience becomes critical. The tools and frameworks discussed in this chapter, including fault injection techniques, error analysis methods, and robustness evaluation frameworks, provide practitioners with the means to thoroughly test and harden their ML systems against various failure modes and adversarial conditions.</p>
<p>Moving forward, resilience must be a central focus throughout the entire AI development lifecycle, from data collection and model training to deployment and monitoring. By proactively addressing the multifaceted challenges to robustness, we can develop trustworthy, reliable ML systems that can navigate the complexities and uncertainties of real-world environments.</p>
<p>Future research in robust ML should continue to advance techniques for detecting and mitigating faults, attacks, and distributional shifts. Additionally, exploring novel paradigms for developing inherently resilient AI architectures, such as self-healing systems or fail-safe mechanisms, will be crucial in pushing the boundaries of AI robustness. By prioritizing resilience and investing in developing robust AI systems, we can unlock the full potential of machine learning technologies while ensuring their safe, reliable, and responsible deployment in real-world applications. As AI continues to shape our future, building resilient systems that can withstand the challenges of the real world will be a defining factor in the success and societal impact of this transformative technology.</p>
</section>
<section id="sec-robust-ai-resource" class="level2" data-number="17.8">
<h2 data-number="17.8" class="anchored" data-anchor-id="sec-robust-ai-resource"><span class="header-section-number">17.8</span> Resources</h2>
<p>Here is a curated list of resources to support students and instructors in their learning and teaching journeys. We are continuously working on expanding this collection and will add new exercises soon.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-6-contents" aria-controls="callout-6" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Slides
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-6" class="callout-6-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<p>These slides are a valuable tool for instructors to deliver lectures and for students to review the material at their own pace. We encourage both students and instructors to leverage these slides to enhance their understanding and facilitate effective knowledge transfer.</p>
<ul>
<li><em>Coming soon.</em></li>
</ul>
</div>
</div>
</div>
<div class="callout callout-style-default callout-caution callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-7-contents" aria-controls="callout-7" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Exercises
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-7" class="callout-7-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<p>To reinforce the concepts covered in this chapter, we have curated a set of exercises that challenge students to apply their knowledge and deepen their understanding.</p>
<ul>
<li><p><a href="#exr-ad" class="quarto-xref">Exercise&nbsp;<span>17.1</span></a></p></li>
<li><p><a href="#exr-aa" class="quarto-xref">Exercise&nbsp;<span>17.2</span></a></p></li>
<li><p><a href="#exr-pa" class="quarto-xref">Exercise&nbsp;<span>17.3</span></a></p></li>
<li><p><a href="#exr-ft" class="quarto-xref">Exercise&nbsp;<span>17.4</span></a></p></li>
</ul>
</div>
</div>
</div>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-8-contents" aria-controls="callout-8" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Labs
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-8" class="callout-8-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<p>In addition to exercises, we offer a series of hands-on labs allowing students to gain practical experience with embedded AI technologies. These labs provide step-by-step guidance, enabling students to develop their skills in a structured and supportive environment. We are excited to announce that new labs will be available soon, further enriching the learning experience.</p>
<ul>
<li><em>Coming soon.</em></li>
</ul>
</div>
</div>
</div>



</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
          // target, if specified
          link.setAttribute("target", "_blank");
          if (link.getAttribute("rel") === null) {
            link.setAttribute("rel", "noopener");
          }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../../contents/sustainable_ai/sustainable_ai.html" class="pagination-link  aria-label=" &lt;span="" ai&lt;="" span&gt;"="">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Sustainable AI</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../../contents/generative_ai/generative_ai.html" class="pagination-link" aria-label="<span class='chapter-number'>18</span>&nbsp; <span class='chapter-title'>Generative AI</span>">
        <span class="nav-page-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Generative AI</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>Written, edited and curated by Prof.&nbsp;Vijay Janapa Reddi (Harvard University) with special thanks to the community for their contributions and support.</p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    <div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/harvard-edge/cs249r_book/edit/main/contents/robust_ai/robust_ai.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li><li><a href="https://github.com/harvard-edge/cs249r_book/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li><li><a href="https://github.com/harvard-edge/cs249r_book/blob/main/contents/robust_ai/robust_ai.qmd" class="toc-action"><i class="bi empty"></i>View source</a></li></ul></div></div>
    <div class="nav-footer-right">
<p>This book was built with <a href="https://quarto.org/">Quarto</a>.</p>
</div>
  </div>
</footer>




</body></html>