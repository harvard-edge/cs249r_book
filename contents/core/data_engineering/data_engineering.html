<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.31">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>ML Systems Textbook</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../">
<link href="../../../contents/core/frameworks/frameworks.html" rel="next">
<link href="../../../contents/core/workflow/workflow.html" rel="prev">
<link href="../../../assets/images/icons/favicon.png" rel="icon" type="image/png">
<script src="../../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-4a990d8dcb58f517c7c86712b8f2ac7c.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../site_libs/bootstrap/bootstrap-cbc843cc95873402613d6df7a37f2654.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<link href="../../../site_libs/quarto-contrib/foldbox/foldbox.css" rel="stylesheet">
<script src="../../../site_libs/quarto-contrib/glightbox/glightbox.min.js"></script>
<link href="../../../site_libs/quarto-contrib/glightbox/glightbox.min.css" rel="stylesheet">
<link href="../../../site_libs/quarto-contrib/glightbox/lightbox.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-M21L0CBCVN"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-M21L0CBCVN', { 'anonymize_ip': true});
</script>
<script type="application/json" class="js-hypothesis-config">
{
  "theme": "clean",
  "openSidebar": false
}
</script>
<script async="" src="https://hypothes.is/embed.js"></script>
<script>
  window.document.addEventListener("DOMContentLoaded", function (_event) {
    document.body.classList.add('hypothesis-enabled');
  });
</script>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">

<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-M21L0CBCVN"></script>
<script type="module" src="../../../tools/scripts/ai_menu/dist/bundle.js" defer=""></script>
<script type="module" src="../../../tools/scripts/ai_menu/dist/sqlite3-opfs-async-proxy-B_ImRJXp.js"></script>
<script type="module" src="../../../tools/scripts/ai_menu/dist/sqlite3-worker1-bundler-friendly-CbDNa4by.js"></script>
<script type="module" src="../../../tools/scripts/ai_menu/dist/worker-voUF5YDa.js"></script>
<script src="../../../assets/scripts/sidebar-auto-collapse.js" defer=""></script>
<style>
.callout-quiz-question {
  --color1: #F0F0F8;
  --color2: #5B4B8A;
}
.callout-resource-exercises {
  --color1: #E0F2F1;
  --color2: #20B2AA;
}
.callout-quiz-answer {
  --color1: #E8F2EA;
  --color2: #4a7c59;
}
.callout-resource-videos {
  --color1: #E0F2F1;
  --color2: #20B2AA;
}
.callout-code {
  --color1: #F2F4F8;
  --color2: #D1D7E0;
}
.callout-resource-slides {
  --color1: #E0F2F1;
  --color2: #20B2AA;
}
.callout-chapter-connection {
  --color1: #FDF2F7;
  --color2: #A51C30;
}
</style>
<style>
details.callout-quiz-question > summary::before {
  background-image: url("../../../assets/images/icons/callouts/icon_callout-quiz-question.png");
}
details.callout-quiz-answer > summary::before {
  background-image: url("../../../assets/images/icons/callouts/icon_callout-quiz-answer.png");
}
details.callout-chapter-connection > summary::before {
  background-image: url("../../../assets/images/icons/callouts/icon_callout-chapter-connection.png");
}
details.callout-resource-slides > summary::before {
  background-image: url("../../../assets/images/icons/callouts/icon_callout-resource-slides.png");
}
details.callout-resource-videos > summary::before {
  background-image: url("../../../assets/images/icons/callouts/icon_callout-resource-videos.png");
}
details.callout-resource-exercises > summary::before {
  background-image: url("../../../assets/images/icons/callouts/icon_callout-resource-exercises.png");
}
</style>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating nav-fixed slimcontent quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-md " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="../../../index.html" class="navbar-brand navbar-brand-logo">
    <img src="../../../assets/images/icons/favicon.png" alt="" class="navbar-logo">
    </a>
    <a class="navbar-brand" href="../../../index.html">
    <span class="navbar-title">Machine Learning Systems</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link active" href="../../../contents/labs/labs.html" aria-current="page"> <i class="bi bi-code" role="img">
</i> 
<span class="menu-text">Labs</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../contents/labs/kits.html"> <i class="bi bi-box" role="img">
</i> 
<span class="menu-text">Kits</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://mlsysbook.ai/pdf" target="_blank"> <i class="bi bi-file-pdf" role="img">
</i> 
<span class="menu-text">PDF</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://mlsysbook.ai/epub" target="_blank"> <i class="bi bi-book" role="img">
</i> 
<span class="menu-text">EPUB</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://github.com/harvard-edge/cs249r_book" target="_blank"> <i class="bi bi-star" role="img">
</i> 
<span class="menu-text">Star</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://opencollective.com/mlsysbook" target="_blank"> <i class="bi bi-heart" role="img">
</i> 
<span class="menu-text">Support</span></a>
  </li>  
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-github" role="link" data-bs-toggle="dropdown" aria-expanded="false">
      <i class="bi bi-github" role="img">
</i> 
 <span class="menu-text">GitHub</span>
    </a>
    <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="nav-menu-github">    
        <li>
    <a class="dropdown-item" href="https://github.com/harvard-edge/cs249r_book" target="_blank"><i class="bi bi-pencil" role="img">
</i> 
 <span class="dropdown-text">Edit this page</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://github.com/harvard-edge/cs249r_book/issues/new" target="_blank"><i class="bi bi-bug" role="img">
</i> 
 <span class="dropdown-text">Report an issue</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://github.com/harvard-edge/cs249r_book/discussions" target="_blank"><i class="bi bi-chat" role="img">
</i> 
 <span class="dropdown-text">Discussions</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://github.com/harvard-edge/cs249r_book" target="_blank"><i class="bi bi-code" role="img">
</i> 
 <span class="dropdown-text">View source</span></a>
  </li>  
    </ul>
  </li>
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../../contents/core/workflow/workflow.html">Design Principles</a></li><li class="breadcrumb-item"><a href="../../../contents/core/data_engineering/data_engineering.html">Data Engineering</a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
    </div>
  </nav>
  <div id="quarto-announcement" data-announcement-id="80cf830e7ea2136b91547bb117b654b4" class="alert alert-primary hidden"><i class="bi bi-megaphone quarto-announcement-icon"></i><div class="quarto-announcement-content">
<p>🎉 <strong>Just Announced:</strong> <em>Machine Learning Systems</em> will be published by <strong>MIT Press</strong>. <a href="https://www.linkedin.com/posts/vijay-janapa-reddi-63a6a173_tinyml-tikz-ai-activity-7338324711145136128-6WU-?utm_source=share&amp;utm_medium=member_desktop&amp;rcm=ACoAAA-V7E4BcYtyZgLSeGhXC2t9jRSlmazfp-I">See the news →</a><br></p>
<p>🚀 <strong>Sneak Peek:</strong> <a href="https://mlsysbook.github.io/TinyTorch/intro.html">Tiny🔥Torch</a>. Build your own machine learning framework from scratch!<br></p>
<p>🧠 <strong>Self-checks:</strong> Added lightweight <a href="../../../contents/core/introduction/introduction.html#quiz-question-sec-introduction-ai-ml-basics-041a">quizzes</a> to each chapter for self-assessment.<br></p>
<p>📦 <strong>New Hardware:</strong> <a href="../../../contents/labs/kits.html">Seeed TinyML Kit</a>. Latest hands-on learning platform.</p>
</div><i class="bi bi-x-lg quarto-announcement-action"></i></div>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Homepage</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/frontmatter/foreword.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Foreword</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/frontmatter/about/about.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">About the Book</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/frontmatter/changelog/changelog.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Book Changelog</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/frontmatter/acknowledgements/acknowledgements.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Acknowledgements</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/frontmatter/socratiq/socratiq.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">SocratiQ AI</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Systems Foundations</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/introduction/introduction.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduction</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/ml_systems/ml_systems.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">ML Systems</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/dl_primer/dl_primer.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">DL Primer</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/dnn_architectures/dnn_architectures.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">DNN Architectures</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">Design Principles</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/workflow/workflow.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">AI Workflow</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/data_engineering/data_engineering.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">Data Engineering</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/frameworks/frameworks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">AI Frameworks</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/training/training.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">AI Training</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true">
 <span class="menu-text">Performance Engineering</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/efficient_ai/efficient_ai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Efficient AI</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/optimizations/optimizations.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Model Optimizations</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/hw_acceleration/hw_acceleration.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">AI Acceleration</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/benchmarking/benchmarking.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Benchmarking AI</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true">
 <span class="menu-text">Robust Deployment</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/ops/ops.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">ML Operations</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/ondevice_learning/ondevice_learning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">On-Device Learning</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/robust_ai/robust_ai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Robust AI</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/privacy_security/privacy_security.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Security &amp; Privacy</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true">
 <span class="menu-text">Trustworthy Systems</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/responsible_ai/responsible_ai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Responsible AI</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/sustainable_ai/sustainable_ai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Sustainable AI</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/ai_for_good/ai_for_good.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">AI for Good</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true">
 <span class="menu-text">Frontiers of ML Systems</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/conclusion/conclusion.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Conclusion</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="true">
 <span class="menu-text">Hands-on Labs</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-7" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/labs.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Getting Started</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/kits.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Hardware Kits</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/ide_setup.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">IDE Setup</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" role="navigation" aria-expanded="true">
 <span class="menu-text">Arduino</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-8" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/arduino/nicla_vision/nicla_vision.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Overview</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/arduino/nicla_vision/setup/setup.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Setup</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/arduino/nicla_vision/image_classification/image_classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Image Classification</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/arduino/nicla_vision/object_detection/object_detection.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Object Detection</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/arduino/nicla_vision/kws/kws.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Keyword Spotting (KWS)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/arduino/nicla_vision/motion_classification/motion_classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Motion Classification and Anomaly Detection</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-9" role="navigation" aria-expanded="true">
 <span class="menu-text">Seeed XIAO ESP32S3</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-9" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-9" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/seeed/xiao_esp32s3/xiao_esp32s3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Overview</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/seeed/xiao_esp32s3/setup/setup.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Setup</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/seeed/xiao_esp32s3/image_classification/image_classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Image Classification</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/seeed/xiao_esp32s3/object_detection/object_detection.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Object Detection</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/seeed/xiao_esp32s3/kws/kws.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Keyword Spotting (KWS)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/seeed/xiao_esp32s3/motion_classification/motion_classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Motion Classification and Anomaly Detection</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-10" role="navigation" aria-expanded="true">
 <span class="menu-text">Grove Vision</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-10" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-10" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/seeed/grove_vision_ai_v2/grove_vision_ai_v2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Overview</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/seeed/grove_vision_ai_v2/setup_and_no_code_apps/setup_and_no_code_apps.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Setup and No-Code Applications</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/seeed/grove_vision_ai_v2/image_classification/image_classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Image Classification</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/seeed/grove_vision_ai_v2/object_detection/object_detection.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Object Detection</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-11" role="navigation" aria-expanded="true">
 <span class="menu-text">Raspberry Pi</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-11" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-11" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/raspi/raspi.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Overview</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/raspi/setup/setup.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Setup</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/raspi/image_classification/image_classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Image Classification</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/raspi/object_detection/object_detection.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Object Detection</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/raspi/llm/llm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Small Language Models (SLM)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/raspi/vlm/vlm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Vision-Language Models (VLM)</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-12" role="navigation" aria-expanded="true">
 <span class="menu-text">Shared</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-12" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-12" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/shared/shared.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Overview</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/shared/kws_feature_eng/kws_feature_eng.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">KWS Feature Engineering</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/shared/dsp_spectral_features_block/dsp_spectral_features_block.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">DSP Spectral Features</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-13" role="navigation" aria-expanded="true">
 <span class="menu-text">Resources</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-13" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-13" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/backmatter/resources/phd_survival_guide.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">PhD Survival Guide</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active" data-toc-expanded="99">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#data-engineering" id="toc-data-engineering" class="nav-link active" data-scroll-target="#data-engineering">Data Engineering</a>
  <ul>
  <li><a href="#purpose" id="toc-purpose" class="nav-link" data-scroll-target="#purpose">Purpose</a></li>
  <li><a href="#sec-data-engineering-overview-d12f" id="toc-sec-data-engineering-overview-d12f" class="nav-link" data-scroll-target="#sec-data-engineering-overview-d12f">Overview</a></li>
  <li><a href="#sec-data-engineering-problem-definition-1064" id="toc-sec-data-engineering-problem-definition-1064" class="nav-link" data-scroll-target="#sec-data-engineering-problem-definition-1064">Problem Definition</a>
  <ul class="collapse">
  <li><a href="#sec-data-engineering-keyword-spotting-example-8e41" id="toc-sec-data-engineering-keyword-spotting-example-8e41" class="nav-link" data-scroll-target="#sec-data-engineering-keyword-spotting-example-8e41">Keyword Spotting Example</a></li>
  </ul></li>
  <li><a href="#sec-data-engineering-pipeline-basics-053a" id="toc-sec-data-engineering-pipeline-basics-053a" class="nav-link" data-scroll-target="#sec-data-engineering-pipeline-basics-053a">Pipeline Basics</a></li>
  <li><a href="#sec-data-engineering-data-sources-50f8" id="toc-sec-data-engineering-data-sources-50f8" class="nav-link" data-scroll-target="#sec-data-engineering-data-sources-50f8">Data Sources</a>
  <ul class="collapse">
  <li><a href="#sec-data-engineering-existing-datasets-7a14" id="toc-sec-data-engineering-existing-datasets-7a14" class="nav-link" data-scroll-target="#sec-data-engineering-existing-datasets-7a14">Existing Datasets</a></li>
  <li><a href="#sec-data-engineering-web-scraping-22e8" id="toc-sec-data-engineering-web-scraping-22e8" class="nav-link" data-scroll-target="#sec-data-engineering-web-scraping-22e8">Web Scraping</a></li>
  <li><a href="#sec-data-engineering-crowdsourcing-9d32" id="toc-sec-data-engineering-crowdsourcing-9d32" class="nav-link" data-scroll-target="#sec-data-engineering-crowdsourcing-9d32">Crowdsourcing</a></li>
  <li><a href="#sec-data-engineering-anonymization-techniques-b65d" id="toc-sec-data-engineering-anonymization-techniques-b65d" class="nav-link" data-scroll-target="#sec-data-engineering-anonymization-techniques-b65d">Anonymization Techniques</a></li>
  <li><a href="#sec-data-engineering-synthetic-data-creation-b775" id="toc-sec-data-engineering-synthetic-data-creation-b775" class="nav-link" data-scroll-target="#sec-data-engineering-synthetic-data-creation-b775">Synthetic Data Creation</a></li>
  <li><a href="#sec-data-engineering-continuing-kws-example-b2f4" id="toc-sec-data-engineering-continuing-kws-example-b2f4" class="nav-link" data-scroll-target="#sec-data-engineering-continuing-kws-example-b2f4">Continuing the KWS Example</a></li>
  </ul></li>
  <li><a href="#sec-data-engineering-data-ingestion-81f3" id="toc-sec-data-engineering-data-ingestion-81f3" class="nav-link" data-scroll-target="#sec-data-engineering-data-ingestion-81f3">Data Ingestion</a>
  <ul class="collapse">
  <li><a href="#sec-data-engineering-ingestion-patterns-0f03" id="toc-sec-data-engineering-ingestion-patterns-0f03" class="nav-link" data-scroll-target="#sec-data-engineering-ingestion-patterns-0f03">Ingestion Patterns</a></li>
  <li><a href="#sec-data-engineering-etl-elt-comparison-ac5b" id="toc-sec-data-engineering-etl-elt-comparison-ac5b" class="nav-link" data-scroll-target="#sec-data-engineering-etl-elt-comparison-ac5b">ETL and ELT Comparison</a></li>
  <li><a href="#sec-data-engineering-data-source-integration-f2aa" id="toc-sec-data-engineering-data-source-integration-f2aa" class="nav-link" data-scroll-target="#sec-data-engineering-data-source-integration-f2aa">Data Source Integration</a></li>
  <li><a href="#sec-data-engineering-validation-techniques-0eff" id="toc-sec-data-engineering-validation-techniques-0eff" class="nav-link" data-scroll-target="#sec-data-engineering-validation-techniques-0eff">Validation Techniques</a></li>
  <li><a href="#sec-data-engineering-error-management-695b" id="toc-sec-data-engineering-error-management-695b" class="nav-link" data-scroll-target="#sec-data-engineering-error-management-695b">Error Management</a></li>
  <li><a href="#sec-data-engineering-continuing-kws-example-8152" id="toc-sec-data-engineering-continuing-kws-example-8152" class="nav-link" data-scroll-target="#sec-data-engineering-continuing-kws-example-8152">Continuing the KWS Example</a></li>
  </ul></li>
  <li><a href="#sec-data-engineering-data-processing-60bd" id="toc-sec-data-engineering-data-processing-60bd" class="nav-link" data-scroll-target="#sec-data-engineering-data-processing-60bd">Data Processing</a>
  <ul class="collapse">
  <li><a href="#sec-data-engineering-cleaning-techniques-4078" id="toc-sec-data-engineering-cleaning-techniques-4078" class="nav-link" data-scroll-target="#sec-data-engineering-cleaning-techniques-4078">Cleaning Techniques</a></li>
  <li><a href="#sec-data-engineering-data-quality-assessment-4cc7" id="toc-sec-data-engineering-data-quality-assessment-4cc7" class="nav-link" data-scroll-target="#sec-data-engineering-data-quality-assessment-4cc7">Data Quality Assessment</a></li>
  <li><a href="#sec-data-engineering-transformation-techniques-e976" id="toc-sec-data-engineering-transformation-techniques-e976" class="nav-link" data-scroll-target="#sec-data-engineering-transformation-techniques-e976">Transformation Techniques</a></li>
  <li><a href="#sec-data-engineering-feature-engineering-20d6" id="toc-sec-data-engineering-feature-engineering-20d6" class="nav-link" data-scroll-target="#sec-data-engineering-feature-engineering-20d6">Feature Engineering</a></li>
  <li><a href="#sec-data-engineering-processing-pipeline-design-e27a" id="toc-sec-data-engineering-processing-pipeline-design-e27a" class="nav-link" data-scroll-target="#sec-data-engineering-processing-pipeline-design-e27a">Processing Pipeline Design</a></li>
  <li><a href="#sec-data-engineering-scalability-considerations-d217" id="toc-sec-data-engineering-scalability-considerations-d217" class="nav-link" data-scroll-target="#sec-data-engineering-scalability-considerations-d217">Scalability Considerations</a></li>
  <li><a href="#sec-data-engineering-continuing-kws-example-fe8e" id="toc-sec-data-engineering-continuing-kws-example-fe8e" class="nav-link" data-scroll-target="#sec-data-engineering-continuing-kws-example-fe8e">Continuing the KWS Example</a></li>
  </ul></li>
  <li><a href="#sec-data-engineering-data-labeling-044f" id="toc-sec-data-engineering-data-labeling-044f" class="nav-link" data-scroll-target="#sec-data-engineering-data-labeling-044f">Data Labeling</a>
  <ul class="collapse">
  <li><a href="#sec-data-engineering-types-labels-c660" id="toc-sec-data-engineering-types-labels-c660" class="nav-link" data-scroll-target="#sec-data-engineering-types-labels-c660">Types of Labels</a></li>
  <li><a href="#sec-data-engineering-annotation-techniques-d939" id="toc-sec-data-engineering-annotation-techniques-d939" class="nav-link" data-scroll-target="#sec-data-engineering-annotation-techniques-d939">Annotation Techniques</a></li>
  <li><a href="#sec-data-engineering-label-quality-assessment-0ce4" id="toc-sec-data-engineering-label-quality-assessment-0ce4" class="nav-link" data-scroll-target="#sec-data-engineering-label-quality-assessment-0ce4">Label Quality Assessment</a></li>
  <li><a href="#sec-data-engineering-ai-annotation-3b26" id="toc-sec-data-engineering-ai-annotation-3b26" class="nav-link" data-scroll-target="#sec-data-engineering-ai-annotation-3b26">AI in Annotation</a></li>
  <li><a href="#sec-data-engineering-labeling-challenges-ae8c" id="toc-sec-data-engineering-labeling-challenges-ae8c" class="nav-link" data-scroll-target="#sec-data-engineering-labeling-challenges-ae8c">Labeling Challenges</a></li>
  <li><a href="#sec-data-engineering-continuing-kws-example-61b4" id="toc-sec-data-engineering-continuing-kws-example-61b4" class="nav-link" data-scroll-target="#sec-data-engineering-continuing-kws-example-61b4">Continuing the KWS Example</a></li>
  </ul></li>
  <li><a href="#sec-data-engineering-data-storage-6651" id="toc-sec-data-engineering-data-storage-6651" class="nav-link" data-scroll-target="#sec-data-engineering-data-storage-6651">Data Storage</a>
  <ul class="collapse">
  <li><a href="#sec-data-engineering-storage-system-types-519d" id="toc-sec-data-engineering-storage-system-types-519d" class="nav-link" data-scroll-target="#sec-data-engineering-storage-system-types-519d">Storage System Types</a></li>
  <li><a href="#sec-data-engineering-storage-considerations-d89f" id="toc-sec-data-engineering-storage-considerations-d89f" class="nav-link" data-scroll-target="#sec-data-engineering-storage-considerations-d89f">Storage Considerations</a></li>
  <li><a href="#sec-data-engineering-performance-factors-7465" id="toc-sec-data-engineering-performance-factors-7465" class="nav-link" data-scroll-target="#sec-data-engineering-performance-factors-7465">Performance Factors</a></li>
  <li><a href="#sec-data-engineering-storage-ml-lifecycle-4b39" id="toc-sec-data-engineering-storage-ml-lifecycle-4b39" class="nav-link" data-scroll-target="#sec-data-engineering-storage-ml-lifecycle-4b39">Storage in ML Lifecycle</a>
  <ul class="collapse">
  <li><a href="#sec-data-engineering-development-phase-b85e" id="toc-sec-data-engineering-development-phase-b85e" class="nav-link" data-scroll-target="#sec-data-engineering-development-phase-b85e">Development Phase</a></li>
  <li><a href="#sec-data-engineering-training-phase-5d40" id="toc-sec-data-engineering-training-phase-5d40" class="nav-link" data-scroll-target="#sec-data-engineering-training-phase-5d40">Training Phase</a></li>
  <li><a href="#sec-data-engineering-deployment-phase-ecde" id="toc-sec-data-engineering-deployment-phase-ecde" class="nav-link" data-scroll-target="#sec-data-engineering-deployment-phase-ecde">Deployment Phase</a></li>
  <li><a href="#sec-data-engineering-maintenance-phase-900e" id="toc-sec-data-engineering-maintenance-phase-900e" class="nav-link" data-scroll-target="#sec-data-engineering-maintenance-phase-900e">Maintenance Phase</a></li>
  </ul></li>
  <li><a href="#sec-data-engineering-feature-storage-dca1" id="toc-sec-data-engineering-feature-storage-dca1" class="nav-link" data-scroll-target="#sec-data-engineering-feature-storage-dca1">Feature Storage</a></li>
  <li><a href="#sec-data-engineering-caching-techniques-bc7d" id="toc-sec-data-engineering-caching-techniques-bc7d" class="nav-link" data-scroll-target="#sec-data-engineering-caching-techniques-bc7d">Caching Techniques</a></li>
  <li><a href="#sec-data-engineering-data-access-patterns-b236" id="toc-sec-data-engineering-data-access-patterns-b236" class="nav-link" data-scroll-target="#sec-data-engineering-data-access-patterns-b236">Data Access Patterns</a></li>
  <li><a href="#sec-data-engineering-continuing-kws-example-dd18" id="toc-sec-data-engineering-continuing-kws-example-dd18" class="nav-link" data-scroll-target="#sec-data-engineering-continuing-kws-example-dd18">Continuing the KWS Example</a></li>
  </ul></li>
  <li><a href="#sec-data-engineering-data-governance-6f5e" id="toc-sec-data-engineering-data-governance-6f5e" class="nav-link" data-scroll-target="#sec-data-engineering-data-governance-6f5e">Data Governance</a></li>
  <li><a href="#sec-data-engineering-summary-286b" id="toc-sec-data-engineering-summary-286b" class="nav-link" data-scroll-target="#sec-data-engineering-summary-286b">Summary</a></li>
  <li><a href="#self-check-answers" id="toc-self-check-answers" class="nav-link" data-scroll-target="#self-check-answers">Self-Check Answers</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content page-columns page-full" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../../contents/core/workflow/workflow.html">Design Principles</a></li><li class="breadcrumb-item"><a href="../../../contents/core/data_engineering/data_engineering.html">Data Engineering</a></li></ol></nav></header>




<section id="data-engineering" class="level1 page-columns page-full">
<h1>Data Engineering</h1>
<div class="{layout-narrow} page-columns page-full">

<div class="no-row-height column-margin column-container"><div class="">
<p><em>DALL·E 3 Prompt: Create a rectangular illustration visualizing the concept of data engineering. Include elements such as raw data sources, data processing pipelines, storage systems, and refined datasets. Show how raw data is transformed through cleaning, processing, and storage to become valuable information that can be analyzed and used for decision-making.</em></p>
</div></div><p> <img src="images/png/cover_data_engineering.png" class="img-fluid"></p>
</div>
<section id="purpose" class="level2 unnumbered">
<h2 class="unnumbered anchored" data-anchor-id="purpose">Purpose</h2>
<p><em>How does data shape ML systems engineering?</em></p>
<p>In the field of machine learning, data engineering is often overshadowed by the allure of sophisticated algorithms, when in fact data plays a foundational role in determining an AI system’s capabilities and limitations. We need to understand the core principles of data in ML systems, exploring how the acquisition, processing, storage, and governance of data directly impact the performance, reliability, and ethical considerations of AI systems. By understanding these fundamental concepts, we can unlock the true potential of AI and build a solid foundation of high-quality ML solutions.</p>
<div class="callout callout-style-default callout-tip callout-titled" title="Learning Objectives">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Learning Objectives
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><p>Analyze different data sourcing methods (datasets, web scraping, crowdsourcing, synthetic data).</p></li>
<li><p>Explain the importance of data labeling and ensure label quality.</p></li>
<li><p>Evaluate data storage systems for ML workloads (databases, data warehouses, data lakes).</p></li>
<li><p>Describe the role of data pipelines in ML systems.</p></li>
<li><p>Explain the importance of data governance in ML (security, privacy, ethics).</p></li>
<li><p>Identify key challenges in data engineering for ML.</p></li>
</ul>
</div>
</div>
</section>
<section id="sec-data-engineering-overview-d12f" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="sec-data-engineering-overview-d12f">Overview</h2>
<p>Data is the foundation of modern machine learning systems, as success is governed by the quality and accessibility of training and evaluation data. Despite its pivotal role, data engineering is often overlooked compared to algorithm design and model development. However, the effectiveness of any machine learning system hinges on the robustness of its data pipeline. As machine learning applications become more sophisticated, the challenges associated with curating, cleaning, organizing, and storing data have grown significantly. These activities have emerged as some of the most resource-intensive aspects of the data engineering process, requiring sustained effort and attention.</p>
<div class="callout callout-style-default callout-note callout-titled" title="Definition of Data Engineering">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Definition of Data Engineering
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Data Engineering</strong> is the <em>process of designing, building, and maintaining</em> the infrastructure and systems that <em>collect, store, and process</em> data for analysis and machine learning. It involves <em>data acquisition, transformation, and management</em>, ensuring data is <em>reliable, accessible, and optimized</em> for downstream applications. Data engineering focuses on <em>building robust data pipelines</em> and architectures that support the <em>efficient and scalable handling</em> of large datasets.</p>
</div>
</div>
<p>The concept of “Data Cascades,” introduced by <span class="citation" data-cites="sambasivan2021everyone">Sambasivan et al. (<a href="#ref-sambasivan2021everyone" role="doc-biblioref">2021</a>)</span>, highlights the systemic failures that can arise when data quality issues are left unaddressed. Errors originating during data collection or processing stages can compound over time, creating cascading effects that lead to model failures, costly retraining, or even project termination. The failures of IBM Watson Health in 2019, where flawed training data resulted in unsafe and incorrect cancer treatment recommendations <span class="citation" data-cites="strickland2019ibm">(<a href="#ref-strickland2019ibm" role="doc-biblioref">Strickland 2019</a>)</span>, show the real-world consequences of neglecting data quality and its associated engineering requirements.</p>
<div class="no-row-height column-margin column-container"><div id="ref-strickland2019ibm" class="csl-entry" role="listitem">
Strickland, Eliza. 2019. <span>“IBM Watson, Heal Thyself: How IBM Overpromised and Underdelivered on AI Health Care.”</span> <em>IEEE Spectrum</em> 56 (4): 24–31. <a href="https://doi.org/10.1109/mspec.2019.8678513">https://doi.org/10.1109/mspec.2019.8678513</a>.
</div></div><p>It is therefore unsurprising that data scientists spend the majority of their time, up to 60% as shown in <a href="#fig-ds-time" class="quarto-xref">Figure&nbsp;1</a>, is spent on cleaning and organizing data. This statistic highlights the critical need to prioritize data-related challenges early in the pipeline to avoid downstream issues and ensure the effectiveness of machine learning systems.</p>
<div id="fig-ds-time" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-env="figure" data-fig-pos="htb">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-ds-time-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="bfa17ab7428a59073217342626b3309e90c451c4.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="Figure&nbsp;1: Data Scientist Time Allocation: Data preparation consumes a majority of data science effort—up to 60%—underscoring the critical need for robust data engineering practices to prevent downstream model failures and ensure project success. Prioritizing data quality and pipeline development yields greater returns than solely focusing on complex model architectures. Source: Various industry reports."><img src="data_engineering_files/mediabag/bfa17ab7428a59073217342626b3309e90c451c4.svg" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-ds-time-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: <strong>Data Scientist Time Allocation</strong>: Data preparation consumes a majority of data science effort—up to 60%—underscoring the critical need for robust data engineering practices to prevent downstream model failures and ensure project success. Prioritizing data quality and pipeline development yields greater returns than solely focusing on complex model architectures. Source: Various industry reports.
</figcaption>
</figure>
</div>
<p>Data engineering encompasses multiple critical stages in machine learning systems, from initial data collection through processing and storage. The discussion begins with the identification and sourcing of data, exploring diverse origins such as pre-existing datasets, web scraping, crowdsourcing, and synthetic data generation. Special attention is given to the complexities of integrating heterogeneous sources, validating incoming data, and handling errors during ingestion.</p>
<p>Next, the exploration covers the transformation of raw data into machine learning-ready formats. This process involves cleaning, normalizing, and extracting features, tasks that are critical to optimizing model learning and ensuring robust performance. The challenges of scale and computational efficiency are also discussed, as they are particularly important for systems that operate on vast and complex datasets.</p>
<p>Beyond data processing, the text addresses the intricacies of data labeling, a crucial step for supervised learning systems. Effective labeling requires sound annotation methodologies and advanced techniques such as AI-assisted annotation to ensure the accuracy and consistency of labeled data. Challenges such as bias and ambiguity in labeling are explored, with examples illustrating their potential impact on downstream tasks.</p>
<p>The discussion also examines the storage and organization of data, a vital aspect of supporting machine learning pipelines across their lifecycle. Topics such as storage system design, feature stores, caching strategies, and access patterns are discussed, with a focus on ensuring scalability and efficiency. Governance is highlighted as a key component of data storage and management, emphasizing the importance of compliance with privacy regulations, ethical considerations, and the use of documentation frameworks to maintain transparency and accountability.</p>
<p>This chapter provides an exploration of data engineering practices necessary for building and maintaining effective machine learning systems. The end goal is to emphasize the often-overlooked importance of data in enabling the success of machine learning applications.</p>
</section>
<section id="sec-data-engineering-problem-definition-1064" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="sec-data-engineering-problem-definition-1064">Problem Definition</h2>
<p>As discussed in the overview, <span class="citation" data-cites="sambasivan2021everyone">Sambasivan et al. (<a href="#ref-sambasivan2021everyone" role="doc-biblioref">2021</a>)</span> observes that neglecting the fundamental importance of data quality gives rise to “Data Cascades” — events where lapses in data quality compound, leading to negative downstream consequences such as flawed predictions, project terminations, and even potential harm to communities. Despite many ML professionals recognizing the importance of data, numerous practitioners report facing these cascades.</p>
<div class="no-row-height column-margin column-container"></div><p><a href="#fig-cascades" class="quarto-xref">Figure&nbsp;2</a> illustrates these potential data pitfalls at every stage and how they influence the entire process down the line. The influence of data collection errors is especially pronounced. As illustrated in the figure, any lapses in this initial stage will become apparent at later stages (in model evaluation and deployment) and might lead to costly consequences, such as abandoning the entire model and restarting anew. Therefore, investing in data engineering techniques from the onset will help us detect errors early, mitigating these cascading effects.</p>
<div id="fig-cascades" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full" data-fig-env="figure" data-fig-pos="htb">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-cascades-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="42e7fff97c3a3c84a75ca00f3e1adc11fddbd994.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-2" title="Figure&nbsp;2: Data Quality Cascades: Errors introduced early in the machine learning workflow amplify across subsequent stages, increasing costs and potentially leading to flawed predictions or harmful outcomes. Recognizing these cascades motivates proactive investment in data engineering and quality control to mitigate risks and ensure reliable system performance. Source: [@sambasivan2021everyone]."><img src="data_engineering_files/mediabag/42e7fff97c3a3c84a75ca00f3e1adc11fddbd994.svg" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-cascades-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: <strong>Data Quality Cascades</strong>: Errors introduced early in the machine learning workflow amplify across subsequent stages, increasing costs and potentially leading to flawed predictions or harmful outcomes. Recognizing these cascades motivates proactive investment in data engineering and quality control to mitigate risks and ensure reliable system performance. Source: <span class="citation" data-cites="sambasivan2021everyone">(<a href="#ref-sambasivan2021everyone" role="doc-biblioref">Sambasivan et al. 2021</a>)</span>.
</figcaption>
<div class="no-row-height column-margin column-container"><div id="ref-sambasivan2021everyone" class="csl-entry" role="listitem">
Sambasivan, Nithya, Shivani Kapania, Hannah Highfill, Diana Akrong, Praveen Paritosh, and Lora M Aroyo. 2021. <span>“<span>‘Everyone Wants to Do the Model Work, Not the Data Work’</span>: Data Cascades in High-Stakes AI.”</span> In <em>Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems</em>, 1–15. ACM. <a href="https://doi.org/10.1145/3411764.3445518">https://doi.org/10.1145/3411764.3445518</a>.
</div></div></figure>
</div>
<p>This emphasis on data quality and proper problem definition is fundamental across all types of ML systems. As <span class="citation" data-cites="sculley2015hidden">Sculley et al. (<a href="#ref-sculley2015hidden" role="doc-biblioref">2021</a>)</span> emphasize, it is important to distinguish ML-specific problem framing from the broader context of general software development. Whether developing recommendation engines processing millions of user interactions, computer vision systems analyzing medical images, or natural language models handling diverse text data, each system brings unique challenges that must be carefully considered from the outset. Production ML systems are particularly sensitive to data quality issues, as they must handle continuous data streams, maintain consistent processing pipelines, and adapt to evolving patterns while maintaining performance standards.</p>
<div class="no-row-height column-margin column-container"><div id="ref-sculley2015hidden" class="csl-entry" role="listitem">
Sculley, David, Gary Holt, Daniel Golovin, Eugene Davydov, Todd Phillips, Dietmar Ebner, Vinay Chaudhary, Michael Young, Jean-Francois Crespo, and Dan Dennison. 2021. <span>“Technical Debt in Machine Learning Systems.”</span> In <em>Technical Debt in Practice</em>, 177–92. The MIT Press. <a href="https://doi.org/10.7551/mitpress/12440.003.0011">https://doi.org/10.7551/mitpress/12440.003.0011</a>.
</div></div><p>A solid project foundation is essential for setting the trajectory and ensuring the eventual success of any initiative. At the heart of this foundation lies the crucial first step: identifying a clear problem to solve. This could involve challenges like developing a recommendation system that effectively handles cold-start scenarios, or creating a classification model that maintains consistent accuracy across diverse population segments.</p>
<p>As we will explore later in this chapter, establishing clear objectives provides a unified direction that guides the entire project. These objectives might include creating representative datasets that account for various real-world scenarios. Equally important is defining specific benchmarks, such as prediction accuracy and system latency, which offer measurable outcomes to gauge progress and success.</p>
<p>Throughout this process, engaging with stakeholders, including end-users and business leaders, provides invaluable insights that ensure the project remains aligned with real-world needs and expectations.</p>
<p>In particular, a cardinal sin in ML is to begin collecting data (or augmenting an existing dataset) without clearly specifying the underlying problem definition to guide the data collection. We identify the key steps that should precede any data collection effort here:</p>
<ol type="1">
<li>Identify and clearly state the problem definition</li>
<li>Set clear objectives to meet</li>
<li>Establish success benchmarks</li>
<li>Understand end-user engagement/use</li>
<li>Understand the constraints and limitations of deployment</li>
<li>Perform data collection.</li>
<li>Iterate and refine.</li>
</ol>
<section id="sec-data-engineering-keyword-spotting-example-8e41" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-data-engineering-keyword-spotting-example-8e41">Keyword Spotting Example</h3>
<p>Keyword Spotting (KWS) is an excellent example to illustrate all of the general steps in action. This technology is critical for voice-enabled interfaces on endpoint devices such as smartphones. Typically functioning as lightweight wake-word engines, KWS systems are constantly active, listening for a specific phrase to trigger further actions.</p>
<p>As shown in <a href="#fig-keywords" class="quarto-xref">Figure&nbsp;3</a>, when we say “OK, Google” or “Alexa,” this initiates a process on a microcontroller embedded within the device.</p>
<div id="fig-keywords" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-keywords-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="images/png/data_engineering_kws.png" class="lightbox" data-gallery="quarto-lightbox-gallery-3" title="Figure&nbsp;3: Keyword Spotting System: A typical deployment of keyword spotting (KWS) technology in a voice-activated device, where a constantly-listening system detects a wake word to initiate further processing. this example demonstrates how KWS serves as a lightweight, always-on front-end for more complex voice interfaces."><img src="images/png/data_engineering_kws.png" class="img-fluid figure-img" style="width:55.0%"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-keywords-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: <strong>Keyword Spotting System</strong>: A typical deployment of keyword spotting (KWS) technology in a voice-activated device, where a constantly-listening system detects a wake word to initiate further processing. this example demonstrates how KWS serves as a lightweight, always-on front-end for more complex voice interfaces.
</figcaption>
</figure>
</div>
<p>Building a reliable KWS model is a complex task. It demands a deep understanding of the deployment scenario, encompassing where and how these devices will operate. For instance, a KWS model’s effectiveness is not just about recognizing a word; it’s about discerning it among various accents and background noises, whether in a bustling cafe or amid the blaring sound of a television in a living room or a kitchen where these devices are commonly found. It’s about ensuring that a whispered “Alexa” in the dead of night or a shouted “OK Google” in a noisy marketplace are recognized with equal precision.</p>
<p>Moreover, many current KWS voice assistants support a limited number of languages, leaving a substantial portion of the world’s linguistic diversity unrepresented. This limitation is partly due to the difficulty in gathering and monetizing data for languages spoken by smaller populations. In the long-tail distribution of languages, most languages have limited or zero speech training data available, making the development of voice assistants challenging.</p>
<p>Keyword spotting models can run on low-power, low-price microcontrollers, so theoretically voice interfaces could be expanded to a huge gamut of devices worldwide, beyond smartphones and home assistants. But the level of accuracy and robustness that end-users expect hinges on the availability and quality of speech data, and the ability to label the data correctly. Developing a keyword-spotting model for an arbitrary word or phrase in an arbitrary language begins with clearly understanding the problem statement or definition. Using KWS as an example, we can break down each of the steps as follows:</p>
<ol type="1">
<li><p><strong>Identifying the Problem</strong>: KWS detects specific keywords amidst ambient sounds and other spoken words. The primary problem is to design a system that can recognize these keywords with high accuracy, low latency, and minimal false positives or negatives, especially when deployed on devices with limited computational resources. A well-specified problem definition for developing a new KWS model should identify the desired keywords along with the envisioned application and deployment scenario.</p></li>
<li><p><strong>Setting Clear Objectives</strong>: The objectives for a KWS system might include:</p>
<ul>
<li>Achieving a specific accuracy rate (e.g., 98% accuracy in keyword detection).</li>
<li>Ensuring low latency (e.g., keyword detection and response within 200 milliseconds).</li>
<li>Minimizing power consumption to extend battery life on embedded devices.</li>
<li>Ensuring the model’s size is optimized for the available memory on the device.</li>
</ul></li>
<li><p><strong>Benchmarks for Success</strong>: Establish clear metrics to measure the success of the KWS system. This could include:</p>
<ul>
<li><em>True Positive Rate:</em> The percentage of correctly identified keywords relative to all spoken keywords.</li>
<li><em>False Positive Rate:</em> The percentage of non-keywords (including silence, background noise, and out-of-vocabulary words) incorrectly identified as keywords.</li>
<li><em>Detection/Error Tradeoff</em> These curves evaluate KWS on streaming audio representative of a real-world deployment scenario, by comparing the number of false accepts per hour (the number of false positives over the total duration of the evaluation audio) against the false rejection rate (the number of missed keywords relative to the number of spoken keywords in the evaluation audio). <span class="citation" data-cites="nayak2022improving">Nayak et al. (<a href="#ref-nayak2022improving" role="doc-biblioref">2022</a>)</span> provides one example of this.</li>
<li><em>Response Time:</em> The time taken from keyword utterance to system response.</li>
<li><em>Power Consumption:</em> Average power used during keyword detection.</li>
</ul></li>
<li><p><strong>Stakeholder Engagement and Understanding</strong>: Engage with stakeholders, which include device manufacturers, hardware and software developers, and end-users. Understand their needs, capabilities, and constraints. For instance:</p>
<ul>
<li>Device manufacturers might prioritize low power consumption.</li>
<li>Software developers might emphasize ease of integration.</li>
<li>End-users would prioritize accuracy and responsiveness.</li>
</ul></li>
<li><p><strong>Understanding the Constraints and Limitations of Embedded Systems</strong>: Embedded devices come with their own set of challenges:</p>
<ul>
<li><em>Memory Limitations:</em> KWS models must be lightweight to fit within the memory constraints of embedded devices. Typically, KWS models need to be as small as 16 KB to fit in the always-on island<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> of the SoC. Moreover, this is just the model size. Additional application code for preprocessing may also need to fit within the memory constraints.</li>
<li>Processing Power: The computational capabilities of embedded devices are limited (a few hundred MHz of clock speed), so the KWS model must be optimized for efficiency.</li>
<li><em>Power Consumption:</em> Since many embedded devices are battery-powered, the KWS system must be power-efficient.</li>
<li><em>Environmental Challenges:</em> Devices might be deployed in various environments, from quiet bedrooms to noisy industrial settings. The KWS system must be robust enough to function effectively across these scenarios.</li>
</ul></li>
<li><p><strong>Data Collection and Analysis</strong>: For a KWS system, the quality and diversity of data are paramount. Considerations might include:</p>
<ul>
<li><em>Demographics:</em> Collect data from speakers with various accents across age and gender to ensure wide-ranging recognition support.</li>
<li><em>Keyword Variations:</em> People might pronounce keywords differently or express slight variations in the wake word itself. Ensure the dataset captures these nuances.</li>
<li><em>Background Noises:</em> Include or augment data samples with different ambient noises to train the model for real-world scenarios.</li>
</ul></li>
<li><p><strong>Iterative Feedback and Refinement</strong>: Once a prototype KWS system is developed, it is important to do the following to ensure that the system remains aligned with the defined problem and objectives as the deployment scenarios change over time and as use-cases evolve.</p>
<ul>
<li>Test it in real-world scenarios</li>
<li>Gather feedback - are some users or deployment scenarios encountering underperformance relative to others?</li>
<li>Iteratively refine the dataset and model</li>
</ul></li>
</ol>
<div class="no-row-height column-margin column-container"><div id="ref-nayak2022improving" class="csl-entry" role="listitem">
Nayak, Prateeth, Takuya Higuchi, Anmol Gupta, Shivesh Ranjan, Stephen Shum, Siddharth Sigtia, Erik Marchi, et al. 2022. <span>“Improving Voice Trigger Detection with Metric Learning.”</span> <em>arXiv Preprint arXiv:2204.02455</em>, April. <a href="http://arxiv.org/abs/2204.02455v2">http://arxiv.org/abs/2204.02455v2</a>.
</div><div id="fn1"><p><sup>1</sup>&nbsp;<strong>Always-on Island</strong>: A specialized, low-power subsystem within an SoC that continuously monitors sensors and manages wake-up functions. It enables efficient power management by keeping only essential components active while allowing rapid system wake-up when needed.</p></div></div><p>The KWS example illustrates the broader principles of problem definition, showing how initial decisions about data requirements ripple throughout a project’s lifecycle. By carefully considering each aspect, from core problem identification, through performance benchmarks, to deployment constraints, teams can build a strong foundation for their ML systems. The methodical problem definition process provides a framework applicable across the ML spectrum. Whether developing computer vision systems for medical diagnostics, recommendation engines processing millions of user interactions, or natural language models analyzing diverse text corpora, this structured approach helps teams anticipate and plan for their data needs.</p>
<p>This brings us to data pipelines, the foundational infrastructure that transforms raw data into ML-ready formats, while maintaining quality and reliability throughout the process. These pipelines implement our carefully defined requirements in production systems, handling everything from initial data ingestion to final feature generation.</p>
<div id="quiz-question-sec-data-engineering-problem-definition-1064" class="callout callout-quiz-question">
<details class="callout-quiz-question fbx-default closebutton"><summary><strong>Self-Check: Question 1.1</strong></summary><div>
<ol type="1">
<li><p>Why is it crucial to define the problem clearly before beginning data collection in ML projects?</p>
<ol type="a">
<li>To ensure data collection aligns with project objectives</li>
<li>To minimize the cost of data storage</li>
<li>To avoid the need for model evaluation</li>
<li>To increase the speed of model training</li>
</ol></li>
<li><p>True or False: Data cascades refer to the positive effects of high-quality data on ML model performance.</p></li>
<li><p>Explain how the concept of ‘data cascades’ influences the design and implementation of data pipelines in ML systems.</p></li>
<li><p>Arrange the following steps in the correct order for defining a problem in ML projects: [Set clear objectives, Identify and clearly state the problem definition, Perform data collection, Establish success benchmarks, Understand end-user engagement/use].</p></li>
</ol>
<p><a href="#quiz-answer-sec-data-engineering-problem-definition-1064" class="question-label">See Answers →</a></p>
</div></details>
</div>
</section>
</section>
<section id="sec-data-engineering-pipeline-basics-053a" class="level2">
<h2 class="anchored" data-anchor-id="sec-data-engineering-pipeline-basics-053a">Pipeline Basics</h2>
<p>Modern machine learning systems depend on data pipelines to process massive amounts of data efficiently and reliably. For instance, recommendation systems at companies like Netflix process billions of user interactions daily, while autonomous vehicle systems must handle terabytes of sensor data in real-time. These pipelines serve as the backbone of ML systems, acting as the infrastructure through which raw data transforms into ML-ready training data.</p>
<p>These data pipelines are not simple linear paths but rather complex systems. They must manage data acquisition, transformation, storage, and delivery while ensuring data quality and system reliability. The design of these pipelines fundamentally shapes what is possible with an ML system.</p>
<div id="fig-pipeline-flow" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-env="figure" data-fig-pos="htb">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-pipeline-flow-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="4213eeaeb926472ac24f617144bb4a8344e1ec2b.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-4" title="Figure&nbsp;4: Data Pipeline Architecture: Modular pipelines ingest, process, and deliver data for machine learning tasks, enabling independent scaling of components and improved data quality control. Distinct stages – ingestion, storage, and preparation – transform raw data into a format suitable for model training and validation, forming the foundation of reliable ML systems."><img src="data_engineering_files/mediabag/4213eeaeb926472ac24f617144bb4a8344e1ec2b.svg" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-pipeline-flow-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4: <strong>Data Pipeline Architecture</strong>: Modular pipelines ingest, process, and deliver data for machine learning tasks, enabling independent scaling of components and improved data quality control. Distinct stages – ingestion, storage, and preparation – transform raw data into a format suitable for model training and validation, forming the foundation of reliable ML systems.
</figcaption>
</figure>
</div>
<p>ML data pipelines consist of several distinct layers: data sources, ingestion, processing, labeling, storage, and eventually ML training (<a href="#fig-pipeline-flow" class="quarto-xref">Figure&nbsp;4</a>). Each layer plays a specific role in the data preparation workflow. The interactions between these layers are crucial to the system’s overall effectiveness. The flow from raw data sources to ML training demonstrates the importance of maintaining data quality and meeting system requirements throughout the pipeline.</p>
<div id="quiz-question-sec-data-engineering-pipeline-basics-053a" class="callout callout-quiz-question">
<details class="callout-quiz-question fbx-default closebutton"><summary><strong>Self-Check: Question 1.2</strong></summary><div>
<ol type="1">
<li><p>Which of the following components is responsible for transforming raw data into a format suitable for ML training in a data pipeline?</p>
<ol type="a">
<li>Data Ingestion</li>
<li>Processing Layer</li>
<li>Storage Layer</li>
<li>Data Sources</li>
</ol></li>
<li><p>True or False: In a data pipeline, the storage layer is only used for storing raw data before it is processed.</p></li>
<li><p>Explain why data quality checks are critical in the processing layer of a data pipeline.</p></li>
<li><p>Arrange the following components of a data pipeline in the correct order of data flow: [Processing Layer, Data Sources, Storage Layer, Data Ingestion].</p></li>
</ol>
<p><a href="#quiz-answer-sec-data-engineering-pipeline-basics-053a" class="question-label">See Answers →</a></p>
</div></details>
</div>
</section>
<section id="sec-data-engineering-data-sources-50f8" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="sec-data-engineering-data-sources-50f8">Data Sources</h2>
<p>The first stage of the pipeline architecture sourcing appropriate data to meet the training needs. The quality and diversity of this data will fundamentally determine our ML system’s learning and prediction capabilities and limitations. ML systems can obtain their training data through several different approaches, each with their own advantages and challenges. Let’s examine each of these approaches in detail.</p>
<section id="sec-data-engineering-existing-datasets-7a14" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-data-engineering-existing-datasets-7a14">Existing Datasets</h3>
<p>Platforms like <a href="https://www.kaggle.com/">Kaggle</a> and <a href="https://archive.ics.uci.edu/">UCI Machine Learning Repository</a> provide ML practitioners with ready-to-use datasets that can jumpstart system development. These pre-existing datasets are particularly valuable when building ML systems as they offer immediate access to cleaned, formatted data with established benchmarks. One of their primary advantages is cost efficiency, as creating datasets from scratch requires significant time and resources, especially when building production ML systems that need large amounts of high-quality training data.</p>
<p>Many of these datasets, such as <a href="https://www.image-net.org/">ImageNet</a>,<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a> have become standard benchmarks in the machine learning community, enabling consistent performance comparisons across different models and architectures. For ML system developers, this standardization provides clear metrics for evaluating model improvements and system performance. The immediate availability of these datasets allows teams to begin experimentation and prototyping without delays in data collection and preprocessing.</p>
<div class="no-row-height column-margin column-container"><div id="fn2"><p><sup>2</sup>&nbsp;It is essential to be aware of the several limitations of these benchmark datasets that might not be immediately clear. For example, models trained on these datasets may be overly optimized to specifics in the dataset and will begin to overfit to these characteristics. Many benchmark datasets are not updated overtime which may make them outdated and biased towards a different time period.</p></div><div id="ref-gebru2018datasheets" class="csl-entry" role="listitem">
Gebru, Timnit, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman Vaughan, Hanna Wallach, Hal Daumé III, and Kate Crawford. 2021. <span>“Datasheets for Datasets.”</span> <em>Communications of the ACM</em> 64 (12): 86–92. <a href="https://doi.org/10.1145/3458723">https://doi.org/10.1145/3458723</a>.
</div><div id="fn3"><p><sup>3</sup>&nbsp;Biases can infiltrate all stages of the ML workflow from data collection and feature engineering through model training and deployment. Each stage presents opportunities for bias to be introduced or amplified. The impacts of bias and approaches for mitigation are covered in depth in later chapters.</p></div></div><p>However, ML practitioners must carefully consider the quality assurance aspects of pre-existing datasets. For instance, the ImageNet dataset was found to have label errors on 6.4% of the validation set <span class="citation" data-cites="northcutt2021pervasive">(<a href="#ref-northcutt2021pervasive" role="doc-biblioref">Northcutt, Athalye, and Mueller 2021</a>)</span>. While popular datasets benefit from community scrutiny that helps identify and correct errors and biases, most datasets remain “untended gardens” where quality issues can significantly impact downstream system performance if not properly addressed. Moreover, as <span class="citation" data-cites="gebru2018datasheets">(<a href="#ref-gebru2018datasheets" role="doc-biblioref">Gebru et al. 2021</a>)</span> highlighted in her paper, simply providing a dataset without documentation can lead to misuse and misinterpretation, potentially amplifying biases present in the data.<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a></p>
<p>Supporting documentation accompanying existing datasets is invaluable, yet is often only present in widely-used datasets. Good documentation provides insights into the data collection process and variable definitions and sometimes even offers baseline model performances. This information not only aids understanding but also promotes reproducibility in research, a cornerstone of scientific integrity; currently, there is a crisis around improving reproducibility in machine learning systems <span class="citation" data-cites="pineau2021improving">(<a href="#ref-pineau2021improving" role="doc-biblioref">Pineau et al. 2021</a>)</span>. When other researchers have access to the same data, they can validate findings, test new hypotheses, or apply different methodologies, thus allowing us to build on each other’s work more rapidly.</p>
<div class="no-row-height column-margin column-container"><div id="ref-pineau2021improving" class="csl-entry" role="listitem">
Pineau, Joelle, Philippe Vincent-Lamarre, Koustuv Sinha, Vincent Larivière, Alina Beygelzimer, Florence d’Alché-Buc, Emily Fox, and Hugo Larochelle. 2021. <span>“Improving Reproducibility in Machine Learning Research (a Report from the Neurips 2019 Reproducibility Program).”</span> <em>Journal of Machine Learning Research</em> 22 (164): 1–20.
</div><div id="ref-beyer2020we" class="csl-entry" role="listitem">
Beyer, Lucas, Olivier J. Hénaff, Alexander Kolesnikov, Xiaohua Zhai, and Aäron van den Oord. 2020. <span>“Are We Done with ImageNet?”</span> <em>arXiv Preprint arXiv:2006.07159</em>, June. <a href="http://arxiv.org/abs/2006.07159v1">http://arxiv.org/abs/2006.07159v1</a>.
</div></div><p>While existing datasets are invaluable resources, it’s essential to understand the context in which the data was collected. Researchers should be wary of potential overfitting when using popular datasets such as ImageNet <span class="citation" data-cites="beyer2020we">(<a href="#ref-beyer2020we" role="doc-biblioref">Beyer et al. 2020</a>)</span>, leading to inflated performance metrics. Sometimes, these <a href="https://venturebeat.com/uncategorized/3-big-problems-with-datasets-in-ai-and-machine-learning/">datasets do not reflect the real-world data</a>.</p>
<p>A key consideration for ML systems is how well pre-existing datasets reflect real-world deployment conditions. Relying on standard datasets can create a concerning disconnect between training and production environments. This misalignment becomes particularly problematic when multiple ML systems are trained on the same datasets (<a href="#fig-misalignment" class="quarto-xref">Figure&nbsp;5</a>), potentially propagating biases and limitations throughout an entire ecosystem of deployed models.</p>
<div id="fig-misalignment" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-env="figure" data-fig-pos="htb">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-misalignment-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="468ba33927b8b88f56facc3c9e5a72db5d0773f0.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-5" title="Figure&nbsp;5: Dataset Convergence: Shared datasets can mask limitations and propagate biases across multiple machine learning systems, potentially leading to overoptimistic performance evaluations and reduced generalization to unseen data. Reliance on common datasets creates a false sense of progress within an ecosystem of models, hindering the development of robust and reliable AI applications."><img src="data_engineering_files/mediabag/468ba33927b8b88f56facc3c9e5a72db5d0773f0.svg" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-misalignment-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5: <strong>Dataset Convergence</strong>: Shared datasets can mask limitations and propagate biases across multiple machine learning systems, potentially leading to overoptimistic performance evaluations and reduced generalization to unseen data. Reliance on common datasets creates a false sense of progress within an ecosystem of models, hindering the development of robust and reliable AI applications.
</figcaption>
</figure>
</div>
</section>
<section id="sec-data-engineering-web-scraping-22e8" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-data-engineering-web-scraping-22e8">Web Scraping</h3>
<p>When building ML systems, particularly in domains where pre-existing datasets are insufficient, web scraping offers a powerful approach to gathering training data at scale. This automated technique for extracting data from websites has become a powerful tool in modern ML system development. It enables teams to build custom datasets tailored to their specific needs.</p>
<p>Web scraping has proven particularly valuable for building large-scale ML systems when human-labeled data is scarce. Consider computer vision systems: major datasets like <a href="https://www.image-net.org/">ImageNet</a> and <a href="https://storage.googleapis.com/openimages/web/index.html">OpenImages</a> were built through systematic web scraping, fundamentally advancing the field of computer vision. In production environments, companies regularly scrape e-commerce sites to gather product images for recognition systems or social media platforms for computer vision applications. Stanford’s <a href="https://people.csail.mit.edu/torralba/publications/labelmeApplications.pdf">LabelMe</a> project demonstrated this approach’s potential early on, scraping Flickr to create a diverse dataset of over 63,000 annotated images.</p>
<p>The impact of web scraping extends well beyond computer vision systems. In natural language processing, web-scraped data has enabled the development of increasingly sophisticated ML systems. Large language models, such as ChatGPT and Claude, rely on vast amounts of text scraped from the public internet and media to learn language patterns and generate responses <span class="citation" data-cites="groeneveld2024olmo">(<a href="#ref-groeneveld2024olmo" role="doc-biblioref">Groeneveld et al. 2024</a>)</span>. Similarly, specialized ML systems like GitHub’s Copilot demonstrate how targeted web scraping, in this case of code repositories, can create powerful domain-specific assistants <span class="citation" data-cites="chen2021evaluating">(<a href="#ref-chen2021evaluating" role="doc-biblioref">Chen et al. 2021</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-groeneveld2024olmo" class="csl-entry" role="listitem">
Groeneveld, Dirk, Iz Beltagy, Pete Walsh, Akshita Bhagia, Rodney Kinney, Oyvind Tafjord, Ananya Harsh Jha, et al. 2024. <span>“OLMo: Accelerating the Science of Language Models.”</span> <em>arXiv Preprint arXiv:2402.00838</em>, February. <a href="http://arxiv.org/abs/2402.00838v4">http://arxiv.org/abs/2402.00838v4</a>.
</div><div id="ref-chen2021evaluating" class="csl-entry" role="listitem">
Chen, Mark, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, et al. 2021. <span>“Evaluating Large Language Models Trained on Code.”</span> <em>arXiv Preprint arXiv:2107.03374</em>, July. <a href="http://arxiv.org/abs/2107.03374v2">http://arxiv.org/abs/2107.03374v2</a>.
</div></div><p>Production ML systems often require continuous data collection to maintain relevance and performance. Web scraping facilitates this by gathering structured data like stock prices, weather patterns, or product information for analytical applications. However, this continuous collection introduces unique challenges for ML systems. Data consistency becomes crucial, as variations in website structure or content formatting can disrupt the data pipeline and affect model performance. Proper data management through databases or warehouses becomes essential not just for storage, but for maintaining data quality and enabling model updates.</p>
<p>Despite its utility, web scraping presents several challenges that ML system developers must carefully consider. Legal and ethical constraints can limit data collection, as not all websites permit scraping, and violating these restrictions can have <a href="https://hls.harvard.edu/today/does-chatgpt-violate-new-york-times-copyrights/">serious consequences</a>. When building ML systems with scraped data, teams must carefully document data sources and ensure compliance with terms of service and copyright laws. Privacy considerations become particularly critical when dealing with user-generated content, often requiring robust anonymization procedures.</p>
<p>Technical limitations also affect the reliability of web-scraped training data. Rate limiting by websites can slow data collection, while the dynamic nature of web content can introduce inconsistencies that impact model training. As shown in <a href="#fig-traffic-light" class="quarto-xref">Figure&nbsp;6</a>, web scraping can yield unexpected or irrelevant data, for example, historical images appearing in contemporary image searches, that can pollute training datasets and degrade model performance. These issues highlight the importance of thorough data validation and cleaning processes in ML pipelines built on web-scraped data.</p>
<div id="fig-traffic-light" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-traffic-light-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="images/jpg/1914_traffic.jpeg" class="lightbox" data-gallery="quarto-lightbox-gallery-6" title="Figure&nbsp;6: Data Source Noise: Web scraping introduces irrelevant or outdated data into training sets, requiring robust data validation and cleaning to maintain model performance and prevent spurious correlations. Historical images appearing in contemporary searches exemplify this noise, underscoring the need for careful filtering and quality control in web-sourced datasets. Source: Vox."><img src="images/jpg/1914_traffic.jpeg" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-traffic-light-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6: <strong>Data Source Noise</strong>: Web scraping introduces irrelevant or outdated data into training sets, requiring robust data validation and cleaning to maintain model performance and prevent spurious correlations. Historical images appearing in contemporary searches exemplify this noise, underscoring the need for careful filtering and quality control in web-sourced datasets. Source: Vox.
</figcaption>
</figure>
</div>
</section>
<section id="sec-data-engineering-crowdsourcing-9d32" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-data-engineering-crowdsourcing-9d32">Crowdsourcing</h3>
<p>Crowdsourcing is a collaborative approach to data collection, leveraging the collective efforts of distributed individuals via the internet to tackle tasks requiring human judgment<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a>. By engaging a global pool of contributors, this method accelerates the creation of high-quality, labeled datasets for machine learning systems, especially in scenarios where pre-existing data is scarce or domain-specific. Platforms like <a href="https://www.mturk.com/">Amazon Mechanical Turk</a> exemplify how crowdsourcing facilitates this process by distributing annotation tasks to a global workforce. This enables the rapid collection of labels for complex tasks such as sentiment analysis, image recognition, and speech transcription, significantly expediting the data preparation phase.</p>
<div class="no-row-height column-margin column-container"><div id="fn4"><p><sup>4</sup>&nbsp;Crowdsourcing became popular for data labelling because it is cost effective and offers wide coverage across different demographics and cultures. However, this cost advantage often comes at the expense of the inadequate worker compensation, unpaid training time, and unstable income. Organizations using crowdsourcing platforms should prioritize ethical considerations related to the financial and emotional wellbeing of the workers.</p></div></div><p>One of the most impactful examples of crowdsourcing in machine learning is the creation of the <a href="https://image-net.org/">ImageNet dataset</a>. ImageNet, which revolutionized computer vision, was built by distributing image labeling tasks to contributors via Amazon Mechanical Turk. The contributors categorized millions of images into thousands of classes, enabling researchers to train and benchmark models for a wide variety of visual recognition tasks.</p>
<p>The dataset’s availability spurred advancements in deep learning, including the breakthrough AlexNet model in 2012, which demonstrated how large-scale, crowdsourced datasets could drive innovation. ImageNet’s success highlights how leveraging a diverse group of contributors for annotation can enable machine learning systems to achieve unprecedented performance.</p>
<p>Another example of crowdsourcing’s potential is Google’s <a href="https://crowdsource.google.com/">Crowdsource</a>, a platform where volunteers contribute labeled data to improve AI systems in applications like language translation, handwriting recognition, and image understanding. By gamifying the process and engaging global participants, Google harnesses diverse datasets, particularly for underrepresented languages. This approach not only enhances the quality of AI systems but also empowers communities by enabling their contributions to influence technological development.</p>
<p>Crowdsourcing has also been instrumental in applications beyond traditional dataset annotation. For instance, the navigation app <a href="https://www.waze.com/">Waze</a> uses crowdsourced data from its users to provide real-time traffic updates, route suggestions, and incident reporting. While this involves dynamic data collection rather than static dataset labeling, it demonstrates how crowdsourcing can generate continuously updated datasets essential for applications like mobile or edge ML systems. These systems often require real-time input to maintain relevance and accuracy in changing environments.</p>
<p>One of the primary advantages of crowdsourcing is its scalability. By distributing microtasks to a large audience, projects can process enormous volumes of data quickly and cost-effectively. This scalability is particularly beneficial for machine learning systems that require extensive datasets to achieve high performance. Additionally, the diversity of contributors introduces a wide range of perspectives, cultural insights, and linguistic variations, enriching datasets and improving models’ ability to generalize across populations.</p>
<p>Flexibility is a key benefit of crowdsourcing. Tasks can be adjusted dynamically based on initial results, allowing for iterative improvements in data collection. For example, Google’s <a href="https://www.google.com/recaptcha/about/">reCAPTCHA</a> system uses crowdsourcing to verify human users while simultaneously labeling datasets for training machine learning models. Users identify objects in images, including street signs and cars, contributing to the training of autonomous systems. This clever integration demonstrates how crowdsourcing can scale seamlessly when embedded into everyday workflows.</p>
<p>Despite its advantages, crowdsourcing presents challenges that require careful management. Quality control is a major concern, as the variability in contributors’ expertise and attention can lead to inconsistent or inaccurate annotations. Providing clear instructions and training materials helps ensure participants understand the task requirements. Techniques such as embedding known test cases, leveraging consensus algorithms, or using redundant annotations can mitigate quality issues and align the process with the problem definition discussed earlier.</p>
<p>Ethical considerations are paramount in crowdsourcing, especially when datasets are built at scale using global contributors. It is essential to ensure that participants are fairly compensated for their work and that they are informed about how their contributions will be used. Additionally, privacy concerns must be addressed, particularly when dealing with sensitive or personal information. Transparent sourcing practices, clear communication with contributors, and robust auditing mechanisms are crucial for building trust and maintaining ethical standards.</p>
<p>The issue of fair compensation and ethical data sourcing was brought into sharp focus during the development of large-scale AI systems like OpenAI’s ChatGPT. Reports revealed that <a href="https://time.com/6247678/openai-chatgpt-kenya-workers/">OpenAI outsourced data annotation tasks to workers in Kenya</a>, employing them to moderate content and identify harmful or inappropriate material that the model might generate. This involved reviewing and labeling distressing content, such as graphic violence and explicit material, to train the AI in recognizing and avoiding such outputs. While this approach enabled OpenAI to improve the safety and utility of ChatGPT, significant ethical concerns arose around the working conditions, the nature of the tasks, and the compensation provided to Kenyan workers.</p>
<p>Many of the contributors were reportedly paid as little as $1.32 per hour for reviewing and labeling highly traumatic material. The emotional toll of such work, coupled with low wages, raised serious questions about the fairness and transparency of the crowdsourcing process. This controversy highlights a critical gap in ethical crowdsourcing practices. The workers, often from economically disadvantaged regions, were not adequately supported to cope with the psychological impact of their tasks. The lack of mental health resources and insufficient compensation underscored the power imbalances that can emerge when outsourcing data annotation tasks to lower-income regions.</p>
<p>The challenges highlighted by the ChatGPT, particularly the Kenya controversy, are not unique to OpenAI. Many organizations that rely on crowdsourcing for data annotation face similar issues. As machine learning systems grow more complex and require larger datasets, the demand for annotated data will continue to increase. This shows the need for industry-wide standards and best practices to ensure ethical data sourcing. This case emphasizes the importance of considering the human labor behind AI systems. While crowdsourcing offers scalability and diversity, it also brings ethical responsibilities that cannot be overlooked. Organizations must prioritize the well-being and fair treatment of contributors as they build the datasets that drive AI innovation.</p>
<p>Moreover, when dealing with specialized applications like mobile ML, edge ML, or cloud ML, additional challenges may arise. These applications often require data collected from specific environments or devices, which can be difficult to gather through general crowdsourcing platforms. For example, data for mobile applications utilizing smartphone sensors may necessitate participants with specific hardware features or software versions. Similarly, edge ML systems deployed in industrial settings may require data involving proprietary processes or secure environments, introducing privacy and accessibility challenges.</p>
<p>Hybrid approaches that combine crowdsourcing with other data collection methods can address these challenges. Organizations may engage specialized communities, partner with relevant stakeholders, or create targeted initiatives to collect domain-specific data. Additionally, synthetic data generation, as discussed in the next section, can augment real-world data when crowdsourcing falls short.</p>
</section>
<section id="sec-data-engineering-anonymization-techniques-b65d" class="level3">
<h3 class="anchored" data-anchor-id="sec-data-engineering-anonymization-techniques-b65d">Anonymization Techniques</h3>
<p>Protecting the privacy of individuals while still enabling data-driven insights is a central challenge in the modern data landscape. As organizations collect and analyze vast quantities of information, the risk of exposing sensitive details, whether inadvertently or via intentional breaches, heightens. To mitigate these risks, practitioners have developed a commonly used range of anonymization techniques. These methods transform datasets such that individual identities and sensitive attributes become difficult or nearly impossible to re-identify, while preserving, to varying extents, the overall utility of the data for analysis.</p>
<p>Masking involves altering or obfuscating sensitive values so that they cannot be directly traced back to the original data subject. For instance, digits in financial account numbers or credit card numbers can be replaced with asterisks, a fixed set of dummy characters, or hashed values to protect sensitive information during display or logging. This anonymization technique is straightforward to implement and understand while clearly protecting identifiable values from being viewed, but may struggle with protecting broader context (e.g.&nbsp;relationships between data points).</p>
<p>Generalization reduces the precision or granularity of data to decrease the likelihood of re-identification. Instead of revealing an exact date of birth or address, the data is aggregated into broader categories (e.g., age ranges, zip code prefixes). For example, a user’s exact age of 37 might be generalized to an age range of 30-39, while their exact address might be bucketed into a city level granularity. This technique clearly reduces the risk of identifying an individual by sharing data in aggregated form; however, we might consequently lose analytical prediction. Furthermore, if granularity is not chosen correctly, individuals may still be able to be identified under certain conditions.</p>
<p>Pseudonymization is the process of replacing direct identifiers (like names, Social Security numbers, or email addresses) with artificial identifiers, or “pseudonyms.” These pseudonyms must not reveal, or be easily traceable to, the original data subject. This is commonly used in health records or in any situation where datasets need personal identities removed, but maintain unique entries. This approach allow maintaining individual-level data for analysis (since records can be traced through pseudonyms), while reducing the risk of direct identification. However, if the “key” linking the pseudonym to the real identifier is compromised, re-identification becomes possible.</p>
<p><span class="math inline">\(k\)</span>-anonymity ensures that each record in a dataset is indistinguishable from at least <span class="math inline">\(𝑘−1\)</span> other records. This is achieved by suppressing or generalizing quasi-identifiers, or attributes that, in combination, could be used to re-identify an individual (e.g., zip code, age, gender). For example, if <span class="math inline">\(k=5\)</span>, every record in the dataset must share the same combination of quasi-identifiers with at least four other records. Thus, an attacker cannot pinpoint a single individual simply by looking at these attributes. This approach provides a formal privacy guarantee that helps reduce chances of individual re-identification. However, it is extremely high touch and may require a significant level of data distortion and does not protect against things like <a href="https://en.wikipedia.org/wiki/K-anonymity#Attacks">homogeneity or background knowledge attacks</a>.</p>
<p>Differential privacy (DP) adds carefully <a href="https://digitalprivacy.ieee.org/publications/topics/what-is-differential-privacy#:~:text=At%20its%20roots%2C%20differential%20privacy,a%20result%20of%20providing%20data.">calibrated “noise” or randomized data perturbations</a> to query results or datasets. The goal is to ensure that the inclusion or exclusion of any single individual’s data does not significantly affect the output, thereby concealing their presence. Introduced noise is controlled by the <span class="math inline">\(\epsilon\)</span> parameter in <span class="math inline">\(\epsilon\)</span>-Differential Privacy, balancing data utility and privacy guarantees. The clear advantages this approach provides are strong mathematical guarantees of privacy, and DP is widely used in academic and industrial settings (e.g., large-scale data analysis). However, the added noise can affect data accuracy and subsequent model performance; proper parameter tuning is crucial to ensure both privacy and usefulness.</p>
<p>In summary, effective data anonymization is a balancing act between privacy and utility. Techniques such as masking, generalization, pseudonymization, k-anonymity, and differential privacy each target different aspects of re-identification risk. By carefully selecting and combining these methods, organizations can responsibly derive value from sensitive datasets while respecting the privacy rights and expectations of the individuals represented within them.</p>
</section>
<section id="sec-data-engineering-synthetic-data-creation-b775" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-data-engineering-synthetic-data-creation-b775">Synthetic Data Creation</h3>
<p>Synthetic data generation has emerged as a powerful tool for addressing limitations in data collection, particularly in machine learning applications where real-world data is scarce, expensive, or ethically challenging to obtain. This approach involves creating artificial data using algorithms, simulations, or generative models to mimic real-world datasets. The generated data can be used to supplement or replace real-world data, expanding the possibilities for training robust and accurate machine learning systems. <a href="#fig-synthetic-data" class="quarto-xref">Figure&nbsp;7</a> illustrates the process of combining synthetic data with historical datasets to create larger, more diverse training sets.</p>
<div id="fig-synthetic-data" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-env="figure" data-fig-pos="htb">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-synthetic-data-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="9bbc195613b2889c8434fe22b8cc694ba65e2f91.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-7" title="Figure&nbsp;7: Synthetic Data Augmentation: Combining algorithmically generated data with historical datasets expands training set size and diversity, mitigating limitations caused by scarce or biased real-world data and improving model generalization. This approach enables robust machine learning system development when acquiring sufficient real-world data is impractical or unethical. Source: anylogic."><img src="data_engineering_files/mediabag/9bbc195613b2889c8434fe22b8cc694ba65e2f91.svg" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-synthetic-data-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;7: <strong>Synthetic Data Augmentation</strong>: Combining algorithmically generated data with historical datasets expands training set size and diversity, mitigating limitations caused by scarce or biased real-world data and improving model generalization. This approach enables robust machine learning system development when acquiring sufficient real-world data is impractical or unethical. Source: <a href="HTTPS://www.anylogic.com/features/artificial-intelligence/synthetic-data/">anylogic</a>.
</figcaption>
</figure>
</div>
<p>Advancements in generative modeling techniques, such as diffusion models and flow-matching algorithms<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a>, Generative Adversarial Networks (GANs)<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a>, and Variational Autoencoders (VAEs)<a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a>, have greatly enhanced the quality of synthetic data. These techniques can produce data that closely resembles real-world distributions, making it suitable for applications ranging from computer vision to natural language processing. For example, GANs have been used to generate synthetic images for object recognition tasks, creating diverse datasets that are almost indistinguishable from real-world images. Similarly, synthetic data has been leveraged to simulate speech patterns, enhancing the robustness of voice recognition systems.</p>
<div class="no-row-height column-margin column-container"><div id="fn5"><p><sup>5</sup>&nbsp;<strong>Diffusion models</strong> use noise prediction across time to simulate generation, while <strong>flow-matching</strong> algorithms minimize the displacement between source and target distributions.</p></div><div id="fn6"><p><sup>6</sup>&nbsp;<strong>Generative Adversarial Networks (GANs)</strong>: Machine learning models with a generator creating data and a discriminator assessing its realism.</p></div><div id="fn7"><p><sup>7</sup>&nbsp;<strong>Variational Autoencoders (VAEs)</strong>: Generative models that encode data into a latent space and decode it to generate new samples.</p></div></div><p>Synthetic data has become particularly valuable in domains where obtaining real-world data is either impractical or costly. The automotive industry has embraced synthetic data to train autonomous vehicle systems; there are only so many cars you can physically crash to get crash-test data that might help an ML system know how to avoid crashes in the first place. Capturing real-world scenarios, especially rare edge cases such as near-accidents or unusual road conditions, is inherently difficult. Synthetic data allows researchers to <a href="https://www.nvidia.com/en-us/use-cases/autonomous-vehicle-simulation/">simulate these scenarios in a controlled virtual environment</a>, ensuring that models are trained to handle a wide range of conditions. This approach has proven invaluable for advancing the capabilities of self-driving cars.</p>
<p>Another important application of synthetic data lies in augmenting existing datasets. Introducing variations into datasets enhances model robustness by exposing the model to diverse conditions. For instance, in speech recognition, data augmentation techniques like SpecAugment <span class="citation" data-cites="park2019specaugment">(<a href="#ref-park2019specaugment" role="doc-biblioref">Park et al. 2019</a>)</span> introduce noise, shifts, or pitch variations, enabling models to generalize better across different environments and speaker styles. This principle extends to other domains as well, where synthetic data can fill gaps in underrepresented scenarios or edge cases.</p>
<div class="no-row-height column-margin column-container"><div id="ref-park2019specaugment" class="csl-entry" role="listitem">
Park, Daniel S., William Chan, Yu Zhang, Chung-Cheng Chiu, Barret Zoph, Ekin D. Cubuk, and Quoc V. Le. 2019. <span>“SpecAugment: A Simple Data Augmentation Method for Automatic Speech Recognition.”</span> <em>arXiv Preprint arXiv:1904.08779</em>, April. <a href="http://arxiv.org/abs/1904.08779v3">http://arxiv.org/abs/1904.08779v3</a>.
</div><div id="fn8"><p><sup>8</sup>&nbsp;<strong>General Data Protection Regulation (GDPR)</strong>: A regulation in EU law on data protection and privacy in the European Union and the European Economic Area.</p></div><div id="fn9"><p><sup>9</sup>&nbsp;<strong>Health Insurance Portability and Accountability Act (HIPAA)</strong>: A US law designed to provide privacy standards to protect patients’ medical records and other health information.</p></div></div><p>In addition to expanding datasets, synthetic data addresses critical ethical and privacy concerns. Unlike real-world data, synthetic data attempts to not tie back to specific individuals or entities. This makes it especially useful in sensitive domains such as finance, healthcare, or human resources, where data confidentiality is paramount. The ability to preserve statistical properties while removing identifying information allows researchers to maintain high ethical standards without compromising the quality of their models. In healthcare, privacy regulations such as <a href="https://gdpr.eu/">GDPR</a><a href="#fn8" class="footnote-ref" id="fnref8" role="doc-noteref"><sup>8</sup></a> and <a href="https://www.hhs.gov/hipaa/for-professionals/privacy/laws-regulations/index.html">HIPAA</a><a href="#fn9" class="footnote-ref" id="fnref9" role="doc-noteref"><sup>9</sup></a> limit the sharing of sensitive patient information. Synthetic data generation enables the creation of realistic yet anonymized datasets that can be used for training diagnostic models without compromising patient privacy.</p>
<p>Poorly generated data can misrepresent underlying real-world distributions, introducing biases or inaccuracies that degrade model performance. Validating synthetic data against real-world benchmarks is essential to ensure its reliability. Additionally, models trained primarily on synthetic data must be rigorously tested in real-world scenarios to confirm their ability to generalize effectively. Another challenge is the potential amplification of biases present in the original datasets used to inform synthetic data generation. If these biases are not carefully addressed, they may be inadvertently reinforced in the resulting models. A critical consideration is maintaining proper balance between synthetic and real-world data during training - if models are overly trained on synthetic data, their outputs may become nonsensical and model performance may collapse.</p>
<p>Synthetic data has revolutionized the way machine learning systems are trained, providing flexibility, diversity, and scalability in data preparation. However, as its adoption grows, practitioners must remain vigilant about its limitations and ethical implications. By combining synthetic data with rigorous validation and thoughtful application, machine learning researchers and engineers can unlock its full potential while ensuring reliability and fairness in their systems.</p>
</section>
<section id="sec-data-engineering-continuing-kws-example-b2f4" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-data-engineering-continuing-kws-example-b2f4">Continuing the KWS Example</h3>
<p>KWS is an excellent case study of how different data collection approaches can be combined effectively. Each method we’ve discussed plays a role in building robust wake word detection systems, albeit with different trade-offs:</p>
<p>Pre-existing datasets like Google’s Speech Commands <span class="citation" data-cites="warden2018speech">(<a href="#ref-warden2018speech" role="doc-biblioref">Warden 2018</a>)</span> provide a foundation for initial development, offering carefully curated voice samples for common wake words. However, these datasets often lack diversity in accents, environments, and languages, necessitating additional data collection strategies.</p>
<div class="no-row-height column-margin column-container"><div id="ref-warden2018speech" class="csl-entry" role="listitem">
Warden, Pete. 2018. <span>“Speech Commands: A Dataset for Limited-Vocabulary Speech Recognition.”</span> <em>arXiv Preprint arXiv:1804.03209</em>, April. <a href="http://arxiv.org/abs/1804.03209v1">http://arxiv.org/abs/1804.03209v1</a>.
</div></div><p>Web scraping can supplement these baseline datasets by gathering diverse voice samples from video platforms, podcast repositories, and speech databases. This helps capture natural speech patterns and wake word variations, though careful attention must be paid to audio quality and privacy considerations when scraping voice data.</p>
<p>Crowdsourcing becomes valuable for collecting specific wake word samples across different demographics and environments. Platforms like Amazon Mechanical Turk can engage contributors to record wake words in various accents, speaking styles, and background conditions. This approach is particularly useful for gathering data for underrepresented languages or specific acoustic environments.</p>
<p>Synthetic data generation helps fill remaining gaps by creating unlimited variations of wake word utterances. Using speech synthesis <span class="citation" data-cites="werchniak2021exploring">(<a href="#ref-werchniak2021exploring" role="doc-biblioref">Werchniak et al. 2021</a>)</span> and audio augmentation techniques, developers can generate training data that captures different acoustic environments (busy streets, quiet rooms, moving vehicles), speaker characteristics (age, accent, gender), and background noise conditions.</p>
<div class="no-row-height column-margin column-container"><div id="ref-werchniak2021exploring" class="csl-entry" role="listitem">
Werchniak, Andrew, Roberto Barra Chicote, Yuriy Mishchenko, Jasha Droppo, Jeff Condal, Peng Liu, and Anish Shah. 2021. <span>“Exploring the Application of Synthetic Audio in Training Keyword Spotters.”</span> In <em>ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>, 7993–96. IEEE; IEEE. <a href="https://doi.org/10.1109/icassp39728.2021.9413448">https://doi.org/10.1109/icassp39728.2021.9413448</a>.
</div></div><p>This multi-faceted approach to data collection enables the development of KWS systems that perform robustly across diverse real-world conditions. The combination of methods helps address the unique challenges of wake word detection, from handling various accents and background noise to maintaining consistent performance across different devices and environments.</p>
<div id="quiz-question-sec-data-engineering-data-sources-50f8" class="callout callout-quiz-question">
<details class="callout-quiz-question fbx-default closebutton"><summary><strong>Self-Check: Question 1.3</strong></summary><div>
<ol type="1">
<li><p>Which of the following is a primary advantage of using existing datasets like ImageNet for ML system development?</p>
<ol type="a">
<li>They provide real-time data updates.</li>
<li>They offer cost efficiency and established benchmarks.</li>
<li>They eliminate the need for data preprocessing.</li>
<li>They ensure complete alignment with real-world deployment conditions.</li>
</ol></li>
<li><p>Explain why web scraping is a valuable method for data collection in ML systems, and identify one major challenge associated with it.</p></li>
<li><p>Crowdsourcing for data collection in ML systems can introduce a wide range of perspectives and cultural insights, enriching datasets and improving models’ ability to generalize across populations. However, a primary concern with crowdsourcing is ensuring ____.</p></li>
<li><p>True or False: Synthetic data generation can fully replace real-world data in training ML systems without any limitations.</p></li>
</ol>
<p><a href="#quiz-answer-sec-data-engineering-data-sources-50f8" class="question-label">See Answers →</a></p>
</div></details>
</div>
</section>
</section>
<section id="sec-data-engineering-data-ingestion-81f3" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="sec-data-engineering-data-ingestion-81f3">Data Ingestion</h2>
<p>The collected data must be reliably and efficiently ingested into our ML systems through well-designed data pipelines. This transformation presents several challenges that ML engineers must address.</p>
<section id="sec-data-engineering-ingestion-patterns-0f03" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-data-engineering-ingestion-patterns-0f03">Ingestion Patterns</h3>
<p>In ML systems, data ingestion typically follows two primary patterns: batch ingestion and stream ingestion. Each pattern has distinct characteristics and use cases that students should understand to design effective ML systems.</p>
<p>Batch ingestion involves collecting data in groups or batches over a specified period before processing. This method is appropriate when real-time data processing is not critical and data can be processed at scheduled intervals. It’s also useful for loading large volumes of historical data. For example, a retail company might use batch ingestion to process daily sales data overnight, updating their ML models for inventory prediction each morning <span class="citation" data-cites="akidau2015dataflow">(<a href="#ref-akidau2015dataflow" role="doc-biblioref">Akidau et al. 2015</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-akidau2015dataflow" class="csl-entry" role="listitem">
Akidau, Tyler, Robert Bradshaw, Craig Chambers, Slava Chernyak, Rafael J. Fernández-Moctezuma, Reuven Lax, Sam McVeety, et al. 2015. <span>“The Dataflow Model: A Practical Approach to Balancing Correctness, Latency, and Cost in Massive-Scale, Unbounded, Out-of-Order Data Processing.”</span> <em>Proceedings of the VLDB Endowment</em> 8 (12): 1792–1803. <a href="https://doi.org/10.14778/2824032.2824076">https://doi.org/10.14778/2824032.2824076</a>.
</div></div><p>In contrast, stream ingestion processes data in real-time as it arrives. This pattern is crucial for applications requiring immediate data processing, scenarios where data loses value quickly, and systems that need to respond to events as they occur. A financial institution, for instance, might use stream ingestion for real-time fraud detection, processing each transaction as it occurs to flag suspicious activity immediately <span class="citation" data-cites="kleppmann2017designing">(<a href="#ref-kleppmann2017designing" role="doc-biblioref">Kleppmann 2016</a>)</span>.</p>
<p>Many modern ML systems employ a hybrid approach, combining both batch and stream ingestion to handle different data velocities and use cases. This flexibility allows systems to process both historical data in batches and real-time data streams, providing a comprehensive view of the data landscape.</p>
</section>
<section id="sec-data-engineering-etl-elt-comparison-ac5b" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-data-engineering-etl-elt-comparison-ac5b">ETL and ELT Comparison</h3>
<p>When designing data ingestion pipelines for ML systems, it’s necessary to understand the differences between Extract, Transform, Load (ETL) and Extract, Load, Transform (ELT) approaches, as illustrated in <a href="#fig-etl-vs-elt" class="quarto-xref">Figure&nbsp;8</a>. These paradigms determine when data transformations occur relative to the loading phase, significantly impacting the flexibility and efficiency of your ML pipeline.</p>
<div id="fig-etl-vs-elt" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-env="figure" data-fig-pos="htb">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-etl-vs-elt-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="ecff59fc5f0797ded4b8d4637d9036cc3e85ac9f.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-8" title="Figure&nbsp;8: Data Pipeline Architectures: ETL pipelines transform data before loading it into a data warehouse, while ELT pipelines load raw data first and transform it within the warehouse, impacting system flexibility and resource allocation for machine learning workflows. Choosing between ETL and ELT depends on data volume, transformation complexity, and the capabilities of the target data storage system."><img src="data_engineering_files/mediabag/ecff59fc5f0797ded4b8d4637d9036cc3e85ac9f.svg" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-etl-vs-elt-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;8: <strong>Data Pipeline Architectures</strong>: ETL pipelines transform data <em>before</em> loading it into a data warehouse, while ELT pipelines load raw data first and transform it within the warehouse, impacting system flexibility and resource allocation for machine learning workflows. Choosing between ETL and ELT depends on data volume, transformation complexity, and the capabilities of the target data storage system.
</figcaption>
</figure>
</div>
<p>ETL is a well-established paradigm in which data is first gathered from a source, then transformed to match the target schema or model, and finally loaded into a data warehouse or other repository. This approach typically results in data being stored in a ready-to-query format, which can be advantageous for ML systems that require consistent, pre-processed data. For instance, an ML system predicting customer churn might use ETL to standardize and aggregate customer interaction data from multiple sources before loading it into a format suitable for model training <span class="citation" data-cites="inmon2005building">(<a href="#ref-inmon2005building" role="doc-biblioref">Inmon 2005</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-inmon2005building" class="csl-entry" role="listitem">
Inmon, W. H. 2005. <em>Building the Data Warehouse</em>. John Wiley Sons.
</div><div id="fn10"><p><sup>10</sup>&nbsp;<strong>Schema-on-read</strong>: A flexible approach where data structure is defined at access time, not during ingestion, enabling versatile use of raw data.</p></div></div><p>However, ETL can be less flexible when schemas or requirements change frequently, a common occurrence in evolving ML projects. This is where the ELT approach comes into play. ELT reverses the order by first loading raw data and then applying transformations as needed. This method is often seen in modern data lake or schema-on-read<a href="#fn10" class="footnote-ref" id="fnref10" role="doc-noteref"><sup>10</sup></a> environments, allowing for a more agile approach when addressing evolving analytical needs in ML systems.</p>
<p>By deferring transformations, ELT can accommodate varying uses of the same dataset, which is particularly useful in exploratory data analysis phases of ML projects or when multiple models with different data requirements are being developed simultaneously. However, it’s important to note that ELT places greater demands on storage systems and query engines, which must handle large amounts of unprocessed information.</p>
<p>In practice, many ML systems employ a hybrid approach, selecting ETL or ELT on a case-by-case basis depending on the specific requirements of each data source or ML model. For example, a system might use ETL for structured data from relational databases where schemas are well-defined and stable, while employing ELT for unstructured data like text or images where transformation requirements may evolve as the ML models are refined.</p>
</section>
<section id="sec-data-engineering-data-source-integration-f2aa" class="level3">
<h3 class="anchored" data-anchor-id="sec-data-engineering-data-source-integration-f2aa">Data Source Integration</h3>
<p>Integrating diverse data sources is a key challenge in data ingestion for ML systems. Data may come from various origins, including databases, APIs, file systems, and IoT devices. Each source may have its own data format, access protocol, and update frequency.</p>
<p>To effectively integrate these sources, ML engineers must develop robust connectors or adapters for each data source. These connectors handle the specifics of data extraction, including authentication, rate limiting, and error handling. For example, when integrating with a REST API, the connector would manage API keys, respect rate limits, and handle HTTP status codes appropriately.</p>
<p>Furthermore, source integration often involves data transformation at the ingestion point. This might include parsing JSON or XML responses, converting timestamps to a standard format, or performing basic data cleaning operations. The goal is to standardize the data format as it enters the ML pipeline, simplifying downstream processing.</p>
<p>It’s also essential to consider the reliability and availability of data sources. Some sources may experience downtime or have inconsistent data quality. Implementing retry mechanisms, data quality checks, and fallback procedures can help ensure a steady flow of reliable data into the ML system.</p>
</section>
<section id="sec-data-engineering-validation-techniques-0eff" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-data-engineering-validation-techniques-0eff">Validation Techniques</h3>
<p>Data validation is an important step in the ingestion process, ensuring that incoming data meets quality standards and conforms to expected schemas. This step helps prevent downstream issues in ML pipelines caused by data anomalies or inconsistencies.</p>
<p>At the ingestion stage, validation typically encompasses several key aspects. First, it checks for schema conformity, ensuring that incoming data adheres to the expected structure, including data types and field names. Next, it verifies data ranges and constraints, confirming that numeric fields fall within expected ranges and that categorical fields contain valid values. Completeness checks are also performed, looking for missing or null values in required fields. Additionally, consistency checks ensure that related data points are logically coherent <span class="citation" data-cites="gudivada2017data">(<a href="#ref-gudivada2017data" role="doc-biblioref">Gudivada, Rao, et al. 2017</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-gudivada2017data" class="csl-entry" role="listitem">
Gudivada, Venkat N., Dhana Rao Rao, et al. 2017. <span>“Data Quality Considerations for Big Data and Machine Learning: Going Beyond Data Cleaning and Transformations.”</span> <em>IEEE Transactions on Knowledge and Data Engineering</em>.
</div></div><p>For example, in a healthcare ML system ingesting patient data, validation might include checking that age values are positive integers, diagnosis codes are from a predefined set, and admission dates are not in the future. By implementing robust validation at the ingestion stage, ML engineers can detect and handle data quality issues early, significantly reducing the risk of training models on flawed or inconsistent data.</p>
</section>
<section id="sec-data-engineering-error-management-695b" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-data-engineering-error-management-695b">Error Management</h3>
<p>Error handling in data ingestion is essential for building resilient ML systems. Errors can occur at various points in the ingestion process, from source connection issues to data validation failures. Effective error handling strategies ensure that the ML pipeline can continue to operate even when faced with data ingestion challenges.</p>
<p>A key concept in error handling is graceful degradation. This involves designing systems to continue functioning, possibly with reduced capabilities, when faced with partial data loss or temporary source unavailability. Implementing intelligent retry logic for transient errors, such as network interruptions or temporary service outages, is another important aspect of robust error handling. Many ML systems employ the concept of dead letter queues<a href="#fn11" class="footnote-ref" id="fnref11" role="doc-noteref"><sup>11</sup></a>, using separate storage for data that fails processing. This allows for later analysis and potential reprocessing of problematic data <span class="citation" data-cites="kleppmann2017designing">(<a href="#ref-kleppmann2017designing" role="doc-biblioref">Kleppmann 2016</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="fn11"><p><sup>11</sup>&nbsp;<strong>Dead Letter Queues</strong>: Queues that store unprocessed messages for analysis or reprocessing.</p></div><div id="ref-kleppmann2017designing" class="csl-entry" role="listitem">
Kleppmann, Martin. 2016. <em>Designing Data-Intensive Applications: The Big Ideas Behind Reliable, Scalable, and Maintainable Systems</em>. O’Reilly Media. <a href="http://shop.oreilly.com/product/0636920032175.do">http://shop.oreilly.com/product/0636920032175.do</a>.
</div></div><p>For instance, in a financial ML system ingesting market data, error handling might involve falling back to slightly delayed data sources if real-time feeds fail, while simultaneously alerting the operations team to the issue. This approach ensures that the system continues to function and that responsible parties are aware of and can address the problem.</p>
<p>This ensures that downstream processes have access to reliable, high-quality data for training and inference tasks, even in the face of ingestion challenges. Understanding these concepts of data validation and error handling is essential for students and practitioners aiming to build robust, production-ready ML systems.</p>
<p>Once ingestion is complete and data is validated, it is typically loaded into a storage environment suited to the organization’s analytical or machine learning needs. Some datasets flow into data warehouses for structured queries, whereas others are retained in data lakes for exploratory or large-scale analyses. Advanced systems may also employ feature stores to provide standardized features for machine learning.</p>
</section>
<section id="sec-data-engineering-continuing-kws-example-8152" class="level3">
<h3 class="anchored" data-anchor-id="sec-data-engineering-continuing-kws-example-8152">Continuing the KWS Example</h3>
<p>A production KWS system typically employs both streaming and batch ingestion patterns. The streaming pattern handles real-time audio data from active devices, where wake words must be detected with minimal latency. This requires careful implementation of pub/sub mechanisms—for example, using Apache Kafka-like streams to buffer incoming audio data and enable parallel processing across multiple inference servers.</p>
<p>Simultaneously, the system processes batch data for model training and updates. This includes ingesting new wake word recordings from crowdsourcing efforts, synthetic data from voice generation systems, and validated user interactions. The batch processing typically follows an ETL pattern, where audio data is preprocessed (normalized, filtered, segmented) before being stored in a format optimized for model training.</p>
<p>KWS systems must integrate data from diverse sources, such as real-time audio streams from deployed devices, crowdsourced recordings from data collection platforms etc. Each source presents unique challenges. Real-time audio streams require rate limiting to prevent system overload during usage spikes. Crowdsourced data needs robust validation to ensure recording quality and correct labeling. Synthetic data must be verified for realistic representation of wake word variations.</p>
<p>KWS systems employ sophisticated error handling mechanisms due to the nature of voice interaction. When processing real-time audio, dead letter queues store failed recognition attempts for analysis, helping identify patterns in false negatives or system failures. Data validation becomes particularly important for maintaining wake word detection accuracy—incoming audio must be checked for quality issues like clipping, noise levels, and appropriate sampling rates.</p>
<p>For example, consider a smart home device processing the wake word “Alexa.” The ingestion pipeline must validate:</p>
<ul>
<li>Audio quality metrics (signal-to-noise ratio, sample rate, bit depth)</li>
<li>Recording duration (typically 1-2 seconds for wake words)</li>
<li>Background noise levels</li>
<li>Speaker proximity indicators</li>
</ul>
<p>Invalid samples are routed to dead letter queues for analysis, while valid samples are processed in real-time for wake word detection.</p>
<p>This case study illustrates how real-world ML systems must carefully balance different ingestion patterns, handle multiple data sources, and maintain robust error handling—all while meeting strict latency and reliability requirements. The lessons from KWS systems apply broadly to other ML applications requiring real-time processing capabilities alongside continuous model improvement.</p>
<div id="quiz-question-sec-data-engineering-data-ingestion-81f3" class="callout callout-quiz-question">
<details class="callout-quiz-question fbx-default closebutton"><summary><strong>Self-Check: Question 1.4</strong></summary><div>
<ol type="1">
<li><p>Which data ingestion pattern is most appropriate for applications requiring immediate data processing and response to events as they occur?</p>
<ol type="a">
<li>Batch ingestion</li>
<li>Stream ingestion</li>
<li>Hybrid ingestion</li>
<li>Delayed ingestion</li>
</ol></li>
<li><p>Explain the primary tradeoff between ETL and ELT approaches in the context of ML systems.</p></li>
<li><p>True or False: In a hybrid data ingestion approach, both batch and stream ingestion methods are used to handle different data velocities and use cases.</p></li>
<li><p>In data ingestion, a ____ is used to store unprocessed messages for later analysis or reprocessing, helping to manage errors effectively.</p></li>
<li><p>Describe how error management strategies, such as graceful degradation and retry logic, contribute to the reliability of ML systems during data ingestion.</p></li>
</ol>
<p><a href="#quiz-answer-sec-data-engineering-data-ingestion-81f3" class="question-label">See Answers →</a></p>
</div></details>
</div>
</section>
</section>
<section id="sec-data-engineering-data-processing-60bd" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="sec-data-engineering-data-processing-60bd">Data Processing</h2>
<p>Data processing is a stage in the machine learning pipeline that transforms raw data into a format suitable for model training and inference. This stage encompasses several key activities, each playing a role in preparing data for effective use in ML systems. The approach to data processing is closely tied to the ETL (Extract, Transform, Load) or ELT (Extract, Load, Transform) paradigms discussed earlier.</p>
<p>In traditional ETL workflows, much of the data processing occurs before the data is loaded into the target system. This approach front-loads the cleaning, transformation, and feature engineering steps, ensuring that data is in a ready-to-use state when it reaches the data warehouse or ML pipeline. ETL is often preferred when dealing with structured data or when there’s a need for significant data cleansing before analysis.</p>
<p>Conversely, in ELT workflows, raw data is first loaded into the target system, and transformations are applied afterwards. This approach, often used with data lakes, allows for more flexibility in data processing. It’s particularly useful when dealing with unstructured or semi-structured data, or when the exact transformations needed are not known in advance. In ELT, many of the data processing steps we’ll discuss might be performed on-demand or as part of the ML pipeline itself.</p>
<p>The choice between ETL and ELT can impact how and when data processing occurs in an ML system. For instance, in an ETL-based system, data cleaning and initial transformations might happen before the data even reaches the ML team. In contrast, an ELT-based system might require ML engineers to handle more of the data processing tasks as part of their workflow.</p>
<p>Regardless of whether an organization follows an ETL or ELT approach, understanding the following data processing steps is crucial for ML practitioners. These processes ensure that data is clean, relevant, and optimally formatted for machine learning algorithms.</p>
<section id="sec-data-engineering-cleaning-techniques-4078" class="level3">
<h3 class="anchored" data-anchor-id="sec-data-engineering-cleaning-techniques-4078">Cleaning Techniques</h3>
<p>Data cleaning involves identifying and correcting errors, inconsistencies, and inaccuracies in datasets. Raw data frequently contains issues such as missing values, duplicates, or outliers that can significantly impact model performance if left unaddressed.</p>
<p>In practice, data cleaning might involve removing duplicate records, handling missing values through imputation or deletion, and correcting formatting inconsistencies. For instance, in a customer database, names might be inconsistently capitalized or formatted. A data cleaning process would standardize these entries, ensuring that “John Doe,” “john doe,” and “DOE, John” are all treated as the same entity.</p>
<p>Outlier detection and treatment is another important aspect of data cleaning. Outliers can sometimes represent valuable information about rare events, but they can also be the result of measurement errors or data corruption. ML practitioners must carefully consider the nature of their data and the requirements of their models when deciding how to handle outliers.</p>
</section>
<section id="sec-data-engineering-data-quality-assessment-4cc7" class="level3">
<h3 class="anchored" data-anchor-id="sec-data-engineering-data-quality-assessment-4cc7">Data Quality Assessment</h3>
<p>Quality assessment goes hand in hand with data cleaning, providing a systematic approach to evaluating the reliability and usefulness of data. This process involves examining various aspects of data quality, including accuracy, completeness, consistency, and timeliness.</p>
<p>Tools and techniques for quality assessment range from simple statistical measures to more complex machine learning-based approaches. For example, data profiling tools can provide summary statistics and visualizations that help identify potential quality issues. More advanced techniques might involve using unsupervised learning algorithms to detect anomalies or inconsistencies in large datasets.</p>
<p>Establishing clear quality metrics and thresholds is essential for maintaining data quality over time. These metrics might include the percentage of missing values, the frequency of outliers, or measures of data freshness. Regular quality assessments help ensure that data entering the ML pipeline meets the necessary standards for reliable model training and inference.</p>
</section>
<section id="sec-data-engineering-transformation-techniques-e976" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-data-engineering-transformation-techniques-e976">Transformation Techniques</h3>
<p>Data transformation converts the data from its raw form into a format more suitable for analysis and modeling. This process can include a wide range of operations, from simple conversions to complex mathematical transformations.</p>
<p>Common transformation tasks include normalization and standardization, which scale numerical features to a common range or distribution. For example, in a housing price prediction model, features like square footage and number of rooms might be on vastly different scales. Normalizing these features ensures that they contribute more equally to the model’s predictions <span class="citation" data-cites="bishop2006pattern">(<a href="#ref-bishop2006pattern" role="doc-biblioref">Bishop 2006</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-bishop2006pattern" class="csl-entry" role="listitem">
Bishop, Christopher M. 2006. <em>Pattern Recognition and Machine Learning</em>. Springer.
</div><div id="fn12"><p><sup>12</sup>&nbsp;<strong>One-Hot Encoding</strong>: Converts categorical variables into binary vectors, where each category is represented by a unique vector with one element set to 1 and the rest to 0. This allows categorical data to be used in ML models requiring numerical input.</p></div></div><p>Other transformations might involve encoding categorical variables, handling date and time data, or creating derived features. For instance, one-hot encoding<a href="#fn12" class="footnote-ref" id="fnref12" role="doc-noteref"><sup>12</sup></a> is often used to convert categorical variables into a format that can be readily understood by many machine learning algorithms.</p>
</section>
<section id="sec-data-engineering-feature-engineering-20d6" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-data-engineering-feature-engineering-20d6">Feature Engineering</h3>
<p>Feature engineering is the process of using domain knowledge to create new features that make machine learning algorithms work more effectively. This step is often considered more of an art than a science, requiring creativity and deep understanding of both the data and the problem at hand.</p>
<p>Feature engineering might involve combining existing features, extracting information from complex data types, or creating entirely new features based on domain insights. For example, in a retail recommendation system, engineers might create features that capture the recency, frequency, and monetary value of customer purchases, known as RFM analysis <span class="citation" data-cites="kuhn2013applied">(<a href="#ref-kuhn2013applied" role="doc-biblioref">Kuhn and Johnson 2013</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-kuhn2013applied" class="csl-entry" role="listitem">
Kuhn, Max, and Kjell Johnson. 2013. <em>Applied Predictive Modeling</em>. Springer New York. <a href="https://doi.org/10.1007/978-1-4614-6849-3">https://doi.org/10.1007/978-1-4614-6849-3</a>.
</div></div><p>The importance of feature engineering cannot be overstated. Well-engineered features can often lead to significant improvements in model performance, sometimes outweighing the impact of algorithm selection or hyperparameter tuning.</p>
</section>
<section id="sec-data-engineering-processing-pipeline-design-e27a" class="level3">
<h3 class="anchored" data-anchor-id="sec-data-engineering-processing-pipeline-design-e27a">Processing Pipeline Design</h3>
<p>Processing pipelines bring together the various data processing steps into a coherent, reproducible workflow. These pipelines ensure that data is consistently prepared across training and inference stages, reducing the risk of data leakage and improving the reliability of ML systems.</p>
<p>Modern ML frameworks and tools often provide capabilities for building and managing data processing pipelines. For instance, Apache Beam and TensorFlow Transform allow developers to define data processing steps that can be applied consistently during both model training and serving.</p>
<p>Effective pipeline design involves considerations such as modularity, scalability, and version control. Modular pipelines allow for easy updates and maintenance of individual processing steps. Version control for pipelines is crucial, ensuring that changes in data processing can be tracked and correlated with changes in model performance. This modular breakdown of pipeline components is well illustrated by TensorFlow Extended in <a href="#fig-tfx-pipeline-example" class="quarto-xref">Figure&nbsp;9</a>, which shows the complete flow from initial data ingestion through to final model deployment.</p>
<div id="fig-tfx-pipeline-example" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-env="figure" data-fig-pos="htb">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-tfx-pipeline-example-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="9daff432776d54e821a3a38d383ac78f17ea1f27.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-9" title="Figure&nbsp;9: Data Processing Pipeline: A modular end-to-end ML pipeline, as implemented in TensorFlow extended, highlighting key stages from raw data ingestion to trained model deployment and serving. this decomposition enables independent development, versioning, and scaling of each component, improving maintainability and reproducibility of ML systems."><img src="data_engineering_files/mediabag/9daff432776d54e821a3a38d383ac78f17ea1f27.svg" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-tfx-pipeline-example-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;9: <strong>Data Processing Pipeline</strong>: A modular end-to-end ML pipeline, as implemented in TensorFlow extended, highlighting key stages from raw data ingestion to trained model deployment and serving. this decomposition enables independent development, versioning, and scaling of each component, improving maintainability and reproducibility of ML systems.
</figcaption>
</figure>
</div>
</section>
<section id="sec-data-engineering-scalability-considerations-d217" class="level3">
<h3 class="anchored" data-anchor-id="sec-data-engineering-scalability-considerations-d217">Scalability Considerations</h3>
<p>As datasets grow larger and ML systems become more complex, the scalability of data processing becomes increasingly important. Processing large volumes of data efficiently often requires distributed computing approaches and careful consideration of computational resources.</p>
<p>Techniques for scaling data processing include parallel processing, where data is divided across multiple machines or processors for simultaneous processing. Distributed frameworks like Apache Spark are commonly used for this purpose, allowing data processing tasks to be scaled across large clusters of computers.</p>
<p>Another important consideration is the balance between preprocessing and on-the-fly computation. While extensive preprocessing can speed up model training and inference, it can also lead to increased storage requirements and potential data staleness. Some ML systems opt for a hybrid approach, preprocessing certain features while computing others on-the-fly during model training or inference.</p>
<p>Effective data processing is fundamental to the success of ML systems. By carefully cleaning, transforming, and engineering data, practitioners can significantly improve the performance and reliability of their models. As the field of machine learning continues to evolve, so too do the techniques and tools for data processing, making this an exciting and dynamic area of study and practice.</p>
</section>
<section id="sec-data-engineering-continuing-kws-example-fe8e" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-data-engineering-continuing-kws-example-fe8e">Continuing the KWS Example</h3>
<p>A KWS system requires careful cleaning of audio recordings to ensure reliable wake word detection. Raw audio data often contains various imperfections—background noise, clipped signals, varying volumes, and inconsistent sampling rates. For example, when processing the wake word “Alexa,” the system must clean recordings to standardize volume levels, remove ambient noise, and ensure consistent audio quality across different recording environments, all while preserving the essential characteristics that make the wake word recognizable.</p>
<p>Building on clean data, quality assessment becomes important for KWS systems. Quality metrics for KWS data are uniquely focused on audio characteristics, including signal-to-noise ratio (SNR), audio clarity scores, and speaking rate consistency. For instance, a KWS quality assessment pipeline might automatically flag recordings where background noise exceeds acceptable thresholds or where the wake word is spoken too quickly or unclearly, ensuring only high-quality samples are used for model training.</p>
<p>These quality metrics must be carefully calibrated to reflect real-world operating conditions. A robust training dataset incorporates both pristine recordings and samples containing controlled levels of environmental variations. For instance, while recordings with signal-masking interference are excluded, the dataset should include samples with measured background acoustics, variable speaker distances, and concurrent speech or other forms of audio signals. This approach to data diversity ensures the model maintains wake word detection reliability across the full spectrum of deployment environments and acoustic conditions.</p>
<p>Once quality is assured, transforming audio data for KWS involves converting raw waveforms into formats suitable for ML models. The typical transformation pipeline converts audio signals into spectrograms<a href="#fn13" class="footnote-ref" id="fnref13" role="doc-noteref"><sup>13</sup></a> or mel-frequency cepstral coefficients (MFCCs)<a href="#fn14" class="footnote-ref" id="fnref14" role="doc-noteref"><sup>14</sup></a>, standardizing the representation across different recording conditions. This transformation must be consistently applied across both training and inference, often with additional considerations for real-time processing on edge devices.</p>
<div class="no-row-height column-margin column-container"><div id="fn13"><p><sup>13</sup>&nbsp;<strong>Spectrogram</strong>: A visual representation of the spectrum of frequencies in a signal as it varies over time, commonly used in audio processing.</p></div><div id="fn14"><p><sup>14</sup>&nbsp;<strong>Mel-Frequency Cepstral Coefficients (MFCCs)</strong>: Features extracted from audio signals that represent the short-term power spectrum, widely used in speech and audio analysis.</p></div></div><p><a href="#fig-spectrogram-example" class="quarto-xref">Figure&nbsp;10</a> illustrates this transformation process. The top panel is a raw waveform of a simulated audio signal, which consists of a sine wave mixed with noise. This time-domain representation highlights the challenges posed by real-world recordings, where noise and variability must be addressed. The middle panel shows the spectrogram of the signal, which maps its frequency content over time. The spectrogram provides a detailed view of how energy is distributed across frequencies, making it easier to analyze patterns that could influence wake word recognition, such as the presence of background noise or signal distortions The bottom panel shows the MFCCs, derived from the spectrogram. These coefficients compress the audio information into a format that emphasizes speech-related characteristics, making them well-suited for KWS tasks.</p>
<div id="fig-spectrogram-example" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-pos="t!">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-spectrogram-example-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="images/png/kws_spectrogram.png" class="lightbox" data-gallery="quarto-lightbox-gallery-10" title="Figure&nbsp;10: Audio Feature Transformation: Spectrograms and mel-frequency cepstral coefficients (mfccs) compress raw audio waveforms into representations that emphasize perceptually relevant characteristics for machine learning tasks. This transformation reduces noise and data dimensionality while preserving essential speech information, improving model performance in applications like keyword spotting."><img src="images/png/kws_spectrogram.png" class="img-fluid figure-img" data-fig-pos="t!"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-spectrogram-example-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;10: <strong>Audio Feature Transformation</strong>: Spectrograms and mel-frequency cepstral coefficients (mfccs) compress raw audio waveforms into representations that emphasize perceptually relevant characteristics for machine learning tasks. This transformation reduces noise and data dimensionality while preserving essential speech information, improving model performance in applications like keyword spotting.
</figcaption>
</figure>
</div>
<p>With transformed data in hand, feature engineering for KWS focuses on extracting characteristics that help distinguish wake words from background speech. Engineers might create features capturing tonal variations, speech energy patterns, or temporal characteristics. For the wake word “Alexa,” features might include energy distribution across frequency bands, pitch contours, and duration patterns that characterize typical pronunciations. While hand-engineered speech features have seen much success, learned features <span class="citation" data-cites="zeghidour2021leaf">(<a href="#ref-zeghidour2021leaf" role="doc-biblioref">Zeghidour et al. 2021</a>)</span> are increasingly common.</p>
<div class="no-row-height column-margin column-container"><div id="ref-zeghidour2021leaf" class="csl-entry" role="listitem">
Zeghidour, Neil, Olivier Teboul, Félix de Chaumont Quitry, and Marco Tagliasacchi. 2021. <span>“LEAF: A Learnable Frontend for Audio Classification.”</span> <em>arXiv Preprint arXiv:2101.08596</em>, January. <a href="http://arxiv.org/abs/2101.08596v1">http://arxiv.org/abs/2101.08596v1</a>.
</div></div><p>In practice, bringing all these elements together, KWS processing pipelines must handle both batch processing for training and real-time processing for inference. The pipeline typically includes stages for audio preprocessing, feature extraction, and quality filtering. Importantly, these pipelines must be designed to operate efficiently on edge devices while maintaining consistent processing steps between training and deployment.</p>
<div id="quiz-question-sec-data-engineering-data-processing-60bd" class="callout callout-quiz-question">
<details class="callout-quiz-question fbx-default closebutton"><summary><strong>Self-Check: Question 1.5</strong></summary><div>
<ol type="1">
<li><p>Which of the following is a key advantage of using ELT over ETL in data processing for ML systems?</p>
<ol type="a">
<li>Allows for front-loading data cleaning and transformation</li>
<li>Provides flexibility in processing unstructured data</li>
<li>Reduces storage requirements for raw data</li>
<li>Ensures data quality before loading into the target system</li>
</ol></li>
<li><p>Explain why modularity and version control are important considerations in processing pipeline design for ML systems.</p></li>
<li><p>In data processing, ____ involves converting raw data into a format more suitable for analysis and modeling, often including tasks like normalization and encoding.</p></li>
<li><p>True or False: In a KWS system, feature engineering is primarily focused on extracting characteristics that help distinguish wake words from background speech.</p></li>
<li><p>Given a dataset with 10,000 audio recordings, each requiring 5 seconds for processing, calculate the total processing time if the system can handle 50 recordings concurrently. What does this imply for scalability?</p></li>
</ol>
<p><a href="#quiz-answer-sec-data-engineering-data-processing-60bd" class="question-label">See Answers →</a></p>
</div></details>
</div>
</section>
</section>
<section id="sec-data-engineering-data-labeling-044f" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="sec-data-engineering-data-labeling-044f">Data Labeling</h2>
<p>While data engineering encompasses many aspects of preparing data for machine learning systems, data labeling represents a particularly complex systems challenge. As training datasets grow to millions or billions of examples,<a href="#fn15" class="footnote-ref" id="fnref15" role="doc-noteref"><sup>15</sup></a> the infrastructure supporting labeling operations becomes increasingly critical to system performance.</p>
<div class="no-row-height column-margin column-container"><div id="fn15"><p><sup>15</sup>&nbsp;Modern ML models often contain millions or billions of parameters to capture complex data patterns. A general rule of thumb in ML is to have a training dataset that is at least 10 times larger than the model’s parameter count to ensure robust learning and avoid overfitting.</p></div></div><p>Modern machine learning systems must efficiently handle the creation, storage, and management of labels across their data pipeline. The systems architecture must support various labeling workflows while maintaining data consistency, ensuring quality, and managing computational resources effectively. These requirements compound when dealing with large-scale datasets or real-time labeling needs.</p>
<p>The systematic challenges extend beyond just storing and managing labels. Production ML systems need robust pipelines that integrate labeling workflows with data ingestion, preprocessing, and training components. These pipelines must maintain high throughput while ensuring label quality and adapting to changing requirements. For instance, a speech recognition system might need to continuously update its training data with new audio samples and corresponding transcription labels, requiring careful coordination between data collection, labeling, and training subsystems.</p>
<p>Infrastructure requirements vary significantly based on labeling approach and scale. Manual expert labeling may require specialized interfaces and security controls, while automated labeling systems need substantial compute resources for inference. Organizations must carefully balance these requirements against performance needs and resource constraints.</p>
<p>We explore how data labeling fundamentally shapes machine learning system design. From storage architectures to quality control pipelines, each aspect of the labeling process introduces unique technical challenges that ripple throughout the ML infrastructure. Understanding these systems-level implications is essential for building robust, scalable labeling solutions which are an integral part of data negineering.</p>
<section id="sec-data-engineering-types-labels-c660" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-data-engineering-types-labels-c660">Types of Labels</h3>
<p>To build effective machine learning systems, we must first understand how different types of labels affect our system architecture and resource requirements. Let’s explore this through a practical example: imagine building a smart city system that needs to detect and track various objects like vehicles, pedestrians, and traffic signs from video feeds. Labels capture information about key tasks or concepts.</p>
<ul>
<li><p><strong>Classification labels</strong> are the simplest form, categorizing images with a specific tag or (in multi-label classification) tags (e.g., labeling an image as “car” or “pedestrian”). While conceptually straightforward, a production system processing millions of video frames must efficiently store and retrieve these labels.</p></li>
<li><p><strong>Bounding boxes</strong> go further by identifying object locations, drawing a box around each object of interest. Our system now needs to track not just what objects exist, but where they are in each frame. This spatial information introduces new storage and processing challenges, especially when tracking moving objects across video frames.</p></li>
<li><p><strong>Segmentation maps</strong> provide the most detailed information by classifying objects at the pixel level, highlighting each object in a distinct color. For our traffic monitoring system, this might mean precisely outlining each vehicle, pedestrian, and road sign. These detailed annotations significantly increase our storage and processing requirements.</p></li>
</ul>
<p><a href="#fig-labels" class="quarto-xref">Figure&nbsp;11</a> illustrates the common label types:</p>
<div id="fig-labels" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-labels-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="images/png/cs249r_labels_new.png" class="lightbox" data-gallery="quarto-lightbox-gallery-11" title="Figure&nbsp;11: Data Annotation Granularity: Increasing levels of detail in data labeling—from bounding boxes to pixel-level segmentation—impact both annotation cost and potential model accuracy. Fine-grained segmentation provides richer information for training but demands significantly more labeling effort and storage capacity than coarser annotations."><img src="images/png/cs249r_labels_new.png" class="img-fluid figure-img" style="width:90.0%"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-labels-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;11: <strong>Data Annotation Granularity</strong>: Increasing levels of detail in data labeling—from bounding boxes to pixel-level segmentation—impact both annotation cost and potential model accuracy. Fine-grained segmentation provides richer information for training but demands significantly more labeling effort and storage capacity than coarser annotations.
</figcaption>
</figure>
</div>
<p>The choice of label format depends heavily on our system requirements and resource constraints <span class="citation" data-cites="10.1109/ICRA.2017.7989092">(<a href="#ref-10.1109/ICRA.2017.7989092" role="doc-biblioref">Johnson-Roberson et al. 2017</a>)</span>. While classification labels might suffice for simple traffic counting, autonomous vehicles need detailed segmentation maps to make precise navigation decisions. Leading autonomous vehicle companies often maintain hybrid systems that store multiple label types for the same data, allowing flexible use across different applications.</p>
<div class="no-row-height column-margin column-container"><div id="ref-10.1109/ICRA.2017.7989092" class="csl-entry" role="listitem">
Johnson-Roberson, Matthew, Charles Barto, Rounak Mehta, Sharath Nittur Sridhar, Karl Rosaen, and Ram Vasudevan. 2017. <span>“Driving in the Matrix: Can Virtual Worlds Replace Human-Generated Annotations for Real World Tasks?”</span> In <em>2017 IEEE International Conference on Robotics and Automation (ICRA)</em>, 746–53. Singapore, Singapore: IEEE. <a href="https://doi.org/10.1109/icra.2017.7989092">https://doi.org/10.1109/icra.2017.7989092</a>.
</div><div id="ref-ardila2020common" class="csl-entry" role="listitem">
Ardila, Rosana, Megan Branson, Kelly Davis, Michael Kohler, Josh Meyer, Michael Henretty, Reuben Morais, Lindsay Saunders, Francis Tyers, and Gregor Weber. 2020. <span>“Common Voice: A Massively-Multilingual Speech Corpus.”</span> In <em>Proceedings of the Twelfth Language Resources and Evaluation Conference</em>, 4218–22. Marseille, France: European Language Resources Association. <a href="https://aclanthology.org/2020.lrec-1.520">https://aclanthology.org/2020.lrec-1.520</a>.
</div></div><p>Beyond the core labels, production systems must also handle rich metadata. The Common Voice dataset <span class="citation" data-cites="ardila2020common">(<a href="#ref-ardila2020common" role="doc-biblioref">Ardila et al. 2020</a>)</span>, for instance, exemplifies this in its management of audio data for speech recognition. The system tracks speaker demographics for model fairness, recording quality metrics for data filtering, validation status for label reliability, and language information for multilingual support.</p>
<p>Modern labeling platforms have built sophisticated metadata management systems to handle these complex relationships. This metadata becomes important for maintaining and managing data quality and debugging model behavior. If our traffic monitoring system performs poorly in rainy conditions, having metadata about weather conditions during data collection helps identify and address the issue. The infrastructure must efficiently index and query this metadata alongside the primary labels.</p>
<p>The choice of label type cascades through our entire system design. A system built for simple classification labels would need significant modifications to handle segmentation maps efficiently. The infrastructure must optimize storage systems for the chosen label format, implement efficient data retrieval patterns for training, maintain quality control pipelines for validation, and manage version control for label updates. Resource allocation becomes particularly critical as data volume grows, requiring careful capacity planning across storage, compute, and networking components.</p>
</section>
<section id="sec-data-engineering-annotation-techniques-d939" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-data-engineering-annotation-techniques-d939">Annotation Techniques</h3>
<p>Manual labeling by experts is the primary approach in many annotation pipelines. This method produces high-quality results but also raises considerable system design challenges. For instance, in medical imaging systems, experienced radiologists offer essential annotations. Such systems necessitate specialized interfaces for accurate labeling, secure data access controls to protect patient privacy, and reliable version control mechanisms to monitor annotation revisions. Despite the dependable outcomes of expert labeling, the scarcity and high expenses of specialists render it challenging to implement on a large scale for extensive datasets.</p>
<p>As we discussed earlier, crowdsourcing offers a path to greater scalability by distributing annotation tasks across many annotators <span class="citation" data-cites="victor2019machine">(<a href="#ref-victor2019machine" role="doc-biblioref">Sheng and Zhang 2019</a>)</span>. Crowdsourcing enables non-experts to distribute annotation tasks, often through dedicated platforms <span class="citation" data-cites="victor2019machine">(<a href="#ref-victor2019machine" role="doc-biblioref">Sheng and Zhang 2019</a>)</span>. Several companies have emerged as leaders in this space, building sophisticated platforms for large-scale annotation. For instance, companies such as <a href="https://scale.com/">Scale AI</a> specialize in managing thousands of concurrent annotators through their platform. <a href="https://www.appen.com/">Appen</a> focuses on linguistic annotation and text data, while <a href="https://labelbox.com/">Labelbox</a> has developed specialized tools for computer vision tasks. These platforms allow dataset creators to access a large pool of annotators, making it possible to label vast amounts of data relatively quickly.</p>
<div class="no-row-height column-margin column-container"><div id="ref-victor2019machine" class="csl-entry" role="listitem">
Sheng, Victor S., and Jing Zhang. 2019. <span>“Machine Learning with Crowdsourcing: A Brief Summary of the Past Research and Future Directions.”</span> <em>Proceedings of the AAAI Conference on Artificial Intelligence</em> 33 (01): 9837–43. <a href="https://doi.org/10.1609/aaai.v33i01.33019837">https://doi.org/10.1609/aaai.v33i01.33019837</a>.
</div><div id="ref-ratner2018snorkel" class="csl-entry" role="listitem">
Ratner, Alex, Braden Hancock, Jared Dunnmon, Roger Goldman, and Christopher Ré. 2018. <span>“Snorkel MeTaL: Weak Supervision for Multi-Task Learning.”</span> In <em>Proceedings of the Second Workshop on Data Management for End-to-End Machine Learning</em>, 1–4. ACM. <a href="https://doi.org/10.1145/3209889.3209898">https://doi.org/10.1145/3209889.3209898</a>.
</div></div><p>Weakly supervised and programmatic methods represent a third approach, using automation to reduce manual effort <span class="citation" data-cites="ratner2018snorkel">(<a href="#ref-ratner2018snorkel" role="doc-biblioref">Ratner et al. 2018</a>)</span>. These systems leverage existing knowledge bases and heuristics to automatically generate labels. For example, distant supervision techniques might use a knowledge base to label mentions of companies in text data. While these methods can rapidly label large datasets, they require substantial compute resources for inference, sophisticated caching systems to avoid redundant computation, and careful monitoring to manage potential noise and bias.</p>
<p>Most production systems combine multiple annotation approaches to balance speed, cost, and quality. A common pattern employs programmatic labeling for initial coverage, followed by crowdsourced verification and expert review of uncertain cases. This hybrid approach requires careful system design to manage the flow of data between different annotation stages. The infrastructure must track label provenance, manage quality control at each stage, and ensure consistent data access patterns across different annotator types.</p>
<p>The choice of annotation method significantly impacts system architecture. Expert-only systems might employ centralized architectures with high-speed access to a single data store. Crowdsourcing demands distributed architectures to handle concurrent annotators. Automated systems need substantial compute resources and caching infrastructure. Many organizations implement tiered architectures where different annotation methods operate on different subsets of data based on complexity and criticality.</p>
<p>Clear guidelines and thorough training remain essential regardless of the chosen architecture. The system must provide consistent interfaces, documentation, and quality metrics across all annotation methods. This becomes particularly challenging when managing diverse annotator pools with varying levels of expertise. Some platforms address this by offering access to specialized annotators. For instance, providing medical professionals for healthcare datasets or domain experts for technical content.</p>
</section>
<section id="sec-data-engineering-label-quality-assessment-0ce4" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-data-engineering-label-quality-assessment-0ce4">Label Quality Assessment</h3>
<p>Label quality is extremely important for machine learning system performance. A model can only be as good as its training data. However, ensuring quality at scale presents significant systems challenges. The fundamental challenge stems from label uncertainty.</p>
<p><a href="#fig-hard-labels" class="quarto-xref">Figure&nbsp;12</a> illustrates common failure modes in labeling systems: some errors arise from data quality issues (like the blurred frog image), while others require deep domain expertise (as with the black stork identification). Even with clear instructions and careful system design, some fraction of labels will inevitably be incorrect <span class="citation" data-cites="northcutt2021pervasive">Thyagarajan et al. (<a href="#ref-thyagarajan2023multilabel" role="doc-biblioref">2022</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-thyagarajan2023multilabel" class="csl-entry" role="listitem">
Thyagarajan, Aditya, Elías Snorrason, Curtis G. Northcutt, and Jonas Mueller 0001. 2022. <span>“Identifying Incorrect Annotations in Multi-Label Classification Data.”</span> <em>CoRR</em>. <a href="https://doi.org/10.48550/ARXIV.2211.13895">https://doi.org/10.48550/ARXIV.2211.13895</a>.
</div></div><div id="fig-hard-labels" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-hard-labels-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="images/png/label-errors-examples_new.png" class="lightbox" data-gallery="quarto-lightbox-gallery-12" title="Figure&nbsp;12: Labeling Ambiguity: How subjective or difficult examples, such as blurry images or rare species, can introduce errors during data labeling, highlighting the need for careful quality control and potentially expert annotation. Source: [@northcutt2021pervasive]."><img src="images/png/label-errors-examples_new.png" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-hard-labels-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;12: <strong>Labeling Ambiguity</strong>: How subjective or difficult examples, such as blurry images or rare species, can introduce errors during data labeling, highlighting the need for careful quality control and potentially expert annotation. Source: <span class="citation" data-cites="northcutt2021pervasive">(<a href="#ref-northcutt2021pervasive" role="doc-biblioref">Northcutt, Athalye, and Mueller 2021</a>)</span>.
</figcaption>
<div class="no-row-height column-margin column-container"><div id="ref-northcutt2021pervasive" class="csl-entry" role="listitem">
Northcutt, Curtis G, Anish Athalye, and Jonas Mueller. 2021. <span>“Pervasive Label Errors in Test Sets Destabilize Machine Learning Benchmarks.”</span> <em>arXiv</em>. https://doi.org/<a href="https://doi.org/10.48550/arXiv.2103.14749 arXiv-issued DOI via DataCite">https://doi.org/10.48550/arXiv.2103.14749 arXiv-issued DOI via DataCite</a>.
</div></div></figure>
</div>
<p>Production ML systems implement multiple layers of quality control to address these challenges. Typically, systematic quality checks continuously monitor the labeling pipeline. These systems randomly sample labeled data for expert review and employ statistical methods to flag potential errors. The infrastructure must efficiently process these checks across millions of examples without creating bottlenecks in the labeling pipeline.</p>
<p>Collecting multiple labels per data point, often referred to as “consensus labeling,” can help identify controversial or ambiguous cases. Professional labeling companies have developed sophisticated infrastructure for this process. For example, <a href="https://labelbox.com/">Labelbox</a> has consensus tools that track inter-annotator agreement rates and automatically route controversial cases for expert review. <a href="https://scale.com">Scale AI</a> implements tiered quality control, where experienced annotators verify the work of newer team members.</p>
<p>Beyond technical infrastructure, successful labeling systems must consider human factors. When working with annotators, organizations need robust systems for training and guidance. This includes good documentation, clear examples of correct labeling, and regular feedback mechanisms. For complex or domain-specific tasks, the system might implement tiered access levels, routing challenging cases to annotators with appropriate expertise.</p>
<p>Ethical considerations also significantly impact system design. For datasets containing potentially disturbing content, systems should implement protective features like grayscale viewing options <span class="citation" data-cites="googleinformation">(<a href="#ref-googleinformation" role="doc-biblioref">Blackwood et al. 2019</a>)</span>. This requires additional image processing pipelines and careful interface design. We need to develop workload management systems that track annotator exposure to sensitive content and enforce appropriate limits.</p>
<div class="no-row-height column-margin column-container"><div id="ref-googleinformation" class="csl-entry" role="listitem">
Blackwood, Jayden, Frances C. Wright, Nicole J. Look Hong, and Anna R. Gagliardi. 2019. <span>“Quality of DCIS Information on the Internet: A Content Analysis.”</span> <em>Breast Cancer Research and Treatment</em> 177 (2): 295–305. <a href="https://doi.org/10.1007/s10549-019-05315-8">https://doi.org/10.1007/s10549-019-05315-8</a>.
</div></div><p>The quality control system itself generates substantial data that must be efficiently processed and monitored. Organizations typically track inter-annotator agreement rates, label confidence scores, time spent per annotation, error patterns and types, annotator performance metrics, and bias indicators. These metrics must be computed and updated efficiently across millions of examples, often requiring dedicated analytics pipelines.</p>
<p>Regular bias audits are another critical component of quality control. Systems must monitor for cultural, personal, or professional biases that could skew the labeled dataset. This requires infrastructure for collecting and analyzing demographic information, measuring label distributions across different annotator groups, identifying systematic biases in the labeling process, and implementing corrective measures when biases are detected.</p>
<p>Perhaps the most important aspect is that the process must remain iterative. As new challenges emerge, quality control systems must adapt and evolve. Through careful system design and implementation of these quality control mechanisms, organizations can maintain high label quality even at a massive scale.</p>
</section>
<section id="sec-data-engineering-ai-annotation-3b26" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-data-engineering-ai-annotation-3b26">AI in Annotation</h3>
<p>As machine learning systems grow in scale and complexity, organizations increasingly leverage AI to accelerate and enhance their labeling pipelines. This approach introduces new system design considerations around model deployment, resource management, and human-AI collaboration. The fundamental challenge stems from data volume. Manual annotation alone cannot keep pace with modern ML systems’ data needs. As illustrated in <a href="#fig-weak-supervision" class="quarto-xref">Figure&nbsp;13</a>, AI assistance offers several paths to scale labeling operations, each requiring careful system design to balance speed, quality, and resource usage.</p>
<div id="fig-weak-supervision" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-env="figure" data-fig-pos="htb">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-weak-supervision-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="a271bd786f6723a93e5e1b3dff59701d8362b9f2.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-13" title="Figure&nbsp;13: AI-Augmented Labeling: Programmatic labeling, distant supervision, and active learning scale data annotation by trading potential labeling errors for increased throughput, necessitating careful system design to balance labeling speed, cost, and model quality. These strategies enable machine learning systems to overcome limitations imposed by manual annotation alone, facilitating deployment in data-scarce environments. Source: Stanford AI Lab."><img src="data_engineering_files/mediabag/a271bd786f6723a93e5e1b3dff59701d8362b9f2.svg" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-weak-supervision-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;13: <strong>AI-Augmented Labeling</strong>: Programmatic labeling, distant supervision, and active learning scale data annotation by trading potential labeling errors for increased throughput, necessitating careful system design to balance labeling speed, cost, and model quality. These strategies enable machine learning systems to overcome limitations imposed by manual annotation alone, facilitating deployment in data-scarce environments. Source: Stanford AI Lab.
</figcaption>
</figure>
</div>
<p>Modern AI-assisted labeling typically employs a combination of approaches. Pre-annotation involves using AI models to generate preliminary labels for a dataset, which humans can then review and correct. Major labeling platforms have made significant investments in this technology. <a href="https://snorkel.ai/">Snorkel AI</a> uses programmatic labeling to automatically generate initial labels at scale. Scale AI deploys pre-trained models to accelerate annotation in specific domains like autonomous driving, while manycompanies like <a href="https://www.superannotate.com/">SuperAnnotate</a> provide automated pre-labeling tools that can reduce manual effort drastically. This method, which often employs semi-supervised learning techniques <span class="citation" data-cites="chapelle2009semisupervised">(<a href="#ref-chapelle2009semisupervised" role="doc-biblioref">Chapelle, Scholkopf, and Zien 2009</a>)</span>, can save a significant amount of time, especially for extremely large datasets.</p>
<div class="no-row-height column-margin column-container"><div id="ref-chapelle2009semisupervised" class="csl-entry" role="listitem">
Chapelle, O., B. Scholkopf, and A. Zien Eds. 2009. <span>“Semi-Supervised Learning (Chapelle, o. Et Al., Eds.; 2006) [Book Reviews].”</span> <em>IEEE Transactions on Neural Networks</em> 20 (3): 542–42. <a href="https://doi.org/10.1109/tnn.2009.2015974">https://doi.org/10.1109/tnn.2009.2015974</a>.
</div></div><p>The emergence of Large Language Models (LLMs) like ChatGPT has further transformed labeling pipelines. Beyond simple classification, LLMs can generate rich text descriptions, create labeling guidelines, and even explain their reasoning. For instance, content moderation systems use LLMs to perform initial content classification and generate explanations for policy violations. However, integrating LLMs introduces new system challenges around inference costs, rate limiting, and output validation. Many organizations adopt a tiered approach, using smaller specialized models for routine cases while reserving larger LLMs for complex scenarios.</p>
<p>Methods such as active learning<a href="#fn16" class="footnote-ref" id="fnref16" role="doc-noteref"><sup>16</sup></a> complement these approaches by intelligently prioritizing which examples need human attention <span class="citation" data-cites="coleman2022similarity">(<a href="#ref-coleman2022similarity" role="doc-biblioref">Coleman et al. 2022</a>)</span>. These systems continuously analyze model uncertainty to identify valuable labeling candidates for humans to label. The infrastructure must efficiently compute uncertainty metrics, maintain task queues, and adapt prioritization strategies based on incoming labels. Consider a medical imaging system: active learning might identify unusual pathologies for expert review while handling routine cases automatically.</p>
<div class="no-row-height column-margin column-container"><div id="fn16"><p><sup>16</sup>&nbsp;A machine learning approach where the model selects the most informative data points for labeling to improve learning efficiency.</p></div><div id="ref-coleman2022similarity" class="csl-entry" role="listitem">
Coleman, Cody, Edward Chou, Julian Katz-Samuels, Sean Culatana, Peter Bailis, Alexander C. Berg, Robert Nowak, Roshan Sumbaly, Matei Zaharia, and I. Zeki Yalniz. 2022. <span>“Similarity Search for Efficient Active Learning and Search of Rare Concepts.”</span> <em>Proceedings of the AAAI Conference on Artificial Intelligence</em> 36 (6): 6402–10. <a href="https://doi.org/10.1609/aaai.v36i6.20591">https://doi.org/10.1609/aaai.v36i6.20591</a>.
</div></div><p>Quality control becomes increasingly crucial as these AI components interact. The system must monitor both AI and human performance, detect potential errors, and maintain clear label provenance. This requires dedicated infrastructure tracking metrics like model confidence and human-AI agreement rates. In safety-critical domains like self-driving cars, these systems must maintain particularly rigorous standards while processing massive streams of sensor data.</p>
<p>Real-world deployments demonstrate these principles at scale. Medical imaging systems <span class="citation" data-cites="krishnan2022selfsupervised">(<a href="#ref-krishnan2022selfsupervised" role="doc-biblioref">Krishnan, Rajpurkar, and Topol 2022</a>)</span> combine pre-annotation for common conditions with active learning for unusual cases, all while maintaining strict patient privacy.</p>
<div class="no-row-height column-margin column-container"><div id="ref-krishnan2022selfsupervised" class="csl-entry" role="listitem">
Krishnan, Rayan, Pranav Rajpurkar, and Eric J. Topol. 2022. <span>“Self-Supervised Learning in Medicine and Healthcare.”</span> <em>Nature Biomedical Engineering</em> 6 (12): 1346–52. <a href="https://doi.org/10.1038/s41551-022-00914-1">https://doi.org/10.1038/s41551-022-00914-1</a>.
</div></div><p>Self-driving vehicle systems coordinate multiple AI models to label diverse sensor data in real-time. Social media platforms process millions of items hourly, using tiered approaches where simpler models handle clear cases while complex content routes to more sophisticated models or human reviewers.</p>
<p>While AI assistance offers clear benefits, it also introduces new failure modes. Systems must guard against bias amplification, where AI models trained on biased data perpetuate those biases in new labels. The infrastructure needs robust monitoring to detect such issues and mechanisms to break problematic feedback loops. Human oversight remains essential, requiring careful interface design to help annotators effectively supervise and correct AI output.</p>
</section>
<section id="sec-data-engineering-labeling-challenges-ae8c" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-data-engineering-labeling-challenges-ae8c">Labeling Challenges</h3>
<p>While data labeling is essential for the development of supervised machine learning models, it comes with its own set of challenges and limitations that practitioners must be aware of and address. One of the primary challenges in data labeling is the inherent subjectivity in many labeling tasks. Even with clear guidelines, human annotators<a href="#fn17" class="footnote-ref" id="fnref17" role="doc-noteref"><sup>17</sup></a> may interpret data differently, leading to inconsistencies in labeling. This is particularly evident in tasks involving sentiment analysis, image classification of ambiguous objects, or labeling of complex medical conditions. For instance, in a study of medical image annotation, <span class="citation" data-cites="oakden2020hidden">Oakden-Rayner et al. (<a href="#ref-oakden2020hidden" role="doc-biblioref">2020</a>)</span> found significant variability in labels assigned by different radiologists, highlighting the challenge of obtaining “ground truth” in inherently subjective tasks.</p>
<div class="no-row-height column-margin column-container"><div id="fn17"><p><sup>17</sup>&nbsp;When involving human annotators in data labeling, organizations must protect individual privacy through robust anonymization, sanitization of identifying information, and secure data handling practices. Additionally, annotators themselves should be protected from exposure to potentially harmful content through appropriate content filtering and support systems.</p></div><div id="ref-oakden2020hidden" class="csl-entry" role="listitem">
Oakden-Rayner, Luke, Jared Dunnmon, Gustavo Carneiro, and Christopher Re. 2020. <span>“Hidden Stratification Causes Clinically Meaningful Failures in Machine Learning for Medical Imaging.”</span> In <em>Proceedings of the ACM Conference on Health, Inference, and Learning</em>, 151–59. ACM. <a href="https://doi.org/10.1145/3368555.3384468">https://doi.org/10.1145/3368555.3384468</a>.
</div></div><p>Scalability presents another significant challenge, especially as datasets grow larger and more complex. Manual labeling is time-consuming and expensive, often becoming a bottleneck in the machine learning pipeline. While crowdsourcing and AI-assisted methods can help address this issue to some extent, they introduce their own complications in terms of quality control and potential biases.</p>
<p>The issue of bias in data labeling is particularly concerning. Annotators bring their own cultural, personal, and professional biases to the labeling process, which can be reflected in the resulting dataset. For example, <span class="citation" data-cites="wang2019balanced">Wang et al. (<a href="#ref-wang2019balanced" role="doc-biblioref">2019</a>)</span> found that image datasets labeled predominantly by annotators from one geographic region showed biases in object recognition tasks, performing poorly on images from other regions. This highlights the need for diverse annotator pools and careful consideration of potential biases in the labeling process.</p>
<div class="no-row-height column-margin column-container"><div id="ref-wang2019balanced" class="csl-entry" role="listitem">
Wang, Tianlu, Jieyu Zhao, Mark Yatskar, Kai-Wei Chang, and Vicente Ordonez. 2019. <span>“Balanced Datasets Are Not Enough: Estimating and Mitigating Gender Bias in Deep Image Representations.”</span> In <em>2019 IEEE/CVF International Conference on Computer Vision (ICCV)</em>, 5309–18. IEEE. <a href="https://doi.org/10.1109/iccv.2019.00541">https://doi.org/10.1109/iccv.2019.00541</a>.
</div></div><p>Data privacy and ethical considerations also pose challenges in data labeling. Leading data labeling companies have developed specialized solutions for these challenges. Scale AI, for instance, maintains dedicated teams and secure infrastructure for handling sensitive data in healthcare and finance. Appen implements strict data access controls and anonymization protocols, while Labelbox offers private cloud deployments for organizations with strict security requirements. When dealing with sensitive data, such as medical records or personal communications, ensuring annotator access while maintaining data privacy can be complex.</p>
<p>The dynamic nature of real-world data presents another limitation. Labels that are accurate at the time of annotation may become outdated or irrelevant as the underlying distribution of data changes over time. This concept, known as concept drift, necessitates ongoing labeling efforts and periodic re-evaluation of existing labels.</p>
<p>Lastly, the limitations of current labeling approaches become apparent when dealing with edge cases or rare events. In many real-world applications, it’s the unusual or rare instances that are often most critical (e.g., rare diseases in medical diagnosis, or unusual road conditions in autonomous driving). However, these cases are, by definition, underrepresented in most datasets and may be overlooked or mislabeled in large-scale annotation efforts.</p>
</section>
<section id="sec-data-engineering-continuing-kws-example-61b4" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-data-engineering-continuing-kws-example-61b4">Continuing the KWS Example</h3>
<p>The complex requirements of KWS reveal the role of automated data labeling in modern machine learning. The Multilingual Spoken Words Corpus (MSWC) <span class="citation" data-cites="mazumder2021multilingual">(<a href="#ref-mazumder2021multilingual" role="doc-biblioref">Mazumder et al. 2021</a>)</span> illustrates this through its innovative approach to generating labeled wake word data at scale. MSWC is large, containing over 23.4 million one-second spoken examples across 340,000 keywords in 50 different languages.</p>
<div class="no-row-height column-margin column-container"><div id="ref-mazumder2021multilingual" class="csl-entry" role="listitem">
Mazumder, Mark, Sharad Chitlangia, Colby Banbury, Yiping Kang, Juan Manuel Ciro, Keith Achorn, Daniel Galvez, et al. 2021. <span>“Multilingual Spoken Words Corpus.”</span> In <em>Thirty-Fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2)</em>.
</div><div id="fn18"><p><sup>18</sup>&nbsp;<strong>Forced Alignment</strong>: A technique in audio processing that synchronizes spoken words in an audio file with their corresponding text transcription by analyzing phoneme-level timing.</p></div></div><p>The core of this system, as illustrated in <a href="#fig-mswc" class="quarto-xref">Figure&nbsp;14</a>, begins with paired sentence audio recordings and corresponding transcriptions, which can be sourced from projects like <a href="https://commonvoice.mozilla.org/en">Common Voice</a> or multilingual captioned content platforms such as YouTube. The system processes paired audio-text inputs through forced alignment<a href="#fn18" class="footnote-ref" id="fnref18" role="doc-noteref"><sup>18</sup></a> to identify word boundaries, extracts individual keywords as one-second segments, and generates a large-scale multilingual dataset suitable for training keyword spotting models. For example, when a speaker says, “He gazed up the steep bank,” their voice generates a complex acoustic signal that conveys more than just the words themselves. This signal encapsulates subtle transitions between words, variations in pronunciation, and the natural rhythm of speech. The primary challenge lies in accurately pinpointing the exact location of each word within this continuous audio stream.</p>
<div id="fig-mswc" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-mswc-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="images/png/data_engineering_kws2.png" class="lightbox" data-gallery="quarto-lightbox-gallery-14" title="Figure&nbsp;14: Multilingual Data Preparation: Forced alignment and segmentation transform paired audio-text data into labeled one-second segments, creating a large-scale corpus for training keyword spotting models across 50+ languages. This automated process enables scalable development of KWS systems by efficiently generating training examples from readily available speech resources like common voice and multilingual captioned content."><img src="images/png/data_engineering_kws2.png" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-mswc-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;14: <strong>Multilingual Data Preparation</strong>: Forced alignment and segmentation transform paired audio-text data into labeled one-second segments, creating a large-scale corpus for training keyword spotting models across 50+ languages. This automated process enables scalable development of KWS systems by efficiently generating training examples from readily available speech resources like common voice and multilingual captioned content.
</figcaption>
</figure>
</div>
<p>This is where automated forced alignment proves useful. Tools such as the Montreal Forced Aligner <span class="citation" data-cites="mcauliffe17_interspeech">(<a href="#ref-mcauliffe17_interspeech" role="doc-biblioref">McAuliffe et al. 2017</a>)</span> analyze both the audio and its transcription, mapping the timing relationship between written words and spoken sounds, and attempts to mark the boundaries of when each word begins and ends in a speech recording at millisecond-level precision. For high-resource languages such as English, high-quality automated alignments are available “out-of-box” while alignments for low-resource languages must be bootstrapped on the speech data and transcriptions themselves, which can negatively impact timing quality.</p>
<div class="no-row-height column-margin column-container"><div id="ref-mcauliffe17_interspeech" class="csl-entry" role="listitem">
McAuliffe, Michael, Michaela Socolof, Sarah Mihuc, Michael Wagner, and Morgan Sonderegger. 2017. <span>“Montreal Forced Aligner: Trainable Text-Speech Alignment Using Kaldi.”</span> In <em>Interspeech 2017</em>, 498–502. ISCA. <a href="https://doi.org/10.21437/interspeech.2017-1386">https://doi.org/10.21437/interspeech.2017-1386</a>.
</div></div><p>With these precise timestamps, the extraction system can generate clean, one-second samples of individual keywords. However, this process requires careful engineering decisions. Background noise might interfere with detecting word boundaries. Speakers may stretch, compress, or mispronounce words in unexpected ways. Longer words may not fit within the default 1-second boundary. In order to aid ML practitioners in filtering out lower-quality samples in an automated fashion, MSWC provides a self-supervised anomaly detection algorithm, using acoustic embeddings to identify potential issues based on embedding distances to k-means clusters. This automated validation becomes particularly crucial given the scale of the dataset, which includes over 23 million samples across more than 340,000 words in 50+ languages. Traditional manual review could not maintain consistent standards across such volume without significant expense.</p>
<p>Modern voice assistant developers often build upon this type of labeling foundation. An automated corpus like MSWC may not contain the specific keywords an application developer wishes to use for their envisioned KWS system, but the corpus can provide a starting point for KWS prototyping in many underserved languages spoken around the world. While MSWC provides automated labeling at scale, production systems may add targeted human recording and verification for challenging cases, rare words, or difficult acoustic environments. The infrastructure must gracefully coordinate between automated processing and human expertise.</p>
<p>The impact of this careful engineering extends far beyond the dataset itself. Automated labeling pipelines open new avenues to how we approach wake word detection and other ML tasks across languages or other demographic boundaries. Where manual collection and annotation might yield thousands of examples, automated dataset generation can yield millions while maintaining consistent quality. This enables voice interfaces to understand an ever-expanding vocabulary across the world’s languages.</p>
<p>Through this approach to data labeling, MSWC demonstrates how thoughtful data engineering directly impacts production machine learning systems. The careful orchestration of forced alignment, extraction, and quality control creates a foundation for reliable voice interaction across languages. When a voice assistant responds to its wake word, it draws upon this sophisticated labeling infrastructure, which is a testament to the power of automated data processing in modern machine learning systems.</p>
<div id="quiz-question-sec-data-engineering-data-labeling-044f" class="callout callout-quiz-question">
<details class="callout-quiz-question fbx-default closebutton"><summary><strong>Self-Check: Question 1.6</strong></summary><div>
<ol type="1">
<li><p>Which type of label requires the most storage and processing resources in a machine learning system?</p>
<ol type="a">
<li>Classification labels</li>
<li>Bounding boxes</li>
<li>Segmentation maps</li>
<li>Metadata labels</li>
</ol></li>
<li><p>Explain how the choice of annotation technique impacts the system architecture of a machine learning system.</p></li>
<li><p>True or False: Automated labeling systems eliminate the need for human oversight in machine learning data pipelines.</p></li>
<li><p>In data labeling, ____ labeling involves using AI models to generate preliminary labels that are later reviewed by humans.</p></li>
<li><p>Discuss the challenges of maintaining label quality at scale in production ML systems.</p></li>
</ol>
<p><a href="#quiz-answer-sec-data-engineering-data-labeling-044f" class="question-label">See Answers →</a></p>
</div></details>
</div>
</section>
</section>
<section id="sec-data-engineering-data-storage-6651" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="sec-data-engineering-data-storage-6651">Data Storage</h2>
<p>Machine learning workloads have data access patterns that differ markedly from those of traditional transactional systems or routine analytics. Whereas transactional databases optimize for frequent writes and row-level updates, most ML pipelines rely on high-throughput reads, large-scale data scans, and evolving schemas. This difference reflects the iterative nature of model development: data scientists repeatedly load and transform vast datasets to engineer features, test new hypotheses, and refine models.</p>
<p>Additionally, ML pipelines must accommodate real-world considerations such as evolving business requirements, new data sources, and changes in data availability. These realities push storage solutions to be both scalable and flexible, ensuring that organizations can manage data collected from diverse channels, ranging from sensor feeds to social media text, without constantly retooling the entire infrastructure. In this section, we will compare the practical use of databases, data warehouses, and data lakes for ML projects, then delve into how specialized services, metadata, and governance practices unify these varied systems into a coherent strategy.</p>
<section id="sec-data-engineering-storage-system-types-519d" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-data-engineering-storage-system-types-519d">Storage System Types</h3>
<p>All raw and labeled data needs to be stored and accessed efficiently. When considering storage systems for ML, it is essential to understand the differences among different storage systems: databases, data warehouses, and data lakes. Each system has its strengths and is suited to different aspects of ML workflows.</p>
<p><a href="#tbl-storage" class="quarto-xref">Table&nbsp;1</a> provides an overview of these storage systems. Databases usually support operational and transactional purposes. They work well for smaller, well-structured datasets, but can become cumbersome and expensive when applied to large-scale ML contexts involving unstructured data (such as images, audio, or free-form text).</p>
<div id="tbl-storage" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-storage-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;1: <strong>Storage System Characteristics</strong>: Different storage systems suit distinct stages of machine learning workflows based on data structure and purpose; databases manage transactional data, data warehouses support analytical reporting, and data lakes accommodate diverse, raw data for future processing. Understanding these characteristics enables efficient data management and supports the scalability of machine learning applications.
</figcaption>
<div aria-describedby="tbl-storage-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<colgroup>
<col style="width: 20%">
<col style="width: 24%">
<col style="width: 24%">
<col style="width: 27%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Attribute</th>
<th style="text-align: left;">Conventional Database</th>
<th style="text-align: left;">Data Warehouse</th>
<th style="text-align: left;">Data Lake</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Purpose</td>
<td style="text-align: left;">Operational and transactional</td>
<td style="text-align: left;">Analytical and reporting</td>
<td style="text-align: left;">Storage for raw and diverse data for future processing</td>
</tr>
<tr class="even">
<td style="text-align: left;">Data type</td>
<td style="text-align: left;">Structured</td>
<td style="text-align: left;">Structured</td>
<td style="text-align: left;">Structured, semi-structured, and unstructured</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Scale</td>
<td style="text-align: left;">Small to medium volumes</td>
<td style="text-align: left;">Medium to large volumes</td>
<td style="text-align: left;">Large volumes of diverse data</td>
</tr>
<tr class="even">
<td style="text-align: left;">Performance Optimization</td>
<td style="text-align: left;">Optimized for transactional queries (OLTP)</td>
<td style="text-align: left;">Optimized for analytical queries (OLAP)</td>
<td style="text-align: left;">Optimized for scalable storage and retrieval</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Examples</td>
<td style="text-align: left;">MySQL, PostgreSQL, Oracle DB</td>
<td style="text-align: left;">Google BigQuery, Amazon Redshift, Microsoft Azure Synapse</td>
<td style="text-align: left;">Google Cloud Storage, AWS S3, Azure Data Lake Storage</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p>Data warehouses, by contrast, are optimized for analytical queries across integrated datasets that have been transformed into a standardized schema. As indicated in the table, they handle large volumes of integrated data. Many ML systems successfully draw on data warehouses to power model training because the structured environment simplifies data exploration and feature engineering. Yet one limitation remains: a data warehouse may not accommodate truly unstructured data or rapidly changing data formats, particularly if the data originates from web scraping or Internet of Things (IoT) sensors.</p>
<p>Data lakes address this gap by storing structured, semi-structured, and unstructured data in its native format, deferring schema definitions until the point of reading or analysis (sometimes called <em>schema-on-read</em>)<a href="#fn19" class="footnote-ref" id="fnref19" role="doc-noteref"><sup>19</sup></a>. As <a href="#tbl-storage" class="quarto-xref">Table&nbsp;1</a> shows, data lakes can handle large volumes of diverse data types. This approach grants data scientists tremendous latitude when dealing with experimental use cases or novel data types. However, data lakes also demand careful cataloging and metadata management. Without sufficient governance, these expansive repositories risk devolving into unsearchable, disorganized silos.</p>
<div class="no-row-height column-margin column-container"><div id="fn19"><p><sup>19</sup>&nbsp;<strong>Schema-on-read</strong>: A data management approach where data schema definitions are applied at the time of query or analysis rather than during initial data storage.</p></div></div><p>The examples provided in <a href="#tbl-storage" class="quarto-xref">Table&nbsp;1</a> illustrate the range of technologies available for each storage system type. For instance, MySQL represents a traditional database system, while solutions like Google BigQuery and Amazon Redshift are examples of modern, cloud-based data warehouses. For data lakes, cloud storage solutions such as Google Cloud Storage, AWS S3, and Azure Data Lake Storage are commonly used due to their scalability and flexibility.</p>
</section>
<section id="sec-data-engineering-storage-considerations-d89f" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-data-engineering-storage-considerations-d89f">Storage Considerations</h3>
<p>While traditional storage systems provide a foundation for ML workflows, the unique characteristics of machine learning workloads necessitate additional considerations. These ML-specific storage needs stem from the nature of ML development, training, and deployment processes, and addressing them is necessary for building efficient and scalable ML systems.</p>
<p>One of the primary challenges in ML storage is handling large model weights. Modern ML models, especially deep learning models, can have millions or even billions of parameters. For instance, GPT-3, a large language model, has 175 billion parameters, requiring approximately 350 GB of storage just for the model weights <span class="citation" data-cites="brown2020language">(<a href="#ref-brown2020language" role="doc-biblioref">Brown et al. 2020</a>)</span>. Storage systems need to be capable of handling these large, often dense, numerical arrays efficiently, both in terms of storage capacity and access speed. This requirement goes beyond traditional data storage and enters the realm of high-performance computing storage solutions.</p>
<div class="no-row-height column-margin column-container"><div id="ref-brown2020language" class="csl-entry" role="listitem">
Brown, Tom B., Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, et al. 2020. <span>“Language Models Are Few-Shot Learners.”</span> <em>Advances in Neural Information Processing Systems</em> 33: 1877–1901.
</div></div><p>The iterative nature of ML development introduces another critical storage consideration: versioning for both datasets and models. Unlike traditional software version control, ML versioning needs to track large binary files efficiently. As data scientists experiment with different model architectures and hyperparameters, they generate numerous versions of models and datasets. Effective storage systems for ML must provide mechanisms to track these changes, revert to previous versions, and maintain reproducibility throughout the ML lifecycle. This capability is essential not only for development efficiency but also for regulatory compliance and model auditing in production environments.</p>
<p>Distributed training, often necessary for large models or datasets, generates substantial intermediate data, including partial model updates, gradients, and checkpoints. Storage systems for ML need to handle frequent, possibly concurrent, read and write operations of these intermediate results. Moreover, they should provide low-latency access to support efficient synchronization between distributed workers. This requirement pushes storage systems to balance between high throughput for large data transfers and low latency for quick synchronization operations.</p>
<p>The diversity of data types in ML workflows presents another unique challenge. ML systems often work with a wide variety of data, ranging from structured tabular data to unstructured images, audio, and text. Storage systems need to efficiently handle this diversity, often requiring a combination of different storage technologies optimized for specific data types. For instance, a single ML project might need to store and process tabular data in a columnar format for efficient feature extraction, while also managing large volumes of image data for computer vision tasks.</p>
<p>As organizations collect more data and create more sophisticated models, storage systems need to scale seamlessly. This scalability should support not just growing data volumes, but also increasing concurrent access from multiple data scientists and ML models. Cloud-based object storage systems have emerged as a popular solution due to their virtually unlimited scalability, but they introduce their own challenges in terms of data access latency and cost management.</p>
<p>The tension between sequential read performance for training and random access for inference is another key consideration. While training on large datasets benefits from high-throughput sequential reads, many ML serving scenarios require fast random access to individual data points or features. Storage systems for ML need to balance these potentially conflicting requirements, often leading to tiered storage architectures where frequently accessed data is kept in high-performance storage while less frequently used data is moved to cheaper, higher-latency storage.</p>
<p>The choice and configuration of storage systems can significantly impact the performance, cost-effectiveness, and overall success of ML initiatives. As the field of machine learning continues to evolve, storage solutions will need to adapt to meet the changing demands of increasingly sophisticated ML workflows.</p>
</section>
<section id="sec-data-engineering-performance-factors-7465" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-data-engineering-performance-factors-7465">Performance Factors</h3>
<p>The performance of storage systems is critical in ML workflows, directly impacting the efficiency of model training, the responsiveness of inference, and the overall productivity of data science teams. Understanding and optimizing storage performance requires a focus on several key metrics and strategies tailored to ML workloads.</p>
<p>One of the primary performance metrics for ML storage is throughput, particularly for large-scale data processing and model training. High throughput is essential when ingesting and preprocessing vast datasets or when reading large batches of data during model training. For instance, distributed training of deep learning models on large datasets may require sustained read throughput of several gigabytes per second to keep GPU accelerators fully utilized.</p>
<p>Latency is another metric, especially for online inference and interactive data exploration. Low latency access to individual data points or small batches of data is vital for maintaining responsive ML services. In recommendation systems or real-time fraud detection, for example, storage systems must be able to retrieve relevant features or model parameters within milliseconds to meet strict service level agreements (SLAs).</p>
<p>The choice of file format can significantly impact both throughput and latency. Columnar storage formats such as Parquet or ORC<a href="#fn20" class="footnote-ref" id="fnref20" role="doc-noteref"><sup>20</sup></a> are particularly well-suited for ML workloads. These formats allow for efficient retrieval of specific features without reading entire records, substantially reducing I/O operations and speeding up data loading for model training and inference. For example, when training a model that only requires a subset of features from a large dataset, columnar formats can reduce data read times by an order of magnitude compared to row-based formats.</p>
<div class="no-row-height column-margin column-container"><div id="fn20"><p><sup>20</sup>&nbsp;<strong>Parquet and ORC</strong>: Columnar storage formats optimized for analytical workloads and machine learning pipelines. They store data by columns rather than rows, enabling selective retrieval of specific features and reducing I/O overhead for large datasets.</p></div></div><p>Compression is another key factor in storage performance optimization. While compression reduces storage costs and can improve read performance by reducing the amount of data transferred from disk, it also introduces computational overhead for decompression. The choice of compression algorithm often involves a trade-off between compression ratio and decompression speed. For ML workloads, fast decompression is usually prioritized over maximum compression, with algorithms like Snappy or LZ4 being popular choices.</p>
<p>Data partitioning strategies play a role in optimizing query performance for ML workloads. By intelligently partitioning data based on frequently used query parameters (such as date ranges or categorical variables), systems can dramatically improve the efficiency of data retrieval operations. For instance, in a recommendation system processing user interactions, partitioning data by user demographic attributes and time periods can significantly speed up the retrieval of relevant training data for personalized models.</p>
<p>To handle the scale of data in modern ML systems, distributed storage architectures are often employed. These systems, such as <a href="https://hadoop.apache.org/docs/r1.2.1/hdfs_design.html">HDFS (Hadoop Distributed File System)</a> or cloud-based object stores like <a href="https://aws.amazon.com/s3/">Amazon S3</a>, distribute data across multiple machines or data centers. This approach not only provides scalability but also enables parallel data access, which can substantially improve read performance for large-scale data processing tasks common in ML workflows.</p>
<p>Caching strategies are also vital for optimizing storage performance in ML systems. In-memory caching of frequently accessed data or computed features can significantly reduce latency and computational overhead. Distributed caching systems like Redis or Memcached are often used to scale caching capabilities across clusters of machines, providing low-latency access to hot data for distributed training or serving systems.</p>
<p>As ML workflows increasingly span from cloud to edge devices, storage performance considerations must extend to these distributed environments. Edge caching and intelligent data synchronization strategies become needed for maintaining performance in scenarios where network connectivity may be limited or unreliable. In the end, the goal is to create a storage infrastructure that can handle the volume and velocity of data in ML workflows while providing the low-latency access needed for responsive model training and inference.</p>
</section>
<section id="sec-data-engineering-storage-ml-lifecycle-4b39" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-data-engineering-storage-ml-lifecycle-4b39">Storage in ML Lifecycle</h3>
<p>The storage needs of machine learning systems evolve significantly across different phases of the ML lifecycle. Understanding these changing requirements is important for designing effective and efficient ML data infrastructures.</p>
<section id="sec-data-engineering-development-phase-b85e" class="level4">
<h4 class="anchored" data-anchor-id="sec-data-engineering-development-phase-b85e">Development Phase</h4>
<p>In the development phase, storage systems play a critical role in supporting exploratory data analysis and iterative model development. This stage demands flexibility and collaboration, as data scientists often work with various datasets, experiment with feature engineering techniques, and rapidly iterate on model designs to refine their approaches.</p>
<p>One of the key challenges at this stage is managing the versions of datasets used in experiments. While traditional version control systems like Git excel at tracking code changes, they fall short when dealing with large datasets. This gap has led to the emergence of specialized tools like <a href="https://dvc.org/">DVC (Data Version Control)</a>, which enable data scientists to efficiently track dataset changes, revert to previous versions, and share large files without duplication. These tools ensure that teams can maintain reproducibility and transparency throughout the iterative development process.</p>
<p>Balancing data accessibility and security further complicates the storage requirements in this phase. Data scientists require seamless access to datasets for experimentation, but organizations must simultaneously safeguard sensitive data. This tension often results in the implementation of sophisticated access control mechanisms, ensuring that datasets remain both accessible and protected. Secure data sharing systems enhance collaboration while adhering to strict organizational and regulatory requirements, enabling teams to work productively without compromising data integrity.</p>
</section>
<section id="sec-data-engineering-training-phase-5d40" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="sec-data-engineering-training-phase-5d40">Training Phase</h4>
<p>The training phase presents unique storage challenges due to the sheer volume of data processed and the computational intensity of model training. At this stage, the interplay between storage performance and computational efficiency becomes critical, as modern ML algorithms demand seamless integration between data access and processing.</p>
<p>To meet these demands, high-performance storage systems must provide the throughput required to feed data to multiple GPU or TPU accelerators simultaneously. Distributed training scenarios amplify this need, often requiring data transfer rates in the gigabytes per second range to ensure that accelerators remain fully utilized. This highlights the importance of optimizing storage for both capacity and speed.</p>
<p>Beyond data ingestion, managing intermediate results and checkpoints is another critical challenge in the training phase. Long-running training jobs frequently save intermediate model states to allow for resumption in case of interruptions. These checkpoints can grow significantly in size, especially for large-scale models, necessitating storage solutions that enable efficient saving and retrieval without impacting overall performance.</p>
<p>Complementing these systems is the concept of burst buffers<a href="#fn21" class="footnote-ref" id="fnref21" role="doc-noteref"><sup>21</sup></a>, borrowed from high-performance computing. These high-speed, temporary storage layers are particularly valuable during training, as they can absorb large, bursty I/O operations. By buffering these spikes in demand, burst buffers help smooth out performance fluctuations and reduce the load on primary storage systems, ensuring that training pipelines remain efficient and reliable.</p>
<div class="no-row-height column-margin column-container"><div id="fn21"><p><sup>21</sup>&nbsp;<strong>Burst Buffers</strong>: High-speed storage layers used to absorb large, temporary I/O demands in high-performance computing, smoothing performance during data-intensive operations.</p></div></div></section>
<section id="sec-data-engineering-deployment-phase-ecde" class="level4">
<h4 class="anchored" data-anchor-id="sec-data-engineering-deployment-phase-ecde">Deployment Phase</h4>
<p>In the deployment and serving phase, the focus shifts from high-throughput batch operations during training to low-latency, often real-time, data access. This transition highlights the need to balance conflicting requirements, where storage systems must simultaneously support responsive model serving and enable continued learning in dynamic environments.</p>
<p>Real-time inference demands storage solutions capable of extremely fast access to model parameters and relevant features. To achieve this, systems often rely on in-memory databases or sophisticated caching strategies, ensuring that predictions can be made within milliseconds. These requirements become even more challenging in edge deployment scenarios, where devices operate with limited storage resources and intermittent connectivity to central data stores.</p>
<p>Adding to this complexity is the need to manage model updates in production environments. Storage systems must facilitate smooth transitions between model versions, ensuring minimal disruption to ongoing services. Techniques like shadow deployment, where new models run alongside existing ones for validation, allow organizations to iteratively roll out updates while monitoring their performance in real-world conditions.</p>
</section>
<section id="sec-data-engineering-maintenance-phase-900e" class="level4">
<h4 class="anchored" data-anchor-id="sec-data-engineering-maintenance-phase-900e">Maintenance Phase</h4>
<p>The monitoring and maintenance phase brings its own set of storage challenges, centered on ensuring the long-term reliability and performance of ML systems. At this stage, the focus shifts to capturing and analyzing data to monitor model behavior, detect issues, and maintain compliance with regulatory requirements.</p>
<p>A critical aspect of this phase is managing data drift, where the characteristics of incoming data change over time. Storage systems must efficiently capture and store incoming data along with prediction results, enabling ongoing analysis to detect and address shifts in data distributions. This ensures that models remain accurate and aligned with their intended use cases.</p>
<p>The sheer volume of logging and monitoring data generated by high-traffic ML services introduces questions of data retention and accessibility. Organizations must balance the need to retain historical data for analysis against the cost and complexity of storing it. Strategies such as tiered storage and compression can help manage costs while ensuring that critical data remains accessible when needed.</p>
<p>Regulated industries often require immutable storage to support auditing and compliance efforts. Storage systems designed for this purpose guarantee data integrity and non-repudiability, ensuring that stored data cannot be altered or deleted. Blockchain-inspired solutions and write-once-read-many (WORM) technologies are commonly employed to meet these stringent requirements.</p>
</section>
</section>
<section id="sec-data-engineering-feature-storage-dca1" class="level3">
<h3 class="anchored" data-anchor-id="sec-data-engineering-feature-storage-dca1">Feature Storage</h3>
<p>Feature stores are a centralized repository that stores and serves pre-computed features for machine learning models, ensuring consistency between training and inference workflows. They have emerged as a critical component in the ML infrastructure stack, addressing the unique challenges of managing and serving features for machine learning models. They act as a central repository for storing, managing, and serving machine learning features, bridging the gap between data engineering and machine learning operations.</p>
<p>What makes feature stores particularly interesting is their role in solving several key challenges in ML pipelines. First, they address the problem of feature consistency between training and serving environments. In traditional ML workflows, features are often computed differently in offline (training) and online (serving) environments, leading to discrepancies that can degrade model performance. Feature stores provide a single source of truth for feature definitions, ensuring consistency across all stages of the ML lifecycle.</p>
<p>Another fascinating aspect of feature stores is their ability to promote feature reuse across different models and teams within an organization. By centralizing feature computation and storage, feature stores can significantly reduce redundant work. For instance, if multiple teams are working on different models that require similar features (e.g., customer lifetime value in a retail context), these features can be computed once and reused across projects, improving efficiency and consistency.</p>
<p>Feature stores also play a role in managing the temporal aspects of features. Many ML use cases require correct point-in-time feature values, especially in scenarios involving time-series data or where historical context is important. Feature stores typically offer time-travel capabilities, allowing data scientists to retrieve feature values as they were at any point in the past. This is crucial for training models on historical data and for ensuring consistency between training and serving environments, as illustrated in <a href="#fig-feature-store-overview" class="quarto-xref">Figure&nbsp;15</a> which shows how data flows through these systems to eventually yield a model.</p>
<div id="fig-feature-store-overview" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-env="figure" data-fig-pos="htb">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-feature-store-overview-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="6c07b7c41e5d48f52b66da7790e17275138a59b4.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-15" title="Figure&nbsp;15: Feature Store Architecture: Centralizing feature engineering and storage enables consistent feature access across model training and serving, resolving data inconsistencies and reducing redundant computation. Time-travel capabilities ensure point-in-time correctness, critical for historical analysis and maintaining model performance in dynamic environments."><img src="data_engineering_files/mediabag/6c07b7c41e5d48f52b66da7790e17275138a59b4.svg" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-feature-store-overview-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;15: <strong>Feature Store Architecture</strong>: Centralizing feature engineering and storage enables consistent feature access across model training and serving, resolving data inconsistencies and reducing redundant computation. Time-travel capabilities ensure point-in-time correctness, critical for historical analysis and maintaining model performance in dynamic environments.
</figcaption>
</figure>
</div>
<p>The performance characteristics of feature stores are particularly intriguing from a storage perspective. They need to support both high-throughput batch retrieval for model training and low-latency lookups for online inference. This often leads to hybrid architectures where feature stores maintain both an offline store (optimized for batch operations) and an online store (optimized for real-time serving). Synchronization between these stores becomes a critical consideration.</p>
<p>Feature stores also introduce interesting challenges in terms of data freshness and update strategies. Some features may need to be updated in real-time (e.g., current user session information), while others might be updated on a daily or weekly basis (e.g., aggregated customer behavior metrics). Managing these different update frequencies and ensuring that the most up-to-date features are always available for inference can be complex.</p>
<p>From a storage perspective, feature stores often leverage a combination of different storage technologies to meet their diverse requirements. This might include columnar storage formats like Parquet for the offline store, in-memory databases or key-value stores for the online store, and streaming platforms like Apache Kafka for real-time feature updates.</p>
</section>
<section id="sec-data-engineering-caching-techniques-bc7d" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-data-engineering-caching-techniques-bc7d">Caching Techniques</h3>
<p>Caching plays a role in optimizing the performance of ML systems, particularly in scenarios involving frequent data access or computation-intensive operations. In the context of machine learning, caching strategies extend beyond traditional web or database caching, addressing unique challenges posed by ML workflows.</p>
<p>One of the primary applications of caching in ML systems is in feature computation and serving. Many features used in ML models are computationally expensive to calculate, especially those involving complex aggregations or time-window operations. By caching these computed features, systems can significantly reduce latency in both training and inference scenarios. For instance, in a recommendation system, caching user embedding vectors can dramatically speed up the generation of personalized recommendations.</p>
<p>Caching strategies in ML systems often need to balance between memory usage and computation time. This trade-off is particularly evident in large-scale distributed training scenarios. Caching frequently accessed data shards or mini-batches in memory can significantly reduce I/O overhead, but it requires careful memory management to avoid out-of-memory errors, especially when working with large datasets or models.</p>
<p>Another interesting application of caching in ML systems is model caching. In scenarios where multiple versions of a model are deployed (e.g., for A/B testing or gradual rollout), caching the most frequently used model versions in memory can significantly reduce inference latency. This becomes especially important in edge computing scenarios, where storage and computation resources are limited.</p>
<p>Caching also plays a vital role in managing intermediate results in ML pipelines. For instance, in feature engineering pipelines that involve multiple transformation steps, caching intermediate results can prevent redundant computations when rerunning pipelines with minor changes. This is particularly useful during the iterative process of model development and experimentation.</p>
<p>One of the challenges in implementing effective caching strategies for ML is managing cache invalidation and updates. ML systems often deal with dynamic data where feature values or model parameters may change over time. Implementing efficient cache update mechanisms that balance between data freshness and system performance is an ongoing area of research and development.</p>
<p>Distributed caching becomes particularly important in large-scale ML systems. Technologies like Redis or Memcached are often employed to create distributed caching layers that can serve multiple training or inference nodes. These distributed caches need to handle challenges like maintaining consistency across nodes and managing failover scenarios.</p>
<p>Edge caching is another fascinating area in ML systems, especially with the growing trend of edge AI. In these scenarios, caching strategies need to account for limited storage and computational resources on edge devices, as well as potentially intermittent network connectivity. Intelligent caching strategies that prioritize the most relevant data or model components for each edge device can significantly improve the performance and reliability of edge ML systems.</p>
<p>Lastly, the concept of semantic caching<a href="#fn22" class="footnote-ref" id="fnref22" role="doc-noteref"><sup>22</sup></a> is gaining traction in ML systems. Unlike traditional caching that operates on exact matches, semantic caching attempts to reuse cached results for semantically similar queries. This can be particularly useful in ML systems where slight variations in input may not significantly change the output, potentially leading to substantial performance improvements.</p>
<div class="no-row-height column-margin column-container"><div id="fn22"><p><sup>22</sup>&nbsp;<strong>Semantic Caching</strong>: A caching technique that reuses results of previous computations for semantically similar queries, reducing redundancy in data processing.</p></div></div></section>
<section id="sec-data-engineering-data-access-patterns-b236" class="level3">
<h3 class="anchored" data-anchor-id="sec-data-engineering-data-access-patterns-b236">Data Access Patterns</h3>
<p>Understanding the access patterns in ML systems is useful for designing efficient storage solutions and optimizing the overall system performance. ML workloads exhibit distinct data access patterns that often differ significantly from traditional database or analytics workloads.</p>
<p>One of the most prominent access patterns in ML systems is sequential reading of large datasets during model training. Unlike transactional systems that typically access small amounts of data randomly, ML training often involves reading entire datasets multiple times (epochs) in a sequential manner. This pattern is particularly evident in deep learning tasks, where large volumes of data are fed through neural networks repeatedly. Storage systems optimized for high-throughput sequential reads, such as distributed file systems or object stores, are well-suited for this access pattern.</p>
<p>However, the sequential read pattern is often combined with random shuffling between epochs to prevent overfitting and improve model generalization. This introduces an interesting challenge for storage systems, as they need to efficiently support both sequential and random access patterns, often within the same training job.</p>
<p>In contrast to the bulk sequential reads common in training, inference workloads often require fast random access to specific data points or features. For example, a recommendation system might need to quickly retrieve user and item features for real-time personalization. This necessitates storage solutions with low-latency random read capabilities, often leading to the use of in-memory databases or caching layers.</p>
<p>Feature stores, which we discussed earlier, introduce their own unique access patterns. They typically need to support both high-throughput batch reads for offline training and low-latency point lookups for online inference. This dual-nature access pattern often leads to the implementation of separate offline and online storage layers, each optimized for its specific access pattern.</p>
<p>Time-series data, common in many ML applications such as financial forecasting or IoT analytics, presents another interesting access pattern. These workloads often involve reading contiguous blocks of time-ordered data, but may also require efficient retrieval of specific time ranges or periodic patterns. Specialized time-series databases or carefully designed partitioning schemes in general-purpose databases are often employed to optimize these access patterns.</p>
<p>Another important consideration is the write access pattern in ML systems. While training workloads are often read-heavy, there are scenarios that involve significant write operations. For instance, continual learning systems may frequently update model parameters, and online learning systems may need to efficiently append new training examples to existing datasets.</p>
<p>Understanding these diverse access patterns is helpful in designing and optimizing storage systems for ML workloads. It often leads to hybrid storage architectures that combine different technologies to address various access patterns efficiently. For example, a system might use object storage for large-scale sequential reads during training, in-memory databases for low-latency random access during inference, and specialized time-series storage for temporal data analysis.</p>
<p>As ML systems continue to evolve, new access patterns are likely to emerge, driving further innovation in storage technologies and architectures. The challenge lies in creating flexible, scalable storage solutions that can efficiently support the diverse and often unpredictable access patterns of modern ML workloads.</p>
</section>
<section id="sec-data-engineering-continuing-kws-example-dd18" class="level3">
<h3 class="anchored" data-anchor-id="sec-data-engineering-continuing-kws-example-dd18">Continuing the KWS Example</h3>
<p>During development and training, KWS systems must efficiently store and manage large collections of audio data. This includes raw audio recordings from various sources (crowd-sourced, synthetic, and real-world captures), processed features (like spectrograms or MFCCs), and model checkpoints. A typical architecture might use a data lake for raw audio files, allowing flexible storage of diverse audio formats, while processed features are stored in a more structured data warehouse for efficient access during training.</p>
<p>KWS systems benefit significantly from feature stores, particularly for managing pre-computed audio features. For example, commonly used spectrogram representations or audio embeddings can be computed once and stored for reuse across different experiments or model versions. The feature store must handle both batch access for training and real-time access for inference, often implementing a dual storage architecture, which includes an offline store for training data and an online store for low-latency inference.</p>
<p>In production, KWS systems require careful consideration of edge storage requirements. The models must be compact enough to fit on resource-constrained devices while maintaining quick access to necessary parameters for real-time wake word detection. This often involves optimized storage formats and careful caching strategies to balance between memory usage and inference speed.</p>
<div id="quiz-question-sec-data-engineering-data-storage-6651" class="callout callout-quiz-question">
<details class="callout-quiz-question fbx-default closebutton"><summary><strong>Self-Check: Question 1.7</strong></summary><div>
<ol type="1">
<li><p>Which storage system is best suited for handling large volumes of unstructured data in machine learning projects?</p>
<ol type="a">
<li>Conventional Database</li>
<li>Data Warehouse</li>
<li>Data Lake</li>
<li>In-memory Database</li>
</ol></li>
<li><p>Explain why versioning is a critical consideration for storage systems in machine learning workflows.</p></li>
<li><p>True or False: Data warehouses are the most suitable storage systems for ML projects that require handling rapidly changing data formats.</p></li>
<li><p>In machine learning systems, ____ is a key metric for storage performance, especially for online inference scenarios requiring fast access to data.</p></li>
</ol>
<p><a href="#quiz-answer-sec-data-engineering-data-storage-6651" class="question-label">See Answers →</a></p>
</div></details>
</div>
</section>
</section>
<section id="sec-data-engineering-data-governance-6f5e" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="sec-data-engineering-data-governance-6f5e">Data Governance</h2>
<p>Data governance is a significant component in the development and deployment of ML systems. It encompasses a set of practices and policies that ensure data is accurate, secure, compliant, and ethically used throughout the ML lifecycle. As ML systems become increasingly integral to decision-making processes across various domains, the importance of robust data governance has grown significantly.</p>
<p>One of the central challenges of data governance is addressing the unique complexities posed by ML workflows. These workflows often involve opaque processes, such as feature engineering and model training, which can obscure how data is being used. As shown in <a href="#fig-data-governance-pillars" class="quarto-xref">Figure&nbsp;16</a>, governance practices aim to tackle these issues by focusing on maintaining data privacy, ensuring fairness, and providing transparency in decision-making processes. These practices go beyond traditional data management to address the evolving needs of ML systems.</p>
<div id="fig-data-governance-pillars" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-env="figure" data-fig-pos="htb">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-data-governance-pillars-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="9317061a5de6846f3ef7bfa46e760b22f51b49e4.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-16" title="Figure&nbsp;16: Data Governance Pillars: Robust data governance establishes ethical and reliable machine learning systems by prioritizing privacy, fairness, transparency, and accountability throughout the data lifecycle. These interconnected pillars address unique challenges in ML workflows, ensuring responsible data usage and auditable decision-making processes."><img src="data_engineering_files/mediabag/9317061a5de6846f3ef7bfa46e760b22f51b49e4.svg" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-data-governance-pillars-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;16: <strong>Data Governance Pillars</strong>: Robust data governance establishes ethical and reliable machine learning systems by prioritizing privacy, fairness, transparency, and accountability throughout the data lifecycle. These interconnected pillars address unique challenges in ML workflows, ensuring responsible data usage and auditable decision-making processes.
</figcaption>
</figure>
</div>
<p>Security and access control form an essential aspect of data governance. Implementing measures to protect data from unauthorized access or breaches is critical in ML systems, which often deal with sensitive or proprietary information. For instance, a healthcare application may require granular access controls to ensure that only authorized personnel can view patient data. Encrypting data both at rest and in transit is another common approach to safeguarding information while enabling secure collaboration among ML teams.</p>
<p>Privacy protection is another key pillar of data governance. As ML models often rely on large-scale datasets, there is a risk of infringing on individual privacy rights. Techniques such as differential privacy<a href="#fn23" class="footnote-ref" id="fnref23" role="doc-noteref"><sup>23</sup></a> can address this concern by adding carefully calibrated noise to the data. This ensures that individual identities are protected while preserving the statistical patterns necessary for model training. These techniques allow ML systems to benefit from data-driven insights without compromising ethical considerations <span class="citation" data-cites="dwork2008differential">(<a href="#ref-dwork2008differential" role="doc-biblioref">Dwork, n.d.</a>)</span>, which we will learn more about in the Responsible AI chapter.</p>
<div class="no-row-height column-margin column-container"><div id="fn23"><p><sup>23</sup>&nbsp;<strong>Differential Privacy</strong>: A technique that preserves privacy by adding random noise to outputs, ensuring individual data points remain unidentifiable.</p></div><div id="ref-dwork2008differential" class="csl-entry" role="listitem">
Dwork, Cynthia. n.d. <span>“Differential Privacy: A Survey of Results.”</span> In <em>Theory and Applications of Models of Computation</em>, 1–19. Springer Berlin Heidelberg. <a href="https://doi.org/10.1007/978-3-540-79228-4\_1">https://doi.org/10.1007/978-3-540-79228-4\_1</a>.
</div><div id="ref-wachter2017counterfactual" class="csl-entry" role="listitem">
Wachter, Sandra, Brent Mittelstadt, and Chris Russell. 2017. <span>“Counterfactual Explanations Without Opening the Black Box: Automated Decisions and the GDPR.”</span> <em>SSRN Electronic Journal</em> 31: 841. <a href="https://doi.org/10.2139/ssrn.3063289">https://doi.org/10.2139/ssrn.3063289</a>.
</div></div><p>Regulatory compliance is a critical area where data governance plays a central role. Laws such as the GDPR in Europe and the HIPAA in the United States impose strict requirements on data handling. Compliance with these regulations often involves implementing features like the ability to delete data upon request or providing individuals with copies of their data, and a “right to explanation” on decisions made by algorithms <span class="citation" data-cites="wachter2017counterfactual">(<a href="#ref-wachter2017counterfactual" role="doc-biblioref">Wachter, Mittelstadt, and Russell 2017</a>)</span>. These measures not only protect individuals but also ensure organizations avoid legal and reputational risks.</p>
<div id="fig-data-card" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-env="figure" data-fig-pos="t!">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-data-card-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="46a3789f28bd896ab313114f2d298c046bb537fb.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-17" title="Figure&nbsp;17: Data Governance Documentation: Data cards standardize critical dataset information, enabling transparency and accountability required for regulatory compliance with laws like GDPR and HIPAA. By providing a structured overview of dataset characteristics, intended uses, and potential risks, data cards facilitate responsible AI practices and support data subject rights."><img src="data_engineering_files/mediabag/46a3789f28bd896ab313114f2d298c046bb537fb.svg" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-data-card-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;17: <strong>Data Governance Documentation</strong>: Data cards standardize critical dataset information, enabling transparency and accountability required for regulatory compliance with laws like GDPR and HIPAA. By providing a structured overview of dataset characteristics, intended uses, and potential risks, data cards facilitate responsible AI practices and support data subject rights.
</figcaption>
</figure>
</div>
<p>Documentation and metadata management, which are often less discussed, are just as important for transparency and reproducibility in ML systems. Clear records of data lineage, including how data flows and transforms throughout the ML pipeline, are essential for accountability. Standardized documentation frameworks, such as Data Cards proposed by <span class="citation" data-cites="pushkarna2022data">Pushkarna, Zaldivar, and Kjartansson (<a href="#ref-pushkarna2022data" role="doc-biblioref">2022</a>)</span>, offer a structured way to document the characteristics, limitations, and potential biases of datasets. For example, as shown in <a href="#fig-data-card" class="quarto-xref">Figure&nbsp;17</a>, the <a href="https://storage.googleapis.com/openimages/web/extended.html">Open Images Extended, More Inclusively Annotated People (MIAP) dataset,</a> uses a data card to provide detailed information about its motivations, intended use cases, and known risks. This type of documentation enables developers to evaluate datasets effectively and promotes responsible use.</p>
<div class="no-row-height column-margin column-container"><div id="ref-pushkarna2022data" class="csl-entry" role="listitem">
Pushkarna, Mahima, Andrew Zaldivar, and Oddur Kjartansson. 2022. <span>“Data Cards: Purposeful and Transparent Dataset Documentation for Responsible AI.”</span> In <em>2022 ACM Conference on Fairness Accountability and Transparency</em>, 1776–826. ACM. <a href="https://doi.org/10.1145/3531146.3533231">https://doi.org/10.1145/3531146.3533231</a>.
</div></div><p>Audit trails are another important component of data governance. These detailed logs track data access and usage throughout the lifecycle of ML models, from collection to deployment. Comprehensive audit trails are invaluable for troubleshooting and accountability, especially in cases of data breaches or unexpected model behavior. They help organizations understand what actions were taken and why, providing a clear path for resolving issues and ensuring compliance.</p>
<p>Consider a hypothetical ML system designed to predict patient outcomes in a hospital. Such a system would need to address several governance challenges. It would need to ensure that patient data is securely stored and accessed only by authorized personnel, with privacy-preserving techniques in place to protect individual identities. The system would also need to comply with healthcare regulations governing the use of patient data, including detailed documentation of how data is processed and transformed. Comprehensive audit logs would be necessary to track data usage and ensure accountability.</p>
<p>As ML systems grow more complex and influential, the challenges of data governance will continue to evolve. Emerging trends, such as blockchain-inspired technologies for tamper-evident logs and automated governance tools, offer promising solutions for real-time monitoring and issue detection. By adopting robust data governance practices, including tools like Data Cards, organizations can build ML systems that are transparent, ethical, and trustworthy.</p>
<div id="quiz-question-sec-data-engineering-data-governance-6f5e" class="callout callout-quiz-question">
<details class="callout-quiz-question fbx-default closebutton"><summary><strong>Self-Check: Question 1.8</strong></summary><div>
<ol type="1">
<li><p>Which of the following is a key component of data governance that ensures transparency and accountability in ML systems?</p>
<ol type="a">
<li>Data encryption</li>
<li>Audit trails</li>
<li>Feature engineering</li>
<li>Data augmentation</li>
</ol></li>
<li><p>True or False: Differential privacy ensures that individual data points remain unidentifiable by adding random noise to the outputs.</p></li>
<li><p>Explain why regulatory compliance is critical in the data governance of ML systems, particularly in sectors like healthcare.</p></li>
<li><p>In ML systems, ____ management involves maintaining clear records of data lineage, including how data flows and transforms throughout the pipeline.</p></li>
<li><p>Discuss how data governance practices can enhance the ethical use of ML systems.</p></li>
</ol>
<p><a href="#quiz-answer-sec-data-engineering-data-governance-6f5e" class="question-label">See Answers →</a></p>
</div></details>
</div>
</section>
<section id="sec-data-engineering-summary-286b" class="level2">
<h2 class="anchored" data-anchor-id="sec-data-engineering-summary-286b">Summary</h2>
<p>Data engineering is the backbone of any successful ML system. By thoughtfully defining problems, designing robust pipelines, and practicing rigorous data governance, teams establish a foundation that directly influences model performance, reliability, and ethical standing. Effective data acquisition strategies, whether by utilizing existing datasets, employing web scraping techniques, or engaging in crowdsourcing, must balance the realities of domain constraints, privacy obligations, and labeling complexities. Likewise, decisions around data ingestion (batch or streaming) and transformation (ETL or ELT) affect both cost and throughput, with monitoring and observability essential to detect shifting data quality.</p>
<p>Throughout this chapter, we saw how critical it is to prepare data well in advance of modeling. Data labeling emerges as a particularly delicate phase: it involves human effort, requires strong quality control practices, and has ethical ramifications. Storage choices, such as relational databases, data warehouses, data lakes, or specialized systems, must align with both the volume and velocity of ML workloads. Feature stores and caching strategies support efficient retrieval across training and serving pipelines, while good data governance ensures adherence to legal regulations, protects privacy, and maintains stakeholder trust.</p>
<p>All these elements interlock to create an ecosystem that reliably supplies ML models with the high-quality data they need. When done well, data engineering empowers teams to iterate faster, confidently deploy new features, and build systems capable of adapting to real-world complexity. The next chapters will build on these foundations, exploring how optimized training, robust model operations, and security considerations together form a holistic approach to delivering AI solutions that perform reliably and responsibly at scale.</p>


</section>
<section id="self-check-answers" class="level2">
<h2 class="anchored" data-anchor-id="self-check-answers">Self-Check Answers</h2>
<div id="quiz-answer-sec-data-engineering-problem-definition-1064" class="callout callout-quiz-answer">
<details class="callout-quiz-answer fbx-answer closebutton"><summary><strong>Self-Check: Answer 1.1</strong></summary><div>
<ol type="1">
<li><p><strong>Why is it crucial to define the problem clearly before beginning data collection in ML projects?</strong></p>
<ol type="a">
<li>To ensure data collection aligns with project objectives</li>
<li>To minimize the cost of data storage</li>
<li>To avoid the need for model evaluation</li>
<li>To increase the speed of model training</li>
</ol>
<p><em>Answer</em>: The correct answer is A. Defining the problem clearly ensures that data collection aligns with project objectives, which helps in collecting relevant and high-quality data, ultimately leading to better model performance and reduced risk of data cascades.</p>
<p><em>Learning Objective</em>: Understand the importance of clear problem definition in guiding data collection and ensuring project success.</p></li>
<li><p><strong>True or False: Data cascades refer to the positive effects of high-quality data on ML model performance.</strong></p>
<p><em>Answer</em>: False. Data cascades refer to the negative consequences of poor data quality, where errors in data collection can compound and lead to flawed predictions and other downstream issues.</p>
<p><em>Learning Objective</em>: Recognize the concept of data cascades and their impact on ML systems.</p></li>
<li><p><strong>Explain how the concept of ‘data cascades’ influences the design and implementation of data pipelines in ML systems.</strong></p>
<p><em>Answer</em>: Data cascades highlight the need for robust data pipelines that can detect and correct errors early in the data lifecycle. This ensures high data quality, preventing compounding errors that could lead to flawed model predictions and costly project failures.</p>
<p><em>Learning Objective</em>: Analyze the influence of data cascades on data pipeline design and implementation.</p></li>
<li><p><strong>Arrange the following steps in the correct order for defining a problem in ML projects: [Set clear objectives, Identify and clearly state the problem definition, Perform data collection, Establish success benchmarks, Understand end-user engagement/use].</strong></p>
<p><em>Answer</em>: 1. Identify and clearly state the problem definition, 2. Set clear objectives, 3. Establish success benchmarks, 4. Understand end-user engagement/use, 5. Perform data collection. This sequence ensures a structured approach to problem definition, aligning data collection with project goals.</p>
<p><em>Learning Objective</em>: Apply the structured steps of problem definition in ML projects.</p></li>
</ol>
<p><a href="#quiz-question-sec-data-engineering-problem-definition-1064" class="answer-label">← Back to Questions</a></p>
</div></details>
</div>
<div id="quiz-answer-sec-data-engineering-pipeline-basics-053a" class="callout callout-quiz-answer">
<details class="callout-quiz-answer fbx-answer closebutton"><summary><strong>Self-Check: Answer 1.2</strong></summary><div>
<ol type="1">
<li><p><strong>Which of the following components is responsible for transforming raw data into a format suitable for ML training in a data pipeline?</strong></p>
<ol type="a">
<li>Data Ingestion</li>
<li>Processing Layer</li>
<li>Storage Layer</li>
<li>Data Sources</li>
</ol>
<p><em>Answer</em>: The correct answer is B. The Processing Layer is responsible for transforming raw data into a format suitable for ML training, including data validation, transformation, and feature engineering.</p>
<p><em>Learning Objective</em>: Identify the role of the processing layer in data pipelines.</p></li>
<li><p><strong>True or False: In a data pipeline, the storage layer is only used for storing raw data before it is processed.</strong></p>
<p><em>Answer</em>: False. The storage layer is used not only for storing raw data but also for storing intermediate and processed data that can be used for training and other purposes.</p>
<p><em>Learning Objective</em>: Understand the multiple roles of the storage layer in data pipelines.</p></li>
<li><p><strong>Explain why data quality checks are critical in the processing layer of a data pipeline.</strong></p>
<p><em>Answer</em>: Data quality checks are critical in the processing layer to ensure that the data used for ML training is accurate, consistent, and reliable. Poor data quality can lead to incorrect model predictions and affect the overall performance of the ML system.</p>
<p><em>Learning Objective</em>: Analyze the importance of data quality checks in the processing layer.</p></li>
<li><p><strong>Arrange the following components of a data pipeline in the correct order of data flow: [Processing Layer, Data Sources, Storage Layer, Data Ingestion].</strong></p>
<p><em>Answer</em>: Data Sources, Data Ingestion, Storage Layer, Processing Layer. Data flows from sources through ingestion into storage and is then processed.</p>
<p><em>Learning Objective</em>: Understand the sequential flow of data through the components of a data pipeline.</p></li>
</ol>
<p><a href="#quiz-question-sec-data-engineering-pipeline-basics-053a" class="answer-label">← Back to Questions</a></p>
</div></details>
</div>
<div id="quiz-answer-sec-data-engineering-data-sources-50f8" class="callout callout-quiz-answer">
<details class="callout-quiz-answer fbx-answer closebutton"><summary><strong>Self-Check: Answer 1.3</strong></summary><div>
<ol type="1">
<li><p><strong>Which of the following is a primary advantage of using existing datasets like ImageNet for ML system development?</strong></p>
<ol type="a">
<li>They provide real-time data updates.</li>
<li>They offer cost efficiency and established benchmarks.</li>
<li>They eliminate the need for data preprocessing.</li>
<li>They ensure complete alignment with real-world deployment conditions.</li>
</ol>
<p><em>Answer</em>: The correct answer is B. Existing datasets like ImageNet offer cost efficiency and established benchmarks, allowing for immediate experimentation and prototyping without the need for extensive data collection and preprocessing.</p>
<p><em>Learning Objective</em>: Understand the advantages of using existing datasets in ML system development.</p></li>
<li><p><strong>Explain why web scraping is a valuable method for data collection in ML systems, and identify one major challenge associated with it.</strong></p>
<p><em>Answer</em>: Web scraping is valuable because it allows for the collection of large-scale, customized datasets tailored to specific needs, particularly in domains where pre-existing datasets are insufficient. A major challenge is the legal and ethical constraints, as not all websites permit scraping, and violating these restrictions can have serious consequences.</p>
<p><em>Learning Objective</em>: Analyze the benefits and challenges of web scraping as a data collection method in ML systems.</p></li>
<li><p><strong>Crowdsourcing for data collection in ML systems can introduce a wide range of perspectives and cultural insights, enriching datasets and improving models’ ability to generalize across populations. However, a primary concern with crowdsourcing is ensuring ____.</strong></p>
<p><em>Answer</em>: quality control. Ensuring quality control is crucial because variability in contributors’ expertise and attention can lead to inconsistent or inaccurate annotations, affecting the reliability of the data.</p>
<p><em>Learning Objective</em>: Identify the primary concerns associated with crowdsourcing in ML data collection.</p></li>
<li><p><strong>True or False: Synthetic data generation can fully replace real-world data in training ML systems without any limitations.</strong></p>
<p><em>Answer</em>: False. While synthetic data generation can supplement or replace real-world data in certain scenarios, it has limitations such as potential biases, inaccuracies, and the need for validation against real-world benchmarks to ensure reliability.</p>
<p><em>Learning Objective</em>: Evaluate the role and limitations of synthetic data generation in ML systems.</p></li>
</ol>
<p><a href="#quiz-question-sec-data-engineering-data-sources-50f8" class="answer-label">← Back to Questions</a></p>
</div></details>
</div>
<div id="quiz-answer-sec-data-engineering-data-ingestion-81f3" class="callout callout-quiz-answer">
<details class="callout-quiz-answer fbx-answer closebutton"><summary><strong>Self-Check: Answer 1.4</strong></summary><div>
<ol type="1">
<li><p><strong>Which data ingestion pattern is most appropriate for applications requiring immediate data processing and response to events as they occur?</strong></p>
<ol type="a">
<li>Batch ingestion</li>
<li>Stream ingestion</li>
<li>Hybrid ingestion</li>
<li>Delayed ingestion</li>
</ol>
<p><em>Answer</em>: The correct answer is B. Stream ingestion is used for real-time data processing, which is essential for applications that need immediate responses to events, such as real-time fraud detection.</p>
<p><em>Learning Objective</em>: Understand the use cases and characteristics of stream ingestion in ML systems.</p></li>
<li><p><strong>Explain the primary tradeoff between ETL and ELT approaches in the context of ML systems.</strong></p>
<p><em>Answer</em>: ETL transforms data before loading, ensuring consistency but lacking flexibility for schema changes. ELT loads raw data first, allowing agile transformations but demanding more from storage and query systems. This tradeoff impacts how adaptable an ML system is to changing data requirements.</p>
<p><em>Learning Objective</em>: Analyze the tradeoffs between ETL and ELT approaches in ML data pipelines.</p></li>
<li><p><strong>True or False: In a hybrid data ingestion approach, both batch and stream ingestion methods are used to handle different data velocities and use cases.</strong></p>
<p><em>Answer</em>: True. A hybrid approach combines batch and stream ingestion to process both historical and real-time data, providing a comprehensive data landscape for ML systems.</p>
<p><em>Learning Objective</em>: Understand the benefits of using a hybrid data ingestion approach in ML systems.</p></li>
<li><p><strong>In data ingestion, a ____ is used to store unprocessed messages for later analysis or reprocessing, helping to manage errors effectively.</strong></p>
<p><em>Answer</em>: dead letter queue. Dead letter queues store data that fails processing, allowing for later analysis and potential reprocessing, which is crucial for error management in ML systems.</p>
<p><em>Learning Objective</em>: Identify the role of dead letter queues in error management during data ingestion.</p></li>
<li><p><strong>Describe how error management strategies, such as graceful degradation and retry logic, contribute to the reliability of ML systems during data ingestion.</strong></p>
<p><em>Answer</em>: Error management strategies like graceful degradation and retry logic ensure ML systems can continue operating despite data ingestion challenges. Graceful degradation allows systems to function with reduced capabilities, while retry logic handles transient errors, maintaining data flow and system reliability.</p>
<p><em>Learning Objective</em>: Apply error management strategies to enhance the reliability of ML systems during data ingestion.</p></li>
</ol>
<p><a href="#quiz-question-sec-data-engineering-data-ingestion-81f3" class="answer-label">← Back to Questions</a></p>
</div></details>
</div>
<div id="quiz-answer-sec-data-engineering-data-processing-60bd" class="callout callout-quiz-answer">
<details class="callout-quiz-answer fbx-answer closebutton"><summary><strong>Self-Check: Answer 1.5</strong></summary><div>
<ol type="1">
<li><p><strong>Which of the following is a key advantage of using ELT over ETL in data processing for ML systems?</strong></p>
<ol type="a">
<li>Allows for front-loading data cleaning and transformation</li>
<li>Provides flexibility in processing unstructured data</li>
<li>Reduces storage requirements for raw data</li>
<li>Ensures data quality before loading into the target system</li>
</ol>
<p><em>Answer</em>: The correct answer is B. ELT provides flexibility in processing unstructured data by allowing transformations to occur after loading, which is beneficial when dealing with data lakes or when transformations are not predefined.</p>
<p><em>Learning Objective</em>: Understand the advantages of ELT in handling unstructured data within ML systems.</p></li>
<li><p><strong>Explain why modularity and version control are important considerations in processing pipeline design for ML systems.</strong></p>
<p><em>Answer</em>: Modularity allows for easy updates and maintenance of individual processing steps, while version control ensures that changes in data processing can be tracked and correlated with changes in model performance. This improves reliability and reproducibility in ML systems.</p>
<p><em>Learning Objective</em>: Analyze the importance of modularity and version control in designing robust data processing pipelines.</p></li>
<li><p><strong>In data processing, ____ involves converting raw data into a format more suitable for analysis and modeling, often including tasks like normalization and encoding.</strong></p>
<p><em>Answer</em>: transformation. Data transformation involves converting raw data into a format more suitable for analysis and modeling, often including tasks like normalization and encoding.</p>
<p><em>Learning Objective</em>: Recall the key activities involved in data transformation for ML systems.</p></li>
<li><p><strong>True or False: In a KWS system, feature engineering is primarily focused on extracting characteristics that help distinguish wake words from background speech.</strong></p>
<p><em>Answer</em>: True. In a KWS system, feature engineering focuses on extracting characteristics like tonal variations and speech energy patterns to distinguish wake words from background speech, enhancing model accuracy.</p>
<p><em>Learning Objective</em>: Understand the role of feature engineering in enhancing KWS system performance.</p></li>
<li><p><strong>Given a dataset with 10,000 audio recordings, each requiring 5 seconds for processing, calculate the total processing time if the system can handle 50 recordings concurrently. What does this imply for scalability?</strong></p>
<p><em>Answer</em>: Total processing time = (10,000 recordings / 50 concurrent recordings) * 5 seconds = 1,000 seconds. This implies that the system’s scalability is limited by its concurrent processing capacity, highlighting the need for efficient parallel processing techniques in large-scale ML systems.</p>
<p><em>Learning Objective</em>: Calculate and analyze the implications of processing time on the scalability of ML systems.</p></li>
</ol>
<p><a href="#quiz-question-sec-data-engineering-data-processing-60bd" class="answer-label">← Back to Questions</a></p>
</div></details>
</div>
<div id="quiz-answer-sec-data-engineering-data-labeling-044f" class="callout callout-quiz-answer">
<details class="callout-quiz-answer fbx-answer closebutton"><summary><strong>Self-Check: Answer 1.6</strong></summary><div>
<ol type="1">
<li><p><strong>Which type of label requires the most storage and processing resources in a machine learning system?</strong></p>
<ol type="a">
<li>Classification labels</li>
<li>Bounding boxes</li>
<li>Segmentation maps</li>
<li>Metadata labels</li>
</ol>
<p><em>Answer</em>: The correct answer is C. Segmentation maps require the most storage and processing resources because they classify objects at the pixel level, which significantly increases data volume and complexity compared to other label types.</p>
<p><em>Learning Objective</em>: Understand the resource implications of different label types in ML systems.</p></li>
<li><p><strong>Explain how the choice of annotation technique impacts the system architecture of a machine learning system.</strong></p>
<p><em>Answer</em>: The choice of annotation technique affects system architecture by dictating the infrastructure requirements. Expert-only systems may need centralized architectures with secure data access, while crowdsourcing requires distributed systems for concurrent annotators. Automated methods demand high compute resources and caching. These choices influence data flow, storage, and processing capabilities.</p>
<p><em>Learning Objective</em>: Analyze the impact of annotation techniques on system architecture and infrastructure requirements.</p></li>
<li><p><strong>True or False: Automated labeling systems eliminate the need for human oversight in machine learning data pipelines.</strong></p>
<p><em>Answer</em>: False. Automated labeling systems reduce manual effort but do not eliminate the need for human oversight. Human review is essential for quality control, especially in complex or ambiguous cases, to ensure label accuracy and address potential biases.</p>
<p><em>Learning Objective</em>: Evaluate the role of human oversight in automated labeling systems.</p></li>
<li><p><strong>In data labeling, ____ labeling involves using AI models to generate preliminary labels that are later reviewed by humans.</strong></p>
<p><em>Answer</em>: pre-annotation. Pre-annotation uses AI models to create initial labels, which humans review and correct, combining the efficiency of automation with human judgment for accuracy.</p>
<p><em>Learning Objective</em>: Understand the concept and application of pre-annotation in data labeling workflows.</p></li>
<li><p><strong>Discuss the challenges of maintaining label quality at scale in production ML systems.</strong></p>
<p><em>Answer</em>: Maintaining label quality at scale involves addressing label uncertainty, managing consensus labeling processes, and implementing robust quality control systems. Challenges include processing large data volumes without bottlenecks, ensuring inter-annotator agreement, and monitoring for biases. Systems must adapt to evolving requirements and maintain high standards across diverse datasets.</p>
<p><em>Learning Objective</em>: Identify and analyze the challenges of maintaining label quality in large-scale ML systems.</p></li>
</ol>
<p><a href="#quiz-question-sec-data-engineering-data-labeling-044f" class="answer-label">← Back to Questions</a></p>
</div></details>
</div>
<div id="quiz-answer-sec-data-engineering-data-storage-6651" class="callout callout-quiz-answer">
<details class="callout-quiz-answer fbx-answer closebutton"><summary><strong>Self-Check: Answer 1.7</strong></summary><div>
<ol type="1">
<li><p><strong>Which storage system is best suited for handling large volumes of unstructured data in machine learning projects?</strong></p>
<ol type="a">
<li>Conventional Database</li>
<li>Data Warehouse</li>
<li>Data Lake</li>
<li>In-memory Database</li>
</ol>
<p><em>Answer</em>: The correct answer is C. Data Lake. Data lakes are optimized for storing large volumes of structured, semi-structured, and unstructured data, making them ideal for ML projects that involve diverse data types.</p>
<p><em>Learning Objective</em>: Understand the suitability of different storage systems for handling diverse data types in ML projects.</p></li>
<li><p><strong>Explain why versioning is a critical consideration for storage systems in machine learning workflows.</strong></p>
<p><em>Answer</em>: Versioning is critical because it allows data scientists to track changes in datasets and models, ensuring reproducibility and compliance. It supports iterative development by enabling rollback to previous versions and auditing of model changes.</p>
<p><em>Learning Objective</em>: Analyze the importance of versioning in maintaining reproducibility and compliance in ML workflows.</p></li>
<li><p><strong>True or False: Data warehouses are the most suitable storage systems for ML projects that require handling rapidly changing data formats.</strong></p>
<p><em>Answer</em>: False. Data warehouses are optimized for structured data and may not accommodate rapidly changing data formats, which are better handled by data lakes that support schema-on-read.</p>
<p><em>Learning Objective</em>: Identify the limitations of data warehouses in handling rapidly changing data formats in ML projects.</p></li>
<li><p><strong>In machine learning systems, ____ is a key metric for storage performance, especially for online inference scenarios requiring fast access to data.</strong></p>
<p><em>Answer</em>: latency. Latency is crucial for online inference as it affects the speed at which data can be accessed and processed, impacting the responsiveness of ML services.</p>
<p><em>Learning Objective</em>: Understand the significance of latency as a performance metric in ML storage systems.</p></li>
</ol>
<p><a href="#quiz-question-sec-data-engineering-data-storage-6651" class="answer-label">← Back to Questions</a></p>
</div></details>
</div>
<div id="quiz-answer-sec-data-engineering-data-governance-6f5e" class="callout callout-quiz-answer">
<details class="callout-quiz-answer fbx-answer closebutton"><summary><strong>Self-Check: Answer 1.8</strong></summary><div>
<ol type="1">
<li><p><strong>Which of the following is a key component of data governance that ensures transparency and accountability in ML systems?</strong></p>
<ol type="a">
<li>Data encryption</li>
<li>Audit trails</li>
<li>Feature engineering</li>
<li>Data augmentation</li>
</ol>
<p><em>Answer</em>: The correct answer is B. Audit trails are essential for tracking data access and usage, ensuring transparency and accountability in ML systems.</p>
<p><em>Learning Objective</em>: Understand the role of audit trails in data governance for ML systems.</p></li>
<li><p><strong>True or False: Differential privacy ensures that individual data points remain unidentifiable by adding random noise to the outputs.</strong></p>
<p><em>Answer</em>: True. Differential privacy is designed to protect individual identities by adding noise to data outputs, preserving privacy while maintaining statistical integrity.</p>
<p><em>Learning Objective</em>: Explain the concept of differential privacy and its importance in data governance.</p></li>
<li><p><strong>Explain why regulatory compliance is critical in the data governance of ML systems, particularly in sectors like healthcare.</strong></p>
<p><em>Answer</em>: Regulatory compliance is crucial in ML systems to ensure that data handling adheres to laws such as GDPR and HIPAA. This compliance protects individual rights and prevents legal and reputational risks for organizations, especially in sensitive sectors like healthcare where data privacy is paramount.</p>
<p><em>Learning Objective</em>: Analyze the importance of regulatory compliance in data governance for ML systems.</p></li>
<li><p><strong>In ML systems, ____ management involves maintaining clear records of data lineage, including how data flows and transforms throughout the pipeline.</strong></p>
<p><em>Answer</em>: metadata. Metadata management is crucial for accountability and transparency in ML systems, ensuring that data transformations are well-documented.</p>
<p><em>Learning Objective</em>: Understand the role of metadata management in data governance.</p></li>
<li><p><strong>Discuss how data governance practices can enhance the ethical use of ML systems.</strong></p>
<p><em>Answer</em>: Data governance practices, such as privacy protection, compliance with regulations, and transparency through documentation, ensure ethical use of ML systems by safeguarding individual rights and promoting trust. These practices help prevent misuse of data and ensure that ML systems operate fairly and responsibly.</p>
<p><em>Learning Objective</em>: Evaluate how data governance contributes to the ethical deployment of ML systems.</p></li>
</ol>
<p><a href="#quiz-question-sec-data-engineering-data-governance-6f5e" class="answer-label">← Back to Questions</a></p>
</div></details>
</div>

</section>
</section>


<a onclick="window.scrollTo(0, 0); return false;" role="button" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> Back to top</a></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
            // target, if specified
            link.setAttribute("target", "_blank");
            if (link.getAttribute("rel") === null) {
              link.setAttribute("rel", "noopener");
            }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../../../contents/core/workflow/workflow.html" class="pagination-link" aria-label="AI Workflow">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">AI Workflow</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../../../contents/core/frameworks/frameworks.html" class="pagination-link" aria-label="AI Frameworks">
        <span class="nav-page-text">AI Frameworks</span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>© 2024 Harvard University. Licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC-BY-NC-SA 4.0</a></p>
</div>   
    <div class="nav-footer-center">
<p>Written, edited and curated by Prof.&nbsp;Vijay Janapa Reddi (Harvard University)</p>
</div>
    <div class="nav-footer-right">
      <ul class="footer-items list-unstyled">
    <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/harvard-edge/cs249r_book">
      <i class="bi bi-github" role="img" aria-label="View source on GitHub">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/harvard-edge/cs249r_book">
      <i class="bi bi-star" role="img" aria-label="Star this repository">
</i> 
    </a>
  </li>  
</ul>
    </div>
  </div>
</footer>
<script>var lightboxQuarto = GLightbox({"closeEffect":"zoom","descPosition":"bottom","loop":false,"openEffect":"zoom","selector":".lightbox"});
(function() {
  let previousOnload = window.onload;
  window.onload = () => {
    if (previousOnload) {
      previousOnload();
    }
    lightboxQuarto.on('slide_before_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      const href = trigger.getAttribute('href');
      if (href !== null) {
        const imgEl = window.document.querySelector(`a[href="${href}"] img`);
        if (imgEl !== null) {
          const srcAttr = imgEl.getAttribute("src");
          if (srcAttr && srcAttr.startsWith("data:")) {
            slideConfig.href = srcAttr;
          }
        }
      } 
    });
  
    lightboxQuarto.on('slide_after_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(slideNode);
      }
    });
  
  };
  
})();
          </script>




</body></html>