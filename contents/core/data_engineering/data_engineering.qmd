--- 
bibliography: data_engineering.bib
--- 

# Data Engineering {#sec-data_engineering}

::: {.content-visible when-format="html"}
Resources: [Slides](#sec-data-engineering-resource), [Videos](#sec-data-engineering-resource), [Exercises](#sec-data-engineering-resource)
:::

![_DALL·E 3 Prompt: Create a rectangular illustration visualizing the concept of data engineering. Include elements such as raw data sources, data processing pipelines, storage systems, and refined datasets. Show how raw data is transformed through cleaning, processing, and storage to become valuable information that can be analyzed and used for decision-making._](images/png/cover_data_engineering.png)

## Purpose {.unnumbered}

_How does data shape ML systems engineering?_

Machine learning is generally often overshadowed by the allure of sophisticated algorithms, when in fact data plays a foundational role in determining an AI system's capabilities and limitations. We need to undersatnd the core principles of data in ML systems, exploring how the acquisition, processing, storage, and governance of data directly impact the performance, reliability, and ethical considerations of AI systems. By understanding these fundamental concepts, we can unlock the true potential of AI and build a solid foundation of high-quality ML solutions.

::: {.callout-tip}

## Learning Objectives

* Analyze different data sourcing methods (datasets, web scraping, crowdsourcing, synthetic data).

* Explain the importance of data labeling and ensure label quality.

* Evaluate data storage systems for ML workloads (databases, data warehouses, data lakes).

* Describe the role of data pipelines in ML systems.

* Explain the importance of data governance in ML (security, privacy, ethics).

* Identify key challenges in data engineering for ML.

:::

## Overview

Data is the bedrock foundation upon which sophisticated AI capabilities are built, with data quality and accessibility governing system effectiveness. Recent studies have illuminated the critical impact of data quality on ML system success. Notably, @sambasivan2021everyone introduced the concept of "Data Cascades"---systematic failures that occur when initial data quality issues compound throughout the ML pipeline, leading to model failures, project terminations, and potential harm to end users.

Understanding data engineering's role in ML systems requires examining the complete lifecycle of data within these systems. The past two decades have witnessed unprecedented data growth, with estimates suggesting that over 90% of today's data was generated during this period. However, the mere existence of data does not guarantee its utility in ML systems. 

As shown n @fig-cascades, errors in data collection and processing can propagate through the system, often manifesting more severely in later stages of the ML pipeline. These cascading effects frequently necessitate costly interventions such as model retraining or complete project restructuring. A notable example of this occurred in 2019 when [IBM Watson Health's cancer treatment recommendations were found to be unsafe and incorrect](https://spectrum.ieee.org/how-ibm-watson-overpromised-and-underdelivered-on-ai-health-care) due to flawed training data and inadequate validation [@strickland2019ibm]. This incident highlights the importance of addressing data quality issues early in the ML pipeline to prevent costly and potentially harmful consequences.

![Data cascades: compounded costs. Source: @sambasivan2021everyone.](images/png/data_engineering_cascades.png){#fig-cascades}

The transformation of raw data into ML-ready formats presents numerous technical challenges. This process encompasses data cleaning, normalization, and feature extraction---all of which must be carefully orchestrated to optimize model learning capabilities. Modern ML systems must also address the computational and architectural challenges of processing and storing massive datasets while maintaining efficient access patterns for both training and inference operations.

Contemporary data engineering must also address pressing concerns around data privacy and protection. The implementation of privacy-preserving techniques such as differential privacy and data aggregation has become essential, driven by both regulatory requirements and ethical considerations. These protective measures must be carefully balanced against the need to maintain data utility for ML model training and inference.

Documentation and metadata management is another aspect of data engineering for ML systems. The development of standardized documentation frameworks, such as the Data Cards proposed by @pushkarna2022data, enables developers to better understand dataset characteristics, limitations, and potential biases. Such documentation becomes particularly vital in complex ML systems where datasets interact with multiple components and serve various purposes.

The challenges inherent in data engineering for ML systems require systematic approaches and cross-functional collaboration. Data engineers must work closely with ML researchers, domain experts, and other stakeholders to develop robust data pipelines that support both model development and production deployment. This chapter will systematically explore the fundamental aspects of data engineering within the context of ML systems. Beginning with data collection and preprocessing, we will progress through feature engineering, storage architectures, and maintenance strategies. Throughout this examination, particular attention will be paid to the unique requirements and constraints that ML applications impose on data engineering practices.

## Problem Definition

In many machine learning domains, sophisticated algorithms take center stage, while the fundamental importance of data quality is often overlooked. This neglect gives rise to "Data Cascades" by @sambasivan2021everyone—events where lapses in data quality compound, leading to negative downstream consequences such as flawed predictions, project terminations, and even potential harm to communities.

@fig-cascades illustrates these potential data pitfalls at every stage and how they influence the entire process down the line. The influence of data collection errors is especially pronounced. As depicted in the figure, any lapses in this initial stage will become apparent at later stages (in model evaluation and deployment) and might lead to costly consequences, such as abandoning the entire model and restarting anew. Therefore, investing in data engineering techniques from the onset will help us detect errors early, mitigating the cascading effects illustrated in the figure.

Despite many ML professionals recognizing the importance of data, numerous practitioners report facing these cascades. This highlights a systemic issue: while the allure of developing advanced models remains, data often needs to be more appreciated.

This emphasis on data quality and proper problem definition is fundamental across all types of ML systems. As  @sculley2015hidden  emphasize, "It's important to distinguish ML-specific problem framing from the broader context of general software development." Whether developing recommendation engines processing millions of user interactions, computer vision systems analyzing medical images, or natural language models handling diverse text data, each system brings unique challenges that must be carefully considered from the outset. Production ML systems are particularly sensitive to data quality issues, as they must handle continuous data streams, maintain consistent processing pipelines, and adapt to evolving patterns while maintaining performance standards.

A solid project foundation is essential for setting the trajectory and ensuring the eventual success of any initiative. At the heart of this foundation lies the crucial first step: identifying a clear problem to solve. This could involve challenges like developing a recommendation system that effectively handles cold-start scenarios, or creating a classification model that maintains consistent accuracy across diverse population segments.

As we will explore later in this chapter, establishing clear objectives provides a unified direction that guides the entire project. These objectives might include creating representative datasets that account for various real-world scenarios. Equally important is defining specific benchmarks, such as prediction accuracy and system latency, which offer measurable outcomes to gauge progress and success.

Throughout this process, engaging with stakeholders—from end-users to business leaders—provides invaluable insights that ensure the project remains aligned with real-world needs and expectations.

Generally, in ML, problem definition has a few key steps:

1. Identifying the problem definition clearly
2. Setting clear objectives
3. Establishing success benchmark
4. Understanding end-user engagement/use
5. Understanding the constraints and limitations of deployment
6. Followed by finally doing the data collection.

**Keyword Spotting Example**

Keyword Spotting (KWS) is an excellent example to illustrate all of the general steps in action. This technology is critical for voice-enabled interfaces on endpoint devices such as smartphones. Typically functioning as lightweight wake-word engines, KWS systems are consistently active, listening for a specific phrase to trigger further actions. 

As shown in @fig-keywords, when we say "OK, Google" or "Alexa," this initiates a process on a microcontroller embedded within the device.

![Keyword Spotting example: interacting with Alexa. Source: Amazon.](images/png/data_engineering_kws.png){#fig-keywords}

Building a reliable KWS model is a complex task. It demands a deep understanding of the deployment scenario, encompassing where and how these devices will operate. For instance, a KWS model's effectiveness is not just about recognizing a word; it's about discerning it among various accents and background noises, whether in a bustling cafe or amid the blaring sound of a television in a living room or a kitchen where these devices are commonly found. It's about ensuring that a whispered "Alexa" in the dead of night or a shouted "OK Google" in a noisy marketplace are recognized with equal precision.

Moreover, many current KWS voice assistants support a limited number of languages, leaving a substantial portion of the world's linguistic diversity unrepresented. This limitation is partly due to the difficulty in gathering and monetizing data for languages spoken by smaller populations. The long-tail distribution of languages implies that many languages have limited data, making the development of supportive technologies challenging.

This level of accuracy and robustness hinges on the availability and quality of data, the ability to label the data correctly, and the transparency of the data for the end user before it is used to train the model. However, it all begins with clearly understanding the problem statement or definition. Using this KWS as an example, we can break each of the steps out as follows:

1. **Identifying the Problem:** KWS detects specific keywords amidst ambient sounds and other spoken words. The primary problem is to design a system that can recognize these keywords with high accuracy, low latency, and minimal false positives or negatives, especially when deployed on devices with limited computational resources.

2. **Setting Clear Objectives:** The objectives for a KWS system might include:
   * Achieving a specific accuracy rate (e.g., 98% accuracy in keyword detection).
   * Ensuring low latency (e.g., keyword detection and response within 200 milliseconds).
   * Minimizing power consumption to extend battery life on embedded devices.
   * Ensuring the model's size is optimized for the available memory on the device.

3. **Benchmarks for Success:** Establish clear metrics to measure the success of the KWS system. This could include:
   * *True Positive Rate:* The percentage of correctly identified keywords.
   * *False Positive Rate:* The percentage of non-keywords incorrectly identified as keywords.
   * *Response Time:* The time taken from keyword utterance to system response.
   * *Power Consumption:* Average power used during keyword detection.

4. **Stakeholder Engagement and Understanding:** Engage with stakeholders, which include device manufacturers, hardware and software developers, and end-users. Understand their needs, capabilities, and constraints. For instance:
   * Device manufacturers might prioritize low power consumption.
   * Software developers might emphasize ease of integration.
   * End-users would prioritize accuracy and responsiveness.

5. **Understanding the Constraints and Limitations of Embedded Systems:** Embedded devices come with their own set of challenges:
   * *Memory Limitations:* KWS models must be lightweight to fit within the memory constraints of embedded devices. Typically, KWS models need to be as small as 16KB to fit in the always-on island of the SoC. Moreover, this is just the model size. Additional application code for preprocessing may also need to fit within the memory constraints.
   * Processing Power: The computational capabilities of embedded devices are limited (a few hundred MHz of clock speed), so the KWS model must be optimized for efficiency.
   * *Power Consumption:* Since many embedded devices are battery-powered, the KWS system must be power-efficient.
   * *Environmental Challenges:* Devices might be deployed in various environments, from quiet bedrooms to noisy industrial settings. The KWS system must be robust enough to function effectively across these scenarios.

6. **Data Collection and Analysis:** For a KWS system, the quality and diversity of data are paramount. Considerations might include:
   * *Variety of Accents:* Collect data from speakers with various accents to ensure wide-ranging recognition.
   * *Background Noises:* Include data samples with different ambient noises to train the model for real-world scenarios.
   * *Keyword Variations:* People might either pronounce keywords differently or have slight variations in the wake word itself. Ensure the dataset captures these nuances.

7. **Iterative Feedback and Refinement:** Once a prototype KWS system is developed, it is important to do the following to ensure that the system remains aligned with the defined problem and objectives as the deployment scenarios change over time as things evolve.

   * Test it in real-world scenarios
   * Gather feedback
   * Iteratively refine the model
 
The KWS example illustrates the broader principles of problem definition, showing how initial decisions about data requirements ripple throughout a project's lifecycle. By carefully considering each aspect---from core problem identification through performance benchmarks to deployment constraints---teams can build a strong foundation for their ML systems. The methodical problem definition process provides a framework applicable across the ML spectrum. Whether developing computer vision systems for medical diagnostics, recommendation engines processing millions of user interactions, or natural language models analyzing diverse text corpora, this structured approach helps teams anticipate and plan for their data needs. 

This brings us to data pipelines---the foundational infrastructure that transforms raw data into ML---ready formats while maintaining quality and reliability throughout the process. These pipelines implement our carefully defined requirements in production systems, handling everything from initial data ingestion to final feature generation.

:::{#exr-kws .callout-caution collapse="true"}

### Keyword Spotting with TensorFlow Lite Micro

Explore a hands-on guide for building and deploying Keyword Spotting systems using TensorFlow Lite Micro. Follow steps from data collection to model training and deployment to microcontrollers. Learn to create efficient KWS models that recognize specific keywords amidst background noise. Perfect for those interested in machine learning on embedded systems. Unlock the potential of voice-enabled devices with TensorFlow Lite Micro!  

[![](https://colab.research.google.com/assets/colab-badge.png)](https://colab.research.google.com/drive/17I7GL8WTieGzXYKRtQM2FrFi3eLQIrOM)
:::

## Pipeline Fundamentals

Data flows through data pipelines that form the backbone of machine learning systems, serving as the infrastructure through which data flows from its raw form to become ML-ready training data. These pipelines are not simple linear paths but rather complex systems that must handle data acquisition, transformation, storage, and delivery while maintaining data quality and system reliability.

Modern ML systems depend on these pipelines to process massive amounts of data efficiently and reliably. For instance, recommendation systems at companies like Netflix process billions of user interactions daily, while autonomous vehicle systems must handle terabytes of sensor data in real-time. The design of these pipelines fundamentally shapes what's possible with the ML system.

ML data pipelines consist of several distinct layers: data sources, ingestion, processing, labeling, storage and eventually ML training (@fig-pipeline-flow). Each layer serves a specific purpose in the data preparation workflow, and the interactions between these layers determine the system's overall effectiveness. The flow from raw data sources through to ML training illustrates how data quality and system requirements must be maintained throughout the pipeline.

![Overview of the data pipeline.](images/png/data_pipeline.png){#fig-pipeline-flow}

## Data Sources

The first stage of the pipeline architecture sourcing appropriate data to meet the training needs. The quality and diversity of our this data will fundamentally determine our ML system's learning and prediction capabilities and limitations. ML systems can obtain their training data through several different approaches, each with their own advantages and challenges. Let's examine each of these approaches in detail.

### Pre-existing datasets

Platforms like [Kaggle](https://www.kaggle.com/) and [UCI Machine Learning Repository](https://archive.ics.uci.edu/) provide ML practitioners with ready-to-use datasets that can jumpstart system development. These pre-existing datasets are particularly valuable when building ML systems as they offer immediate access to cleaned, formatted data with established benchmarks. One of their primary advantages is cost efficiency - creating datasets from scratch requires significant time and resources, especially when building production ML systems that need large amounts of high-quality training data.

Many of these datasets, such as [ImageNet](https://www.image-net.org/), have become standard benchmarks in the machine learning community, enabling consistent performance comparisons across different models and architectures. For ML system developers, this standardization provides clear metrics for evaluating model improvements and system performance. The immediate availability of these datasets allows teams to begin experimentation and prototyping without delays in data collection and preprocessing.

However, ML practitioners must carefully consider the quality assurance aspects of pre-existing datasets. For instance, the ImageNet dataset was found to have over 6.4% errors [@northcutt2021pervasive]. While popular datasets benefit from community scrutiny that helps identify and correct errors and biases, these issues can significantly impact system performance if not properly addressed. Moreover, as @gebru2018datasheets highlighted in her paper, simply providing a dataset without documentation can lead to misuse and misinterpretation, potentially amplifying biases present in the data."

Supporting documentation often accompanying existing datasets is invaluable, though this generally applies only to widely used datasets. Good documentation provides insights into the data collection process and variable definitions and sometimes even offers baseline model performances. This information not only aids understanding but also promotes reproducibility in research, a cornerstone of scientific integrity; currently, there is a crisis around [improving reproducibility in machine learning systems](https://arxiv.org/abs/2003.12206). When other researchers have access to the same data, they can validate findings, test new hypotheses, or apply different methodologies, thus allowing us to build on each other's work more rapidly.

While platforms like Kaggle and UCI Machine Learning Repository are invaluable resources, it's essential to understand the context in which the data was collected. Researchers should be wary of potential overfitting when using popular datasets, as multiple models might have been trained on them, leading to inflated performance metrics. Sometimes, these [datasets do not reflect the real-world data](https://venturebeat.com/uncategorized/3-big-problems-with-datasets-in-ai-and-machine-learning/).

A key consideration for ML systems is how well pre-existing datasets reflect real-world deployment conditions. Relying on standard datasets can create a concerning disconnect between training and production environments. This misalignment becomes particularly problematic when multiple ML systems are trained on the same datasets (@fig-misalignment), potentially propagating biases and limitations throughout an entire ecosystem of deployed models.

![Training different models on the same dataset.](images/png/dataset_myopia.png){#fig-misalignment}

### Web Scraping

When building ML systems, particularly in domains where pre-existing datasets are insufficient, web scraping offers a powerful approach to gathering training data at scale. This automated technique for extracting data from websites has become a powerful tool in modern ML system development. It enables teams to build custom datasets tailored to their specific needs. 

Web scraping has proven particularly valuable for building large-scale ML systems when human-labeled data is scarce. Consider computer vision systems: major datasets like [ImageNet](https://www.image-net.org/) and [OpenImages](https://storage.googleapis.com/openimages/web/index.html) were built through systematic web scraping, fundamentally advancing the field of computer vision. In production environments, companies regularly scrape e-commerce sites to gather product images for recognition systems or social media platforms for computer vision applications. Stanford's [LabelMe](https://people.csail.mit.edu/torralba/publications/labelmeApplications.pdf) project demonstrated this approach's potential early on, scraping Flickr to create a diverse dataset of over 63,000 annotated images.

The impact of web scraping extends well beyond computer vision systems. In natural language processing, web-scraped data has enabled the development of increasingly sophisticated ML systems. Large language models, such as ChatGPT and Claude, rely on vast amounts of text scraped from the public internet to learn language patterns and generate responses. Similarly, specialized ML systems like GitHub's Copilot demonstrate how targeted web scraping---in this case, of code repositories---can create powerful domain-specific assistants.

Production ML systems often require continuous data collection to maintain relevance and performance. Web scraping facilitates this by gathering structured data like stock prices, weather patterns, or product information for analytical applications. However, this continuous collection introduces unique challenges for ML systems. Data consistency becomes crucial---variations in website structure or content formatting can disrupt the data pipeline and affect model performance. Proper data management through databases or warehouses becomes essential not just for storage, but for maintaining data quality and enabling model updates.

Despite its utility, web scraping presents several challenges that ML system developers must carefully consider. Legal and ethical constraints can limit data collection---not all websites permit scraping, and violating these restrictions can have serious consequences. When building ML systems with scraped data, teams must carefully document data sources and ensure compliance with terms of service and copyright laws. Privacy considerations become particularly critical when dealing with user-generated content, often requiring robust anonymization procedures.

Technical limitations also affect the reliability of web-scraped training data. Rate limiting by websites can slow data collection, while the dynamic nature of web content can introduce inconsistencies that impact model training. As shown in @fig-traffic-light, web scraping can yield unexpected or irrelevant data---such as historical images appearing in contemporary image searches---that can pollute training datasets and degrade model performance. These issues highlight the importance of thorough data validation and cleaning processes in ML pipelines built on web-scraped data.

![A picture of old traffic lights (1914). Source: [Vox.](https://www.vox.com/2015/8/5/9097713/when-was-the-first-traffic-light-installed)](images/jpg/1914_traffic.jpeg){#fig-traffic-light}

:::{#exr-ws .callout-caution collapse="true"}

#### Web Scraping

Discover the power of web scraping with Python using libraries like Beautiful Soup and Pandas. This exercise will scrape Python documentation for function names and descriptions and explore NBA player stats. By the end, you'll have the skills to extract and analyze data from real-world websites. Ready to dive in? Access the Google Colab notebook below and start practicing!  

[![](https://colab.research.google.com/assets/colab-badge.png)](https://colab.research.google.com/github/Andy-Pham-72/Web-Scraping-with-BeautifulSoup-and-Pandas/blob/master/Web_scraping_with_beautiful_soup_and_pandas_complete.ipynb)
:::

### Crowdsourcing

Crowdsourcing is a collaborative approach to data collection, leveraging the collective efforts of distributed individuals via the internet to tackle tasks requiring human judgment. By engaging a global pool of contributors, this method accelerates the creation of high-quality, labeled datasets for machine learning systems, especially in scenarios where pre-existing data is scarce or domain-specific. Platforms like [Amazon Mechanical Turk](https://www.mturk.com/) exemplify how crowdsourcing facilitates this process by distributing annotation tasks to a global workforce. This enables the rapid collection of labels for complex tasks such as sentiment analysis, image recognition, and speech transcription, significantly expediting the data preparation phase.

One of the most impactful examples of crowdsourcing in machine learning is the creation of the [ImageNet dataset](https://image-net.org/). ImageNet, which revolutionized computer vision, was built by distributing image labeling tasks to contributors via Amazon Mechanical Turk. The contributors categorized millions of images into thousands of classes, enabling researchers to train and benchmark models for a wide variety of visual recognition tasks. 

The dataset's availability spurred advancements in deep learning, including the breakthrough AlexNet model in 2012, which demonstrated how large-scale, crowdsourced datasets could drive innovation. ImageNet's success highlights how leveraging a diverse group of contributors for annotation can enable machine learning systems to achieve unprecedented performance.

Another example of crowdsourcing's potential is Google's [Crowdsource](https://crowdsource.google.com/), a platform where volunteers contribute labeled data to improve AI systems in applications like language translation, handwriting recognition, and image understanding. By gamifying the process and engaging global participants, Google harnesses diverse datasets, particularly for underrepresented languages. This approach not only enhances the quality of AI systems but also empowers communities by enabling their contributions to influence technological development.

Crowdsourcing has also been instrumental in applications beyond traditional dataset annotation. For instance, the navigation app [Waze](https://www.waze.com/) uses crowdsourced data from its users to provide real-time traffic updates, route suggestions, and incident reporting. While this involves dynamic data collection rather than static dataset labeling, it demonstrates how crowdsourcing can generate continuously updated datasets essential for applications like mobile or edge ML systems. These systems often require real-time input to maintain relevance and accuracy in changing environments.

One of the primary advantages of crowdsourcing is its scalability. By distributing microtasks to a large audience, projects can process enormous volumes of data quickly and cost-effectively. This scalability is particularly beneficial for machine learning systems that require extensive datasets to achieve high performance. Additionally, the diversity of contributors introduces a wide range of perspectives, cultural insights, and linguistic variations, enriching datasets and improving models' ability to generalize across populations.

Flexibility is a key benefit of crowdsourcing. Tasks can be adjusted dynamically based on initial results, allowing for iterative improvements in data collection. For example, Google's [reCAPTCHA](https://www.google.com/recaptcha/about/) system uses crowdsourcing to verify human users while simultaneously labeling datasets for training machine learning models. Users identify objects in images—such as street signs or cars—contributing to the training of autonomous systems. This clever integration demonstrates how crowdsourcing can scale seamlessly when embedded into everyday workflows.

Despite its advantages, crowdsourcing presents challenges that require careful management. Quality control is a major concern, as the variability in contributors' expertise and attention can lead to inconsistent or inaccurate annotations. Providing clear instructions and training materials helps ensure participants understand the task requirements. Techniques such as embedding known test cases, leveraging consensus algorithms, or using redundant annotations can mitigate quality issues and align the process with the problem definition discussed earlier.

Ethical considerations are paramount in crowdsourcing, especially when datasets are built at scale using global contributors. It is essential to ensure that participants are fairly compensated for their work and that they are informed about how their contributions will be used. Additionally, privacy concerns must be addressed, particularly when dealing with sensitive or personal information. Transparent sourcing practices, clear communication with contributors, and robust auditing mechanisms are crucial for building trust and maintaining ethical standards.

The issue of fair compensation and ethical data sourcing was brought into sharp focus during the development of large-scale AI systems like OpenAI's ChatGPT. Reports revealed that [OpenAI outsourced data annotation tasks to workers in Kenya](https://time.com/6247678/openai-chatgpt-kenya-workers/), employing them to moderate content and identify harmful or inappropriate material that the model might generate. This involved reviewing and labeling distressing content, such as graphic violence and explicit material, to train the AI in recognizing and avoiding such outputs. While this approach enabled OpenAI to improve the safety and utility of ChatGPT, significant ethical concerns arose around the working conditions, the nature of the tasks, and the compensation provided to Kenyan workers.

Many of the contributors were reportedly paid as little as $1.32 per hour for reviewing and labeling highly traumatic material. The emotional toll of such work, coupled with low wages, raised serious questions about the fairness and transparency of the crowdsourcing process. This controversy highlights a critical gap in ethical crowdsourcing practices. The workers, often from economically disadvantaged regions, were not adequately supported to cope with the psychological impact of their tasks. The lack of mental health resources and insufficient compensation underscored the power imbalances that can emerge when outsourcing data annotation tasks to lower-income regions.

The challenges highlighted by the ChatGPT---Kenya controversy are not unique to OpenAI. Many organizations that rely on crowdsourcing for data annotation face similar issues. As machine learning systems grow more complex and require larger datasets, the demand for annotated data will continue to increase. This shows the need for industry-wide standards and best practices to ensure ethical data sourcing. This case emphasizes the importance of considering the human labor behind AI systems. While crowdsourcing offers scalability and diversity, it also brings ethical responsibilities that cannot be overlooked. Organizations must prioritize the well-being and fair treatment of contributors as they build the datasets that drive AI innovation.

Moreover, when dealing with specialized applications like mobile ML, edge ML, or cloud ML, additional challenges may arise. These applications often require data collected from specific environments or devices, which can be difficult to gather through general crowdsourcing platforms. For example, data for mobile applications utilizing smartphone sensors may necessitate participants with specific hardware features or software versions. Similarly, edge ML systems deployed in industrial settings may require data involving proprietary processes or secure environments, introducing privacy and accessibility challenges.

Hybrid approaches that combine crowdsourcing with other data collection methods can address these challenges. Organizations may engage specialized communities, partner with relevant stakeholders, or create targeted initiatives to collect domain-specific data. Additionally, synthetic data generation, as discussed in the next section, can augment real-world data when crowdsourcing falls short.

### Synthetic Data

Synthetic data generation has emerged as a powerful tool for addressing limitations in data collection, particularly in machine learning applications where real-world data is scarce, expensive, or ethically challenging to obtain. This approach involves creating artificial data using algorithms, simulations, or generative models to mimic real-world datasets. The generated data can be used to supplement or replace real-world data, expanding the possibilities for training robust and accurate machine learning systems. @fig-synthetic-data illustrates the process of combining synthetic data with historical datasets to create larger, more diverse training sets.

![Increasing training data size with synthetic data generation. Source: [AnyLogic](https://www.anylogic.com/features/artificial-intelligence/synthetic-data/).](images/jpg/synthetic_data.jpg){#fig-synthetic-data}

Advancements in generative modeling techniques, such as Generative Adversarial Networks (GANs)[^defn-gan] and Variational Autoencoders (VAEs)[^defn-vae], have greatly enhanced the quality of synthetic data. These techniques can produce data that closely resembles real-world distributions, making it suitable for applications ranging from computer vision to natural language processing. For example, GANs have been used to generate synthetic images for object recognition tasks, creating diverse datasets that are almost indistinguishable from real-world images. Similarly, synthetic data has been leveraged to simulate speech patterns, enhancing the robustness of voice recognition systems.

[^defn-gan]: **Generative Adversarial Networks (GANs):** A class of machine learning frameworks where two neural networks, a generator and a discriminator, are trained simultaneously in a competitive setting. The generator creates synthetic data, while the discriminator evaluates its authenticity, pushing the generator to improve over time.  
[^defn-vae]: **Variational Autoencoders (VAEs):** A type of generative model that uses an encoder-decoder architecture to map input data to a latent space and then reconstruct it, allowing for the generation of new data by sampling from the latent space.

This labeling style makes the footnotes more descriptive and easier to manage in larger documents. Synthetic data has become particularly valuable in domains where obtaining real-world data is either impractical or costly. In security applications, for instance, training a system to detect the sound of breaking glass would require physically breaking numerous windows under controlled conditions. Synthetic data provides a practical alternative by simulating these sounds, allowing the model to learn effectively without the logistical challenges of real-world collection. In healthcare, privacy regulations such as [GDPR](https://gdpr.eu/)[^defn-GDPR] and [HIPAA](https://www.hhs.gov/hipaa/index.html)[^defn-HIPAA] limit the sharing of sensitive patient information. Synthetic data generation enables the creation of realistic yet anonymized datasets that can be used for training diagnostic models without compromising patient privacy.

[^defn-GDPR]: **General Data Protection Regulation (GDPR):** A regulation in EU law on data protection and privacy in the European Union and the European Economic Area.
[^defn-HIPAA]: **Health Insurance Portability and Accountability Act (HIPAA):** A US law designed to provide privacy standards to protect patients' medical records and other health information.

The automotive industry has also embraced synthetic data to train autonomous vehicle systems; there are only so many cars you can physically crash to get crash-test data that might help an ML system know how to avoid crashes in the first place. Capturing real-world scenarios, especially rare edge cases such as near-accidents or unusual road conditions, is inherently difficult. Synthetic data allows researchers to simulate these scenarios in a controlled virtual environment, ensuring that models are trained to handle a wide range of conditions. This approach has proven invaluable for advancing the capabilities of self-driving cars.

Another important application of synthetic data lies in augmenting existing datasets. Introducing variations into datasets enhances model robustness by exposing the model to diverse conditions. For instance, in speech recognition, data augmentation techniques like SpecAugment introduce noise, shifts, or pitch variations, enabling models to generalize better across different environments and speaker styles. This principle extends to other domains as well, where synthetic data can fill gaps in underrepresented scenarios or edge cases.

In addition to expanding datasets, synthetic data addresses critical ethical and privacy concerns. Unlike real-world data, synthetic data is artificially generated and does not tie back to specific individuals or entities. This makes it especially useful in sensitive domains such as finance, healthcare, or human resources, where data confidentiality is paramount. The ability to preserve statistical properties while removing identifying information allows researchers to maintain high ethical standards without compromising the quality of their models.

Poorly generated data can misrepresent underlying real-world distributions, introducing biases or inaccuracies that degrade model performance. Validating synthetic data against real-world benchmarks is essential to ensure its reliability. Additionally, models trained primarily on synthetic data must be rigorously tested in real-world scenarios to confirm their ability to generalize effectively. Another challenge is the potential amplification of biases present in the original datasets used to inform synthetic data generation. If these biases are not carefully addressed, they may be inadvertently reinforced in the resulting models.

Synthetic data has revolutionized the way machine learning systems are trained, providing flexibility, diversity, and scalability in data preparation. However, as its adoption grows, practitioners must remain vigilant about its limitations and ethical implications. By combining synthetic data with rigorous validation and thoughtful application, machine learning researchers and engineers can unlock its full potential while ensuring reliability and fairness in their systems.

:::{#exr-sd .callout-caution collapse="true"}

#### Synthetic Data

Let us learn about synthetic data generation using GANs on tabular data. We'll take a hands-on approach, diving into the workings of the CTGAN model and applying it to the Synthea dataset from the healthcare domain. From data preprocessing to model training and evaluation, we'll go step-by-step, learning how to create synthetic data, assess its quality, and unlock the potential of GANs for data augmentation and real-world applications.  

[![](https://colab.research.google.com/assets/colab-badge.png)](https://colab.research.google.com/drive/1nwbvkg32sOUC69zATCfXOygFUBeo0dsx?usp=sharing#scrollTo=TkwYknr44eFn)
:::

### Case Study: KWS

KWS is an excellent case study of how different data collection approaches can be combined effectively. Each method we've discussed plays a role in building robust wake word detection systems, albeit with different trade-offs: 

Pre-existing datasets like Google's Speech Commands [@warden2018speech] provide a foundation for initial development, offering carefully curated voice samples for common wake words. However, these datasets often lack diversity in accents, environments, and languages, necessitating additional data collection strategies.

Web scraping can supplement these baseline datasets by gathering diverse voice samples from video platforms, podcast repositories, and speech databases. This helps capture natural speech patterns and wake word variations, though careful attention must be paid to audio quality and privacy considerations when scraping voice data.

Crowdsourcing becomes valuable for collecting specific wake word samples across different demographics and environments. Platforms like Amazon Mechanical Turk can engage contributors to record wake words in various accents, speaking styles, and background conditions. This approach is particularly useful for gathering data for underrepresented languages or specific acoustic environments.

Synthetic data generation helps fill remaining gaps by creating unlimited variations of wake word utterances. Using speech synthesis and audio augmentation techniques, developers can generate training data that captures different acoustic environments (busy streets, quiet rooms, moving vehicles), speaker characteristics (age, accent, gender), and background noise conditions.

This multi-faceted approach to data collection enables the development of KWS systems that perform robustly across diverse real-world conditions. The combination of methods helps address the unique challenges of wake word detection, from handling various accents and background noise to maintaining consistent performance across different devices and environments.

## Data Ingestion

The collected data must be reliably and efficiently ingested into our ML systems through well-designed data pipelines. This transformation presents several challenges that ML engineers must address.

### Ingestion Patterns

In ML systems, data ingestion typically follows two primary patterns: batch ingestion and stream ingestion. Each pattern has distinct characteristics and use cases that students should understand to design effective ML systems.

Batch ingestion involves collecting data in groups or batches over a specified period before processing. This method is appropriate when real-time data processing is not critical and data can be processed at scheduled intervals. It's also useful for loading large volumes of historical data. For example, a retail company might use batch ingestion to process daily sales data overnight, updating their ML models for inventory prediction each morning.

In contrast, stream ingestion processes data in real-time as it arrives. This pattern is crucial for applications requiring immediate data processing, scenarios where data loses value quickly, and systems that need to respond to events as they occur. A financial institution, for instance, might use stream ingestion for real-time fraud detection, processing each transaction as it occurs to flag suspicious activity immediately.

Many modern ML systems employ a hybrid approach, combining both batch and stream ingestion to handle different data velocities and use cases. This flexibility allows systems to process both historical data in batches and real-time data streams, providing a comprehensive view of the data landscape.

### ETL vs. ELT

When designing data ingestion pipelines for ML systems, it's necessary to understand the differences between Extract, Transform, Load (ETL) and Extract, Load, Transform (ELT) approaches. These paradigms determine when data transformations occur relative to the loading phase. They can significantly impacting the flexibility and efficiency of your ML pipeline.

ETL is a well-established paradigm in which data is first gathered from a source, then transformed to match the target schema or model, and finally loaded into a data warehouse or other repository. This approach typically results in data being stored in a ready-to-query format, which can be advantageous for ML systems that require consistent, pre-processed data. For instance, an ML system predicting customer churn might use ETL to standardize and aggregate customer interaction data from multiple sources before loading it into a format suitable for model training.

However, ETL can be less flexible when schemas or requirements change frequently, a common occurrence in evolving ML projects. This is where the ELT approach comes into play. ELT reverses the order by first loading raw data and then applying transformations as needed. This method is often seen in modern data lake or schema-on-read[^defn-schema] environments, allowing for a more agile approach when addressing evolving analytical needs in ML systems.

[^defn-schema]: Schema-on-read is a flexible data handling approach where the structure of data is not enforced when it is ingested (written), but rather when it is accessed (read). This contrasts with traditional schema-on-write approaches where data must conform to a predefined schema before storage. In ML systems, schema-on-read provides significant flexibility, allowing the same raw data to be interpreted differently for various use cases. For instance, text data might be tokenized differently depending on the ML model's requirements, or numerical features might be normalized using different scales based on the specific application.

By deferring transformations, ELT can accommodate varying uses of the same dataset, which is particularly useful in exploratory data analysis phases of ML projects or when multiple models with different data requirements are being developed simultaneously. However, it's important to note that ELT places greater demands on storage systems and query engines, which must handle large amounts of unprocessed information.

In practice, many ML systems employ a hybrid approach, selecting ETL or ELT on a case-by-case basis depending on the specific requirements of each data source or ML model. For example, a system might use ETL for structured data from relational databases where schemas are well-defined and stable, while employing ELT for unstructured data like text or images where transformation requirements may evolve as the ML models are refined.

### Source Integration

Integrating diverse data sources is a key challenge in data ingestion for ML systems. Data may come from various origins, including databases, APIs, file systems, and IoT devices. Each source may have its own data format, access protocol, and update frequency.

To effectively integrate these sources, ML engineers must develop robust connectors or adapters for each data source. These connectors handle the specifics of data extraction, including authentication, rate limiting, and error handling. For example, when integrating with a REST API, the connector would manage API keys, respect rate limits, and handle HTTP status codes appropriately.

Furthermore, source integration often involves data transformation at the ingestion point. This might include parsing JSON or XML responses, converting timestamps to a standard format, or performing basic data cleaning operations. The goal is to standardize the data format as it enters the ML pipeline, simplifying downstream processing.

It's also essential to consider the reliability and availability of data sources. Some sources may experience downtime or have inconsistent data quality. Implementing retry mechanisms, data quality checks, and fallback procedures can help ensure a steady flow of reliable data into the ML system.

### Data Validation

Data validation is an important step in the ingestion process, ensuring that incoming data meets quality standards and conforms to expected schemas. This step helps prevent downstream issues in ML pipelines caused by data anomalies or inconsistencies.

At the ingestion stage, validation typically encompasses several key aspects. First, it checks for schema conformity, ensuring that incoming data adheres to the expected structure, including data types and field names. Next, it verifies data ranges and constraints, confirming that numeric fields fall within expected ranges and that categorical fields contain valid values. Completeness checks are also performed, looking for missing or null values in required fields. Additionally, consistency checks ensure that related data points are logically coherent.

For example, in a healthcare ML system ingesting patient data, validation might include checking that age values are positive integers, diagnosis codes are from a predefined set, and admission dates are not in the future. By implementing robust validation at the ingestion stage, ML engineers can detect and handle data quality issues early, significantly reducing the risk of training models on flawed or inconsistent data.

### Error Handling

Error handling in data ingestion is essential for building resilient ML systems. Errors can occur at various points in the ingestion process, from source connection issues to data validation failures. Effective error handling strategies ensure that the ML pipeline can continue to operate even when faced with data ingestion challenges.

A key concept in error handling is graceful degradation. This involves designing systems to continue functioning, possibly with reduced capabilities, when faced with partial data loss or temporary source unavailability. Implementing intelligent retry logic for transient errors, such as network interruptions or temporary service outages, is another important aspect of robust error handling.

Many ML systems employ the concept of dead letter queues[^defn-dlq], using separate storage for data that fails processing. This allows for later analysis and potential reprocessing of problematic data. Alongside this, comprehensive alerting and monitoring systems are utilized to notify operators of persistent ingestion issues or data quality problems, enabling quick responses to maintain data flow and quality.

[^defn-dlq]: A DLQ is a specialized queue that stores messages that fail to be processed successfully. When data fails validation or processing, instead of being discarded, it is moved to a DLQ for later analysis and potential reprocessing.

For instance, in a financial ML system ingesting market data, error handling might involve falling back to slightly delayed data sources if real-time feeds fail, while simultaneously alerting the operations team to the issue. This approach ensures that the system continues to function and that responsible parties are aware of and can address the problem.

This ensures that downstream processes have access to reliable, high-quality data for training and inference tasks, even in the face of ingestion challenges. Understanding these concepts of data validation and error handling is essential for students and practitioners aiming to build robust, production-ready ML systems.

Once ingestion is complete and data is validated, it is typically loaded into a storage environment suited to the organization's analytical or machine learning needs. Some datasets flow into data warehouses for structured queries, whereas others are retained in data lakes for exploratory or large-scale analyses. Advanced systems may also employ feature stores to provide standardized features for machine learning.

### Case Study: KWS

A production KWS system typically employs both streaming and batch ingestion patterns. The streaming pattern handles real-time audio data from active devices, where wake words must be detected with minimal latency. This requires careful implementation of pub/sub mechanisms—for example, using Kafka-like streams to buffer incoming audio data and enable parallel processing across multiple inference servers.

Simultaneously, the system processes batch data for model training and updates. This includes ingesting new wake word recordings from crowdsourcing efforts, synthetic data from voice generation systems, and validated user interactions. The batch processing typically follows an ETL pattern, where audio data is preprocessed (normalized, filtered, segmented) before being stored in a format optimized for model training.

KWS systems must integrate data from diverse sources, such as real-time audio streams from deployed devices, rrowdsourced recordings from data collection platforms etc Each source presents unique challenges. Real-time audio streams require rate limiting to prevent system overload during usage spikes. Crowdsourced data needs robust validation to ensure recording quality and correct labeling. Synthetic data must be verified for realistic representation of wake word variations.

KWS systems employ sophisticated error handling mechanisms due to the nature of voice interaction. When processing real-time audio, dead letter queues store failed recognition attempts for analysis, helping identify patterns in false negatives or system failures. Data validation becomes particularly important for maintaining wake word detection accuracy—incoming audio must be checked for quality issues like clipping, noise levels, and appropriate sampling rates.

For example, consider a smart home device processing the wake word "Alexa." The ingestion pipeline must validate:

* Audio quality metrics (signal-to-noise ratio, sample rate, bit depth)
* Recording duration (typically 1-2 seconds for wake words)
* Background noise levels
* Speaker proximity indicators

Invalid samples are routed to dead letter queues for analysis, while valid samples are processed in real-time for wake word detection.

This case study illustrates how real-world ML systems must carefully balance different ingestion patterns, handle multiple data sources, and maintain robust error handling—all while meeting strict latency and reliability requirements. The lessons from KWS systems apply broadly to other ML applications requiring real-time processing capabilities alongside continuous model improvement.

## Data Processing

Data processing is a stage in the machine learning pipeline that transforms raw data into a format suitable for model training and inference. This stage encompasses several key activities, each playing a role in preparing data for effective use in ML systems. The approach to data processing is closely tied to the ETL (Extract, Transform, Load) or ELT (Extract, Load, Transform) paradigms discussed earlier.

In traditional ETL workflows, much of the data processing occurs before the data is loaded into the target system. This approach front-loads the cleaning, transformation, and feature engineering steps, ensuring that data is in a ready-to-use state when it reaches the data warehouse or ML pipeline. ETL is often preferred when dealing with structured data or when there's a need for significant data cleansing before analysis.

Conversely, in ELT workflows, raw data is first loaded into the target system, and transformations are applied afterwards. This approach, often used with data lakes, allows for more flexibility in data processing. It's particularly useful when dealing with unstructured or semi-structured data, or when the exact transformations needed are not known in advance. In ELT, many of the data processing steps we'll discuss might be performed on-demand or as part of the ML pipeline itself.

The choice between ETL and ELT can impact how and when data processing occurs in an ML system. For instance, in an ETL-based system, data cleaning and initial transformations might happen before the data even reaches the ML team. In contrast, an ELT-based system might require ML engineers to handle more of the data processing tasks as part of their workflow.

Regardless of whether an organization follows an ETL or ELT approach, understanding the following data processing steps is crucial for ML practitioners. These processes ensure that data is clean, relevant, and optimally formatted for machine learning algorithms.

### Data Cleaning

Data cleaning involves identifying and correcting errors, inconsistencies, and inaccuracies in datasets. Raw data frequently contains issues such as missing values, duplicates, or outliers that can significantly impact model performance if left unaddressed.

In practice, data cleaning might involve removing duplicate records, handling missing values through imputation or deletion, and correcting formatting inconsistencies. For instance, in a customer database, names might be inconsistently capitalized or formatted. A data cleaning process would standardize these entries, ensuring that "John Doe," "john doe," and "DOE, John" are all treated as the same entity.

Outlier detection and treatment is another important aspect of data cleaning. Outliers can sometimes represent valuable information about rare events, but they can also be the result of measurement errors or data corruption. ML practitioners must carefully consider the nature of their data and the requirements of their models when deciding how to handle outliers.

### Quality Assessment

Quality assessment goes hand in hand with data cleaning, providing a systematic approach to evaluating the reliability and usefulness of data. This process involves examining various aspects of data quality, including accuracy, completeness, consistency, and timeliness.

Tools and techniques for quality assessment range from simple statistical measures to more complex machine learning-based approaches. For example, data profiling tools can provide summary statistics and visualizations that help identify potential quality issues. More advanced techniques might involve using unsupervised learning algorithms to detect anomalies or inconsistencies in large datasets.

Establishing clear quality metrics and thresholds is essential for maintaining data quality over time. These metrics might include the percentage of missing values, the frequency of outliers, or measures of data freshness. Regular quality assessments help ensure that data entering the ML pipeline meets the necessary standards for reliable model training and inference.

### Data Transformation

Data transformation converts the data from its raw form into a format more suitable for analysis and modeling. This process can include a wide range of operations, from simple conversions to complex mathematical transformations.

Common transformation tasks include normalization and standardization, which scale numerical features to a common range or distribution. For example, in a housing price prediction model, features like square footage and number of rooms might be on vastly different scales. Normalizing these features ensures that they contribute more equally to the model's predictions.

Other transformations might involve encoding categorical variables, handling date and time data, or creating derived features. For instance, one-hot encoding[^defn-one-hot] is often used to convert categorical variables into a format that can be readily understood by many machine learning algorithms.

[^defn-one-hot]: **One-Hot Encoding:** Converts categorical variables into binary vectors, where each category is represented by a unique vector with one element set to 1 and the rest to 0. This allows categorical data to be used in ML models requiring numerical input.

### Feature Engineering

Feature engineering is the process of using domain knowledge to create new features that make machine learning algorithms work more effectively. This step is often considered more of an art than a science, requiring creativity and deep understanding of both the data and the problem at hand.

Feature engineering might involve combining existing features, extracting information from complex data types, or creating entirely new features based on domain insights. For example, in a retail recommendation system, engineers might create features that capture the recency, frequency, and monetary value of customer purchases, known as RFM analysis.

The importance of feature engineering cannot be overstated. Well-engineered features can often lead to significant improvements in model performance, sometimes outweighing the impact of algorithm selection or hyperparameter tuning.

### Processing Pipelines

Processing pipelines bring together the various data processing steps into a coherent, reproducible workflow. These pipelines ensure that data is consistently prepared across training and inference stages, reducing the risk of data leakage and improving the reliability of ML systems.

Modern ML frameworks and tools often provide capabilities for building and managing data processing pipelines. For instance, Apache Beam and TensorFlow Transform allow developers to define data processing steps that can be applied consistently during both model training and serving.

Effective pipeline design involves considerations such as modularity, scalability, and version control. Modular pipelines allow for easy updates and maintenance of individual processing steps. Version control for pipelines is crucial, ensuring that changes in data processing can be tracked and correlated with changes in model performance.

### Scale Considerations

As datasets grow larger and ML systems become more complex, the scalability of data processing becomes increasingly important. Processing large volumes of data efficiently often requires distributed computing approaches and careful consideration of computational resources.

Techniques for scaling data processing include parallel processing, where data is divided across multiple machines or processors for simultaneous processing. Distributed frameworks like Apache Spark are commonly used for this purpose, allowing data processing tasks to be scaled across large clusters of computers.

Another important consideration is the balance between preprocessing and on-the-fly computation. While extensive preprocessing can speed up model training and inference, it can also lead to increased storage requirements and potential data staleness. Some ML systems opt for a hybrid approach, preprocessing certain features while computing others on-the-fly during model training or inference.

Effective data processing is fundamental to the success of ML systems. By carefully cleaning, transforming, and engineering data, practitioners can significantly improve the performance and reliability of their models. As the field of machine learning continues to evolve, so too do the techniques and tools for data processing, making this an exciting and dynamic area of study and practice.

### Case Study: KWS

A KWS system requires careful cleaning of audio recordings to ensure reliable wake word detection. Raw audio data often contains various imperfections - background noise, clipped signals, varying volumes, and inconsistent sampling rates. For example, when processing the wake word "Alexa," the system must clean recordings to standardize volume levels, remove ambient noise, and ensure consistent audio quality across different recording environments, all while preserving the essential characteristics that make the wake word recognizable.

Building on clean data, quality assessment becomes crucial for KWS systems. Quality metrics for KWS data are uniquely focused on audio characteristics, including signal-to-noise ratio (SNR), audio clarity scores, and speaking rate consistency. For instance, a KWS quality assessment pipeline might automatically flag recordings where background noise exceeds acceptable thresholds or where the wake word is spoken too quickly or unclearly, ensuring only high-quality samples are used for model training.

Once quality is assured, transforming audio data for KWS involves converting raw waveforms into formats suitable for ML models. The typical transformation pipeline converts audio signals into spectrograms[^defn-spectrogram] or mel-frequency cepstral coefficients (MFCCs)[^defn-mfcc], standardizing the representation across different recording conditions. This transformation must be consistently applied across both training and inference, often with additional considerations for real-time processing on edge devices.

[^defn-spectrogram]: **Spectrogram:** A visual representation of the spectrum of frequencies in a signal as it varies over time, commonly used in audio processing.

[^defn-mfcc]: **Mel-Frequency Cepstral Coefficients (MFCCs):** Features extracted from audio signals that represent the short-term power spectrum, widely used in speech and audio analysis.

With transformed data in hand, feature engineering for KWS focuses on extracting characteristics that help distinguish wake words from background speech. Engineers might create features capturing tonal variations, speech energy patterns, or temporal characteristics. For the wake word "Alexa," features might include energy distribution across frequency bands, pitch contours, and duration patterns that characterize typical pronunciations.

Finally, bringing all these elements together, KWS processing pipelines must handle both batch processing for training and real-time processing for inference. The pipeline typically includes stages for audio preprocessing, feature extraction, and quality filtering. Importantly, these pipelines must be designed to operate efficiently on edge devices while maintaining consistent processing steps between training and deployment.

## Data Labeling

Data labeling is a process in creating high-quality training datasets for machine learning models. Labels provide ground truth information, allowing models to learn relationships between inputs and desired outputs. This section explores key considerations for selecting label types, formats, and content to capture the necessary information for various ML tasks.

The importance of data labeling cannot be overstated in the context of supervised learning. It forms the foundation upon which models build their understanding of the world. However, labeling is often a resource-intensive process, requiring careful planning and execution to ensure the resulting dataset is both accurate and representative.

We will discuss common annotation approaches, ranging from manual labeling by experts to crowdsourcing and AI-assisted methods. Each approach has its strengths and challenges, and the choice often depends on the specific requirements of the project, including budget constraints, time limitations, and the level of expertise required.

Ensuring label quality is another aspect we will explore. This includes best practices for training annotators, developing comprehensive guidelines, and implementing robust quality control measures. We will also emphasize the ethical treatment of human annotators, an often overlooked but essential consideration in the labeling process.

The integration of AI to accelerate and augment human annotation is an exciting development in this field. We'll examine how AI-assisted annotation can improve efficiency and consistency while discussing potential pitfalls and best practices for its implementation.

Understanding labeling needs, challenges, and strategies is essential for constructing reliable, useful datasets to train performant, trustworthy machine learning systems. As we delve into each aspect of data labeling, we'll provide practical insights and examples to help practitioners navigate this stage of the ML pipeline.

### Label Types

Labels capture information about key tasks or concepts. @fig-labels illustrates some common label types:

* Classification labels categorize images with specific tags (e.g., labeling an image as "dog" if it features a dog)
* Bounding boxes identify object locations by drawing a box around the object of interest
* Segmentation maps classify objects at the pixel level, highlighting the object in a distinct color
* Captions provide descriptive annotations, detailing aspects like the dog's actions, position, or color
* Transcripts denote audio content

The choice of label format depends on the use case and resource constraints, as more detailed labels typically require greater effort to collect [@10.1109/ICRA.2017.7989092].

![An overview of common label types.](images/png/CS249r_Labels.png){#fig-labels}

Unless focused on self-supervised learning, a dataset will likely provide labels addressing one or more tasks of interest. Dataset creators must consider what information labels should capture and how they can practically obtain the necessary labels, given their unique resource constraints.

Creators must first decide what type(s) of content labels should capture. For example, a creator interested in car detection would primarily want to label cars in their dataset. However, they might also consider simultaneously collecting labels for other potentially useful tasks, such as pedestrian detection.

Additionally, annotators can provide metadata that offers insight into how the dataset represents different characteristics of interest. The Common Voice dataset exemplifies this approach, including various types of metadata that provide information about speakers, recordings, and dataset quality for each language represented [@ardila2020common]. This metadata includes demographic splits showing the number of recordings by speaker age range and gender, allowing users to see who contributed recordings for each language. It also includes statistics like average recording duration and total hours of validated recordings, providing insights into the nature and size of the datasets for each language.

Quality control metrics, such as the percentage of recordings that have been validated, are useful for assessing how complete and clean the datasets are. The metadata also includes normalized demographic splits scaled to 100% for comparison across languages, highlighting representation differences between higher and lower resource languages.

After determining the content, creators must choose the format of the labels. For car detection, options might include binary classification labels indicating car presence, bounding boxes showing general car locations, or pixel-wise segmentation labels showing exact car locations. The choice of label format often depends on the specific use case and resource constraints, as finer-grained labels are typically more expensive and time-consuming to acquire. By carefully considering both the content and format of labels, dataset creators can ensure that their labeled data effectively supports the intended machine learning tasks while balancing practical constraints.

### Annotation Methods

Common annotation approaches include manual labeling, crowdsourcing, and semi-automated techniques. Each method has its own strengths and weaknesses, making them suitable for different scenarios in the data labeling process.

Manual labeling by experts typically yields high-quality results but lacks scalability. This approach is particularly valuable for specialized domains where expert knowledge is crucial. For instance, in medical imaging tasks, having experienced radiologists annotate images can provide highly accurate labels. However, the limited availability and high cost of experts make this method challenging to scale for large datasets.

Crowdsourcing enables non-experts to distribute annotation tasks, often through dedicated platforms [@victor2019machine]. This method can significantly increase the speed and scale of labeling efforts. Platforms like Amazon Mechanical Turk or Appen allow dataset creators to access a large pool of annotators, making it possible to label vast amounts of data relatively quickly. However, the quality of crowdsourced labels can vary, necessitating robust quality control measures.

Weakly supervised and programmatic methods can reduce manual effort by heuristically or automatically generating labels [@ratner2018snorkel]. These approaches are particularly useful when dealing with large, unlabeled datasets. For example, distant supervision techniques can automatically label data by leveraging existing knowledge bases. While these methods can rapidly generate large amounts of labeled data, they may introduce noise or biases that need to be carefully managed.

After deciding on their labels' desired content and format, creators begin the annotation process. To collect large numbers of labels from human annotators, creators frequently rely on dedicated annotation platforms, which can connect them to teams of human annotators. When using these platforms, creators may have limited insight into annotators' backgrounds and experience levels with topics of interest. However, some platforms offer access to annotators with specific expertise (e.g., doctors for medical annotation tasks).

The choice of annotation method often depends on factors such as the complexity of the labeling task, the required level of expertise, the size of the dataset, budget constraints, and the desired quality of the labels. In many cases, a combination of these methods may be employed to balance speed, cost, and quality effectively.

For instance, a project might use crowdsourcing for initial labeling, followed by expert review of uncertain or critical cases. Alternatively, programmatic labeling might be used to generate a large set of initial labels, which are then refined through crowdsourcing or expert review.

It's important to note that regardless of the chosen method, clear guidelines and thorough training for annotators are needed. These ensure consistency across labels and help mitigate potential biases or misunderstandings in the annotation process.

### Ensuring Label Quality

Maintaining high label quality is importnat for the success of machine learning models, as the quality of the training data directly impacts model performance. However, there is no guarantee that the data labels are actually correct. @fig-hard-labels illustrates some examples of challenging labeling cases: some errors arise from blurred pictures that make objects hard to identify (as seen in the frog image), while others stem from a lack of domain knowledge (as in the black stork case). It's possible that despite providing the best instructions to labelers, some images may still be mislabeled [@northcutt2021pervasive].

![Some examples of hard labeling cases. Source: @northcutt2021pervasive.](https://raw.githubusercontent.com/cleanlab/assets/master/cleanlab/label-errors-examples.png){#fig-hard-labels}

To address these challenges and ensure label quality, several strategies can be employed. Implementing systematic quality checks through random sampling of labeled data for expert review or using statistical methods to flag potentially incorrect labels can help identify and correct errors. Providing comprehensive training to annotators, including clear guidelines and examples of correct labeling, can significantly improve label quality. This is particularly important for complex or domain-specific tasks.

Collecting multiple labels per datapoint, often called "consensus labeling," can help identify controversial or ambiguous cases. This approach not only helps identify difficult datapoints but also quantifies levels of disagreement, providing insights into the inherent difficulty or subjectivity of certain labeling tasks. For ambiguous tasks, this method can be particularly valuable in understanding the range of possible interpretations.

Iterative refinement, involving continuous review and updating of labels based on model performance and expert feedback, can help improve dataset quality over time. Additionally, incorporating pre-labeled, high-quality data as a "gold standard" can serve as a benchmark to assess annotator performance and calibrate labeling guidelines.

When working with human annotators, it's essential to prioritize ethical treatment and offer fair compensation. Annotators can be subject to exploitation or other forms of harm during the labeling process (Perrigo, 2023). For instance, if a dataset is likely to contain disturbing content, annotators may benefit from having the option to view images in grayscale [@googleinformation]. This can help reduce the potential psychological impact of viewing distressing images.

Furthermore, it's important to consider the potential biases that annotators may bring to the labeling process. These biases can be cultural, personal, or professional, and may inadvertently influence the labels assigned. Strategies to mitigate this include diversifying the annotator pool, providing clear and objective labeling criteria, and conducting regular bias audits of the labeled data.

By implementing these quality assurance measures and ethical considerations, dataset creators can significantly enhance the reliability and usefulness of their labeled data, ultimately leading to more robust and trustworthy machine learning models. The process of ensuring label quality is ongoing and iterative, requiring constant vigilance and adaptation as new challenges and insights emerge throughout the data labeling process.

### AI-Assisted Annotation

Machine learning's insatiable demand for data has led to the development of innovative approaches to data labeling. Rather than always generating and curating data manually, we can now leverage existing AI models to help label datasets more quickly and cost-effectively. While this approach often results in lower quality than human annotation, it can significantly accelerate the labeling process and reduce costs. AI-assisted annotation can be implemented in various ways, as illustrated in @fig-weak-supervision. Some key approaches include the following:

Pre-annotation involves using AI models to generate preliminary labels for a dataset, which humans can then review and correct. This method, which often employs semi-supervised learning techniques [@chapelle2009semisupervised], can save a significant amount of time, especially for large datasets. Pre-annotation can speed up the labeling process and help maintain consistency across labels.

![Strategies for acquiring additional labeled training data. Source: [Standford AI Lab.](https://ai.stanford.edu/blog/weak-supervision/)](https://ai.stanford.edu/blog//assets/img/posts/2019-03-03-weak_supervision/WS_mapping.png){#fig-weak-supervision}

Active learning is another powerful technique in AI-assisted annotation. In this approach, AI models identify the most informative or uncertain data points in a dataset, which are then prioritized for human annotation. This targeted approach can help improve the quality of the labeled dataset while reducing the overall annotation time and effort. 

Quality control is a critical aspect of AI-assisted annotation. AI models can be employed to identify and flag potential errors in human annotations, helping to ensure the accuracy and consistency of the labeled dataset. This can involve detecting outliers, inconsistencies, or patterns that deviate from expected norms, allowing for targeted review and correction of potentially problematic labels.

The application of AI-assisted annotation has been proposed and implemented across various domains. In medical imaging, AI-assisted annotation is being used to label complex medical images such as MRI scans and X-rays [@krishnan2022selfsupervised]. This approach is particularly valuable in the medical field, where expert annotators are scarce and expensive. 

The development of self-driving cars has also benefited from AI-assisted annotation. In this context, AI is used to label images and videos from vehicle-mounted cameras and sensors. This can help train AI models to identify objects on the road, such as other vehicles, pedestrians, and traffic signs. The sheer volume of data generated by self-driving car tests makes AI assistance invaluable in the labeling process.

Social media platforms are another area where AI-assisted annotation is proving useful. Here, AI is employed to label various types of content, including images, videos, and text posts. This can help train AI models to identify and classify different types of content, such as news, advertising, and personal posts, which is important for content moderation, personalized recommendations, and trend analysis.

While AI-assisted annotation offers significant benefits, it's important to note that it also introduces new challenges. The potential for AI systems to propagate or amplify biases present in their training data is a serious concern. Additionally, over-reliance on AI for annotation can lead to a feedback loop where model errors are reinforced rather than corrected. Therefore, it's important to implement robust quality control measures and maintain human oversight in AI-assisted annotation processes.

### Challenges and Limitations in Data Labeling

While data labeling is essential for the development of supervised machine learning models, it comes with its own set of challenges and limitations that practitioners must be aware of and address.

One of the primary challenges in data labeling is the inherent subjectivity in many labeling tasks. Even with clear guidelines, human annotators may interpret data differently, leading to inconsistencies in labeling. This is particularly evident in tasks involving sentiment analysis, image classification of ambiguous objects, or labeling of complex medical conditions. For instance, in a study of medical image annotation, @oakden2020hidden found significant variability in labels assigned by different radiologists, highlighting the challenge of obtaining "ground truth" in inherently subjective tasks.

Scalability presents another significant challenge, especially as datasets grow larger and more complex. Manual labeling is time-consuming and expensive, often becoming a bottleneck in the machine learning pipeline. While crowdsourcing and AI-assisted methods can help address this issue to some extent, they introduce their own complications in terms of quality control and potential biases.

The issue of bias in data labeling is particularly concerning. Annotators bring their own cultural, personal, and professional biases to the labeling process, which can be reflected in the resulting dataset. For example, @wang2019balanced found that image datasets labeled predominantly by annotators from one geographic region showed biases in object recognition tasks, performing poorly on images from other regions. This highlights the need for diverse annotator pools and careful consideration of potential biases in the labeling process.

Data privacy and ethical considerations also pose challenges in data labeling. When dealing with sensitive data, such as medical records or personal communications, ensuring annotator access while maintaining data privacy can be complex. Furthermore, exposing annotators to potentially disturbing content, as often encountered in content moderation tasks, raises ethical concerns about annotator well-being [@steiger2021psychological].

The dynamic nature of real-world data presents another limitation. Labels that are accurate at the time of annotation may become outdated or irrelevant as the underlying distribution of data changes over time. This concept, known as concept drift, necessitates ongoing labeling efforts and periodic re-evaluation of existing labels.

Lastly, the limitations of current labeling approaches become apparent when dealing with edge cases or rare events. In many real-world applications, it's the unusual or rare instances that are often most critical (e.g., rare diseases in medical diagnosis, or unusual road conditions in autonomous driving). However, these cases are, by definition, underrepresented in most datasets and may be overlooked or mislabeled in large-scale annotation efforts.

Addressing these challenges requires a multifaceted approach. This may include developing more sophisticated annotation tools, implementing rigorous quality control measures, fostering diverse annotator pools, and continually refining labeling guidelines. Additionally, emerging techniques such as active learning and semi-supervised learning can help mitigate some of these issues by reducing the amount of labeled data required or focusing human annotation efforts where they're most needed.

As the field continues to evolve, so too must our approaches to data labeling. By acknowledging and addressing these challenges and limitations, we can work towards creating more robust, reliable, and representative labeled datasets, ultimately leading to better performing and more trustworthy machine learning models.

### Case Study: KWS

KWS systems present unique challenges in data labeling. A fundamental labeling challenge in KWS is defining what constitutes a valid wake word utterance. Audio clips must be labeled to indicate not just the presence of the wake word, but also its precise timing boundaries. This requires careful consideration of label types - while simple binary labels (wake word present/absent) might suffice for initial development, production systems often need more detailed labels including start/end times, speech quality indicators, and background conditions.

The labeling process typically combines multiple annotation methods. Expert annotators (often linguists or speech scientists) establish initial labeling guidelines and create gold-standard datasets. These experts define criteria for acceptable pronunciations, handling of accents, and treatment of background noise. Crowdsourcing then helps scale the labeling process, with platforms like Amazon Mechanical Turk used to collect diverse examples of wake word utterances across different speakers and environments.

Quality assurance becomes particularly critical for KWS labeling. Inter-annotator agreement helps identify ambiguous cases - for instance, when background noise makes the wake word unclear, or when similar-sounding phrases create confusion. Multiple annotators typically label each audio clip, with disagreements flagged for expert review. AI-assisted labeling supports this process by pre-screening audio clips for quality issues and helping identify potential mislabels.

This case study demonstrates how data labeling decisions directly impact the performance and reliability of production ML systems. For KWS, effective labeling strategies must balance accuracy requirements with practical constraints while ensuring the resulting dataset captures the diversity of real-world usage scenarios.

## Data Storage

Machine learning workloads have data access patterns that differ markedly from those of traditional transactional systems or routine analytics. Whereas transactional databases optimize for frequent writes and row-level updates, most ML pipelines rely on high-throughput reads, large-scale data scans, and evolving schemas. This difference reflects the iterative nature of model development: data scientists repeatedly load and transform vast datasets to engineer features, test new hypotheses, and refine models.

Additionally, ML pipelines must accommodate real-world considerations such as evolving business requirements, new data sources, and changes in data availability. These realities push storage solutions to be both scalable and flexible, ensuring that organizations can manage data collected from diverse channels—from sensor feeds to social media text—without constantly retooling the entire infrastructure. In this section, we will compare the practical use of databases, data warehouses, and data lakes for ML projects, then delve into how specialized services, metadata, and governance practices unify these varied systems into a coherent strategy.

### Storage Systems

When considering storage systems for ML, it's essential to understand the differences among databases, data warehouses, and data lakes. Each system has its strengths and is suited to different aspects of ML workflows. 

Table @tbl-storage provides a comparative overview of these storage systems. Databases usually support operational and transactional purposes. They work well for smaller, well-structured datasets, but can become cumbersome and expensive when applied to large-scale ML contexts involving unstructured data (such as images, audio, or free-form text).


+-------------------------------------+--------------------------+-------------------------------------------------------------+
| Database                            | Data Warehouse           | Data Lake                                                   |
+:====================================+:=========================+:============================================================+
| Purpose                             | Operational and          | Analytical                                                  |
|                                     | transactional            |                                                             |
+-------------------------------------+--------------------------+-------------------------------------------------------------+
| Data type                           | Structured               | Structured, semi-structured, and/or unstructured            |
+-------------------------------------+--------------------------+-------------------------------------------------------------+
| Scale                               | Small to large volumes   | Large volumes of integrated data                            |
|                                     | of data                  | Large volumes of diverse data                               |
+-------------------------------------+--------------------------+-------------------------------------------------------------+
| Examples                            | MySQL                    | Google BigQuery, Amazon Redshift, Microsoft Azure Synapse,  |
|                                     |                          | Google Cloud Storage, AWS S3, Azure Data Lake Storage       |
+-------------------------------------+--------------------------+-------------------------------------------------------------+

: Comparative overview of the database, data warehouse, and data lake. {#tbl-storage .striped .hover}

Data warehouses, by contrast, are optimized for analytical queries across integrated datasets that have been transformed into a standardized schema. As indicated in the table, they handle large volumes of integrated data. Many ML systems successfully draw on data warehouses to power model training because the structured environment simplifies data exploration and feature engineering. Yet one limitation remains: a data warehouse may not accommodate truly unstructured data or rapidly changing data formats, particularly if the data originates from web scraping or Internet of Things (IoT) sensors.

Data lakes address this gap by storing structured, semi-structured, and unstructured data in its native format, deferring schema definitions until the point of reading or analysis (sometimes called _schema-on-read_)[^defn-schema-on-read]. As Table @tbl-storage shows, data lakes can handle large volumes of diverse data types. This approach grants data scientists tremendous latitude when dealing with experimental use cases or novel data types. However, data lakes also demand careful cataloging and metadata management. Without sufficient governance, these expansive repositories risk devolving into unsearchable, disorganized silos.

The examples provided in Table @tbl-storage illustrate the range of technologies available for each storage system type. For instance, MySQL represents a traditional database system, while solutions like Google BigQuery and Amazon Redshift are examples of modern, cloud-based data warehouses. For data lakes, cloud storage solutions such as Google Cloud Storage, AWS S3, and Azure Data Lake Storage are commonly used due to their scalability and flexibility.

### Storage Considerations

While traditional storage systems provide a foundation for ML workflows, the unique characteristics of machine learning workloads necessitate additional considerations. These ML-specific storage needs stem from the nature of ML development, training, and deployment processes, and addressing them is necessary for building efficient and scalable ML systems.

One of the primary challenges in ML storage is handling large model weights. Modern ML models, especially deep learning models, can have millions or even billions of parameters. For instance, GPT-3, a large language model, has 175 billion parameters, requiring approximately 350 GB of storage just for the model weights. Storage systems need to be capable of handling these large, often dense, numerical arrays efficiently, both in terms of storage capacity and access speed. This requirement goes beyond traditional data storage and enters the realm of high-performance computing storage solutions.

The iterative nature of ML development introduces another critical storage consideration: versioning for both datasets and models. Unlike traditional software version control, ML versioning needs to track large binary files efficiently. As data scientists experiment with different model architectures and hyperparameters, they generate numerous versions of models and datasets. Effective storage systems for ML must provide mechanisms to track these changes, revert to previous versions, and maintain reproducibility throughout the ML lifecycle. This capability is essential not only for development efficiency but also for regulatory compliance and model auditing in production environments.

Distributed training, often necessary for large models or datasets, generates substantial intermediate data, including partial model updates, gradients, and checkpoints. Storage systems for ML need to handle frequent, possibly concurrent, read and write operations of these intermediate results. Moreover, they should provide low-latency access to support efficient synchronization between distributed workers. This requirement pushes storage systems to balance between high throughput for large data transfers and low latency for quick synchronization operations.

The diversity of data types in ML workflows presents another unique challenge. ML systems often work with a wide variety of data - from structured tabular data to unstructured images, audio, and text. Storage systems need to efficiently handle this diversity, often requiring a combination of different storage technologies optimized for specific data types. For instance, a single ML project might need to store and process tabular data in a columnar format for efficient feature extraction, while also managing large volumes of image data for computer vision tasks.

As organizations collect more data and create more sophisticated models, storage systems need to scale seamlessly. This scalability should support not just growing data volumes, but also increasing concurrent access from multiple data scientists and ML models. Cloud-based object storage systems have emerged as a popular solution due to their virtually unlimited scalability, but they introduce their own challenges in terms of data access latency and cost management.

The tension between sequential read performance for training and random access for inference is another key consideration. While training on large datasets benefits from high-throughput sequential reads, many ML serving scenarios require fast random access to individual data points or features. Storage systems for ML need to balance these potentially conflicting requirements, often leading to tiered storage architectures where frequently accessed data is kept in high-performance storage while less frequently used data is moved to cheaper, higher-latency storage.

The choice and configuration of storage systems can significantly impact the performance, cost-effectiveness, and overall success of ML initiatives. As the field of machine learning continues to evolve, storage solutions will need to adapt to meet the changing demands of increasingly sophisticated ML workflows.

### Performance Considerations

The performance of storage systems is critical in ML workflows, directly impacting the efficiency of model training, the responsiveness of inference, and the overall productivity of data science teams. Understanding and optimizing storage performance requires a focus on several key metrics and strategies tailored to ML workloads.

One of the primary performance metrics for ML storage is throughput, particularly for large-scale data processing and model training. High throughput is essential when ingesting and preprocessing vast datasets or when reading large batches of data during model training. For instance, distributed training of deep learning models on large datasets may require sustained read throughput of several gigabytes per second to keep GPU accelerators fully utilized.

Latency is another metric, especially for online inference and interactive data exploration. Low latency access to individual data points or small batches of data is vital for maintaining responsive ML services. In recommendation systems or real-time fraud detection, for example, storage systems must be able to retrieve relevant features or model parameters within milliseconds to meet strict service level agreements (SLAs).

The choice of file format can significantly impact both throughput and latency. Columnar storage formats such as Parquet or ORC are particularly well-suited for ML workloads. These formats allow for efficient retrieval of specific features without reading entire records, substantially reducing I/O operations and speeding up data loading for model training and inference. For example, when training a model that only requires a subset of features from a large dataset, columnar formats can reduce data read times by an order of magnitude compared to row-based formats.

Compression is another key factor in storage performance optimization. While compression reduces storage costs and can improve read performance by reducing the amount of data transferred from disk, it also introduces computational overhead for decompression. The choice of compression algorithm often involves a trade-off between compression ratio and decompression speed. For ML workloads, fast decompression is usually prioritized over maximum compression, with algorithms like Snappy or LZ4 being popular choices.

Data partitioning strategies play a role in optimizing query performance for ML workloads. By intelligently partitioning data based on frequently used query parameters (such as date ranges or categorical variables), systems can dramatically improve the efficiency of data retrieval operations. For instance, in a recommendation system processing user interactions, partitioning data by user demographic attributes and time periods can significantly speed up the retrieval of relevant training data for personalized models.

To handle the scale of data in modern ML systems, distributed storage architectures are often employed. These systems, such as [HDFS (Hadoop Distributed File System)](https://hadoop.apache.org/docs/r1.2.1/hdfs_design.html) or cloud-based object stores like [Amazon S3](https://aws.amazon.com/s3/), distribute data across multiple machines or data centers. This approach not only provides scalability but also enables parallel data access, which can substantially improve read performance for large-scale data processing tasks common in ML workflows.

Caching strategies are also vital for optimizing storage performance in ML systems. In-memory caching of frequently accessed data or computed features can significantly reduce latency and computational overhead. Distributed caching systems like Redis or Memcached are often used to scale caching capabilities across clusters of machines, providing low-latency access to hot data for distributed training or serving systems.

As ML workflows increasingly span from cloud to edge devices, storage performance considerations must extend to these distributed environments. Edge caching and intelligent data synchronization strategies become needed for maintaining performance in scenarios where network connectivity may be limited or unreliable. In the end, the goal is to create a storage infrastructure that can handle the volume and velocity of data in ML workflows while providing the low-latency access needed for responsive model training and inference.

### Storage Across ML Lifecycle Phases

The storage needs of machine learning systems evolve significantly across different phases of the ML lifecycle. Understanding these changing requirements is important for designing effective and efficient ML infrastructures.

#### Development Phase

In the development phase, storage systems play a critical role in supporting exploratory data analysis and iterative model development. This stage demands flexibility and collaboration, as data scientists often work with various datasets, experiment with feature engineering techniques, and rapidly iterate on model designs to refine their approaches.

One of the key challenges at this stage is managing the versions of datasets used in experiments. While traditional version control systems like Git excel at tracking code changes, they fall short when dealing with large datasets. This gap has led to the emergence of specialized tools like DVC (Data Version Control), which enable data scientists to efficiently track dataset changes, revert to previous versions, and share large files without duplication. These tools ensure that teams can maintain reproducibility and transparency throughout the iterative development process.

Balancing data accessibility and security further complicates the storage requirements in this phase. Data scientists require seamless access to datasets for experimentation, but organizations must simultaneously safeguard sensitive data. This tension often results in the implementation of sophisticated access control mechanisms, ensuring that datasets remain both accessible and protected. Secure data sharing systems enhance collaboration while adhering to strict organizational and regulatory requirements, enabling teams to work productively without compromising data integrity.

#### Training Phase

The training phase presents unique storage challenges due to the sheer volume of data processed and the computational intensity of model training. At this stage, the interplay between storage performance and computational efficiency becomes critical, as modern ML algorithms demand seamless integration between data access and processing.

To meet these demands, high-performance storage systems must provide the throughput required to feed data to multiple GPU or TPU accelerators simultaneously. Distributed training scenarios amplify this need, often requiring data transfer rates in the gigabytes per second range to ensure that accelerators remain fully utilized. This highlights the importance of optimizing storage for both capacity and speed.

Beyond data ingestion, managing intermediate results and checkpoints is another critical challenge in the training phase. Long-running training jobs frequently save intermediate model states to allow for resumption in case of interruptions. These checkpoints can grow significantly in size, especially for large-scale models, necessitating storage solutions that enable efficient saving and retrieval without impacting overall performance.

Complementing these systems is the concept of burst buffers[^defn-burst-buffers], borrowed from high-performance computing. These high-speed, temporary storage layers are particularly valuable during training, as they can absorb large, bursty I/O operations. By buffering these spikes in demand, burst buffers help smooth out performance fluctuations and reduce the load on primary storage systems, ensuring that training pipelines remain efficient and reliable.

[^defn-burst-buffers]: **Burst Buffers:** High-speed storage layers used to absorb large, temporary I/O demands in high-performance computing, smoothing performance during data-intensive operations.

#### Deployment and Serving Phase

In the deployment and serving phase, the focus shifts from high-throughput batch operations during training to low-latency, often real-time, data access. This transition highlights the need to balance conflicting requirements, where storage systems must simultaneously support responsive model serving and enable continued learning in dynamic environments.

Real-time inference demands storage solutions capable of extremely fast access to model parameters and relevant features. To achieve this, systems often rely on in-memory databases or sophisticated caching strategies, ensuring that predictions can be made within milliseconds. These requirements become even more challenging in edge deployment scenarios, where devices operate with limited storage resources and intermittent connectivity to central data stores.

Adding to this complexity is the need to manage model updates in production environments. Storage systems must facilitate smooth transitions between model versions, ensuring minimal disruption to ongoing services. Techniques like shadow deployment, where new models run alongside existing ones for validation, allow organizations to iteratively roll out updates while monitoring their performance in real-world conditions.

#### Monitoring and Maintenance Phase

The monitoring and maintenance phase brings its own set of storage challenges, centered on ensuring the long-term reliability and performance of ML systems. At this stage, the focus shifts to capturing and analyzing data to monitor model behavior, detect issues, and maintain compliance with regulatory requirements.

A critical aspect of this phase is managing data drift, where the characteristics of incoming data change over time. Storage systems must efficiently capture and store incoming data along with prediction results, enabling ongoing analysis to detect and address shifts in data distributions. This ensures that models remain accurate and aligned with their intended use cases.

The sheer volume of logging and monitoring data generated by high-traffic ML services introduces questions of data retention and accessibility. Organizations must balance the need to retain historical data for analysis against the cost and complexity of storing it. Strategies such as tiered storage and compression can help manage costs while ensuring that critical data remains accessible when needed.

Regulated industries often require immutable storage to support auditing and compliance efforts. Storage systems designed for this purpose guarantee data integrity and non-repudiability, ensuring that stored data cannot be altered or deleted. Blockchain-inspired solutions and write-once-read-many (WORM) technologies are commonly employed to meet these stringent requirements.

### Feature Stores

Feature stores are a centralized repository that stores and serves pre-computed features for machine learning models, ensuring consistency between training and inference workflows. They have emerged as a critical component in the ML infrastructure stack, addressing the unique challenges of managing and serving features for machine learning models. They act as a central repository for storing, managing, and serving machine learning features, bridging the gap between data engineering and machine learning operations.

What makes feature stores particularly interesting is their role in solving several key challenges in ML pipelines. First, they address the problem of feature consistency between training and serving environments. In traditional ML workflows, features are often computed differently in offline (training) and online (serving) environments, leading to discrepancies that can degrade model performance. Feature stores provide a single source of truth for feature definitions, ensuring consistency across all stages of the ML lifecycle.

Another fascinating aspect of feature stores is their ability to promote feature reuse across different models and teams within an organization. By centralizing feature computation and storage, feature stores can significantly reduce redundant work. For instance, if multiple teams are working on different models that require similar features (e.g., customer lifetime value in a retail context), these features can be computed once and reused across projects, improving efficiency and consistency.

Feature stores also play a role in managing the temporal aspects of features. Many ML use cases require correct point-in-time feature values, especially in scenarios involving time-series data or where historical context is important. Feature stores typically offer time-travel capabilities, allowing data scientists to retrieve feature values as they were at any point in the past. This is crucial for training models on historical data and for ensuring consistency between training and serving environments.

The performance characteristics of feature stores are particularly intriguing from a storage perspective. They need to support both high-throughput batch retrieval for model training and low-latency lookups for online inference. This often leads to hybrid architectures where feature stores maintain both an offline store (optimized for batch operations) and an online store (optimized for real-time serving). Synchronization between these stores becomes a critical consideration.

Feature stores also introduce interesting challenges in terms of data freshness and update strategies. Some features may need to be updated in real-time (e.g., current user session information), while others might be updated on a daily or weekly basis (e.g., aggregated customer behavior metrics). Managing these different update frequencies and ensuring that the most up-to-date features are always available for inference can be complex.

From a storage perspective, feature stores often leverage a combination of different storage technologies to meet their diverse requirements. This might include columnar storage formats like Parquet for the offline store, in-memory databases or key-value stores for the online store, and streaming platforms like Kafka for real-time feature updates.

As feature stores continue to evolve, we're seeing interesting developments in areas like automated feature discovery, where systems analyze raw data to suggest potentially useful features, and feature monitoring, where the distribution and quality of features are continuously tracked to detect data drift or quality issues.

### Caching Strategies

Caching plays a role in optimizing the performance of ML systems, particularly in scenarios involving frequent data access or computation-intensive operations. In the context of machine learning, caching strategies extend beyond traditional web or database caching, addressing unique challenges posed by ML workflows.

One of the primary applications of caching in ML systems is in feature computation and serving. Many features used in ML models are computationally expensive to calculate, especially those involving complex aggregations or time-window operations. By caching these computed features, systems can significantly reduce latency in both training and inference scenarios. For instance, in a recommendation system, caching user embedding vectors can dramatically speed up the generation of personalized recommendations.

Caching strategies in ML systems often need to balance between memory usage and computation time. This trade-off is particularly evident in large-scale distributed training scenarios. Caching frequently accessed data shards or mini-batches in memory can significantly reduce I/O overhead, but it requires careful memory management to avoid out-of-memory errors, especially when working with large datasets or models.

Another interesting application of caching in ML systems is model caching. In scenarios where multiple versions of a model are deployed (e.g., for A/B testing or gradual rollout), caching the most frequently used model versions in memory can significantly reduce inference latency. This becomes especially important in edge computing scenarios, where storage and computation resources are limited.

Caching also plays a vital role in managing intermediate results in ML pipelines. For instance, in feature engineering pipelines that involve multiple transformation steps, caching intermediate results can prevent redundant computations when rerunning pipelines with minor changes. This is particularly useful during the iterative process of model development and experimentation.

One of the challenges in implementing effective caching strategies for ML is managing cache invalidation and updates. ML systems often deal with dynamic data where feature values or model parameters may change over time. Implementing efficient cache update mechanisms that balance between data freshness and system performance is an ongoing area of research and development.

Distributed caching becomes particularly important in large-scale ML systems. Technologies like Redis or Memcached are often employed to create distributed caching layers that can serve multiple training or inference nodes. These distributed caches need to handle challenges like maintaining consistency across nodes and managing failover scenarios.

Edge caching is another fascinating area in ML systems, especially with the growing trend of edge AI. In these scenarios, caching strategies need to account for limited storage and computational resources on edge devices, as well as potentially intermittent network connectivity. Intelligent caching strategies that prioritize the most relevant data or model components for each edge device can significantly improve the performance and reliability of edge ML systems.

Lastly, the concept of semantic caching[^defn-semantic-cache] is gaining traction in ML systems. Unlike traditional caching that operates on exact matches, semantic caching attempts to reuse cached results for semantically similar queries. This can be particularly useful in ML systems where slight variations in input may not significantly change the output, potentially leading to substantial performance improvements.

[^defn-semantic-cache]: **Semantic Caching:** A caching technique that reuses results of previous computations for semantically similar queries, reducing redundancy in data processing.

### Access Patterns

Understanding the access patterns in ML systems is useful for designing efficient storage solutions and optimizing the overall system performance. ML workloads exhibit distinct data access patterns that often differ significantly from traditional database or analytics workloads.

One of the most prominent access patterns in ML systems is sequential reading of large datasets during model training. Unlike transactional systems that typically access small amounts of data randomly, ML training often involves reading entire datasets multiple times (epochs) in a sequential manner. This pattern is particularly evident in deep learning tasks, where large volumes of data are fed through neural networks repeatedly. Storage systems optimized for high-throughput sequential reads, such as distributed file systems or object stores, are well-suited for this access pattern.

However, the sequential read pattern is often combined with random shuffling between epochs to prevent overfitting and improve model generalization. This introduces an interesting challenge for storage systems, as they need to efficiently support both sequential and random access patterns, often within the same training job.

In contrast to the bulk sequential reads common in training, inference workloads often require fast random access to specific data points or features. For example, a recommendation system might need to quickly retrieve user and item features for real-time personalization. This necessitates storage solutions with low-latency random read capabilities, often leading to the use of in-memory databases or caching layers.

Feature stores, which we discussed earlier, introduce their own unique access patterns. They typically need to support both high-throughput batch reads for offline training and low-latency point lookups for online inference. This dual-nature access pattern often leads to the implementation of separate offline and online storage layers, each optimized for its specific access pattern.

Time-series data, common in many ML applications such as financial forecasting or IoT analytics, presents another interesting access pattern. These workloads often involve reading contiguous blocks of time-ordered data, but may also require efficient retrieval of specific time ranges or periodic patterns. Specialized time-series databases or carefully designed partitioning schemes in general-purpose databases are often employed to optimize these access patterns.

<!-- Distributed training introduces additional complexity to access patterns. In scenarios using data parallelism, multiple nodes may be reading different portions of the dataset simultaneously, requiring storage systems that can efficiently handle parallel access. Model parallelism, where different parts of a large model are distributed across nodes, can lead to more complex access patterns where nodes frequently exchange parameter updates.

Emerging techniques like federated learning introduce novel access patterns where models, rather than data, are transferred between edge devices and central servers. This inverts the traditional data access pattern and requires storage systems to efficiently manage and version model updates from multiple sources. -->

Another important consideration is the write access pattern in ML systems. While training workloads are often read-heavy, there are scenarios that involve significant write operations. For instance, continual learning systems may frequently update model parameters, and online learning systems may need to efficiently append new training examples to existing datasets.

<!-- The rise of AutoML and hyperparameter tuning has also influenced access patterns in ML systems. These techniques often involve training many model variants in parallel, each potentially accessing the same underlying dataset. This can lead to high concurrent read access to shared datasets, requiring storage systems that can efficiently serve multiple parallel training jobs. -->

Understanding these diverse access patterns is helpful in designing and optimizing storage systems for ML workloads. It often leads to hybrid storage architectures that combine different technologies to address various access patterns efficiently. For example, a system might use object storage for large-scale sequential reads during training, in-memory databases for low-latency random access during inference, and specialized time-series storage for temporal data analysis.

As ML systems continue to evolve, new access patterns are likely to emerge, driving further innovation in storage technologies and architectures. The challenge lies in creating flexible, scalable storage solutions that can efficiently support the diverse and often unpredictable access patterns of modern ML workloads.

### Case Study: KWS

During development and training, KWS systems must efficiently store and manage large collections of audio data. This includes raw audio recordings from various sources (crowd-sourced, synthetic, and real-world captures), processed features (like spectrograms or MFCCs), and model checkpoints. A typical architecture might use a data lake for raw audio files, allowing flexible storage of diverse audio formats, while processed features are stored in a more structured data warehouse for efficient access during training.

KWS systems benefit significantly from feature stores, particularly for managing pre-computed audio features. For example, commonly used spectrogram representations or audio embeddings can be computed once and stored for reuse across different experiments or model versions. The feature store must handle both batch access for training and real-time access for inference, often implementing a dual storage architecture - an offline store for training data and an online store for low-latency inference.

In production, KWS systems require careful consideration of edge storage requirements. The models must be compact enough to fit on resource-constrained devices while maintaining quick access to necessary parameters for real-time wake word detection. This often involves optimized storage formats and careful caching strategies to balance between memory usage and inference speed.

## Data Governance

Data governance is a significant component in the development and deployment of ML systems. It encompasses a set of practices and policies that ensure data is accurate, secure, compliant, and ethically used throughout the ML lifecycle. As ML systems become increasingly integral to decision-making processes across various domains, the importance of robust data governance has grown significantly.

Data governance extends beyond traditional data management practices. It addresses unique challenges posed by the complex and often opaque nature of ML workflows. These challenges include maintaining data privacy, ensuring model fairness, and providing transparency in decision-making processes.

One of the primary aspects of data governance in ML systems is security and access control. This involves implementing measures to protect data from unauthorized access or breaches. In ML systems, this often means implementing fine-grained access controls and encrypting data both at rest and in transit. For instance, in a healthcare ML application, not all researchers may need access to all patient data. A well-designed governance framework would ensure that data scientists only have access to the specific data required for their work.

Privacy protection is another element of data governance. ML models often require large amounts of data, which can conflict with individual privacy rights. Techniques such as differential privacy can be employed to protect individual privacy while maintaining the statistical utility of the data. This approach adds carefully calibrated noise to the data, making it difficult to identify individuals while preserving the overall patterns necessary for effective model training.

Regulatory compliance forms a non-negotiable part of data governance in many industries. For example, the [General Data Protection Regulation (GDPR)](https://gdpr-info.eu/) in Europe and the [Health Insurance Portability and Accountability Act (HIPAA)](https://www.cdc.gov/phlp/php/resources/health-insurance-portability-and-accountability-act-of-1996-hipaa.html) in the U.S. healthcare sector impose strict requirements on data handling and usage. These regulations may dictate how data can be collected, used, stored, and deleted. Compliance with these regulations often necessitates features like the ability to provide individuals with copies of their data or to delete it upon request.

Documentation and metadata management play a role in ensuring transparency and reproducibility in ML systems. This includes maintaining clear records of data lineage - tracking how data flows and transforms through the ML pipeline. Additionally, implementing standardized documentation practices like "model cards," as introduced by @mitchell2019model, can provide a consistent way to document the performance characteristics, intended uses, and limitations of ML models.

Comprehensive audit trails are another key aspect of data governance. These detailed logs of data access and usage are necessary for accountability and troubleshooting. Audit trails should track the entire lifecycle of ML models, from data collection through to deployment. In the event of a data breach or questions about model behavior, these audit trails become invaluable for understanding what happened and why.

Consider a hypothetical ML system designed to predict patient outcomes in a hospital. Such a system would need to address several governance challenges. It would need to ensure that patient data is securely stored and accessed only by authorized personnel. Privacy-preserving techniques would be necessary to protect individual patient identities while allowing for aggregate analysis. The system would need to comply with healthcare regulations regarding the use of patient data for research and decision-making. Detailed documentation of how patient data is used in the model, including any transformations or feature engineering steps, would be essential. Finally, comprehensive audit logs of who accessed the data and for what purpose would need to be maintained.

As ML systems grow more complex and influential, the challenges of data governance will likely increase. Emerging trends in this field may include the use of blockchain-inspired technologies for maintaining tamper-evident logs of data usage, or the development of automated governance tools that can detect potential issues in real-time.

Understanding and implementing robust data governance practices is necessary for ensuring that ML systems not only perform well technically but also maintain ethical standards and public trust. As the field of ML continues to evolve, so too will the strategies and technologies used for effective data governance. For students and practitioners in the field of ML, mastering these governance principles is as important as understanding the technical aspects of building ML models. The responsible and ethical use of data is fundamental to the long-term success and acceptance of ML technologies in society.

## Conclusion

Data engineering is the backbone of any successful ML system. By thoughtfully defining problems, designing robust pipelines, and practicing rigorous data governance, teams establish a foundation that directly influences model performance, reliability, and ethical standing. Effective data acquisition strategies—whether through existing datasets, web scraping, or crowdsourcing—must balance the realities of domain constraints, privacy obligations, and labeling complexities. Likewise, decisions around data ingestion (batch or streaming) and transformation (ETL or ELT) affect both cost and throughput, with monitoring and observability essential to detect shifting data quality.

Throughout this chapter, we saw how critical it is to prepare data well in advance of modeling. Data labeling emerges as a particularly delicate phase: it involves human effort, requires strong quality control practices, and has ethical ramifications. Storage choices—relational databases, data warehouses, data lakes, or specialized systems—must align with both the volume and velocity of ML workloads. Feature stores and caching strategies support efficient retrieval across training and serving pipelines, while good data governance ensures adherence to legal regulations, protects privacy, and maintains stakeholder trust.

All these elements interlock to create an ecosystem that reliably supplies ML models with the high-quality data they need. When done well, data engineering empowers teams to iterate faster, confidently deploy new features, and build systems capable of adapting to real-world complexity. The next chapters will build on these foundations, exploring how optimized training, robust model operations, and security considerations together form a holistic approach to delivering AI solutions that perform reliably and responsibly at scale.

## Resources {#sec-data-engineering-resource}

Here is a curated list of resources to support students and instructors in their learning and teaching journeys. We are continuously working on expanding this collection and will add new exercises soon.

:::{.callout-note collapse="false"}

#### Slides

These slides are a valuable tool for instructors to deliver lectures and for students to review the material at their own pace. We encourage students and instructors to leverage these slides to improve their understanding and facilitate effective knowledge transfer.

* [Data Engineering: Overview.](https://docs.google.com/presentation/d/1jlIfD6RtQWG8314jCAu1qdnG7YyESy60Yt5-zXhEsVA/edit#slide=id.g202a7c05d1a_0_0)

* [Feature engineering.](https://docs.google.com/presentation/d/1AIM1H-GfvjNPHQw9urxJz3vtMgb_9kizfthbymISPR4/edit#slide=id.g202a83498d1_0_0)

* [Data Standards: Speech Commands.](https://docs.google.com/presentation/d/1qDoHc7yzZ2lEha9NTMZ07Ls4tkIz-1f7kUYRlvjzsI4/edit?usp=drive_link&resourcekey=0-ol4Oqk_y706P_zIB5mbu7Q)

* [Crowdsourcing Data for the Long Tail.](https://docs.google.com/presentation/d/1d3KUit64L-4dXecCNBpikCxx7VO0xIJ13r9v1Ad22S4/edit#slide=id.ga4ca29c69e_0_179)

* [Reusing and Adapting Existing Datasets.](https://docs.google.com/presentation/d/1mHecDoCYHQD9nWSRYCrXXG0IOp9wYQk-fbxhoNIsGMY/edit#slide=id.ga4ca29c69e_0_206)

* [Responsible Data Collection.](https://docs.google.com/presentation/d/1vcmuhLVNFT2asKSCSGh_Ix9ht0mJZxMii8MufEMQhFA/edit?resourcekey=0-_pYLcW5aF3p3Bvud0PPQNg#slide=id.ga4ca29c69e_0_195)

* Data Anomaly Detection:
  * [Anomaly Detection: Overview.](https://docs.google.com/presentation/d/1R8A_5zKDZDZOdAb1XF9ovIOUTLWSIuFWDs20-avtxbM/edit?resourcekey=0-pklEaPv8PmLQ3ZzRYgRNxw#slide=id.g94db9f9f78_0_2)

  * [Anomaly Detection: Challenges.](https://docs.google.com/presentation/d/1JZxx2kLaO1a8O6z6rRVFpK0DN-8VMkaSrNnmk_VGbI4/edit#slide=id.g53eb988857_0_91)

  * [Anomaly Detection: Datasets.](https://docs.google.com/presentation/d/1wPDhp4RxVrOonp6pU0Capk0LWXZOGZ3x9BzW_VjpTQw/edit?resourcekey=0-y6wKAnuxrLWqhleq9ruLOA#slide=id.g53eb988857_0_91)

  * [Anomaly Detection: using Autoencoders.](https://docs.google.com/presentation/d/1Q4h7XrayNRIP0r52Hlk5VjxRcli-GY2xmyZ53nCd6CI/edit#slide=id.g53eb988857_0_91)

:::

:::{.callout-important collapse="false"}

#### Videos

* _Coming soon._
:::

:::{.callout-caution collapse="false"}

#### Exercises

To reinforce the concepts covered in this chapter, we have curated a set of exercises that challenge students to apply their knowledge and deepen their understanding.

* @exr-kws

* @exr-ws

* @exr-sd

* @exr-dp

* @exr-bl
