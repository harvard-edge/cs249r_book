---
bibliography: workflow.bib
---

# AI Workflow {#sec-ai_workflow}

::: {.content-visible when-format="html"}
Resources: [Slides](#sec-ai-workflow-resource), [Videos](#sec-ai-workflow-resource), [Exercises](#sec-ai-workflow-resource)
:::

![_DALL·E 3 Prompt: Create a rectangular illustration of a stylized flowchart representing the AI workflow/pipeline. From left to right, depict the stages as follows: 'Data Collection' with a database icon, 'Data Preprocessing' with a filter icon, 'Model Design' with a brain icon, 'Training' with a weight icon, 'Evaluation' with a checkmark, and 'Deployment' with a rocket. Connect each stage with arrows to guide the viewer horizontally through the AI processes, emphasizing these steps' sequential and interconnected nature._](images/png/cover_ai_workflow.png)

## Purpose {.unnumbered}

_What are the diverse elements of AI systems and how do we combine to create effective machine learning system solutions?_

The creation of practical AI solutions requires the orchestration of multiple system components into coherent workflows. Architectural patterns serve as building blocks, while workflow design highlights the connections and interactions that animate these components. This systematic perspective reveals how data flow, model training, and deployment considerations are intertwined to form robust AI systems. Analyzing these interconnections offers crucial insights into system-level design choices, establishing a framework for understanding how theoretical concepts can be translated into deployable solutions that meet real-world needs.

::: {.callout-tip}

## Learning Objectives

* Understand the ML workflow and gain insights into the structured approach and stages of developing, deploying, and maintaining machine learning models.

* Learn about the unique challenges and distinctions between workflows for Traditional machine learning and embedded AI.

* Appreciate the roles in ML projects and understand their responsibilities and significance.

* Understanding the importance, applications, and considerations for implementing ML models in resource-constrained environments.

* Gain awareness about the ethical and legal aspects that must be considered and adhered to in ML and embedded AI projects.

* Establish a basic understanding of ML workflows and roles to be well-prepared for deeper exploration in the following chapters.

:::

## Overview

The ML workflow is a structured approach encompassing the entire machine learning system lifecycle, from initial development through deployment and maintenance. While individual algorithmic components are important, the success of ML systems heavily depends on the surrounding infrastructure, tooling, and practices---aspects that are often overlooked but create significant technical debt when neglected.

Building effective ML systems requires orchestrating both the model development pipeline and the broader system architecture that supports it. From a model development perspective, teams face challenges in selecting appropriate algorithms, tuning hyperparameters, managing training data quality, and ensuring model convergence. These fundamental ML challenges are amplified when moving from experimental environments to production systems, where models must maintain performance while meeting strict operational requirements.

The system-level challenges of ML workflows present equally complex demands on infrastructure and architecture. Teams must design scalable data pipelines, efficient model serving infrastructure, comprehensive monitoring systems, and robust resource management frameworks. Each component must work in concert while adhering to constraints around computational efficiency, memory usage, power consumption, and latency requirements. Success requires careful attention to both the technical aspects of model deployment and the operational considerations of running ML systems at scale.

In this chapter, we explore the machine learning workflow from a systems perspective, examining how different components interact and the infrastructure needed to support them at a high level. We begin by introducing the ML lifecycle and its key stages. As @fig-lifecycle-overview illustrates, it typically involves the following key steps:

![ML lifecycle overview.](images/png/ML_life_cycle_overview.png){#fig-lifecycle-overview}

1. **Problem Definition:** Start by clearly articulating the specific problem you want to solve. This focuses on your efforts during data collection and model building.
2. **Data Collection and Preparation:** Gather relevant, high-quality training data that captures all aspects of the problem. Clean and preprocess the data to prepare it for modeling.
3. **Model Development and Training:** Choose a machine learning algorithm suited to your problem type and data. Consider the pros and cons of different approaches. Feed the prepared data into the model to train it. Training time varies based on data size and model complexity.
4. **Model Evaluation and Validation:** Test the trained model on new unseen data to measure its predictive accuracy. Identify any limitations.
5. **Model Deployment and Integration:** Integrate the validated model into applications or systems to start operationalization.
6. **Monitoring and Maintenance:** Track model performance in production. Retrain periodically on new data to keep it current.

We will continue this discussion with an examination of how these stages are manifested in various deployment scenarios. We will also discuss the roles and responsibilities in ML systems development, and conclude with practical considerations for implementation. While we are providing a high-level overview here, subsequent chapters will delve into core concepts such as data engineering, ML frameworks, training infrastructure, and deployment optimizations.

This foundation is essential for understanding the complexities of building production ML systems. Whether deploying models in cloud environments, edge devices, or embedded systems, the principles and practices covered here will help guide the development of robust, maintainable, and effective ML solutions. By considering both the algorithmic and systems aspects of ML workflows from the start, teams can better navigate the challenges of bringing machine learning from concept to production.

## ML Lifecycle

### Defining the ML Lifecycle

Machine learning has evolved rapidly over the past decade, transforming from an academic discipline into a foundational component of modern software systems. Despite its growing significance, the field lacks a universally accepted definition of the ML lifecycle. While various organizations and researchers have proposed frameworks to guide ML development, no single standard has emerged. Here, we propose a definition that synthesizes insights from industry practices and academic research.

:::{.callout-note}
## Definition of the ML Lifecycle

The Machine Learning (ML) lifecycle is a **structured, iterative process** that outlines the **key stages** required to develop, evaluate, and refine ML systems.
:::

Unlike focusing on implementation details, the lifecycle emphasizes what needs to be achieved at each stage. This process forms a virtuous cycle of continuous improvement, where insights from evaluation, and real-world feedback inform iterative refinements. The stages typically include problem formulation, data acquisition, preprocessing, model training, evaluation, and optimization, each building on the outcomes of the previous one. 

Understanding and defining the ML lifecycle is more than an academic exercise—it provides a foundation for systematically approaching the complexities of machine learning system development. By framing the lifecycle as a structured, iterative process, practitioners can navigate the inherent challenges of working with data-driven systems, ensuring consistency, efficiency, and reliability in their workflows.

From a pedagogical perspective, the ML lifecycle also serves as a conceptual framework that allows us to break down the complexities of machine learning into manageable, interconnected components. Each stage of the lifecycle—from problem formulation to iterative improvement—can be studied in depth, providing students with a clear roadmap to understand and master the distinct subcomponents of a machine learning system.

This decomposition not only mirrors how ML systems are developed in practice but also enables a step-by-step exploration of the core concepts and techniques that underpin the field. In the following chapters, we will study each stage of the lifecycle, building a comprehensive understanding of machine learning systems from the ground up.

### Comparison with Traditional Lifecycles

Software development lifecycles have evolved through decades of engineering practice, establishing well-defined patterns for system development. Whether following waterfall or agile methodologies, these lifecycles consist of sequential phases: requirements gathering, system design, implementation, testing, and deployment. Each phase produces specific artifacts that serve as inputs to subsequent phases. In financial software development, for instance, the requirements phase produces detailed specifications for transaction processing, security protocols, and regulatory compliance - specifications that directly translate into system behavior through explicit programming.

Machine learning systems require a fundamentally different approach to this traditional lifecycle model. The deterministic nature of conventional software, where behavior is explicitly programmed, contrasts sharply with the probabilistic nature of ML systems. Consider financial transaction processing: traditional systems follow predetermined rules (if account balance > transaction amount, then allow transaction), while ML-based fraud detection systems learn to recognize suspicious patterns from historical transaction data. This shift from explicit programming to learned behavior fundamentally reshapes the development lifecycle.

@tbl-lifecycle emphasizes fundamental differences in how these systems are conceived, developed, and evolved, rather than focusing on operational aspects. Each row captures a key philosophical or architectural distinction between traditional and ML-based approaches. The learning-based paradigm of ML systems introduces several fundamental characteristics that differentiate their lifecycle. 

+---------------------+-----------------------------------------+------------------------------------------+
| Development Stage   | Traditional Software Lifecycle          | Machine Learning Lifecycle               |
+:====================+:========================================+:=========================================+
| System Design       | Fixed functional specifications         | Performance-driven objectives            |
+---------------------+-----------------------------------------+------------------------------------------+
| Core Behavior       | Explicit programming logic              | Learning from data patterns              |
+---------------------+-----------------------------------------+------------------------------------------+
| Development Process | Sequential progression of features      | Iterative experimentation and refinement |
+---------------------+-----------------------------------------+------------------------------------------+
| Success Criteria    | Binary correctness (works/doesn't work) | Statistical performance metrics          |
+---------------------+-----------------------------------------+------------------------------------------+
| System Evolution    | Discrete version updates                | Continuous adaptation to new data        |
+---------------------+-----------------------------------------+------------------------------------------+
| Quality Assurance   | Deterministic testing                   | Probabilistic validation                 |
+---------------------+-----------------------------------------+------------------------------------------+
| Failure Modes       | Logic errors and edge cases             | Data quality and distribution shifts     |
+---------------------+-----------------------------------------+------------------------------------------+
| Performance Changes | Only with code modifications            | Can occur without code changes           |
+---------------------+-----------------------------------------+------------------------------------------+

: Distinctions between traditional software and machine learning lifecycles. {#tbl-lifecycle .striped .hover}

Data dependencies form the foundation of ML system behavior. Unlike traditional software where behavior emerges from programmed logic, which we first discussed in [Chapter 1](../introduction/introduction.qmd), ML systems derive their capabilities from training data. This dependency means that system performance is intrinsically linked to data quality, completeness, and representativeness.

Performance characteristics also differ significantly. Traditional software operates on binary correctness criteria---a function either works correctly or it doesn't. ML systems, however, operate on a continuous spectrum of performance, where effectiveness must be measured across multiple metrics and contexts. 

System evolution follows a distinct pattern in ML systems. While traditional software remains static after deployment until explicitly updated, ML systems require continuous monitoring and adaptation to maintain performance as real-world data distributions shift. 

Finally, development patterns in ML systems follow iterative refinement cycles. The system's current behavior influences future data collection and model development decisions, creating feedback loops that don't exist in traditional software development.

### Importance of a Structured Approach

The complexity and dynamic nature of machine learning systems necessitate a structured development approach. While traditional software development methodologies provide some guidance, the unique characteristics of ML systems demand specialized frameworks to manage their development lifecycle. Understanding why this structure matters helps teams build more reliable and effective ML systems.

ML systems present three fundamental challenges that a structured approach addresses. First, these systems exhibit inherent complexity arising from the interaction between data, models, and deployment environments. Data quality affects model performance, model architecture influences computational requirements, and deployment constraints impact both data collection and model design. Without a systematic framework, teams risk creating brittle systems that fail to account for these complex interdependencies.

Second, ML development involves multiple specialized roles—data scientists, ML engineers, domain experts, and operations teams—each with distinct responsibilities and expertise. A structured lifecycle delineates clear interfaces between these roles while maintaining their interdependence. For example, data scientists' decisions about feature engineering must align with both domain experts' knowledge and ML engineers' deployment constraints. The lifecycle framework provides a shared language and process for managing these interactions.

Third, ML systems must maintain reproducibility despite their probabilistic nature. A structured approach enforces rigorous tracking of experiments, data versions, and model artifacts. This systematic recording enables teams to debug performance issues, validate improvements, and understand how system behavior evolves over time. As ML systems scale and team sizes grow, this reproducibility becomes crucial for maintaining system reliability.

It is worth noting the distinction between the ML lifecycle and MLOps (Machine Learning Operations), as these terms are often confused in practice. The ML lifecycle, as discussed in this chapter, describes the fundamental stages and evolution of ML systems—the what and why of ML system development. MLOps, which we will explore in detail in the [MLOps Chapter](../ops/ops.qmd), focuses on the how: the specific practices, tools, and automation that enable efficient implementation of the lifecycle stages. This distinction helps clarify why we begin with the lifecycle; it provides the conceptual foundation necessary for later discussions of operational considerations.

## Stages of the AI Workflow

### Problem Definition and Requirements

The first stage in any machine learning project involves clearly defining the problem and establishing requirements. This foundational step shapes all subsequent decisions in both model development and system design. A comprehensive problem definition must address both the machine learning aspects and the systems engineering considerations that will ultimately determine the solution's success in production.

#### Model Perspective

The definition phase for a deep learning system involves the systematic translation of business or research objectives into specific modeling requirements. This translation process is fundamental because it determines both the architectural approach and the resources required for implementation. Consider an automated customer support system: the high-level objective must be decomposed into specific technical requirements, such as natural language processing for query understanding and response generation, or classification systems for automated routing.

The problem definition establishes the deep learning paradigm most suitable for the task at hand. Modern AI systems leverage deep neural networks for their capacity to learn complex patterns from data. The definition must specify whether the task requires supervised learning, as in image classification systems, or self-supervised learning, which has become prevalent in large language models. This technical specification shapes subsequent decisions about data collection, processing requirements, and computational resources.

Success criteria constitute another essential component of problem definition. While traditional metrics such as accuracy or mean squared error provide quantitative measures, deep learning systems often demand additional performance indicators. A computer vision system's problem definition, for example, must specify not only target accuracy but also acceptable inference latency and resource utilization bounds. Large-scale language or generative models require specific benchmarks for output quality, coherence, and computational efficiency.

Data requirements represent a critical element of problem definition in deep learning systems. The extraordinary capacity of deep neural networks comes with a corresponding demand for extensive training data. The problem definition must specify the volume and characteristics of required data: the diversity of examples, annotation standards, and quality thresholds. These specifications are integral to the problem definition as they determine project feasibility and influence fundamental architectural decisions in the development process.

#### Systems Perspective

The systems perspective introduces additional complexities that must be considered alongside the model requirements. Operational constraints play a fundamental role in shaping the solution architecture. These constraints include latency requirements, which may dictate whether the system can operate in real-time or must process predictions in batches. Throughput expectations, expressed in queries per second or similar metrics, influence the scaling architecture and resource allocation strategies. Resource limitations in terms of memory, compute capacity, and storage must be carefully considered, particularly when deploying to edge devices where power consumption constraints may be critical.

Integration requirements represent another crucial aspect of the systems perspective. The machine learning solution must interface effectively with existing systems, which requires careful consideration of data pipeline dependencies, API specifications, and security requirements. These integration points often introduce additional constraints that may not be immediately apparent when considering the model in isolation.

Scalability considerations must address both immediate and future needs. This includes planning for expected growth in the user base or request volume, geographic distribution of users, and the accompanying data storage and processing requirements. The system must also accommodate model updating and versioning needs, ensuring that the solution remains maintainable and adaptable over time.

#### Balancing Model and System Requirements

The synthesis of model capabilities and system constraints forms a critical aspect of problem definition. This interplay becomes particularly evident in edge computing scenarios. Consider a keyword spotting system intended for deployment on a microcontroller. The problem definition must explicitly state not only the recognition accuracy requirements but also the strict hardware constraints—perhaps a memory limit of 50KB and real-time inference requirements on a low-power processor. Without these system specifications in the initial problem definition, model architects might develop a deep neural network that achieves 98% accuracy but requires megabytes of parameter storage, rendering it impossible to deploy on the target device.

This example illustrates why system constraints must inform the earliest stages of model architecture design. The memory limitation directly influences fundamental modeling choices: the number of layers, the activation functions, the input preprocessing strategy, and even the basic network architecture type. What might appear to be implementation details from a pure modeling perspective become essential constraints that shape the entire solution approach.

The challenges extend beyond simple resource constraints. A real-time recommendation system, for instance, must balance multiple competing demands. From a modeling perspective, such a system requires high prediction accuracy and the ability to quickly incorporate user feedback while handling sparse user interaction data. The systems perspective introduces stringent response time requirements, often demanding sub-100ms latency, along with the ability to handle millions of requests per day. The system must efficiently store and retrieve user histories while maintaining a scalable feature computation pipeline.

These competing demands often necessitate careful trade-offs. A highly accurate deep learning model might meet all ML requirements but prove impractical due to inference latency or memory constraints. The final problem definition must therefore accommodate both sets of requirements, potentially leading to architectural decisions such as employing simpler models that meet latency requirements, implementing sophisticated caching strategies, or decomposing the system into manageable microservices.

This interplay between model and system requirements sets the foundation for all subsequent stages of the ML workflow. Success depends on considering both perspectives from the start and maintaining this balanced view throughout the development process. The decisions made during problem definition will cascade through the entire project lifecycle, influencing everything from data collection strategies to deployment architectures.

### Data Collection and Preparation

Data collection and preparation transform raw information into a form suitable for training deep learning systems. This stage involves gathering relevant data from various sources and processing it into a format that neural networks can effectively learn from. While Chapter 6 will provide an in-depth examination of data engineering practices, understanding the fundamental aspects of this stage provides essential context for the overall machine learning workflow.

Consider a deep learning system designed to transcribe medical speech. A medical facility might have thousands of hours of recorded patient consultations. However, the path from these raw audio recordings to a usable training dataset illuminates key challenges in both model development and systems design. The recordings might come from different devices with varying audio quality, contain background noise, and include sensitive patient information that must be carefully handled.

#### Model Perspective

The success of deep learning models depends fundamentally on the quality and quantity of training data. In the medical transcription example, the model requires not just raw audio files, but also accurate transcriptions for training. These transcriptions must capture medical terminology correctly and indicate speaker changes. The data must span different accents, speaking speeds, and medical specialties to ensure the model generalizes well.

Data preparation from a modeling perspective involves converting raw data into a format suitable for neural network training. The audio files must be preprocessed consistently: perhaps sampled at 16kHz, segmented into fixed-length chunks, and converted into spectrograms. The corresponding transcriptions need tokenization and vocabulary mapping. A mismatch in any of these steps between training and deployment can severely impact model performance.

#### Systems Perspective

The systems architecture for data preparation in production environments presents fundamental distributed systems challenges. Consider the medical transcription pipeline: audio data streams from multiple sources must be processed, stored, and served to training systems while maintaining consistency and fault tolerance. This distributed nature introduces classical computer systems problems in a new context.

The system architecture typically follows a lambda pattern, handling both batch and streaming data processing. Batch processing enables thorough data cleaning and feature extraction, while streaming capabilities support real-time data validation and online learning scenarios. For audio processing, this means balancing between batch spectral analysis for training datasets and real-time spectral computation for inference.

Data consistency becomes particularly critical in distributed preprocessing pipelines. When multiple preprocessing workers operate in parallel, they must maintain identical transformation parameters (normalization statistics, feature extraction configurations) to ensure uniform data processing. This introduces distributed synchronization challenges similar to those in traditional database systems, but with additional complexity due to the stochastic nature of many preprocessing operations.

Resource management presents another key challenge. Audio preprocessing operations such as Short-Time Fourier Transform (STFT) computation are computationally intensive, requiring careful orchestration across distributed workers. The system must optimize resource utilization while ensuring preprocessing throughput meets both training and inference demands. This often involves complex scheduling algorithms that consider data locality, computational requirements, and storage constraints.

#### Balancing Model and System Requirements

The medical transcription example highlights critical trade-offs between model and system requirements. While longer audio segments might improve transcription accuracy, they increase memory usage and processing time. Similarly, while higher audio sampling rates might capture more detail, they demand more storage space and computational resources. These trade-offs must be evaluated early in the workflow since they affect both the training infrastructure and the final deployed system.

Such considerations extend beyond medical applications. A real-time translation system for video calls faces similar challenges: preprocessing must be fast enough to maintain natural conversation flow, but thorough enough to ensure accurate translation. A security system processing multiple video streams must balance high-resolution imagery against processing speed and storage constraints.

The next chapter will explore the technical details of data engineering, including advanced preprocessing techniques, pipeline optimization, and data quality management. However, understanding these high-level considerations helps in appreciating how data-related decisions influence both model development and system architecture throughout the machine learning workflow.

### Model Development and Training
### Evaluation and Validation
### Deployment and Integration
### Monitoring and Maintenance

## The Iterative Nature of AI Development
### Feedback Loops and Continuous Refinement
### Adapting to Changing Requirements
### Importance of Flexibility in Workflow

## Collaborative Aspects of AI Projects
### Key Roles in AI Development
### Interdisciplinary Teamwork
### Communication Across Domains

## Challenges in AI Workflows
### Data-related Challenges
### Model Development Hurdles
### Deployment and Scaling Issues
### Ethical and Legal Considerations

## Principles and Practices in AI Development
### Core Principles Guiding AI Workflows
### Best Practices in Implementation
### Balancing Theory and Application

## Conclusion: Evolving Landscapes in AI Development
### Recap of Key Workflow Concepts
### Preparing for Advanced Topics
### Future Trends in AI Workflows