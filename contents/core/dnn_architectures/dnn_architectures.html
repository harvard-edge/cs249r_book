<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.32">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>4&nbsp; DNN Architectures – Machine Learning Systems</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../">
<link href="../../../contents/core/workflow/workflow.html" rel="next">
<link href="../../../contents/core/dl_primer/dl_primer.html" rel="prev">
<link href="../../../favicon.png" rel="icon" type="image/png">
<script src="../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-fc6d358c97f25a8ea829b86655043430.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-dark-5c6c0ad7bdfb89369003da8042cd4f02.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../site_libs/bootstrap/bootstrap-71dacee0c6392ff771ab8565bd7b5b7b.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../../site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script src="../../../site_libs/quarto-contrib/glightbox/glightbox.min.js"></script>
<link href="../../../site_libs/quarto-contrib/glightbox/glightbox.min.css" rel="stylesheet">
<link href="../../../site_libs/quarto-contrib/glightbox/lightbox.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "~",
    "/"
  ],
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-M21L0CBCVN"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-M21L0CBCVN', { 'anonymize_ip': true});
</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-M21L0CBCVN"></script>
<script type="module" src="../../../scripts/ai_menu/dist/bundle.js" defer=""></script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating nav-fixed slimcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../../index.html">
    <span class="navbar-title">Machine Learning Systems</span>
    </a>
  </div>
        <div class="quarto-navbar-tools tools-wide tools-end">
    <a href="https://github.com/harvard-edge/cs249r_book" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
    <a href="../../../Machine-Learning-Systems.pdf" title="Download PDF" class="quarto-navigation-tool px-1" aria-label="Download PDF"><i class="bi bi-file-pdf"></i></a>
    <div class="dropdown">
      <a href="" title="Share" id="quarto-navigation-tool-dropdown-0" class="quarto-navigation-tool dropdown-toggle px-1" data-bs-toggle="dropdown" aria-expanded="false" role="link" aria-label="Share"><i class="bi bi-share"></i></a>
      <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="quarto-navigation-tool-dropdown-0">
          <li>
            <a class="dropdown-item quarto-navbar-tools-item" href="https://twitter.com/intent/tweet?url=|url|">
              <i class="bi bi-twitter pe-1"></i>
            Twitter
            </a>
          </li>
          <li>
            <a class="dropdown-item quarto-navbar-tools-item" href="https://www.facebook.com/sharer/sharer.php?u=|url|">
              <i class="bi bi-facebook pe-1"></i>
            Facebook
            </a>
          </li>
      </ul>
    </div>
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
  <a href="" class="quarto-reader-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleReader(); return false;" title="Toggle reader mode">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../../contents/core/dnn_architectures/dnn_architectures.html"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">DNN Architectures</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
  <div id="quarto-announcement" data-announcement-id="468ec4500d92e615b1615ac01f7d119e" class="alert alert-primary hidden"><i class="bi bi-star-half quarto-announcement-icon"></i><div class="quarto-announcement-content">
<p>📘 <b>What’s new?</b> 🎉 Happy New Year 2025! Chapters 1–5 expanded and improved, with a brand new Chapter 4!<br> 🚀 <b>Every learner deserves to be a ⭐</b> 1 GitHub ⭐ = 1 👩‍🎓 Learner. Your support fuels free, impactful educational resources on AI.<br> 🙏 <b>Thank you for your support!</b> Every GitHub star shows a learner engaging or a supporter driving our mission. You can click <a href="https://github.com/harvard-edge/cs249r_book">here</a> to star us.</p>
</div><i class="bi bi-x-lg quarto-announcement-action"></i></div>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/frontmatter/foreword.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Author’s Note</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/frontmatter/about/about.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">About the Book</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/frontmatter/acknowledgements/acknowledgements.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Acknowledgements</span></a>
  </div>
</li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/frontmatter/ai/socratiq.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">🤖 SocratiQ AI</span></a>
  </div>
</li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item sidebar-item-section">
      <span class="sidebar-item-text sidebar-link text-start">
 <span class="menu-text"><div class="part">The Essentials</div></span></span>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/introduction/introduction.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/ml_systems/ml_systems.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">ML Systems</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/dl_primer/dl_primer.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">DL Primer</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/dnn_architectures/dnn_architectures.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">DNN Architectures</span></span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <span class="sidebar-item-text sidebar-link text-start">
 <span class="menu-text"><div class="part">Engineering Principles</div></span></span>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/workflow/workflow.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">AI Workflow</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/data_engineering/data_engineering.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Data Engineering</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/frameworks/frameworks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">AI Frameworks</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/training/training.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">AI Training</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/efficient_ai/efficient_ai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Efficient AI</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/optimizations/optimizations.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Model Optimizations</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/hw_acceleration/hw_acceleration.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">AI Acceleration</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/benchmarking/benchmarking.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Benchmarking AI</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/ops/ops.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">ML Operations</span></span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <span class="sidebar-item-text sidebar-link text-start">
 <span class="menu-text"><div class="part">Best Practices in AI</div></span></span>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/ondevice_learning/ondevice_learning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">On-Device Learning</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/privacy_security/privacy_security.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Security &amp; Privacy</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/responsible_ai/responsible_ai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Responsible AI</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/sustainable_ai/sustainable_ai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Sustainable AI</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/robust_ai/robust_ai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Robust AI</span></span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <span class="sidebar-item-text sidebar-link text-start">
 <span class="menu-text"><div class="part">Closing Perspectives</div></span></span>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/ai_for_good/ai_for_good.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">AI for Good</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/conclusion/conclusion.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Conclusion</span></span></a>
  </div>
</li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item sidebar-item-section">
      <span class="sidebar-item-text sidebar-link text-start">
 <span class="menu-text">LABS</span></span>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/overview.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Overview</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/getting_started.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Getting Started</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../../contents/labs/arduino/nicla_vision/nicla_vision.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Nicla Vision</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/arduino/nicla_vision/setup/setup.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Setup</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/arduino/nicla_vision/image_classification/image_classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Image Classification</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/arduino/nicla_vision/object_detection/object_detection.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Object Detection</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/arduino/nicla_vision/kws/kws.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Keyword Spotting (KWS)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/arduino/nicla_vision/motion_classification/motion_classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Motion Classification and Anomaly Detection</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../../contents/labs/seeed/xiao_esp32s3/xiao_esp32s3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">XIAO ESP32S3</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-7" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/seeed/xiao_esp32s3/setup/setup.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Setup</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/seeed/xiao_esp32s3/image_classification/image_classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Image Classification</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/seeed/xiao_esp32s3/object_detection/object_detection.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Object Detection</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/seeed/xiao_esp32s3/kws/kws.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Keyword Spotting (KWS)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/seeed/xiao_esp32s3/motion_classification/motion_classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Motion Classification and Anomaly Detection</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../../contents/labs/raspi/raspi.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Raspberry Pi</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-8" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/raspi/setup/setup.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Setup</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/raspi/image_classification/image_classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Image Classification</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/raspi/object_detection/object_detection.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Object Detection</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/raspi/llm/llm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Small Language Models (SLM)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/raspi/vlm/vlm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Vision-Language Models (VLM)</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../../contents/labs/shared/shared.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Shared Labs</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-9" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-9" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/shared/kws_feature_eng/kws_feature_eng.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">KWS Feature Engineering</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/shared/dsp_spectral_features_block/dsp_spectral_features_block.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">DSP Spectral Features</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item sidebar-item-section">
      <span class="sidebar-item-text sidebar-link text-start">
 <span class="menu-text">REFERENCES</span></span>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#purpose" id="toc-purpose" class="nav-link active" data-scroll-target="#purpose">Purpose</a></li>
  <li><a href="#overview" id="toc-overview" class="nav-link" data-scroll-target="#overview"><span class="header-section-number">4.1</span> Overview</a></li>
  <li><a href="#multi-layer-perceptrons-dense-pattern-processing" id="toc-multi-layer-perceptrons-dense-pattern-processing" class="nav-link" data-scroll-target="#multi-layer-perceptrons-dense-pattern-processing"><span class="header-section-number">4.2</span> Multi-Layer Perceptrons: Dense Pattern Processing</a>
  <ul>
  <li><a href="#pattern-processing-needs" id="toc-pattern-processing-needs" class="nav-link" data-scroll-target="#pattern-processing-needs"><span class="header-section-number">4.2.1</span> Pattern Processing Needs</a></li>
  <li><a href="#algorithmic-structure" id="toc-algorithmic-structure" class="nav-link" data-scroll-target="#algorithmic-structure"><span class="header-section-number">4.2.2</span> Algorithmic Structure</a></li>
  <li><a href="#computational-mapping" id="toc-computational-mapping" class="nav-link" data-scroll-target="#computational-mapping"><span class="header-section-number">4.2.3</span> Computational Mapping</a></li>
  <li><a href="#system-implications" id="toc-system-implications" class="nav-link" data-scroll-target="#system-implications"><span class="header-section-number">4.2.4</span> System Implications</a>
  <ul class="collapse">
  <li><a href="#memory-requirements" id="toc-memory-requirements" class="nav-link" data-scroll-target="#memory-requirements">Memory Requirements</a></li>
  <li><a href="#computation-needs" id="toc-computation-needs" class="nav-link" data-scroll-target="#computation-needs">Computation Needs</a></li>
  <li><a href="#data-movement" id="toc-data-movement" class="nav-link" data-scroll-target="#data-movement">Data Movement</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#convolutional-neural-networks-spatial-pattern-processing" id="toc-convolutional-neural-networks-spatial-pattern-processing" class="nav-link" data-scroll-target="#convolutional-neural-networks-spatial-pattern-processing"><span class="header-section-number">4.3</span> Convolutional Neural Networks: Spatial Pattern Processing</a>
  <ul>
  <li><a href="#pattern-processing-needs-1" id="toc-pattern-processing-needs-1" class="nav-link" data-scroll-target="#pattern-processing-needs-1"><span class="header-section-number">4.3.1</span> Pattern Processing Needs</a></li>
  <li><a href="#algorithmic-structure-1" id="toc-algorithmic-structure-1" class="nav-link" data-scroll-target="#algorithmic-structure-1"><span class="header-section-number">4.3.2</span> Algorithmic Structure</a></li>
  <li><a href="#computational-mapping-1" id="toc-computational-mapping-1" class="nav-link" data-scroll-target="#computational-mapping-1"><span class="header-section-number">4.3.3</span> Computational Mapping</a></li>
  <li><a href="#system-implications-1" id="toc-system-implications-1" class="nav-link" data-scroll-target="#system-implications-1"><span class="header-section-number">4.3.4</span> System Implications</a>
  <ul class="collapse">
  <li><a href="#memory-requirements-1" id="toc-memory-requirements-1" class="nav-link" data-scroll-target="#memory-requirements-1">Memory Requirements</a></li>
  <li><a href="#computation-needs-1" id="toc-computation-needs-1" class="nav-link" data-scroll-target="#computation-needs-1">Computation Needs</a></li>
  <li><a href="#data-movement-1" id="toc-data-movement-1" class="nav-link" data-scroll-target="#data-movement-1">Data Movement</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#recurrent-neural-networks-sequential-pattern-processing" id="toc-recurrent-neural-networks-sequential-pattern-processing" class="nav-link" data-scroll-target="#recurrent-neural-networks-sequential-pattern-processing"><span class="header-section-number">4.4</span> Recurrent Neural Networks: Sequential Pattern Processing</a>
  <ul>
  <li><a href="#pattern-processing-needs-2" id="toc-pattern-processing-needs-2" class="nav-link" data-scroll-target="#pattern-processing-needs-2"><span class="header-section-number">4.4.1</span> Pattern Processing Needs</a></li>
  <li><a href="#algorithmic-structure-2" id="toc-algorithmic-structure-2" class="nav-link" data-scroll-target="#algorithmic-structure-2"><span class="header-section-number">4.4.2</span> Algorithmic Structure</a></li>
  <li><a href="#computational-mapping-2" id="toc-computational-mapping-2" class="nav-link" data-scroll-target="#computational-mapping-2"><span class="header-section-number">4.4.3</span> Computational Mapping</a></li>
  <li><a href="#system-implications-2" id="toc-system-implications-2" class="nav-link" data-scroll-target="#system-implications-2"><span class="header-section-number">4.4.4</span> System Implications</a>
  <ul class="collapse">
  <li><a href="#memory-requirements-2" id="toc-memory-requirements-2" class="nav-link" data-scroll-target="#memory-requirements-2">Memory Requirements</a></li>
  <li><a href="#computation-needs-2" id="toc-computation-needs-2" class="nav-link" data-scroll-target="#computation-needs-2">Computation Needs</a></li>
  <li><a href="#data-movement-2" id="toc-data-movement-2" class="nav-link" data-scroll-target="#data-movement-2">Data Movement</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#attention-mechanisms-dynamic-pattern-processing" id="toc-attention-mechanisms-dynamic-pattern-processing" class="nav-link" data-scroll-target="#attention-mechanisms-dynamic-pattern-processing"><span class="header-section-number">4.5</span> Attention Mechanisms: Dynamic Pattern Processing</a>
  <ul>
  <li><a href="#pattern-processing-needs-3" id="toc-pattern-processing-needs-3" class="nav-link" data-scroll-target="#pattern-processing-needs-3"><span class="header-section-number">4.5.1</span> Pattern Processing Needs</a></li>
  <li><a href="#basic-attention-mechanism" id="toc-basic-attention-mechanism" class="nav-link" data-scroll-target="#basic-attention-mechanism"><span class="header-section-number">4.5.2</span> Basic Attention Mechanism</a>
  <ul class="collapse">
  <li><a href="#algorithmic-structure-3" id="toc-algorithmic-structure-3" class="nav-link" data-scroll-target="#algorithmic-structure-3">Algorithmic Structure</a></li>
  <li><a href="#computational-mapping-3" id="toc-computational-mapping-3" class="nav-link" data-scroll-target="#computational-mapping-3">Computational Mapping</a></li>
  <li><a href="#system-implications-3" id="toc-system-implications-3" class="nav-link" data-scroll-target="#system-implications-3">System Implications</a></li>
  </ul></li>
  <li><a href="#transformers-and-self-attention" id="toc-transformers-and-self-attention" class="nav-link" data-scroll-target="#transformers-and-self-attention"><span class="header-section-number">4.5.3</span> Transformers and Self-Attention</a>
  <ul class="collapse">
  <li><a href="#algorithmic-structure-4" id="toc-algorithmic-structure-4" class="nav-link" data-scroll-target="#algorithmic-structure-4">Algorithmic Structure</a></li>
  <li><a href="#computational-mapping-4" id="toc-computational-mapping-4" class="nav-link" data-scroll-target="#computational-mapping-4">Computational Mapping</a></li>
  <li><a href="#system-implications-4" id="toc-system-implications-4" class="nav-link" data-scroll-target="#system-implications-4">System Implications</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#architectural-building-blocks" id="toc-architectural-building-blocks" class="nav-link" data-scroll-target="#architectural-building-blocks"><span class="header-section-number">4.6</span> Architectural Building Blocks</a>
  <ul>
  <li><a href="#from-perceptron-to-multi-layer-networks" id="toc-from-perceptron-to-multi-layer-networks" class="nav-link" data-scroll-target="#from-perceptron-to-multi-layer-networks"><span class="header-section-number">4.6.1</span> From Perceptron to Multi-Layer Networks</a></li>
  <li><a href="#from-dense-to-spatial-processing" id="toc-from-dense-to-spatial-processing" class="nav-link" data-scroll-target="#from-dense-to-spatial-processing"><span class="header-section-number">4.6.2</span> From Dense to Spatial Processing</a></li>
  <li><a href="#the-evolution-of-sequence-processing" id="toc-the-evolution-of-sequence-processing" class="nav-link" data-scroll-target="#the-evolution-of-sequence-processing"><span class="header-section-number">4.6.3</span> The Evolution of Sequence Processing</a></li>
  <li><a href="#modern-architectures-synthesis-and-innovation" id="toc-modern-architectures-synthesis-and-innovation" class="nav-link" data-scroll-target="#modern-architectures-synthesis-and-innovation"><span class="header-section-number">4.6.4</span> Modern Architectures: Synthesis and Innovation</a></li>
  </ul></li>
  <li><a href="#system-level-building-blocks" id="toc-system-level-building-blocks" class="nav-link" data-scroll-target="#system-level-building-blocks"><span class="header-section-number">4.7</span> System-Level Building Blocks</a>
  <ul>
  <li><a href="#core-computational-primitives" id="toc-core-computational-primitives" class="nav-link" data-scroll-target="#core-computational-primitives"><span class="header-section-number">4.7.1</span> Core Computational Primitives</a></li>
  <li><a href="#memory-access-primitives" id="toc-memory-access-primitives" class="nav-link" data-scroll-target="#memory-access-primitives"><span class="header-section-number">4.7.2</span> Memory Access Primitives</a></li>
  <li><a href="#data-movement-primitives" id="toc-data-movement-primitives" class="nav-link" data-scroll-target="#data-movement-primitives"><span class="header-section-number">4.7.3</span> Data Movement Primitives</a></li>
  <li><a href="#system-design-impact" id="toc-system-design-impact" class="nav-link" data-scroll-target="#system-design-impact"><span class="header-section-number">4.7.4</span> System Design Impact</a></li>
  </ul></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion"><span class="header-section-number">4.8</span> Conclusion</a></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/harvard-edge/cs249r_book/edit/dev/contents/core/dnn_architectures/dnn_architectures.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li><li><a href="https://github.com/harvard-edge/cs249r_book/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li><li><a href="https://github.com/harvard-edge/cs249r_book/blob/dev/contents/core/dnn_architectures/dnn_architectures.qmd" class="toc-action"><i class="bi empty"></i>View source</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content page-columns page-full" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span id="sec-dl_arch" class="quarto-section-identifier"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">DNN Architectures</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>Resources: <a href="#sec-deep-learning-primer-resource">Slides</a>, <a href="#sec-deep-learning-primer-resource">Videos</a>, <a href="#sec-deep-learning-primer-resource">Exercises</a></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/png/cover_dl_arch.png" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="DALL·E 3 Prompt: A visually striking rectangular image illustrating the interplay between deep learning algorithms like CNNs, RNNs, and Attention Networks, interconnected with machine learning systems. The composition features neural network diagrams blending seamlessly with representations of computational systems such as processors, graphs, and data streams. Bright neon tones contrast against a dark futuristic background, symbolizing cutting-edge technology and intricate system complexity."><img src="images/png/cover_dl_arch.png" class="img-fluid figure-img" alt="DALL·E 3 Prompt: A visually striking rectangular image illustrating the interplay between deep learning algorithms like CNNs, RNNs, and Attention Networks, interconnected with machine learning systems. The composition features neural network diagrams blending seamlessly with representations of computational systems such as processors, graphs, and data streams. Bright neon tones contrast against a dark futuristic background, symbolizing cutting-edge technology and intricate system complexity."></a></p>
<figcaption><em>DALL·E 3 Prompt: A visually striking rectangular image illustrating the interplay between deep learning algorithms like CNNs, RNNs, and Attention Networks, interconnected with machine learning systems. The composition features neural network diagrams blending seamlessly with representations of computational systems such as processors, graphs, and data streams. Bright neon tones contrast against a dark futuristic background, symbolizing cutting-edge technology and intricate system complexity.</em></figcaption>
</figure>
</div>
<section id="purpose" class="level2 unnumbered">
<h2 class="unnumbered anchored" data-anchor-id="purpose">Purpose</h2>
<p><em>What recurring patterns emerge across modern deep learning architectures, and how do these patterns enable systematic approaches to AI system design?</em></p>
<p>Deep learning architectures represent a convergence of computational patterns that form the building blocks of modern AI systems. These foundational patterns — from convolutional structures to attention mechanisms — reveal how complex models arise from simple, repeatable components. The examination of these architectural elements provides insights into the systematic construction of flexible, efficient AI systems, establishing core principles that influence every aspect of system design and deployment. These structural insights illuminate the path toward creating scalable, adaptable solutions across diverse application domains.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Learning Objectives
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><p>Map fundamental neural network concepts to deep learning architectures (dense, spatial, temporal, attention-based).</p></li>
<li><p>Analyze how architectural patterns shape computational and memory demands.</p></li>
<li><p>Evaluate system-level impacts of architectural choices on system attributes.</p></li>
<li><p>Compare architectures’ hardware mapping and identify optimization strategies.</p></li>
<li><p>Assess trade-offs between complexity and system needs for specific applications.</p></li>
</ul>
</div>
</div>
</section>
<section id="overview" class="level2" data-number="4.1">
<h2 data-number="4.1" class="anchored" data-anchor-id="overview"><span class="header-section-number">4.1</span> Overview</h2>
<p>Deep learning architecture stands for specific representation or organizations of neural network components—the neurons, weights, and connections (as introduced in <a href="../../../contents/core/dl_primer/dl_primer.html#sec-dl_primer">Chapter 3</a>)—arranged to efficiently process different types of patterns in data. While the previous chapter established the fundamental building blocks of neural networks, in this chapter we examine how these components are structured into architectures that map efficiently to computer systems.</p>
<p>Neural network architectures have evolved to address specific pattern processing challenges. Whether processing arbitrary feature relationships, exploiting spatial patterns, managing temporal dependencies, or handling dynamic information flow, each architectural pattern emerged from particular computational needs. These architectures, from a computer systems perspective, require an examination of how their computational patterns map to system resources.</p>
<p>Most often the architectures are discussed in terms of their algorithmic structures (MLPs, CNNs, RNNs, Transformers). However, in this chapter we take a more fundamental approach by examining how their computational patterns map to hardware resources. Each section analyzes how specific pattern processing needs influence algorithmic structure and how these structures map to computer system resources. The implications for computer system design require examining how their computational patterns map to hardware resources. The mapping from algorithmic requirements to computer system design involves several key considerations:</p>
<ol type="1">
<li>Memory access patterns: How data moves through the memory hierarchy</li>
<li>Computation characteristics: The nature and organization of arithmetic operations</li>
<li>Data movement: Requirements for on-chip and off-chip data transfer</li>
<li>Resource utilization: How computational and memory resources are allocated</li>
</ol>
<p>For example, dense connectivity patterns generate different memory bandwidth demands than localized processing structures. Similarly, stateful processing creates distinct requirements for on-chip memory organization compared to stateless operations. Getting a firm grasph on these mappings is important for modern computer architects and system designers who must implement these algorithms efficiently in hardware.</p>
</section>
<section id="multi-layer-perceptrons-dense-pattern-processing" class="level2 page-columns page-full" data-number="4.2">
<h2 data-number="4.2" class="anchored" data-anchor-id="multi-layer-perceptrons-dense-pattern-processing"><span class="header-section-number">4.2</span> Multi-Layer Perceptrons: Dense Pattern Processing</h2>
<p>Multi-Layer Perceptrons (MLPs) represent the most direct extension of neural networks into deep architectures. Unlike more specialized networks, MLPs process each input element with equal importance, making them versatile but computationally intensive. Their architecture, while simple, establishes fundamental computational patterns that appear throughout deep learning systems. These patterns were initially formalized by the introduction of the Universal Approximation Theorem (UAT) <span class="citation" data-cites="cybenko1989approximation hornik1989multilayer">(<a href="../references.html#ref-cybenko1989approximation" role="doc-biblioref">Cybenko 1989</a>; <a href="../references.html#ref-hornik1989multilayer" role="doc-biblioref">Hornik, Stinchcombe, and White 1989</a>)</span>, which states that a sufficiently large MLP with non-linear activation functions can approximate any continuous function on a compact domain, given suitable weights and biases.</p>
<div class="no-row-height column-margin column-container"><div id="ref-cybenko1989approximation" class="csl-entry" role="listitem">
Cybenko, George. 1989. <span>“Approximation by Superpositions of a Sigmoidal Function.”</span> <em>Mathematics of Control, Signals and Systems</em> 2 (4): 303–14.
</div><div id="ref-hornik1989multilayer" class="csl-entry" role="listitem">
Hornik, Kurt, Maxwell Stinchcombe, and Halbert White. 1989. <span>“Multilayer Feedforward Networks Are Universal Approximators.”</span> <em>Neural Networks</em> 2 (5): 359–66.
</div></div><p>When applied to the MNIST handwritten digit recognition challenge, an MLP reveals its computational power by transforming a complex 28×28 pixel image into a precise digit classification. By treating each of the 784 pixels as an equally weighted input, the network learns to decompose visual information through a systematic progression of layers, converting raw pixel intensities into increasingly abstract representations that capture the essential characteristics of handwritten digits.</p>
<section id="pattern-processing-needs" class="level3" data-number="4.2.1">
<h3 data-number="4.2.1" class="anchored" data-anchor-id="pattern-processing-needs"><span class="header-section-number">4.2.1</span> Pattern Processing Needs</h3>
<p>Deep learning systems frequently encounter problems where any input feature could potentially influence any output—there are no inherent constraints on these relationships. Consider analyzing financial market data: any economic indicator might affect any market outcome or in natural language processing, where the meaning of a word could depend on any other word in the sentence. These scenarios demand an architectural pattern capable of learning arbitrary relationships across all input features.</p>
<p>Dense pattern processing addresses this fundamental need by enabling several key capabilities. First, it allows unrestricted feature interactions where each output can depend on any combination of inputs. Second, it facilitates learned feature importance, allowing the system to determine which connections matter rather than having them prescribed. Finally, it provides adaptive representation, enabling the network to reshape its internal representations based on the data.</p>
<p>For example, in the MNIST digit recognition task, while humans might focus on specific parts of digits (like loops in ‘6’ or crossings in ‘8’), we cannot definitively say which pixel combinations are important for classification. A ‘7’ written with a serif could share pixel patterns with a ‘2’, while variations in handwriting mean discriminative features might appear anywhere in the image. This uncertainty about feature relationships necessitates a dense processing approach where every pixel can potentially influence the classification decision.</p>
</section>
<section id="algorithmic-structure" class="level3 page-columns page-full" data-number="4.2.2">
<h3 data-number="4.2.2" class="anchored" data-anchor-id="algorithmic-structure"><span class="header-section-number">4.2.2</span> Algorithmic Structure</h3>
<p>To enable unrestricted feature interactions, MLPs implement a direct algorithmic solution: connect everything to everything. This is realized through a series of fully-connected layers, where each neuron connects to every neuron in adjacent layers. The dense connectivity pattern translates mathematically into matrix multiplication operations. As shown in <a href="#fig-mlp" class="quarto-xref">Figure&nbsp;<span>4.1</span></a>, each layer transforms its input through matrix multiplication followed by element-wise activation:</p>
<div id="fig-mlp" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-mlp-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="images/png/mlp_mm.png" class="lightbox" data-gallery="quarto-lightbox-gallery-2" title="Figure&nbsp;4.1: MLP layers and its associated matrix representation. Source: @reagen2017deep"><img src="images/png/mlp_mm.png" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-mlp-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4.1: MLP layers and its associated matrix representation. Source: <span class="citation" data-cites="reagen2017deep">Reagen et al. (<a href="../references.html#ref-reagen2017deep" role="doc-biblioref">2017</a>)</span>
</figcaption>
<div class="no-row-height column-margin column-container"><div id="ref-reagen2017deep" class="csl-entry" role="listitem">
Reagen, Brandon, Robert Adolf, Paul Whatmough, Gu-Yeon Wei, David Brooks, and Margaret Martonosi. 2017. <em>Deep Learning for Computer Architects</em>. Springer.
</div></div></figure>
</div>
<p><span class="math display">\[
\mathbf{h}^{(l)} = f(\mathbf{W}^{(l)}\mathbf{h}^{(l-1)} + \mathbf{b}^{(l)})
\]</span></p>
<p>The dimensions of these operations reveal the computational scale of dense pattern processing:</p>
<ul>
<li>Input vector: <span class="math inline">\(\mathbf{h}^{(0)} \in \mathbb{R}^{d_{in}}\)</span> represents all potential input features</li>
<li>Weight matrices: <span class="math inline">\(\mathbf{W}^{(l)} \in \mathbb{R}^{d_{out} \times d_{in}}\)</span> capture all possible input-output relationships</li>
<li>Output vector: <span class="math inline">\(\mathbf{h}^{(l)} \in \mathbb{R}^{d_{out}}\)</span> produces transformed representations</li>
</ul>
<p>In the MNIST example, this means:</p>
<ul>
<li>Each 784-dimensional input (<span class="math inline">\(28 \times 28\)</span> pixels) connects to every neuron in the first hidden layer</li>
<li>A hidden layer with 100 neurons requires a 784 × 100 weight matrix</li>
<li>Each weight in this matrix represents a learnable relationship between an input pixel and a hidden feature</li>
</ul>
<p>This algorithmic structure directly addresses our need for arbitrary feature relationships but creates specific computational patterns that must be handled efficiently by computer systems.</p>
</section>
<section id="computational-mapping" class="level3" data-number="4.2.3">
<h3 data-number="4.2.3" class="anchored" data-anchor-id="computational-mapping"><span class="header-section-number">4.2.3</span> Computational Mapping</h3>
<p>The elegant mathematical representation of dense matrix multiplication maps to specific computational patterns that systems must handle. Let’s examine how this mapping progresses from mathematical abstraction to computational reality.</p>
<p>The first implementation, <code>mlp_layer_matrix</code>, directly mirrors our mathematical equation. It uses high-level matrix operations (<code>matmul</code>) to express the computation in a single line, hiding the underlying complexity. This is the style commonly used in deep learning frameworks, where optimized libraries handle the actual computation.</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Mathematical abstraction in code</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> mlp_layer_matrix(X, W, b):</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>    <span class="co"># X: input matrix (batch_size × num_inputs)</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># W: weight matrix (num_inputs × num_outputs)</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>    <span class="co"># b: bias vector (num_outputs)</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>    H <span class="op">=</span> activation(matmul(X, W) <span class="op">+</span> b)    <span class="co"># One clean line of math</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> H</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>The second implementation, <code>mlp_layer_compute</code>, exposes the actual computational pattern through nested loops. This version shows us what really happens when we compute a layer’s output: we process each sample in the batch, computing each output neuron by accumulating weighted contributions from all inputs.</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Core computational pattern</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> mlp_layer_compute(X, W, b):</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Process each sample in the batch</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> batch <span class="kw">in</span> <span class="bu">range</span>(batch_size):</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Compute each output neuron</span></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> out <span class="kw">in</span> <span class="bu">range</span>(num_outputs):</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Initialize with bias</span></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>            Z[batch,out] <span class="op">=</span> b[out]</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Accumulate weighted inputs</span></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> in_ <span class="kw">in</span> <span class="bu">range</span>(num_inputs):</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>                Z[batch,out] <span class="op">+=</span> X[batch,in_] <span class="op">*</span> W[in_,out]</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>    H <span class="op">=</span> activation(Z)</span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> H</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>This translation from mathematical abstraction to concrete computation exposes how dense matrix multiplication decomposes into nested loops of simpler operations. The outer loop processes each sample in the batch, while the middle loop computes values for each output neuron. Within the innermost loop, the system performs repeated multiply-accumulate operations, combining each input with its corresponding weight.</p>
<p>In the MNIST example, each output neuron requires 784 multiply-accumulate operations and at least 1,568 memory accesses (784 for inputs, 784 for weights). While actual implementations use sophisticated optimizations through libraries like <a href="https://www.netlib.org/blas/">BLAS</a> or <a href="https://developer.nvidia.com/cublas">cuBLAS</a>, these fundamental patterns drive key system design decisions.</p>
</section>
<section id="system-implications" class="level3" data-number="4.2.4">
<h3 data-number="4.2.4" class="anchored" data-anchor-id="system-implications"><span class="header-section-number">4.2.4</span> System Implications</h3>
<p>When analyzing how computational patterns impact computer systems, we typically examine three fundamental dimensions: memory requirements, computation needs, and data movement. This framework enables a systematic analysis of how algorithmic patterns influence system design decisions. We will use this framework for analyzing other network architectures, allowing us to compare and contrast their different characteristics.</p>
<section id="memory-requirements" class="level4">
<h4 class="anchored" data-anchor-id="memory-requirements">Memory Requirements</h4>
<p>For dense pattern processing, the memory requirements stem from storing and accessing weights, inputs, and intermediate results. In our MNIST example, connecting our 784-dimensional input layer to a hidden layer of 100 neurons requires 78,400 weight parameters. Each forward pass must access all these weights, along with input data and intermediate results. The all-to-all connectivity pattern means there’s no inherent locality in these accesses—every output needs every input and its corresponding weights.</p>
<p>These memory access patterns suggest opportunities for optimization through careful data organization and reuse. Modern processors handle these patterns differently—CPUs leverage their cache hierarchy for data reuse, while GPUs employ specialized memory hierarchies designed for high-bandwidth access. Deep learning frameworks abstract these hardware-specific details through optimized matrix multiplication implementations.</p>
</section>
<section id="computation-needs" class="level4">
<h4 class="anchored" data-anchor-id="computation-needs">Computation Needs</h4>
<p>The core computation revolves around multiply-accumulate operations arranged in nested loops. Each output value requires as many multiply-accumulates as there are inputs. For MNIST, this means 784 multiply-accumulates per output neuron. With 100 neurons in our hidden layer, we’re performing 78,400 multiply-accumulates for a single input image. While these operations are simple, their volume and arrangement create specific demands on processing resources.</p>
<p>This computational structure lends itself to particular optimization strategies in modern hardware. The dense matrix multiplication pattern can be efficiently parallelized across multiple processing units, with each handling different subsets of neurons. Modern hardware accelerators take advantage of this through specialized matrix multiplication units, while deep learning frameworks automatically convert these operations into optimized BLAS (Basic Linear Algebra Subprograms) calls. CPUs and GPUs can both exploit cache locality by carefully tiling the computation to maximize data reuse, though their specific approaches differ based on their architectural strengths.</p>
</section>
<section id="data-movement" class="level4">
<h4 class="anchored" data-anchor-id="data-movement">Data Movement</h4>
<p>The all-to-all connectivity pattern in MLPs creates significant data movement requirements. Each multiply-accumulate operation needs three pieces of data: an input value, a weight value, and the running sum. For our MNIST example layer, computing a single output value requires moving 784 inputs and 784 weights to wherever the computation occurs. This movement pattern repeats for each of the 100 output neurons, creating substantial data transfer demands between memory and compute units.</p>
<p>The predictable nature of these data movement patterns enables strategic data staging and transfer optimizations. Different architectures address this challenge through various mechanisms—CPUs use sophisticated prefetching and multi-level caches, while GPUs employ high-bandwidth memory systems and latency hiding through massive threading. Deep learning frameworks orchestrate these data movements through optimized memory management systems.</p>
<!-- :::{.callout-caution #exr-mlp collapse="false"}

#### Multilayer Perceptrons (MLPs)

We've just scratched the surface of neural networks. Now, you'll get to try and apply these concepts in practical examples. In the provided Colab notebooks, you'll explore:

**Predicting house prices:** Learn how neural networks can analyze housing data to estimate property values.
[![](https://colab.research.google.com/assets/colab-badge.png)](https://colab.research.google.com/github/Mjrovai/UNIFEI-IESTI01-TinyML-2022.1/blob/main/00_Curse_Folder/1_Fundamentals/Class_07/TF_Boston_Housing_Regression.ipynb)

**Image Classification:** Discover how to build a network to understand the famous MNIST handwritten digit dataset.
[![](https://colab.research.google.com/assets/colab-badge.png)](https://colab.research.google.com/github/Mjrovai/UNIFEI-IESTI01-TinyML-2022.1/blob/main/00_Curse_Folder/1_Fundamentals/Class_09/TF_MNIST_Classification_v2.ipynb)

**Real-world medical diagnosis:** Use deep learning to tackle the important task of breast cancer classification.
[![](https://colab.research.google.com/assets/colab-badge.png)](https://colab.research.google.com/github/Mjrovai/UNIFEI-IESTI01-TinyML-2022.1/blob/main/00_Curse_Folder/1_Fundamentals/Class_13/docs/WDBC_Project/Breast_Cancer_Classification.ipynb)

::: -->
</section>
</section>
</section>
<section id="convolutional-neural-networks-spatial-pattern-processing" class="level2 page-columns page-full" data-number="4.3">
<h2 data-number="4.3" class="anchored" data-anchor-id="convolutional-neural-networks-spatial-pattern-processing"><span class="header-section-number">4.3</span> Convolutional Neural Networks: Spatial Pattern Processing</h2>
<p>While MLPs treat each input element independently, many real-world data types exhibit strong spatial relationships. Images, for example, derive their meaning from the spatial arrangement of pixels—a pattern of edges and textures that form recognizable objects. Audio signals show temporal patterns of frequency components, and sensor data often contains spatial or temporal correlations. These spatial relationships suggest that treating every input-output connection with equal importance, as MLPs do, might not be the most effective approach.</p>
<section id="pattern-processing-needs-1" class="level3 page-columns page-full" data-number="4.3.1">
<h3 data-number="4.3.1" class="anchored" data-anchor-id="pattern-processing-needs-1"><span class="header-section-number">4.3.1</span> Pattern Processing Needs</h3>
<p>Spatial pattern processing addresses scenarios where the relationship between data points depends on their relative positions or proximity. Consider processing a natural image: a pixel’s relationship with its neighbors is important for detecting edges, textures, and shapes. These local patterns then combine hierarchically to form more complex features—edges form shapes, shapes form objects, and objects form scenes.</p>
<p>This hierarchical spatial pattern processing appears across many domains. In computer vision, local pixel patterns form edges and textures that combine into recognizable objects. Speech processing relies on patterns across nearby time segments to identify phonemes and words. Sensor networks analyze correlations between physically proximate sensors to understand environmental patterns. Medical imaging depends on recognizing tissue patterns that indicate biological structures.</p>
<p>Taking image processing as an example, if we want to detect a cat in an image, certain spatial patterns must be recognized: the triangular shape of ears, the round contours of the face, the texture of fur. Importantly, these patterns maintain their meaning regardless of where they appear in the image—a cat is still a cat whether it’s in the top-left or bottom-right corner. This suggests two key requirements for spatial pattern processing: the ability to detect local patterns and the ability to recognize these patterns regardless of their position.</p>
<p>This leads us to the convolutional neural network architecture (CNN), introduced by <span class="citation" data-cites="lecun1989backpropagation">LeCun et al. (<a href="../references.html#ref-lecun1989backpropagation" role="doc-biblioref">1989</a>)</span>. CNNs address spatial pattern processing through a fundamentally different connection pattern than MLPs. Instead of connecting every input to every output, CNNs use a local connection pattern where each output connects only to a small, spatially contiguous region of the input. This local receptive field moves across the input space, applying the same set of weights at each position—a process known as convolution.</p>
<div class="no-row-height column-margin column-container"><div id="ref-lecun1989backpropagation" class="csl-entry" role="listitem">
LeCun, Yann, Bernhard Boser, John S Denker, Donnie Henderson, Richard E Howard, Wayne Hubbard, and Lawrence D Jackel. 1989. <span>“Backpropagation Applied to Handwritten Zip Code Recognition.”</span> <em>Neural Computation</em> 1 (4): 541–51.
</div></div></section>
<section id="algorithmic-structure-1" class="level3" data-number="4.3.2">
<h3 data-number="4.3.2" class="anchored" data-anchor-id="algorithmic-structure-1"><span class="header-section-number">4.3.2</span> Algorithmic Structure</h3>
<p>The core operation in a CNN can be expressed mathematically as:</p>
<p><span class="math display">\[
\mathbf{H}^{(l)}_{i,j,k} = f(\sum_{di}\sum_{dj}\sum_{c} \mathbf{W}^{(l)}_{di,dj,c,k}\mathbf{H}^{(l-1)}_{i+di,j+dj,c} + \mathbf{b}^{(l)}_k)
\]</span></p>
<p>Here, <span class="math inline">\((i,j)\)</span> represents spatial positions, <span class="math inline">\(k\)</span> indexes output channels, <span class="math inline">\(c\)</span> indexes input channels, and <span class="math inline">\((di,dj)\)</span> spans the local receptive field. Unlike the dense matrix multiplication of MLPs, this operation:</p>
<ul>
<li>Processes local neighborhoods (typically 3×3 or 5×5)</li>
<li>Reuses the same weights at each spatial position</li>
<li>Maintains spatial structure in its output</li>
</ul>
<p>For a concrete example, consider our MNIST digit classification task with 28×28 grayscale images. Each convolutional layer applies a set of filters (say 3×3) that slide across the image, computing local weighted sums. If we use 32 filters, the layer produces a 28×28×32 output, where each spatial position contains 32 different feature measurements of its local neighborhood. This is in stark contrast to our MLP approach where we flattened the entire image into a 784-dimensional vector.</p>
<p>This algorithmic structure directly implements the requirements we identified for spatial pattern processing, creating distinct computational patterns that influence system design.</p>
<div id="fig-cnn" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-cnn-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="images/gif/cnn.gif" class="lightbox" data-gallery="quarto-lightbox-gallery-3" title="Figure&nbsp;4.2: Convolution operation, image data (blue) and 3x3 filter (green). Source: V. Dumoulin, F. Visin, MIT"><img src="images/gif/cnn.gif" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-cnn-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4.2: Convolution operation, image data (blue) and 3x3 filter (green). Source: V. Dumoulin, F. Visin, MIT
</figcaption>
</figure>
</div>
</section>
<section id="computational-mapping-1" class="level3" data-number="4.3.3">
<h3 data-number="4.3.3" class="anchored" data-anchor-id="computational-mapping-1"><span class="header-section-number">4.3.3</span> Computational Mapping</h3>
<p>The elegant spatial structure of convolution operations maps to computational patterns quite different from the dense matrix multiplication of MLPs. Let’s examine how this mapping progresses from mathematical abstraction to computational reality.</p>
<p>The first implementation, <code>conv_layer_spatial</code>, uses high-level convolution operations to express the computation concisely. This is typical in deep learning frameworks, where optimized libraries handle the underlying complexity.</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Mathematical abstraction - simple and clean</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> conv_layer_spatial(<span class="bu">input</span>, kernel, bias):</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>    output <span class="op">=</span> convolution(<span class="bu">input</span>, kernel) <span class="op">+</span> bias</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> activation(output)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>The second implementation, <code>conv_layer_compute</code>, reveals the actual computational pattern: nested loops that process each spatial position, applying the same filter weights to local regions of the input. The nested loops in <code>conv_layer_compute</code> reveal the true nature of convolution’s computational pattern.</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># System reality - nested loops of computation</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> conv_layer_compute(<span class="bu">input</span>, kernel, bias):</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Loop 1: Process each image in batch</span></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> image <span class="kw">in</span> <span class="bu">range</span>(batch_size):</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Loop 2&amp;3: Move across image spatially</span></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> y <span class="kw">in</span> <span class="bu">range</span>(height):</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> x <span class="kw">in</span> <span class="bu">range</span>(width):</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>                </span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>                <span class="co"># Loop 4: Compute each output feature</span></span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>                <span class="cf">for</span> out_channel <span class="kw">in</span> <span class="bu">range</span>(num_output_channels):</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>                    result <span class="op">=</span> bias[out_channel]</span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>                    </span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a>                    <span class="co"># Loop 5&amp;6: Move across kernel window</span></span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a>                    <span class="cf">for</span> ky <span class="kw">in</span> <span class="bu">range</span>(kernel_height):</span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a>                        <span class="cf">for</span> kx <span class="kw">in</span> <span class="bu">range</span>(kernel_width):</span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a>                            </span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a>                            <span class="co"># Loop 7: Process each input feature</span></span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a>                            <span class="cf">for</span> in_channel <span class="kw">in</span> <span class="bu">range</span>(num_input_channels):</span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a>                                <span class="co"># Get input value from correct window position</span></span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a>                                in_y <span class="op">=</span> y <span class="op">+</span> ky  </span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a>                                in_x <span class="op">=</span> x <span class="op">+</span> kx</span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a>                                <span class="co"># Perform multiply-accumulate operation</span></span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a>                                result <span class="op">+=</span> <span class="bu">input</span>[image, in_y, in_x, in_channel] <span class="op">*</span> <span class="op">\</span></span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a>                                         kernel[ky, kx, in_channel, out_channel]</span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a>                    </span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a>                    <span class="co"># Store result for this output position</span></span>
<span id="cb4-28"><a href="#cb4-28" aria-hidden="true" tabindex="-1"></a>                    output[image, y, x, out_channel] <span class="op">=</span> result</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>The seven nested loops reveal different aspects of the computation:</p>
<ul>
<li>Outer loops (1-3) manage position: which image and where in the image</li>
<li>Middle loop (4) handles output features: computing different learned patterns</li>
<li>Inner loops (5-7) perform the actual convolution: sliding the kernel window</li>
</ul>
<p>Let’s take a closer look. The outer two loops (<code>for y</code> and <code>for x</code>) traverse each spatial position in the output feature map—for our MNIST example, this means moving across all 28×28 positions. At each position, we compute values for each output channel (<code>for k</code> loop), which represents different learned features or patterns—our 32 different feature detectors.</p>
<p>The inner three loops implement the actual convolution operation at each position. For each output value, we process a local 3×3 region of the input (the <code>dy</code> and <code>dx</code> loops) across all input channels (<code>for c</code> loop). This creates a sliding window effect, where the same 3×3 filter moves across the image, performing multiply-accumulates between the filter weights and the local input values. Unlike the MLP’s global connectivity, this local processing pattern means each output value depends only on a small neighborhood of the input.</p>
<p>For our MNIST example with 3×3 filters and 32 output channels, each output position requires only 9 multiply-accumulate operations per input channel, compared to the 784 operations needed in our MLP layer. However, this operation must be repeated for every spatial position (28×28) and every output channel (32).</p>
<p>While using fewer operations per output, the spatial structure creates different patterns of memory access and computation that systems must handle efficiently. These patterns fundamentally influence system design, creating both challenges and opportunities for optimization, which we’ll examine next.</p>
</section>
<section id="system-implications-1" class="level3" data-number="4.3.4">
<h3 data-number="4.3.4" class="anchored" data-anchor-id="system-implications-1"><span class="header-section-number">4.3.4</span> System Implications</h3>
<p>When analyzing how computational patterns impact computer systems, we examine three fundamental dimensions: memory requirements, computation needs, and data movement. For CNNs, the spatial nature of processing creates distinctive patterns in each dimension that differ significantly from the dense connectivity of MLPs.</p>
<section id="memory-requirements-1" class="level4">
<h4 class="anchored" data-anchor-id="memory-requirements-1">Memory Requirements</h4>
<p>For convolutional layers, memory requirements center around two key components: filter weights and feature maps. Unlike MLPs that require storing full connection matrices, CNNs use small, reusable filters. In our MNIST example, a convolutional layer with 32 filters of size 3×3 requires storing only 288 weight parameters (3×3×32), in contrast to the 78,400 weights needed for our MLP’s fully-connected layer. However, the system must store feature maps for all spatial positions, creating a different memory demand—a 28×28 input with 32 output channels requires storing 25,088 activation values (28×28×32).</p>
<p>These memory access patterns suggest opportunities for optimization through weight reuse and careful feature map management. Modern processors handle these patterns by caching filter weights, which are reused across spatial positions, while streaming through feature map data. Deep learning frameworks typically implement this through specialized memory layouts that optimize for both filter reuse and spatial locality in feature map access. CPUs and GPUs approach this differently—CPUs leverage their cache hierarchy to keep frequently used filters resident, while GPUs use specialized memory architectures designed for the spatial access patterns of image processing.</p>
</section>
<section id="computation-needs-1" class="level4">
<h4 class="anchored" data-anchor-id="computation-needs-1">Computation Needs</h4>
<p>The core computation in CNNs involves repeatedly applying small filters across spatial positions. Each output value requires a local multiply-accumulate operation over the filter region. For our MNIST example with 3×3 filters and 32 output channels, computing one spatial position involves 288 multiply-accumulates (3×3×32), and this must be repeated for all 784 spatial positions (28×28). While each individual computation involves fewer operations than an MLP layer, the total computational load remains substantial due to spatial repetition.</p>
<p>This computational pattern presents different optimization opportunities than MLPs. The regular, repeated nature of convolution operations enables efficient hardware utilization through structured parallelism. Modern processors exploit this pattern in various ways. CPUs leverage SIMD instructions to process multiple filter positions simultaneously, while GPUs parallelize computation across spatial positions and channels. Deep learning frameworks further optimize this through specialized convolution algorithms that transform the computation to better match hardware capabilities.</p>
</section>
<section id="data-movement-1" class="level4">
<h4 class="anchored" data-anchor-id="data-movement-1">Data Movement</h4>
<p>The sliding window pattern of convolutions creates a distinctive data movement profile. Unlike MLPs where each weight is used once per forward pass, CNN filter weights are reused many times as the filter slides across spatial positions. For our MNIST example, each 3×3 filter weight is reused 784 times (once for each position in the 28×28 feature map). However, this creates a different challenge: the system must stream input features through the computation unit while keeping filter weights stable.</p>
<p>The predictable spatial access pattern enables strategic data movement optimizations. Different architectures handle this movement pattern through specialized mechanisms. CPUs maintain frequently used filter weights in cache while streaming through input features. GPUs employ memory architectures optimized for spatial locality and provide hardware support for efficient sliding window operations. Deep learning frameworks orchestrate these movements by organizing computations to maximize filter weight reuse and minimize redundant feature map accesses.</p>
</section>
</section>
</section>
<section id="recurrent-neural-networks-sequential-pattern-processing" class="level2 page-columns page-full" data-number="4.4">
<h2 data-number="4.4" class="anchored" data-anchor-id="recurrent-neural-networks-sequential-pattern-processing"><span class="header-section-number">4.4</span> Recurrent Neural Networks: Sequential Pattern Processing</h2>
<p>While MLPs handle arbitrary relationships and CNNs process spatial patterns, many real-world problems involve sequential data where the order and relationship between elements over time matters. Text processing requires understanding how words relate to previous context, speech recognition needs to track how sounds form coherent patterns, and time-series analysis must capture how values evolve over time. These sequential relationships suggest that treating each time step independently misses crucial temporal patterns.</p>
<section id="pattern-processing-needs-2" class="level3" data-number="4.4.1">
<h3 data-number="4.4.1" class="anchored" data-anchor-id="pattern-processing-needs-2"><span class="header-section-number">4.4.1</span> Pattern Processing Needs</h3>
<p>Sequential pattern processing addresses scenarios where the meaning of current input depends on what came before it. Consider natural language processing: the meaning of a word often depends heavily on previous words in the sentence. The word “bank” means something different in “river bank” versus “bank account.” Similarly, in speech recognition, a phoneme’s interpretation often depends on surrounding sounds, and in financial forecasting, future predictions require understanding patterns in historical data.</p>
<p>The key challenge in sequential processing is maintaining and updating relevant context over time. When reading text, humans don’t start fresh with each word—we maintain a running understanding that evolves as we process new information. Similarly, when processing time-series data, patterns might span different timescales, from immediate dependencies to long-term trends. This suggests we need an architecture that can both maintain state over time and update it based on new inputs.</p>
<p>These requirements demand specific capabilities from our processing architecture. The system must maintain internal state to capture temporal context, update this state based on new inputs, and learn which historical information is relevant for current predictions. Unlike MLPs and CNNs, which process fixed-size inputs, sequential processing must handle variable-length sequences while maintaining computational efficiency. This leads us to the recurrent neural network (RNN) architecture.</p>
</section>
<section id="algorithmic-structure-2" class="level3 page-columns page-full" data-number="4.4.2">
<h3 data-number="4.4.2" class="anchored" data-anchor-id="algorithmic-structure-2"><span class="header-section-number">4.4.2</span> Algorithmic Structure</h3>
<p>RNNs address sequential processing through a fundamentally different approach than MLPs or CNNs by introducing recurrent connections. Instead of just mapping inputs to outputs, RNNs maintain an internal state that is updated at each time step. This creates a memory mechanism that allows the network to carry information forward in time. This unique ability to model temporal dependencies was first explored by <span class="citation" data-cites="elman1990finding">Elman (<a href="../references.html#ref-elman1990finding" role="doc-biblioref">1990</a>)</span>, who demonstrated how RNNs could find structure in time-dependent data.</p>
<div class="no-row-height column-margin column-container"></div><p>The core operation in a basic RNN can be expressed mathematically as:</p>
<p><span class="math display">\[
\mathbf{h}_t = f(\mathbf{W}_{hh}\mathbf{h}_{t-1} + \mathbf{W}_{xh}\mathbf{x}_t + \mathbf{b}_h)
\]</span></p>
<p>where <span class="math inline">\(\mathbf{h}_t\)</span> represents the hidden state at time <span class="math inline">\(t\)</span>, <span class="math inline">\(\mathbf{x}_t\)</span> is the input at time <span class="math inline">\(t\)</span>, <span class="math inline">\(\mathbf{W}_{hh}\)</span> contains the recurrent weights, and <span class="math inline">\(\mathbf{W}_{xh}\)</span> contains the input weights.</p>
<p>For example, in processing a sequence of words, each word might be represented as a 100-dimensional vector (<span class="math inline">\(\mathbf{x}_t\)</span>), and we might maintain a hidden state of 128 dimensions (<span class="math inline">\(\mathbf{h}_t\)</span>). At each time step, the network combines the current input with its previous state to update its understanding of the sequence. This creates a form of memory that can capture patterns across time steps.</p>
<p>This recurrent structure directly implements our requirements for sequential processing through the introduction of recurrent connections, which maintain internal state and allow the network to carry information forward in time. Instead of processing all inputs independently, RNNs process sequences of data by iteratively updating a hidden state based on the current input and the previous hidden state. This makes RNNs well-suited for tasks such as language modeling, speech recognition, and time-series forecasting.</p>
</section>
<section id="computational-mapping-2" class="level3" data-number="4.4.3">
<h3 data-number="4.4.3" class="anchored" data-anchor-id="computational-mapping-2"><span class="header-section-number">4.4.3</span> Computational Mapping</h3>
<p>The sequential structure of RNNs maps to computational patterns quite different from both MLPs and CNNs. Let’s examine how this mapping progresses from mathematical abstraction to computational reality.</p>
<p>The <code>rnn_layer_step</code> function shows how the operation looks when using high-level matrix operations found in deep learning frameworks. It handles a single time step, taking the current input <code>x_t</code> and previous hidden state <code>h_prev</code>, along with two weight matrices: <code>W_hh</code> for hidden-to-hidden connections and <code>W_xh</code> for input-to-hidden connections. Through matrix multiplication operations (<code>matmul</code>), it merges the previous state and current input to generate the next hidden state.</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Mathematical abstraction in code</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> rnn_layer_step(x_t, h_prev, W_hh, W_xh, b):</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>    <span class="co"># x_t: input at time t (batch_size × input_dim)</span></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># h_prev: previous hidden state (batch_size × hidden_dim)</span></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>    <span class="co"># W_hh: recurrent weights (hidden_dim × hidden_dim)</span></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>    <span class="co"># W_xh: input weights (input_dim × hidden_dim)</span></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>    h_t <span class="op">=</span> activation(matmul(h_prev, W_hh) <span class="op">+</span> matmul(x_t, W_xh) <span class="op">+</span> b)</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> h_t</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>This simplified view masks the underlying complexity of the nested loops and individual computations shown in the detailed implementation. Its actual implementation reveals a more detailed computational reality:</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Core computational pattern</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> rnn_layer_compute(x_t, h_prev, W_hh, W_xh, b):</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Initialize next hidden state</span></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>    h_t <span class="op">=</span> np.zeros_like(h_prev)</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Loop 1: Process each sequence in the batch</span></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> batch <span class="kw">in</span> <span class="bu">range</span>(batch_size):</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Loop 2: Compute recurrent contribution (h_prev × W_hh)</span></span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(hidden_dim):</span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(hidden_dim):</span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>                h_t[batch,i] <span class="op">+=</span> h_prev[batch,j] <span class="op">*</span> W_hh[j,i]</span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>                </span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Loop 3: Compute input contribution (x_t × W_xh)</span></span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(hidden_dim):</span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(input_dim):</span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a>                h_t[batch,i] <span class="op">+=</span> x_t[batch,j] <span class="op">*</span> W_xh[j,i]</span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a>                </span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Loop 4: Add bias and apply activation</span></span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(hidden_dim):</span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a>            h_t[batch,i] <span class="op">=</span> activation(h_t[batch,i] <span class="op">+</span> b[i])</span>
<span id="cb6-21"><a href="#cb6-21" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb6-22"><a href="#cb6-22" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> h_t</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>The nested loops in <code>rnn_layer_compute</code> expose the core computational pattern of RNNs. Loop 1 processes each sequence in the batch independently, allowing for batch-level parallelism. Within each batch item, Loop 2 computes how the previous hidden state influences the next state through the recurrent weights <code>W_hh</code>. Loop 3 then incorporates new information from the current input through the input weights <code>W_xh</code>. Finally, Loop 4 adds biases and applies the activation function to produce the new hidden state.</p>
<p>For a sequence processing task with input dimension 100 and hidden state dimension 128, each time step requires two matrix multiplications: one 128×128 for the recurrent connection and one 100×128 for the input projection. While individual time steps can process in parallel across batch elements, the time steps themselves must process sequentially. This creates a unique computational pattern that systems must handle efficiently.</p>
</section>
<section id="system-implications-2" class="level3" data-number="4.4.4">
<h3 data-number="4.4.4" class="anchored" data-anchor-id="system-implications-2"><span class="header-section-number">4.4.4</span> System Implications</h3>
<p>For RNNs, the sequential nature of processing creates distinctive patterns in each dimension (memory requirements, computation needs, and data movement) that differ significantly from both MLPs and CNNs.</p>
<section id="memory-requirements-2" class="level4">
<h4 class="anchored" data-anchor-id="memory-requirements-2">Memory Requirements</h4>
<p>RNNs require storing two sets of weights (input-to-hidden and hidden-to-hidden) along with the hidden state. For our example with input dimension 100 and hidden state dimension 128, this means storing 12,800 weights for input projection (100×128) and 16,384 weights for recurrent connections (128×128). Unlike CNNs where weights are reused across spatial positions, RNN weights are reused across time steps. Additionally, the system must maintain the hidden state, which becomes a critical factor in memory usage and access patterns.</p>
<p>These memory access patterns create a different profile from MLPs and CNNs. Modern processors handle these patterns by keeping the weight matrices in cache while streaming through sequence elements. Deep learning frameworks optimize memory access by batching sequences together and carefully managing hidden state storage between time steps. CPUs and GPUs approach this through different strategies—CPUs leverage their cache hierarchy for weight reuse, while GPUs use specialized memory architectures designed for maintaining state across sequential operations.</p>
</section>
<section id="computation-needs-2" class="level4">
<h4 class="anchored" data-anchor-id="computation-needs-2">Computation Needs</h4>
<p>The core computation in RNNs involves repeatedly applying weight matrices across time steps. For each time step, we perform two matrix multiplications: one with the input weights and one with the recurrent weights. In our example, processing a single time step requires 12,800 multiply-accumulates for the input projection (100×128) and 16,384 multiply-accumulates for the recurrent connection (128×128).</p>
<p>This computational pattern differs from both MLPs and CNNs in a key way: while we can parallelize across batch elements, we cannot parallelize across time steps due to the sequential dependency. Each time step must wait for the previous step’s hidden state before it can begin computation. This creates a tension between the inherent sequential nature of the algorithm and the desire for parallel execution in modern hardware.</p>
<p>Modern processors handle these patterns through different approaches. CPUs pipeline operations within each time step while maintaining the sequential order across steps. GPUs batch multiple sequences together to maintain high throughput despite sequential dependencies. Deep learning frameworks optimize this further by techniques like sequence packing and unrolling computations across multiple time steps when possible.</p>
</section>
<section id="data-movement-2" class="level4">
<h4 class="anchored" data-anchor-id="data-movement-2">Data Movement</h4>
<p>The sequential processing in RNNs creates a distinctive data movement pattern that differs from both MLPs and CNNs. While MLPs need each weight only once per forward pass and CNNs reuse weights across spatial positions, RNNs reuse their weights across time steps while requiring careful management of the hidden state data flow.</p>
<p>For our example with a 128-dimensional hidden state, each time step must: load the previous hidden state (128 values), access both weight matrices (29,184 total weights from both input and recurrent connections), and store the new hidden state (128 values). This pattern repeats for every element in the sequence. Unlike CNNs where we can predict and prefetch data based on spatial patterns, RNN data movement is driven by temporal dependencies.</p>
<p>Different architectures handle this sequential data movement through specialized mechanisms. CPUs maintain weight matrices in cache while streaming through sequence elements and managing hidden state updates. GPUs employ memory architectures optimized for maintaining state information across sequential operations while processing multiple sequences in parallel. Deep learning frameworks orchestrate these movements by managing data transfers between time steps and optimizing batch operations.</p>
</section>
</section>
</section>
<section id="attention-mechanisms-dynamic-pattern-processing" class="level2 page-columns page-full" data-number="4.5">
<h2 data-number="4.5" class="anchored" data-anchor-id="attention-mechanisms-dynamic-pattern-processing"><span class="header-section-number">4.5</span> Attention Mechanisms: Dynamic Pattern Processing</h2>
<p>While previous architectures process patterns in fixed ways—MLPs with dense connectivity, CNNs with spatial operations, and RNNs with sequential updates—many tasks require dynamic relationships between elements that change based on content. Language understanding, for instance, needs to capture relationships between words that depend on meaning rather than just position. Graph analysis requires understanding connections that vary by node. These dynamic relationships suggest we need an architecture that can learn and adapt its processing patterns based on the data itself.</p>
<section id="pattern-processing-needs-3" class="level3" data-number="4.5.1">
<h3 data-number="4.5.1" class="anchored" data-anchor-id="pattern-processing-needs-3"><span class="header-section-number">4.5.1</span> Pattern Processing Needs</h3>
<p>Dynamic pattern processing addresses scenarios where relationships between elements aren’t fixed by architecture but instead emerge from content. Consider language translation: when translating “the bank by the river,” understanding “bank” requires attending to “river,” but in “the bank approved the loan,” the important relationship is with “approved” and “loan.” Unlike RNNs that process information sequentially or CNNs that use fixed spatial patterns, we need an architecture that can dynamically determine which relationships matter.</p>
<p>This requirement for dynamic processing appears across many domains. In protein structure prediction, interactions between amino acids depend on their chemical properties and spatial arrangements. In graph analysis, node relationships vary based on graph structure and node features. In document analysis, connections between different sections depend on semantic content rather than just proximity.</p>
<p>These scenarios demand specific capabilities from our processing architecture. The system must compute relationships between all pairs of elements, weigh these relationships based on content, and use these weights to selectively combine information. Unlike previous architectures with fixed connectivity patterns, dynamic processing requires the flexibility to modify its computation graph based on the input itself. This leads us to the Transformer architecture, which implements these capabilities through attention mechanisms.</p>
</section>
<section id="basic-attention-mechanism" class="level3 page-columns page-full" data-number="4.5.2">
<h3 data-number="4.5.2" class="anchored" data-anchor-id="basic-attention-mechanism"><span class="header-section-number">4.5.2</span> Basic Attention Mechanism</h3>
<section id="algorithmic-structure-3" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="algorithmic-structure-3">Algorithmic Structure</h4>
<p>Attention mechanisms form the foundation of dynamic pattern processing by computing weighted connections between elements based on their content <span class="citation" data-cites="bahdanau2014neural">(<a href="../references.html#ref-bahdanau2014neural" role="doc-biblioref">Bahdanau, Cho, and Bengio 2014</a>)</span>. This approach allows for the processing of relationships that aren’t fixed by architecture but instead emerge from the data itself. At the core of an attention mechanism lies a fundamental operation that can be expressed mathematically as:</p>
<div class="no-row-height column-margin column-container"></div><p><span class="math display">\[
\text{Attention}(\mathbf{Q}, \mathbf{K}, \mathbf{V}) = \text{softmax}(\frac{\mathbf{Q}\mathbf{K}^T}{\sqrt{d_k}})\mathbf{V}
\]</span></p>
<p>In this equation, Q (queries), K (keys), and V (values) represent learned projections of the input. For a sequence of length N with dimension d, this operation creates an N×N attention matrix, determining how each position should attend to all others.</p>
<p>The attention operation involves several key steps. First, it computes query, key, and value projections for each position in the sequence. Next, it generates an N×N attention matrix through query-key interactions. Finally, it uses these attention weights to combine value vectors, producing the output. Unlike the fixed weight matrices found in previous architectures, these attention weights are computed dynamically for each input, allowing the model to adapt its processing based on the content at hand.</p>
</section>
<section id="computational-mapping-3" class="level4">
<h4 class="anchored" data-anchor-id="computational-mapping-3">Computational Mapping</h4>
<p>The dynamic structure of attention operations maps to computational patterns that differ significantly from those of previous architectures. To understand this mapping, let’s examine how it progresses from mathematical abstraction to computational reality:</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Mathematical abstraction in code</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> attention_layer_matrix(Q, K, V):</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Q, K, V: (batch_size × seq_len × d_model)</span></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>    scores <span class="op">=</span> matmul(Q, K.transpose(<span class="op">-</span><span class="dv">2</span>, <span class="op">-</span><span class="dv">1</span>)) <span class="op">/</span> sqrt(d_k)  <span class="co"># Compute attention scores</span></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>    weights <span class="op">=</span> softmax(scores)                            <span class="co"># Normalize scores</span></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>    output <span class="op">=</span> matmul(weights, V)                         <span class="co"># Combine values</span></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> output</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Core computational pattern</span></span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> attention_layer_compute(Q, K, V):</span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Initialize outputs</span></span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a>    scores <span class="op">=</span> np.zeros((batch_size, seq_len, seq_len))</span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a>    outputs <span class="op">=</span> np.zeros_like(V)</span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Loop 1: Process each sequence in batch</span></span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> b <span class="kw">in</span> <span class="bu">range</span>(batch_size):</span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Loop 2: Compute attention for each query position</span></span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(seq_len):</span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Loop 3: Compare with each key position</span></span>
<span id="cb7-20"><a href="#cb7-20" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(seq_len):</span>
<span id="cb7-21"><a href="#cb7-21" aria-hidden="true" tabindex="-1"></a>                <span class="co"># Compute attention score</span></span>
<span id="cb7-22"><a href="#cb7-22" aria-hidden="true" tabindex="-1"></a>                <span class="cf">for</span> d <span class="kw">in</span> <span class="bu">range</span>(d_model):</span>
<span id="cb7-23"><a href="#cb7-23" aria-hidden="true" tabindex="-1"></a>                    scores[b,i,j] <span class="op">+=</span> Q[b,i,d] <span class="op">*</span> K[b,j,d]</span>
<span id="cb7-24"><a href="#cb7-24" aria-hidden="true" tabindex="-1"></a>                scores[b,i,j] <span class="op">/=</span> sqrt(d_k)</span>
<span id="cb7-25"><a href="#cb7-25" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb7-26"><a href="#cb7-26" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Apply softmax to scores</span></span>
<span id="cb7-27"><a href="#cb7-27" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(seq_len):</span>
<span id="cb7-28"><a href="#cb7-28" aria-hidden="true" tabindex="-1"></a>            scores[b,i] <span class="op">=</span> softmax(scores[b,i])</span>
<span id="cb7-29"><a href="#cb7-29" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb7-30"><a href="#cb7-30" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Loop 4: Combine values using attention weights</span></span>
<span id="cb7-31"><a href="#cb7-31" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(seq_len):</span>
<span id="cb7-32"><a href="#cb7-32" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(seq_len):</span>
<span id="cb7-33"><a href="#cb7-33" aria-hidden="true" tabindex="-1"></a>                <span class="cf">for</span> d <span class="kw">in</span> <span class="bu">range</span>(d_model):</span>
<span id="cb7-34"><a href="#cb7-34" aria-hidden="true" tabindex="-1"></a>                    outputs[b,i,d] <span class="op">+=</span> scores[b,i,j] <span class="op">*</span> V[b,j,d]</span>
<span id="cb7-35"><a href="#cb7-35" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb7-36"><a href="#cb7-36" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> outputs</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>The nested loops in <code>attention_layer_compute</code> reveal the true nature of attention’s computational pattern. The first loop processes each sequence in the batch independently. The second and third loops compute attention scores between all pairs of positions, creating a quadratic computation pattern with respect to sequence length. The fourth loop uses these attention weights to combine values from all positions, producing the final output.</p>
</section>
<section id="system-implications-3" class="level4">
<h4 class="anchored" data-anchor-id="system-implications-3">System Implications</h4>
<p>The attention mechanism creates distinctive patterns in memory requirements, computation needs, and data movement that set it apart from previous architectures.</p>
<section id="memory-requirements-3" class="level5">
<h5 class="anchored" data-anchor-id="memory-requirements-3">Memory Requirements</h5>
<p>In terms of memory requirements, attention mechanisms necessitate storage for attention weights, key-query-value projections, and intermediate feature representations. For a sequence length N and dimension d, each attention layer must store an N×N attention weight matrix for each sequence in the batch, three sets of projection matrices for queries, keys, and values (each sized d×d), and input and output feature maps of size N×d.&nbsp;The dynamic generation of attention weights for every input creates a memory access pattern where intermediate attention weights become a significant factor in memory usage.</p>
</section>
<section id="computation-needs-3" class="level5">
<h5 class="anchored" data-anchor-id="computation-needs-3">Computation Needs</h5>
<p>Computation needs in attention mechanisms center around two main phases: generating attention weights and applying them to values. For each attention layer, the system performs substantial multiply-accumulate operations across multiple computational stages. The query-key interactions alone require N×N×d multiply-accumulates, with an equal number needed for applying attention weights to values. Additional computations are required for the projection matrices and softmax operations. This computational pattern differs from previous architectures due to its quadratic scaling with sequence length and the need to perform fresh computations for each input.</p>
</section>
<section id="data-movement-3" class="level5">
<h5 class="anchored" data-anchor-id="data-movement-3">Data Movement</h5>
<p>Data movement in attention mechanisms presents unique challenges. Each attention operation involves projecting and moving query, key, and value vectors for each position, storing and accessing the full attention weight matrix, and coordinating the movement of value vectors during the weighted combination phase. This creates a data movement pattern where intermediate attention weights become a major factor in system bandwidth requirements. Unlike the more predictable access patterns of CNNs or the sequential access of RNNs, attention operations require frequent movement of dynamically computed weights across the memory hierarchy.</p>
<p>These distinctive characteristics of attention mechanisms in terms of memory, computation, and data movement have significant implications for system design and optimization, setting the stage for the development of more advanced architectures like Transformers.</p>
</section>
</section>
</section>
<section id="transformers-and-self-attention" class="level3 page-columns page-full" data-number="4.5.3">
<h3 data-number="4.5.3" class="anchored" data-anchor-id="transformers-and-self-attention"><span class="header-section-number">4.5.3</span> Transformers and Self-Attention</h3>
<p>Transformers, first introduced by <span class="citation" data-cites="vaswani2017attention">Vaswani et al. (<a href="../references.html#ref-vaswani2017attention" role="doc-biblioref">2017</a>)</span>, represent a significant evolution in the application of attention mechanisms, introducing the concept of self-attention to create a powerful architecture for dynamic pattern processing. While the basic attention mechanism allows for content-based weighting of information from a source sequence, Transformers extend this idea by applying attention within a single sequence, enabling each element to attend to all other elements including itself.</p>
<div class="no-row-height column-margin column-container"><div id="ref-vaswani2017attention" class="csl-entry" role="listitem">
Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. <span>“Attention Is All You Need.”</span> <em>Advances in Neural Information Processing Systems</em> 30: 5998–6008.
</div></div><section id="algorithmic-structure-4" class="level4">
<h4 class="anchored" data-anchor-id="algorithmic-structure-4">Algorithmic Structure</h4>
<p>The key innovation in Transformers lies in their use of self-attention layers. In a self-attention layer, the queries, keys, and values are all derived from the same input sequence. This allows the model to weigh the importance of different positions within the same sequence when encoding each position. For instance, in processing the sentence “The animal didn’t cross the street because it was too wide,” self-attention allows the model to link “it” with “street,” capturing long-range dependencies that are challenging for traditional sequential models.</p>
<p>Transformers typically employ multi-head attention, which involves multiple sets of query/key/value projections. Each set, or “head,” can focus on different aspects of the input, allowing the model to jointly attend to information from different representation subspaces. This multi-head structure provides the model with a richer representational capability, enabling it to capture various types of relationships within the data simultaneously.</p>
<p>The self-attention mechanism in Transformers can be expressed mathematically in a form similar to the basic attention mechanism:</p>
<p><span class="math display">\[
\text{SelfAttention}(\mathbf{X}) = \text{softmax}(\frac{\mathbf{XW_Q}(\mathbf{XW_K})^T}{\sqrt{d_k}})\mathbf{XW_V}
\]</span></p>
<p>Here, X is the input sequence, and <span class="math inline">\(W_Q\)</span>, <span class="math inline">\(W_K\)</span>, and <span class="math inline">\(W_V\)</span> are learned weight matrices for queries, keys, and values respectively. This formulation highlights how self-attention derives all its components from the same input, creating a dynamic, content-dependent processing pattern.</p>
<p>The Transformer architecture leverages this self-attention mechanism within a broader structure that typically includes feed-forward layers, layer normalization, and residual connections. This combination allows Transformers to process input sequences in parallel, capturing complex dependencies without the need for sequential computation. As a result, Transformers have demonstrated remarkable effectiveness across a wide range of tasks, from natural language processing to computer vision, revolutionizing the landscape of deep learning architectures.</p>
</section>
<section id="computational-mapping-4" class="level4">
<h4 class="anchored" data-anchor-id="computational-mapping-4">Computational Mapping</h4>
<p>While Transformer self-attention builds upon the basic attention mechanism, it introduces distinct computational patterns that set it apart. To understand these patterns, we must examine the typical implementation of self-attention in Transformers:</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> self_attention_layer(X, W_Q, W_K, W_V, d_k):</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># X: input tensor (batch_size × seq_len × d_model)</span></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>    <span class="co"># W_Q, W_K, W_V: weight matrices (d_model × d_k)</span></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>    Q <span class="op">=</span> matmul(X, W_Q)</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>    K <span class="op">=</span> matmul(X, W_K)</span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>    V <span class="op">=</span> matmul(X, W_V)</span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>    scores <span class="op">=</span> matmul(Q, K.transpose(<span class="op">-</span><span class="dv">2</span>, <span class="op">-</span><span class="dv">1</span>)) <span class="op">/</span> sqrt(d_k)</span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a>    attention_weights <span class="op">=</span> softmax(scores, dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a>    output <span class="op">=</span> matmul(attention_weights, V)</span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> output</span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> multi_head_attention(X, W_Q, W_K, W_V, W_O, num_heads, d_k):</span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a>    outputs <span class="op">=</span> []</span>
<span id="cb8-17"><a href="#cb8-17" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(num_heads):</span>
<span id="cb8-18"><a href="#cb8-18" aria-hidden="true" tabindex="-1"></a>        head_output <span class="op">=</span> self_attention_layer(X, W_Q[i], W_K[i], W_V[i], d_k)</span>
<span id="cb8-19"><a href="#cb8-19" aria-hidden="true" tabindex="-1"></a>        outputs.append(head_output)</span>
<span id="cb8-20"><a href="#cb8-20" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb8-21"><a href="#cb8-21" aria-hidden="true" tabindex="-1"></a>    concat_output <span class="op">=</span> torch.cat(outputs, dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb8-22"><a href="#cb8-22" aria-hidden="true" tabindex="-1"></a>    final_output <span class="op">=</span> matmul(concat_output, W_O)</span>
<span id="cb8-23"><a href="#cb8-23" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb8-24"><a href="#cb8-24" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> final_output</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="system-implications-4" class="level4">
<h4 class="anchored" data-anchor-id="system-implications-4">System Implications</h4>
<p>This implementation reveals several key computational characteristics of Transformer self-attention. First, self-attention enables parallel processing across all positions in the sequence. This is evident in the matrix multiplications that compute Q, K, and V simultaneously for all positions. Unlike recurrent architectures that process inputs sequentially, this parallel nature allows for more efficient computation, especially on modern hardware designed for parallel operations.</p>
<p>Second, the attention score computation results in a matrix of size (seq_len × seq_len), leading to quadratic complexity with respect to sequence length. This quadratic relationship becomes a significant computational bottleneck when processing long sequences, a challenge that has spurred research into more efficient attention mechanisms.</p>
<p>Third, the multi-head attention mechanism effectively runs multiple self-attention operations in parallel, each with its own set of learned projections. While this increases the computational load linearly with the number of heads, it allows the model to capture different types of relationships within the same input, enhancing the model’s representational power.</p>
<p>Fourth, the core computations in self-attention are dominated by large matrix multiplications. For a sequence of length N and embedding dimension d, the main operations involve matrices of sizes (N × d), (d × d), and (N × N). These intensive matrix operations are well-suited for acceleration on specialized hardware like GPUs, but they also contribute significantly to the overall computational cost of the model.</p>
<p>Finally, self-attention generates memory-intensive intermediate results. The attention weights matrix (N × N) and the intermediate results for each attention head create substantial memory requirements, especially for long sequences. This can pose challenges for deployment on memory-constrained devices and necessitates careful memory management in implementations.</p>
<p>These computational patterns create a unique profile for Transformer self-attention, distinct from previous architectures. The parallel nature of the computations makes Transformers well-suited for modern parallel processing hardware, but the quadratic complexity with sequence length poses challenges for processing long sequences. As a result, much research has focused on developing optimization techniques, such as sparse attention patterns or low-rank approximations, to address these challenges. Each of these optimizations presents its own trade-offs between computational efficiency and model expressiveness, a balance that must be carefully considered in practical applications.</p>
</section>
</section>
</section>
<section id="architectural-building-blocks" class="level2 page-columns page-full" data-number="4.6">
<h2 data-number="4.6" class="anchored" data-anchor-id="architectural-building-blocks"><span class="header-section-number">4.6</span> Architectural Building Blocks</h2>
<p>Deep learning architectures, while we presented them as distinct approaches in the previous sections, are better understood as compositions of fundamental building blocks that evolved over time. Much like how complex LEGO structures are built from basic bricks, modern neural networks combine and iterate on core computational patterns that emerged through decades of research <span class="citation" data-cites="lecun2015deep">(<a href="../references.html#ref-lecun2015deep" role="doc-biblioref">LeCun, Bengio, and Hinton 2015</a>)</span>. Each architectural innovation introduced new building blocks while finding novel ways to use existing ones.</p>
<div class="no-row-height column-margin column-container"><div id="ref-lecun2015deep" class="csl-entry" role="listitem">
LeCun, Yann, Yoshua Bengio, and Geoffrey Hinton. 2015. <span>“Deep Learning.”</span> <em>Nature</em> 521 (7553): 436–44.
</div><div id="ref-rosenblatt1958perceptron" class="csl-entry" role="listitem">
Rosenblatt, Frank. 1958. <span>“The Perceptron: A Probabilistic Model for Information Storage and Organization in the Brain.”</span> <em>Psychological Review</em> 65 (6): 386.
</div><div id="ref-rumelhart1986learning" class="csl-entry" role="listitem">
Rumelhart, David E, Geoffrey E Hinton, and Ronald J Williams. 1986. <span>“Learning Representations by Back-Propagating Errors.”</span> <em>Nature</em> 323 (6088): 533–36.
</div></div><p>These building blocks and their evolution provide insight into modern architectures. What began with the simple perceptron <span class="citation" data-cites="rosenblatt1958perceptron">(<a href="../references.html#ref-rosenblatt1958perceptron" role="doc-biblioref">Rosenblatt 1958</a>)</span> evolved into multi-layer networks <span class="citation" data-cites="rumelhart1986learning">(<a href="../references.html#ref-rumelhart1986learning" role="doc-biblioref">Rumelhart, Hinton, and Williams 1986</a>)</span>, which then spawned specialized patterns for spatial and sequential processing. Each advancement maintained useful elements from its predecessors while introducing new computational primitives. Today’s sophisticated architectures, like Transformers, can be seen as carefully engineered combinations of these fundamental building blocks.</p>
<p>This progression reveals not just the evolution of neural networks, but also the discovery and refinement of core computational patterns that remain relevant. As we have seen through our exploration of different neural network architectures, deep learning has evolved significantly, with each new architecture bringing its own set of computational demands and system-level challenges.</p>
<p><a href="#tbl-dl-evolution" class="quarto-xref">Table&nbsp;<span>4.1</span></a> summarizes this evolution, highlighting the key primitives and system focus for each era of deep learning development. This table encapsulates the major shifts in deep learning architecture design and the corresponding changes in system-level considerations. From the early focus on dense matrix operations optimized for CPUs, we see a progression through convolutions leveraging GPU acceleration, to sequential operations necessitating sophisticated memory hierarchies, and finally to the current era of attention mechanisms requiring flexible accelerators and high-bandwidth memory.</p>
<div id="tbl-dl-evolution" class="hover striped quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-dl-evolution-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;4.1: Evolution of deep learning architectures and their system implications
</figcaption>
<div aria-describedby="tbl-dl-evolution-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="table-hover table-striped caption-top table">
<colgroup>
<col style="width: 16%">
<col style="width: 19%">
<col style="width: 23%">
<col style="width: 38%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Era</th>
<th style="text-align: left;">Dominant Architecture</th>
<th style="text-align: left;">Key Primitives</th>
<th style="text-align: left;">System Focus</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Early NN</td>
<td style="text-align: left;">MLP</td>
<td style="text-align: left;">Dense Matrix Ops</td>
<td style="text-align: left;">CPU optimization</td>
</tr>
<tr class="even">
<td style="text-align: left;">CNN Revolution</td>
<td style="text-align: left;">CNN</td>
<td style="text-align: left;">Convolutions</td>
<td style="text-align: left;">GPU acceleration</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Sequence Modeling</td>
<td style="text-align: left;">RNN</td>
<td style="text-align: left;">Sequential Ops</td>
<td style="text-align: left;">Memory hierarchies</td>
</tr>
<tr class="even">
<td style="text-align: left;">Attention Era</td>
<td style="text-align: left;">Transformer</td>
<td style="text-align: left;">Attention, Dynamic Compute</td>
<td style="text-align: left;">Flexible accelerators, High-bandwidth memory</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p>As we dive deeper into each of these building blocks, we see how these primitives evolved and combined to create increasingly powerful and complex neural network architectures.</p>
<section id="from-perceptron-to-multi-layer-networks" class="level3" data-number="4.6.1">
<h3 data-number="4.6.1" class="anchored" data-anchor-id="from-perceptron-to-multi-layer-networks"><span class="header-section-number">4.6.1</span> From Perceptron to Multi-Layer Networks</h3>
<p>While we examined MLPs earlier as a mechanism for dense pattern processing, here we focus on how they established fundamental building blocks that appear throughout deep learning. The evolution from perceptron to MLP introduced several key concepts: the power of layer stacking, the importance of non-linear transformations, and the basic feedforward computation pattern.</p>
<p>The introduction of hidden layers between input and output created a template for feature transformation that appears in virtually every modern architecture. Even in sophisticated networks like Transformers, we find MLP-style feedforward layers performing feature processing. The concept of transforming data through successive non-linear layers has become a fundamental paradigm that transcends the specific architecture types.</p>
<p>Perhaps most importantly, the development of MLPs established the backpropagation algorithm, which to this day remains the cornerstone of neural network training. This key contribution has enabled the training of deep architectures and influenced how later architectures would be designed to maintain gradient flow.</p>
<p>These building blocks—layered feature transformation, non-linear activation, and gradient-based learning—set the foundation for more specialized architectures. Subsequent innovations often focused on structuring these basic components in new ways rather than replacing them entirely.</p>
</section>
<section id="from-dense-to-spatial-processing" class="level3 page-columns page-full" data-number="4.6.2">
<h3 data-number="4.6.2" class="anchored" data-anchor-id="from-dense-to-spatial-processing"><span class="header-section-number">4.6.2</span> From Dense to Spatial Processing</h3>
<p>The development of CNNs marked a significant architectural innovation—the realization that we could specialize the dense connectivity of MLPs for spatial patterns. While retaining the core concept of layer-wise processing, CNNs introduced several fundamental building blocks that would influence all future architectures.</p>
<p>The first key innovation was the concept of parameter sharing. Unlike MLPs where each connection had its own weight, CNNs showed how the same parameters could be reused across different parts of the input. This not only made the networks more efficient but introduced the powerful idea that architectural structure could encode useful priors about the data <span class="citation" data-cites="lecun1998gradient">(<a href="../references.html#ref-lecun1998gradient" role="doc-biblioref">LeCun et al. 1998</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-lecun1998gradient" class="csl-entry" role="listitem">
LeCun, Yann, Léon Bottou, Yoshua Bengio, and Patrick Haffner. 1998. <span>“Gradient-Based Learning Applied to Document Recognition.”</span> <em>Proceedings of the IEEE</em> 86 (11): 2278–2324.
</div><div id="ref-he2016deep" class="csl-entry" role="listitem">
He, Kaiming, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. <span>“Deep Residual Learning for Image Recognition.”</span> <em>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</em>, 770–78.
</div></div><p>Perhaps even more influential was the introduction of skip connections through ResNets <span class="citation" data-cites="he2016deep">(<a href="../references.html#ref-he2016deep" role="doc-biblioref">He et al. 2016</a>)</span>. Originally they were designed to help train very deep CNNs, skip connections have become a fundamental building block that appears in virtually every modern architecture. They showed how direct paths through the network could help gradient flow and information propagation, a concept now central to Transformer designs.</p>
<p>CNNs also introduced batch normalization, a technique for stabilizing neural network training by normalizing intermediate features <span class="citation" data-cites="ioffe2015batch">(<a href="../references.html#ref-ioffe2015batch" role="doc-biblioref">Ioffe and Szegedy 2015</a>)</span>; we will learn more about this in the AI Training chapter. This concept of feature normalization, while originating in CNNs, evolved into layer normalization and is now a key component in modern architectures.</p>
<div class="no-row-height column-margin column-container"><div id="ref-ioffe2015batch" class="csl-entry" role="listitem">
Ioffe, Sergey, and Christian Szegedy. 2015. <span>“Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift.”</span> <em>International Conference on Machine Learning</em>, 448–56.
</div></div><p>These innovations—parameter sharing, skip connections, and normalization—transcended their origins in spatial processing to become essential building blocks in the deep learning toolkit.</p>
</section>
<section id="the-evolution-of-sequence-processing" class="level3 page-columns page-full" data-number="4.6.3">
<h3 data-number="4.6.3" class="anchored" data-anchor-id="the-evolution-of-sequence-processing"><span class="header-section-number">4.6.3</span> The Evolution of Sequence Processing</h3>
<p>While CNNs specialized MLPs for spatial patterns, sequence models adapted neural networks for temporal dependencies. RNNs introduced the fundamental concept of maintaining and updating state—a building block that influenced how networks could process sequential information <span class="citation" data-cites="elman1990finding">(<a href="../references.html#ref-elman1990finding" role="doc-biblioref">Elman 1990</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-elman1990finding" class="csl-entry" role="listitem">
Elman, Jeffrey L. 1990. <span>“Finding Structure in Time.”</span> <em>Cognitive Science</em> 14 (2): 179–211.
</div><div id="ref-hochreiter1997long" class="csl-entry" role="listitem">
Hochreiter, Sepp, and Jürgen Schmidhuber. 1997. <span>“Long Short-Term Memory.”</span> <em>Neural Computation</em> 9 (8): 1735–80.
</div><div id="ref-cho2014properties" class="csl-entry" role="listitem">
Cho, Kyunghyun, Bart van Merrienboer, Dzmitry Bahdanau, and Yoshua Bengio. 2014. <span>“On the Properties of Neural Machine Translation: Encoder-Decoder Approaches.”</span> In <em>Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation (SSST-8)</em>, 103–11. Association for Computational Linguistics.
</div></div><p>The development of LSTMs and GRUs brought sophisticated gating mechanisms to neural networks <span class="citation" data-cites="hochreiter1997long cho2014properties">(<a href="../references.html#ref-hochreiter1997long" role="doc-biblioref">Hochreiter and Schmidhuber 1997</a>; <a href="../references.html#ref-cho2014properties" role="doc-biblioref">Cho et al. 2014</a>)</span>. These gates, themselves small MLPs, showed how simple feedforward computations could be composed to control information flow. This concept of using neural networks to modulate other neural networks became a recurring pattern in architecture design.</p>
<p>Perhaps most significantly, sequence models demonstrated the power of adaptive computation paths. Unlike the fixed patterns of MLPs and CNNs, RNNs showed how networks could process variable-length inputs by reusing weights over time. This insight—that architectural patterns could adapt to input structure—laid groundwork for more flexible architectures.</p>
<p>Sequence models also popularized the concept of attention through encoder-decoder architectures <span class="citation" data-cites="bahdanau2014neural">(<a href="../references.html#ref-bahdanau2014neural" role="doc-biblioref">Bahdanau, Cho, and Bengio 2014</a>)</span>. Initially introduced as an improvement to machine translation, attention mechanisms showed how networks could learn to dynamically focus on relevant information. This building block would later become the foundation of Transformer architectures.</p>
<div class="no-row-height column-margin column-container"><div id="ref-bahdanau2014neural" class="csl-entry" role="listitem">
Bahdanau, Dzmitry, Kyunghyun Cho, and Yoshua Bengio. 2014. <span>“Neural Machine Translation by Jointly Learning to Align and Translate.”</span> <em>arXiv Preprint arXiv:1409.0473</em>.
</div></div></section>
<section id="modern-architectures-synthesis-and-innovation" class="level3 page-columns page-full" data-number="4.6.4">
<h3 data-number="4.6.4" class="anchored" data-anchor-id="modern-architectures-synthesis-and-innovation"><span class="header-section-number">4.6.4</span> Modern Architectures: Synthesis and Innovation</h3>
<p>Modern architectures, particularly Transformers, represent a sophisticated synthesis of these fundamental building blocks. Rather than introducing entirely new patterns, they innovate through clever combination and refinement of existing components. Consider the Transformer architecture: at its core, we find MLP-style feedforward networks processing features between attention layers. The attention mechanism itself builds on ideas from sequence models but removes the recurrent connection, instead using position embeddings inspired by CNN intuitions. Skip connections, inherited from ResNets, appear throughout the architecture, while layer normalization, evolved from CNN’s batch normalization, stabilizes training <span class="citation" data-cites="ba2016layer">(<a href="../references.html#ref-ba2016layer" role="doc-biblioref">Ba, Kiros, and Hinton 2016</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-ba2016layer" class="csl-entry" role="listitem">
Ba, Jimmy Lei, Jamie Ryan Kiros, and Geoffrey E Hinton. 2016. <span>“Layer Normalization.”</span> <em>arXiv Preprint arXiv:1607.06450</em>.
</div></div><p>This composition of building blocks creates something greater than the sum of its parts. The self-attention mechanism, while building on previous attention concepts, enables a new form of dynamic pattern processing. The arrangement of these components—attention followed by feedforward layers, with skip connections and normalization—has proven so effective it’s become a template for new architectures.</p>
<p>Even recent innovations in vision and language models follow this pattern of recombining fundamental building blocks. Vision Transformers adapt the Transformer architecture to images while maintaining its essential components <span class="citation" data-cites="dosovitskiy2021image">(<a href="../references.html#ref-dosovitskiy2021image" role="doc-biblioref">Dosovitskiy et al. 2021</a>)</span>. Large language models scale up these patterns while introducing refinements like grouped-query attention or sliding window attention, yet still rely on the core building blocks established through this architectural evolution <span class="citation" data-cites="brown2020language">(<a href="../references.html#ref-brown2020language" role="doc-biblioref">Brown et al. 2020</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-dosovitskiy2021image" class="csl-entry" role="listitem">
Dosovitskiy, Alexey, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, et al. 2021. <span>“An Image Is Worth 16x16 Words: Transformers for Image Recognition at Scale.”</span> <em>International Conference on Learning Representations</em>.
</div><div id="ref-brown2020language" class="csl-entry" role="listitem">
Brown, Tom B, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, et al. 2020. <span>“Language Models Are Few-Shot Learners.”</span> <em>Advances in Neural Information Processing Systems</em> 33: 1877–1901.
</div></div><p>To illustrate how these modern architectures synthesize and innovate upon previous approaches, consider the following comparison of primitive utilization across different neural network architectures:</p>
<div id="tbl-primitive-comparison" class="hover striped quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-primitive-comparison-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;4.2: Comparison of primitive utilization across neural network architectures.
</figcaption>
<div aria-describedby="tbl-primitive-comparison-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="table-hover table-striped caption-top table">
<colgroup>
<col style="width: 13%">
<col style="width: 18%">
<col style="width: 22%">
<col style="width: 23%">
<col style="width: 20%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Primitive Type</th>
<th style="text-align: left;">MLP</th>
<th style="text-align: left;">CNN</th>
<th style="text-align: left;">RNN</th>
<th style="text-align: left;">Transformer</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Computational</td>
<td style="text-align: left;">Matrix Multiplication</td>
<td style="text-align: left;">Convolution (Matrix Mult.)</td>
<td style="text-align: left;">Matrix Mult. + State Update</td>
<td style="text-align: left;">Matrix Mult. + Attention</td>
</tr>
<tr class="even">
<td style="text-align: left;">Memory Access</td>
<td style="text-align: left;">Sequential</td>
<td style="text-align: left;">Strided</td>
<td style="text-align: left;">Sequential + Random</td>
<td style="text-align: left;">Random (Attention)</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Data Movement</td>
<td style="text-align: left;">Broadcast</td>
<td style="text-align: left;">Sliding Window</td>
<td style="text-align: left;">Sequential</td>
<td style="text-align: left;">Broadcast + Gather</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p>As shown in <a href="#tbl-primitive-comparison" class="quarto-xref">Table&nbsp;<span>4.2</span></a>, Transformers combine elements from previous architectures while introducing new patterns. They retain the core matrix multiplication operations common to all architectures but introduce a more complex memory access pattern with their attention mechanism. Their data movement patterns blend the broadcast operations of MLPs with the gather operations reminiscent of more dynamic architectures.</p>
<p>This synthesis of primitives in Transformers exemplifies how modern architectures innovate by recombining and refining existing building blocks, rather than inventing entirely new computational paradigms. Also, this evolutionary process provides insight into the development of future architectures and helps to guide the design of efficient systems to support them.</p>
</section>
</section>
<section id="system-level-building-blocks" class="level2" data-number="4.7">
<h2 data-number="4.7" class="anchored" data-anchor-id="system-level-building-blocks"><span class="header-section-number">4.7</span> System-Level Building Blocks</h2>
<p>After having examined different deep learning architectures, we can distill their system requirements into fundamental primitives that underpin both hardware and software implementations. These primitives represent operations that cannot be broken down further while maintaining their essential characteristics. Just as complex molecules are built from basic atoms, sophisticated neural networks are constructed from these fundamental operations.</p>
<section id="core-computational-primitives" class="level3" data-number="4.7.1">
<h3 data-number="4.7.1" class="anchored" data-anchor-id="core-computational-primitives"><span class="header-section-number">4.7.1</span> Core Computational Primitives</h3>
<p>Three fundamental operations serve as the building blocks for all deep learning computations: matrix multiplication, sliding window operations, and dynamic computation. What makes these operations primitive is that they cannot be further decomposed without losing their essential computational properties and efficiency characteristics.</p>
<p>Matrix multiplication represents the most basic form of transforming sets of features. When we multiply a matrix of inputs by a matrix of weights, we’re computing weighted combinations—the fundamental operation of neural networks. For example, in our MNIST network, each 784-dimensional input vector multiplies with a 784×100 weight matrix. This pattern appears everywhere: MLPs use it directly for layer computations, CNNs reshape convolutions into matrix multiplications through <code>im2col</code> (turning a 3×3 convolution into a matrix operation), and Transformers use it extensively in their attention mechanisms.</p>
<p>In modern systems, matrix multiplication maps to specific hardware and software implementations. Hardware accelerators provide specialized tensor cores that can perform thousands of multiply-accumulates in parallel—NVIDIA’s A100 tensor cores can achieve up to 312 TFLOPS (32-bit) through massive parallelization of these operations. Software frameworks like PyTorch and TensorFlow automatically map these high-level operations to optimized matrix libraries (NVIDIA <a href="https://developer.nvidia.com/cublas">cuBLAS</a>, Intel <a href="https://www.intel.com/content/www/us/en/developer/tools/oneapi/onemkl.html">MKL</a>) that exploit these hardware capabilities.</p>
<p>Sliding window operations compute local relationships by applying the same operation to chunks of data. In CNNs processing MNIST images, a 3×3 convolution filter slides across the 28×28 input, requiring 26×26 windows of computation. Modern hardware accelerators implement this through specialized memory access patterns and data buffering schemes that optimize data reuse. For example, Google’s TPU uses a 128×128 systolic array where data flows systematically through processing elements, allowing each input value to be reused across multiple computations without accessing memory. Software frameworks optimize these operations by transforming them into efficient matrix multiplications (a 3×3 convolution becomes a 9×N matrix multiplication) and carefully managing data layout in memory to maximize spatial locality.</p>
<p>Dynamic computation, where the operation itself depends on the input data, emerged prominently with attention mechanisms but represents a fundamental capability needed for adaptive processing. In Transformer attention, each query dynamically determines its interaction weights with all keys—for a sequence of length 512, this means 512 different weight patterns must be computed on the fly. Unlike fixed patterns where we know the computation graph in advance, dynamic computation requires runtime decisions. This creates specific implementation challenges—hardware must provide flexible routing of data (modern GPUs use dynamic scheduling) and support variable computation patterns, while software frameworks need efficient mechanisms for handling data-dependent execution paths (PyTorch’s dynamic computation graphs, TensorFlow’s dynamic control flow).</p>
<p>These primitives combine in sophisticated ways in modern architectures. A Transformer layer processing a sequence of 512 tokens demonstrates this clearly: it uses matrix multiplications for feature projections (512×512 operations implemented through tensor cores), may employ sliding windows for efficient attention over long sequences (using specialized memory access patterns for local regions), and requires dynamic computation for attention weights (computing 512×512 attention patterns at runtime). The way these primitives interact creates specific demands on system design—from memory hierarchy organization to computation scheduling.</p>
<p>The building blocks we’ve discussed help explain why certain hardware features exist (like tensor cores for matrix multiplication) and why software frameworks organize computations in particular ways (like batching similar operations together). As we move from computational primitives to consider memory access and data movement patterns, it’s important to recognize how these fundamental operations shape the demands placed on memory systems and data transfer mechanisms. The way computational primitives are implemented and combined has direct implications for how data needs to be stored, accessed, and moved within the system.</p>
</section>
<section id="memory-access-primitives" class="level3" data-number="4.7.2">
<h3 data-number="4.7.2" class="anchored" data-anchor-id="memory-access-primitives"><span class="header-section-number">4.7.2</span> Memory Access Primitives</h3>
<p>The efficiency of deep learning systems heavily depends on how they access and manage memory. In fact, memory access often becomes the primary bottleneck in modern ML systems—while a matrix multiplication unit might be capable of performing thousands of operations per cycle, it will sit idle if data isn’t available at the right time. For example, accessing data from DRAM typically takes hundreds of cycles, while on-chip computation takes only a few cycles.</p>
<p>Three fundamental memory access patterns dominate in deep learning architectures: sequential access, strided access, and random access. Each pattern creates different demands on the memory system and offers different opportunities for optimization.</p>
<p>Sequential access represents the simplest and most efficient pattern. Consider an MLP performing matrix multiplication with a batch of MNIST images: it needs to access both the 784×100 weight matrix and the input vectors sequentially. This pattern maps well to modern memory systems—DRAM can operate in burst mode for sequential reads (achieving up to 400 GB/s in modern GPUs), and hardware prefetchers can effectively predict and fetch upcoming data. Software frameworks optimize for this by ensuring data is laid out contiguously in memory and aligning data to cache line boundaries.</p>
<p>Strided access appears prominently in CNNs, where each output position needs to access a window of input values at regular intervals. For a CNN processing MNIST images with 3×3 filters, each output position requires accessing 9 input values with a stride matching the input width. While less efficient than sequential access, hardware supports this through pattern-aware caching strategies and specialized memory controllers. Software frameworks often transform these strided patterns into sequential access through data layout reorganization—the im2col transformation in deep learning frameworks converts convolution’s strided access into efficient matrix multiplications.</p>
<p>Random access poses the greatest challenge for system efficiency. In a Transformer processing a sequence of 512 tokens, each attention operation potentially needs to access any position in the sequence, creating unpredictable memory access patterns. Random access can severely impact performance through cache misses (potentially causing 100+ cycle stalls per access) and unpredictable memory latencies. Systems address this through large cache hierarchies (modern GPUs have several MB of L2 cache) and sophisticated prefetching strategies, while software frameworks employ techniques like attention pattern pruning to reduce random access requirements.</p>
<p>These different memory access patterns contribute significantly to the overall memory requirements of each architecture. To illustrate this, let’s compare the memory complexity of MLPs, CNNs, RNNs, and Transformers.</p>
<div id="tbl-arch-complexity" class="hover striped quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-arch-complexity-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;4.3: DNN architecture complexity.
</figcaption>
<div aria-describedby="tbl-arch-complexity-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="table-hover table-striped caption-top table">
<colgroup>
<col style="width: 14%">
<col style="width: 17%">
<col style="width: 19%">
<col style="width: 29%">
<col style="width: 17%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Architecture</th>
<th style="text-align: left;">Input Dependency</th>
<th style="text-align: left;">Parameter Storage</th>
<th style="text-align: left;">Activation Storage</th>
<th style="text-align: left;">Scaling Behavior</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">MLP</td>
<td style="text-align: left;">Linear</td>
<td style="text-align: left;">O(N × W)</td>
<td style="text-align: left;">O(B × W)</td>
<td style="text-align: left;">Predictable</td>
</tr>
<tr class="even">
<td style="text-align: left;">CNN</td>
<td style="text-align: left;">Constant</td>
<td style="text-align: left;">O(K × C)</td>
<td style="text-align: left;">O(B × H<span class="math inline">\(_{img}\)</span> × W<span class="math inline">\(_{img}\)</span>)</td>
<td style="text-align: left;">Efficient</td>
</tr>
<tr class="odd">
<td style="text-align: left;">RNN</td>
<td style="text-align: left;">Linear</td>
<td style="text-align: left;">O(h<span class="math inline">\(^{2}\)</span>)</td>
<td style="text-align: left;">O(B × T × h)</td>
<td style="text-align: left;">Challenging</td>
</tr>
<tr class="even">
<td style="text-align: left;">Transformer</td>
<td style="text-align: left;">Quadratic</td>
<td style="text-align: left;">O(N × d)</td>
<td style="text-align: left;">O(B × N<span class="math inline">\(^{2}\)</span>)</td>
<td style="text-align: left;">Problematic</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p>Where:</p>
<ul>
<li>N: Input or sequence size</li>
<li>W: Layer width</li>
<li>B: Batch size</li>
<li>K: Kernel size</li>
<li>C: Number of channels</li>
<li>H_img: Height of input feature map (CNN)</li>
<li>W_img: Width of input feature map (CNN)</li>
<li>h: Hidden state size (RNN)</li>
<li>T: Sequence length</li>
<li>d: Model dimensionality</li>
</ul>
<p><a href="#tbl-arch-complexity" class="quarto-xref">Table&nbsp;<span>4.3</span></a> reveals how memory requirements scale with different architectural choices. The quadratic scaling of activation storage in Transformers, for instance, highlights the need for large memory capacities and efficient memory management in systems designed for Transformer-based workloads. In contrast, CNNs exhibit more favorable memory scaling due to their parameter sharing and localized processing. These memory complexity considerations are crucial when making system-level design decisions, such as choosing memory hierarchy configurations and developing memory optimization strategies.</p>
<p>The impact of these patterns becomes clearer when we consider data reuse opportunities. In CNNs, each input pixel participates in multiple convolution windows (typically 9 times for a 3×3 filter), making effective data reuse fundamental for performance. Modern GPUs provide multi-level cache hierarchies (L1, L2, shared memory) to capture this reuse, while software techniques like loop tiling ensure data remains in cache once loaded.</p>
<p>Working set size—the amount of data needed simultaneously for computation—varies dramatically across architectures. An MLP layer processing MNIST images might need only a few hundred KB (weights plus activations), while a Transformer processing long sequences can require several MB just for storing attention patterns. These differences directly influence hardware design choices, like the balance between compute units and on-chip memory, and software optimizations like activation checkpointing or attention approximation techniques.</p>
<p>Having a good grasp of these memory access patterns is essential as architectures evolve. The shift from CNNs to Transformers, for instance, has driven the development of hardware with larger on-chip memories and more sophisticated caching strategies to handle increased working sets and more dynamic access patterns. Future architectures will likely continue to be shaped by their memory access characteristics as much as their computational requirements.</p>
</section>
<section id="data-movement-primitives" class="level3" data-number="4.7.3">
<h3 data-number="4.7.3" class="anchored" data-anchor-id="data-movement-primitives"><span class="header-section-number">4.7.3</span> Data Movement Primitives</h3>
<p>While computational and memory access patterns define what operations occur where, data movement primitives characterize how information flows through the system. These patterns are key because data movement often consumes more time and energy than computation itself—moving data from off-chip memory typically requires 100-1000x more energy than performing a floating-point operation.</p>
<p>Four fundamental data movement patterns are prevalent in deep learning architectures: broadcast, scatter, gather, and reduction. These patterns determine how data is distributed and collected across computational units.</p>
<p>Broadcast operations send the same data to multiple destinations simultaneously. In matrix multiplication with batch size 32, each weight must be broadcast to process different inputs in parallel. Modern hardware supports this through specialized interconnects—NVIDIA GPUs provide hardware multicast capabilities achieving up to 600GB/s broadcast bandwidth, while TPUs use dedicated broadcast buses. Software frameworks optimize broadcasts by restructuring computations (like matrix tiling) to maximize data reuse.</p>
<p>Scatter operations distribute different elements to different destinations. When parallelizing a 512×512 matrix multiplication across GPU cores, each core receives a subset of the computation. This parallelization is important for performance but challenging—memory conflicts and load imbalance can reduce efficiency by 50% or more. Hardware provides flexible interconnects (like NVIDIA’s NVLink offering 600GB/s bi-directional bandwidth), while software frameworks employ sophisticated work distribution algorithms to maintain high utilization.</p>
<p>Gather operations collect data from multiple sources. In Transformer attention with sequence length 512, each query must gather information from 512 different key-value pairs. These irregular access patterns are challenging—random gathering can be 10x slower than sequential access. Hardware supports this through high-bandwidth interconnects and large caches, while software frameworks employ techniques like attention pattern pruning to reduce gathering overhead.</p>
<p>Reduction operations combine multiple values into a single result through operations like summation. When computing attention scores in Transformers or layer outputs in MLPs, efficient reduction is essential. Hardware implements tree-structured reduction networks (reducing latency from O(n) to O(log n)), while software frameworks use optimized parallel reduction algorithms that can achieve near-theoretical peak performance.</p>
<p>These patterns combine in sophisticated ways. A Transformer attention operation with sequence length 512 and batch size 32 involves:</p>
<ul>
<li>Broadcasting query vectors (512×64 elements)</li>
<li>Gathering relevant keys and values (512×512×64 elements)</li>
<li>Reducing attention scores (512×512 elements per sequence)</li>
</ul>
<p>The evolution from CNNs to Transformers has increased reliance on gather and reduction operations, driving hardware innovations like more flexible interconnects and larger on-chip memories. As models grow (some now exceeding 100 billion parameters), efficient data movement becomes increasingly critical, leading to innovations like near-memory processing and sophisticated data flow optimizations.</p>
</section>
<section id="system-design-impact" class="level3" data-number="4.7.4">
<h3 data-number="4.7.4" class="anchored" data-anchor-id="system-design-impact"><span class="header-section-number">4.7.4</span> System Design Impact</h3>
<p>The computational, memory access, and data movement primitives we’ve explored form the foundational requirements that shape the design of systems for deep learning. The way these primitives influence hardware design, create common bottlenecks, and drive trade-offs is important for developing efficient and effective deep learning systems.</p>
<p>One of the most significant impacts of these primitives on system design is the push towards specialized hardware. The prevalence of matrix multiplications and convolutions in deep learning has led to the development of tensor processing units (TPUs) and tensor cores in GPUs, which are specifically designed to perform these operations efficiently. These specialized units can perform many multiply-accumulate operations in parallel, dramatically accelerating the core computations of neural networks.</p>
<p>Memory systems have also been profoundly influenced by the demands of deep learning primitives. The need to support both sequential and random access patterns efficiently has driven the development of sophisticated memory hierarchies. High-bandwidth memory (HBM) has become common in AI accelerators to support the massive data movement requirements, especially for operations like attention mechanisms in Transformers. On-chip memory hierarchies have grown in complexity, with multiple levels of caching and scratchpad memories to support the diverse working set sizes of different neural network layers.</p>
<p>The data movement primitives have particularly influenced the design of interconnects and on-chip networks. The need to support efficient broadcasts, gathers, and reductions has led to the development of more flexible and higher-bandwidth interconnects. Some AI chips now feature specialized networks-on-chip designed to accelerate common data movement patterns in neural networks.</p>
<p><a href="#tbl-sys-design-implications" class="quarto-xref">Table&nbsp;<span>4.4</span></a> summarizes the system implications of these primitives:</p>
<div id="tbl-sys-design-implications" class="hover striped quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-sys-design-implications-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;4.4: System implications of primitives.
</figcaption>
<div aria-describedby="tbl-sys-design-implications-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="table-hover table-striped caption-top table">
<colgroup>
<col style="width: 21%">
<col style="width: 25%">
<col style="width: 24%">
<col style="width: 26%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Primitive</th>
<th style="text-align: left;">Hardware Impact</th>
<th style="text-align: left;">Software Optimization</th>
<th style="text-align: left;">Key Challenges</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Matrix Multiplication</td>
<td style="text-align: left;">Tensor Cores</td>
<td style="text-align: left;">Batching, GEMM libraries</td>
<td style="text-align: left;">Parallelization, precision</td>
</tr>
<tr class="even">
<td style="text-align: left;">Sliding Window</td>
<td style="text-align: left;">Specialized datapaths</td>
<td style="text-align: left;">Data layout optimization</td>
<td style="text-align: left;">Stride handling</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Dynamic Computation</td>
<td style="text-align: left;">Flexible routing</td>
<td style="text-align: left;">Dynamic graph execution</td>
<td style="text-align: left;">Load balancing</td>
</tr>
<tr class="even">
<td style="text-align: left;">Sequential Access</td>
<td style="text-align: left;">Burst mode DRAM</td>
<td style="text-align: left;">Contiguous allocation</td>
<td style="text-align: left;"></td>
</tr>
<tr class="odd">
<td style="text-align: left;">Random Access</td>
<td style="text-align: left;">Large caches</td>
<td style="text-align: left;">Memory-aware scheduling</td>
<td style="text-align: left;">Cache misses</td>
</tr>
<tr class="even">
<td style="text-align: left;">Broadcast</td>
<td style="text-align: left;">Specialized interconnects</td>
<td style="text-align: left;">Operation fusion</td>
<td style="text-align: left;">Bandwidth</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Gather/Scatter</td>
<td style="text-align: left;">High-bandwidth memory</td>
<td style="text-align: left;">Work distribution</td>
<td style="text-align: left;">Load balancing</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p>Despite these advancements, several common bottlenecks persist in deep learning systems. Memory bandwidth often remains a key limitation, particularly for models with large working sets or those that require frequent random access. The energy cost of data movement, especially between off-chip memory and processing units, continues to be a significant concern. For large-scale models, the communication overhead in distributed training can become a bottleneck, limiting scaling efficiency.</p>
<p>System designers must navigate complex trade-offs in supporting different primitives, each with unique characteristics that influence system design and performance. For example, optimizing for the dense matrix operations common in MLPs and CNNs might come at the cost of flexibility needed for the more dynamic computations in attention mechanisms. Supporting large working sets for Transformers might require sacrificing energy efficiency.</p>
<p>Balancing these trade-offs requires careful consideration of the target workloads and deployment scenarios. Having a good grip on the nature of each primitive guides the development of both hardware and software optimizations in deep learning systems, allowing designers to make informed decisions about system architecture and resource allocation.</p>
</section>
</section>
<section id="conclusion" class="level2" data-number="4.8">
<h2 data-number="4.8" class="anchored" data-anchor-id="conclusion"><span class="header-section-number">4.8</span> Conclusion</h2>
<p>Deep learning architectures, despite their diversity, exhibit common patterns in their algorithmic structures that significantly influence computational requirements and system design. In this chapter, we explored the intricate relationship between high-level architectural concepts and their practical implementation in computing systems.</p>
<p>From the straightforward dense connections of MLPs to the complex, dynamic patterns of Transformers, each architecture builds upon a set of fundamental building blocks. These core computational primitives—such as matrix multiplication, sliding windows, and dynamic computation—recur across various architectures, forming a universal language of deep learning computation.</p>
<p>The identification of these shared elements provides a valuable framework for understanding and designing deep learning systems. Each primitive brings its own set of requirements in terms of memory access patterns and data movement, which in turn shape both hardware and software design decisions. This relationship between algorithmic intent and system implementation is crucial for optimizing performance and efficiency.</p>
<p>As the field of deep learning continues to evolve, the ability to efficiently support and optimize these fundamental building blocks will be key to the development of more powerful and scalable systems. Future advancements in deep learning are likely to stem not only from novel architectural designs but also from innovative approaches to implementing and optimizing these essential computational patterns.</p>
<p>In conclusion, understanding the mapping between neural architectures and their computational requirements is vital for pushing the boundaries of what’s possible in artificial intelligence. As we look to the future, the interplay between algorithmic innovation and systems optimization will continue to drive progress in this rapidly advancing field.</p>



</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
          // target, if specified
          link.setAttribute("target", "_blank");
          if (link.getAttribute("rel") === null) {
            link.setAttribute("rel", "noopener");
          }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
});
</script>
<script src="https://giscus.app/client.js" data-repo="harvard-edge/cs249r_book" data-repo-id="R_kgDOKQSOaw" data-category="General" data-category-id="DIC_kwDOKQSOa84CZ8Ry" data-mapping="title" data-reactions-enabled="1" data-emit-metadata="0" data-input-position="top" data-theme="light" data-lang="en" crossorigin="anonymous" async="">
</script>
<input type="hidden" id="giscus-base-theme" value="light">
<input type="hidden" id="giscus-alt-theme" value="dark">
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../../../contents/core/dl_primer/dl_primer.html" class="pagination-link" aria-label="DL Primer">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">DL Primer</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../../../contents/core/workflow/workflow.html" class="pagination-link" aria-label="AI Workflow">
        <span class="nav-page-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">AI Workflow</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>Written, edited and curated by Prof.&nbsp;Vijay Janapa Reddi (Harvard University)</p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    <div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/harvard-edge/cs249r_book/edit/dev/contents/core/dnn_architectures/dnn_architectures.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li><li><a href="https://github.com/harvard-edge/cs249r_book/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li><li><a href="https://github.com/harvard-edge/cs249r_book/blob/dev/contents/core/dnn_architectures/dnn_architectures.qmd" class="toc-action"><i class="bi empty"></i>View source</a></li></ul></div></div>
    <div class="nav-footer-right">
<p>This book was built with <a href="https://quarto.org/">Quarto</a>.</p>
</div>
  </div>
</footer>
<script>var lightboxQuarto = GLightbox({"closeEffect":"zoom","descPosition":"bottom","loop":false,"openEffect":"zoom","selector":".lightbox"});
(function() {
  let previousOnload = window.onload;
  window.onload = () => {
    if (previousOnload) {
      previousOnload();
    }
    lightboxQuarto.on('slide_before_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      const href = trigger.getAttribute('href');
      if (href !== null) {
        const imgEl = window.document.querySelector(`a[href="${href}"] img`);
        if (imgEl !== null) {
          const srcAttr = imgEl.getAttribute("src");
          if (srcAttr && srcAttr.startsWith("data:")) {
            slideConfig.href = srcAttr;
          }
        }
      } 
    });
  
    lightboxQuarto.on('slide_after_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(slideNode);
      }
    });
  
  };
  
})();
          </script>




</body></html>