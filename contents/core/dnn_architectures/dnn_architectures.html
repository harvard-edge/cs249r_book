<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.31">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>ML Systems Textbook</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../">
<link href="../../../contents/core/workflow/workflow.html" rel="next">
<link href="../../../contents/core/dl_primer/dl_primer.html" rel="prev">
<link href="../../../assets/images/icons/favicon.png" rel="icon" type="image/png">
<script src="../../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-92151ed919028c7172e396588fd5eb2d.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-dark-92151ed919028c7172e396588fd5eb2d.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-92151ed919028c7172e396588fd5eb2d.css" rel="stylesheet" class="quarto-color-scheme-extra" id="quarto-text-highlighting-styles">
<script src="../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../site_libs/bootstrap/bootstrap-05e89e679d74be0ec03b4dfad47f0489.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../../site_libs/bootstrap/bootstrap-dark-b37cd2072f26f54614d6b324860e7473.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="light">
<link href="../../../site_libs/bootstrap/bootstrap-05e89e679d74be0ec03b4dfad47f0489.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme-extra" id="quarto-bootstrap" data-mode="light">
<link href="../../../site_libs/quarto-contrib/foldbox/foldbox.css" rel="stylesheet">
<script src="../../../site_libs/quarto-contrib/glightbox/glightbox.min.js"></script>
<link href="../../../site_libs/quarto-contrib/glightbox/glightbox.min.css" rel="stylesheet">
<link href="../../../site_libs/quarto-contrib/glightbox/lightbox.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "/"
  ],
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-M21L0CBCVN"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-M21L0CBCVN', { 'anonymize_ip': true});
</script>
<script type="application/json" class="js-hypothesis-config">
{
  "theme": "clean",
  "openSidebar": false
}
</script>
<script async="" src="https://hypothes.is/embed.js"></script>
<script>
  window.document.addEventListener("DOMContentLoaded", function (_event) {
    document.body.classList.add('hypothesis-enabled');
  });
</script>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="">
<link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&amp;family=JetBrains+Mono:wght@400;500&amp;display=swap" rel="stylesheet">
<link rel="manifest" href="../../../site.webmanifest">
<link rel="apple-touch-icon" href="../../../assets/images/icons/favicon.png">
<meta name="theme-color" content="#A51C30">

<script type="module" src="../../../tools/scripts/socratiQ/bundle.js" defer=""></script>
<script src="../../../assets/scripts/sidebar-auto-collapse.js" defer=""></script>
<script src="../../../assets/scripts/version-link.js" defer=""></script>
<script src="../../../assets/scripts/subscribe-modal.js" defer=""></script>
<style>
.callout-code {
  --color1: #F2F4F8;
  --color2: #D1D7E0;
}
.callout-resource-exercises {
  --color1: #E0F2F1;
  --color2: #20B2AA;
}
.callout-quiz-answer {
  --color1: #E8F2EA;
  --color2: #4a7c59;
}
.callout-example {
  --color1: #F0F8F6;
  --color2: #148F77;
}
.callout-quiz-question {
  --color1: #F0F0F8;
  --color2: #5B4B8A;
}
.callout-chapter-connection {
  --color1: #FDF2F7;
  --color2: #A51C30;
}
.callout-definition {
  --color1: #F0F4F8;
  --color2: #1B4F72;
}
.callout-resource-slides {
  --color1: #E0F2F1;
  --color2: #20B2AA;
}
.callout-colab {
  --color1: #FFF5E6;
  --color2: #FF6B35;
}
.callout-resource-videos {
  --color1: #E0F2F1;
  --color2: #20B2AA;
}
</style>
<style>
details.callout-code > summary::before {
  background-image: url("../../../assets/images/icons/callouts/icon_callout_code.png");
}
details.callout-resource-exercises > summary::before {
  background-image: url("../../../assets/images/icons/callouts/icon_callout_resource_exercises.png");
}
details.callout-quiz-answer > summary::before {
  background-image: url("../../../assets/images/icons/callouts/icon_callout_quiz_answer.png");
}
details.callout-example > summary::before {
  background-image: url("../../../assets/images/icons/callouts/icon_callout_example.png");
}
details.callout-quiz-question > summary::before {
  background-image: url("../../../assets/images/icons/callouts/icon_callout_quiz_question.png");
}
details.callout-chapter-connection > summary::before {
  background-image: url("../../../assets/images/icons/callouts/icon_callout_chapter_connection.png");
}
details.callout-definition > summary::before {
  background-image: url("../../../assets/images/icons/callouts/icon_callout_definition.png");
}
details.callout-resource-slides > summary::before {
  background-image: url("../../../assets/images/icons/callouts/icon_callout_resource_slides.png");
}
details.callout-colab > summary::before {
  background-image: url("../../../assets/images/icons/callouts/icon_callout_colab.png");
}
details.callout-resource-videos > summary::before {
  background-image: url("../../../assets/images/icons/callouts/icon_callout_resource_videos.png");
}
</style>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<meta property="og:title" content="ML Systems Textbook">
<meta property="og:image" content="https://mlsysbook.ai/book/contents/core/dnn_architectures/assets/images/covers/cover-hardcover-book.png">
<meta property="og:site_name" content="Machine Learning Systems">
<meta property="og:locale" content="en_US">
<meta name="twitter:title" content="ML Systems Textbook">
<meta name="twitter:image" content="https://mlsysbook.ai/book/contents/core/dnn_architectures/assets/images/covers/cover-hardcover-book.png">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="nav-sidebar floating nav-fixed slimcontent quarto-light"><script id="quarto-html-before-body" type="application/javascript">
    const toggleBodyColorMode = (bsSheetEl) => {
      const mode = bsSheetEl.getAttribute("data-mode");
      const bodyEl = window.document.querySelector("body");
      if (mode === "dark") {
        bodyEl.classList.add("quarto-dark");
        bodyEl.classList.remove("quarto-light");
      } else {
        bodyEl.classList.add("quarto-light");
        bodyEl.classList.remove("quarto-dark");
      }
    }
    const toggleBodyColorPrimary = () => {
      const bsSheetEl = window.document.querySelector("link#quarto-bootstrap:not([rel=disabled-stylesheet])");
      if (bsSheetEl) {
        toggleBodyColorMode(bsSheetEl);
      }
    }
    const setColorSchemeToggle = (alternate) => {
      const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
      for (let i=0; i < toggles.length; i++) {
        const toggle = toggles[i];
        if (toggle) {
          if (alternate) {
            toggle.classList.add("alternate");
          } else {
            toggle.classList.remove("alternate");
          }
        }
      }
    };
    const toggleColorMode = (alternate) => {
      // Switch the stylesheets
      const primaryStylesheets = window.document.querySelectorAll('link.quarto-color-scheme:not(.quarto-color-alternate)');
      const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
      manageTransitions('#quarto-margin-sidebar .nav-link', false);
      if (alternate) {
        // note: dark is layered on light, we don't disable primary!
        enableStylesheet(alternateStylesheets);
        for (const sheetNode of alternateStylesheets) {
          if (sheetNode.id === "quarto-bootstrap") {
            toggleBodyColorMode(sheetNode);
          }
        }
      } else {
        disableStylesheet(alternateStylesheets);
        enableStylesheet(primaryStylesheets)
        toggleBodyColorPrimary();
      }
      manageTransitions('#quarto-margin-sidebar .nav-link', true);
      // Switch the toggles
      setColorSchemeToggle(alternate)
      // Hack to workaround the fact that safari doesn't
      // properly recolor the scrollbar when toggling (#1455)
      if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
        manageTransitions("body", false);
        window.scrollTo(0, 1);
        setTimeout(() => {
          window.scrollTo(0, 0);
          manageTransitions("body", true);
        }, 40);
      }
    }
    const disableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        stylesheet.rel = 'disabled-stylesheet';
      }
    }
    const enableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        if(stylesheet.rel !== 'stylesheet') { // for Chrome, which will still FOUC without this check
          stylesheet.rel = 'stylesheet';
        }
      }
    }
    const manageTransitions = (selector, allowTransitions) => {
      const els = window.document.querySelectorAll(selector);
      for (let i=0; i < els.length; i++) {
        const el = els[i];
        if (allowTransitions) {
          el.classList.remove('notransition');
        } else {
          el.classList.add('notransition');
        }
      }
    }
    const isFileUrl = () => {
      return window.location.protocol === 'file:';
    }
    const hasAlternateSentinel = () => {
      let styleSentinel = getColorSchemeSentinel();
      if (styleSentinel !== null) {
        return styleSentinel === "alternate";
      } else {
        return false;
      }
    }
    const setStyleSentinel = (alternate) => {
      const value = alternate ? "alternate" : "default";
      if (!isFileUrl()) {
        window.localStorage.setItem("quarto-color-scheme", value);
      } else {
        localAlternateSentinel = value;
      }
    }
    const getColorSchemeSentinel = () => {
      if (!isFileUrl()) {
        const storageValue = window.localStorage.getItem("quarto-color-scheme");
        return storageValue != null ? storageValue : localAlternateSentinel;
      } else {
        return localAlternateSentinel;
      }
    }
    const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
      const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
      const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
      let newTheme = '';
      if(authorPrefersDark) {
        newTheme = isAlternate ? baseTheme : alternateTheme;
      } else {
        newTheme = isAlternate ? alternateTheme : baseTheme;
      }
      const changeGiscusTheme = () => {
        // From: https://github.com/giscus/giscus/issues/336
        const sendMessage = (message) => {
          const iframe = document.querySelector('iframe.giscus-frame');
          if (!iframe) return;
          iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
        }
        sendMessage({
          setConfig: {
            theme: newTheme
          }
        });
      }
      const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
      if (isGiscussLoaded) {
        changeGiscusTheme();
      }
    };
    const authorPrefersDark = false;
    const queryPrefersDark = window.matchMedia('(prefers-color-scheme: dark)');
    const darkModeDefault = queryPrefersDark.matches;
      document.querySelector('link#quarto-text-highlighting-styles.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
      document.querySelector('link#quarto-bootstrap.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
    let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
    // Dark / light mode switch
    window.quartoToggleColorScheme = () => {
      // Read the current dark / light value
      let toAlternate = !hasAlternateSentinel();
      toggleColorMode(toAlternate);
      setStyleSentinel(toAlternate);
      toggleGiscusIfUsed(toAlternate, darkModeDefault);
      window.dispatchEvent(new Event('resize'));
    };
    queryPrefersDark.addEventListener("change", e => {
      if(window.localStorage.getItem("quarto-color-scheme") !== null)
        return;
      const alternate = e.matches
      toggleColorMode(alternate);
      localAlternateSentinel = e.matches ? 'alternate' : 'default'; // this is used alongside local storage!
      toggleGiscusIfUsed(alternate, darkModeDefault);
    });
    // Switch to dark mode if need be
    if (hasAlternateSentinel()) {
      toggleColorMode(true);
    } else {
      toggleColorMode(false);
    }
  </script>

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="../../../index.html" class="navbar-brand navbar-brand-logo">
    <img src="../../../assets/images/icons/favicon.png" alt="" class="navbar-logo">
    </a>
    <a class="navbar-brand" href="../../../index.html">
    <span class="navbar-title">Machine Learning Systems</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-textbook" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Textbook</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-textbook">    
        <li>
    <a class="dropdown-item" href="../../../../book/"><i class="bi bi-book-half" role="img">
</i> 
 <span class="dropdown-text">Textbook</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../../tinytorch/"><i class="bi bi-fire" role="img">
</i> 
 <span class="dropdown-text">TinyTorch</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../../kits/"><i class="bi bi-cpu" role="img">
</i> 
 <span class="dropdown-text">Hardware Kits</span></a>
  </li>  
        <li><hr class="dropdown-divider"></li>
        <li>
    <a class="dropdown-item" href="../../../../labs/"><i class="bi bi-lightbulb" role="img">
</i> 
 <span class="dropdown-text">Labs (Coming 2026)</span></a>
  </li>  
    </ul>
  </li>
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-downloads" role="link" data-bs-toggle="dropdown" aria-expanded="false">
      <i class="bi bi-download" role="img">
</i> 
 <span class="menu-text">Downloads</span>
    </a>
    <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="nav-menu-downloads">    
        <li>
    <a class="dropdown-item" href="../../../assets/downloads/Machine-Learning-Systems.pdf" target="_blank"><i class="bi bi-file-pdf" role="img">
</i> 
 <span class="dropdown-text">Textbook PDF</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../assets/downloads/Machine-Learning-Systems.epub" target="_blank"><i class="bi bi-journal-text" role="img">
</i> 
 <span class="dropdown-text">Textbook EPUB</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item">
    <a class="nav-link" href="https://github.com/harvard-edge/cs249r_book#support-this-work" target="_blank"> <i class="bi bi-star" role="img">
</i> 
<span class="menu-text">Star</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://opencollective.com/mlsysbook" target="_blank"> <i class="bi bi-heart" role="img">
</i> 
<span class="menu-text">Support</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../#subscribe"> <i class="bi bi-envelope" role="img">
</i> 
<span class="menu-text">Subscribe</span></a>
  </li>  
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-github" role="link" data-bs-toggle="dropdown" aria-expanded="false">
      <i class="bi bi-github" role="img">
</i> 
 <span class="menu-text">GitHub</span>
    </a>
    <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="nav-menu-github">    
        <li>
    <a class="dropdown-item" href="https://github.com/harvard-edge/cs249r_book" target="_blank"><i class="bi bi-pencil" role="img">
</i> 
 <span class="dropdown-text">Edit this page</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://github.com/harvard-edge/cs249r_book/issues/new" target="_blank"><i class="bi bi-bug" role="img">
</i> 
 <span class="dropdown-text">Report an issue</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://github.com/harvard-edge/cs249r_book/discussions" target="_blank"><i class="bi bi-chat" role="img">
</i> 
 <span class="dropdown-text">Discussions</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://github.com/harvard-edge/cs249r_book" target="_blank"><i class="bi bi-code" role="img">
</i> 
 <span class="dropdown-text">View source</span></a>
  </li>  
    </ul>
  </li>
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../../contents/core/introduction/introduction.html">Systems Foundations</a></li><li class="breadcrumb-item"><a href="../../../contents/core/dnn_architectures/dnn_architectures.html">DNN Architectures</a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
    </div>
  </nav>
  <div id="quarto-announcement" data-announcement-id="b43e2aeb169c88acb08fe42121c141fd" class="alert alert-primary hidden"><i class="bi bi-megaphone quarto-announcement-icon"></i><div class="quarto-announcement-content">
<p>ðŸŽ‰ <strong>Happy New Year!</strong> New navbar with dropdown menus. Try them out!<br> ðŸ”¥ <strong>TinyTorch:</strong> Build your own ML framework from scratch. <a href="https://mlsysbook.ai/tinytorch">Start â†’</a><br> ðŸ“¦ <strong>Hardware Kits:</strong> Arduino, Seeed &amp; Raspberry Pi labs. <a href="https://mlsysbook.ai/kits">Explore â†’</a><br> ðŸ“¬ <strong>Newsletter:</strong> ML Systems insights &amp; updates. <a href="#subscribe">Subscribe â†’</a></p>
</div><i class="bi bi-x-lg quarto-announcement-action"></i></div>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Homepage</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/frontmatter/foreword.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Foreword</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/frontmatter/about/about.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">About the Book</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/frontmatter/changelog/changelog.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Book Changelog</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/frontmatter/acknowledgements/acknowledgements.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Acknowledgements</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/frontmatter/socratiq/socratiq.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">SocratiQ AI</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Systems Foundations</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/introduction/introduction.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduction</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/ml_systems/ml_systems.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">ML Systems</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/dl_primer/dl_primer.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">DL Primer</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/dnn_architectures/dnn_architectures.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">DNN Architectures</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">Design Principles</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/workflow/workflow.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">AI Workflow</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/data_engineering/data_engineering.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Data Engineering</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/frameworks/frameworks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">AI Frameworks</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/training/training.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">AI Training</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true">
 <span class="menu-text">Performance Engineering</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/efficient_ai/efficient_ai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Efficient AI</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/optimizations/optimizations.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Model Optimizations</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/hw_acceleration/hw_acceleration.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">AI Acceleration</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/benchmarking/benchmarking.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Benchmarking AI</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true">
 <span class="menu-text">Robust Deployment</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/ops/ops.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">ML Operations</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/ondevice_learning/ondevice_learning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">On-Device Learning</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/privacy_security/privacy_security.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Security &amp; Privacy</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/robust_ai/robust_ai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Robust AI</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true">
 <span class="menu-text">Trustworthy Systems</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/responsible_ai/responsible_ai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Responsible AI</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/sustainable_ai/sustainable_ai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Sustainable AI</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/ai_for_good/ai_for_good.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">AI for Good</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true">
 <span class="menu-text">Frontiers of ML Systems</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/frontiers/frontiers.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">AGI Systems</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/conclusion/conclusion.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Conclusion</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="true">
 <span class="menu-text">Glossary</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-7" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/backmatter/glossary/glossary.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Complete Glossary</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active" data-toc-expanded="99">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#sec-dnn-architectures" id="toc-sec-dnn-architectures" class="nav-link active" data-scroll-target="#sec-dnn-architectures">DNN Architectures</a>
  <ul>
  <li><a href="#purpose" id="toc-purpose" class="nav-link" data-scroll-target="#purpose">Purpose</a></li>
  <li><a href="#sec-dnn-architectures-architectural-principles-engineering-tradeoffs-89de" id="toc-sec-dnn-architectures-architectural-principles-engineering-tradeoffs-89de" class="nav-link" data-scroll-target="#sec-dnn-architectures-architectural-principles-engineering-tradeoffs-89de">Architectural Principles and Engineering Trade-offs</a></li>
  <li><a href="#sec-dnn-architectures-multilayer-perceptrons-dense-pattern-processing-259f" id="toc-sec-dnn-architectures-multilayer-perceptrons-dense-pattern-processing-259f" class="nav-link" data-scroll-target="#sec-dnn-architectures-multilayer-perceptrons-dense-pattern-processing-259f">Multi-Layer Perceptrons: Dense Pattern Processing</a>
  <ul class="collapse">
  <li><a href="#sec-dnn-architectures-pattern-processing-needs-c45a" id="toc-sec-dnn-architectures-pattern-processing-needs-c45a" class="nav-link" data-scroll-target="#sec-dnn-architectures-pattern-processing-needs-c45a">Pattern Processing Needs</a></li>
  <li><a href="#sec-dnn-architectures-algorithmic-structure-c012" id="toc-sec-dnn-architectures-algorithmic-structure-c012" class="nav-link" data-scroll-target="#sec-dnn-architectures-algorithmic-structure-c012">Algorithmic Structure</a>
  <ul class="collapse">
  <li><a href="#sec-dnn-architectures-architectural-characteristics-47b4" id="toc-sec-dnn-architectures-architectural-characteristics-47b4" class="nav-link" data-scroll-target="#sec-dnn-architectures-architectural-characteristics-47b4">Architectural Characteristics</a></li>
  </ul></li>
  <li><a href="#sec-dnn-architectures-computational-mapping-fe7e" id="toc-sec-dnn-architectures-computational-mapping-fe7e" class="nav-link" data-scroll-target="#sec-dnn-architectures-computational-mapping-fe7e">Computational Mapping</a></li>
  <li><a href="#sec-dnn-architectures-system-implications-7a8f" id="toc-sec-dnn-architectures-system-implications-7a8f" class="nav-link" data-scroll-target="#sec-dnn-architectures-system-implications-7a8f">System Implications</a>
  <ul class="collapse">
  <li><a href="#sec-dnn-architectures-memory-requirements-4900" id="toc-sec-dnn-architectures-memory-requirements-4900" class="nav-link" data-scroll-target="#sec-dnn-architectures-memory-requirements-4900">Memory Requirements</a></li>
  <li><a href="#sec-dnn-architectures-computation-needs-9cb4" id="toc-sec-dnn-architectures-computation-needs-9cb4" class="nav-link" data-scroll-target="#sec-dnn-architectures-computation-needs-9cb4">Computation Needs</a></li>
  <li><a href="#sec-dnn-architectures-data-movement-fc16" id="toc-sec-dnn-architectures-data-movement-fc16" class="nav-link" data-scroll-target="#sec-dnn-architectures-data-movement-fc16">Data Movement</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#sec-dnn-architectures-cnns-spatial-pattern-processing-f8ff" id="toc-sec-dnn-architectures-cnns-spatial-pattern-processing-f8ff" class="nav-link" data-scroll-target="#sec-dnn-architectures-cnns-spatial-pattern-processing-f8ff">CNNs: Spatial Pattern Processing</a>
  <ul class="collapse">
  <li><a href="#sec-dnn-architectures-pattern-processing-needs-a4ce" id="toc-sec-dnn-architectures-pattern-processing-needs-a4ce" class="nav-link" data-scroll-target="#sec-dnn-architectures-pattern-processing-needs-a4ce">Pattern Processing Needs</a></li>
  <li><a href="#sec-dnn-architectures-algorithmic-structure-a10c" id="toc-sec-dnn-architectures-algorithmic-structure-a10c" class="nav-link" data-scroll-target="#sec-dnn-architectures-algorithmic-structure-a10c">Algorithmic Structure</a>
  <ul class="collapse">
  <li><a href="#sec-dnn-architectures-architectural-characteristics-e309" id="toc-sec-dnn-architectures-architectural-characteristics-e309" class="nav-link" data-scroll-target="#sec-dnn-architectures-architectural-characteristics-e309">Architectural Characteristics</a></li>
  </ul></li>
  <li><a href="#sec-dnn-architectures-computational-mapping-fea5" id="toc-sec-dnn-architectures-computational-mapping-fea5" class="nav-link" data-scroll-target="#sec-dnn-architectures-computational-mapping-fea5">Computational Mapping</a></li>
  <li><a href="#sec-dnn-architectures-system-implications-f25d" id="toc-sec-dnn-architectures-system-implications-f25d" class="nav-link" data-scroll-target="#sec-dnn-architectures-system-implications-f25d">System Implications</a>
  <ul class="collapse">
  <li><a href="#sec-dnn-architectures-memory-requirements-7e2b" id="toc-sec-dnn-architectures-memory-requirements-7e2b" class="nav-link" data-scroll-target="#sec-dnn-architectures-memory-requirements-7e2b">Memory Requirements</a></li>
  <li><a href="#sec-dnn-architectures-computation-needs-22a5" id="toc-sec-dnn-architectures-computation-needs-22a5" class="nav-link" data-scroll-target="#sec-dnn-architectures-computation-needs-22a5">Computation Needs</a></li>
  <li><a href="#sec-dnn-architectures-data-movement-9a0e" id="toc-sec-dnn-architectures-data-movement-9a0e" class="nav-link" data-scroll-target="#sec-dnn-architectures-data-movement-9a0e">Data Movement</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#sec-dnn-architectures-rnns-sequential-pattern-processing-ea14" id="toc-sec-dnn-architectures-rnns-sequential-pattern-processing-ea14" class="nav-link" data-scroll-target="#sec-dnn-architectures-rnns-sequential-pattern-processing-ea14">RNNs: Sequential Pattern Processing</a>
  <ul class="collapse">
  <li><a href="#sec-dnn-architectures-pattern-processing-needs-c18e" id="toc-sec-dnn-architectures-pattern-processing-needs-c18e" class="nav-link" data-scroll-target="#sec-dnn-architectures-pattern-processing-needs-c18e">Pattern Processing Needs</a></li>
  <li><a href="#sec-dnn-architectures-algorithmic-structure-279b" id="toc-sec-dnn-architectures-algorithmic-structure-279b" class="nav-link" data-scroll-target="#sec-dnn-architectures-algorithmic-structure-279b">Algorithmic Structure</a>
  <ul class="collapse">
  <li><a href="#sec-dnn-architectures-efficiency-optimization-ce92" id="toc-sec-dnn-architectures-efficiency-optimization-ce92" class="nav-link" data-scroll-target="#sec-dnn-architectures-efficiency-optimization-ce92">Efficiency and Optimization</a></li>
  </ul></li>
  <li><a href="#sec-dnn-architectures-computational-mapping-0096" id="toc-sec-dnn-architectures-computational-mapping-0096" class="nav-link" data-scroll-target="#sec-dnn-architectures-computational-mapping-0096">Computational Mapping</a></li>
  <li><a href="#sec-dnn-architectures-system-implications-ecf5" id="toc-sec-dnn-architectures-system-implications-ecf5" class="nav-link" data-scroll-target="#sec-dnn-architectures-system-implications-ecf5">System Implications</a>
  <ul class="collapse">
  <li><a href="#sec-dnn-architectures-memory-requirements-eb37" id="toc-sec-dnn-architectures-memory-requirements-eb37" class="nav-link" data-scroll-target="#sec-dnn-architectures-memory-requirements-eb37">Memory Requirements</a></li>
  <li><a href="#sec-dnn-architectures-computation-needs-29be" id="toc-sec-dnn-architectures-computation-needs-29be" class="nav-link" data-scroll-target="#sec-dnn-architectures-computation-needs-29be">Computation Needs</a></li>
  <li><a href="#sec-dnn-architectures-data-movement-8591" id="toc-sec-dnn-architectures-data-movement-8591" class="nav-link" data-scroll-target="#sec-dnn-architectures-data-movement-8591">Data Movement</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#sec-dnn-architectures-attention-mechanisms-dynamic-pattern-processing-566d" id="toc-sec-dnn-architectures-attention-mechanisms-dynamic-pattern-processing-566d" class="nav-link" data-scroll-target="#sec-dnn-architectures-attention-mechanisms-dynamic-pattern-processing-566d">Attention Mechanisms: Dynamic Pattern Processing</a>
  <ul class="collapse">
  <li><a href="#sec-dnn-architectures-pattern-processing-needs-b5e0" id="toc-sec-dnn-architectures-pattern-processing-needs-b5e0" class="nav-link" data-scroll-target="#sec-dnn-architectures-pattern-processing-needs-b5e0">Pattern Processing Needs</a></li>
  <li><a href="#sec-dnn-architectures-basic-attention-mechanism-9500" id="toc-sec-dnn-architectures-basic-attention-mechanism-9500" class="nav-link" data-scroll-target="#sec-dnn-architectures-basic-attention-mechanism-9500">Basic Attention Mechanism</a>
  <ul class="collapse">
  <li><a href="#sec-dnn-architectures-algorithmic-structure-1af4" id="toc-sec-dnn-architectures-algorithmic-structure-1af4" class="nav-link" data-scroll-target="#sec-dnn-architectures-algorithmic-structure-1af4">Algorithmic Structure</a></li>
  <li><a href="#sec-dnn-architectures-computational-mapping-6c7e" id="toc-sec-dnn-architectures-computational-mapping-6c7e" class="nav-link" data-scroll-target="#sec-dnn-architectures-computational-mapping-6c7e">Computational Mapping</a></li>
  <li><a href="#sec-dnn-architectures-system-implications-b5aa" id="toc-sec-dnn-architectures-system-implications-b5aa" class="nav-link" data-scroll-target="#sec-dnn-architectures-system-implications-b5aa">System Implications</a></li>
  </ul></li>
  <li><a href="#sec-dnn-architectures-transformers-attentiononly-architecture-c4f0" id="toc-sec-dnn-architectures-transformers-attentiononly-architecture-c4f0" class="nav-link" data-scroll-target="#sec-dnn-architectures-transformers-attentiononly-architecture-c4f0">Transformers: Attention-Only Architecture</a>
  <ul class="collapse">
  <li><a href="#sec-dnn-architectures-algorithmic-structure-9d4b" id="toc-sec-dnn-architectures-algorithmic-structure-9d4b" class="nav-link" data-scroll-target="#sec-dnn-architectures-algorithmic-structure-9d4b">Algorithmic Structure</a></li>
  <li><a href="#sec-dnn-architectures-efficiency-optimization-94d7" id="toc-sec-dnn-architectures-efficiency-optimization-94d7" class="nav-link" data-scroll-target="#sec-dnn-architectures-efficiency-optimization-94d7">Efficiency and Optimization</a></li>
  <li><a href="#sec-dnn-architectures-computational-mapping-7fe9" id="toc-sec-dnn-architectures-computational-mapping-7fe9" class="nav-link" data-scroll-target="#sec-dnn-architectures-computational-mapping-7fe9">Computational Mapping</a></li>
  <li><a href="#sec-dnn-architectures-system-implications-76dd" id="toc-sec-dnn-architectures-system-implications-76dd" class="nav-link" data-scroll-target="#sec-dnn-architectures-system-implications-76dd">System Implications</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#sec-dnn-architectures-architectural-building-blocks-a575" id="toc-sec-dnn-architectures-architectural-building-blocks-a575" class="nav-link" data-scroll-target="#sec-dnn-architectures-architectural-building-blocks-a575">Architectural Building Blocks</a>
  <ul class="collapse">
  <li><a href="#sec-dnn-architectures-evolution-perceptron-multilayer-networks-0f3f" id="toc-sec-dnn-architectures-evolution-perceptron-multilayer-networks-0f3f" class="nav-link" data-scroll-target="#sec-dnn-architectures-evolution-perceptron-multilayer-networks-0f3f">Evolution from Perceptron to Multi-Layer Networks</a></li>
  <li><a href="#sec-dnn-architectures-evolution-dense-spatial-processing-1d3b" id="toc-sec-dnn-architectures-evolution-dense-spatial-processing-1d3b" class="nav-link" data-scroll-target="#sec-dnn-architectures-evolution-dense-spatial-processing-1d3b">Evolution from Dense to Spatial Processing</a></li>
  <li><a href="#sec-dnn-architectures-evolution-sequence-processing-8a78" id="toc-sec-dnn-architectures-evolution-sequence-processing-8a78" class="nav-link" data-scroll-target="#sec-dnn-architectures-evolution-sequence-processing-8a78">Evolution of Sequence Processing</a></li>
  <li><a href="#sec-dnn-architectures-modern-architectures-synthesis-unification-b5ef" id="toc-sec-dnn-architectures-modern-architectures-synthesis-unification-b5ef" class="nav-link" data-scroll-target="#sec-dnn-architectures-modern-architectures-synthesis-unification-b5ef">Modern Architectures: Synthesis and Unification</a></li>
  </ul></li>
  <li><a href="#sec-dnn-architectures-systemlevel-building-blocks-72f6" id="toc-sec-dnn-architectures-systemlevel-building-blocks-72f6" class="nav-link" data-scroll-target="#sec-dnn-architectures-systemlevel-building-blocks-72f6">System-Level Building Blocks</a>
  <ul class="collapse">
  <li><a href="#sec-dnn-architectures-core-computational-primitives-bd67" id="toc-sec-dnn-architectures-core-computational-primitives-bd67" class="nav-link" data-scroll-target="#sec-dnn-architectures-core-computational-primitives-bd67">Core Computational Primitives</a>
  <ul class="collapse">
  <li><a href="#sec-dnn-architectures-computational-building-blocks-c3c0" id="toc-sec-dnn-architectures-computational-building-blocks-c3c0" class="nav-link" data-scroll-target="#sec-dnn-architectures-computational-building-blocks-c3c0">Computational Building Blocks</a></li>
  </ul></li>
  <li><a href="#sec-dnn-architectures-memory-access-primitives-4e2e" id="toc-sec-dnn-architectures-memory-access-primitives-4e2e" class="nav-link" data-scroll-target="#sec-dnn-architectures-memory-access-primitives-4e2e">Memory Access Primitives</a></li>
  <li><a href="#sec-dnn-architectures-data-movement-primitives-101a" id="toc-sec-dnn-architectures-data-movement-primitives-101a" class="nav-link" data-scroll-target="#sec-dnn-architectures-data-movement-primitives-101a">Data Movement Primitives</a></li>
  <li><a href="#sec-dnn-architectures-system-design-impact-cd41" id="toc-sec-dnn-architectures-system-design-impact-cd41" class="nav-link" data-scroll-target="#sec-dnn-architectures-system-design-impact-cd41">System Design Impact</a>
  <ul class="collapse">
  <li><a href="#sec-dnn-architectures-energy-consumption-analysis-across-architectures-b681" id="toc-sec-dnn-architectures-energy-consumption-analysis-across-architectures-b681" class="nav-link" data-scroll-target="#sec-dnn-architectures-energy-consumption-analysis-across-architectures-b681">Energy Consumption Analysis Across Architectures</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#sec-dnn-architectures-architecture-selection-framework-7a37" id="toc-sec-dnn-architectures-architecture-selection-framework-7a37" class="nav-link" data-scroll-target="#sec-dnn-architectures-architecture-selection-framework-7a37">Architecture Selection Framework</a>
  <ul class="collapse">
  <li><a href="#sec-dnn-architectures-datatoarchitecture-mapping-0b9c" id="toc-sec-dnn-architectures-datatoarchitecture-mapping-0b9c" class="nav-link" data-scroll-target="#sec-dnn-architectures-datatoarchitecture-mapping-0b9c">Data-to-Architecture Mapping</a></li>
  <li><a href="#sec-dnn-architectures-computational-complexity-considerations-93fb" id="toc-sec-dnn-architectures-computational-complexity-considerations-93fb" class="nav-link" data-scroll-target="#sec-dnn-architectures-computational-complexity-considerations-93fb">Computational Complexity Considerations</a>
  <ul class="collapse">
  <li><a href="#sec-dnn-architectures-scalability-production-considerations-dcb0" id="toc-sec-dnn-architectures-scalability-production-considerations-dcb0" class="nav-link" data-scroll-target="#sec-dnn-architectures-scalability-production-considerations-dcb0">Scalability and Production Considerations</a></li>
  <li><a href="#sec-dnn-architectures-hardware-mapping-optimization-strategies-5a66" id="toc-sec-dnn-architectures-hardware-mapping-optimization-strategies-5a66" class="nav-link" data-scroll-target="#sec-dnn-architectures-hardware-mapping-optimization-strategies-5a66">Hardware Mapping and Optimization Strategies</a></li>
  </ul></li>
  <li><a href="#sec-dnn-architectures-architectural-comparison-summary-f918" id="toc-sec-dnn-architectures-architectural-comparison-summary-f918" class="nav-link" data-scroll-target="#sec-dnn-architectures-architectural-comparison-summary-f918">Architectural Comparison Summary</a></li>
  <li><a href="#sec-dnn-architectures-decision-framework-dbe8" id="toc-sec-dnn-architectures-decision-framework-dbe8" class="nav-link" data-scroll-target="#sec-dnn-architectures-decision-framework-dbe8">Decision Framework</a></li>
  </ul></li>
  <li><a href="#sec-dnn-architectures-unified-framework-inductive-biases-099d" id="toc-sec-dnn-architectures-unified-framework-inductive-biases-099d" class="nav-link" data-scroll-target="#sec-dnn-architectures-unified-framework-inductive-biases-099d">Unified Framework: Inductive Biases</a></li>
  <li><a href="#sec-dnn-architectures-fallacies-pitfalls-3e82" id="toc-sec-dnn-architectures-fallacies-pitfalls-3e82" class="nav-link" data-scroll-target="#sec-dnn-architectures-fallacies-pitfalls-3e82">Fallacies and Pitfalls</a></li>
  <li><a href="#sec-dnn-architectures-summary-c495" id="toc-sec-dnn-architectures-summary-c495" class="nav-link" data-scroll-target="#sec-dnn-architectures-summary-c495">Summary</a></li>
  <li><a href="#self-check-answers" id="toc-self-check-answers" class="nav-link" data-scroll-target="#self-check-answers">Self-Check Answers</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content page-columns page-full" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../../contents/core/introduction/introduction.html">Systems Foundations</a></li><li class="breadcrumb-item"><a href="../../../contents/core/dnn_architectures/dnn_architectures.html">DNN Architectures</a></li></ol></nav></header>





<section id="sec-dnn-architectures" class="level1 page-columns page-full">
<h1>DNN Architectures</h1>
<div class="{layout-narrow} page-columns page-full">

<div class="no-row-height column-margin column-container"><div class="">
<p><em>DALLÂ·E 3 Prompt: A visually striking rectangular image illustrating the interplay between deep learning algorithms like CNNs, RNNs, and Attention Networks, interconnected with machine learning systems. The composition features neural network diagrams blending seamlessly with representations of computational systems such as processors, graphs, and data streams. Bright neon tones contrast against a dark futuristic background, symbolizing cutting-edge technology and intricate system complexity.</em></p>
</div></div><p> <img src="images/png/cover_dl_arch.png" class="img-fluid"></p>
</div>
<section id="purpose" class="level2 unnumbered">
<h2 class="unnumbered anchored" data-anchor-id="purpose">Purpose</h2>
<p><em>Why do architectural choices in neural networks affect system design decisions that determine computational feasibility, hardware requirements, and deployment constraints?</em></p>
<p>Neural network architectures represent engineering decisions that directly determine system performance and deployment viability. Each architectural choice creates cascading effects throughout the system stack: memory bandwidth demands, computational complexity patterns, parallelization opportunities, and hardware acceleration compatibility. Understanding these architectural implications enables engineers to make informed trade-offs between model capability and system constraints, predict computational bottlenecks before they occur, and select appropriate hardware platforms. Architectural decisions determine whether machine learning systems meet performance requirements within available computational resources. This understanding proves essential for building scalable AI systems that can be deployed effectively across diverse environments.</p>
<div class="callout callout-style-default callout-tip callout-titled" title="Learning Objectives">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Learning Objectives
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><p>Distinguish the computational characteristics and inductive biases of the four main neural network architectural families (MLPs, CNNs, RNNs, Transformers)</p></li>
<li><p>Analyze how architectural design choices determine computational complexity, memory requirements, and parallelization opportunities</p></li>
<li><p>Evaluate the system-level implications of architectural patterns on hardware utilization, memory bandwidth, and deployment constraints</p></li>
<li><p>Apply the architecture selection framework to match data characteristics with appropriate neural network designs for specific applications</p></li>
<li><p>Assess computational and memory trade-offs between different architectural approaches using complexity analysis</p></li>
<li><p>Examine how fundamental computational primitives (matrix multiplication, convolution, attention) map to hardware acceleration opportunities</p></li>
<li><p>Critique common architectural selection fallacies and their impact on system performance and deployment success</p></li>
<li><p>Synthesize the unified inductive bias framework explaining architecture-data compatibility patterns</p></li>
</ul>
</div>
</div>
</section>
<section id="sec-dnn-architectures-architectural-principles-engineering-tradeoffs-89de" class="level2">
<h2 class="anchored" data-anchor-id="sec-dnn-architectures-architectural-principles-engineering-tradeoffs-89de">Architectural Principles and Engineering Trade-offs</h2>
<p>The systematic organization of neural computations into effective architectures represents one of the most consequential developments in contemporary machine learning systems. Building on the mathematical foundations of neural computation established in <strong><a href="../dl_primer/dl_primer.html#sec-dl-primer">Chapter 3: Deep Learning Primer</a></strong>, this chapter investigates the architectural principles that govern how operations (matrix multiplications, nonlinear activations, and gradient-based optimization) are structured to address complex computational problems. This architectural perspective bridges the gap between mathematical theory and practical systems implementation, examining how design choices at the network level determine system-wide performance characteristics.</p>
<p>This chapter centers on an engineering trade-off that permeates machine learning systems design. While mathematical theory, particularly universal approximation results, establishes that neural networks possess remarkable representational flexibility, practical deployment necessitates computational efficiency achievable only through judicious architectural specialization. This tension manifests across multiple dimensions: theoretical universality versus computational tractability, representational completeness versus memory efficiency, and mathematical generality versus domain-specific optimization. The resolution of these tensions through architectural innovation constitutes a primary driver of progress in machine learning systems.</p>
<p>Contemporary neural architectures emerge from systematic responses to specific computational challenges encountered when deploying general mathematical frameworks on structured data. Each architectural paradigm embodies distinct inductive biases (implicit assumptions about data structure and relationships) that enable efficient learning while constraining the hypothesis space in domain-appropriate ways. These architectural innovations represent engineering solutions to the challenge of organizing computational primitives into patterns that achieve optimal balance between representational capacity and computational efficiency.</p>
<p>This chapter examines four architectural families that collectively define the conceptual landscape of modern neural computation. Multi-Layer Perceptrons serve as the canonical implementation of universal approximation theory, demonstrating how dense connectivity enables general pattern recognition while illustrating the computational costs of architectural generality. Convolutional Neural Networks introduce the paradigm of spatial architectural specialization, exploiting translational invariance and local connectivity to achieve significant efficiency gains while preserving representational power for spatial data. Recurrent Neural Networks extend architectural specialization to temporal domains, incorporating explicit memory mechanisms that enable sequential processing capabilities absent from feedforward architectures. Attention mechanisms and Transformer architectures represent the current evolutionary frontier, replacing fixed structural assumptions with dynamic, content-dependent computation that achieves remarkable capability while maintaining computational efficiency through parallelizable operations.</p>
<p>The systems engineering significance of these architectural patterns extends beyond mere algorithmic considerations. Each architectural choice creates distinct computational signatures that propagate through every level of the implementation stack, determining memory access patterns, parallelization strategies, hardware utilization characteristics, and ultimately system feasibility within resource constraints. Understanding these architectural implications proves essential for engineers responsible for system design, resource allocation, and performance optimization in production environments.</p>
<p>This chapter adopts a systems-oriented analytical framework that illuminates the relationships between architectural abstractions and concrete implementation requirements. For each architectural family, we systematically examine the computational primitives that determine hardware resource demands, the organizational principles that enable efficient algorithmic implementation, the memory hierarchy implications that affect system scalability, and the trade-offs between architectural sophistication and computational overhead.</p>
<p>The analytical approach builds systematically upon the neural network foundations established in <strong><a href="../dl_primer/dl_primer.html#sec-dl-primer">Chapter 3: Deep Learning Primer</a></strong>, extending core concepts of forward propagation, backpropagation, and gradient-based optimization by examining how architectural specialization organizes these operations to exploit problem-specific structure. Understanding the evolutionary relationships connecting these architectural paradigms and their distinct computational characteristics, practitioners develop the conceptual tools necessary for principled decision-making regarding architectural selection, resource planning, and system optimization in complex deployment scenarios.</p>
<div id="quiz-question-sec-dnn-architectures-architectural-principles-engineering-tradeoffs-89de" class="callout callout-quiz-question">
<details class="callout-quiz-question fbx-default closebutton"><summary><strong>Self-Check: Question 1.1</strong></summary><div>
<ol type="1">
<li><p>Which architectural paradigm is primarily used to exploit translational invariance and local connectivity for spatial data?</p>
<ol type="a">
<li>Multi-Layer Perceptrons</li>
<li>Recurrent Neural Networks</li>
<li>Convolutional Neural Networks</li>
<li>Transformer architectures</li>
</ol></li>
<li><p>Explain the trade-off between theoretical universality and computational tractability in neural network architectures.</p></li>
<li><p>Which architectural innovation is characterized by dynamic, content-dependent computation?</p>
<ol type="a">
<li>Multi-Layer Perceptrons</li>
<li>Convolutional Neural Networks</li>
<li>Recurrent Neural Networks</li>
<li>Attention mechanisms and Transformer architectures</li>
</ol></li>
<li><p>How do architectural choices in neural networks impact hardware resource demands and system feasibility?</p></li>
</ol>
<p><a href="#quiz-answer-sec-dnn-architectures-architectural-principles-engineering-tradeoffs-89de" class="question-label">See Answers â†’</a></p>
</div></details>
</div>
</section>
<section id="sec-dnn-architectures-multilayer-perceptrons-dense-pattern-processing-259f" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="sec-dnn-architectures-multilayer-perceptrons-dense-pattern-processing-259f">Multi-Layer Perceptrons: Dense Pattern Processing</h2>
<p>Multi-Layer Perceptrons (MLPs) represent the fully-connected architectures introduced in <strong><a href="../dl_primer/dl_primer.html#sec-dl-primer">Chapter 3: Deep Learning Primer</a></strong>, now examined through the lens of architectural choice and systems trade-offs. MLPs embody an inductive bias: <strong>they assume no prior structure in the data, allowing any input to relate to any output</strong>. This architectural choice enables maximum flexibility by treating all input relationships as equally plausible, making MLPs versatile but computationally intensive compared to specialized alternatives. Their computational power was established theoretically by the Universal Approximation Theorem (UAT)<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> <span class="citation" data-cites="cybenko1989approximation hornik1989multilayer">(<a href="#ref-cybenko1989approximation" role="doc-biblioref">Cybenko 1989</a>; <a href="#ref-hornik1989multilayer" role="doc-biblioref">Hornik, Stinchcombe, and White 1989</a>)</span>, which we encountered as a footnote in <strong><a href="../dl_primer/dl_primer.html#sec-dl-primer">Chapter 3: Deep Learning Primer</a></strong>. This theorem states that a sufficiently large MLP with non-linear activation functions can approximate any continuous function on a compact domain, given suitable weights and biases.</p>
<div class="no-row-height column-margin column-container"><div id="fn1"><p><sup>1</sup>&nbsp;<strong>Universal Approximation Theorem</strong>: Proven independently by Cybenko (1989) and Hornik (1989), this result showed that neural networks could theoretically learn any function, a discovery that reinvigorated interest in neural networks after the â€œAI Winterâ€ of the 1980s and established mathematical foundations for modern deep learning.</p></div><div id="ref-cybenko1989approximation" class="csl-entry" role="listitem">
Cybenko, G. 1989. <span>â€œApproximation by Superpositions of a Sigmoidal Function.â€</span> <em>Mathematics of Control, Signals, and Systems</em> 2 (4): 303â€“14. <a href="https://doi.org/10.1007/bf02551274">https://doi.org/10.1007/bf02551274</a>.
</div><div id="ref-hornik1989multilayer" class="csl-entry" role="listitem">
Hornik, Kurt, Maxwell Stinchcombe, and Halbert White. 1989. <span>â€œMultilayer Feedforward Networks Are Universal Approximators.â€</span> <em>Neural Networks</em> 2 (5): 359â€“66. <a href="https://doi.org/10.1016/0893-6080(89)90020-8">https://doi.org/10.1016/0893-6080(89)90020-8</a>.
</div></div><div id="callout-definition*-1.1" class="callout callout-definition" title="Multi-Layer Perceptrons">
<p></p><details class="callout-definition fbx-default closebutton" open=""><summary><strong>Definition: </strong>Multi-Layer Perceptrons</summary><div><strong><em>Multi-Layer Perceptrons (MLPs)</em></strong> are <em>fully-connected neural networks</em> where every neuron connects to all neurons in adjacent layers, providing <em>maximum flexibility</em> through <em>universal approximation</em> at the cost of <em>high parameter counts</em> and <em>computational intensity</em>.<p></p>
</div></details>
</div>
<p>In practice, the UAT explains why MLPs succeed across diverse tasks while revealing the gap between theoretical capability and practical implementation. The theorem guarantees that <em>some</em> MLP can approximate any function, yet provides no guidance on requisite network size or weight determination. This gap becomes critical in real-world applications: while MLPs can theoretically solve any pattern recognition problem, achieving this capability may require impractically large networks or extensive computation. This theoretical power drives the selection of MLPs for tabular data, recommendation systems, and problems where input relationships are unknown, while these practical limitations motivated the development of specialized architectures that exploit data structure for computational efficiency, as detailed in <a href="#sec-dnn-architectures-architectural-principles-engineering-tradeoffs-89de" class="quarto-xref">Section&nbsp;1.1</a>.</p>
<p>When applied to the MNIST handwritten digit recognition challenge<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a>, an MLP demonstrates its computational approach by transforming a <span class="math inline">\(28\times 28\)</span> pixel image into digit classification.</p>
<div class="no-row-height column-margin column-container"><div id="fn2"><p><sup>2</sup>&nbsp;<strong>MNIST Dataset</strong>: Created by Yann LeCun, Corinna Cortes, and Chris Burges in 1998 from NISTâ€™s database of handwritten digits, MNISTâ€™s 60,000 training images became the â€œfruit flyâ€ of machine learning research. Despite human-level accuracy of 99.77% being achieved by various models, MNIST remains valuable for education because its simplicity allows students to focus on architectural concepts without data complexity distractions.</p></div></div><section id="sec-dnn-architectures-pattern-processing-needs-c45a" class="level3">
<h3 class="anchored" data-anchor-id="sec-dnn-architectures-pattern-processing-needs-c45a">Pattern Processing Needs</h3>
<p>Deep learning models frequently encounter problems where any input feature may influence any output, absent inherent constraints on these relationships. Financial market analysis exemplifies this challenge: any economic indicator may affect any market outcome. Similarly, in natural language processing, the meaning of a word may depend on any other word in the sentence. These scenarios demand an architectural pattern capable of learning arbitrary relationships across all input features.</p>
<p>Dense pattern processing addresses these challenges through several key capabilities. First, it enables unrestricted feature interactions where each output can depend on any combination of inputs. Second, it supports learned feature importance, enabling the system to determine which connections matter rather than relying on prescribed relationships. Finally, it provides adaptive representation, enabling the network to reshape its internal representations based on the data.</p>
<p>The MNIST digit recognition task illustrates this uncertainty: while humans might focus on specific parts of digits (loops in â€˜6â€™ or crossings in â€˜8â€™), the pixel combinations critical for classification remain indeterminate. A â€˜7â€™ written with a serif may share pixel patterns with a â€˜2â€™, while variations in handwriting mean discriminative features may appear anywhere in the image. This uncertainty about feature relationships necessitates a dense processing approach where every pixel can potentially influence the classification decision.</p>
<p>This requirement for unrestricted connectivity leads directly to the mathematical foundation of MLPs.</p>
</section>
<section id="sec-dnn-architectures-algorithmic-structure-c012" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-dnn-architectures-algorithmic-structure-c012">Algorithmic Structure</h3>
<p>MLPs enable unrestricted feature interactions through a direct algorithmic solution: complete connectivity between all nodes. This connectivity requirement manifests through a series of fully-connected layers, where each neuron connects to every neuron in adjacent layers, the â€œdenseâ€ connectivity pattern introduced in <strong><a href="../dl_primer/dl_primer.html#sec-dl-primer">Chapter 3: Deep Learning Primer</a></strong>.</p>
<p>This architectural principle translates the dense connectivity pattern into matrix multiplication operations<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a>, establishing the mathematical foundation that makes MLPs computationally tractable. As illustrated in <a href="#fig-mlp" class="quarto-xref">Figure&nbsp;1</a>, each layer transforms its input through the fundamental operation introduced in <strong><a href="../dl_primer/dl_primer.html#sec-dl-primer">Chapter 3: Deep Learning Primer</a></strong>:</p>
<div class="no-row-height column-margin column-container"><div id="fn3"><p><sup>3</sup>&nbsp;<strong>GEMM (General Matrix Multiply)</strong>: The fundamental operation underlying neural networks, accounting for 80-95% of computation time in dense neural networks. GEMM performs C = Î±AB + Î²C and has been optimized for decades. Modern implementations like cuBLAS achieve 80-95% of theoretical peak performance on well-optimized GPU workloads, making GEMM optimization important for ML systems.</p></div></div><p><span class="math display">\[
\mathbf{h}^{(l)} = f\big(\mathbf{h}^{(l-1)}\mathbf{W}^{(l)} + \mathbf{b}^{(l)}\big)
\]</span></p>
<p>Recall that <span class="math inline">\(\mathbf{h}^{(l)}\)</span> represents the layer <span class="math inline">\(l\)</span> output (activation vector), <span class="math inline">\(\mathbf{h}^{(l-1)}\)</span> represents the input from the previous layer, <span class="math inline">\(\mathbf{W}^{(l)}\)</span> denotes the weight matrix for layer <span class="math inline">\(l\)</span>, <span class="math inline">\(\mathbf{b}^{(l)}\)</span> denotes the bias vector, and <span class="math inline">\(f(\cdot)\)</span> denotes the activation function (such as ReLU, as detailed in <strong><a href="../dl_primer/dl_primer.html#sec-dl-primer">Chapter 3: Deep Learning Primer</a></strong>). This layer-wise transformation, while conceptually simple, creates computational patterns whose efficiency depends critically on how we organize these operations for different problem structures.</p>
<div id="fig-mlp" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full" data-fig-env="figure" data-fig-pos="htb">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-mlp-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="34f1ff4eb2e4f04cbfc72069d106a4d9bd7e066d.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="Figure&nbsp;1: Layered Transformations: Multi-Layer Perceptrons (MLPs) implement dense connectivity through sequential matrix multiplications and non-linear activations, supporting complex feature interactions and hierarchical representations of input data. Each layer transforms the input vector from the previous layer, producing a new vector that serves as input to the subsequent layer, as defined by the equation in the text. Source: [@reagen2017deep]."><img src="dnn_architectures_files/mediabag/34f1ff4eb2e4f04cbfc72069d106a4d9bd7e066d.svg" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-mlp-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: <strong>Layered Transformations</strong>: Multi-Layer Perceptrons (MLPs) implement dense connectivity through sequential matrix multiplications and non-linear activations, supporting complex feature interactions and hierarchical representations of input data. Each layer transforms the input vector from the previous layer, producing a new vector that serves as input to the subsequent layer, as defined by the equation in the text. Source: <span class="citation" data-cites="reagen2017deep">(<a href="#ref-reagen2017deep" role="doc-biblioref">Reagen et al. 2017</a>)</span>.
</figcaption>
<div class="no-row-height column-margin column-container"><div id="ref-reagen2017deep" class="csl-entry" role="listitem">
Reagen, Brandon, Robert Adolf, Paul Whatmough, Gu-Yeon Wei, and David Brooks. 2017. <em>Deep Learning for Computer Architects</em>. Springer International Publishing. <a href="https://doi.org/10.1007/978-3-031-01756-8">https://doi.org/10.1007/978-3-031-01756-8</a>.
</div></div></figure>
</div>
<p>The dimensions of these operations reveal the computational scale of dense pattern processing:</p>
<ul>
<li>Input vector: <span class="math inline">\(\mathbf{h}^{(0)} \in \mathbb{R}^{d_{\text{in}}}\)</span> (treated as a row vector in this formulation) represents all potential input features</li>
<li>Weight matrices: <span class="math inline">\(\mathbf{W}^{(l)} \in \mathbb{R}^{d_{\text{in}} \times d_{\text{out}}}\)</span> capture all possible input-output relationships</li>
<li>Output vector: <span class="math inline">\(\mathbf{h}^{(l)} \in \mathbb{R}^{d_{\text{out}}}\)</span> produces transformed representations</li>
</ul>
<div id="callout-example*-1.2" class="callout callout-example" title="Concrete Computation Example">
<p></p><details class="callout-example fbx-default closebutton" open=""><summary><strong>Example: </strong>Concrete Computation Example</summary><div>Consider a simplified 4-pixel image processed by a 3-neuron hidden layer:<p></p>
<p><strong>Input</strong>: <span class="math inline">\(\mathbf{h}^{(0)} = [0.8, 0.2, 0.9, 0.1]\)</span> (4 pixel intensities)</p>
<p><strong>Weight matrix</strong>: <span class="math inline">\(\mathbf{W}^{(1)} = \begin{bmatrix} 0.5 &amp; 0.1 &amp; -0.2 \\ -0.3 &amp; 0.8 &amp; 0.4 \\ 0.2 &amp; -0.4 &amp; 0.6 \\ 0.7 &amp; 0.3 &amp; -0.1 \end{bmatrix}\)</span> (4Ã—3 matrix)</p>
<p><strong>Computation</strong>: <span class="math display">\[\begin{gather*}
\mathbf{z}^{(1)} = \mathbf{h}^{(0)}\mathbf{W}^{(1)} = \begin{bmatrix} 0.5Ã—0.8 + (-0.3)Ã—0.2 + 0.2Ã—0.9 + 0.7Ã—0.1 \\ 0.1Ã—0.8 + 0.8Ã—0.2 + (-0.4)Ã—0.9 + 0.3Ã—0.1 \\ (-0.2)Ã—0.8 + 0.4Ã—0.2 + 0.6Ã—0.9 + (-0.1)Ã—0.1 \end{bmatrix}
\\
= \begin{bmatrix} 0.65 \\ -0.17 \\ 0.47 \end{bmatrix}
\end{gather*}\]</span> <strong>After ReLU</strong>: <span class="math inline">\(\mathbf{h}^{(1)} = [0.65, 0, 0.47]\)</span> (negative values zeroed)</p>
<p>Each hidden neuron combines ALL input pixels with different weights, demonstrating unrestricted feature interaction.</p>
</div></details>
</div>
<p>The MNIST example demonstrates the practical scale of these operations:</p>
<ul>
<li>Each 784-dimensional input (<span class="math inline">\(28\times 28\)</span> pixels) connects to every neuron in the first hidden layer</li>
<li>A hidden layer with 100 neurons requires a <span class="math inline">\(784\times 100\)</span> weight matrix</li>
<li>Each weight in this matrix represents a learnable relationship between an input pixel and a hidden feature</li>
</ul>
<p>This algorithmic structure addresses the need for arbitrary feature relationships while creating specific computational patterns that computer systems must accommodate.</p>
<section id="sec-dnn-architectures-architectural-characteristics-47b4" class="level4">
<h4 class="anchored" data-anchor-id="sec-dnn-architectures-architectural-characteristics-47b4">Architectural Characteristics</h4>
<p>This dense connectivity approach creates both advantages and trade-offs. Dense connectivity provides the universal approximation capability established earlier but introduces computational redundancy. While this theoretical power enables MLPs to model any continuous function given sufficient width, this flexibility necessitates numerous parameters to learn relatively simple patterns. The dense connections ensure that every input feature influences every output, yielding maximum expressiveness at the cost of maximum computational expense.</p>
<p>These trade-offs motivate sophisticated optimization techniques that reduce computational demands while preserving model capability. Structured pruning can eliminate 80-90% of connections with minimal accuracy loss, while quantization reduces precision requirements from 32-bit to 8-bit or lower. While <strong><a href="../optimizations/optimizations.html#sec-model-optimizations">Chapter 10: Model Optimizations</a></strong> details these compression strategies, the architectural foundations established here determine which optimization approaches prove most effective for dense connectivity patterns, with <strong><a href="../hw_acceleration/hw_acceleration.html#sec-ai-acceleration">Chapter 11: AI Acceleration</a></strong> exploring hardware-specific implementations that exploit regular matrix operation structure.</p>
</section>
</section>
<section id="sec-dnn-architectures-computational-mapping-fe7e" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-dnn-architectures-computational-mapping-fe7e">Computational Mapping</h3>
<p>The mathematical representation of dense matrix multiplication maps to specific computational patterns that systems must handle. This mapping progresses from mathematical abstraction to computational reality, as demonstrated in the first implementation shown in <a href="#lst-mlp_layer_matrix" class="quarto-xref">Listing&nbsp;1</a>.</p>
<p>The function mlp_layer_matrix directly mirrors the mathematical equation, employing high-level matrix operations (<code>matmul</code>) to express the computation in a single line while abstracting the underlying complexity. This implementation style characterizes deep learning frameworks, where optimized libraries manage the actual computation.</p>
<div id="lst-mlp_layer_matrix" class="listing quarto-float quarto-figure quarto-figure-left anchored">
<figure class="quarto-float quarto-float-lst figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-lst" id="lst-mlp_layer_matrix-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Listing&nbsp;1: This implementation shows neural networks performing weighted sum and activation functions across layers using matrix operations. The code emphasizes the core computational pattern in multi-layer perceptrons.
</figcaption>
<div aria-describedby="lst-mlp_layer_matrix-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="sourceCode" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> mlp_layer_matrix(X, W, b):</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># X: input matrix (batch_size Ã— num_inputs)</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>    <span class="co"># W: weight matrix (num_inputs Ã— num_outputs)</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># b: bias vector (num_outputs)</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>    H <span class="op">=</span> activation(matmul(X, W) <span class="op">+</span> b)</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>    <span class="co"># One clean line of math</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> H</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</figure>
</div>
<p>To understand the system implications of this architecture, we must look â€œunder the hoodâ€ of the high-level framework call. The elegant one-line matrix multiplication <code>output = matmul(X, W)</code> is, from the hardwareâ€™s perspective, a series of nested loops that expose the true computational demands on the system. This translation from logical model to physical execution reveals critical patterns that determine memory access, parallelization strategies, and hardware utilization.</p>
<p>The second implementation, <code>mlp_layer_compute</code> (shown in <a href="#lst-mlp_layer_compute" class="quarto-xref">Listing&nbsp;2</a>), exposes the actual computational pattern through nested loops. This version reveals what really happens when we compute a layerâ€™s output: we process each sample in the batch, computing each output neuron by accumulating weighted contributions from all inputs.</p>
<div id="lst-mlp_layer_compute" class="listing quarto-float quarto-figure quarto-figure-left anchored">
<figure class="quarto-float quarto-float-lst figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-lst" id="lst-mlp_layer_compute-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Listing&nbsp;2: This implementation computes each output neuron by accumulating weighted contributions from all inputs across the batch. The detailed step-by-step process exposes how a single layer in a neural network processes data, emphasizing the role of biases and weighted sums in producing outputs.
</figcaption>
<div aria-describedby="lst-mlp_layer_compute-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="sourceCode" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> mlp_layer_compute(X, W, b):</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Process each sample in the batch</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> batch <span class="kw">in</span> <span class="bu">range</span>(batch_size):</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Compute each output neuron</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> out <span class="kw">in</span> <span class="bu">range</span>(num_outputs):</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Initialize with bias</span></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>            Z[batch, out] <span class="op">=</span> b[out]</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Accumulate weighted inputs</span></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> in_ <span class="kw">in</span> <span class="bu">range</span>(num_inputs):</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>                Z[batch, out] <span class="op">+=</span> X[batch, in_] <span class="op">*</span> W[in_, out]</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>    H <span class="op">=</span> activation(Z)</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> H</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</figure>
</div>
<p>This translation from mathematical abstraction to concrete computation exposes how dense matrix multiplication decomposes into nested loops of simpler operations. The outer loop processes each sample in the batch, while the middle loop computes values for each output neuron. Within the innermost loop, the system performs repeated multiply-accumulate operations<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a>, combining each input with its corresponding weight.</p>
<div class="no-row-height column-margin column-container"><div id="fn4"><p><sup>4</sup>&nbsp;<strong>Multiply-Accumulate (MAC)</strong>: The atomic operation in neural networks: multiply two values and add to running sum (result += a Ã— b). Modern accelerators measure performance in MACs/second: NVIDIA A100 achieves 312 trillion MACs/second, while mobile chips achieve 1-10 trillion. Energy cost: ~4.6 picojoules per MAC, plus 640pJ for data movement.</p></div><div id="fn5"><p><sup>5</sup>&nbsp;<strong>Basic Linear Algebra Subprograms (BLAS)</strong>: Developed in the 1970s as a standard for basic vector and matrix operations, BLAS became the foundation for virtually all scientific computing. Modern implementations like Intel MKL and OpenBLAS can achieve 80-95% of theoretical peak performance on well-optimized workloads, making them necessary for neural network efficiency.</p></div><div id="fn6"><p><sup>6</sup>&nbsp;<strong>Tensor Cores</strong>: Specialized matrix multiplication units in modern GPUs that perform mixed-precision operations on 4Ã—4 matrices per clock cycle. NVIDIA V100 tensor cores deliver 125 TFLOPS vs 15 TFLOPS from standard coresâ€”a 8Ã— improvement that revolutionized deep learning performance and made large model training feasible.</p></div></div><p>In the MNIST example, each output neuron requires 784 multiply-accumulate operations and at least 1,568 memory accesses (784 for inputs, 784 for weights). While actual implementations use optimizations through libraries like BLAS<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a> or cuBLAS, these patterns drive key system design decisions. The hardware architectures that accelerate these matrix operations, including GPU tensor cores<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a> and specialized AI accelerators, are covered in <strong><a href="../hw_acceleration/hw_acceleration.html#sec-ai-acceleration">Chapter 11: AI Acceleration</a></strong>.</p>
</section>
<section id="sec-dnn-architectures-system-implications-7a8f" class="level3">
<h3 class="anchored" data-anchor-id="sec-dnn-architectures-system-implications-7a8f">System Implications</h3>
<p>Neural network architectures exhibit distinct system-level characteristics that exhibit three core dimensions for systematic analysis: memory requirements, computation needs, and data movement. This framework enables consistent analysis of how algorithmic patterns influence system design decisions, revealing both commonalities and architecture-specific optimizations. We apply this framework throughout our analysis of each architecture family. These system-level considerations build directly on the foundational concepts of neural network computation patterns, memory systems, and system scaling discussed in <strong><a href="../dl_primer/dl_primer.html#sec-dl-primer">Chapter 3: Deep Learning Primer</a></strong>.</p>
<section id="sec-dnn-architectures-memory-requirements-4900" class="level4">
<h4 class="anchored" data-anchor-id="sec-dnn-architectures-memory-requirements-4900">Memory Requirements</h4>
<p>For dense pattern processing, the memory requirements stem from storing and accessing weights, inputs, and intermediate results. In our MNIST example, connecting our 784-dimensional input layer to a hidden layer of 100 neurons requires 78,400 weight parameters. Each forward pass must access all these weights, along with input data and intermediate results. The all-to-all connectivity pattern means thereâ€™s no inherent locality in these accesses; every output needs every input and its corresponding weights.</p>
<p>These memory access patterns enable optimization through careful data organization and reuse. Modern processors handle these dense access patterns through specialized approaches: CPUs leverage their cache hierarchy for data reuse, while GPUs employ memory architectures designed for high-bandwidth access to large parameter matrices. Frameworks abstract these optimizations through high-performance matrix operations (as detailed in our earlier analysis).</p>
</section>
<section id="sec-dnn-architectures-computation-needs-9cb4" class="level4">
<h4 class="anchored" data-anchor-id="sec-dnn-architectures-computation-needs-9cb4">Computation Needs</h4>
<p>The core computation revolves around multiply-accumulate operations arranged in nested loops. Each output value requires as many multiply-accumulates as there are inputs. For MNIST, this requires 784 multiply-accumulates per output neuron. With 100 neurons in the hidden layer, 78,400 multiply-accumulates are performed for a single input image. While these operations are simple, their volume and arrangement create specific demands on processing resources.</p>
<p>This computational structure enables specific optimization strategies in modern hardware. The dense matrix multiplication pattern parallelizes across multiple processing units, with each handling different subsets of neurons. Modern hardware accelerators take advantage of this through specialized matrix multiplication units, while software frameworks automatically convert these operations into optimized BLAS (Basic Linear Algebra Subprograms) calls. CPUs and GPUs can both exploit cache locality by carefully tiling the computation to maximize data reuse, though their specific approaches differ based on their architectural strengths.</p>
</section>
<section id="sec-dnn-architectures-data-movement-fc16" class="level4">
<h4 class="anchored" data-anchor-id="sec-dnn-architectures-data-movement-fc16">Data Movement</h4>
<p>The all-to-all connectivity pattern in MLPs creates significant data movement requirements. Each multiply-accumulate operation needs three pieces of data: an input value, a weight value, and the running sum. For our MNIST example layer, computing a single output value requires moving 784 inputs and 784 weights to wherever the computation occurs. This movement pattern repeats for each of the 100 output neurons, creating large data transfer demands between memory and compute units.</p>
<p>The predictable data movement patterns enable strategic data staging and transfer optimizations. Different architectures address this challenge through various mechanisms; CPUs use prefetching and multi-level caches, while GPUs employ high-bandwidth memory systems and latency hiding through massive threading. Software frameworks orchestrate these data movements through memory management systems that reduce redundant transfers and increase data reuse.</p>
<p>This analysis of MLP computational demands reveals a crucial insight: while dense connectivity provides universal approximation capabilities, it creates significant inefficiencies when data exhibits inherent structure. This mismatch between architectural assumptions and data characteristics motivated the development of specialized approaches that could exploit structural patterns for computational gain.</p>
<div id="quiz-question-sec-dnn-architectures-multilayer-perceptrons-dense-pattern-processing-259f" class="callout callout-quiz-question">
<details class="callout-quiz-question fbx-default closebutton"><summary><strong>Self-Check: Question 1.2</strong></summary><div>
<ol type="1">
<li><p>What is a key advantage of using Multi-Layer Perceptrons (MLPs) in machine learning systems?</p>
<ol type="a">
<li>They assume no prior structure in the data, allowing maximum flexibility.</li>
<li>They exploit inherent data structures for computational efficiency.</li>
<li>They require minimal computational resources compared to specialized architectures.</li>
<li>They are inherently more interpretable than other neural network architectures.</li>
</ol></li>
<li><p>Explain how the Universal Approximation Theorem influences the architectural choice of using MLPs in machine learning systems.</p></li>
<li><p>In the context of MNIST digit recognition, why might an MLP be chosen over specialized architectures?</p>
<ol type="a">
<li>MLPs are more efficient in handling high-dimensional data like images.</li>
<li>MLPs can exploit the spatial locality of pixel data better than convolutional networks.</li>
<li>MLPs provide flexibility to learn arbitrary pixel relationships without assuming spatial structure.</li>
<li>MLPs are less computationally intensive than convolutional neural networks.</li>
</ol></li>
<li><p>The computational operation that forms the backbone of MLPs and accounts for most of their computation time is known as ____. This operation is crucial for the dense connectivity pattern.</p></li>
<li><p>How do memory requirements and data movement patterns in MLPs influence system design decisions?</p></li>
</ol>
<p><a href="#quiz-answer-sec-dnn-architectures-multilayer-perceptrons-dense-pattern-processing-259f" class="question-label">See Answers â†’</a></p>
</div></details>
</div>
</section>
</section>
</section>
<section id="sec-dnn-architectures-cnns-spatial-pattern-processing-f8ff" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="sec-dnn-architectures-cnns-spatial-pattern-processing-f8ff">CNNs: Spatial Pattern Processing</h2>
<p>The computational intensity and parameter requirements of MLPs reveal a mismatch when applied to structured data. Building on the computational complexity considerations outlined in <a href="#sec-dnn-architectures-architectural-principles-engineering-tradeoffs-89de" class="quarto-xref">Section&nbsp;1.1</a>, this inefficiency motivated the development of architectural patterns that exploit inherent data structure.</p>
<p>Convolutional Neural Networks emerged as the solution to this challenge <span class="citation" data-cites="lecun1998gradient krizhevsky2012imagenet">(<a href="#ref-lecun1998gradient" role="doc-biblioref">Lecun et al. 1998</a>; <a href="#ref-krizhevsky2012imagenet" role="doc-biblioref">Krizhevsky, Sutskever, and Hinton 2017</a>)</span>, embodying a specific inductive bias: they assume spatial locality and translation invariance, where nearby pixels are related and patterns can appear anywhere. This architectural assumption enables two key innovations that enhance efficiency for spatially structured data. Parameter sharing allows the same feature detector to be applied across different spatial positions, reducing parameters from millions to thousands while improving generalization. Local connectivity restricts connections to spatially adjacent regions, reflecting the insight that spatial proximity correlates with feature relevance.</p>
<div class="no-row-height column-margin column-container"></div><div id="callout-definition*-1.3" class="callout callout-definition" title="Convolutional Neural Networks">
<p></p><details class="callout-definition fbx-default closebutton" open=""><summary><strong>Definition: </strong>Convolutional Neural Networks</summary><div><strong><em>Convolutional Neural Networks (CNNs)</em></strong> are neural architectures that exploit <em>spatial structure</em> through <em>local connectivity</em> and <em>parameter sharing</em>, using <em>learnable filters</em> to build <em>hierarchical representations</em> with substantially fewer parameters than fully-connected networks.<p></p>
</div></details>
</div>
<p>These architectural innovations represent a trade-off in deep learning design: sacrificing the theoretical generality of MLPs for practical efficiency gains when data exhibits known structure. While MLPs treat each input element independently, CNNs exploit spatial relationships to achieve computational savings and improved performance on vision tasks.</p>
<section id="sec-dnn-architectures-pattern-processing-needs-a4ce" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-dnn-architectures-pattern-processing-needs-a4ce">Pattern Processing Needs</h3>
<p>Spatial pattern processing addresses scenarios where the relationship between data points depends on their relative positions or proximity. Consider processing a natural image: a pixelâ€™s relationship with its neighbors is important for detecting edges, textures, and shapes. These local patterns then combine hierarchically to form more complex features: edges form shapes, shapes form objects, and objects form scenes.</p>
<p>This hierarchical spatial pattern processing appears across many domains. In computer vision, local pixel patterns form edges and textures that combine into recognizable objects. Speech processing relies on patterns across nearby time segments to identify phonemes and words. Sensor networks analyze correlations between physically proximate sensors to understand environmental patterns. Medical imaging depends on recognizing tissue patterns that indicate biological structures.</p>
<p>Focusing on image processing to illustrate these principles, if we want to detect a cat in an image, certain spatial patterns must be recognized: the triangular shape of ears, the round contours of the face, the texture of fur. Importantly, these patterns maintain their meaning regardless of where they appear in the image. A cat is still a cat whether it appears in the top-left or bottom-right corner. This indicates two key requirements for spatial pattern processing: the ability to detect local patterns and the ability to recognize these patterns regardless of their position<a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a>. <a href="#fig-cnn-spatial-processing" class="quarto-xref">Figure&nbsp;2</a> shows convolutional neural networks achieving this through hierarchical feature extraction, where simple patterns compose into increasingly complex representations at successive layers.</p>
<div class="no-row-height column-margin column-container"><div id="fn7"><p><sup>7</sup>&nbsp;<strong>ImageNet Revolution</strong>: AlexNetâ€™s dramatic victory in the 2012 ImageNet challenge <span class="citation" data-cites="krizhevsky2012imagenet">(<a href="#ref-krizhevsky2012imagenet" role="doc-biblioref">Krizhevsky, Sutskever, and Hinton 2017</a>)</span> (reducing top-5 error from 25.8% to 15.3%) sparked the deep learning renaissance. ImageNetâ€™s 14 million labeled images across 20,000 categories provided the scale needed to train deep CNNs, proving that â€œbig data + big compute + big modelsâ€ could achieve superhuman performance.</p><div id="ref-krizhevsky2012imagenet" class="csl-entry" role="listitem">
Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E. Hinton. 2017. <span>â€œImageNet Classification with Deep Convolutional Neural Networks.â€</span> <em>Communications of the ACM</em> 60 (6): 84â€“90. <a href="https://doi.org/10.1145/3065386">https://doi.org/10.1145/3065386</a>.
</div></div></div><div id="fig-cnn-spatial-processing" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-env="figure" data-fig-pos="htb">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-cnn-spatial-processing-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="c8b06853acf5ccdc49c2acfab35b58b178bac9b1.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-2" title="Figure&nbsp;2: Spatial Feature Extraction: Convolutional neural networks identify patterns independent of their location in an image by applying learnable filters across the input, enabling robust object recognition. These filters detect local features, and their repeated application across the image creates translation invariance, the ability to recognize a pattern regardless of its position."><img src="dnn_architectures_files/mediabag/c8b06853acf5ccdc49c2acfab35b58b178bac9b1.svg" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-cnn-spatial-processing-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: <strong>Spatial Feature Extraction</strong>: Convolutional neural networks identify patterns independent of their location in an image by applying learnable filters across the input, enabling robust object recognition. These filters detect local features, and their repeated application across the image creates translation invariance, the ability to recognize a pattern regardless of its position.
</figcaption>
</figure>
</div>
<p>This leads us to the convolutional neural network architecture (CNN), pioneered by Yann LeCun<a href="#fn8" class="footnote-ref" id="fnref8" role="doc-noteref"><sup>8</sup></a> and <span class="citation" data-cites="lecun1989backpropagation">Y. LeCun et al. (<a href="#ref-lecun1989backpropagation" role="doc-biblioref">1989</a>)</span>. CNNs achieve this through several key innovations: parameter sharing<a href="#fn9" class="footnote-ref" id="fnref9" role="doc-noteref"><sup>9</sup></a>, local connectivity, and translation invariance<a href="#fn10" class="footnote-ref" id="fnref10" role="doc-noteref"><sup>10</sup></a>.</p>
<div class="no-row-height column-margin column-container"><div id="fn8"><p><sup>8</sup>&nbsp;<strong>Yann LeCun and CNNs</strong>: LeCunâ€™s 1989 LeNet architecture was inspired by Hubel and Wieselâ€™s discovery of simple and complex cells in cat visual cortex <span class="citation" data-cites="hubel1962receptive">(<a href="#ref-hubel1962receptive" role="doc-biblioref">Hubel and Wiesel 1962</a>)</span>. LeNet-5 achieved 0.8% error rate on MNIST in 1998 (though this was measured on the original NIST database, which differs slightly from the modern MNIST benchmark) and was deployed by banks to read millions of checks daily, among the first large-scale commercial applications of neural networks.</p><div id="ref-hubel1962receptive" class="csl-entry" role="listitem">
Hubel, D. H., and T. N. Wiesel. 1962. <span>â€œReceptive Fields, Binocular Interaction and Functional Architecture in the Catâ€™s Visual Cortex.â€</span> <em>The Journal of Physiology</em> 160 (1): 106â€“54. <a href="https://doi.org/10.1113/jphysiol.1962.sp006837">https://doi.org/10.1113/jphysiol.1962.sp006837</a>.
</div></div><div id="ref-lecun1989backpropagation" class="csl-entry" role="listitem">
LeCun, Y., B. Boser, J. S. Denker, D. Henderson, R. E. Howard, W. Hubbard, and L. D. Jackel. 1989. <span>â€œBackpropagation Applied to Handwritten Zip Code Recognition.â€</span> <em>Neural Computation</em> 1 (4): 541â€“51. <a href="https://doi.org/10.1162/neco.1989.1.4.541">https://doi.org/10.1162/neco.1989.1.4.541</a>.
</div><div id="fn9"><p><sup>9</sup>&nbsp;<strong>Parameter Sharing</strong>: CNNs reuse the same filter weights across spatial positions, reducing parameters substantially. A CNN processing 224Ã—224 images might use 3Ã—3 filters with only 9 parameters per channel, versus an equivalent MLP requiring 50,176 parameters per neuron, a ~5,575x reduction per neuron enabling practical computer vision.</p></div><div id="fn10"><p><sup>10</sup>&nbsp;<strong>Translation Invariance</strong>: CNNs detect features regardless of spatial position. A catâ€™s ear is recognized whether in the top-left or bottom-right corner. This property emerges from convolutionâ€™s sliding window design and is important for computer vision, where objects appear at arbitrary locations in images.</p></div></div></section>
<section id="sec-dnn-architectures-algorithmic-structure-a10c" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-dnn-architectures-algorithmic-structure-a10c">Algorithmic Structure</h3>
<p>The core operation in a CNN can be expressed mathematically as:</p>
<p><span class="math display">\[
\mathbf{H}^{(l)}_{i,j,k} = f\left(\sum_{di}\sum_{dj}\sum_{c} \mathbf{W}^{(l)}_{di,dj,c,k}\mathbf{H}^{(l-1)}_{i+di,j+dj,c} + \mathbf{b}^{(l)}_k\right)
\]</span></p>
<p>This equation describes how CNNs process spatial data. <span class="math inline">\(\mathbf{H}^{(l)}_{i,j,k}\)</span> is the output at spatial position <span class="math inline">\((i,j)\)</span> in channel <span class="math inline">\(k\)</span> of layer <span class="math inline">\(l\)</span>. The triple sum iterates over the filter dimensions: <span class="math inline">\((di,dj)\)</span> scans the spatial filter size, and <span class="math inline">\(c\)</span> covers input channels. <span class="math inline">\(\mathbf{W}^{(l)}_{di,dj,c,k}\)</span> represents the filter weights, capturing local spatial patterns. Unlike MLPs that connect all inputs to outputs, CNNs only connect local spatial neighborhoods.</p>
<p>Breaking down the notation further, <span class="math inline">\((i,j)\)</span> corresponds to spatial positions, <span class="math inline">\(k\)</span> indexes output channels, <span class="math inline">\(c\)</span> indexes input channels, and <span class="math inline">\((di,dj)\)</span> spans the local receptive field<a href="#fn11" class="footnote-ref" id="fnref11" role="doc-noteref"><sup>11</sup></a>. Unlike the dense matrix multiplication of MLPs, this operation:</p>
<div class="no-row-height column-margin column-container"><div id="fn11"><p><sup>11</sup>&nbsp;<strong>Receptive Field</strong>: The region of the input that influences a particular output neuron. In CNNs, receptive fields grow with depth. A neuron in layer 3 might â€œseeâ€ a 7Ã—7 region even with 3Ã—3 filters, due to stacking. Understanding receptive field size is important for ensuring networks can capture features at the right scale for the task.</p></div></div><ul>
<li>Processes local neighborhoods (typically <span class="math inline">\(3\times 3\)</span> or <span class="math inline">\(5\times 5\)</span>)</li>
<li>Reuses the same weights at each spatial position</li>
<li>Maintains spatial structure in its output</li>
</ul>
<p>To illustrate this process concretely, consider the MNIST digit classification task with <span class="math inline">\(28\times 28\)</span> grayscale images. Each convolutional layer applies a set of filters (e.g., <span class="math inline">\(3\times 3\)</span>) that slide across the image, computing local weighted sums. If we use 32 filters with padding to preserve dimensions, the layer produces a <span class="math inline">\(28\times 28\times 32\)</span> output, where each spatial position contains 32 different feature measurements of its local neighborhood. This contrasts sharply with the Multi-Layer Perceptron (MLP) approach, where the entire image is flattened into a 784-dimensional vector before processing.</p>
<p>This algorithmic structure directly implements the requirements for spatial pattern processing, creating distinct computational patterns that influence system design. Unlike MLPs, convolutional networks preserve spatial locality, leveraging the hierarchical feature extraction principles established above. These properties drive architectural optimizations in AI accelerators, where operations such as data reuse, tiling, and parallel filter computation are important for performance.</p>
<div class="callout callout-style-default callout-note callout-titled" title="Mathematical Background">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Mathematical Background
</div>
</div>
<div class="callout-body-container callout-body">
<p>Group theory provides the mathematical framework for understanding symmetries and transformations in data. Translation equivariance means that shifting an input produces a correspondingly shifted outputâ€”a key property that enables CNNs to recognize patterns regardless of position.</p>
</div>
</div>
<p>Group theory provides the framework for understanding CNN effectiveness<a href="#fn12" class="footnote-ref" id="fnref12" role="doc-noteref"><sup>12</sup></a>, which provides a mathematical framework for understanding symmetries in data. Translation invariance emerges because convolution is equivariant with respect to the translation groupâ€”if we shift the input image, the output feature maps shift by the same amount. Mathematically, if <span class="math inline">\(T_v\)</span> represents translation by vector <span class="math inline">\(v\)</span>, then a convolutional layer <span class="math inline">\(f\)</span> satisfies: <span class="math inline">\(f(T_v x) = T_v f(x)\)</span>. This equivariance property allows CNNs to learn features that generalize across spatial locations.</p>
<div class="no-row-height column-margin column-container"><div id="fn12"><p><sup>12</sup>&nbsp;<strong>Group Theory in Neural Networks</strong>: Mathematical framework describing how CNNs preserve spatial relationships. Translation equivariance means shifting an input image shifts the output feature maps by the same amountâ€”a property enabling CNNs to recognize objects regardless of position, foundational to computer vision success.</p></div><div id="fn13"><p><sup>13</sup>&nbsp;<strong>Inductive Bias</strong>: Prior assumptions built into model architecture about the structure of data. CNNs assume spatial locality and translation invariance, drastically reducing the space of functions they can learn compared to MLPs. This constraint enables better generalization with fewer parametersâ€”a key principle in machine learning architecture design.</p></div><div id="fn14"><p><sup>14</sup>&nbsp;<strong>Hypothesis Space</strong>: The set of all possible functions a model can represent given its architecture and parameters. MLPs have a larger hypothesis space than CNNs for images, but CNNsâ€™ constrained space contains better solutions for visual tasks, demonstrating that architectural constraints often improve rather than limit performance. Recent work has extended these principles to other symmetry groups, developing Group-Equivariant CNNs that handle rotations and reflections <span class="citation" data-cites="cohen2016group">(<a href="#ref-cohen2016group" role="doc-biblioref">Cohen and Welling 2016</a>)</span>.</p><div id="ref-cohen2016group" class="csl-entry" role="listitem">
Cohen, Taco, and Max Welling. 2016. <span>â€œGroup Equivariant Convolutional Networks.â€</span> <em>International Conference on Machine Learning</em>, 2990â€“99.
</div></div></div><p>The choice of convolution reflects deeper principles about inductive bias<a href="#fn13" class="footnote-ref" id="fnref13" role="doc-noteref"><sup>13</sup></a> in neural architecture design. By restricting connectivity to local neighborhoods and sharing parameters across spatial positions, CNNs encode prior knowledge about the structure of visual data: that important features are local and translation-invariant. This architectural constraint reduces the hypothesis space<a href="#fn14" class="footnote-ref" id="fnref14" role="doc-noteref"><sup>14</sup></a> that the network must search, enabling more efficient learning from limited data compared to fully connected networks.</p>
<p>CNNs naturally implement hierarchical representation learning through their layered structure. Early layers detect low-level features like edges and textures with small receptive fields, while deeper layers combine these into increasingly complex patterns with larger receptive fields. This hierarchical organization mirrors the structure of the visual cortex and enables CNNs to build compositional representations: complex objects are represented as compositions of simpler parts. The mathematical foundation for this emerges from the fact that stacking convolutional layers creates a tree-like dependency structure, where each deep neuron depends on an exponentially large set of input pixels, enabling efficient representation of hierarchical patterns.</p>
<section id="sec-dnn-architectures-architectural-characteristics-e309" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="sec-dnn-architectures-architectural-characteristics-e309">Architectural Characteristics</h4>
<p>Parameter sharing dramatically reduces complexity compared to MLPs by reusing the same filters across spatial locations. This sharing embodies the assumption that useful features (such as edges or textures) can appear anywhere in an image, making the same feature detector valuable across all spatial positions.</p>
<p>The architectural efficiency of CNNs enables further optimization through specialized techniques. Depthwise separable convolutions decompose standard convolutions into depthwise and pointwise operations, reducing computation by 8-9Ã— for typical mobile deployments. Channel pruning eliminates entire feature maps based on importance metrics, achieving 40-50% FLOPs reduction with &lt;1% accuracy loss. These optimization strategies build on spatial locality principles, with <strong><a href="../optimizations/optimizations.html#sec-model-optimizations">Chapter 10: Model Optimizations</a></strong> exploring hardware-specific implementations and <strong><a href="../hw_acceleration/hw_acceleration.html#sec-ai-acceleration">Chapter 11: AI Acceleration</a></strong> detailing how modern processors exploit convolutionâ€™s inherent data reuse patterns.</p>
<p>As illustrated in <a href="#fig-cnn" class="quarto-xref">Figure&nbsp;3</a>, convolution operations involve sliding a small filter over the input image to generate a feature map<a href="#fn15" class="footnote-ref" id="fnref15" role="doc-noteref"><sup>15</sup></a>. This process captures local structures while maintaining translation invariance. For an interactive visual exploration of convolutional networks, the <a href="https://poloclub.github.io/cnn-explainer/">CNN Explainer</a> project provides an insightful demonstration of how these networks are constructed.</p>
<div class="no-row-height column-margin column-container"><div id="fn15"><p><sup>15</sup>&nbsp;<strong>Feature Map</strong>: The output of applying a convolutional filter to an input, representing detected features at different spatial locations. A 64-filter layer produces 64 feature maps, each highlighting different patterns like edges, textures, or shapes. Feature maps become more abstract (detecting objects, faces) in deeper layers compared to early layers (detecting edges, colors).</p></div></div><div id="fig-cnn" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-env="figure" data-fig-pos="htb">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-cnn-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="fc7cbac26cbac9fcfc0d4c00b2bae655b6fafb65.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-3" title="Figure&nbsp;3: The convolution operation processes input data through localized feature extraction using filters that slide across the image to identify patterns regardless of their position."><img src="dnn_architectures_files/mediabag/fc7cbac26cbac9fcfc0d4c00b2bae655b6fafb65.svg" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-cnn-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: The convolution operation processes input data through localized feature extraction using filters that slide across the image to identify patterns regardless of their position.
</figcaption>
</figure>
</div>
</section>
</section>
<section id="sec-dnn-architectures-computational-mapping-fea5" class="level3">
<h3 class="anchored" data-anchor-id="sec-dnn-architectures-computational-mapping-fea5">Computational Mapping</h3>
<p>Convolution operations create computational patterns different from MLP dense matrix multiplication. This translation from mathematical operations to implementation details reveals distinct computational characteristics.</p>
<p>The first implementation, <code>conv_layer_spatial</code> (shown in <a href="#lst-conv_layer_spatial" class="quarto-xref">Listing&nbsp;3</a>), uses high-level convolution operations to express the computation concisely. This is typical in deep learning frameworks, where optimized libraries handle the underlying complexity.</p>
<div id="lst-conv_layer_spatial" class="listing quarto-float quarto-figure quarto-figure-left anchored">
<figure class="quarto-float quarto-float-lst figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-lst" id="lst-conv_layer_spatial-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Listing&nbsp;3: This hierarchical approach processes input data through feature extraction using a convolution operation that combines a kernel and bias before applying an activation function.
</figcaption>
<div aria-describedby="lst-conv_layer_spatial-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="sourceCode" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> conv_layer_spatial(<span class="bu">input</span>, kernel, bias):</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>    output <span class="op">=</span> convolution(<span class="bu">input</span>, kernel) <span class="op">+</span> bias</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> activation(output)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</figure>
</div>
<p>The bridge between the logical model and physical execution becomes critical for understanding CNN system requirements. While the high-level convolution operation appears as a simple sliding window computation, the hardware must orchestrate complex data movement patterns and exploit spatial locality for efficiency.</p>
<p>The second implementation, conv_layer_compute (see <a href="#lst-conv_layer_compute" class="quarto-xref">Listing&nbsp;4</a>), reveals the actual computational pattern: nested loops that process each spatial position, applying the same filter weights to local regions of the input. These seven nested loops expose the true nature of convolutionâ€™s computational structure and the optimization opportunities it creates.</p>
<div id="lst-conv_layer_compute" class="listing quarto-float quarto-figure quarto-figure-left anchored">
<figure class="quarto-float quarto-float-lst figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-lst" id="lst-conv_layer_compute-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Listing&nbsp;4: <strong>Nested Loops</strong>: Convolutional layers process input through multiple nested loops that handle batched images, spatial dimensions, output channels, kernel windows, and input features, revealing the detailed computational structure of convolution operations.
</figcaption>
<div aria-describedby="lst-conv_layer_compute-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="sourceCode" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> conv_layer_compute(<span class="bu">input</span>, kernel, bias):</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Loop 1: Process each image in batch</span></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> image <span class="kw">in</span> <span class="bu">range</span>(batch_size):</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Loop 2&amp;3: Move across image spatially</span></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> y <span class="kw">in</span> <span class="bu">range</span>(height):</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> x <span class="kw">in</span> <span class="bu">range</span>(width):</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>                <span class="co"># Loop 4: Compute each output feature</span></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>                <span class="cf">for</span> out_channel <span class="kw">in</span> <span class="bu">range</span>(num_output_channels):</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>                    result <span class="op">=</span> bias[out_channel]</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>                    <span class="co"># Loop 5&amp;6: Move across kernel window</span></span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a>                    <span class="cf">for</span> ky <span class="kw">in</span> <span class="bu">range</span>(kernel_height):</span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a>                        <span class="cf">for</span> kx <span class="kw">in</span> <span class="bu">range</span>(kernel_width):</span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a>                            <span class="co"># Loop 7: Process each input feature</span></span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a>                            <span class="cf">for</span> in_channel <span class="kw">in</span> <span class="bu">range</span>(</span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a>                                num_input_channels</span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a>                            ):</span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a>                                <span class="co"># Get input value from correct window position</span></span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a>                                in_y <span class="op">=</span> y <span class="op">+</span> ky</span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a>                                in_x <span class="op">=</span> x <span class="op">+</span> kx</span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a>                                <span class="co"># Perform multiply-accumulate operation</span></span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a>                                result <span class="op">+=</span> (</span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a>                                    <span class="bu">input</span>[</span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a>                                        image, in_y, in_x, in_channel</span>
<span id="cb4-28"><a href="#cb4-28" aria-hidden="true" tabindex="-1"></a>                                    ]</span>
<span id="cb4-29"><a href="#cb4-29" aria-hidden="true" tabindex="-1"></a>                                    <span class="op">*</span> kernel[</span>
<span id="cb4-30"><a href="#cb4-30" aria-hidden="true" tabindex="-1"></a>                                        ky,</span>
<span id="cb4-31"><a href="#cb4-31" aria-hidden="true" tabindex="-1"></a>                                        kx,</span>
<span id="cb4-32"><a href="#cb4-32" aria-hidden="true" tabindex="-1"></a>                                        in_channel,</span>
<span id="cb4-33"><a href="#cb4-33" aria-hidden="true" tabindex="-1"></a>                                        out_channel,</span>
<span id="cb4-34"><a href="#cb4-34" aria-hidden="true" tabindex="-1"></a>                                    ]</span>
<span id="cb4-35"><a href="#cb4-35" aria-hidden="true" tabindex="-1"></a>                                )</span>
<span id="cb4-36"><a href="#cb4-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-37"><a href="#cb4-37" aria-hidden="true" tabindex="-1"></a>                    <span class="co"># Store result for this output position</span></span>
<span id="cb4-38"><a href="#cb4-38" aria-hidden="true" tabindex="-1"></a>                    output[image, y, x, out_channel] <span class="op">=</span> result</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</figure>
</div>
<p>The seven nested loops reveal different aspects of the computation:</p>
<ul>
<li>Outer loops (1-3) manage position: which image and where in the image</li>
<li>Middle loop (4) handles output features: computing different learned patterns</li>
<li>Inner loops (5-7) perform the actual convolution: sliding the kernel window</li>
</ul>
<p>Examining this process in detail, the outer two loops (<code>for y</code> and <code>for x</code>) traverse each spatial position in the output feature map (for the MNIST example, this traverses all <span class="math inline">\(28\times 28\)</span> positions). At each position, values are computed for each output channel (<code>for k</code> loop), representing different learned features or patternsâ€”the 32 different feature detectors.</p>
<p>The inner three loops implement the actual convolution operation at each position. For each output value, we process a local <span class="math inline">\(3\times 3\)</span> region of the input (the <code>dy</code> and <code>dx</code> loops) across all input channels (<code>for c</code> loop). This creates a sliding window effect, where the same <span class="math inline">\(3\times 3\)</span> filter moves across the image, performing multiply-accumulates between the filter weights and the local input values. Unlike the MLPâ€™s global connectivity, this local processing pattern means each output value depends only on a small neighborhood of the input.</p>
<p>For our MNIST example with <span class="math inline">\(3\times 3\)</span> filters and 32 output channels, each output position requires only 9 multiply-accumulate operations per input channel, compared to the 784 operations needed in our MLP layer. This operation must be repeated for every spatial position <span class="math inline">\((28\times 28)\)</span> and every output channel (32).</p>
<p>While using fewer operations per output, the spatial structure creates different patterns of memory access and computation that systems must handle. These patterns influence system design, creating both challenges and opportunities for optimization.</p>
</section>
<section id="sec-dnn-architectures-system-implications-f25d" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-dnn-architectures-system-implications-f25d">System Implications</h3>
<p>CNNs exhibit distinctive system-level patterns that differ significantly from MLP dense connectivity across all three analysis dimensions.</p>
<section id="sec-dnn-architectures-memory-requirements-7e2b" class="level4">
<h4 class="anchored" data-anchor-id="sec-dnn-architectures-memory-requirements-7e2b">Memory Requirements</h4>
<p>For convolutional layers, memory requirements center around two key components: filter weights and feature maps. Unlike MLPs that require storing full connection matrices, CNNs use small, reusable filters. For a typical CNN processing 224Ã—224 ImageNet images, a convolutional layer with 64 filters of size <span class="math inline">\(3\times 3\)</span> requires storing only 576 weight parameters <span class="math inline">\((3\times 3\times 64)\)</span>, dramatically less than the millions of weights needed for equivalent fully-connected processing. The system must store feature maps for all spatial positions, creating a different memory demand. A 224Ã—224 input with 64 output channels requires storing 3.2 million activation values (224Ã—224Ã—64).</p>
<p>These memory access patterns suggest opportunities for optimization through weight reuse and careful feature map management. Processors optimize these spatial patterns by caching filter weights for reuse across positions while streaming feature map data. Frameworks implement spatial optimizations through specialized memory layouts that enable filter reuse and spatial locality in feature map access. CPUs and GPUs approach this differently. CPUs use their cache hierarchy to keep frequently used filters resident, while GPUs employ specialized memory architectures designed for the spatial access patterns of image processing. The detailed architecture design principles for these specialized processors are covered in <strong><a href="../hw_acceleration/hw_acceleration.html#sec-ai-acceleration">Chapter 11: AI Acceleration</a></strong>.</p>
</section>
<section id="sec-dnn-architectures-computation-needs-22a5" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="sec-dnn-architectures-computation-needs-22a5">Computation Needs</h4>
<p>The core computation in CNNs involves repeatedly applying small filters across spatial positions. Each output value requires a local multiply-accumulate operation over the filter region. For ImageNet processing with <span class="math inline">\(3\times 3\)</span> filters and 64 output channels, computing one spatial position involves 576 multiply-accumulates <span class="math inline">\((3\times 3\times 64)\)</span>, and this must be repeated for all 50,176 spatial positions (224Ã—224). While each individual computation involves fewer operations than an MLP layer, the total computational load remains large due to spatial repetition.</p>
<p>This computational pattern presents different optimization opportunities than MLPs. The regular, repeated nature of convolution operations enables efficient hardware utilization through structured parallelism. Modern processors exploit this pattern in various ways. CPUs leverage SIMD instructions<a href="#fn16" class="footnote-ref" id="fnref16" role="doc-noteref"><sup>16</sup></a> to process multiple filter positions simultaneously, while GPUs parallelize computation across spatial positions and channels. The model optimization techniques that further reduce these computational demands, including specialized convolution optimizations and sparsity patterns, are detailed in <strong><a href="../optimizations/optimizations.html#sec-model-optimizations">Chapter 10: Model Optimizations</a></strong>.</p>
<div class="no-row-height column-margin column-container"><div id="fn16"><p><sup>16</sup>&nbsp;<strong>SIMD (Single Instruction, Multiple Data)</strong>: CPU instructions that perform the same operation on multiple data elements simultaneously. Modern x86 processors support AVX-512, enabling 16 single-precision operations per instruction, a 16x speedup over scalar code. SIMD is important for efficient neural network inference on CPUs, especially for edge deployment. Deep learning frameworks further optimize this through specialized convolution algorithms that transform the computation to better match hardware capabilities.</p></div></div></section>
<section id="sec-dnn-architectures-data-movement-9a0e" class="level4">
<h4 class="anchored" data-anchor-id="sec-dnn-architectures-data-movement-9a0e">Data Movement</h4>
<p>The sliding window pattern of convolutions creates a distinctive data movement profile. Unlike MLPs where each weight is used once per forward pass, CNN filter weights are reused many times as the filter slides across spatial positions. For ImageNet processing, each <span class="math inline">\(3\times 3\)</span> filter weight is reused 50,176 times (once for each position in the 224Ã—224 feature map). This creates a different challenge: the system must stream input features through the computation unit while keeping filter weights stable.</p>
<p>The predictable spatial access pattern enables strategic data movement optimizations. Different architectures handle this movement pattern through specialized mechanisms. CPUs maintain frequently used filter weights in cache while streaming through input features. GPUs employ memory architectures optimized for spatial locality and provide hardware support for efficient sliding window operations. Deep learning frameworks orchestrate these movements by organizing computations to maximize filter weight reuse and minimize redundant feature map accesses.</p>
<div id="quiz-question-sec-dnn-architectures-cnns-spatial-pattern-processing-f8ff" class="callout callout-quiz-question">
<details class="callout-quiz-question fbx-default closebutton"><summary><strong>Self-Check: Question 1.3</strong></summary><div>
<ol type="1">
<li><p>Which architectural feature of CNNs allows them to efficiently process spatially structured data?</p>
<ol type="a">
<li>Global connectivity</li>
<li>Parameter sharing</li>
<li>Both B and D</li>
<li>Local connectivity</li>
</ol></li>
<li><p>Explain how parameter sharing in CNNs contributes to computational efficiency compared to MLPs.</p></li>
<li><p>In CNNs, the ability to detect features regardless of their spatial position is known as ____.</p></li>
<li><p>Order the following CNN operations as they occur in a typical layer: (1) Apply filter, (2) Activation function, (3) Bias addition.</p></li>
<li><p>In a production system, how might the architectural characteristics of CNNs influence hardware design decisions?</p></li>
</ol>
<p><a href="#quiz-answer-sec-dnn-architectures-cnns-spatial-pattern-processing-f8ff" class="question-label">See Answers â†’</a></p>
</div></details>
</div>
</section>
</section>
</section>
<section id="sec-dnn-architectures-rnns-sequential-pattern-processing-ea14" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="sec-dnn-architectures-rnns-sequential-pattern-processing-ea14">RNNs: Sequential Pattern Processing</h2>
<p>Convolutional Neural Networks achieved efficiency gains by exploiting spatial locality, yet their architectural assumptions fail when patterns depend on temporal order rather than spatial proximity. While CNNs excel at recognizing "what" is present in data through shared feature detectors, they cannot capture "when" events occur or how they relate across time. This limitation manifests in domains such as natural language processing, where word meaning depends on sentential context, and time-series analysis, where future values depend on historical patterns.</p>
<p>Sequential data presents a challenge distinct from spatial processing: patterns can span arbitrary temporal distances, rendering fixed-size kernels ineffective. While spatial convolution leverages the principle that nearby pixels are typically related, temporal relationships operate differently. Important connections may span hundreds or thousands of time steps with no correlation to proximity. Traditional feedforward architectures, including CNNs, process each input independently and cannot maintain the temporal context necessary for these long-range dependencies.</p>
<p>Recurrent Neural Networks address this architectural limitation <span class="citation" data-cites="elman1990finding hochreiter1997long">(<a href="#ref-elman1990finding" role="doc-biblioref">Elman 1990</a>; <a href="#ref-hochreiter1997long" role="doc-biblioref">Hochreiter and Schmidhuber 1997</a>)</span> by embodying a temporal inductive bias: they assume sequential dependence, where the order of information matters and the past influences the present. This architectural assumption guides the introduction of memory as a component of the computational model. Rather than processing inputs in isolation, RNNs maintain an internal state that propagates information from previous time steps, enabling the network to condition its current output on historical context. This architecture embodies another trade-off: while CNNs sacrifice theoretical generality for spatial efficiency, RNNs introduce computational dependencies that challenge parallel execution in exchange for temporal processing capabilities.</p>
<div class="no-row-height column-margin column-container"></div><div id="callout-definition*-1.4" class="callout callout-definition" title="Recurrent Neural Networks">
<p></p><details class="callout-definition fbx-default closebutton" open=""><summary><strong>Definition: </strong>Recurrent Neural Networks</summary><div><strong><em>Recurrent Neural Networks (RNNs)</em></strong> are sequential neural architectures that maintain <em>internal memory state</em> across time steps through <em>recurrent connections</em>, enabling <em>variable-length sequence processing</em> at the cost of <em>sequential computation</em> that prevents parallelization.<p></p>
</div></details>
</div>
<div class="callout callout-style-default callout-note callout-titled" title="Coverage Note">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Coverage Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>This section covers of RNNs, emphasizing their core contributions to sequential processing and the architectural principles that influenced modern attention mechanisms. While RNNs introduced critical conceptsâ€”memory states, temporal dependencies, and sequential computationâ€”contemporary practice increasingly favors attention-based architectures for sequence modeling. We focus on foundational principles rather than extensive implementation variants, dedicating significant depth to the attention mechanisms and Transformers (<a href="#sec-dnn-architectures-attention-mechanisms-dynamic-pattern-processing-566d" class="quarto-xref">Section&nbsp;1.5</a>) that have largely superseded RNNs in production systems while building directly on the insights gained from recurrent architectures.</p>
</div>
</div>
<section id="sec-dnn-architectures-pattern-processing-needs-c18e" class="level3">
<h3 class="anchored" data-anchor-id="sec-dnn-architectures-pattern-processing-needs-c18e">Pattern Processing Needs</h3>
<p>Sequential pattern processing addresses scenarios where current input interpretation depends on preceding information. In natural language processing, word meaning often depends heavily on previous words in the sentence. Context determines interpretation, as evidenced by the varying meanings of words based on surrounding terms. Similarly, in speech recognition, phoneme interpretation depends on surrounding sounds, while financial forecasting requires understanding historical data patterns.</p>
<p>The challenge in sequential processing lies in maintaining and updating relevant context over time. Human text comprehension does not restart with each word; rather, a running understanding evolves as new information is processed. Similarly, time-series data processing encounters patterns spanning different timescales, from immediate dependencies to long-term trends. This necessitates an architecture capable of both maintaining state over time and updating it based on new inputs.</p>
<p>These requirements translate into specific architectural demands: the system must maintain internal state to capture temporal context, update this state based on new inputs, and learn which historical information is relevant for current predictions. Unlike MLPs and CNNs, which process fixed-size inputs, sequential processing must accommodate variable-length sequences while maintaining computational efficiency. These requirements culminate in the recurrent neural network (RNN) architecture.</p>
</section>
<section id="sec-dnn-architectures-algorithmic-structure-279b" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-dnn-architectures-algorithmic-structure-279b">Algorithmic Structure</h3>
<p>RNNs address sequential processing through recurrent connections, distinguishing them from MLPs and CNNs. Rather than merely mapping inputs to outputs, RNNs maintain an internal state updated at each time step, creating a memory mechanism that propagates information forward in time. This temporal dependency modeling capability was first explored by <span class="citation" data-cites="elman1990finding">Elman (<a href="#ref-elman1990finding" role="doc-biblioref">1990</a>)</span>, who demonstrated RNN capacity to identify structure in time-dependent data. Basic RNNs suffer from the vanishing gradient problem<a href="#fn17" class="footnote-ref" id="fnref17" role="doc-noteref"><sup>17</sup></a>, constraining their ability to learn long-term dependencies.</p>
<div class="no-row-height column-margin column-container"><div id="fn17"><p><sup>17</sup>&nbsp;<strong>Vanishing Gradient Problem</strong>: During backpropagation through time, gradients shrink exponentially as they propagate backward through RNN layers. When recurrent weights have magnitude &lt; 1, gradients multiply by values &lt; 1 at each time step, vanishing after 5-10 steps and preventing learning of long-term dependenciesâ€”a key limitation solved by LSTMs and attention mechanisms.</p></div></div><p>The core operation in a basic RNN can be expressed mathematically as: <span class="math display">\[
\mathbf{h}_t = f(\mathbf{W}_{hh}\mathbf{h}_{t-1} + \mathbf{W}_{xh}\mathbf{x}_t + \mathbf{b}_h)
\]</span> where <span class="math inline">\(\mathbf{h}_t\)</span> denotes the hidden state at time <span class="math inline">\(t\)</span>, <span class="math inline">\(\mathbf{x}_t\)</span> denotes the input at time <span class="math inline">\(t\)</span>, <span class="math inline">\(\mathbf{W}_{hh}\)</span> contains the recurrent weights, and <span class="math inline">\(\mathbf{W}_{xh}\)</span> contains the input weights, as illustrated in the unfolded network structure in <a href="#fig-rnn" class="quarto-xref">Figure&nbsp;4</a>.</p>
<p>In word sequence processing, each word may be represented as a 100-dimensional vector (<span class="math inline">\(\mathbf{x}_t\)</span>), with a hidden state of 128 dimensions (<span class="math inline">\(\mathbf{h}_t\)</span>). At each time step, the network combines the current input with its previous state to update its sequential understanding, establishing a memory mechanism capable of capturing patterns across time steps.</p>
<p>This recurrent structure fulfills sequential processing requirements through connections that maintain internal state and propagate information forward in time. Rather than processing all inputs independently, RNNs process sequential data by iteratively updating a hidden state based on the current input and the previous hidden state, as depicted in <a href="#fig-rnn" class="quarto-xref">Figure&nbsp;4</a>. This architecture suits tasks including language modeling, speech recognition, and time-series forecasting.</p>
<p>RNNs implement a recursive algorithm where each time stepâ€™s function call depends on the result of the previous call. Analogous to recursive functions that maintain state through the call stack, RNNs maintain state through their hidden vectors. The mathematical formula <span class="math inline">\(\mathbf{h}_t = f(\mathbf{h}_{t-1}, \mathbf{x}_t)\)</span> directly parallels recursive function definitions where <code>f(n) = g(f(n-1), input(n))</code>. This correspondence explains RNN capacity to handle variable-length sequences: just as recursive algorithms process lists of arbitrary length by applying the same function recursively, RNNs process sequences of any length by applying the same recurrent computation.</p>
<section id="sec-dnn-architectures-efficiency-optimization-ce92" class="level4">
<h4 class="anchored" data-anchor-id="sec-dnn-architectures-efficiency-optimization-ce92">Efficiency and Optimization</h4>
<p>Sequential processing creates computational bottlenecks but enables unique efficiency characteristics for memory usage. RNNs achieve constant memory overhead for hidden state storage regardless of sequence length, making them extremely memory-efficient for long sequences. While Transformers require O(nÂ²) memory for sequence length n, RNNs maintain fixed memory usage, enabling processing of sequences thousands of steps long on modest hardware.</p>
<p>Structured pruning of hidden-to-hidden connections can achieve 10x speedup while maintaining sequence modeling capability. The recurrent weight matrix <span class="math inline">\(W_{hh}\)</span> typically dominates parameter count for large hidden states, but magnitude-based pruning reveals that 70-80% of these connections contribute minimally to temporal dependencies. Block-structured pruning maintains computational efficiency while enabling significant model compression.</p>
<p>Sequential operations accumulate quantization errors, requiring careful quantization point placement and gradient scaling for stable low-precision training. Unlike feedforward networks where quantization errors remain localized, RNN errors propagate through time, making INT8 quantization more challenging. Per-timestep quantization schemes and careful handling of hidden state precision are required for maintaining accuracy in quantized RNN deployments.</p>
<div id="fig-rnn" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-env="figure" data-fig-pos="htb">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-rnn-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="00d28078c95f0324945a05b54b6eed89ff2acb28.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-4" title="Figure&nbsp;4: Recurrent Neural Network Unfolding: Rnns process sequential data by maintaining a hidden state that incorporates information from previous time steps through this diagram. the unfolded structure explicitly represents the temporal dependencies modeled by the recurrent weights, enabling the network to learn patterns across variable-length sequences."><img src="dnn_architectures_files/mediabag/00d28078c95f0324945a05b54b6eed89ff2acb28.svg" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-rnn-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4: <strong>Recurrent Neural Network Unfolding</strong>: Rnns process sequential data by maintaining a hidden state that incorporates information from previous time steps through this diagram. the unfolded structure explicitly represents the temporal dependencies modeled by the recurrent weights, enabling the network to learn patterns across variable-length sequences.
</figcaption>
</figure>
</div>
</section>
</section>
<section id="sec-dnn-architectures-computational-mapping-0096" class="level3">
<h3 class="anchored" data-anchor-id="sec-dnn-architectures-computational-mapping-0096">Computational Mapping</h3>
<p>RNN sequential processing creates computational patterns different from both MLPs and CNNs, extending the architectural diversity discussed in <a href="#sec-dnn-architectures-architectural-principles-engineering-tradeoffs-89de" class="quarto-xref">Section&nbsp;1.1</a>. This implementation approach shows temporal dependencies translating into specific computational requirements.</p>
<p>As shown in <a href="#lst-rnn_layer_step" class="quarto-xref">Listing&nbsp;5</a>, the <code>rnn_layer_step</code> function shows the operation using high-level matrix operations found in deep learning frameworks. It handles a single time step, taking the current input <code>x_t</code> and previous hidden state <code>h_prev</code>, along with two weight matrices: <code>W_hh</code> for hidden-to-hidden connections and <code>W_xh</code> for input-to-hidden connections. Through matrix multiplication operations (<code>matmul</code>), it merges the previous state and current input to generate the next hidden state.</p>
<div id="lst-rnn_layer_step" class="listing quarto-float quarto-figure quarto-figure-left anchored">
<figure class="quarto-float quarto-float-lst figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-lst" id="lst-rnn_layer_step-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Listing&nbsp;5: <strong>RNN Layer Step</strong>: Neural networks process sequential data through transformations that integrate current inputs and past states.
</figcaption>
<div aria-describedby="lst-rnn_layer_step-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="sourceCode" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> rnn_layer_step(x_t, h_prev, W_hh, W_xh, b):</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># x_t: input at time t (batch_size Ã— input_dim)</span></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>    <span class="co"># h_prev: previous hidden state (batch_size Ã— hidden_dim)</span></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># W_hh: recurrent weights (hidden_dim Ã— hidden_dim)</span></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>    <span class="co"># W_xh: input weights (input_dim Ã— hidden_dim)</span></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>    h_t <span class="op">=</span> activation(matmul(h_prev, W_hh) <span class="op">+</span> matmul(x_t, W_xh) <span class="op">+</span> b)</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> h_t</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</figure>
</div>
<p>Understanding RNN system implications requires examining how the elegant mathematical abstraction translates into hardware execution patterns. The simple recurrence relation <code>h_t = tanh(W_hh h_{t-1} + W_xh x_t + b)</code> conceals a computational structure that creates unique challenges: sequential dependencies that prevent parallelization, memory access patterns that differ from feedforward networks, and state management requirements that affect system design.</p>
<p>The detailed implementation (<a href="#lst-rnn_layer_compute" class="quarto-xref">Listing&nbsp;6</a>) reveals the computational reality beneath the mathematical abstraction. The nested loop structure exposes how sequential processing creates both limitations and opportunities in system optimization.</p>
<div id="lst-rnn_layer_compute" class="listing quarto-float quarto-figure quarto-figure-left anchored">
<figure class="quarto-float quarto-float-lst figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-lst" id="lst-rnn_layer_compute-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Listing&nbsp;6: <strong>Recurrent Layer Computation</strong>: Computes the hidden state at each time step through sequential transformations involving previous states and current inputs.
</figcaption>
<div aria-describedby="lst-rnn_layer_compute-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="sourceCode" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> rnn_layer_compute(x_t, h_prev, W_hh, W_xh, b):</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Initialize next hidden state</span></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>    h_t <span class="op">=</span> np.zeros_like(h_prev)</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Loop 1: Process each sequence in the batch</span></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> batch <span class="kw">in</span> <span class="bu">range</span>(batch_size):</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Loop 2: Compute recurrent contribution</span></span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>        <span class="co"># (h_prev Ã— W_hh)</span></span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(hidden_dim):</span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(hidden_dim):</span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>                h_t[batch, i] <span class="op">+=</span> h_prev[batch, j] <span class="op">*</span> W_hh[j, i]</span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Loop 3: Compute input contribution (x_t Ã— W_xh)</span></span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(hidden_dim):</span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(input_dim):</span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a>                h_t[batch, i] <span class="op">+=</span> x_t[batch, j] <span class="op">*</span> W_xh[j, i]</span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Loop 4: Add bias and apply activation</span></span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(hidden_dim):</span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a>            h_t[batch, i] <span class="op">=</span> activation(h_t[batch, i] <span class="op">+</span> b[i])</span>
<span id="cb6-21"><a href="#cb6-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-22"><a href="#cb6-22" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> h_t</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</figure>
</div>
<p>The nested loops in <code>rnn_layer_compute</code> expose the core computational pattern of RNNs (see <a href="#lst-rnn_layer_compute" class="quarto-xref">Listing&nbsp;6</a>). Loop 1 processes each sequence in the batch independently, allowing for batch-level parallelism. Within each batch item, Loop 2 computes how the previous hidden state influences the next state through the recurrent weights <code>W_hh</code>. Loop 3 then incorporates new information from the current input through the input weights <code>W_xh</code>. Finally, Loop 4 adds biases and applies the activation function to produce the new hidden state.</p>
<p>For a sequence processing task with input dimension 100 and hidden state dimension 128, each time step requires two matrix multiplications: one <span class="math inline">\(128\times 128\)</span> for the recurrent connection and one <span class="math inline">\(100\times 128\)</span> for the input projection. While individual time steps can process in parallel across batch elements, the time steps themselves must process sequentially. This creates a unique computational pattern that systems must handle.</p>
</section>
<section id="sec-dnn-architectures-system-implications-ecf5" class="level3">
<h3 class="anchored" data-anchor-id="sec-dnn-architectures-system-implications-ecf5">System Implications</h3>
<p>Following the analytical framework established for MLPs, RNNs exhibit distinctive patterns in memory requirements, computation needs, and data movement that differ significantly from both dense and spatial processing architectures.</p>
<section id="sec-dnn-architectures-memory-requirements-eb37" class="level4">
<h4 class="anchored" data-anchor-id="sec-dnn-architectures-memory-requirements-eb37">Memory Requirements</h4>
<p>RNNs require storing two sets of weights (input-to-hidden and hidden-to-hidden) along with the hidden state. For the example with input dimension 100 and hidden state dimension 128, this requires storing 12,800 weights for input projection <span class="math inline">\((100\times 128)\)</span> and 16,384 weights for recurrent connections <span class="math inline">\((128\times 128)\)</span>. Unlike CNNs where weights are reused across spatial positions, RNN weights are reused across time steps. The system must maintain the hidden state, which constitutes a key factor in memory usage and access patterns.</p>
<p>These memory access patterns create a different profile from MLPs and CNNs. Processors optimize sequential patterns by maintaining weight matrices in cache while streaming through temporal elements. Frameworks optimize temporal processing by batching sequences and managing hidden state storage between time steps. CPUs and GPUs approach this through different strategies; CPUs leverage their cache hierarchy for weight reuse; meanwhile, GPUs use specialized memory architectures designed for maintaining state across sequential operations. The specialized hardware optimizations for sequential processing, including memory banking and pipeline architectures, are detailed in <strong><a href="../hw_acceleration/hw_acceleration.html#sec-ai-acceleration">Chapter 11: AI Acceleration</a></strong>.</p>
</section>
<section id="sec-dnn-architectures-computation-needs-29be" class="level4">
<h4 class="anchored" data-anchor-id="sec-dnn-architectures-computation-needs-29be">Computation Needs</h4>
<p>The core computation in RNNs involves repeatedly applying weight matrices across time steps. For each time step, we perform two matrix multiplications: one with the input weights and one with the recurrent weights. In our example, processing a single time step requires 12,800 multiply-accumulates for the input projection <span class="math inline">\((100\times 128)\)</span> and 16,384 multiply-accumulates for the recurrent connection <span class="math inline">\((128\times 128)\)</span>.</p>
<p>This computational pattern differs from both MLPs and CNNs in a key way: while we can parallelize across batch elements, we cannot parallelize across time steps due to the sequential dependency. Each time step must wait for the previous stepâ€™s hidden state before it can begin computation. This creates a tension between the inherent sequential nature of the algorithm and the desire for parallel execution in modern hardware.</p>
<p>Processors address sequential constraints through specialized approaches. CPUs pipeline operations within time steps while maintaining temporal ordering. GPUs batch multiple sequences together to maintain high throughput despite sequential dependencies. Software frameworks optimize this further by techniques like sequence packing and unrolling computations across multiple time steps when possible, enabling more efficient utilization of parallel processing resources while respecting the sequential constraints inherent in recurrent architectures.</p>
</section>
<section id="sec-dnn-architectures-data-movement-8591" class="level4">
<h4 class="anchored" data-anchor-id="sec-dnn-architectures-data-movement-8591">Data Movement</h4>
<p>The sequential processing in RNNs creates a distinctive data movement pattern that differs from both MLPs and CNNs. While MLPs need each weight only once per forward pass and CNNs reuse weights across spatial positions, RNNs reuse their weights across time steps while requiring careful management of the hidden state data flow.</p>
<p>For our example with a 128-dimensional hidden state, each time step must: load the previous hidden state (128 values), access both weight matrices (29,184 total weights from both input and recurrent connections), and store the new hidden state (128 values). This pattern repeats for every element in the sequence. Unlike CNNs where we can predict and prefetch data based on spatial patterns, RNN data movement is driven by temporal dependencies.</p>
<p>Different architectures handle this sequential data movement through specialized mechanisms. CPUs maintain weight matrices in cache while streaming through sequence elements and managing hidden state updates. GPUs employ memory architectures optimized for maintaining state information across sequential operations while processing multiple sequences in parallel. Deep learning frameworks orchestrate these movements by managing data transfers between time steps and optimizing batch operations.</p>
<p>While RNNs established concepts for sequential processing, their architectural constraints create bottlenecks: sequential dependencies prevent parallelization across time steps, fixed-capacity hidden states create information bottlenecks for long sequences, and temporal proximity assumptions break down when important relationships span distant positions. These limitations motivated the development of attention mechanisms, which eliminate sequential processing constraints through dynamic, content-dependent connectivity. The following section examines how attention mechanisms address each of these RNN limitations while introducing new computational challenges. This extensive treatment reflects attention mechanismsâ€™ dominance in modern ML systems and their fundamental reimagining of sequential pattern processing.</p>
<div id="quiz-question-sec-dnn-architectures-rnns-sequential-pattern-processing-ea14" class="callout callout-quiz-question">
<details class="callout-quiz-question fbx-default closebutton"><summary><strong>Self-Check: Question 1.4</strong></summary><div>
<ol type="1">
<li><p>What is the primary advantage of RNNs over CNNs when processing sequential data?</p>
<ol type="a">
<li>RNNs maintain an internal state to capture temporal dependencies.</li>
<li>RNNs can process data in parallel across time steps.</li>
<li>RNNs are more efficient in terms of memory usage than CNNs.</li>
<li>RNNs use fixed-size kernels to capture patterns.</li>
</ol></li>
<li><p>Explain how RNNs handle long-term dependencies in sequential data and discuss one limitation related to this capability.</p></li>
<li><p>The phenomenon where gradients shrink exponentially as they propagate backward through RNN layers is known as the ____. This limits the ability of RNNs to learn long-term dependencies.</p></li>
<li><p>In a production system, how might the sequential processing nature of RNNs influence hardware design and optimization strategies?</p></li>
</ol>
<p><a href="#quiz-answer-sec-dnn-architectures-rnns-sequential-pattern-processing-ea14" class="question-label">See Answers â†’</a></p>
</div></details>
</div>
</section>
</section>
</section>
<section id="sec-dnn-architectures-attention-mechanisms-dynamic-pattern-processing-566d" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="sec-dnn-architectures-attention-mechanisms-dynamic-pattern-processing-566d">Attention Mechanisms: Dynamic Pattern Processing</h2>
<p>Recurrent Neural Networks successfully introduced memory to handle sequential dependencies, but their fixed sequential processing creates limitations. RNNs process information in temporal order, making it difficult to capture relationships between distant elements and impossible to parallelize computation across sequence positions. More critically, RNNs assume that temporal proximity correlates with importanceâ€”that nearby words or time steps are more relevant than distant ones. This assumption breaks down in many real-world scenarios.</p>
<p>Consider the sentence "The cat, which was sitting by the window overlooking the garden, was sleeping." Here, "cat" and "sleeping" are separated by multiple intervening words, yet they form the core subject-predicate relationship. RNN architectures would process all the intervening elements sequentially, potentially losing this crucial connection in their fixed-capacity hidden state. This limitation revealed the need for architectures that could identify and weight relationships based on content rather than position.</p>
<p>Attention mechanisms emerged as the solution to this architectural constraint <span class="citation" data-cites="bahdanau2014neural">(<a href="#ref-bahdanau2014neural" role="doc-biblioref">Bahdanau, Cho, and Bengio 2014</a>)</span> by introducing dynamic connectivity patterns that adapt based on input content. Rather than processing elements in predetermined order with fixed relationships, attention mechanisms compute the relevance between all pairs of elements and weight their interactions accordingly. This represents a shift from structural constraints to learned, data-dependent processing patterns.</p>
<div class="no-row-height column-margin column-container"></div><div id="callout-definition*-1.5" class="callout callout-definition" title="Attention Mechanisms">
<p></p><details class="callout-definition fbx-default closebutton" open=""><summary><strong>Definition: </strong>Attention Mechanisms</summary><div><strong><em>Attention Mechanisms</em></strong> are neural components that compute <em>content-dependent relationships</em> between sequence elements through <em>query-key-value operations</em>, enabling <em>selective focus</em> on relevant information and <em>long-range dependencies</em> without positional constraints.<p></p>
</div></details>
</div>
<p>While attention mechanisms were initially used as components within recurrent architectures, the Transformer architecture <span class="citation" data-cites="vaswani2017attention">(<a href="#ref-vaswani2017attention" role="doc-biblioref">Vaswani et al. 2017</a>)</span> demonstrated that attention alone could entirely replace sequential processing, creating a new architectural paradigm.</p>
<div class="no-row-height column-margin column-container"></div><div id="callout-definition*-1.6" class="callout callout-definition" title="Transformers">
<p></p><details class="callout-definition fbx-default closebutton" open=""><summary><strong>Definition: </strong>Transformers</summary><div><strong><em>Transformers</em></strong> are neural architectures based entirely on <em>attention mechanisms</em>, using <em>multi-head self-attention</em> and <em>position encodings</em> to process sequences in <em>parallel</em> rather than sequentially, enabling efficient training and inference at scale.<p></p>
</div></details>
</div>
<section id="sec-dnn-architectures-pattern-processing-needs-b5e0" class="level3">
<h3 class="anchored" data-anchor-id="sec-dnn-architectures-pattern-processing-needs-b5e0">Pattern Processing Needs</h3>
<p>Dynamic pattern processing addresses scenarios where relationships between elements are not fixed by architecture but instead emerge from content. Language translation exemplifies this challenge: when translating â€œthe bank by the river,â€ understanding â€œbankâ€ requires attending to â€œriver,â€ but in â€œthe bank approved the loan,â€ the important relationship is with â€œapprovedâ€ and â€œloan.â€ Unlike RNNs that process information sequentially or CNNs that use fixed spatial patterns, an architecture is required that can dynamically determine which relationships matter.</p>
<p>Expanding beyond language, this requirement for dynamic processing appears across many domains. In protein structure prediction, interactions between amino acids depend on their chemical properties and spatial arrangements. In graph analysis, node relationships vary based on graph structure and node features. In document analysis, connections between different sections depend on semantic content rather than just proximity.</p>
<p>Synthesizing these requirements, dynamic processing demands specific capabilities from our processing architecture. The system must compute relationships between all pairs of elements, weigh these relationships based on content, and use these weights to selectively combine information. Unlike previous architectures with fixed connectivity patterns, dynamic processing requires the flexibility to modify its computation graph based on the input itself. These capabilities naturally lead us to the attention mechanism, which serves as the foundation for the Transformer architecture examined in detail in the following sections. <a href="#fig-transformer-attention-visualized" class="quarto-xref">Figure&nbsp;5</a> shows attention enabling this dynamic information flow.</p>
<div id="fig-transformer-attention-visualized" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-env="figure" data-fig-pos="htb">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-transformer-attention-visualized-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="0ac6a22c6278235896e8ca59ca4a0d5274e79296.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-5" title="Figure&nbsp;5: Attention Weights: Transformer attention mechanisms dynamically assess relationships between subwords, assigning higher weights to more relevant connections within a sequence and enabling the model to focus on key information. These learned weights, visualized as connection strengths, reveal how the model attends to different parts of the input when processing language."><img src="dnn_architectures_files/mediabag/0ac6a22c6278235896e8ca59ca4a0d5274e79296.svg" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-transformer-attention-visualized-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5: <strong>Attention Weights</strong>: Transformer attention mechanisms dynamically assess relationships between subwords, assigning higher weights to more relevant connections within a sequence and enabling the model to focus on key information. These learned weights, visualized as connection strengths, reveal how the model attends to different parts of the input when processing language.
</figcaption>
</figure>
</div>
</section>
<section id="sec-dnn-architectures-basic-attention-mechanism-9500" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-dnn-architectures-basic-attention-mechanism-9500">Basic Attention Mechanism</h3>
<p>Attention mechanisms represent a shift from fixed architectural connections to dynamic, content-based interactions between sequence elements. This section explores the mathematical foundations of attention, examining how query-key-value operations enable flexible pattern processing. We analyze the computational requirements, memory access patterns, and system implications that make attention both powerful and computationally demanding.</p>
<section id="sec-dnn-architectures-algorithmic-structure-1af4" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="sec-dnn-architectures-algorithmic-structure-1af4">Algorithmic Structure</h4>
<p>Attention mechanisms form the foundation of dynamic pattern processing by computing weighted connections between elements based on their content <span class="citation" data-cites="bahdanau2014neural">(<a href="#ref-bahdanau2014neural" role="doc-biblioref">Bahdanau, Cho, and Bengio 2014</a>)</span>. This approach processes relationships that are not fixed by architecture but instead emerge from the data itself. At the core of an attention mechanism lies an operation that can be expressed mathematically as:</p>
<div class="no-row-height column-margin column-container"></div><p><span class="math display">\[
\text{Attention}(\mathbf{Q}, \mathbf{K}, \mathbf{V}) = \text{softmax}
\left(\frac{\mathbf{Q}\mathbf{K}^T}{\sqrt{d_k}}\right)\mathbf{V}
\]</span></p>
<p>This equation shows scaled dot-product attention. <span class="math inline">\(\mathbf{Q}\)</span> (queries) and <span class="math inline">\(\mathbf{K}\)</span> (keys) are matrix-multiplied to compute similarity scores, divided by <span class="math inline">\(\sqrt{d_k}\)</span> (key dimension) for numerical stability, then normalized with softmax<a href="#fn18" class="footnote-ref" id="fnref18" role="doc-noteref"><sup>18</sup></a> to get attention weights. These weights are applied to <span class="math inline">\(\mathbf{V}\)</span> (values) to produce the output. The result is a weighted combination where each position receives information from all relevant positions based on content similarity.</p>
<div class="no-row-height column-margin column-container"><div id="fn18"><p><sup>18</sup>&nbsp;<strong>Softmax Function</strong>: Converts a vector of real numbers into a probability distribution where all values sum to 1. Defined as <span class="math inline">\(\text{softmax}(x_i) = \frac{e^{x_i}}{\sum_j e^{x_j}}\)</span>, softmax amplifies differences between inputs (larger values get disproportionately higher probabilities) while ensuring valid attention weights for combining information sources.</p></div><div id="fn19"><p><sup>19</sup>&nbsp;<strong>Query-Key-Value Attention</strong>: Inspired by information retrieval systems where queries search through keys to retrieve values. In neural attention, queries and keys compute similarity scores (like a search engine matching queries to documents), while values contain the actual information to retrieveâ€”a design that enables flexible, content-based information access.</p></div></div><p>In this equation, <span class="math inline">\(\mathbf{Q}\)</span> (queries), <span class="math inline">\(\mathbf{K}\)</span> (keys), and <span class="math inline">\(\mathbf{V}\)</span> (values)<a href="#fn19" class="footnote-ref" id="fnref19" role="doc-noteref"><sup>19</sup></a> represent learned projections of the input. For a sequence of length <span class="math inline">\(N\)</span> with dimension <span class="math inline">\(d\)</span>, this operation creates an <span class="math inline">\(N\times N\)</span> attention matrix, determining how each position should attend to all others.</p>
<p>The attention operation involves several key steps. First, it computes query, key, and value projections for each position in the sequence. Next, it generates an <span class="math inline">\(N\times N\)</span> attention matrix through query-key interactions. These steps are illustrated in <a href="#fig-attention" class="quarto-xref">Figure&nbsp;6</a>. Finally, it uses these attention weights to combine value vectors, producing the output.</p>
<div id="fig-attention" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-env="figure" data-fig-pos="htb">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-attention-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="0f88d09d466d8e0373bbe53f1318aa4872ac0a9f.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-6" title="Figure&nbsp;6: Query-Key-Value Interaction: Transformer attention mechanisms dynamically weigh input sequence elements by computing relationships between queries, keys, and values, enabling the model to focus on relevant information. these projections facilitate the creation of an attention matrix that determines the contribution of each value vector to the final output, effectively capturing contextual dependencies within the sequence. Source: transformer explainer."><img src="dnn_architectures_files/mediabag/0f88d09d466d8e0373bbe53f1318aa4872ac0a9f.svg" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-attention-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6: <strong>Query-Key-Value Interaction</strong>: Transformer attention mechanisms dynamically weigh input sequence elements by computing relationships between queries, keys, and values, enabling the model to focus on relevant information. these projections facilitate the creation of an attention matrix that determines the contribution of each value vector to the final output, effectively capturing contextual dependencies within the sequence. Source: <a href="HTTPS://poloclub.GitHub.io/transformer-explainer/">transformer explainer</a>.
</figcaption>
</figure>
</div>
<p>The key is that, unlike the fixed weight matrices found in previous architectures, as shown in <a href="#fig-attention-weightcalc" class="quarto-xref">Figure&nbsp;7</a>, these attention weights are computed dynamically for each input. This allows the model to adapt its processing based on the dynamic content at hand.</p>
<div id="fig-attention-weightcalc" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-env="figure" data-fig-pos="htb">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-attention-weightcalc-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="1e93307e04f830cf169e6fba1b1a53df334cdae6.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-7" title="Figure&nbsp;7: Dynamic Attention Weights: Transformer models calculate attention weights dynamically based on the relationships between query, key, and value vectors, allowing the model to focus on relevant parts of the input sequence for each processing step. this contrasts with fixed-weight architectures and enables adaptive pattern processing for handling variable-length inputs and complex dependencies. Source: transformer explainer."><img src="dnn_architectures_files/mediabag/1e93307e04f830cf169e6fba1b1a53df334cdae6.svg" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-attention-weightcalc-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;7: <strong>Dynamic Attention Weights</strong>: Transformer models calculate attention weights dynamically based on the relationships between query, key, and value vectors, allowing the model to focus on relevant parts of the input sequence for each processing step. this contrasts with fixed-weight architectures and enables adaptive pattern processing for handling variable-length inputs and complex dependencies. Source: <a href="HTTPS://poloclub.GitHub.io/transformer-explainer/">transformer explainer</a>.
</figcaption>
</figure>
</div>
</section>
<section id="sec-dnn-architectures-computational-mapping-6c7e" class="level4">
<h4 class="anchored" data-anchor-id="sec-dnn-architectures-computational-mapping-6c7e">Computational Mapping</h4>
<p>Attention mechanisms create computational patterns that differ significantly from previous architectures. The implementation approach shown in <a href="#lst-attention_layer_compute" class="quarto-xref">Listing&nbsp;7</a> shows dynamic connectivity translating into specific computational requirements.</p>
<div id="lst-attention_layer_compute" class="listing quarto-float quarto-figure quarto-figure-left anchored">
<figure class="quarto-float quarto-float-lst figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-lst" id="lst-attention_layer_compute-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Listing&nbsp;7: <strong>Attention Mechanism</strong>: Transformer models compute attention through query-key-value interactions, enabling dynamic focus across input sequences for improved language understanding.
</figcaption>
<div aria-describedby="lst-attention_layer_compute-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="sourceCode" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> attention_layer_matrix(Q, K, V):</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Q, K, V: (batch_size Ã— seq_len Ã— d_model)</span></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>    scores <span class="op">=</span> matmul(Q, K.transpose(<span class="op">-</span><span class="dv">2</span>, <span class="op">-</span><span class="dv">1</span>)) <span class="op">/</span> sqrt(</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>        d_k</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>    )  <span class="co"># Compute attention scores</span></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>    weights <span class="op">=</span> softmax(scores)  <span class="co"># Normalize scores</span></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>    output <span class="op">=</span> matmul(weights, V)  <span class="co"># Combine values</span></span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> output</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Core computational pattern</span></span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> attention_layer_compute(Q, K, V):</span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Initialize outputs</span></span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a>    scores <span class="op">=</span> np.zeros((batch_size, seq_len, seq_len))</span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a>    outputs <span class="op">=</span> np.zeros_like(V)</span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Loop 1: Process each sequence in batch</span></span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> b <span class="kw">in</span> <span class="bu">range</span>(batch_size):</span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Loop 2: Compute attention for each query position</span></span>
<span id="cb7-20"><a href="#cb7-20" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(seq_len):</span>
<span id="cb7-21"><a href="#cb7-21" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Loop 3: Compare with each key position</span></span>
<span id="cb7-22"><a href="#cb7-22" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(seq_len):</span>
<span id="cb7-23"><a href="#cb7-23" aria-hidden="true" tabindex="-1"></a>                <span class="co"># Compute attention score</span></span>
<span id="cb7-24"><a href="#cb7-24" aria-hidden="true" tabindex="-1"></a>                <span class="cf">for</span> d <span class="kw">in</span> <span class="bu">range</span>(d_model):</span>
<span id="cb7-25"><a href="#cb7-25" aria-hidden="true" tabindex="-1"></a>                    scores[b, i, j] <span class="op">+=</span> Q[b, i, d] <span class="op">*</span> K[b, j, d]</span>
<span id="cb7-26"><a href="#cb7-26" aria-hidden="true" tabindex="-1"></a>                scores[b, i, j] <span class="op">/=</span> sqrt(d_k)</span>
<span id="cb7-27"><a href="#cb7-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-28"><a href="#cb7-28" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Apply softmax to scores</span></span>
<span id="cb7-29"><a href="#cb7-29" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(seq_len):</span>
<span id="cb7-30"><a href="#cb7-30" aria-hidden="true" tabindex="-1"></a>            scores[b, i] <span class="op">=</span> softmax(scores[b, i])</span>
<span id="cb7-31"><a href="#cb7-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-32"><a href="#cb7-32" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Loop 4: Combine values using attention weights</span></span>
<span id="cb7-33"><a href="#cb7-33" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(seq_len):</span>
<span id="cb7-34"><a href="#cb7-34" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(seq_len):</span>
<span id="cb7-35"><a href="#cb7-35" aria-hidden="true" tabindex="-1"></a>                <span class="cf">for</span> d <span class="kw">in</span> <span class="bu">range</span>(d_model):</span>
<span id="cb7-36"><a href="#cb7-36" aria-hidden="true" tabindex="-1"></a>                    outputs[b, i, d] <span class="op">+=</span> scores[b, i, j] <span class="op">*</span> V[b, j, d]</span>
<span id="cb7-37"><a href="#cb7-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-38"><a href="#cb7-38" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> outputs</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</figure>
</div>
<p>The translation from attentionâ€™s mathematical elegance to hardware execution reveals the computational price of dynamic connectivity. While the attention equation <code>Attention(Q,K,V) = softmax(QK^T/âˆšd_k)V</code> appears as a straightforward matrix operation, the physical implementation requires orchestrating quadratic numbers of pairwise computations that create different system demands than previous architectures.</p>
<p>The nested loops in <code>attention_layer_compute</code> expose attentionâ€™s true computational signature (see <a href="#lst-attention_layer_compute" class="quarto-xref">Listing&nbsp;7</a>). The first loop processes each sequence in the batch independently. The second and third loops compute attention scores between all pairs of positions, creating the quadratic computation pattern that makes attention both powerful and computationally demanding. The fourth loop uses these attention weights to combine values from all positions, completing the dynamic connectivity pattern that defines attention mechanisms.</p>
</section>
<section id="sec-dnn-architectures-system-implications-b5aa" class="level4">
<h4 class="anchored" data-anchor-id="sec-dnn-architectures-system-implications-b5aa">System Implications</h4>
<p>Attention mechanisms exhibit distinctive system-level patterns that differ from previous architectures through their dynamic connectivity requirements.</p>
<section id="sec-dnn-architectures-memory-requirements-3dc1" class="level5">
<h5 class="anchored" data-anchor-id="sec-dnn-architectures-memory-requirements-3dc1">Memory Requirements</h5>
<p>In terms of memory requirements, attention mechanisms necessitate storage for attention weights, key-query-value projections, and intermediate feature representations. For a sequence length <span class="math inline">\(N\)</span> and dimension d, each attention layer must store an <span class="math inline">\(N\times N\)</span> attention weight matrix for each sequence in the batch, three sets of projection matrices for queries, keys, and values (each sized <span class="math inline">\(d\times d\)</span>), and input and output feature maps of size <span class="math inline">\(N\times d\)</span>. The dynamic generation of attention weights for every input creates a memory access pattern where intermediate attention weights become a significant factor in memory usage.</p>
</section>
<section id="sec-dnn-architectures-computation-needs-a41e" class="level5">
<h5 class="anchored" data-anchor-id="sec-dnn-architectures-computation-needs-a41e">Computation Needs</h5>
<p>Computation needs in attention mechanisms center around two main phases: generating attention weights and applying them to values. For each attention layer, the system performs many multiply-accumulate operations across multiple computational stages. The query-key interactions alone require <span class="math inline">\(N\times N\times d\)</span> multiply-accumulates, with an equal number needed for applying attention weights to values. Additional computations are required for the projection matrices and softmax operations. This computational pattern differs from previous architectures due to its quadratic scaling with sequence length and the need to perform fresh computations for each input.</p>
</section>
<section id="sec-dnn-architectures-data-movement-12b1" class="level5">
<h5 class="anchored" data-anchor-id="sec-dnn-architectures-data-movement-12b1">Data Movement</h5>
<p>Data movement in attention mechanisms presents unique challenges. Each attention operation involves projecting and moving query, key, and value vectors for each position, storing and accessing the full attention weight matrix, and coordinating the movement of value vectors during the weighted combination phase. This creates a data movement pattern where intermediate attention weights become a major factor in system bandwidth requirements. Unlike the more predictable access patterns of CNNs or the sequential access of RNNs, attention operations require frequent movement of dynamically computed weights across the memory hierarchy.</p>
<p>These distinctive characteristics of attention mechanisms in terms of memory, computation, and data movement have significant implications for system design and optimization, setting the stage for the development of more advanced architectures like Transformers.</p>
</section>
</section>
</section>
<section id="sec-dnn-architectures-transformers-attentiononly-architecture-c4f0" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-dnn-architectures-transformers-attentiononly-architecture-c4f0">Transformers: Attention-Only Architecture</h3>
<p>While attention mechanisms introduced the concept of dynamic pattern processing, they were initially applied as additions to existing architectures, particularly RNNs for sequence-to-sequence tasks. This hybrid approach still suffered from the fundamental limitations of recurrent architectures: sequential processing constraints that prevented efficient parallelization and difficulties with very long sequences. The breakthrough insight was recognizing that attention mechanisms alone could replace both convolutional and recurrent processing entirely.</p>
<p>Transformers, introduced in the landmark "Attention is All You Need" paper<a href="#fn20" class="footnote-ref" id="fnref20" role="doc-noteref"><sup>20</sup></a> by <span class="citation" data-cites="vaswani2017attention">Vaswani et al. (<a href="#ref-vaswani2017attention" role="doc-biblioref">2017</a>)</span>, embody a revolutionary inductive bias: <strong>they assume no prior structure but allow the model to learn all pairwise relationships dynamically based on content</strong>. This architectural assumption represents the culmination of the architectural evolution detailed in <a href="#sec-dnn-architectures-architectural-principles-engineering-tradeoffs-89de" class="quarto-xref">Section&nbsp;1.1</a> by eliminating all structural constraints in favor of pure content-dependent processing. Rather than adding attention to RNNs, Transformers built the entire architecture around attention mechanisms, introducing self-attention as the primary computational pattern. This architectural decision traded the parameter efficiency of CNNs and the sequential coherence of RNNs for maximum flexibility and parallelizability.</p>
<div class="no-row-height column-margin column-container"><div id="fn20"><p><sup>20</sup>&nbsp;<strong>â€œAttention is All You Needâ€</strong>: This 2017 paper by Google researchers eliminated recurrence entirely, showing that attention mechanisms alone could achieve state-of-the-art results. The title itself became a rallying cry, and within 5 years, transformer-based models achieved breakthrough performance in language (GPT, BERT), vision (ViT), and beyond <span class="citation" data-cites="radford2018improving devlin2018bert dosovitskiy2021image">(<a href="#ref-radford2018improving" role="doc-biblioref">Radford et al. 2018</a>; <a href="#ref-devlin2018bert" role="doc-biblioref">Devlin et al. 2018</a>; <a href="#ref-dosovitskiy2021image" role="doc-biblioref">Dosovitskiy et al. 2021</a>)</span>. This paper marked a historical turning point in deep learning, demonstrating that the sequential processing that defined RNNs and LSTMs was no longer necessary; attention mechanisms could capture both short and long-range dependencies through parallel computation. While the basic attention mechanism allows for content-based weighting of information from a source sequence, Transformers extend this idea by applying attention within a single sequence, enabling each element to attend to all other elements including itself.</p><div id="ref-radford2018improving" class="csl-entry" role="listitem">
Radford, Alec, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2018. <span>â€œImproving Language Understanding by Generative Pre-Training.â€</span>
</div><div id="ref-devlin2018bert" class="csl-entry" role="listitem">
Devlin, Jacob, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. <span>â€œBERT: Pre-Training of Deep Bidirectional Transformers for Language Understanding.â€</span> <em>arXiv Preprint arXiv:1810.04805</em>, October. <a href="http://arxiv.org/abs/1810.04805v2">http://arxiv.org/abs/1810.04805v2</a>.
</div><div id="ref-dosovitskiy2021image" class="csl-entry" role="listitem">
Dosovitskiy, Alexey, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, et al. 2021. <span>â€œAn Image Is Worth 16x16 Words: Transformers for Image Recognition at Scale.â€</span> <em>International Conference on Learning Representations</em>.
</div></div><div id="ref-vaswani2017attention" class="csl-entry" role="listitem">
Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Kaiser, and Illia Polosukhin. 2017. <span>â€œAttention Is All You Need.â€</span> <em>Advances in Neural Information Processing Systems</em> 30.
</div></div><p>This represents the final step in our architectural journey: from MLPs that connected everything to everything, to CNNs that connected locally, to RNNs that connected sequentially, to Transformers that connect dynamically based on learned content relationships. Each evolution sacrificed constraints for capabilities, with Transformers achieving maximum expressivity at the computational cost established in <a href="#sec-dnn-architectures-architectural-principles-engineering-tradeoffs-89de" class="quarto-xref">Section&nbsp;1.1</a>.</p>
<section id="sec-dnn-architectures-algorithmic-structure-9d4b" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="sec-dnn-architectures-algorithmic-structure-9d4b">Algorithmic Structure</h4>
<p>The key innovation in Transformers lies in their use of self-attention layers. In the self-attention mechanism used by Transformers, the Query, Key, and Value vectors are all derived from the same input sequence. This is the key distinction from earlier attention mechanisms where the query might come from a decoder while the keys and values came from an encoder. By making all components self-referential, self-attention allows the model to weigh the importance of different positions within the same sequence when encoding each position. For instance, in processing the sentence â€œThe animal didnâ€™t cross the street because it was too wide,â€ self-attention allows the model to link â€œitâ€ with â€œstreet,â€ capturing long-range dependencies that are challenging for traditional sequential models.</p>
<p>The self-attention mechanism can be expressed mathematically in a form similar to the basic attention mechanism: <span class="math display">\[
\text{SelfAttention}(\mathbf{X}) = \text{softmax}
\left(\frac{\mathbf{XW_Q}(\mathbf{XW_K})^T}{\sqrt{d_k}}\right)\mathbf{XW_V}
\]</span></p>
<p>Here, <span class="math inline">\(\mathbf{X}\)</span> is the input sequence, and <span class="math inline">\(\mathbf{W_Q}\)</span>, <span class="math inline">\(\mathbf{W_K}\)</span>, and <span class="math inline">\(\mathbf{W_V}\)</span> are learned weight matrices for queries, keys, and values respectively. This formulation highlights how self-attention derives all its components from the same input, creating a dynamic, content-dependent processing pattern.</p>
<p>Building on this foundation, Transformers employ multi-head attention, which extends the self-attention mechanism by running multiple attention functions in parallel. Each â€œheadâ€ involves a separate set of query/key/value projections that can focus on different aspects of the input, allowing the model to jointly attend to information from different representation subspaces. This multi-head structure provides the model with a richer representational capability, enabling it to capture various types of relationships within the data simultaneously.</p>
<p>The mathematical formulation for multi-head attention is: <span class="math display">\[
\text{MultiHead}(\mathbf{Q}, \mathbf{K}, \mathbf{V}) = \text{Concat}(\text{head}_1, \ldots, \text{head}_h)\mathbf{W}^O
\]</span> where each attention head is computed as: <span class="math display">\[
\text{head}_i = \text{Attention}(\mathbf{Q}\mathbf{W}_i^Q, \mathbf{K}\mathbf{W}_i^K, \mathbf{V}\mathbf{W}_i^V)
\]</span></p>
<p>A critical component in both self-attention and multi-head attention is the scaling factor <span class="math inline">\(\sqrt{d_k}\)</span>, which serves an important mathematical purpose. This factor prevents the dot products from growing too large, which would push the softmax function into regions with extremely small gradients. For queries and keys of dimension <span class="math inline">\(d_k\)</span>, their dot product has variance <span class="math inline">\(d_k\)</span>, so dividing by <span class="math inline">\(\sqrt{d_k}\)</span> normalizes the variance to 1, maintaining stable gradients and enabling effective learning.<a href="#fn21" class="footnote-ref" id="fnref21" role="doc-noteref"><sup>21</sup></a></p>
<div class="no-row-height column-margin column-container"><div id="fn21"><p><sup>21</sup>&nbsp;<strong>Attention Scaling</strong>: Without the <span class="math inline">\(\sqrt{d_k}\)</span> scaling factor, large dot products would cause the softmax to saturate, producing gradients close to zero and hindering learning. This mathematical insight enables stable optimization of large Transformer models.</p></div></div><p>Beyond the mathematical mechanics, attention mechanisms can be understood conceptually as implementing a form of content-addressable memory system. Like hash tables that retrieve values based on key matching, attention computes similarity between a query and all available keys, then retrieves a weighted combination of corresponding values. The dot product similarity <code>QÂ·K</code> functions like a hash function that measures how well each key matches the query. The softmax normalization ensures the weights sum to 1, implementing a probabilistic retrieval mechanism. This connection explains why attention proves effective for tasks requiring flexible information retrievalâ€”it provides a differentiable approximation to database lookup operations.</p>
<p>From an information-theoretic perspective, attention mechanisms implement optimal information aggregation under uncertainty. The attention weights represent uncertainty about which parts of the input contain relevant information for the current processing step. The softmax operation implements a maximum entropy principle: among all possible ways to distribute attention across input positions, softmax selects the distribution with maximum entropy subject to the constraint that similarity scores determine relative importance <span class="citation" data-cites="cover2006elements">(<a href="#ref-cover2006elements" role="doc-biblioref">Cover and Thomas 2001</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-cover2006elements" class="csl-entry" role="listitem">
Cover, Thomas M., and Joy A. Thomas. 2001. <em>Elements of Information Theory</em>. 2nd ed. Wiley. <a href="https://doi.org/10.1002/0471200611">https://doi.org/10.1002/0471200611</a>.
</div></div></section>
<section id="sec-dnn-architectures-efficiency-optimization-94d7" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="sec-dnn-architectures-efficiency-optimization-94d7">Efficiency and Optimization</h4>
<p>Attention mechanisms are highly redundant, with many heads learning similar patterns. Head pruning and low-rank attention factorization can reduce computation by 50-80% with careful implementation. Analysis of large Transformer models reveals that most attention heads fall into a few common patterns (positional, syntactic, semantic), suggesting that explicit architectural specialization could replace learned redundancy.</p>
<p>Attention operations are particularly sensitive to quantization due to the softmax operation and the quadratic number of attention scores. Separate quantization schemes for Q, K, V projections and careful handling of softmax operations are required for stable quantization. Post-training INT8 quantization typically achieves 2-3% accuracy loss, while INT4 quantization requires more sophisticated quantization-aware training approaches.</p>
<p>The quadratic scaling with sequence length creates efficiency limitations. Sparse attention patterns (such as local windows, strided patterns, or learned sparsity) can reduce complexity from O(nÂ²) to O(n log n) or O(n) while maintaining most modeling capability. Linear attention approximations trade some expressive power for linear scaling, enabling processing of much longer sequences on limited hardware.</p>
<p>This information-theoretic interpretation reveals why attention is so effective for selective processing. The mechanism automatically balances two competing objectives: focusing on the most relevant information (minimizing entropy) while maintaining sufficient breadth to avoid missing important details (maximizing entropy). The attention pattern emerges as the optimal trade-off between these objectives, explaining why transformers can effectively handle long sequences and complex dependencies.</p>
<p>Self-attention learns dynamic activation patterns across the input sequence. Unlike CNNs which apply fixed filters or RNNs which use fixed recurrence patterns, attention learns which elements should activate together based on their content. This creates a form of adaptive connectivity where the effective network topology changes for each input. Recent research has shown that attention heads in trained models often specialize in detecting specific linguistic or semantic patterns <span class="citation" data-cites="clark2019what">(<a href="#ref-clark2019what" role="doc-biblioref">Clark et al. 2019</a>)</span>, suggesting that the mechanism naturally discovers interpretable structural regularities in data.</p>
<div class="no-row-height column-margin column-container"><div id="ref-clark2019what" class="csl-entry" role="listitem">
Clark, Kevin, Urvashi Khandelwal, Omer Levy, and Christopher D. Manning. 2019. <span>â€œWhat Does BERT Look at? An Analysis of BERTâ€™s Attention.â€</span> In <em>Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP</em>, 276â€“86. Association for Computational Linguistics. <a href="https://doi.org/10.18653/v1/w19-4828">https://doi.org/10.18653/v1/w19-4828</a>.
</div></div><p>The Transformer architecture leverages this self-attention mechanism within a broader structure that typically includes feed-forward layers, layer normalization, and residual connections (see <a href="#fig-transformer" class="quarto-xref">Figure&nbsp;8</a>). This combination allows Transformers to process input sequences in parallel, capturing complex dependencies without the need for sequential computation. As a result, Transformers have demonstrated significant effectiveness across a wide range of tasks, from natural language processing to computer vision, transforming deep learning architectures across domains.</p>
<div id="fig-transformer" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-env="figure" data-fig-pos="htb">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-transformer-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="aa3aa170a3acdaf91ddfd26c9ea74821e1dd5a86.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-8" title="Figure&nbsp;8: Attention Head: Neural networks compute attention through query-key-value interactions, enabling dynamic focus across subwords for improved sentence understanding. Source: Attention Is All You Need."><img src="dnn_architectures_files/mediabag/aa3aa170a3acdaf91ddfd26c9ea74821e1dd5a86.svg" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-transformer-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;8: <strong>Attention Head</strong>: Neural networks compute attention through query-key-value interactions, enabling dynamic focus across subwords for improved sentence understanding. Source: Attention Is All You Need.
</figcaption>
</figure>
</div>
</section>
<section id="sec-dnn-architectures-computational-mapping-7fe9" class="level4">
<h4 class="anchored" data-anchor-id="sec-dnn-architectures-computational-mapping-7fe9">Computational Mapping</h4>
<p>While Transformer self-attention builds upon the basic attention mechanism, it introduces distinct computational patterns that set it apart. To understand these patterns, we must examine the typical implementation of self-attention in Transformers (see <a href="#lst-self_attention_layer" class="quarto-xref">Listing&nbsp;8</a>):</p>
<div id="lst-self_attention_layer" class="listing quarto-float quarto-figure quarto-figure-left anchored">
<figure class="quarto-float quarto-float-lst figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-lst" id="lst-self_attention_layer-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Listing&nbsp;8: <strong>Self-Attention Mechanism</strong>: Transformer models compute attention through query-key-value interactions, enabling dynamic focus across input sequences for improved language understanding.
</figcaption>
<div aria-describedby="lst-self_attention_layer-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="sourceCode" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> self_attention_layer(X, W_Q, W_K, W_V, d_k):</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># X: input tensor (batch_size Ã— seq_len Ã— d_model)</span></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>    <span class="co"># W_Q, W_K, W_V: weight matrices (d_model Ã— d_k)</span></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>    Q <span class="op">=</span> matmul(X, W_Q)</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>    K <span class="op">=</span> matmul(X, W_K)</span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>    V <span class="op">=</span> matmul(X, W_V)</span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>    scores <span class="op">=</span> matmul(Q, K.transpose(<span class="op">-</span><span class="dv">2</span>, <span class="op">-</span><span class="dv">1</span>)) <span class="op">/</span> sqrt(d_k)</span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a>    attention_weights <span class="op">=</span> softmax(scores, dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a>    output <span class="op">=</span> matmul(attention_weights, V)</span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> output</span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> multi_head_attention(X, W_Q, W_K, W_V, W_O, num_heads, d_k):</span>
<span id="cb8-17"><a href="#cb8-17" aria-hidden="true" tabindex="-1"></a>    outputs <span class="op">=</span> []</span>
<span id="cb8-18"><a href="#cb8-18" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(num_heads):</span>
<span id="cb8-19"><a href="#cb8-19" aria-hidden="true" tabindex="-1"></a>        head_output <span class="op">=</span> self_attention_layer(</span>
<span id="cb8-20"><a href="#cb8-20" aria-hidden="true" tabindex="-1"></a>            X, W_Q[i], W_K[i], W_V[i], d_k</span>
<span id="cb8-21"><a href="#cb8-21" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb8-22"><a href="#cb8-22" aria-hidden="true" tabindex="-1"></a>        outputs.append(head_output)</span>
<span id="cb8-23"><a href="#cb8-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-24"><a href="#cb8-24" aria-hidden="true" tabindex="-1"></a>    concat_output <span class="op">=</span> torch.cat(outputs, dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb8-25"><a href="#cb8-25" aria-hidden="true" tabindex="-1"></a>    final_output <span class="op">=</span> matmul(concat_output, W_O)</span>
<span id="cb8-26"><a href="#cb8-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-27"><a href="#cb8-27" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> final_output</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</figure>
</div>
</section>
<section id="sec-dnn-architectures-system-implications-76dd" class="level4">
<h4 class="anchored" data-anchor-id="sec-dnn-architectures-system-implications-76dd">System Implications</h4>
<p>This implementation reveals key computational characteristics that apply to basic attention mechanisms, with Transformer self-attention representing a specific case. First, self-attention enables parallel processing across all positions in the sequence. This is evident in the matrix multiplications that compute <code>Q</code>, <code>K</code>, and <code>V</code> simultaneously for all positions. Unlike recurrent architectures that process inputs sequentially, this parallel nature allows for more efficient computation, especially on modern hardware designed for parallel operations.</p>
<p>Second, the attention score computation results in a matrix of size <code>(seq_len Ã— seq_len)</code>, leading to quadratic complexity with respect to sequence length. This quadratic relationship becomes a significant computational bottleneck when processing long sequences, a challenge that has spurred research into more efficient attention mechanisms.</p>
<p>Third, the multi-head attention mechanism effectively runs multiple self-attention operations in parallel, each with its own set of learned projections. While this increases the computational load linearly with the number of heads, it allows the model to capture different types of relationships within the same input, enhancing the modelâ€™s representational power.</p>
<p>Fourth, the core computations in self-attention are dominated by large matrix multiplications. For a sequence of length <span class="math inline">\(N\)</span> and embedding dimension <span class="math inline">\(d\)</span>, the main operations involve matrices of sizes <span class="math inline">\((N\times d)\)</span>, <span class="math inline">\((d\times d)\)</span>, and <span class="math inline">\((N\times N)\)</span>. These intensive matrix operations are well-suited for acceleration on specialized hardware like GPUs, but they also contribute significantly to the overall computational cost of the model.</p>
<p>Finally, self-attention generates memory-intensive intermediate results. The attention weights matrix <span class="math inline">\((N\times N)\)</span> and the intermediate results for each attention head create large memory requirements, especially for long sequences. This can pose challenges for deployment on memory-constrained devices and necessitates careful memory management in implementations.</p>
<p>These computational patterns create a unique profile for Transformer self-attention, distinct from previous architectures. The parallel nature of the computations makes Transformers well-suited for modern parallel processing hardware, but the quadratic complexity with sequence length poses challenges for processing long sequences. As a result, much research has focused on developing optimization techniques, such as sparse attention patterns or low-rank approximations, to address these challenges. Each of these optimizations presents its own trade-offs between computational efficiency and model expressiveness, a balance that must be carefully considered in practical applications.</p>
<p>This examination of four distinct architectural families reveals both their individual characteristics and their collective evolution. Rather than viewing these architectures in isolation, a deeper understanding emerges when we consider how they relate to each other and build upon shared foundations.</p>
<div id="quiz-question-sec-dnn-architectures-attention-mechanisms-dynamic-pattern-processing-566d" class="callout callout-quiz-question">
<details class="callout-quiz-question fbx-default closebutton"><summary><strong>Self-Check: Question 1.5</strong></summary><div>
<ol type="1">
<li><p>Which of the following best describes the primary advantage of attention mechanisms over RNNs?</p>
<ol type="a">
<li>Attention mechanisms process sequences in parallel, capturing long-range dependencies more effectively.</li>
<li>Attention mechanisms use fixed spatial patterns to process data.</li>
<li>Attention mechanisms rely on sequential processing to capture temporal dependencies.</li>
<li>Attention mechanisms are less computationally demanding than RNNs.</li>
</ol></li>
<li><p>Explain how attention mechanisms dynamically determine which relationships in input data are important.</p></li>
<li><p>In attention mechanisms, the operation that normalizes similarity scores to create a probability distribution is called the ____.</p></li>
<li><p>In a production system, what are the computational trade-offs of implementing attention mechanisms compared to RNNs?</p></li>
</ol>
<p><a href="#quiz-answer-sec-dnn-architectures-attention-mechanisms-dynamic-pattern-processing-566d" class="question-label">See Answers â†’</a></p>
</div></details>
</div>
</section>
</section>
</section>
<section id="sec-dnn-architectures-architectural-building-blocks-a575" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="sec-dnn-architectures-architectural-building-blocks-a575">Architectural Building Blocks</h2>
<p>Having examined four major architectural familiesâ€”MLPs, CNNs, RNNs, and Transformersâ€”each with distinct computational characteristics and system implications, a unifying perspective emerges. Deep learning architectures, while presented as distinct approaches in previous sections, are better understood as compositions of building blocks that evolved over time. Like complex LEGO structures built from basic bricks, modern neural networks combine and iterate on core computational patterns that emerged through decades of research <span class="citation" data-cites="lecun2015deep">(<a href="#ref-lecun2015deep" role="doc-biblioref">Yann LeCun, Bengio, and Hinton 2015</a>)</span>. Each architectural innovation introduced new building blocks while discovering novel applications of existing ones.</p>
<div class="no-row-height column-margin column-container"><div id="ref-lecun2015deep" class="csl-entry" role="listitem">
LeCun, Yann, Yoshua Bengio, and Geoffrey Hinton. 2015. <span>â€œDeep Learning.â€</span> <em>Nature</em> 521 (7553): 436â€“44. <a href="https://doi.org/10.1038/nature14539">https://doi.org/10.1038/nature14539</a>.
</div><div id="ref-rosenblatt1958perceptron" class="csl-entry" role="listitem">
Rosenblatt, F. 1958. <span>â€œThe Perceptron: A Probabilistic Model for Information Storage and Organization in the Brain.â€</span> <em>Psychological Review</em> 65 (6): 386â€“408. <a href="https://doi.org/10.1037/h0042519">https://doi.org/10.1037/h0042519</a>.
</div><div id="ref-rumelhart1986learning" class="csl-entry" role="listitem">
Rumelhart, David E., Geoffrey E. Hinton, and Ronald J. Williams. 1986. <span>â€œLearning Representations by Back-Propagating Errors.â€</span> <em>Nature</em> 323 (6088): 533â€“36. <a href="https://doi.org/10.1038/323533a0">https://doi.org/10.1038/323533a0</a>.
</div></div><p>These building blocks and their evolution illuminate modern architectural design. The simple perceptron <span class="citation" data-cites="rosenblatt1958perceptron">(<a href="#ref-rosenblatt1958perceptron" role="doc-biblioref">Rosenblatt 1958</a>)</span> evolved into multi-layer networks <span class="citation" data-cites="rumelhart1986learning">(<a href="#ref-rumelhart1986learning" role="doc-biblioref">Rumelhart, Hinton, and Williams 1986</a>)</span>, which subsequently spawned specialized patterns for spatial and sequential processing. Each advancement preserved useful elements from predecessors while introducing new computational primitives. Contemporary architectures, such as Transformers, represent carefully engineered combinations of these building blocks.</p>
<p>This progression reveals both the evolution of neural networks and the discovery and refinement of core computational patterns that remain relevant. Building on the architectural progression outlined in <a href="#sec-dnn-architectures-architectural-principles-engineering-tradeoffs-89de" class="quarto-xref">Section&nbsp;1.1</a>, each new architecture introduces distinct computational demands and system-level challenges.</p>
<p><a href="#tbl-dl-evolution" class="quarto-xref">Table&nbsp;1</a> summarizes this evolution, highlighting the key primitives and system focus for each era of deep learning development. This table captures the major shifts in deep learning architecture design and corresponding changes in system-level considerations. The progression spans from early dense matrix operations optimized for CPUs, through convolutions leveraging GPU acceleration and sequential operations necessitating sophisticated memory hierarchies, to the current era of attention mechanisms requiring flexible accelerators and high-bandwidth memory.</p>
<div id="tbl-dl-evolution" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-dl-evolution-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;1: <strong>Deep Learning Evolution</strong>: Neural network architectures have progressed from simple, fully connected layers to complex models leveraging specialized hardware and addressing sequential data dependencies. This table maps architectural eras to key computational primitives and corresponding system-level optimizations, revealing a historical trend toward increased parallelism and memory bandwidth requirements.
</figcaption>
<div aria-describedby="tbl-dl-evolution-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<colgroup>
<col style="width: 24%">
<col style="width: 28%">
<col style="width: 21%">
<col style="width: 25%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;"><strong>Era</strong></th>
<th style="text-align: left;"><strong>Dominant Architecture</strong></th>
<th style="text-align: left;"><strong>Key Primitives</strong></th>
<th style="text-align: left;"><strong>System Focus</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><strong>Early NN</strong></td>
<td style="text-align: left;">MLP</td>
<td style="text-align: left;">Dense Matrix Ops</td>
<td style="text-align: left;">CPU optimization</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>CNN Revolution</strong></td>
<td style="text-align: left;">CNN</td>
<td style="text-align: left;">Convolutions</td>
<td style="text-align: left;">GPU acceleration</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>Sequence Modeling</strong></td>
<td style="text-align: left;">RNN</td>
<td style="text-align: left;">Sequential Ops</td>
<td style="text-align: left;">Memory hierarchies</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Attention Era</strong></td>
<td style="text-align: left;">Transformer</td>
<td style="text-align: left;">Attention, Dynamic Compute</td>
<td style="text-align: left;">Flexible accelerators, High-bandwidth memory</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p>Examination of these building blocks shows primitives evolving and combining to create increasingly powerful neural network architectures.</p>
<section id="sec-dnn-architectures-evolution-perceptron-multilayer-networks-0f3f" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-dnn-architectures-evolution-perceptron-multilayer-networks-0f3f">Evolution from Perceptron to Multi-Layer Networks</h3>
<p>While we examined MLPs in <a href="#sec-dnn-architectures-multilayer-perceptrons-dense-pattern-processing-259f" class="quarto-xref">Section&nbsp;1.2</a> as a mechanism for dense pattern processing, here we focus on how they established building blocks that appear throughout deep learning. The evolution from perceptron to MLP introduced several key concepts: the power of layer stacking, the importance of non-linear transformations, and the basic feedforward computation pattern.</p>
<p>The introduction of hidden layers between input and output created a template for feature transformation that appears in virtually every modern architecture. Even in sophisticated networks like Transformers, we find MLP-style feedforward layers performing feature processing. The concept of transforming data through successive non-linear layers has become a paradigm that transcends specific architecture types.</p>
<p>Most significantly, the development of MLPs established the backpropagation algorithm<a href="#fn22" class="footnote-ref" id="fnref22" role="doc-noteref"><sup>22</sup></a>, which to this day remains the cornerstone of neural network optimization. This key contribution has enabled the development of deep architectures and influenced how later architectures would be designed to maintain gradient flow.</p>
<div class="no-row-height column-margin column-container"><div id="fn22"><p><sup>22</sup>&nbsp;<strong>Backpropagation Algorithm</strong>: While the chain rule was known since the 1600s, Rumelhart, Hinton, and Williams (1986) showed how to efficiently apply it to train multi-layer networks. This â€œlearning by error propagationâ€ algorithm made deep networks practical and remains virtually unchanged in modern systemsâ€”a testament to its importance.</p></div></div><p>These building blocks, layered feature transformation, non-linear activation, and gradient-based learning, set the foundation for more specialized architectures. Subsequent innovations often focused on structuring these basic components in new ways rather than replacing them entirely.</p>
</section>
<section id="sec-dnn-architectures-evolution-dense-spatial-processing-1d3b" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-dnn-architectures-evolution-dense-spatial-processing-1d3b">Evolution from Dense to Spatial Processing</h3>
<p>The development of CNNs marked an architectural innovation, specifically the realization that we could specialize the dense connectivity of MLPs for spatial patterns. While retaining the core concept of layer-wise processing, CNNs introduced several building blocks that would influence all future architectures.</p>
<p>The first key innovation was the concept of parameter sharing. Unlike MLPs where each connection had its own weight, CNNs showed how the same parameters could be reused across different parts of the input. This not only made the networks more efficient but introduced the powerful idea that architectural structure could encode useful priors about the data <span class="citation" data-cites="lecun1998gradient">(<a href="#ref-lecun1998gradient" role="doc-biblioref">Lecun et al. 1998</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-lecun1998gradient" class="csl-entry" role="listitem">
Lecun, Y., L. Bottou, Y. Bengio, and P. Haffner. 1998. <span>â€œGradient-Based Learning Applied to Document Recognition.â€</span> <em>Proceedings of the IEEE</em> 86 (11): 2278â€“2324. <a href="https://doi.org/10.1109/5.726791">https://doi.org/10.1109/5.726791</a>.
</div><div id="fn23"><p><sup>23</sup>&nbsp;<strong>ResNet Revolution</strong>: ResNet (2016) solved the â€œdegradation problemâ€ where deeper networks performed worse than shallow ones. The key insight: adding identity shortcuts (<span class="math inline">\(\mathcal{F}(\mathbf{x}) + \mathbf{x}\)</span>) let networks learn residual mappings instead of full transformations, enabling training of 1000+ layer networks and winning ImageNet 2015.</p></div><div id="ref-he2016deep" class="csl-entry" role="listitem">
He, Kaiming, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. <span>â€œDeep Residual Learning for Image Recognition.â€</span> In <em>2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 770â€“78. IEEE. <a href="https://doi.org/10.1109/cvpr.2016.90">https://doi.org/10.1109/cvpr.2016.90</a>.
</div></div><p>Perhaps even more influential was the introduction of skip connections through ResNets<a href="#fn23" class="footnote-ref" id="fnref23" role="doc-noteref"><sup>23</sup></a> <span class="citation" data-cites="he2016deep">(<a href="#ref-he2016deep" role="doc-biblioref">He et al. 2016</a>)</span>. Originally they were designed to help train very deep CNNs, skip connections have become a building block that appears in virtually every modern architecture. They showed how direct paths through the network could help gradient flow and information propagation, a concept now central to Transformer designs.</p>
<p>CNNs also introduced batch normalization, a technique for stabilizing neural network optimization by normalizing intermediate features <span class="citation" data-cites="ioffe2015batch">(<a href="#ref-ioffe2015batch" role="doc-biblioref">Ioffe and Szegedy 2015</a>)</span>. This concept of feature normalization, while originating in CNNs, evolved into layer normalization and is now a key component in modern architectures.</p>
<div class="no-row-height column-margin column-container"><div id="ref-ioffe2015batch" class="csl-entry" role="listitem">
Ioffe, Sergey, and Christian Szegedy. 2015. <span>â€œBatch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift.â€</span> <em>International Conference on Machine Learning</em>, 448â€“56.
</div></div><p>These innovations, such as parameter sharing, skip connections, and normalization, transcended their origins in spatial processing to become essential building blocks in the deep learning toolkit.</p>
</section>
<section id="sec-dnn-architectures-evolution-sequence-processing-8a78" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-dnn-architectures-evolution-sequence-processing-8a78">Evolution of Sequence Processing</h3>
<p>While CNNs specialized MLPs for spatial patterns, sequence models adapted neural networks for temporal dependencies. RNNs introduced the concept of maintaining and updating state, a building block that influenced how networks could process sequential information, <span class="citation" data-cites="elman1990finding">(<a href="#ref-elman1990finding" role="doc-biblioref">Elman 1990</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-elman1990finding" class="csl-entry" role="listitem">
Elman, Jeffrey L. 1990. <span>â€œFinding Structure in Time.â€</span> <em>Cognitive Science</em> 14 (2): 179â€“211. <a href="https://doi.org/10.1207/s15516709cog1402\_1">https://doi.org/10.1207/s15516709cog1402\_1</a>.
</div><div id="fn24"><p><sup>24</sup>&nbsp;<strong>LSTM Origins</strong>: Sepp Hochreiter and JÃ¼rgen Schmidhuber invented LSTMs in 1997 to solve the â€œvanishing gradient problemâ€ that plagued RNNs. Their gating mechanism was inspired by biological neuronsâ€™ ability to selectively retain informationâ€”a breakthrough that enabled sequence modeling and facilitated modern language models.</p></div><div id="fn25"><p><sup>25</sup>&nbsp;<strong>Gated Recurrent Unit (GRU)</strong>: Simplified version of LSTM introduced by Cho et al.&nbsp;(2014) with only 2 gates instead of 3, reducing parameters by ~25% while maintaining similar performance. GRUs became popular for their computational efficiency and easier training, proving that architectural simplification can sometimes improve rather than hurt performance.</p></div><div id="ref-hochreiter1997long" class="csl-entry" role="listitem">
Hochreiter, Sepp, and JÃ¼rgen Schmidhuber. 1997. <span>â€œLong Short-Term Memory.â€</span> <em>Neural Computation</em> 9 (8): 1735â€“80. <a href="https://doi.org/10.1162/neco.1997.9.8.1735">https://doi.org/10.1162/neco.1997.9.8.1735</a>.
</div><div id="ref-cho2014properties" class="csl-entry" role="listitem">
Cho, Kyunghyun, Bart van Merrienboer, Dzmitry Bahdanau, and Yoshua Bengio. 2014. <span>â€œOn the Properties of Neural Machine Translation: Encoder-Decoder Approaches.â€</span> In <em>Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation (SSST-8)</em>, 103â€“11. Association for Computational Linguistics.
</div></div><p>The development of LSTMs<a href="#fn24" class="footnote-ref" id="fnref24" role="doc-noteref"><sup>24</sup></a> and GRUs<a href="#fn25" class="footnote-ref" id="fnref25" role="doc-noteref"><sup>25</sup></a> brought sophisticated gating mechanisms to neural networks <span class="citation" data-cites="hochreiter1997long cho2014properties">(<a href="#ref-hochreiter1997long" role="doc-biblioref">Hochreiter and Schmidhuber 1997</a>; <a href="#ref-cho2014properties" role="doc-biblioref">Cho et al. 2014</a>)</span>. These gates, themselves small MLPs, showed how simple feedforward computations could be composed to control information flow. This concept of using neural networks to modulate other neural networks became a recurring pattern in architecture design.</p>
<p>Perhaps most significantly, sequence models demonstrated the power of adaptive computation paths. Unlike the fixed patterns of MLPs and CNNs, RNNs showed how networks could process variable-length inputs by reusing weights over time. This insight, that architectural patterns could adapt to input structure, laid groundwork for more flexible architectures.</p>
<p>Sequence models also popularized the concept of attention through encoder-decoder architectures <span class="citation" data-cites="bahdanau2014neural">(<a href="#ref-bahdanau2014neural" role="doc-biblioref">Bahdanau, Cho, and Bengio 2014</a>)</span>. Initially introduced as an improvement to machine translation, attention mechanisms showed how networks could learn to dynamically focus on relevant information. This building block would later become the foundation of Transformer architectures.</p>
<div class="no-row-height column-margin column-container"><div id="ref-bahdanau2014neural" class="csl-entry" role="listitem">
Bahdanau, Dzmitry, Kyunghyun Cho, and Yoshua Bengio. 2014. <span>â€œNeural Machine Translation by Jointly Learning to Align and Translate.â€</span> <em>arXiv Preprint arXiv:1409.0473</em>, September. <a href="http://arxiv.org/abs/1409.0473v7">http://arxiv.org/abs/1409.0473v7</a>.
</div></div></section>
<section id="sec-dnn-architectures-modern-architectures-synthesis-unification-b5ef" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-dnn-architectures-modern-architectures-synthesis-unification-b5ef">Modern Architectures: Synthesis and Unification</h3>
<p>Modern architectures, particularly Transformers, represent a sophisticated synthesis of these fundamental building blocks. Rather than introducing entirely new patterns, they innovate through strategic combination and refinement of existing components. The Transformer architecture exemplifies this approach: at its core, MLP-style feedforward networks process features between attention layers. The attention mechanism itself builds on sequence model concepts while eliminating recurrent connections, instead employing position embeddings inspired by CNN intuitions. The architecture extensively utilizes skip connections (see <a href="#fig-example-skip-connection" class="quarto-xref">Figure&nbsp;9</a>), inherited from ResNets, while layer normalization, evolved from CNN batch normalization, stabilizes optimization <span class="citation" data-cites="ba2016layer">(<a href="#ref-ba2016layer" role="doc-biblioref">Ba, Kiros, and Hinton 2016</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-ba2016layer" class="csl-entry" role="listitem">
Ba, Jimmy Lei, Jamie Ryan Kiros, and Geoffrey E. Hinton. 2016. <span>â€œLayer Normalization.â€</span> <em>arXiv Preprint arXiv:1607.06450</em>, July. <a href="http://arxiv.org/abs/1607.06450v1">http://arxiv.org/abs/1607.06450v1</a>.
</div></div><div id="fig-example-skip-connection" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-env="figure" data-fig-pos="htb">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-example-skip-connection-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="f8fb940934a7e785fed405e5a532e1f2176738ca.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-9" title="Figure&nbsp;9: Residual Connection: Skip connections add the input of a layer to its output, enabling gradients to flow directly through the network and mitigating the vanishing gradient problem in deep architectures. This allows training of significantly deeper networks, as seen in resnets and adopted in modern transformer architectures to improve optimization and performance."><img src="dnn_architectures_files/mediabag/f8fb940934a7e785fed405e5a532e1f2176738ca.svg" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-example-skip-connection-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;9: <strong>Residual Connection</strong>: Skip connections add the input of a layer to its output, enabling gradients to flow directly through the network and mitigating the vanishing gradient problem in deep architectures. This allows training of significantly deeper networks, as seen in resnets and adopted in modern transformer architectures to improve optimization and performance.
</figcaption>
</figure>
</div>
<p>This composition of building blocks creates emergent capabilities exceeding the sum of individual components. The self-attention mechanism, while building on previous attention concepts, enables novel forms of dynamic pattern processing. The arrangement of these componentsâ€”attention followed by feedforward layers, with skip connections and normalizationâ€”has proven sufficiently effective to become a template for new architectures.</p>
<p>Recent innovations in vision and language models follow this pattern of recombining building blocks. Vision Transformers<a href="#fn26" class="footnote-ref" id="fnref26" role="doc-noteref"><sup>26</sup></a> adapt the Transformer architecture to images while maintaining its essential components <span class="citation" data-cites="dosovitskiy2021image">(<a href="#ref-dosovitskiy2021image" role="doc-biblioref">Dosovitskiy et al. 2021</a>)</span>. Large language models scale up these patterns while introducing refinements like grouped-query attention or sliding window attention, yet still rely on the core building blocks established through this architectural evolution <span class="citation" data-cites="brown2020language">(<a href="#ref-brown2020language" role="doc-biblioref">Brown et al. 2020</a>)</span>. These modern architectural innovations demonstrate the principles of efficient scaling covered in <strong><a href="../efficient_ai/efficient_ai.html#sec-efficient-ai">Chapter 9: Efficient AI</a></strong>, while their practical implementation challenges and optimizations are explored in <strong><a href="../optimizations/optimizations.html#sec-model-optimizations">Chapter 10: Model Optimizations</a></strong>.</p>
<div class="no-row-height column-margin column-container"><div id="fn26"><p><sup>26</sup>&nbsp;<strong>Vision Transformers (ViTs)</strong>: Googleâ€™s 2021 breakthrough showed that pure transformers could match CNN performance on ImageNet by treating image patches as â€œwords.â€ ViTs split a <span class="math inline">\(224\times 224\)</span> image into <span class="math inline">\(16\times 16\)</span> patches (196 â€œtokensâ€), proving that attention mechanisms could replace convolutional inductive biases with sufficient data.</p></div><div id="ref-brown2020language" class="csl-entry" role="listitem">
Brown, Tom B, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, et al. 2020. <span>â€œLanguage Models Are Few-Shot Learners.â€</span> <em>Advances in Neural Information Processing Systems</em> 33: 1877â€“1901.
</div></div><p>The following comparison of primitive utilization across different neural network architectures shows modern architectures synthesizing and innovating upon previous approaches:</p>
<div id="tbl-primitive-comparison" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-primitive-comparison-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;2: <strong>Primitive Utilization</strong>: Neural network architectures differ in their core computational and memory access patterns, impacting hardware requirements and efficiency. Transformers uniquely combine matrix multiplication with attention mechanisms, resulting in random memory access and data movement patterns distinct from sequential rnns or strided cnns.
</figcaption>
<div aria-describedby="tbl-primitive-comparison-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<colgroup>
<col style="width: 20%">
<col style="width: 16%">
<col style="width: 16%">
<col style="width: 21%">
<col style="width: 20%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;"><strong>Primitive Type</strong></th>
<th style="text-align: left;"><strong>MLP</strong></th>
<th style="text-align: left;"><strong>CNN</strong></th>
<th style="text-align: left;"><strong>RNN</strong></th>
<th style="text-align: left;"><strong>Transformer</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><strong>Computational</strong></td>
<td style="text-align: left;">Matrix Multiplication</td>
<td style="text-align: left;">Convolution (Matrix Mult.)</td>
<td style="text-align: left;">Matrix Mult. + State Update</td>
<td style="text-align: left;">Matrix Mult. + Attention</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Memory Access</strong></td>
<td style="text-align: left;">Sequential</td>
<td style="text-align: left;">Strided</td>
<td style="text-align: left;">Sequential + Random</td>
<td style="text-align: left;">Random (Attention)</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>Data Movement</strong></td>
<td style="text-align: left;">Broadcast</td>
<td style="text-align: left;">Sliding Window</td>
<td style="text-align: left;">Sequential</td>
<td style="text-align: left;">Broadcast + Gather</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p>As shown in <a href="#tbl-primitive-comparison" class="quarto-xref">Table&nbsp;2</a>, Transformers combine elements from previous architectures while introducing new patterns. They retain the core matrix multiplication operations common to all architectures but introduce a more complex memory access pattern with their attention mechanism. Their data movement patterns blend the broadcast operations of MLPs with the gather operations reminiscent of more dynamic architectures.</p>
<p>This synthesis of primitives in Transformers shows modern architectures innovating by recombining and refining existing building blocks from the architectural progression established in <a href="#sec-dnn-architectures-architectural-principles-engineering-tradeoffs-89de" class="quarto-xref">Section&nbsp;1.1</a>, rather than inventing entirely new computational paradigms. This evolutionary process guides the development of future architectures and helps design of efficient systems to support them.</p>
<div id="quiz-question-sec-dnn-architectures-architectural-building-blocks-a575" class="callout callout-quiz-question">
<details class="callout-quiz-question fbx-default closebutton"><summary><strong>Self-Check: Question 1.6</strong></summary><div>
<ol type="1">
<li><p>Which architectural innovation introduced the concept of parameter sharing, significantly improving computational efficiency?</p>
<ol type="a">
<li>Multi-Layer Perceptrons (MLPs)</li>
<li>Convolutional Neural Networks (CNNs)</li>
<li>Recurrent Neural Networks (RNNs)</li>
<li>Transformers</li>
</ol></li>
<li><p>Explain how skip connections, originally introduced in ResNets, have influenced modern neural network architectures.</p></li>
<li><p>Order the following architectural innovations by their introduction in neural network evolution: (1) Attention Mechanisms, (2) Skip Connections, (3) Parameter Sharing, (4) Gating Mechanisms.</p></li>
<li><p>The introduction of ____ in CNNs allowed for the reuse of the same parameters across different parts of the input, enhancing computational efficiency.</p></li>
<li><p>In a production system, what are the system-level implications of using Transformer architectures, particularly in terms of memory access and data movement?</p></li>
</ol>
<p><a href="#quiz-answer-sec-dnn-architectures-architectural-building-blocks-a575" class="question-label">See Answers â†’</a></p>
</div></details>
</div>
</section>
</section>
<section id="sec-dnn-architectures-systemlevel-building-blocks-72f6" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="sec-dnn-architectures-systemlevel-building-blocks-72f6">System-Level Building Blocks</h2>
<p>Examination of different deep learning architectures enables distillation of their system requirements into primitives that underpin both hardware and software implementations. These primitives represent operations that cannot be decomposed further while maintaining their essential characteristics. Just as complex molecules are built from basic atoms, sophisticated neural networks are constructed from these operations.</p>
<section id="sec-dnn-architectures-core-computational-primitives-bd67" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-dnn-architectures-core-computational-primitives-bd67">Core Computational Primitives</h3>
<p>Three operations serve as the building blocks for all deep learning computations: matrix multiplication, sliding window operations, and dynamic computation. These operations are primitive because they cannot be further decomposed without losing their essential computational properties and efficiency characteristics.</p>
<p>Matrix multiplication represents the basic form of transforming sets of features. When we multiply a matrix of inputs by a matrix of weights, weâ€™re computing weighted combinations, which is the core operation of neural networks. For example, in our MNIST network, each 784-dimensional input vector multiplies with a <span class="math inline">\(784\times 100\)</span> weight matrix. This pattern appears everywhere: MLPs use it directly for layer computations, CNNs reshape convolutions into matrix multiplications (turning a <span class="math inline">\(3\times 3\)</span> convolution into a matrix operation, as illustrated in <a href="#fig-im2col-diagram" class="quarto-xref">Figure&nbsp;10</a>), and Transformers use it extensively in their attention mechanisms.</p>
<section id="sec-dnn-architectures-computational-building-blocks-c3c0" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="sec-dnn-architectures-computational-building-blocks-c3c0">Computational Building Blocks</h4>
<p>Modern neural networks operate through three computational patterns that appear across all architectures. These patterns explain how different architectures achieve their computational goals and why certain hardware optimizations are effective.</p>
<p>The detailed analysis of sparse computation patterns, including structured and unstructured sparsity, hardware-aware optimization strategies, and algorithm-hardware co-design principles, is addressed in <strong><a href="../optimizations/optimizations.html#sec-model-optimizations">Chapter 10: Model Optimizations</a></strong> and <strong><a href="../hw_acceleration/hw_acceleration.html#sec-ai-acceleration">Chapter 11: AI Acceleration</a></strong>.</p>
<div id="fig-im2col-diagram" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-env="figure" data-fig-pos="htb">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-im2col-diagram-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="b9c64687bff555767c7f2022da3f5ab9e7d582d5.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-10" title="Figure&nbsp;10: Convolution as Matrix Multiplication: Reshaping convolutional layers into matrix multiplications using the im2col technique, enables efficient computation using optimized BLAS libraries and allows for parallel processing on standard hardware. This transformation is crucial for accelerating cnns and forms the basis for implementing convolutions on diverse platforms."><img src="dnn_architectures_files/mediabag/b9c64687bff555767c7f2022da3f5ab9e7d582d5.svg" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-im2col-diagram-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;10: <strong>Convolution as Matrix Multiplication</strong>: Reshaping convolutional layers into matrix multiplications using the <code>im2col</code> technique, enables efficient computation using optimized BLAS libraries and allows for parallel processing on standard hardware. This transformation is crucial for accelerating cnns and forms the basis for implementing convolutions on diverse platforms.
</figcaption>
</figure>
</div>
<p>The im2col<a href="#fn27" class="footnote-ref" id="fnref27" role="doc-noteref"><sup>27</sup></a> (image to column) technique, developed by Intel in the 1990s, accomplishes matrix reshaping by unfolding overlapping image patches into columns of a matrix, as illustrated in <a href="#fig-im2col-diagram" class="quarto-xref">Figure&nbsp;10</a>. Each sliding window position in the convolution becomes a column in the transformed matrix, while the filter kernels are arranged as rows. This allows the convolution operation to be expressed as a standard GEMM (General Matrix Multiply) operation. The transformation trades memory consumptionâ€”duplicating data where windows overlapâ€”for computational efficiency, enabling CNNs to leverage decades of BLAS optimizations and achieving 5-10x speedups on CPUs. In modern systems, these matrix multiplications map to specific hardware and software implementations. Hardware accelerators provide specialized tensor cores that can perform thousands of multiply-accumulates in parallel; NVIDIAâ€™s A100 tensor cores can achieve up to 312 TFLOPS for mixed-precision (TF32) workloads, or 156 TFLOPS for FP32 through massive parallelization of these operations. Software frameworks like PyTorch and TensorFlow automatically map these high-level operations to optimized matrix libraries (NVIDIA <a href="https://developer.nvidia.com/cublas">cuBLAS</a>, Intel <a href="https://www.intel.com/content/www/us/en/developer/tools/oneapi/onemkl.html#gs.kxb9ve">MKL</a>) that exploit these hardware capabilities.</p>
<div class="no-row-height column-margin column-container"><div id="fn27"><p><sup>27</sup>&nbsp;<strong>im2col (Image to Column)</strong>: A data layout transformation developed by Intel in the 1990s that converts convolution operations into matrix multiplications by unfolding image patches into columns. This approach trades memory consumption (through data duplication) for computational efficiency, enabling CNNs to leverage decades of GEMM optimizations and achieving 5-10x speedups on CPUs.</p></div><div id="fn28"><p><sup>28</sup>&nbsp;<strong>Systolic Array</strong>: A network of processing elements that rhythmically compute and pass data through neighbors, like a â€œheartbeatâ€ of computation. Invented by H.T. Kung and Charles Leiserson in 1978, systolic arrays achieve high throughput by overlapping computation with data movementâ€”Googleâ€™s TPU systolic arrays perform 65,536 multiply-accumulate operations per clock cycle.</p></div></div><p>Sliding window operations compute local relationships by applying the same operation to chunks of data. In CNNs processing MNIST images, a <span class="math inline">\(3\times 3\)</span> convolution filter slides across the <span class="math inline">\(28\times 28\)</span> input, requiring <span class="math inline">\(26\times 26\)</span> windows of computation, assuming a stride size of 1. Modern hardware accelerators implement this through specialized memory access patterns and data buffering schemes that optimize data reuse. For example, Googleâ€™s TPU uses a <span class="math inline">\(128\times 128\)</span> systolic array<a href="#fn28" class="footnote-ref" id="fnref28" role="doc-noteref"><sup>28</sup></a> where data flows systematically through processing elements, allowing each input value to be reused across multiple computations without accessing memory.</p>
<p>Dynamic computation, where the operation itself depends on the input data, emerged prominently with attention mechanisms but represents a capability needed for adaptive processing. In Transformer attention, each query dynamically determines its interaction weights with all keys; for a sequence of length 512, 512 different weight patterns must be computed on the fly. Unlike fixed patterns where the computation graph is known in advance, dynamic computation requires runtime decisions. This creates specific implementation challenges: hardware must provide flexible data routing (modern GPUs employ dynamic scheduling) and support variable computation patterns, while software frameworks require efficient mechanisms for handling data-dependent execution paths (PyTorchâ€™s dynamic computation graphs, TensorFlowâ€™s dynamic control flow).</p>
<p>These primitives combine in sophisticated ways in modern architectures. A Transformer layer processing a sequence of 512 tokens demonstrates this clearly: it uses matrix multiplications for feature projections (<span class="math inline">\(512\times 512\)</span> operations implemented through tensor cores), may employ sliding windows for efficient attention over long sequences (using specialized memory access patterns for local regions), and requires dynamic computation for attention weights (computing <span class="math inline">\(512\times 512\)</span> attention patterns at runtime). The way these primitives interact creates specific demands on system design, ranging from memory hierarchy organization to computation scheduling.</p>
<p>The building blocks weâ€™ve discussed help explain why certain hardware features exist (like tensor cores for matrix multiplication) and why software frameworks organize computations in particular ways (like batching similar operations together). As we move from computational primitives to consider memory access and data movement patterns, recognizing how these operations shape the demands placed on memory systems becomes essential and data transfer mechanisms. The way computational primitives are implemented and combined has direct implications for how data needs to be stored, accessed, and moved within the system.</p>
</section>
</section>
<section id="sec-dnn-architectures-memory-access-primitives-4e2e" class="level3">
<h3 class="anchored" data-anchor-id="sec-dnn-architectures-memory-access-primitives-4e2e">Memory Access Primitives</h3>
<p>The efficiency of deep learning models depends heavily on memory access and management. Memory access often constitutes the primary bottleneck in modern ML systems; even though a matrix multiplication unit may be capable of performing thousands of operations per cycle, it will remain idle if data is not available at the requisite time. For example, accessing data from DRAM typically requires hundreds of cycles, while on-chip computation requires only a few cycles.</p>
<p>Three memory access patterns dominate in deep learning architectures: sequential access, strided access, and random access. Each pattern creates different demands on the memory system and offers different opportunities for optimization.</p>
<p>Sequential access is the simplest and most efficient pattern. Consider an MLP performing matrix multiplication with a batch of MNIST images: it needs to access both the <span class="math inline">\(784\times 100\)</span> weight matrix and the input vectors sequentially. This pattern maps well to modern memory systems; DRAM can operate in burst mode for sequential reads (achieving up to 400 GB/s in modern GPUs), and hardware prefetchers can effectively predict and fetch upcoming data. Software frameworks optimize for this by ensuring data is laid out contiguously in memory and aligning data to cache line boundaries.</p>
<p>Strided access appears prominently in CNNs, where each output position needs to access a window of input values at regular intervals. For a CNN processing MNIST images with <span class="math inline">\(3\times 3\)</span> filters, each output position requires accessing 9 input values with a stride matching the input width. While less efficient than sequential access, hardware supports this through pattern-aware caching strategies and specialized memory controllers. Software frameworks often transform these strided patterns into sequential access through data layout reorganization, where the im2col transformation in deep learning frameworks converts convolutionâ€™s strided access into efficient matrix multiplications.</p>
<p>Random access poses the greatest challenge for system efficiency. In a Transformer processing a sequence of 512 tokens, each attention operation potentially needs to access any position in the sequence, creating unpredictable memory access patterns. Random access can severely impact performance through cache misses (potentially causing 100+ cycle stalls per access) and unpredictable memory latencies. Systems address this through large cache hierarchies (modern GPUs have several MB of L2 cache) and sophisticated prefetching strategies, while software frameworks employ techniques like attention pattern pruning to reduce random access requirements.</p>
<p>These different memory access patterns contribute to the overall memory requirements of each architecture. To illustrate this, <a href="#tbl-arch-complexity" class="quarto-xref">Table&nbsp;3</a> compares the memory complexity of MLPs, CNNs, RNNs, and Transformers.</p>
<div id="tbl-arch-complexity" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-arch-complexity-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;3: <strong>Memory Access Complexity</strong>: Different neural network architectures exhibit varying memory access patterns and storage requirements, impacting system performance and scalability. Parameter storage scales with input dependency and model size, while activation storage represents a significant runtime cost, particularly for sequence-based models where rnns offer a parameter efficiency advantage when sequence length exceeds hidden state size (<span class="math inline">\(n &gt; h\)</span>).
</figcaption>
<div aria-describedby="tbl-arch-complexity-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<colgroup>
<col style="width: 15%">
<col style="width: 19%">
<col style="width: 19%">
<col style="width: 23%">
<col style="width: 19%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;"><strong>Architecture</strong></th>
<th style="text-align: left;"><strong>Input Dependency</strong></th>
<th style="text-align: left;"><strong>Parameter Storage</strong></th>
<th style="text-align: left;"><strong>Activation Storage</strong></th>
<th style="text-align: left;"><strong>Scaling Behavior</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><strong>MLP</strong></td>
<td style="text-align: left;">Linear</td>
<td style="text-align: left;"><span class="math inline">\(O(N \times W)\)</span></td>
<td style="text-align: left;"><span class="math inline">\(O(B \times W)\)</span></td>
<td style="text-align: left;">Predictable</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>CNN</strong></td>
<td style="text-align: left;">Constant</td>
<td style="text-align: left;"><span class="math inline">\(O(K \times C)\)</span></td>
<td style="text-align: left;"><span class="math inline">\(O(B\times H_{\text{img}}\)</span> <span class="math inline">\(\times W_{\text{img}})\)</span></td>
<td style="text-align: left;">Efficient</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>RNN</strong></td>
<td style="text-align: left;">Linear</td>
<td style="text-align: left;"><span class="math inline">\(O(h^2)\)</span></td>
<td style="text-align: left;"><span class="math inline">\(O(B \times T \times h)\)</span></td>
<td style="text-align: left;">Challenging</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Transformer</strong></td>
<td style="text-align: left;">Quadratic</td>
<td style="text-align: left;"><span class="math inline">\(O(N \times d)\)</span></td>
<td style="text-align: left;"><span class="math inline">\(O(B \times N^2)\)</span></td>
<td style="text-align: left;">Problematic</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p>Where:</p>
<ul>
<li><span class="math inline">\(N\)</span>: Input or sequence size</li>
<li><span class="math inline">\(W\)</span>: Layer width</li>
<li><span class="math inline">\(B\)</span>: Batch size</li>
<li><span class="math inline">\(K\)</span>: Kernel size</li>
<li><span class="math inline">\(C\)</span>: Number of channels</li>
<li><span class="math inline">\(H_{\text{img}}\)</span>: Height of input feature map (CNN)</li>
<li><span class="math inline">\(W_{\text{img}}\)</span>: Width of input feature map (CNN)</li>
<li><span class="math inline">\(h\)</span>: Hidden state size (RNN)</li>
<li><span class="math inline">\(T\)</span>: Sequence length</li>
<li><span class="math inline">\(d\)</span>: Model dimensionality</li>
</ul>
<p><a href="#tbl-arch-complexity" class="quarto-xref">Table&nbsp;3</a> reveals how memory requirements scale with different architectural choices. The quadratic scaling of activation storage in Transformers, for instance, highlights the need for large memory capacities and efficient memory management in systems designed for Transformer-based workloads. In contrast, CNNs exhibit more favorable memory scaling due to their parameter sharing and localized processing. These memory access patterns complement the computational scaling behaviors examined later in <a href="#tbl-computational-complexity" class="quarto-xref">Table&nbsp;6</a>, completing the picture of each architectureâ€™s resource requirements. These memory complexity considerations inform system-level design decisions, such as choosing memory hierarchy configurations and developing memory optimization strategies.</p>
<p>The impact of these patterns becomes clear when we consider data reuse opportunities. In CNNs, each input pixel participates in multiple convolution windows (typically 9 times for a <span class="math inline">\(3\times 3\)</span> filter), making effective data reuse necessary for performance. Modern GPUs provide multi-level cache hierarchies (L1, L2, shared memory) to capture this reuse, while software techniques like loop tiling ensure data remains in cache once loaded.</p>
<p>Working set size, the amount of data needed simultaneously for computation, varies dramatically across architectures. An MLP layer processing MNIST images might need only a few hundred KB (weights plus activations), while a Transformer processing long sequences can require several MB just for storing attention patterns. These differences directly influence hardware design choices, like the balance between compute units and on-chip memory, and software optimizations like activation checkpointing or attention approximation techniques.</p>
<p>Understanding these memory access patterns is essential as architectures evolve. The shift from CNNs to Transformers, for instance, has driven the development of hardware with larger on-chip memories and more sophisticated caching strategies to handle increased working sets and more dynamic access patterns. Future architectures will likely continue to be shaped by their memory access characteristics as much as their computational requirements.</p>
</section>
<section id="sec-dnn-architectures-data-movement-primitives-101a" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-dnn-architectures-data-movement-primitives-101a">Data Movement Primitives</h3>
<p>While computational and memory access patterns define what operations occur where, data movement primitives characterize how information flows through the system. These patterns are key because data movement often consumes more time and energy than computation itself, as moving data from off-chip memory typically requires 100-1000<span class="math inline">\(\times\)</span> more energy than performing a floating-point operation.</p>
<p>Four data movement patterns are prevalent in deep learning architectures: broadcast, scatter, gather, and reduction. <a href="#fig-collective-comm" class="quarto-xref">Figure&nbsp;11</a> illustrates these patterns and their relationships. Broadcast operations send the same data to multiple destinations simultaneously. In matrix multiplication with batch size 32, each weight must be broadcast to process different inputs in parallel. Modern hardware supports this through specialized interconnects, NVIDIA GPUs provide hardware multicast capabilities, achieving up to 600 GB/s broadcast bandwidth, while TPUs use dedicated broadcast buses. Software frameworks optimize broadcasts by restructuring computations (like matrix tiling) to maximize data reuse.</p>
<div id="fig-collective-comm" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-env="figure" data-fig-pos="htb">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-collective-comm-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="f8436bde574fcdbb7c55649318497efa63ec8c95.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-11" title="Figure&nbsp;11: Collective Communication Patterns: Deep learning training and inference frequently require data exchange between processing units; this figure outlines four core patterns (broadcast, scatter, gather, and reduction) that define how data moves within a distributed system and impact overall performance. Understanding these patterns enables optimization of data movement, critical because communication costs often dominate computation in modern machine learning workloads."><img src="dnn_architectures_files/mediabag/f8436bde574fcdbb7c55649318497efa63ec8c95.svg" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-collective-comm-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;11: <strong>Collective Communication Patterns</strong>: Deep learning training and inference frequently require data exchange between processing units; this figure outlines four core patterns (broadcast, scatter, gather, and reduction) that define how data moves within a distributed system and impact overall performance. Understanding these patterns enables optimization of data movement, critical because communication costs often dominate computation in modern machine learning workloads.
</figcaption>
</figure>
</div>
<p>Scatter operations distribute different elements to different destinations. When parallelizing a <span class="math inline">\(512\times 512\)</span> matrix multiplication across GPU cores, each core receives a subset of the computation. This parallelization is important for performance but challenging, as memory conflicts and load imbalance, can reduce efficiency by 50% or more. Hardware provides flexible interconnects (like NVIDIAâ€™s NVLink offering 600 GB/s bi-directional bandwidth), while software frameworks employ sophisticated work distribution algorithms to maintain high utilization.</p>
<p>Gather operations collect data from multiple sources. In Transformer attention with sequence length 512, each query must gather information from 512 different key-value pairs. These irregular access patterns are challenging, random gathering can be <span class="math inline">\(10\times\)</span> slower than sequential access. Hardware supports this through high-bandwidth interconnects and large caches, while software frameworks employ techniques like attention pattern pruning to reduce gathering overhead.</p>
<p>Reduction operations combine multiple values into a single result through operations like summation. When computing attention scores in Transformers or layer outputs in MLPs, efficient reduction is essential. Hardware implements tree-structured reduction networks (reducing latency from <span class="math inline">\(O(n)\)</span> to <span class="math inline">\(O(\log n)\)</span>), while software frameworks use optimized parallel reduction algorithms that can achieve near-theoretical peak performance.</p>
<p>These patterns combine in sophisticated ways. A Transformer attention operation with sequence length 512 and batch size 32 involves:</p>
<ul>
<li>Broadcasting query vectors (<span class="math inline">\(512\times 64\)</span> elements)</li>
<li>Gathering relevant keys and values (<span class="math inline">\(512\times 512\times 64\)</span> elements)</li>
<li>Reducing attention scores (<span class="math inline">\(512\times 512\)</span> elements per sequence)</li>
</ul>
<p>The evolution from CNNs to Transformers has increased reliance on gather and reduction operations, driving hardware innovations like more flexible interconnects and larger on-chip memories. As models grow (some now exceeding 100 billion parameters<a href="#fn29" class="footnote-ref" id="fnref29" role="doc-noteref"><sup>29</sup></a>), efficient data movement becomes increasingly critical, leading to innovations like near-memory processing and sophisticated data flow optimizations.</p>
<div class="no-row-height column-margin column-container"><div id="fn29"><p><sup>29</sup>&nbsp;<strong>Parameter Scaling</strong>: The leap from AlexNetâ€™s 62 million parameters (2012) to GPT-3â€™s 175 billion parameters (2020) represents a 3,000x increase in just 8 years. Modern models like GPT-4 may exceed 1 trillion parameters, requiring specialized distributed computing infrastructure and consuming megawatts of power during training.</p></div></div></section>
<section id="sec-dnn-architectures-system-design-impact-cd41" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-dnn-architectures-system-design-impact-cd41">System Design Impact</h3>
<p>The computational, memory access, and data movement primitives weâ€™ve explored form the foundational requirements that shape the design of systems for deep learning. The way these primitives influence hardware design, create common bottlenecks, and drive trade-offs is important for developing efficient and effective ML systems.</p>
<p>One of the most significant impacts of these primitives on system design is the push towards specialized hardware. The prevalence of matrix multiplications and convolutions in deep learning has led to the development of tensor processing units (TPUs)<a href="#fn30" class="footnote-ref" id="fnref30" role="doc-noteref"><sup>30</sup></a> and tensor cores in GPUs, which are specifically designed to perform these operations efficiently.</p>
<div class="no-row-height column-margin column-container"><div id="fn30"><p><sup>30</sup>&nbsp;<strong>Tensor Processing Units</strong>: Googleâ€™s TPUs emerged from their need to run neural networks on billions of searches daily. First deployed secretly in 2015, TPUs achieve 15-30x better performance per watt than GPUs for inference. The TPUâ€™s <span class="math inline">\(128\times 128\)</span> systolic array performs 65,536 multiply-accumulate operations per clock cycle, revolutionizing AI hardware design. These specialized units can perform many multiply-accumulate operations in parallel, dramatically accelerating the core computations of neural networks.</p></div><div id="fn31"><p><sup>31</sup>&nbsp;<strong>High Bandwidth Memory (HBM)</strong>: Stacked DRAM technology providing 1+ TB/s bandwidth compared to 500 GB/s for traditional GDDR6, developed by AMD and Hynix. HBM enables the massive data movement required by modern AI workloadsâ€”GPT-3 training requires moving 1.75 TB of parameters through memory during each forward pass.</p></div><div id="fn32"><p><sup>32</sup>&nbsp;<strong>Scratchpad Memory</strong>: Programmer-controlled on-chip memory providing predictable, fast access without cache management overhead. Unlike caches, scratchpads require explicit data movement but enable precise control over memory allocationâ€”critical for neural network accelerators where memory access patterns are known and performance must be deterministic.</p></div></div><p>Memory systems have also been profoundly influenced by the demands of deep learning primitives. The need to support both sequential and random access patterns efficiently has driven the development of sophisticated memory hierarchies. High-bandwidth memory (HBM)<a href="#fn31" class="footnote-ref" id="fnref31" role="doc-noteref"><sup>31</sup></a> has become common in AI accelerators to support the massive data movement requirements, especially for operations like attention mechanisms in Transformers. On-chip memory hierarchies have grown in complexity, with multiple levels of caching and scratchpad memories<a href="#fn32" class="footnote-ref" id="fnref32" role="doc-noteref"><sup>32</sup></a> to support the diverse working set sizes of different neural network layers.</p>
<p>The data movement primitives have particularly influenced the design of interconnects and on-chip networks. The need to support efficient broadcasts, gathers, and reductions has led to the development of more flexible and higher-bandwidth interconnects. Some AI chips now feature specialized networks-on-chip designed to accelerate common data movement patterns in neural networks.</p>
<p><a href="#tbl-sys-design-implications" class="quarto-xref">Table&nbsp;4</a> summarizes the system implications of these primitives:</p>
<div id="tbl-sys-design-implications" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-sys-design-implications-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;4: <strong>Primitive-Hardware Co-Design</strong>: Efficient machine learning systems require tight integration between algorithmic primitives and underlying hardware; this table maps common primitives to specific hardware accelerations and software optimizations, highlighting key challenges in their implementation. Specialized hardware, such as tensor cores and datapaths, address the computational demands of primitives like matrix multiplication and sliding windows, while software techniques like batching and dynamic graph execution further enhance performance.
</figcaption>
<div aria-describedby="tbl-sys-design-implications-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<colgroup>
<col style="width: 24%">
<col style="width: 24%">
<col style="width: 24%">
<col style="width: 25%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;"><strong>Primitive</strong></th>
<th style="text-align: left;"><strong>Hardware Impact</strong></th>
<th style="text-align: left;"><strong>Software Optimization</strong></th>
<th style="text-align: left;"><strong>Key Challenges</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><strong>Matrix Multiplication</strong></td>
<td style="text-align: left;">Tensor Cores</td>
<td style="text-align: left;">Batching, GEMM libraries</td>
<td style="text-align: left;">Parallelization, precision</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Sliding Window</strong></td>
<td style="text-align: left;">Specialized datapaths</td>
<td style="text-align: left;">Data layout optimization</td>
<td style="text-align: left;">Stride handling</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>Dynamic Computation</strong></td>
<td style="text-align: left;">Flexible routing</td>
<td style="text-align: left;">Dynamic graph execution</td>
<td style="text-align: left;">Load balancing</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Sequential Access</strong></td>
<td style="text-align: left;">Burst mode DRAM</td>
<td style="text-align: left;">Contiguous allocation</td>
<td style="text-align: left;">Access latency</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>Random Access</strong></td>
<td style="text-align: left;">Large caches</td>
<td style="text-align: left;">Memory-aware scheduling</td>
<td style="text-align: left;">Cache misses</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Broadcast</strong></td>
<td style="text-align: left;">Specialized interconnects</td>
<td style="text-align: left;">Operation fusion</td>
<td style="text-align: left;">Bandwidth</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>Gather/Scatter</strong></td>
<td style="text-align: left;">High-bandwidth memory</td>
<td style="text-align: left;">Work distribution</td>
<td style="text-align: left;">Load balancing</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p>Despite these advancements, several bottlenecks persist in deep learning models. Memory bandwidth often remains a key limitation, particularly for models with large working sets or those that require frequent random access. The energy cost of data movement, especially between off-chip memory and processing units, continues to be a significant concern. For large-scale models, the communication overhead in distributed training can become a bottleneck, limiting scaling efficiency.</p>
<section id="sec-dnn-architectures-energy-consumption-analysis-across-architectures-b681" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="sec-dnn-architectures-energy-consumption-analysis-across-architectures-b681">Energy Consumption Analysis Across Architectures</h4>
<p>Energy consumption patterns vary dramatically across neural network architectures, with implications for both datacenter deployment and edge computing scenarios. Each architectural pattern exhibits distinct energy characteristics that inform deployment decisions and optimization strategies.</p>
<p>Dense matrix operations in MLPs achieve excellent arithmetic intensity<a href="#fn33" class="footnote-ref" id="fnref33" role="doc-noteref"><sup>33</sup></a> (computation per data movement) but consume significant absolute energy. Each multiply-accumulate operation consumes approximately 4.6pJ, while data movement from DRAM costs 640pJ per 32-bit value. For typical MLP inference, 70-80% of energy goes to data movement rather than computation, making memory bandwidth optimization critical for energy efficiency.</p>
<div class="no-row-height column-margin column-container"><div id="fn33"><p><sup>33</sup>&nbsp;<strong>Arithmetic Intensity</strong>: The ratio of floating-point operations to memory accesses, measured in FLOPS per byte. High arithmetic intensity (&gt;10 FLOPS/byte) enables efficient hardware utilization, while low intensity (&lt;1 FLOPS/byte) makes workloads memory-bound. Attention mechanisms typically have low arithmetic intensity, explaining their energy inefficiency.</p></div></div><p>Convolutional operations reduce energy consumption through data reuse but exhibit variable efficiency depending on implementation. Im2col-based convolution implementations trade memory for simplicity, often doubling memory requirements and energy consumption. Direct convolution implementations achieve 3-5x better energy efficiency by eliminating redundant data movement, particularly for larger kernel sizes.</p>
<p>Sequential processing in RNNs creates energy efficiency opportunities through temporal data reuse. The constant memory footprint of RNN hidden states enables aggressive caching strategies, reducing DRAM access energy by 80-90% for long sequences. The sequential dependencies limit parallelization opportunities, often resulting in suboptimal hardware utilization and higher energy per operation.</p>
<p>Attention mechanisms in Transformers exhibit the highest energy consumption per operation due to quadratic scaling and complex data movement patterns. Self-attention operations consume 2-3x more energy per FLOP than standard matrix multiplication due to irregular memory access patterns and the need to store attention matrices. This energy cost scales quadratically with sequence length, making long-sequence processing energy-prohibitive without architectural modifications.</p>
<p>System designers must navigate trade-offs in supporting different primitives, each with unique characteristics that influence system design and performance. For example, optimizing for the dense matrix operations common in MLPs and CNNs might come at the cost of flexibility needed for the more dynamic computations in attention mechanisms. Supporting large working sets for Transformers might require sacrificing energy efficiency.</p>
<p>Balancing these trade-offs requires consideration of the target workloads and deployment scenarios. Understanding the nature of each primitive guides the development of both hardware and software optimizations in ML systems, allowing designers to make informed decisions about system architecture and resource allocation.</p>
<p>The analysis of architectural patterns, computational primitives, and system implications establishes the foundation for addressing a practical challenge: how do engineers systematically choose the right architecture for their specific problem? The diversity of neural network architectures, each optimized for different data patterns and computational constraints, requires a structured approach to architecture selection. This selection process must consider not only algorithmic performance but also deployment constraints covered in <strong><a href="../ml_systems/ml_systems.html#sec-ml-systems">Chapter 2: ML Systems</a></strong> and operational efficiency requirements detailed in <strong><a href="../ops/ops.html#sec-ml-operations">Chapter 13: ML Operations</a></strong>.</p>
<div id="quiz-question-sec-dnn-architectures-systemlevel-building-blocks-72f6" class="callout callout-quiz-question">
<details class="callout-quiz-question fbx-default closebutton"><summary><strong>Self-Check: Question 1.7</strong></summary><div>
<ol type="1">
<li><p>Which of the following operations is considered a core computational primitive in deep learning architectures?</p>
<ol type="a">
<li>Dropout</li>
<li>Gradient descent</li>
<li>Backpropagation</li>
<li>Matrix multiplication</li>
</ol></li>
<li><p>Explain how the im2col technique transforms convolution operations into matrix multiplications and its implications for computational efficiency.</p></li>
<li><p>True or False: Dynamic computation in neural networks requires runtime decisions and poses challenges for hardware design.</p></li>
<li><p>The systolic array used in Googleâ€™s TPU is an example of a specialized hardware component designed to optimize ____ operations.</p></li>
<li><p>In a production system, what trade-offs must be considered when designing memory systems to support both sequential and random access patterns in deep learning models?</p></li>
</ol>
<p><a href="#quiz-answer-sec-dnn-architectures-systemlevel-building-blocks-72f6" class="question-label">See Answers â†’</a></p>
</div></details>
</div>
</section>
</section>
</section>
<section id="sec-dnn-architectures-architecture-selection-framework-7a37" class="level2">
<h2 class="anchored" data-anchor-id="sec-dnn-architectures-architecture-selection-framework-7a37">Architecture Selection Framework</h2>
<p>The exploration of neural network architectures, from dense MLPs to dynamic Transformers, demonstrates how each design embodies specific assumptions about data structure and computational patterns. MLPs assume arbitrary feature relationships, CNNs exploit spatial locality, RNNs capture temporal dependencies, and Transformers model complex relational patterns. For practitioners facing real-world problems, a question emerges: how to systematically select the appropriate architecture for a specific use case?</p>
<p>The diversity of available architectures overwhelms practitioners, when each claims superiority for different scenarios. Successful architecture selection requires understanding principles rather than following trends: matching data characteristics to architectural strengths, evaluating computational constraints against system capabilities, and balancing accuracy requirements with deployment realities.</p>
<p>This systematic approach to architecture selection draws upon the computational patterns and system implications explored in the preceding analysis. By understanding how different architectures process information and their corresponding resource requirements, engineers can make informed decisions that align with both problem requirements and practical constraints. The framework integrates principles from efficient AI design <strong><a href="../efficient_ai/efficient_ai.html#sec-efficient-ai">Chapter 9: Efficient AI</a></strong> with practical deployment considerations as discussed in ML operations <strong><a href="../ops/ops.html#sec-ml-operations">Chapter 13: ML Operations</a></strong>.</p>
<section id="sec-dnn-architectures-datatoarchitecture-mapping-0b9c" class="level3">
<h3 class="anchored" data-anchor-id="sec-dnn-architectures-datatoarchitecture-mapping-0b9c">Data-to-Architecture Mapping</h3>
<p>The first step in systematic architecture selection involves understanding how different data types align with architectural strengths. Each neural network architecture evolved to address specific patterns in data: MLPs handle arbitrary relationships in tabular data, CNNs exploit spatial locality in images, RNNs capture temporal dependencies in sequences, and Transformers model complex relational patterns where any element might influence any other.</p>
<p>This alignment is not coincidental. It reflects computational trade-offs. Architectures that match data characteristics can leverage natural structure for efficiency, while mismatched architectures must work against their design assumptions, leading to poor performance or excessive resource consumption.</p>
<p><a href="#tbl-architecture-selection" class="quarto-xref">Table&nbsp;5</a> provides a systematic framework for matching data characteristics to appropriate architectures:</p>
<div id="tbl-architecture-selection" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-architecture-selection-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;5: <strong>Architecture Selection Framework</strong>: Systematic matching of data characteristics to neural network architectures based on computational requirements and pattern types.
</figcaption>
<div aria-describedby="tbl-architecture-selection-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<colgroup>
<col style="width: 16%">
<col style="width: 18%">
<col style="width: 33%">
<col style="width: 29%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;"><strong>Architecture</strong></th>
<th style="text-align: left;"><strong>Data Type</strong></th>
<th style="text-align: left;"><strong>Key Characteristics</strong></th>
<th style="text-align: left;"><strong>Example Applications</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><strong>MLPs</strong></td>
<td style="text-align: left;">Tabular/Structured</td>
<td style="text-align: left;">â€¢ No spatial/temporal â€¢&nbsp;Arbitrary&nbsp;relationships â€¢&nbsp;Dense&nbsp;connectivity</td>
<td style="text-align: left;">â€¢ Financial modeling â€¢&nbsp;Medical&nbsp;measurements â€¢&nbsp;Recommendation systems</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>CNNs</strong></td>
<td style="text-align: left;">Spatial/Grid-like</td>
<td style="text-align: left;">â€¢ Local patterns â€¢&nbsp;Translation&nbsp;equivariance â€¢&nbsp;Parameter&nbsp;sharing</td>
<td style="text-align: left;">â€¢ Image recognition â€¢&nbsp;2D&nbsp;sensor&nbsp;data â€¢&nbsp;Signal&nbsp;processing</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>RNNs</strong></td>
<td style="text-align: left;">Sequential/Temporal</td>
<td style="text-align: left;">â€¢ Temporal dependencies â€¢&nbsp;Variable&nbsp;length â€¢&nbsp;Memory&nbsp;across time</td>
<td style="text-align: left;">â€¢ Time series forecasting â€¢&nbsp;Simple language tasks â€¢&nbsp;Speech recognition</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Transformers</strong></td>
<td style="text-align: left;">Complex Relational</td>
<td style="text-align: left;">â€¢ Long-range dependencies â€¢&nbsp;Attention&nbsp;mechanisms â€¢&nbsp;Dynamic relationships</td>
<td style="text-align: left;">â€¢ Language understanding â€¢&nbsp;Machine translation â€¢&nbsp;Complex reasoning tasks</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p>While data characteristics guide initial architecture selection, computational constraints often determine final feasibility. Understanding the scaling behavior of each architecture enables realistic resource planning and deployment decisions.</p>
</section>
<section id="sec-dnn-architectures-computational-complexity-considerations-93fb" class="level3">
<h3 class="anchored" data-anchor-id="sec-dnn-architectures-computational-complexity-considerations-93fb">Computational Complexity Considerations</h3>
<p>Architecture selection must account for computational and memory trade-offs that determine deployment feasibility. Each architecture exhibits distinct scaling behaviors that create different bottlenecks as problem size increases. Understanding these patterns enables realistic resource planning and prevents costly architectural mismatches during deployment.</p>
<p>The computational profile of each architecture reflects its underlying design philosophy. Dense architectures like MLPs prioritize representational capacity through full connectivity, while structured architectures like CNNs achieve efficiency through parameter sharing and locality assumptions. Sequential architectures like RNNs trade parallelization for memory efficiency, while attention-based architectures like Transformers exchange memory for computational flexibility. For completeness, we examine these same architectures from both computational scaling and memory access perspectives (see <a href="#tbl-arch-complexity" class="quarto-xref">Table&nbsp;3</a>), as each viewpoint reveals different optimization opportunities and system design considerations.</p>
<p><a href="#tbl-computational-complexity" class="quarto-xref">Table&nbsp;6</a> summarizes the key computational characteristics of each architecture:</p>
<div id="tbl-computational-complexity" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-computational-complexity-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;6: <strong>Computational Complexity Comparison</strong>: Scaling behaviors and resource requirements for major neural network architectures. Variables: <span class="math inline">\(d\)</span> = dimension, <span class="math inline">\(h\)</span> = hidden size, <span class="math inline">\(k\)</span> = kernel size, <span class="math inline">\(c\)</span>&nbsp;=&nbsp;channels, <span class="math inline">\(H,W\)</span> = spatial dimensions, <span class="math inline">\(T\)</span> = time steps, <span class="math inline">\(n\)</span> = sequence length, <span class="math inline">\(b\)</span> = batch size.
</figcaption>
<div aria-describedby="tbl-computational-complexity-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<colgroup>
<col style="width: 15%">
<col style="width: 21%">
<col style="width: 21%">
<col style="width: 20%">
<col style="width: 19%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;"><strong>Architecture</strong></th>
<th style="text-align: left;"><strong>Parameters</strong></th>
<th style="text-align: left;"><strong>Forward Pass</strong></th>
<th style="text-align: left;"><strong>Memory</strong></th>
<th style="text-align: left;"><strong>Parallelization</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><strong>MLPs</strong></td>
<td style="text-align: left;"><span class="math inline">\(O(d_{\text{in}}\times\)</span> <span class="math inline">\(d_{\text{out}})\)</span> per&nbsp;layer</td>
<td style="text-align: left;"><span class="math inline">\(O(d_{\text{in}}\times\)</span> <span class="math inline">\(d_{\text{out}})\)</span> per&nbsp;layer</td>
<td style="text-align: left;"><span class="math inline">\(O(d^2)\)</span> weights <span class="math inline">\(O(d\times b)\)</span> activations</td>
<td style="text-align: left;">Excellent Matrix ops parallel</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>CNNs</strong></td>
<td style="text-align: left;"><span class="math inline">\(O(k^2\times\)</span> <span class="math inline">\(c_{\text{in}}\times\)</span> <span class="math inline">\(c_{\text{out}})\)</span> per layer</td>
<td style="text-align: left;"><span class="math inline">\(O(H\times W\times\)</span> <span class="math inline">\(k^2\times\)</span> <span class="math inline">\(c_{\text{in}}\times\)</span> <span class="math inline">\(c_{\text{out}})\)</span></td>
<td style="text-align: left;"><span class="math inline">\(O(H\times W\times c)\)</span> features <span class="math inline">\(O(k^2\times c^2)\)</span> weights</td>
<td style="text-align: left;">Good Spatial independence</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>RNNs</strong></td>
<td style="text-align: left;"><span class="math inline">\(O(h^2+h\times d)\)</span> total</td>
<td style="text-align: left;"><span class="math inline">\(O(T\times h^2)\)</span> for <span class="math inline">\(T\)</span> time&nbsp;steps</td>
<td style="text-align: left;"><span class="math inline">\(O(h)\)</span> hidden state (constant)</td>
<td style="text-align: left;">Poor Sequential deps</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Transformers</strong></td>
<td style="text-align: left;"><span class="math inline">\(O(d^2)\)</span> projections <span class="math inline">\(O(d^2\times h)\)</span> multi-head</td>
<td style="text-align: left;"><span class="math inline">\(O(n^2\times d+n\)</span> <span class="math inline">\(\times dÂ²)\)</span> per layer</td>
<td style="text-align: left;"><span class="math inline">\(O(n^2)\)</span> attention <span class="math inline">\(O(n\times d)\)</span> sequences</td>
<td style="text-align: left;">Excellent (positions) Limited by memory</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<section id="sec-dnn-architectures-scalability-production-considerations-dcb0" class="level4">
<h4 class="anchored" data-anchor-id="sec-dnn-architectures-scalability-production-considerations-dcb0">Scalability and Production Considerations</h4>
<p>Production deployment introduces constraints beyond algorithmic performance, including latency requirements, memory limitations, energy budgets, and fault tolerance needs. Each architecture exhibits distinct production characteristics that determine real-world feasibility.</p>
<p>MLPs and CNNs scale well across multiple devices through data parallelism, achieving near-linear speedups with proper batch size scaling. RNNs face parallelization challenges due to sequential dependencies, requiring pipeline parallelism or other specialized techniques. Transformers achieve excellent parallelization across sequence positions but suffer from quadratic memory scaling that limits batch sizes and effective utilization.</p>
<p>MLPs provide predictable latency proportional to layer size, making them suitable for real-time applications with strict SLA requirements. CNNs exhibit variable latency depending on implementation strategy and hardware capabilities, with optimized implementations achieving sub-millisecond inference. RNNs create latency dependencies on sequence length, making them challenging for interactive applications. Transformers provide excellent throughput for batch processing but struggle with single-inference latency due to attention overhead.</p>
<p>Memory requirements vary significantly across architectures in production environments. MLPs require fixed memory proportional to model size, enabling straightforward capacity planning. CNNs need variable memory for feature maps that scales with input resolution, requiring dynamic memory management for variable-size inputs. RNNs maintain constant memory for hidden states but may require unbounded memory for very long sequences. Transformers face quadratic memory growth that creates hard limits on sequence length in production.</p>
<p>Fault tolerance and recovery characteristics differ significantly between architectures. MLPs and CNNs exhibit stateless computation that enables straightforward checkpointing and recovery. RNNs maintain temporal state that complicates distributed training and failure recovery procedures. Transformers combine stateless computation with massive memory requirements, making checkpoint sizes a practical concern for large models.</p>
<p>Hardware mapping efficiency varies considerably across architectural patterns. Modern MLPs achieve 80-90% of peak hardware performance on specialized tensor units. CNNs reach 60-75% efficiency depending on layer configuration and memory hierarchy design. RNNs typically achieve 30-50% of peak performance due to sequential constraints and irregular memory access patterns. Transformers achieve 70-85% efficiency for large batch sizes but drop significantly for small batches due to attention overhead.</p>
</section>
<section id="sec-dnn-architectures-hardware-mapping-optimization-strategies-5a66" class="level4">
<h4 class="anchored" data-anchor-id="sec-dnn-architectures-hardware-mapping-optimization-strategies-5a66">Hardware Mapping and Optimization Strategies</h4>
<p>Different architectural patterns require distinct optimization strategies for efficient hardware mapping. Understanding these patterns enables systematic performance tuning and hardware selection decisions.</p>
<p>Dense matrix operations in MLPs map naturally to tensor processing units and GPU tensor cores. These operations benefit from several key optimizations: matrix tiling to fit cache hierarchies, mixed-precision computation to double throughput, and operation fusion to reduce memory traffic. Optimal tile sizes depend on cache hierarchy, typically 64x64 for L1 cache and 256x256 for L2, while tensor cores achieve peak efficiency with specific dimension multiples such as 16x16 blocks for Volta architecture.</p>
<p>CNNs benefit from specialized convolution algorithms and data layout optimizations that differ significantly from dense matrix operations. Im2col transformations convert convolutions to matrix multiplication but double memory usage. Winograd algorithms reduce arithmetic complexity by 2.25x for 3x3 convolutions at the cost of numerical stability. Direct convolution with custom kernels achieves optimal memory efficiency but requires architecture-specific tuning.</p>
<p>RNNs require different optimization approaches due to their temporal dependencies. Loop unrolling reduces control overhead but increases memory usage. State vectorization enables SIMD operations across multiple sequences. Wavefront parallelization exploits independence across timesteps for bidirectional processing.</p>
<p>Transformers demand specialized attention optimizations due to their quadratic complexity. FlashAttention algorithms reduce memory usage from O(nÂ²) to O(n) through online softmax computation and gradient recomputation. Sparse attention patterns including local, strided, and random approaches maintain modeling capability while reducing complexity. Multi-query attention shares key and value projections across heads, reducing memory bandwidth by 30-50%.</p>
<p>Multi-Layer Perceptrons represent the most straightforward computational pattern, with costs dominated by matrix multiplications. The dense connectivity that enables MLPs to model arbitrary relationships comes at the price of quadratic parameter growth with layer width. Each neuron connects to every neuron in the previous layer, creating large parameter counts that grow quadratically with network width. The computation is dominated by matrix-vector products, which are highly optimized on modern hardware. Matrix operations are inherently parallel and map efficiently to GPU architectures, with each output neuron computed independently. The optimization techniques for reducing these parameter counts, including pruning and low-rank approximations specifically targeting dense layers, are covered in <strong><a href="../optimizations/optimizations.html#sec-model-optimizations">Chapter 10: Model Optimizations</a></strong>.</p>
<p>Convolutional Neural Networks achieve computational efficiency through parameter sharing and spatial locality, but their costs scale with both spatial dimensions and channel depth. The convolution operationâ€™s computational intensity depends heavily on kernel size and feature map resolution. Parameter sharing across spatial locations dramatically reduces memory compared to equivalent MLPs, while computational cost grows linearly with image resolution and quadratically with kernel size. Feature map memory dominates usage and becomes prohibitive for high-resolution inputs. Spatial independence enables parallel processing across different spatial locations and channels, though memory bandwidth often becomes the limiting factor.</p>
<p>Recurrent Neural Networks optimize for memory efficiency at the cost of parallelization. Their sequential nature creates computational bottlenecks but enables processing of variable-length sequences with constant memory overhead. The hidden-to-hidden connections (<span class="math inline">\(h^2\)</span> term) dominate parameter count for large hidden states. Sequential dependencies prevent parallel processing across time, making RNNs inherently slower than feedforward alternatives. Their constant memory usage for hidden state storage makes RNNs memory-efficient for long sequences, with this efficiency coming at the cost of computational speed.</p>
<p>Transformers achieve maximum flexibility through attention mechanisms but pay a steep price in memory usage. Their quadratic scaling with sequence length creates limits on the sequences they can process. Parameter count scales with model dimension but remains independent of sequence length. The <span class="math inline">\(n^2\)</span> term from attention computation dominates for long sequences, while the <span class="math inline">\(n \times d^2\)</span> term from feed-forward layers dominates for short sequences. Attention matrices create the primary memory bottleneck, as each attention head must store pairwise similarities between all sequence positions, leading to prohibitive memory usage for long sequences. While parallelization is excellent across sequence positions and attention heads, the quadratic memory requirement often forces smaller batch sizes, limiting effective parallelization.</p>
<p>These complexity patterns define optimal domains for each architecture. MLPs excel when parameter efficiency is not critical, CNNs dominate for moderate-resolution spatial data, RNNs remain viable for very long sequences where memory is constrained, and Transformers excel for complex relational tasks where their computational cost is justified through superior performance.</p>
</section>
</section>
<section id="sec-dnn-architectures-architectural-comparison-summary-f918" class="level3">
<h3 class="anchored" data-anchor-id="sec-dnn-architectures-architectural-comparison-summary-f918">Architectural Comparison Summary</h3>
<p>The systematic analysis of each architectural family reveals distinct computational signatures that determine their suitability for different deployment scenarios. <a href="#tbl-architecture-comparison" class="quarto-xref">Table&nbsp;7</a> provides a quantitative comparison across key systems metrics, enabling engineers to make informed trade-offs between model capability and computational constraints.</p>
<div id="tbl-architecture-comparison" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-architecture-comparison-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;7: <strong>Quantitative Architecture Comparison</strong>: Computational complexity analysis across four major neural network architectures. Parameters scale with network dimensions (N=neurons, M=inputs, K=kernel size, C=channels, D=depth, H=hidden size, T=time steps, d=model dimension). Memory requirements reflect peak activation storage during training. Parallelism indicates amenability to parallel computation. Key bottlenecks represent primary performance limiting factors in typical deployments.
</figcaption>
<div aria-describedby="tbl-architecture-comparison-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<colgroup>
<col style="width: 24%">
<col style="width: 13%">
<col style="width: 16%">
<col style="width: 21%">
<col style="width: 20%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;"><strong>Metric</strong></th>
<th style="text-align: left;"><strong>MLP</strong></th>
<th style="text-align: left;"><strong>CNN</strong></th>
<th style="text-align: left;"><strong>RNN</strong></th>
<th style="text-align: right;"><strong>Transformer</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><strong>Parameters</strong></td>
<td style="text-align: left;">O(NÃ—M)</td>
<td style="text-align: left;">O(KÂ²Ã—CÃ—D)</td>
<td style="text-align: left;">O(HÂ²)</td>
<td style="text-align: right;">O(NÃ—dÂ²)</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>FLOPs/Sample</strong></td>
<td style="text-align: left;">O(NÃ—M)</td>
<td style="text-align: left;">O(KÂ²Ã—HÃ—WÃ—C)</td>
<td style="text-align: left;">O(TÃ—HÂ²)</td>
<td style="text-align: right;">O(NÂ²Ã—d)</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>Memory</strong></td>
<td style="text-align: left;">O(BÃ—M)</td>
<td style="text-align: left;">O(BÃ—HÃ—WÃ—C)</td>
<td style="text-align: left;">O(BÃ—TÃ—H)</td>
<td style="text-align: right;">O(BÃ—NÂ²)</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>(Activations)</strong></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: right;"></td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>Parallelism</strong></td>
<td style="text-align: left;">High</td>
<td style="text-align: left;">High</td>
<td style="text-align: left;">Low (Sequential)</td>
<td style="text-align: right;">High</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Key Bottleneck</strong></td>
<td style="text-align: left;">Memory BW</td>
<td style="text-align: left;">Memory BW</td>
<td style="text-align: left;">Sequential Dep.</td>
<td style="text-align: right;">Memory (NÂ²)</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p>This quantitative framework enables systematic architecture selection by explicitly revealing the scaling behaviors that determine computational feasibility. MLPs and CNNs achieve high parallelism but face memory bandwidth constraints as model size grows. RNNs maintain constant memory usage but sacrifice parallelism for sequential processing. Transformers achieve maximum expressivity but face quadratic memory scaling that limits sequence length. Understanding these trade-offs proves essential for matching architectural choices to deployment constraints.</p>
</section>
<section id="sec-dnn-architectures-decision-framework-dbe8" class="level3">
<h3 class="anchored" data-anchor-id="sec-dnn-architectures-decision-framework-dbe8">Decision Framework</h3>
<p>Effective architecture selection requires balancing multiple competing factors: data characteristics, computational resources, performance requirements, and deployment constraints. While data patterns provide initial guidance and complexity analysis establishes feasibility bounds, final architectural choices often involve nuanced trade-offs demanding systematic evaluation.</p>
<div id="fig-dnn-fm-framework" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-env="figure" data-fig-pos="htb">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-dnn-fm-framework-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="4ac702c3bcbdc8d4be392737471a9b509237236e.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-12" title="Figure&nbsp;12: Architecture Selection Decision Framework: A systematic flowchart for choosing neural network architectures based on data characteristics and deployment constraints. The process begins with data type identification (text/sequences/images/tabular) to select initial architecture candidates (Transformers/RNNs/CNNs/MLPs), then iteratively evaluates memory budget, computational cost, inference speed, accuracy targets, and hardware compatibility."><img src="dnn_architectures_files/mediabag/4ac702c3bcbdc8d4be392737471a9b509237236e.svg" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-dnn-fm-framework-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;12: <strong>Architecture Selection Decision Framework</strong>: A systematic flowchart for choosing neural network architectures based on data characteristics and deployment constraints. The process begins with data type identification (text/sequences/images/tabular) to select initial architecture candidates (Transformers/RNNs/CNNs/MLPs), then iteratively evaluates memory budget, computational cost, inference speed, accuracy targets, and hardware compatibility.
</figcaption>
</figure>
</div>
<p><a href="#fig-dnn-fm-framework" class="quarto-xref">Figure&nbsp;12</a> provides a structured approach to architecture selection decisions, ensuring consideration of all relevant factors while avoiding common pitfalls such as selection based on novelty or perceived sophistication. The decision flowchart guides systematic architecture selection by first matching data characteristics to architectural strengths, then validating against practical constraints. The process is inherently iterativeâ€”resource limitations or performance gaps often necessitate reconsidering earlier choices.</p>
<p>This framework applies through four key steps. First, data analysis: pattern types in data provide the strongest initial signal. Spatial data naturally aligns with CNNs, sequential data with RNNs. Second, progressive constraint validation: each constraint check (memory, computational budget, inference speed) acts as a filter. Failing any constraint necessitates either scaling down the current architecture or considering a fundamentally different approach.</p>
<p>Third, iterative trade-off handling when accuracy targets remain unmet. Additional model capacity may be required, necessitating a return to constraint checking. If deployment hardware cannot support the chosen architecture, reconsidering the entire architectural approach may be necessary. Fourth, anticipate multiple iterations, as real projects typically cycle through this framework several times before achieving optimal balance between data fit, computational feasibility, and deployment requirements.</p>
<p>This systematic approach prevents architecture selection based solely on novelty or perceived sophistication, ensuring alignment of choices with both problem requirements and system capabilities.</p>
<div id="quiz-question-sec-dnn-architectures-architecture-selection-framework-7a37" class="callout callout-quiz-question">
<details class="callout-quiz-question fbx-default closebutton"><summary><strong>Self-Check: Question 1.8</strong></summary><div>
<ol type="1">
<li><p>Which neural network architecture is most suitable for processing tabular data with arbitrary feature relationships?</p>
<ol type="a">
<li>CNNs</li>
<li>MLPs</li>
<li>RNNs</li>
<li>Transformers</li>
</ol></li>
<li><p>True or False: Transformers are generally more efficient than CNNs for image recognition tasks due to their ability to model complex relational patterns.</p></li>
<li><p>Explain how computational constraints influence the final architecture selection for a deployment scenario.</p></li>
<li><p>The process of systematically matching data characteristics to neural network architectures is known as ____.</p></li>
<li><p>In a production system, what trade-offs must be considered when selecting an architecture for a task with strict latency requirements?</p></li>
</ol>
<p><a href="#quiz-answer-sec-dnn-architectures-architecture-selection-framework-7a37" class="question-label">See Answers â†’</a></p>
</div></details>
</div>
</section>
</section>
<section id="sec-dnn-architectures-unified-framework-inductive-biases-099d" class="level2">
<h2 class="anchored" data-anchor-id="sec-dnn-architectures-unified-framework-inductive-biases-099d">Unified Framework: Inductive Biases</h2>
<p>The architectural diversity exploredâ€”from MLPs to Transformersâ€”share a unified theoretical framework: each architecture embodies specific inductive biases that constrain the hypothesis space and guide learning toward solutions appropriate for different data types and problem structures.</p>
<p>Different architectures form a hierarchy of decreasing inductive bias. CNNs exhibit the strongest inductive biases through local connectivity, parameter sharing, and translation equivariance. These constraints dramatically reduce the parameter space while limiting flexibility to spatial data with local structure. RNNs demonstrate moderate inductive bias through sequential processing and shared temporal weights. The hidden state mechanism assumes that past information influences current processing, rendering them appropriate for temporal sequences.</p>
<p>MLPs maintain minimal architectural bias beyond layer-wise processing. Dense connectivity allows modeling arbitrary relationships but requires more data to learn structure that other architectures encode explicitly. Transformers represent adaptive inductive bias through learned attention patterns. The architecture can dynamically adjust its inductive bias based on the data, combining flexibility with the ability to discover relevant structural regularities.</p>
<p>All successful architectures implement forms of hierarchical representation learning, but through different mechanisms. CNNs build spatial hierarchies through progressive receptive field expansion, applying the spatial pattern processing framework detailed in <a href="#sec-dnn-architectures-cnns-spatial-pattern-processing-f8ff" class="quarto-xref">Section&nbsp;1.3</a>. RNNs build temporal hierarchies through hidden state evolution, extending the sequential processing approach from <a href="#sec-dnn-architectures-rnns-sequential-pattern-processing-ea14" class="quarto-xref">Section&nbsp;1.4</a>. Transformers build content-dependent hierarchies through multi-head attention, applying the dynamic pattern processing mechanisms described in <a href="#sec-dnn-architectures-attention-mechanisms-dynamic-pattern-processing-566d" class="quarto-xref">Section&nbsp;1.5</a>.</p>
<p>This hierarchical organization reflects a principle: complex patterns can be efficiently represented through composition of simpler components. The success of deep learning stems from the discovery that gradient-based optimization can effectively learn these compositional structures when provided with appropriate architectural inductive biases.</p>
<p>The theoretical insights about representation learning have direct implications for systems engineering. Hierarchical representations require computational patterns that can efficiently compose lower-level features into higher-level abstractions. This drives system design decisions:</p>
<ul>
<li>Memory hierarchies must align with representational hierarchies to minimize data movement costs</li>
<li>Parallelization strategies must respect the dependency structure of hierarchical computation</li>
<li>Hardware accelerators must efficiently support the matrix operations that implement feature composition</li>
<li>Software frameworks must provide abstractions that enable efficient hierarchical computation across diverse architectures</li>
</ul>
<p>Understanding architectures as embodying different inductive biases helps explain both their strengths and their systems requirements, providing a principled foundation for architecture selection and system optimization decisions.</p>
<div id="quiz-question-sec-dnn-architectures-unified-framework-inductive-biases-099d" class="callout callout-quiz-question">
<details class="callout-quiz-question fbx-default closebutton"><summary><strong>Self-Check: Question 1.9</strong></summary><div>
<ol type="1">
<li><p>Which neural network architecture is characterized by the strongest inductive biases due to local connectivity and parameter sharing?</p>
<ol type="a">
<li>MLPs</li>
<li>CNNs</li>
<li>RNNs</li>
<li>Transformers</li>
</ol></li>
<li><p>True or False: Transformers have a fixed inductive bias that limits their ability to adapt to different data structures.</p></li>
<li><p>Explain how the hierarchical representation learning in neural networks influences system design decisions.</p></li>
<li><p>Order the following architectures by decreasing inductive bias strength: (1) CNNs, (2) RNNs, (3) MLPs, (4) Transformers.</p></li>
</ol>
<p><a href="#quiz-answer-sec-dnn-architectures-unified-framework-inductive-biases-099d" class="question-label">See Answers â†’</a></p>
</div></details>
</div>
</section>
<section id="sec-dnn-architectures-fallacies-pitfalls-3e82" class="level2">
<h2 class="anchored" data-anchor-id="sec-dnn-architectures-fallacies-pitfalls-3e82">Fallacies and Pitfalls</h2>
<p>Neural network architectures represent specialized computational structures designed for different data types and problem domains, which creates common misconceptions about their selection and deployment. The rich variety of architectural patternsâ€”from dense networks to transformersâ€”often leads engineers to make choices based on novelty or perceived sophistication rather than task-specific requirements and computational constraints.</p>
<p><strong>Fallacy:</strong> <em>More complex architectures always perform better than simpler ones.</em></p>
<p>This misconception prompts teams to immediately adopt transformer-based models or elaborate architectures without understanding their requirements. While sophisticated architectures such as transformers excel at complex tasks requiring long-range dependencies, they require significantly more computational resources and memory. For numerous problems, particularly those with limited data or clear structural patterns, simpler architectures such as MLPs or CNNs achieve comparable accuracy with significantly less computational overhead. Architecture selection should correspond to problem complexity rather than defaulting to the most advanced option.</p>
<p><strong>Pitfall:</strong> <em>Ignoring the computational implications of architectural choices during model selection.</em></p>
<p>Many practitioners select architectures based solely on accuracy metrics from academic papers without considering computational requirements. A CNNâ€™s spatial locality assumptions might deliver excellent accuracy for image tasks but require specialized memory access patterns. Similarly, RNNsâ€™ sequential dependencies create serialization bottlenecks that limit parallelization opportunities. This oversight leads to deployment failures when models cannot meet latency requirements or exceed memory constraints in production environments.</p>
<p><strong>Fallacy:</strong> <em>Architecture performance is independent of hardware characteristics.</em></p>
<p>This belief assumes that all architectures perform equally well across different hardware platforms. In reality, different architectures exploit different hardware features: CNNs benefit from specialized tensor cores, MLPs leverage high-bandwidth memory, and RNNs require efficient sequential processing capabilities. A model that achieves optimal performance on GPUs might perform poorly on mobile devices or embedded processors. Understanding hardware-architecture alignment is crucial for effective deployment strategies.</p>
<p><strong>Pitfall:</strong> <em>Mixing architectural patterns without understanding their interaction effects.</em></p>
<p>Combining different architectural components (such as adding attention layers to CNNs or using skip connections in RNNs) can create unexpected computational bottlenecks. Each architectural pattern exhibits distinct memory access patterns and computational characteristics. Naive combinations may eliminate the performance benefits of individual components or create memory bandwidth conflicts. Successful hybrid architectures require careful analysis of how different patterns interact at the system level.</p>
<p><strong>Pitfall:</strong> <em>Designing architectures without considering the full hardware-software co-design implications across the deployment pipeline.</em></p>
<p>Many architecture decisions optimize for high-end GPU performance without considering the complete system lifecycle from development through deployment. An architecture designed for large-scale compute clusters may be poorly suited for edge deployment due to memory constraints, lack of specialized compute units, or limited parallelization capabilities. Similarly, architectures optimized for inference latency might sacrifice development efficiency, leading to longer development cycles and higher computational costs. Effective architecture selection requires analyzing the entire system stack including compute infrastructure, model compilation and optimization tools, target deployment hardware, and operational constraints. The choice between CNN depth and width, transformer head configurations, or activation functions has cascading effects on memory bandwidth utilization, cache efficiency, and numerical precision requirements that must be considered holistically rather than in isolation.</p>
<div id="quiz-question-sec-dnn-architectures-fallacies-pitfalls-3e82" class="callout callout-quiz-question">
<details class="callout-quiz-question fbx-default closebutton"><summary><strong>Self-Check: Question 1.10</strong></summary><div>
<ol type="1">
<li><p>Which of the following misconceptions might lead a team to choose a transformer-based model over a simpler architecture?</p>
<ol type="a">
<li>Transformers always require less computational resources.</li>
<li>Transformers are universally optimal for all tasks.</li>
<li>Simpler architectures cannot handle complex tasks.</li>
<li>More complex architectures inherently perform better.</li>
</ol></li>
<li><p>True or False: Ignoring computational implications during model selection can lead to deployment failures.</p></li>
<li><p>Explain why understanding hardware-architecture alignment is crucial for effective deployment strategies.</p></li>
<li><p>The practice of combining different architectural components without understanding their interaction effects can lead to unexpected ____.</p></li>
<li><p>In a production system, what are the trade-offs of designing architectures without considering the full hardware-software co-design implications?</p></li>
</ol>
<p><a href="#quiz-answer-sec-dnn-architectures-fallacies-pitfalls-3e82" class="question-label">See Answers â†’</a></p>
</div></details>
</div>
</section>
<section id="sec-dnn-architectures-summary-c495" class="level2">
<h2 class="anchored" data-anchor-id="sec-dnn-architectures-summary-c495">Summary</h2>
<p>Neural network architectures form specialized computational structures tailored to process different types of data and solve distinct classes of problems. Multi-Layer Perceptrons handle tabular data through dense connections, convolutional networks exploit spatial locality in images, and recurrent networks process sequential information. Each architecture embodies specific assumptions about data structure and computational patterns. Modern transformer architectures unify many of these concepts through attention mechanisms that dynamically route information based on relevance rather than fixed connectivity patterns.</p>
<p>Despite their apparent diversity, these architectures share fundamental computational primitives that recur across different designs. Matrix multiplication operations form the computational core, whether in dense layers, convolutions, or attention mechanisms. Memory access patterns vary significantly between architectures, with some requiring sliding window operations for local processing while others demand global information aggregation. Dynamic computation patterns in attention mechanisms create data-dependent execution flows that challenge traditional optimization approaches.</p>
<div class="callout callout-style-default callout-important callout-titled" title="Key Takeaways">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Key Takeaways
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>Different architectures embody specific assumptions about data structure: MLPs for tabular data, CNNs for spatial relationships, RNNs for sequences, Transformers for flexible attention</li>
<li>Shared computational primitives including matrix operations, sliding windows, and dynamic routing form the foundation across diverse architectures</li>
<li>Memory access patterns and data movement requirements vary significantly between architectures, directly impacting system performance and optimization strategies</li>
<li>Understanding the mapping between algorithmic intent and system implementation enables effective performance optimization and hardware selection</li>
</ul>
</div>
</div>
<p>The architectural foundations established in this chapterâ€”computational patterns, memory access characteristics, and data movement primitivesâ€”directly inform the design of specialized hardware and optimization strategies explored in subsequent chapters. Understanding that CNNs exhibit spatial locality enables the development of systolic arrays optimized for convolution operations (<strong><a href="../hw_acceleration/hw_acceleration.html#sec-ai-acceleration">Chapter 11: AI Acceleration</a></strong>). Recognizing that Transformers demand quadratic memory scaling motivates attention-specific optimizations such as FlashAttention and sparse attention patterns (<strong><a href="../optimizations/optimizations.html#sec-model-optimizations">Chapter 10: Model Optimizations</a></strong>). The progression from architectural understanding to hardware design to algorithmic optimization represents a systematic approach to ML systems engineering.</p>
<p>As architectures become more dynamic and sophisticated, the relationship between algorithmic innovation and systems optimization becomes increasingly critical for achieving practical performance gains in real-world deployments. The operational challenges of deploying and maintaining these sophisticated architectures in production environments are addressed in <strong><a href="../ops/ops.html#sec-ml-operations">Chapter 13: ML Operations</a></strong>, while the broader implications for sustainable AI development, including energy efficiency considerations stemming from architectural choices, are explored in <strong><a href="../sustainable_ai/sustainable_ai.html#sec-sustainable-ai">Chapter 18: Sustainable AI</a></strong>.</p>
<!-- This is here to make sure that quizzes are inserted properly before a part begins. -->
<div id="quiz-question-sec-dnn-architectures-summary-c495" class="callout callout-quiz-question">
<details class="callout-quiz-question fbx-default closebutton"><summary><strong>Self-Check: Question 1.11</strong></summary><div>
<ol type="1">
<li><p>Which neural network architecture is best suited for processing sequential data?</p>
<ol type="a">
<li>Multi-Layer Perceptrons (MLPs)</li>
<li>Convolutional Neural Networks (CNNs)</li>
<li>Recurrent Neural Networks (RNNs)</li>
<li>Transformers</li>
</ol></li>
<li><p>Explain how matrix multiplication serves as a fundamental computational primitive across various neural network architectures.</p></li>
<li><p>Order the following architectures by their typical memory access patterns, from local to global: (1) CNNs, (2) MLPs, (3) Transformers.</p></li>
<li><p>In a production system, what are the implications of using transformer architectures in terms of memory and computation?</p></li>
</ol>
<p><a href="#quiz-answer-sec-dnn-architectures-summary-c495" class="question-label">See Answers â†’</a></p>
</div></details>
</div>
</section>
<section id="self-check-answers" class="level2">
<h2 class="anchored" data-anchor-id="self-check-answers">Self-Check Answers</h2>
<div id="quiz-answer-sec-dnn-architectures-architectural-principles-engineering-tradeoffs-89de" class="callout callout-quiz-answer">
<details class="callout-quiz-answer fbx-answer closebutton"><summary><strong>Self-Check: Answer 1.1</strong></summary><div>
<ol type="1">
<li><p><strong>Which architectural paradigm is primarily used to exploit translational invariance and local connectivity for spatial data?</strong></p>
<ol type="a">
<li>Multi-Layer Perceptrons</li>
<li>Recurrent Neural Networks</li>
<li>Convolutional Neural Networks</li>
<li>Transformer architectures</li>
</ol>
<p><em>Answer</em>: The correct answer is C. Convolutional Neural Networks. This is correct because CNNs are designed to handle spatial data by using local connectivity and translational invariance. Other options like MLPs and RNNs do not specifically address spatial data in this manner.</p>
<p><em>Learning Objective</em>: Understand the specific architectural paradigms used for different data structures.</p></li>
<li><p><strong>Explain the trade-off between theoretical universality and computational tractability in neural network architectures.</strong></p>
<p><em>Answer</em>: Theoretical universality refers to the ability of neural networks to approximate any function, as per universal approximation theory. However, achieving this in practice requires complex architectures, which can be computationally expensive and inefficient. The trade-off involves balancing the networkâ€™s representational power with the need for efficient computation. For example, while a fully connected network can theoretically model any function, it may be impractical due to high computational costs. This is important because efficient architectures are crucial for deploying models in resource-constrained environments.</p>
<p><em>Learning Objective</em>: Analyze the trade-offs involved in neural network design concerning universality and efficiency.</p></li>
<li><p><strong>Which architectural innovation is characterized by dynamic, content-dependent computation?</strong></p>
<ol type="a">
<li>Multi-Layer Perceptrons</li>
<li>Convolutional Neural Networks</li>
<li>Recurrent Neural Networks</li>
<li>Attention mechanisms and Transformer architectures</li>
</ol>
<p><em>Answer</em>: The correct answer is D. Attention mechanisms and Transformer architectures. This is correct because these architectures use attention mechanisms to dynamically adjust computations based on input content, unlike fixed structural assumptions in other architectures.</p>
<p><em>Learning Objective</em>: Identify the characteristics and innovations of modern neural architectures.</p></li>
<li><p><strong>How do architectural choices in neural networks impact hardware resource demands and system feasibility?</strong></p>
<p><em>Answer</em>: Architectural choices determine memory access patterns, parallelization strategies, and hardware utilization. For example, CNNs reduce computational demands by leveraging local connectivity, which allows for more efficient use of memory and processing power. This impacts system feasibility as efficient architectures can be deployed on limited hardware resources, while complex architectures may require more advanced infrastructure. This is important because understanding these implications helps in designing systems that are both effective and resource-efficient.</p>
<p><em>Learning Objective</em>: Understand the impact of neural network architectures on system-level hardware and resource considerations.</p></li>
</ol>
<p><a href="#quiz-question-sec-dnn-architectures-architectural-principles-engineering-tradeoffs-89de" class="answer-label">â† Back to Questions</a></p>
</div></details>
</div>
<div id="quiz-answer-sec-dnn-architectures-multilayer-perceptrons-dense-pattern-processing-259f" class="callout callout-quiz-answer">
<details class="callout-quiz-answer fbx-answer closebutton"><summary><strong>Self-Check: Answer 1.2</strong></summary><div>
<ol type="1">
<li><p><strong>What is a key advantage of using Multi-Layer Perceptrons (MLPs) in machine learning systems?</strong></p>
<ol type="a">
<li>They assume no prior structure in the data, allowing maximum flexibility.</li>
<li>They exploit inherent data structures for computational efficiency.</li>
<li>They require minimal computational resources compared to specialized architectures.</li>
<li>They are inherently more interpretable than other neural network architectures.</li>
</ol>
<p><em>Answer</em>: The correct answer is A. They assume no prior structure in the data, allowing maximum flexibility. This flexibility enables MLPs to model any input-output relationship, but it comes at the cost of higher computational demands.</p>
<p><em>Learning Objective</em>: Understand the flexibility and computational trade-offs of MLPs.</p></li>
<li><p><strong>Explain how the Universal Approximation Theorem influences the architectural choice of using MLPs in machine learning systems.</strong></p>
<p><em>Answer</em>: The Universal Approximation Theorem states that a sufficiently large MLP can approximate any continuous function, which supports the use of MLPs for diverse tasks. However, it does not specify the network size or weights needed, leading to practical challenges in implementation. This theorem justifies MLPsâ€™ flexibility but also highlights the need for optimization to manage computational demands.</p>
<p><em>Learning Objective</em>: Analyze the impact of the Universal Approximation Theorem on MLP architecture choices.</p></li>
<li><p><strong>In the context of MNIST digit recognition, why might an MLP be chosen over specialized architectures?</strong></p>
<ol type="a">
<li>MLPs are more efficient in handling high-dimensional data like images.</li>
<li>MLPs can exploit the spatial locality of pixel data better than convolutional networks.</li>
<li>MLPs provide flexibility to learn arbitrary pixel relationships without assuming spatial structure.</li>
<li>MLPs are less computationally intensive than convolutional neural networks.</li>
</ol>
<p><em>Answer</em>: The correct answer is C. MLPs provide flexibility to learn arbitrary pixel relationships without assuming spatial structure. This makes them suitable for tasks where input relationships are unknown or unstructured.</p>
<p><em>Learning Objective</em>: Evaluate the suitability of MLPs for tasks with unknown input relationships.</p></li>
<li><p><strong>The computational operation that forms the backbone of MLPs and accounts for most of their computation time is known as ____. This operation is crucial for the dense connectivity pattern.</strong></p>
<p><em>Answer</em>: GEMM (General Matrix Multiply). This operation is crucial for the dense connectivity pattern, as it enables the transformation of input vectors through matrix multiplications.</p>
<p><em>Learning Objective</em>: Recall the key computational operation underlying MLPs.</p></li>
<li><p><strong>How do memory requirements and data movement patterns in MLPs influence system design decisions?</strong></p>
<p><em>Answer</em>: MLPs require significant memory to store weights and intermediate results due to their dense connectivity. This necessitates efficient data organization and reuse strategies, such as caching and high-bandwidth memory systems, to manage data movement. These requirements drive the design of hardware accelerators and software frameworks to optimize performance.</p>
<p><em>Learning Objective</em>: Understand how MLPsâ€™ memory and data movement needs affect system design.</p></li>
</ol>
<p><a href="#quiz-question-sec-dnn-architectures-multilayer-perceptrons-dense-pattern-processing-259f" class="answer-label">â† Back to Questions</a></p>
</div></details>
</div>
<div id="quiz-answer-sec-dnn-architectures-cnns-spatial-pattern-processing-f8ff" class="callout callout-quiz-answer">
<details class="callout-quiz-answer fbx-answer closebutton"><summary><strong>Self-Check: Answer 1.3</strong></summary><div>
<ol type="1">
<li><p><strong>Which architectural feature of CNNs allows them to efficiently process spatially structured data?</strong></p>
<ol type="a">
<li>Global connectivity</li>
<li>Parameter sharing</li>
<li>Both B and D</li>
<li>Local connectivity</li>
</ol>
<p><em>Answer</em>: The correct answer is C. Both B and D. CNNs use parameter sharing and local connectivity to efficiently process spatially structured data, reducing the number of parameters and focusing on local spatial relationships.</p>
<p><em>Learning Objective</em>: Understand the architectural features that enable CNNs to efficiently handle spatial data.</p></li>
<li><p><strong>Explain how parameter sharing in CNNs contributes to computational efficiency compared to MLPs.</strong></p>
<p><em>Answer</em>: Parameter sharing in CNNs means using the same filter weights across different spatial positions, significantly reducing the number of parameters compared to MLPs. This leads to lower computational costs and improved generalization, as the same feature detector can be applied across the entire input space.</p>
<p><em>Learning Objective</em>: Analyze the role of parameter sharing in enhancing CNN computational efficiency.</p></li>
<li><p><strong>In CNNs, the ability to detect features regardless of their spatial position is known as ____.</strong></p>
<p><em>Answer</em>: translation invariance. This property allows CNNs to recognize patterns anywhere in the input image, making them effective for tasks like object recognition.</p>
<p><em>Learning Objective</em>: Recall the concept of translation invariance and its significance in CNNs.</p></li>
<li><p><strong>Order the following CNN operations as they occur in a typical layer: (1) Apply filter, (2) Activation function, (3) Bias addition.</strong></p>
<p><em>Answer</em>: The correct order is: (1) Apply filter, (3) Bias addition, (2) Activation function. Filters are applied first to extract features, biases are added to adjust the output, and finally, an activation function is applied to introduce non-linearity.</p>
<p><em>Learning Objective</em>: Understand the sequence of operations in a CNN layer.</p></li>
<li><p><strong>In a production system, how might the architectural characteristics of CNNs influence hardware design decisions?</strong></p>
<p><em>Answer</em>: The architectural characteristics of CNNs, such as parameter sharing and local connectivity, influence hardware design by encouraging optimizations for weight reuse and spatial data locality. Hardware systems may use specialized memory architectures and parallel processing capabilities to efficiently handle the repetitive and structured computations typical of CNNs.</p>
<p><em>Learning Objective</em>: Evaluate how CNN architecture affects hardware design in production systems.</p></li>
</ol>
<p><a href="#quiz-question-sec-dnn-architectures-cnns-spatial-pattern-processing-f8ff" class="answer-label">â† Back to Questions</a></p>
</div></details>
</div>
<div id="quiz-answer-sec-dnn-architectures-rnns-sequential-pattern-processing-ea14" class="callout callout-quiz-answer">
<details class="callout-quiz-answer fbx-answer closebutton"><summary><strong>Self-Check: Answer 1.4</strong></summary><div>
<ol type="1">
<li><p><strong>What is the primary advantage of RNNs over CNNs when processing sequential data?</strong></p>
<ol type="a">
<li>RNNs maintain an internal state to capture temporal dependencies.</li>
<li>RNNs can process data in parallel across time steps.</li>
<li>RNNs are more efficient in terms of memory usage than CNNs.</li>
<li>RNNs use fixed-size kernels to capture patterns.</li>
</ol>
<p><em>Answer</em>: The correct answer is A. RNNs maintain an internal state to capture temporal dependencies, allowing them to process sequential data effectively. Options B, C, and D are incorrect because RNNs process sequentially, may have higher computational demands, and do not use fixed-size kernels.</p>
<p><em>Learning Objective</em>: Understand the architectural advantage of RNNs in handling temporal dependencies.</p></li>
<li><p><strong>Explain how RNNs handle long-term dependencies in sequential data and discuss one limitation related to this capability.</strong></p>
<p><em>Answer</em>: RNNs handle long-term dependencies by maintaining a hidden state that propagates information through time steps. However, they suffer from the vanishing gradient problem, which makes learning long-term dependencies difficult. For example, gradients diminish as they are backpropagated through many time steps, hindering the learning of distant temporal relationships. This is important because it limits RNN effectiveness in tasks requiring long-term memory.</p>
<p><em>Learning Objective</em>: Analyze how RNNs manage long-term dependencies and recognize their limitations.</p></li>
<li><p><strong>The phenomenon where gradients shrink exponentially as they propagate backward through RNN layers is known as the ____. This limits the ability of RNNs to learn long-term dependencies.</strong></p>
<p><em>Answer</em>: vanishing gradient problem. This limits the ability of RNNs to learn long-term dependencies because gradients become too small to effectively update weights over long sequences.</p>
<p><em>Learning Objective</em>: Recall and understand the vanishing gradient problem in the context of RNNs.</p></li>
<li><p><strong>In a production system, how might the sequential processing nature of RNNs influence hardware design and optimization strategies?</strong></p>
<p><em>Answer</em>: The sequential processing nature of RNNs requires hardware to handle dependencies across time steps, impacting parallelization strategies. For example, CPUs may use pipelining, while GPUs batch sequences for throughput. This is important because efficient hardware utilization is critical for performance in real-time applications like speech recognition.</p>
<p><em>Learning Objective</em>: Evaluate the impact of RNN sequential processing on hardware design and optimization.</p></li>
</ol>
<p><a href="#quiz-question-sec-dnn-architectures-rnns-sequential-pattern-processing-ea14" class="answer-label">â† Back to Questions</a></p>
</div></details>
</div>
<div id="quiz-answer-sec-dnn-architectures-attention-mechanisms-dynamic-pattern-processing-566d" class="callout callout-quiz-answer">
<details class="callout-quiz-answer fbx-answer closebutton"><summary><strong>Self-Check: Answer 1.5</strong></summary><div>
<ol type="1">
<li><p><strong>Which of the following best describes the primary advantage of attention mechanisms over RNNs?</strong></p>
<ol type="a">
<li>Attention mechanisms process sequences in parallel, capturing long-range dependencies more effectively.</li>
<li>Attention mechanisms use fixed spatial patterns to process data.</li>
<li>Attention mechanisms rely on sequential processing to capture temporal dependencies.</li>
<li>Attention mechanisms are less computationally demanding than RNNs.</li>
</ol>
<p><em>Answer</em>: The correct answer is A. Attention mechanisms process sequences in parallel, capturing long-range dependencies more effectively. RNNs process information sequentially, which limits their ability to capture relationships between distant elements. Attention mechanisms overcome this by dynamically weighing relationships based on content, allowing for parallel processing.</p>
<p><em>Learning Objective</em>: Understand the architectural advantages of attention mechanisms over RNNs in processing sequences.</p></li>
<li><p><strong>Explain how attention mechanisms dynamically determine which relationships in input data are important.</strong></p>
<p><em>Answer</em>: Attention mechanisms compute the relevance between all pairs of elements in a sequence using query-key-value interactions. They generate attention weights based on content similarity, allowing the model to focus on important relationships dynamically. This flexibility contrasts with the fixed sequential processing of RNNs. For example, in language translation, attention can focus on contextually relevant words regardless of their position in the sentence.</p>
<p><em>Learning Objective</em>: Explain the dynamic nature of attention mechanisms in determining important relationships in input data.</p></li>
<li><p><strong>In attention mechanisms, the operation that normalizes similarity scores to create a probability distribution is called the ____.</strong></p>
<p><em>Answer</em>: softmax. The softmax function converts similarity scores into a probability distribution, allowing the model to weigh the importance of different elements in a sequence.</p>
<p><em>Learning Objective</em>: Recall the role of the softmax function in attention mechanisms.</p></li>
<li><p><strong>In a production system, what are the computational trade-offs of implementing attention mechanisms compared to RNNs?</strong></p>
<p><em>Answer</em>: Attention mechanisms require more memory and computational resources due to their quadratic scaling with sequence length and the need to compute attention weights for all element pairs. However, they enable parallel processing and capture long-range dependencies more effectively than RNNs. This trade-off involves balancing increased computational demands with improved model performance and flexibility. For example, attention mechanisms can handle complex dependencies in tasks like language translation more efficiently than RNNs.</p>
<p><em>Learning Objective</em>: Analyze the computational trade-offs of using attention mechanisms in production systems compared to RNNs.</p></li>
</ol>
<p><a href="#quiz-question-sec-dnn-architectures-attention-mechanisms-dynamic-pattern-processing-566d" class="answer-label">â† Back to Questions</a></p>
</div></details>
</div>
<div id="quiz-answer-sec-dnn-architectures-architectural-building-blocks-a575" class="callout callout-quiz-answer">
<details class="callout-quiz-answer fbx-answer closebutton"><summary><strong>Self-Check: Answer 1.6</strong></summary><div>
<ol type="1">
<li><p><strong>Which architectural innovation introduced the concept of parameter sharing, significantly improving computational efficiency?</strong></p>
<ol type="a">
<li>Multi-Layer Perceptrons (MLPs)</li>
<li>Convolutional Neural Networks (CNNs)</li>
<li>Recurrent Neural Networks (RNNs)</li>
<li>Transformers</li>
</ol>
<p><em>Answer</em>: The correct answer is B. Convolutional Neural Networks (CNNs). This is correct because CNNs utilize parameter sharing to apply the same filter across different parts of the input, reducing the number of parameters and computational load. MLPs and RNNs do not inherently use parameter sharing in the same way.</p>
<p><em>Learning Objective</em>: Understand the concept of parameter sharing and its impact on computational efficiency in CNNs.</p></li>
<li><p><strong>Explain how skip connections, originally introduced in ResNets, have influenced modern neural network architectures.</strong></p>
<p><em>Answer</em>: Skip connections allow gradients to flow more easily through deep networks by adding the input of a layer to its output, mitigating the vanishing gradient problem. They enable the training of deeper networks and have been adopted in architectures like Transformers to improve optimization and performance. This is important because it facilitates the development of more complex and capable models.</p>
<p><em>Learning Objective</em>: Analyze the role of skip connections in enhancing the trainability and performance of deep neural networks.</p></li>
<li><p><strong>Order the following architectural innovations by their introduction in neural network evolution: (1) Attention Mechanisms, (2) Skip Connections, (3) Parameter Sharing, (4) Gating Mechanisms.</strong></p>
<p><em>Answer</em>: The correct order is: (3) Parameter Sharing, (4) Gating Mechanisms, (2) Skip Connections, (1) Attention Mechanisms. Parameter sharing was introduced with CNNs, gating mechanisms with LSTMs and GRUs, skip connections with ResNets, and attention mechanisms with Transformers.</p>
<p><em>Learning Objective</em>: Understand the chronological development of key architectural innovations in neural networks.</p></li>
<li><p><strong>The introduction of ____ in CNNs allowed for the reuse of the same parameters across different parts of the input, enhancing computational efficiency.</strong></p>
<p><em>Answer</em>: parameter sharing. This technique reduces the number of parameters and computational load by applying the same filter across various parts of the input.</p>
<p><em>Learning Objective</em>: Recall the concept of parameter sharing and its significance in CNNs.</p></li>
<li><p><strong>In a production system, what are the system-level implications of using Transformer architectures, particularly in terms of memory access and data movement?</strong></p>
<p><em>Answer</em>: Transformers require high-bandwidth memory and flexible accelerators due to their attention mechanisms, which involve random memory access and broadcast data movement patterns. This is important because it impacts the choice of hardware and system design to efficiently support the computational demands of Transformers.</p>
<p><em>Learning Objective</em>: Evaluate the system-level implications of deploying Transformer architectures in production environments.</p></li>
</ol>
<p><a href="#quiz-question-sec-dnn-architectures-architectural-building-blocks-a575" class="answer-label">â† Back to Questions</a></p>
</div></details>
</div>
<div id="quiz-answer-sec-dnn-architectures-systemlevel-building-blocks-72f6" class="callout callout-quiz-answer">
<details class="callout-quiz-answer fbx-answer closebutton"><summary><strong>Self-Check: Answer 1.7</strong></summary><div>
<ol type="1">
<li><p><strong>Which of the following operations is considered a core computational primitive in deep learning architectures?</strong></p>
<ol type="a">
<li>Dropout</li>
<li>Gradient descent</li>
<li>Backpropagation</li>
<li>Matrix multiplication</li>
</ol>
<p><em>Answer</em>: The correct answer is D. Matrix multiplication. This operation is fundamental because it forms the basis for transforming features in neural networks. Other options like gradient descent and backpropagation are optimization techniques, not computational primitives.</p>
<p><em>Learning Objective</em>: Identify core computational primitives essential for deep learning architectures.</p></li>
<li><p><strong>Explain how the im2col technique transforms convolution operations into matrix multiplications and its implications for computational efficiency.</strong></p>
<p><em>Answer</em>: The im2col technique reshapes overlapping image patches into columns of a matrix, allowing convolution to be expressed as a matrix multiplication. This transformation enables the use of optimized BLAS libraries, improving computational efficiency by leveraging existing matrix operation optimizations. For example, im2col allows CNNs to achieve 5-10x speedups on CPUs by using efficient matrix libraries. This is important because it maximizes hardware utilization and accelerates CNN computations.</p>
<p><em>Learning Objective</em>: Understand the im2col transformation and its impact on computational efficiency in CNNs.</p></li>
<li><p><strong>True or False: Dynamic computation in neural networks requires runtime decisions and poses challenges for hardware design.</strong></p>
<p><em>Answer</em>: True. Dynamic computation involves operations that depend on input data, requiring runtime decisions. This poses challenges for hardware design as it necessitates flexible data routing and support for variable computation patterns, unlike fixed computation graphs.</p>
<p><em>Learning Objective</em>: Recognize the challenges dynamic computation poses for hardware design in neural networks.</p></li>
<li><p><strong>The systolic array used in Googleâ€™s TPU is an example of a specialized hardware component designed to optimize ____ operations.</strong></p>
<p><em>Answer</em>: matrix multiplication. The systolic array is designed to optimize matrix multiplication operations by performing many multiply-accumulate operations in parallel, which is critical for efficient neural network computations.</p>
<p><em>Learning Objective</em>: Identify specialized hardware components designed to optimize specific computational primitives.</p></li>
<li><p><strong>In a production system, what trade-offs must be considered when designing memory systems to support both sequential and random access patterns in deep learning models?</strong></p>
<p><em>Answer</em>: Designing memory systems requires balancing the efficiency of sequential access, which benefits from burst mode and prefetching, against the flexibility needed for random access, which can cause cache misses and latency. For example, while DRAM can handle sequential access efficiently, random access may require large caches and sophisticated prefetching strategies. This is important because optimizing memory systems impacts overall system performance and energy consumption.</p>
<p><em>Learning Objective</em>: Analyze the trade-offs in memory system design for supporting diverse access patterns in ML models.</p></li>
</ol>
<p><a href="#quiz-question-sec-dnn-architectures-systemlevel-building-blocks-72f6" class="answer-label">â† Back to Questions</a></p>
</div></details>
</div>
<div id="quiz-answer-sec-dnn-architectures-architecture-selection-framework-7a37" class="callout callout-quiz-answer">
<details class="callout-quiz-answer fbx-answer closebutton"><summary><strong>Self-Check: Answer 1.8</strong></summary><div>
<ol type="1">
<li><p><strong>Which neural network architecture is most suitable for processing tabular data with arbitrary feature relationships?</strong></p>
<ol type="a">
<li>CNNs</li>
<li>MLPs</li>
<li>RNNs</li>
<li>Transformers</li>
</ol>
<p><em>Answer</em>: The correct answer is B. MLPs. This is correct because MLPs are designed to handle arbitrary feature relationships typically found in tabular data. CNNs and RNNs are better suited for spatial and sequential data, respectively, while Transformers excel in modeling complex relational patterns.</p>
<p><em>Learning Objective</em>: Understand the alignment between data types and neural network architectures.</p></li>
<li><p><strong>True or False: Transformers are generally more efficient than CNNs for image recognition tasks due to their ability to model complex relational patterns.</strong></p>
<p><em>Answer</em>: False. This is false because CNNs are specifically designed to exploit spatial locality in images, making them more efficient for image recognition tasks. Transformers excel in tasks requiring modeling of complex relational patterns, such as language understanding.</p>
<p><em>Learning Objective</em>: Challenge the misconception that Transformers are universally superior due to their complex relational modeling capabilities.</p></li>
<li><p><strong>Explain how computational constraints influence the final architecture selection for a deployment scenario.</strong></p>
<p><em>Answer</em>: Computational constraints such as memory, processing power, and latency requirements significantly influence architecture selection. For example, RNNs may be unsuitable for real-time applications due to their sequential nature, while Transformers might be limited by memory requirements for long sequences. Engineers must balance these constraints with performance needs to choose the most feasible architecture.</p>
<p><em>Learning Objective</em>: Analyze how computational constraints impact architecture selection decisions.</p></li>
<li><p><strong>The process of systematically matching data characteristics to neural network architectures is known as ____.</strong></p>
<p><em>Answer</em>: data-to-architecture mapping. This process involves aligning the strengths of neural network architectures with the specific patterns present in the data.</p>
<p><em>Learning Objective</em>: Recall the concept of data-to-architecture mapping in architecture selection.</p></li>
<li><p><strong>In a production system, what trade-offs must be considered when selecting an architecture for a task with strict latency requirements?</strong></p>
<p><em>Answer</em>: In a production system with strict latency requirements, trade-offs include choosing architectures that provide fast inference times, such as MLPs or optimized CNNs, over more complex architectures like Transformers, which may have higher latency due to attention mechanisms. The decision must also consider the trade-off between model accuracy and computational efficiency.</p>
<p><em>Learning Objective</em>: Evaluate trade-offs in architecture selection for production systems with specific constraints.</p></li>
</ol>
<p><a href="#quiz-question-sec-dnn-architectures-architecture-selection-framework-7a37" class="answer-label">â† Back to Questions</a></p>
</div></details>
</div>
<div id="quiz-answer-sec-dnn-architectures-unified-framework-inductive-biases-099d" class="callout callout-quiz-answer">
<details class="callout-quiz-answer fbx-answer closebutton"><summary><strong>Self-Check: Answer 1.9</strong></summary><div>
<ol type="1">
<li><p><strong>Which neural network architecture is characterized by the strongest inductive biases due to local connectivity and parameter sharing?</strong></p>
<ol type="a">
<li>MLPs</li>
<li>CNNs</li>
<li>RNNs</li>
<li>Transformers</li>
</ol>
<p><em>Answer</em>: The correct answer is B. CNNs. CNNs have strong inductive biases due to local connectivity, parameter sharing, and translation equivariance, making them well-suited for spatial data with local structure.</p>
<p><em>Learning Objective</em>: Understand the concept of inductive biases in CNNs and their impact on data processing.</p></li>
<li><p><strong>True or False: Transformers have a fixed inductive bias that limits their ability to adapt to different data structures.</strong></p>
<p><em>Answer</em>: False. Transformers have adaptive inductive biases through learned attention patterns, allowing them to dynamically adjust based on the data.</p>
<p><em>Learning Objective</em>: Recognize the adaptive nature of inductive biases in Transformer architectures.</p></li>
<li><p><strong>Explain how the hierarchical representation learning in neural networks influences system design decisions.</strong></p>
<p><em>Answer</em>: Hierarchical representation learning requires system designs that efficiently compose lower-level features into higher-level abstractions. This affects memory hierarchies, parallelization strategies, and hardware accelerators, which must support matrix operations and hierarchical computation. For example, memory hierarchies should align with representational hierarchies to minimize data movement costs. This is important because it directly impacts computational efficiency and system performance.</p>
<p><em>Learning Objective</em>: Understand the system design implications of hierarchical representation learning in neural networks.</p></li>
<li><p><strong>Order the following architectures by decreasing inductive bias strength: (1) CNNs, (2) RNNs, (3) MLPs, (4) Transformers.</strong></p>
<p><em>Answer</em>: The correct order is: (1) CNNs, (2) RNNs, (3) Transformers, (4) MLPs. CNNs have the strongest inductive biases due to local connectivity and parameter sharing, followed by RNNs with sequential processing. Transformers have adaptive biases, and MLPs have the weakest biases, relying on dense connectivity.</p>
<p><em>Learning Objective</em>: Classify neural network architectures based on the strength of their inductive biases.</p></li>
</ol>
<p><a href="#quiz-question-sec-dnn-architectures-unified-framework-inductive-biases-099d" class="answer-label">â† Back to Questions</a></p>
</div></details>
</div>
<div id="quiz-answer-sec-dnn-architectures-fallacies-pitfalls-3e82" class="callout callout-quiz-answer">
<details class="callout-quiz-answer fbx-answer closebutton"><summary><strong>Self-Check: Answer 1.10</strong></summary><div>
<ol type="1">
<li><p><strong>Which of the following misconceptions might lead a team to choose a transformer-based model over a simpler architecture?</strong></p>
<ol type="a">
<li>Transformers always require less computational resources.</li>
<li>Transformers are universally optimal for all tasks.</li>
<li>Simpler architectures cannot handle complex tasks.</li>
<li>More complex architectures inherently perform better.</li>
</ol>
<p><em>Answer</em>: The correct answer is D. More complex architectures inherently perform better. This misconception can lead to the inappropriate selection of complex models without considering task-specific needs and computational constraints. Options A, B, and C are incorrect because they misrepresent the trade-offs and applicability of transformers.</p>
<p><em>Learning Objective</em>: Identify and understand common misconceptions in neural network architecture selection.</p></li>
<li><p><strong>True or False: Ignoring computational implications during model selection can lead to deployment failures.</strong></p>
<p><em>Answer</em>: True. This is true because overlooking computational requirements can result in models that fail to meet latency or memory constraints in production environments.</p>
<p><em>Learning Objective</em>: Understand the importance of considering computational constraints in model selection.</p></li>
<li><p><strong>Explain why understanding hardware-architecture alignment is crucial for effective deployment strategies.</strong></p>
<p><em>Answer</em>: Understanding hardware-architecture alignment is crucial because different architectures exploit different hardware features. For example, CNNs benefit from specialized tensor cores, while RNNs require efficient sequential processing. This alignment ensures optimal performance and prevents inefficiencies when deploying models across various hardware platforms.</p>
<p><em>Learning Objective</em>: Analyze the impact of hardware characteristics on neural network performance and deployment.</p></li>
<li><p><strong>The practice of combining different architectural components without understanding their interaction effects can lead to unexpected ____.</strong></p>
<p><em>Answer</em>: bottlenecks. This can occur because each architectural pattern has distinct computational characteristics, and naive combinations may create conflicts.</p>
<p><em>Learning Objective</em>: Recognize the potential pitfalls of combining architectural components without careful analysis.</p></li>
<li><p><strong>In a production system, what are the trade-offs of designing architectures without considering the full hardware-software co-design implications?</strong></p>
<p><em>Answer</em>: Designing architectures without considering hardware-software co-design can lead to inefficiencies. For instance, an architecture optimized for GPUs may not suit edge devices due to memory constraints. This oversight can increase development cycles and computational costs. Effective design requires holistic analysis of the entire system stack, including compute infrastructure and operational constraints.</p>
<p><em>Learning Objective</em>: Evaluate the system-level trade-offs in neural network architecture design and deployment.</p></li>
</ol>
<p><a href="#quiz-question-sec-dnn-architectures-fallacies-pitfalls-3e82" class="answer-label">â† Back to Questions</a></p>
</div></details>
</div>
<div id="quiz-answer-sec-dnn-architectures-summary-c495" class="callout callout-quiz-answer">
<details class="callout-quiz-answer fbx-answer closebutton"><summary><strong>Self-Check: Answer 1.11</strong></summary><div>
<ol type="1">
<li><p><strong>Which neural network architecture is best suited for processing sequential data?</strong></p>
<ol type="a">
<li>Multi-Layer Perceptrons (MLPs)</li>
<li>Convolutional Neural Networks (CNNs)</li>
<li>Recurrent Neural Networks (RNNs)</li>
<li>Transformers</li>
</ol>
<p><em>Answer</em>: The correct answer is C. Recurrent Neural Networks (RNNs). RNNs are designed to handle sequential data due to their ability to maintain a state that captures information about previous inputs. MLPs and CNNs do not inherently handle sequences, and Transformers use attention mechanisms instead.</p>
<p><em>Learning Objective</em>: Understand the data assumptions underlying different neural network architectures.</p></li>
<li><p><strong>Explain how matrix multiplication serves as a fundamental computational primitive across various neural network architectures.</strong></p>
<p><em>Answer</em>: Matrix multiplication is central to neural networks as it underpins operations in dense layers, convolutions, and attention mechanisms. For example, in CNNs, convolutions can be expressed as matrix multiplications through techniques like im2col. This is important because it allows for efficient computation using optimized linear algebra libraries.</p>
<p><em>Learning Objective</em>: Comprehend the role of computational primitives in neural network operations.</p></li>
<li><p><strong>Order the following architectures by their typical memory access patterns, from local to global: (1) CNNs, (2) MLPs, (3) Transformers.</strong></p>
<p><em>Answer</em>: The correct order is: (1) CNNs, (2) MLPs, (3) Transformers. CNNs typically use local memory access patterns due to their spatial locality, MLPs have more global memory access due to dense connections, and Transformers require global memory access due to their attention mechanisms.</p>
<p><em>Learning Objective</em>: Understand the memory access patterns of different neural network architectures.</p></li>
<li><p><strong>In a production system, what are the implications of using transformer architectures in terms of memory and computation?</strong></p>
<p><em>Answer</em>: Transformers require significant memory due to their quadratic scaling with input size and compute-intensive attention mechanisms. For example, large models like GPT-3 need extensive memory and computational resources. This is important because it affects hardware selection and optimization strategies in deployment.</p>
<p><em>Learning Objective</em>: Analyze the system-level implications of deploying transformer architectures.</p></li>
</ol>
<p><a href="#quiz-question-sec-dnn-architectures-summary-c495" class="answer-label">â† Back to Questions</a></p>
</div></details>
</div>



</section>
</section>


<a onclick="window.scrollTo(0, 0); return false;" role="button" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> Back to top</a></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    // Ensure there is a toggle, if there isn't float one in the top right
    if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
      const a = window.document.createElement('a');
      a.classList.add('top-right');
      a.classList.add('quarto-color-scheme-toggle');
      a.href = "";
      a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
      const i = window.document.createElement("i");
      i.classList.add('bi');
      a.appendChild(i);
      window.document.body.appendChild(a);
    }
    setColorSchemeToggle(hasAlternateSentinel())
    const icon = "î§‹";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/mlsysbook\.ai\/book\/");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
            // target, if specified
            link.setAttribute("target", "_blank");
            if (link.getAttribute("rel") === null) {
              link.setAttribute("rel", "noopener");
            }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../../../contents/core/dl_primer/dl_primer.html" class="pagination-link" aria-label="DL Primer">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">DL Primer</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../../../contents/core/workflow/workflow.html" class="pagination-link" aria-label="AI Workflow">
        <span class="nav-page-text">AI Workflow</span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>Â© 2024-2025 Harvard University. Licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC-BY-NC-SA 4.0</a></p>
</div>   
    <div class="nav-footer-center">
<p>Written, edited and curated by Prof.&nbsp;Vijay Janapa Reddi (Harvard University)</p>
</div>
    <div class="nav-footer-right">
      <ul class="footer-items list-unstyled">
    <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/harvard-edge/cs249r_book">
      <i class="bi bi-github" role="img" aria-label="View source on GitHub">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/harvard-edge/cs249r_book">
      <i class="bi bi-star" role="img" aria-label="Star this repository">
</i> 
    </a>
  </li>  
    <li class="nav-item">
    <a class="nav-link" href="https://www.netlify.com">
<p><img src="https://www.netlify.com/v3/img/components/netlify-color-accent.svg" alt="Deploys by Netlify" style="height: 15px; vertical-align: middle; margin-left: 3px;"></p>
</a>
  </li>  
</ul>
    </div>
  </div>
</footer>
<script>var lightboxQuarto = GLightbox({"closeEffect":"zoom","descPosition":"bottom","loop":false,"openEffect":"zoom","selector":".lightbox"});
(function() {
  let previousOnload = window.onload;
  window.onload = () => {
    if (previousOnload) {
      previousOnload();
    }
    lightboxQuarto.on('slide_before_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      const href = trigger.getAttribute('href');
      if (href !== null) {
        const imgEl = window.document.querySelector(`a[href="${href}"] img`);
        if (imgEl !== null) {
          const srcAttr = imgEl.getAttribute("src");
          if (srcAttr && srcAttr.startsWith("data:")) {
            slideConfig.href = srcAttr;
          }
        }
      } 
    });
  
    lightboxQuarto.on('slide_after_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(slideNode);
      }
    });
  
  };
  
})();
          </script>




</body></html>