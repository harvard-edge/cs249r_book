<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.31">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>ML Systems Textbook</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../">
<link href="../../../contents/core/workflow/workflow.html" rel="next">
<link href="../../../contents/core/dl_primer/dl_primer.html" rel="prev">
<link href="../../../assets/images/icons/favicon.png" rel="icon" type="image/png">
<script src="../../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-4a990d8dcb58f517c7c86712b8f2ac7c.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../site_libs/bootstrap/bootstrap-30b86197b7ded4c9dddbbc9b93dd1506.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<link href="../../../site_libs/quarto-contrib/foldbox/foldbox.css" rel="stylesheet">
<script src="../../../site_libs/quarto-contrib/glightbox/glightbox.min.js"></script>
<link href="../../../site_libs/quarto-contrib/glightbox/glightbox.min.css" rel="stylesheet">
<link href="../../../site_libs/quarto-contrib/glightbox/lightbox.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-M21L0CBCVN"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-M21L0CBCVN', { 'anonymize_ip': true});
</script>
<script type="application/json" class="js-hypothesis-config">
{
  "theme": "clean",
  "openSidebar": false
}
</script>
<script async="" src="https://hypothes.is/embed.js"></script>
<script>
  window.document.addEventListener("DOMContentLoaded", function (_event) {
    document.body.classList.add('hypothesis-enabled');
  });
</script>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">

<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-M21L0CBCVN"></script>
<script type="module" src="../../../tools/scripts/ai_menu/dist/bundle.js" defer=""></script>
<script type="module" src="../../../tools/scripts/ai_menu/dist/sqlite3-opfs-async-proxy-B_ImRJXp.js"></script>
<script type="module" src="../../../tools/scripts/ai_menu/dist/sqlite3-worker1-bundler-friendly-CbDNa4by.js"></script>
<script type="module" src="../../../tools/scripts/ai_menu/dist/worker-voUF5YDa.js"></script>
<script src="../../../assets/scripts/sidebar-auto-collapse.js" defer=""></script>
<style>
.callout-resource-exercises {
  --color1: #E0F2F1;
  --color2: #20B2AA;
}
.callout-quiz-question {
  --color1: #F0F0F8;
  --color2: #5B4B8A;
}
.callout-resource-slides {
  --color1: #E0F2F1;
  --color2: #20B2AA;
}
.callout-chapter-connection {
  --color1: #FDF2F7;
  --color2: #A51C30;
}
.callout-quiz-answer {
  --color1: #E8F2EA;
  --color2: #4a7c59;
}
.callout-resource-videos {
  --color1: #E0F2F1;
  --color2: #20B2AA;
}
.callout-code {
  --color1: #F2F4F8;
  --color2: #D1D7E0;
}
</style>
<style>
details.callout-quiz-question > summary::before {
  background-image: url("../../../assets/images/icons/callouts/icon_callout-quiz-question.png");
}
details.callout-quiz-answer > summary::before {
  background-image: url("../../../assets/images/icons/callouts/icon_callout-quiz-answer.png");
}
details.callout-chapter-connection > summary::before {
  background-image: url("../../../assets/images/icons/callouts/icon_callout-chapter-connection.png");
}
details.callout-resource-slides > summary::before {
  background-image: url("../../../assets/images/icons/callouts/icon_callout-resource-slides.png");
}
details.callout-resource-videos > summary::before {
  background-image: url("../../../assets/images/icons/callouts/icon_callout-resource-videos.png");
}
details.callout-resource-exercises > summary::before {
  background-image: url("../../../assets/images/icons/callouts/icon_callout-resource-exercises.png");
}
</style>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating nav-fixed slimcontent quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-md " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="../../../index.html" class="navbar-brand navbar-brand-logo">
    <img src="../../../assets/images/icons/favicon.png" alt="" class="navbar-logo">
    </a>
    <a class="navbar-brand" href="../../../index.html">
    <span class="navbar-title">Machine Learning Systems</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link active" href="../../../contents/labs/labs.html" aria-current="page"> <i class="bi bi-code" role="img">
</i> 
<span class="menu-text">Labs</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../contents/labs/kits.html"> <i class="bi bi-box" role="img">
</i> 
<span class="menu-text">Kits</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://mlsysbook.ai/pdf" target="_blank"> <i class="bi bi-download" role="img">
</i> 
<span class="menu-text">PDF</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://github.com/harvard-edge/cs249r_book" target="_blank"> <i class="bi bi-star" role="img">
</i> 
<span class="menu-text">Star</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://opencollective.com/mlsysbook" target="_blank"> <i class="bi bi-heart" role="img">
</i> 
<span class="menu-text">Support</span></a>
  </li>  
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-github" role="link" data-bs-toggle="dropdown" aria-expanded="false">
      <i class="bi bi-github" role="img">
</i> 
 <span class="menu-text">GitHub</span>
    </a>
    <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="nav-menu-github">    
        <li>
    <a class="dropdown-item" href="https://github.com/harvard-edge/cs249r_book" target="_blank"><i class="bi bi-pencil" role="img">
</i> 
 <span class="dropdown-text">Edit this page</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://github.com/harvard-edge/cs249r_book/issues/new" target="_blank"><i class="bi bi-bug" role="img">
</i> 
 <span class="dropdown-text">Report an issue</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://github.com/harvard-edge/cs249r_book/discussions" target="_blank"><i class="bi bi-chat" role="img">
</i> 
 <span class="dropdown-text">Discussions</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://github.com/harvard-edge/cs249r_book" target="_blank"><i class="bi bi-code" role="img">
</i> 
 <span class="dropdown-text">View source</span></a>
  </li>  
    </ul>
  </li>
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../../contents/core/introduction/introduction.html">Systems Foundations</a></li><li class="breadcrumb-item"><a href="../../../contents/core/dnn_architectures/dnn_architectures.html">DNN Architectures</a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
    </div>
  </nav>
  <div id="quarto-announcement" data-announcement-id="80cf830e7ea2136b91547bb117b654b4" class="alert alert-primary hidden"><i class="bi bi-megaphone quarto-announcement-icon"></i><div class="quarto-announcement-content">
<p>🎉 <strong>Just Announced:</strong> <em>Machine Learning Systems</em> will be published by <strong>MIT Press</strong>. <a href="https://www.linkedin.com/posts/vijay-janapa-reddi-63a6a173_tinyml-tikz-ai-activity-7338324711145136128-6WU-?utm_source=share&amp;utm_medium=member_desktop&amp;rcm=ACoAAA-V7E4BcYtyZgLSeGhXC2t9jRSlmazfp-I">See the news →</a><br></p>
<p>🚀 <strong>Sneak Peek:</strong> <a href="https://mlsysbook.github.io/TinyTorch/intro.html">Tiny🔥Torch</a>. Build your own machine learning framework from scratch!<br></p>
<p>🧠 <strong>Self-checks:</strong> Added lightweight <a href="../../../contents/core/introduction/introduction.html#quiz-question-sec-introduction-ai-ml-basics-041a">quizzes</a> to each chapter for self-assessment.<br></p>
<p>📦 <strong>New Hardware:</strong> <a href="../../../contents/labs/kits.html">Seeed TinyML Kit</a>. Latest hands-on learning platform.</p>
</div><i class="bi bi-x-lg quarto-announcement-action"></i></div>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Homepage</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/frontmatter/foreword.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Foreword</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/frontmatter/about/about.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">About the Book</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/frontmatter/changelog/changelog.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Book Changelog</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/frontmatter/acknowledgements/acknowledgements.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Acknowledgements</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/frontmatter/socratiq/socratiq.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">SocratiQ AI</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Systems Foundations</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/introduction/introduction.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduction</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/ml_systems/ml_systems.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">ML Systems</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/dl_primer/dl_primer.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">DL Primer</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/dnn_architectures/dnn_architectures.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">DNN Architectures</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">Design Principles</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/workflow/workflow.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">AI Workflow</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/data_engineering/data_engineering.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Data Engineering</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/frameworks/frameworks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">AI Frameworks</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/training/training.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">AI Training</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true">
 <span class="menu-text">Performance Engineering</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/efficient_ai/efficient_ai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Efficient AI</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/optimizations/optimizations.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Model Optimizations</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/hw_acceleration/hw_acceleration.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">AI Acceleration</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/benchmarking/benchmarking.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Benchmarking AI</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true">
 <span class="menu-text">Robust Deployment</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/ops/ops.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">ML Operations</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/ondevice_learning/ondevice_learning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">On-Device Learning</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/robust_ai/robust_ai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Robust AI</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/privacy_security/privacy_security.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Security &amp; Privacy</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true">
 <span class="menu-text">Trustworthy Systems</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/responsible_ai/responsible_ai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Responsible AI</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/sustainable_ai/sustainable_ai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Sustainable AI</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/ai_for_good/ai_for_good.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">AI for Good</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true">
 <span class="menu-text">Frontiers of ML Systems</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/core/conclusion/conclusion.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Conclusion</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="true">
 <span class="menu-text">Hands-on Labs</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-7" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/labs.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Getting Started</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/kits.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Hardware Kits</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/ide_setup.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">IDE Setup</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" role="navigation" aria-expanded="true">
 <span class="menu-text">Arduino</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-8" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/arduino/nicla_vision/nicla_vision.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Overview</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/arduino/nicla_vision/setup/setup.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Setup</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/arduino/nicla_vision/image_classification/image_classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Image Classification</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/arduino/nicla_vision/object_detection/object_detection.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Object Detection</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/arduino/nicla_vision/kws/kws.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Keyword Spotting (KWS)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/arduino/nicla_vision/motion_classification/motion_classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Motion Classification and Anomaly Detection</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-9" role="navigation" aria-expanded="true">
 <span class="menu-text">Seeed XIAO ESP32S3</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-9" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-9" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/seeed/xiao_esp32s3/xiao_esp32s3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Overview</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/seeed/xiao_esp32s3/setup/setup.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Setup</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/seeed/xiao_esp32s3/image_classification/image_classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Image Classification</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/seeed/xiao_esp32s3/object_detection/object_detection.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Object Detection</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/seeed/xiao_esp32s3/kws/kws.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Keyword Spotting (KWS)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/seeed/xiao_esp32s3/motion_classification/motion_classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Motion Classification and Anomaly Detection</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-10" role="navigation" aria-expanded="true">
 <span class="menu-text">Grove Vision</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-10" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-10" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/seeed/grove_vision_ai_v2/grove_vision_ai_v2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Overview</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/seeed/grove_vision_ai_v2/setup_and_no_code_apps/setup_and_no_code_apps.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Setup and No-Code Applications</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/seeed/grove_vision_ai_v2/image_classification/image_classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Image Classification</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/seeed/grove_vision_ai_v2/object_detection/object_detection.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Object Detection</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-11" role="navigation" aria-expanded="true">
 <span class="menu-text">Raspberry Pi</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-11" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-11" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/raspi/raspi.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Overview</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/raspi/setup/setup.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Setup</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/raspi/image_classification/image_classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Image Classification</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/raspi/object_detection/object_detection.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Object Detection</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/raspi/llm/llm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Small Language Models (SLM)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/raspi/vlm/vlm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Vision-Language Models (VLM)</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-12" role="navigation" aria-expanded="true">
 <span class="menu-text">Shared</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-12" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-12" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/shared/shared.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Overview</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/shared/kws_feature_eng/kws_feature_eng.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">KWS Feature Engineering</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/labs/shared/dsp_spectral_features_block/dsp_spectral_features_block.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">DSP Spectral Features</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-13" role="navigation" aria-expanded="true">
 <span class="menu-text">Resources</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-13" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-13" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../contents/backmatter/resources/phd_survival_guide.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">PhD Survival Guide</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active" data-toc-expanded="99">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#dnn-architectures" id="toc-dnn-architectures" class="nav-link active" data-scroll-target="#dnn-architectures">DNN Architectures</a>
  <ul>
  <li><a href="#purpose" id="toc-purpose" class="nav-link" data-scroll-target="#purpose">Purpose</a></li>
  <li><a href="#sec-dnn-architectures-overview-aa0c" id="toc-sec-dnn-architectures-overview-aa0c" class="nav-link" data-scroll-target="#sec-dnn-architectures-overview-aa0c">Overview</a></li>
  <li><a href="#sec-dnn-architectures-multilayer-perceptrons-dense-pattern-processing-b8f9" id="toc-sec-dnn-architectures-multilayer-perceptrons-dense-pattern-processing-b8f9" class="nav-link" data-scroll-target="#sec-dnn-architectures-multilayer-perceptrons-dense-pattern-processing-b8f9">Multi-Layer Perceptrons: Dense Pattern Processing</a>
  <ul class="collapse">
  <li><a href="#sec-dnn-architectures-pattern-processing-needs-5e57" id="toc-sec-dnn-architectures-pattern-processing-needs-5e57" class="nav-link" data-scroll-target="#sec-dnn-architectures-pattern-processing-needs-5e57">Pattern Processing Needs</a></li>
  <li><a href="#sec-dnn-architectures-algorithmic-structure-6871" id="toc-sec-dnn-architectures-algorithmic-structure-6871" class="nav-link" data-scroll-target="#sec-dnn-architectures-algorithmic-structure-6871">Algorithmic Structure</a></li>
  <li><a href="#sec-dnn-architectures-computational-mapping-0b85" id="toc-sec-dnn-architectures-computational-mapping-0b85" class="nav-link" data-scroll-target="#sec-dnn-architectures-computational-mapping-0b85">Computational Mapping</a></li>
  <li><a href="#sec-dnn-architectures-system-implications-0c71" id="toc-sec-dnn-architectures-system-implications-0c71" class="nav-link" data-scroll-target="#sec-dnn-architectures-system-implications-0c71">System Implications</a>
  <ul class="collapse">
  <li><a href="#sec-dnn-architectures-memory-requirements-c5d5" id="toc-sec-dnn-architectures-memory-requirements-c5d5" class="nav-link" data-scroll-target="#sec-dnn-architectures-memory-requirements-c5d5">Memory Requirements</a></li>
  <li><a href="#sec-dnn-architectures-computation-needs-8dbb" id="toc-sec-dnn-architectures-computation-needs-8dbb" class="nav-link" data-scroll-target="#sec-dnn-architectures-computation-needs-8dbb">Computation Needs</a></li>
  <li><a href="#sec-dnn-architectures-data-movement-2ee4" id="toc-sec-dnn-architectures-data-movement-2ee4" class="nav-link" data-scroll-target="#sec-dnn-architectures-data-movement-2ee4">Data Movement</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#sec-dnn-architectures-convolutional-neural-networks-spatial-pattern-processing-660a" id="toc-sec-dnn-architectures-convolutional-neural-networks-spatial-pattern-processing-660a" class="nav-link" data-scroll-target="#sec-dnn-architectures-convolutional-neural-networks-spatial-pattern-processing-660a">Convolutional Neural Networks: Spatial Pattern Processing</a>
  <ul class="collapse">
  <li><a href="#sec-dnn-architectures-pattern-processing-needs-73a5" id="toc-sec-dnn-architectures-pattern-processing-needs-73a5" class="nav-link" data-scroll-target="#sec-dnn-architectures-pattern-processing-needs-73a5">Pattern Processing Needs</a></li>
  <li><a href="#sec-dnn-architectures-algorithmic-structure-d226" id="toc-sec-dnn-architectures-algorithmic-structure-d226" class="nav-link" data-scroll-target="#sec-dnn-architectures-algorithmic-structure-d226">Algorithmic Structure</a></li>
  <li><a href="#sec-dnn-architectures-computational-mapping-54c4" id="toc-sec-dnn-architectures-computational-mapping-54c4" class="nav-link" data-scroll-target="#sec-dnn-architectures-computational-mapping-54c4">Computational Mapping</a></li>
  <li><a href="#sec-dnn-architectures-system-implications-505c" id="toc-sec-dnn-architectures-system-implications-505c" class="nav-link" data-scroll-target="#sec-dnn-architectures-system-implications-505c">System Implications</a>
  <ul class="collapse">
  <li><a href="#sec-dnn-architectures-memory-requirements-a52f" id="toc-sec-dnn-architectures-memory-requirements-a52f" class="nav-link" data-scroll-target="#sec-dnn-architectures-memory-requirements-a52f">Memory Requirements</a></li>
  <li><a href="#sec-dnn-architectures-computation-needs-5ae3" id="toc-sec-dnn-architectures-computation-needs-5ae3" class="nav-link" data-scroll-target="#sec-dnn-architectures-computation-needs-5ae3">Computation Needs</a></li>
  <li><a href="#sec-dnn-architectures-data-movement-80f1" id="toc-sec-dnn-architectures-data-movement-80f1" class="nav-link" data-scroll-target="#sec-dnn-architectures-data-movement-80f1">Data Movement</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#sec-dnn-architectures-recurrent-neural-networks-sequential-pattern-processing-cc67" id="toc-sec-dnn-architectures-recurrent-neural-networks-sequential-pattern-processing-cc67" class="nav-link" data-scroll-target="#sec-dnn-architectures-recurrent-neural-networks-sequential-pattern-processing-cc67">Recurrent Neural Networks: Sequential Pattern Processing</a>
  <ul class="collapse">
  <li><a href="#sec-dnn-architectures-pattern-processing-needs-b61d" id="toc-sec-dnn-architectures-pattern-processing-needs-b61d" class="nav-link" data-scroll-target="#sec-dnn-architectures-pattern-processing-needs-b61d">Pattern Processing Needs</a></li>
  <li><a href="#sec-dnn-architectures-algorithmic-structure-3a7b" id="toc-sec-dnn-architectures-algorithmic-structure-3a7b" class="nav-link" data-scroll-target="#sec-dnn-architectures-algorithmic-structure-3a7b">Algorithmic Structure</a></li>
  <li><a href="#sec-dnn-architectures-computational-mapping-7112" id="toc-sec-dnn-architectures-computational-mapping-7112" class="nav-link" data-scroll-target="#sec-dnn-architectures-computational-mapping-7112">Computational Mapping</a></li>
  <li><a href="#sec-dnn-architectures-system-implications-18c3" id="toc-sec-dnn-architectures-system-implications-18c3" class="nav-link" data-scroll-target="#sec-dnn-architectures-system-implications-18c3">System Implications</a>
  <ul class="collapse">
  <li><a href="#sec-dnn-architectures-memory-requirements-5966" id="toc-sec-dnn-architectures-memory-requirements-5966" class="nav-link" data-scroll-target="#sec-dnn-architectures-memory-requirements-5966">Memory Requirements</a></li>
  <li><a href="#sec-dnn-architectures-computation-needs-cb19" id="toc-sec-dnn-architectures-computation-needs-cb19" class="nav-link" data-scroll-target="#sec-dnn-architectures-computation-needs-cb19">Computation Needs</a></li>
  <li><a href="#sec-dnn-architectures-data-movement-c045" id="toc-sec-dnn-architectures-data-movement-c045" class="nav-link" data-scroll-target="#sec-dnn-architectures-data-movement-c045">Data Movement</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#sec-dnn-architectures-attention-mechanisms-dynamic-pattern-processing-ea2d" id="toc-sec-dnn-architectures-attention-mechanisms-dynamic-pattern-processing-ea2d" class="nav-link" data-scroll-target="#sec-dnn-architectures-attention-mechanisms-dynamic-pattern-processing-ea2d">Attention Mechanisms: Dynamic Pattern Processing</a>
  <ul class="collapse">
  <li><a href="#sec-dnn-architectures-pattern-processing-needs-1d60" id="toc-sec-dnn-architectures-pattern-processing-needs-1d60" class="nav-link" data-scroll-target="#sec-dnn-architectures-pattern-processing-needs-1d60">Pattern Processing Needs</a></li>
  <li><a href="#sec-dnn-architectures-basic-attention-mechanism-916f" id="toc-sec-dnn-architectures-basic-attention-mechanism-916f" class="nav-link" data-scroll-target="#sec-dnn-architectures-basic-attention-mechanism-916f">Basic Attention Mechanism</a>
  <ul class="collapse">
  <li><a href="#sec-dnn-architectures-algorithmic-structure-886d" id="toc-sec-dnn-architectures-algorithmic-structure-886d" class="nav-link" data-scroll-target="#sec-dnn-architectures-algorithmic-structure-886d">Algorithmic Structure</a></li>
  <li><a href="#sec-dnn-architectures-computational-mapping-b1e6" id="toc-sec-dnn-architectures-computational-mapping-b1e6" class="nav-link" data-scroll-target="#sec-dnn-architectures-computational-mapping-b1e6">Computational Mapping</a></li>
  <li><a href="#sec-dnn-architectures-system-implications-c457" id="toc-sec-dnn-architectures-system-implications-c457" class="nav-link" data-scroll-target="#sec-dnn-architectures-system-implications-c457">System Implications</a></li>
  </ul></li>
  <li><a href="#sec-dnn-architectures-transformers-selfattention-e89e" id="toc-sec-dnn-architectures-transformers-selfattention-e89e" class="nav-link" data-scroll-target="#sec-dnn-architectures-transformers-selfattention-e89e">Transformers and Self-Attention</a>
  <ul class="collapse">
  <li><a href="#sec-dnn-architectures-algorithmic-structure-4d25" id="toc-sec-dnn-architectures-algorithmic-structure-4d25" class="nav-link" data-scroll-target="#sec-dnn-architectures-algorithmic-structure-4d25">Algorithmic Structure</a></li>
  <li><a href="#sec-dnn-architectures-computational-mapping-56b8" id="toc-sec-dnn-architectures-computational-mapping-56b8" class="nav-link" data-scroll-target="#sec-dnn-architectures-computational-mapping-56b8">Computational Mapping</a></li>
  <li><a href="#sec-dnn-architectures-system-implications-6010" id="toc-sec-dnn-architectures-system-implications-6010" class="nav-link" data-scroll-target="#sec-dnn-architectures-system-implications-6010">System Implications</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#sec-dnn-architectures-architectural-building-blocks-e63a" id="toc-sec-dnn-architectures-architectural-building-blocks-e63a" class="nav-link" data-scroll-target="#sec-dnn-architectures-architectural-building-blocks-e63a">Architectural Building Blocks</a>
  <ul class="collapse">
  <li><a href="#sec-dnn-architectures-perceptron-multilayer-networks-c64f" id="toc-sec-dnn-architectures-perceptron-multilayer-networks-c64f" class="nav-link" data-scroll-target="#sec-dnn-architectures-perceptron-multilayer-networks-c64f">From Perceptron to Multi-Layer Networks</a></li>
  <li><a href="#sec-dnn-architectures-dense-spatial-processing-3a56" id="toc-sec-dnn-architectures-dense-spatial-processing-3a56" class="nav-link" data-scroll-target="#sec-dnn-architectures-dense-spatial-processing-3a56">From Dense to Spatial Processing</a></li>
  <li><a href="#sec-dnn-architectures-evolution-sequence-processing-379a" id="toc-sec-dnn-architectures-evolution-sequence-processing-379a" class="nav-link" data-scroll-target="#sec-dnn-architectures-evolution-sequence-processing-379a">The Evolution of Sequence Processing</a></li>
  <li><a href="#sec-dnn-architectures-modern-architectures-synthesis-innovation-f734" id="toc-sec-dnn-architectures-modern-architectures-synthesis-innovation-f734" class="nav-link" data-scroll-target="#sec-dnn-architectures-modern-architectures-synthesis-innovation-f734">Modern Architectures: Synthesis and Innovation</a></li>
  </ul></li>
  <li><a href="#sec-dnn-architectures-systemlevel-building-blocks-56ed" id="toc-sec-dnn-architectures-systemlevel-building-blocks-56ed" class="nav-link" data-scroll-target="#sec-dnn-architectures-systemlevel-building-blocks-56ed">System-Level Building Blocks</a>
  <ul class="collapse">
  <li><a href="#sec-dnn-architectures-core-computational-primitives-97a1" id="toc-sec-dnn-architectures-core-computational-primitives-97a1" class="nav-link" data-scroll-target="#sec-dnn-architectures-core-computational-primitives-97a1">Core Computational Primitives</a></li>
  <li><a href="#sec-dnn-architectures-memory-access-primitives-8150" id="toc-sec-dnn-architectures-memory-access-primitives-8150" class="nav-link" data-scroll-target="#sec-dnn-architectures-memory-access-primitives-8150">Memory Access Primitives</a></li>
  <li><a href="#sec-dnn-architectures-data-movement-primitives-8540" id="toc-sec-dnn-architectures-data-movement-primitives-8540" class="nav-link" data-scroll-target="#sec-dnn-architectures-data-movement-primitives-8540">Data Movement Primitives</a></li>
  <li><a href="#sec-dnn-architectures-system-design-impact-9a93" id="toc-sec-dnn-architectures-system-design-impact-9a93" class="nav-link" data-scroll-target="#sec-dnn-architectures-system-design-impact-9a93">System Design Impact</a></li>
  </ul></li>
  <li><a href="#sec-dnn-architectures-summary-36be" id="toc-sec-dnn-architectures-summary-36be" class="nav-link" data-scroll-target="#sec-dnn-architectures-summary-36be">Summary</a></li>
  <li><a href="#self-check-answers" id="toc-self-check-answers" class="nav-link" data-scroll-target="#self-check-answers">Self-Check Answers</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content page-columns page-full" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../../contents/core/introduction/introduction.html">Systems Foundations</a></li><li class="breadcrumb-item"><a href="../../../contents/core/dnn_architectures/dnn_architectures.html">DNN Architectures</a></li></ol></nav></header>




<section id="dnn-architectures" class="level1 page-columns page-full">
<h1>DNN Architectures</h1>
<div class="{layout-narrow} page-columns page-full">

<div class="no-row-height column-margin column-container"><div class="">
<p><em>DALL·E 3 Prompt: A visually striking rectangular image illustrating the interplay between deep learning algorithms like CNNs, RNNs, and Attention Networks, interconnected with machine learning systems. The composition features neural network diagrams blending seamlessly with representations of computational systems such as processors, graphs, and data streams. Bright neon tones contrast against a dark futuristic background, symbolizing cutting-edge technology and intricate system complexity.</em></p>
</div></div><p> <img src="images/png/cover_dl_arch.png" class="img-fluid"></p>
</div>
<section id="purpose" class="level2 unnumbered">
<h2 class="unnumbered anchored" data-anchor-id="purpose">Purpose</h2>
<p><em>What recurring patterns emerge across modern deep learning architectures, and how do these patterns enable systematic approaches to AI system design?</em></p>
<p>Deep learning architectures represent a convergence of computational patterns that form the building blocks of modern AI systems. These foundational patterns, ranging from convolutional structures to attention mechanisms, reveal how complex models arise from simple, repeatable components. The examination of these architectural elements provides insights into the systematic construction of flexible, efficient AI systems, establishing core principles that influence every aspect of system design and deployment. These structural insights illuminate the path toward creating scalable, adaptable solutions across diverse application domains.</p>
<div class="callout callout-style-default callout-tip callout-titled" title="Learning Objectives">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Learning Objectives
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><p>Map fundamental neural network concepts to deep learning architectures (dense, spatial, temporal, attention-based).</p></li>
<li><p>Analyze how architectural patterns shape computational and memory demands.</p></li>
<li><p>Evaluate system-level impacts of architectural choices on system attributes.</p></li>
<li><p>Compare architectures’ hardware mapping and identify optimization strategies.</p></li>
<li><p>Assess trade-offs between complexity and system needs for specific applications.</p></li>
</ul>
</div>
</div>
</section>
<section id="sec-dnn-architectures-overview-aa0c" class="level2">
<h2 class="anchored" data-anchor-id="sec-dnn-architectures-overview-aa0c">Overview</h2>
<p>Deep learning architecture stands for specific representation or organizations of neural network components, the neurons, weights, and connections (as introduced in <a href="../../../contents/core/dl_primer/dl_primer.html#sec-dl_primer">Chapter 3</a>), arranged to efficiently process different types of patterns in data. While the previous chapter established the fundamental building blocks of neural networks, in this chapter we examine how these components are structured into architectures that map efficiently to computer systems.</p>
<p>Neural network architectures have evolved to address specific pattern processing challenges. Whether processing arbitrary feature relationships, exploiting spatial patterns, managing temporal dependencies, or handling dynamic information flow, each architectural pattern emerged from particular computational needs. These architectures, from a computer systems perspective, require an examination of how their computational patterns map to system resources.</p>
<p>Most often the architectures are discussed in terms of their algorithmic structures (MLPs, CNNs, RNNs, Transformers). However, in this chapter we take a more fundamental approach by examining how their computational patterns map to hardware resources. Each section analyzes how specific pattern processing needs influence algorithmic structure and how these structures map to computer system resources. The implications for computer system design require examining how their computational patterns map to hardware resources. The mapping from algorithmic requirements to computer system design involves several key considerations:</p>
<ol type="1">
<li>Memory access patterns: How data moves through the memory hierarchy</li>
<li>Computation characteristics: The nature and organization of arithmetic operations</li>
<li>Data movement: Requirements for on-chip and off-chip data transfer</li>
<li>Resource utilization: How computational and memory resources are allocated</li>
</ol>
<p>For example, dense connectivity patterns generate different memory bandwidth demands than localized processing structures. Similarly, stateful processing creates distinct requirements for on-chip memory organization compared to stateless operations. Getting a firm grasp on these mappings is important for modern computer architects and system designers who must implement these algorithms efficiently in hardware.</p>
<div id="quiz-question-sec-dnn-architectures-overview-aa0c" class="callout callout-quiz-question">
<details class="callout-quiz-question fbx-default closebutton"><summary><strong>Self-Check: Question 1.1</strong></summary><div>
<ol type="1">
<li><p>Which aspect of neural network architecture is directly concerned with how data moves through the memory hierarchy?</p>
<ol type="a">
<li>Computation characteristics</li>
<li>Memory access patterns</li>
<li>Data movement</li>
<li>Resource utilization</li>
</ol></li>
<li><p>Explain why dense connectivity patterns in neural networks generate different memory bandwidth demands compared to localized processing structures.</p></li>
<li><p>Stateful processing in neural networks requires different on-chip memory organization compared to stateless operations.</p></li>
</ol>
<p><a href="#quiz-answer-sec-dnn-architectures-overview-aa0c" class="question-label">See Answers →</a></p>
</div></details>
</div>
</section>
<section id="sec-dnn-architectures-multilayer-perceptrons-dense-pattern-processing-b8f9" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="sec-dnn-architectures-multilayer-perceptrons-dense-pattern-processing-b8f9">Multi-Layer Perceptrons: Dense Pattern Processing</h2>
<p>Multi-Layer Perceptrons (MLPs) represent the most direct extension of neural networks into deep architectures. Unlike more specialized networks, MLPs process each input element with equal importance, making them versatile but computationally intensive. Their architecture, while simple, establishes fundamental computational patterns that appear throughout deep learning systems. These patterns were initially formalized by the introduction of the Universal Approximation Theorem (UAT) <span class="citation" data-cites="cybenko1989approximation hornik1989multilayer">(<a href="#ref-cybenko1989approximation" role="doc-biblioref">Cybenko 1992</a>; <a href="#ref-hornik1989multilayer" role="doc-biblioref">Hornik, Stinchcombe, and White 1989</a>)</span>, which states that a sufficiently large MLP with non-linear activation functions can approximate any continuous function on a compact domain, given suitable weights and biases.</p>
<div class="no-row-height column-margin column-container"><div id="ref-cybenko1989approximation" class="csl-entry" role="listitem">
Cybenko, G. 1992. <span>“Approximation by Superpositions of a Sigmoidal Function.”</span> <em>Mathematics of Control, Signals, and Systems</em> 5 (4): 455–55. <a href="https://doi.org/10.1007/bf02134016">https://doi.org/10.1007/bf02134016</a>.
</div><div id="ref-hornik1989multilayer" class="csl-entry" role="listitem">
Hornik, Kurt, Maxwell Stinchcombe, and Halbert White. 1989. <span>“Multilayer Feedforward Networks Are Universal Approximators.”</span> <em>Neural Networks</em> 2 (5): 359–66. <a href="https://doi.org/10.1016/0893-6080(89)90020-8">https://doi.org/10.1016/0893-6080(89)90020-8</a>.
</div></div><p>When applied to the MNIST handwritten digit recognition challenge, an MLP reveals its computational power by transforming a complex <span class="math inline">\(28\times 28\)</span> pixel image into a precise digit classification. By treating each of the 784 pixels as an equally weighted input, the network learns to decompose visual information through a systematic progression of layers, converting raw pixel intensities into increasingly abstract representations that capture the essential characteristics of handwritten digits.</p>
<section id="sec-dnn-architectures-pattern-processing-needs-5e57" class="level3">
<h3 class="anchored" data-anchor-id="sec-dnn-architectures-pattern-processing-needs-5e57">Pattern Processing Needs</h3>
<p>Deep learning systems frequently encounter problems where any input feature could potentially influence any output, as there are no inherent constraints on these relationships. Consider analyzing financial market data: any economic indicator might affect any market outcome or in natural language processing, where the meaning of a word could depend on any other word in the sentence. These scenarios demand an architectural pattern capable of learning arbitrary relationships across all input features.</p>
<p>Dense pattern processing addresses this fundamental need by enabling several key capabilities. First, it allows unrestricted feature interactions where each output can depend on any combination of inputs. Second, it facilitates learned feature importance, allowing the system to determine which connections matter rather than having them prescribed. Finally, it provides adaptive representation, enabling the network to reshape its internal representations based on the data.</p>
<p>For example, in the MNIST digit recognition task, while humans might focus on specific parts of digits (like loops in ‘6’ or crossings in ‘8’), we cannot definitively say which pixel combinations are important for classification. A ‘7’ written with a serif could share pixel patterns with a ‘2’, while variations in handwriting mean discriminative features might appear anywhere in the image. This uncertainty about feature relationships necessitates a dense processing approach where every pixel can potentially influence the classification decision.</p>
</section>
<section id="sec-dnn-architectures-algorithmic-structure-6871" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-dnn-architectures-algorithmic-structure-6871">Algorithmic Structure</h3>
<p>To enable unrestricted feature interactions, MLPs implement a direct algorithmic solution: connect everything to everything. This is realized through a series of fully-connected layers, where each neuron connects to every neuron in adjacent layers. The dense connectivity pattern translates mathematically into matrix multiplication operations. As shown in <a href="#fig-mlp" class="quarto-xref">Figure&nbsp;1</a>, each layer transforms its input through matrix multiplication followed by element-wise activation: <span class="math display">\[
\mathbf{h}^{(l)} = f\big(\mathbf{W}^{(l)}\mathbf{h}^{(l-1)} + \mathbf{b}^{(l)}\big)
\]</span></p>
<div id="fig-mlp" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full" data-fig-env="figure" data-fig-pos="htb">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-mlp-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="5aab87381f560ddc7f720233b9e89a654d299fa1.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="Figure&nbsp;1: Layered Transformations: Multi-layer perceptrons (mlps) implement dense connectivity through sequential matrix multiplications and non-linear activations, enabling complex feature interactions and hierarchical representations of input data. Each layer transforms the input vector from the previous layer, producing a new vector that serves as input to the subsequent layer, as defined by the equation in the text. Source: [@reagen2017deep]."><img src="dnn_architectures_files/mediabag/5aab87381f560ddc7f720233b9e89a654d299fa1.svg" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-mlp-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: <strong>Layered Transformations</strong>: Multi-layer perceptrons (mlps) implement dense connectivity through sequential matrix multiplications and non-linear activations, enabling complex feature interactions and hierarchical representations of input data. Each layer transforms the input vector from the previous layer, producing a new vector that serves as input to the subsequent layer, as defined by the equation in the text. Source: <span class="citation" data-cites="reagen2017deep">(<a href="#ref-reagen2017deep" role="doc-biblioref">Reagen et al. 2017</a>)</span>.
</figcaption>
<div class="no-row-height column-margin column-container"><div id="ref-reagen2017deep" class="csl-entry" role="listitem">
Reagen, Brandon, Robert Adolf, Paul Whatmough, Gu-Yeon Wei, and David Brooks. 2017. <em>Deep Learning for Computer Architects</em>. Springer International Publishing. <a href="https://doi.org/10.1007/978-3-031-01756-8">https://doi.org/10.1007/978-3-031-01756-8</a>.
</div></div></figure>
</div>
<p>The dimensions of these operations reveal the computational scale of dense pattern processing:</p>
<ul>
<li>Input vector: <span class="math inline">\(\mathbf{h}^{(0)} \in \mathbb{R}^{d_{\text{in}}}\)</span> represents all potential input features</li>
<li>Weight matrices: <span class="math inline">\(\mathbf{W}^{(l)} \in \mathbb{R}^{d_{\text{out}} \times d_{\text{in}}}\)</span> capture all possible input-output relationships</li>
<li>Output vector: <span class="math inline">\(\mathbf{h}^{(l)} \in \mathbb{R}^{d_{\text{out}}}\)</span> produces transformed representations</li>
</ul>
<p>In the MNIST example, this means:</p>
<ul>
<li>Each 784-dimensional input (<span class="math inline">\(28\times 28\)</span> pixels) connects to every neuron in the first hidden layer</li>
<li>A hidden layer with 100 neurons requires a <span class="math inline">\(784\times 100\)</span> weight matrix</li>
<li>Each weight in this matrix is a learnable relationship between an input pixel and a hidden feature</li>
</ul>
<p>This algorithmic structure directly addresses our need for arbitrary feature relationships but creates specific computational patterns that must be handled efficiently by computer systems.</p>
</section>
<section id="sec-dnn-architectures-computational-mapping-0b85" class="level3">
<h3 class="anchored" data-anchor-id="sec-dnn-architectures-computational-mapping-0b85">Computational Mapping</h3>
<p>The elegant mathematical representation of dense matrix multiplication maps to specific computational patterns that systems must handle. Let’s examine how this mapping progresses from mathematical abstraction to computational reality.</p>
<p>The first implementation is shown in <a href="#lst-mlp_layer_matrix" class="quarto-xref">Listing&nbsp;1</a>. The function mlp_layer_matrix directly mirrors our mathematical equation. It uses high-level matrix operations (<code>matmul</code>) to express the computation in a single line, hiding the underlying complexity. This is the style commonly used in deep learning frameworks, where optimized libraries handle the actual computation.</p>
<div id="lst-mlp_layer_matrix" class="listing quarto-float quarto-figure quarto-figure-left anchored">
<figure class="quarto-float quarto-float-lst figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-lst" id="lst-mlp_layer_matrix-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Listing&nbsp;1: <strong>Dense Layer Implementation</strong>: Neural networks perform weighted sum and activation functions across layers using matrix operations through The code. This emphasizes the core computational pattern in multi-layer perceptrons.
</figcaption>
<div aria-describedby="lst-mlp_layer_matrix-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="sourceCode" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> mlp_layer_matrix(X, W, b):</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># X: input matrix (batch_size × num_inputs)</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>    <span class="co"># W: weight matrix (num_inputs × num_outputs)</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># b: bias vector (num_outputs)</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>    H <span class="op">=</span> activation(matmul(X, W) <span class="op">+</span> b)</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>    <span class="co"># One clean line of math</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> H</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</figure>
</div>
<p>The second implementation, <code>mlp_layer_compute</code> (shown in <a href="#lst-mlp_layer_compute" class="quarto-xref">Listing&nbsp;2</a>), exposes the actual computational pattern through nested loops. This version shows us what really happens when we compute a layer’s output: we process each sample in the batch, computing each output neuron by accumulating weighted contributions from all inputs.</p>
<div id="lst-mlp_layer_compute" class="listing quarto-float quarto-figure quarto-figure-left anchored">
<figure class="quarto-float quarto-float-lst figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-lst" id="lst-mlp_layer_compute-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Listing&nbsp;2: <strong>Core Computational Pattern</strong>: Computes each output neuron by accumulating weighted contributions from all inputs across the batch. This implementation exposes the detailed step-by-step process of how a single layer in a neural network processes data, emphasizing the role of biases and weighted sums in producing outputs.
</figcaption>
<div aria-describedby="lst-mlp_layer_compute-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="sourceCode" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> mlp_layer_compute(X, W, b):</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Process each sample in the batch</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> batch <span class="kw">in</span> <span class="bu">range</span>(batch_size):</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Compute each output neuron</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> out <span class="kw">in</span> <span class="bu">range</span>(num_outputs):</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Initialize with bias</span></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>            Z[batch,out] <span class="op">=</span> b[out]</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Accumulate weighted inputs</span></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> in_ <span class="kw">in</span> <span class="bu">range</span>(num_inputs):</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>                Z[batch,out] <span class="op">+=</span> X[batch,in_] <span class="op">*</span> W[in_,out]</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>    H <span class="op">=</span> activation(Z)</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> H</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</figure>
</div>
<p>This translation from mathematical abstraction to concrete computation exposes how dense matrix multiplication decomposes into nested loops of simpler operations. The outer loop processes each sample in the batch, while the middle loop computes values for each output neuron. Within the innermost loop, the system performs repeated multiply-accumulate operations, combining each input with its corresponding weight.</p>
<p>In the MNIST example, each output neuron requires 784 multiply-accumulate operations and at least 1,568 memory accesses (784 for inputs, 784 for weights). While actual implementations use sophisticated optimizations through libraries like <a href="https://www.netlib.org/blas/">BLAS</a> or <a href="https://developer.nvidia.com/cublas">cuBLAS</a>, these fundamental patterns drive key system design decisions.</p>
</section>
<section id="sec-dnn-architectures-system-implications-0c71" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-dnn-architectures-system-implications-0c71">System Implications</h3>
<p>When analyzing how computational patterns impact computer systems, we typically examine three fundamental dimensions: memory requirements, computation needs, and data movement. This framework enables a systematic analysis of how algorithmic patterns influence system design decisions. We will use this framework for analyzing other network architectures, allowing us to compare and contrast their different characteristics.</p>
<section id="sec-dnn-architectures-memory-requirements-c5d5" class="level4">
<h4 class="anchored" data-anchor-id="sec-dnn-architectures-memory-requirements-c5d5">Memory Requirements</h4>
<p>For dense pattern processing, the memory requirements stem from storing and accessing weights, inputs, and intermediate results. In our MNIST example, connecting our 784-dimensional input layer to a hidden layer of 100 neurons requires 78,400 weight parameters. Each forward pass must access all these weights, along with input data and intermediate results. The all-to-all connectivity pattern means there’s no inherent locality in these accesses—every output needs every input and its corresponding weights.</p>
<p>These memory access patterns suggest opportunities for optimization through careful data organization and reuse. Modern processors handle these patterns differently; CPUs leverage their cache hierarchy for data reuse, while GPUs employ specialized memory hierarchies designed for high-bandwidth access. Deep learning frameworks abstract these hardware-specific details through optimized matrix multiplication implementations.</p>
</section>
<section id="sec-dnn-architectures-computation-needs-8dbb" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="sec-dnn-architectures-computation-needs-8dbb">Computation Needs</h4>
<p>The core computation revolves around multiply-accumulate operations<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> arranged in nested loops. Each output value requires as many multiply-accumulates as there are inputs. For MNIST, this means 784 multiply-accumulates per output neuron. With 100 neurons in our hidden layer, we’re performing 78,400 multiply-accumulates for a single input image. While these operations are simple, their volume and arrangement create specific demands on processing resources.</p>
<div class="no-row-height column-margin column-container"><div id="fn1"><p><sup>1</sup>&nbsp;<strong>Multiply-Accumulate Operation</strong>: A basic operation in digital computing and neural networks that multiplies two numbers and adds the result to an accumulator.</p></div></div><p>This computational structure lends itself to particular optimization strategies in modern hardware. The dense matrix multiplication pattern can be efficiently parallelized across multiple processing units, with each handling different subsets of neurons. Modern hardware accelerators take advantage of this through specialized matrix multiplication units, while deep learning frameworks automatically convert these operations into optimized BLAS (Basic Linear Algebra Subprograms) calls. CPUs and GPUs can both exploit cache locality by carefully tiling the computation to maximize data reuse, though their specific approaches differ based on their architectural strengths.</p>
</section>
<section id="sec-dnn-architectures-data-movement-2ee4" class="level4">
<h4 class="anchored" data-anchor-id="sec-dnn-architectures-data-movement-2ee4">Data Movement</h4>
<p>The all-to-all connectivity pattern in MLPs creates significant data movement requirements. Each multiply-accumulate operation needs three pieces of data: an input value, a weight value, and the running sum. For our MNIST example layer, computing a single output value requires moving 784 inputs and 784 weights to wherever the computation occurs. This movement pattern repeats for each of the 100 output neurons, creating substantial data transfer demands between memory and compute units.</p>
<p>The predictable nature of these data movement patterns enables strategic data staging and transfer optimizations. Different architectures address this challenge through various mechanisms; CPUs use sophisticated prefetching and multi-level caches; meanwhile, GPUs employ high-bandwidth memory systems and latency hiding through massive threading. Deep learning frameworks orchestrate these data movements through optimized memory management systems.</p>
<div id="quiz-question-sec-dnn-architectures-multilayer-perceptrons-dense-pattern-processing-b8f9" class="callout callout-quiz-question">
<details class="callout-quiz-question fbx-default closebutton"><summary><strong>Self-Check: Question 1.2</strong></summary><div>
<ol type="1">
<li><p>What is the primary computational operation used in Multi-Layer Perceptrons (MLPs) for dense pattern processing?</p>
<ol type="a">
<li>Convolution</li>
<li>Matrix multiplication</li>
<li>Pooling</li>
<li>Recurrent connections</li>
</ol></li>
<li><p>Explain why dense pattern processing in MLPs is suitable for tasks like MNIST digit recognition.</p></li>
<li><p>True or False: In MLPs, each output neuron requires the same number of multiply-accumulate operations as there are input features.</p></li>
<li><p>The dense connectivity pattern in MLPs translates mathematically into ____ operations.</p></li>
<li><p>Discuss the system implications of the data movement requirements in MLPs.</p></li>
</ol>
<p><a href="#quiz-answer-sec-dnn-architectures-multilayer-perceptrons-dense-pattern-processing-b8f9" class="question-label">See Answers →</a></p>
</div></details>
</div>
</section>
</section>
</section>
<section id="sec-dnn-architectures-convolutional-neural-networks-spatial-pattern-processing-660a" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="sec-dnn-architectures-convolutional-neural-networks-spatial-pattern-processing-660a">Convolutional Neural Networks: Spatial Pattern Processing</h2>
<p>While MLPs treat each input element independently, many real-world data types exhibit strong spatial relationships. Images, for example, derive their meaning from the spatial arrangement of pixels—a pattern of edges and textures that form recognizable objects. Audio signals show temporal patterns of frequency components, and sensor data often contains spatial or temporal correlations. These spatial relationships suggest that treating every input-output connection with equal importance, as MLPs do, might not be the most effective approach.</p>
<section id="sec-dnn-architectures-pattern-processing-needs-73a5" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-dnn-architectures-pattern-processing-needs-73a5">Pattern Processing Needs</h3>
<p>Spatial pattern processing addresses scenarios where the relationship between data points depends on their relative positions or proximity. Consider processing a natural image: a pixel’s relationship with its neighbors is important for detecting edges, textures, and shapes. These local patterns then combine hierarchically to form more complex features—edges form shapes, shapes form objects, and objects form scenes.</p>
<p>This hierarchical spatial pattern processing appears across many domains. In computer vision, local pixel patterns form edges and textures that combine into recognizable objects. Speech processing relies on patterns across nearby time segments to identify phonemes and words. Sensor networks analyze correlations between physically proximate sensors to understand environmental patterns. Medical imaging depends on recognizing tissue patterns that indicate biological structures.</p>
<p>Taking image processing as an example, if we want to detect a cat in an image, certain spatial patterns must be recognized: the triangular shape of ears, the round contours of the face, the texture of fur. Importantly, these patterns maintain their meaning regardless of where they appear in the image—a cat is still a cat whether it’s in the top-left or bottom-right corner. This suggests two key requirements for spatial pattern processing: the ability to detect local patterns and the ability to recognize these patterns regardless of their position.</p>
<div id="fig-cnn-spatial-processing" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-env="figure" data-fig-pos="htb">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-cnn-spatial-processing-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="c8b06853acf5ccdc49c2acfab35b58b178bac9b1.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-2" title="Figure&nbsp;2: Spatial Feature Extraction: Convolutional neural networks identify patterns independent of their location in an image by applying learnable filters across the input, enabling robust object recognition. These filters detect local features, and their repeated application across the image creates translation invariance—the ability to recognize a pattern regardless of its position."><img src="dnn_architectures_files/mediabag/c8b06853acf5ccdc49c2acfab35b58b178bac9b1.svg" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-cnn-spatial-processing-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: <strong>Spatial Feature Extraction</strong>: Convolutional neural networks identify patterns independent of their location in an image by applying learnable filters across the input, enabling robust object recognition. These filters detect local features, and their repeated application across the image creates translation invariance—the ability to recognize a pattern regardless of its position.
</figcaption>
</figure>
</div>
<p>This leads us to the convolutional neural network architecture (CNN), introduced by <span class="citation" data-cites="lecun1989backpropagation">Y. LeCun et al. (<a href="#ref-lecun1989backpropagation" role="doc-biblioref">1989</a>)</span>. As illustrated in <a href="#fig-cnn-spatial-processing" class="quarto-xref">Figure&nbsp;2</a>, CNNs address spatial pattern processing through a fundamentally different connection pattern than MLPs. Instead of connecting every input to every output, CNNs use a local connection pattern where each output connects only to a small, spatially contiguous region of the input. This local receptive field moves across the input space, applying the same set of weights at each position—a process known as convolution<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-lecun1989backpropagation" class="csl-entry" role="listitem">
LeCun, Y., B. Boser, J. S. Denker, D. Henderson, R. E. Howard, W. Hubbard, and L. D. Jackel. 1989. <span>“Backpropagation Applied to Handwritten Zip Code Recognition.”</span> <em>Neural Computation</em> 1 (4): 541–51. <a href="https://doi.org/10.1162/neco.1989.1.4.541">https://doi.org/10.1162/neco.1989.1.4.541</a>.
</div><div id="fn2"><p><sup>2</sup>&nbsp;<strong>Convolution</strong>: A mathematical operation on two functions producing a third function expressing how the shape of one is modified by the other.</p></div></div></section>
<section id="sec-dnn-architectures-algorithmic-structure-d226" class="level3">
<h3 class="anchored" data-anchor-id="sec-dnn-architectures-algorithmic-structure-d226">Algorithmic Structure</h3>
<p>The core operation in a CNN can be expressed mathematically as: <span class="math display">\[
\mathbf{H}^{(l)}_{i,j,k} = f\left(\sum_{di}\sum_{dj}\sum_{c} \mathbf{W}^{(l)}_{di,dj,c,k}\mathbf{H}^{(l-1)}_{i+di,j+dj,c} + \mathbf{b}^{(l)}_k\right)
\]</span></p>
<p>Here, <span class="math inline">\((i,j)\)</span> corresponds to spatial positions, <span class="math inline">\(k\)</span> indexes output channels, <span class="math inline">\(c\)</span> indexes input channels, and <span class="math inline">\((di,dj)\)</span> spans the local receptive field. Unlike the dense matrix multiplication of MLPs, this operation:</p>
<ul>
<li>Processes local neighborhoods (typically <span class="math inline">\(3\times 3\)</span> or <span class="math inline">\(5\times 5\)</span>)</li>
<li>Reuses the same weights at each spatial position</li>
<li>Maintains spatial structure in its output</li>
</ul>
<p>For a concrete example, consider the MNIST digit classification task with <span class="math inline">\(28\times 28\)</span> grayscale images. Each convolutional layer applies a set of filters (e.g., <span class="math inline">\(3\times 3\)</span>) that slide across the image, computing local weighted sums. If we use 32 filters, the layer produces a <span class="math inline">\(28\times 28\times 32\)</span> output, where each spatial position contains 32 different feature measurements of its local neighborhood. This contrasts sharply with the multi-layer perceptron (MLP) approach, where the entire image is flattened into a 784-dimensional vector before processing.</p>
<p>This algorithmic structure directly implements the requirements for spatial pattern processing, creating distinct computational patterns that influence system design. Unlike MLPs, convolutional networks preserve spatial locality, allowing for efficient hierarchical feature extraction. These properties drive architectural optimizations in AI accelerators, where operations such as data reuse, tiling, and parallel filter computation are critical for performance.</p>
<p>As illustrated in <a href="#fig-cnn" class="quarto-xref">Figure&nbsp;3</a>, convolution operations involve sliding a small filter over the input image to generate a feature map. This process efficiently captures local structures while maintaining translation invariance, making it a fundamental component of modern deep learning architectures. For an interactive visual exploration of convolutional networks, the <a href="https://poloclub.github.io/cnn-explainer/">CNN Explainer</a> project provides an insightful demonstration of how these networks are constructed.</p>
<div id="fig-cnn" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-env="figure" data-fig-pos="htb">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-cnn-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="882e49426cbb658957aeca430a4622acd963a001.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-3" title="Figure&nbsp;3: Convolution Operation: Neural networks process input data through localized feature extraction using filters that slide across the image to identify patterns regardless of their position."><img src="dnn_architectures_files/mediabag/882e49426cbb658957aeca430a4622acd963a001.svg" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-cnn-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: <strong>Convolution Operation</strong>: Neural networks process input data through localized feature extraction using filters that slide across the image to identify patterns regardless of their position.
</figcaption>
</figure>
</div>
</section>
<section id="sec-dnn-architectures-computational-mapping-54c4" class="level3">
<h3 class="anchored" data-anchor-id="sec-dnn-architectures-computational-mapping-54c4">Computational Mapping</h3>
<p>The elegant spatial structure of convolution operations maps to computational patterns quite different from the dense matrix multiplication of MLPs. Let’s examine how this mapping progresses from mathematical abstraction to computational reality.</p>
<p>The first implementation, <code>conv_layer_spatial</code> (shown in <a href="#lst-conv_layer_spatial" class="quarto-xref">Listing&nbsp;3</a>), uses high-level convolution operations to express the computation concisely. This is typical in deep learning frameworks, where optimized libraries handle the underlying complexity.</p>
<div id="lst-conv_layer_spatial" class="listing quarto-float quarto-figure quarto-figure-left anchored">
<figure class="quarto-float quarto-float-lst figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-lst" id="lst-conv_layer_spatial-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Listing&nbsp;3: <strong>Convolution Operation</strong>: Neural networks process input data through hierarchical feature extraction using a simple convolution operation that combines a kernel and bias before applying an activation function.
</figcaption>
<div aria-describedby="lst-conv_layer_spatial-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="sourceCode" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> conv_layer_spatial(<span class="bu">input</span>, kernel, bias):</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>    output <span class="op">=</span> convolution(<span class="bu">input</span>, kernel) <span class="op">+</span> bias</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> activation(output)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</figure>
</div>
<p>The second implementation, conv_layer_compute (see <a href="#lst-conv_layer_compute" class="quarto-xref">Listing&nbsp;4</a>), reveals the actual computational pattern: nested loops that process each spatial position, applying the same filter weights to local regions of the input. These nested loops reveal the true nature of convolution’s computational structure.</p>
<div id="lst-conv_layer_compute" class="listing quarto-float quarto-figure quarto-figure-left anchored">
<figure class="quarto-float quarto-float-lst figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-lst" id="lst-conv_layer_compute-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Listing&nbsp;4: <strong>Nested Loops</strong>: Convolutional layers process input through multiple nested loops that handle batched images, spatial dimensions, output channels, kernel windows, and input features, revealing the detailed computational structure of convolution operations.
</figcaption>
<div aria-describedby="lst-conv_layer_compute-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="sourceCode" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> conv_layer_compute(<span class="bu">input</span>, kernel, bias):</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Loop 1: Process each image in batch</span></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> image <span class="kw">in</span> <span class="bu">range</span>(batch_size):</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Loop 2&amp;3: Move across image spatially</span></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> y <span class="kw">in</span> <span class="bu">range</span>(height):</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>      <span class="cf">for</span> x <span class="kw">in</span> <span class="bu">range</span>(width):</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>       <span class="co"># Loop 4: Compute each output feature</span></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>       <span class="cf">for</span> out_channel <span class="kw">in</span> <span class="bu">range</span>(num_output_channels):</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>         result <span class="op">=</span> bias[out_channel]</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>         <span class="co"># Loop 5&amp;6: Move across kernel window</span></span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a>         <span class="cf">for</span> ky <span class="kw">in</span> <span class="bu">range</span>(kernel_height):</span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a>           <span class="cf">for</span> kx <span class="kw">in</span> <span class="bu">range</span>(kernel_width):</span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a>             <span class="co"># Loop 7: Process each input feature</span></span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a>             <span class="cf">for</span> in_channel <span class="kw">in</span> <span class="bu">range</span>(num_input_channels):</span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a>             <span class="co"># Get input value from correct window position</span></span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a>              in_y <span class="op">=</span> y <span class="op">+</span> ky</span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a>              in_x <span class="op">=</span> x <span class="op">+</span> kx</span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a>              <span class="co"># Perform multiply-accumulate operation</span></span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a>              result <span class="op">+=</span> (</span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a>                <span class="bu">input</span>[image, in_y, in_x, in_channel]</span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a>                <span class="op">*</span> kernel[ky, kx, in_channel, out_channel]</span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a>              )</span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-28"><a href="#cb4-28" aria-hidden="true" tabindex="-1"></a>         <span class="co"># Store result for this output position</span></span>
<span id="cb4-29"><a href="#cb4-29" aria-hidden="true" tabindex="-1"></a>         output[image, y, x, out_channel] <span class="op">=</span> result</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</figure>
</div>
<p>The seven nested loops reveal different aspects of the computation:</p>
<ul>
<li>Outer loops (1-3) manage position: which image and where in the image</li>
<li>Middle loop (4) handles output features: computing different learned patterns</li>
<li>Inner loops (5-7) perform the actual convolution: sliding the kernel window</li>
</ul>
<p>Let’s take a closer look. The outer two loops (<code>for y</code> and <code>for x</code>) traverse each spatial position in the output feature map (for our MNIST example, this means moving across all <span class="math inline">\(28\times 28\)</span> positions). At each position, we compute values for each output channel (<code>for k</code> loop), which represents different learned features or patterns—our 32 different feature detectors.</p>
<p>The inner three loops implement the actual convolution operation at each position. For each output value, we process a local <span class="math inline">\(3\times 3\)</span> region of the input (the <code>dy</code> and <code>dx</code> loops) across all input channels (<code>for c</code> loop). This creates a sliding window effect, where the same <span class="math inline">\(3\times 3\)</span> filter moves across the image, performing multiply-accumulates between the filter weights and the local input values. Unlike the MLP’s global connectivity, this local processing pattern means each output value depends only on a small neighborhood of the input.</p>
<p>For our MNIST example with <span class="math inline">\(3\times 3\)</span> filters and 32 output channels, each output position requires only 9 multiply-accumulate operations per input channel, compared to the 784 operations needed in our MLP layer. However, this operation must be repeated for every spatial position <span class="math inline">\((28\times 28)\)</span> and every output channel (32).</p>
<p>While using fewer operations per output, the spatial structure creates different patterns of memory access and computation that systems must handle efficiently. These patterns fundamentally influence system design, creating both challenges and opportunities for optimization, which we’ll examine next.</p>
</section>
<section id="sec-dnn-architectures-system-implications-505c" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-dnn-architectures-system-implications-505c">System Implications</h3>
<p>When analyzing how computational patterns impact computer systems, we examine three fundamental dimensions: memory requirements, computation needs, and data movement. For CNNs, the spatial nature of processing creates distinctive patterns in each dimension that differ significantly from the dense connectivity of MLPs.</p>
<section id="sec-dnn-architectures-memory-requirements-a52f" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="sec-dnn-architectures-memory-requirements-a52f">Memory Requirements</h4>
<p>For convolutional layers, memory requirements center around two key components: filter weights and feature maps<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a>. Unlike MLPs that require storing full connection matrices, CNNs use small, reusable filters. In our MNIST example, a convolutional layer with 32 filters of size <span class="math inline">\(3\times 3\)</span> requires storing only 288 weight parameters <span class="math inline">\((3\times 3\times 32)\)</span>, in contrast to the 78,400 weights needed for our MLP’s fully-connected layer. However, the system must store feature maps for all spatial positions, creating a different memory demand—a <span class="math inline">\(28\times 28\)</span> input with 32 output channels requires storing 25,088 activation values <span class="math inline">\((28\times 28\times 32)\)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="fn3"><p><sup>3</sup>&nbsp;<strong>Feature Map</strong>: The output of one layer of a neural network, which serves as the input for the next layer.</p></div></div><p>These memory access patterns suggest opportunities for optimization through weight reuse and careful feature map management. Modern processors handle these patterns by caching filter weights, which are reused across spatial positions, while streaming through feature map data. Deep learning frameworks typically implement this through specialized memory layouts that optimize for both filter reuse and spatial locality in feature map access. CPUs and GPUs approach this differently—CPUs leverage their cache hierarchy to keep frequently used filters resident, while GPUs use specialized memory architectures designed for the spatial access patterns of image processing.</p>
</section>
<section id="sec-dnn-architectures-computation-needs-5ae3" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="sec-dnn-architectures-computation-needs-5ae3">Computation Needs</h4>
<p>The core computation in CNNs involves repeatedly applying small filters across spatial positions. Each output value requires a local multiply-accumulate operation over the filter region. For our MNIST example with <span class="math inline">\(3\times 3\)</span> filters and 32 output channels, computing one spatial position involves 288 multiply-accumulates <span class="math inline">\((3\times 3\times 32)\)</span>, and this must be repeated for all 784 spatial positions <span class="math inline">\((28\times 28)\)</span>. While each individual computation involves fewer operations than an MLP layer, the total computational load remains substantial due to spatial repetition.</p>
<p>This computational pattern presents different optimization opportunities than MLPs. The regular, repeated nature of convolution operations enables efficient hardware utilization through structured parallelism. Modern processors exploit this pattern in various ways. CPUs leverage SIMD<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a> instructions to process multiple filter positions simultaneously, while GPUs parallelize computation across spatial positions and channels. Deep learning frameworks further optimize this through specialized convolution algorithms that transform the computation to better match hardware capabilities.</p>
<div class="no-row-height column-margin column-container"><div id="fn4"><p><sup>4</sup>&nbsp;<strong>Single Instruction, Multiple Data (SIMD)</strong>: A type of parallel computing used in processors.</p></div></div></section>
<section id="sec-dnn-architectures-data-movement-80f1" class="level4">
<h4 class="anchored" data-anchor-id="sec-dnn-architectures-data-movement-80f1">Data Movement</h4>
<p>The sliding window pattern of convolutions creates a distinctive data movement profile. Unlike MLPs where each weight is used once per forward pass, CNN filter weights are reused many times as the filter slides across spatial positions. For our MNIST example, each <span class="math inline">\(3\times 3\)</span> filter weight is reused 784 times (once for each position in the <span class="math inline">\(28\times 28\)</span> feature map). However, this creates a different challenge: the system must stream input features through the computation unit while keeping filter weights stable.</p>
<p>The predictable spatial access pattern enables strategic data movement optimizations. Different architectures handle this movement pattern through specialized mechanisms. CPUs maintain frequently used filter weights in cache while streaming through input features. GPUs employ memory architectures optimized for spatial locality and provide hardware support for efficient sliding window operations. Deep learning frameworks orchestrate these movements by organizing computations to maximize filter weight reuse and minimize redundant feature map accesses.</p>
<div id="quiz-question-sec-dnn-architectures-convolutional-neural-networks-spatial-pattern-processing-660a" class="callout callout-quiz-question">
<details class="callout-quiz-question fbx-default closebutton"><summary><strong>Self-Check: Question 1.3</strong></summary><div>
<ol type="1">
<li><p>What is the primary advantage of using convolutional layers over fully connected layers in neural networks?</p>
<ol type="a">
<li>They require fewer parameters by reusing weights.</li>
<li>They process data faster by using more parameters.</li>
<li>They eliminate the need for activation functions.</li>
<li>They increase the model complexity significantly.</li>
</ol></li>
<li><p>True or False: Convolutional neural networks maintain spatial locality by connecting each output to all input pixels.</p></li>
<li><p>Explain how the spatial pattern processing of CNNs influences their memory and computation needs compared to MLPs.</p></li>
<li><p>In CNNs, the operation that involves sliding a small filter over the input image to generate a feature map is known as ____.</p></li>
<li><p>Discuss the system-level tradeoffs involved in deploying CNNs on GPUs versus CPUs.</p></li>
</ol>
<p><a href="#quiz-answer-sec-dnn-architectures-convolutional-neural-networks-spatial-pattern-processing-660a" class="question-label">See Answers →</a></p>
</div></details>
</div>
</section>
</section>
</section>
<section id="sec-dnn-architectures-recurrent-neural-networks-sequential-pattern-processing-cc67" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="sec-dnn-architectures-recurrent-neural-networks-sequential-pattern-processing-cc67">Recurrent Neural Networks: Sequential Pattern Processing</h2>
<p>While MLPs handle arbitrary relationships and CNNs process spatial patterns, many real-world problems involve sequential data where the order and relationship between elements over time matters. Text processing requires understanding how words relate to previous context, speech recognition needs to track how sounds form coherent patterns, and time-series analysis must capture how values evolve over time. These sequential relationships suggest that treating each time step independently misses crucial temporal patterns.</p>
<section id="sec-dnn-architectures-pattern-processing-needs-b61d" class="level3">
<h3 class="anchored" data-anchor-id="sec-dnn-architectures-pattern-processing-needs-b61d">Pattern Processing Needs</h3>
<p>Sequential pattern processing addresses scenarios where the meaning of current input depends on what came before it. Consider natural language processing: the meaning of a word often depends heavily on previous words in the sentence. The word “bank” means something different in “river bank” versus “bank account.” Similarly, in speech recognition, a phoneme’s interpretation often depends on surrounding sounds, and in financial forecasting, future predictions require understanding patterns in historical data.</p>
<p>The key challenge in sequential processing is maintaining and updating relevant context over time. When reading text, humans don’t start fresh with each word—we maintain a running understanding that evolves as we process new information. Similarly, when processing time-series data, patterns might span different timescales, from immediate dependencies to long-term trends. This suggests we need an architecture that can both maintain state over time and update it based on new inputs.</p>
<p>These requirements demand specific capabilities from our processing architecture. The system must maintain internal state to capture temporal context, update this state based on new inputs, and learn which historical information is relevant for current predictions. Unlike MLPs and CNNs, which process fixed-size inputs, sequential processing must handle variable-length sequences while maintaining computational efficiency. This leads us to the recurrent neural network (RNN) architecture.</p>
</section>
<section id="sec-dnn-architectures-algorithmic-structure-3a7b" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-dnn-architectures-algorithmic-structure-3a7b">Algorithmic Structure</h3>
<p>RNNs address sequential processing through a fundamentally different approach than MLPs or CNNs by introducing recurrent connections. Instead of just mapping inputs to outputs, RNNs maintain an internal state that is updated at each time step. This creates a memory mechanism that allows the network to carry information forward in time. This unique ability to model temporal dependencies was first explored by <span class="citation" data-cites="elman1990finding">Elman (<a href="#ref-elman1990finding" role="doc-biblioref">2002</a>)</span>, who demonstrated how RNNs could find structure in time-dependent data.</p>
<div class="no-row-height column-margin column-container"></div><p>The core operation in a basic RNN can be expressed mathematically as: <span class="math display">\[
\mathbf{h}_t = f(\mathbf{W}_{hh}\mathbf{h}_{t-1} + \mathbf{W}_{xh}\mathbf{x}_t + \mathbf{b}_h)
\]</span> where <span class="math inline">\(\mathbf{h}_t\)</span> corresponds to the hidden state at time <span class="math inline">\(t\)</span>, <span class="math inline">\(\mathbf{x}_t\)</span> is the input at time <span class="math inline">\(t\)</span>, <span class="math inline">\(\mathbf{W}_{hh}\)</span> contains the recurrent weights, and <span class="math inline">\(\mathbf{W}_{xh}\)</span> contains the input weights, as shown in the unfolded network structure in <a href="#fig-rnn" class="quarto-xref">Figure&nbsp;4</a>.</p>
<p>For example, in processing a sequence of words, each word might be represented as a 100-dimensional vector (<span class="math inline">\(\mathbf{x}_t\)</span>), and we might maintain a hidden state of 128 dimensions (<span class="math inline">\(\mathbf{h}_t\)</span>). At each time step, the network combines the current input with its previous state to update its understanding of the sequence. This creates a form of memory that can capture patterns across time steps.</p>
<p>This recurrent structure directly implements our requirements for sequential processing through the introduction of recurrent connections, which maintain internal state and allow the network to carry information forward in time. Instead of processing all inputs independently, RNNs process sequences of data by iteratively updating a hidden state based on the current input and the previous hidden state, as depicted in <a href="#fig-rnn" class="quarto-xref">Figure&nbsp;4</a>. This makes RNNs well-suited for tasks such as language modeling, speech recognition, and time-series forecasting.</p>
<div id="fig-rnn" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-env="figure" data-fig-pos="htb">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-rnn-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="10af5d186ea0ef0b5def0091c4f709088e9283ea.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-4" title="Figure&nbsp;4: Recurrent Neural Network Unfolding: Rnns process sequential data by maintaining a hidden state that incorporates information from previous time steps through this diagram. the unfolded structure explicitly represents the temporal dependencies modeled by the recurrent weights, enabling the network to learn patterns across variable-length sequences."><img src="dnn_architectures_files/mediabag/10af5d186ea0ef0b5def0091c4f709088e9283ea.svg" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-rnn-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4: <strong>Recurrent Neural Network Unfolding</strong>: Rnns process sequential data by maintaining a hidden state that incorporates information from previous time steps through this diagram. the unfolded structure explicitly represents the temporal dependencies modeled by the recurrent weights, enabling the network to learn patterns across variable-length sequences.
</figcaption>
</figure>
</div>
</section>
<section id="sec-dnn-architectures-computational-mapping-7112" class="level3">
<h3 class="anchored" data-anchor-id="sec-dnn-architectures-computational-mapping-7112">Computational Mapping</h3>
<p>The sequential structure of RNNs maps to computational patterns quite different from both MLPs and CNNs. Let’s examine how this mapping progresses from mathematical abstraction to computational reality.</p>
<p>As shown in <a href="#lst-rnn_layer_step" class="quarto-xref">Listing&nbsp;5</a>, the <code>rnn_layer_step</code> function demonstrates how the operation looks using high-level matrix operations found in deep learning frameworks. It handles a single time step, taking the current input <code>x_t</code> and previous hidden state <code>h_prev</code>, along with two weight matrices: <code>W_hh</code> for hidden-to-hidden connections and <code>W_xh</code> for input-to-hidden connections. Through matrix multiplication operations (<code>matmul</code>), it merges the previous state and current input to generate the next hidden state.</p>
<div id="lst-rnn_layer_step" class="listing quarto-float quarto-figure quarto-figure-left anchored">
<figure class="quarto-float quarto-float-lst figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-lst" id="lst-rnn_layer_step-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Listing&nbsp;5: <strong>RNN Layer Step</strong>: Neural networks process sequential data through transformations that integrate current inputs and past states.
</figcaption>
<div aria-describedby="lst-rnn_layer_step-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="sourceCode" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> rnn_layer_step(x_t, h_prev, W_hh, W_xh, b):</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>  <span class="co"># x_t: input at time t (batch_size × input_dim)</span></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>  <span class="co"># h_prev: previous hidden state (batch_size × hidden_dim)</span></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>  <span class="co"># W_hh: recurrent weights (hidden_dim × hidden_dim)</span></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>  <span class="co"># W_xh: input weights (input_dim × hidden_dim)</span></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>  h_t <span class="op">=</span> activation(</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>    matmul(h_prev, W_hh)</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>    <span class="op">+</span> matmul(x_t, W_xh)</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>    <span class="op">+</span> b</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> h_t</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</figure>
</div>
<p>This simplified view masks the underlying complexity of the nested loops and individual computations shown in the detailed implementation (<a href="#lst-rnn_layer_compute" class="quarto-xref">Listing&nbsp;6</a>). Its actual implementation reveals a more detailed computational reality.</p>
<div id="lst-rnn_layer_compute" class="listing quarto-float quarto-figure quarto-figure-left anchored">
<figure class="quarto-float quarto-float-lst figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-lst" id="lst-rnn_layer_compute-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Listing&nbsp;6: <strong>Recurrent Layer Computation</strong>: Computes the hidden state at each time step through sequential transformations involving previous states and current inputs.
</figcaption>
<div aria-describedby="lst-rnn_layer_compute-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="sourceCode" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> rnn_layer_compute(x_t, h_prev, W_hh, W_xh, b):</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Initialize next hidden state</span></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>    h_t <span class="op">=</span> np.zeros_like(h_prev)</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Loop 1: Process each sequence in the batch</span></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> batch <span class="kw">in</span> <span class="bu">range</span>(batch_size):</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Loop 2: Compute recurrent contribution</span></span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>        <span class="co"># (h_prev × W_hh)</span></span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(hidden_dim):</span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(hidden_dim):</span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>                h_t[batch,i] <span class="op">+=</span> h_prev[batch,j] <span class="op">*</span> W_hh[j,i]</span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Loop 3: Compute input contribution (x_t × W_xh)</span></span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(hidden_dim):</span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(input_dim):</span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a>                h_t[batch,i] <span class="op">+=</span> x_t[batch,j] <span class="op">*</span> W_xh[j,i]</span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Loop 4: Add bias and apply activation</span></span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(hidden_dim):</span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a>            h_t[batch,i] <span class="op">=</span> activation(h_t[batch,i] <span class="op">+</span> b[i])</span>
<span id="cb6-21"><a href="#cb6-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-22"><a href="#cb6-22" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> h_t</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</figure>
</div>
<p>The nested loops in <code>rnn_layer_compute</code> expose the core computational pattern of RNNs (see <a href="#lst-rnn_layer_compute" class="quarto-xref">Listing&nbsp;6</a>). Loop 1 processes each sequence in the batch independently, allowing for batch-level parallelism. Within each batch item, Loop 2 computes how the previous hidden state influences the next state through the recurrent weights <code>W_hh</code>. Loop 3 then incorporates new information from the current input through the input weights <code>W_xh</code>. Finally, Loop 4 adds biases and applies the activation function to produce the new hidden state.</p>
<p>For a sequence processing task with input dimension 100 and hidden state dimension 128, each time step requires two matrix multiplications: one <span class="math inline">\(128\times 128\)</span> for the recurrent connection and one <span class="math inline">\(100\times 128\)</span> for the input projection. While individual time steps can process in parallel across batch elements, the time steps themselves must process sequentially. This creates a unique computational pattern that systems must handle efficiently.</p>
</section>
<section id="sec-dnn-architectures-system-implications-18c3" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-dnn-architectures-system-implications-18c3">System Implications</h3>
<p>For RNNs, the sequential nature of processing creates distinctive patterns in each dimension (memory requirements, computation needs, and data movement) that differ significantly from both MLPs and CNNs.</p>
<section id="sec-dnn-architectures-memory-requirements-5966" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="sec-dnn-architectures-memory-requirements-5966">Memory Requirements</h4>
<p>RNNs require storing two sets of weights (input-to-hidden and hidden-to-hidden) along with the hidden state. For our example with input dimension 100 and hidden state dimension 128, this means storing 12,800 weights for input projection <span class="math inline">\((100\times 128)\)</span> and 16,384 weights for recurrent connections <span class="math inline">\((128\times 128)\)</span>. Unlike CNNs where weights are reused across spatial positions, RNN weights are reused across time steps. Additionally, the system must maintain the hidden state, which becomes a critical factor in memory usage and access patterns.</p>
<p>These memory access patterns create a different profile from MLPs and CNNs. Modern processors handle these patterns by keeping the weight matrices in cache<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a> while streaming through sequence elements. Deep learning frameworks optimize memory access by batching sequences together and carefully managing hidden state storage between time steps. CPUs and GPUs approach this through different strategies; CPUs leverage their cache hierarchy for weight reuse; meanwhile, GPUs use specialized memory architectures designed for maintaining state across sequential operations.</p>
<div class="no-row-height column-margin column-container"><div id="fn5"><p><sup>5</sup>&nbsp;Memory storage area where frequently accessed data can be stored for rapid access.</p></div></div></section>
<section id="sec-dnn-architectures-computation-needs-cb19" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="sec-dnn-architectures-computation-needs-cb19">Computation Needs</h4>
<p>The core computation in RNNs involves repeatedly applying weight matrices across time steps. For each time step, we perform two matrix multiplications: one with the input weights and one with the recurrent weights. In our example, processing a single time step requires 12,800 multiply-accumulates for the input projection <span class="math inline">\((100\times 128)\)</span> and 16,384 multiply-accumulates for the recurrent connection <span class="math inline">\((128\times 128)\)</span>.</p>
<p>This computational pattern differs from both MLPs and CNNs in a key way: while we can parallelize across batch elements, we cannot parallelize across time steps due to the sequential dependency. Each time step must wait for the previous step’s hidden state before it can begin computation. This creates a tension between the inherent sequential nature of the algorithm and the desire for parallel execution in modern hardware.</p>
<p>Modern processors handle these patterns through different approaches. CPUs pipeline operations within each time step while maintaining the sequential order across steps. GPUs batch multiple sequences together to maintain high throughput despite sequential dependencies. Deep learning frameworks optimize this further by techniques like sequence packing<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a> and unrolling computations across multiple time steps when possible.</p>
<div class="no-row-height column-margin column-container"><div id="fn6"><p><sup>6</sup>&nbsp;<strong>Sequence Packing</strong>: A technique in deep learning where sequences of different lengths are packed together to optimize memory and processing efficiency.</p></div></div></section>
<section id="sec-dnn-architectures-data-movement-c045" class="level4">
<h4 class="anchored" data-anchor-id="sec-dnn-architectures-data-movement-c045">Data Movement</h4>
<p>The sequential processing in RNNs creates a distinctive data movement pattern that differs from both MLPs and CNNs. While MLPs need each weight only once per forward pass and CNNs reuse weights across spatial positions, RNNs reuse their weights across time steps while requiring careful management of the hidden state data flow.</p>
<p>For our example with a 128-dimensional hidden state, each time step must: load the previous hidden state (128 values), access both weight matrices (29,184 total weights from both input and recurrent connections), and store the new hidden state (128 values). This pattern repeats for every element in the sequence. Unlike CNNs where we can predict and prefetch data based on spatial patterns, RNN data movement is driven by temporal dependencies.</p>
<p>Different architectures handle this sequential data movement through specialized mechanisms. CPUs maintain weight matrices in cache while streaming through sequence elements and managing hidden state updates. GPUs employ memory architectures optimized for maintaining state information across sequential operations while processing multiple sequences in parallel. Deep learning frameworks orchestrate these movements by managing data transfers between time steps and optimizing batch operations.</p>
<div id="quiz-question-sec-dnn-architectures-recurrent-neural-networks-sequential-pattern-processing-cc67" class="callout callout-quiz-question">
<details class="callout-quiz-question fbx-default closebutton"><summary><strong>Self-Check: Question 1.4</strong></summary><div>
<ol type="1">
<li><p>What is the primary challenge that RNNs address in sequential data processing?</p>
<ol type="a">
<li>Handling fixed-size input sequences</li>
<li>Maintaining and updating relevant context over time</li>
<li>Reducing computational complexity</li>
<li>Improving spatial pattern recognition</li>
</ol></li>
<li><p>Explain how the recurrent connections in RNNs contribute to their ability to process sequential data.</p></li>
<li><p>In RNNs, the operation that updates the hidden state based on the previous state and current input is known as ____.</p></li>
<li><p>True or False: RNNs can parallelize computations across time steps just like they do across batch elements.</p></li>
<li><p>Discuss the system-level tradeoffs involved in deploying RNNs on CPUs versus GPUs.</p></li>
</ol>
<p><a href="#quiz-answer-sec-dnn-architectures-recurrent-neural-networks-sequential-pattern-processing-cc67" class="question-label">See Answers →</a></p>
</div></details>
</div>
</section>
</section>
</section>
<section id="sec-dnn-architectures-attention-mechanisms-dynamic-pattern-processing-ea2d" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="sec-dnn-architectures-attention-mechanisms-dynamic-pattern-processing-ea2d">Attention Mechanisms: Dynamic Pattern Processing</h2>
<p>While previous architectures process patterns in fixed ways, such as MLPs with dense connectivity, CNNs with spatial operations, and RNNs with sequential updates, many tasks require dynamic relationships between elements that change based on content. Language understanding, for instance, needs to capture relationships between words that depend on meaning rather than just position. Graph analysis requires understanding connections that vary by node. These dynamic relationships suggest we need an architecture that can learn and adapt its processing patterns based on the data itself.</p>
<section id="sec-dnn-architectures-pattern-processing-needs-1d60" class="level3">
<h3 class="anchored" data-anchor-id="sec-dnn-architectures-pattern-processing-needs-1d60">Pattern Processing Needs</h3>
<p>Dynamic pattern processing addresses scenarios where relationships between elements aren’t fixed by architecture but instead emerge from content. Consider language translation: when translating “the bank by the river,” understanding “bank” requires attending to “river,” but in “the bank approved the loan,” the important relationship is with “approved” and “loan.” Unlike RNNs that process information sequentially or CNNs that use fixed spatial patterns, we need an architecture that can dynamically determine which relationships matter.</p>
<p>This requirement for dynamic processing appears across many domains. In protein structure prediction, interactions between amino acids depend on their chemical properties and spatial arrangements. In graph analysis, node relationships vary based on graph structure and node features. In document analysis, connections between different sections depend on semantic content rather than just proximity.</p>
<p>These scenarios demand specific capabilities from our processing architecture. The system must compute relationships between all pairs of elements, weigh these relationships based on content, and use these weights to selectively combine information. Unlike previous architectures with fixed connectivity patterns, dynamic processing requires the flexibility to modify its computation graph based on the input itself. This leads us to the Transformer architecture, which implements these capabilities through attention mechanisms. <a href="#fig-transformer-attention-visualized" class="quarto-xref">Figure&nbsp;5</a> shows the relationships learned for an attention head between subwords in a sentence.</p>
<div id="fig-transformer-attention-visualized" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-env="figure" data-fig-pos="htb">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-transformer-attention-visualized-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="bf7890383c7114f36e56cdc538c944d644b84d20.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-5" title="Figure&nbsp;5: Attention Weights: Transformer attention mechanisms dynamically assess relationships between subwords, assigning higher weights to more relevant connections within a sequence and enabling the model to focus on key information. These learned weights, visualized as connection strengths, reveal how the model attends to different parts of the input when processing language."><img src="dnn_architectures_files/mediabag/bf7890383c7114f36e56cdc538c944d644b84d20.svg" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-transformer-attention-visualized-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5: <strong>Attention Weights</strong>: Transformer attention mechanisms dynamically assess relationships between subwords, assigning higher weights to more relevant connections within a sequence and enabling the model to focus on key information. These learned weights, visualized as connection strengths, reveal how the model attends to different parts of the input when processing language.
</figcaption>
</figure>
</div>
</section>
<section id="sec-dnn-architectures-basic-attention-mechanism-916f" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-dnn-architectures-basic-attention-mechanism-916f">Basic Attention Mechanism</h3>
<section id="sec-dnn-architectures-algorithmic-structure-886d" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="sec-dnn-architectures-algorithmic-structure-886d">Algorithmic Structure</h4>
<p>Attention mechanisms form the foundation of dynamic pattern processing by computing weighted connections between elements based on their content <span class="citation" data-cites="bahdanau2014neural">(<a href="#ref-bahdanau2014neural" role="doc-biblioref">Bahdanau, Cho, and Bengio 2014</a>)</span>. This approach allows for the processing of relationships that aren’t fixed by architecture but instead emerge from the data itself. At the core of an attention mechanism is a fundamental operation that can be expressed mathematically as: <span class="math display">\[
\text{Attention}(\mathbf{Q}, \mathbf{K}, \mathbf{V}) = \text{softmax}
\left(\frac{\mathbf{Q}\mathbf{K}^T}{\sqrt{d_k}}\right)\mathbf{V}
\]</span></p>
<div class="no-row-height column-margin column-container"></div><p>In this equation, <span class="math inline">\(\mathbf{Q}\)</span> (queries), <span class="math inline">\(\mathbf{K}\)</span> (keys), and <span class="math inline">\(\mathbf{V}\)</span> (values) represent learned projections of the input. For a sequence of length <span class="math inline">\(N\)</span> with dimension <span class="math inline">\(d\)</span>, this operation creates an <span class="math inline">\(N\times N\)</span> attention matrix, determining how each position should attend to all others.</p>
<p>The attention operation involves several key steps. First, it computes query, key, and value projections for each position in the sequence. Next, it generates an <span class="math inline">\(N\times N\)</span> attention matrix through query-key interactions. These steps are illustrated in <a href="#fig-attention" class="quarto-xref">Figure&nbsp;6</a>. Finally, it uses these attention weights to combine value vectors, producing the output.</p>
<div id="fig-attention" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-env="figure" data-fig-pos="htb">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-attention-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="0f88d09d466d8e0373bbe53f1318aa4872ac0a9f.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-6" title="Figure&nbsp;6: Query-Key-Value Interaction: Transformer attention mechanisms dynamically weigh input sequence elements by computing relationships between queries, keys, and values, enabling the model to focus on relevant information. these projections facilitate the creation of an attention matrix that determines the contribution of each value vector to the final output, effectively capturing contextual dependencies within the sequence. Source: transformer explainer."><img src="dnn_architectures_files/mediabag/0f88d09d466d8e0373bbe53f1318aa4872ac0a9f.svg" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-attention-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6: <strong>Query-Key-Value Interaction</strong>: Transformer attention mechanisms dynamically weigh input sequence elements by computing relationships between queries, keys, and values, enabling the model to focus on relevant information. these projections facilitate the creation of an attention matrix that determines the contribution of each value vector to the final output, effectively capturing contextual dependencies within the sequence. Source: <a href="HTTPS://poloclub.GitHub.io/transformer-explainer/">transformer explainer</a>.
</figcaption>
</figure>
</div>
<p>The key is that, unlike the fixed weight matrices found in previous architectures, as shown in <a href="#fig-attention-weightcalc" class="quarto-xref">Figure&nbsp;7</a>, these attention weights are computed dynamically for each input. This allows the model to adapt its processing based on the dynamic content at hand.</p>
<div id="fig-attention-weightcalc" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-env="figure" data-fig-pos="htb">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-attention-weightcalc-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="d93791656d5c30974fc66075dc4eac6ec410a489.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-7" title="Figure&nbsp;7: Dynamic Attention Weights: Transformer models calculate attention weights dynamically based on the relationships between query, key, and value vectors, allowing the model to focus on relevant parts of the input sequence for each processing step. this contrasts with fixed-weight architectures and enables adaptive pattern processing crucial for handling variable-length inputs and complex dependencies. Source: transformer explainer."><img src="dnn_architectures_files/mediabag/d93791656d5c30974fc66075dc4eac6ec410a489.svg" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-attention-weightcalc-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;7: <strong>Dynamic Attention Weights</strong>: Transformer models calculate attention weights dynamically based on the relationships between query, key, and value vectors, allowing the model to focus on relevant parts of the input sequence for each processing step. this contrasts with fixed-weight architectures and enables adaptive pattern processing crucial for handling variable-length inputs and complex dependencies. Source: <a href="HTTPS://poloclub.GitHub.io/transformer-explainer/">transformer explainer</a>.
</figcaption>
</figure>
</div>
</section>
<section id="sec-dnn-architectures-computational-mapping-b1e6" class="level4">
<h4 class="anchored" data-anchor-id="sec-dnn-architectures-computational-mapping-b1e6">Computational Mapping</h4>
<p>The dynamic structure of attention operations maps to computational patterns that differ significantly from those of previous architectures. To understand this mapping, let’s examine how it progresses from mathematical abstraction to computational reality (see <a href="#lst-attention_layer_compute" class="quarto-xref">Listing&nbsp;7</a>).</p>
<div id="lst-attention_layer_compute" class="listing quarto-float quarto-figure quarto-figure-left anchored">
<figure class="quarto-float quarto-float-lst figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-lst" id="lst-attention_layer_compute-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Listing&nbsp;7: <strong>Attention Mechanism</strong>: Transformer models compute attention through query-key-value interactions, enabling dynamic focus across input sequences for improved language understanding.
</figcaption>
<div aria-describedby="lst-attention_layer_compute-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="sourceCode" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> attention_layer_matrix(Q, K, V):</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Q, K, V: (batch_size × seq_len × d_model)</span></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>    scores <span class="op">=</span> matmul(Q, K.transpose(<span class="op">-</span><span class="dv">2</span>, <span class="op">-</span><span class="dv">1</span>)) <span class="op">/</span> <span class="op">\</span></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>             sqrt(d_k)           <span class="co"># Compute attention scores</span></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>    weights <span class="op">=</span> softmax(scores)    <span class="co"># Normalize scores</span></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>    output <span class="op">=</span> matmul(weights, V)  <span class="co"># Combine values</span></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> output</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Core computational pattern</span></span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> attention_layer_compute(Q, K, V):</span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Initialize outputs</span></span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a>    scores <span class="op">=</span> np.zeros((batch_size, seq_len, seq_len))</span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a>    outputs <span class="op">=</span> np.zeros_like(V)</span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Loop 1: Process each sequence in batch</span></span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> b <span class="kw">in</span> <span class="bu">range</span>(batch_size):</span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Loop 2: Compute attention for each query position</span></span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(seq_len):</span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Loop 3: Compare with each key position</span></span>
<span id="cb7-20"><a href="#cb7-20" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(seq_len):</span>
<span id="cb7-21"><a href="#cb7-21" aria-hidden="true" tabindex="-1"></a>                <span class="co"># Compute attention score</span></span>
<span id="cb7-22"><a href="#cb7-22" aria-hidden="true" tabindex="-1"></a>                <span class="cf">for</span> d <span class="kw">in</span> <span class="bu">range</span>(d_model):</span>
<span id="cb7-23"><a href="#cb7-23" aria-hidden="true" tabindex="-1"></a>                    scores[b,i,j] <span class="op">+=</span> Q[b,i,d] <span class="op">*</span> K[b,j,d]</span>
<span id="cb7-24"><a href="#cb7-24" aria-hidden="true" tabindex="-1"></a>                scores[b,i,j] <span class="op">/=</span> sqrt(d_k)</span>
<span id="cb7-25"><a href="#cb7-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-26"><a href="#cb7-26" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Apply softmax to scores</span></span>
<span id="cb7-27"><a href="#cb7-27" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(seq_len):</span>
<span id="cb7-28"><a href="#cb7-28" aria-hidden="true" tabindex="-1"></a>            scores[b,i] <span class="op">=</span> softmax(scores[b,i])</span>
<span id="cb7-29"><a href="#cb7-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-30"><a href="#cb7-30" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Loop 4: Combine values using attention weights</span></span>
<span id="cb7-31"><a href="#cb7-31" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(seq_len):</span>
<span id="cb7-32"><a href="#cb7-32" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(seq_len):</span>
<span id="cb7-33"><a href="#cb7-33" aria-hidden="true" tabindex="-1"></a>                <span class="cf">for</span> d <span class="kw">in</span> <span class="bu">range</span>(d_model):</span>
<span id="cb7-34"><a href="#cb7-34" aria-hidden="true" tabindex="-1"></a>                    outputs[b, i, d] <span class="op">+=</span> (</span>
<span id="cb7-35"><a href="#cb7-35" aria-hidden="true" tabindex="-1"></a>                       scores[b, i, j]</span>
<span id="cb7-36"><a href="#cb7-36" aria-hidden="true" tabindex="-1"></a>                       <span class="op">*</span> V[b, j, d]</span>
<span id="cb7-37"><a href="#cb7-37" aria-hidden="true" tabindex="-1"></a>                    )</span>
<span id="cb7-38"><a href="#cb7-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-39"><a href="#cb7-39" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> outputs</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</figure>
</div>
<p>The nested loops in <code>attention_layer_compute</code> reveal the true nature of attention’s computational pattern (see <a href="#lst-attention_layer_compute" class="quarto-xref">Listing&nbsp;7</a>). The first loop processes each sequence in the batch independently. The second and third loops compute attention scores between all pairs of positions, creating a quadratic computation pattern with respect to sequence length. The fourth loop uses these attention weights to combine values from all positions, producing the final output.</p>
</section>
<section id="sec-dnn-architectures-system-implications-c457" class="level4">
<h4 class="anchored" data-anchor-id="sec-dnn-architectures-system-implications-c457">System Implications</h4>
<p>The attention mechanism creates distinctive patterns in memory requirements, computation needs, and data movement that set it apart from previous architectures.</p>
<section id="sec-dnn-architectures-memory-requirements-cdef" class="level5">
<h5 class="anchored" data-anchor-id="sec-dnn-architectures-memory-requirements-cdef">Memory Requirements</h5>
<p>In terms of memory requirements, attention mechanisms necessitate storage for attention weights, key-query-value projections, and intermediate feature representations. For a sequence length <span class="math inline">\(N\)</span> and dimension d, each attention layer must store an <span class="math inline">\(N\times N\)</span> attention weight matrix for each sequence in the batch, three sets of projection matrices for queries, keys, and values (each sized <span class="math inline">\(d\times d\)</span>), and input and output feature maps of size <span class="math inline">\(N\times d\)</span>. The dynamic generation of attention weights for every input creates a memory access pattern where intermediate attention weights become a significant factor in memory usage.</p>
</section>
<section id="sec-dnn-architectures-computation-needs-3c98" class="level5">
<h5 class="anchored" data-anchor-id="sec-dnn-architectures-computation-needs-3c98">Computation Needs</h5>
<p>Computation needs in attention mechanisms center around two main phases: generating attention weights and applying them to values. For each attention layer, the system performs substantial multiply-accumulate operations across multiple computational stages. The query-key interactions alone require <span class="math inline">\(N\times N\times d\)</span> multiply-accumulates, with an equal number needed for applying attention weights to values. Additional computations are required for the projection matrices and softmax operations. This computational pattern differs from previous architectures due to its quadratic scaling with sequence length and the need to perform fresh computations for each input.</p>
</section>
<section id="sec-dnn-architectures-data-movement-f255" class="level5">
<h5 class="anchored" data-anchor-id="sec-dnn-architectures-data-movement-f255">Data Movement</h5>
<p>Data movement in attention mechanisms presents unique challenges. Each attention operation involves projecting and moving query, key, and value vectors for each position, storing and accessing the full attention weight matrix, and coordinating the movement of value vectors during the weighted combination phase. This creates a data movement pattern where intermediate attention weights become a major factor in system bandwidth requirements. Unlike the more predictable access patterns of CNNs or the sequential access of RNNs, attention operations require frequent movement of dynamically computed weights across the memory hierarchy.</p>
<p>These distinctive characteristics of attention mechanisms in terms of memory, computation, and data movement have significant implications for system design and optimization, setting the stage for the development of more advanced architectures like Transformers.</p>
</section>
</section>
</section>
<section id="sec-dnn-architectures-transformers-selfattention-e89e" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-dnn-architectures-transformers-selfattention-e89e">Transformers and Self-Attention</h3>
<p>Transformers, first introduced by <span class="citation" data-cites="vaswani2017attention">Chen et al. (<a href="#ref-vaswani2017attention" role="doc-biblioref">2018</a>)</span>, represent a significant evolution in the application of attention mechanisms, introducing the concept of self-attention to create a powerful architecture for dynamic pattern processing. While the basic attention mechanism allows for content-based weighting of information from a source sequence, Transformers extend this idea by applying attention within a single sequence, enabling each element to attend to all other elements including itself.</p>
<div class="no-row-height column-margin column-container"><div id="ref-vaswani2017attention" class="csl-entry" role="listitem">
Chen, Mia Xu, Orhan Firat, Ankur Bapna, Melvin Johnson, Wolfgang Macherey, George Foster, Llion Jones, et al. 2018. <span>“The Best of Both Worlds: Combining Recent Advances in Neural Machine Translation.”</span> In <em>Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, 30:5998–6008. Association for Computational Linguistics. <a href="https://doi.org/10.18653/v1/p18-1008">https://doi.org/10.18653/v1/p18-1008</a>.
</div></div><section id="sec-dnn-architectures-algorithmic-structure-4d25" class="level4">
<h4 class="anchored" data-anchor-id="sec-dnn-architectures-algorithmic-structure-4d25">Algorithmic Structure</h4>
<p>The key innovation in Transformers lies in their use of self-attention layers. In a self-attention layer, the queries, keys, and values are all derived from the same input sequence. This allows the model to weigh the importance of different positions within the same sequence when encoding each position. For instance, in processing the sentence “The animal didn’t cross the street because it was too wide,” self-attention allows the model to link “it” with “street,” capturing long-range dependencies that are challenging for traditional sequential models.</p>
<p>Transformers typically employ multi-head attention, which involves multiple sets of query/key/value projections. Each set, or “head,” can focus on different aspects of the input, allowing the model to jointly attend to information from different representation subspaces. This multi-head structure provides the model with a richer representational capability, enabling it to capture various types of relationships within the data simultaneously.</p>
<p>The self-attention mechanism in Transformers can be expressed mathematically in a form similar to the basic attention mechanism: <span class="math display">\[
\text{SelfAttention}(\mathbf{X}) = \text{softmax}
\left(\frac{\mathbf{XW_Q}(\mathbf{XW_K})^T}{\sqrt{d_k}}\right)\mathbf{XW_V}
\]</span></p>
<p>Here, <span class="math inline">\(\mathbf{X}\)</span> is the input sequence, and <span class="math inline">\(\mathbf{W_Q}\)</span>, <span class="math inline">\(\mathbf{W_K}\)</span>, and <span class="math inline">\(\mathbf{W_V}\)</span> are learned weight matrices for queries, keys, and values respectively. This formulation highlights how self-attention derives all its components from the same input, creating a dynamic, content-dependent processing pattern.</p>
<p>The Transformer architecture leverages this self-attention mechanism within a broader structure that typically includes feed-forward layers, layer normalization, and residual connections (see <a href="#fig-transformer" class="quarto-xref">Figure&nbsp;8</a>). This combination allows Transformers to process input sequences in parallel, capturing complex dependencies without the need for sequential computation. As a result, Transformers have demonstrated remarkable effectiveness across a wide range of tasks, from natural language processing to computer vision, revolutionizing the landscape of deep learning architectures.</p>
<div id="fig-transformer" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-env="figure" data-fig-pos="htb">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-transformer-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="6bbdb81ec20257b1c908d817f60bbcd804d75efb.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-8" title="Figure&nbsp;8: Attention Head: Neural networks compute attention through query-key-value interactions, enabling dynamic focus across subwords for improved sentence understanding. Source: Attention Is All You Need."><img src="dnn_architectures_files/mediabag/6bbdb81ec20257b1c908d817f60bbcd804d75efb.svg" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-transformer-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;8: <strong>Attention Head</strong>: Neural networks compute attention through query-key-value interactions, enabling dynamic focus across subwords for improved sentence understanding. Source: Attention Is All You Need.
</figcaption>
</figure>
</div>
</section>
<section id="sec-dnn-architectures-computational-mapping-56b8" class="level4">
<h4 class="anchored" data-anchor-id="sec-dnn-architectures-computational-mapping-56b8">Computational Mapping</h4>
<p>While Transformer self-attention builds upon the basic attention mechanism, it introduces distinct computational patterns that set it apart. To understand these patterns, we must examine the typical implementation of self-attention in Transformers (see <a href="#lst-self_attention_layer" class="quarto-xref">Listing&nbsp;8</a>):</p>
<div id="lst-self_attention_layer" class="listing quarto-float quarto-figure quarto-figure-left anchored">
<figure class="quarto-float quarto-float-lst figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-lst" id="lst-self_attention_layer-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Listing&nbsp;8: <strong>Self-Attention Mechanism</strong>: Transformer models compute attention through query-key-value interactions, enabling dynamic focus across input sequences for improved language understanding.
</figcaption>
<div aria-describedby="lst-self_attention_layer-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="sourceCode" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> self_attention_layer(X, W_Q, W_K, W_V, d_k):</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># X: input tensor (batch_size × seq_len × d_model)</span></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>    <span class="co"># W_Q, W_K, W_V: weight matrices (d_model × d_k)</span></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>    Q <span class="op">=</span> matmul(X, W_Q)</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>    K <span class="op">=</span> matmul(X, W_K)</span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>    V <span class="op">=</span> matmul(X, W_V)</span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>    scores <span class="op">=</span> matmul(Q, K.transpose(<span class="op">-</span><span class="dv">2</span>, <span class="op">-</span><span class="dv">1</span>)) <span class="op">/</span> sqrt(d_k)</span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a>    attention_weights <span class="op">=</span> softmax(scores, dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a>    output <span class="op">=</span> matmul(attention_weights, V)</span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> output</span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> multi_head_attention(</span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a>    X, W_Q, W_K, W_V, W_O, num_heads, d_k</span>
<span id="cb8-17"><a href="#cb8-17" aria-hidden="true" tabindex="-1"></a>):</span>
<span id="cb8-18"><a href="#cb8-18" aria-hidden="true" tabindex="-1"></a>    outputs <span class="op">=</span> []</span>
<span id="cb8-19"><a href="#cb8-19" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(num_heads):</span>
<span id="cb8-20"><a href="#cb8-20" aria-hidden="true" tabindex="-1"></a>        head_output <span class="op">=</span> self_attention_layer(</span>
<span id="cb8-21"><a href="#cb8-21" aria-hidden="true" tabindex="-1"></a>            X, W_Q[i], W_K[i], W_V[i], d_k</span>
<span id="cb8-22"><a href="#cb8-22" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb8-23"><a href="#cb8-23" aria-hidden="true" tabindex="-1"></a>        outputs.append(head_output)</span>
<span id="cb8-24"><a href="#cb8-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-25"><a href="#cb8-25" aria-hidden="true" tabindex="-1"></a>    concat_output <span class="op">=</span> torch.cat(outputs, dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb8-26"><a href="#cb8-26" aria-hidden="true" tabindex="-1"></a>    final_output <span class="op">=</span> matmul(concat_output, W_O)</span>
<span id="cb8-27"><a href="#cb8-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-28"><a href="#cb8-28" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> final_output</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</figure>
</div>
</section>
<section id="sec-dnn-architectures-system-implications-6010" class="level4">
<h4 class="anchored" data-anchor-id="sec-dnn-architectures-system-implications-6010">System Implications</h4>
<p>This implementation reveals several key computational characteristics of Transformer self-attention. First, self-attention enables parallel processing across all positions in the sequence. This is evident in the matrix multiplications that compute <code>Q</code>, <code>K</code>, and <code>V</code> simultaneously for all positions. Unlike recurrent architectures that process inputs sequentially, this parallel nature allows for more efficient computation, especially on modern hardware designed for parallel operations.</p>
<p>Second, the attention score computation results in a matrix of size <code>(seq_len × seq_len)</code>, leading to quadratic complexity with respect to sequence length. This quadratic relationship becomes a significant computational bottleneck when processing long sequences, a challenge that has spurred research into more efficient attention mechanisms.</p>
<p>Third, the multi-head attention mechanism effectively runs multiple self-attention operations in parallel, each with its own set of learned projections. While this increases the computational load linearly with the number of heads, it allows the model to capture different types of relationships within the same input, enhancing the model’s representational power.</p>
<p>Fourth, the core computations in self-attention are dominated by large matrix multiplications. For a sequence of length <span class="math inline">\(N\)</span> and embedding dimension <span class="math inline">\(d\)</span>, the main operations involve matrices of sizes <span class="math inline">\((N\times d)\)</span>, <span class="math inline">\((d\times d)\)</span>, and <span class="math inline">\((N\times N)\)</span>. These intensive matrix operations are well-suited for acceleration on specialized hardware like GPUs, but they also contribute significantly to the overall computational cost of the model.</p>
<p>Finally, self-attention generates memory-intensive intermediate results. The attention weights matrix <span class="math inline">\((N\times N)\)</span> and the intermediate results for each attention head create substantial memory requirements, especially for long sequences. This can pose challenges for deployment on memory-constrained devices and necessitates careful memory management in implementations.</p>
<p>These computational patterns create a unique profile for Transformer self-attention, distinct from previous architectures. The parallel nature of the computations makes Transformers well-suited for modern parallel processing hardware, but the quadratic complexity with sequence length poses challenges for processing long sequences. As a result, much research has focused on developing optimization techniques, such as sparse attention patterns or low-rank approximations, to address these challenges. Each of these optimizations presents its own trade-offs between computational efficiency and model expressiveness, a balance that must be carefully considered in practical applications.</p>
<div id="quiz-question-sec-dnn-architectures-attention-mechanisms-dynamic-pattern-processing-ea2d" class="callout callout-quiz-question">
<details class="callout-quiz-question fbx-default closebutton"><summary><strong>Self-Check: Question 1.5</strong></summary><div>
<ol type="1">
<li><p>What is the primary computational challenge associated with attention mechanisms in terms of sequence length?</p>
<ol type="a">
<li>Linear scaling with sequence length</li>
<li>Quadratic scaling with sequence length</li>
<li>Constant scaling with sequence length</li>
<li>Exponential scaling with sequence length</li>
</ol></li>
<li><p>Explain why attention mechanisms require dynamic computation of weights and how this differs from fixed connectivity patterns in previous architectures.</p></li>
<li><p>In Transformer architectures, the mechanism that allows each element to attend to all other elements within the same sequence is known as ____.</p></li>
<li><p>True or False: The parallel nature of Transformer computations makes them less suited for modern parallel processing hardware.</p></li>
<li><p>Discuss the system-level tradeoffs involved in deploying Transformer models on memory-constrained devices.</p></li>
</ol>
<p><a href="#quiz-answer-sec-dnn-architectures-attention-mechanisms-dynamic-pattern-processing-ea2d" class="question-label">See Answers →</a></p>
</div></details>
</div>
</section>
</section>
</section>
<section id="sec-dnn-architectures-architectural-building-blocks-e63a" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="sec-dnn-architectures-architectural-building-blocks-e63a">Architectural Building Blocks</h2>
<p>Deep learning architectures, while we presented them as distinct approaches in the previous sections, are better understood as compositions of fundamental building blocks that evolved over time. Much like how complex LEGO structures are built from basic bricks, modern neural networks combine and iterate on core computational patterns that emerged through decades of research <span class="citation" data-cites="lecun2015deep">(<a href="#ref-lecun2015deep" role="doc-biblioref">Yann LeCun, Bengio, and Hinton 2015</a>)</span>. Each architectural innovation introduced new building blocks while finding novel ways to use existing ones.</p>
<div class="no-row-height column-margin column-container"><div id="ref-lecun2015deep" class="csl-entry" role="listitem">
LeCun, Yann, Yoshua Bengio, and Geoffrey Hinton. 2015. <span>“Deep Learning.”</span> <em>Nature</em> 521 (7553): 436–44. <a href="https://doi.org/10.1038/nature14539">https://doi.org/10.1038/nature14539</a>.
</div><div id="ref-rosenblatt1958perceptron" class="csl-entry" role="listitem">
Rosenblatt, F. 1958. <span>“The Perceptron: A Probabilistic Model for Information Storage and Organization in the Brain.”</span> <em>Psychological Review</em> 65 (6): 386–408. <a href="https://doi.org/10.1037/h0042519">https://doi.org/10.1037/h0042519</a>.
</div><div id="ref-rumelhart1986learning" class="csl-entry" role="listitem">
Rumelhart, David E., Geoffrey E. Hinton, and Ronald J. Williams. 1986. <span>“Learning Representations by Back-Propagating Errors.”</span> <em>Nature</em> 323 (6088): 533–36. <a href="https://doi.org/10.1038/323533a0">https://doi.org/10.1038/323533a0</a>.
</div></div><p>These building blocks and their evolution provide insight into modern architectures. What began with the simple perceptron <span class="citation" data-cites="rosenblatt1958perceptron">(<a href="#ref-rosenblatt1958perceptron" role="doc-biblioref">Rosenblatt 1958</a>)</span> evolved into multi-layer networks <span class="citation" data-cites="rumelhart1986learning">(<a href="#ref-rumelhart1986learning" role="doc-biblioref">Rumelhart, Hinton, and Williams 1986</a>)</span>, which then spawned specialized patterns for spatial and sequential processing. Each advancement maintained useful elements from its predecessors while introducing new computational primitives. Today’s sophisticated architectures, like Transformers, can be seen as carefully engineered combinations of these fundamental building blocks.</p>
<p>This progression reveals not just the evolution of neural networks, but also the discovery and refinement of core computational patterns that remain relevant. As we have seen through our exploration of different neural network architectures, deep learning has evolved significantly, with each new architecture bringing its own set of computational demands and system-level challenges.</p>
<p><a href="#tbl-dl-evolution" class="quarto-xref">Table&nbsp;1</a> summarizes this evolution, highlighting the key primitives and system focus for each era of deep learning development. This table encapsulates the major shifts in deep learning architecture design and the corresponding changes in system-level considerations. From the early focus on dense matrix operations optimized for CPUs, we see a progression through convolutions leveraging GPU acceleration, to sequential operations necessitating sophisticated memory hierarchies, and finally to the current era of attention mechanisms requiring flexible accelerators and high-bandwidth memory.</p>
<div id="tbl-dl-evolution" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-dl-evolution-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;1: <strong>Deep Learning Evolution</strong>: Neural network architectures have progressed from simple, fully connected layers to complex models leveraging specialized hardware and addressing sequential data dependencies. This table maps architectural eras to key computational primitives and corresponding system-level optimizations, revealing a historical trend toward increased parallelism and memory bandwidth requirements.
</figcaption>
<div aria-describedby="tbl-dl-evolution-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<colgroup>
<col style="width: 16%">
<col style="width: 19%">
<col style="width: 23%">
<col style="width: 38%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Era</th>
<th style="text-align: left;">Dominant Architecture</th>
<th style="text-align: left;">Key Primitives</th>
<th style="text-align: left;">System Focus</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Early NN</td>
<td style="text-align: left;">MLP</td>
<td style="text-align: left;">Dense Matrix Ops</td>
<td style="text-align: left;">CPU optimization</td>
</tr>
<tr class="even">
<td style="text-align: left;">CNN Revolution</td>
<td style="text-align: left;">CNN</td>
<td style="text-align: left;">Convolutions</td>
<td style="text-align: left;">GPU acceleration</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Sequence Modeling</td>
<td style="text-align: left;">RNN</td>
<td style="text-align: left;">Sequential Ops</td>
<td style="text-align: left;">Memory hierarchies</td>
</tr>
<tr class="even">
<td style="text-align: left;">Attention Era</td>
<td style="text-align: left;">Transformer</td>
<td style="text-align: left;">Attention, Dynamic Compute</td>
<td style="text-align: left;">Flexible accelerators, High-bandwidth memory</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p>As we dive deeper into each of these building blocks, we see how these primitives evolved and combined to create increasingly powerful and complex neural network architectures.</p>
<section id="sec-dnn-architectures-perceptron-multilayer-networks-c64f" class="level3">
<h3 class="anchored" data-anchor-id="sec-dnn-architectures-perceptron-multilayer-networks-c64f">From Perceptron to Multi-Layer Networks</h3>
<p>While we examined MLPs earlier as a mechanism for dense pattern processing, here we focus on how they established fundamental building blocks that appear throughout deep learning. The evolution from perceptron to MLP introduced several key concepts: the power of layer stacking, the importance of non-linear transformations, and the basic feedforward computation pattern.</p>
<p>The introduction of hidden layers between input and output created a template for feature transformation that appears in virtually every modern architecture. Even in sophisticated networks like Transformers, we find MLP-style feedforward layers performing feature processing. The concept of transforming data through successive non-linear layers has become a fundamental paradigm that transcends the specific architecture types.</p>
<p>Perhaps most importantly, the development of MLPs established the backpropagation algorithm, which to this day remains the cornerstone of neural network training. This key contribution has enabled the training of deep architectures and influenced how later architectures would be designed to maintain gradient flow.</p>
<p>These building blocks, layered feature transformation, non-linear activation, and gradient-based learning, set the foundation for more specialized architectures. Subsequent innovations often focused on structuring these basic components in new ways rather than replacing them entirely.</p>
</section>
<section id="sec-dnn-architectures-dense-spatial-processing-3a56" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-dnn-architectures-dense-spatial-processing-3a56">From Dense to Spatial Processing</h3>
<p>The development of CNNs marked a significant architectural innovation, specifically the realization that we could specialize the dense connectivity of MLPs for spatial patterns. While retaining the core concept of layer-wise processing, CNNs introduced several fundamental building blocks that would influence all future architectures.</p>
<p>The first key innovation was the concept of parameter sharing. Unlike MLPs where each connection had its own weight, CNNs showed how the same parameters could be reused across different parts of the input. This not only made the networks more efficient but introduced the powerful idea that architectural structure could encode useful priors about the data <span class="citation" data-cites="lecun1998gradient">(<a href="#ref-lecun1998gradient" role="doc-biblioref">Lecun et al. 1998</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-lecun1998gradient" class="csl-entry" role="listitem">
Lecun, Y., L. Bottou, Y. Bengio, and P. Haffner. 1998. <span>“Gradient-Based Learning Applied to Document Recognition.”</span> <em>Proceedings of the IEEE</em> 86 (11): 2278–2324. <a href="https://doi.org/10.1109/5.726791">https://doi.org/10.1109/5.726791</a>.
</div><div id="ref-he2016deep" class="csl-entry" role="listitem">
He, Kaiming, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. <span>“Deep Residual Learning for Image Recognition.”</span> In <em>2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 770–78. IEEE. <a href="https://doi.org/10.1109/cvpr.2016.90">https://doi.org/10.1109/cvpr.2016.90</a>.
</div></div><p>Perhaps even more influential was the introduction of skip connections through ResNets <span class="citation" data-cites="he2016deep">(<a href="#ref-he2016deep" role="doc-biblioref">He et al. 2016</a>)</span>. Originally they were designed to help train very deep CNNs, skip connections have become a fundamental building block that appears in virtually every modern architecture. They showed how direct paths through the network could help gradient flow and information propagation, a concept now central to Transformer designs.</p>
<p>CNNs also introduced batch normalization, a technique for stabilizing neural network training by normalizing intermediate features <span class="citation" data-cites="ioffe2015batch">(<a href="#ref-ioffe2015batch" role="doc-biblioref">Ioffe and Szegedy 2015</a>)</span>; we will learn more about this in the AI Training chapter. This concept of feature normalization, while originating in CNNs, evolved into layer normalization and is now a key component in modern architectures.</p>
<div class="no-row-height column-margin column-container"><div id="ref-ioffe2015batch" class="csl-entry" role="listitem">
Ioffe, Sergey, and Christian Szegedy. 2015. <span>“Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift.”</span> <em>International Conference on Machine Learning</em>, 448–56.
</div></div><p>These innovations, such as parameter sharing, skip connections, and normalization, transcended their origins in spatial processing to become essential building blocks in the deep learning toolkit.</p>
</section>
<section id="sec-dnn-architectures-evolution-sequence-processing-379a" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-dnn-architectures-evolution-sequence-processing-379a">The Evolution of Sequence Processing</h3>
<p>While CNNs specialized MLPs for spatial patterns, sequence models adapted neural networks for temporal dependencies. RNNs introduced the fundamental concept of maintaining and updating state, a building block that influenced how networks could process sequential information, <span class="citation" data-cites="elman1990finding">(<a href="#ref-elman1990finding" role="doc-biblioref">Elman 2002</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-elman1990finding" class="csl-entry" role="listitem">
Elman, Jeffrey L. 2002. <span>“Finding Structure in Time.”</span> In <em>Cognitive Modeling</em>, 14:257–88. 2. The MIT Press. <a href="https://doi.org/10.7551/mitpress/1888.003.0015">https://doi.org/10.7551/mitpress/1888.003.0015</a>.
</div><div id="ref-hochreiter1997long" class="csl-entry" role="listitem">
Hochreiter, Sepp, and Jürgen Schmidhuber. 1997. <span>“Long Short-Term Memory.”</span> <em>Neural Computation</em> 9 (8): 1735–80. <a href="https://doi.org/10.1162/neco.1997.9.8.1735">https://doi.org/10.1162/neco.1997.9.8.1735</a>.
</div><div id="ref-cho2014properties" class="csl-entry" role="listitem">
Cho, Kyunghyun, Bart van Merrienboer, Dzmitry Bahdanau, and Yoshua Bengio. 2014. <span>“On the Properties of Neural Machine Translation: Encoder-Decoder Approaches.”</span> In <em>Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation (SSST-8)</em>, 103–11. Association for Computational Linguistics.
</div></div><p>The development of LSTMs and GRUs brought sophisticated gating mechanisms to neural networks <span class="citation" data-cites="hochreiter1997long cho2014properties">(<a href="#ref-hochreiter1997long" role="doc-biblioref">Hochreiter and Schmidhuber 1997</a>; <a href="#ref-cho2014properties" role="doc-biblioref">Cho et al. 2014</a>)</span>. These gates, themselves small MLPs, showed how simple feedforward computations could be composed to control information flow. This concept of using neural networks to modulate other neural networks became a recurring pattern in architecture design.</p>
<p>Perhaps most significantly, sequence models demonstrated the power of adaptive computation paths. Unlike the fixed patterns of MLPs and CNNs, RNNs showed how networks could process variable-length inputs by reusing weights over time. This insight, that architectural patterns could adapt to input structure, laid groundwork for more flexible architectures.</p>
<p>Sequence models also popularized the concept of attention through encoder-decoder architectures <span class="citation" data-cites="bahdanau2014neural">(<a href="#ref-bahdanau2014neural" role="doc-biblioref">Bahdanau, Cho, and Bengio 2014</a>)</span>. Initially introduced as an improvement to machine translation, attention mechanisms showed how networks could learn to dynamically focus on relevant information. This building block would later become the foundation of Transformer architectures.</p>
<div class="no-row-height column-margin column-container"><div id="ref-bahdanau2014neural" class="csl-entry" role="listitem">
Bahdanau, Dzmitry, Kyunghyun Cho, and Yoshua Bengio. 2014. <span>“Neural Machine Translation by Jointly Learning to Align and Translate.”</span> <em>arXiv Preprint arXiv:1409.0473</em>, September. <a href="http://arxiv.org/abs/1409.0473v7">http://arxiv.org/abs/1409.0473v7</a>.
</div></div></section>
<section id="sec-dnn-architectures-modern-architectures-synthesis-innovation-f734" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-dnn-architectures-modern-architectures-synthesis-innovation-f734">Modern Architectures: Synthesis and Innovation</h3>
<p>Modern architectures, particularly Transformers, represent a sophisticated synthesis of these fundamental building blocks. Rather than introducing entirely new patterns, they innovate through clever combination and refinement of existing components. Consider the Transformer architecture: at its core, we find MLP-style feedforward networks processing features between attention layers. The attention mechanism itself builds on ideas from sequence models but removes the recurrent connection, instead using position embeddings<a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a> inspired by CNN intuitions. The architecture extensively utilizes skip connections (see <a href="#fig-example-skip-connection" class="quarto-xref">Figure&nbsp;9</a>)<a href="#fn8" class="footnote-ref" id="fnref8" role="doc-noteref"><sup>8</sup></a>, inherited from ResNets, while layer normalization, evolved from CNN’s batch normalization, stabilizes training <span class="citation" data-cites="ba2016layer">(<a href="#ref-ba2016layer" role="doc-biblioref">Ba, Kiros, and Hinton 2016</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="fn7"><p><sup>7</sup>&nbsp;<strong>Position Embeddings</strong>: Vector representations that encode the position of elements within a sequence in neural network processing.</p></div><div id="fn8"><p><sup>8</sup>&nbsp;<strong>Skip Connections</strong>: Connections that skip one or more layers in a neural network by feeding the output of one layer as the input to subsequent layers, enhancing gradient flow during training.</p></div><div id="ref-ba2016layer" class="csl-entry" role="listitem">
Ba, Jimmy Lei, Jamie Ryan Kiros, and Geoffrey E. Hinton. 2016. <span>“Layer Normalization.”</span> <em>arXiv Preprint arXiv:1607.06450</em>, July. <a href="http://arxiv.org/abs/1607.06450v1">http://arxiv.org/abs/1607.06450v1</a>.
</div></div><div id="fig-example-skip-connection" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-env="figure" data-fig-pos="htb">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-example-skip-connection-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="b6f5beebfe11ffd52682a16cd0e369491efe837c.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-9" title="Figure&nbsp;9: Residual Connection: Skip connections add the input of a layer to its output, enabling gradients to flow directly through the network and mitigating the vanishing gradient problem in deep architectures. This allows training of significantly deeper networks, as seen in resnets and adopted in modern transformer architectures to improve optimization and performance."><img src="dnn_architectures_files/mediabag/b6f5beebfe11ffd52682a16cd0e369491efe837c.svg" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-example-skip-connection-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;9: <strong>Residual Connection</strong>: Skip connections add the input of a layer to its output, enabling gradients to flow directly through the network and mitigating the vanishing gradient problem in deep architectures. This allows training of significantly deeper networks, as seen in resnets and adopted in modern transformer architectures to improve optimization and performance.
</figcaption>
</figure>
</div>
<p>This composition of building blocks creates something greater than the sum of its parts. The self-attention mechanism, while building on previous attention concepts, enables a new form of dynamic pattern processing. The arrangement of these components, attention followed by feedforward layers, with skip connections and normalization, has proven so effective it’s become a template for new architectures.</p>
<p>Even recent innovations in vision and language models follow this pattern of recombining fundamental building blocks. Vision Transformers adapt the Transformer architecture to images while maintaining its essential components <span class="citation" data-cites="dosovitskiy2021image">(<a href="#ref-dosovitskiy2021image" role="doc-biblioref">Dosovitskiy et al. 2021</a>)</span>. Large language models scale up these patterns while introducing refinements like grouped-query attention or sliding window attention, yet still rely on the core building blocks established through this architectural evolution <span class="citation" data-cites="brown2020language">(<a href="#ref-brown2020language" role="doc-biblioref">Brown et al. 2020</a>)</span>.</p>
<div class="no-row-height column-margin column-container"><div id="ref-dosovitskiy2021image" class="csl-entry" role="listitem">
Dosovitskiy, Alexey, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, et al. 2021. <span>“An Image Is Worth 16x16 Words: Transformers for Image Recognition at Scale.”</span> <em>International Conference on Learning Representations</em>.
</div><div id="ref-brown2020language" class="csl-entry" role="listitem">
Brown, Tom B, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, et al. 2020. <span>“Language Models Are Few-Shot Learners.”</span> <em>Advances in Neural Information Processing Systems</em> 33: 1877–1901.
</div></div><p>To illustrate how these modern architectures synthesize and innovate upon previous approaches, consider the following comparison of primitive utilization across different neural network architectures:</p>
<div id="tbl-primitive-comparison" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-primitive-comparison-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;2: <strong>Primitive Utilization</strong>: Neural network architectures differ in their core computational and memory access patterns, impacting hardware requirements and efficiency. Transformers uniquely combine matrix multiplication with attention mechanisms, resulting in random memory access and data movement patterns distinct from sequential rnns or strided cnns.
</figcaption>
<div aria-describedby="tbl-primitive-comparison-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<colgroup>
<col style="width: 13%">
<col style="width: 18%">
<col style="width: 22%">
<col style="width: 23%">
<col style="width: 20%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Primitive Type</th>
<th style="text-align: left;">MLP</th>
<th style="text-align: left;">CNN</th>
<th style="text-align: left;">RNN</th>
<th style="text-align: left;">Transformer</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Computational</td>
<td style="text-align: left;">Matrix Multiplication</td>
<td style="text-align: left;">Convolution (Matrix Mult.)</td>
<td style="text-align: left;">Matrix Mult. + State Update</td>
<td style="text-align: left;">Matrix Mult. + Attention</td>
</tr>
<tr class="even">
<td style="text-align: left;">Memory Access</td>
<td style="text-align: left;">Sequential</td>
<td style="text-align: left;">Strided</td>
<td style="text-align: left;">Sequential + Random</td>
<td style="text-align: left;">Random (Attention)</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Data Movement</td>
<td style="text-align: left;">Broadcast</td>
<td style="text-align: left;">Sliding Window</td>
<td style="text-align: left;">Sequential</td>
<td style="text-align: left;">Broadcast + Gather</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p>As shown in <a href="#tbl-primitive-comparison" class="quarto-xref">Table&nbsp;2</a>, Transformers combine elements from previous architectures while introducing new patterns. They retain the core matrix multiplication operations common to all architectures but introduce a more complex memory access pattern with their attention mechanism. Their data movement patterns blend the broadcast operations of MLPs with the gather operations reminiscent of more dynamic architectures.</p>
<p>This synthesis of primitives in Transformers exemplifies how modern architectures innovate by recombining and refining existing building blocks, rather than inventing entirely new computational paradigms. Also, this evolutionary process provides insight into the development of future architectures and helps to guide the design of efficient systems to support them.</p>
<div id="quiz-question-sec-dnn-architectures-architectural-building-blocks-e63a" class="callout callout-quiz-question">
<details class="callout-quiz-question fbx-default closebutton"><summary><strong>Self-Check: Question 1.6</strong></summary><div>
<ol type="1">
<li><p>Which architectural innovation introduced the concept of parameter sharing, significantly influencing future neural network designs?</p>
<ol type="a">
<li>Multi-Layer Perceptrons (MLPs)</li>
<li>Convolutional Neural Networks (CNNs)</li>
<li>Recurrent Neural Networks (RNNs)</li>
<li>Transformers</li>
</ol></li>
<li><p>Explain how the evolution of neural network architectures from MLPs to Transformers reflects a synthesis of fundamental building blocks.</p></li>
<li><p>True or False: Modern architectures like Transformers rely solely on new computational paradigms rather than recombining existing building blocks.</p></li>
<li><p>In the evolution of neural network architectures, the introduction of ____ connections in ResNets helped improve gradient flow and information propagation.</p></li>
</ol>
<p><a href="#quiz-answer-sec-dnn-architectures-architectural-building-blocks-e63a" class="question-label">See Answers →</a></p>
</div></details>
</div>
</section>
</section>
<section id="sec-dnn-architectures-systemlevel-building-blocks-56ed" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="sec-dnn-architectures-systemlevel-building-blocks-56ed">System-Level Building Blocks</h2>
<p>After having examined different deep learning architectures, we can distill their system requirements into fundamental primitives that underpin both hardware and software implementations. These primitives represent operations that cannot be broken down further while maintaining their essential characteristics. Just as complex molecules are built from basic atoms, sophisticated neural networks are constructed from these fundamental operations.</p>
<section id="sec-dnn-architectures-core-computational-primitives-97a1" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-dnn-architectures-core-computational-primitives-97a1">Core Computational Primitives</h3>
<p>Three fundamental operations serve as the building blocks for all deep learning computations: matrix multiplication, sliding window operations<a href="#fn9" class="footnote-ref" id="fnref9" role="doc-noteref"><sup>9</sup></a>, and dynamic computation<a href="#fn10" class="footnote-ref" id="fnref10" role="doc-noteref"><sup>10</sup></a>. What makes these operations primitive is that they cannot be further decomposed without losing their essential computational properties and efficiency characteristics.</p>
<div class="no-row-height column-margin column-container"><div id="fn9"><p><sup>9</sup>&nbsp;A technique in signal processing and computer vision where a window moves across data, computing results from subsets, essential in CNNs.</p></div><div id="fn10"><p><sup>10</sup>&nbsp;Computational processes where the operations adjust based on input data, used prominently in machine learning models like the Transformer.</p></div></div><p>Matrix multiplication represents the most basic form of transforming sets of features. When we multiply a matrix of inputs by a matrix of weights, we’re computing weighted combinations, which is the fundamental operation of neural networks. For example, in our MNIST network, each 784-dimensional input vector multiplies with a <span class="math inline">\(784\times 100\)</span> weight matrix. This pattern appears everywhere: MLPs use it directly for layer computations, CNNs reshape convolutions into matrix multiplications (turning a <span class="math inline">\(3\times 3\)</span> convolution into a matrix operation, as illustrated in <a href="#fig-im2col-diagram" class="quarto-xref">Figure&nbsp;10</a>), and Transformers use it extensively in their attention mechanisms.</p>
<div id="fig-im2col-diagram" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-env="figure" data-fig-pos="htb">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-im2col-diagram-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="b9c64687bff555767c7f2022da3f5ab9e7d582d5.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-10" title="Figure&nbsp;10: Convolution as Matrix Multiplication: Reshaping convolutional layers into matrix multiplications—using the im2col technique—enables efficient computation using optimized BLAS libraries and allows for parallel processing on standard hardware. This transformation is crucial for accelerating cnns and forms the basis for implementing convolutions on diverse platforms."><img src="dnn_architectures_files/mediabag/b9c64687bff555767c7f2022da3f5ab9e7d582d5.svg" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-im2col-diagram-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;10: <strong>Convolution as Matrix Multiplication</strong>: Reshaping convolutional layers into matrix multiplications—using the <code>im2col</code> technique—enables efficient computation using optimized BLAS libraries and allows for parallel processing on standard hardware. This transformation is crucial for accelerating cnns and forms the basis for implementing convolutions on diverse platforms.
</figcaption>
</figure>
</div>
<p>In modern systems, matrix multiplication maps to specific hardware and software implementations. Hardware accelerators provide specialized tensor cores that can perform thousands of multiply-accumulates in parallel, NVIDIA’s A100 tensor cores can achieve up to 312 TFLOPS (32-bit) through massive parallelization of these operations. Software frameworks like PyTorch and TensorFlow automatically map these high-level operations to optimized matrix libraries (NVIDIA <a href="https://developer.nvidia.com/cublas">cuBLAS</a>, Intel <a href="https://www.intel.com/content/www/us/en/developer/tools/oneapi/onemkl.html#gs.kxb9ve">MKL</a>) that exploit these hardware capabilities.</p>
<p>Sliding window operations compute local relationships by applying the same operation to chunks of data. In CNNs processing MNIST images, a <span class="math inline">\(3\times 3\)</span> convolution filter slides across the <span class="math inline">\(28\times 28\)</span> input, requiring <span class="math inline">\(26\times 26\)</span> windows of computation,<a href="#fn11" class="footnote-ref" id="fnref11" role="doc-noteref"><sup>11</sup></a> assuming a stride size of 1. Modern hardware accelerators implement this through specialized memory access patterns and data buffering schemes that optimize data reuse. For example, Google’s TPU uses a <span class="math inline">\(128\times 128\)</span> systolic array where data flows systematically through processing elements, allowing each input value to be reused across multiple computations without accessing memory. Software frameworks optimize these operations by transforming them into efficient matrix multiplications (a <span class="math inline">\(3\times 3\)</span> convolution becomes a <span class="math inline">\(9\times N\)</span> matrix multiplication) and carefully managing data layout in memory to maximize spatial locality.</p>
<div class="no-row-height column-margin column-container"><div id="fn11"><p><sup>11</sup>&nbsp;The <span class="math inline">\(26\times 26\)</span> output dimension comes from the formula <span class="math inline">\((N-F+1)\)</span> where <span class="math inline">\(N\)</span> is the input dimension (28) and <span class="math inline">\(F\)</span> is the filter size (3), calculated as: <span class="math inline">\(28-3+1=26\)</span> for both dimensions.</p></div></div><p>Dynamic computation, where the operation itself depends on the input data, emerged prominently with attention mechanisms but represents a fundamental capability needed for adaptive processing. In Transformer attention, each query dynamically determines its interaction weights with all keys; for a sequence of length 512, this means 512 different weight patterns must be computed on the fly. Unlike fixed patterns where we know the computation graph in advance, dynamic computation requires runtime decisions. This creates specific implementation challenges; hardware must provide flexible routing of data (modern GPUs use dynamic scheduling) and support variable computation patterns, while software frameworks need efficient mechanisms for handling data-dependent execution paths (PyTorch’s dynamic computation graphs, TensorFlow’s dynamic control flow).</p>
<p>These primitives combine in sophisticated ways in modern architectures. A Transformer layer processing a sequence of 512 tokens demonstrates this clearly: it uses matrix multiplications for feature projections (<span class="math inline">\(512\times 512\)</span> operations implemented through tensor cores), may employ sliding windows for efficient attention over long sequences (using specialized memory access patterns for local regions), and requires dynamic computation for attention weights (computing <span class="math inline">\(512\times 512\)</span> attention patterns at runtime). The way these primitives interact creates specific demands on system design, ranging from memory hierarchy organization to computation scheduling.</p>
<p>The building blocks we’ve discussed help explain why certain hardware features exist (like tensor cores for matrix multiplication) and why software frameworks organize computations in particular ways (like batching similar operations together). As we move from computational primitives to consider memory access and data movement patterns, it’s important to recognize how these fundamental operations shape the demands placed on memory systems and data transfer mechanisms. The way computational primitives are implemented and combined has direct implications for how data needs to be stored, accessed, and moved within the system.</p>
</section>
<section id="sec-dnn-architectures-memory-access-primitives-8150" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-dnn-architectures-memory-access-primitives-8150">Memory Access Primitives</h3>
<p>The efficiency of deep learning systems heavily depends on how they access and manage memory. In fact, memory access often becomes the primary bottleneck in modern ML systems, even though a matrix multiplication unit might be capable of performing thousands of operations per cycle, it will sit idle if data isn’t available at the right time. For example, accessing data from DRAM<a href="#fn12" class="footnote-ref" id="fnref12" role="doc-noteref"><sup>12</sup></a> typically takes hundreds of cycles, while on-chip computation takes only a few cycles.</p>
<div class="no-row-height column-margin column-container"><div id="fn12"><p><sup>12</sup>&nbsp;<strong>DRAM</strong>: Dynamic Random Access Memory, used for main system memory.</p></div></div><p>Three fundamental memory access patterns dominate in deep learning architectures: sequential access, strided access, and random access. Each pattern creates different demands on the memory system and offers different opportunities for optimization.</p>
<p>Sequential access is the simplest and most efficient pattern. Consider an MLP performing matrix multiplication with a batch of MNIST images: it needs to access both the <span class="math inline">\(784\times 100\)</span> weight matrix and the input vectors sequentially. This pattern maps well to modern memory systems; DRAM can operate in burst mode for sequential reads (achieving up to 400 GB/s in modern GPUs), and hardware prefetchers can effectively predict and fetch upcoming data. Software frameworks optimize for this by ensuring data is laid out contiguously in memory and aligning data to cache line boundaries.</p>
<p>Strided access appears prominently in CNNs, where each output position needs to access a window of input values at regular intervals. For a CNN processing MNIST images with <span class="math inline">\(3\times 3\)</span> filters, each output position requires accessing 9 input values with a stride matching the input width. While less efficient than sequential access, hardware supports this through pattern-aware caching strategies and specialized memory controllers. Software frameworks often transform these strided patterns into sequential access through data layout reorganization, where the im2col transformation<a href="#fn13" class="footnote-ref" id="fnref13" role="doc-noteref"><sup>13</sup></a> in deep learning frameworks converts convolution’s strided access into efficient matrix multiplications.</p>
<div class="no-row-height column-margin column-container"><div id="fn13"><p><sup>13</sup>&nbsp;<strong>im2col</strong>: An algorithm that transforms input data for efficient matrix multiplication in CNNs.</p></div></div><p>Random access poses the greatest challenge for system efficiency. In a Transformer processing a sequence of 512 tokens, each attention operation potentially needs to access any position in the sequence, creating unpredictable memory access patterns. Random access can severely impact performance through cache misses (potentially causing 100+ cycle stalls per access) and unpredictable memory latencies. Systems address this through large cache hierarchies (modern GPUs have several MB of L2 cache) and sophisticated prefetching strategies, while software frameworks employ techniques like attention pattern pruning to reduce random access requirements.</p>
<p>These different memory access patterns contribute significantly to the overall memory requirements of each architecture. To illustrate this, <a href="#tbl-arch-complexity" class="quarto-xref">Table&nbsp;3</a> compares the memory complexity of MLPs, CNNs, RNNs, and Transformers.</p>
<div id="tbl-arch-complexity" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-arch-complexity-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;3: <strong>Memory Access Complexity</strong>: Different neural network architectures exhibit varying memory access patterns and storage requirements, impacting system performance and scalability. Parameter storage scales with input dependency and model size, while activation storage represents a significant runtime cost, particularly for sequence-based models where rnns offer a parameter efficiency advantage when sequence length exceeds hidden state size (<span class="math inline">\(n &gt; h\)</span>).
</figcaption>
<div aria-describedby="tbl-arch-complexity-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<colgroup>
<col style="width: 13%">
<col style="width: 13%">
<col style="width: 18%">
<col style="width: 37%">
<col style="width: 13%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Architecture</th>
<th style="text-align: left;">Input Dependency</th>
<th style="text-align: left;">Parameter Storage</th>
<th style="text-align: left;">Activation Storage</th>
<th style="text-align: left;">Scaling Behavior</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">MLP</td>
<td style="text-align: left;">Linear</td>
<td style="text-align: left;"><span class="math inline">\(O(N \times W)\)</span></td>
<td style="text-align: left;"><span class="math inline">\(O(B \times W)\)</span></td>
<td style="text-align: left;">Predictable</td>
</tr>
<tr class="even">
<td style="text-align: left;">CNN</td>
<td style="text-align: left;">Constant</td>
<td style="text-align: left;"><span class="math inline">\(O(K \times C)\)</span></td>
<td style="text-align: left;"><span class="math inline">\(O(B\times H_{\text{img}}\times W_{\text{img}})\)</span></td>
<td style="text-align: left;">Efficient</td>
</tr>
<tr class="odd">
<td style="text-align: left;">RNN</td>
<td style="text-align: left;">Linear</td>
<td style="text-align: left;"><span class="math inline">\(O(h^2)\)</span></td>
<td style="text-align: left;"><span class="math inline">\(O(B \times T \times h)\)</span></td>
<td style="text-align: left;">Challenging</td>
</tr>
<tr class="even">
<td style="text-align: left;">Transformer</td>
<td style="text-align: left;">Quadratic</td>
<td style="text-align: left;"><span class="math inline">\(O(N \times d)\)</span></td>
<td style="text-align: left;"><span class="math inline">\(O(B \times N^2)\)</span></td>
<td style="text-align: left;">Problematic</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p>Where:</p>
<ul>
<li><span class="math inline">\(N\)</span>: Input or sequence size</li>
<li><span class="math inline">\(W\)</span>: Layer width</li>
<li><span class="math inline">\(B\)</span>: Batch size</li>
<li><span class="math inline">\(K\)</span>: Kernel size</li>
<li><span class="math inline">\(C\)</span>: Number of channels</li>
<li><span class="math inline">\(H_{\text{img}}\)</span>: Height of input feature map (CNN)</li>
<li><span class="math inline">\(W_{\text{img}}\)</span>: Width of input feature map (CNN)</li>
<li><span class="math inline">\(h\)</span>: Hidden state size (RNN)</li>
<li><span class="math inline">\(T\)</span>: Sequence length</li>
<li><span class="math inline">\(d\)</span>: Model dimensionality</li>
</ul>
<p><a href="#tbl-arch-complexity" class="quarto-xref">Table&nbsp;3</a> reveals how memory requirements scale with different architectural choices. The quadratic scaling of activation storage in Transformers, for instance, highlights the need for large memory capacities and efficient memory management in systems designed for Transformer-based workloads. In contrast, CNNs exhibit more favorable memory scaling due to their parameter sharing and localized processing. These memory complexity considerations are crucial when making system-level design decisions, such as choosing memory hierarchy configurations and developing memory optimization strategies.</p>
<p>The impact of these patterns becomes clearer when we consider data reuse opportunities. In CNNs, each input pixel participates in multiple convolution windows (typically 9 times for a <span class="math inline">\(3\times 3\)</span> filter), making effective data reuse fundamental for performance. Modern GPUs provide multi-level cache hierarchies (L1, L2, shared memory) to capture this reuse, while software techniques like loop tiling ensure data remains in cache once loaded.</p>
<p>Working set size, the amount of data needed simultaneously for computation, varies dramatically across architectures. An MLP layer processing MNIST images might need only a few hundred KB (weights plus activations), while a Transformer processing long sequences can require several MB just for storing attention patterns. These differences directly influence hardware design choices, like the balance between compute units and on-chip memory, and software optimizations like activation checkpointing or attention approximation techniques.</p>
<p>Having a good grasp of these memory access patterns is essential as architectures evolve. The shift from CNNs to Transformers, for instance, has driven the development of hardware with larger on-chip memories and more sophisticated caching strategies to handle increased working sets and more dynamic access patterns. Future architectures will likely continue to be shaped by their memory access characteristics as much as their computational requirements.</p>
</section>
<section id="sec-dnn-architectures-data-movement-primitives-8540" class="level3">
<h3 class="anchored" data-anchor-id="sec-dnn-architectures-data-movement-primitives-8540">Data Movement Primitives</h3>
<p>While computational and memory access patterns define what operations occur where, data movement primitives characterize how information flows through the system. These patterns are key because data movement often consumes more time and energy than computation itself, as moving data from off-chip memory typically requires 100-1000$ imes$ more energy than performing a floating-point operation.</p>
<p>Four fundamental data movement patterns are prevalent in deep learning architectures: broadcast, scatter, gather, and reduction. <a href="#fig-collective-comm" class="quarto-xref">Figure&nbsp;11</a> illustrates these patterns and their relationships. Broadcast operations send the same data to multiple destinations simultaneously. In matrix multiplication with batch size 32, each weight must be broadcast to process different inputs in parallel. Modern hardware supports this through specialized interconnects, NVIDIA GPUs provide hardware multicast capabilities, achieving up to 600 GB/s broadcast bandwidth, while TPUs use dedicated broadcast buses. Software frameworks optimize broadcasts by restructuring computations (like matrix tiling) to maximize data reuse.</p>
<div id="fig-collective-comm" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-env="figure" data-fig-pos="htb">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-collective-comm-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="f8436bde574fcdbb7c55649318497efa63ec8c95.svg" class="lightbox" data-gallery="quarto-lightbox-gallery-11" title="Figure&nbsp;11: Collective Communication Patterns: Deep learning training and inference frequently require data exchange between processing units; this figure outlines four fundamental patterns—broadcast, scatter, gather, and reduction—that define how data moves within a distributed system and impact overall performance. Understanding these patterns enables optimization of data movement, critical because communication costs often dominate computation in modern machine learning workloads."><img src="dnn_architectures_files/mediabag/f8436bde574fcdbb7c55649318497efa63ec8c95.svg" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-collective-comm-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;11: <strong>Collective Communication Patterns</strong>: Deep learning training and inference frequently require data exchange between processing units; this figure outlines four fundamental patterns—broadcast, scatter, gather, and reduction—that define how data moves within a distributed system and impact overall performance. Understanding these patterns enables optimization of data movement, critical because communication costs often dominate computation in modern machine learning workloads.
</figcaption>
</figure>
</div>
<p>Scatter operations distribute different elements to different destinations. When parallelizing a <span class="math inline">\(512\times 512\)</span> matrix multiplication across GPU cores, each core receives a subset of the computation. This parallelization is important for performance but challenging, as memory conflicts and load imbalance, can reduce efficiency by 50% or more. Hardware provides flexible interconnects (like NVIDIA’s NVLink offering 600 GB/s bi-directional bandwidth), while software frameworks employ sophisticated work distribution algorithms to maintain high utilization.</p>
<p>Gather operations collect data from multiple sources. In Transformer attention with sequence length 512, each query must gather information from 512 different key-value pairs. These irregular access patterns are challenging, random gathering can be <span class="math inline">\(10\times\)</span> slower than sequential access. Hardware supports this through high-bandwidth interconnects and large caches, while software frameworks employ techniques like attention pattern pruning to reduce gathering overhead.</p>
<p>Reduction operations combine multiple values into a single result through operations like summation. When computing attention scores in Transformers or layer outputs in MLPs, efficient reduction is essential. Hardware implements tree-structured reduction networks (reducing latency from <span class="math inline">\(O(n)\)</span> to <span class="math inline">\(O(\log n)\)</span>), while software frameworks use optimized parallel reduction algorithms that can achieve near-theoretical peak performance.</p>
<p>These patterns combine in sophisticated ways. A Transformer attention operation with sequence length 512 and batch size 32 involves:</p>
<ul>
<li>Broadcasting query vectors (<span class="math inline">\(512\times 64\)</span> elements)</li>
<li>Gathering relevant keys and values (<span class="math inline">\(512\times 512\times 64\)</span> elements)</li>
<li>Reducing attention scores (<span class="math inline">\(512\times 512\)</span> elements per sequence)</li>
</ul>
<p>The evolution from CNNs to Transformers has increased reliance on gather and reduction operations, driving hardware innovations like more flexible interconnects and larger on-chip memories. As models grow (some now exceeding 100 billion parameters), efficient data movement becomes increasingly critical, leading to innovations like near-memory processing and sophisticated data flow optimizations.</p>
</section>
<section id="sec-dnn-architectures-system-design-impact-9a93" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-dnn-architectures-system-design-impact-9a93">System Design Impact</h3>
<p>The computational, memory access, and data movement primitives we’ve explored form the foundational requirements that shape the design of systems for deep learning. The way these primitives influence hardware design, create common bottlenecks, and drive trade-offs is important for developing efficient and effective deep learning systems.</p>
<p>One of the most significant impacts of these primitives on system design is the push towards specialized hardware. The prevalence of matrix multiplications and convolutions in deep learning has led to the development of tensor processing units (TPUs) and tensor cores in GPUs, which are specifically designed to perform these operations efficiently. These specialized units can perform many multiply-accumulate operations in parallel, dramatically accelerating the core computations of neural networks.</p>
<p>Memory systems have also been profoundly influenced by the demands of deep learning primitives. The need to support both sequential and random access patterns efficiently has driven the development of sophisticated memory hierarchies. High-bandwidth memory (HBM)<a href="#fn14" class="footnote-ref" id="fnref14" role="doc-noteref"><sup>14</sup></a> has become common in AI accelerators to support the massive data movement requirements, especially for operations like attention mechanisms in Transformers. On-chip memory hierarchies have grown in complexity, with multiple levels of caching and scratchpad memories to support the diverse working set sizes of different neural network layers.</p>
<div class="no-row-height column-margin column-container"><div id="fn14"><p><sup>14</sup>&nbsp;<strong>High-bandwidth memory (HBM)</strong>: A type of stacked DRAM designed to provide high-speed data access for processing units.</p></div></div><p>The data movement primitives have particularly influenced the design of interconnects and on-chip networks. The need to support efficient broadcasts, gathers, and reductions has led to the development of more flexible and higher-bandwidth interconnects. Some AI chips now feature specialized networks-on-chip designed to accelerate common data movement patterns in neural networks.</p>
<p><a href="#tbl-sys-design-implications" class="quarto-xref">Table&nbsp;4</a> summarizes the system implications of these primitives:</p>
<div id="tbl-sys-design-implications" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-sys-design-implications-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;4: <strong>Primitive-Hardware Co-Design</strong>: Efficient machine learning systems require tight integration between algorithmic primitives and underlying hardware; this table maps common primitives to specific hardware accelerations and software optimizations, highlighting key challenges in their implementation. Specialized hardware, such as tensor cores and datapaths, address the computational demands of primitives like matrix multiplication and sliding windows, while software techniques like batching and dynamic graph execution further enhance performance.
</figcaption>
<div aria-describedby="tbl-sys-design-implications-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<colgroup>
<col style="width: 21%">
<col style="width: 25%">
<col style="width: 24%">
<col style="width: 26%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Primitive</th>
<th style="text-align: left;">Hardware Impact</th>
<th style="text-align: left;">Software Optimization</th>
<th style="text-align: left;">Key Challenges</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Matrix Multiplication</td>
<td style="text-align: left;">Tensor Cores</td>
<td style="text-align: left;">Batching, GEMM libraries</td>
<td style="text-align: left;">Parallelization, precision</td>
</tr>
<tr class="even">
<td style="text-align: left;">Sliding Window</td>
<td style="text-align: left;">Specialized datapaths</td>
<td style="text-align: left;">Data layout optimization</td>
<td style="text-align: left;">Stride handling</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Dynamic Computation</td>
<td style="text-align: left;">Flexible routing</td>
<td style="text-align: left;">Dynamic graph execution</td>
<td style="text-align: left;">Load balancing</td>
</tr>
<tr class="even">
<td style="text-align: left;">Sequential Access</td>
<td style="text-align: left;">Burst mode DRAM</td>
<td style="text-align: left;">Contiguous allocation</td>
<td style="text-align: left;">Access latency</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Random Access</td>
<td style="text-align: left;">Large caches</td>
<td style="text-align: left;">Memory-aware scheduling</td>
<td style="text-align: left;">Cache misses</td>
</tr>
<tr class="even">
<td style="text-align: left;">Broadcast</td>
<td style="text-align: left;">Specialized interconnects</td>
<td style="text-align: left;">Operation fusion</td>
<td style="text-align: left;">Bandwidth</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Gather/Scatter</td>
<td style="text-align: left;">High-bandwidth memory</td>
<td style="text-align: left;">Work distribution</td>
<td style="text-align: left;">Load balancing</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p>Despite these advancements, several common bottlenecks persist in deep learning systems. Memory bandwidth often remains a key limitation, particularly for models with large working sets or those that require frequent random access. The energy cost of data movement, especially between off-chip memory and processing units, continues to be a significant concern. For large-scale models, the communication overhead in distributed training can become a bottleneck, limiting scaling efficiency.</p>
<p>System designers must navigate complex trade-offs in supporting different primitives, each with unique characteristics that influence system design and performance. For example, optimizing for the dense matrix operations common in MLPs and CNNs might come at the cost of flexibility needed for the more dynamic computations in attention mechanisms. Supporting large working sets for Transformers might require sacrificing energy efficiency.</p>
<p>Balancing these trade-offs requires careful consideration of the target workloads and deployment scenarios. Having a good grip on the nature of each primitive guides the development of both hardware and software optimizations in deep learning systems, allowing designers to make informed decisions about system architecture and resource allocation.</p>
<div id="quiz-question-sec-dnn-architectures-systemlevel-building-blocks-56ed" class="callout callout-quiz-question">
<details class="callout-quiz-question fbx-default closebutton"><summary><strong>Self-Check: Question 1.7</strong></summary><div>
<ol type="1">
<li><p>Which of the following operations is considered a core computational primitive in deep learning architectures?</p>
<ol type="a">
<li>Matrix multiplication</li>
<li>Batch normalization</li>
<li>Dropout</li>
<li>Pooling</li>
</ol></li>
<li><p>Explain why memory access patterns are critical in the design of deep learning systems.</p></li>
<li><p>True or False: Random access patterns are more efficient than sequential access patterns in deep learning systems.</p></li>
<li><p>In deep learning systems, the operation that combines multiple values into a single result, such as summation, is known as ____.</p></li>
<li><p>Discuss the system-level trade-offs involved in supporting both matrix multiplication and dynamic computation in deep learning hardware.</p></li>
</ol>
<p><a href="#quiz-answer-sec-dnn-architectures-systemlevel-building-blocks-56ed" class="question-label">See Answers →</a></p>
</div></details>
</div>
</section>
</section>
<section id="sec-dnn-architectures-summary-36be" class="level2">
<h2 class="anchored" data-anchor-id="sec-dnn-architectures-summary-36be">Summary</h2>
<p>Deep learning architectures, despite their diversity, exhibit common patterns in their algorithmic structures that significantly influence computational requirements and system design. In this chapter, we explored the intricate relationship between high-level architectural concepts and their practical implementation in computing systems.</p>
<p>From the straightforward dense connections of MLPs to the complex, dynamic patterns of Transformers, each architecture builds upon a set of fundamental building blocks. These core computational primitives, including matrix multiplication, sliding windows, and dynamic computation, recur across various architectures, forming a universal language of deep learning computation.</p>
<p>The identification of these shared elements provides a valuable framework for understanding and designing deep learning systems. Each primitive brings its own set of requirements in terms of memory access patterns and data movement, which in turn shape both hardware and software design decisions. This relationship between algorithmic intent and system implementation is crucial for optimizing performance and efficiency.</p>
<p>As the field of deep learning continues to evolve, the ability to efficiently support and optimize these fundamental building blocks will be key to the development of more powerful and scalable systems. Future advancements in deep learning are likely to stem not only from novel architectural designs but also from innovative approaches to implementing and optimizing these essential computational patterns.</p>
<p>In conclusion, understanding the mapping between neural architectures and their computational requirements is vital for pushing the boundaries of what’s possible in artificial intelligence. As we look to the future, the interplay between algorithmic innovation and systems optimization will continue to drive progress in this rapidly advancing field.</p>
<!-- This is here to make sure that quizzes are inserted properly before a part begins. -->
</section>
<section id="self-check-answers" class="level2">
<h2 class="anchored" data-anchor-id="self-check-answers">Self-Check Answers</h2>
<div id="quiz-answer-sec-dnn-architectures-overview-aa0c" class="callout callout-quiz-answer">
<details class="callout-quiz-answer fbx-answer closebutton"><summary><strong>Self-Check: Answer 1.1</strong></summary><div>
<ol type="1">
<li><p><strong>Which aspect of neural network architecture is directly concerned with how data moves through the memory hierarchy?</strong></p>
<ol type="a">
<li>Computation characteristics</li>
<li>Memory access patterns</li>
<li>Data movement</li>
<li>Resource utilization</li>
</ol>
<p><em>Answer</em>: The correct answer is B. Memory access patterns are concerned with how data moves through the memory hierarchy, which is crucial for understanding how neural network architectures map to system resources.</p>
<p><em>Learning Objective</em>: Understand the role of memory access patterns in mapping neural network architectures to system resources.</p></li>
<li><p><strong>Explain why dense connectivity patterns in neural networks generate different memory bandwidth demands compared to localized processing structures.</strong></p>
<p><em>Answer</em>: Dense connectivity patterns require more extensive data movement and memory bandwidth because they involve connections between many neurons, leading to higher data transfer demands compared to localized processing structures, which limit data movement to nearby neurons.</p>
<p><em>Learning Objective</em>: Analyze the impact of connectivity patterns on memory bandwidth demands in neural network architectures.</p></li>
<li><p><strong>Stateful processing in neural networks requires different on-chip memory organization compared to stateless operations.</strong></p>
<p><em>Answer</em>: True. Stateful processing involves maintaining information across time steps, which requires specific on-chip memory organization to efficiently store and access state information, unlike stateless operations that do not maintain such information.</p>
<p><em>Learning Objective</em>: Understand the implications of stateful processing on memory organization in neural network architectures.</p></li>
</ol>
<p><a href="#quiz-question-sec-dnn-architectures-overview-aa0c" class="answer-label">← Back to Questions</a></p>
</div></details>
</div>
<div id="quiz-answer-sec-dnn-architectures-multilayer-perceptrons-dense-pattern-processing-b8f9" class="callout callout-quiz-answer">
<details class="callout-quiz-answer fbx-answer closebutton"><summary><strong>Self-Check: Answer 1.2</strong></summary><div>
<ol type="1">
<li><p><strong>What is the primary computational operation used in Multi-Layer Perceptrons (MLPs) for dense pattern processing?</strong></p>
<ol type="a">
<li>Convolution</li>
<li>Matrix multiplication</li>
<li>Pooling</li>
<li>Recurrent connections</li>
</ol>
<p><em>Answer</em>: The correct answer is B. Matrix multiplication is the primary operation used in MLPs to enable dense pattern processing, allowing each neuron to connect to every neuron in adjacent layers.</p>
<p><em>Learning Objective</em>: Understand the core computational operation in MLPs for dense pattern processing.</p></li>
<li><p><strong>Explain why dense pattern processing in MLPs is suitable for tasks like MNIST digit recognition.</strong></p>
<p><em>Answer</em>: Dense pattern processing is suitable for MNIST because it allows the network to learn arbitrary relationships across all input pixels, capturing essential features for classification despite variations in handwriting.</p>
<p><em>Learning Objective</em>: Analyze the suitability of dense pattern processing for specific tasks like MNIST digit recognition.</p></li>
<li><p><strong>True or False: In MLPs, each output neuron requires the same number of multiply-accumulate operations as there are input features.</strong></p>
<p><em>Answer</em>: True. Each output neuron in an MLP requires multiply-accumulate operations equal to the number of input features, as every input contributes to every output through dense connectivity.</p>
<p><em>Learning Objective</em>: Understand the computational needs of MLPs in terms of multiply-accumulate operations.</p></li>
<li><p><strong>The dense connectivity pattern in MLPs translates mathematically into ____ operations.</strong></p>
<p><em>Answer</em>: matrix multiplication. This operation allows for the transformation of input features through fully-connected layers, enabling dense pattern processing.</p>
<p><em>Learning Objective</em>: Recall the mathematical operation that underpins dense connectivity in MLPs.</p></li>
<li><p><strong>Discuss the system implications of the data movement requirements in MLPs.</strong></p>
<p><em>Answer</em>: The all-to-all connectivity in MLPs leads to significant data movement requirements, necessitating efficient data transfer strategies. Systems must handle large volumes of data movement between memory and compute units, optimizing through caching, prefetching, and high-bandwidth memory systems.</p>
<p><em>Learning Objective</em>: Analyze the system-level implications of data movement in MLPs.</p></li>
</ol>
<p><a href="#quiz-question-sec-dnn-architectures-multilayer-perceptrons-dense-pattern-processing-b8f9" class="answer-label">← Back to Questions</a></p>
</div></details>
</div>
<div id="quiz-answer-sec-dnn-architectures-convolutional-neural-networks-spatial-pattern-processing-660a" class="callout callout-quiz-answer">
<details class="callout-quiz-answer fbx-answer closebutton"><summary><strong>Self-Check: Answer 1.3</strong></summary><div>
<ol type="1">
<li><p><strong>What is the primary advantage of using convolutional layers over fully connected layers in neural networks?</strong></p>
<ol type="a">
<li>They require fewer parameters by reusing weights.</li>
<li>They process data faster by using more parameters.</li>
<li>They eliminate the need for activation functions.</li>
<li>They increase the model complexity significantly.</li>
</ol>
<p><em>Answer</em>: The correct answer is A. Convolutional layers require fewer parameters by reusing the same weights across different spatial positions, which reduces the number of parameters compared to fully connected layers.</p>
<p><em>Learning Objective</em>: Understand the parameter efficiency of CNNs compared to fully connected layers.</p></li>
<li><p><strong>True or False: Convolutional neural networks maintain spatial locality by connecting each output to all input pixels.</strong></p>
<p><em>Answer</em>: False. CNNs maintain spatial locality by connecting each output only to a small, spatially contiguous region of the input, not all input pixels.</p>
<p><em>Learning Objective</em>: Recognize how CNNs maintain spatial locality through local connections.</p></li>
<li><p><strong>Explain how the spatial pattern processing of CNNs influences their memory and computation needs compared to MLPs.</strong></p>
<p><em>Answer</em>: CNNs use small, reusable filters that reduce memory needs for weights but require storing feature maps for all spatial positions, affecting memory differently than MLPs. Computationally, CNNs perform repetitive operations across spatial positions, enabling structured parallelism and efficient hardware utilization.</p>
<p><em>Learning Objective</em>: Analyze the impact of spatial pattern processing on CNNs’ memory and computation needs.</p></li>
<li><p><strong>In CNNs, the operation that involves sliding a small filter over the input image to generate a feature map is known as ____. </strong></p>
<p><em>Answer</em>: convolution. This operation captures local structures and maintains translation invariance, which is fundamental to CNNs.</p>
<p><em>Learning Objective</em>: Recall the key operation in CNNs that processes spatial patterns.</p></li>
<li><p><strong>Discuss the system-level tradeoffs involved in deploying CNNs on GPUs versus CPUs.</strong></p>
<p><em>Answer</em>: GPUs are optimized for parallel processing, making them ideal for the repetitive and parallelizable nature of CNN computations, while CPUs leverage cache hierarchies to handle memory access patterns. The choice between them involves tradeoffs in terms of parallel efficiency, memory handling, and power consumption.</p>
<p><em>Learning Objective</em>: Evaluate the tradeoffs of deploying CNNs on different hardware architectures.</p></li>
</ol>
<p><a href="#quiz-question-sec-dnn-architectures-convolutional-neural-networks-spatial-pattern-processing-660a" class="answer-label">← Back to Questions</a></p>
</div></details>
</div>
<div id="quiz-answer-sec-dnn-architectures-recurrent-neural-networks-sequential-pattern-processing-cc67" class="callout callout-quiz-answer">
<details class="callout-quiz-answer fbx-answer closebutton"><summary><strong>Self-Check: Answer 1.4</strong></summary><div>
<ol type="1">
<li><p><strong>What is the primary challenge that RNNs address in sequential data processing?</strong></p>
<ol type="a">
<li>Handling fixed-size input sequences</li>
<li>Maintaining and updating relevant context over time</li>
<li>Reducing computational complexity</li>
<li>Improving spatial pattern recognition</li>
</ol>
<p><em>Answer</em>: The correct answer is B. RNNs are designed to maintain and update relevant context over time, which is crucial for processing sequential data where the meaning of current input depends on previous context.</p>
<p><em>Learning Objective</em>: Understand the primary challenge RNNs address in sequential data processing.</p></li>
<li><p><strong>Explain how the recurrent connections in RNNs contribute to their ability to process sequential data.</strong></p>
<p><em>Answer</em>: Recurrent connections in RNNs allow the network to maintain an internal state that is updated at each time step. This creates a memory mechanism that carries information forward, enabling the network to capture temporal dependencies and process sequences effectively.</p>
<p><em>Learning Objective</em>: Explain the role of recurrent connections in RNNs for sequential data processing.</p></li>
<li><p><strong>In RNNs, the operation that updates the hidden state based on the previous state and current input is known as ____. </strong></p>
<p><em>Answer</em>: recurrent update. The recurrent update operation combines the previous hidden state with the current input to generate the next hidden state, allowing the network to process sequential data effectively.</p>
<p><em>Learning Objective</em>: Recall the operation that updates the hidden state in RNNs.</p></li>
<li><p><strong>True or False: RNNs can parallelize computations across time steps just like they do across batch elements.</strong></p>
<p><em>Answer</em>: False. While RNNs can parallelize computations across batch elements, they cannot parallelize across time steps due to the sequential dependency of each step on the previous hidden state.</p>
<p><em>Learning Objective</em>: Understand the limitations of parallelization in RNNs.</p></li>
<li><p><strong>Discuss the system-level tradeoffs involved in deploying RNNs on CPUs versus GPUs.</strong></p>
<p><em>Answer</em>: Deploying RNNs on CPUs leverages cache hierarchies for weight reuse and can efficiently handle sequential dependencies through pipelining. GPUs, on the other hand, optimize for high throughput by processing multiple sequences in parallel, despite sequential dependencies. The choice depends on the specific workload and hardware capabilities.</p>
<p><em>Learning Objective</em>: Analyze system-level tradeoffs in deploying RNNs on different hardware architectures.</p></li>
</ol>
<p><a href="#quiz-question-sec-dnn-architectures-recurrent-neural-networks-sequential-pattern-processing-cc67" class="answer-label">← Back to Questions</a></p>
</div></details>
</div>
<div id="quiz-answer-sec-dnn-architectures-attention-mechanisms-dynamic-pattern-processing-ea2d" class="callout callout-quiz-answer">
<details class="callout-quiz-answer fbx-answer closebutton"><summary><strong>Self-Check: Answer 1.5</strong></summary><div>
<ol type="1">
<li><p><strong>What is the primary computational challenge associated with attention mechanisms in terms of sequence length?</strong></p>
<ol type="a">
<li>Linear scaling with sequence length</li>
<li>Quadratic scaling with sequence length</li>
<li>Constant scaling with sequence length</li>
<li>Exponential scaling with sequence length</li>
</ol>
<p><em>Answer</em>: The correct answer is B. Attention mechanisms involve computing an N×N attention matrix, leading to quadratic scaling with respect to sequence length, which can be a computational bottleneck for long sequences.</p>
<p><em>Learning Objective</em>: Understand the computational complexity of attention mechanisms and its implications for system performance.</p></li>
<li><p><strong>Explain why attention mechanisms require dynamic computation of weights and how this differs from fixed connectivity patterns in previous architectures.</strong></p>
<p><em>Answer</em>: Attention mechanisms compute weights based on content, allowing for dynamic relationships between elements. This differs from fixed connectivity patterns, like in CNNs or RNNs, where connections are predetermined and do not adapt based on input content.</p>
<p><em>Learning Objective</em>: Analyze the dynamic nature of attention mechanisms compared to fixed architectures and its impact on processing capabilities.</p></li>
<li><p><strong>In Transformer architectures, the mechanism that allows each element to attend to all other elements within the same sequence is known as ____. </strong></p>
<p><em>Answer</em>: self-attention. Self-attention enables elements within the same sequence to dynamically weigh their relationships, capturing dependencies without sequential processing.</p>
<p><em>Learning Objective</em>: Recall the key mechanism in Transformers that enables dynamic pattern processing within sequences.</p></li>
<li><p><strong>True or False: The parallel nature of Transformer computations makes them less suited for modern parallel processing hardware.</strong></p>
<p><em>Answer</em>: False. The parallel nature of Transformer computations makes them well-suited for modern parallel processing hardware, enabling efficient processing of sequences.</p>
<p><em>Learning Objective</em>: Evaluate the suitability of Transformer architectures for parallel processing environments.</p></li>
<li><p><strong>Discuss the system-level tradeoffs involved in deploying Transformer models on memory-constrained devices.</strong></p>
<p><em>Answer</em>: Deploying Transformers on memory-constrained devices is challenging due to the memory-intensive nature of attention weights and intermediate results. Optimizations like sparse attention or low-rank approximations can reduce memory usage but may affect model expressiveness.</p>
<p><em>Learning Objective</em>: Analyze the tradeoffs in deploying complex ML models like Transformers on devices with limited memory resources.</p></li>
</ol>
<p><a href="#quiz-question-sec-dnn-architectures-attention-mechanisms-dynamic-pattern-processing-ea2d" class="answer-label">← Back to Questions</a></p>
</div></details>
</div>
<div id="quiz-answer-sec-dnn-architectures-architectural-building-blocks-e63a" class="callout callout-quiz-answer">
<details class="callout-quiz-answer fbx-answer closebutton"><summary><strong>Self-Check: Answer 1.6</strong></summary><div>
<ol type="1">
<li><p><strong>Which architectural innovation introduced the concept of parameter sharing, significantly influencing future neural network designs?</strong></p>
<ol type="a">
<li>Multi-Layer Perceptrons (MLPs)</li>
<li>Convolutional Neural Networks (CNNs)</li>
<li>Recurrent Neural Networks (RNNs)</li>
<li>Transformers</li>
</ol>
<p><em>Answer</em>: The correct answer is B. Convolutional Neural Networks (CNNs) introduced parameter sharing, allowing the same parameters to be reused across different parts of the input, which made networks more efficient and influenced future designs.</p>
<p><em>Learning Objective</em>: Understand the significance of parameter sharing introduced by CNNs and its impact on future neural network designs.</p></li>
<li><p><strong>Explain how the evolution of neural network architectures from MLPs to Transformers reflects a synthesis of fundamental building blocks.</strong></p>
<p><em>Answer</em>: The evolution from MLPs to Transformers reflects a synthesis of fundamental building blocks by combining and refining existing components. MLPs introduced layer stacking and non-linear transformations. CNNs added parameter sharing and skip connections. RNNs contributed state maintenance and attention mechanisms. Transformers integrate these by using feedforward layers, attention, and skip connections, creating a powerful architecture that builds on past innovations.</p>
<p><em>Learning Objective</em>: Analyze how modern architectures synthesize and innovate upon fundamental building blocks from previous neural network designs.</p></li>
<li><p><strong>True or False: Modern architectures like Transformers rely solely on new computational paradigms rather than recombining existing building blocks.</strong></p>
<p><em>Answer</em>: False. Modern architectures like Transformers innovate by recombining and refining existing building blocks, such as feedforward layers, attention mechanisms, and skip connections, rather than inventing entirely new computational paradigms.</p>
<p><em>Learning Objective</em>: Recognize the importance of recombining existing building blocks in modern neural network architectures.</p></li>
<li><p><strong>In the evolution of neural network architectures, the introduction of ____ connections in ResNets helped improve gradient flow and information propagation.</strong></p>
<p><em>Answer</em>: skip. Skip connections in ResNets provided direct paths through the network, improving gradient flow and information propagation, and have become a fundamental building block in modern architectures.</p>
<p><em>Learning Objective</em>: Recall the role of skip connections in improving neural network training and their influence on modern architectures.</p></li>
</ol>
<p><a href="#quiz-question-sec-dnn-architectures-architectural-building-blocks-e63a" class="answer-label">← Back to Questions</a></p>
</div></details>
</div>
<div id="quiz-answer-sec-dnn-architectures-systemlevel-building-blocks-56ed" class="callout callout-quiz-answer">
<details class="callout-quiz-answer fbx-answer closebutton"><summary><strong>Self-Check: Answer 1.7</strong></summary><div>
<ol type="1">
<li><p><strong>Which of the following operations is considered a core computational primitive in deep learning architectures?</strong></p>
<ol type="a">
<li>Matrix multiplication</li>
<li>Batch normalization</li>
<li>Dropout</li>
<li>Pooling</li>
</ol>
<p><em>Answer</em>: The correct answer is A. Matrix multiplication is a core computational primitive that underpins many operations in deep learning architectures, such as feature transformations and attention mechanisms.</p>
<p><em>Learning Objective</em>: Identify core computational primitives in deep learning architectures.</p></li>
<li><p><strong>Explain why memory access patterns are critical in the design of deep learning systems.</strong></p>
<p><em>Answer</em>: Memory access patterns are critical because they often become the primary bottleneck in ML systems. Efficient memory access ensures that data is available when needed, preventing computation units from idling and optimizing system performance.</p>
<p><em>Learning Objective</em>: Understand the importance of memory access patterns in ML system design.</p></li>
<li><p><strong>True or False: Random access patterns are more efficient than sequential access patterns in deep learning systems.</strong></p>
<p><em>Answer</em>: False. Sequential access patterns are generally more efficient than random access patterns because they align well with modern memory systems, reducing cache misses and improving data throughput.</p>
<p><em>Learning Objective</em>: Recognize the efficiency differences between memory access patterns in ML systems.</p></li>
<li><p><strong>In deep learning systems, the operation that combines multiple values into a single result, such as summation, is known as ____.</strong></p>
<p><em>Answer</em>: reduction. Reduction operations are crucial for efficiently computing outputs like attention scores or layer outputs in neural networks.</p>
<p><em>Learning Objective</em>: Recall specific data movement operations used in deep learning systems.</p></li>
<li><p><strong>Discuss the system-level trade-offs involved in supporting both matrix multiplication and dynamic computation in deep learning hardware.</strong></p>
<p><em>Answer</em>: Supporting matrix multiplication requires specialized units like tensor cores for efficient parallel processing, while dynamic computation demands flexible routing and adaptive execution paths. Balancing these needs can lead to trade-offs in hardware design, such as sacrificing flexibility for performance or vice versa.</p>
<p><em>Learning Objective</em>: Analyze system-level trade-offs in hardware design for deep learning operations.</p></li>
</ol>
<p><a href="#quiz-question-sec-dnn-architectures-systemlevel-building-blocks-56ed" class="answer-label">← Back to Questions</a></p>
</div></details>
</div>



</section>
</section>


<a onclick="window.scrollTo(0, 0); return false;" role="button" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> Back to top</a></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
            // target, if specified
            link.setAttribute("target", "_blank");
            if (link.getAttribute("rel") === null) {
              link.setAttribute("rel", "noopener");
            }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../../../contents/core/dl_primer/dl_primer.html" class="pagination-link" aria-label="DL Primer">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">DL Primer</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../../../contents/core/workflow/workflow.html" class="pagination-link" aria-label="AI Workflow">
        <span class="nav-page-text">AI Workflow</span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>© 2024 Harvard University. Licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC-BY-NC-SA 4.0</a></p>
</div>   
    <div class="nav-footer-center">
<p>Written, edited and curated by Prof.&nbsp;Vijay Janapa Reddi (Harvard University)</p>
</div>
    <div class="nav-footer-right">
      <ul class="footer-items list-unstyled">
    <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/harvard-edge/cs249r_book">
      <i class="bi bi-github" role="img" aria-label="View source on GitHub">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/harvard-edge/cs249r_book">
      <i class="bi bi-star" role="img" aria-label="Star this repository">
</i> 
    </a>
  </li>  
</ul>
    </div>
  </div>
</footer>
<script>var lightboxQuarto = GLightbox({"closeEffect":"zoom","descPosition":"bottom","loop":false,"openEffect":"zoom","selector":".lightbox"});
(function() {
  let previousOnload = window.onload;
  window.onload = () => {
    if (previousOnload) {
      previousOnload();
    }
    lightboxQuarto.on('slide_before_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      const href = trigger.getAttribute('href');
      if (href !== null) {
        const imgEl = window.document.querySelector(`a[href="${href}"] img`);
        if (imgEl !== null) {
          const srcAttr = imgEl.getAttribute("src");
          if (srcAttr && srcAttr.startsWith("data:")) {
            slideConfig.href = srcAttr;
          }
        }
      } 
    });
  
    lightboxQuarto.on('slide_after_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(slideNode);
      }
    });
  
  };
  
})();
          </script>




</body></html>