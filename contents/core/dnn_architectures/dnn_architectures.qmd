---
bibliography: dnn_architectures.bib
---

# DNN Architectures {#sec-dl_arch}

::: {.content-visible when-format="html"}
Resources: [Slides](#sec-deep-learning-primer-resource), [Videos](#sec-deep-learning-primer-resource), [Exercises](#sec-deep-learning-primer-resource)
:::

![_DALL·E 3 Prompt: A visually striking rectangular image illustrating the interplay between deep learning algorithms like CNNs, RNNs, and Attention Networks, interconnected with machine learning systems. The composition features neural network diagrams blending seamlessly with representations of computational systems such as processors, graphs, and data streams. Bright neon tones contrast against a dark futuristic background, symbolizing cutting-edge technology and intricate system complexity._](images/png/cover_dl_arch.png)

## Purpose {.unnumbered}

_What recurring patterns emerge across modern deep learning architectures, and how do these patterns enable systematic approaches to AI system design?_

Deep learning architectures represent a convergence of computational patterns that form the building blocks of modern AI systems. Understanding these foundational patterns—from convolutional structures to attention mechanisms—reveals how complex models arise from simple, repeatable components. The examination of these architectural elements provides insights into the systematic construction of flexible, efficient AI systems, establishing core principles that influence every aspect of system design and deployment. These structural insights illuminate the path toward creating scalable, adaptable solutions across diverse application domains.

::: {.callout-tip}

## Learning Objectives

* Map fundamental neural network concepts to deep learning architectures (dense, spatial, temporal, attention-based).

* Analyze how architectural patterns shape computational and memory demands.

* Evaluate system-level impacts of architectural choices on system attributes.

* Compare architectures' hardware mapping and identify optimization strategies.

* Assess trade-offs between complexity and system needs for specific applications.

:::

## Overview

Deep learning architecture stands for specific representation or organizations of neural network components---the neurons, weights, and connections (as introduced in @sec-chapter3)---arranged to efficiently process different types of patterns in data. While the previous chapter established the fundamental building blocks of neural networks, in this chapter we examine how these components are structured into architectures that map efficiently to computer systems.

These neural network architectures evolved to address specific pattern processing challenges. Whether processing arbitrary feature relationships, exploiting spatial patterns, managing temporal dependencies, or handling dynamic information flow, each architectural pattern emerged from particular computational needs. Understanding these architectures from a computer systems perspective requires examining how their computational patterns map to system resources.

Most often the architectures are discussed in terms of their algorithmic structures (MLPs, CNNs, RNNs, Transformers). However, in this chapter we take a more fundamental approach by examining how their computational patterns map to hardware resources. Each section analyzes how specific pattern processing needs influence algorithmic structure and how these structures map to computer system resources.

Understanding their implications for computer system design requires examining how their computational patterns map to hardware resources.

The mapping from algorithmic requirements to computer system design involves several key considerations:

1. Memory access patterns: How data moves through the memory hierarchy
2. Computation characteristics: The nature and organization of arithmetic operations
3. Data movement: Requirements for on-chip and off-chip data transfer
4. Resource utilization: How computational and memory resources are allocated

For example, dense connectivity patterns generate different memory bandwidth demands than localized processing structures. Similarly, stateful processing creates distinct requirements for on-chip memory organization compared to stateless operations. Getting a firm grasph on these mappings is important for modern computer architects and system designers who must implement these algorithms efficiently in hardware.

## Multi-Layer Perceptrons: Dense Pattern Processing

Multi-Layer Perceptrons (MLPs) represent the most direct extension of neural networks into deep architectures. Unlike more specialized networks, MLPs process each input element with equal importance, making them versatile but computationally intensive. Their architecture, while simple, establishes fundamental computational patterns that appear throughout deep learning systems.

When applied to the MNIST handwritten digit recognition challenge, an MLP reveals its computational power by transforming a complex 28×28 pixel image into a precise digit classification. By treating each of the 784 pixels as an equally weighted input, the network learns to decompose visual information through a systematic progression of layers, converting raw pixel intensities into increasingly abstract representations that capture the essential characteristics of handwritten digits.

### Pattern Processing Need

Deep learning systems frequently encounter problems where any input feature could potentially influence any output - there are no inherent constraints on these relationships. Consider analyzing financial market data: any economic indicator might affect any market outcome or in natural language processing, where the meaning of a word could depend on any other word in the sentence. These scenarios demand an architectural pattern capable of learning arbitrary relationships across all input features.

Dense pattern processing addresses this fundamental need by enabling several key capabilities. First, it allows unrestricted feature interactions where each output can depend on any combination of inputs. Second, it facilitates learned feature importance, allowing the system to determine which connections matter rather than having them prescribed. Finally, it provides adaptive representation, enabling the network to reshape its internal representations based on the data.

For example, in the MNIST digit recognition task, while humans might focus on specific parts of digits (like loops in '6' or crossings in '8'), we cannot definitively say which pixel combinations are crucial for classification. A '7' written with a serif could share pixel patterns with a '2', while variations in handwriting mean discriminative features might appear anywhere in the image. This uncertainty about feature relationships necessitates a dense processing approach where every pixel can potentially influence the classification decision.

The need for processing arbitrary relationships, however, comes with significant computational implications. When every output potentially depends on every input, the system must:

* Access all input values for each computation
* Store weights for all possible connections
* Compute across all these connections
* Move data between all elements of the network

These requirements directly influence how we structure both algorithms and computer systems to handle dense pattern processing efficiently.

### Algorithmic Structure

To enable unrestricted feature interactions, MLPs implement a direct algorithmic solution: connect everything to everything. This is realized through a series of fully-connected layers, where each neuron connects to every neuron in adjacent layers. This structure not only enables unrestricted feature interactions but also leverages a fundamental theoretical result: the Universal Approximation Theorem (UAT) [@cybenko1989approximation]. The UAT states that a sufficiently large MLP with non-linear activation functions can approximate any continuous function on a compact domain, given suitable weights and biases [hornik1989multilayer]. This highlights why dense architectures like MLPs are particularly well-suited for tasks requiring complex feature transformations and interactions.

The dense connectivity pattern translates mathematically into matrix multiplication operations. As shown in @fig-mlp, each layer transforms its input through matrix multiplication followed by element-wise activation:

![MLP layers and its associated matrix representation. Source: @reagen2017deep](images/png/mlp_mm.png){#fig-mlp}

$$
\mathbf{h}^{(l)} = f(\mathbf{W}^{(l)}\mathbf{h}^{(l-1)} + \mathbf{b}^{(l)})
$$

The dimensions of these operations reveal the computational scale of dense pattern processing:

* Input vector: $\mathbf{h}^{(0)} \in \mathbb{R}^{d_{in}}$ represents all potential input features
* Weight matrices: $\mathbf{W}^{(l)} \in \mathbb{R}^{d_{out} \times d_{in}}$ capture all possible input-output relationships
* Output vector: $\mathbf{h}^{(l)} \in \mathbb{R}^{d_{out}}$ produces transformed representations

In the MNIST example, this means:

* Each 784-dimensional input ($28 \times 28$ pixels) connects to every neuron in the first hidden layer
* A hidden layer with 100 neurons requires a 784 × 100 weight matrix
* Each weight in this matrix represents a learnable relationship between an input pixel and a hidden feature

This algorithmic structure directly addresses our need for arbitrary feature relationships but creates specific computational patterns that must be handled efficiently by computer systems.

### Computational Mapping

The elegant mathematical representation of dense matrix multiplication maps to specific computational patterns that systems must handle. Let's examine how this mapping progresses from mathematical abstraction to computational reality.

The fundamental computation in MLPs is dense matrix multiplication, where each output depends on every input element. When processing multiple inputs simultaneously through batch processing, we can express this mathematically as:

$$
\mathbf{H}^{(l)} = f(\mathbf{H}^{(l-1)}\mathbf{W}^{(l)} + \mathbf{b}^{(l)})
$$

where $\mathbf{H}^{(l)} \in \mathbb{R}^{B \times d_{out}}$ for batch size B.

While this mathematical view is elegant, its actual implementation reveals a more detailed computational reality. Let's examine two different implementations of the same computation - one that mirrors the mathematical abstraction and one that exposes the core computational pattern:

```{.python}
# Mathematical abstraction in code
def mlp_layer_matrix(X, W, b):
    # X: input matrix (batch_size × num_inputs)
    # W: weight matrix (num_inputs × num_outputs)
    # b: bias vector (num_outputs)
    H = activation(matmul(X, W) + b)    # One clean line of math
    return H
```

The first implementation, `mlp_layer_matrix`, directly mirrors our mathematical equation. It uses high-level matrix operations (`matmul`) to express the computation in a single line, hiding the underlying complexity. This is the style commonly used in deep learning frameworks, where optimized libraries handle the actual computation.

```{.python}
# Core computational pattern
def mlp_layer_compute(X, W, b):
    # Process each sample in the batch
    for batch in range(batch_size):
        # Compute each output neuron
        for out in range(num_outputs):
            # Initialize with bias
            Z[batch,out] = b[out]
            # Accumulate weighted inputs
            for in_ in range(num_inputs):
                Z[batch,out] += X[batch,in_] * W[in_,out]
    
    H = activation(Z)
    return H
```

The second implementation, `mlp_layer_compute`, exposes the actual computational pattern through nested loops. This version shows us what really happens when we compute a layer's output: we process each sample in the batch, computing each output neuron by accumulating weighted contributions from all inputs.

This translation from mathematical abstraction to concrete computation exposes how dense matrix multiplication decomposes into nested loops of simpler operations. The outer loop processes each sample in the batch, while the middle loop computes values for each output neuron. Within the innermost loop, the system performs repeated multiply-accumulate operations, combining each input with its corresponding weight.

In the MNIST example, each output neuron requires 784 multiply-accumulate operations and at least 1,568 memory accesses (784 for inputs, 784 for weights). While actual implementations use sophisticated optimizations through libraries like [BLAS](https://www.netlib.org/blas/) or [cuBLAS](https://developer.nvidia.com/cublas) (which we'll explore in Chapter 8 on AI Training and Chapter 11 on AI Acceleration), these fundamental patterns drive key system design decisions.

The computational mapping reveals several critical patterns that influence system design:

1. Each output depends on every input, creating an all-to-all communication pattern
2. Memory access is extensive and regular, with complete rows and columns being accessed
3. The basic operation (multiply-accumulate) repeats many times with different data
4. Computation can be parallelized across batches and output neurons

These patterns create both challenges in implementation and opportunities for optimization, which we'll examine in the next [section](#system-implications)

### System Implications

When analyzing how computational patterns impact computer systems, we examine three fundamental dimensions: memory requirements, computation needs, and data movement. This framework enables systematic analysis of how algorithmic patterns influence system design decisions.

#### Memory Requirements

For dense pattern processing, the memory requirements stem from storing and accessing weights, inputs, and intermediate results. In our MNIST example, connecting our 784-dimensional input layer to a hidden layer of 100 neurons requires 78,400 weight parameters. Each forward pass must access all these weights, along with input data and intermediate results. The all-to-all connectivity pattern means there's no inherent locality in these accesses—every output needs every input and its corresponding weights.

These memory access patterns suggest opportunities for optimization through careful data organization and reuse. Modern processors handle these patterns differently - CPUs leverage their cache hierarchy for data reuse, while GPUs employ specialized memory hierarchies designed for high-bandwidth access. Deep learning frameworks abstract these hardware-specific details through optimized matrix multiplication implementations.

#### Computation Needs

The core computation revolves around multiply-accumulate operations arranged in nested loops. Each output value requires as many multiply-accumulates as there are inputs. For MNIST, this means 784 multiply-accumulates per output neuron. With 100 neurons in our hidden layer, we're performing 78,400 multiply-accumulates for a single input image. While these operations are simple, their volume and arrangement create specific demands on processing resources.

#### Data Movement

The all-to-all connectivity pattern in MLPs creates significant data movement requirements. Each multiply-accumulate operation needs three pieces of data: an input value, a weight value, and the running sum. For our MNIST example layer, computing a single output value requires moving 784 inputs and 784 weights to wherever the computation occurs. This movement pattern repeats for each of the 100 output neurons, creating substantial data transfer demands between memory and compute units.

The predictable nature of these data movement patterns enables strategic data staging and transfer optimizations. Different architectures address this challenge through various mechanisms - CPUs use sophisticated prefetching and multi-level caches, while GPUs employ high-bandwidth memory systems and latency hiding through massive threading. Deep learning frameworks orchestrate these data movements through optimized memory management systems.

The analysis of MLPs demonstrates how dense pattern processing translates into specific computational patterns and system implications. These characteristics - high memory bandwidth requirements, intensive multiply-accumulate computations, and significant data movement needs - establish a baseline for understanding neural network implementations in computer systems. As we examine other architectural patterns in subsequent sections, this same analytical framework will reveal how different approaches to pattern processing create their own unique demands on system resources.

### Summary and Next Steps

The analysis of MLPs demonstrates how dense pattern processing translates into specific computational patterns and system implications. These characteristics - high memory bandwidth requirements, intensive multiply-accumulate computations, and significant data movement needs - establish a baseline for understanding neural network implementations in computer systems. As we examine other architectural patterns in subsequent sections, this same analytical framework will reveal how different approaches to pattern processing create their own unique demands on system resources.

:::{.callout-caution #exr-mlp collapse="false"}

#### Multilayer Perceptrons (MLPs)

We've just scratched the surface of neural networks. Now, you'll get to try and apply these concepts in practical examples. In the provided Colab notebooks, you'll explore:

**Predicting house prices:** Learn how neural networks can analyze housing data to estimate property values.
[![](https://colab.research.google.com/assets/colab-badge.png)](https://colab.research.google.com/github/Mjrovai/UNIFEI-IESTI01-TinyML-2022.1/blob/main/00_Curse_Folder/1_Fundamentals/Class_07/TF_Boston_Housing_Regression.ipynb)

**Image Classification:** Discover how to build a network to understand the famous MNIST handwritten digit dataset.
[![](https://colab.research.google.com/assets/colab-badge.png)](https://colab.research.google.com/github/Mjrovai/UNIFEI-IESTI01-TinyML-2022.1/blob/main/00_Curse_Folder/1_Fundamentals/Class_09/TF_MNIST_Classification_v2.ipynb)

**Real-world medical diagnosis:** Use deep learning to tackle the important task of breast cancer classification.
[![](https://colab.research.google.com/assets/colab-badge.png)](https://colab.research.google.com/github/Mjrovai/UNIFEI-IESTI01-TinyML-2022.1/blob/main/00_Curse_Folder/1_Fundamentals/Class_13/docs/WDBC_Project/Breast_Cancer_Classification.ipynb)

:::

## Convolutional Neural Networks: Spatial Pattern Processing

While MLPs treat each input element independently, many real-world data types exhibit strong spatial relationships. Images, for example, derive their meaning from the spatial arrangement of pixels—a pattern of edges and textures that form recognizable objects. Audio signals show temporal patterns of frequency components, and sensor data often contains spatial or temporal correlations. These spatial relationships suggest that treating every input-output connection with equal importance, as MLPs do, might not be the most effective approach.

### Pattern Processing Need

Spatial pattern processing addresses scenarios where the relationship between data points depends on their relative positions or proximity. Consider processing a natural image: a pixel's relationship with its neighbors is crucial for detecting edges, textures, and shapes. These local patterns then combine hierarchically to form more complex features—edges form shapes, shapes form objects, and objects form scenes.

This hierarchical spatial pattern processing appears across many domains. In computer vision, local pixel patterns form edges and textures that combine into recognizable objects. Speech processing relies on patterns across nearby time segments to identify phonemes and words. Sensor networks analyze correlations between physically proximate sensors to understand environmental patterns. Medical imaging depends on recognizing tissue patterns that indicate biological structures.

Taking image processing as an example, if we want to detect a cat in an image, certain spatial patterns must be recognized: the triangular shape of ears, the round contours of the face, the texture of fur. Importantly, these patterns maintain their meaning regardless of where they appear in the image—a cat is still a cat whether it's in the top-left or bottom-right corner. This suggests two key requirements for spatial pattern processing: the ability to detect local patterns, and the ability to recognize these patterns regardless of their position.

These requirements create specific demands on our processing architecture. The system needs to support local connectivity to detect spatial patterns while enabling parameter sharing to recognize patterns independent of position. It must facilitate hierarchical processing to combine simple patterns into complex features, and efficiently handle shifting patterns across the input space. Unlike the dense connectivity of MLPs, spatial pattern processing suggests an architecture that explicitly encodes these spatial relationships while maintaining computational efficiency. This leads us to the convolutional neural network architecture, which we'll examine next.

### Algorithmic Structure

CNNs address spatial pattern processing through a fundamentally different connection pattern than MLPs. Instead of connecting every input to every output, CNNs use a local connection pattern where each output connects only to a small, spatially contiguous region of the input. This local receptive field moves across the input space, applying the same set of weights at each position—a process known as convolution.

The core operation in a CNN can be expressed mathematically as:

$$
\mathbf{H}^{(l)}_{i,j,k} = f(\sum_{di}\sum_{dj}\sum_{c} \mathbf{W}^{(l)}_{di,dj,c,k}\mathbf{H}^{(l-1)}_{i+di,j+dj,c} + \mathbf{b}^{(l)}_k)
$$

Here, $(i,j)$ represents spatial positions, $k$ indexes output channels, $c$ indexes input channels, and $(di,dj)$ spans the local receptive field. Unlike the dense matrix multiplication of MLPs, this operation:

- Processes local neighborhoods (typically 3×3 or 5×5)
- Reuses the same weights at each spatial position
- Maintains spatial structure in its output

For a concrete example, consider our MNIST digit classification task with 28×28 grayscale images. Each convolutional layer applies a set of filters (say 3×3) that slide across the image, computing local weighted sums. If we use 32 filters, the layer produces a 28×28×32 output, where each spatial position contains 32 different feature measurements of its local neighborhood. This is in stark contrast to our MLP approach where we flattened the entire image into a 784-dimensional vector.

This algorithmic structure directly implements the requirements we identified for spatial pattern processing, creating distinct computational patterns that influence system design.

::: {.content-visible when-format="html"}
![Convolution operation, image data (blue) and 3x3 filter (green). Source: V. Dumoulin, F. Visin, MIT](images/gif/cnn.gif){#fig-cnn}
:::

::: {.content-visible when-format="pdf"}
![Convolution operation, image data (blue) and 3x3 filter (green). Source: V. Dumoulin, F. Visin, MIT](images/png/cnn.png){#fig-cnn}
:::

### Computational Mapping

The elegant spatial structure of convolution operations maps to computational patterns quite different from the dense matrix multiplication of MLPs. Let's examine how this mapping progresses from mathematical abstraction to computational reality.

The fundamental computation in CNNs is the sliding window convolution, where each output value depends only on a local region of the input. For a basic forward pass through a convolutional layer, we can express this mathematically as:

$$
\mathbf{H}^{(l)}_{i,j,k} = f(\sum_{di}\sum_{dj}\sum_{c} \mathbf{W}^{(l)}_{di,dj,c,k}\mathbf{H}^{(l-1)}_{i+di,j+dj,c} + \mathbf{b}^{(l)}_k)
$$

While this mathematical view is elegant, its actual implementation reveals a more detailed computational reality. Let's examine two different implementations of the same computation - one that mirrors the mathematical abstraction and one that exposes the core computational pattern:

```{.python}
# Mathematical abstraction - simple and clean
def conv_layer_math(input, kernel, bias):
    output = convolution(input, kernel) + bias
    return activation(output)

# System reality - nested loops of computation
def conv_layer_compute(input, kernel, bias):
    # Loop 1: Process each image in batch
    for image in range(batch_size):
        
        # Loop 2&3: Move across image spatially
        for y in range(height):
            for x in range(width):
                
                # Loop 4: Compute each output feature
                for out_channel in range(num_output_channels):
                    result = bias[out_channel]
                    
                    # Loop 5&6: Move across kernel window
                    for ky in range(kernel_height):
                        for kx in range(kernel_width):
                            
                            # Loop 7: Process each input feature
                            for in_channel in range(num_input_channels):
                                # Get input value from correct window position
                                in_y = y + ky  
                                in_x = x + kx
                                # Perform multiply-accumulate operation
                                result += input[image, in_y, in_x, in_channel] * \
                                         kernel[ky, kx, in_channel, out_channel]
                    
                    # Store result for this output position
                    output[image, y, x, out_channel] = result
```

The first implementation, `conv_layer_spatial`, uses high-level convolution operations to express the computation concisely. This is typical in deep learning frameworks, where optimized libraries handle the underlying complexity. The second implementation, `conv_layer_compute`, reveals the actual computational pattern: nested loops that process each spatial position, applying the same filter weights to local regions of the input.

The nested loops in `conv_layer_compute` reveal the true nature of convolution's computational pattern. The seven nested loops reveal different aspects of the computation:

* Outer loops (1-3) manage position: which image and where in the image
* Middle loop (4) handles output features: computing different learned patterns
* Inner loops (5-7) perform the actual convolution: sliding the kernel window

Let's take a closer look. The outer two loops (`for y` and `for x`) traverse each spatial position in the output feature map - for our MNIST example, this means moving across all 28×28 positions. At each position, we compute values for each output channel (`for k` loop), which represents different learned features or patterns - our 32 different feature detectors.

The inner three loops implement the actual convolution operation at each position. For each output value, we process a local 3×3 region of the input (the `dy` and `dx` loops) across all input channels (`for c` loop). This creates a sliding window effect, where the same 3×3 filter moves across the image, performing multiply-accumulates between the filter weights and the local input values. Unlike the MLP's global connectivity, this local processing pattern means each output value depends only on a small neighborhood of the input.

For our MNIST example with 3×3 filters and 32 output channels, each output position requires only 9 multiply-accumulate operations per input channel, compared to the 784 operations needed in our MLP layer. However, this operation must be repeated for every spatial position (28×28) and every output channel (32). While using fewer operations per output, the spatial structure creates different patterns of memory access and computation that systems must handle efficiently. These patterns fundamentally influence system design, creating both challenges and opportunities for optimization, which we'll examine next.

### System Implications

When analyzing how computational patterns impact computer systems, we examine three fundamental dimensions: memory requirements, computation needs, and data movement. For CNNs, the spatial nature of processing creates distinctive patterns in each dimension that differ significantly from the dense connectivity of MLPs.

#### Memory Requirements

For convolutional layers, memory requirements center around two key components: filter weights and feature maps. Unlike MLPs that require storing full connection matrices, CNNs use small, reusable filters. In our MNIST example, a convolutional layer with 32 filters of size 3×3 requires storing only 288 weight parameters (3×3×32), in contrast to the 78,400 weights needed for our MLP's fully-connected layer. However, the system must store feature maps for all spatial positions, creating a different memory demand—a 28×28 input with 32 output channels requires storing 25,088 activation values (28×28×32).

These memory access patterns suggest opportunities for optimization through weight reuse and careful feature map management. Modern processors handle these patterns by caching filter weights, which are reused across spatial positions, while streaming through feature map data. Deep learning frameworks typically implement this through specialized memory layouts that optimize for both filter reuse and spatial locality in feature map access. CPUs and GPUs approach this differently—CPUs leverage their cache hierarchy to keep frequently used filters resident, while GPUs use specialized memory architectures designed for the spatial access patterns of image processing.

#### Computation Needs

The core computation in CNNs involves repeatedly applying small filters across spatial positions. Each output value requires a local multiply-accumulate operation over the filter region. For our MNIST example with 3×3 filters and 32 output channels, computing one spatial position involves 288 multiply-accumulates (3×3×32), and this must be repeated for all 784 spatial positions (28×28). While each individual computation involves fewer operations than an MLP layer, the total computational load remains substantial due to spatial repetition.

This computational pattern presents different optimization opportunities than MLPs. The regular, repeated nature of convolution operations enables efficient hardware utilization through structured parallelism. Modern processors exploit this pattern in various ways. CPUs leverage SIMD instructions to process multiple filter positions simultaneously, while GPUs parallelize computation across spatial positions and channels. Deep learning frameworks further optimize this through specialized convolution algorithms that transform the computation to better match hardware capabilities.

#### Data Movement

The sliding window pattern of convolutions creates a distinctive data movement profile. Unlike MLPs where each weight is used once per forward pass, CNN filter weights are reused many times as the filter slides across spatial positions. For our MNIST example, each 3×3 filter weight is reused 784 times (once for each position in the 28×28 feature map). However, this creates a different challenge: the system must stream input features through the computation unit while keeping filter weights stable.

The predictable spatial access pattern enables strategic data movement optimizations. Different architectures handle this movement pattern through specialized mechanisms. CPUs maintain frequently used filter weights in cache while streaming through input features. GPUs employ memory architectures optimized for spatial locality and provide hardware support for efficient sliding window operations. Deep learning frameworks orchestrate these movements by organizing computations to maximize filter weight reuse and minimize redundant feature map accesses.

### Summary and Next Steps

The analysis of CNNs reveals how spatial pattern processing creates fundamentally different computational patterns from the dense connectivity of MLPs. While MLPs require high memory bandwidth for large weight matrices, CNNs benefit from weight reuse but must manage streaming access to feature maps. The computational pattern shifts from large matrix multiplications to repeated local operations, creating new opportunities for parallel execution. Data movement focuses on filter reuse and efficient handling of sliding window operations rather than managing all-to-all connectivity.

These characteristics demonstrate why specialized hardware accelerators and optimized software frameworks have evolved to handle CNNs efficiently. As we examine more specialized architectures in subsequent sections, such as recurrent neural networks for sequential processing and transformers for dynamic information flow, we will see how different pattern processing needs create their own unique demands on computer systems.

## Recurrent Neural Networks: Sequential Pattern Processing

While MLPs handle arbitrary relationships and CNNs process spatial patterns, many real-world problems involve sequential data where the order and relationship between elements over time matters. Text processing requires understanding how words relate to previous context, speech recognition needs to track how sounds form coherent patterns, and time-series analysis must capture how values evolve over time. These sequential relationships suggest that treating each time step independently misses crucial temporal patterns.

### Pattern Processing Need

Sequential pattern processing addresses scenarios where the meaning of current input depends on what came before it. Consider natural language processing: the meaning of a word often depends heavily on previous words in the sentence. The word "bank" means something different in "river bank" versus "bank account." Similarly, in speech recognition, a phoneme's interpretation often depends on surrounding sounds, and in financial forecasting, future predictions require understanding patterns in historical data.

The key challenge in sequential processing is maintaining and updating relevant context over time. When reading text, humans don't start fresh with each word—we maintain a running understanding that evolves as we process new information. Similarly, when processing time-series data, patterns might span different timescales, from immediate dependencies to long-term trends. This suggests we need an architecture that can both maintain state over time and update it based on new inputs.

These requirements demand specific capabilities from our processing architecture. The system must maintain internal state to capture temporal context, update this state based on new inputs, and learn which historical information is relevant for current predictions. Unlike MLPs and CNNs, which process fixed-size inputs, sequential processing must handle variable-length sequences while maintaining computational efficiency. This leads us to the recurrent neural network architecture.

### Algorithmic Structure

RNNs address sequential processing through a fundamentally different approach than MLPs or CNNs by introducing recurrent connections. Instead of just mapping inputs to outputs, RNNs maintain an internal state that is updated at each time step. This creates a memory mechanism that allows the network to carry information forward in time.

The core operation in a basic RNN can be expressed mathematically as:

$$
\mathbf{h}_t = f(\mathbf{W}_{hh}\mathbf{h}_{t-1} + \mathbf{W}_{xh}\mathbf{x}_t + \mathbf{b}_h)
$$

where $\mathbf{h}_t$ represents the hidden state at time $t$, $\mathbf{x}_t$ is the input at time $t$, $\mathbf{W}_{hh}$ contains the recurrent weights, and $\mathbf{W}_{xh}$ contains the input weights.

For example, in processing a sequence of words, each word might be represented as a 100-dimensional vector ($\mathbf{x}_t$), and we might maintain a hidden state of 128 dimensions ($\mathbf{h}_t$). At each time step, the network combines the current input with its previous state to update its understanding of the sequence. This creates a form of memory that can capture patterns across time steps.

This recurrent structure directly implements our requirements for sequential processing, but its sequential nature creates distinct computational patterns that significantly influence system design.

### Computational Mapping

The sequential structure of RNNs maps to computational patterns quite different from both MLPs and CNNs. Let's examine how this mapping progresses from mathematical abstraction to computational reality.

The fundamental computation in RNNs involves both the current input and the previous state. For a basic forward pass through an RNN layer, we can express this mathematically as:

$$
\mathbf{h}_t = f(\mathbf{W}_{hh}\mathbf{h}_{t-1} + \mathbf{W}_{xh}\mathbf{x}_t + \mathbf{b}_h)
$$

While this mathematical view is elegant, its actual implementation reveals a more detailed computational reality. Let's examine two different implementations of the same computation:

```python
# Mathematical abstraction in code
def rnn_layer_step(x_t, h_prev, W_hh, W_xh, b):
    # x_t: input at time t (batch_size × input_dim)
    # h_prev: previous hidden state (batch_size × hidden_dim)
    # W_hh: recurrent weights (hidden_dim × hidden_dim)
    # W_xh: input weights (input_dim × hidden_dim)
    h_t = activation(matmul(h_prev, W_hh) + matmul(x_t, W_xh) + b)
    return h_t

# Core computational pattern
def rnn_layer_compute(x_t, h_prev, W_hh, W_xh, b):
    # Initialize next hidden state
    h_t = np.zeros_like(h_prev)
    
    # Process each element in the batch
    for batch in range(batch_size):
        # First compute recurrent contribution
        for i in range(hidden_dim):
            for j in range(hidden_dim):
                h_t[batch,i] += h_prev[batch,j] * W_hh[j,i]
                
        # Then add input contribution
        for i in range(hidden_dim):
            for j in range(input_dim):
                h_t[batch,i] += x_t[batch,j] * W_xh[j,i]
                
        # Add bias and apply activation
        for i in range(hidden_dim):
            h_t[batch,i] = activation(h_t[batch,i] + b[i])
    
    return h_t
```

The `rnn_layer_compute` implementation reveals the true nature of RNN computation. Unlike MLPs or CNNs where computations can be highly parallel, RNNs have an inherent sequential dependency - each time step's computation requires the result from the previous step. The inner loops show two main computational components: processing the previous hidden state and incorporating the new input. This creates a distinctive pattern where state must be maintained and updated sequentially.

For a sequence processing task with input dimension 100 and hidden state dimension 128, each time step requires two matrix multiplications: one 128×128 for the recurrent connection and one 100×128 for the input projection. While individual time steps can process in parallel across batch elements, the time steps themselves must process sequentially. This creates a unique computational pattern that systems must handle efficiently.

### Computational Mapping

The sequential structure of RNNs maps to computational patterns quite different from both MLPs and CNNs. Let's examine how this mapping progresses from mathematical abstraction to computational reality.

The fundamental computation in RNNs involves both the current input and the previous state. For a basic forward pass through an RNN layer, we can express this mathematically as:

$$
\mathbf{h}_t = f(\mathbf{W}_{hh}\mathbf{h}_{t-1} + \mathbf{W}_{xh}\mathbf{x}_t + \mathbf{b}_h)
$$

While this mathematical view is elegant, let's take a look at its code. Let's examine two different implementations of the same computation:

```{.python}
# Mathematical abstraction in code
def rnn_layer_step(x_t, h_prev, W_hh, W_xh, b):
    # x_t: input at time t (batch_size × input_dim)
    # h_prev: previous hidden state (batch_size × hidden_dim)
    # W_hh: recurrent weights (hidden_dim × hidden_dim)
    # W_xh: input weights (input_dim × hidden_dim)
    h_t = activation(matmul(h_prev, W_hh) + matmul(x_t, W_xh) + b)
    return h_t
```

The `rnn_layer_step` function shows how the operation looks when using high-level matrix operations found in deep learning frameworks. It handles a single time step, taking the current input `x_t` and previous hidden state `h_prev`, along with two weight matrices: `W_hh` for hidden-to-hidden connections and `W_xh` for input-to-hidden connections. Through matrix multiplication operations (`matmul`), it merges the previous state and current input to generate the next hidden state. 

This simplified view masks the underlying complexity of the nested loops and individual computations shown in the detailed implementation. Its actual implementation reveals a more detailed computational reality:

```{.python}
# Core computational pattern
def rnn_layer_compute(x_t, h_prev, W_hh, W_xh, b):
    # Initialize next hidden state
    h_t = np.zeros_like(h_prev)
    
    # Loop 1: Process each sequence in the batch
    for batch in range(batch_size):
        # Loop 2: Compute recurrent contribution (h_prev × W_hh)
        for i in range(hidden_dim):
            for j in range(hidden_dim):
                h_t[batch,i] += h_prev[batch,j] * W_hh[j,i]
                
        # Loop 3: Compute input contribution (x_t × W_xh)
        for i in range(hidden_dim):
            for j in range(input_dim):
                h_t[batch,i] += x_t[batch,j] * W_xh[j,i]
                
        # Loop 4: Add bias and apply activation
        for i in range(hidden_dim):
            h_t[batch,i] = activation(h_t[batch,i] + b[i])
    
    return h_t
```

The nested loops in `rnn_layer_compute` expose the core computational pattern of RNNs. Loop 1 processes each sequence in the batch independently, allowing for batch-level parallelism. Within each batch item, Loop 2 computes how the previous hidden state influences the next state through the recurrent weights `W_hh`. Loop 3 then incorporates new information from the current input through the input weights `W_xh`. Finally, Loop 4 adds biases and applies the activation function to produce the new hidden state.

For a sequence processing task with input dimension 100 and hidden state dimension 128, each time step requires two matrix multiplications: one 128×128 for the recurrent connection and one 100×128 for the input projection. While individual time steps can process in parallel across batch elements, the time steps themselves must process sequentially. This creates a unique computational pattern that systems must handle efficiently.

### System Implications

We examine the three dimensions: memory requirements, computation needs, and data movement. For RNNs, the sequential nature of processing creates distinctive patterns in each dimension that differ significantly from both MLPs and CNNs.

#### Memory Requirements

RNNs require storing two sets of weights (input-to-hidden and hidden-to-hidden) along with the hidden state. For our example with input dimension 100 and hidden state dimension 128, this means storing 12,800 weights for input projection (100×128) and 16,384 weights for recurrent connections (128×128). Unlike CNNs where weights are reused across spatial positions, RNN weights are reused across time steps. Additionally, the system must maintain the hidden state, which becomes a critical factor in memory usage and access patterns.

These memory access patterns create a different profile from MLPs and CNNs. Modern processors handle these patterns by keeping the weight matrices in cache while streaming through sequence elements. Deep learning frameworks optimize memory access by batching sequences together and carefully managing hidden state storage between time steps. CPUs and GPUs approach this through different strategies - CPUs leverage their cache hierarchy for weight reuse, while GPUs use specialized memory architectures designed for maintaining state across sequential operations.

#### Computation Needs

The core computation in RNNs involves repeatedly applying weight matrices across time steps. For each time step, we perform two matrix multiplications: one with the input weights and one with the recurrent weights. In our example, processing a single time step requires 12,800 multiply-accumulates for the input projection (100×128) and 16,384 multiply-accumulates for the recurrent connection (128×128). 

This computational pattern differs from both MLPs and CNNs in a key way: while we can parallelize across batch elements, we cannot parallelize across time steps due to the sequential dependency. Each time step must wait for the previous step's hidden state before it can begin computation. This creates a tension between the inherent sequential nature of the algorithm and the desire for parallel execution in modern hardware.

Modern processors handle these patterns through different approaches. CPUs pipeline operations within each time step while maintaining the sequential order across steps. GPUs batch multiple sequences together to maintain high throughput despite sequential dependencies. Deep learning frameworks optimize this further by techniques like sequence packing and unrolling computations across multiple time steps when possible.

#### Data Movement

The sequential processing in RNNs creates a distinctive data movement pattern that differs from both MLPs and CNNs. While MLPs need each weight only once per forward pass and CNNs reuse weights across spatial positions, RNNs reuse their weights across time steps while requiring careful management of the hidden state data flow.

For our example with a 128-dimensional hidden state, each time step must: load the previous hidden state (128 values), access both weight matrices (29,184 total weights from both input and recurrent connections), and store the new hidden state (128 values). This pattern repeats for every element in the sequence. Unlike CNNs where we can predict and prefetch data based on spatial patterns, RNN data movement is driven by temporal dependencies.

Different architectures handle this sequential data movement through specialized mechanisms. CPUs maintain weight matrices in cache while streaming through sequence elements and managing hidden state updates. GPUs employ memory architectures optimized for maintaining state information across sequential operations while processing multiple sequences in parallel. Deep learning frameworks orchestrate these movements by managing data transfers between time steps and optimizing batch operations.

### Summary and Next Steps

The analysis of RNNs demonstrates how sequential pattern processing creates fundamentally different computational patterns from both the dense connectivity of MLPs and the spatial operations of CNNs. While MLPs process all inputs simultaneously and CNNs reuse weights across spatial positions, RNNs must handle temporal dependencies that create inherent sequential processing requirements. This sequential nature manifests in distinct system demands: memory systems must manage both weight reuse across time steps and hidden state updates, computation must balance sequential dependencies with parallel execution, and data movement centers around maintaining and updating state information efficiently.

These characteristics illustrate why different optimization strategies have evolved for RNN processing, and why certain applications began shifting toward alternative architectures like attention mechanisms, which we'll examine next. As we explore these newer architectural patterns, we'll see how they address some of the fundamental challenges of sequential processing while creating their own unique demands on computer systems.
