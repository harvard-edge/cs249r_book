---
bibliography: dnn_architectures.bib
---

# DNN Architectures {#sec-dl_arch}

::: {.content-visible when-format="html"}
Resources: [Slides](#sec-deep-learning-primer-resource), [Videos](#sec-deep-learning-primer-resource), [Exercises](#sec-deep-learning-primer-resource)
:::

![_DALL·E 3 Prompt: A visually striking rectangular image illustrating the interplay between deep learning algorithms like CNNs, RNNs, and Attention Networks, interconnected with machine learning systems. The composition features neural network diagrams blending seamlessly with representations of computational systems such as processors, graphs, and data streams. Bright neon tones contrast against a dark futuristic background, symbolizing cutting-edge technology and intricate system complexity._](images/png/cover_dl_arch.png)

## Purpose {.unnumbered}

_What recurring patterns emerge across modern deep learning architectures, and how do these patterns enable systematic approaches to AI system design?_

Deep learning architectures represent a convergence of computational patterns that form the building blocks of modern AI systems. Understanding these foundational patterns—from convolutional structures to attention mechanisms—reveals how complex models arise from simple, repeatable components. The examination of these architectural elements provides insights into the systematic construction of flexible, efficient AI systems, establishing core principles that influence every aspect of system design and deployment. These structural insights illuminate the path toward creating scalable, adaptable solutions across diverse application domains.

::: {.callout-tip}

## Learning Objectives

* Map fundamental neural network concepts to deep learning architectures (dense, spatial, temporal, attention-based).

* Analyze how architectural patterns shape computational and memory demands.

* Evaluate system-level impacts of architectural choices on system attributes.

* Compare architectures' hardware mapping and identify optimization strategies.

* Assess trade-offs between complexity and system needs for specific applications.

:::

## Overview

Deep learning architecture stands for specific representation or organizations of neural network components---the neurons, weights, and connections (as introduced in @sec-chapter3)---arranged to efficiently process different types of patterns in data. While the previous chapter established the fundamental building blocks of neural networks, in this chapter we examine how these components are structured into architectures that map efficiently to computer systems.

These neural network architectures evolved to address specific pattern processing challenges. Whether processing arbitrary feature relationships, exploiting spatial patterns, managing temporal dependencies, or handling dynamic information flow, each architectural pattern emerged from particular computational needs. Understanding these architectures from a computer systems perspective requires examining how their computational patterns map to system resources.

Most often the architectures are discussed in terms of their algorithmic structures (MLPs, CNNs, RNNs, Transformers). However, in this chapter we take a more fundamental approach by examining how their computational patterns map to hardware resources. Each section analyzes how specific pattern processing needs influence algorithmic structure and how these structures map to computer system resources.

Understanding their implications for computer system design requires examining how their computational patterns map to hardware resources.

The mapping from algorithmic requirements to computer system design involves several key considerations:

1. Memory access patterns: How data moves through the memory hierarchy
2. Computation characteristics: The nature and organization of arithmetic operations
3. Data movement: Requirements for on-chip and off-chip data transfer
4. Resource utilization: How computational and memory resources are allocated

For example, dense connectivity patterns generate different memory bandwidth demands than localized processing structures. Similarly, stateful processing creates distinct requirements for on-chip memory organization compared to stateless operations. Getting a firm grasph on these mappings is important for modern computer architects and system designers who must implement these algorithms efficiently in hardware.

## Multi-Layer Perceptrons: Dense Pattern Processing

Multi-Layer Perceptrons (MLPs) represent the most direct extension of neural networks into deep architectures. Unlike more specialized networks, MLPs process each input element with equal importance, making them versatile but computationally intensive. Their architecture, while simple, establishes fundamental computational patterns that appear throughout deep learning systems. These patterns were initially formalized by the introduction of the Universal Approximation Theorem (UAT) [@cybenko1989approximation; @hornik1989multilayer], which states that a sufficiently large MLP with non-linear activation functions can approximate any continuous function on a compact domain, given suitable weights and biases.


When applied to the MNIST handwritten digit recognition challenge, an MLP reveals its computational power by transforming a complex 28×28 pixel image into a precise digit classification. By treating each of the 784 pixels as an equally weighted input, the network learns to decompose visual information through a systematic progression of layers, converting raw pixel intensities into increasingly abstract representations that capture the essential characteristics of handwritten digits.

### Pattern Processing Need

Deep learning systems frequently encounter problems where any input feature could potentially influence any output - there are no inherent constraints on these relationships. Consider analyzing financial market data: any economic indicator might affect any market outcome or in natural language processing, where the meaning of a word could depend on any other word in the sentence. These scenarios demand an architectural pattern capable of learning arbitrary relationships across all input features.

Dense pattern processing addresses this fundamental need by enabling several key capabilities. First, it allows unrestricted feature interactions where each output can depend on any combination of inputs. Second, it facilitates learned feature importance, allowing the system to determine which connections matter rather than having them prescribed. Finally, it provides adaptive representation, enabling the network to reshape its internal representations based on the data.

For example, in the MNIST digit recognition task, while humans might focus on specific parts of digits (like loops in '6' or crossings in '8'), we cannot definitively say which pixel combinations are crucial for classification. A '7' written with a serif could share pixel patterns with a '2', while variations in handwriting mean discriminative features might appear anywhere in the image. This uncertainty about feature relationships necessitates a dense processing approach where every pixel can potentially influence the classification decision.

The need for processing arbitrary relationships, however, comes with significant computational implications. When every output potentially depends on every input, the system must:

* Access all input values for each computation
* Store weights for all possible connections
* Compute across all these connections
* Move data between all elements of the network

These requirements directly influence how we structure both algorithms and computer systems to handle dense pattern processing efficiently.

### Algorithmic Structure

To enable unrestricted feature interactions, MLPs implement a direct algorithmic solution: connect everything to everything. This is realized through a series of fully-connected layers, where each neuron connects to every neuron in adjacent layers. The dense connectivity pattern translates mathematically into matrix multiplication operations. As shown in @fig-mlp, each layer transforms its input through matrix multiplication followed by element-wise activation:

![MLP layers and its associated matrix representation. Source: @reagen2017deep](images/png/mlp_mm.png){#fig-mlp}

$$
\mathbf{h}^{(l)} = f(\mathbf{W}^{(l)}\mathbf{h}^{(l-1)} + \mathbf{b}^{(l)})
$$

The dimensions of these operations reveal the computational scale of dense pattern processing:

* Input vector: $\mathbf{h}^{(0)} \in \mathbb{R}^{d_{in}}$ represents all potential input features
* Weight matrices: $\mathbf{W}^{(l)} \in \mathbb{R}^{d_{out} \times d_{in}}$ capture all possible input-output relationships
* Output vector: $\mathbf{h}^{(l)} \in \mathbb{R}^{d_{out}}$ produces transformed representations

In the MNIST example, this means:

* Each 784-dimensional input ($28 \times 28$ pixels) connects to every neuron in the first hidden layer
* A hidden layer with 100 neurons requires a 784 × 100 weight matrix
* Each weight in this matrix represents a learnable relationship between an input pixel and a hidden feature

This algorithmic structure directly addresses our need for arbitrary feature relationships but creates specific computational patterns that must be handled efficiently by computer systems.

### Computational Mapping

The elegant mathematical representation of dense matrix multiplication maps to specific computational patterns that systems must handle. Let's examine how this mapping progresses from mathematical abstraction to computational reality.

The fundamental computation in MLPs is dense matrix multiplication, where each output depends on every input element. When processing multiple inputs simultaneously through batch processing, we can express this mathematically as:

$$
\mathbf{H}^{(l)} = f(\mathbf{H}^{(l-1)}\mathbf{W}^{(l)} + \mathbf{b}^{(l)})
$$

where $\mathbf{H}^{(l)} \in \mathbb{R}^{B \times d_{out}}$ for batch size B.

While this mathematical view is elegant, its actual implementation reveals a more detailed computational reality. Let's examine two different implementations of the same computation - one that mirrors the mathematical abstraction and one that exposes the core computational pattern:

```{.python}
# Mathematical abstraction in code
def mlp_layer_matrix(X, W, b):
    # X: input matrix (batch_size × num_inputs)
    # W: weight matrix (num_inputs × num_outputs)
    # b: bias vector (num_outputs)
    H = activation(matmul(X, W) + b)    # One clean line of math
    return H
```

The first implementation, `mlp_layer_matrix`, directly mirrors our mathematical equation. It uses high-level matrix operations (`matmul`) to express the computation in a single line, hiding the underlying complexity. This is the style commonly used in deep learning frameworks, where optimized libraries handle the actual computation.

```{.python}
# Core computational pattern
def mlp_layer_compute(X, W, b):
    # Process each sample in the batch
    for batch in range(batch_size):
        # Compute each output neuron
        for out in range(num_outputs):
            # Initialize with bias
            Z[batch,out] = b[out]
            # Accumulate weighted inputs
            for in_ in range(num_inputs):
                Z[batch,out] += X[batch,in_] * W[in_,out]
    
    H = activation(Z)
    return H
```

The second implementation, `mlp_layer_compute`, exposes the actual computational pattern through nested loops. This version shows us what really happens when we compute a layer's output: we process each sample in the batch, computing each output neuron by accumulating weighted contributions from all inputs.

This translation from mathematical abstraction to concrete computation exposes how dense matrix multiplication decomposes into nested loops of simpler operations. The outer loop processes each sample in the batch, while the middle loop computes values for each output neuron. Within the innermost loop, the system performs repeated multiply-accumulate operations, combining each input with its corresponding weight.

In the MNIST example, each output neuron requires 784 multiply-accumulate operations and at least 1,568 memory accesses (784 for inputs, 784 for weights). While actual implementations use sophisticated optimizations through libraries like [BLAS](https://www.netlib.org/blas/) or [cuBLAS](https://developer.nvidia.com/cublas) (which we'll explore in Chapter 8 on AI Training and Chapter 11 on AI Acceleration), these fundamental patterns drive key system design decisions.

The computational mapping reveals several critical patterns that influence system design:

1. Each output depends on every input, creating an all-to-all communication pattern
2. Memory access is extensive and regular, with complete rows and columns being accessed
3. The basic operation (multiply-accumulate) repeats many times with different data
4. Computation can be parallelized across batches and output neurons

These patterns create both challenges in implementation and opportunities for optimization, which we'll examine in the next [section](#system-implications)

### System Implications

When analyzing how computational patterns impact computer systems, we examine three fundamental dimensions: memory requirements, computation needs, and data movement. This framework enables systematic analysis of how algorithmic patterns influence system design decisions.

#### Memory Requirements

For dense pattern processing, the memory requirements stem from storing and accessing weights, inputs, and intermediate results. In our MNIST example, connecting our 784-dimensional input layer to a hidden layer of 100 neurons requires 78,400 weight parameters. Each forward pass must access all these weights, along with input data and intermediate results. The all-to-all connectivity pattern means there's no inherent locality in these accesses—every output needs every input and its corresponding weights.

These memory access patterns suggest opportunities for optimization through careful data organization and reuse. Modern processors handle these patterns differently - CPUs leverage their cache hierarchy for data reuse, while GPUs employ specialized memory hierarchies designed for high-bandwidth access. Deep learning frameworks abstract these hardware-specific details through optimized matrix multiplication implementations.

#### Computation Needs

The core computation revolves around multiply-accumulate operations arranged in nested loops. Each output value requires as many multiply-accumulates as there are inputs. For MNIST, this means 784 multiply-accumulates per output neuron. With 100 neurons in our hidden layer, we're performing 78,400 multiply-accumulates for a single input image. While these operations are simple, their volume and arrangement create specific demands on processing resources.

#### Data Movement

The all-to-all connectivity pattern in MLPs creates significant data movement requirements. Each multiply-accumulate operation needs three pieces of data: an input value, a weight value, and the running sum. For our MNIST example layer, computing a single output value requires moving 784 inputs and 784 weights to wherever the computation occurs. This movement pattern repeats for each of the 100 output neurons, creating substantial data transfer demands between memory and compute units.

The predictable nature of these data movement patterns enables strategic data staging and transfer optimizations. Different architectures address this challenge through various mechanisms - CPUs use sophisticated prefetching and multi-level caches, while GPUs employ high-bandwidth memory systems and latency hiding through massive threading. Deep learning frameworks orchestrate these data movements through optimized memory management systems.

The analysis of MLPs demonstrates how dense pattern processing translates into specific computational patterns and system implications. These characteristics - high memory bandwidth requirements, intensive multiply-accumulate computations, and significant data movement needs - establish a baseline for understanding neural network implementations in computer systems. As we examine other architectural patterns in subsequent sections, this same analytical framework will reveal how different approaches to pattern processing create their own unique demands on system resources.

### Summary and Next Steps

The analysis of MLPs demonstrates how dense pattern processing translates into specific computational patterns and system implications. These characteristics - high memory bandwidth requirements, intensive multiply-accumulate computations, and significant data movement needs - establish a baseline for understanding neural network implementations in computer systems. As we examine other architectural patterns in subsequent sections, this same analytical framework will reveal how different approaches to pattern processing create their own unique demands on system resources.

:::{.callout-caution #exr-mlp collapse="false"}

#### Multilayer Perceptrons (MLPs)

We've just scratched the surface of neural networks. Now, you'll get to try and apply these concepts in practical examples. In the provided Colab notebooks, you'll explore:

**Predicting house prices:** Learn how neural networks can analyze housing data to estimate property values.
[![](https://colab.research.google.com/assets/colab-badge.png)](https://colab.research.google.com/github/Mjrovai/UNIFEI-IESTI01-TinyML-2022.1/blob/main/00_Curse_Folder/1_Fundamentals/Class_07/TF_Boston_Housing_Regression.ipynb)

**Image Classification:** Discover how to build a network to understand the famous MNIST handwritten digit dataset.
[![](https://colab.research.google.com/assets/colab-badge.png)](https://colab.research.google.com/github/Mjrovai/UNIFEI-IESTI01-TinyML-2022.1/blob/main/00_Curse_Folder/1_Fundamentals/Class_09/TF_MNIST_Classification_v2.ipynb)

**Real-world medical diagnosis:** Use deep learning to tackle the important task of breast cancer classification.
[![](https://colab.research.google.com/assets/colab-badge.png)](https://colab.research.google.com/github/Mjrovai/UNIFEI-IESTI01-TinyML-2022.1/blob/main/00_Curse_Folder/1_Fundamentals/Class_13/docs/WDBC_Project/Breast_Cancer_Classification.ipynb)

:::

## Convolutional Neural Networks: Spatial Pattern Processing

While MLPs treat each input element independently, many real-world data types exhibit strong spatial relationships. Images, for example, derive their meaning from the spatial arrangement of pixels—a pattern of edges and textures that form recognizable objects. Audio signals show temporal patterns of frequency components, and sensor data often contains spatial or temporal correlations. These spatial relationships suggest that treating every input-output connection with equal importance, as MLPs do, might not be the most effective approach.

### Pattern Processing Need

Spatial pattern processing addresses scenarios where the relationship between data points depends on their relative positions or proximity. Consider processing a natural image: a pixel's relationship with its neighbors is crucial for detecting edges, textures, and shapes. These local patterns then combine hierarchically to form more complex features—edges form shapes, shapes form objects, and objects form scenes.

This hierarchical spatial pattern processing appears across many domains. In computer vision, local pixel patterns form edges and textures that combine into recognizable objects. Speech processing relies on patterns across nearby time segments to identify phonemes and words. Sensor networks analyze correlations between physically proximate sensors to understand environmental patterns. Medical imaging depends on recognizing tissue patterns that indicate biological structures.

Taking image processing as an example, if we want to detect a cat in an image, certain spatial patterns must be recognized: the triangular shape of ears, the round contours of the face, the texture of fur. Importantly, these patterns maintain their meaning regardless of where they appear in the image—a cat is still a cat whether it's in the top-left or bottom-right corner. This suggests two key requirements for spatial pattern processing: the ability to detect local patterns and the ability to recognize these patterns regardless of their position. This leads us to the convolutional neural network architecture, introduced by @lecun1989backpropagation.

These requirements create specific demands on our processing architecture. The system needs to support local connectivity to detect spatial patterns while enabling parameter sharing to recognize patterns independent of position. It must facilitate hierarchical processing to combine simple patterns into complex features, and efficiently handle shifting patterns across the input space. Unlike the dense connectivity of MLPs, spatial pattern processing suggests an architecture that explicitly encodes these spatial relationships while maintaining computational efficiency. This leads us to the convolutional neural network architecture, which we'll examine next.

### Algorithmic Structure

CNNs address spatial pattern processing through a fundamentally different connection pattern than MLPs. Instead of connecting every input to every output, CNNs use a local connection pattern where each output connects only to a small, spatially contiguous region of the input. This local receptive field moves across the input space, applying the same set of weights at each position—a process known as convolution.

The core operation in a CNN can be expressed mathematically as:

$$
\mathbf{H}^{(l)}_{i,j,k} = f(\sum_{di}\sum_{dj}\sum_{c} \mathbf{W}^{(l)}_{di,dj,c,k}\mathbf{H}^{(l-1)}_{i+di,j+dj,c} + \mathbf{b}^{(l)}_k)
$$

Here, $(i,j)$ represents spatial positions, $k$ indexes output channels, $c$ indexes input channels, and $(di,dj)$ spans the local receptive field. Unlike the dense matrix multiplication of MLPs, this operation:

- Processes local neighborhoods (typically 3×3 or 5×5)
- Reuses the same weights at each spatial position
- Maintains spatial structure in its output

For a concrete example, consider our MNIST digit classification task with 28×28 grayscale images. Each convolutional layer applies a set of filters (say 3×3) that slide across the image, computing local weighted sums. If we use 32 filters, the layer produces a 28×28×32 output, where each spatial position contains 32 different feature measurements of its local neighborhood. This is in stark contrast to our MLP approach where we flattened the entire image into a 784-dimensional vector.

This algorithmic structure directly implements the requirements we identified for spatial pattern processing, creating distinct computational patterns that influence system design.

::: {.content-visible when-format="html"}
![Convolution operation, image data (blue) and 3x3 filter (green). Source: V. Dumoulin, F. Visin, MIT](images/gif/cnn.gif){#fig-cnn}
:::

::: {.content-visible when-format="pdf"}
![Convolution operation, image data (blue) and 3x3 filter (green). Source: V. Dumoulin, F. Visin, MIT](images/png/cnn.png){#fig-cnn}
:::

### Computational Mapping

The elegant spatial structure of convolution operations maps to computational patterns quite different from the dense matrix multiplication of MLPs. Let's examine how this mapping progresses from mathematical abstraction to computational reality.

The fundamental computation in CNNs is the sliding window convolution, where each output value depends only on a local region of the input. For a basic forward pass through a convolutional layer, we can express this mathematically as:

$$
\mathbf{H}^{(l)}_{i,j,k} = f(\sum_{di}\sum_{dj}\sum_{c} \mathbf{W}^{(l)}_{di,dj,c,k}\mathbf{H}^{(l-1)}_{i+di,j+dj,c} + \mathbf{b}^{(l)}_k)
$$

While this mathematical view is elegant, its actual implementation reveals a more detailed computational reality. Let's examine two different implementations of the same computation - one that mirrors the mathematical abstraction and one that exposes the core computational pattern:

```{.python}
# Mathematical abstraction - simple and clean
def conv_layer_math(input, kernel, bias):
    output = convolution(input, kernel) + bias
    return activation(output)

# System reality - nested loops of computation
def conv_layer_compute(input, kernel, bias):
    # Loop 1: Process each image in batch
    for image in range(batch_size):
        
        # Loop 2&3: Move across image spatially
        for y in range(height):
            for x in range(width):
                
                # Loop 4: Compute each output feature
                for out_channel in range(num_output_channels):
                    result = bias[out_channel]
                    
                    # Loop 5&6: Move across kernel window
                    for ky in range(kernel_height):
                        for kx in range(kernel_width):
                            
                            # Loop 7: Process each input feature
                            for in_channel in range(num_input_channels):
                                # Get input value from correct window position
                                in_y = y + ky  
                                in_x = x + kx
                                # Perform multiply-accumulate operation
                                result += input[image, in_y, in_x, in_channel] * \
                                         kernel[ky, kx, in_channel, out_channel]
                    
                    # Store result for this output position
                    output[image, y, x, out_channel] = result
```

The first implementation, `conv_layer_spatial`, uses high-level convolution operations to express the computation concisely. This is typical in deep learning frameworks, where optimized libraries handle the underlying complexity. The second implementation, `conv_layer_compute`, reveals the actual computational pattern: nested loops that process each spatial position, applying the same filter weights to local regions of the input.

The nested loops in `conv_layer_compute` reveal the true nature of convolution's computational pattern. The seven nested loops reveal different aspects of the computation:

* Outer loops (1-3) manage position: which image and where in the image
* Middle loop (4) handles output features: computing different learned patterns
* Inner loops (5-7) perform the actual convolution: sliding the kernel window

Let's take a closer look. The outer two loops (`for y` and `for x`) traverse each spatial position in the output feature map - for our MNIST example, this means moving across all 28×28 positions. At each position, we compute values for each output channel (`for k` loop), which represents different learned features or patterns - our 32 different feature detectors.

The inner three loops implement the actual convolution operation at each position. For each output value, we process a local 3×3 region of the input (the `dy` and `dx` loops) across all input channels (`for c` loop). This creates a sliding window effect, where the same 3×3 filter moves across the image, performing multiply-accumulates between the filter weights and the local input values. Unlike the MLP's global connectivity, this local processing pattern means each output value depends only on a small neighborhood of the input.

For our MNIST example with 3×3 filters and 32 output channels, each output position requires only 9 multiply-accumulate operations per input channel, compared to the 784 operations needed in our MLP layer. However, this operation must be repeated for every spatial position (28×28) and every output channel (32). While using fewer operations per output, the spatial structure creates different patterns of memory access and computation that systems must handle efficiently. These patterns fundamentally influence system design, creating both challenges and opportunities for optimization, which we'll examine next.

### System Implications

When analyzing how computational patterns impact computer systems, we examine three fundamental dimensions: memory requirements, computation needs, and data movement. For CNNs, the spatial nature of processing creates distinctive patterns in each dimension that differ significantly from the dense connectivity of MLPs.

#### Memory Requirements

For convolutional layers, memory requirements center around two key components: filter weights and feature maps. Unlike MLPs that require storing full connection matrices, CNNs use small, reusable filters. In our MNIST example, a convolutional layer with 32 filters of size 3×3 requires storing only 288 weight parameters (3×3×32), in contrast to the 78,400 weights needed for our MLP's fully-connected layer. However, the system must store feature maps for all spatial positions, creating a different memory demand—a 28×28 input with 32 output channels requires storing 25,088 activation values (28×28×32).

These memory access patterns suggest opportunities for optimization through weight reuse and careful feature map management. Modern processors handle these patterns by caching filter weights, which are reused across spatial positions, while streaming through feature map data. Deep learning frameworks typically implement this through specialized memory layouts that optimize for both filter reuse and spatial locality in feature map access. CPUs and GPUs approach this differently—CPUs leverage their cache hierarchy to keep frequently used filters resident, while GPUs use specialized memory architectures designed for the spatial access patterns of image processing.

#### Computation Needs

The core computation in CNNs involves repeatedly applying small filters across spatial positions. Each output value requires a local multiply-accumulate operation over the filter region. For our MNIST example with 3×3 filters and 32 output channels, computing one spatial position involves 288 multiply-accumulates (3×3×32), and this must be repeated for all 784 spatial positions (28×28). While each individual computation involves fewer operations than an MLP layer, the total computational load remains substantial due to spatial repetition.

This computational pattern presents different optimization opportunities than MLPs. The regular, repeated nature of convolution operations enables efficient hardware utilization through structured parallelism. Modern processors exploit this pattern in various ways. CPUs leverage SIMD instructions to process multiple filter positions simultaneously, while GPUs parallelize computation across spatial positions and channels. Deep learning frameworks further optimize this through specialized convolution algorithms that transform the computation to better match hardware capabilities.

#### Data Movement

The sliding window pattern of convolutions creates a distinctive data movement profile. Unlike MLPs where each weight is used once per forward pass, CNN filter weights are reused many times as the filter slides across spatial positions. For our MNIST example, each 3×3 filter weight is reused 784 times (once for each position in the 28×28 feature map). However, this creates a different challenge: the system must stream input features through the computation unit while keeping filter weights stable.

The predictable spatial access pattern enables strategic data movement optimizations. Different architectures handle this movement pattern through specialized mechanisms. CPUs maintain frequently used filter weights in cache while streaming through input features. GPUs employ memory architectures optimized for spatial locality and provide hardware support for efficient sliding window operations. Deep learning frameworks orchestrate these movements by organizing computations to maximize filter weight reuse and minimize redundant feature map accesses.

### Summary and Next Steps

The analysis of CNNs reveals how spatial pattern processing creates fundamentally different computational patterns from the dense connectivity of MLPs. While MLPs require high memory bandwidth for large weight matrices, CNNs benefit from weight reuse but must manage streaming access to feature maps. The computational pattern shifts from large matrix multiplications to repeated local operations, creating new opportunities for parallel execution. Data movement focuses on filter reuse and efficient handling of sliding window operations rather than managing all-to-all connectivity.

These characteristics demonstrate why specialized hardware accelerators and optimized software frameworks have evolved to handle CNNs efficiently. As we examine more specialized architectures in subsequent sections, such as recurrent neural networks for sequential processing and transformers for dynamic information flow, we will see how different pattern processing needs create their own unique demands on computer systems.

## Recurrent Neural Networks: Sequential Pattern Processing

While MLPs handle arbitrary relationships and CNNs process spatial patterns, many real-world problems involve sequential data where the order and relationship between elements over time matters. Text processing requires understanding how words relate to previous context, speech recognition needs to track how sounds form coherent patterns, and time-series analysis must capture how values evolve over time. These sequential relationships suggest that treating each time step independently misses crucial temporal patterns.

### Pattern Processing Need

Sequential pattern processing addresses scenarios where the meaning of current input depends on what came before it. Consider natural language processing: the meaning of a word often depends heavily on previous words in the sentence. The word "bank" means something different in "river bank" versus "bank account." Similarly, in speech recognition, a phoneme's interpretation often depends on surrounding sounds, and in financial forecasting, future predictions require understanding patterns in historical data.

The key challenge in sequential processing is maintaining and updating relevant context over time. When reading text, humans don't start fresh with each word—we maintain a running understanding that evolves as we process new information. Similarly, when processing time-series data, patterns might span different timescales, from immediate dependencies to long-term trends. This suggests we need an architecture that can both maintain state over time and update it based on new inputs.

These requirements demand specific capabilities from our processing architecture. The system must maintain internal state to capture temporal context, update this state based on new inputs, and learn which historical information is relevant for current predictions. Unlike MLPs and CNNs, which process fixed-size inputs, sequential processing must handle variable-length sequences while maintaining computational efficiency. This leads us to the recurrent neural network architecture.

### Algorithmic Structure

RNNs address sequential processing through a fundamentally different approach than MLPs or CNNs by introducing recurrent connections. Instead of just mapping inputs to outputs, RNNs maintain an internal state that is updated at each time step. This creates a memory mechanism that allows the network to carry information forward in time. This unique ability to model temporal dependencies was first explored by @elman1990finding, who demonstrated how RNNs could find structure in time-dependent data.

The core operation in a basic RNN can be expressed mathematically as:

$$
\mathbf{h}_t = f(\mathbf{W}_{hh}\mathbf{h}_{t-1} + \mathbf{W}_{xh}\mathbf{x}_t + \mathbf{b}_h)
$$

where $\mathbf{h}_t$ represents the hidden state at time $t$, $\mathbf{x}_t$ is the input at time $t$, $\mathbf{W}_{hh}$ contains the recurrent weights, and $\mathbf{W}_{xh}$ contains the input weights.

For example, in processing a sequence of words, each word might be represented as a 100-dimensional vector ($\mathbf{x}_t$), and we might maintain a hidden state of 128 dimensions ($\mathbf{h}_t$). At each time step, the network combines the current input with its previous state to update its understanding of the sequence. This creates a form of memory that can capture patterns across time steps.

This recurrent structure directly implements our requirements for sequential processing through the introduction of recurrent connections, which maintain internal state and allow the network to carry information forward in time. Instead of processing all inputs independently, RNNs process sequences of data by iteratively updating a hidden state based on the current input and the previous hidden state. This unique ability to model temporal dependencies was first explored @elman1990finding, who demonstrated how RNNs could find structure in time-dependent data. This makes RNNs well-suited for tasks such as language modeling, speech recognition, and time-series forecasting.


### Computational Mapping

The sequential structure of RNNs maps to computational patterns quite different from both MLPs and CNNs. Let's examine how this mapping progresses from mathematical abstraction to computational reality.

The fundamental computation in RNNs involves both the current input and the previous state. For a basic forward pass through an RNN layer, we can express this mathematically as:

$$
\mathbf{h}_t = f(\mathbf{W}_{hh}\mathbf{h}_{t-1} + \mathbf{W}_{xh}\mathbf{x}_t + \mathbf{b}_h)
$$

While this mathematical view is elegant, its actual implementation reveals a more detailed computational reality. Let's examine two different implementations of the same computation:

```python
# Mathematical abstraction in code
def rnn_layer_step(x_t, h_prev, W_hh, W_xh, b):
    # x_t: input at time t (batch_size × input_dim)
    # h_prev: previous hidden state (batch_size × hidden_dim)
    # W_hh: recurrent weights (hidden_dim × hidden_dim)
    # W_xh: input weights (input_dim × hidden_dim)
    h_t = activation(matmul(h_prev, W_hh) + matmul(x_t, W_xh) + b)
    return h_t

# Core computational pattern
def rnn_layer_compute(x_t, h_prev, W_hh, W_xh, b):
    # Initialize next hidden state
    h_t = np.zeros_like(h_prev)
    
    # Process each element in the batch
    for batch in range(batch_size):
        # First compute recurrent contribution
        for i in range(hidden_dim):
            for j in range(hidden_dim):
                h_t[batch,i] += h_prev[batch,j] * W_hh[j,i]
                
        # Then add input contribution
        for i in range(hidden_dim):
            for j in range(input_dim):
                h_t[batch,i] += x_t[batch,j] * W_xh[j,i]
                
        # Add bias and apply activation
        for i in range(hidden_dim):
            h_t[batch,i] = activation(h_t[batch,i] + b[i])
    
    return h_t
```

The `rnn_layer_compute` implementation reveals the true nature of RNN computation. Unlike MLPs or CNNs where computations can be highly parallel, RNNs have an inherent sequential dependency - each time step's computation requires the result from the previous step. The inner loops show two main computational components: processing the previous hidden state and incorporating the new input. This creates a distinctive pattern where state must be maintained and updated sequentially.

For a sequence processing task with input dimension 100 and hidden state dimension 128, each time step requires two matrix multiplications: one 128×128 for the recurrent connection and one 100×128 for the input projection. While individual time steps can process in parallel across batch elements, the time steps themselves must process sequentially. This creates a unique computational pattern that systems must handle efficiently.

### Computational Mapping

The sequential structure of RNNs maps to computational patterns quite different from both MLPs and CNNs. Let's examine how this mapping progresses from mathematical abstraction to computational reality.

The fundamental computation in RNNs involves both the current input and the previous state. For a basic forward pass through an RNN layer, we can express this mathematically as:

$$
\mathbf{h}_t = f(\mathbf{W}_{hh}\mathbf{h}_{t-1} + \mathbf{W}_{xh}\mathbf{x}_t + \mathbf{b}_h)
$$

While this mathematical view is elegant, let's take a look at its code. Let's examine two different implementations of the same computation:

```{.python}
# Mathematical abstraction in code
def rnn_layer_step(x_t, h_prev, W_hh, W_xh, b):
    # x_t: input at time t (batch_size × input_dim)
    # h_prev: previous hidden state (batch_size × hidden_dim)
    # W_hh: recurrent weights (hidden_dim × hidden_dim)
    # W_xh: input weights (input_dim × hidden_dim)
    h_t = activation(matmul(h_prev, W_hh) + matmul(x_t, W_xh) + b)
    return h_t
```

The `rnn_layer_step` function shows how the operation looks when using high-level matrix operations found in deep learning frameworks. It handles a single time step, taking the current input `x_t` and previous hidden state `h_prev`, along with two weight matrices: `W_hh` for hidden-to-hidden connections and `W_xh` for input-to-hidden connections. Through matrix multiplication operations (`matmul`), it merges the previous state and current input to generate the next hidden state. 

This simplified view masks the underlying complexity of the nested loops and individual computations shown in the detailed implementation. Its actual implementation reveals a more detailed computational reality:

```{.python}
# Core computational pattern
def rnn_layer_compute(x_t, h_prev, W_hh, W_xh, b):
    # Initialize next hidden state
    h_t = np.zeros_like(h_prev)
    
    # Loop 1: Process each sequence in the batch
    for batch in range(batch_size):
        # Loop 2: Compute recurrent contribution (h_prev × W_hh)
        for i in range(hidden_dim):
            for j in range(hidden_dim):
                h_t[batch,i] += h_prev[batch,j] * W_hh[j,i]
                
        # Loop 3: Compute input contribution (x_t × W_xh)
        for i in range(hidden_dim):
            for j in range(input_dim):
                h_t[batch,i] += x_t[batch,j] * W_xh[j,i]
                
        # Loop 4: Add bias and apply activation
        for i in range(hidden_dim):
            h_t[batch,i] = activation(h_t[batch,i] + b[i])
    
    return h_t
```

The nested loops in `rnn_layer_compute` expose the core computational pattern of RNNs. Loop 1 processes each sequence in the batch independently, allowing for batch-level parallelism. Within each batch item, Loop 2 computes how the previous hidden state influences the next state through the recurrent weights `W_hh`. Loop 3 then incorporates new information from the current input through the input weights `W_xh`. Finally, Loop 4 adds biases and applies the activation function to produce the new hidden state.

For a sequence processing task with input dimension 100 and hidden state dimension 128, each time step requires two matrix multiplications: one 128×128 for the recurrent connection and one 100×128 for the input projection. While individual time steps can process in parallel across batch elements, the time steps themselves must process sequentially. This creates a unique computational pattern that systems must handle efficiently.

### System Implications

We examine the three dimensions: memory requirements, computation needs, and data movement. For RNNs, the sequential nature of processing creates distinctive patterns in each dimension that differ significantly from both MLPs and CNNs.

#### Memory Requirements

RNNs require storing two sets of weights (input-to-hidden and hidden-to-hidden) along with the hidden state. For our example with input dimension 100 and hidden state dimension 128, this means storing 12,800 weights for input projection (100×128) and 16,384 weights for recurrent connections (128×128). Unlike CNNs where weights are reused across spatial positions, RNN weights are reused across time steps. Additionally, the system must maintain the hidden state, which becomes a critical factor in memory usage and access patterns.

These memory access patterns create a different profile from MLPs and CNNs. Modern processors handle these patterns by keeping the weight matrices in cache while streaming through sequence elements. Deep learning frameworks optimize memory access by batching sequences together and carefully managing hidden state storage between time steps. CPUs and GPUs approach this through different strategies - CPUs leverage their cache hierarchy for weight reuse, while GPUs use specialized memory architectures designed for maintaining state across sequential operations.

#### Computation Needs

The core computation in RNNs involves repeatedly applying weight matrices across time steps. For each time step, we perform two matrix multiplications: one with the input weights and one with the recurrent weights. In our example, processing a single time step requires 12,800 multiply-accumulates for the input projection (100×128) and 16,384 multiply-accumulates for the recurrent connection (128×128). 

This computational pattern differs from both MLPs and CNNs in a key way: while we can parallelize across batch elements, we cannot parallelize across time steps due to the sequential dependency. Each time step must wait for the previous step's hidden state before it can begin computation. This creates a tension between the inherent sequential nature of the algorithm and the desire for parallel execution in modern hardware.

Modern processors handle these patterns through different approaches. CPUs pipeline operations within each time step while maintaining the sequential order across steps. GPUs batch multiple sequences together to maintain high throughput despite sequential dependencies. Deep learning frameworks optimize this further by techniques like sequence packing and unrolling computations across multiple time steps when possible.

#### Data Movement

The sequential processing in RNNs creates a distinctive data movement pattern that differs from both MLPs and CNNs. While MLPs need each weight only once per forward pass and CNNs reuse weights across spatial positions, RNNs reuse their weights across time steps while requiring careful management of the hidden state data flow.

For our example with a 128-dimensional hidden state, each time step must: load the previous hidden state (128 values), access both weight matrices (29,184 total weights from both input and recurrent connections), and store the new hidden state (128 values). This pattern repeats for every element in the sequence. Unlike CNNs where we can predict and prefetch data based on spatial patterns, RNN data movement is driven by temporal dependencies.

Different architectures handle this sequential data movement through specialized mechanisms. CPUs maintain weight matrices in cache while streaming through sequence elements and managing hidden state updates. GPUs employ memory architectures optimized for maintaining state information across sequential operations while processing multiple sequences in parallel. Deep learning frameworks orchestrate these movements by managing data transfers between time steps and optimizing batch operations.

### Summary and Next Steps

The analysis of RNNs demonstrates how sequential pattern processing creates fundamentally different computational patterns from both the dense connectivity of MLPs and the spatial operations of CNNs. While MLPs process all inputs simultaneously and CNNs reuse weights across spatial positions, RNNs must handle temporal dependencies that create inherent sequential processing requirements. This sequential nature manifests in distinct system demands: memory systems must manage both weight reuse across time steps and hidden state updates, computation must balance sequential dependencies with parallel execution, and data movement centers around maintaining and updating state information efficiently.

These characteristics illustrate why different optimization strategies have evolved for RNN processing, and why certain applications began shifting toward alternative architectures like attention mechanisms, which we'll examine next. As we explore these newer architectural patterns, we'll see how they address some of the fundamental challenges of sequential processing while creating their own unique demands on computer systems.

## Transformers: Dynamic Pattern Processing

While previous architectures process patterns in fixed ways—MLPs with dense connectivity, CNNs with spatial operations, and RNNs with sequential updates—many tasks require dynamic relationships between elements that change based on content. Language understanding, for instance, needs to capture relationships between words that depend on meaning rather than just position. Graph analysis requires understanding connections that vary by node. These dynamic relationships suggest we need an architecture that can learn and adapt its processing patterns based on the data itself.

### Pattern Processing Need

Dynamic pattern processing addresses scenarios where relationships between elements aren't fixed by architecture but instead emerge from content. Consider language translation: when translating "the bank by the river," understanding "bank" requires attending to "river," but in "the bank approved the loan," the important relationship is with "approved" and "loan." Unlike RNNs that process information sequentially or CNNs that use fixed spatial patterns, we need an architecture that can dynamically determine which relationships matter.

This requirement for dynamic processing appears across many domains. In protein structure prediction, interactions between amino acids depend on their chemical properties and spatial arrangements. In graph analysis, node relationships vary based on graph structure and node features. In document analysis, connections between different sections depend on semantic content rather than just proximity.

These scenarios demand specific capabilities from our processing architecture. The system must compute relationships between all pairs of elements, weigh these relationships based on content, and use these weights to selectively combine information. Unlike previous architectures with fixed connectivity patterns, dynamic processing requires the flexibility to modify its computation graph based on the input itself. This leads us to the Transformer architecture, which implements these capabilities through attention mechanisms.

### Algorithmic Structure

Transformers implement dynamic pattern processing through attention mechanisms, which compute weighted connections between elements based on their content. The core operation in a Transformer is self-attention, where each element queries every other element to determine their relevance. This creates a dynamic connectivity pattern that adapts to the input data.

The attention operation can be expressed mathematically as:

$$
\text{Attention}(\mathbf{Q}, \mathbf{K}, \mathbf{V}) = \text{softmax}(\frac{\mathbf{Q}\mathbf{K}^T}{\sqrt{d_k}})\mathbf{V}
$$

Here, Q (queries), K (keys), and V (values) are learned projections of the input. For a sequence of length N with dimension d, this creates an N×N attention matrix that determines how each position should attend to all others. Unlike the fixed weight matrices in previous architectures, these attention weights are computed dynamically for each input.

For example, in processing a sentence with 50 words, each word position computes attention scores with all other positions. If each word is represented by a 512-dimensional vector, the operation involves:

- Computing query, key, and value projections for each position
- Generating a 50×50 attention matrix through query-key interactions
- Using these attention weights to combine value vectors

This algorithmic structure directly implements our need for dynamic relationship processing but creates distinctive computational patterns that significantly influence system design.

### Computational Mapping

The dynamic structure of attention operations maps to computational patterns quite different from our previous architectures. Let's examine how this mapping progresses from mathematical abstraction to computational reality.

The fundamental computation in Transformers involves computing attention weights and using them to combine values. Let's examine two different implementations of the same computation:

```{.python}
# Mathematical abstraction in code
def attention_layer_matrix(Q, K, V):
    # Q, K, V: (batch_size × seq_len × d_model)
    scores = matmul(Q, K.transpose(-2, -1)) / sqrt(d_k)  # Compute attention scores
    weights = softmax(scores)                            # Normalize scores
    output = matmul(weights, V)                         # Combine values
    return output

# Core computational pattern
def attention_layer_compute(Q, K, V):
    # Initialize outputs
    scores = np.zeros((batch_size, seq_len, seq_len))
    outputs = np.zeros_like(V)
    
    # Loop 1: Process each sequence in batch
    for b in range(batch_size):
        # Loop 2: Compute attention for each query position
        for i in range(seq_len):
            # Loop 3: Compare with each key position
            for j in range(seq_len):
                # Compute attention score
                for d in range(d_model):
                    scores[b,i,j] += Q[b,i,d] * K[b,j,d]
                scores[b,i,j] /= sqrt(d_k)
        
        # Apply softmax to scores
        for i in range(seq_len):
            scores[b,i] = softmax(scores[b,i])
            
        # Loop 4: Combine values using attention weights
        for i in range(seq_len):
            for j in range(seq_len):
                for d in range(d_model):
                    outputs[b,i,d] += scores[b,i,j] * V[b,j,d]
    
    return outputs
```

The nested loops in `attention_layer_compute` reveal the true nature of attention's computational pattern. Loop 1 processes each sequence in the batch independently. Loop 2 and 3 compute attention scores between all pairs of positions, creating a quadratic computation pattern with respect to sequence length. Finally, Loop 4 uses these attention weights to combine values from all positions.

For our example with a sequence length of 50 and dimension 512, computing attention scores requires 1,250,000 multiply-accumulates (50×50×512) just for the query-key interactions. This quadratic scaling with sequence length creates a distinctive computational challenge that differs from the linear scaling of MLPs, CNNs, and RNNs.

These patterns fundamentally influence system design, creating both challenges and opportunities for optimization, which we'll examine next.

### System Implications

Let's once again examine three fundamental dimensions: memory requirements, computation needs, and data movement. For Transformers, the dynamic nature of attention creates distinctive patterns in each dimension that differ significantly from previous architectures.

#### Memory Requirements

Transformer memory requirements center around three key components: attention weights, key-query-value projections, and intermediate feature representations. For our example with sequence length 50 and dimension 512, each attention layer must store the attention weight matrix, which is 50×50 for each sequence in the batch. The layer also needs three sets of projection matrices for queries, keys, and values, each sized 512×512, along with input and output feature maps of size 50×512.

Unlike CNNs where weights are reused across spatial positions or RNNs where weights are reused across time steps, Transformers generate new attention weights for every input. This dynamic weight generation creates a different memory access pattern where intermediate attention weights become a significant factor in memory usage.

These memory access patterns suggest different optimization requirements than previous architectures. Modern processors handle these patterns by carefully managing the attention weight computations and storage. Deep learning frameworks typically implement sophisticated memory management schemes to handle the quadratic memory scaling of attention weights. CPUs and GPUs approach this through different strategies - CPUs often compute attention weights in blocks to fit in cache, while GPUs use specialized memory architectures designed for handling large intermediate results.

#### Computation Needs

The core computation in Transformers involves two main phases: generating attention weights and applying them to values. For each attention layer with sequence length 50 and dimension 512, the system performs substantial multiply-accumulate operations across multiple computational stages. The query-key interactions alone require 1,250,000 multiply-accumulates (50×50×512), with an equal number needed for applying attention weights to values. Additional computations are needed for the projection matrices and softmax operations.

This computational pattern differs from previous architectures in two key ways. First, the quadratic scaling with sequence length creates intensive computation requirements even for moderate sequence lengths. Second, the dynamic nature of attention weights means all these computations must be performed fresh for each input, unlike the weight reuse in CNNs or RNNs.

Modern processors handle these patterns through different approaches. CPUs pipeline operations and use SIMD instructions to parallelize across the dimension axis while managing the quadratic sequence interactions. GPUs leverage their massive parallel processing capabilities to handle multiple attention computations simultaneously. Deep learning frameworks optimize this further through techniques like attention masking and blocking to reduce unnecessary computations.

#### Data Movement

The dynamic attention patterns in Transformers create distinctive data movement requirements that differ fundamentally from previous architectures. While CNNs reuse filter weights spatially and RNNs reuse weights temporally, Transformers must compute and move new attention weights for every input. Each attention operation involves a complex dance of data movement: the system must project and move query, key, and value vectors for each position, store and access the full attention weight matrix, and coordinate the movement of value vectors during the weighted combination phase.

This creates a challenging data movement pattern where intermediate attention weights become a major factor in system bandwidth requirements. Unlike the predictable access patterns of CNNs or the sequential access of RNNs, attention operations require frequent movement of dynamically computed weights across the memory hierarchy.

Different architectures handle these data movement patterns through specialized mechanisms. CPUs manage data movement through blocked computations that maximize cache utilization. GPUs employ memory hierarchies optimized for the parallel movement of attention weights and feature vectors. Deep learning frameworks orchestrate these movements by carefully scheduling attention computations and managing intermediate storage.

### Summary and Next Steps

The analysis of Transformers reveals how dynamic pattern processing creates fundamentally different computational patterns from all previous architectures. While MLPs use fixed dense connections, CNNs reuse weights spatially, and RNNs process sequentially, Transformers dynamically compute their connectivity pattern based on content. This manifests in distinctive system demands: memory systems must handle quadratic scaling of attention weights, computation must process intensive query-key interactions, and data movement centers around dynamically generated weights and intermediate results.

These characteristics explain both the power and challenges of Transformer architectures. Their ability to capture dynamic relationships enables breakthrough performance on many tasks, but their computational intensity drives the need for specialized hardware and optimized implementations. As we continue exploring deep learning architectures, these fundamental patterns and their system implications will guide our understanding of efficient implementation strategies.

## Pattern Classification and Building Blocks

Deep learning architectures, while we presented them as distinct approaches in the previous sections, are better understood as compositions of fundamental building blocks that evolved over time. Much like how complex LEGO structures are built from basic bricks, modern neural networks combine and iterate on core computational patterns that emerged through decades of research [@lecun2015deep]. Each architectural innovation introduced new building blocks while finding novel ways to use existing ones.

Understanding these building blocks and their evolution provides insight into modern architectures. What began with the simple perceptron [@rosenblatt1958perceptron] evolved into multi-layer networks [@rumelhart1986learning], which then spawned specialized patterns for spatial and sequential processing. Each advancement maintained useful elements from its predecessors while introducing new computational primitives. Today's sophisticated architectures, like Transformers [@vaswani2017attention], can be seen as carefully engineered combinations of these fundamental building blocks.

This progression reveals not just the evolution of neural networks, but also the discovery and refinement of core computational patterns that remain relevant. As we examine this evolution, we'll see how each architecture contributed essential building blocks that continue to influence modern network design.

### From Perceptron to Multi-Layer Networks

While we examined MLPs earlier as a mechanism for dense pattern processing, here we focus on how they established fundamental building blocks that appear throughout deep learning. The evolution from perceptron to MLP introduced several key concepts: the power of layer stacking, the importance of non-linear transformations, and the basic feedforward computation pattern.

The introduction of hidden layers between input and output created a template for feature transformation that appears in virtually every modern architecture. Even in sophisticated networks like Transformers, we find MLP-style feedforward layers performing crucial feature processing. The concept of transforming data through successive non-linear layers has become a fundamental paradigm that transcends the specific architecture types.

Perhaps most importantly, the development of MLPs established the backpropagation algorithm, which to this day remains the cornerstone of neural network training. This key contribution has enabled the training of deep architectures and influenced how later architectures would be designed to maintain gradient flow.

These building blocks - layered feature transformation, non-linear activation, and gradient-based learning - set the foundation for more specialized architectures. As we'll see, subsequent innovations often focused on structuring these basic components in new ways rather than replacing them entirely.

### From Dense to Spatial Processing

The development of CNNs marked a significant architectural innovation - the realization that we could specialize the dense connectivity of MLPs for spatial patterns. While retaining the core concept of layer-wise processing, CNNs introduced several fundamental building blocks that would influence all future architectures.

The first key innovation was the concept of parameter sharing. Unlike MLPs where each connection had its own weight, CNNs showed how the same parameters could be reused across different parts of the input. This not only made the networks more efficient but introduced the powerful idea that architectural structure could encode useful priors about the data [@lecun1998gradient].

Perhaps even more influential was the introduction of skip connections through ResNets [@he2016deep]. Originally they were designed to help train very deep CNNs, skip connections have become a fundamental building block that appears in virtually every modern architecture. They showed how direct paths through the network could help gradient flow and information propagation, a concept now central to Transformer designs [@vaswani2017attention].

CNNs also introduced batch normalization, a technique for stabilizing neural network training by normalizing intermediate features [@ioffe2015batch]; we will learn more about this in the AI Training chapter. This concept of feature normalization, while originating in CNNs, evolved into layer normalization and is now a crucial component in modern architectures.

These innovations - parameter sharing, skip connections, and normalization - transcended their origins in spatial processing to become essential building blocks in the deep learning toolkit.

### The Evolution of Sequence Processing

While CNNs specialized MLPs for spatial patterns, sequence models adapted neural networks for temporal dependencies. RNNs introduced the fundamental concept of maintaining and updating state - a building block that influenced how networks could process sequential information [@elman1990finding].

The development of LSTMs and GRUs brought sophisticated gating mechanisms to neural networks [@hochreiter1997long; @cho2014properties]. These gates, themselves small MLPs, showed how simple feedforward computations could be composed to control information flow. This concept of using neural networks to modulate other neural networks became a recurring pattern in architecture design.

Perhaps most significantly, sequence models demonstrated the power of adaptive computation paths. Unlike the fixed patterns of MLPs and CNNs, RNNs showed how networks could process variable-length inputs by reusing weights over time. This insight - that architectural patterns could adapt to input structure - laid groundwork for more flexible architectures.

Sequence models also popularized the concept of attention through encoder-decoder architectures [@bahdanau2014neural]. Initially introduced as an improvement to machine translation, attention mechanisms showed how networks could learn to dynamically focus on relevant information. This building block would later become the foundation of Transformer architectures [@vaswani2017attention].

### Modern Architectures: Synthesis and Innovation

Modern architectures, particularly Transformers, represent a sophisticated synthesis of these fundamental building blocks. Rather than introducing entirely new patterns, they innovate through clever combination and refinement of existing components.

Consider the Transformer architecture: at its core, we find MLP-style feedforward networks processing features between attention layers. The attention mechanism itself builds on ideas from sequence models but removes the recurrent connection, instead using position embeddings inspired by CNN intuitions. Skip connections, inherited from ResNets, appear throughout the architecture, while layer normalization, evolved from CNN's batch normalization, stabilizes training [@ba2016layer].

This composition of building blocks creates something greater than the sum of its parts. The self-attention mechanism, while building on previous attention concepts, enables a new form of dynamic pattern processing. The arrangement of these components - attention followed by feedforward layers, with skip connections and normalization - has proven so effective it's become a template for new architectures [@vaswani2017attention].

Even recent innovations in vision and language models follow this pattern of recombining fundamental building blocks. Vision Transformers adapt the Transformer architecture to images while maintaining its essential components [@dosovitskiy2021image]. Large language models scale up these patterns while introducing refinements like grouped-query attention or sliding window attention, yet still rely on the core building blocks established through this architectural evolution [@brown2020language].

### Summary: Building Blocks and Architecture Design

The evolution of deep learning architectures reveals a consistent pattern of innovation through composition. What began with the simple perceptron led to MLPs that established foundational concepts of layered processing and gradient-based learning. CNNs introduced parameter sharing, skip connections, and normalization techniques, while sequence models contributed state management, gating mechanisms, and early forms of attention. Modern architectures like Transformers succeed not by reinventing these patterns, but by finding novel ways to combine and refine them.

This compositional nature of neural architectures has important implications for the field. New architectures rarely emerge in isolation; instead, they build upon proven building blocks while introducing carefully chosen innovations. Understanding these building blocks - from basic feedforward computation to attention mechanisms - provides a powerful framework for analyzing and designing neural networks.

As we move forward to examine the system implications of these architectures, this building block perspective will help us understand how fundamental computational patterns map to hardware resources. Each building block creates specific demands on memory, computation, and data movement, and their composition in modern architectures drives key system design decisions.

