---
bibliography: dl_primer.bib
---

# DL Primer NEW 🚀 {#sec-dl_primer}

::: {.content-visible when-format="html"}
Resources: [Slides](#sec-deep-learning-primer-resource), [Videos](#sec-deep-learning-primer-resource), [Exercises](#sec-deep-learning-primer-resource)
:::

![_DALL·E 3 Prompt: Photo of a classic classroom with a large blackboard dominating one wall. Chalk drawings showcase a detailed deep neural network with several hidden layers, and each node and connection is precisely labeled with white chalk. The rustic wooden floor and brick walls provide a contrast to the modern concepts. Surrounding the room, posters mounted on frames emphasize deep learning themes: convolutional networks, transformers, neurons, activation functions, and more._](images/png/cover_dl_primer.png)

This chapter is a primer on deep learning from a systems perspective, providing us with essential context and the foundational knowledge that is needed to implement and optimize deep learning solutions effectively. ML systems represents the bridge between theoretical machine learning concepts and their practical implementation in real-world computing systems. Our goal is to help readers understand how theoretical constructs translate into computational requirements, resource constraints, and system design decisions.

We begin with an overview of deep learning's evolution and its significance in modern computing systems, from edge devices to data centers. We introduce the core concepts like neural networks with a dual focus: first understanding their theoretical foundations, then examining their practical implications for system implementation. The primer explores major deep learning architectures through this systems lens, examining how architectural choices impact compute, memory, and communication requirements.

We then analyze common computational patterns in deep learning and their implications for system design. We examine how different architectures translate to specific hardware demands, software stack requirements, and deployment considerations. Through practical case studies, we demonstrate how these concepts apply to real-world scenarios, from scaling large language models to optimizing models for edge devices. We also compare deep learning to traditional machine learning approaches, helping readers make informed architectural choices based on system constraints and requirements. This comprehensive overview sets the context for the more detailed techniques and optimizations covered in subsequent chapters.

::: {.callout-tip}

## Learning Objectives

* Bridge theoretical deep learning concepts with their practical system implementations.

* Understand how different deep learning architectures impact system requirements.

* Compare traditional ML and deep learning approaches from a systems perspective.

* Identify core computational patterns in deep learning and their deployment implications.

* Apply systems thinking to deep learning model development and deployment.
  
:::

## Overview

Deep learning, a specialized area within machine learning and artificial intelligence (AI), utilizes algorithms modeled after the structure and function of neural networks to process and learn from large amounts of data. From a systems perspective, deep learning represents a unique intersection of algorithms, hardware, and software infrastructure working together to solve complex computational problems.

When implementing deep learning in real-world systems, we need to consider it as part of a broader computing stack. @fig-ai-ml-dl provides a visual representation of how deep learning fits within the broader context of AI and machine learning. As shown in the figure, AI represents the overarching field, encompassing all computational methods that mimic human cognitive functions. Machine learning, shown as a subset of AI, includes algorithms capable of learning from data. Deep learning, the smallest subset in the diagram, specifically involves neural networks that are able to learn more complex patterns from large volumes of data.

![The diagram illustrates artificial intelligence as the overarching field encompassing all computational methods that mimic human cognitive functions. Machine learning is a subset of AI that includes algorithms capable of learning from data. Deep learning, a further subset of ML, specifically involves neural networks that are able to learn more complex patterns in large volumes of data. Source: NVIDIA.](images/png/ai_dl_progress_nvidia.png){#fig-ai-ml-dl}

This progression from AI to deep learning represents key shifts across three fundamental dimensions:

1. **Data**: From structured, rule-based knowledge to massive datasets of raw, unstructured information. Deep learning's ability to learn from raw data eliminates the need for manual feature engineering but demands substantially larger datasets.

2. **Algorithms**: From explicit programming to learned patterns. While traditional AI relies on handcrafted rules and machine learning uses statistical methods, deep learning algorithms automatically discover hierarchical representations in data through multiple layers of abstraction.

3. **Computation**: From basic arithmetic operations to massive parallel processing. The computational demands grow significantly as we move from simple AI algorithms to complex deep neural networks, driving innovations in specialized hardware and distributed computing.

This evolution has profound implications for how we build and deploy AI systems. Deep learning's appetite for data and computation has spurred developments in specialized hardware like GPUs and TPUs, scalable software frameworks, and efficient data pipelines. Understanding these relationships is important for effectively implementing deep learning solutions across different computing environments, from powerful data centers to resource-constrained edge devices. To build this understanding, we must start with the fundamentals. In the following sections, we first examine the historical development and computational growth of deep learning systems. We then explore core neural network concepts, focusing on how theoretical principles translate into practical implementations. This foundation will enable us to better understand modern architectures, their computational patterns, and ultimately, how to build ML systems.

## Understanding Deep Learning

### What Makes Deep Learning Different

Deep learning represents a fundamental shift in how we approach problem solving with computers. To understand this shift, let's consider the classic example of computer vision---specifically, the task of identifying objects in images.

#### Traditional Programming: The Era of Explicit Rules

Traditional programming requires developers to explicitly define rules that tell computers how to process inputs and produce outputs. Consider a simple game like Breakout, shown in @fig-breakout. The program needs explicit rules for every interaction: when the ball hits a brick, the code must specify that the brick should be removed and the ball's direction should be reversed. While this approach works well for games with clear physics and limited states, it demonstrates an inherent limitation of rule-based systems.

![Rule-based programming.](images/png/breakout.png){#fig-breakout}

This rules-based paradigm extends to all traditional programming, as illustrated in @fig-traditional. The program takes both rules for processing and input data to produce outputs. Early artificial intelligence research explored whether this approach could scale to solve complex problems by encoding sufficient rules to capture intelligent behavior.

![Traditional programming.](images/png/traditional.png){#fig-traditional}

However, the limitations of rule-based approaches become evident when addressing complex real-world tasks. Consider the problem of recognizing human activities, shown in @fig-activity-rules. Initial rules might appear straightforward: classify movement below 4 mph as walking and faster movement as running. Yet real-world complexity quickly emerges. The classification must account for variations in speed, transitions between activities, and numerous edge cases. Each new consideration requires additional rules, leading to increasingly complex decision trees.

![Activity rules.](images/png/activities.png){#fig-activity-rules}

This challenge extends to computer vision tasks. Detecting objects like cats in images would require rules about System Implications: pointed ears, whiskers, typical body shapes. Such rules would need to account for variations in viewing angle, lighting conditions, partial occlusions, and natural variations among instances. Early computer vision systems attempted this approach through geometric rules but achieved success only in controlled environments with well-defined objects.

This knowledge engineering approach characterized artificial intelligence research in the 1970s and 1980s. Expert systems encoded domain knowledge as explicit rules, showing promise in specific domains with well-defined parameters but struggling with tasks humans perform naturally---such as object recognition, speech understanding, or natural language interpretation. These limitations highlighted a fundamental challenge: many aspects of intelligent behavior rely on implicit knowledge that resists explicit rule-based representation.

#### Machine Learning: Learning from Engineered Patterns

The limitations of pure rule-based systems led researchers to explore approaches that could learn from data. Machine learning offered a promising direction: instead of writing rules for every situation, we could write programs that found patterns in examples. However, the success of these methods still depended heavily on human insight to define what patterns might be important---a process known as feature engineering.

Feature engineering involves transforming raw data into representations that make patterns more apparent to learning algorithms. In computer vision, researchers developed sophisticated methods to extract meaningful patterns from images. The Histogram of Oriented Gradients (HOG) method, shown in @fig-hog, exemplifies this approach. HOG works by first identifying edges in an image---places where brightness changes sharply, often indicating object boundaries. It then divides the image into small cells and measures how edges are oriented within each cell, summarizing these orientations in a histogram. This transformation converts raw pixel values into a representation that captures important shape information while being robust to variations in lighting and small changes in position.

![Histogram of Oriented Gradients (HOG) requires explicit feature engineering.](images/png/hog.png){#fig-hog}

Other feature extraction methods like SIFT (Scale-Invariant Feature Transform) and Gabor filters provided different ways to capture patterns in images. SIFT found distinctive points that could be recognized even when an object's size or orientation changed. Gabor filters helped identify textures and repeated patterns. Each method encoded different types of human insight about what makes visual patterns recognizable.

These engineered features enabled significant advances in computer vision during the 2000s. Systems could now recognize objects with some robustness to real-world variations, leading to applications in face detection, pedestrian detection, and object recognition. However, the approach had fundamental limitations. Experts needed to carefully design feature extractors for each new problem, and the resulting features might miss important patterns that weren't anticipated in their design.

#### Deep Learning Paradigm

Deep learning fundamentally differs by learning directly from raw data. Traditional programming, as we saw earlier in @fig-traditional, required both rules and data as inputs to produce answers. Machine learning inverts this relationship, as shown in @fig-deeplearning. Instead of writing rules, we provide examples (data) and their correct answers to discover the underlying rules automatically. This shift eliminates the need for humans to specify what patterns are important.

![Deep learning.](images/png/ml_rules.png){#fig-deeplearning}

The system discovers these patterns automatically from examples. When shown millions of images of cats, the system learns to identify increasingly complex visual patterns---from simple edges to more sophisticated combinations that make up cat-like features. This mirrors how our own visual system works, building up understanding from basic visual elements to complex objects.

Unlike traditional approaches where performance often plateaus with more data and computation, deep learning systems continue to improve as we provide more resources. More training examples help the system recognize more variations and nuances. More computational power enables the system to discover more subtle patterns. This scalability has led to dramatic improvements in performance---for example, the accuracy of image recognition systems has improved from 74% in 2012 to over 95% today.

This different approach has profound implications for how we build AI systems. Deep learning's ability to learn directly from raw data eliminates the need for manual feature engineering, but it comes with new demands. We need sophisticated infrastructure to handle massive datasets, powerful computers to process this data, and specialized hardware to perform the complex mathematical calculations efficiently. The computational requirements of deep learning have even driven the development of new types of computer chips optimized for these calculations.

The success of deep learning in computer vision exemplifies how this approach, when given sufficient data and computation, can surpass traditional methods. This pattern has repeated across many domains, from speech recognition to game playing, establishing deep learning as a transformative approach to artificial intelligence.

### From Brain to Artificial Neurons

The quest to create artificial intelligence has been profoundly influenced by our understanding of biological intelligence, particularly the human brain. This isn't surprising---the brain represents the most sophisticated information processing system we know of, capable of learning, adapting, and solving complex problems while maintaining remarkable energy efficiency. The way our brains function has provided fundamental insights that continue to shape how we approach artificial intelligence.

#### Biological Intelligence

When we observe biological intelligence, several key principles emerge. The brain demonstrates an extraordinary ability to learn from experience, constantly modifying its neural connections based on new information and interactions with the environment. This adaptability is fundamental---every experience potentially alters the brain's structure, refining its responses for future situations. This biological capability directly inspired one of the core principles of machine learning: the ability to learn and improve from data rather than following fixed, pre-programmed rules.

Another striking feature of biological intelligence is its parallel processing capability. The brain processes vast amounts of information simultaneously, with different regions specializing in specific functions while working in concert. This distributed, parallel architecture stands in stark contrast to traditional sequential computing and has significantly influenced modern AI system design. The brain's ability to efficiently coordinate these parallel processes while maintaining coherent function represents a level of sophistication we're still working to fully understand and replicate.

The brain's pattern recognition capabilities are particularly noteworthy. Biological systems excel at identifying patterns in complex, noisy data---whether recognizing faces in a crowd, understanding speech in a noisy environment, or identifying objects from partial information. This remarkable ability has inspired numerous AI applications, particularly in computer vision and speech recognition systems. The brain accomplishes these tasks with an efficiency that artificial systems are still striving to match.

Perhaps most remarkably, biological systems achieve all this with incredible energy efficiency. The human brain operates on approximately 20 watts of power---about the same as a low-power light bulb---while performing complex cognitive tasks that would require orders of magnitude more power in current artificial systems. This efficiency hasn't just impressed researchers; it has become a crucial goal in the development of AI hardware and algorithms.

These biological principles have led to two distinct but complementary approaches in artificial intelligence. The first attempts to directly mimic neural structure and function, leading to artificial neural networks and deep learning architectures that structurally resemble biological neural networks. The second takes a more abstract approach, adapting biological principles to work efficiently within the constraints of computer hardware without necessarily copying biological structures exactly. In the following sections, we will explore how these approaches manifest in practice, beginning with the fundamental building block of neural networks: the neuron itself.

#### Biological to Artificial Neurons

To understand how biological principles translate into artificial systems, we must first examine the basic unit of biological information processing: the neuron. This cellular building block provides the blueprint for its artificial counterpart and helps us understand how complex neural networks emerge from simple components working in concert.

In biological systems, the neuron (or cell) is the basic functional unit of the nervous system. Understanding its structure is crucial before we draw parallels to artificial systems. @fig-bio_nn2ai_nn illustrates the structure of a biological neuron.

![Bilogical structure of a neuron and its mapping to an artificial neuron. Source: Geeksforgeeks](images/png/bio_nn2ai_nn.png){#fig-bio_nn2ai_nn}

A biological neuron consists of several key components. The central part is the cell body, or soma, which contains the nucleus and performs the cell's basic life processes. Extending from the soma are branch-like structures called dendrites, which receive signals from other neurons. At the junctions where signals are passed between neurons are synapses. Finally, a long, slender projection called the axon conducts electrical impulses away from the cell body to other neurons.

The neuron functions as follows: Dendrites receive inputs from other neurons, with synapses determining the strength of the connections. The soma integrates these signals and decides whether to trigger an output signal. If triggered, the axon transmits this signal to other neurons.

Each element of a biological neuron has a computational analog in artificial systems, reflecting the principles of learning, adaptability, and efficiency found in nature. To better understand how biological intelligence informs artificial systems, @tbl-bio_nn2ai_nn captures the mapping between the components of biological and artificial neurons. This should be viewed alongside @fig-bio_nn2ai_nn for a complete picture. Together, they paint a picture of the biological-to-artificial neuron mapping.

+-----------------------+-----------------------+
| Biological Neuron     | Artificial Neuron     |
+:======================+:======================+
| Cell                  | Neuron / Node         |
+-----------------------+-----------------------+
| Dendrites / Synapse   | Weights               |
+-----------------------+-----------------------+
| Soma                  | Net Input             |
+-----------------------+-----------------------+
| Axon                  | Output                |
+-----------------------+-----------------------+

: Mapping the biological neuron structure to an artificial neuron. {#tbl-bio_nn2ai_nn .striped .hover}

Each component serves a similar function, albeit through vastly different mechanisms. Here, we explain these mappings and their implications for artificial neural networks.

1. **Cell ↔ Neuron/Node**: The artificial neuron or node serves as the fundamental computational unit, mirroring the cell's role in biological systems.

2. **Dendrites/Synapse ↔ Weights**  Weights in artificial neurons represent connection strengths, analogous to synapses in biological neurons. These weights are adjustable, enabling learning and optimization over time.

3. **Soma ↔ Net Input**  The net input in artificial neurons sums weighted inputs to determine activation, similar to how the soma integrates signals in biological neurons.

4. **Axon ↔ Output** The output of an artificial neuron passes processed information to subsequent network layers, much like an axon transmits signals to other neurons.

This mapping illustrates how artificial neural networks simplify and abstract biological processes while preserving their essential computational principles. However, understanding individual neurons is just the beginning—the true power of neural networks emerges from how these basic units work together in larger systems.

#### Artificial Intelligence

The translation from biological principles to artificial computation requires a deep appreciation of what makes biological neural networks so effective at both the cellular and network levels. The brain processes information through distributed computation across billions of neurons, each operating relatively slowly compared to silicon transistors. A biological neuron fires at approximately 200Hz, while modern processors operate at gigahertz frequencies. Despite this speed limitation, the brain's parallel architecture enables sophisticated real-time processing of complex sensory input, decision making, and control of behavior.

This computational efficiency emerges from the brain's basic organizational principles. Each neuron acts as a simple processing unit, integrating inputs from thousands of other neurons and producing a binary output signal based on whether this integrated input exceeds a threshold. The connection strengths between neurons, mediated by synapses, are continuously modified through experience. This synaptic plasticity forms the basis for learning and adaptation in biological neural networks. These biological principles suggest key computational elements needed in artificial neural systems:

* Simple processing units that integrate multiple inputs
* Adjustable connection strengths between units
* Nonlinear activation based on input thresholds
* Parallel processing architecture
* Learning through modification of connection strengths

#### Computational Translation

We now face the challenge of capturing the essence of neural computation within the rigid framework of digital systems. This translation requires careful consideration of how to represent neural activity, connection strengths, and learning mechanisms in ways that can be efficiently computed while preserving the essential computational properties that make biological neural networks effective.

The challenge in artificial neural networks is in implementing these principles within the constraints of digital computing systems. This translation requires careful consideration of how to represent neural activity, connection strengths, and learning mechanisms in ways that can be efficiently computed while preserving the essential computational properties that make biological neural networks effective.

The implementation of biological principles in artificial neural systems represents a nuanced balance between biological fidelity and computational efficiency. At its core, an artificial neuron captures the essential computational properties of its biological counterpart through mathematical operations that can be efficiently executed on digital hardware.

@tbl-bio2comp provides a systematic view of how key biological features map to their computational counterparts. Each biological feature has an analog in computational systems, revealing both the possibilities and limitations of digital neural implementation, which we will learn more about later.

+---------------------+---------------------------+
| Biological Feature  | Computational Translation |
+:====================+:==========================+
| Neuron firing       | Activation function       |
+---------------------+---------------------------+
| Synaptic strength   | Weighted connections      |
+---------------------+---------------------------+
| Signal integration  | Summation operation       |
+---------------------+---------------------------+
| Distributed memory  | Weight matrices           |
+---------------------+---------------------------+
| Parallel processing | Concurrent computation    |
+---------------------+---------------------------+

: Translating biological features to the computing domain. {#tbl-bio2comp .striped .hover}

The basic computational unit in artificial neural networks, the artificial neuron, simplifies the complex electrochemical processes of biological neurons into three fundamental operations. First, input signals are weighted, mimicking how biological synapses modulate incoming signals with different strengths. Second, these weighted inputs are summed together, analogous to how a biological neuron integrates incoming signals in its cell body. Finally, the summed input passes through an activation function that determines the neuron's output, similar to how a biological neuron fires based on whether its membrane potential exceeds a threshold.

This mathematical abstraction preserves key computational principles while enabling efficient digital implementation. The weighting of inputs allows the network to learn which connections are important, just as biological neural networks strengthen or weaken synaptic connections through experience. The summation operation captures how biological neurons integrate multiple inputs into a single decision. The activation function introduces nonlinearity essential for learning complex patterns, much like the threshold-based firing of biological neurons.

Memory in artificial neural networks takes a markedly different form from biological systems. While biological memories are distributed across synaptic connections and neural patterns, artificial networks store information in discrete weights and parameters. This architectural difference reflects the constraints of current computing hardware, where memory and processing are physically separated rather than integrated as in biological systems. Despite these implementation differences, artificial neural networks achieve similar functional capabilities in pattern recognition and learning.

The brain's massive parallelism finds a limited analog in modern AI hardware. While biological neural networks process information through billions of neurons operating simultaneously, artificial systems approximate this parallelism through specialized hardware like GPUs and tensor processing units. These devices efficiently compute the matrix operations that form the mathematical foundation of artificial neural networks, achieving parallel processing at a different scale and granularity than biological systems.

#### System Requirements

The computational translation that we explored creates specific demands on the underlying computing infrastructure, each arising from the fundamental differences between biological and artificial implementations. These requirements emerge directly from the fundamental differences between biological and artificial implementations of neural processing.

The computational elements we identified in our biological-to-computational translation each create specific demands on the computing infrastructure. @tbl-comp2sys shows how each computational element drives particular system requirements. From this mapping, we can see how the choices made in computational translation directly influence the hardware and system architecture needed for implementation.

+-----------------------+---------------------------------+
| Computational Element | System Requirements             |
+:======================+:================================+
| Activation functions  | Fast nonlinear operation units  |
+-----------------------+---------------------------------+
| Weight operations     | High-bandwidth memory access    |
+-----------------------+---------------------------------+
| Parallel computation  | Specialized parallel processors |
+-----------------------+---------------------------------+
| Weight storage        | Large-scale memory systems      |
+-----------------------+---------------------------------+
| Learning algorithms   | Gradient computation hardware   |
+-----------------------+---------------------------------+

: From computation to system requirements. {#tbl-comp2sys .striped .hover}

The first major requirement stems from the challenge of replicating the brain's parallel processing capabilities. While biological neurons operate naturally in parallel across billions of units, artificial systems must explicitly architect for parallelism. This necessitates specialized processors capable of handling many simultaneous calculations, fundamentally different from traditional sequential computing architectures. Unlike the brain's distributed processing, our artificial systems must coordinate these parallel operations through carefully designed hardware and software structures.

Storage architecture represents another important requirement, driven by the fundamental difference in how biological and artificial systems handle memory. In biological systems, memory and processing are intrinsically integrated—synapses both store connection strengths and process signals. Artificial systems, however, must maintain a clear separation between processing units and memory. This creates a need for both high-capacity storage to hold millions or billions of connection weights and high-bandwidth pathways to move this data quickly between storage and processing units. The efficiency of this data movement often becomes a critical bottleneck that biological systems do not face.

The learning process itself imposes distinct requirements on artificial systems. While biological networks modify synaptic strengths through local chemical processes, artificial networks must coordinate weight updates across the entire network. This creates substantial computational and memory demands during training—systems must not only store current weights but also maintain space for gradients and intermediate calculations. The requirement to backpropagate error signals, with no real biological analog, further complicates the system architecture.

Energy efficiency emerges as a final critical requirement, highlighting perhaps the starkest contrast between biological and artificial implementations. The human brain's remarkable energy efficiency—operating on roughly 20 watts—stands in sharp contrast to the substantial power demands of artificial neural networks. Current systems often require orders of magnitude more energy to implement similar capabilities. This gap drives ongoing research in more efficient hardware architectures and has profound implications for the practical deployment of neural networks, particularly in resource-constrained environments like mobile devices or edge computing systems.

#### Evolution and Impact

We can now better appreciate how the field of deep learning evolved to meet these challenges through advances in hardware and algorithms. This journey began with early artificial neural networks in the 1950s, marked by the introduction of the Perceptron. While groundbreaking in concept, these early systems were severely limited by the computational capabilities of their era—primarily mainframe computers that lacked both the processing power and memory capacity needed for complex networks.

The development of backpropagation algorithms in the 1980s [@rumelhart1986learning], which we will learn about later, represented a  theoretical breakthrough and povided a systematic way to train multi-layer networks. However, the computational demands of this algorithm far exceeded available hardware capabilities. Training even modest networks could take weeks, making experimentation and practical applications challenging. This mismatch between algorithmic requirements and hardware capabilities contributed to a period of reduced interest in neural networks.

The term "deep learning" gained prominence in the 2000s, coinciding with significant advances in computational power and data accessibility. The field has since experienced exponential growth, as illustrated in @fig-trends. The graph reveals two remarkable trends: computational capabilities measured in the number of Floating Point Operations per Second (FLOPS) initially followed a 1.4x improvement pattern from 1952 to 2010, then accelerated to a 3.4-month doubling cycle from 2012 to 2022. Perhaps more striking is the emergence of large-scale models between 2015 and 2022 (not explicitly shown or easily seen in the figure), which scaled 2 to 3 orders of magnitude faster than the general trend, following an aggressive 10-month doubling cycle.

![Growth of deep learning models. Source: EOOCHS AI](https://epochai.org/assets/images/posts/2022/compute-trends.png){#fig-trends}

The evolutionary trends were driven by parallel advances across three fundamental dimensions: data availability, algorithmic innovations, and computing infrastructure. These three factors—data, algorithms, and infrastructure—reinforced each other in a virtuous cycle that continues to drive progress in the field today.

These three factors—data, algorithms, and infrastructure—reinforced each other. As @fig-virtuous-cycle shows, more powerful computing infrastructure enabled processing larger datasets. Larger datasets drove algorithmic innovations. Better algorithms demanded more sophisticated computing systems. This virtuous cycle continues to drive progress in the field today.

![The virtuous cycle enabled by key breakthroughs in each layer.](images/png/virtuous-cycle.png){#fig-virtuous-cycle width=40%}

The data revolution transformed what was possible with neural networks. The rise of the internet and digital devices created unprecedented access to training data. Image sharing platforms provided millions of labeled images. Digital text collections enabled language processing at scale. Sensor networks and IoT devices generated continuous streams of real-world data. This abundance of data provided the raw material needed for neural networks to learn complex patterns effectively.

Algorithmic innovations made it possible to harness this data effectively. New methods for initializing networks and controlling learning rates made training more stable. Techniques for preventing overfitting allowed models to generalize better to new data. Most importantly, researchers discovered that neural network performance scaled predictably with model size, computation, and data quantity, leading to increasingly ambitious architectures.

Computing infrastructure evolved to meet these growing demands. On the hardware side, graphics processing units (GPUs) provided the parallel processing capabilities needed for efficient neural network computation. Specialized AI accelerators like TPUs [@jouppi2017datacenter] pushed performance further. High-bandwidth memory systems and fast interconnects addressed data movement challenges. Equally important were software advances—frameworks and libraries that made it easier to build and train networks, distributed computing systems that enabled training at scale, and tools for optimizing model deployment.

## Neural Network Foundations

We can now examine the fundamental building blocks that make machine learning systems work. While the field has grown tremendously in sophistication, all modern neural networks—from simple classifiers to large language models—share a common architectural foundation built upon basic computational units and principles.

This foundation begins with understanding how individual artificial neurons process information, how they are organized into layers, and how these layers are connected to form complete networks. By starting with these fundamental concepts, we can progressively build up to understanding more complex architectures and their applications.

### Basic Architecture

The architecture of a neural network determines how information flows through the system, from input to output. While modern networks can be tremendously complex, they all build upon a few key organizational principles that we will explore in the following sections. Understanding these principles is essential for both implementing neural networks and appreciating how they achieve their remarkable capabilities.

#### Neurons and Activations

The Perceptron is the basic unit or node that forms the foundation for more complex structures. It functions by taking multiple inputs, each representing a feature of the object under analysis, such as the characteristics of a home for predicting its price or the attributes of a song to forecast its popularity in music streaming services. These inputs are denoted as $x_1, x_2, ..., x_n$. A perceptron can be configured to perform either regression or classification tasks. For regression, the actual numerical output $\hat{y}$ is used. For classification, the output depends on whether $\hat{y}$ crosses a certain threshold. If $\hat{y}$ exceeds this threshold, the perceptron might output one class (e.g., 'yes'), and if it does not, another class (e.g., 'no').

@fig-perceptron illustrates the fundamental building blocks of a perceptron, which serves as the foundation for more complex neural networks. A perceptron can be thought of as a miniature decision-maker, utilizing its weights, bias, and activation function to process inputs and generate outputs based on learned parameters. This concept forms the basis for understanding more intricate neural network architectures, such as multilayer perceptrons. In these advanced structures, layers of perceptrons work in concert, with each layer's output serving as the input for the subsequent layer. This hierarchical arrangement creates a deep learning model capable of comprehending and modeling complex, abstract patterns within data. By stacking these simple units, neural networks gain the ability to tackle increasingly sophisticated tasks, from image recognition to natural language processing.

![Perceptron. Conceived in the 1950s, perceptrons paved the way for developing more intricate neural networks and have been a fundamental building block in deep learning. Source: Wikimedia - Chrislb.](images/png/Rosenblattperceptron.png){#fig-perceptron}

Each input $x_i$ has a corresponding weight $w_{ij}$, and the perceptron simply multiplies each input by its matching weight. This operation is similar to linear regression, where the intermediate output, $z$, is computed as the sum of the products of inputs and their weights:

$$
z = \sum (x_i \cdot w_{ij})
$$

To this intermediate calculation, a bias term $b$ is added, allowing the model to better fit the data by shifting the linear output function up or down. Thus, the intermediate linear combination computed by the perceptron including the bias becomes:

$$
z = \sum (x_i \cdot w_{ij}) + b
$$

This basic form of a perceptron can only model linear relationships between the input and output. Patterns found in nature are often complex and extend beyond linear relationships. To enable the perceptron to handle non-linear relationships, an activation function is applied to the linear output $z$.

$$
\hat{y} = \sigma(z)
$$

@fig-nonlinear illustrates an example where data exhibit a nonlinear pattern that could not be adequately modeled with a linear approach. The activation function, such as sigmoid, tanh, or ReLU, transforms the linear input sum into a non-linear output. The primary objective of this function is to introduce non-linearity into the model, enabling it to learn and perform more sophisticated tasks. Thus, the final output of the perceptron, including the activation function, can be expressed as:

![Activation functions enable the modeling of complex non-linear relationships. Source: Medium - Sachin Kaushik.](images/png/nonlinear_patterns.png){#fig-nonlinear}

#### Layers and Connections

While a single perceptron can model simple decisions, the power of neural networks comes from combining multiple neurons into layers. A layer is a collection of neurons that process information in parallel. Each neuron in a layer operates independently on the same input but with its own set of weights and bias, allowing the layer to learn different features or patterns from the same input data.

In a typical neural network, we organize these layers hierarchically:

1. **Input Layer**: Receives the raw data features
2. **Hidden Layers**: Process and transform the data through multiple stages
3. **Output Layer**: Produces the final prediction or decision

@fig-layers illustrates this layered architecture. When data flows through these layers, each successive layer transforms the representation of the data, gradually building more complex and abstract features. This hierarchical processing is what gives deep neural networks their remarkable ability to learn complex patterns.

![Neural network layers. Source: BrunelloN](images/png/nnlayers.png){#fig-layers}

#### Data Flow and Layer Transformations

When data moves from one layer to the next, it undergoes a series of transformations. For a layer with $m$ neurons receiving input from a previous layer with $n$ features, we can express this transformation mathematically as:

$$
\mathbf{z}^{(l)} = \mathbf{W}^{(l)}\mathbf{x}^{(l-1)} + \mathbf{b}^{(l)}
$$

Where:

* $\mathbf{x}^{(l-1)}$ is the input vector from the previous layer

* $\mathbf{W}^{(l)}$ is the weight matrix for the current layer

* $\mathbf{b}^{(l)}$ is the bias vector

* $\mathbf{z}^{(l)}$ is the pre-activation output

### Weights and Biases

#### Weight Matrices

While we introduced weights in the context of a single perceptron, in practice neural networks organize weights into matrices to enable efficient computation across entire layers. For a layer with $n$ input features and $m$ neurons, the weights form a matrix $\mathbf{W} \in \mathbb{R}^{n \times m}$. Each column in this matrix represents the weights for a single neuron in the layer.

Let's consider how this extends our previous perceptron equations to handle multiple neurons simultaneously. For a layer of $m$ neurons, instead of computing each neuron's output separately:

$$
z_j = \sum_{i=1}^n (x_i \cdot w_{ij}) + b_j
$$

We can compute all outputs at once using matrix multiplication:

$$
\mathbf{z} = \mathbf{x}^T\mathbf{W} + \mathbf{b}
$$

This matrix organization is more than just mathematical convenience---it reflects how modern neural networks are implemented for efficiency. Each weight $w_{ij}$ represents the strength of the connection between input feature $i$ and neuron $j$ in the layer.

#### Connection Patterns

In the simplest and most common case, each neuron in a layer is connected to every neuron in the previous layer, forming what we call a "dense" or "fully-connected" layer. This pattern means that each neuron has the opportunity to learn from all available features from the previous layer.

@fig-connections illustrates these dense connections between layers. For a network with layers of sizes $(n_1, n_2, n_3)$, the weight matrices would have dimensions:

![Dense connections between layers in a MLP. Source: J. McCaffrey](images/png/mlp_connection_weights.png){#fig-connections}

* Between first and second layer: $\mathbf{W}^{(1)} \in \mathbb{R}^{n_1 \times n_2}$
* Between second and third layer: $\mathbf{W}^{(2)} \in \mathbb{R}^{n_2 \times n_3}$

#### Bias Terms

Each neuron in a layer also has an associated bias term. While weights determine the relative importance of inputs, biases allow neurons to shift their activation functions. This shifting is crucial for learning, as it gives the network flexibility to fit more complex patterns.

For a layer with $m$ neurons, the bias terms form a vector $\mathbf{b} \in \mathbb{R}^m$. When we compute the layer's output, this bias vector is added to the weighted sum of inputs:

$$
\mathbf{z} = \mathbf{x}^T\mathbf{W} + \mathbf{b}
$$

The bias terms effectively allow each neuron to have a different "threshold" for activation, making the network more expressive.

#### Parameter Organization

The organization of weights and biases across a neural network follows a systematic pattern. For a network with $L$ layers, we maintain:

* A weight matrix $\mathbf{W}^{(l)}$ for each layer $l$

* A bias vector $\mathbf{b}^{(l)}$ for each layer $l$f

* Activation functions $f^{(l)}$ for each layer $l$

This gives us the complete layer computation:

$$
\mathbf{h}^{(l)} = f^{(l)}(\mathbf{z}^{(l)}) = f^{(l)}(\mathbf{h}^{(l-1)T}\mathbf{W}^{(l)} + \mathbf{b}^{(l)})
$$

Where $\mathbf{h}^{(l)}$ represents the layer's output after applying the activation function.

### Network Topology

Network topology describes how the basic building blocks we've discussed—neurons, layers, and connections—come together to form a complete neural network. We can best understand network topology through a concrete example. Consider the task of recognizing handwritten digits, a classic problem in deep learning using the MNIST[^defn-MNIST] dataset.

[^defn-MNIST]: MNIST (Modified National Institute of Standards and Technology) is a large database of handwritten digits that has been widely used to train and test machine learning systems since its creation in 1998. The dataset consists of 60,000 training images and 10,000 testing images, each being a 28×28 pixel grayscale image of a single handwritten digit from 0 to 9.

#### Basic Structure

The fundamental structure of a neural network consists of three main components: input layer, hidden layers, and output layer. As shown in @fig-mnist-topology-1, a 28×28 pixel grayscale image of a handwritten digit must be processed through these layers to produce a classification output.

::: {layout-nrow=1}

![A neural network topology for classifying MNIST digits, showing how a 28x28 pixel image is processed. The image on the left shows the original digit, with dimensions labeled. The network on the right shows how each pixel connects to the hidden layers, ultimately producing 10 outputs for digit classification.](images/png/topology_28x28.png){#fig-mnist-topology-1}

![Alternative visualization of the MNIST network topology, showing how the 2D image is flattened into a 784-dimensional vector before being processed by the network. This representation emphasizes how spatial data is transformed into a format suitable for neural network processing.](images/png/topology_flatten.png){#fig-mnist-topology-2}

:::

The input layer's width is directly determined by our data format. As shown in @fig-mnist-topology-2, for a 28×28 pixel image, each pixel becomes an input feature, requiring 784 input neurons (28 × 28 = 784). We can think of this either as a 2D grid of pixels or as a flattened vector of 784 values, where each value represents the intensity of one pixel.

The output layer's structure is determined by our task requirements. For digit classification, we use 10 output neurons, one for each possible digit (0-9). When presented with an image, the network produces a value for each output neuron, where higher values indicate greater confidence that the image represents that particular digit.

Between these fixed input and output layers, we have flexibility in designing the hidden layer topology. The choice of hidden layer structure—how many layers to use and how wide to make them—represents one of the fundamental design decisions in neural networks. Additional layers increase the network's depth, allowing it to learn more abstract features through successive transformations. The width of each layer provides capacity for learning different features at each level of abstraction.

These basic topological choices have significant implications for both the network's capabilities and its computational requirements. Each additional layer or neuron increases the number of parameters that must be stored and computed during both training and inference. However, without sufficient depth or width, the network may lack the capacity to learn complex patterns in the data.

#### Design Trade-offs

The design of neural network topology centers on three fundamental decisions: the number of layers (depth), the size of each layer (width), and how these layers connect. Each choice affects both the network's learning capability and its computational requirements.

Network depth determines the level of abstraction the network can achieve. Each layer transforms its input into a new representation, and stacking multiple layers allows the network to build increasingly complex features. In our MNIST example, a deeper network might first learn to detect edges, then combine these edges into strokes, and finally assemble strokes into complete digit patterns. However, adding layers isn't always beneficial—each additional layer increases computational cost and can make the network harder to train.

The width of each layer—the number of neurons it contains—controls how much information the network can process in parallel at each stage. Wider layers can learn more features simultaneously but require proportionally more parameters and computation. For instance, if a hidden layer is processing edge features in our digit recognition task, its width determines how many different edge patterns it can detect simultaneously.

A very important consideration in topology design is the total parameter count. For a network with layers of size $(n_1, n_2, ..., n_L)$, each pair of adjacent layers $l$ and $l+1$ requires $n_l \times n_{l+1}$ weight parameters, plus $n_{l+1}$ bias parameters. These parameters must be stored in memory and updated during training, making the parameter count a key constraint in practical applications.

When designing networks, we need to balance learning capacity, computational efficiency, and ease of training. While the basic approach connects every neuron to every neuron in the next layer, this isn't always the most effective strategy. Sometimes, using fewer but more strategic connections can achieve better results with less computation. Consider our MNIST example---when humans recognize digits, we don't analyze every pixel independently but look for meaningful patterns like lines and curves. Similarly, we can design our network to focus on local patterns in the image rather than treating each pixel as completely independent.

Another important consideration is how information flows through the network. While the basic flow is from input to output, some network designs include additional paths for information to flow. These alternative paths can make the network easier to train and more effective at learning complex patterns. Think of these as shortcuts that help information flow more directly when needed, similar to how our brain can combine both detailed and general impressions when recognizing objects.

These design decisions have significant practical implications for:

* Memory usage for storing network parameters
* Computational costs during both training and inference
* Training behavior and convergence
* The network's ability to generalize to new examples

#### Common Patterns

Neural networks can be structured with different connection patterns between layers, each offering distinct advantages for learning and computation. Understanding these fundamental patterns provides insight into how networks process information and learn representations from data.

Dense connectivity represents the standard pattern where each neuron connects to every neuron in the subsequent layer. In our MNIST example, connecting our 784-dimensional input layer to a hidden layer of 100 neurons requires 78,400 weight parameters. This full connectivity enables the network to learn arbitrary relationships between inputs and outputs, but the number of parameters scales quadratically with layer width.

Sparse connectivity patterns introduce purposeful restrictions in how neurons connect between layers. Rather than maintaining all possible connections, neurons connect to only a subset of neurons in the adjacent layer. This approach draws inspiration from biological neural systems, where neurons typically form connections with a limited number of other neurons. In visual processing tasks like our MNIST example, neurons might connect only to inputs representing nearby pixels, reflecting the local nature of visual features.

As networks grow deeper, the path from input to output becomes longer, potentially complicating the learning process. Skip connections address this by adding direct paths between non-adjacent layers. These connections provide alternative routes for information flow, supplementing the standard layer-by-layer progression. In our digit recognition example, skip connections might allow later layers to reference both high-level patterns and the original pixel values directly.

These connection patterns have significant implications for both the theoretical capabilities and practical implementation of neural networks. Dense connections maximize learning flexibility at the cost of computational efficiency. Sparse connections can reduce computational requirements while potentially improving the network's ability to learn structured patterns. Skip connections help maintain effective information flow in deeper networks.

#### Weight Considerations

The arrangement of weights in a neural network fundamentally determines both its learning capacity and computational requirements. While topology defines the network's structure, the initialization and organization of weights within this structure plays a crucial role in how the network learns and performs.

The number of weights in a network grows with both width and depth. For our MNIST example, consider a network with a 784-dimensional input layer, two hidden layers of 100 neurons each, and a 10-neuron output layer. The first layer requires 78,400 weights (784 × 100), the second layer 10,000 weights (100 × 100), and the output layer 1,000 weights (100 × 10), totaling 89,400 weights. Each of these weights must be stored in memory and updated during learning.

Weight initialization plays a fundamental role in network behavior. When we create a new neural network, these weights must be set to initial values that enable effective learning. Setting all weights to zero would cause all neurons in a layer to behave identically, preventing the network from learning diverse features. Instead, weights are typically initialized randomly, but the scale of these random values matters significantly. Too large or too small initial weights can lead to poor learning dynamics.

The distribution of weights across the network affects how information flows through layers. Consider our digit recognition task: if weights in early layers are too small, important details from the input image might not be preserved for later layers to process. Conversely, if weights are too large, the network might amplify noise in the input, making it harder to identify relevant patterns.

Different network architectures may impose specific constraints on how weights are organized. Some architectures share weights across different parts of the network to encode specific properties, such as the ability to recognize patterns regardless of their position in an image. Other architectures might restrict certain weights to be zero, effectively implementing the sparse connectivity patterns discussed earlier.

### Learning Process

Neural networks learn to perform tasks through a process of training on examples. This process transforms the network from its initial state, where its weights are randomly initialized, to a trained state where the weights encode meaningful patterns from the training data. Understanding this learning process is fundamental to both the theoretical foundations and practical implementations of deep learning systems.

#### Training Overview

The core principle of neural network training is supervised learning from labeled examples. Consider our MNIST digit recognition task: we have a dataset of 60,000 training images, each a 28×28 pixel grayscale image paired with its correct digit label. The network must learn the relationship between these images and their corresponding digits through an iterative process of prediction and weight adjustment.

Training operates as a loop, where each iteration involves processing a subset of training examples called a batch. For each batch, the network performs several key operations:

* Forward computation through the network layers to generate predictions
* Evaluation of prediction accuracy using a loss function
* Computation of weight adjustments based on prediction errors
* Update of network weights to improve future predictions

This process can be expressed mathematically. Given an input image $x$ and its true label $y$, the network computes its prediction:

$$
\hat{y} = f(x; \theta)
$$

where $f$ represents the neural network function and $\theta$ represents all trainable parameters (weights and biases, which we discussed earlier). The network's error is measured by a loss function $L$:

$$
\text{loss} = L(\hat{y}, y)
$$

This error measurement drives the adjustment of network parameters through a process called "backpropagation," which we will examine in detail later.

In practice, training operates on batches of examples rather than individual inputs. For the MNIST dataset, each training iteration might process 32, 64, or 128 images simultaneously. This batch processing serves two purposes: it enables efficient use of modern computing hardware through parallel processing, and it provides more stable parameter updates by averaging errors across multiple examples.

The training cycle continues until the network achieves sufficient accuracy or reaches a predetermined number of iterations. Throughout this process, the loss function serves as a guide, with its minimization indicating improved network performance.

#### Forward Propagation

Forward propagation, as illustrated in @fig-forward-propagation, is the core computational process in a neural network, where input data flows through the network's layers to generate predictions. Understanding this process is essential as it forms the foundation for both network inference and training. Let's examine how forward propagation works using our MNIST digit recognition example.

![Neural networks - forward and backward propagation. Source: [Linkedin](https://www.linkedin.com/pulse/lecture2-unveiling-theoretical-foundations-ai-machine-underdown-phd-oqsuc/)](images/png/forwardpropagation.png){#fig-forward-propagation}

When an image of a handwritten digit enters our network, it undergoes a series of transformations through the layers. Each transformation combines the weighted inputs with learned patterns to progressively extract relevant features. In our MNIST example, a 28×28 pixel image is processed through multiple layers to ultimately produce probabilities for each possible digit (0-9).

The process begins with the input layer, where each pixel's grayscale value becomes an input feature. For MNIST, this means 784 input values (28 × 28 = 784), each normalized between 0 and 1. These values then propagate forward through the hidden layers, where each neuron combines its inputs according to its learned weights and applies a nonlinear activation function.

##### Layer-by-Layer Computation

The forward computation through a neural network proceeds systematically, with each layer transforming its inputs into increasingly abstract representations. In our MNIST network, this transformation process occurs in distinct stages.

At each layer, the computation involves two key steps: a linear transformation of inputs followed by a nonlinear activation. The linear transformation combines all inputs to a neuron using learned weights and a bias term. For a single neuron receiving inputs from the previous layer, this computation takes the form:

$$
z = \sum_{i=1}^n w_ix_i + b
$$

where $w_i$ represents the weights, $x_i$ the inputs, and $b$ the bias term. For an entire layer of neurons, we can express this more efficiently using matrix operations:

$$
\mathbf{Z}^{(l)} = \mathbf{W}^{(l)}\mathbf{A}^{(l-1)} + \mathbf{b}^{(l)}
$$

Here, $\mathbf{W}^{(l)}$ represents the weight matrix for layer $l$, $\mathbf{A}^{(l-1)}$ contains the activations from the previous layer, and $\mathbf{b}^{(l)}$ is the bias vector.

Following this linear transformation, each layer applies a nonlinear activation function $f$:

$$
\mathbf{A}^{(l)} = f(\mathbf{Z}^{(l)})
$$

This process repeats at each layer, creating a chain of transformations:

Input → Linear Transform → Activation → Linear Transform → Activation → ... → Output

In our MNIST example, the pixel values first undergo a transformation by the first hidden layer's weights, converting the 784-dimensional input into an intermediate representation. Each subsequent layer further transforms this representation, ultimately producing a 10-dimensional output vector representing the network's confidence in each possible digit.

##### Mathematical Representation

The complete forward propagation process can be expressed as a composition of functions, each representing a layer's transformation. Let us formalize this mathematically, building on our MNIST example.

For a network with L layers, we can express the full forward computation as:

$$
\mathbf{A}^{(L)} = f^{(L)}(\mathbf{W}^{(L)}f^{(L-1)}(\mathbf{W}^{(L-1)}...(f^{(1)}(\mathbf{W}^{(1)}\mathbf{X} + \mathbf{b}^{(1)}))... + \mathbf{b}^{(L-1)}) + \mathbf{b}^{(L)})
$$

While this nested expression captures the complete process, we typically compute it step by step:

1. First layer:

$$
\mathbf{Z}^{(1)} = \mathbf{W}^{(1)}\mathbf{X} + \mathbf{b}^{(1)}
$$
$$
\mathbf{A}^{(1)} = f^{(1)}(\mathbf{Z}^{(1)})
$$

2. Hidden layers (l = 2, ..., L-1):

$$
\mathbf{Z}^{(l)} = \mathbf{W}^{(l)}\mathbf{A}^{(l-1)} + \mathbf{b}^{(l)}
$$
$$
\mathbf{A}^{(l)} = f^{(l)}(\mathbf{Z}^{(l)})
$$

3. Output layer:

$$
\mathbf{Z}^{(L)} = \mathbf{W}^{(L)}\mathbf{A}^{(L-1)} + \mathbf{b}^{(L)}
$$
$$
\mathbf{A}^{(L)} = f^{(L)}(\mathbf{Z}^{(L)})
$$

In our MNIST example, if we have a batch of B images, the dimensions of these operations are:

* Input $\mathbf{X}$: B × 784
* First layer weights $\mathbf{W}^{(1)}$: n₁ × 784
* Hidden layer weights $\mathbf{W}^{(l)}$: nₗ × n_{l-1}
* Output layer weights $\mathbf{W}^{(L)}$: 10 × n_{L-1}

##### Computational Process

To understand how these mathematical operations translate into actual computation, let's walk through the forward propagation process for a batch of MNIST images. This process illustrates how data is transformed from raw pixel values to digit predictions.

Consider a batch of 32 images entering our network. Each image starts as a 28×28 grid of pixel values, which we flatten into a 784-dimensional vector. For the entire batch, this gives us an input matrix $\mathbf{X}$ of size 32×784, where each row represents one image. The values are typically normalized to lie between 0 and 1.

The transformation at each layer proceeds as follows:

1. Input Layer Processing
   The network takes our input matrix $\mathbf{X}$ (32×784) and transforms it using the first layer's weights. If our first hidden layer has 128 neurons, $\mathbf{W}^{(1)}$ is a 784×128 matrix. The resulting computation $\mathbf{X}\mathbf{W}^{(1)}$ produces a 32×128 matrix.

2. Hidden Layer Transformations
   Each element in this matrix then has its corresponding bias added and passes through an activation function. For example, with a ReLU activation, any negative values become zero while positive values remain unchanged. This nonlinear transformation enables the network to learn complex patterns in the data.

3. Output Generation
   The final layer transforms its inputs into a 32×10 matrix, where each row contains 10 values corresponding to the network's confidence scores for each possible digit. Often, these scores are converted to probabilities using a softmax function:

$$
P(\text{digit } j) = \frac{e^{z_j}}{\sum_{k=1}^{10} e^{z_k}}
$$

For each image in our batch, this gives us a probability distribution over the possible digits. The digit with the highest probability becomes the network's prediction.

##### Practical Considerations

The implementation of forward propagation requires careful attention to several practical aspects that affect both computational efficiency and memory usage. These considerations become particularly important when processing large batches of data or working with deep networks.

Memory management plays a crucial role during forward propagation. Each layer's activations must be stored for potential use in the backward pass during training. For our MNIST example with a batch size of 32, if we have three hidden layers of sizes 128, 256, and 128, the activation storage requirements are:

* First hidden layer: 32 × 128 = 4,096 values
* Second hidden layer: 32 × 256 = 8,192 values
* Third hidden layer: 32 × 128 = 4,096 values
* Output layer: 32 × 10 = 320 values

This gives us a total of 16,704 values that must be maintained in memory for each batch during training. The memory requirements scale linearly with batch size and can become substantial for larger networks.

Batch processing introduces important trade-offs. Larger batches enable more efficient matrix operations and better hardware utilization but require more memory. For example, doubling the batch size to 64 would double our memory requirements for activations. This relationship between batch size, memory usage, and computational efficiency often guides the choice of batch size in practice.

The organization of computations also affects performance. Matrix operations can be optimized through careful memory layout and the use of specialized libraries. The choice of activation functions impacts not only the network's learning capabilities but also its computational efficiency, as some functions (like ReLU) are less expensive to compute than others (like tanh or sigmoid).

These considerations form the foundation for understanding the system requirements of neural networks, which we will explore in more detail in later chapters.

#### Loss Functions

Neural networks learn by measuring and minimizing their prediction errors. Loss functions provide the Algorithmic Structure for quantifying these errors, serving as the essential feedback mechanism that guides the learning process. Through loss functions, we can convert the abstract goal of "making good predictions" into a concrete optimization problem.

To understand the role of loss functions, let's continue with our MNIST digit recognition example. When the network processes a handwritten digit image, it outputs ten numbers representing its confidence in each possible digit (0-9). The loss function measures how far these predictions deviate from the true answer. For instance, if an image shows a "7", we want high confidence for digit "7" and low confidence for all other digits. The loss function penalizes the network when its prediction differs from this ideal.

Consider a concrete example: if the network sees an image of "7" and outputs confidences:

```
[0.1, 0.1, 0.1, 0.0, 0.0, 0.0, 0.2, 0.3, 0.1, 0.1]
```

The highest confidence (0.3) is assigned to digit "7", but this confidence is quite low, indicating uncertainty in the prediction. A good loss function would produce a high loss value here, signaling that the network needs significant improvement. Conversely, if the network outputs:

```
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9, 0.0, 0.1]
```

The loss function should produce a lower value, as this prediction is much closer to ideal.

##### Basic Concepts

Loss functions quantify the difference between the network's predictions and the desired outputs through a scalar value. A lower loss indicates better predictions, while a higher loss signals poorer performance. For our neural network to learn effectively, the loss function must provide meaningful gradients that guide weight updates during training.

Mathematically, a loss function $L$ takes two inputs: the network's predictions $\hat{y}$ and the true values $y$. For a single training example in our MNIST task:

$$
L(\hat{y}, y) = \text{measure of discrepancy between prediction and truth}
$$

When training with batches of data, we typically compute the average loss across all examples in the batch:

$$
L_{\text{batch}} = \frac{1}{B}\sum_{i=1}^B L(\hat{y}_i, y_i)
$$

where $B$ is the batch size and $(\hat{y}_i, y_i)$ represents the prediction and truth for the $i$-th example.

The choice of loss function depends on the type of task. For our MNIST classification problem, we need a loss function that can:

1. Handle probability distributions over multiple classes
2. Provide meaningful gradients for learning
3. Penalize wrong predictions effectively
4. Scale well with batch processing

##### Common Classification Losses

For classification tasks like MNIST digit recognition, "cross-entropy" loss has emerged as the standard choice. This loss function is particularly well-suited for comparing predicted probability distributions with true class labels.

For a single digit image, our network outputs a probability distribution over the ten possible digits. We represent the true label as a one-hot vector where all entries are 0 except for a 1 at the correct digit's position. For instance, if the true digit is "7", the label would be:

$$
y = [0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
$$

The cross-entropy loss for this example is:

$$
L(\hat{y}, y) = -\sum_{j=1}^{10} y_j \log(\hat{y}_j)
$$

where $\hat{y}_j$ represents the network's predicted probability for digit j. Given our one-hot encoding, this simplifies to:

$$
L(\hat{y}, y) = -\log(\hat{y}_c)
$$

where c is the index of the correct class. This means the loss depends only on the predicted probability for the correct digit—the network is penalized based on how confident it is in the right answer.

For example, if our network predicts the following probabilities for an image of "7":

```
Predicted: [0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8, 0.0, 0.1]
True: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
```

The loss would be $-\log(0.8)$, which is approximately 0.223. If the network were more confident and predicted 0.9 for the correct digit, the loss would decrease to approximately 0.105.

##### Loss Computation

The practical computation of loss involves considerations for both numerical stability and batch processing. When working with batches of data, we compute the average loss across all examples in the batch.

For a batch of B examples, the cross-entropy loss becomes:

$$
L_{\text{batch}} = -\frac{1}{B}\sum_{i=1}^B \sum_{j=1}^{10} y_{ij} \log(\hat{y}_{ij})
$$

Computing this loss efficiently requires careful consideration of numerical precision. Taking the logarithm of very small probabilities can lead to numerical instability. Consider a case where our network predicts a probability of 0.0001 for the correct class. Computing $\log(0.0001)$ directly might cause underflow or result in imprecise values.

To address this, we typically implement the loss computation with two key modifications:

1. Add a small epsilon to prevent taking log of zero:

$$
L = -\log(\hat{y} + \epsilon)
$$

2. Apply the log-sum-exp trick for numerical stability:

$$
\text{softmax}(z_i) = \frac{\exp(z_i - \max(z))}{\sum_j \exp(z_j - \max(z))}
$$

For our MNIST example with a batch size of 32, this means:

* Processing 32 sets of 10 probabilities
* Computing 32 individual loss values
* Averaging these values to produce the final batch loss

##### Training Implications

Understanding how loss functions influence training helps explain key implementation decisions in deep learning systems.

During each training iteration, the loss value serves multiple purposes:

1. Performance Metric: It quantifies current network accuracy
2. Optimization Target: Its gradients guide weight updates
3. Convergence Signal: Its trend indicates training progress

For our MNIST classifier, monitoring the loss during training reveals the network's learning trajectory. A typical pattern might show:

* Initial high loss (~2.3, equivalent to random guessing among 10 classes)
* Rapid decrease in early training iterations
* Gradual improvement as the network fine-tunes its predictions
* Eventually stabilizing at a lower loss (~0.1, indicating confident correct predictions)

The loss function's gradients with respect to the network's outputs provide the initial error signal that drives backpropagation. For cross-entropy loss, these gradients have a particularly simple form: the difference between predicted and true probabilities. This mathematical property makes cross-entropy loss especially suitable for classification tasks, as it provides strong gradients even when predictions are very wrong.

The choice of loss function also influences other training decisions:

* Learning rate selection (larger loss gradients might require smaller learning rates)
* Batch size (loss averaging across batches affects gradient stability)
* Optimization algorithm behavior
* Convergence criteria

#### Backward Propagation

Backward propagation, often called backpropagation, is the algorithmic cornerstone of neural network training. While forward propagation computes predictions, backward propagation determines how to adjust the network's weights to improve these predictions. This process enables neural networks to learn from their mistakes.

To understand backward propagation, let's continue with our MNIST example. When the network predicts a "3" for an image of "7", we need a systematic way to adjust weights throughout the network to make this mistake less likely in the future. Backward propagation provides this by calculating how each weight contributed to the error.

The process begins at the network's output, where we compare the predicted digit probabilities with the true label. This error then flows backward through the network, with each layer's weights receiving an update signal based on their contribution to the final prediction. The computation follows the chain rule of calculus, breaking down the complex relationship between weights and final error into manageable steps.

:::{#vid-bp .callout-important}

{{< video <https://www.youtube.com/watch?v=Ilg3gGewQ5U&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&index=3> >}}

:::

##### Gradient Flow

The flow of gradients through a neural network follows a path opposite to the forward propagation. Starting from the loss at the output layer, gradients propagate backwards, computing how each layer, and ultimately each weight, influenced the final prediction error.

In our MNIST example, consider what happens when the network misclassifies a "7" as a "3". The loss function generates an initial error signal at the output layer - essentially indicating that the probability for "7" should increase while the probability for "3" should decrease. This error signal then propagates backward through the network layers.

For a network with L layers, the gradient flow can be expressed mathematically. At each layer l, we compute how the layer's output affected the final loss:

$$
\frac{\partial L}{\partial \mathbf{A}^{(l)}} = \frac{\partial L}{\partial \mathbf{A}^{(l+1)}} \frac{\partial \mathbf{A}^{(l+1)}}{\partial \mathbf{A}^{(l)}}
$$

This computation cascades backward through the network, with each layer's gradients depending on the gradients computed in the layer above it. The process reveals how each layer's transformation contributed to the final prediction error. For instance, if certain weights in an early layer strongly influenced a misclassification, they will receive larger gradient values, indicating a need for more substantial adjustment.

:::{#vid-gd .callout-important}

{{< video <https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&index=2> >}}

:::

##### Computing Gradients

The actual computation of gradients involves calculating several partial derivatives at each layer. For each layer, we need to determine how changes in weights, biases, and activations affect the final loss. These computations follow directly from the chain rule of calculus but must be implemented efficiently for practical neural network training.

At each layer l, we compute three main gradient components:

1. Weight Gradients:

$$
\frac{\partial L}{\partial \mathbf{W}^{(l)}} = \frac{\partial L}{\partial \mathbf{Z}^{(l)}} {\mathbf{A}^{(l-1)}}^T
$$

2. Bias Gradients:

$$
\frac{\partial L}{\partial \mathbf{b}^{(l)}} = \frac{\partial L}{\partial \mathbf{Z}^{(l)}}
$$

3. Input Gradients (for propagating to previous layer):

$$
\frac{\partial L}{\partial \mathbf{A}^{(l-1)}} = {\mathbf{W}^{(l)}}^T \frac{\partial L}{\partial \mathbf{Z}^{(l)}}
$$

In our MNIST example, consider the final layer where the network outputs digit probabilities. If the network predicted [0.1, 0.2, 0.5, ..., 0.05] for an image of "7", the gradient computation would:

1. Start with the error in these probabilities
2. Compute how weight adjustments would affect this error
3. Propagate these gradients backward to help adjust earlier layer weights

##### Implementation Aspects

The practical implementation of backward propagation requires careful consideration of computational resources and memory management. These implementation details significantly impact training efficiency and scalability.

Memory requirements during backward propagation stem from two main sources. First, we need to store the intermediate activations from the forward pass, as these are required for computing gradients. For our MNIST network with a batch size of 32, each layer's activations must be maintained:

* Input layer: 32 × 784 values
* Hidden layers: 32 × h values (where h is the layer width)
* Output layer: 32 × 10 values

Second, we need storage for the gradients themselves. For each layer, we must maintain gradients of similar dimensions to the weights and biases. Taking our previous example of a network with hidden layers of size 128, 256, and 128, this means storing:

* First layer gradients: 784 × 128 values
* Second layer gradients: 128 × 256 values
* Third layer gradients: 256 × 128 values
* Output layer gradients: 128 × 10 values

The computational pattern of backward propagation follows a specific sequence:

1. Compute gradients at current layer
2. Update stored gradients
3. Propagate error signal to previous layer
4. Repeat until input layer is reached

For batch processing, these computations are performed simultaneously across all examples in the batch, enabling efficient use of matrix operations and parallel processing capabilities.

#### Optimization Process

##### Gradient Descent Basics

The optimization process in neural networks involves systematically adjusting weights using the gradients computed during backpropagation. This process, known as gradient descent, aims to minimize the loss function by iteratively updating network parameters.

The fundamental update rule for gradient descent is:

$$
\theta_{\text{new}} = \theta_{\text{old}} - \alpha \nabla_{\theta}L
$$

where $\theta$ represents any network parameter (weights or biases), $\alpha$ is the learning rate, and $\nabla_{\theta}L$ is the gradient of the loss with respect to that parameter.

For our MNIST example, this means adjusting weights to improve digit classification accuracy. If the network frequently confuses "7"s with "1"s, gradient descent will modify the weights to better distinguish between these digits. The learning rate $\alpha$ controls how large these adjustments are—too large, and the network might overshoot optimal values; too small, and training will progress very slowly.

##### Batch Processing

Neural networks typically process multiple examples simultaneously during training, an approach known as mini-batch gradient descent. Rather than updating weights after each individual image, we compute the average gradient over a batch of examples before performing the update.

For a batch of size B, the loss gradient becomes:

$$
\nabla_{\theta}L_{\text{batch}} = \frac{1}{B}\sum_{i=1}^B \nabla_{\theta}L_i
$$

In our MNIST training, with a typical batch size of 32, this means:

1. Process 32 images through forward propagation
2. Compute loss for all 32 predictions
3. Average the gradients across all 32 examples
4. Update weights using this averaged gradient

##### Training Loop

The complete training process combines forward propagation, backward propagation, and weight updates into a systematic training loop. This loop repeats until the network achieves satisfactory performance or reaches a predetermined number of iterations.

A single pass through the entire training dataset is called an epoch. For MNIST, with 60,000 training images and a batch size of 32, each epoch consists of approximately 1,875 batch iterations. The training loop structure is:

1. For each epoch:
   * Shuffle training data to prevent learning order-dependent patterns
   * For each batch:
     * Perform forward propagation
     * Compute loss
     * Execute backward propagation
     * Update weights using gradient descent
   * Evaluate network performance

During training, we monitor several key metrics:

* Training loss: average loss over recent batches
* Validation accuracy: performance on held-out test data
* Learning progress: how quickly the network improves

For our digit recognition task, we might observe the network's accuracy improve from 10% (random guessing) to over 95% through multiple epochs of training.

##### Practical Considerations

The successful implementation of neural network training requires attention to several key practical aspects that significantly impact learning effectiveness. These considerations bridge the gap between theoretical understanding and practical implementation.

Learning rate selection is perhaps the most critical parameter affecting training. For our MNIST network, the choice of learning rate dramatically influences the training dynamics. A large learning rate of 0.1 might cause unstable training where the loss oscillates or explodes as weight updates overshoot optimal values. Conversely, a very small learning rate of 0.0001 might result in extremely slow convergence, requiring many more epochs to achieve good performance. A moderate learning rate of 0.01 often provides a good balance between training speed and stability, allowing the network to make steady progress while maintaining stable learning.

Convergence monitoring provides crucial feedback during the training process. As training progresses, we typically observe the loss value stabilizing around a particular value, indicating the network is approaching a local optimum. The validation accuracy often plateaus as well, suggesting the network has extracted most of the learnable patterns from the data. The gap between training and validation performance offers insights into whether the network is overfitting or generalizing well to new examples.

Resource requirements become increasingly important as we scale neural network training. The memory footprint must accommodate both model parameters and the intermediate computations needed for backpropagation. Computation scales linearly with batch size, affecting training speed and hardware utilization. Modern training often leverages GPU acceleration, making efficient use of parallel computing capabilities crucial for practical implementation.

Training neural networks also presents several fundamental challenges. Overfitting occurs when the network becomes too specialized to the training data, performing well on seen examples but poorly on new ones. Gradient instability can manifest as either vanishing or exploding gradients, making learning difficult. The interplay between batch size, available memory, and computational resources often requires careful balancing to achieve efficient training while working within hardware constraints.

## Modern Neural Network Architectures

The basic principles of neural networks have evolved into more sophisticated architectures that power today's most advanced AI systems. While the fundamental concepts of neurons, activations, and learning remain the same, modern architectures introduce specialized structures and organizational patterns that dramatically enhance the network's ability to process specific types of data and solve increasingly complex problems.

These architectural innovations reflect both theoretical insights about how networks learn and practical lessons from applying neural networks in the real world. Each new architecture represents a careful balance between computational efficiency, learning capacity, and the specific demands of different problem domains. By examining these architectures, we will see how the field has moved from general-purpose networks to highly specialized designs optimized for particular tasks, while also understanding the implicatiuons of these changes on the unlderying system that enables their computation.

### Multi-Layer Perceptrons: Dense Pattern Processing

Multi-Layer Perceptrons (MLPs) represent the most direct extension of neural networks into deep architectures. Unlike more specialized networks, MLPs process each input element with equal importance, making them versatile but computationally intensive. Their architecture, while simple, establishes fundamental computational patterns that appear throughout deep learning systems.

When applied to the MNIST handwritten digit recognition challenge, an MLP reveals its computational power by transforming a complex 28×28 pixel image into a precise digit classification. By treating each of the 784 pixels as an equally weighted input, the network learns to decompose visual information through a systematic progression of layers, converting raw pixel intensities into increasingly abstract representations that capture the essential characteristics of handwritten digits.

#### Algorithmic Structure

The core structure of an MLP is a series of fully-connected layers, where each neuron connects to every neuron in adjacent layers. As shown in @fig-mlp, each layer transforms its input through matrix multiplication followed by element-wise activation:

![MLP layers and its associated matrix representation. Source: @reagen2017deep](images/png/mlp_mm.png)

$$
\mathbf{h}^{(l)} = f(\mathbf{W}^{(l)}\mathbf{h}^{(l-1)} + \mathbf{b}^{(l)})
$$

The dimensions at each layer illustrate the computation scale:
* Input vector: $\mathbf{h}^{(0)} \in \mathbb{R}^{d_{in}}$
* Weight matrices: $\mathbf{W}^{(l)} \in \mathbb{R}^{d_{out} \times d_{in}}$
* Output vector: $\mathbf{h}^{(l)} \in \mathbb{R}^{d_{out}}$

#### Computational Mapping

The fundamental computation in MLPs is dense matrix multiplication, which creates specific computational patterns:

1. **Full Connectivity**: Each output depends on every input element
   * Complete rows and columns must be accessed
   * All-to-all communication pattern
   * High memory bandwidth requirement

2. **Batch Processing**: Multiple inputs processed simultaneously via matrix-matrix multiply:
   $$
   \mathbf{H}^{(l)} = f(\mathbf{H}^{(l-1)}\mathbf{W}^{(l)} + \mathbf{b}^{(l)})
   $$
   where $\mathbf{H}^{(l)} \in \mathbb{R}^{B \times d_{out}}$ for batch size B

While this mathematical view is elegant, its actual implementation reveals a more detailed computational reality:

```{python}
# Mathematical abstraction
def mlp_layer_matrix(X, W, b):
    H = activation(matmul(X, W) + b)    # One clean line of math
    return H

# System reality
def mlp_layer_compute(X, W, b):
    for batch in range(batch_size):
        for out in range(num_outputs):
            Z[batch,out] = b[out]
            for in_ in range(num_inputs):
                Z[batch,out] += X[batch,in_] * W[in_,out]
    
    H = activation(Z)
    return H
```

This translation from mathematical abstraction to concrete computation exposes how dense matrix multiplication decomposes into nested loops of simpler operations. The clean mathematical notation of $\mathbf{W}\mathbf{x}$ becomes hundreds of individual multiply-accumulate operations, each requiring multiple memory accesses. These patterns fundamentally influence system design, creating both challenges in implementation and opportunities for optimization.

#### System Implications

The implementation of MLPs presents several key challenges and opportunities that shape system design.

##### System Challenges

1. **Memory Bandwidth Pressure**
   * Each output neuron must read all input values and weights. For MNIST, this means 784 memory accesses for inputs and another 784 for weights per output.
   * Weights must be accessed repeatedly. In a batch of 32 images, each weight is read 32 times.
   * The system must constantly write intermediate results to memory. For a layer with 128 outputs, this means 128 writes per image.
   These intensive memory operations create a fundamental bottleneck. For just one fully-connected layer processing a batch of MNIST images, the system performs over 50,000 memory operations.

2. **Computation Volume**
   * Each output requires hundreds of multiply-accumulate (MAC) operations. For MNIST, each output performs 784 multiply-accumulates.
   * These operations repeat for every output neuron in each layer. A layer with 128 outputs performs over 100,000 multiply-accumulates per image.
   * A typical network processes millions of operations per input. Even a modest three-layer network can require over a million operations per image.
   The massive number of operations means even small inefficiencies can significantly impact performance.

##### System Opportunities

1. Regular Computation Structure
   * The core operation is a simple multiply-accumulate. This enables specialized hardware units optimized for this specific operation.
   * Memory access patterns are predictable. The system knows it needs all 784 inputs and weights in a fixed order.
   * The same operations repeat throughout the network. A single optimization in the multiply-accumulate unit gets reused millions of times.
   * These patterns remain consistent across different network sizes. Whether processing 28×28 MNIST images or 224×224 ImageNet images, the basic computational pattern stays the same.

2. Parallelism Potential
   * Input samples can process independently. A batch of 32 MNIST images can process on 32 separate units.
   * Output neurons have no dependencies. All 128 outputs in a layer can compute in parallel.
   * Individual multiply-accumulates can execute together. A vector unit could process 8 or 16 multiplications at once.
   * Layers operate independently. While one layer processes batch 1, another layer can start on batch 0.

These challenges and opportunities drive the development of specialized neural processing engines fo machine learning systems. While memory bandwidth limitations push designs toward sophisticated memory hierarchies (needing to handle >50,000 memory operations efficiently), the regular patterns and parallel opportunities enable efficient implementations through specialized processing units. The patterns established by MLPs form a baseline that more specialized neural architectures must consider in their implementations.

### Convolutional Neural Networks: Spatial Pattern Processing

Convolutional Neural Networks (CNNs) represent a specialized neural architecture designed to efficiently process data with spatial relationships, such as images. While MLPs treat each input independently, CNNs exploit local patterns and spatial hierarchies, establishing computational patterns that have revolutionized computer vision and spatial data processing.

In the MNIST digit recognition task, CNNs demonstrate their unique approach by recognizing that visual information contains inherent spatial dependencies. Using sliding kernels that move across the image, these networks detect local features like edges, curves, and intersections specific to handwritten digits. This spatially-aware method allows the network to learn and distinguish digit characteristics by focusing on their most distinctive local patterns, fundamentally different from the uniform processing of MLPs.

#### Algorithmic Structure

The core innovation of CNNs is the convolutional layer, where each neuron processes only a local region of the input. As shown in @fig-cnn, a kernel (or filter) slides across the input data, performing the same transformation at each position:

::: {.content-visible when-format="html"}
![Convolution operation, image data (blue) and 3x3 kernel (green). Source: V. Dumoulin, F. Visin, MIT](images/gif/cnn.gif){#fig-cnn}
:::

::: {.content-visible when-format="pdf"}
![Convolution operation, image data (blue) and 3x3 kernel (green). Source: V. Dumoulin, F. Visin, MIT](images/png/cnn.png){#fig-cnn}
:::

For a convolutional layer with kernel $\mathbf{K}$, the computation at each spatial position $(i,j)$ is:

$$
\mathbf{h}^{(l)}_{i,j} = f(\sum_{p,q} \mathbf{K}_{p,q} \cdot \mathbf{h}^{(l-1)}_{i+p,j+q} + b)
$$

where the dimensions depend on the layer configuration:

* Input: $\mathbf{h}^{(l-1)} \in \mathbb{R}^{H \times W \times C_{in}}$
* Kernel: $\mathbf{K} \in \mathbb{R}^{k \times k \times C_{in} \times C_{out}}$
* Output: $\mathbf{h}^{(l)} \in \mathbb{R}^{H' \times W' \times C_{out}}$

#### Computational Mapping

The fundamental computation in CNNs is the convolution operation, which exhibits distinct patterns:

1. **Sliding Window**: The kernel moves systematically across spatial dimensions, creating regular access patterns.
2. **Weight Sharing**: The same kernel weights are reused at each position, reducing parameter count.
3. **Local Connectivity**: Each output depends only on a small neighborhood of input values.

For a batch of B inputs, the convolution operation processes multiple images simultaneously:

$$
\mathbf{H}^{(l)}_{b,i,j} = f(\sum_{p,q} \mathbf{K}_{p,q} \cdot \mathbf{H}^{(l-1)}_{b,i+p,j+q} + b)
$$

where $b$ indexes the batch dimension.

The convolution operation involves systematically applying a kernel across an input. While mathematically elegant, its implementation reveals the complex patterns of data movement and computation:

```{python}
# Mathematical abstraction - simple and clean
def conv_layer_math(input, kernel, bias):
    output = convolution(input, kernel) + bias
    return activation(output)

# System reality - nested loops of computation
def conv_layer_compute(input, kernel, bias):
    # Loop 1: Process each image in batch
    for image in range(batch_size):
        
        # Loop 2&3: Move across image spatially
        for y in range(height):
            for x in range(width):
                
                # Loop 4: Compute each output feature
                for out_channel in range(num_output_channels):
                    result = bias[out_channel]
                    
                    # Loop 5&6: Move across kernel window
                    for ky in range(kernel_height):
                        for kx in range(kernel_width):
                            
                            # Loop 7: Process each input feature
                            for in_channel in range(num_input_channels):
                                # Get input value from correct window position
                                in_y = y + ky  
                                in_x = x + kx
                                # Perform multiply-accumulate operation
                                result += input[image, in_y, in_x, in_channel] * \
                                         kernel[ky, kx, in_channel, out_channel]
                    
                    # Store result for this output position
                    output[image, y, x, out_channel] = result
```                    

This implementation, while simplified, exposes the core patterns in convolution. The seven nested loops reveal different aspects of the computation:

* Outer loops (1-3) manage position: which image and where in the image
* Middle loop (4) handles output features: computing different learned patterns
* Inner loops (5-7) perform the actual convolution: sliding the kernel window

Each level creates different system implications. The outer loops show where we can parallelize across images or spatial positions. The inner loops reveal opportunities for data reuse, as adjacent positions share input values. While real implementations use sophisticated optimizations, these basic patterns drive the key design decisions in CNN execution.

#### System Implications

The implementation of CNNs reveals a distinct set of challenges and opportunities that fundamentally shape system design.

**Data Access Complexity:** The sliding window nature of convolution creates complex data access patterns. Consider a basic 3×3 convolution kernel. Each output computation requires reading 9 input values per channel. For a simple MNIST image with one channel, this means 9 values per output pixel. The complexity grows substantially with modern CNNs. A 3×3 kernel operating on a 224×224 RGB image with 64 output channels must access 1,728 values (9 × 3 × 64) for each output position. Moreover, these windows overlap, meaning adjacent outputs need much of the same input data.

**Computation Organization:** The computational workload in CNNs follows the sliding window pattern. Each position requires a complete set of kernel computations. Even for a modest 3×3 kernel on MNIST, this means 9 multiply-accumulate operations per output. The computational demands scale dramatically with channels. In a modern network, a 3×3 kernel with 64 input and output channels requires 36,864 multiply-accumulate operations (3 × 3 × 64 × 64) per window position.

**Spatial Locality:** CNNs exhibit strong spatial data reuse patterns. In a 3×3 convolution, each input value contributes to 9 different output computations. This natural overlap creates opportunities for efficient caching strategies. A well-designed memory hierarchy can reduce memory bandwidth by a factor of 9 for a 3×3 kernel. The potential for reuse grows with kernel size - a 5×5 kernel allows each input value to be reused up to 25 times.

**Computation Structure:** The computational pattern of CNNs offers multiple paths to parallelism. The same kernel weights apply across the entire image, simplifying weight management. Within each window, the 9 multiplications of a 3×3 kernel can execute in parallel. The computation can also parallelize across output channels and spatial positions, as these calculations are independent.

###### System Design Impact

These patterns create fundamentally different system demands than MLPs. While MLPs struggle with global data access patterns, CNNs must efficiently manage sliding windows of computation. The opportunity for data reuse is significantly higher in CNNs, but capturing this reuse requires more sophisticated memory systems. Many CNN accelerators specifically target these patterns through specialized memory hierarchies and computational arrays that align with the natural structure of convolution operations.

### Recurrent Neural Networks: Sequential Pattern Processing

Recurrent Neural Networks (RNNs) introduce a fundamental shift in deep learning architecture by maintaining state across time steps. Unlike MLPs and CNNs that process fixed-size inputs, RNNs handle variable-length sequences by reusing weights across time steps while maintaining an internal memory state. This architecture enables sequential data processing but introduces new computational patterns and dependencies.

RNNs excel in processing sequential data that changes over time. In tasks like predicting stock prices or analyzing sentiment in customer reviews, RNNs demonstrate their unique ability to process data as a temporal sequence. By maintaining an internal hidden state that captures information from previous time steps, these networks can recognize patterns that unfold over time. For instance, in sentiment analysis, an RNN can progressively understand the emotional trajectory of a sentence by processing each word in context, allowing it to capture nuanced changes in tone and meaning that traditional fixed-input models might miss.

#### Algorithmic Structure

The core innovation of RNNs is the recurrent connection, where each step's computation depends on both the current input and the previous hidden state. As shown in @fig-rnn, the same weights are applied repeatedly across the sequence:

![RNN unrolled.](images/jpg/rnn_unrolled.jpg){#fig-rnn}

For a sequence of T time steps, each step t computes:

$$
\mathbf{h}_t = f(\mathbf{W}_h\mathbf{h}_{t-1} + \mathbf{W}_x\mathbf{x}_t + \mathbf{b})
$$

The dimensions reveal the computation structure:

* Input vector: $\mathbf{x}_t \in \mathbb{R}^{d_{in}}$
* Hidden state: $\mathbf{h}_t \in \mathbb{R}^{d_h}$
* Weight matrices: $\mathbf{W}_h \in \mathbb{R}^{d_h \times d_h}$, $\mathbf{W}_x \in \mathbb{R}^{d_h \times d_{in}}$

#### Computational Mapping

RNN computation exhibits distinct patterns due to its sequential nature:

1. **State Update**: Each time step combines current input with previous state
   * Two matrix multiplications per step
   * State must be maintained across sequence
   * Sequential dependency limits parallelization

2. **Batch Processing**: Sequences processed in parallel with shape transformations:
   $$
   \mathbf{H}_t = f(\mathbf{H}_{t-1}\mathbf{W}_h + \mathbf{X}_t\mathbf{W}_x + \mathbf{b})
   $$
   where $\mathbf{H}_t \in \mathbb{R}^{B \times d_h}$ for batch size B

The translation from mathematical abstraction to implementation reveals how these patterns manifest in practice:

```python
# Mathematical abstraction - clean and stateless
def rnn_step_math(x_t, h_prev, W_h, W_x, b):
    h_t = activation(matmul(h_prev, W_h) + matmul(x_t, W_x) + b)
    return h_t

# System reality - managing state and sequence
def rnn_sequence_compute(X, W_h, W_x, b):
    # Process each sequence in batch
    for batch in range(batch_size):
        # Hidden state must be maintained
        h = zeros(hidden_size)
        
        # Steps must process sequentially
        for t in range(sequence_length):
            # State update computation
            h_prev = h
            h = activation(matmul(h_prev, W_h) + 
                         matmul(X[batch,t], W_x) + b)
            outputs[batch,t] = h
```

While real implementations use sophisticated optimizations, this simplified code reveals how RNNs must carefully manage state and sequential dependencies.

#### System Implications

The implementation of RNNs reveals unique challenges and opportunities that shape system design. Consider processing a batch of text sequences, each 100 words long, with a hidden state size of 256:

**Sequential Data Dependencies:** The fundamental challenge in RNNs stems from their sequential nature. Each time step must wait for the completion of the previous step's computation, creating a chain of dependencies throughout the sequence. For a 100-word input, this means 100 strictly sequential operations, each producing a 256-dimensional hidden state that must be preserved and passed forward. This creates a computational pipeline that cannot be parallelized across time steps, fundamentally limiting system throughput.

**Variable Computation and Storage:** Unlike MLPs or CNNs where computation is fixed, RNNs must handle varying sequence lengths. A batch might contain sequences ranging from a few words to hundreds of words, each requiring a different amount of computation. This variability creates complex resource management challenges. A system processing sequences of length 100 might suddenly need to accommodate sequences of length 500, requiring five times more computation and memory. Load balancing becomes particularly challenging when different sequences in a batch have widely varying lengths.

**Weight Reuse Structure:** RNNs offer significant opportunities for weight reuse. The same weight matrices (W_h and W_x) are applied at every time step in the sequence. For a 100-word sequence, each weight is reused 100 times. With a hidden state size of 256, the W_h matrix has only 65,536 parameters (256 × 256) but participates in tens of thousands of computations. This intensive reuse makes weight-stationary architectures particularly effective for RNN computation.

**Regular Computation Pattern:** Despite their sequential nature, RNNs exhibit highly regular computation patterns. Each time step performs identical operations: two matrix multiplications with fixed dimensions followed by an activation function. This regularity means that once a system is optimized for a single step's computation, that optimization applies throughout the sequence. The matrix sizes remain constant (256 × 256 for W_h, 256 × d_in for W_x), enabling efficient hardware utilization despite the sequential constraints.

These patterns create fundamentally different demands than MLPs or CNNs. While MLPs focus on parallel matrix multiplication and CNNs on spatial data reuse, RNNs must carefully manage sequential state while maximizing batch-level parallelism. This has led to specialized architectures that pipeline sequential operations while exploiting weight reuse across time steps.

Modern systems address these challenges through deep pipelines that overlap sequential computations, sophisticated batch scheduling for varying sequence lengths, and hierarchical memory systems optimized for weight reuse. The patterns established by RNNs highlight how sequential dependencies can fundamentally reshape system architecture decisions, even while maintaining the core mathematical operations common to deep learning.

### Attention Mechanisms: Dynamic Information Flow

Attention mechanisms revolutionized deep learning by enabling dynamic, content-based information processing. Unlike the fixed patterns of MLPs, CNNs, or RNNs, attention allows each element in a sequence to selectively focus on relevant parts of the input. This dynamic routing of information creates new computational patterns that scale differently from traditional architectures.

In language translation tasks, attention mechanisms reveal their power by allowing the network to dynamically align and focus on relevant words across different languages. Instead of processing a sentence sequentially with fixed weights, an attention-based model can simultaneously consider multiple parts of the input, creating contextually rich representations that capture subtle linguistic nuances. This approach enables more accurate and contextually aware translations by letting the model dynamically determine which parts of the input are most relevant for generating each output word.

#### Mathematical Structure

The core operation in attention computes relevance scores between pairs of elements in sequences. Each output is a weighted combination of all inputs, where weights are learned from content:

$$
\text{Attention}(\mathbf{Q}, \mathbf{K}, \mathbf{V}) = \text{softmax}(\frac{\mathbf{Q}\mathbf{K}^T}{\sqrt{d_k}})\mathbf{V}
$$

The dimensions highlight the scale of computation:
* Queries: $\mathbf{Q} \in \mathbb{R}^{N \times d_k}$
* Keys: $\mathbf{K} \in \mathbb{R}^{N \times d_k}$
* Values: $\mathbf{V} \in \mathbb{R}^{N \times d_v}$
* Output: $\mathbf{O} \in \mathbb{R}^{N \times d_v}$

#### Computational Mapping

Attention computation exhibits distinct patterns:

1. **All-to-All Interaction**: Every query computes relevance with every key
   * Quadratic scaling with sequence length
   * Large intermediate matrices
   * Global information access

2. **Batch Processing**: Multiple sequences processed simultaneously:
   $$
   \mathbf{A} = \text{softmax}(\frac{\mathbf{Q}\mathbf{K}^T}{\sqrt{d_k}}) \in \mathbb{R}^{B \times N \times N}
   $$
   where B is batch size and A is the attention matrix

The translation from mathematical abstraction to implementation reveals the computational complexity:

```{python}
# Mathematical abstraction - clean matrix operations
def attention_math(Q, K, V):
    scores = matmul(Q, K.transpose()) / sqrt(d_k)
    weights = softmax(scores)
    output = matmul(weights, V)
    return output

# System reality - explicit computation
def attention_compute(Q, K, V):
    # Process each sequence in batch
    for b in range(batch_size):
        # Every query must interact with every key
        for i in range(seq_length):
            for j in range(seq_length):
                # Compute attention score
                score = 0
                for d in range(d_k):
                    score += Q[b,i,d] * K[b,j,d]
                scores[b,i,j] = score / sqrt(d_k)
        
        # Softmax normalization
        weights = softmax(scores[b])
        
        # Weighted combination of values
        for i in range(seq_length):
            for j in range(seq_length):
                for d in range(d_v):
                    output[b,i,d] += weights[i,j] * V[b,j,d]
```

This simplified code view exposes the quadratic nature of attention computation.

#### System Implications

The implementation of attention mechanisms reveals unique challenges and opportunities that shape system design. Consider processing a batch of sentences, each containing 512 tokens, with an embedding dimension of 64:

**Memory Intensity:** The defining characteristic of attention is its quadratic memory scaling. For a sequence of 512 tokens, the attention matrix alone requires 512 × 512 = 262,144 elements per head. With 8 attention heads, this grows to over 2 million elements just for intermediate storage. This quadratic growth means doubling sequence length quadruples memory requirements, creating a fundamental scaling challenge that often limits batch sizes in practice.

**Computation Volume:** Attention mechanisms create an intense computational workload. Computing attention scores requires 512 × 512 × 64 multiply-accumulate operations just for the Q-K interaction in a single head. This amounts to over 16 million operations per layer. Unlike CNNs where computation is local, or RNNs where it's sequential, attention requires global interaction between all elements, making the computational pattern both dense and extensive.

**Parallelization Opportunity:** Despite its computational intensity, attention offers extensive parallelism opportunities. Unlike RNNs, attention has no sequential dependencies. All attention scores can compute simultaneously. Moreover, different attention heads can process independently, and operations across the batch dimension are entirely separate. This creates multiple levels of parallelism that modern hardware can exploit.

**Dynamic Data Access:** The content-dependent nature of attention creates unique data access patterns. Unlike CNNs with fixed sliding windows or MLPs with regular matrix multiplication, attention's data access depends on the computed relevance scores. This dynamic pattern means the system cannot predict which values will be most important, requiring efficient access to the entire sequence at each step.

These characteristics drive the development of specialized attention accelerators. While CNNs focus on reusing spatial data and RNNs on managing sequential state, attention accelerators must handle large, dense matrix operations while managing significant memory requirements. 

Modern systems address these challenges through:

* Sophisticated memory hierarchies to handle the quadratic scaling
* Highly parallel matrix multiplication units
* Custom data layouts optimized for attention patterns
* Memory management strategies to handle variable sequence lengths

The patterns established by attention mechanisms highlight how dynamic, content-dependent computation can create fundamentally different system requirements, even while building on the basic operations of deep learning.

### Transformers: Parallel Sequence Processing

Transformers represent a culmination of deep learning architecture development, combining attention mechanisms with positional encoding and parallel processing. Unlike RNNs that process sequences step by step, Transformers handle entire sequences simultaneously through self-attention layers. This architecture has enabled breakthrough performance in language processing and increasingly in other domains, while introducing distinct computational patterns that influence system design.

#### Mathematical Structure

The Transformer architecture processes sequences through alternating self-attention and feedforward layers. The core computation in each attention head is:

$$
\text{head}_i = \text{Attention}(\mathbf{X}\mathbf{W}^Q_i, \mathbf{X}\mathbf{W}^K_i, \mathbf{X}\mathbf{W}^V_i)
$$

Multiple heads are concatenated and projected:
$$
\text{MultiHead}(\mathbf{X}) = \text{Concat}(\text{head}_1,...,\text{head}_h)\mathbf{W}^O
$$

Dimensions scale with model size:
* Input sequence: $\mathbf{X} \in \mathbb{R}^{N \times d_{model}}$
* Per-head projections: $\mathbf{W}^Q_i, \mathbf{W}^K_i, \mathbf{W}^V_i \in \mathbb{R}^{d_{model} \times d_k}$
* Output projection: $\mathbf{W}^O \in \mathbb{R}^{hd_v \times d_{model}}$

#### Computational Mapping

Transformer computation exhibits several key patterns:

1. **Multi-Head Processing**: Attention computed in parallel heads
   * Multiple attention patterns learned simultaneously
   * Head outputs concatenated and projected
   * Parameter count scales with head count

2. **Layer Operations**: Each block performs sequence transformation:
   $$
   \mathbf{X}' = \text{FFN}(\text{LayerNorm}(\mathbf{X} + \text{MultiHead}(\mathbf{X})))
   $$
   where FFN is a position-wise feedforward network

The translation from mathematical abstraction to implementation reveals the computational complexity:

```python
# Mathematical abstraction - clean composition
def transformer_layer(X):
    # Multi-head attention
    attention_out = multi_head_attention(X)
    residual1 = layer_norm(X + attention_out)
    
    # Position-wise feedforward
    ff_out = feedforward(residual1)
    output = layer_norm(residual1 + ff_out)
    return output

# System reality - explicit computation
def transformer_compute(X):
    # Process each sequence in batch
    for b in range(batch_size):
        # Compute attention in each head
        for h in range(num_heads):
            # Project inputs to Q,K,V
            Q = matmul(X[b], W_q[h])
            K = matmul(X[b], W_k[h])
            V = matmul(X[b], W_v[h])
            
            # Compute attention scores
            for i in range(seq_length):
                for j in range(seq_length):
                    scores[h,i,j] = dot(Q[i], K[j]) / sqrt(d_k)
            
            # Apply attention and transform
            weights = softmax(scores[h])
            head_out[h] = matmul(weights, V)
        
        # Combine heads and apply FFN
        combined = concat(head_out) @ W_o
        residual1 = layer_norm(X[b] + combined)
        
        # Position-wise feedforward
        for pos in range(seq_length):
            ff_out[pos] = feedforward(residual1[pos])
        output[b] = layer_norm(residual1 + ff_out)
```

#### System Implications

The implementation of Transformers reveals unique challenges and opportunities that shape system design. Consider a typical model with 12 layers, 12 attention heads, and processing sequences of 512 tokens with model dimension 768:

**Memory Hierarchy Complexity:** Transformers create intricate memory demands across multiple scales. Each attention head in each layer requires its own attention matrix (512 × 512 elements), leading to 12 × 12 × 512 × 512 = 37.7M elements just for attention scores. Additionally, residual connections mean each layer's activations (512 × 768 = 393K elements) must be preserved until the corresponding addition operation. Layer normalization requires computing and storing statistics across the full sequence length. For a batch size of 32, the total active memory footprint can exceed 1GB, creating complex memory management challenges.

**Computational Pattern Diversity:** Unlike MLPs or CNNs with uniform computation patterns, Transformers alternate between two fundamentally different computations. The self-attention mechanism requires the quadratic-scaling attention computation we saw earlier (262K elements per head), while the position-wise feedforward network applies identical dense layers (768 × 3072 for the first FFN layer) at each position independently. This creates a unique rhythm of global interaction followed by local computation that systems must manage efficiently.

**Parallelization Structure:** Transformers offer multiple levels of parallel computation. Each attention head can process independently, the feedforward networks can run separately for each position, and different layers can potentially pipeline their operations. However, this parallelism comes with data dependencies: layer normalization needs global statistics, attention requires the full key-value space, and residual connections need preserved activations. For our example model, each layer offers 12-way parallelism across heads and 512-way parallelism across positions, but must synchronize between major operations.

**Scaling Implications:** The most distinctive aspect of Transformer computation is how it scales with different dimensions. Doubling sequence length quadruples memory requirements for attention (from 262K to 1M elements per head). Doubling model dimension increases both attention projection sizes and feedforward layer sizes. Adding heads or layers creates more independent computation streams but requires proportionally more parameter storage. These scaling properties create fundamental tensions in system design between memory capacity, computational throughput, and parallelization strategy.

These characteristics drive the development of specialized Transformer accelerators. Systems must balance:

* Complex memory hierarchies to handle multiple scales of data reuse
* Mixed computation units for attention and feedforward operations
* Sophisticated scheduling to exploit available parallelism
* Memory management strategies for growing model sizes

The patterns established by Transformers show how combining multiple computational motifs (attention and feedforward networks) creates new system-level challenges that go beyond simply implementing each component separately. Modern hardware must carefully co-design their memory systems, computational units, and control logic to handle these diverse demands efficiently.

## Computational Patterns and System Impact

Having examined major deep learning architectures, we can now compare and contrast their computational patterns and system implications collectively. This synthesis reveals common themes and distinct challenges that drive system design decisions in deep learning.

### Memory Access Patterns

Deep learning architectures exhibit various memory access patterns that significantly impact system performance. MLPs demonstrate the simplest pattern with dense, regular access to weight matrices. Each layer requires loading its entire weight matrix, creating substantial memory bandwidth demands that grow with layer width. CNNs, in contrast, exploit spatial locality through weight sharing. The same kernel parameters are reused across spatial positions, reducing memory requirements but necessitating efficient caching mechanisms for kernel weights.

RNNs introduce temporal dependencies in memory access, requiring state maintenance across sequence steps. While they reuse weights efficiently across time steps, the sequential nature of computation limits opportunities for parallel processing. Attention mechanisms and Transformers create perhaps the most demanding memory patterns, with quadratic scaling in sequence length due to their all-to-all interaction matrices.

Consider the memory scaling for processing a sequence of length N:

* MLP: Linear scaling with layer width
* CNN: Constant kernel size, independent of input
* RNN: Linear scaling with hidden state size
* Attention: Quadratic scaling with sequence length
* Transformer: Quadratic in sequence length, linear in model width

### Computation Characteristics

The computational structure of these architectures reveals a spectrum of parallelization opportunities and constraints. MLPs and CNNs offer straightforward parallelization—each output element can be computed independently. CNNs add the complexity of sliding window operations but maintain regular computation patterns amenable to hardware acceleration.

RNNs represent a sharp departure, with inherent sequential dependencies limiting parallelization across time steps. While batch processing enables some parallelism, the fundamental sequential nature remains a bottleneck. Attention mechanisms swing to the opposite extreme, enabling full parallelization but at the cost of quadratic computation growth.

Key computation patterns across architectures:

1. **Matrix Operations**

* MLPs: Dense matrix multiplication
* CNNs: Convolution as structured sparse multiplication
* RNNs: Sequential matrix operations with state updates
* Attention/Transformers: Batched matrix multiplication with dynamic weights

2. **Data Dependencies**

* MLPs: Layer-wise dependencies only
* CNNs: Local spatial dependencies
* RNNs: Sequential temporal dependencies
* Attention: Global but parallel dependencies

### System Design Implications

These patterns drive specific system design decisions across the deep learning stack:

#### Memory Hierarchy

Modern deep learning systems must balance multiple memory access patterns:

* Fast access to frequently reused weights (CNN kernels)
* Efficient handling of large, dense matrices (MLP layers)
* Support for dynamic, content-dependent access (Attention)
* Management of growing intermediate states (Transformers)

#### Computation Units

Different architectures benefit from specialized compute capabilities:

* Matrix multiplication units for MLPs and Transformers
* Convolution accelerators for CNNs
* Sequential processing optimization for RNNs
* High-bandwidth units for attention computation

#### Data Movement

The cost of data movement often dominates energy and performance:

* Weight streaming in MLPs
* Feature map movement in CNNs
* State propagation in RNNs
* Attention score distribution in Transformers

Understanding these patterns helps system designers to make informed decisions about hardware architecture, memory hierarchy, and optimization strategies. The diversity of patterns across architectures also explains the trend toward heterogeneous computing systems that can efficiently handle multiple types of deep learning workloads.

The computational characteristics of neural network architectures reveal fundamental trade-offs in deep learning system design. @tbl-arch-comparison synthesizes the key computational patterns across major neural network architectures, highlighting how different design choices impact performance, memory usage, and computational efficiency.

The diversity of architectural approaches reflects the complex challenges of processing different types of data. Each architecture emerges as a specialized solution to specific computational demands, balancing factors like parallelization, memory efficiency, and the ability to capture complex patterns in data.

+--------------+------------------------------------+--------------------------------+-----------------------+----------------------------------------------+---------------------------------------------------------+
| Architecture |         Primary Computation        |         Memory Scaling         |    Parallelization    |          Key Computational Patterns          |                     Typical Use Case                    |
+:=============+:===================================+:===============================+:======================+:=============================================+:========================================================+
| MLP          | Dense Matrix Multiplication        | Linear with layer width        | High                  | Full connectivity, uniform weight access     | General-purpose classification, regression              |
+--------------+------------------------------------+--------------------------------+-----------------------+----------------------------------------------+---------------------------------------------------------+
| CNN          | Convolution with Sliding Kernel    | Constant with kernel size      | High                  | Spatial local computation, weight sharing    | Image processing, spatial data                          |
+--------------+------------------------------------+--------------------------------+-----------------------+----------------------------------------------+---------------------------------------------------------+
| RNN          | Sequential Matrix Operations       | Linear with hidden state size  | Low                   | Temporal dependencies, state maintenance     | Sequential data, time series                            |
+--------------+------------------------------------+--------------------------------+-----------------------+----------------------------------------------+---------------------------------------------------------+
| Attention    | Batched Matrix Multiplication      | Quadratic with sequence length | Very High             | Global, content-dependent interactions       | Natural language processing, sequence-to-sequence tasks |
+--------------+------------------------------------+--------------------------------+-----------------------+----------------------------------------------+---------------------------------------------------------+
| Transformer  | Multi-head Attention + Feedforward | Quadratic in sequence length   | Highly Parallelizable | Parallel sequence processing, global context | Large language models, complex sequence tasks           |
+--------------+------------------------------------+--------------------------------+-----------------------+----------------------------------------------+---------------------------------------------------------+

: Comparative computational characteristics of neural network architectures. {#tbl-arch-comparison .striped .hover}

This comparative view shows the fundamental design principles driving neural network architectures, demonstrating how computational requirements shape the evolution of deep learning systems.

### Architectural Complexity Analysis

#### Computational Complexity

For an input of size N, different architectures exhibit distinct computational complexities:

* **MLP (Multi-Layer Perceptron)**: 
  * Time Complexity: O(N * W)
  * Where W is the width of the layers
  * Example: For a 784-100-10 MNIST network, complexity scales linearly with layer sizes

* **CNN (Convolutional Neural Network)**:
  * Time Complexity: O(N * K * C)
  * Where K is kernel size, C is number of channels
  * Significantly more efficient for spatial data due to local connectivity

* **RNN (Recurrent Neural Network)**:
  * Time Complexity: O(N * T * H)
  * Where T is sequence length, H is hidden state size
  * Quadratic complexity with sequence length creates scaling challenges

* **Transformer**:
  * Time Complexity: O(N² * d)
  * Where N is sequence length, d is model dimensionality
  * Quadratic scaling makes long sequence processing expensive

#### Memory Complexity Comparison

+---------------+-------------------+-------------------+---------------------+-------------------+
| Architecture  | Input Dependency  | Parameter Storage | Activation Storage  | Scaling Behavior  |
+:==============+:==================+:==================+:====================+:==================+
| MLP           | Linear            | O(N * W)          | O(B * W)            | Predictable       |
+---------------+-------------------+-------------------+---------------------+-------------------+
| CNN           | Constant          | O(K * C)          | O(B * H * W)        | Efficient         |
+---------------+-------------------+-------------------+---------------------+-------------------+
| RNN           | Linear            | O(H²)             | O(B * T * H)        | Challenging       |
+---------------+-------------------+-------------------+---------------------+-------------------+
| Transformer   | Quadratic         | O(N * d)          | O(B * N²)           | Problematic       |
+---------------+-------------------+-------------------+---------------------+-------------------+

Where:
* N: Input/sequence size
* W: Layer width
* B: Batch size
* K: Kernel size
* C: Channels
* H: Hidden state size
* T: Sequence length
* d: Model dimensionality

## System Demands

Deep learning systems must manage significant computational and memory resources during both training and inference. While the previous section examined computational patterns of different architectures, here we focus on the practical system requirements for running these models. Understanding these fundamental demands is crucial for later chapters where we explore optimization and deployment strategies in detail.

### Training Demands

#### Batch Processing

Training neural networks requires processing multiple examples simultaneously through batch processing. This approach not only improves statistical learning by computing gradients across multiple examples but also enables better hardware utilization. The batch size choice directly impacts memory usage since activations must be stored for each example in the batch, creating a fundamental trade-off between training efficiency and memory constraints.

From a systems perspective, batch processing creates opportunities for optimization through parallel computation and memory access patterns. Modern deep learning systems employ techniques like gradient accumulation to work around memory limitations, and dynamic batch sizing to adapt to available resources. These considerations become particularly important in distributed training settings, which we will explore in detail in subsequent chapters.

#### Gradient Computation

The backward pass in neural network training requires maintaining activation values from the forward pass to compute gradients. This effectively doubles the memory requirements compared to inference, as each layer's outputs must be preserved until they're needed for gradient calculation. The computational graph grows with model depth, making gradient computation increasingly demanding for deeper networks.

System design must carefully manage this memory-computation trade-off. Techniques exist to reduce memory pressure by recomputing activations instead of storing them, or by using checkpointing to save memory at the cost of additional computation. While we will explore these optimization strategies thoroughly in Chapter 8, the fundamental challenge of balancing gradient computation requirements with system resources shapes basic design decisions.

#### Parameter Updates and State Management

Training requires maintaining and updating millions to billions of parameters across iterations. Each training step not only computes gradients but must safely update these parameters, often while maintaining optimizer states like momentum values. This creates substantial memory requirements beyond the basic model parameters.

Systems must efficiently handle these frequent, large-scale parameter updates. Memory hierarchies need to be designed for both fast access during forward/backward passes and efficient updates during the optimization step. While various optimization techniques exist to reduce this overhead, which we will explore in Chapter 8, the basic challenge of parameter management influences fundamental system design choices.

### Inference Requirements

#### Latency Management

Inference workloads often demand real-time or near-real-time responses, particularly in applications like autonomous systems, recommendation engines, or interactive services. Unlike training, where throughput is the primary concern, inference must often meet strict latency requirements. These timing constraints fundamentally shape how the system processes inputs and manages resources.

From a systems perspective, meeting latency requirements involves carefully orchestrating computation and memory access. While techniques like batching can improve throughput, they must be balanced against latency constraints. The system must maintain consistent response times while efficiently utilizing available compute resources, a challenge we will examine more deeply in Chapter 9.

#### Resource Utilization

Inference deployments must operate within specific resource budgets, whether running on edge devices with limited power and memory or in cloud environments with cost constraints. The model's computational and memory demands must fit within these boundaries while maintaining acceptable performance. This creates a fundamental tension between model capability and resource efficiency.

System designs address this challenge through various approaches to resource management. Memory usage can be optimized through quantization and compression, while computation can be streamlined through operation fusion and hardware acceleration. While we will explore these optimization techniques in detail in later chapters, understanding the basic resource constraints is essential for system design.

### Memory Management

#### Working Memory

Deep learning systems require substantial working memory to hold intermediate activations during computation. For training, these memory demands are particularly high as activations must be preserved for gradient computation. Even during inference, the system needs sufficient memory to handle intermediate results, especially for large models or batch processing.

Memory management systems must efficiently handle these dynamic memory requirements. Techniques like gradient checkpointing during training or activation recomputation during inference can trade computation for memory savings. The fundamental challenge lies in balancing memory usage against computational overhead, a trade-off that influences system architecture.

#### Storage Requirements

Beyond working memory, deep learning systems need efficient storage and access patterns for model parameters. Modern architectures can require anywhere from megabytes to hundreds of gigabytes for weight storage. This creates challenges not just in terms of capacity but also in terms of efficient parameter access during computation.

Systems must carefully manage this parameter storage across the memory hierarchy. Fast access to frequently used parameters must be balanced against total storage capacity. While advanced techniques like parameter sharding and caching exist, which we will discuss in a future chapter, the basic requirements for parameter storage and access shape fundamental system design decisions.

## Case Studies

### Scaling Architectures: Transformers and Large Language Models

* Evolution from Attention Mechanisms to GPT and BERT  
* System challenges in scaling, memory, and deployment  

### Efficiency and Specialization: EfficientNet and Mobile AI

* Optimizing for constrained environments  
* Trade-offs between accuracy and efficiency  

### Distributed Systems: Training at Scale

* Challenges in multi-GPU communication and resource scaling  
* Tools and frameworks like Horovod and DeepSpeed  

## Conclusion

:::{.callout-note collapse="false"}

#### Slides

These slides are a valuable tool for instructors to deliver lectures and for students to review the material at their own pace. We encourage students and instructors to leverage these slides to improve their understanding and facilitate effective knowledge transfer.

* [Past, Present, and Future of ML.](https://docs.google.com/presentation/d/16ensKAKBG8DOUHF4f5thTJklVGTadxjm3kPkdoPyabI/edit#slide=id.g94db9f9f78_0_2)

* [Thinking About Loss.](https://docs.google.com/presentation/d/1X92JqVkUY7k6yJXQcT2u83dpdrx5UzGFAJkkDMDfKe0/edit#slide=id.g94db9f9f78_0_2)

* [Minimizing Loss.](https://docs.google.com/presentation/d/1x3xbZHo4VtaZgoXfueCbOGGXuWRYj0nOsKwAAoGsrD0/edit#slide=id.g94db9f9f78_0_2)

* [First Neural Network.](https://docs.google.com/presentation/d/1zQwhTwF_plXBPQLxluahpzoQg-VdMyJbctaJxSUncag/edit?usp=drive_link)

* [Understanding Neurons.](https://docs.google.com/presentation/d/1jXCAC6IT5f9XFKZbfhJ4p2D5URVTYqgAnkcQR4ALhSk/edit?usp=drive_link&resourcekey=0-K228bxVdwO2w3kr0daV2cw)

* [Intro to CLassification.](https://docs.google.com/presentation/d/1VtWV9LAVLJ0uAkhFMbDJFjsUH6IvBDnPde4lR1cD2mo/edit?usp=drive_link)

* [Training, Validation, and Test Data.](https://docs.google.com/presentation/d/1G56D0-qG9YWnzQQeje9LMpcLSotMgBCiMyfj53yz7lY/edit?usp=drive_link)

* [Intro to Convolutions.](https://docs.google.com/presentation/d/1hQDabWqaKUWRb60Cze-MhAyeUUVyNgyTUMBpLnqhtvc/edit?resourcekey=0-uHZoNwsbjeY3EIMD3fYAfg#slide=id.g94db9f9f78_0_2)

:::

:::{.callout-important collapse="false"}

#### Videos

* @vid-nn

* @vid-gd

* @vid-bp

:::

:::{.callout-caution collapse="false"}

#### Exercises

To reinforce the concepts covered in this chapter, we have curated a set of exercises that challenge students to apply their knowledge and deepen their understanding.

* @exr-mlp
  
* @exr-cnn
:::
