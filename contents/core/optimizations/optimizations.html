<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">

<head>

<meta charset="utf-8" />
<meta name="generator" content="quarto-1.6.32" />

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />


<title>9  Model Optimizations – Machine Learning Systems</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>

<!-- htmldependencies:E3FAD763 -->
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "~",
    "/"
  ],
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async src="https://www.googletagmanager.com/gtag/js?id=G-M21L0CBCVN"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-M21L0CBCVN', { 'anonymize_ip': true});
</script>
<script async src="https://www.googletagmanager.com/gtag/js?id=G-M21L0CBCVN"></script>
<script src="/scripts/ai_menu/dist/bundle.js" defer></script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body>

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="/index.html">
    <span class="navbar-title">Machine Learning Systems</span>
    </a>
  </div>
        <div class="quarto-navbar-tools tools-wide tools-end">
    <a href="https://github.com/harvard-edge/cs249r_book"  title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
    <a href="/Machine-Learning-Systems.pdf"  title="Download PDF" class="quarto-navigation-tool px-1" aria-label="Download PDF"><i class="bi bi-file-pdf"></i></a>
    <div class="dropdown">
      <a href="" title="Share" id="quarto-navigation-tool-dropdown-0" class="quarto-navigation-tool dropdown-toggle px-1" data-bs-toggle="dropdown" aria-expanded="false" role="link" aria-label="Share"><i class="bi bi-share"></i></a>
      <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="quarto-navigation-tool-dropdown-0">
          <li>
            <a class="dropdown-item quarto-navbar-tools-item" href="https://twitter.com/intent/tweet?url=|url|">
              <i class="bi bi-twitter pe-1"></i>
            Twitter
            </a>
          </li>
          <li>
            <a class="dropdown-item quarto-navbar-tools-item" href="https://www.facebook.com/sharer/sharer.php?u=|url|">
              <i class="bi bi-facebook pe-1"></i>
            Facebook
            </a>
          </li>
      </ul>
    </div>
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
  <a href="" class="quarto-reader-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleReader(); return false;" title="Toggle reader mode">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn"
      data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" 
      aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation"
      onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <h1 class="quarto-secondary-nav-title no-breadcrumbs"></h1>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" role="link"
        aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation"
        onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="/contents/copyright.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Copyright</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="/contents/dedication.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Dedication</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="/contents/core/acknowledgements/acknowledgements.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Acknowledgements</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="/contents/contributors.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Contributors & Thanks</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="/contents/about.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">About the Book</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="/contents/core/introduction/introduction.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="/contents/core/ml_systems/ml_systems.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class='chapter-number'>2</span>  <span class='chapter-title'>ML Systems</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="/contents/core/dl_primer/dl_primer.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class='chapter-number'>3</span>  <span class='chapter-title'>DL Primer</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="/contents/core/workflow/workflow.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class='chapter-number'>4</span>  <span class='chapter-title'>AI Workflow</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="/contents/core/data_engineering/data_engineering.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class='chapter-number'>5</span>  <span class='chapter-title'>Data Engineering</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="/contents/core/frameworks/frameworks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class='chapter-number'>6</span>  <span class='chapter-title'>AI Frameworks</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="/contents/core/training/training.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class='chapter-number'>7</span>  <span class='chapter-title'>AI Training</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="/contents/core/efficient_ai/efficient_ai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class='chapter-number'>8</span>  <span class='chapter-title'>Efficient AI</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="/contents/core/optimizations/optimizations.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class='chapter-number'>9</span>  <span class='chapter-title'>Model Optimizations</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="/contents/core/hw_acceleration/hw_acceleration.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class='chapter-number'>10</span>  <span class='chapter-title'>AI Acceleration</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="/contents/core/benchmarking/benchmarking.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class='chapter-number'>11</span>  <span class='chapter-title'>Benchmarking AI</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="/contents/core/ondevice_learning/ondevice_learning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class='chapter-number'>12</span>  <span class='chapter-title'>On-Device Learning</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="/contents/core/ops/ops.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class='chapter-number'>13</span>  <span class='chapter-title'>ML Operations</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="/contents/core/privacy_security/privacy_security.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class='chapter-number'>14</span>  <span class='chapter-title'>Security & Privacy</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="/contents/core/responsible_ai/responsible_ai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class='chapter-number'>15</span>  <span class='chapter-title'>Responsible AI</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="/contents/core/sustainable_ai/sustainable_ai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class='chapter-number'>16</span>  <span class='chapter-title'>Sustainable AI</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="/contents/core/robust_ai/robust_ai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class='chapter-number'>17</span>  <span class='chapter-title'>Robust AI</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="/contents/core/generative_ai/generative_ai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class='chapter-number'>18</span>  <span class='chapter-title'>Generative AI</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="/contents/core/ai_for_good/ai_for_good.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class='chapter-number'>19</span>  <span class='chapter-title'>AI for Good</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="/contents/core/conclusion/conclusion.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class='chapter-number'>20</span>  <span class='chapter-title'>Conclusion</span></span></a>
  </div>
</li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1"  role="navigation" aria-expanded="true">
 <span class="menu-text">LABS</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="/contents/labs/labs.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Overview</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="/contents/labs/getting_started.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Getting Started</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="/contents/labs/arduino/nicla_vision/nicla_vision.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Nicla Vision</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="/contents/labs/arduino/nicla_vision/setup/setup.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Setup</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="/contents/labs/arduino/nicla_vision/image_classification/image_classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Image Classification</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="/contents/labs/arduino/nicla_vision/object_detection/object_detection.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Object Detection</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="/contents/labs/arduino/nicla_vision/kws/kws.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Keyword Spotting (KWS)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="/contents/labs/arduino/nicla_vision/motion_classification/motion_classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Motion Classification and Anomaly Detection</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="/contents/labs/seeed/xiao_esp32s3/xiao_esp32s3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">XIAO ESP32S3</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="/contents/labs/seeed/xiao_esp32s3/setup/setup.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Setup</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="/contents/labs/seeed/xiao_esp32s3/image_classification/image_classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Image Classification</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="/contents/labs/seeed/xiao_esp32s3/object_detection/object_detection.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Object Detection</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="/contents/labs/seeed/xiao_esp32s3/kws/kws.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Keyword Spotting (KWS)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="/contents/labs/seeed/xiao_esp32s3/motion_classification/motion_classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Motion Classification and Anomaly Detection</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="/contents/labs/raspi/raspi.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Raspberry Pi</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="/contents/labs/raspi/setup/setup.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Setup</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="/contents/labs/raspi/image_classification/image_classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Image Classification</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="/contents/labs/raspi/object_detection/object_detection.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Object Detection</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="/contents/labs/raspi/llm/llm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Small Language Models (SLM)</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="/contents/labs/shared/shared.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Shared Labs</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="/contents/labs/shared/kws_feature_eng/kws_feature_eng.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">KWS Feature Engineering</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="/contents/labs/shared/dsp_spectral_features_block/dsp_spectral_features_block.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">DSP Spectral Features</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6"  role="navigation" aria-expanded="true">
 <span class="menu-text">REFERENCES</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="/references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" ></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <div id="quarto-toc-target"></div>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span id="sec-model_optimizations" class="quarto-section-identifier"><span class="chapter-number">9</span>  <span class="chapter-title">Model Optimizations</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>

<nav id="TOC" role="doc-toc">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction"><span class="header-section-number">9.1</span> Introduction</a></li>
  <li><a href="#sec-model_ops_representation" id="toc-sec-model_ops_representation"><span class="header-section-number">9.2</span> Efficient Model Representation</a>
  <ul>
  <li><a href="#sec-pruning" id="toc-sec-pruning"><span class="header-section-number">9.2.1</span> Pruning</a>
  <ul>
  <li><a href="#overview" id="toc-overview">Overview</a></li>
  <li><a href="#structured-pruning" id="toc-structured-pruning">Structured Pruning</a></li>
  <li><a href="#advantages-of-structured-pruning" id="toc-advantages-of-structured-pruning">Advantages of Structured Pruning</a></li>
  <li><a href="#unstructured-pruning" id="toc-unstructured-pruning">Unstructured Pruning</a></li>
  <li><a href="#lottery-ticket-hypothesis" id="toc-lottery-ticket-hypothesis">Lottery Ticket Hypothesis</a></li>
  <li><a href="#challenges-limitations" id="toc-challenges-limitations">Challenges &amp; Limitations</a></li>
  </ul></li>
  <li><a href="#model-compression" id="toc-model-compression"><span class="header-section-number">9.2.2</span> Model Compression</a>
  <ul>
  <li><a href="#sec-kd" id="toc-sec-kd">Knowledge Distillation</a></li>
  <li><a href="#low-rank-matrix-factorization" id="toc-low-rank-matrix-factorization">Low-rank Matrix Factorization</a></li>
  <li><a href="#tensor-decomposition" id="toc-tensor-decomposition">Tensor Decomposition</a></li>
  </ul></li>
  <li><a href="#edge-aware-model-design" id="toc-edge-aware-model-design"><span class="header-section-number">9.2.3</span> Edge-Aware Model Design</a>
  <ul>
  <li><a href="#model-design-techniques" id="toc-model-design-techniques">Model Design Techniques</a></li>
  <li><a href="#example-model-architectures" id="toc-example-model-architectures">Example Model Architectures</a></li>
  <li><a href="#streamlining-model-architecture-search" id="toc-streamlining-model-architecture-search">Streamlining Model Architecture Search</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#sec-model_ops_numerics" id="toc-sec-model_ops_numerics"><span class="header-section-number">9.3</span> Efficient Numerics Representation</a>
  <ul>
  <li><a href="#motivation" id="toc-motivation">Motivation</a></li>
  <li><a href="#the-basics" id="toc-the-basics"><span class="header-section-number">9.3.1</span> The Basics</a>
  <ul>
  <li><a href="#types" id="toc-types">Types</a></li>
  <li><a href="#precision" id="toc-precision">Precision</a></li>
  <li><a href="#numeric-encoding-and-storage" id="toc-numeric-encoding-and-storage">Numeric Encoding and Storage</a></li>
  </ul></li>
  <li><a href="#efficiency-benefits" id="toc-efficiency-benefits"><span class="header-section-number">9.3.2</span> Efficiency Benefits</a></li>
  <li><a href="#numeric-representation-nuances" id="toc-numeric-representation-nuances"><span class="header-section-number">9.3.3</span> Numeric Representation Nuances</a>
  <ul>
  <li><a href="#memory-usage" id="toc-memory-usage">Memory Usage</a></li>
  <li><a href="#computational-complexity" id="toc-computational-complexity">Computational Complexity</a></li>
  <li><a href="#hardware-compatibility" id="toc-hardware-compatibility">Hardware Compatibility</a></li>
  <li><a href="#precision-and-accuracy-trade-offs" id="toc-precision-and-accuracy-trade-offs">Precision and Accuracy Trade-offs</a></li>
  <li><a href="#trade-off-examples" id="toc-trade-off-examples">Trade-off Examples</a></li>
  </ul></li>
  <li><a href="#sec-quant" id="toc-sec-quant"><span class="header-section-number">9.3.4</span> Quantization</a>
  <ul>
  <li><a href="#initial-breakdown" id="toc-initial-breakdown">Initial Breakdown</a></li>
  </ul></li>
  <li><a href="#types-1" id="toc-types-1"><span class="header-section-number">9.3.5</span> Types</a>
  <ul>
  <li><a href="#uniform-quantization" id="toc-uniform-quantization">Uniform Quantization</a></li>
  <li><a href="#non-uniform-quantization" id="toc-non-uniform-quantization">Non-uniform Quantization</a></li>
  <li><a href="#stochastic-quantization" id="toc-stochastic-quantization">Stochastic Quantization</a></li>
  <li><a href="#zero-shot-quantization" id="toc-zero-shot-quantization">Zero Shot Quantization</a></li>
  </ul></li>
  <li><a href="#calibration" id="toc-calibration"><span class="header-section-number">9.3.6</span> Calibration</a>
  <ul>
  <li><a href="#symmetric-quantization" id="toc-symmetric-quantization">Symmetric Quantization</a></li>
  <li><a href="#asymmetric-quantization" id="toc-asymmetric-quantization">Asymmetric Quantization</a></li>
  <li><a href="#granularity" id="toc-granularity">Granularity</a></li>
  <li><a href="#static-and-dynamic-quantization" id="toc-static-and-dynamic-quantization">Static and Dynamic Quantization</a></li>
  </ul></li>
  <li><a href="#techniques" id="toc-techniques"><span class="header-section-number">9.3.7</span> Techniques</a></li>
  <li><a href="#weights-vs.-activations" id="toc-weights-vs.-activations"><span class="header-section-number">9.3.8</span> Weights vs. Activations</a></li>
  <li><a href="#trade-offs" id="toc-trade-offs"><span class="header-section-number">9.3.9</span> Trade-offs</a></li>
  <li><a href="#quantization-and-pruning" id="toc-quantization-and-pruning"><span class="header-section-number">9.3.10</span> Quantization and Pruning</a></li>
  <li><a href="#edge-aware-quantization" id="toc-edge-aware-quantization"><span class="header-section-number">9.3.11</span> Edge-aware Quantization</a></li>
  </ul></li>
  <li><a href="#sec-model_ops_hw" id="toc-sec-model_ops_hw"><span class="header-section-number">9.4</span> Efficient Hardware Implementation</a>
  <ul>
  <li><a href="#hardware-aware-neural-architecture-search" id="toc-hardware-aware-neural-architecture-search"><span class="header-section-number">9.4.1</span> Hardware-Aware Neural Architecture Search</a>
  <ul>
  <li><a href="#single-target-fixed-platform-configuration" id="toc-single-target-fixed-platform-configuration">Single Target, Fixed Platform Configuration</a></li>
  <li><a href="#single-target-multiple-platform-configurations" id="toc-single-target-multiple-platform-configurations">Single Target, Multiple Platform Configurations</a></li>
  <li><a href="#multiple-targets" id="toc-multiple-targets">Multiple Targets</a></li>
  <li><a href="#examples-of-hardware-aware-neural-architecture-search" id="toc-examples-of-hardware-aware-neural-architecture-search">Examples of Hardware-Aware Neural Architecture Search</a></li>
  <li><a href="#topology-aware-nas" id="toc-topology-aware-nas">Topology-Aware NAS</a></li>
  </ul></li>
  <li><a href="#challenges-of-hardware-aware-neural-architecture-search" id="toc-challenges-of-hardware-aware-neural-architecture-search"><span class="header-section-number">9.4.2</span> Challenges of Hardware-Aware Neural Architecture Search</a></li>
  <li><a href="#kernel-optimizations" id="toc-kernel-optimizations"><span class="header-section-number">9.4.3</span> Kernel Optimizations</a>
  <ul>
  <li><a href="#general-kernel-optimizations" id="toc-general-kernel-optimizations">General Kernel Optimizations</a></li>
  </ul></li>
  <li><a href="#compute-in-memory-cim" id="toc-compute-in-memory-cim"><span class="header-section-number">9.4.4</span> Compute-in-Memory (CiM)</a></li>
  <li><a href="#memory-access-optimization" id="toc-memory-access-optimization"><span class="header-section-number">9.4.5</span> Memory Access Optimization</a>
  <ul>
  <li><a href="#leveraging-sparsity" id="toc-leveraging-sparsity">Leveraging Sparsity</a></li>
  <li><a href="#optimization-frameworks" id="toc-optimization-frameworks">Optimization Frameworks</a></li>
  <li><a href="#hardware-built-around-software" id="toc-hardware-built-around-software">Hardware Built Around Software</a></li>
  <li><a href="#splitnets" id="toc-splitnets">SplitNets</a></li>
  <li><a href="#hardware-specific-data-augmentation" id="toc-hardware-specific-data-augmentation">Hardware Specific Data Augmentation</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#software-and-framework-support" id="toc-software-and-framework-support"><span class="header-section-number">9.5</span> Software and Framework Support</a>
  <ul>
  <li><a href="#built-in-optimization-apis" id="toc-built-in-optimization-apis"><span class="header-section-number">9.5.1</span> Built-in Optimization APIs</a></li>
  <li><a href="#automated-optimization-tools" id="toc-automated-optimization-tools"><span class="header-section-number">9.5.2</span> Automated Optimization Tools</a></li>
  <li><a href="#hardware-optimization-libraries" id="toc-hardware-optimization-libraries"><span class="header-section-number">9.5.3</span> Hardware Optimization Libraries</a></li>
  <li><a href="#visualizing-optimizations" id="toc-visualizing-optimizations"><span class="header-section-number">9.5.4</span> Visualizing Optimizations</a></li>
  <li><a href="#model-conversion-and-deployment" id="toc-model-conversion-and-deployment"><span class="header-section-number">9.5.5</span> Model Conversion and Deployment</a></li>
  </ul></li>
  <li><a href="#conclusion" id="toc-conclusion"><span class="header-section-number">9.6</span> Conclusion</a></li>
  <li><a href="#sec-model-optimizations-resource" id="toc-sec-model-optimizations-resource"><span class="header-section-number">9.7</span> Resources</a></li>
  </ul>
</nav>
<p>Resources: <a href="#sec-model-optimizations-resource">Slides</a>, <a href="#sec-model-optimizations-resource">Videos</a>, <a href="#sec-model-optimizations-resource">Exercises</a>, <a href="#sec-model-optimizations-resource">Labs</a></p>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img src="images/png/cover_model_optimizations.png" class="img-fluid" /></p>
<figcaption><em>DALL·E 3 Prompt: Illustration of a neural network model represented as a busy construction site, with a diverse group of construction workers, both male and female, of various ethnicities, labeled as ‘pruning’, ‘quantization’, and ‘sparsity’. They are working together to make the neural network more efficient and smaller, while maintaining high accuracy. The ‘pruning’ worker, a Hispanic female, is cutting unnecessary connections from the middle of the network. The ‘quantization’ worker, a Caucasian male, is adjusting or tweaking the weights all over the place. The ‘sparsity’ worker, an African female, is removing unnecessary nodes to shrink the model. Construction trucks and cranes are in the background, assisting the workers in their tasks. The neural network is visually transforming from a complex and large structure to a more streamlined and smaller one.</em></figcaption>
</figure>
</div>
<p>When machine learning models are deployed on systems, especially on resource-constrained embedded systems, the optimization of models is a necessity. While machine learning inherently often demands substantial computational resources, the systems are inherently limited in memory, processing power, and energy. This chapter will dive into the art and science of optimizing machine learning models to ensure they are lightweight, efficient, and effective when deployed in TinyML scenarios.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class='callout-icon'></i>
</div>
<div class="callout-title-container flex-fill">
Learning Objectives
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><p>Learn techniques like pruning, knowledge distillation and specialized model architectures to represent models more efficiently</p></li>
<li><p>Understand quantization methods to reduce model size and enable faster inference through reduced precision numerics</p></li>
<li><p>Explore hardware-aware optimization approaches to match models to target device capabilities</p></li>
<li><p>Develop holistic thinking to balance tradeoffs in model complexity, accuracy, latency, power etc. based on application requirements</p></li>
<li><p>Discover software tools like frameworks and model conversion platforms that enable deployment of optimized models</p></li>
<li><p>Gain strategic insight into selecting and applying model optimizations based on use case constraints and hardware targets</p></li>
</ul>
</div>
</div>
<section id="introduction" class="level2" data-number="9.1">
<h2 data-number="9.1"><span class="header-section-number">9.1</span> Introduction</h2>
<p>The optimization of machine learning models for practical deployment is a critical aspect of AI systems. This chapter focuses on exploring model optimization techniques as they relate to the development of ML systems, ranging from high-level model architecture considerations to low-level hardware adaptations. <a href="#fig-3-sections" class="quarto-xref">Figure <span class="quarto-unresolved-ref">fig-3-sections</span></a> Illustrates the three layers of the optimization stack we cover.</p>
<div id="fig-3-sections" class="quarto-float quarto-figure quarto-figure-center">
<figure class="quarto-float quarto-float-fig">
<div aria-describedby="fig-3-sections-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/modeloptimization_structure.png" class="img-fluid" style="width:50.0%" />
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-3-sections-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure 9.1: Three layers to be covered.
</figcaption>
</figure>
</div>
<p>At the highest level, we examine methodologies for reducing the complexity of model parameters without compromising inferential capabilities. Techniques such as pruning and knowledge distillation offer powerful approaches to compress and refine models while maintaining or even improving their performance, not only in terms of model quality but also in actual system runtime performance. These methods are crucial for creating efficient models that can be deployed in resource-constrained environments.</p>
<p>Furthermore, we explore the role of numerical precision in model computations. Understanding how different levels of numerical precision impact model size, speed, and accuracy is essential for optimizing performance. We investigate various numerical formats and the application of reduced-precision arithmetic, particularly relevant for embedded system deployments where computational resources are often limited.</p>
<p>At the lowest level, we navigate the intricate landscape of hardware-software co-design. This exploration reveals how models can be tailored to leverage the specific characteristics and capabilities of target hardware platforms. By aligning model design with hardware architecture, we can significantly enhance performance and efficiency.</p>
<p>This collective approach focuses on helping us develop and deploy efficient, powerful, and hardware-aware machine learning models. From simplifying model architectures to fine-tuning numerical precision and adapting to specific hardware, this chapter covers the full spectrum of optimization strategies. By the conclusion of this chapter, readers will have gained a thorough understanding of various optimization techniques and their practical applications in real-world scenarios. This knowledge is important for creating machine learning models that not only perform well but are also optimized for the constraints and opportunities presented by modern computing environments.</p>
</section>
<section id="sec-model_ops_representation" class="level2" data-number="9.2">
<h2 data-number="9.2"><span class="header-section-number">9.2</span> Efficient Model Representation</h2>
<p>The first avenue of attack for model optimization starts in familiar territory for most ML practitioners: efficient model representation is often first tackled at the highest level of parametrization abstraction - the model’s architecture itself.</p>
<p>Most traditional ML practitioners design models with a general high-level objective in mind, whether it be image classification, person detection, or keyword spotting as mentioned previously in this textbook. Their designs generally end up naturally fitting into some soft constraints due to limited compute resources during development, but generally these designs are not aware of later constraints, such as those required if the model is to be deployed on a more constrained device instead of the cloud.</p>
<p>In this section, we’ll discuss how practitioners can harness principles of hardware-software co-design even at a model’s high level architecture to make their models compatible with edge devices. From most to least hardware aware at this level of modification, we discuss several of the most common strategies for efficient model parametrization: pruning, model compression, and edge-friendly model architectures. You were introduced to pruning and model compression in <a href="#sec-efficient-model-compression" class="quarto-xref"><span class="quarto-unresolved-ref">sec-efficient-model-compression</span></a>; now, this section will go one step beyond the definitions to provide you with a technical understanding of how these techniques work.</p>
<section id="sec-pruning" class="level3" data-number="9.2.1">
<h3 data-number="9.2.1"><span class="header-section-number">9.2.1</span> Pruning</h3>
<section id="overview" class="level4">
<h4>Overview</h4>
<p>Model pruning is a technique in machine learning that reduces the size and complexity of a neural network model while maintaining its predictive capabilities as much as possible. The goal of model pruning is to remove redundant or non-essential components of the model, including connections between neurons, individual neurons, or even entire layers of the network.</p>
<p>This process typically involves analyzing the machine learning model to identify and remove weights, nodes, or layers that have little impact on the model’s outputs. By selectively pruning a model in this way, the total number of parameters can be reduced significantly without substantial declines in model accuracy. The resulting compressed model requires less memory and computational resources to train and run while enabling faster inference times.</p>
<p>Model pruning is especially useful when deploying machine learning models to devices with limited compute resources, such as mobile phones or TinyML systems. The technique facilitates the deployment of larger, more complex models on these devices by reducing their resource demands. Additionally, smaller models require less data to generalize well and are less prone to overfitting. By providing an efficient way to simplify models, model pruning has become a vital technique for optimizing neural networks in machine learning.</p>
<p>There are several common pruning techniques used in machine learning, these include structured pruning, unstructured pruning, iterative pruning, bayesian pruning, and even random pruning. In addition to pruning the weights, one can also prune the activations. Activation pruning specifically targets neurons or filters that activate rarely or have overall low activation. There are numerous other methods, such as sensitivity and movement pruning. For a comprehensive list of methods, the reader is encouraged to read the following paper: <a href="https://arxiv.org/pdf/2308.06767.pdf">“A Survey on Deep Neural Network Pruning: Taxonomy, Comparison, Analysis, and Recommendations” (2023)</a>.</p>
<p>So how does one choose the type of pruning methods? Many variations of pruning techniques exist where each varies the heuristic of what should be kept and pruned from the model as well as number of times pruning occurs. Traditionally, pruning happens after the model is fully trained, where the pruned model may experience mild accuracy loss. However, as we will discuss further, recent discoveries have found that pruning can be used during training (i.e., iteratively) to identify more efficient and accurate model representations.</p>
</section>
<section id="structured-pruning" class="level4">
<h4>Structured Pruning</h4>
<p>We start with structured pruning, a technique that reduces the size of a neural network by eliminating entire model-specific substructures while maintaining the overall model structure. It removes entire neurons/channels or layers based on importance criteria. For example, for a convolutional neural network (CNN), this could be certain filter instances or channels. For fully connected networks, this could be neurons themselves while maintaining full connectivity or even be elimination of entire model layers that are deemed to be insignificant. This type of pruning often leads to regular, structured sparse networks that are hardware friendly.</p>
<p>Best practices have started to emerge on how to think about structured pruning. There are three main components:</p>
<section id="structures-to-target-for-pruning" class="level5">
<h5>1. Structures to Target for Pruning</h5>
<p>Given the variety of approaches, different structures within a neural network are pruned based on specific criteria. The primary structures for pruning include neurons, channels, and sometimes entire layers, each with its unique implications and methodologies. The goal in each approach is to ensure that the reduced model retains as much of the original model’s predictive prowess as possible while improving computational efficiency and reducing size.</p>
<p>When <strong>neurons</strong> are pruned, we are removing entire neurons along with their associated weights and biases, thereby reducing the width of the layer. This type of pruning is often utilized in fully connected layers.</p>
<p>With <strong>channel</strong> pruning, which is predominantly applied in convolutional neural networks (CNNs), it involves eliminating entire channels or filters, which in turn reduces the depth of the feature maps and impacts the network’s ability to extract certain features from the input data. This is particularly crucial in image processing tasks where computational efficiency is paramount.</p>
<p>Finally, <strong>layer</strong> pruning takes a more aggressive approach by removing entire layers of the network. This significantly reduces the network’s depth and thereby its capacity to model complex patterns and hierarchies in the data. This approach necessitates a careful balance to ensure that the model’s predictive capability is not unduly compromised.</p>
<p><a href="#fig-channel-layer-pruning" class="quarto-xref">Figure <span class="quarto-unresolved-ref">fig-channel-layer-pruning</span></a> demonstrates the difference between channel/filter wise pruning and layer pruning. When we prune a channel, we have to reconfigure the model’s architecture in order to adapt to the structural changes. One adjustment is changing the number of input channels in the subsequent layer (here, the third and deepest layer): changing the depths of the filters that are applied to the layer with the pruned channel. On the other hand, pruning an entire layer (removing all the channels in the layer) requires more drastic adjustments. The main one involves modifying the connections between the remaining layers to replace or bypass the pruned layer. In our case, we reconfigure to connect the first and last layers. In all pruning cases, we have to fine-tune the new structure to adjust the weights.</p>
<div id="fig-channel-layer-pruning" class="quarto-float quarto-figure quarto-figure-center">
<figure class="quarto-float quarto-float-fig">
<div aria-describedby="fig-channel-layer-pruning-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/jpg/modeloptimization_channel_layer_pruning.jpeg" class="img-fluid" />
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-channel-layer-pruning-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure 9.2: Channel vs layer pruning.
</figcaption>
</figure>
</div>
</section>
<section id="establishing-a-criteria-for-pruning" class="level5">
<h5>2. Establishing a Criteria for Pruning</h5>
<p>Establishing well-defined criteria for determining which specific structures to prune from a neural network model is a crucial component of the model pruning process. The core goal here is to identify and remove components that contribute the least to the model’s predictive capabilities, while retaining structures integral to preserving the model’s accuracy.</p>
<p>A widely adopted and effective strategy for systematically pruning structures relies on computing importance scores for individual components like neurons, filters, channels or layers. These scores serve as quantitative metrics to gauge the significance of each structure and its effect on the model’s output.</p>
<p>There are several techniques for assigning these importance scores:</p>
<ul>
<li><strong>Weight Magnitude-Based Pruning</strong>: This approach assigns importance scores to a structure by evaluating the aggregate magnitude of their associated weights. Structures with smaller overall weight magnitudes are considered less critical to the network’s performance.</li>
<li><strong>Gradient-Based Pruning</strong>: This technique utilizes the gradients of the loss function with respect to the weights associated with a structure. Structures with low cumulative gradient magnitudes, indicating minimal impact on the loss when altered, are prime candidates for pruning.</li>
<li><strong>Activation-Based Pruning</strong>: This method tracks how often a neuron or filter is activated by storing this information in a parameter called the activation counter. Each time the structure is activated, the counter is incremented. A low activation count suggests that the structure is less relevant.</li>
<li><strong>Taylor Expansion-Based Pruning</strong>: This approach approximates the change in the loss function from removing a given weight. By assessing the cumulative loss disturbance from removing all the weights associated with a structure, you can identify structures with negligible impact on the loss, making them suitable candidates for pruning.</li>
</ul>
<p>The idea is to measure, either directly or indirectly, the contribution of each component to the model’s output. Structures with minimal influence according to the defined criteria are pruned first. This enables selective, optimized pruning that maximally compresses models while preserving predictive capacity. In general, it is important to evaluate the impact of removing particular structures on the model’s output, with recent works such as <span class="citation" data-cites="rachwan2022winning">(<a href="#ref-rachwan2022winning" role="doc-biblioref">Rachwan et al. 2022</a>)</span> and <span class="citation" data-cites="lubana2020gradient">(<a href="#ref-lubana2020gradient" role="doc-biblioref">Lubana and Dick 2020</a>)</span> investigating combinations of techniques like magnitude-based pruning and gradient-based pruning.</p>
</section>
<section id="selecting-a-pruning-strategy" class="level5">
<h5>3. Selecting a Pruning Strategy</h5>
<p>Now that you understand some techniques for determining the importance of structures within a neural network, the next step is to decide how to apply these insights. This involves selecting an appropriate pruning strategy, which dictates how and when the identified structures are removed and how the model is fine-tuned to maintain its performance. Two main structured pruning strategies exist: iterative pruning and one-shot pruning.</p>
<p><strong>Iterative pruning</strong> gradually removes structures across multiple cycles of pruning followed by fine-tuning. In each cycle, a small set of structures are pruned based on importance criteria. The model is then fine-tuned, allowing it to adjust smoothly to the structural changes before the next pruning iteration. This gradual, cyclic approach prevents abrupt accuracy drops. It allows the model to slowly adapt as structures are reduced across iterations.</p>
<p>Consider a situation where we wish to prune the 6 least effective channels (based on some specific criteria) from a convolutional neural network. In <a href="#fig-iterative-pruning" class="quarto-xref">Figure <span class="quarto-unresolved-ref">fig-iterative-pruning</span></a>, we show a simplified pruning process carried over 3 iterations. In every iteration, we only prune 2 channels. Removing the channels results in accuracy degradation. In the first iteration, the accuracy drops from 0.995 to 0.971. However, after we fine-tune the model on the new structure, we are able to recover from the performance loss, bringing the accuracy up to 0.992. Since the structural changes are minor and gradual, the network can more easily adapt to them. Running the same process 2 more times, we end up with a final accuracy of 0.991 (a loss of only 0.4% from the original) and 27% decrease in the number of channels. Thus, iterative pruning enables us to maintain performance while benefiting from increased computational efficiency due to the decreased model size.</p>
<div id="fig-iterative-pruning" class="quarto-float quarto-figure quarto-figure-center">
<figure class="quarto-float quarto-float-fig">
<div aria-describedby="fig-iterative-pruning-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/jpg/modeloptimization_iterative_pruning.jpeg" class="img-fluid" />
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-iterative-pruning-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure 9.3: Iterative pruning.
</figcaption>
</figure>
</div>
<p><strong>One-shot pruning</strong> takes a more aggressive approach by pruning a large portion of structures simultaneously in one shot based on predefined importance criteria. This is followed by extensive fine-tuning to recover model accuracy. While faster, this aggressive strategy can degrade accuracy if the model cannot recover during fine-tuning.</p>
<p>The choice between these strategies involves weighing factors like model size, target sparsity level, available compute and acceptable accuracy losses. One-shot pruning can rapidly compress models, but iterative pruning may enable better accuracy retention for a target level of pruning. In practice, the strategy is tailored based on use case constraints. The overarching aim is to generate an optimal strategy that removes redundancy, achieves efficiency gains through pruning, and finely tunes the model to stabilize accuracy at an acceptable level for deployment.</p>
<p>Now consider the same network we had in the iterative pruning example. Whereas in the iterative process we pruned 2 channels at a time, in the one-shot pruning we would prune the 6 channels at once, as shown in <a href="#fig-oneshot-pruning" class="quarto-xref">Figure <span class="quarto-unresolved-ref">fig-oneshot-pruning</span></a>. Removing 27% of the network’s channel simultaneously alters the structure significantly, causing the accuracy to drop from 0.995 to 0.914. Given the major changes, the network is not able to properly adapt during fine-tuning, and the accuracy went up to 0.943, a 5% degradation from the accuracy of the unpruned network. While the final structures in both iterative pruning and oneshot pruning processes are identical, the former is able to maintain high performance while the latter suffers significant degradations.</p>
<div id="fig-oneshot-pruning" class="quarto-float quarto-figure quarto-figure-center">
<figure class="quarto-float quarto-float-fig">
<div aria-describedby="fig-oneshot-pruning-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/jpg/modeloptimization_oneshot_pruning.jpeg" class="img-fluid" />
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-oneshot-pruning-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure 9.4: One-shot pruning.
</figcaption>
</figure>
</div>
</section>
</section>
<section id="advantages-of-structured-pruning" class="level4">
<h4>Advantages of Structured Pruning</h4>
<p>Structured pruning brings forth a myriad of advantages that cater to various facets of model deployment and utilization, especially in environments where computational resources are constrained.</p>
<ul>
<li><p><strong>Computational Efficiency:</strong> By eliminating entire structures, such as neurons or channels, structured pruning significantly diminishes the computational load during both training and inference phases, thereby enabling faster model predictions and training convergence. Moreover, the removal of structures inherently reduces the model’s memory footprint, ensuring that it demands less storage and memory during operation, which is particularly beneficial in memory-constrained environments like TinyML systems.</p></li>
<li><p><strong>Hardware Efficiency:</strong> Structured pruning often results in models that are more amenable to deployment on specialized hardware, such as Field-Programmable Gate Arrays (FPGAs) or Application-Specific Integrated Circuits (ASICs), due to the regularity and simplicity of the pruned architecture. With reduced computational requirements, it translates to lower energy consumption, which is crucial for battery-powered devices and sustainable computing practices.</p></li>
<li><p><strong>Maintenance and Deployment:</strong> The pruned model, while smaller, retains its original architectural form, which can simplify the deployment pipeline and ensure compatibility with existing systems and frameworks. Also, with fewer parameters and simpler structures, the pruned model becomes easier to manage and monitor in production environments, potentially reducing the overhead associated with model maintenance and updates. Later on, when we dive into <a href="../ops/ops.qmd">MLOps</a>, this need will become apparent.</p></li>
</ul>
</section>
<section id="unstructured-pruning" class="level4">
<h4>Unstructured Pruning</h4>
<p>Unstructured pruning is, as its name suggests, pruning the model without regard to model-specific substructure. As mentioned above, it offers a greater aggression in pruning and can achieve higher model sparsities while maintaining accuracy given less constraints on what can and can’t be pruned. Generally, post-training unstructured pruning consists of an importance criterion for individual model parameters/weights, pruning/removal of weights that fall below the criteria, and optional fine-tuning after to try and recover the accuracy lost during weight removal.</p>
<p>Unstructured pruning has some advantages over structured pruning: removing individual weights instead of entire model substructures often leads in practice to lower model accuracy decreases. Furthermore, generally determining the criterion of importance for an individual weight is much simpler than for an entire substructure of parameters in structured pruning, making the former preferable for cases where that overhead is hard or unclear to compute. Similarly, the actual process of structured pruning is generally less flexible, as removing individual weights is generally simpler than removing entire substructures and ensuring the model still works.</p>
<p>Unstructured pruning, while offering the potential for significant model size reduction and enhanced deployability, brings with it challenges related to managing sparse representations and ensuring computational efficiency. It is particularly useful in scenarios where achieving the highest possible model compression is paramount and where the deployment environment can handle sparse computations efficiently.</p>
<p><a href="#tbl-pruning_methods" class="quarto-xref">Table <span class="quarto-unresolved-ref">tbl-pruning_methods</span></a> provides a concise comparison between structured and unstructured pruning. In this table, aspects related to the nature and architecture of the pruned model (Definition, Model Regularity, and Compression Level) are grouped together, followed by aspects related to computational considerations (Computational Efficiency and Hardware Compatibility), and ending with aspects related to the implementation and adaptation of the pruned model (Implementation Complexity and Fine-Tuning Complexity). Both pruning strategies offer unique advantages and challenges, as shown in <a href="#tbl-pruning_methods" class="quarto-xref">Table <span class="quarto-unresolved-ref">tbl-pruning_methods</span></a>, and the selection between them should be influenced by specific project and deployment requirements.</p>
<div id="tbl-pruning_methods" class="striped hover quarto-float quarto-figure quarto-figure-center">
<figure class="quarto-float quarto-float-tbl">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-pruning_methods-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table 9.1: Comparison of structured versus unstructured pruning.
</figcaption>
<div aria-describedby="tbl-pruning_methods-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="table-striped table-hover caption-top" style="width:100%;">
<colgroup>
<col style="width: 12%" />
<col style="width: 42%" />
<col style="width: 44%" />
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Aspect</th>
<th style="text-align: left;">Structured Pruning</th>
<th style="text-align: left;">Unstructured Pruning</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Definition</td>
<td style="text-align: left;">Pruning entire structures (e.g., neurons, channels, layers) within the network</td>
<td style="text-align: left;">Pruning individual weights or neurons, resulting in sparse matrices or non-regular network structures</td>
</tr>
<tr class="even">
<td style="text-align: left;">Model Regularity</td>
<td style="text-align: left;">Maintains a regular, structured network architecture</td>
<td style="text-align: left;">Results in irregular, sparse network architectures</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Compression Level</td>
<td style="text-align: left;">May offer limited model compression compared to unstructured pruning</td>
<td style="text-align: left;">Can achieve higher model compression due to fine-grained pruning</td>
</tr>
<tr class="even">
<td style="text-align: left;">Computational Efficiency</td>
<td style="text-align: left;">Typically more computationally efficient due to maintaining regular structures</td>
<td style="text-align: left;">Can be computationally inefficient due to sparse weight matrices, unless specialized hardware/software is used</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Hardware Compatibility</td>
<td style="text-align: left;">Generally better compatible with various hardware due to regular structures</td>
<td style="text-align: left;">May require hardware that efficiently handles sparse computations to realize benefits</td>
</tr>
<tr class="even">
<td style="text-align: left;">Implementation Complexity</td>
<td style="text-align: left;">Often simpler to implement and manage due to maintaining network structure</td>
<td style="text-align: left;">Can be complex to manage and compute due to sparse representations</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Fine-Tuning Complexity</td>
<td style="text-align: left;">May require less complex fine-tuning strategies post-pruning</td>
<td style="text-align: left;">Might necessitate more complex retraining or fine-tuning strategies post-pruning</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p>In <a href="#fig-structured-unstructured" class="quarto-xref">Figure <span class="quarto-unresolved-ref">fig-structured-unstructured</span></a> we have examples that illustrate the differences between unstructured and structured pruning. Observe that unstructured pruning can lead to models that no longer obey high-level structural guarantees of their original unpruned counterparts: the left network is no longer a fully connected network after pruning. Structured pruning on the other hand maintains those invariants: in the middle, the fully connected network is pruned in a way that the pruned network is still fully connected; likewise, the CNN maintains its convolutional structure, albeit with fewer filters.</p>
<div id="fig-structured-unstructured" class="quarto-float quarto-figure quarto-figure-center">
<figure class="quarto-float quarto-float-fig">
<div aria-describedby="fig-structured-unstructured-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/modeloptimization_pruning_comparison.png" class="img-fluid" />
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-structured-unstructured-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure 9.5: Unstructured vs structured pruning. Source: <span class="citation" data-cites="qi2021efficient">Qi et al. (<a href="#ref-qi2021efficient" role="doc-biblioref">2021</a>)</span>.
</figcaption>
</figure>
</div>
</section>
<section id="lottery-ticket-hypothesis" class="level4">
<h4>Lottery Ticket Hypothesis</h4>
<p>Pruning has evolved from a purely post-training technique that came at the cost of some accuracy, to a powerful meta-learning approach applied during training to reduce model complexity. This advancement in turn improves compute, memory, and latency efficiency at both training and inference.</p>
<p>A breakthrough finding that catalyzed this evolution was the <a href="https://arxiv.org/abs/1803.03635">lottery ticket hypothesis</a> by <span class="citation" data-cites="jonathan2019lottery">Frankle and Carbin (<a href="#ref-jonathan2019lottery" role="doc-biblioref">2019</a>)</span>. Their work states that within dense neural networks, there exist sparse subnetworks, referred to as “winning tickets,” that can match or even exceed the performance of the original model when trained in isolation. Specifically, these winning tickets, when initialized using the same weights as the original network, can achieve similarly high training convergence and accuracy on a given task. It is worthwhile pointing out that they empirically discovered the lottery ticket hypothesis, which was later formalized.</p>
<p>The intuition behind this hypothesis is that, during the training process of a neural network, many neurons and connections become redundant or unimportant, particularly with the inclusion of training techniques encouraging redundancy like dropout. Identifying, pruning out, and initializing these “winning tickets’’ allows for faster training and more efficient models, as they contain the essential model decision information for the task. Furthermore, as generally known with the bias-variance tradeoff theory, these tickets suffer less from overparameterization and thus generalize better rather than overfitting to the task.</p>
<p>In <a href="#fig-lottery-ticket-hypothesis" class="quarto-xref">Figure <span class="quarto-unresolved-ref">fig-lottery-ticket-hypothesis</span></a> we have an example experiment showing pruning and training experiments on a fully connected LeNet over a variety of pruning ratios. In the left plot, notice how heavy pruning reveals a more efficient subnetwork (in green) that is 21.1% the size of the original network (in blue), The subnetwork achieves higher accuracy and in a faster manner than the unpruned version (green line is above the blue line). However, pruning has a limit (sweet spot), and further pruning will produce performance degradations and eventually drop below the unpruned version’s performance (notice how the red, purple, and brown subnetworks gradually drop in accuracy performance) due to the significant loss in the number of parameters.</p>
<div id="fig-lottery-ticket-hypothesis" class="quarto-float quarto-figure quarto-figure-center">
<figure class="quarto-float quarto-float-fig">
<div aria-describedby="fig-lottery-ticket-hypothesis-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/modeloptimization_lottery_ticket_hypothesis.png" class="img-fluid" />
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-lottery-ticket-hypothesis-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure 9.6: Lottery ticket hypothesis experiments.
</figcaption>
</figure>
</div>
<p>To uncover these winning lottery tickets within a neural network, a systematic process is followed. This process, which is illustrated in <a href="#fig-winning-ticket" class="quarto-xref">Figure <span class="quarto-unresolved-ref">fig-winning-ticket</span></a> (left side), involves iteratively training, pruning, and reinitializing the network. The steps below outline this approach:</p>
<ol type="1">
<li><p>Initialize the network’s weights to random values.</p></li>
<li><p>Train the network until it converges to the desired performance.</p></li>
<li><p>Prune out some percentage of the edges with the lowest weight values.</p></li>
<li><p>Reinitialize the network with the same random values from step 1.</p></li>
<li><p>Repeat steps 2-4 for a number of times, or as long as the accuracy doesn’t significantly degrade.</p></li>
</ol>
<p>When we finish, we are left with a pruned network (<a href="#fig-winning-ticket" class="quarto-xref">Figure <span class="quarto-unresolved-ref">fig-winning-ticket</span></a> right side), which is a subnetwork of the one we start with. The subnetwork should have a significantly smaller structure, while maintaining a comparable level of accuracy.</p>
<div id="fig-winning-ticket" class="quarto-float quarto-figure quarto-figure-center">
<figure class="quarto-float quarto-float-fig">
<div aria-describedby="fig-winning-ticket-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/jpg/modeloptimization_winning_ticket.jpeg" class="img-fluid" />
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-winning-ticket-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure 9.7: Finding the winning ticket subnetwork.
</figcaption>
</figure>
</div>
</section>
<section id="challenges-limitations" class="level4">
<h4>Challenges &amp; Limitations</h4>
<p>There is no free lunch with pruning optimizations, with some choices coming with both improvements and costs to considers. Below we discuss some tradeoffs for practitioners to consider.</p>
<ul>
<li><p><strong>Managing Sparse Weight Matrices:</strong> A sparse weight matrix is a matrix in which many of the elements are zero. Unstructured pruning often results in sparse weight matrices, where many weights are pruned to zero. While this reduces model size, it also introduces several challenges. Computational inefficiency can arise because standard hardware is optimized for dense matrix operations. Without optimizations that take advantage of sparsity, the computational savings from pruning can be lost. Although sparse matrices can be stored without specialized formats, effectively leveraging their sparsity requires careful handling to avoid wasting resources. Algorithmically, navigating sparse structures requires efficiently skipping over zero entries, which adds complexity to the computation and model updates.</p></li>
<li><p><strong>Quality vs. Size Reduction:</strong> A key challenge in both structured and unstructured pruning is balancing size reduction with maintaining or improving predictive performance. Establishing robust pruning criteria, whether for removing entire structures (structured pruning) or individual weights (unstructured pruning), is essential. These pruning criteria chosen must accurately identify elements whose removal minimally impacts performance. Careful experimentation is often needed to ensure the pruned model remains efficient while maintaining its predictive performance.</p></li>
<li><p><strong>Fine-Tuning and Retraining:</strong> Post-pruning fine-tuning is imperative in both structured and unstructured pruning to recover lost performance and stabilize the model. The challenge encompasses determining the extent, duration, and nature of the fine-tuning process, which can be influenced by the pruning method and the degree of pruning applied.</p></li>
<li><p><strong>Hardware Compatibility and Efficiency:</strong> Especially pertinent to unstructured pruning, hardware compatibility and efficiency become critical. Unstructured pruning often results in sparse weight matrices, which may not be efficiently handled by certain hardware, potentially negating the computational benefits of pruning (see <a href="#fig-sparse-matrix" class="quarto-xref">Figure <span class="quarto-unresolved-ref">fig-sparse-matrix</span></a>). Ensuring that pruned models, particularly those resulting from unstructured pruning, are scalable, compatible, and efficient on the target hardware is a significant consideration.</p></li>
<li><p><strong>Legal and Ethical Considerations:</strong> Last but not least, adherence to legal and ethical guidelines is important, especially in domains with significant consequences. Pruning methods must undergo rigorous validation, testing, and potentially certification processes to ensure compliance with relevant regulations and standards, though arguably at this time no such formal standards and best practices exist that are vetted and validated by 3rd party entities. This is particularly crucial in high-stakes applications like medical AI and autonomous driving, where quality drops due to pruning-like optimizations can be life-threatening. Moreover, ethical considerations extend beyond safety to fairness and equality; recent work by <span class="citation" data-cites="tran2022pruning">(<a href="#ref-tran2022pruning" role="doc-biblioref">Tran et al. 2022</a>)</span> has revealed that pruning can disproportionately impact people of color, underscoring the need for comprehensive ethical evaluation in the pruning process.</p></li>
</ul>
<div id="fig-sparse-matrix" class="quarto-float quarto-figure quarto-figure-center">
<figure class="quarto-float quarto-float-fig">
<div aria-describedby="fig-sparse-matrix-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/jpg/modeloptimization_sprase_matrix.jpeg" class="img-fluid" />
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-sparse-matrix-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure 9.8: Sparse weight matrix.
</figcaption>
</figure>
</div>
<div id="exr-p" class="callout callout-style-default callout-caution callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class='callout-icon'></i>
</div>
<div class="callout-title-container flex-fill">
Exercise 9.1: Pruning
</div>
<div class='callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end'><i class='callout-toggle'></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Imagine your neural network is a giant, overgrown bush. Pruning is like strategically trimming away branches to make it stronger and more efficient! In the Colab, you’ll learn how to do this trimming in TensorFlow. Understanding these concepts will give you the foundation to see how pruning makes models small enough to run on your phone!</p>
<p><a href="https://colab.research.google.com/github/tensorflow/model-optimization/blob/master/tensorflow_model_optimization/g3doc/guide/pruning/pruning_with_keras.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.png" class="img-fluid" /></a></p>
</div>
</div>
</div>
</section>
</section>
<section id="model-compression" class="level3" data-number="9.2.2">
<h3 data-number="9.2.2"><span class="header-section-number">9.2.2</span> Model Compression</h3>
<p>Model compression techniques are crucial for deploying deep learning models on resource-constrained devices. These techniques aim to create smaller, more efficient models that preserve the predictive performance of the original models.</p>
<section id="sec-kd" class="level4">
<h4>Knowledge Distillation</h4>
<p>One popular technique is <strong>knowledge distillation (KD)</strong>, which transfers knowledge from a large, complex “teacher” model to a smaller “student” model. The key idea is to train the student model to mimic the teacher’s outputs. The concept of KD was first popularized by <span class="citation" data-cites="hinton2015distilling">Hinton (<a href="#ref-hinton2015distilling" role="doc-biblioref">2005</a>)</span>.</p>
<section id="overview-and-benefits" class="level5">
<h5>Overview and Benefits</h5>
<p>Knowledge distillation involves transferring knowledge from a large, complex teacher model to a smaller student model. The core idea is to use the teacher’s outputs, known as <strong>soft targets</strong>, to guide the training of the student model. Unlike traditional “hard targets” (the true labels), soft targets are the probability distributions over classes that the teacher model predicts. These distributions provide richer information about the relationships between classes, which can help the student model learn more effectively.</p>
<p>You have learned that the softmax function converts a model’s raw outputs into a probability distribution over classes. A key technique in KD is <strong>temperature scaling</strong>, which is applied to the softmax function of the teacher model’s outputs. By introducing a temperature parameter, the distribution can be adjusted: a higher temperature produces softer probabilities, meaning the differences between class probabilities become less extreme. This softening effect results in a more uniform distribution, where the model’s confidence in the most likely class is reduced, and other classes have higher, non-zero probabilities. This is valuable for the student model because it allows it to learn not just from the most likely class but from the relative probabilities of all classes, capturing subtle patterns that might be missed if trained only on hard targets. Thus, temperature scaling facilitates the transfer of more nuanced knowledge from the teacher to the student model.</p>
<p>The loss function in knowledge distillation typically combines two components: a distillation loss and a classification loss. The distillation loss, often calculated using Kullback-Leibler (KL) divergence, measures the difference between the soft targets produced by the teacher model and the outputs of the student model, encouraging the student to mimic the teacher’s predictions. Meanwhile, the classification loss ensures that the student model correctly predicts the true labels based on the original data. Together, these two components help the student model retain the knowledge of the teacher while adhering to the ground truth labels.</p>
<p>These components, when adeptly configured and harmonized, enable the student model to assimilate the teacher model’s knowledge, crafting a pathway towards efficient and robust smaller models that retain the predictive prowess of their larger counterparts. <a href="#fig-knowledge-distillation" class="quarto-xref">Figure <span class="quarto-unresolved-ref">fig-knowledge-distillation</span></a> visualizes the training procedure of knowledge distillation. Note how the logits or soft labels of the teacher model are used to provide a distillation loss for the student model to learn from.</p>
<div id="fig-knowledge-distillation" class="quarto-float quarto-figure quarto-figure-center">
<figure class="quarto-float quarto-float-fig">
<div aria-describedby="fig-knowledge-distillation-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/modeloptimization_knowledge_distillation.png" class="img-fluid" />
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-knowledge-distillation-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure 9.9: Knowledge distillation training process. Source: <span class="citation" data-cites="intellabs2023knowledge">IntelLabs (<a href="#ref-intellabs2023knowledge" role="doc-biblioref">2023</a>)</span>.
</figcaption>
</figure>
</div>
</section>
<section id="challenges" class="level5">
<h5>Challenges</h5>
<p>However, KD has a unique set of challenges and considerations that researchers and practitioners must attentively address. One of the challenges is in the meticulous tuning of hyperparameters, such as the temperature parameter in the softmax function and the weighting between the distillation and classification loss in the objective function. Striking a balance that effectively leverages the softened outputs of the teacher model while maintaining fidelity to the true data labels is non-trivial and can significantly impact the student model’s performance and generalization capabilities.</p>
<p>Furthermore, the architecture of the student model itself poses a considerable challenge. Designing a model that is compact to meet computational and memory constraints, while still being capable of assimilating the essential knowledge from the teacher model, demands a nuanced understanding of model capacity and the inherent trade-offs involved in compression. The student model must be carefully architected to navigate the dichotomy of size and performance, ensuring that the distilled knowledge is meaningfully captured and utilized. Moreover, the choice of teacher model, which inherently influences the quality and nature of the knowledge to be transferred, is important and it introduces an added layer of complexity to the KD process.</p>
<p>These challenges underscore the necessity for a thorough and nuanced approach to implementing KD, ensuring that the resultant student models are both efficient and effective in their operational contexts.</p>
</section>
</section>
<section id="low-rank-matrix-factorization" class="level4">
<h4>Low-rank Matrix Factorization</h4>
<p>Similar in approximation theme, low-rank matrix factorization (LRMF) is a mathematical technique used in linear algebra and data analysis to approximate a given matrix by decomposing it into two or more lower-dimensional matrices. The fundamental idea is to express a high-dimensional matrix as a product of lower-rank matrices, which can help reduce the complexity of data while preserving its essential structure. Mathematically, given a matrix <span class="math inline">\(A \in \mathbb{R}^{m \times n}\)</span>, LRMF seeks matrices <span class="math inline">\(U \in \mathbb{R}^{m \times k}\)</span> and <span class="math inline">\(V \in \mathbb{R}^{k \times n}\)</span> such that <span class="math inline">\(A \approx UV\)</span>, where <span class="math inline">\(k\)</span> is the rank and is typically much smaller than <span class="math inline">\(m\)</span> and <span class="math inline">\(n\)</span>.</p>
<section id="background-and-benefits" class="level5">
<h5>Background and Benefits</h5>
<p>One of the seminal works in the realm of matrix factorization, particularly in the context of recommendation systems, is the paper by <span class="citation" data-cites="koren2009matrix">Koren, Bell, and Volinsky (<a href="#ref-koren2009matrix" role="doc-biblioref">2009</a>)</span>. The authors look into various factorization models, providing insights into their efficacy in capturing the underlying patterns in the data and enhancing predictive accuracy in collaborative filtering. LRMF has been widely applied in recommendation systems (such as Netflix, Facebook, etc.), where the user-item interaction matrix is factorized to capture latent factors corresponding to user preferences and item attributes.</p>
<p>The main advantage of low-rank matrix factorization lies in its ability to reduce data dimensionality as shown in <a href="#fig-matrix-factorization" class="quarto-xref">Figure <span class="quarto-unresolved-ref">fig-matrix-factorization</span></a>, where there are fewer parameters to store, making it computationally more efficient and reducing storage requirements at the cost of some additional compute. This can lead to faster computations and more compact data representations, which is especially valuable when dealing with large datasets. Additionally, it may aid in noise reduction and can reveal underlying patterns and relationships in the data.</p>
<p><a href="#fig-matrix-factorization" class="quarto-xref">Figure <span class="quarto-unresolved-ref">fig-matrix-factorization</span></a> illustrates the decrease in parameterization enabled by low-rank matrix factorization. Observe how the matrix <span class="math inline">\(M\)</span> can be approximated by the product of matrices <span class="math inline">\(L_k\)</span> and <span class="math inline">\(R_k^T\)</span>. For intuition, most fully connected layers in networks are stored as a projection matrix <span class="math inline">\(M\)</span>, which requires <span class="math inline">\(m \times n\)</span> parameter to be loaded on computation. However, by decomposing and approximating it as the product of two lower rank matrices, we thus only need to store <span class="math inline">\(m \times k + k\times n\)</span> parameters in terms of storage while incurring an additional compute cost of the matrix multiplication. So long as <span class="math inline">\(k &lt; n/2\)</span>, this factorization has fewer parameters total to store while adding a computation of runtime <span class="math inline">\(O(mkn)\)</span> <span class="citation" data-cites="gu2023deep">(<a href="#ref-gu2023deep" role="doc-biblioref">Gu 2023</a>)</span>.</p>
<div id="fig-matrix-factorization" class="quarto-float quarto-figure quarto-figure-center">
<figure class="quarto-float quarto-float-fig">
<div aria-describedby="fig-matrix-factorization-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/modeloptimization_low_rank_matrix_factorization.png" class="img-fluid" />
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-matrix-factorization-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure 9.10: Low matrix factorization. Source: <a href="https://dustinstansbury.github.io/theclevermachine/svd-data-compression">The Clever Machine.</a>
</figcaption>
</figure>
</div>
</section>
<section id="challenges-1" class="level5">
<h5>Challenges</h5>
<p>But practitioners and researchers encounter a spectrum of challenges and considerations that necessitate careful attention and strategic approaches. As with any lossy compression technique, we may lose information during this approximation process: choosing the correct rank that balances the information lost and the computational costs is tricky as well and adds an additional hyper-parameter to tune for.</p>
<p>Low-rank matrix factorization is a valuable tool for dimensionality reduction and making compute fit onto edge devices but, like other techniques, needs to be carefully tuned to the model and task at hand. A key challenge resides in managing the computational complexity inherent to LRMF, especially when grappling with high-dimensional and large-scale data. The computational burden, particularly in the context of real-time applications and massive datasets, remains a significant hurdle for effectively using LRMF.</p>
<p>Moreover, the conundrum of choosing the optimal rank <span class="math inline">\(k\)</span>, for the factorization introduces another layer of complexity. The selection of <span class="math inline">\(k\)</span> inherently involves a trade-off between approximation accuracy and model simplicity, and identifying a rank that adeptly balances these conflicting objectives often demands a combination of domain expertise, empirical validation, and sometimes, heuristic approaches. The challenge is further amplified when the data encompasses noise or when the inherent low-rank structure is not pronounced, making the determination of a suitable <span class="math inline">\(k\)</span> even more elusive.</p>
<p>Handling missing or sparse data, a common occurrence in applications like recommendation systems, poses another substantial challenge. Traditional matrix factorization techniques, such as Singular Value Decomposition (SVD), are not directly applicable to matrices with missing entries, necessitating the development and application of specialized algorithms that can factorize incomplete matrices while mitigating the risks of overfitting to the observed entries. This often involves incorporating regularization terms or constraining the factorization in specific ways, which in turn introduces additional hyperparameters that need to be judiciously selected.</p>
<p>Furthermore, in scenarios where data evolves or grows over time, developing LRMF models that can adapt to new data without necessitating a complete re-factorization is a critical yet challenging endeavor. Online and incremental matrix factorization algorithms seek to address this by enabling the update of factorized matrices as new data arrives, yet ensuring stability, accuracy, and computational efficiency in these dynamic settings remains an intricate task. This is particularly challenging in the space of TinyML, where edge redeployment for refreshed models can be quite challenging.</p>
</section>
</section>
<section id="tensor-decomposition" class="level4">
<h4>Tensor Decomposition</h4>
<p>You have learned in <a href="#sec-tensor-data-structures" class="quarto-xref"><span class="quarto-unresolved-ref">sec-tensor-data-structures</span></a> that tensors are flexible structures, commonly used by ML Frameworks, that can represent data in higher dimensions. Similar to low-rank matrix factorization, more complex models may store weights in higher dimensions, such as tensors. Tensor decomposition is the higher-dimensional analogue of matrix factorization, where a model tensor is decomposed into lower-rank components (see <a href="#fig-tensor-decomposition" class="quarto-xref">Figure <span class="quarto-unresolved-ref">fig-tensor-decomposition</span></a>). These lower-rank components are easier to compute on and store but may suffer from the same issues mentioned above, such as information loss and the need for nuanced hyperparameter tuning. Mathematically, given a tensor <span class="math inline">\(\mathcal{A}\)</span>, tensor decomposition seeks to represent <span class="math inline">\(\mathcal{A}\)</span> as a combination of simpler tensors, facilitating a compressed representation that approximates the original data while minimizing the loss of information.</p>
<div id="fig-tensor-decomposition" class="quarto-float quarto-figure quarto-figure-center">
<figure class="quarto-float quarto-float-fig">
<div aria-describedby="fig-tensor-decomposition-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/modeloptimization_tensor_decomposition.png" class="img-fluid" />
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-tensor-decomposition-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure 9.11: Tensor decomposition. Source: <span class="citation" data-cites="xinyu">Xinyu (<a href="#ref-xinyu" role="doc-biblioref">n.d.</a>)</span>.
</figcaption>
</figure>
</div>
<p>The work of Tamara G. Kolda and Brett W. Bader, <a href="https://epubs.siam.org/doi/abs/10.1137/07070111X">“Tensor Decompositions and Applications”</a> (2009), stands out as a seminal paper in the field of tensor decompositions. The authors provide a comprehensive overview of various tensor decomposition methods, exploring their mathematical underpinnings, algorithms, and a wide array of applications, ranging from signal processing to data mining. Of course, the reason we are discussing it is because it has huge potential for system performance improvements, particularly in the space of TinyML, where throughput and memory footprint savings are crucial to feasibility of deployments.</p>
<div id="exr-mc" class="callout callout-style-default callout-caution callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-3-contents" aria-controls="callout-3" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class='callout-icon'></i>
</div>
<div class="callout-title-container flex-fill">
Exercise 9.2: Scalable Model Compression with TensorFlow
</div>
<div class='callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end'><i class='callout-toggle'></i></div>
</div>
<div id="callout-3" class="callout-3-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>This Colab dives into a technique for compressing models while maintaining high accuracy. The key idea is to train a model with an extra penalty term that encourages the model to be more compressible. Then, the model is encoded using a special coding scheme that aligns with this penalty. This approach allows you to achieve compressed models that perform just as well as the original models and is useful in deploying models to devices with limited resources like mobile phones and edge devices.</p>
<p><a href="https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/optimization/compression.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.png" class="img-fluid" /></a></p>
</div>
</div>
</div>
</section>
</section>
<section id="edge-aware-model-design" class="level3" data-number="9.2.3">
<h3 data-number="9.2.3"><span class="header-section-number">9.2.3</span> Edge-Aware Model Design</h3>
<p>Now, we reach the other end of the hardware-software gradient, where we specifically make model architecture decisions directly given knowledge of the edge devices we wish to deploy on.</p>
<p>As covered in previous sections, edge devices are constrained specifically with limitations on memory and parallelizable computations: as such, if there are critical inference speed requirements, computations must be flexible enough to satisfy hardware constraints, something that can be designed at the model architecture level. Furthermore, trying to cram SOTA large ML models onto edge devices even after pruning and compression is generally infeasible purely due to size: the model complexity itself must be chosen with more nuance as to more feasibly fit the device. Edge ML developers have approached this architectural challenge both through designing bespoke edge ML model architectures and through device-aware neural architecture search (NAS), which can more systematically generate feasible on-device model architectures.</p>
<section id="model-design-techniques" class="level4">
<h4>Model Design Techniques</h4>
<p>One edge friendly architecture design, commonly used in deep learning for image processing, is depthwise separable convolutions. It consists of two distinct steps: the first is the depthwise convolution, where each input channel is convolved independently with its own set of learnable filters, as shown in <a href="#fig-depthwise-convolution" class="quarto-xref">Figure <span class="quarto-unresolved-ref">fig-depthwise-convolution</span></a>. This step reduces computational complexity by a significant margin compared to standard convolutions, as it drastically reduces the number of parameters and computations involved. The second step is the pointwise convolution, which combines the output of the depthwise convolution channels through a 1x1 convolution, creating inter-channel interactions. This approach offers several advantages. Benefits include reduced model size, faster inference times, and often better generalization due to fewer parameters, making it suitable for mobile and embedded applications. However, depthwise separable convolutions may not capture complex spatial interactions as effectively as standard convolutions and might require more depth (layers) to achieve the same level of representational power, potentially leading to longer training times. Nonetheless, their efficiency in terms of parameters and computation makes them a popular choice in modern convolutional neural network architectures.</p>
<div id="fig-depthwise-convolution" class="quarto-float quarto-figure quarto-figure-center">
<figure class="quarto-float quarto-float-fig">
<div aria-describedby="fig-depthwise-convolution-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/modeloptimization_depthwise_separable_convolution.png" class="img-fluid" />
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-depthwise-convolution-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure 9.12: Depthwise separable convolutions. Source: <span class="citation" data-cites="hegde2023introduction">Hegde (<a href="#ref-hegde2023introduction" role="doc-biblioref">2023</a>)</span>.
</figcaption>
</figure>
</div>
</section>
<section id="example-model-architectures" class="level4">
<h4>Example Model Architectures</h4>
<p>In this vein, a number of recent architectures have been, from inception, specifically designed for maximizing accuracy on an edge deployment, notably SqueezeNet, MobileNet, and EfficientNet.</p>
<ul>
<li><p><a href="https://arxiv.org/abs/1602.07360">SqueezeNet</a> by <span class="citation" data-cites="iandola2016squeezenet">Iandola et al. (<a href="#ref-iandola2016squeezenet" role="doc-biblioref">2016</a>)</span> for instance, utilizes a compact architecture with 1x1 convolutions and fire modules to minimize the number of parameters while maintaining strong accuracy.</p></li>
<li><p><a href="https://arxiv.org/abs/1704.04861">MobileNet</a> by <span class="citation" data-cites="howard2017mobilenets">Howard et al. (<a href="#ref-howard2017mobilenets" role="doc-biblioref">2017</a>)</span>, on the other hand, employs the aforementioned depthwise separable convolutions to reduce both computation and model size.</p></li>
<li><p><a href="https://arxiv.org/abs/1905.11946">EfficientNet</a> by <span class="citation" data-cites="tan2020efficientnet">Tan and Le (<a href="#ref-tan2020efficientnet" role="doc-biblioref">2023</a>)</span> takes a different approach by optimizing network scaling (i.e. varying the depth, width and resolution of a network) and compound scaling, a more nuanced variation network scaling, to achieve superior performance with fewer parameters.</p></li>
</ul>
<p>These models are essential in the context of edge computing where limited processing power and memory require lightweight yet effective models that can efficiently perform tasks such as image recognition, object detection, and more. Their design principles showcase the importance of intentionally tailored model architecture for edge computing, where performance and efficiency must fit within constraints.</p>
</section>
<section id="streamlining-model-architecture-search" class="level4">
<h4>Streamlining Model Architecture Search</h4>
<p>Lastly, to address the challenge of finding efficient model architectures that are compatible with edge devices, researchers have developed systematized pipelines that streamline the search for performant designs. Two notable frameworks in this space are <a href="https://arxiv.org/abs/2007.10319">TinyNAS</a> by <span class="citation" data-cites="lin2020mcunet">J. Lin et al. (<a href="#ref-lin2020mcunet" role="doc-biblioref">2020</a>)</span> and <a href="https://arxiv.org/abs/1711.06798">MorphNet</a> by <span class="citation" data-cites="gordon2018morphnet">Gordon et al. (<a href="#ref-gordon2018morphnet" role="doc-biblioref">2018</a>)</span>, which automate the process of optimizing neural network architectures for edge deployment.</p>
<p>TinyNAS is an innovative neural architecture search framework introduced in the MCUNet paper, designed to efficiently discover lightweight neural network architectures for edge devices with limited computational resources. Leveraging reinforcement learning and a compact search space of micro neural modules, TinyNAS optimizes for both accuracy and latency, enabling the deployment of deep learning models on microcontrollers, IoT devices, and other resource-constrained platforms. Specifically, TinyNAS, in conjunction with a network optimizer TinyEngine, generates different search spaces by scaling the input resolution and the model width of a model, then collects the computation FLOPs distribution of satisfying networks within the search space to evaluate its priority. TinyNAS relies on the assumption that a search space that accommodates higher FLOPs under memory constraint can produce higher accuracy models, something that the authors verified in practice in their work. In empirical performance, TinyEngine reduced the peak memory usage of models by around 3.4 times and accelerated inference by 1.7 to 3.3 times compared to <a href="https://www.tensorflow.org/lite">TFLite</a> and <a href="https://www.keil.com/pack/doc/CMSIS/NN/html/index.html">CMSIS-NN</a>.</p>
<p>Similarly, MorphNet is a neural network optimization framework designed to automatically reshape and morph the architecture of deep neural networks, optimizing them for specific deployment requirements. It achieves this through two steps: first, it leverages a set of customizable network morphing operations, such as widening or deepening layers, to dynamically adjust the network’s structure. These operations enable the network to adapt to various computational constraints, including model size, latency, and accuracy targets, which are extremely prevalent in edge computing usage. In the second step, MorphNet uses a reinforcement learning-based approach to search for the optimal permutation of morphing operations, effectively balancing the trade-off between model size and performance. This innovative method allows deep learning practitioners to automatically tailor neural network architectures to specific application and hardware requirements, ensuring efficient and effective deployment across various platforms.</p>
<p>TinyNAS and MorphNet represent a few of the many significant advancements in the field of systematic neural network optimization, allowing architectures to be systematically chosen and generated to fit perfectly within problem constraints.</p>
<div id="exr-md" class="callout callout-style-default callout-caution callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-4-contents" aria-controls="callout-4" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class='callout-icon'></i>
</div>
<div class="callout-title-container flex-fill">
Exercise 9.3: Edge-Aware Model Design
</div>
<div class='callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end'><i class='callout-toggle'></i></div>
</div>
<div id="callout-4" class="callout-4-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Imagine you’re building a tiny robot that can identify different flowers. It needs to be smart, but also small and energy-efficient! In the “Edge-Aware Model Design” world, we learned about techniques like depthwise separable convolutions and architectures like SqueezeNet, MobileNet, and EfficientNet—all designed to pack intelligence into compact models. Now, let’s see these ideas in action with some xColabs:</p>
<p><strong>SqueezeNet in Action:</strong> Maybe you’d like a Colab showing how to train a SqueezeNet model on a flower image dataset. This would demonstrate its small size and how it learns to recognize patterns despite its efficiency.</p>
<p><a href="https://colab.research.google.com/github/GoogleCloudPlatform/training-data-analyst/blob/master/courses/fast-and-lean-data-science/07_Keras_Flowers_TPU_squeezenet.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.png" class="img-fluid" /></a></p>
<p><strong>MobileNet Exploration:</strong> Ever wonder if those tiny image models are just as good as the big ones? Let’s find out! In this Colab, we’re pitting MobileNet, the lightweight champion, against a classic image classification model. We’ll race them for speed, measure their memory needs, and see who comes out on top for accuracy. Get ready for a battle of the image brains!</p>
<p><a href="https://colab.research.google.com/drive/1bOzVaDQo8h6Ngstb7AcfzC35OihpHspt"><img src="https://colab.research.google.com/assets/colab-badge.png" class="img-fluid" /></a></p>
</div>
</div>
</div>
</section>
</section>
</section>
<section id="sec-model_ops_numerics" class="level2" data-number="9.3">
<h2 data-number="9.3"><span class="header-section-number">9.3</span> Efficient Numerics Representation</h2>
<p>Numerics representation involves a myriad of considerations, including, but not limited to, the precision of numbers, their encoding formats, and the arithmetic operations facilitated. It invariably involves a rich array of different trade-offs, where practitioners are tasked with navigating between numerical accuracy and computational efficiency. For instance, while lower-precision numerics may offer the allure of reduced memory usage and expedited computations, they concurrently present challenges pertaining to numerical stability and potential degradation of model accuracy.</p>
<section id="motivation" class="level4">
<h4>Motivation</h4>
<p>The imperative for efficient numerics representation arises, particularly as efficient model optimization alone falls short when adapting models for deployment on low-powered edge devices operating under stringent constraints.</p>
<p>Beyond minimizing memory demands, the tremendous potential of efficient numerics representation lies in, but is not limited to, these fundamental ways. By diminishing computational intensity, efficient numerics can thereby amplify computational speed, allowing more complex models to compute on low-powered devices. Reducing the bit precision of weights and activations on heavily over-parameterized models enables condensation of model size for edge devices without significantly harming the model’s predictive accuracy. With the omnipresence of neural networks in models, efficient numerics has a unique advantage in leveraging the layered structure of NNs to vary numeric precision across layers, minimizing precision in resistant layers while preserving higher precision in sensitive layers.</p>
<p>In this section, we will dive into how practitioners can harness the principles of hardware-software co-design at the lowest levels of a model to facilitate compatibility with edge devices. Kicking off with an introduction to the numerics, we will examine its implications for device memory and computational complexity. Subsequently, we will embark on a discussion regarding the trade-offs entailed in adopting this strategy, followed by a deep dive into a paramount method of efficient numerics: quantization.</p>
</section>
<section id="the-basics" class="level3" data-number="9.3.1">
<h3 data-number="9.3.1"><span class="header-section-number">9.3.1</span> The Basics</h3>
<section id="types" class="level4">
<h4>Types</h4>
<p>Numerical data, the bedrock upon which machine learning models stand, manifest in two primary forms. These are integers and floating point numbers.</p>
<p><strong>Integers:</strong> Whole numbers, devoid of fractional components, integers (e.g., -3, 0, 42) are key in scenarios demanding discrete values. For instance, in ML, class labels in a classification task might be represented as integers, where “cat”, “dog”, and “bird” could be encoded as 0, 1, and 2 respectively.</p>
<p><strong>Floating-Point Numbers:</strong> Encompassing real numbers, floating-point numbers (e.g., -3.14, 0.01, 2.71828) afford the representation of values with fractional components. In ML model parameters, weights might be initialized with small floating-point values, such as 0.001 or -0.045, to commence the training process. Currently, there are 4 popular precision formats discussed below.</p>
<p><strong>Variable bit widths:</strong> Beyond the standard widths, research is ongoing into extremely low bit-width numerics, even down to binary or ternary representations. Extremely low bit-width operations can offer significant speedups and reduce power consumption even further. While challenges remain in maintaining model accuracy with such drastic quantization, advances continue to be made in this area.</p>
</section>
<section id="precision" class="level4">
<h4>Precision</h4>
<p>Precision, delineating the exactness with which a number is represented, bifurcates typically into single, double, half and in recent years there have been a number of other precisions that have emerged to better support machine learning tasks efficiently on the underlying hardware.</p>
<p><strong>Double Precision (Float64):</strong> Allocating 64 bits, double precision (e.g., 3.141592653589793) provides heightened accuracy, albeit demanding augmented memory and computational resources. In scientific computations, where precision is paramount, variables like π might be represented with Float64.</p>
<p><strong>Single Precision (Float32):</strong> With 32 bits at its disposal, single precision (e.g., 3.1415927) strikes a balance between numerical accuracy and memory conservation. In ML, Float32 might be employed to store weights during training to maintain a reasonable level of precision.</p>
<p><strong>Half Precision (Float16):</strong> Constrained to 16 bits, half precision (e.g., 3.14) curtails memory usage and can expedite computations, albeit sacrificing numerical accuracy and range. In ML, especially during inference on resource-constrained devices, Float16 might be utilized to reduce the model’s memory footprint.</p>
<p><strong>Bfloat16:</strong> Brain Floating-Point Format or Bfloat16, also employs 16 bits but allocates them differently compared to FP16: 1 bit for the sign, 8 bits for the exponent (resulting in the same number range as in float32), and 7 bits for the fraction. This format, developed by Google, prioritizes a larger exponent range over precision, making it particularly useful in deep learning applications where the dynamic range is crucial.</p>
<p><a href="#fig-3float" class="quarto-xref">Figure <span class="quarto-unresolved-ref">fig-3float</span></a> illustrates the differences between the three floating-point formats: Float32, Float16, and BFloat16.</p>
<div id="fig-3float" class="quarto-float quarto-figure quarto-figure-center">
<figure class="quarto-float quarto-float-fig">
<div aria-describedby="fig-3float-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/jpg/modeloptimization_3float_types.jpeg" class="img-fluid" style="width:90.0%" />
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-3float-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure 9.13: Three floating-point formats.
</figcaption>
</figure>
</div>
<p><strong>Integer:</strong> Integer representations are made using 8, 4, and 2 bits. They are often used during the inference phase of neural networks, where the weights and activations of the model are quantized to these lower precisions. Integer representations are deterministic and offer significant speed and memory advantages over floating-point representations. For many inference tasks, especially on edge devices, the slight loss in accuracy due to quantization is often acceptable given the efficiency gains. An extreme form of integer numerics is for binary neural networks (BNNs), where weights and activations are constrained to one of two values: either +1 or -1.</p>
<p>You may refer back to <a href="#sec-numerical-formats" class="quarto-xref"><span class="quarto-unresolved-ref">sec-numerical-formats</span></a> for a table comparison between the trade-offs of different numeric types.</p>
</section>
<section id="numeric-encoding-and-storage" class="level4">
<h4>Numeric Encoding and Storage</h4>
<p>Numeric encoding, the art of transmuting numbers into a computer-amenable format, and their subsequent storage are critical for computational efficiency. For instance, floating-point numbers might be encoded using the IEEE 754 standard, which apportions bits among sign, exponent, and fraction components, thereby enabling the representation of a vast array of values with a single format. There are a few new IEEE floating point formats that have been defined specifically for AI workloads:</p>
<ul>
<li><a href="https://cloud.google.com/tpu/docs/bfloat16">bfloat16</a>- A 16-bit floating point format introduced by Google. It has 8 bits for exponent, 7 bits for mantissa and 1 bit for sign. Offers a reduced precision compromise between 32-bit float and 8-bit integers. Supported on many hardware accelerators.</li>
<li><a href="https://ieeexplore.ieee.org/document/9399648">posit</a> - A configurable format that can represent different levels of precision based on exponent bits. It is more efficient than IEEE 754 binary floats. Has adjustable dynamic range and precision.</li>
<li><a href="https://arxiv.org/abs/1711.02213">Flexpoint</a> - A format introduced by Intel that can dynamically adjust precision across layers or within a layer. Allows tuning precision to accuracy and hardware requirements.</li>
<li><a href="https://developer.arm.com/documentation/ddi0596/2020-12/SIMD-FP-Instructions/BFMLALB--BFMLALT--vector---BFloat16-floating-point-widening-multiply-add-long--vector--">BF16ALT</a> - A proposed 16-bit format by ARM as an alternative to bfloat16. Uses additional bit in exponent to prevent overflow/underflow.</li>
<li><a href="https://blogs.nvidia.com/blog/2020/05/14/tensorfloat-32-precision-format/">TF32</a> - Introduced by Nvidia for Ampere GPUs. Uses 10 bits for exponent instead of 8 bits like FP32. Improves model training performance while maintaining accuracy.</li>
<li><a href="https://arxiv.org/abs/2209.05433">FP8</a> - 8-bit floating point format that keeps 6 bits for mantissa and 2 bits for exponent. Enables better dynamic range than integers.</li>
</ul>
<p>The key goals of these new formats are to provide lower precision alternatives to 32-bit floats for better computational efficiency and performance on AI accelerators while maintaining model accuracy. They offer different tradeoffs in terms of precision, range and implementation cost/complexity.</p>
</section>
</section>
<section id="efficiency-benefits" class="level3" data-number="9.3.2">
<h3 data-number="9.3.2"><span class="header-section-number">9.3.2</span> Efficiency Benefits</h3>
<p>As you learned in <a href="#sec-efficiency-benefits" class="quarto-xref"><span class="quarto-unresolved-ref">sec-efficiency-benefits</span></a>, numerical efficiency matters for machine learning workloads for a number of reasons. Efficient numerics is not just about reducing the bit-width of numbers but understanding the trade-offs between accuracy and efficiency. As machine learning models become more pervasive, especially in real-world, resource-constrained environments, the focus on efficient numerics will continue to grow. By thoughtfully selecting and leveraging the appropriate numeric precision, one can achieve robust model performance while optimizing for speed, memory, and energy.</p>
</section>
<section id="numeric-representation-nuances" class="level3" data-number="9.3.3">
<h3 data-number="9.3.3"><span class="header-section-number">9.3.3</span> Numeric Representation Nuances</h3>
<p>There are a number of nuances with numerical representations for ML that require us to have an understanding of both the theoretical and practical aspects of numerics representation, as well as a keen awareness of the specific requirements and constraints of the application domain.</p>
<section id="memory-usage" class="level4">
<h4>Memory Usage</h4>
<p>The memory footprint of ML models, particularly those of considerable complexity and depth, can be substantial, thereby posing a significant challenge in both training and deployment phases. For instance, a deep neural network with 100 million parameters, represented using Float32 (32 bits or 4 bytes per parameter), would necessitate approximately 400 MB of memory just for storing the model weights. This does not account for additional memory requirements during training for storing gradients, optimizer states, and forward pass caches, which can further amplify the memory usage, potentially straining the resources on certain hardware, especially edge devices with limited memory capacity.</p>
<p>The choice of numeric representation further impacts memory usage and computational efficiency. For example, using Float64 for model weights would double the memory requirements compared to Float32, and could potentially increase computational time as well. For a weight matrix with dimensions [1000, 1000], Float64 would consume approximately 8MB of memory, while Float32 would reduce this to about 4MB. Thus, selecting an appropriate numeric format is crucial for optimizing both memory and computational efficiency.</p>
</section>
<section id="computational-complexity" class="level4">
<h4>Computational Complexity</h4>
<p>Numerical precision directly impacts computational complexity, influencing the time and resources required to perform arithmetic operations. For example, operations using Float64 generally consume more computational resources than their Float32 or Float16 counterparts (see <a href="#fig-quantized-energy" class="quarto-xref">Figure <span class="quarto-unresolved-ref">fig-quantized-energy</span></a>). In the realm of ML, where models might need to process millions of operations (e.g., multiplications and additions in matrix operations during forward and backward passes), even minor differences in the computational complexity per operation can aggregate into a substantial impact on training and inference times. As shown in <a href="#fig-models-speeds" class="quarto-xref">Figure <span class="quarto-unresolved-ref">fig-models-speeds</span></a>, quantized models can be many times faster than their unquantized versions.</p>
<div id="fig-quantized-energy" class="quarto-float quarto-figure quarto-figure-center">
<figure class="quarto-float quarto-float-fig">
<div aria-describedby="fig-quantized-energy-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/efficientnumerics_horowitz.png" class="img-fluid" />
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-quantized-energy-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure 9.14: Energy use by quantized operations. Source: Mark Horowitz, Stanford University.
</figcaption>
</figure>
</div>
<div id="fig-models-speeds" class="quarto-float quarto-figure quarto-figure-center">
<figure class="quarto-float quarto-float-fig">
<div aria-describedby="fig-models-speeds-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/efficientnumerics_int8vsfloat.png" class="img-fluid" />
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-models-speeds-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure 9.15: Speed of three different models in normal and quantized form.
</figcaption>
</figure>
</div>
<p>In addition to pure runtimes, there is also a concern over energy efficiency. Not all numerical computations are created equal from the underlying hardware standpoint. Some numerical operations are more energy efficient than others. For example, <a href="#fig-operations-energy-comparison" class="quarto-xref">Figure <span class="quarto-unresolved-ref">fig-operations-energy-comparison</span></a> below shows that integer addition is much more energy efficient than integer multiplication.</p>
<div id="fig-operations-energy-comparison" class="quarto-float quarto-figure quarto-figure-center">
<figure class="quarto-float quarto-float-fig">
<div aria-describedby="fig-operations-energy-comparison-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/efficientnumerics_100x.png" class="img-fluid" />
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-operations-energy-comparison-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure 9.16: Energy use by quantized operations. Source: <span class="citation" data-cites="isscc2014computings">Isscc (<a href="#ref-isscc2014computings" role="doc-biblioref">2014</a>)</span>.
</figcaption>
</figure>
</div>
</section>
<section id="hardware-compatibility" class="level4">
<h4>Hardware Compatibility</h4>
<p>Ensuring compatibility and optimized performance across diverse hardware platforms is another challenge in numerics representation. Different hardware, such as CPUs, GPUs, TPUs, and FPGAs, have varying capabilities and optimizations for handling different numeric precisions. For example, certain GPUs might be optimized for Float32 computations, while others might provide accelerations for Float16. Developing and optimizing ML models that can leverage the specific numerical capabilities of different hardware, while ensuring that the model maintains its accuracy and robustness, requires careful consideration and potentially additional development and testing efforts.</p>
</section>
<section id="precision-and-accuracy-trade-offs" class="level4">
<h4>Precision and Accuracy Trade-offs</h4>
<p>The trade-off between numerical precision and model accuracy is a nuanced challenge in numerics representation. Utilizing lower-precision numerics, such as Float16, might conserve memory and expedite computations but can also introduce issues like quantization error and reduced numerical range. For instance, training a model with Float16 might introduce challenges in representing very small gradient values, potentially impacting the convergence and stability of the training process. Furthermore, in certain applications, such as scientific simulations or financial computations, where high precision is paramount, the use of lower-precision numerics might not be permissible due to the risk of accruing significant errors.</p>
</section>
<section id="trade-off-examples" class="level4">
<h4>Trade-off Examples</h4>
<p>To understand and appreciate the nuances, let’s consider some use case examples. Through these we will realize that the choice of numeric representation is not merely a technical decision but a strategic one, influencing the model’s predictive acumen, its computational demands, and its deployability across diverse computational environments. In this section we will look at a couple of examples to better understand the trade-offs with numerics and how they tie to the real world.</p>
<section id="autonomous-vehicles" class="level5">
<h5>Autonomous Vehicles</h5>
<p>In the domain of autonomous vehicles, ML models are employed to interpret sensor data and make real-time decisions. The models must process high-dimensional data from various sensors (e.g., LiDAR, cameras, radar) and execute numerous computations within a constrained time frame to ensure safe and responsive vehicle operation. So the trade-offs here would include:</p>
<ul>
<li>Memory Usage: Storing and processing high-resolution sensor data, especially in floating-point formats, can consume substantial memory.</li>
<li>Computational Complexity: Real-time processing demands efficient computations, where higher-precision numerics might impede the timely execution of control actions.</li>
</ul>
</section>
<section id="mobile-health-applications" class="level5">
<h5>Mobile Health Applications</h5>
<p>Mobile health applications often use ML models for tasks like activity recognition, health monitoring, or predictive analytics, operating within the resource-constrained environment of mobile devices. The trade-offs here would include:</p>
<ul>
<li>Precision and Accuracy Trade-offs: Employing lower-precision numerics to conserve resources might impact the accuracy of health predictions or anomaly detections, which could have significant implications for user health and safety.</li>
<li>Hardware Compatibility: Models need to be optimized for diverse mobile hardware, ensuring efficient operation across a wide range of devices with varying numerical computation capabilities.</li>
</ul>
</section>
<section id="high-frequency-trading-hft-systems" class="level5">
<h5>High-Frequency Trading (HFT) Systems</h5>
<p>HFT systems leverage ML models to make rapid trading decisions based on real-time market data. These systems demand extremely low-latency responses to capitalize on short-lived trading opportunities.</p>
<ul>
<li>Computational Complexity: The models must process and analyze vast streams of market data with minimal latency, where even slight delays, potentially introduced by higher-precision numerics, can result in missed opportunities.</li>
<li>Precision and Accuracy Trade-offs: Financial computations often demand high numerical precision to ensure accurate pricing and risk assessments, posing challenges in balancing computational efficiency and numerical accuracy.</li>
</ul>
</section>
<section id="edge-based-surveillance-systems" class="level5">
<h5>Edge-Based Surveillance Systems</h5>
<p>Surveillance systems deployed on edge devices, like security cameras, use ML models for tasks like object detection, activity recognition, and anomaly detection, often operating under stringent resource constraints.</p>
<ul>
<li>Memory Usage: Storing pre-trained models and processing video feeds in real-time demands efficient memory usage, which can be challenging with high-precision numerics.</li>
<li>Hardware Compatibility: Ensuring that models can operate efficiently on edge devices with varying hardware capabilities and optimizations for different numeric precisions is crucial for widespread deployment.</li>
</ul>
</section>
<section id="scientific-simulations" class="level5">
<h5>Scientific Simulations</h5>
<p>ML models are increasingly being utilized in scientific simulations, such as climate modeling or molecular dynamics simulations, to improve predictive capabilities and reduce computational demands.</p>
<ul>
<li>Precision and Accuracy Trade-offs: Scientific simulations often require high numerical precision to ensure accurate and reliable results, which can conflict with the desire to reduce computational demands via lower-precision numerics.</li>
<li>Computational Complexity: The models must manage and process complex, high-dimensional simulation data efficiently to ensure timely results and enable large-scale or long-duration simulations.</li>
</ul>
<p>These examples illustrate diverse scenarios where the challenges of numerics representation in ML models are prominently manifested. Each system presents a unique set of requirements and constraints, necessitating tailored strategies and solutions to navigate the challenges of memory usage, computational complexity, precision-accuracy trade-offs, and hardware compatibility.</p>
</section>
</section>
</section>
<section id="sec-quant" class="level3" data-number="9.3.4">
<h3 data-number="9.3.4"><span class="header-section-number">9.3.4</span> Quantization</h3>
<p>Quantization is prevalent in various scientific and technological domains, and it essentially involves the mapping or constraining of a continuous set or range into a discrete counterpart to minimize the number of bits required.</p>
<section id="initial-breakdown" class="level4">
<h4>Initial Breakdown</h4>
<p>We begin our foray into quantization with a brief analysis of one important use for quantization.</p>
<p>In signal processing, the continuous sine wave (shown in <a href="#fig-sine-wave" class="quarto-xref">Figure <span class="quarto-unresolved-ref">fig-sine-wave</span></a>) can be quantized into discrete values through a process known as sampling. This is a fundamental concept in digital signal processing and is crucial for converting analog signals (like the continuous sine wave) into a digital form that can be processed by computers. The sine wave is a prevalent example due to its periodic and smooth nature, making it a useful tool for explaining concepts like frequency, amplitude, phase, and, of course, quantization.</p>
<div id="fig-sine-wave" class="quarto-float quarto-figure quarto-figure-center">
<figure class="quarto-float quarto-float-fig">
<div aria-describedby="fig-sine-wave-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/efficientnumerics_sinewave.png" class="img-fluid" />
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-sine-wave-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure 9.17: Sine Wave.
</figcaption>
</figure>
</div>
<p>In the quantized version shown in <a href="#fig-quantized-sine-wave" class="quarto-xref">Figure <span class="quarto-unresolved-ref">fig-quantized-sine-wave</span></a>, the continuous sine wave (<a href="#fig-sine-wave" class="quarto-xref">Figure <span class="quarto-unresolved-ref">fig-sine-wave</span></a>) is sampled at regular intervals (in this case, every <span class="math inline">\(\frac{\pi}{4}\)</span> radians), and only these sampled values are represented in the digital version of the signal. The step-wise lines between the points show one way to represent the quantized signal in a piecewise-constant form. This is a simplified example of how analog-to-digital conversion works, where a continuous signal is mapped to a discrete set of values, enabling it to be represented and processed digitally.</p>
<div id="fig-quantized-sine-wave" class="quarto-float quarto-figure quarto-figure-center">
<figure class="quarto-float quarto-float-fig">
<div aria-describedby="fig-quantized-sine-wave-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/efficientnumerics_quantizedsine.png" class="img-fluid" />
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-quantized-sine-wave-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure 9.18: Quantized Sine Wave.
</figcaption>
</figure>
</div>
<p>Returning to the context of Machine Learning (ML), quantization refers to the process of constraining the possible values that numerical parameters (such as weights and biases) can take to a discrete set, thereby reducing the precision of the parameters and consequently, the model’s memory footprint. When properly implemented, quantization can reduce model size by up to 4x and improve inference latency and throughput by up to 2-3x. <a href="#fig-quantized-models-size" class="quarto-xref">Figure <span class="quarto-unresolved-ref">fig-quantized-models-size</span></a> illustrates the impact that quantization has on different models’ sizes: for example, an Image Classification model like ResNet-v2 can be compressed from 180MB down to 45MB with 8-bit quantization. There is typically less than 1% loss in model accuracy from well tuned quantization. Accuracy can often be recovered by re-training the quantized model with quantization-aware training techniques. Therefore, this technique has emerged to be very important in deploying ML models to resource-constrained environments, such as mobile devices, IoT devices, and edge computing platforms, where computational resources (memory and processing power) are limited.</p>
<div id="fig-quantized-models-size" class="quarto-float quarto-figure quarto-figure-center">
<figure class="quarto-float quarto-float-fig">
<div aria-describedby="fig-quantized-models-size-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/efficientnumerics_reducedmodelsize.png" class="img-fluid" />
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-quantized-models-size-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure 9.19: Effect of quantization on model sizes. Source: HarvardX.
</figcaption>
</figure>
</div>
<p>There are several dimensions to quantization such as uniformity, stochasticity (or determinism), symmetry, granularity (across layers/channels/groups or even within channels), range calibration considerations (static vs dynamic), and fine-tuning methods (QAT, PTQ, ZSQ). We examine these below.</p>
</section>
</section>
<section id="types-1" class="level3" data-number="9.3.5">
<h3 data-number="9.3.5"><span class="header-section-number">9.3.5</span> Types</h3>
<section id="uniform-quantization" class="level4">
<h4>Uniform Quantization</h4>
<p>Uniform quantization involves mapping continuous or high-precision values to a lower-precision representation using a uniform scale. This means that the interval between each possible quantized value is consistent. For example, if weights of a neural network layer are quantized to 8-bit integers (values between 0 and 255), a weight with a floating-point value of 0.56 might be mapped to an integer value of 143, assuming a linear mapping between the original and quantized scales. Due to its use of integer or fixed-point math pipelines, this form of quantization allows computation on the quantized domain without the need to dequantize beforehand.</p>
<p>The process for implementing uniform quantization starts with choosing a range of real numbers to be quantized. The next step is to select a quantization function and map the real values to the integers representable by the bit-width of the quantized representation. For instance, a popular choice for a quantization function is:</p>
<p><span class="math display">\[
Q(r)=Int(r/S) - Z
\]</span></p>
<p>where <span class="math inline">\(Q\)</span> is the quantization operator, <span class="math inline">\(r\)</span> is a real valued input (in our case, an activation or weight), <span class="math inline">\(S\)</span> is a real valued scaling factor, and <span class="math inline">\(Z\)</span> is an integer zero point. The <code>Int</code> function maps a real value to an integer value through a rounding operation. Through this function, we have effectively mapped real values <span class="math inline">\(r\)</span> to some integer values, resulting in quantized levels which are uniformly spaced.</p>
<p>When the need arises for practitioners to retrieve the original higher precision values, real values <span class="math inline">\(r\)</span> can be recovered from quantized values through an operation known as <strong>dequantization</strong>. In the example above, this would mean performing the following operation on our quantized value:</p>
<p><span class="math display">\[
\bar{r} = S(Q(r) + Z)
\]</span></p>
<p>As discussed, some precision in the real value is lost by quantization. In this case, the recovered value <span class="math inline">\(\bar{r}\)</span> will not exactly match <span class="math inline">\(r\)</span> due to the rounding operation. This is an important tradeoff to note; however, in many successful uses of quantization, the loss of precision can be negligible and the test accuracy remains high. Despite this, uniform quantization continues to be the current de-facto choice due to its simplicity and efficient mapping to hardware.</p>
</section>
<section id="non-uniform-quantization" class="level4">
<h4>Non-uniform Quantization</h4>
<p>Non-uniform quantization, on the other hand, does not maintain a consistent interval between quantized values. This approach might be used to allocate more possible discrete values in regions where the parameter values are more densely populated, thereby preserving more detail where it is most needed. For instance, in bell-shaped distributions of weights with long tails, a set of weights in a model predominantly lies within a certain range; thus, more quantization levels might be allocated to that range to preserve finer details, enabling us to better capture information. However, one major weakness of non-uniform quantization is that it requires dequantization before higher precision computations due to its non-uniformity, restricting its ability to accelerate computation compared to uniform quantization.</p>
<p>Typically, a rule-based non-uniform quantization uses a logarithmic distribution of exponentially increasing steps and levels as opposed to linearly. Another popular branch lies in binary-code-based quantization where real number vectors are quantized into binary vectors with a scaling factor. Notably, there is no closed form solution for minimizing errors between the real value and non-uniformly quantized value, so most quantizations in this field rely on heuristic solutions. For instance, <a href="https://arxiv.org/abs/1802.00150">recent work</a> by <span class="citation" data-cites="xu2018alternating">Xu et al. (<a href="#ref-xu2018alternating" role="doc-biblioref">2018</a>)</span> formulates non-uniform quantization as an optimization problem where the quantization steps/levels in quantizer <span class="math inline">\(Q\)</span> are adjusted to minimize the difference between the original tensor and quantized counterpart.</p>
<p><span class="math display">\[
\min_Q ||Q(r)-r||^2
\]</span></p>
<p>Furthermore, learnable quantizers can be jointly trained with model parameters, and the quantization steps/levels are generally trained with iterative optimization or gradient descent. Additionally, clustering has been used to alleviate information loss from quantization. While capable of capturing higher levels of detail, non-uniform quantization schemes can be difficult to deploy efficiently on general computation hardware, making it less-preferred to methods which use uniform quantization.</p>
<div id="fig-quantization-uniformity" class="quarto-float quarto-figure quarto-figure-center">
<figure class="quarto-float quarto-float-fig">
<div aria-describedby="fig-quantization-uniformity-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/efficientnumerics_uniformnonuniform.png" class="img-fluid" />
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-quantization-uniformity-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure 9.20: Quantization uniformity. Source: <span class="citation" data-cites="gholami2021survey">Gholami et al. (<a href="#ref-gholami2021survey" role="doc-biblioref">2021</a>)</span>.
</figcaption>
</figure>
</div>
</section>
<section id="stochastic-quantization" class="level4">
<h4>Stochastic Quantization</h4>
<p>Unlike the two previous approaches which generate deterministic mappings, there is some work exploring the idea of stochastic quantization for quantization-aware training and reduced precision training. This approach maps floating numbers up or down with a probability associated to the magnitude of the weight update. The hope generated by high level intuition is that such a probabilistic approach may allow a neural network to explore more, as compared to deterministic quantization. Supposedly, enabling a stochastic rounding may allow neural networks to escape local optimums, thereby updating its parameters. Below are two example stochastic mapping functions:</p>
<p><img src="images/png/efficientnumerics_nonuniform.png" class="img-fluid" /></p>
<div id="fig-integer-binary-quantization" class="quarto-float quarto-figure quarto-figure-center">
<figure class="quarto-float quarto-float-fig">
<div aria-describedby="fig-integer-binary-quantization-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/efficientnumerics_binary.png" class="img-fluid" />
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-integer-binary-quantization-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure 9.21: Integer vs Binary quantization functions.
</figcaption>
</figure>
</div>
</section>
<section id="zero-shot-quantization" class="level4">
<h4>Zero Shot Quantization</h4>
<p>Zero-shot quantization refers to the process of converting a full-precision deep learning model directly into a low-precision, quantized model without the need for any retraining or fine-tuning on the quantized model. The primary advantage of this approach is its efficiency, as it eliminates the often time-consuming and resource-intensive process of retraining a model post-quantization. By leveraging techniques that anticipate and minimize quantization errors, zero-shot quantization maintains the model’s original accuracy even after reducing its numerical precision. It is particularly useful for Machine Learning as a Service (MLaaS) providers aiming to expedite the deployment of their customer’s workloads without having to access their datasets.</p>
</section>
</section>
<section id="calibration" class="level3" data-number="9.3.6">
<h3 data-number="9.3.6"><span class="header-section-number">9.3.6</span> Calibration</h3>
<p>Calibration is the process of selecting the most effective clipping range [<span class="math inline">\(\alpha\)</span>, <span class="math inline">\(\beta\)</span>] for weights and activations to be quantized to. For example, consider quantizing activations that originally have a floating-point range between -6 and 6 to 8-bit integers. If you just take the minimum and maximum possible 8-bit integer values (-128 to 127) as your quantization range, it might not be the most effective. Instead, calibration would involve passing a representative dataset then use this observed range for quantization.</p>
<p>There are many calibration methods but a few commonly used include:</p>
<ul>
<li>Max: Use the maximum absolute value seen during calibration. However, this method is susceptible to outlier data. Notice how in <a href="#fig-resnet-activations-histogram" class="quarto-xref">Figure <span class="quarto-unresolved-ref">fig-resnet-activations-histogram</span></a>, we have an outlier cluster around 2.1, while the rest are clustered around smaller values.</li>
<li>Entropy: Use KL divergence to minimize information loss between the original floating-point values and values that could be represented by the quantized format. This is the default method used by TensorRT.</li>
<li>Percentile: Set the range to a percentile of the distribution of absolute values seen during calibration. For example, 99% calibration would clip 1% of the largest magnitude values.</li>
</ul>
<div id="fig-resnet-activations-histogram" class="quarto-float quarto-figure quarto-figure-center">
<figure class="quarto-float quarto-float-fig">
<div aria-describedby="fig-resnet-activations-histogram-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/efficientnumerics_calibrationcopy.png" class="img-fluid" />
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-resnet-activations-histogram-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure 9.22: Input activations to layer 3 in ResNet50. Source: @<span class="citation" data-cites="wu2020integer">Wu, Judd, and Isaev (<a href="#ref-wu2020integer" role="doc-biblioref">2020</a>)</span>.
</figcaption>
</figure>
</div>
<p>Importantly, the quality of calibration can make a difference between a quantized model that retains most of its accuracy and one that degrades significantly. Hence, it’s an essential step in the quantization process. When choosing a calibration range, there are two types: symmetric and asymmetric.</p>
<section id="symmetric-quantization" class="level4">
<h4>Symmetric Quantization</h4>
<p>Symmetric quantization maps real values to a symmetrical clipping range centered around 0. This involves choosing a range [<span class="math inline">\(\alpha\)</span>, <span class="math inline">\(\beta\)</span>] where <span class="math inline">\(\alpha = -\beta\)</span>. For example, one symmetrical range would be based on the min/max values of the real values such that:</p>
<p><span class="math display">\[
\alpha = \beta = max(abs(r_{max}), abs(r_{min}))
\]</span></p>
<p>Symmetric clipping ranges are the most widely adopted in practice as they have the advantage of easier implementation. In particular, the mapping of zero to zero in the clipping range (sometimes called “zeroing out of the zero point”) can lead to reduction in computational cost during inference <a href="https://arxiv.org/abs/2004.09602"><span class="citation" data-cites="wu2020integer">(<span>Wu, Judd, and Isaev 2020</span>)</span></a>.</p>
</section>
<section id="asymmetric-quantization" class="level4">
<h4>Asymmetric Quantization</h4>
<p>Asymmetric quantization maps real values to an asymmetrical clipping range that isn’t necessarily centered around 0, as shown in <a href="#fig-quantization-symmetry" class="quarto-xref">Figure <span class="quarto-unresolved-ref">fig-quantization-symmetry</span></a> on the right. It involves choosing a range [<span class="math inline">\(\alpha\)</span>, <span class="math inline">\(\beta\)</span>] where <span class="math inline">\(\alpha \neq -\beta\)</span>. For example, selecting a range based on the minimum and maximum real values, or where <span class="math inline">\(\alpha = r_{min}\)</span> and <span class="math inline">\(\beta = r_{max}\)</span>, creates an asymmetric range. Typically, asymmetric quantization produces tighter clipping ranges compared to symmetric quantization, which is important when target weights and activations are imbalanced, e.g., the activation after the ReLU always has non-negative values. Despite producing tighter clipping ranges, asymmetric quantization is less preferred to symmetric quantization as it doesn’t always zero out the real value zero.</p>
<div id="fig-quantization-symmetry" class="quarto-float quarto-figure quarto-figure-center">
<figure class="quarto-float quarto-float-fig">
<div aria-describedby="fig-quantization-symmetry-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/efficientnumerics_symmetry.png" class="img-fluid" />
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-quantization-symmetry-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure 9.23: Quantization (a)symmetry. Source: <span class="citation" data-cites="gholami2021survey">Gholami et al. (<a href="#ref-gholami2021survey" role="doc-biblioref">2021</a>)</span>.
</figcaption>
</figure>
</div>
</section>
<section id="granularity" class="level4">
<h4>Granularity</h4>
<p>Upon deciding the type of clipping range, it is essential to tighten the range to allow a model to retain as much of its accuracy as possible. We’ll be taking a look at convolutional neural networks as our way of exploring methods that fine tune the granularity of clipping ranges for quantization. The input activation of a layer in our CNN undergoes convolution with multiple convolutional filters. Every convolutional filter can possess a unique range of values. Notice how in <a href="#fig-quantization-granularity" class="quarto-xref">Figure <span class="quarto-unresolved-ref">fig-quantization-granularity</span></a>, the range for Filter 1 is much smaller than that for Filter 3. Consequently, one distinguishing feature of quantization approaches is the precision with which the clipping range [α,β] is determined for the weights.</p>
<div id="fig-quantization-granularity" class="quarto-float quarto-figure quarto-figure-center">
<figure class="quarto-float quarto-float-fig">
<div aria-describedby="fig-quantization-granularity-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/efficientnumerics_granularity.png" class="img-fluid" />
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-quantization-granularity-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure 9.24: Quantization granularity: variable ranges. Source: <span class="citation" data-cites="gholami2021survey">Gholami et al. (<a href="#ref-gholami2021survey" role="doc-biblioref">2021</a>)</span>.
</figcaption>
</figure>
</div>
<ol type="1">
<li><strong>Layerwise Quantization:</strong> This approach determines the clipping range by considering all of the weights in the convolutional filters of a layer. Then, the same clipping range is used for all convolutional filters. It’s the simplest to implement, and, as such, it often results in sub-optimal accuracy due the wide variety of differing ranges between filters. For example, a convolutional kernel with a narrower range of parameters loses its quantization resolution due to another kernel in the same layer having a wider range.</li>
<li><strong>Groupwise Quantization:</strong> This approach groups different channels inside a layer to calculate the clipping range. This method can be helpful when the distribution of parameters across a single convolution/activation varies a lot. In practice, this method was useful in Q-BERT <span class="citation" data-cites="sheng2019qbert">(<a href="#ref-sheng2019qbert" role="doc-biblioref">Shen et al. 2020</a>)</span> for quantizing Transformer <span class="citation" data-cites="vaswani2017attention">(<a href="#ref-vaswani2017attention" role="doc-biblioref">Vaswani et al. 2017</a>)</span> models that consist of fully-connected attention layers. The downside with this approach comes with the extra cost of accounting for different scaling factors.</li>
<li><strong>Channelwise Quantization:</strong> This popular method uses a fixed range for each convolutional filter that is independent of other channels. Because each channel is assigned a dedicated scaling factor, this method ensures a higher quantization resolution and often results in higher accuracy.</li>
<li><strong>Sub-channelwise Quantization:</strong> Taking channelwise quantization to the extreme, this method determines the clipping range with respect to any groups of parameters in a convolution or fully-connected layer. It may result in considerable overhead since different scaling factors need to be taken into account when processing a single convolution or fully-connected layer.</li>
</ol>
<p>Of these, channelwise quantization is the current standard used for quantizing convolutional kernels, since it enables the adjustment of clipping ranges for each individual kernel with negligible overhead.</p>
</section>
<section id="static-and-dynamic-quantization" class="level4">
<h4>Static and Dynamic Quantization</h4>
<p>After determining the type and granularity of the clipping range, practitioners must decide when ranges are determined in their range calibration algorithms. There are two approaches to quantizing activations: static quantization and dynamic quantization.</p>
<p>Static quantization is the most frequently used approach. In this, the clipping range is pre-calculated and static during inference. It does not add any computational overhead, but, consequently, results in lower accuracy as compared to dynamic quantization. A popular method of implementing this is to run a series of calibration inputs to compute the typical range of activations <span class="citation" data-cites="jacob2018quantization yao2021hawq">(<a href="#ref-jacob2018quantization" role="doc-biblioref">Jacob et al. 2018</a>; <a href="#ref-yao2021hawq" role="doc-biblioref">Yao et al. 2021</a>)</span>.</p>
<p>Dynamic quantization is an alternative approach which dynamically calculates the range for each activation map during runtime. The approach requires real-time computations which might have a very high overhead. By doing this, dynamic quantization often achieves the highest accuracy as the range is calculated specifically for each input.</p>
<p>Between the two, calculating the range dynamically usually is very costly, so most practitioners will often use static quantization instead.</p>
</section>
</section>
<section id="techniques" class="level3" data-number="9.3.7">
<h3 data-number="9.3.7"><span class="header-section-number">9.3.7</span> Techniques</h3>
<p>When optimizing machine learning models for deployment, various quantization techniques are used to balance model efficiency, accuracy, and adaptability. Each method—post-training quantization, quantization-aware training, and dynamic quantization–offers unique advantages and trade-offs, impacting factors such as implementation complexity, computational overhead, and performance optimization.</p>
<p><a href="#tbl-quantization_methods" class="quarto-xref">Table <span class="quarto-unresolved-ref">tbl-quantization_methods</span></a> provides an overview of these quantization methods, highlighting their respective strengths, limitations, and trade-offs. We will delve deeper into each of these methods because they are widely deployed and used across all ML systems of wildly different scales.</p>
<div id="tbl-quantization_methods" class="striped hover quarto-float quarto-figure quarto-figure-center">
<figure class="quarto-float quarto-float-tbl">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-quantization_methods-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table 9.2: Comparison of post-training quantization, quantization-aware training, and dynamic quantization.
</figcaption>
<div aria-describedby="tbl-quantization_methods-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="table-striped table-hover caption-top" style="width:98%;">
<colgroup>
<col style="width: 24%" />
<col style="width: 24%" />
<col style="width: 24%" />
<col style="width: 24%" />
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Aspect</th>
<th style="text-align: left;">Post Training Quantization</th>
<th style="text-align: left;">Quantization-Aware Training</th>
<th style="text-align: left;">Dynamic Quantization</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><strong>Pros</strong></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr class="even">
<td style="text-align: left;">Simplicity</td>
<td style="text-align: left;">✓</td>
<td style="text-align: left;">✗</td>
<td style="text-align: left;">✗</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Accuracy Preservation</td>
<td style="text-align: left;">✗</td>
<td style="text-align: left;">✓</td>
<td style="text-align: left;">✓</td>
</tr>
<tr class="even">
<td style="text-align: left;">Adaptability</td>
<td style="text-align: left;">✗</td>
<td style="text-align: left;">✗</td>
<td style="text-align: left;">✓</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Optimized Performance</td>
<td style="text-align: left;">✗</td>
<td style="text-align: left;">✓</td>
<td style="text-align: left;">Potentially</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Cons</strong></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr class="odd">
<td style="text-align: left;">Accuracy Degradation</td>
<td style="text-align: left;">✓</td>
<td style="text-align: left;">✗</td>
<td style="text-align: left;">Potentially</td>
</tr>
<tr class="even">
<td style="text-align: left;">Computational Overhead</td>
<td style="text-align: left;">✗</td>
<td style="text-align: left;">✓</td>
<td style="text-align: left;">✓</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Implementation Complexity</td>
<td style="text-align: left;">✗</td>
<td style="text-align: left;">✓</td>
<td style="text-align: left;">✓</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Tradeoffs</strong></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr class="odd">
<td style="text-align: left;">Speed vs. Accuracy</td>
<td style="text-align: left;">✓</td>
<td style="text-align: left;">✗</td>
<td style="text-align: left;">✗</td>
</tr>
<tr class="even">
<td style="text-align: left;">Accuracy vs. Cost</td>
<td style="text-align: left;">✗</td>
<td style="text-align: left;">✓</td>
<td style="text-align: left;">✗</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Adaptability vs. Overhead</td>
<td style="text-align: left;">✗</td>
<td style="text-align: left;">✗</td>
<td style="text-align: left;">✓</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p><strong>Post Training Quantization:</strong> Post-training quantization (PTQ) is a quantization technique where the model is quantized after it has been trained. The model is trained in floating point and then weights and activations are quantized as a post-processing step. This is the simplest approach and does not require access to the training data. Unlike Quantization-Aware Training (QAT), PTQ sets weight and activation quantization parameters directly, making it low-overhead and suitable for limited or unlabeled data situations. However, not readjusting the weights after quantizing, especially in low-precision quantization can lead to very different behavior and thus lower accuracy. To tackle this, techniques like bias correction, equalizing weight ranges, and adaptive rounding methods have been developed. PTQ can also be applied in zero-shot scenarios, where no training or testing data are available. This method has been made even more efficient to benefit compute- and memory- intensive large language models. Recently, SmoothQuant, a training-free, accuracy-preserving, and general-purpose PTQ solution which enables 8-bit weight, 8-bit activation quantization for LLMs, has been developed, demonstrating up to 1.56x speedup and 2x memory reduction for LLMs with negligible loss in accuracy <a href="https://arxiv.org/abs/2211.10438"><span class="citation" data-cites="xiao2022smoothquant">(<span>Xiao et al. 2022</span>)</span></a>.</p>
<p>In PTQ, a pretrained model undergoes a calibration process, as shown in <a href="#fig-PTQ-diagram" class="quarto-xref">Figure <span class="quarto-unresolved-ref">fig-PTQ-diagram</span></a>. Calibration involves using a separate dataset known as calibration data, a specific subset of the training data reserved for quantization to help find the appropriate clipping ranges and scaling factors.</p>
<div id="fig-PTQ-diagram" class="quarto-float quarto-figure quarto-figure-center">
<figure class="quarto-float quarto-float-fig">
<div aria-describedby="fig-PTQ-diagram-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/efficientnumerics_PTQ.png" class="img-fluid" />
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-PTQ-diagram-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure 9.25: Post-Training Quantization and calibration. Source: <span class="citation" data-cites="gholami2021survey">Gholami et al. (<a href="#ref-gholami2021survey" role="doc-biblioref">2021</a>)</span>.
</figcaption>
</figure>
</div>
<p><strong>Quantization-Aware Training:</strong> Quantization-aware training (QAT) is a fine-tuning of the PTQ model. The model is trained aware of quantization, allowing it to adjust for quantization effects. This produces better accuracy with quantized inference. Quantizing a trained neural network model with methods such as PTQ introduces perturbations that can deviate the model from its original convergence point. For instance, Krishnamoorthi showed that even with per-channel quantization, networks like MobileNet do not reach baseline accuracy with int8 PTQ and require QAT <a href="https://arxiv.org/abs/1806.08342"><span class="citation" data-cites="krishnamoorthi2018quantizing">(<span>Krishnamoorthi 2018</span>)</span></a>.To address this, QAT retrains the model with quantized parameters, employing forward and backward passes in floating point but quantizing parameters after each gradient update. Handling the non-differentiable quantization operator is crucial; a widely used method is the Straight Through Estimator (STE), approximating the rounding operation as an identity function. While other methods and variations exist, STE remains the most commonly used due to its practical effectiveness. In QAT, a pretrained model is quantized and then finetuned using training data to adjust parameters and recover accuracy degradation, as shown in <a href="#fig-QAT-diagram" class="quarto-xref">Figure <span class="quarto-unresolved-ref">fig-QAT-diagram</span></a>. The calibration process is often conducted in parallel with the finetuning process for QAT.</p>
<div id="fig-QAT-diagram" class="quarto-float quarto-figure quarto-figure-center">
<figure class="quarto-float quarto-float-fig">
<div aria-describedby="fig-QAT-diagram-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/efficientnumerics_QAT.png" class="img-fluid" />
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-QAT-diagram-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure 9.26: Quantization-Aware Training. Source: <span class="citation" data-cites="gholami2021survey">Gholami et al. (<a href="#ref-gholami2021survey" role="doc-biblioref">2021</a>)</span>.
</figcaption>
</figure>
</div>
<p>Quantization-Aware Training serves as a natural extension of Post-Training Quantization. Following the initial quantization performed by PTQ, QAT is used to further refine and fine-tune the quantized parameters - see how in <a href="#fig-QAT-PTQ-relation" class="quarto-xref">Figure <span class="quarto-unresolved-ref">fig-QAT-PTQ-relation</span></a>, the PTQ model undergoes an additional step, QAT. It involves a retraining process where the model is exposed to additional training iterations using the original data. This dynamic training approach allows the model to adapt and adjust its parameters, compensating for the performance degradation caused by quantization.</p>
<div id="fig-QAT-PTQ-relation" class="quarto-float quarto-figure quarto-figure-center">
<figure class="quarto-float quarto-float-fig">
<div aria-describedby="fig-QAT-PTQ-relation-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/efficientnumerics_PTQQAT.png" class="img-fluid" />
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-QAT-PTQ-relation-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure 9.27: PTQ and QAT. Source: <span class="citation" data-cites="ultimate"><span>“The Ultimate Guide to Deep Learning Model Quantization and Quantization-Aware Training”</span> (<a href="#ref-ultimate" role="doc-biblioref">n.d.</a>)</span>.
</figcaption>
</figure>
</div>
<p><a href="#fig-quantization-methods-summary" class="quarto-xref">Figure <span class="quarto-unresolved-ref">fig-quantization-methods-summary</span></a> shows the relative accuracy of different models after PTQ and QAT. In almost all cases, QAT yields a better accuracy than PTQ. Consider for example EfficientNet b0. After PTQ, the accuracy drops from 76.85% to 72.06%. But when we apply QAT, the accuracy rebounds to 76.95% (with even a slight improvement over the original accuracy).</p>
<div id="fig-quantization-methods-summary" class="quarto-float quarto-figure quarto-figure-center">
<figure class="quarto-float quarto-float-fig">
<div aria-describedby="fig-quantization-methods-summary-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/efficientnumerics_PTQQATsummary.png" class="img-fluid" />
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-quantization-methods-summary-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure 9.28: Relative accuracies of PTQ and QAT. Source: <span class="citation" data-cites="wu2020integer">Wu, Judd, and Isaev (<a href="#ref-wu2020integer" role="doc-biblioref">2020</a>)</span>.
</figcaption>
</figure>
</div>
</section>
<section id="weights-vs.-activations" class="level3" data-number="9.3.8">
<h3 data-number="9.3.8"><span class="header-section-number">9.3.8</span> Weights vs. Activations</h3>
<p><strong>Weight Quantization:</strong> Involves converting the continuous or high-precision weights of a model to lower-precision, such as converting Float32 weights to quantized INT8 (integer) weights - in <a href="#fig-weight-activations-quantization" class="quarto-xref">Figure <span class="quarto-unresolved-ref">fig-weight-activations-quantization</span></a>, weight quantization is taking place in the second step (red squares) when we multiply the inputs. This reduces the model size, thereby reducing the memory required to store the model and the computational resources needed to perform inference. For example, consider a weight matrix in a neural network layer with Float32 weights as [0.215, -1.432, 0.902, …]. Through weight quantization, these might be mapped to INT8 values like [27, -183, 115, …], significantly reducing the memory required to store them.</p>
<div id="fig-weight-activations-quantization" class="quarto-float quarto-figure quarto-figure-center">
<figure class="quarto-float quarto-float-fig">
<div aria-describedby="fig-weight-activations-quantization-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/efficientnumerics_weightsactivations.png" class="img-fluid" />
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-weight-activations-quantization-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure 9.29: Weight and activation quantization. Source: HarvardX.
</figcaption>
</figure>
</div>
<p><strong>Activation Quantization:</strong> Involves quantizing the activation values (outputs of layers) during model inference. This can reduce the computational resources required during inference, but it introduces additional challenges in maintaining model accuracy due to the reduced precision of intermediate computations. For example, in a convolutional neural network (CNN), the activation maps (feature maps) produced by convolutional layers, originally in Float32, might be quantized to INT8 during inference to accelerate computation, especially on hardware optimized for integer arithmetic. Additionally, recent work has explored the use of Activation-aware Weight Quantization for LLM compression and acceleration, which involves protecting only 1% of the most important salient weights by observing the activations not weights <a href="https://arxiv.org/pdf/2306.00978.pdf"><span class="citation" data-cites="lin2023awq">(<span>Lin et al. 2023</span>)</span></a>.</p>
</section>
<section id="trade-offs" class="level3" data-number="9.3.9">
<h3 data-number="9.3.9"><span class="header-section-number">9.3.9</span> Trade-offs</h3>
<p>Quantization invariably introduces a trade-off between model size/performance and accuracy. While it significantly reduces the memory footprint and can accelerate inference, especially on hardware optimized for low-precision arithmetic, the reduced precision can degrade model accuracy.</p>
<p><strong>Model Size:</strong> A model with weights represented as Float32 being quantized to INT8 can theoretically reduce the model size by a factor of 4, enabling it to be deployed on devices with limited memory. The model size of large language models is developing at a faster pace than the GPU memory in recent years, leading to a big gap between the supply and demand for memory. <a href="#fig-model-size-pace" class="quarto-xref">Figure <span class="quarto-unresolved-ref">fig-model-size-pace</span></a> illustrates the recent trend of the widening gap between model size (red line) and accelerator memory (yellow line). Quantization and model compression techniques can help bridge the gap</p>
<div id="fig-model-size-pace" class="quarto-float quarto-figure quarto-figure-center">
<figure class="quarto-float quarto-float-fig">
<div aria-describedby="fig-model-size-pace-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/efficientnumerics_modelsizes.png" class="img-fluid" />
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-model-size-pace-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure 9.30: Model size vs. accelerator memory. Source: <span class="citation" data-cites="xiao2022smoothquant">Xiao et al. (<a href="#ref-xiao2022smoothquant" role="doc-biblioref">2022</a>)</span>.
</figcaption>
</figure>
</div>
<p><strong>Inference Speed:</strong> Quantization can also accelerate inference, as lower-precision arithmetic is computationally less expensive. For example, certain hardware accelerators, like Google’s Edge TPU, are optimized for INT8 arithmetic and can perform inference significantly faster with INT8 quantized models compared to their floating-point counterparts. The reduction in memory from quantization helps reduce the amount of data transmission, saving up memory and speeding the process. <a href="#fig-nvidia-turing" class="quarto-xref">Figure <span class="quarto-unresolved-ref">fig-nvidia-turing</span></a> compares the increase in throughput and the reduction in bandwidth memory for different data type on the NVIDIA Turing GPU.</p>
<div id="fig-nvidia-turing" class="quarto-float quarto-figure quarto-figure-center">
<figure class="quarto-float quarto-float-fig">
<div aria-describedby="fig-nvidia-turing-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/efficientnumerics_benefitsofprecision.png" class="img-fluid" />
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-nvidia-turing-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure 9.31: Benefits of lower precision data types. Source: <span class="citation" data-cites="wu2020integer">Wu, Judd, and Isaev (<a href="#ref-wu2020integer" role="doc-biblioref">2020</a>)</span>.
</figcaption>
</figure>
</div>
<p><strong>Accuracy:</strong> The reduction in numerical precision post-quantization can lead to a degradation in model accuracy, which might be acceptable in certain applications (e.g., image classification) but not in others (e.g., medical diagnosis). Therefore, post-quantization, the model typically requires re-calibration or fine-tuning to mitigate accuracy loss. Furthermore, recent work has explored the use of <a href="https://arxiv.org/pdf/2306.00978.pdf">Activation-aware Weight Quantization <span class="citation" data-cites="lin2023awq">(<span>Lin et al. 2023</span>)</span></a> which is based on the observation that protecting only 1% of salient weights can greatly reduce quantization error.</p>
</section>
<section id="quantization-and-pruning" class="level3" data-number="9.3.10">
<h3 data-number="9.3.10"><span class="header-section-number">9.3.10</span> Quantization and Pruning</h3>
<p>Pruning and quantization work well together, and it’s been found that pruning doesn’t hinder quantization. In fact, pruning can help reduce quantization error. Intuitively, this is due to pruning reducing the number of weights to quantize, thereby reducing the accumulated error from quantization. For example, an unpruned AlexNet has 60 million weights to quantize whereas a pruned AlexNet only has 6.7 million weights to quantize. This significant drop in weights helps reduce the error between quantizing the unpruned AlexNet vs. the pruned AlexNet. Furthermore, recent work has found that quantization-aware pruning generates more computationally efficient models than either pruning or quantization alone; It typically performs similar to or better in terms of computational efficiency compared to other neural architecture search techniques like Bayesian optimization <a href="https://arxiv.org/pdf/2102.11289.pdf"><span class="citation" data-cites="hawks2021psandqs">(<span>Hawks et al. 2021</span>)</span></a>.</p>
<div id="fig-compression-methods" class="quarto-float quarto-figure quarto-figure-center">
<figure class="quarto-float quarto-float-fig">
<div aria-describedby="fig-compression-methods-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/efficientnumerics_qp1.png" class="img-fluid" />
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-compression-methods-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure 9.32: Accuracy vs. compression rate under different compression methods. Source: <span class="citation" data-cites="han2015deep">Han, Mao, and Dally (<a href="#ref-han2015deep" role="doc-biblioref">2015</a>)</span>.
</figcaption>
</figure>
</div>
</section>
<section id="edge-aware-quantization" class="level3" data-number="9.3.11">
<h3 data-number="9.3.11"><span class="header-section-number">9.3.11</span> Edge-aware Quantization</h3>
<p>Quantization not only reduces model size but also enables faster computations and draws less power, making it vital to edge development. Edge devices typically have tight resource constraints with compute, memory, and power, which are impossible to meet for many of the deep NN models of today. Furthermore, edge processors do not support floating point operations, making integer quantization particularly important for chips like GAP-8, a RISC-V SoC for edge inference with a dedicated CNN accelerator, which only support integer arithmetic.</p>
<p>One hardware platform utilizing quantization is the ARM Cortex-M group of 32-bit RISC ARM processor cores. They leverage fixed-point quantization with power of two scaling factors so that quantization and dequantization can be efficiently done by bit shifting. Additionally, Google Edge TPUs, Google’s emerging solution for running inference at the edge, is designed for small, low-powered devices and can only support 8-bit arithmetic. Many complex neural network models that could only be deployed on servers due to their high computational needs can now be run on edge devices thanks to recent advancements (e.g. quantization methods) in edge computing field.</p>
<p>In addition to being an indispensable technique for many edge processors, quantization has also brought noteworthy improvements to non-edge processors such as encouraging such processors to meet the Service Level Agreement (SLA) requirements such as 99th percentile latency.</p>
<p>Thus, quantization combined with efficient low-precision logic and dedicated deep learning accelerators, has been one crucial driving force for the evolution of such edge processors.</p>
<p><a href="#vid-quant" class="quarto-xref">Video <span class="quarto-unresolved-ref">vid-quant</span></a> is a lecture on quantization and the different quantization methods.</p>
<div id="vid-quant" class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class='callout-icon'></i>
</div>
<div class="callout-title-container flex-fill">
Important 9.1: Quantization
</div>
</div>
<div class="callout-body-container callout-body">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/AlASZb93rrc" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></div>
</div>
</div>
</section>
</section>
<section id="sec-model_ops_hw" class="level2" data-number="9.4">
<h2 data-number="9.4"><span class="header-section-number">9.4</span> Efficient Hardware Implementation</h2>
<p>Efficient hardware implementation transcends the selection of suitable components; it requires a holistic understanding of how software will interact with underlying architectures. The essence of achieving peak performance in TinyML applications lies not only in refining algorithms to hardware but also in ensuring that the hardware is strategically tailored to support these algorithms. This synergy between hardware and software is crucial. As we look deeper into the intricacies of efficient hardware implementation, the significance of a co-design approach, where hardware and software are developed in tandem, becomes increasingly evident. This section provides an overview of the techniques of how hardware and the interactions between hardware and software can be optimized to improve models performance.</p>
<section id="hardware-aware-neural-architecture-search" class="level3" data-number="9.4.1">
<h3 data-number="9.4.1"><span class="header-section-number">9.4.1</span> Hardware-Aware Neural Architecture Search</h3>
<p>Focusing only on the accuracy when performing Neural Architecture Search leads to models that are exponentially complex and require increasing memory and compute. This has lead to hardware constraints limiting the exploitation of the deep learning models at their full potential. Manually designing the architecture of the model is even harder when considering the hardware variety and limitations. This has lead to the creation of Hardware-aware Neural Architecture Search that incorporate the hardware contractions into their search and optimize the search space for a specific hardware and accuracy. HW-NAS can be categorized based how it optimizes for hardware. We will briefly explore these categories and leave links to related papers for the interested reader.</p>
<section id="single-target-fixed-platform-configuration" class="level4">
<h4>Single Target, Fixed Platform Configuration</h4>
<p>The goal here is to find the best architecture in terms of accuracy and hardware efficiency for one fixed target hardware. For a specific hardware, the Arduino Nicla Vision for example, this category of HW-NAS will look for the architecture that optimizes accuracy, latency, energy consumption, etc.</p>
<section id="hardware-aware-search-strategy" class="level5">
<h5>Hardware-aware Search Strategy</h5>
<p>Here, the search is a multi-objective optimization problem, where both the accuracy and hardware cost guide the searching algorithm to find the most efficient architecture <span class="citation" data-cites="tan2019mnasnet cai2018proxylessnas wu2019fbnet">(<a href="#ref-tan2019mnasnet" role="doc-biblioref">Tan et al. 2019</a>; <a href="#ref-cai2018proxylessnas" role="doc-biblioref">Cai, Zhu, and Han 2019</a>; <a href="#ref-wu2019fbnet" role="doc-biblioref">B. Wu et al. 2019</a>)</span>.</p>
</section>
<section id="hardware-aware-search-space" class="level5">
<h5>Hardware-aware Search Space</h5>
<p>Here, the search space is restricted to the architectures that perform well on the specific hardware. This can be achieved by either measuring the operators (Conv operator, Pool operator, …) performance, or define a set of rules that limit the search space. <span class="citation" data-cites="zhang2020fast">(<a href="#ref-zhang2020fast" role="doc-biblioref">L. L. Zhang et al. 2020</a>)</span></p>
</section>
</section>
<section id="single-target-multiple-platform-configurations" class="level4">
<h4>Single Target, Multiple Platform Configurations</h4>
<p>Some hardware may have different configurations. For example, FPGAs have Configurable Logic Blocks (CLBs) that can be configured by the firmware. This method allows for the HW-NAS to explore different configurations. <span class="citation" data-cites="jiang2019accuracy yang2020coexploration">(<a href="#ref-jiang2019accuracy" role="doc-biblioref">Hu et al. 2023</a>; <a href="#ref-yang2020coexploration" role="doc-biblioref">Ho Yoon et al. 2012</a>)</span></p>
</section>
<section id="multiple-targets" class="level4">
<h4>Multiple Targets</h4>
<p>This category aims at optimizing a single model for multiple hardware. This can be helpful for mobile devices development as it can optimize to different phones models. <span class="citation" data-cites="chu2021discovering jiang2019accuracy">(<a href="#ref-chu2021discovering" role="doc-biblioref">Chu et al. 2021</a>; <a href="#ref-jiang2019accuracy" role="doc-biblioref">Hu et al. 2023</a>)</span></p>
</section>
<section id="examples-of-hardware-aware-neural-architecture-search" class="level4">
<h4>Examples of Hardware-Aware Neural Architecture Search</h4>
<section id="tinynas" class="level5">
<h5>TinyNAS</h5>
<p>TinyNAS adopts a two stage approach to finding an optimal architecture for model with the constraints of the specific microcontroller in mind.</p>
<p>First, TinyNAS generate multiple search spaces by varying the input resolution of the model, and the number of channels of the layers of the model. Then, TinyNAS chooses a search space based on the FLOPs (Floating Point Operations Per Second) of each search space. Spaces with a higher probability of containing architectures with a large number of FLOPs yields models with higher accuracies - compare the red line vs. the black line in <a href="#fig-search-space-flops" class="quarto-xref">Figure <span class="quarto-unresolved-ref">fig-search-space-flops</span></a>. Since a higher number FLOPs means the model has a higher computational capacity, the model is more likely to have a higher accuracy.</p>
<p>Then, TinyNAS performs a search operation on the chosen space to find the optimal architecture for the specific constraints of the microcontroller. <span class="citation" data-cites="lin2020mcunet">(<a href="#ref-lin2020mcunet" role="doc-biblioref">J. Lin et al. 2020</a>)</span></p>
<div id="fig-search-space-flops" class="quarto-float quarto-figure quarto-figure-center">
<figure class="quarto-float quarto-float-fig">
<div aria-describedby="fig-search-space-flops-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/modeloptimization_TinyNAS.png" class="img-fluid" />
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-search-space-flops-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure 9.33: Search spaces accuracy. Source: <span class="citation" data-cites="lin2020mcunet">J. Lin et al. (<a href="#ref-lin2020mcunet" role="doc-biblioref">2020</a>)</span>.
</figcaption>
</figure>
</div>
</section>
</section>
<section id="topology-aware-nas" class="level4">
<h4>Topology-Aware NAS</h4>
<p>Focuses on creating and optimizing a search space that aligns with the hardware topology of the device. <span class="citation" data-cites="zhang2019autoshrink">(<a href="#ref-zhang2019autoshrink" role="doc-biblioref">T. Zhang et al. 2020</a>)</span></p>
</section>
</section>
<section id="challenges-of-hardware-aware-neural-architecture-search" class="level3" data-number="9.4.2">
<h3 data-number="9.4.2"><span class="header-section-number">9.4.2</span> Challenges of Hardware-Aware Neural Architecture Search</h3>
<p>While HW-NAS carries high potential for finding optimal architectures for TinyML, it comes with some challenges. Hardware Metrics like latency, energy consumption and hardware utilization are harder to evaluate than the metrics of accuracy or loss. They often require specialized tools for precise measurements. Moreover, adding all these metrics leads to a much bigger search space. This leads to HW-NAS being time-consuming and expensive. It has to be applied to every hardware for optimal results, moreover, meaning that if one needs to deploy the model on multiple devices, the search has to be conducted multiple times and will result in different models, unless optimizing for all of them which means less accuracy. Finally, hardware changes frequently, and HW-NAS may need to be conducted on each version.</p>
</section>
<section id="kernel-optimizations" class="level3" data-number="9.4.3">
<h3 data-number="9.4.3"><span class="header-section-number">9.4.3</span> Kernel Optimizations</h3>
<p>Kernel Optimizations are modifications made to the kernel to improve the performance of machine learning models on resource-constrained devices. We will separate kernel optimizations into two types.</p>
<section id="general-kernel-optimizations" class="level4">
<h4>General Kernel Optimizations</h4>
<p>These are kernel optimizations that all devices can benefit from. They provide technics to convert the code to more efficient instructions.</p>
<section id="loop-unrolling" class="level5">
<h5>Loop unrolling</h5>
<p>Instead of having a loop with loop control (incrementing the loop counter, checking the loop termination condition) the loop can be unrolled and the overhead of loop control can be omitted. This may also provide additional opportunities for parallelism that may not be possible with the loop structure. This can be particularly beneficial for tight loops, where the body of the loop is a small number of instructions with a lot of iterations.</p>
</section>
<section id="blocking" class="level5">
<h5>Blocking</h5>
<p>Blocking is used to make memory access patterns more efficient. If we have three computations the first and the last need to access cache A and the second needs to access cache B, blocking blocks the first two computations together to reduce the number of memory reads needed.</p>
</section>
<section id="tiling" class="level5">
<h5>Tiling</h5>
<p>Similarly to blocking, tiling divides data and computation into chunks, but extends beyond cache improvements. Tiling creates independent partitions of computation that can be run in parallel, which can result in significant performance improvements.</p>
</section>
<section id="optimized-kernel-libraries" class="level5">
<h5>Optimized Kernel Libraries</h5>
<p>This comprises developing optimized kernels that take full advantage of a specific hardware. One example is the CMSIS-NN library, which is a collection of efficient neural network kernels developed to optimize the performance and minimize the memory footprint of models on Arm Cortex-M processors, which are common on IoT edge devices. The kernel leverage multiple hardware capabilities of Cortex-M processors like Single Instruction Multiple Data (SIMD), Floating Point Units (FPUs) and M-Profile Vector Extensions (MVE). These optimization make common operations like matrix multiplications more efficient, boosting the performance of model operations on Cortex-M processors. <span class="citation" data-cites="lai2018cmsisnn">(<a href="#ref-lai2018cmsisnn" role="doc-biblioref">Lai, Suda, and Chandra 2018</a>)</span></p>
</section>
</section>
</section>
<section id="compute-in-memory-cim" class="level3" data-number="9.4.4">
<h3 data-number="9.4.4"><span class="header-section-number">9.4.4</span> Compute-in-Memory (CiM)</h3>
<p>This is one example of Algorithm-Hardware Co-design. CiM is a computing paradigm that performs computation within memory. Therefore, CiM architectures allow for operations to be performed directly on the stored data, without the need to shuttle data back and forth between separate processing and memory units. This design paradigm is particularly beneficial in scenarios where data movement is a primary source of energy consumption and latency, such as in TinyML applications on edge devices. <a href="#fig-computing-memory" class="quarto-xref">Figure <span class="quarto-unresolved-ref">fig-computing-memory</span></a> is one example of using CiM in TinyML: keyword spotting requires an always-on process that looks for certain wake words (such as ‘Hey, Siri’). Given the resource-intensive nature of this task, integrating CiM for the always-on keyword detection model can improve efficiency.</p>
<p>Through algorithm-hardware co-design, the algorithms can be optimized to leverage the unique characteristics of CiM architectures, and conversely, the CiM hardware can be customized or configured to better support the computational requirements and characteristics of the algorithms. This is achieved by using the analog properties of memory cells, such as addition and multiplication in DRAM. <span class="citation" data-cites="zhou2021analognets">(<a href="#ref-zhou2021analognets" role="doc-biblioref">Zhou et al. 2021</a>)</span></p>
<div id="fig-computing-memory" class="quarto-float quarto-figure quarto-figure-center">
<figure class="quarto-float quarto-float-fig">
<div aria-describedby="fig-computing-memory-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/modeloptimization_CiM.png" class="img-fluid" />
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-computing-memory-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure 9.34: CiM for keyword spotting. Source: <span class="citation" data-cites="zhou2021analognets">Zhou et al. (<a href="#ref-zhou2021analognets" role="doc-biblioref">2021</a>)</span>.
</figcaption>
</figure>
</div>
</section>
<section id="memory-access-optimization" class="level3" data-number="9.4.5">
<h3 data-number="9.4.5"><span class="header-section-number">9.4.5</span> Memory Access Optimization</h3>
<p>Different devices may have different memory hierarchies. Optimizing for the specific memory hierarchy in the specific hardware can lead to great performance improvements by reducing the costly operations of reading and writing to memory. Dataflow optimization can be achieved by optimizing for reusing data within a single layer and across multiple layers. This dataflow optimization can be tailored to the specific memory hierarchy of the hardware, which can lead to greater benefits than general optimizations for different hardware.</p>
<section id="leveraging-sparsity" class="level4">
<h4>Leveraging Sparsity</h4>
<p>Pruning is a fundamental approach to compress models to make them compatible with resource constrained devices. This results in sparse models where a lot of weights are 0’s. Therefore, leveraging this sparsity can lead to significant improvements in performance. Tools were created to achieve exactly this. RAMAN, is a sparse TinyML accelerator designed for inference on edge devices. RAMAN overlap input and output activations on the same memory space, reducing storage requirements by up to 50%. <span class="citation" data-cites="krishna2023raman">(<a href="#ref-krishna2023raman" role="doc-biblioref">Krishna et al. 2023</a>)</span></p>
</section>
<section id="optimization-frameworks" class="level4">
<h4>Optimization Frameworks</h4>
<p>Optimization Frameworks have been introduced to exploit the specific capabilities of the hardware to accelerate the software. One example of such a framework is hls4ml - <a href="#fig-hls4ml-workflow" class="quarto-xref">Figure <span class="quarto-unresolved-ref">fig-hls4ml-workflow</span></a> provides an overview of the framework’s workflow. This open-source software-hardware co-design workflow aids in interpreting and translating machine learning algorithms for implementation with both FPGA and ASIC technologies. Features such as network optimization, new Python APIs, quantization-aware pruning, and end-to-end FPGA workflows are embedded into the hls4ml framework, leveraging parallel processing units, memory hierarchies, and specialized instruction sets to optimize models for edge hardware. Moreover, hls4ml is capable of translating machine learning algorithms directly into FPGA firmware.</p>
<div id="fig-hls4ml-workflow" class="quarto-float quarto-figure quarto-figure-center">
<figure class="quarto-float quarto-float-fig">
<div aria-describedby="fig-hls4ml-workflow-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/modeloptimization_hls4ml.png" class="img-fluid" />
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-hls4ml-workflow-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure 9.35: hls4ml framework workflow. Source: <span class="citation" data-cites="fahim2021hls4ml">Fahim et al. (<a href="#ref-fahim2021hls4ml" role="doc-biblioref">2021</a>)</span>.
</figcaption>
</figure>
</div>
<p>One other framework for FPGAs that focuses on a holistic approach is CFU Playground <span class="citation" data-cites="prakash2022cfu">(<a href="#ref-prakash2022cfu" role="doc-biblioref">Prakash et al. 2023</a>)</span></p>
</section>
<section id="hardware-built-around-software" class="level4">
<h4>Hardware Built Around Software</h4>
<p>In a contrasting approach, hardware can be custom-designed around software requirements to optimize the performance for a specific application. This paradigm creates specialized hardware to better adapt to the specifics of the software, thus reducing computational overhead and improving operational efficiency. One example of this approach is a voice-recognition application by <span class="citation" data-cites="kwon2021hardwaresoftware">(<a href="#ref-kwon2021hardwaresoftware" role="doc-biblioref">Kwon and Park 2021</a>)</span>. The paper proposes a structure wherein preprocessing operations, traditionally handled by software, are allocated to custom-designed hardware. This technique was achieved by introducing resistor-transistor logic to an inter-integrated circuit sound module for windowing and audio raw data acquisition in the voice-recognition application. Consequently, this offloading of preprocessing operations led to a reduction in computational load on the software, showcasing a practical application of building hardware around software to improve the efficiency and performance.</p>
<div id="fig-fpga-preprocessing" class="quarto-float quarto-figure quarto-figure-center">
<figure class="quarto-float quarto-float-fig">
<div aria-describedby="fig-fpga-preprocessing-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/modeloptimization_preprocessor.png" class="img-fluid" />
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-fpga-preprocessing-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure 9.36: Delegating data processing to an FPGA. Source: <span class="citation" data-cites="kwon2021hardwaresoftware">Kwon and Park (<a href="#ref-kwon2021hardwaresoftware" role="doc-biblioref">2021</a>)</span>.
</figcaption>
</figure>
</div>
</section>
<section id="splitnets" class="level4">
<h4>SplitNets</h4>
<p>SplitNets were introduced in the context of Head-Mounted systems. They distribute the Deep Neural Networks (DNNs) workload among camera sensors and an aggregator. This is particularly compelling the in context of TinyML. The SplitNet framework is a split-aware NAS to find the optimal neural network architecture to achieve good accuracy, split the model among the sensors and the aggregator, and minimize the communication between the sensors and the aggregator.</p>
<p><a href="#fig-splitnet-performance" class="quarto-xref">Figure <span class="quarto-unresolved-ref">fig-splitnet-performance</span></a> demonstrates how SplitNets (in red) achieves higher accuracy for lower latency (running on ImageNet) than different approaches, such as running the DNN on-sensor (All-on-sensor; in green) or on mobile (All-on-aggregator; in blue). Minimal communication is important in TinyML where memory is highly constrained, this way the sensors conduct some of the processing on their chips and then they send only the necessary information to the aggregator. When testing on ImageNet, SplitNets were able to reduce the latency by one order of magnitude on head-mounted devices. This can be helpful when the sensor has its own chip. <span class="citation" data-cites="dong2022splitnets">(<a href="#ref-dong2022splitnets" role="doc-biblioref">Dong et al. 2022</a>)</span></p>
<div id="fig-splitnet-performance" class="quarto-float quarto-figure quarto-figure-center">
<figure class="quarto-float quarto-float-fig">
<div aria-describedby="fig-splitnet-performance-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/modeloptimization_SplitNets.png" class="img-fluid" />
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-splitnet-performance-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure 9.37: SplitNets vs other approaches. Source: <span class="citation" data-cites="dong2022splitnets">Dong et al. (<a href="#ref-dong2022splitnets" role="doc-biblioref">2022</a>)</span>.
</figcaption>
</figure>
</div>
</section>
<section id="hardware-specific-data-augmentation" class="level4">
<h4>Hardware Specific Data Augmentation</h4>
<p>Each edge device may possess unique sensor characteristics, leading to specific noise patterns that can impact model performance. One example is audio data, where variations stemming from the choice of microphone are prevalent. Applications such as Keyword Spotting can experience substantial enhancements by incorporating data recorded from devices similar to those intended for deployment. Fine-tuning of existing models can be employed to adapt the data precisely to the sensor’s distinctive characteristics.</p>
</section>
</section>
</section>
<section id="software-and-framework-support" class="level2" data-number="9.5">
<h2 data-number="9.5"><span class="header-section-number">9.5</span> Software and Framework Support</h2>
<p>While all of the aforementioned techniques like <a href="#sec-pruning">pruning</a>, <a href="#sec-quant">quantization</a>, and efficient numerics are well-known, they would remain impractical and inaccessible without extensive software support. For example, directly quantizing weights and activations in a model would require manually modifying the model definition and inserting quantization operations throughout. Similarly, directly pruning model weights requires manipulating weight tensors. Such tedious approaches become infeasible at scale.</p>
<p>Without the extensive software innovation across frameworks, optimization tools and hardware integration, most of these techniques would remain theoretical or only viable to experts. Without framework APIs and automation to simplify applying these optimizations, they would not see adoption. Software support makes them accessible to general practitioners and unlocks real-world benefits. In addition, issues such as hyperparameter tuning for pruning, managing the trade-off between model size and accuracy, and ensuring compatibility with target devices pose hurdles that developers must navigate.</p>
<section id="built-in-optimization-apis" class="level3" data-number="9.5.1">
<h3 data-number="9.5.1"><span class="header-section-number">9.5.1</span> Built-in Optimization APIs</h3>
<p>Major machine learning frameworks like TensorFlow, PyTorch, and MXNet provide libraries and APIs to allow common model optimization techniques to be applied without requiring custom implementations. For example, TensorFlow offers the TensorFlow Model Optimization Toolkit which contains modules like:</p>
<ul>
<li><strong><a href="https://www.tensorflow.org/model_optimization/api_docs/python/tfmot/quantization/keras/quantize_model">Quantization</a></strong>: Applies quantization-aware training to convert floating point models to lower precision like int8 with minimal accuracy loss. Handles weight and activation quantization.</li>
<li><strong><a href="https://www.tensorflow.org/model_optimization/api_docs/python/tfmot/sparsity/keras">Sparsity</a></strong>: Provides pruning APIs to induce sparsity and remove unnecessary connections in models like neural networks. Can prune weights, layers, etc.</li>
<li><strong><a href="https://www.tensorflow.org/model_optimization/api_docs/python/tfmot/clustering">Clustering</a></strong>: Supports model compression by clustering weights into groups for higher compression rates.</li>
</ul>
<p>These APIs allow users to enable optimization techniques like quantization and pruning without directly modifying model code. Parameters like target sparsity rates, quantization bit-widths etc. can be configured. Similarly, PyTorch provides torch.quantization for converting models to lower precision representations. TorchTensor and TorchModule form the base classes for quantization support. It also offers torch.nn.utils.prune for built-in pruning of models. MXNet offers gluon.contrib layers that add quantization capabilities like fixed point rounding and stochastic rounding of weights/activations during training. This allows quantization to be readily included in gluon models.</p>
<p>The core benefit of built-in optimizations is that users can apply them without re-implementing complex techniques. This makes optimized models accessible to a broad range of practitioners. It also ensures best practices are followed by building on research and experience implementing the methods. As new optimizations emerge, frameworks strive to provide native support and APIs where possible to further lower the barrier to efficient ML. The availability of these tools is key to widespread adoption.</p>
</section>
<section id="automated-optimization-tools" class="level3" data-number="9.5.2">
<h3 data-number="9.5.2"><span class="header-section-number">9.5.2</span> Automated Optimization Tools</h3>
<p>Automated optimization tools provided by frameworks can analyze models and automatically apply optimizations like quantization, pruning, and operator fusion to make the process easier and accessible without excessive manual tuning. In effect, this builds on top of the previous section. For example, TensorFlow provides the TensorFlow Model Optimization Toolkit which contains modules like:</p>
<ul>
<li><strong><a href="https://www.tensorflow.org/model_optimization/guide/quantization/training">QuantizationAwareTraining</a></strong>: Automatically quantizes weights and activations in a model to lower precision like UINT8 or INT8 with minimal accuracy loss. It inserts fake quantization nodes during training so that the model can learn to be quantization-friendly.</li>
<li><strong><a href="https://www.tensorflow.org/model_optimization/guide/pruning/pruning_with_keras">Pruning</a></strong>: Automatically removes unnecessary connections in a model based on analysis of weight importance. Can prune entire filters in convolutional layers or attention heads in transformers. Handles iterative re-training to recover any accuracy loss.</li>
<li><strong><a href="https://www.tensorflow.org/guide/graph_optimization">GraphOptimizer</a></strong>: Applies graph optimizations like operator fusion to consolidate operations and reduce execution latency, especially for inference. In <a href="#fig-graph-optimizer" class="quarto-xref">Figure <span class="quarto-unresolved-ref">fig-graph-optimizer</span></a>, you can see the original (Source Graph) on the left, and how its operations are transformed (consolidated) on the right. Notice how Block1 in Source Graph has 3 separate steps (Convolution, BiasAdd, and Activation), which are then consolidated together in Block1 on Optimized Graph.</li>
</ul>
<div id="fig-graph-optimizer" class="quarto-float quarto-figure quarto-figure-center">
<figure class="quarto-float quarto-float-fig">
<div aria-describedby="fig-graph-optimizer-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./images/png/source_opt.png" class="img-fluid" />
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-graph-optimizer-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure 9.38: GraphOptimizer. Source: <span class="citation" data-cites="annette2020">Wess et al. (<a href="#ref-annette2020" role="doc-biblioref">2020</a>)</span>.
</figcaption>
</figure>
</div>
<p>These automated modules only require the user to provide the original floating point model, and handle the end-to-end optimization pipeline including any re-training to regain accuracy. Other frameworks like PyTorch also offer increasing automation support, for example through torch.quantization.quantize_dynamic. Automated optimization makes efficient ML accessible to practitioners without optimization expertise.</p>
</section>
<section id="hardware-optimization-libraries" class="level3" data-number="9.5.3">
<h3 data-number="9.5.3"><span class="header-section-number">9.5.3</span> Hardware Optimization Libraries</h3>
<p>Hardware libraries like TensorRT and TensorFlow XLA allow models to be highly optimized for target hardware through techniques that we discussed earlier.</p>
<ul>
<li><p><strong>Quantization:</strong> For example, TensorRT and TensorFlow Lite both support quantization of models during conversion to their format. This provides speedups on mobile SoCs with INT8/INT4 support.</p></li>
<li><p><strong>Kernel Optimization:</strong> For instance, TensorRT does auto-tuning to optimize CUDA kernels based on the GPU architecture for each layer in the model graph. This extracts maximum throughput.</p></li>
<li><p><strong>Operator Fusion:</strong> TensorFlow XLA does aggressive fusion to create optimized binary for TPUs. On mobile, frameworks like NCNN also support fused operators.</p></li>
<li><p><strong>Hardware-Specific Code:</strong> Libraries are used to generate optimized binary code specialized for the target hardware. For example, <a href="https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html">TensorRT</a> uses Nvidia CUDA/cuDNN libraries which are hand-tuned for each GPU architecture. This hardware-specific coding is key for performance. On TinyML devices, this can mean assembly code optimized for a Cortex M4 CPU for example. Vendors provide CMSIS-NN and other libraries.</p></li>
<li><p><strong>Data Layout Optimizations:</strong> We can efficiently leverage memory hierarchy of hardware like cache and registers through techniques like tensor/weight rearrangement, tiling, and reuse. For example, TensorFlow XLA optimizes buffer layouts to maximize TPU utilization. This helps any memory constrained systems.</p></li>
<li><p><strong>Profiling-based Tuning:</strong> We can use profiling tools to identify bottlenecks. For example, adjust kernel fusion levels based on latency profiling. On mobile SoCs, vendors like Qualcomm provide profilers in SNPE to find optimization opportunities in CNNs. This data-driven approach is important for performance.</p></li>
</ul>
<p>By integrating framework models with these hardware libraries through conversion and execution pipelines, ML developers can achieve significant speedups and efficiency gains from low-level optimizations tailored to the target hardware. The tight integration between software and hardware is key to enabling performant deployment of ML applications, especially on mobile and TinyML devices.</p>
</section>
<section id="visualizing-optimizations" class="level3" data-number="9.5.4">
<h3 data-number="9.5.4"><span class="header-section-number">9.5.4</span> Visualizing Optimizations</h3>
<p>Implementing model optimization techniques without visibility into the effects on the model can be challenging. Dedicated tooling or visualization tools can provide critical and useful insight into model changes and helps track the optimization process. Let’s consider the optimizations we considered earlier, such as pruning for sparsity and quantization.</p>
<section id="sparsity" class="level5">
<h5>Sparsity</h5>
<p>For example, consider sparsity optimizations. Sparsity visualization tools can provide critical insights into pruned models by mapping out exactly which weights have been removed. For example, sparsity heat maps can use color gradients to indicate the percentage of weights pruned in each layer of a neural network. Layers with higher percentages pruned appear darker (see <a href="#fig-sprase-heat-map" class="quarto-xref">Figure <span class="quarto-unresolved-ref">fig-sprase-heat-map</span></a>). This identifies which layers have been simplified the most by pruning (<a href="https://www.numenta.com/blog/2020/10/30/case-for-sparsity-in-neural-networks-part-2-dynamic-sparsity/">Souza 2020</a>).</p>
<div id="fig-sprase-heat-map" class="quarto-float quarto-figure quarto-figure-center">
<figure class="quarto-float quarto-float-fig">
<div aria-describedby="fig-sprase-heat-map-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://www.numenta.com/wp-content/uploads/2020/10/Picture1.png" class="img-fluid" />
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-sprase-heat-map-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure 9.39: Sparse network heat map. Source: <a href="https://www.numenta.com/blog/2020/10/30/case-for-sparsity-in-neural-networks-part-2-dynamic-sparsity/">Numenta</a>.
</figcaption>
</figure>
</div>
<p>Trend plots can also track sparsity over successive pruning rounds - they may show initial rapid pruning followed by more gradual incremental increases. Tracking the current global sparsity along with statistics like average, minimum, and maximum sparsity per-layer in tables or plots provides an overview of the model composition. For a sample convolutional network, these tools could reveal that the first convolution layer is pruned 20% while the final classifier layer is pruned 70% given its redundancy. The global model sparsity may increase from 10% after initial pruning to 40% after five rounds.</p>
<p>By making sparsity data visually accessible, practitioners can better understand exactly how their model is being optimized and which areas are being impacted. The visibility enables them to fine-tune and control the pruning process for a given architecture.</p>
<p>Sparsity visualization turns pruning into a transparent technique instead of a black-box operation.</p>
</section>
<section id="quantization-1" class="level5">
<h5>Quantization</h5>
<p>Converting models to lower numeric precisions through quantization introduces errors that can impact model accuracy if not properly tracked and addressed. Visualizing quantization error distributions provides valuable insights into the effects of reduced precision numerics applied to different parts of a model. For this, histograms of the quantization errors for weights and activations can be generated. These histograms can reveal the shape of the error distribution - whether they resemble a Gaussian distribution or contain significant outliers and spikes. <a href="#fig-quantization-error" class="quarto-xref">Figure <span class="quarto-unresolved-ref">fig-quantization-error</span></a> shows the distributions of different quantization methods. Large outliers may indicate issues with particular layers handling the quantization. Comparing the histograms across layers highlights any problem areas standing out with abnormally high errors.</p>
<div id="fig-quantization-error" class="quarto-float quarto-figure quarto-figure-center">
<figure class="quarto-float quarto-float-fig">
<div aria-describedby="fig-quantization-error-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/png/modeloptimization_quant_hist.png" class="img-fluid" />
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-quantization-error-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure 9.40: Quantization errors. Source: <span class="citation" data-cites="kuzmin2022fp8">Kuzmin et al. (<a href="#ref-kuzmin2022fp8" role="doc-biblioref">2022</a>)</span>.
</figcaption>
</figure>
</div>
<p>Activation visualizations are also important to detect overflow issues. By color mapping the activations before and after quantization, any values pushed outside the intended ranges become visible. This reveals saturation and truncation issues that could skew the information flowing through the model. Detecting these errors allows recalibrating activations to prevent loss of information (<a href="https://medium.com/exemplifyml-ai/visualizing-neural-network-activation-a27caa451ff">Mandal 2022</a>). <a href="#fig-color-mapping" class="quarto-xref">Figure <span class="quarto-unresolved-ref">fig-color-mapping</span></a> is a color mapping of the AlexNet convolutional kernels.</p>
<div id="fig-color-mapping" class="quarto-float quarto-figure quarto-figure-center">
<figure class="quarto-float quarto-float-fig">
<div aria-describedby="fig-color-mapping-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://compsci697l.github.io/assets/cnnvis/filt1.jpeg" class="img-fluid" />
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-color-mapping-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure 9.41: Color mapping of activations. Source: <span class="citation" data-cites="alexnet2012">Krizhevsky, Sutskever, and Hinton (<a href="#ref-alexnet2012" role="doc-biblioref">2017</a>)</span>.
</figcaption>
</figure>
</div>
<p>Other techniques, such as tracking the overall mean square quantization error at each step of the quantization-aware training process identifies fluctuations and divergences. Sudden spikes in the tracking plot may indicate points where quantization is disrupting the model training. Monitoring this metric builds intuition on model behavior under quantization. Together these techniques turn quantization into a transparent process. The empirical insights enable practitioners to properly assess quantization effects. They pinpoint areas of the model architecture or training process to recalibrate based on observed quantization issues. This helps achieve numerically stable and accurate quantized models.</p>
<p>Providing this data enables practitioners to properly assess the impact of quantization and identify potential problem areas of the model to recalibrate or redesign to be more quantization friendly. This empirical analysis builds intuition on achieving optimal quantization.</p>
<p>Visualization tools can provide insights that help practitioners better understand the effects of optimizations on their models. The visibility enables correcting issues early before accuracy or performance is impacted significantly. It also aids applying optimizations more effectively for specific models. These optimization analytics help build intuition when transitioning models to more efficient representations.</p>
</section>
</section>
<section id="model-conversion-and-deployment" class="level3" data-number="9.5.5">
<h3 data-number="9.5.5"><span class="header-section-number">9.5.5</span> Model Conversion and Deployment</h3>
<p>Once models have been successfully optimized in frameworks like TensorFlow and PyTorch, specialized model conversion and deployment platforms are needed to bridge the gap to running them on target devices.</p>
<p>TensorFlow Lite - TensorFlow’s platform to convert models to a lightweight format optimized for mobile, embedded and edge devices. Supports optimizations like quantization, kernel fusion, and stripping away unused ops. Models can be executed using optimized TensorFlow Lite kernels on device hardware. Critical for mobile and TinyML deployment.</p>
<p>ONNX Runtime - Performs model conversion and inference for models in the open ONNX model format. Provides optimized kernels, supports hardware accelerators like GPUs, and cross-platform deployment from cloud to edge. Allows framework-agnostic deployment. <a href="#fig-interop" class="quarto-xref">Figure <span class="quarto-unresolved-ref">fig-interop</span></a> is an ONNX interoperability map, including major popular frameworks.</p>
<div id="fig-interop" class="quarto-float quarto-figure quarto-figure-center">
<figure class="quarto-float quarto-float-fig">
<div aria-describedby="fig-interop-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://miro.medium.com/v2/resize:fit:1400/1*3N6uPaLNEYDjtWBW1vdNoQ.jpeg" class="img-fluid" />
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-interop-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure 9.42: Interoperability of ONNX. Source: <a href="https://towardsdatascience.com/onnx-preventing-framework-lock-in-9a798fb34c92">TowardsDataScience</a>.
</figcaption>
</figure>
</div>
<p>PyTorch Mobile - Enables PyTorch models to be run on iOS and Android by converting to mobile-optimized representations. Provides efficient mobile implementations of ops like convolution and special functions optimized for mobile hardware.</p>
<p>These platforms integrate with hardware drivers, operating systems, and accelerator libraries on devices to execute models efficiently using hardware optimization. They also offload operations to dedicated ML accelerators where present. The availability of these proven, robust deployment platforms bridges the gap between optimizing models in frameworks and actual deployment to billions of devices. They allow users to focus on model development rather than building custom mobile runtimes. Continued innovation to support new hardware and optimizations in these platforms is key to widespread ML optimizations.</p>
<p>By providing these optimized deployment pipelines, the entire workflow from training to device deployment can leverage model optimizations to deliver performant ML applications. This end-to-end software infrastructure has helped drive the adoption of on-device ML.</p>
</section>
</section>
<section id="conclusion" class="level2" data-number="9.6">
<h2 data-number="9.6"><span class="header-section-number">9.6</span> Conclusion</h2>
<p>In this chapter we’ve discussed model optimization across the software-hardware span. We dove deep into efficient model representation, where we covered the nuances of structured and unstructured pruning and other techniques for model compression such as knowledge distillation and matrix and tensor decomposition. We also dove briefly into edge-specific model design at the parameter and model architecture level, exploring topics like edge-specific models and hardware-aware NAS.</p>
<p>We then explored efficient numerics representations, where we covered the basics of numerics, numeric encodings and storage, benefits of efficient numerics, and the nuances of numeric representation with memory usage, computational complexity, hardware compatibility, and tradeoff scenarios. We finished by honing in on an efficient numerics staple: quantization, where we examined its history, calibration, techniques, and interaction with pruning.</p>
<p>Finally, we looked at how we can make optimizations specific to the hardware we have. We explored how we can find model architectures tailored to the hardware, make optimizations in the kernel to better handle the model, and frameworks built to make the most use out of the hardware. We also looked at how we can go the other way around and build hardware around our specific software and talked about splitting networks to run on multiple processors available on the edge device.</p>
<p>By understanding the full picture of the degrees of freedom within model optimization both away and close to the hardware and the tradeoffs to consider when implementing these methods, practitioners can develop a more thoughtful pipeline for compressing their workloads onto edge devices.</p>
</section>
<section id="sec-model-optimizations-resource" class="level2" data-number="9.7">
<h2 data-number="9.7"><span class="header-section-number">9.7</span> Resources</h2>
<p>Here is a curated list of resources to support both students and instructors in their learning and teaching journey. We are continuously working on expanding this collection and will be adding new exercises in the near future.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-6-contents" aria-controls="callout-6" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class='callout-icon'></i>
</div>
<div class="callout-title-container flex-fill">
Slides
</div>
<div class='callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end'><i class='callout-toggle'></i></div>
</div>
<div id="callout-6" class="callout-6-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<p>These slides serve as a valuable tool for instructors to deliver lectures and for students to review the material at their own pace. We encourage both students and instructors to leverage these slides to improve their understanding and facilitate effective knowledge transfer.</p>
<ul>
<li><p>Quantization:</p>
<ul>
<li><p><a href="https://docs.google.com/presentation/d/1GOlLUMkd8OTNvrNj7lDSIGricE-569Nk/edit?usp=drive_link&amp;ouid=102419556060649178683&amp;rtpof=true&amp;sd=true">Quantization: Part 1.</a></p></li>
<li><p><a href="https://docs.google.com/presentation/d/18oLTdwa-dZxbBNpvHzZVyMS8bUbed4ao/edit?usp=drive_link&amp;ouid=102419556060649178683&amp;rtpof=true&amp;sd=true">Quantization: Part 2.</a></p></li>
<li><p><a href="https://docs.google.com/presentation/d/1eSOyAOu8Vg_VfIHZ9gWRVjWnmFTOcZ4FavaNMc4reHQ/edit">Post-Training Quantization (PTQ).</a></p></li>
<li><p><a href="https://docs.google.com/presentation/d/1qvoKLjKadK1abqUuuCCy9gaTynMZivDKLbV2Hjftri8/edit?usp=drive_link">Quantization-Aware Training (QAT).</a></p></li>
</ul></li>
<li><p>Pruning:</p>
<ul>
<li><p><a href="https://docs.google.com/presentation/d/1KX_I71smbztdqycPXBDAYjShinrTtQeF/edit#slide=id.p1">Pruning: Part 1.</a></p></li>
<li><p><a href="https://docs.google.com/presentation/d/1kZGDhnkeRcAw1pz3smO837ftotXQEiO7/edit?usp=drive_link&amp;ouid=102419556060649178683&amp;rtpof=true&amp;sd=true">Pruning: Part 2.</a></p></li>
</ul></li>
<li><p><a href="https://docs.google.com/presentation/d/1SXjA3mCSwKmdouuWoxSk7r-Yjd67RG7i/edit#slide=id.g202a77b5f4f_0_110">Knowledge Distillation.</a></p></li>
<li><p><a href="https://docs.google.com/presentation/d/14K9QFUjiba1NvwG0zobsJdgEklomuM_xeaCP7-5dmY8/edit?usp=drive_link">Clustering.</a></p></li>
<li><p>Neural Architecture Search (NAS):</p>
<ul>
<li><p><a href="https://docs.google.com/presentation/d/1aVGjhj1Q-_JEFHr6CYzPeuMOCiDivzhZCBtg1xV14QM/edit#slide=id.g202a67d8ddf_0_0">NAS overview.</a></p></li>
<li><p><a href="https://docs.google.com/presentation/d/1V-ZD6c8KPrFBrrw8xkAQfkqUu4u53zkX/edit?usp=drive_link&amp;ouid=102419556060649178683&amp;rtpof=true&amp;sd=true">NAS: Part 1.</a></p></li>
<li><p><a href="https://docs.google.com/presentation/d/1VUf9zyGP9yascD87VSit58S494EPnd8D/edit?usp=drive_link&amp;ouid=102419556060649178683&amp;rtpof=true&amp;sd=true">NAS: Part 2.</a></p></li>
</ul></li>
</ul>
</div>
</div>
</div>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-7-contents" aria-controls="callout-7" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class='callout-icon'></i>
</div>
<div class="callout-title-container flex-fill">
Videos
</div>
<div class='callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end'><i class='callout-toggle'></i></div>
</div>
<div id="callout-7" class="callout-7-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<ul>
<li><a href="#vid-quant" class="quarto-xref">Video <span class="quarto-unresolved-ref">vid-quant</span></a></li>
</ul>
</div>
</div>
</div>
<div class="callout callout-style-default callout-caution callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-8-contents" aria-controls="callout-8" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class='callout-icon'></i>
</div>
<div class="callout-title-container flex-fill">
Exercises
</div>
<div class='callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end'><i class='callout-toggle'></i></div>
</div>
<div id="callout-8" class="callout-8-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<p>To reinforce the concepts covered in this chapter, we have curated a set of exercises that challenge students to apply their knowledge and deepen their understanding.</p>
<ul>
<li><p><a href="#exr-p" class="quarto-xref">Exercise <span class="quarto-unresolved-ref">exr-p</span></a></p></li>
<li><p><a href="#exr-mc" class="quarto-xref">Exercise <span class="quarto-unresolved-ref">exr-mc</span></a></p></li>
<li><p><a href="#exr-md" class="quarto-xref">Exercise <span class="quarto-unresolved-ref">exr-md</span></a></p></li>
</ul>
</div>
</div>
</div>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-9-contents" aria-controls="callout-9" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class='callout-icon'></i>
</div>
<div class="callout-title-container flex-fill">
Labs
</div>
<div class='callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end'><i class='callout-toggle'></i></div>
</div>
<div id="callout-9" class="callout-9-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<p>In addition to exercises, we also offer a series of hands-on labs that allow students to gain practical experience with embedded AI technologies. These labs provide step-by-step guidance, enabling students to develop their skills in a structured and supportive environment. We are excited to announce that new labs will be available soon, further enriching the learning experience.</p>
<ul>
<li><em>Coming soon.</em></li>
</ul>
</div>
</div>
</div>
<div id="quarto-navigation-envelope" class="hidden">
<p><span class="hidden quarto-markdown-envelope-contents" data-render-id="cXVhcnRvLWludC1zaWRlYmFyLXRpdGxl">Machine Learning Systems</span> <span class="hidden quarto-markdown-envelope-contents" data-render-id="cXVhcnRvLWludC1uYXZiYXItdGl0bGU=">Machine Learning Systems</span> <span class="hidden quarto-markdown-envelope-contents" data-render-id="cXVhcnRvLWludC1uZXh0"><span class="chapter-number">10</span>  <span class="chapter-title">AI Acceleration</span></span> <span class="hidden quarto-markdown-envelope-contents" data-render-id="cXVhcnRvLWludC1wcmV2"><span class="chapter-number">8</span>  <span class="chapter-title">Efficient AI</span></span> <span class="hidden quarto-markdown-envelope-contents" data-render-id="cXVhcnRvLWludC1zaWRlYmFyOi9pbmRleC5odG1sUHJlZmFjZQ==">Preface</span> <span class="hidden quarto-markdown-envelope-contents" data-render-id="cXVhcnRvLWludC1zaWRlYmFyOi9jb250ZW50cy9jb3B5cmlnaHQuaHRtbENvcHlyaWdodA==">Copyright</span> <span class="hidden quarto-markdown-envelope-contents" data-render-id="cXVhcnRvLWludC1zaWRlYmFyOi9jb250ZW50cy9kZWRpY2F0aW9uLmh0bWxEZWRpY2F0aW9u">Dedication</span> <span class="hidden quarto-markdown-envelope-contents" data-render-id="cXVhcnRvLWludC1zaWRlYmFyOi9jb250ZW50cy9jb3JlL2Fja25vd2xlZGdlbWVudHMvYWNrbm93bGVkZ2VtZW50cy5odG1sQWNrbm93bGVkZ2VtZW50cw==">Acknowledgements</span> <span class="hidden quarto-markdown-envelope-contents" data-render-id="cXVhcnRvLWludC1zaWRlYmFyOi9jb250ZW50cy9jb250cmlidXRvcnMuaHRtbENvbnRyaWJ1dG9ycy0mLVRoYW5rcw==">Contributors &amp; Thanks</span> <span class="hidden quarto-markdown-envelope-contents" data-render-id="cXVhcnRvLWludC1zaWRlYmFyOi9jb250ZW50cy9hYm91dC5odG1sQWJvdXQtdGhlLUJvb2s=">About the Book</span> <span class="hidden quarto-markdown-envelope-contents" data-render-id="cXVhcnRvLWludC1zaWRlYmFyOi9jb250ZW50cy9jb3JlL2ludHJvZHVjdGlvbi9pbnRyb2R1Y3Rpb24uaHRtbDxzcGFuLWNsYXNzPSdjaGFwdGVyLW51bWJlcic+MTwvc3Bhbj4tLTxzcGFuLWNsYXNzPSdjaGFwdGVyLXRpdGxlJz5JbnRyb2R1Y3Rpb248L3NwYW4+"><span class="chapter-number">1</span>  <span class="chapter-title">Introduction</span></span> <span class="hidden quarto-markdown-envelope-contents" data-render-id="cXVhcnRvLWludC1zaWRlYmFyOi9jb250ZW50cy9jb3JlL21sX3N5c3RlbXMvbWxfc3lzdGVtcy5odG1sPHNwYW4tY2xhc3M9J2NoYXB0ZXItbnVtYmVyJz4yPC9zcGFuPi0tPHNwYW4tY2xhc3M9J2NoYXB0ZXItdGl0bGUnPk1MLVN5c3RlbXM8L3NwYW4+"><span class="chapter-number">2</span>  <span class="chapter-title">ML Systems</span></span> <span class="hidden quarto-markdown-envelope-contents" data-render-id="cXVhcnRvLWludC1zaWRlYmFyOi9jb250ZW50cy9jb3JlL2RsX3ByaW1lci9kbF9wcmltZXIuaHRtbDxzcGFuLWNsYXNzPSdjaGFwdGVyLW51bWJlcic+Mzwvc3Bhbj4tLTxzcGFuLWNsYXNzPSdjaGFwdGVyLXRpdGxlJz5ETC1QcmltZXI8L3NwYW4+"><span class="chapter-number">3</span>  <span class="chapter-title">DL Primer</span></span> <span class="hidden quarto-markdown-envelope-contents" data-render-id="cXVhcnRvLWludC1zaWRlYmFyOi9jb250ZW50cy9jb3JlL3dvcmtmbG93L3dvcmtmbG93Lmh0bWw8c3Bhbi1jbGFzcz0nY2hhcHRlci1udW1iZXInPjQ8L3NwYW4+LS08c3Bhbi1jbGFzcz0nY2hhcHRlci10aXRsZSc+QUktV29ya2Zsb3c8L3NwYW4+"><span class="chapter-number">4</span>  <span class="chapter-title">AI Workflow</span></span> <span class="hidden quarto-markdown-envelope-contents" data-render-id="cXVhcnRvLWludC1zaWRlYmFyOi9jb250ZW50cy9jb3JlL2RhdGFfZW5naW5lZXJpbmcvZGF0YV9lbmdpbmVlcmluZy5odG1sPHNwYW4tY2xhc3M9J2NoYXB0ZXItbnVtYmVyJz41PC9zcGFuPi0tPHNwYW4tY2xhc3M9J2NoYXB0ZXItdGl0bGUnPkRhdGEtRW5naW5lZXJpbmc8L3NwYW4+"><span class="chapter-number">5</span>  <span class="chapter-title">Data Engineering</span></span> <span class="hidden quarto-markdown-envelope-contents" data-render-id="cXVhcnRvLWludC1zaWRlYmFyOi9jb250ZW50cy9jb3JlL2ZyYW1ld29ya3MvZnJhbWV3b3Jrcy5odG1sPHNwYW4tY2xhc3M9J2NoYXB0ZXItbnVtYmVyJz42PC9zcGFuPi0tPHNwYW4tY2xhc3M9J2NoYXB0ZXItdGl0bGUnPkFJLUZyYW1ld29ya3M8L3NwYW4+"><span class="chapter-number">6</span>  <span class="chapter-title">AI Frameworks</span></span> <span class="hidden quarto-markdown-envelope-contents" data-render-id="cXVhcnRvLWludC1zaWRlYmFyOi9jb250ZW50cy9jb3JlL3RyYWluaW5nL3RyYWluaW5nLmh0bWw8c3Bhbi1jbGFzcz0nY2hhcHRlci1udW1iZXInPjc8L3NwYW4+LS08c3Bhbi1jbGFzcz0nY2hhcHRlci10aXRsZSc+QUktVHJhaW5pbmc8L3NwYW4+"><span class="chapter-number">7</span>  <span class="chapter-title">AI Training</span></span> <span class="hidden quarto-markdown-envelope-contents" data-render-id="cXVhcnRvLWludC1zaWRlYmFyOi9jb250ZW50cy9jb3JlL2VmZmljaWVudF9haS9lZmZpY2llbnRfYWkuaHRtbDxzcGFuLWNsYXNzPSdjaGFwdGVyLW51bWJlcic+ODwvc3Bhbj4tLTxzcGFuLWNsYXNzPSdjaGFwdGVyLXRpdGxlJz5FZmZpY2llbnQtQUk8L3NwYW4+"><span class="chapter-number">8</span>  <span class="chapter-title">Efficient AI</span></span> <span class="hidden quarto-markdown-envelope-contents" data-render-id="cXVhcnRvLWludC1zaWRlYmFyOi9jb250ZW50cy9jb3JlL29wdGltaXphdGlvbnMvb3B0aW1pemF0aW9ucy5odG1sPHNwYW4tY2xhc3M9J2NoYXB0ZXItbnVtYmVyJz45PC9zcGFuPi0tPHNwYW4tY2xhc3M9J2NoYXB0ZXItdGl0bGUnPk1vZGVsLU9wdGltaXphdGlvbnM8L3NwYW4+"><span class="chapter-number">9</span>  <span class="chapter-title">Model Optimizations</span></span> <span class="hidden quarto-markdown-envelope-contents" data-render-id="cXVhcnRvLWludC1zaWRlYmFyOi9jb250ZW50cy9jb3JlL2h3X2FjY2VsZXJhdGlvbi9od19hY2NlbGVyYXRpb24uaHRtbDxzcGFuLWNsYXNzPSdjaGFwdGVyLW51bWJlcic+MTA8L3NwYW4+LS08c3Bhbi1jbGFzcz0nY2hhcHRlci10aXRsZSc+QUktQWNjZWxlcmF0aW9uPC9zcGFuPg=="><span class="chapter-number">10</span>  <span class="chapter-title">AI Acceleration</span></span> <span class="hidden quarto-markdown-envelope-contents" data-render-id="cXVhcnRvLWludC1zaWRlYmFyOi9jb250ZW50cy9jb3JlL2JlbmNobWFya2luZy9iZW5jaG1hcmtpbmcuaHRtbDxzcGFuLWNsYXNzPSdjaGFwdGVyLW51bWJlcic+MTE8L3NwYW4+LS08c3Bhbi1jbGFzcz0nY2hhcHRlci10aXRsZSc+QmVuY2htYXJraW5nLUFJPC9zcGFuPg=="><span class="chapter-number">11</span>  <span class="chapter-title">Benchmarking AI</span></span> <span class="hidden quarto-markdown-envelope-contents" data-render-id="cXVhcnRvLWludC1zaWRlYmFyOi9jb250ZW50cy9jb3JlL29uZGV2aWNlX2xlYXJuaW5nL29uZGV2aWNlX2xlYXJuaW5nLmh0bWw8c3Bhbi1jbGFzcz0nY2hhcHRlci1udW1iZXInPjEyPC9zcGFuPi0tPHNwYW4tY2xhc3M9J2NoYXB0ZXItdGl0bGUnPk9uLURldmljZS1MZWFybmluZzwvc3Bhbj4="><span class="chapter-number">12</span>  <span class="chapter-title">On-Device Learning</span></span> <span class="hidden quarto-markdown-envelope-contents" data-render-id="cXVhcnRvLWludC1zaWRlYmFyOi9jb250ZW50cy9jb3JlL29wcy9vcHMuaHRtbDxzcGFuLWNsYXNzPSdjaGFwdGVyLW51bWJlcic+MTM8L3NwYW4+LS08c3Bhbi1jbGFzcz0nY2hhcHRlci10aXRsZSc+TUwtT3BlcmF0aW9uczwvc3Bhbj4="><span class="chapter-number">13</span>  <span class="chapter-title">ML Operations</span></span> <span class="hidden quarto-markdown-envelope-contents" data-render-id="cXVhcnRvLWludC1zaWRlYmFyOi9jb250ZW50cy9jb3JlL3ByaXZhY3lfc2VjdXJpdHkvcHJpdmFjeV9zZWN1cml0eS5odG1sPHNwYW4tY2xhc3M9J2NoYXB0ZXItbnVtYmVyJz4xNDwvc3Bhbj4tLTxzcGFuLWNsYXNzPSdjaGFwdGVyLXRpdGxlJz5TZWN1cml0eS0mLVByaXZhY3k8L3NwYW4+"><span class="chapter-number">14</span>  <span class="chapter-title">Security &amp; Privacy</span></span> <span class="hidden quarto-markdown-envelope-contents" data-render-id="cXVhcnRvLWludC1zaWRlYmFyOi9jb250ZW50cy9jb3JlL3Jlc3BvbnNpYmxlX2FpL3Jlc3BvbnNpYmxlX2FpLmh0bWw8c3Bhbi1jbGFzcz0nY2hhcHRlci1udW1iZXInPjE1PC9zcGFuPi0tPHNwYW4tY2xhc3M9J2NoYXB0ZXItdGl0bGUnPlJlc3BvbnNpYmxlLUFJPC9zcGFuPg=="><span class="chapter-number">15</span>  <span class="chapter-title">Responsible AI</span></span> <span class="hidden quarto-markdown-envelope-contents" data-render-id="cXVhcnRvLWludC1zaWRlYmFyOi9jb250ZW50cy9jb3JlL3N1c3RhaW5hYmxlX2FpL3N1c3RhaW5hYmxlX2FpLmh0bWw8c3Bhbi1jbGFzcz0nY2hhcHRlci1udW1iZXInPjE2PC9zcGFuPi0tPHNwYW4tY2xhc3M9J2NoYXB0ZXItdGl0bGUnPlN1c3RhaW5hYmxlLUFJPC9zcGFuPg=="><span class="chapter-number">16</span>  <span class="chapter-title">Sustainable AI</span></span> <span class="hidden quarto-markdown-envelope-contents" data-render-id="cXVhcnRvLWludC1zaWRlYmFyOi9jb250ZW50cy9jb3JlL3JvYnVzdF9haS9yb2J1c3RfYWkuaHRtbDxzcGFuLWNsYXNzPSdjaGFwdGVyLW51bWJlcic+MTc8L3NwYW4+LS08c3Bhbi1jbGFzcz0nY2hhcHRlci10aXRsZSc+Um9idXN0LUFJPC9zcGFuPg=="><span class="chapter-number">17</span>  <span class="chapter-title">Robust AI</span></span> <span class="hidden quarto-markdown-envelope-contents" data-render-id="cXVhcnRvLWludC1zaWRlYmFyOi9jb250ZW50cy9jb3JlL2dlbmVyYXRpdmVfYWkvZ2VuZXJhdGl2ZV9haS5odG1sPHNwYW4tY2xhc3M9J2NoYXB0ZXItbnVtYmVyJz4xODwvc3Bhbj4tLTxzcGFuLWNsYXNzPSdjaGFwdGVyLXRpdGxlJz5HZW5lcmF0aXZlLUFJPC9zcGFuPg=="><span class="chapter-number">18</span>  <span class="chapter-title">Generative AI</span></span> <span class="hidden quarto-markdown-envelope-contents" data-render-id="cXVhcnRvLWludC1zaWRlYmFyOi9jb250ZW50cy9jb3JlL2FpX2Zvcl9nb29kL2FpX2Zvcl9nb29kLmh0bWw8c3Bhbi1jbGFzcz0nY2hhcHRlci1udW1iZXInPjE5PC9zcGFuPi0tPHNwYW4tY2xhc3M9J2NoYXB0ZXItdGl0bGUnPkFJLWZvci1Hb29kPC9zcGFuPg=="><span class="chapter-number">19</span>  <span class="chapter-title">AI for Good</span></span> <span class="hidden quarto-markdown-envelope-contents" data-render-id="cXVhcnRvLWludC1zaWRlYmFyOi9jb250ZW50cy9jb3JlL2NvbmNsdXNpb24vY29uY2x1c2lvbi5odG1sPHNwYW4tY2xhc3M9J2NoYXB0ZXItbnVtYmVyJz4yMDwvc3Bhbj4tLTxzcGFuLWNsYXNzPSdjaGFwdGVyLXRpdGxlJz5Db25jbHVzaW9uPC9zcGFuPg=="><span class="chapter-number">20</span>  <span class="chapter-title">Conclusion</span></span> <span class="hidden quarto-markdown-envelope-contents" data-render-id="cXVhcnRvLWludC1zaWRlYmFyOnVuZGVmaW5lZC0tLQ==">—</span> <span class="hidden quarto-markdown-envelope-contents" data-render-id="cXVhcnRvLWludC1zaWRlYmFyOnF1YXJ0by1zaWRlYmFyLXNlY3Rpb24tMQ==">LABS</span> <span class="hidden quarto-markdown-envelope-contents" data-render-id="cXVhcnRvLWludC1zaWRlYmFyOi9jb250ZW50cy9sYWJzL2xhYnMuaHRtbE92ZXJ2aWV3">Overview</span> <span class="hidden quarto-markdown-envelope-contents" data-render-id="cXVhcnRvLWludC1zaWRlYmFyOi9jb250ZW50cy9sYWJzL2dldHRpbmdfc3RhcnRlZC5odG1sR2V0dGluZy1TdGFydGVk">Getting Started</span> <span class="hidden quarto-markdown-envelope-contents" data-render-id="cXVhcnRvLWludC1zaWRlYmFyOnF1YXJ0by1zaWRlYmFyLXNlY3Rpb24tMg==">Nicla Vision</span> <span class="hidden quarto-markdown-envelope-contents" data-render-id="cXVhcnRvLWludC1zaWRlYmFyOi9jb250ZW50cy9sYWJzL2FyZHVpbm8vbmljbGFfdmlzaW9uL3NldHVwL3NldHVwLmh0bWxTZXR1cA==">Setup</span> <span class="hidden quarto-markdown-envelope-contents" data-render-id="cXVhcnRvLWludC1zaWRlYmFyOi9jb250ZW50cy9sYWJzL2FyZHVpbm8vbmljbGFfdmlzaW9uL2ltYWdlX2NsYXNzaWZpY2F0aW9uL2ltYWdlX2NsYXNzaWZpY2F0aW9uLmh0bWxJbWFnZS1DbGFzc2lmaWNhdGlvbg==">Image Classification</span> <span class="hidden quarto-markdown-envelope-contents" data-render-id="cXVhcnRvLWludC1zaWRlYmFyOi9jb250ZW50cy9sYWJzL2FyZHVpbm8vbmljbGFfdmlzaW9uL29iamVjdF9kZXRlY3Rpb24vb2JqZWN0X2RldGVjdGlvbi5odG1sT2JqZWN0LURldGVjdGlvbg==">Object Detection</span> <span class="hidden quarto-markdown-envelope-contents" data-render-id="cXVhcnRvLWludC1zaWRlYmFyOi9jb250ZW50cy9sYWJzL2FyZHVpbm8vbmljbGFfdmlzaW9uL2t3cy9rd3MuaHRtbEtleXdvcmQtU3BvdHRpbmctKEtXUyk=">Keyword Spotting (KWS)</span> <span class="hidden quarto-markdown-envelope-contents" data-render-id="cXVhcnRvLWludC1zaWRlYmFyOi9jb250ZW50cy9sYWJzL2FyZHVpbm8vbmljbGFfdmlzaW9uL21vdGlvbl9jbGFzc2lmaWNhdGlvbi9tb3Rpb25fY2xhc3NpZmljYXRpb24uaHRtbE1vdGlvbi1DbGFzc2lmaWNhdGlvbi1hbmQtQW5vbWFseS1EZXRlY3Rpb24=">Motion Classification and Anomaly Detection</span> <span class="hidden quarto-markdown-envelope-contents" data-render-id="cXVhcnRvLWludC1zaWRlYmFyOnF1YXJ0by1zaWRlYmFyLXNlY3Rpb24tMw==">XIAO ESP32S3</span> <span class="hidden quarto-markdown-envelope-contents" data-render-id="cXVhcnRvLWludC1zaWRlYmFyOi9jb250ZW50cy9sYWJzL3NlZWVkL3hpYW9fZXNwMzJzMy9zZXR1cC9zZXR1cC5odG1sU2V0dXA=">Setup</span> <span class="hidden quarto-markdown-envelope-contents" data-render-id="cXVhcnRvLWludC1zaWRlYmFyOi9jb250ZW50cy9sYWJzL3NlZWVkL3hpYW9fZXNwMzJzMy9pbWFnZV9jbGFzc2lmaWNhdGlvbi9pbWFnZV9jbGFzc2lmaWNhdGlvbi5odG1sSW1hZ2UtQ2xhc3NpZmljYXRpb24=">Image Classification</span> <span class="hidden quarto-markdown-envelope-contents" data-render-id="cXVhcnRvLWludC1zaWRlYmFyOi9jb250ZW50cy9sYWJzL3NlZWVkL3hpYW9fZXNwMzJzMy9vYmplY3RfZGV0ZWN0aW9uL29iamVjdF9kZXRlY3Rpb24uaHRtbE9iamVjdC1EZXRlY3Rpb24=">Object Detection</span> <span class="hidden quarto-markdown-envelope-contents" data-render-id="cXVhcnRvLWludC1zaWRlYmFyOi9jb250ZW50cy9sYWJzL3NlZWVkL3hpYW9fZXNwMzJzMy9rd3Mva3dzLmh0bWxLZXl3b3JkLVNwb3R0aW5nLShLV1Mp">Keyword Spotting (KWS)</span> <span class="hidden quarto-markdown-envelope-contents" data-render-id="cXVhcnRvLWludC1zaWRlYmFyOi9jb250ZW50cy9sYWJzL3NlZWVkL3hpYW9fZXNwMzJzMy9tb3Rpb25fY2xhc3NpZmljYXRpb24vbW90aW9uX2NsYXNzaWZpY2F0aW9uLmh0bWxNb3Rpb24tQ2xhc3NpZmljYXRpb24tYW5kLUFub21hbHktRGV0ZWN0aW9u">Motion Classification and Anomaly Detection</span> <span class="hidden quarto-markdown-envelope-contents" data-render-id="cXVhcnRvLWludC1zaWRlYmFyOnF1YXJ0by1zaWRlYmFyLXNlY3Rpb24tNA==">Raspberry Pi</span> <span class="hidden quarto-markdown-envelope-contents" data-render-id="cXVhcnRvLWludC1zaWRlYmFyOi9jb250ZW50cy9sYWJzL3Jhc3BpL3NldHVwL3NldHVwLmh0bWxTZXR1cA==">Setup</span> <span class="hidden quarto-markdown-envelope-contents" data-render-id="cXVhcnRvLWludC1zaWRlYmFyOi9jb250ZW50cy9sYWJzL3Jhc3BpL2ltYWdlX2NsYXNzaWZpY2F0aW9uL2ltYWdlX2NsYXNzaWZpY2F0aW9uLmh0bWxJbWFnZS1DbGFzc2lmaWNhdGlvbg==">Image Classification</span> <span class="hidden quarto-markdown-envelope-contents" data-render-id="cXVhcnRvLWludC1zaWRlYmFyOi9jb250ZW50cy9sYWJzL3Jhc3BpL29iamVjdF9kZXRlY3Rpb24vb2JqZWN0X2RldGVjdGlvbi5odG1sT2JqZWN0LURldGVjdGlvbg==">Object Detection</span> <span class="hidden quarto-markdown-envelope-contents" data-render-id="cXVhcnRvLWludC1zaWRlYmFyOi9jb250ZW50cy9sYWJzL3Jhc3BpL2xsbS9sbG0uaHRtbFNtYWxsLUxhbmd1YWdlLU1vZGVscy0oU0xNKQ==">Small Language Models (SLM)</span> <span class="hidden quarto-markdown-envelope-contents" data-render-id="cXVhcnRvLWludC1zaWRlYmFyOnF1YXJ0by1zaWRlYmFyLXNlY3Rpb24tNQ==">Shared Labs</span> <span class="hidden quarto-markdown-envelope-contents" data-render-id="cXVhcnRvLWludC1zaWRlYmFyOi9jb250ZW50cy9sYWJzL3NoYXJlZC9rd3NfZmVhdHVyZV9lbmcva3dzX2ZlYXR1cmVfZW5nLmh0bWxLV1MtRmVhdHVyZS1FbmdpbmVlcmluZw==">KWS Feature Engineering</span> <span class="hidden quarto-markdown-envelope-contents" data-render-id="cXVhcnRvLWludC1zaWRlYmFyOi9jb250ZW50cy9sYWJzL3NoYXJlZC9kc3Bfc3BlY3RyYWxfZmVhdHVyZXNfYmxvY2svZHNwX3NwZWN0cmFsX2ZlYXR1cmVzX2Jsb2NrLmh0bWxEU1AtU3BlY3RyYWwtRmVhdHVyZXM=">DSP Spectral Features</span> <span class="hidden quarto-markdown-envelope-contents" data-render-id="cXVhcnRvLWludC1zaWRlYmFyOnF1YXJ0by1zaWRlYmFyLXNlY3Rpb24tNg==">REFERENCES</span> <span class="hidden quarto-markdown-envelope-contents" data-render-id="cXVhcnRvLWludC1zaWRlYmFyOi9yZWZlcmVuY2VzLmh0bWxSZWZlcmVuY2Vz">References</span> <span class="hidden quarto-markdown-envelope-contents" data-render-id="cXVhcnRvLW5hdmJhci10b29sczpodHRwczovL2dpdGh1Yi5jb20vaGFydmFyZC1lZGdlL2NzMjQ5cl9ib29r">https://github.com/harvard-edge/cs249r_book</span> <span class="hidden quarto-markdown-envelope-contents" data-render-id="cXVhcnRvLW5hdmJhci10b29sczovTWFjaGluZS1MZWFybmluZy1TeXN0ZW1zLnBkZg==">/Machine-Learning-Systems.pdf</span> <span class="hidden quarto-markdown-envelope-contents" data-render-id="cXVhcnRvLWJyZWFkY3J1bWJzLTxzcGFuLWNsYXNzPSdjaGFwdGVyLW51bWJlcic+OTwvc3Bhbj4tLTxzcGFuLWNsYXNzPSdjaGFwdGVyLXRpdGxlJz5Nb2RlbC1PcHRpbWl6YXRpb25zPC9zcGFuPg=="><span class="chapter-number">9</span>  <span class="chapter-title">Model Optimizations</span></span></p>
<div class="hidden quarto-markdown-envelope-contents" data-render-id="Zm9vdGVyLWxlZnQ=">
<p>Written, edited and curated by Prof. Vijay Janapa Reddi (Harvard University)</p>
</div>
<div class="hidden quarto-markdown-envelope-contents" data-render-id="Zm9vdGVyLXJpZ2h0">
<p>This book was built with <a href="https://quarto.org/">Quarto</a>.</p>
</div>
</div>
<div id="quarto-meta-markdown" class="hidden">
<p><span class="hidden quarto-markdown-envelope-contents" data-render-id="cXVhcnRvLW1ldGF0aXRsZQ=="><span id="sec-model_optimizations" class="quarto-section-identifier"><span class="chapter-number">9</span>  <span class="chapter-title">Model Optimizations</span></span> – Machine Learning Systems</span> <span class="hidden quarto-markdown-envelope-contents" data-render-id="cXVhcnRvLXR3aXR0ZXJjYXJkdGl0bGU="><span id="sec-model_optimizations" class="quarto-section-identifier"><span class="chapter-number">9</span>  <span class="chapter-title">Model Optimizations</span></span> – Machine Learning Systems</span> <span class="hidden quarto-markdown-envelope-contents" data-render-id="cXVhcnRvLW9nY2FyZHRpdGxl"><span id="sec-model_optimizations" class="quarto-section-identifier"><span class="chapter-number">9</span>  <span class="chapter-title">Model Optimizations</span></span> – Machine Learning Systems</span> <span class="hidden quarto-markdown-envelope-contents" data-render-id="cXVhcnRvLW1ldGFzaXRlbmFtZQ==">Machine Learning Systems</span> <span class="hidden quarto-markdown-envelope-contents" data-render-id="cXVhcnRvLXR3aXR0ZXJjYXJkZGVzYw=="></span> <span class="hidden quarto-markdown-envelope-contents" data-render-id="cXVhcnRvLW9nY2FyZGRkZXNj"></span></p>
</div>
<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-cai2018proxylessnas" class="csl-entry" role="listitem">
Cai, Han, Ligeng Zhu, and Song Han. 2019. <span>“<span>ProxylessNAS:</span> <span>Direct</span> Neural Architecture Search on Target Task and Hardware.”</span> In <em>7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019</em>. OpenReview.net. <a href="https://openreview.net/forum?id=HylVB3AqYm">https://openreview.net/forum?id=HylVB3AqYm</a>.
</div>
<div id="ref-chu2021discovering" class="csl-entry" role="listitem">
Chu, Grace, Okan Arikan, Gabriel Bender, Weijun Wang, Achille Brighton, Pieter-Jan Kindermans, Hanxiao Liu, Berkin Akin, Suyog Gupta, and Andrew Howard. 2021. <span>“Discovering Multi-Hardware Mobile Models via Architecture Search.”</span> In <em>2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</em>, 3022–31. IEEE. <a href="https://doi.org/10.1109/cvprw53098.2021.00337">https://doi.org/10.1109/cvprw53098.2021.00337</a>.
</div>
<div id="ref-dong2022splitnets" class="csl-entry" role="listitem">
Dong, Xin, Barbara De Salvo, Meng Li, Chiao Liu, Zhongnan Qu, H. T. Kung, and Ziyun Li. 2022. <span>“<span>SplitNets:</span> <span>Designing</span> Neural Architectures for Efficient Distributed Computing on Head-Mounted Systems.”</span> In <em>2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 12549–59. IEEE. <a href="https://doi.org/10.1109/cvpr52688.2022.01223">https://doi.org/10.1109/cvpr52688.2022.01223</a>.
</div>
<div id="ref-fahim2021hls4ml" class="csl-entry" role="listitem">
Fahim, Farah, Benjamin Hawks, Christian Herwig, James Hirschauer, Sergo Jindariani, Nhan Tran, Luca P. Carloni, et al. 2021. <span>“Hls4ml: <span>An</span> Open-Source Codesign Workflow to Empower Scientific Low-Power Machine Learning Devices.”</span> <a href="https://arxiv.org/abs/2103.05579">https://arxiv.org/abs/2103.05579</a>.
</div>
<div id="ref-jonathan2019lottery" class="csl-entry" role="listitem">
Frankle, Jonathan, and Michael Carbin. 2019. <span>“The Lottery Ticket Hypothesis: <span>Finding</span> Sparse, Trainable Neural Networks.”</span> In <em>7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019</em>. OpenReview.net. <a href="https://openreview.net/forum?id=rJl-b3RcF7">https://openreview.net/forum?id=rJl-b3RcF7</a>.
</div>
<div id="ref-gholami2021survey" class="csl-entry" role="listitem">
Gholami, Dong Kim, Mahoney Yao, and Keutzer. 2021. <span>“A Survey of Quantization Methods for Efficient Neural Network Inference).”</span> <em>ArXiv Preprint</em>. <a href="https://arxiv.org/abs/2103.13630">https://arxiv.org/abs/2103.13630</a>.
</div>
<div id="ref-gordon2018morphnet" class="csl-entry" role="listitem">
Gordon, Ariel, Elad Eban, Ofir Nachum, Bo Chen, Hao Wu, Tien-Ju Yang, and Edward Choi. 2018. <span>“<span>MorphNet:</span> <span>Fast</span> &amp;Amp; Simple Resource-Constrained Structure Learning of Deep Networks.”</span> In <em>2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, 1586–95. IEEE. <a href="https://doi.org/10.1109/cvpr.2018.00171">https://doi.org/10.1109/cvpr.2018.00171</a>.
</div>
<div id="ref-gu2023deep" class="csl-entry" role="listitem">
Gu, Ivy. 2023. <span>“Deep Learning Model Compression (Ii) by Ivy Gu Medium.”</span> <a href="https://ivygdy.medium.com/deep-learning-model-compression-ii-546352ea9453">https://ivygdy.medium.com/deep-learning-model-compression-ii-546352ea9453</a>.
</div>
<div id="ref-han2015deep" class="csl-entry" role="listitem">
Han, Song, Huizi Mao, and William J Dally. 2015. <span>“Deep Compression: <span>Compressing</span> Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding.”</span> <em>arXiv Preprint arXiv:1510.00149</em>.
</div>
<div id="ref-hawks2021psandqs" class="csl-entry" role="listitem">
Hawks, Benjamin, Javier Duarte, Nicholas J. Fraser, Alessandro Pappalardo, Nhan Tran, and Yaman Umuroglu. 2021. <span>“Ps and Qs: <span class="nocase">Quantization-aware</span> Pruning for Efficient Low Latency Neural Network Inference.”</span> <em>Frontiers in Artificial Intelligence</em> 4 (July). <a href="https://doi.org/10.3389/frai.2021.676564">https://doi.org/10.3389/frai.2021.676564</a>.
</div>
<div id="ref-hegde2023introduction" class="csl-entry" role="listitem">
Hegde, Sumant. 2023. <span>“An Introduction to Separable Convolutions - Analytics Vidhya.”</span> <a href="https://www.analyticsvidhya.com/blog/2021/11/an-introduction-to-separable-convolutions/">https://www.analyticsvidhya.com/blog/2021/11/an-introduction-to-separable-convolutions/</a>.
</div>
<div id="ref-hinton2015distilling" class="csl-entry" role="listitem">
Hinton, Geoffrey. 2005. <span>“Van <span class="nocase">Nostrand’s</span> Scientific Encyclopedia.”</span> Wiley. <a href="https://doi.org/10.1002/0471743984.vse0673">https://doi.org/10.1002/0471743984.vse0673</a>.
</div>
<div id="ref-yang2020coexploration" class="csl-entry" role="listitem">
Ho Yoon, Jung, Hyung-Suk Jung, Min Hwan Lee, Gun Hwan Kim, Seul Ji Song, Jun Yeong Seok, Kyung Jean Yoon, et al. 2012. <span>“Frontiers in Electronic Materials.”</span> Wiley. <a href="https://doi.org/10.1002/9783527667703.ch67">https://doi.org/10.1002/9783527667703.ch67</a>.
</div>
<div id="ref-howard2017mobilenets" class="csl-entry" role="listitem">
Howard, Andrew G., Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, and Hartwig Adam. 2017. <span>“<span>MobileNets:</span> <span>Efficient</span> Convolutional Neural Networks for Mobile Vision Applications.”</span> <em>ArXiv Preprint</em>. <a href="https://arxiv.org/abs/1704.04861">https://arxiv.org/abs/1704.04861</a>.
</div>
<div id="ref-jiang2019accuracy" class="csl-entry" role="listitem">
Hu, Yang, Jie Jiang, Lifu Zhang, Yunfeng Shi, and Jian Shi. 2023. <span>“Halide Perovskite Semiconductors.”</span> Wiley. <a href="https://doi.org/10.1002/9783527829026.ch13">https://doi.org/10.1002/9783527829026.ch13</a>.
</div>
<div id="ref-iandola2016squeezenet" class="csl-entry" role="listitem">
Iandola, Forrest N, Song Han, Matthew W Moskewicz, Khalid Ashraf, William J Dally, and Kurt Keutzer. 2016. <span>“<span>SqueezeNet:</span> <span class="nocase">Alexnet-level</span> Accuracy with 50x Fewer Parameters and 0.5 <span>MB</span> Model Size.”</span> <em>ArXiv Preprint</em> abs/1602.07360. <a href="https://arxiv.org/abs/1602.07360">https://arxiv.org/abs/1602.07360</a>.
</div>
<div id="ref-intellabs2023knowledge" class="csl-entry" role="listitem">
IntelLabs. 2023. <span>“Knowledge Distillation - Neural Network Distiller.”</span> <a href="https://intellabs.github.io/distiller/knowledge_distillation.html">https://intellabs.github.io/distiller/knowledge_distillation.html</a>.
</div>
<div id="ref-isscc2014computings" class="csl-entry" role="listitem">
Isscc. 2014. <span>“Computing’s Energy Problem (and What We Can Do about It).”</span> <a href="https://ieeexplore.ieee.org/document/6757323">https://ieeexplore.ieee.org/document/6757323</a>.
</div>
<div id="ref-jacob2018quantization" class="csl-entry" role="listitem">
Jacob, Benoit, Skirmantas Kligys, Bo Chen, Menglong Zhu, Matthew Tang, Andrew Howard, Hartwig Adam, and Dmitry Kalenichenko. 2018. <span>“Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference.”</span> In <em>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</em>, 2704–13.
</div>
<div id="ref-koren2009matrix" class="csl-entry" role="listitem">
Koren, Yehuda, Robert Bell, and Chris Volinsky. 2009. <span>“Matrix Factorization Techniques for Recommender Systems.”</span> <em>Computer</em> 42 (8): 30–37. <a href="https://doi.org/10.1109/mc.2009.263">https://doi.org/10.1109/mc.2009.263</a>.
</div>
<div id="ref-krishna2023raman" class="csl-entry" role="listitem">
Krishna, Adithya, Srikanth Rohit Nudurupati, Chandana D G, Pritesh Dwivedi, André van Schaik, Mahesh Mehendale, and Chetan Singh Thakur. 2023. <span>“<span>RAMAN:</span> <span>A</span> Re-Configurable and Sparse <span>TinyML</span> Accelerator for Inference on Edge.”</span> <a href="https://arxiv.org/abs/2306.06493">https://arxiv.org/abs/2306.06493</a>.
</div>
<div id="ref-krishnamoorthi2018quantizing" class="csl-entry" role="listitem">
Krishnamoorthi. 2018. <span>“Quantizing Deep Convolutional Networks for Efficient Inference: <span>A</span> Whitepaper.”</span> <em>ArXiv Preprint</em>. <a href="https://arxiv.org/abs/1806.08342">https://arxiv.org/abs/1806.08342</a>.
</div>
<div id="ref-alexnet2012" class="csl-entry" role="listitem">
Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E. Hinton. 2017. <span>“<span>ImageNet</span> Classification with Deep Convolutional Neural Networks.”</span> Edited by F. Pereira, C. J. Burges, L. Bottou, and K. Q. Weinberger. <em>Commun. ACM</em> 60 (6): 84–90. <a href="https://doi.org/10.1145/3065386">https://doi.org/10.1145/3065386</a>.
</div>
<div id="ref-kuzmin2022fp8" class="csl-entry" role="listitem">
Kuzmin, Andrey, Mart Van Baalen, Yuwei Ren, Markus Nagel, Jorn Peters, and Tijmen Blankevoort. 2022. <span>“<span>FP8</span> Quantization: <span>The</span> Power of the Exponent.”</span> <a href="https://arxiv.org/abs/2208.09225">https://arxiv.org/abs/2208.09225</a>.
</div>
<div id="ref-kwon2021hardwaresoftware" class="csl-entry" role="listitem">
Kwon, Jisu, and Daejin Park. 2021. <span>“<span>Hardware/Software</span> Co-Design for <span>TinyML</span> Voice-Recognition Application on Resource Frugal Edge Devices.”</span> <em>Applied Sciences</em> 11 (22): 11073. <a href="https://doi.org/10.3390/app112211073">https://doi.org/10.3390/app112211073</a>.
</div>
<div id="ref-lai2018cmsisnn" class="csl-entry" role="listitem">
Lai, Liangzhen, Naveen Suda, and Vikas Chandra. 2018. <span>“<span>CMSIS</span>-<span>NN:</span> <span>Efficient</span> Neural Network Kernels for Arm Cortex-m <span>CPUs</span>.”</span> <a href="https://arxiv.org/abs/1801.06601">https://arxiv.org/abs/1801.06601</a>.
</div>
<div id="ref-lin2020mcunet" class="csl-entry" role="listitem">
Lin, Ji, Wei-Ming Chen, Yujun Lin, John Cohn, Chuang Gan, and Song Han. 2020. <span>“<span>MCUNet:</span> <span>Tiny</span> Deep Learning on <span>IoT</span> Devices.”</span> In <em>Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, Virtual</em>, edited by Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin. <a href="https://proceedings.neurips.cc/paper/2020/hash/86c51678350f656dcc7f490a43946ee5-Abstract.html">https://proceedings.neurips.cc/paper/2020/hash/86c51678350f656dcc7f490a43946ee5-Abstract.html</a>.
</div>
<div id="ref-lin2023awq" class="csl-entry" role="listitem">
Lin, Tang Tang, Dang Yang, and Han Gan. 2023. <span>“<span>AWQ:</span> <span class="nocase">Activation-aware</span> Weight Quantization for <span>LLM</span> Compression and Acceleration.”</span> <em>ArXiv Preprint</em>. <a href="https://arxiv.org/abs/2306.00978">https://arxiv.org/abs/2306.00978</a>.
</div>
<div id="ref-lubana2020gradient" class="csl-entry" role="listitem">
Lubana, Ekdeep Singh, and Robert P Dick. 2020. <span>“A Gradient Flow Framework for Analyzing Network Pruning.”</span> <em>arXiv Preprint arXiv:2009.11839</em>.
</div>
<div id="ref-prakash2022cfu" class="csl-entry" role="listitem">
Prakash, Shvetank, Tim Callahan, Joseph Bushagour, Colby Banbury, Alan V. Green, Pete Warden, Tim Ansell, and Vijay Janapa Reddi. 2023. <span>“<span>CFU</span> Playground: <span class="nocase">Full-stack</span> Open-Source Framework for Tiny Machine Learning <span>(TinyML)</span> Acceleration on <span>FPGAs</span>.”</span> In <em>2023 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS)</em>. Vol. abs/2201.01863. IEEE. <a href="https://doi.org/10.1109/ispass57527.2023.00024">https://doi.org/10.1109/ispass57527.2023.00024</a>.
</div>
<div id="ref-qi2021efficient" class="csl-entry" role="listitem">
Qi, Chen, Shibo Shen, Rongpeng Li, Zhifeng Zhao, Qing Liu, Jing Liang, and Honggang Zhang. 2021. <span>“An Efficient Pruning Scheme of Deep Neural Networks for Internet of Things Applications.”</span> <em>EURASIP Journal on Advances in Signal Processing</em> 2021 (1): 31. <a href="https://doi.org/10.1186/s13634-021-00744-4">https://doi.org/10.1186/s13634-021-00744-4</a>.
</div>
<div id="ref-rachwan2022winning" class="csl-entry" role="listitem">
Rachwan, John, Daniel Zügner, Bertrand Charpentier, Simon Geisler, Morgane Ayle, and Stephan Günnemann. 2022. <span>“Winning the Lottery Ahead of Time: <span>Efficient</span> Early Network Pruning.”</span> In <em>International Conference on Machine Learning</em>, 18293–309. PMLR.
</div>
<div id="ref-sheng2019qbert" class="csl-entry" role="listitem">
Shen, Sheng, Zhen Dong, Jiayu Ye, Linjian Ma, Zhewei Yao, Amir Gholami, Michael W. Mahoney, and Kurt Keutzer. 2020. <span>“Q-<span>BERT:</span> <span>Hessian</span> Based Ultra Low Precision Quantization of <span>BERT</span>.”</span> <em>Proceedings of the AAAI Conference on Artificial Intelligence</em> 34 (05): 8815–21. <a href="https://doi.org/10.1609/aaai.v34i05.6409">https://doi.org/10.1609/aaai.v34i05.6409</a>.
</div>
<div id="ref-tan2019mnasnet" class="csl-entry" role="listitem">
Tan, Mingxing, Bo Chen, Ruoming Pang, Vijay Vasudevan, Mark Sandler, Andrew Howard, and Quoc V. Le. 2019. <span>“<span>MnasNet:</span> <span class="nocase">Platform-aware</span> Neural Architecture Search for Mobile.”</span> In <em>2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2820–28. IEEE. <a href="https://doi.org/10.1109/cvpr.2019.00293">https://doi.org/10.1109/cvpr.2019.00293</a>.
</div>
<div id="ref-tan2020efficientnet" class="csl-entry" role="listitem">
Tan, Mingxing, and Quoc V. Le. 2023. <span>“Demystifying Deep Learning.”</span> Wiley. <a href="https://doi.org/10.1002/9781394205639.ch6">https://doi.org/10.1002/9781394205639.ch6</a>.
</div>
<div id="ref-ultimate" class="csl-entry" role="listitem">
<span>“The Ultimate Guide to Deep Learning Model Quantization and Quantization-Aware Training.”</span> n.d. <a href="https://deci.ai/quantization-and-quantization-aware-training/">https://deci.ai/quantization-and-quantization-aware-training/</a>.
</div>
<div id="ref-tran2022pruning" class="csl-entry" role="listitem">
Tran, Cuong, Ferdinando Fioretto, Jung-Eun Kim, and Rakshit Naidu. 2022. <span>“Pruning Has a Disparate Impact on Model Accuracy.”</span> <em>Adv Neural Inf Process Syst</em> 35: 17652–64.
</div>
<div id="ref-vaswani2017attention" class="csl-entry" role="listitem">
Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. <span>“Attention Is All You Need.”</span> <em>Adv Neural Inf Process Syst</em> 30.
</div>
<div id="ref-annette2020" class="csl-entry" role="listitem">
Wess, Matthias, Matvey Ivanov, Christoph Unger, and Anvesh Nookala. 2020. <span>“<span>ANNETTE:</span> <span>Accurate</span> Neural Network Execution Time Estimation with Stacked Models.”</span> <em>IEEE</em>. <a href="https://doi.org/10.1109/ACCESS.2020.3047259">https://doi.org/10.1109/ACCESS.2020.3047259</a>.
</div>
<div id="ref-wu2019fbnet" class="csl-entry" role="listitem">
Wu, Bichen, Kurt Keutzer, Xiaoliang Dai, Peizhao Zhang, Yanghan Wang, Fei Sun, Yiming Wu, Yuandong Tian, Peter Vajda, and Yangqing Jia. 2019. <span>“<span>FBNet:</span> <span class="nocase">Hardware-aware</span> Efficient <span>ConvNet</span> Design via Differentiable Neural Architecture Search.”</span> In <em>2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 10734–42. IEEE. <a href="https://doi.org/10.1109/cvpr.2019.01099">https://doi.org/10.1109/cvpr.2019.01099</a>.
</div>
<div id="ref-wu2020integer" class="csl-entry" role="listitem">
Wu, Zhang Judd, and Micikevicius Isaev. 2020. <span>“Integer Quantization for Deep Learning Inference: <span>Principles</span> and Empirical Evaluation).”</span> <em>ArXiv Preprint</em>. <a href="https://arxiv.org/abs/2004.09602">https://arxiv.org/abs/2004.09602</a>.
</div>
<div id="ref-xiao2022smoothquant" class="csl-entry" role="listitem">
Xiao, Seznec Lin, Demouth Wu, and Han. 2022. <span>“<span>SmoothQuant:</span> <span>Accurate</span> and Efficient Post-Training Quantization for Large Language Models.”</span> <em>ArXiv Preprint</em>. <a href="https://arxiv.org/abs/2211.10438">https://arxiv.org/abs/2211.10438</a>.
</div>
<div id="ref-xinyu" class="csl-entry" role="listitem">
Xinyu, Chen. n.d.
</div>
<div id="ref-xu2018alternating" class="csl-entry" role="listitem">
Xu, Chen, Jianqiang Yao, Zhouchen Lin, Wenwu Ou, Yuanbin Cao, Zhirong Wang, and Hongbin Zha. 2018. <span>“Alternating Multi-Bit Quantization for Recurrent Neural Networks.”</span> In <em>6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings</em>. OpenReview.net. <a href="https://openreview.net/forum?id=S19dR9x0b">https://openreview.net/forum?id=S19dR9x0b</a>.
</div>
<div id="ref-yao2021hawq" class="csl-entry" role="listitem">
Yao, Zhewei, Zhen Dong, Zhangcheng Zheng, Amir Gholami, Jiali Yu, Eric Tan, Leyuan Wang, et al. 2021. <span>“Hawq-V3: <span>Dyadic</span> Neural Network Quantization.”</span> In <em>International Conference on Machine Learning</em>, 11875–86. PMLR.
</div>
<div id="ref-zhang2020fast" class="csl-entry" role="listitem">
Zhang, Li Lyna, Yuqing Yang, Yuhang Jiang, Wenwu Zhu, and Yunxin Liu. 2020. <span>“Fast Hardware-Aware Neural Architecture Search.”</span> In <em>2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</em>. IEEE. <a href="https://doi.org/10.1109/cvprw50498.2020.00354">https://doi.org/10.1109/cvprw50498.2020.00354</a>.
</div>
<div id="ref-zhang2019autoshrink" class="csl-entry" role="listitem">
Zhang, Tunhou, Hsin-Pai Cheng, Zhenwen Li, Feng Yan, Chengyu Huang, Hai Helen Li, and Yiran Chen. 2020. <span>“<span>AutoShrink:</span> <span>A</span> Topology-Aware <span>NAS</span> for Discovering Efficient Neural Architecture.”</span> In <em>The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, the Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, the Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020</em>, 6829–36. AAAI Press. <a href="https://aaai.org/ojs/index.php/AAAI/article/view/6163">https://aaai.org/ojs/index.php/AAAI/article/view/6163</a>.
</div>
<div id="ref-zhou2021analognets" class="csl-entry" role="listitem">
Zhou, Chuteng, Fernando Garcia Redondo, Julian Büchel, Irem Boybat, Xavier Timoneda Comas, S. R. Nandakumar, Shidhartha Das, Abu Sebastian, Manuel Le Gallo, and Paul N. Whatmough. 2021. <span>“<span>AnalogNets:</span> <span class="nocase">Ml-hw</span> Co-Design of Noise-Robust <span>TinyML</span> Models and Always-on Analog Compute-in-Memory Accelerator.”</span> <a href="https://arxiv.org/abs/2111.06503">https://arxiv.org/abs/2111.06503</a>.
</div>
</div>
</section>

</main> <!-- /main -->
<script id = "quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
          // target, if specified
          link.setAttribute("target", "_blank");
          if (link.getAttribute("rel") === null) {
            link.setAttribute("rel", "noopener");
          }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
});
</script>
<script src="https://giscus.app/client.js"
        data-repo="harvard-edge/cs249r_book"
        data-repo-id="R_kgDOKQSOaw"
        data-category="General"
        data-category-id="DIC_kwDOKQSOa84CZ8Ry"
        data-mapping="title"
        data-reactions-enabled="1"
        data-emit-metadata="0"
        data-input-position="top"
        data-theme="light"
        data-lang="en"
        crossorigin="anonymous"
        async>
</script>
<input type="hidden" id="giscus-base-theme" value="light">
<input type="hidden" id="giscus-alt-theme" value="dark">
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a  href="/contents/core/efficient_ai/efficient_ai.html" class="pagination-link" aria-label="Efficient AI">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class='chapter-number'>8</span>  <span class='chapter-title'>Efficient AI</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a  href="/contents/core/hw_acceleration/hw_acceleration.html" class="pagination-link" aria-label="AI Acceleration">
        <span class="nav-page-text"><span class='chapter-number'>10</span>  <span class='chapter-title'>AI Acceleration</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      <div class='footer-contents'>Written, edited and curated by Prof. Vijay Janapa Reddi (Harvard University)
</div>  
    </div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">
      <div class='footer-contents'>This book was built with <a href="https://quarto.org/">Quarto</a>.
</div>  
    </div>
  </div>
</footer>

</body>

</html>
